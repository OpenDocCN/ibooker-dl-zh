<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">11 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/>Contextualizing prompts with retrieval-augmented generation</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-218"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">How RAG works</li>
<li class="co-summary-bullet">Using tooling to create a basic RAG setup</li>
<li class="co-summary-bullet">Integrating vector databases into a RAG setup</li>
</ul>
<p class="body">As we learned in the previous chapter, one of the challenges of working with large language models (LLMs) is that they lack visibility of our context. In the second part of this book, we saw different ways in which we can arrange our prompts to help provide small insights into our context. However, these types of prompts are only useful before the lack of extra context leads to less valuable responses. Therefore, to increase the value of an LLM’s response, we need to place more contextual detail into our prompt. In this chapter, we’ll explore how to do this through retrieval-augmented generation, or RAG. We’ll learn how RAG works, why it’s beneficial, and how it’s not a big jump from prompt engineering to building our own RAG framework examples to establish our understanding of how they can help us in a testing context.</p>
<h2 class="fm-head" id="heading_id_3">11.1 Extending prompts with RAG</h2>
<p class="body">To recap, RAG is an approach to improving the quality of an LLM’s response by combining existing corpus of data with a prompt. Although this broadly explains how RAG works, we need to dig a little deeper to better grasp how this combination of data is achieved. The process of a RAG system is relatively straightforward and can be summarized as shown in figure 11.1.<a id="idIndexMarker001"/><a id="marker-219"/><a id="idIndexMarker002"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre43" src="../../OEBPS/Images/CH11_F01_Winteringham2.png"/></p>
<p class="figurecaption">Figure 11.1 A visualization of how a basic RAG system works</p>
</div>
<p class="body">We start with a user input, which would be some sort of query. For example, we might send to our RAG system a query such as “I want to test ideas for deleting bookings.” This query is then sent to a library or tool that will examine our corpus of data for items within the data relevant to our query. In our example, this might be a collection of user stories that define each feature in the system. The library or tool would determine which user stories are most relevant and then return them to be added to a prompt:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">You are a bot that makes recommendations for testing ideas. You answer with suggested risks to test for, based on the provided user story.</p>
<p class="body-table-space">  </p>
<p class="body-table">This is the user story: <code class="fm-code-in-text1">{relevant_document}</code></p>
<p class="body-table">The user input is: <code class="fm-code-in-text1">{user_input}</code></p>
<p class="body-table-space">  </p>
<p class="body-table">Compile a list of suggested risks to test for, based on the user story and the user input.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">The LLM will then consume both the user query and the relevant documents to return a response that is more accurate than if we sent the query “I want test ideas for deleting bookings” directly to an LLM.</p>
<p class="body">By providing data at the <code class="fm-code-in-text">{relevant_document}</code> point within a prompt relevant to the initial query at the <code class="fm-code-in-text">{user_input}</code> point, we get a response that has increased accuracy and value. But it does raise a question of why it would go about finding relevant data in the first place. Can’t we just send the data we have in each prompt and remove the need to do a relevancy check? Targeting what documents we add to a prompt is important for a few reasons. First, consider the size of the prompt we can create. The size of a prompt we can send to an LLM depends on its maximum sequence length or context window. A context window defines how many words, or tokens, can be processed by an LLM. If we add more tokens than the context window allows, then the LLM will either cut off the excess tokens at the end of the prompt (resulting in a partially completed prompt) or return an error. To put this in real terms, Llama-2, Meta’s open-source LLM, has a default context window of 4096 tokens, which is about the average equivalent of 10 pages in a book. This might feel like a lot initially, but it’s not unusual for our testing and development artifacts (for example, user stories, test scripts, code) to be much larger.<a id="marker-220"/><a id="idIndexMarker003"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Tokens and enterprise AI costs</p>
<p class="fm-sidebar-text">How many tokens we send in a prompt is an important consideration if we’re working with models that charge based on the number of tokens that are sent. For example, at the time of writing, the gpt-4 turbo model, which has a context window of 128k, charges $10 per 1 million tokens. So, if we were attempting to max out the context window for each prompt, we would be paying approximately $1.28 per prompt, which would drain our budget fast. Therefore, efficient RAG prompting can be as much about keeping the bills down as it is about getting the most accurate response back from an LLM.</p>
</div>
<p class="body">New LLMs appear with much larger context windows that could potentially solve the problem of a prompt size. However, this leads us to our next reason for using relevancy searches—accuracy. If we were to use a larger context window, such as the 128k context window of gpt-4, we might be tempted to add more contextual data. But it runs the risk of diluting the quality of the response from an LLM. The more data we provide, the more potential noise we add to the prompt for the LLM to parse, which may lead to more generalized or unwanted responses. It can also make debugging of prompts and responses harder. As we’ve explored multiple times in previous chapters, we want to create the right type of prompts that maximize the chances of a desirable response. Therefore, targeting specific information to provide in a prompt can increase that chance, which means striking a balance between not too much context as to water down a response and not too little context as to miss out on important details.</p>
<p class="body">Finally, by storing the corpus of data separately from our prompt generation and LLM, we have better control over said data, which allows us to update the stored data as required. Although vector databases (something we’ll explore in detail later in the chapter) have become a tool that is synonymous with RAG platforms, we can use any source of data we like. As long as we can find the relevant data to be added to our prompt, RAG offers a lot of freedom in accessing data for additional context.<a id="idIndexMarker004"/><a id="idIndexMarker005"/></p>
<h2 class="fm-head" id="heading_id_4">11.2 Building a RAG setup</h2>
<p class="body">Now that we learned how RAG frameworks work, to better appreciate the parts, let’s see how to build a basic RAG setup. We’ll do this by creating a framework that will execute the following steps:<a id="idIndexMarker006"/><a id="marker-221"/></p>
<ol class="calibre18">
<li class="fm-list-bullet">
<p class="list">Ingest a collection of text documents containing user stories.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Query the collection of user stories and find the most relevant document based on a user query.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Add the relevant document and user query to a prompt and send it to gpt-3.5-turbo via the OpenAI platform.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Parse the response and output the details of what the LLM returned.</p>
</li>
</ol>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Activity 11.1</p>
<p class="fm-sidebar-text">In this part of the chapter, we’ll go through the steps required to build a basic example of a RAG system. If you would like to follow along with the example and build your own, download the initial code required for this framework from <a class="url" href="https://mng.bz/gAlR">https://mng.bz/gAlR</a>. All the necessary supporting code can be found in the repository, as well as a completed version of the RAG framework stored in <code class="fm-code-in-text1">CompletedRAGDemo</code> for reference.</p>
</div>
<h3 class="fm-head1" id="heading_id_5">11.2.1 Building our RAG framework</h3>
<p class="body">We’ll begin with a partially completed project that can also be found in the example framework code on GitHub. The project contains the following info to help us get started:<a id="idIndexMarker007"/></p>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">A corpus of data that can be found in <code class="fm-code-in-text">resources/data</code></p>
</li>
<li class="fm-list-bullet">
<p class="list">The necessary dependencies required to build and run our RAG framework in <code class="fm-code-in-text">pom.xml</code></p>
</li>
<li class="fm-list-bullet">
<p class="list"><code class="fm-code-in-text">ActivityRAGDemo</code>, which contains an empty <code class="fm-code-in-text">main</code> method where we’ll add our framework</p>
</li>
</ul>
<p class="body">Before we begin building our RAG framework, let’s review the dependencies stored in our <code class="fm-code-in-text">pom.xml</code> file we’ll be using. These libraries will help us parse our documents and send our prompt to the OpenAI platform:</p>
<pre class="programlisting">&lt;dependencies&gt;
 
    &lt;dependency&gt;                                     <span class="fm-combinumeral">❶</span>
        &lt;groupId&gt;commons-io&lt;/groupId&gt;                <span class="fm-combinumeral">❶</span>
        &lt;artifactId&gt;commons-io&lt;/artifactId&gt;          <span class="fm-combinumeral">❶</span>
        &lt;version&gt;2.16.1&lt;/version&gt;                    <span class="fm-combinumeral">❶</span>
    &lt;/dependency&gt;
 
    &lt;dependency&gt;                                     <span class="fm-combinumeral">❷</span>
        &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;        <span class="fm-combinumeral">❷</span>
        &lt;artifactId&gt;commons-text&lt;/artifactId&gt;        <span class="fm-combinumeral">❷</span>
        &lt;version&gt;1.12.0&lt;/version&gt;                    <span class="fm-combinumeral">❷</span>
    &lt;/dependency&gt;                                    <span class="fm-combinumeral">❷</span>
 
&lt;dependency&gt;                                         <span class="fm-combinumeral">❸</span>
    &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt;               <span class="fm-combinumeral">❸</span>
    &lt;artifactId&gt;langchain4j-open-ai&lt;/artifactId&gt;     <span class="fm-combinumeral">❸</span>
    &lt;version&gt;0.31.0&lt;/version&gt;                        <span class="fm-combinumeral">❸</span>
&lt;/dependency&gt;                                        <span class="fm-combinumeral">❸</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">❶</span> Adds all the user story text files into a string collection</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❷</span> Provides the functionality to do a similarity check on string collection</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❸</span> Sends our prompt to the OpenAI platform</p>
<p class="body">With our dependencies in place, we now need to import the collection of user stories stored in each text file. Each user story focuses on a specific API endpoint for the sandbox application restful-booker-platform (<a class="url" href="https://mng.bz/5Oy1">https://mng.bz/5Oy1</a>). Here’s an example:</p>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">As a guest in order to cancel my booking I want to be able to send a DELETE request with the booking ID.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Acceptance Criteria:</p>
<ul class="calibre17">
<li class="fm-list-bullet">
<p class="list">The endpoint should accept a booking ID as a parameter in the path.</p>
</li>
<li class="fm-list-bullet">
<p class="list">If a valid booking ID is provided, the server should cancel the booking and respond with a status of “OK” (200).</p>
</li>
<li class="fm-list-bullet">
<p class="list">If the booking ID is invalid or missing, the server should respond with a “Bad Request” error (400).</p>
</li>
<li class="fm-list-bullet">
<p class="list">Optionally, a token can be provided in the cookie for authentication.</p>
</li>
</ul>
</li>
</ul>
<p class="body">These user stories have been synthetically generated for this project’s purpose, but we could imagine that this data could have been extracted from a project management platform, a test management tool, or any type of structured data that we feel is relevant, from monitoring metrics to wiki entries.<a id="marker-222"/></p>
<p class="body">To pull in our user stories, we first need to add the following method to our <code class="fm-code-in-text">ActivityRAGDemo</code> class:<a id="idIndexMarker008"/></p>
<pre class="programlisting">public static List&lt;String&gt; loadFilesFromResources(String folderPath)        <span class="fm-combinumeral">❶</span>
<span class="fm-code-continuation-arrow">➥</span>throws IOException {                                                      <span class="fm-combinumeral">❶</span>
    List&lt;String&gt; fileContents = new ArrayList&lt;&gt;();                          <span class="fm-combinumeral">❶</span>
  
    ClassLoader classLoader = CompletedRAGDemo.class.getClassLoader();      <span class="fm-combinumeral">❷</span>
    File folder = new File(classLoader.getResource(folderPath).getFile());  <span class="fm-combinumeral">❷</span>
  
  
    for (File file : folder.listFiles()) {                                  <span class="fm-combinumeral">❸</span>
        if (file.isFile()) {                                                <span class="fm-combinumeral">❸</span>
            String fileContent = FileUtils.readFileToString(file, "UTF-8"); <span class="fm-combinumeral">❸</span>
                fileContents.add(fileContent);                              <span class="fm-combinumeral">❸</span>
        }                                                                   <span class="fm-combinumeral">❸</span>
    }                                                                       <span class="fm-combinumeral">❸</span>
  
  
    return fileContents;                                                    <span class="fm-combinumeral">❹</span>
}</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">❶</span> Takes the folder location as a parameter</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❷</span> Locates the folder within resources</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❸</span> Iterates through each file in the folder and its contents to a list</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❹</span> Returns the list of file contents for further use</p>
<p class="body">The method <code class="fm-code-in-text">loadFilesFromResources</code> gives us the ability to load all the user story files into a list of strings that we can later query. To test whether this has worked, we create a <code class="fm-code-in-text">main</code> method that we can execute to run our RAG setup:<a id="idIndexMarker009"/></p>
<pre class="programlisting">public static void main(String[] args) throws Exception {
  
    List&lt;String&gt; corpus = loadFilesFromResources("data"); <span class="fm-combinumeral">❶</span>
  
    System.out.println(corpus.get(0));                    <span class="fm-combinumeral">❷</span>
}</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">❶</span> Loads files from the data folder inside of resources</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❷</span> Prints out the first file in the collection</p>
<p class="body">After running this code within our IDE, we’ll see the following result output to confirm that our user stories are indeed being added to a list for future querying:<a id="marker-223"/></p>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">As a guest in order to update branding information I want to be able to send a PUT request to /branding/ with necessary parameters.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Acceptance criteria</p>
<ul class="calibre17">
<li class="fm-list-bullet">
<p class="list">I should be able to send a PUT request to /branding/ with the necessary parameters including the branding information in the request body and an optional token in the cookie.</p>
</li>
<li class="fm-list-bullet">
<p class="list">If the request is successful, the response status should be 200 OK.</p>
</li>
<li class="fm-list-bullet">
<p class="list">If the request is unsuccessful due to bad parameters or missing data, the response status should be 400 Bad Request.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The request body should contain valid JSON data conforming to the schema defined in the Swagger JSON.</p>
</li>
</ul>
</li>
</ul>
<p class="body">Next, we want to consider the prompt that we’ll be sending to gpt3.5-turbo. We’ll utilize some of the tactics that should now feel comfortable to us in the following prompt:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">You are an expert software tester that makes recommendations for testing ideas and risks. You answer with suggested risks to test for, based on the provided user story delimited by three hashes and user input that is delimited by three backticks.</p>
<p class="body-table">Compile a list of suggested risks to test for, based on the user story and the user input.</p>
<p class="body-table">###</p>
<p class="body-table">{relevant_document}</p>
<p class="body-table">###</p>
<p class="body-table">```</p>
<p class="body-table">{user_input}</p>
<p class="body-table">```</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Notice how we’ve parameterized the relevant document and user input sections. Eventually, our code will be replacing these two sections with the relevant documentation and our initial query for processing. We’ll come to that shortly, but first, we need to add the prompt to our code base:</p>
<pre class="programlisting">public static void main(String[] args) throws Exception {
  
    List&lt;String&gt; corpus = loadFilesFromResources("data");               <span class="fm-combinumeral">❶</span>
 
 
    String prompt = """                                                 <span class="fm-combinumeral">❷</span>
    You are an expert software tester that makes recommendations for    <span class="fm-combinumeral">❷</span>
    testing ideas and risks. You answer with suggested risks to test    <span class="fm-combinumeral">❷</span>
    for based on the provided user story delimited by three hashes and  <span class="fm-combinumeral">❷</span>
    user input that is delimited by three backticks.                    <span class="fm-combinumeral">❷</span>
 
    Compile a list of suggested risks to test for, based on the user    <span class="fm-combinumeral">❷</span>
    story and the user input.                                           <span class="fm-combinumeral">❷</span>
    ###                                                                 <span class="fm-combinumeral">❷</span>
    {relevant_document}                                                 <span class="fm-combinumeral">❷</span>
    ###                                                                 <span class="fm-combinumeral">❷</span>
    ```                                                                 <span class="fm-combinumeral">❷</span>
    {user_input}                                                        <span class="fm-combinumeral">❷</span>
    ```                                                                 <span class="fm-combinumeral">❷</span>
    """;                                                                <span class="fm-combinumeral">❷</span>
}</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">❶</span> Loads files from the resources folder</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❷</span> Defines the prompt to send to OpenAI</p>
<p class="body"><a id="marker-224"/>Our next step is to work out which of our user stories is most relevant to the query that we will eventually be inputting. For this, we’ll be using the Apache <code class="fm-code-in-text">commons-text</code> library, which offers a range of different relevancy tools such as Levenshtein distance, Jaccard similarity, and the one we’ll be using—Cosine distance. How these different similarity tools work is beyond the scope of this book, but it’s worth noting that this area of RAG can influence what data is returned. Various similarity algorithms work in different ways and can become quite complex in production-ready RAG systems. Still, it’s worthwhile experimenting with basic approaches to gain a sense of how this part of a RAG system works, so we’ll create our similarity-matching method and add it into our class:<a id="idIndexMarker010"/></p>
<pre class="programlisting">public static String findClosestMatch(List&lt;String&gt; list, String query) { <span class="fm-combinumeral">❶</span>
    String closestMatch = null;                                          <span class="fm-combinumeral">❶</span>
    double minDistance = Double.MAX_VALUE;                               <span class="fm-combinumeral">❶</span>
    CosineDistance cosineDistance = new CosineDistance();                <span class="fm-combinumeral">❶</span>
 
 
    for (String item : list) {                                           <span class="fm-combinumeral">❶</span>
 
        double distance = cosineDistance.apply(item, query);             <span class="fm-combinumeral">❷</span>
 
        if (distance &lt; minDistance) {                                    <span class="fm-combinumeral">❸</span>
            minDistance = distance;                                      <span class="fm-combinumeral">❸</span>
            closestMatch = item;                                         <span class="fm-combinumeral">❸</span>
        }                                                                <span class="fm-combinumeral">❸</span>
    }   
 
 
    return closestMatch;                                                 <span class="fm-combinumeral">❹</span>
}</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">❶</span> Takes the list of user stories and the user query as params</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❷</span> Uses cosineDistance to generate a similarity score</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❸</span> Checks whether the current score is lower than the currently most relevant score</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❹</span> Returns the entry in the list that is the closest match</p>
<p class="body">The method loops through each document and uses <code class="fm-code-in-text">cosineDistance</code> to work out a similarity score. The lower the score, the more similar the document is to the query. The lowest-scoring document is eventually the one that is returned to us for use in our prompt.<a id="marker-225"/><a id="idIndexMarker011"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Working with different types of relevancy algorithms</p>
<p class="fm-sidebar-text"><code class="fm-code-in-text1">cosineDistance</code> is just one of many different tools we can use to determine relevancy, and each has its own pros and cons. We’ll look at further tools later in this chapter to improve the relevancy search, but for now, <code class="fm-code-in-text1">cosineDistance</code> will help us build a working prototype that we can iterate on.</p>
</div>
<p class="body">Now we can create the necessary code to complete our prompt generation. To do this, we expand our <code class="fm-code-in-text">main</code> method to first allow a user to enter their query and then do a similarity check before adding it all into a prompt:</p>
<pre class="programlisting">public static void main(String[] args) throws Exception {
 
    System.out.println("What would you like help with?");                     <span class="fm-combinumeral">❶</span>
    Scanner in = new Scanner(System.in);                                      <span class="fm-combinumeral">❶</span>
    String userInput = in.nextLine();                                         <span class="fm-combinumeral">❶</span>
 
 
    List&lt;String&gt; corpus = loadFilesFromResources("data");                     <span class="fm-combinumeral">❷</span>
 
    String prompt = """                                                       <span class="fm-combinumeral">❷</span>
            You are an expert software tester that makes recommendations      <span class="fm-combinumeral">❷</span>
            for testing ideas and risks. You answer with suggested risks to   <span class="fm-combinumeral">❷</span>
            test for, based on the provided user story delimited by three     <span class="fm-combinumeral">❷</span>
            hashes and user input that is delimited by three backticks.       <span class="fm-combinumeral">❷</span>
 
            Compile a list of suggested risks based on the user story         <span class="fm-combinumeral">❷</span>
            provided to test for, based on the user story and the user input. <span class="fm-combinumeral">❷</span>
            Cite which part of the user story the risk is based on. Check     <span class="fm-combinumeral">❷</span>
            that the risk matches the part of the user story before           <span class="fm-combinumeral">❷</span>
            outputting.                                                       <span class="fm-combinumeral">❷</span>
 
            ###                                                               <span class="fm-combinumeral">❷</span>
            {relevant_document}                                               <span class="fm-combinumeral">❷</span>
            ###                                                               <span class="fm-combinumeral">❷</span>
 
            ```                                                               <span class="fm-combinumeral">❷</span>
            {user_input}                                                      <span class="fm-combinumeral">❷</span>
            ```                                                               <span class="fm-combinumeral">❷</span>
            """;                                                              <span class="fm-combinumeral">❷</span>
 
    String closestMatch = findClosestMatch(corpus, userInput);                <span class="fm-combinumeral">❸</span>
 
    prompt = prompt.replace("{relevant_document}", closestMatch)              <span class="fm-combinumeral">❹</span>
                .replace("{user_input}", userInput);                          <span class="fm-combinumeral">❹</span>
 
    System.out.println(prompt);                                               <span class="fm-combinumeral">❹</span>
}</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">❶</span> Waits for a user to input their query via the command line</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❷</span> Loads files from the resources folder</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❸</span> Finds the closest match to the user input in the loaded files</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❹</span> Replaces the placeholder parameters in the prompt</p>
<p class="body"><a id="marker-226"/>We can now run this method, and when asked to add in a query, we can test out the generation of our prompt by submitting a query such as:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">I want test ideas for the GET room endpoint</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Sending this results in the following prompt being built:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">You are an expert software tester that makes recommendations for testing ideas and risks. You answer with suggested risks to test for, based on the provided user story delimited by three hashes and user input that is delimited by three backticks.</p>
<p class="body-table">Compile a list of suggested risks to test for, based on the user story and the user input.</p>
<p class="body-table">###</p>
<p class="body-table-space">  </p>
<p class="body-table">As a guest</p>
<p class="body-table">In order to browse available rooms</p>
<p class="body-table">I want to be able to retrieve a list of all available rooms</p>
<p class="body-table-space">  </p>
<p class="body-table">Acceptance Criteria:</p>
<p class="body-table-list-a">   *   I should receive a response containing a list of available rooms</p>
<p class="body-table-list-a">   *   If there are no available rooms, I should receive an empty list</p>
<p class="body-table-list-a">   *   If there’s an error retrieving the room list, I should receive a 400 Bad Request error</p>
<p class="body-table-space">  </p>
<p class="body-table">HTTP Payload Contract</p>
<pre class="programlisting">{
  "rooms": [
    {
      "roomid": integer,
      "roomName": "string",
      "type": "Single",
      "accessible": true,
      "image": "string",
      "description": "string",
      "features": [
        "string"
      ],
      "roomPrice": integer
    }
  ]
}
    ###</pre>
<p class="body-table">```</p>
<p class="body-table">I want test ideas for the GET room endpoint</p>
<p class="body-table">```</p>
</td>
</tr>
</tbody>
</table>
<p class="body">As we can see, the user query has been added to the bottom of the prompt, and the user story that has been injected is the one that our <code class="fm-code-in-text">findClosestMatch</code> method has deemed the most relevant. It’s at this point that we’ll start to see limitations with our implementation. Trying out different queries will likely result in the selection of a less relevant user story. For example, using this query:<a id="idIndexMarker012"/><a id="marker-227"/></p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">I want a list of risks to test for the delete booking endpoint</p>
</td>
</tr>
</tbody>
</table>
<p class="body">results in the following user story being selected:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">As a guest</p>
<p class="body-table">In order to retrieve information about a booking</p>
<p class="body-table">I want to be able to send a GET request with the booking ID</p>
</td>
</tr>
</tbody>
</table>
<p class="body">This is because the <code class="fm-code-in-text">cosineDistance</code> method is limited in how well it can determine relevancy. We’ll explore how this can be handled later in the chapter, but it does highlight a limitation or risk of working with RAG frameworks.<a id="idIndexMarker013"/></p>
<p class="body">Nevertheless, let’s complete our RAG framework so that it can send the prompt to OpenAI’s GPT model to get a response. For this, we’ll be using LangChain again to send our prompt to OpenAI and output a response:</p>
<pre class="programlisting">public static void main(String[] args) throws Exception {
 
    System.out.println("What would you like help with?");                    <span class="fm-combinumeral">❶</span>
    Scanner in = new Scanner(System.in);                                     <span class="fm-combinumeral">❶</span>
    String userInput = in.nextLine();                                        <span class="fm-combinumeral">❶</span>
 
 
    List&lt;String&gt; corpus = loadFilesFromResources("data");                    <span class="fm-combinumeral">❷</span>
 
 
    String prompt = """                                                      <span class="fm-combinumeral">❸</span>
        You are an expert software tester that makes recommendations         <span class="fm-combinumeral">❸</span>
        for testing ideas and risks. You answer with suggested risks to      <span class="fm-combinumeral">❸</span>
        test for, based on the provided user story delimited by three hashes <span class="fm-combinumeral">❸</span>
        and user input that is delimited by three backticks.                 <span class="fm-combinumeral">❸</span>
 
        Compile a list of suggested risks based on the user story provided   <span class="fm-combinumeral">❸</span>
        to test for, based on the user story and the user input.             <span class="fm-combinumeral">❸</span>
        Cite which part of the user story the risk is based on.              <span class="fm-combinumeral">❸</span>
        Check that the risk matches the part of the user story before        <span class="fm-combinumeral">❸</span>
        outputting.                                                          <span class="fm-combinumeral">❸</span>
            
        ###                                                                  <span class="fm-combinumeral">❸</span>
        {relevant_document}                                                  <span class="fm-combinumeral">❸</span>
        ###                                                                  <span class="fm-combinumeral">❸</span>
        ```                                                                  <span class="fm-combinumeral">❸</span>
        {user_input}                                                         <span class="fm-combinumeral">❸</span>
        ```                                                                  <span class="fm-combinumeral">❸</span>
        """;                                                                 <span class="fm-combinumeral">❸</span>
 
    String closestMatch = findClosestMatch(corpus, userInput);               <span class="fm-combinumeral">❹</span>
 
    prompt = prompt.replace("{relevant_document}", closestMatch)             <span class="fm-combinumeral">❺</span>
                .replace("{user_input}", userInput);                         <span class="fm-combinumeral">❺</span>
 
    System.out.println("Created prompt");                                    <span class="fm-combinumeral">❺</span>
    System.out.println(prompt);                                              <span class="fm-combinumeral">❺</span>
 
 
    OpenAiChatModel model = OpenAiChatModel.withApiKey("enter-api-key");     <span class="fm-combinumeral">❻</span>
 
    String response = model.generate(prompt);                                <span class="fm-combinumeral">❼</span>
    System.out.println("Response received:");                                <span class="fm-combinumeral">❼</span>
    System.out.println(response);                                            <span class="fm-combinumeral">❼</span>
}</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">❶</span> Receives the user’s query for RAG</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❷</span> Loads files from the resources folder</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❸</span> Defines the prompt to be sent to OpenAI</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❹</span> Finds the closest match to the user query in the loaded files</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❺</span> Replaces placeholders in the prompt with the user query and file</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❻</span> Instantiates a new GPT client using an Open AI key</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">❼</span> Sends the prompt to gpt3.5-turbo and prints the response</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Providing an OPEN_AI_KEY</p>
<p class="fm-sidebar-text">To send a request to OpenAI, a Project API key must be provided, which can be generated at <a class="url" href="https://platform.openai.com/api-keys">https://platform.openai.com/api-keys</a>. You will need to create either a new account with the OpenAI platform or, depending on whether your credits have expired, add credit to your account, which can be done via <a class="url" href="https://mng.bz/6YMD">https://mng.bz/6YMD</a>. Once it’s set up, you will need to either add your Project API key directly in the code, replacing <code class="fm-code-in-text1">System.getenv("OPEN_AI_KEY"),</code> or store your key as an environmental variable under the title <code class="fm-code-in-text1">OPEN_AI_KEY</code>.<a id="marker-228"/><a id="idIndexMarker014"/></p>
</div>
<p class="body">With our GPT implementation in place, we should now have a class to run that looks similar to this example:</p>
<pre class="programlisting">public class CompletedRAGDemo {
 
    public static List&lt;String&gt; loadFilesFromResources(
    <span class="fm-code-continuation-arrow">➥</span>String folderPath) throws IOException {
        List&lt;String&gt; fileContents = new ArrayList&lt;&gt;();
        ClassLoader classLoader = CompletedRAGDemo.class.getClassLoader();
        File folder = new
        <span class="fm-code-continuation-arrow">➥</span>File(classLoader.getResource(folderPath).getFile());
 
        for (File file : folder.listFiles()) {
            if (file.isFile()) {
                String fileContent = FileUtils.readFileToString(file, "UTF-8");
                fileContents.add(fileContent);
            }
        }
 
        return fileContents;
    }
 
    public static String findClosestMatch(List&lt;String&gt; list, String query) {
        String closestMatch = null;
        double minDistance = Double.MAX_VALUE;
        CosineDistance cosineDistance = new CosineDistance();
 
        for (String item : list) {
            double distance = cosineDistance.apply(item, query);
            if (distance &lt; minDistance) {
                minDistance = distance;
                closestMatch = item;
            }
        }
 
        return closestMatch;
    }
 
    public static void main(String[] args) throws Exception {
        System.out.println("What would you like help with?");
        Scanner in = new Scanner(System.in);
        String userInput = in.nextLine();
 
        List&lt;String&gt; corpus = loadFilesFromResources("data");
 
        String prompt = """
            You are an expert software tester that makes
            recommendations for testing ideas and risks. You answer with
            suggested risks to test for, based on the provided user story
            delimited by three hashes and user input that is delimited 
            by three backticks.<a id="marker-229"/>
 
            Compile a list of suggested risks based on the user story
            provided to test for, based on the user story and the user 
            input. Cite which part of the user story the risk is based on. 
            Check that the risk matches the part of the user story before 
            outputting.
            
            ###
            {relevant_document}
            ###
            ```
            {user_input}
            ```
            """;
 
        String closestMatch = findClosestMatch(corpus, userInput);
 
        prompt = prompt.replace("{relevant_document}", closestMatch)
                .replace("{user_input}", userInput);
 
        System.out.println("Created prompt");
        System.out.println(prompt);
 
        OpenAiChatModel model = OpenAiChatModel.withApiKey("enter-api-key");
        String response = model.generate(prompt);
        System.out.println("Response received:");
        System.out.println(response);
    }
}</pre>
<p class="body">To recap, we’ve created the necessary code to:</p>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">Load in a corpus of documents (in this instance, user stories)</p>
</li>
<li class="fm-list-bullet">
<p class="list">Take in a user query via the command line</p>
</li>
<li class="fm-list-bullet">
<p class="list">Use the query to find the most relevant document</p>
</li>
<li class="fm-list-bullet">
<p class="list">Add both the query and the document to our prompt</p>
</li>
<li class="fm-list-bullet">
<p class="list">Send the prompt to OpenAI and return a response</p>
</li>
</ul>
<p class="body">With everything in place, we’re ready to start up our RAG framework and test it out.</p>
<h3 class="fm-head1" id="heading_id_6">11.2.2 Testing our RAG framework</h3>
<p class="body"><a id="marker-230"/>Once our RAG framework is up and running, we should be asked what query we would like to input. To test out our RAG framework, we can try inputting the following query:<a id="idIndexMarker015"/></p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">What would you like help with?</p>
</td>
</tr>
</tbody>
</table>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">I want test ideas for the payload portion of the PUT branding endpoint</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Our query retrieves the following user story from our corpus:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">As a guest</p>
<p class="body-table">In order to update branding information</p>
<p class="body-table">I want to be able to send a PUT request to /branding/ with necessary parameters</p>
<p class="body-table-space">  </p>
<p class="body-table">Acceptance criteria</p>
<p class="body-table-list-a">   *   I should be able to send a PUT request to /branding/ with the necessary parameters including the branding information in the request body and an optional token in the cookie.</p>
<p class="body-table-list-a">   *   If the request is successful, the response status should be 200 OK.</p>
<p class="body-table-list-a">   *   If the request is unsuccessful due to bad parameters or missing data, the response status should be 400 Bad Request.</p>
<p class="body-table-list-a">   *   The request body should contain valid JSON data conforming to the schema defined in the Swagger JSON.</p>
<p class="body-table-space">  </p>
<p class="body-table">HTTP Payload contract</p>
<pre class="programlisting">{
  "name": "string",
  "map": {
    "latitude": 0,
    "longitude": 0
  },
  "logoUrl": "string",
  "description": "string",
  "contact": {
    "name": "string",
    "address": "strings",
    "phone": "string",
    "email": "string"
  }
}</pre>
</td>
</tr>
</tbody>
</table>
<p class="body"><a id="marker-231"/>The resulting response from gpt3.5-turbo will look something like the response I received:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">Based on the user input, here are some suggested risks to test for:</p>
<p class="body-table-list-a">   1.  Missing required parameters in the request body, such as name, map (latitude and longitude), logoUrl, description, contact information (name, address, phone, email) - Risk based on the user story mentioning necessary parameters in the request body.</p>
<p class="body-table-list-a">   2.  Sending invalid data types for parameters in the request body, such as sending a number instead of a string for the name field - Risk based on the HTTP Payload contract defined in the user story.</p>
<p class="body-table-list-a">   3.  Sending invalid JSON data that does not conform to the defined schema in the Swagger JSON - Risk based on the acceptance criteria specifying valid JSON data conforming to the schema.</p>
<p class="body-table-list-a">   4.  Not including the optional token in the cookie when sending the PUT request - Risk based on the acceptance criteria mentioning an optional token in the cookie.</p>
<p class="body-table-list-a">   5.  Sending a PUT request to the wrong endpoint or with incorrect formatting - Risk based on the user story requiring the PUT request to be sent to /branding/ with necessary parameters.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">As expected, the response contains the list of risks we asked for, but what is notable is how it has returned context-specific details. For example, for the first risk that is focused on missing required parameters, we can see that it calls out specific parameters that exist in the HTTP payload example within the user story. We can also see that other risks refer to content written in the acceptance criteria, such as the use of an optional token stored within cookies (mentioned in risk 4).</p>
<p class="body">The context-specific information provided in the response exists because of the additional details we provided within the prompt in the form of our user story. To demonstrate how important this additional information is to the quality of an LLM response, here is an example of a response I received using the same prompt without the user story added:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table-list-a">   1.  Data validation: Test that the payload accepts only the allowed data types and formats as specified in the API documentation. This could help ensure that invalid data does not cause unexpected behavior in the system. (Based on the need to test the payload portion of the PUT branding endpoint)</p>
<p class="body-table-list-a">   2.  Boundary testing: Test the system’s behavior when the payload includes data at the upper and lower limits of allowed values. This could help uncover any edge cases where the system may not handle extreme values correctly. (Based on the need to test the payload portion of the PUT branding endpoint)</p>
<p class="body-table-list-a">   3.  Performance testing: Test the system’s response time and resource usage when handling large payloads. This could reveal any performance bottlenecks or issues with scalability. (Based on the need to test the payload portion of the PUT branding endpoint)</p>
<p class="body-table-list-a">   4.  Security testing: Test for any vulnerabilities related to the payload input, such as SQL injection or cross-site scripting attacks. This could help ensure that the system is secure against malicious input. (Based on the need to test the payload portion of the PUT branding endpoint)</p>
</td>
</tr>
</tbody>
</table>
<p class="body"><a id="marker-232"/>As we can see, the items returned are more generic and shallow because our prompt lacks that additional context information, thus demonstrating the value of a RAG system in prompting LLMs.</p>
<p class="body">However, RAG frameworks do have limitations, and our basic framework is no different. For example, if we were to send this query:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">What risks should I consider for the Auth API?</p>
</td>
</tr>
</tbody>
</table>
<p class="body">we might get a response like the following, which is returning information that is of no use to us:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table-list-a">   1.  Risk: Unable to successfully delete a room by providing a valid room ID</p>
<p class="body-table-list-b">   •   This risk is based on the acceptance criteria of being able to successfully delete a room by providing a valid room ID.</p>
<p class="body-table-list-a">   2.  Risk: Incorrect error handling for invalid room ID</p>
<p class="body-table-list-b">   •   This risk is based on the acceptance criteria of receiving a 400 Bad Request error when providing an invalid room ID.</p>
<p class="body-table-list-a">   3.  Risk: Confirmation message not displayed upon successful deletion</p>
<p class="body-table-list-b">   •   This risk is based on the acceptance criteria of receiving a confirmation message upon successful deletion.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Although this might feel like our LLM is hallucinating, the problem lies in our relevancy checking. Our query is focused on the Auth API, which includes login, logout, and validate endpoints, but the mention of <code class="fm-code-in-text">room ID</code> in the response implies that a room-based user story was deemed the most relevant. Once again, we see the challenge of finding the most relevant data to provide in a RAG framework when we start to consider pulling relevant data from multiple data sources at once. Therefore, to progress, we need to consider more advanced tooling to help us improve the performance of our relevancy searches to optimize the LLM’s responses.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Activity 11.2</p>
<p class="fm-sidebar-text">Try out different types of queries based on the user stories found in the project. See which queries returned desired results and which didn’t. Consider what tweaks we could make to improve the erroneous queries.</p>
</div>
<h2 class="fm-head" id="heading_id_7">11.3 Enhancing data storage for RAG</h2>
<p class="body">Now that we have a deeper understanding of how RAG works, we can begin to explore the types of tools in the market that allow us to quickly implement a framework with our data. The process of finding the right type of data to bolster our prompts can be tricky, but there are some tools and platforms on the market that make setting up RAG frameworks easier through the use of SaaS platforms and vector databases. So, let’s conclude our exploration into RAG by discussing briefly what vector databases are, how they help, and how we can use one for our needs.<a id="idIndexMarker016"/><a id="marker-233"/><a id="idIndexMarker017"/></p>
<h3 class="fm-head1" id="heading_id_8">11.3.1 Working with Vector databases</h3>
<p class="body">Unlike a SQL database, in which data is stored as different data types within rows inside tables, <i class="fm-italics">vector</i> databases store data in the form of mathematical representations. Specifically, they are stored as <i class="fm-italics">vectors,</i> which is a collection of numbers that represent an entity’s location across multiple dimensions.<a id="idIndexMarker018"/><a id="idIndexMarker019"/><a id="idIndexMarker020"/></p>
<p class="body">To give an example of how vectors work and why they are useful, let’s consider another area of software development that uses vectors—game development. Let’s say we have a character and two other entities in a 2D world and we want to know which of the entities is nearest to our character. We would use a vector that contains an X and Y position to determine the location of both. For example, if our character was in a central position on a map, their vector would be <code class="fm-code-in-text">(0,0)</code>. Now let’s say our entities were in X/Y positions (our vector dimensions) of <code class="fm-code-in-text">(5,5)</code> and <code class="fm-code-in-text">(10,10)</code>, as shown in figure 11.2.</p>
<p class="body">We can see that the later entity with the position of <code class="fm-code-in-text">(10,10)</code> is farther away. But we can also calculate the distance of vectors mathematically by comparing them. So <code class="fm-code-in-text">(0,0)</code> to <code class="fm-code-in-text">(5,5)</code> generates a distance score of <code class="fm-code-in-text">7.071068</code>, and <code class="fm-code-in-text">(0,0)</code> to <code class="fm-code-in-text">(10,10)</code> has a distance score of <code class="fm-code-in-text">14.14214</code> (calculated using <a class="url" href="https://mng.bz/o0lr">https://mng.bz/o0lr</a>). This, of course, is a basic example, but with vector databases, an entity may have vectors that contain many different dimensions, which makes the distance calculation much more complex.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre44" src="../../OEBPS/Images/CH11_F02_Winteringham2.png"/></p>
<p class="figurecaption">Figure 11.2 Graph showing vectors of a character and entities</p>
</div>
<p class="body">How these vectors and the related dimensions for our documents are calculated is beyond the scope of this book, but it is important to recognize that the purpose of using a vector database is to allow us to programmatically work out how close an item of data we’re interested in is in relation to our query. In other words, we use vector databases to work out relevancy just like we did in our basic RAG framework. However, instead of doing it across one dimension, we can compare it against many different dimensions at once—meaning in the context of the work we’ve done so far, increasing the accuracy of which user stories are deemed relevant to our query. Because it also allows support for multiple relevancy, we can extract more than one entity or document to add to our prompt if it is within a range of relevancy.<a id="marker-234"/></p>
<h3 class="fm-head1" id="heading_id_9">11.3.2 Setting up a vector-database-backed RAG</h3>
<p class="body">There has been massive growth in the vector-database-backed RAG market, with tools such as LlamaIndex (<a class="url" href="https://www.llamaindex.ai/">https://www.llamaindex.ai/</a>) and Weviate (<a class="url" href="https://weaviate.io/">https://weaviate.io/</a>). However, to get set up quickly with minimal setup and coding, we’ll be looking at a tool called Canopy, which is built by the company Pinecone (<a class="url" href="https://www.pinecone.io/">https://www.pinecone.io/</a>). Pinecone offers the ability to create vector databases in the cloud, which are known as indexes on their platform. They have also created Canopy, a RAG framework that integrates with their cloud setup. Canopy is a great choice for a trial RAG framework because, unlike our earlier RAG framework, most of the work is taken care of by the framework. This means we can get started with a vector database-backed RAG framework much faster than if we were to build our own. This of course sacrifices control for convenience, but it will give us what we need to try out a vector-database-backed RAG. You can learn more about the different parts of Canopy in their README (<a class="url" href="https://github.com/pinecone-io/canopy">https://github.com/pinecone-io/canopy</a>).<a id="idIndexMarker021"/><a id="idIndexMarker022"/><a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="idIndexMarker025"/><a id="idIndexMarker026"/><a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="idIndexMarker029"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Canopy prerequisites</p>
<p class="fm-sidebar-text">To run Canopy, you will need Python 3.11 installed on your machine. This is only required to install Canopy. Once installed, we’ll exclusively use the Canopy SLI to set up our framework.<a id="idIndexMarker030"/></p>
</div>
<p class="body"><a id="marker-235"/>To get us started, we’ll first need to install Canopy on our machine, which we do by running the <code class="fm-code-in-text">pip3 install canopy-sdk</code> command.<a id="idIndexMarker031"/></p>
<p class="body">Once it’s installed, we then require a few API keys to get ourselves set up. First, we will need our OpenAI key, which can be found at <a class="url" href="https://platform.openai.com/api-keys">https://platform.openai.com/api-keys</a>. Next, we’ll need to set up an account on Pinecone and extract the API key from it for Canopy to use to create our vector database. To do this, we need to sign up to Pinecone, which can be done here: <a class="url" href="https://app.pinecone.io/?sessionType=login">https://app.pinecone.io/?sessionType=login</a>. During the setup, you will be asked to provide a card for billing details to upgrade the free account to a standard one. We need to upgrade to a standard account to allow Canopy to create the necessary vector database. Failure to do so will cause Canopy to error when we begin to build our index for our RAG framework. At the time of writing the standard account is free, but it is unfortunately necessary to provide our account details to get access to the features we require.<a id="idIndexMarker032"/></p>
<p class="body">Once we have created our Pinecone account and upgraded it to a standard one, we can start working with Canopy to create our RAG framework. To do this, we need to set some environmental variables:</p>
<pre class="programlisting">export PINECONE_API_KEY="&lt;PINECONE_API_KEY&gt;"
export OPENAI_API_KEY="&lt;OPENAI_API_KEY&gt;"
export INDEX_NAME="&lt;INDEX_NAME&gt;"</pre>
<p class="body">Or alternatively, if you are using windows:</p>
<pre class="programlisting">setx PINECONE_API_KEY "&lt;PINECONE_API_KEY&gt;"
setx OPENAI_API_KEY "&lt;OPENAI_API_KEY&gt;"
setx INDEX_NAME "&lt;INDEX_NAME&gt;"</pre>
<p class="body">The API keys for Pinecone and OpenAI are straightforward and can be found in the respective admin sections for each platform. The third variable, though, will set the name of the index that will be created on Pinecone’s platform, so we need to pick a name for our index, such as <code class="fm-code-in-text">test-index</code>. Once we have these variables in place, we can start Canopy by running the <code class="fm-code-in-text">canopy new</code> command.<a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="idIndexMarker036"/></p>
<p class="body">Assuming our API keys are all correct and our Pinecone account is correctly upgraded, Canopy will set up a new index in Pinecone (see figure 11.3) that we can use to upload our documents when we’re ready.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre45" src="../../OEBPS/Images/CH11_F03_Winteringham2.png"/></p>
<p class="figurecaption">Figure 11.3 Pinecone indexes after a Canopy run</p>
</div>
<p class="body">With our index ready, we can begin uploading our user story documents (which can be found in the supporting repository at <a class="url" href="https://mng.bz/n0dg">https://mng.bz/n0dg</a>). We do this by running Canopy’s <code class="fm-code-in-text">upsert</code> command and providing the root folder for our user stories:<a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="marker-236"/></p>
<pre class="programlisting">canopy upsert src/main/resources/data</pre>
<p class="body">This will kick off a process in which our user stories are uploaded into the index, and once the upload is completed, we can head back into Pinecone and confirm that they are present in our index, as shown in figure 11.4.</p>
<p class="body">We now have everything loaded into our index and are ready to start up our RAG framework by running the <code class="fm-code-in-text">canopy start</code> command.<a id="idIndexMarker039"/><a id="idIndexMarker040"/></p>
<p class="body">This starts the Canopy server, which we can now access to send our queries to. To access it, we open a new terminal/command prompt and run the <code class="fm-code-in-text">canopy</code> <code class="fm-code-in-text">chat</code> <code class="fm-code-in-text">--no-rag</code> command. (Be sure to export your <code class="fm-code-in-text">OPENAI_API_KEY</code> again if you start a new terminal window.)<a id="idIndexMarker041"/><a id="idIndexMarker042"/></p>
<p class="body">Notice how we provide a flag of <code class="fm-code-in-text">--no-rag</code>. This enables Canopy to show, for comparison, both an output that uses RAG and an output that doesn’t. Upon starting up the chat feature, we’ll receive the following message to confirm that we’re ready to begin testing out our RAG framework:<a id="idIndexMarker043"/><a id="idIndexMarker044"/></p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">Let’s Chat!</p>
<p class="body-table">User message: ([Esc] followed by [Enter] to accept input)</p>
</td>
</tr>
</tbody>
</table>
<div class="figure">
<p class="figure1"><img alt="" class="calibre46" src="../../OEBPS/Images/CH11_F04_Winteringham2.png"/></p>
<p class="figurecaption">Figure 11.4 An index uploaded with user stories</p>
</div>
<h3 class="fm-head1" id="heading_id_10">11.3.3 Testing a Vector-database-backed RAG framework</h3>
<p class="body">With Canopy now ready to receive our queries, we can begin testing it out with queries and comparing the performance of our RAG framework. For example, sending the following query:<a id="idIndexMarker045"/><a id="marker-237"/><a id="idIndexMarker046"/></p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">List different types of tests for each JSON parameter in the PUT branding endpoint<a id="idIndexMarker047"/></p>
</td>
</tr>
</tbody>
</table>
<p class="body">returned the following response (edited in size to make it easier to read):</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">The different types of tests for each JSON parameter in the PUT branding endpoint include:<a id="idIndexMarker048"/></p>
<p class="body-table-list-a">   1.  **Name**:</p>
<p class="body-table-list-b">   •   Test for providing a valid string value for the name parameter.</p>
<p class="body-table-list-a">   2.  **Map (Latitude and Longitude)**:</p>
<p class="body-table-list-b">   •   Test for providing valid integer values for latitude and longitude parameters.</p>
<p class="body-table-list-a">   3.  **LogoUrl**:</p>
<p class="body-table-list-b">   •   Test for providing a valid string URL for the logoUrl parameter.</p>
<p class="body-table-list-a">   4.  **Description**:</p>
<p class="body-table-list-b">   •   Test for providing a valid string value for the description parameter.</p>
<p class="body-table-list-a">   5.  **Contact (Name, Address, Phone, Email)**:</p>
<p class="body-table-list-b">   •   Test for providing valid string values for name, address, phone, and email parameters in the contact object.</p>
<p class="body-table-space">  </p>
<p class="body-table">These tests ensure that the JSON data provided in the request body of the PUT request to the branding endpoint conforms to the specified schema defined in the Swagger JSON.</p>
<p class="body-table">(Source: src/main/resources/data/put-branding.txt)</p>
</td>
</tr>
</tbody>
</table>
<p class="body"><a id="marker-238"/>The response has a few notable points. First, the response has been structured around the different JSON parameters that have been defined within the user story. Second, we can see that Canopy has provided a reference at the bottom of the output (Source: src/main/resources/data/put-branding.txt) to indicate which sources have been deemed most relevant from the vector database. This means that, similar to the basic RAG framework we built earlier, this RAG framework is working as expected, which is even more striking when compared to how our query performed without RAG:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table-list-a">   1.  Name:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for presence of name parameter</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for invalid values (e.g. empty string, special characters)</p>
<p class="body-table-list-a">   2.  Logo:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for presence of logo parameter</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for valid file format (e.g. PNG, JPEG)</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for size limit (e.g. file size less than 5MB)</p>
<p class="body-table-list-a">   3.  Color:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for presence of color parameter</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for valid color code format (e.g. hex code)</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for invalid color values (e.g. ‘purple’ instead of ‘#123456’)</p>
<p class="body-table-list-a">   4.  Font:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for presence of font parameter</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for valid font name or family</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for invalid font values (e.g. empty string, special characters)</p>
<p class="body-table-list-a">   5.  Description:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for presence of description parameter</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for valid length limit (e.g. less than 100 characters)</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for invalid values (e.g. special characters, long strings)</p>
<p class="body-table-list-a">   6.  Website:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for presence of website parameter</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for valid URL format</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test for invalid values (e.g. invalid domain, special characters)</p>
</td>
</tr>
</tbody>
</table>
<p class="body">In this response, we can see that there is little connection to the context of our branding JSON payload. Again, this demonstrates the challenge we face when using LLMs and ensuring they have access to our context. But what about our previous problem with the basic RAG framework we created and its inability to bring in multiple user stories into a query? With Canopy backed with a vector database, we can test out how it performs with a query that has the potential to draw on multiple user stories in one single prompt:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">List different types of tests for each Booking endpoint</p>
</td>
</tr>
</tbody>
</table>
<p class="body"><a id="marker-239"/>Sending this to Canopy returned the following response (again, edited in size for readability):<a id="idIndexMarker049"/></p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table-list-a">   1.  **GET Booking Endpoint:**<a id="idIndexMarker050"/></p>
<p class="body-table-list-b">   •   Functional Tests:</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Verify that the endpoint returns a list of all bookings.</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Test the functionality of filtering bookings by providing a room ID as a query parameter.</p>
<p class="body-table-list-a">   2.  **POST Booking Endpoint:**<a id="idIndexMarker051"/></p>
<p class="body-table-list-b">   •   Functional Tests:</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Ensure the endpoint accepts booking details in JSON format and creates a new booking successfully.</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Test the response and status code when the booking request is invalid.</p>
<p class="body-table-list-a">   3.  **GET Booking ID Endpoint:**<a id="idIndexMarker052"/></p>
<p class="body-table-list-b">   •   Functional Tests:</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Confirm the endpoint retrieves the correct details for a provided booking ID.</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Test the response when an invalid or missing booking ID is provided.</p>
<p class="body-table-list-a">   4.  **PUT Booking ID Endpoint:**<a id="idIndexMarker053"/></p>
<p class="body-table-list-b">   •   Functional Tests:</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Ensure the endpoint updates the booking information correctly for a given booking ID.</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Test the response and status code when the booking ID is missing or invalid.</p>
<p class="body-table-list-a">   5.  **GET Booking Summary Endpoint:**<a id="idIndexMarker054"/></p>
<p class="body-table-list-b">   •   Functional Tests:</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Verify that the server responds with a summary of bookings for a specified room ID.</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Test the response when the room ID is missing or invalid.</p>
<p class="body-table">(Source: src/main/resources/data)</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Reviewing this response, we can see that the source at the bottom of the prompt is referencing the root folder of our data (Source: src/main/resources/data), indicating that multiple files have been pulled into our query. This is confirmed by how each endpoint has been mentioned in the response as well. We can correlate each entry in the response to a booking-based user story stored within our data set.<a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="idIndexMarker057"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Activity 11.3</p>
<p class="fm-sidebar-text">Using Canopy and Pinecone, prepare and upload your own custom set of data to an index. Try out different queries with your custom data to see how a RAG framework would perform in your context.</p>
</div>
<h3 class="fm-head1" id="heading_id_11">11.3.4 Going forward with RAG frameworks</h3>
<p class="body"><a id="marker-240"/>By building our RAG framework before trying out existing platforms for RAG, we’ve now developed a deeper understanding of how RAG works and how it can be of benefit. However, what we’ve learned simply serves as an introduction to an area of LLM use that has lots of practical applications. What type of data we can store in RAG frameworks, whether we use vector databases or not, offers a lot of scope. Combined with what we’ve learned about writing effective prompts, RAG frameworks can be used to provide live analytics, production data, stored user content, and much more to help us create prompts that are more attuned to our context, which ultimately will help increase LLM use in our testing and elsewhere.<a id="idIndexMarker058"/><a id="idIndexMarker059"/></p>
<h2 class="fm-head" id="heading_id_12">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">Retrieval-augmented generation (RAG) is an approach that combines contextual data and user queries in a prompt to an LLM.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The selection of contextual data is based on its relevancy to the initial user query that has been provided.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Providing selected data improves accuracy and ensures that errors around context windows are avoided.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Having contextual data separate from an LLM makes the process of selecting and updating data sources easier.</p>
</li>
<li class="fm-list-bullet">
<p class="list">For RAG systems to work, we need the ability to upload and store data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">RAG systems use similarity algorithms and tools to determine which data is most relevant to a query.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Building prompts that lack contextual data results in responses that are more generic and shallow.</p>
</li>
<li class="fm-list-bullet">
<p class="list">An LLM can return incorrect responses if the similarity algorithm returns inaccurate data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Finding relevant data becomes complex when queries are broad or multiple data sources need to be added to a prompt at once.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Vector databases store vectors based on multiple dimensions, which are used to determine relevancy to a query.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Some frameworks and tools offer the ability to quickly set up a RAG framework using vector databases.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Utilizing vector databases and related tools and platforms makes it easier for us to query them.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Vector databases allow us to pull multiple relevant files into a query at once.</p>
</li>
<li class="fm-list-bullet">
<p class="list">RAG frameworks can provide a wide range of types of data that has multiple applications for testing and software development as a whole.<a id="marker-241"/></p>
</li>
</ul>
</div></body></html>