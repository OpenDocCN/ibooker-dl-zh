<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">3</span></span> <span class="chapter-title-text">Graph convolutional networks and GraphSAGE</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Introducing GraphSAGE and graph convolutional networks </li>
<li class="readable-text" id="p3">Applying convolutional graph neural networks to generate product bundles from Amazon </li>
<li class="readable-text" id="p4">Key parameters and settings for graph convolutional networks and GraphSAGE</li>
<li class="readable-text" id="p5">More theoretical insights, including convolution and message passing</li>
</ul>
</div>
<div class="readable-text" id="p6">
<p>In the first two chapters of this book, we explored fundamental concepts related to graphs and graph representation learning. All of this served to set us up for part 2, where we’ll explore distinct types of graph neural network (GNN) architectures, including convolutional GNNs, graph attention networks (GATs), and graph autoencoders (GAEs). </p>
</div>
<div class="readable-text intended-text" id="p7">
<p>In this chapter, our goal is to understand and apply graph convolutional networks (GCNs) and GraphSAGE [1, 2]. These two architectures are part of a larger class of GNNs that approach deep learning by applying convolutions to graph data. </p>
</div>
<div class="readable-text intended-text" id="p8">
<p>Convolutional operations are relatively common in deep learning models, particularly for image-based tasks that rely heavily on convolutional neural networks (CNNs). To learn more about CNNs and their application to computer vision, we recommend checking out <em>Deep Learning with Python</em> (Manning, 2024) or <em>Deep Learning with PyTorch</em> (Manning, 2023).</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>We provide a short primer on convolutions later in the chapter, but essentially convolutional operations can be understood as performing a spatial or local averaging across entities. For example, in images, CNN layers form representations at incrementally larger pixel subdomains. For GCNs, we’ll apply the same idea of a local averaging, but with neighborhoods of nodes. </p>
</div>
<div class="readable-text intended-text" id="p10">
<p>In this chapter, you’ll learn how to apply convolutional GNNs to a node prediction problem, key parameters and settings for GCN and GraphSAGE, ways to optimize performance for convolutional GNNs, and relevant theoretical topics, including graph convolution and message passing. Additionally, we’ll explore the Amazon Products dataset. This chapter is structured as follows: first, we jump into the product category prediction problem and create baseline models (section 3.1); then we adjust our models using neighborhood aggregation (section 3.2); next, we optimize our models using general deep learning methods (section 3.3); following that, we explain relevant theory in more detail (section 3.4); and finally, we dig deeper into the Amazon Products dataset used in this chapter and later in the book (section 3.5).</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>This chapter is designed to immerse you immediately in the application of convolutional GNNs, equipping you with the essential knowledge needed to deploy these models effectively. The initial sections provide you with the minimum toolkit for a functioning understanding of convolutional GNNs in practice.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>However, when facing challenging modeling problems, deeper comprehension becomes invaluable. The latter sections of the chapter cover underlying principles of the layers, settings, and parameters introduced earlier. They are crafted to enhance your conceptual grasp, ensuring that your practical skills are complemented by a thorough theoretical understanding. This holistic approach aims to not only enable you to apply GNNs but to innovate and adapt them to the nuanced demands of real-world problems.</p>
</div>
<div class="readable-text print-book-callout" id="p13">
<p><span class="print-book-callout-head">Note</span>  While <em>GraphSAGE</em> refers to a specific individual architecture, it may be confusing that <em>GCN</em> also refers to a specific architecture and not the entire class of GNNs based on convolutions. So, in this chapter, we’ll use <em>convolutional GNNs </em>to refer to this entire class of GNNs, which include GraphSAGE and GCN. We’ll use <em>GCN</em> to refer to the individual architecture introduced by Thomas Kipf and Max Welling [1]. </p>
</div>
<div class="readable-text print-book-callout" id="p14">
<p><span class="print-book-callout-head">Note</span>  Code from this chapter can be found in notebook form at the GitHub repository (<a href="https://mng.bz/wJMW">https://mng.bz/wJMW</a>). Colab links and data from this chapter can be accessed in the same locations.</p>
</div>
<div class="readable-text" id="p15">
<h2 class="readable-text-h2"><span class="num-string">3.1</span> Predicting consumer product categories</h2>
</div>
<div class="readable-text" id="p16">
<p>Let’s start our exploration of convolutional GNNs with a product management problem using the Amazon Products dataset (see table 3.1). Imagine you’re a product manager aiming to enhance sales by identifying and promoting emerging trends in product bundles. You have a dataset derived from Amazon’s product co-purchasing network, containing a rich set of relationships between products based on customer buying behavior. Your task is to use insights about product categories and co-purchasing patterns to uncover hidden and appealing product bundles that resonate with your customers.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p17">
<h5 class="browsable-container-h5"><span class="num-string">Table 3.1</span> Overview of the Amazon Products dataset </h5>
<table>
<thead>
<tr>
<th>
<div>
         Amazon co-purchases organized by product category 
       </div></th>
<th/>
</tr>
</thead>
<tbody>
<tr>
<td>  Number of nodes (products) <br/></td>
<td>  ~2,500,000 <br/></td>
</tr>
<tr>
<td>  Node features <br/></td>
<td>  100 <br/></td>
</tr>
<tr>
<td>  Node categories <br/></td>
<td>  47 <br/></td>
</tr>
<tr>
<td>  Total number of edges <br/></td>
<td>  ~61,900,000 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p18">
<p>To tackle this, we introduce GCNs and GraphSAGE—two convolutional GNN architectures. This section will guide you through training these models on the Amazon Products dataset. We’ll focus on two tasks: identifying a product’s category and finding sets of product bundles by analyzing the similarity between product embeddings produced by the trained models.</p>
</div>
<div class="readable-text print-book-callout" id="p19">
<p><span class="print-book-callout-head">Note </span> If you want to get deeper into the theory behind GCN and GraphSAGE, see section 3.4. For details about the Amazon Products dataset, see section 3.5.</p>
</div>
<div class="readable-text" id="p20">
<p>Following our model training process, in this section, we’ll do the following: </p>
</div>
<ul>
<li class="readable-text" id="p21"> <em>Preprocess our dataset</em>—We’ll take the Amazon Products dataset and reduce its size to work with systems with minimal resources. </li>
<li class="readable-text" id="p22"> <em>Construct our model classes</em>—We’ll focus on two convolutional GNNs: GCN and GraphSAGE. We’ll initially create model classes and instantiate them with default parameters. </li>
<li class="readable-text" id="p23"> <em>Code our training and validation loops</em>—We’ll train the models with a validation step for each epoch. To compare the two models, we’ll train them simultaneously with the same batches. </li>
<li class="readable-text" id="p24"> <em>Assess model performance</em>—We’ll take a look at training curves. Then, we’ll use traditional classification metrics and observe the ability of the model to predict particular categories. </li>
</ul>
<div class="readable-text" id="p25">
<p>Our immediate goal is to develop first passes of our trained models. So, at this point, the emphasis isn’t on performance optimization but on covering the essential steps to get a baseline model working. Subsequent sections will refine these approaches, enhancing performance and efficiency.</p>
</div>
<div class="readable-text" id="p26">
<h3 class="readable-text-h3"><span class="num-string">3.1.1</span> Loading and processing the data</h3>
</div>
<div class="readable-text" id="p27">
<p>We start by downloading the Amazon Products dataset from the Open Graph Benchmark (OGB) site (<a href="https://ogb.stanford.edu/">https://ogb.stanford.edu/</a>). This dataset is large for a single machine, taking up 1.3 GB. This includes 2.5 million nodes (products) and 61.9 million edges (co-purchases).</p>
</div>
<div class="readable-text intended-text" id="p28">
<p>To make working with this data manageable for systems with smaller memory capacity and less powerful processors, we’ll reduce its size. We simply take the nodes that have the first 10,000 node indices in the original graph and create a subgraph based on those. Depending on your problem, there are other strategies to create subgraphs. In chapter 8, we look at creating subgraphs in more depth.</p>
</div>
<div class="readable-text intended-text" id="p29">
<p>In creating a subset graph, there is often bookkeeping that must be done to ensure our node subset has a consistent and logical ordering and is connected to the correct labels and features. We must also filter out edges that are connected to nodes from outside the subset. Lastly, we want to make sure we can call back the original indices of the subset in case we want to call back useful information; for example, for the Amazon Products dataset, we can access the SKU (Amazon Standard Identification Number, ASIN) numbers and product categories of each node using their original indices.</p>
</div>
<div class="readable-text intended-text" id="p30">
<p>So, we relabel the nodes with a consistent ordering. Then, we reassign the respective node features and labels to correspond to the new indices. Even though we choose nodes with the first 10,000 indices, this may not be so in any particular case. Here’s how we’ll refine and prepare the data for modeling in four steps:</p>
</div>
<ol>
<li class="readable-text" id="p31"> <em>Initialize the subset graph </em>—We create a new graph object that will store our subset of data. This graph will hold the edges, features, and labels of the nodes that have indices 0–9,999 in our original graph. </li>
<li class="readable-text" id="p32"> <em>Relabel node indices </em>—To ensure consistency and avoid index mismatches, we relabel the node indices within our subset graph. This relabeling is crucial because operations within GNNs depend heavily on indexing to process node and edge information. </li>
<li class="readable-text" id="p33"> <em>Feature and label assignment </em>—We assign node features (x) and labels (y) to our new graph object. These features and labels are sliced from the original dataset, corresponding to our specified subset indices. </li>
<li class="readable-text" id="p34"> <em>Edge mask utilization </em>—The <code>return_edge_mask</code> option, used during the subgraph extraction, lets us identify which edges were selected during the subgraph creation. This is useful for tracing back to the original graph’s structure or for any structural analysis required later. </li>
</ol>
<div class="readable-text" id="p35">
<p>By restructuring the data in this manner, we not only make it manageable but also tailor it specifically for efficient processing in subsequent graph-based learning tasks. This setup is foundational as we proceed to construct and evaluate our GNN models in the following sections. The following listing shows the code for implementing that process.</p>
</div>
<div class="browsable-container listing-container" id="p36">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.1</span> Reading in data and creating a subgraph</h5>
<div class="code-area-container">
<pre class="code-area">dataset = PygNodePropPredDataset(name='ogbn-products',\
 root=root)  <span class="aframe-location"/> #1
data = dataset[0]  <span class="aframe-location"/> #2

subset_indices = torch.arange(0, 10000)  <span class="aframe-location"/> #3

subset_edge_index, edge_attr, edge_mask = \
subgraph(subset_indices, data.edge_index, \
None, relabel_nodes=True, num_nodes=\
data.num_nodes, return_edge_mask=True)  <span class="aframe-location"/> #4

subset_features = data.x[subset_indices]  <span class="aframe-location"/> #5
subset_labels = data.y[subset_indices]  <span class="aframe-location"/> #6

subset_graph = data.__class__()  <span class="aframe-location"/> #7
subset_graph.edge_index = subset_edge_index  <span class="aframe-location"/> #8
subset_graph.x = subset_features  <span class="aframe-location"/> #9
subset_graph.y = subset_labels  <span class="aframe-location"/> #10</pre>
<div class="code-annotations-overlay-container">
     #1 Loads dataset from the specified root directory and specifies ogbn-products to indicate which dataset is being loaded
     <br/>#2 The first graph object from the dataset is selected for processing.
     <br/>#3 Creates an array of indices for the first 10,000 nodes, which defines our subset for the experiment
     <br/>#4 Calls the subgraph function with subset_indices to extract edges and attributes relevant to these indices. The nodes are relabeled to maintain a consistent zero-based index in the new graph.
     <br/>#5 Indexes node features from the original data according to subset_indices to ensure that only relevant features are transferred to the new graph
     <br/>#6 Similarly, indexes node labels to maintain correspondence with the subset features
     <br/>#7 Creates a new instance of the data class to store our subset graph
     <br/>#8 Assigns the edge index array created during the subgraph extraction to the new graph
     <br/>#9 Assigns subset features to the new graph’s node feature matrix
     <br/>#10 Assigns node labels corresponding to the subset to the new graph
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p37">
<h3 class="readable-text-h3"><span class="num-string">3.1.2</span> Creating our model classes</h3>
</div>
<div class="readable-text" id="p38">
<p>After setting up our dataset and preparing a manageable subgraph, we transition to the core of our graph machine learning pipeline: defining the models. In this section, we focus on two popular types of GNNs provided by the PyTorch Geometric (PyG) library: GCN and GraphSAGE.</p>
</div>
<div class="readable-text" id="p39">
<h4 class="readable-text-h4">Understanding our model architectures</h4>
</div>
<div class="readable-text" id="p40">
<p>PyG simplifies the construction of GNNs through modular layer objects, each encapsulating a specific type of graph convolution. These layers can be stacked and integrated with other PyTorch modules to build complex architectures tailored to various graph-based tasks.</p>
</div>
<div class="readable-text" id="p41">
<h4 class="readable-text-h4">GCN model </h4>
</div>
<div class="readable-text" id="p42">
<p>The GCN model uses the <code>GCNConv</code> layer, which implements the graph convolution operation as described by Kipf and Welling in their seminal paper [1]. It takes advantage of the spectral properties of graphs to facilitate information flow between nodes, allowing the model to learn representations that embed both local graph structure and node features.</p>
</div>
<div class="readable-text intended-text" id="p43">
<p>In listing 3.2, the GCN class sets up a two-layer model. Each layer is represented by the <code>GCNConv</code> module, which processes graph data by applying a convolution operation that directly uses the graph’s structure. </p>
</div>
<div class="readable-text intended-text" id="p44">
<p>To summarize its workings, from an input set of node features and the graph structure (<code>edge_index</code>), the network will update node features by aggregating the neighborhood information from each respective node. After the first layer, we apply a rectified linear unit (ReLU) activation function, which adds nonlinearity to the model. The second layer refines these features further. </p>
</div>
<div class="readable-text intended-text" id="p45">
<p>If we want to look at the node embeddings directly—for instance, to visualize them or to use them in some other analysis—we can just return them right after the second layer. Otherwise, we apply another activation function—in this case, a softmax function—to normalize the outputs for our classification problem.</p>
</div>
<div class="browsable-container listing-container" id="p46">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.2</span> GCN class</h5>
<div class="code-area-container">
<pre class="code-area">class GCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
    super(GCN, self).__init__()
    self.conv1 = GCNConv(in_channels, hidden_channels) <span class="aframe-location"/> #1
    self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, x, edge_index, \
return_embeds=False): <span class="aframe-location"/> #2
        x = self.conv1(x, edge_index)
        x = torch.relu(x) <span class="aframe-location"/> #3
        x = self.conv2(x, edge_index)
        if return_embeds: <span class="aframe-location"/> #4
             return x

    return torch.log_softmax(x, dim=1) <span class="aframe-location"/> #5</pre>
<div class="code-annotations-overlay-container">
     #1 Initializes the first graph convolution layer that transforms input features (in_channels) into hidden features (hidden_channels)
     <br/>#2 Forward method that dictates how data flows through the model from input to output
     <br/>#3 Applies the ReLU activation function after the first convolution to add nonlinearity to the model
     <br/>#4 Optionally returns the raw embeddings from the network, which can be useful for tasks that require raw node representations without classification, such as visualization or further processing
     <br/>#5 Applies a log softmax activation to the final layer’s output
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p47">
<h4 class="readable-text-h4">GraphSAGE model </h4>
</div>
<div class="readable-text" id="p48">
<p>Much like the GCN model, the GraphSAGE model class in our code also sets up a two-layer network, but with the <code>SAGEConv</code> layers. While structurally similar in code, GraphSAGE is a significant shift from GCN in theory. Unlike GCN’s full reliance on the entire graph’s adjacency matrix, GraphSAGE is designed to learn from randomly sampled neighborhood data, making it particularly well-suited for large graphs. This sampling approach allows GraphSAGE to scale effectively by focusing on localized regions of the graph.</p>
</div>
<div class="readable-text intended-text" id="p49">
<p>GraphSAGE uses the <code>SAGEConv</code> layer, which supports various aggregation functions—mean, pool, and long short-term memory (LSTM)—offering flexibility in how node features are aggregated. After each <code>SAGEConv</code> layer, similar to the GCN model, a nonlinearity is applied. If node embeddings are required directly for tasks such as visualization or further analysis, they can be returned immediately following the second layer. Otherwise, a softmax function is applied to normalize the outputs for classification tasks.</p>
</div>
<div class="readable-text intended-text" id="p50">
<p>The key difference in PyG’s implementation of these models lies in their efficiency and scalability with large datasets. Both models learn node representations, but GraphSAGE provides a significant advantage for practical applications involving very large graphs. Unlike GCN, which can operate on sparse data representations but still processes information from the entire graph structure, GraphSAGE doesn’t require the entire adjacency matrix. Instead, it samples local neighborhoods, which allows it to handle vast networks efficiently without overwhelming memory resources by having to load the entire graph representation. </p>
</div>
<div class="browsable-container listing-container" id="p51">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.3</span> GraphSAGE class</h5>
<div class="code-area-container">
<pre class="code-area">class GraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
    super(GraphSAGE, self).__init__()
    self.conv1 = SAGEConv(in_channels, \
    hidden_channels) <span class="aframe-location"/> #1
    self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index, \
    return_embeds=False): <span class="aframe-location"/> #2
        x = self.conv1(x, edge_index)
        x = torch.relu(x) <span class="aframe-location"/> #3
        x = self.conv2(x, edge_index)
        if return_embeds: <span class="aframe-location"/> #4
            return x

     return torch.log_softmax(x, dim=1) <span class="aframe-location"/> #5</pre>
<div class="code-annotations-overlay-container">
     #1 Initializes the first graph convolution layer that transforms input features (in_channels) into hidden features (hidden_channels)
     <br/>#2 Forward method that dictates how data flows through the model from input to output
     <br/>#3 Applies the ReLU activation function after the first convolution to add nonlinearity to the model
     <br/>#4 Optionally returns the raw embeddings from the network, which can be useful for tasks that require raw node representations without classification, such as visualization or further processing
     <br/>#5 Applies a log softmax activation to the final layer’s output
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p52">
<h4 class="readable-text-h4">Integration and customization</h4>
</div>
<div class="readable-text" id="p53">
<p>While we use default settings in this introductory example, both models are highly customizable. Parameters such as the number of layers, hidden dimensions, and types of aggregation functions (for GraphSAGE) can be adjusted to optimize performance for specific datasets or tasks. Next, we’ll train these models on our subset graph and evaluate their performance to demonstrate their practical applications and effectiveness.</p>
</div>
<div class="readable-text" id="p54">
<h3 class="readable-text-h3"><span class="num-string">3.1.3</span> Model training</h3>
</div>
<div class="readable-text" id="p55">
<p>With our data ready and our models set up, let’s get into the training process. Training is relatively straightforward, as it follows typical machine learning routines but applied to graph data. We’ll be training two models simultaneously—GCN and GraphSAGE—by feeding them the same data each epoch. This parallel training allows us to directly compare the performance and efficiency of these two model types under identical conditions. Here’s a concise breakdown of the training loop:</p>
</div>
<ul>
<li class="readable-text" id="p56"> <em>Initialize optimizers</em>—Set up Adam optimizers with a learning rate of 0.01. This helps us fine-tune the model weights effectively during training. </li>
<li class="readable-text" id="p57"> <em>Training and validation loops</em>—For each epoch, run the training function, which processes the data through the model to compute losses and update weights. Concurrently, validate the model on unseen data to monitor overfitting and adjust training strategies accordingly. </li>
<li class="readable-text" id="p58"> <em>Track progress</em>—Record losses for both training and validation phases to visualize the learning curve and adjust parameters if needed. </li>
<li class="readable-text" id="p59"> <em>Conclude with testing</em>—After training, the models are evaluated on a separate test set to gauge their generalization capabilities. </li>
</ul>
<div class="readable-text" id="p60">
<p>By maintaining a consistent training regimen for both models, we ensure that any differences in performance can be attributed to the models’ architectural differences rather than varied training conditions. The following listing contains the annotated code of our training logic.</p>
</div>
<div class="browsable-container listing-container" id="p61">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.4</span> Training loop</h5>
<div class="code-area-container">
<pre class="code-area">gcn_model = GCN(in_channels=dataset.num_features,\
 hidden_channels=64, out_channels=\
dataset.num_classes) <span class="aframe-location"/> #1
graphsage_model = GraphSAGE(in_channels=d\
ataset.num_features, hidden_channels=64, \
out_channels=dataset.num_classes) <span class="aframe-location"/> #2

optimizer_gcn = torch.optim.Adam\
(gcn_model.parameters(), lr=0.01)  <span class="aframe-location"/> #3
optimizer_sage = torch.optim.Adam(\
graphsage_model.parameters(), lr=0.01)  <span class="aframe-location"/> #4
criterion = torch.nn.CrossEntropyLoss()  <span class="aframe-location"/> #5

def train(model, optimizer, data): <span class="aframe-location"/> #6
    model.train()  
    optimizer.zero_grad() 
    out = model(data.x, data.edge_index)  
    loss = criterion(out[data.train_mask], data.y[data.train_mask].squeeze()) 
    loss.backward()  
    optimizer.step()  
    return loss.item()  

def validate(model, data): <span class="aframe-location"/> #7
    model.eval()  
    with torch.no_grad():  
        out = model(data.x, data.edge_index)  
        val_loss = criterion(out[data.val_mask], data.y[data.val_mask].squeeze())  
    return val_loss.item()  


train_loss_gcn = []  <span class="aframe-location"/> #8
val_loss_gcn = []  
train_loss_sage = []  
val_loss_sage = []  


for epoch in range(200): <span class="aframe-location"/> #9
    loss_gcn = train(gcn_model, optimizer_gcn, subset_graph)  
    train_loss_gcn.append(loss_gcn)  
    val_loss_gcn.append(validate(gcn_model, subset_graph))  

    loss_sage = train(graphsage_model, optimizer_sage, subset_graph)  
    train_loss_sage.append(loss_sage)  
    val_loss_sage.append(validate(graphsage_model, subset_graph))  

    if epoch % 10 == 0:  
        print(f'Epoch {epoch}, GCN Loss: \
{loss_gcn:.4f}, GraphSAGE Loss: \
{loss_sage:.4f}, GCN Val Loss: \
{val_loss_gcn[-1]:.4f}, GraphSAGE \
Val Loss: {val_loss_sage[-1]:.4f}')</pre>
<div class="code-annotations-overlay-container">
     #1 Initializes models for the GCN and GraphSAGE 
     <br/>#2 Initializes models for the GCN and GraphSAGE 
     <br/>#3 Sets up optimizers for the GCN and GraphSAGE models
     <br/>#4 Sets up optimizers for the GCN and GraphSAGE models 
     <br/>#5 Initializes the cross-entropy loss function for the classification task
     <br/>#6 Train functions used every epoch
     <br/>#7 Validation functions used every epoch
     <br/>#8 Sets up arrays to capture losses for each model
     <br/>#9 Training and validation loop
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p62">
<p>Now that we’ve set up and trained our models, it’s time to see how well they perform. The next section will look at the training and validation loss curves to understand how the models learned over time. It will check out key metrics such as accuracy, precision, recall, and F1 scores to evaluate how well our models can predict product categories based on our graph data. All of this is to understand our models and to figure out where we can improve them in later sections. </p>
</div>
<div class="readable-text" id="p63">
<h3 class="readable-text-h3"><span class="num-string">3.1.4</span> Model performance analysis</h3>
</div>
<div class="readable-text" id="p64">
<p>For the next section, we’ll look at the model performance of the GCN and GraphSAGE models. We’ll first examine the training curves and point out that we have to improve the overfitting in a subsequent chapter. Then, we’ll look at the F1 and log loss scores, followed by examining accuracy for the product categories.</p>
</div>
<div class="readable-text" id="p65">
<h4 class="readable-text-h4">Training curves</h4>
</div>
<div class="readable-text" id="p66">
<p>During the training process, we saved the losses for each model for every epoch. <em>Loss</em> is a measure of how well our model is able to make correct predictions, with lower values being better. </p>
</div>
<div class="readable-text intended-text" id="p67">
<p>Using <code>Matplotlib</code>, we use this data to plot training loss and validation loss curves, shown in figure 3.1. <span class="aframe-location"/>Such curves track the performance of the model on training and validation datasets over the course of the training process. Ideally, both losses should decline over time. However, in our curves, we see a divergence beginning near epoch 20. The validation loss curves reach a nadir and then begin to climb. Meanwhile, the training loss continues to decline. Our models’ performance continues to improve on the training data but degrades on the validation data past some optimal point. This is the classic overfitting problem, which we’ll address later in the chapter. </p>
</div>
<div class="browsable-container figure-container" id="p68">
<img alt="figure" height="1261" src="../Images/3-1.png" width="982"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.1</span> Training and validation loss curves for the GCN model (left) and GraphSAGE model (right) trained in this section. The divergence of the validation from the training curve signals over-fitting, where the model learns the training data too well and at the expense of generalizing to new data.</h5>
</div>
<div class="readable-text intended-text" id="p69">
<p>In our training process, we’ve saved the instance of the model with the best performance, which is the instance with the lowest validation loss. Next, we look at two classification metrics to assess performance: log loss and F1 score.</p>
</div>
<div class="readable-text" id="p70">
<h4 class="readable-text-h4">Classification performance: F1 and log loss</h4>
</div>
<div class="readable-text" id="p71">
<p>Given the overfitting problems shown earlier, we turn to the classification performance of our models to establish a baseline for our improvement efforts. We use our validation sets to establish F1 and log loss scores, shown in table 3.2. (The F1 score is weighted, which measures F1 for each class separately, then averages them, weighing each class by its proportion of the total data.)</p>
</div>
<div class="readable-text intended-text" id="p72">
<p>The middling scores indicate that the models have much room for improvement. Our F1 scores don’t exceed 80%, while the log loss scores are no lower than 1.25. </p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p73">
<h5 class="browsable-container-h5"><span class="num-string">Table 3.2</span> Classification performance of our models, by F1 score and log loss</h5>
<table>
<thead>
<tr>
<th/>
<th>
<div>
         F1 Score 
       </div></th>
<th>
<div>
         Log Loss 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  GCN <br/></td>
<td>  0.781 <br/></td>
<td>  1.25 <br/></td>
</tr>
<tr>
<td>  GraphSAGE <br/></td>
<td>  0.733 <br/></td>
<td>  1.88 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p74">
<p>In this case, GCN performs better for both metrics. To improve these scores for a multiclass problem, we could look more deeply at the model’s capability to predict individual classes and examine its performance for imbalanced classes. </p>
</div>
<div class="readable-text" id="p75">
<h4 class="readable-text-h4">Model performance at a class level</h4>
</div>
<div class="readable-text" id="p76">
<p>The Amazon Products dataset comes with two useful files that map each node with its class, and each node with its individual Amazon product number (ASIN). To evaluate the performance of our baseline models by class, we take the node class information and create a table, as shown in figure 3.2, summarizing prediction accuracy for the 25 classes containing the most items. </p>
</div>
<div class="readable-text intended-text" id="p77">
<p>Along with accuracy, in this table, we examine the biggest mispredictions for each class. From this information, let’s make some high-level observations:</p>
</div>
<ul>
<li class="readable-text" id="p78"> <em>Performance by category</em>—Both models show variability in their prediction accuracy across different product categories. The Books category and CDs &amp; Vinyl category have high accuracy rates. This can be due to their relatively high number of samples. It could also indicate that these categories are more distinct or well-defined, making it easier for the model to distinguish them. The first factor, number of samples, is easy to adjust because we’re using 10,000 product nodes and can draw millions more from our dataset. You can give it a try by adjusting the size of the subset in the provided code. </li>
</ul>
<div class="readable-text list-body-item" id="p79">
<p>To improve less distinctive classes, we need to make a deeper exploration of the node features to determine how distinctive the classes are relative to each other and to brainstorm ways to enhance those features to bring out their novelty.</p>
</div>
<ul>
<li class="readable-text" id="p80"> <em>Performance by model</em>—Looking over all the classes, GraphSAGE generally appears to perform better than GCN in most categories, as seen from the higher percentages of correct predictions. This suggests that GraphSAGE’s approach of aggregating features from a node’s neighborhood might be more effective for this dataset. </li>
<li class="readable-text" id="p81"> <em>Misclassifications</em>—Common misclassifications tend to occur between categories that might share similar characteristics or be frequently purchased together. For example, the misclassification between Books and Movies &amp; TV or between Electronics and Cell Phones &amp; Accessories suggests that items in these categories may share overlapping features or are often bought by similar customer segments.<span class="aframe-location"/> </li>
</ul>
<div class="browsable-container figure-container" id="p82">
<img alt="figure" height="494" src="../Images/3-2.png" width="1009"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.2</span> Classification performance (accuracy) by product category, comparing GCN and GraphSAGE</h5>
</div>
<div class="readable-text" id="p83">
<p>Though we generally don’t want misclassifications, observing the classes most likely to be misconstrued for another could inform us about common customer perceptions or confusions between product categories, highlighting potential areas for marketing and product placement strategies.</p>
</div>
<div class="readable-text intended-text" id="p84">
<p>The next two sections will improve the models’ performance from these baseline results by taking advantage of the properties of our GNNs (section 3.2) and by using well-known deep learning methods (section 3.3). To end this section, let’s use our models to come up with a product bundle for our product manager.</p>
</div>
<div class="readable-text" id="p85">
<h3 class="readable-text-h3"><span class="num-string">3.1.5</span> Our first product bundle</h3>
</div>
<div class="readable-text" id="p86">
<p>In the beginning of this section, we discussed our use case of a product manager who wants to enhance sales by introducing product bundles. Let’s use one of our newly trained models to suggest a bundle for a given product. We’ll group together the nodes whose embeddings are most similar to a selected node, forming a bundle based on their similarity. Later in the chapter, as we improve the models, we’ll come back to the exercise. </p>
</div>
<div class="readable-text print-book-callout" id="p87">
<p><span class="print-book-callout-head">Note</span>  The code won’t be reviewed extensively here but can be found in the repository. </p>
</div>
<div class="readable-text" id="p88">
<h4 class="readable-text-h4">Node ID to product number</h4>
</div>
<div class="readable-text" id="p89">
<p>One key file provided in the Amazon Products dataset is a comma-separated values (CSV) file mapping for node index to Amazon product ID (ASIN). In the repository, this is used to create a Python dictionary of node ID (key) to ASIN (value). Using a node’s ASIN, we can access information about the product using a URL in this format: www.amazon.com/dp/{ASIN}. (Given the age of the dataset, a few ASINs don’t have web pages currently, but the vast majority we tested do at the time of writing.)</p>
</div>
<div class="readable-text intended-text" id="p90">
<p>To create a product bundle, we work with node embeddings. We choose an individual product node and then find the six most similar products to it. This takes four steps:</p>
</div>
<ol>
<li class="readable-text" id="p91"> Produce node embeddings by running our nodes through our trained GNN. </li>
<li class="readable-text" id="p92"> Create a similarity matrix using the node embeddings. </li>
<li class="readable-text" id="p93"> Sort the top five embeddings by similarity to our chosen product. </li>
<li class="readable-text" id="p94"> Convert the node indices of these top embeddings to product IDs. </li>
</ol>
<div class="readable-text" id="p95">
<p>A seed may be set to ensure reproducibility. Otherwise, your results will differ with every run of your program.</p>
</div>
<div class="readable-text" id="p96">
<h4 class="readable-text-h4">Produce node embeddings</h4>
</div>
<div class="readable-text" id="p97">
<p>Like chapter 2, we run our nodes through the model to produce an embedding instead of a prediction. In contrast to chapter 2, we have a trained model for this purpose that has learned from the node features and the co-purchasing relationships of our dataset. To accomplish this, we put our model into evaluation mode (<code>eval()</code>), disable gradient computations that support backpropagation (<code>no_grad()</code>), and then run a forward pass of the graph data through the model. Earlier, when defining the model class, we enabled an option to return an embedding or a prediction (<code>return_embeds</code>):</p>
</div>
<div class="browsable-container listing-container" id="p98">
<div class="code-area-container">
<pre class="code-area">gcn_model.eval()

with torch.no_grad():
     gcn_embeddings = gcn_model(subset_graph.x, \
subset_graph.edge_index, return_embeds=True)</pre>
</div>
</div>
<div class="readable-text" id="p99">
<h4 class="readable-text-h4">Create a similarity matrix</h4>
</div>
<div class="readable-text" id="p100">
<p>A similarity matrix is a set of data, usually in tabular form, that contains the similarities between all pairs of items in a set. In our case, we use cosine similarity, and compare the embeddings of all the nodes in our set. SciKit Learn’s <code>cosine_similarity</code> function accomplishes this:</p>
</div>
<div class="browsable-container listing-container" id="p101">
<div class="code-area-container">
<pre class="code-area">gcn_similarity_matrix = cosine_similarity(gcn_embeddings.cpu().numpy())</pre>
</div>
</div>
<div class="readable-text" id="p102">
<h4 class="readable-text-h4">List the items closest in similarity to a chosen node</h4>
</div>
<div class="readable-text" id="p103">
<p>To identify items most similar to a specific node, we begin by selecting a node—referred to by its index as <code>product_idx</code>. Using the cosine similarity matrix, we examine how closely related each node is to our chosen node by sorting the similarities in descending order. The top entries from this sorting (specifically, the first six, where <code>top_k</code> is set to <code>6</code>) represent the nodes most similar to our selected node. Notably, the list includes the selected node itself, So, for practical purposes, we consider the next five nodes to effectively create a bundle of similar items:</p>
</div>
<div class="browsable-container listing-container" id="p104">
<div class="code-area-container">
<pre class="code-area">product_idx = 123 
top_k = 6
top_k_similar_indices_gcn = np.argsort(-
gcn_similarity_matrix[product_idx])[:top_k]</pre>
</div>
</div>
<div class="readable-text" id="p105">
<h4 class="readable-text-h4">Convert the node indices to product IDs</h4>
</div>
<div class="readable-text" id="p106">
<p>From here, using the index-to-ASIN dictionary will identify the product bundle from the node indices. With this done, let’s pick a product node at random and generate a product bundle around it.</p>
</div>
<div class="readable-text" id="p107">
<h4 class="readable-text-h4">Product Bundle Demo</h4>
</div>
<div class="readable-text" id="p108">
<p>At random, we pick node #123. Using our index-to-ASIN dictionary, we get ASIN: B00BV1P6GK. This ASIN belongs to the product Funko POP Television: Adventure Time Marceline Vinyl Figure, as shown in figure 3.3. The category of this product is Toys &amp; Games.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p109">
<img alt="figure" height="657" src="../Images/3-3.png" width="923"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.3</span> Our selected product, Funko POP Television: Adventure Time Marceline Vinyl Figure. In this section, a product bundle will be generated for this product.</h5>
</div>
<div class="readable-text" id="p110">
<p>Marceline, the hundreds-of-years-old Vampire Queen, is one of the main characters in the popular animated TV series <em>Adventure Time.</em> Marceline is known for her rock star persona, love of music, and playing her bass guitar, which is often a focal point in her appearances. Her persona is reflected in the figurine, which is smiling and has a relaxed but confident pose.</p>
</div>
<div class="readable-text intended-text" id="p111">
<p><em>Adventure Time</em> is an animated series that follows the surreal and epic adventures of a boy named Finn and his magical dog Jake in the mystical Land of Ooo, filled with princesses, vampires, ice kings, and many other bizarre characters.</p>
</div>
<div class="readable-text intended-text" id="p112">
<p>For a collection based on the <em>Adventure Time</em> series, one may expect a variety of vinyl figures representing the show’s eclectic cast of characters. Let’s see what our system generates.</p>
</div>
<div class="readable-text intended-text" id="p113">
<p>Using the process outlined earlier, the bundle shown in figure 3.4, was generated. There is one <em>Adventure Time</em> vinyl figure included. The rest of the choices seem unrelated at first glance, but maybe this set is a nonintuitive bundle. Let’s take a closer look:</p>
</div>
<ul>
<li class="readable-text" id="p114"> <em>First ranked similarity: Funko POP Television: Adventure Time Finn with Accessories</em>—Finn is the central character from <em>Adventure Time</em>, a recommendation we expected. This suggests that fans of Marceline might also appreciate or collect merchandise related to other main characters from the same show. </li>
<li class="readable-text" id="p115"> <em>Second ranked similarity: Funko My Little Pony: DJ Pon-3 Vinyl Figure</em>—This item might seem out of context at first glance, but it may indicate a crossover interest in animated series. DJ Pon-3, or Vinyl Scratch, from <em>My Little Pony</em> is a musical character like Marceline, appealing to those who enjoy characters associated with music.<span class="aframe-location"/> </li>
</ul>
<div class="browsable-container figure-container" id="p116">
<img alt="figure" height="469" src="../Images/3-4.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.4</span> A product bundle centered on the Marceline product. The recommendations are members of the Toy &amp; Games category. The themes of these products connect loosely to the selected product.</h5>
</div>
<ul>
<li class="readable-text" id="p117"> <em>Third ranked similarity: Funko My Little Pony: Twilight Sparkle Vinyl Figure</em>—Similar to DJ Pon-3, Twilight Sparkle from <em>My Little Pony</em> represents another connection to a popular animated series. This inclusion could appeal to collectors who enjoy fantasy themes and strong female characters. </li>
<li class="readable-text" id="p118"> <em>Fourth and fifth ranked similarities: Pirate-themed accessories (Gold Coins, Tattoos, Handheld Brass Telescope with Wooden Box)</em>—These items are less directly related to “Adventure Time” or “My Little Pony”, but they enhance the theme of adventure and exploration, which is a significant element of both series. </li>
</ul>
<div class="readable-text" id="p119">
<p>All in all, this is not a bad product bundle from our baseline models! Wrapping up this introductory section on model training and evaluation, we’ve now established a solid foundation for understanding and using GNNs. This understanding is crucial as we progress to section 3.2, where we’ll dive deeper into neighborhood aggregation, an effective tool to enhance performance. Then, in section 3.3, we’ll draw from general deep learning approaches to further optimize the models’ performances. </p>
</div>
<div class="readable-text" id="p120">
<h2 class="readable-text-h2"><span class="num-string">3.2</span> Aggregation methods</h2>
</div>
<div class="readable-text" id="p121">
<p>In this section, we extend the product category analysis from the previous section and take a deeper look into the characteristics of GNNs that influence their performance on tasks such as product categorization. Specifically, we explore aggregation methods, techniques that have a large influence on the performance of convolutional GNNs. Neighborhood aggregation allows nodes to gather and integrate feature information from their local node neighborhoods, capturing contextual relevance within the larger network.</p>
</div>
<div class="readable-text intended-text" id="p122">
<p>We start with the simple aggregations mean, sum, and max, each applied over all layers of a model. Then, we survey a few more advanced implementations in PyG: unique aggregations applied per layer, list aggregations, aggregation functions, and a layer-wise aggregation known as jumping knowledge networks (JK-Nets). Finally, we provide some guidelines on applying such methods.</p>
</div>
<div class="readable-text" id="p123">
<h3 class="readable-text-h3"><span class="num-string">3.2.1</span> Neighborhood aggregation</h3>
</div>
<div class="readable-text" id="p124">
<p>One way graph data structures are different is that nodes are interconnected through edges, creating a network where nodes can be directly linked or separated by several degrees. This spatial arrangement means that any given node may be in close proximity to certain other nodes, forming what we call its <em>neighborhood</em>. The concept of a node’s neighborhood is critical as it often holds key insights into the node’s characteristics and that of the overall graph.</p>
</div>
<div class="readable-text intended-text" id="p125">
<p>In convolutional GNNs, node neighborhoods are used through a process known as <em>neighborhood aggregation</em>. This technique involves gathering and combining feature information from a node’s immediate neighbors to capture both their individual and collective properties. By doing so, a node’s representation is enriched with the contextual information provided by its surroundings, which enhances the model’s capability to learn more complex and nuanced patterns within the graph.</p>
</div>
<div class="readable-text intended-text" id="p126">
<p>Neighborhood aggregation operates under the premise that nodes in proximity to each other are likely to influence each other more significantly than those farther away. This is particularly advantageous for tasks where the relationship and interaction between nodes are predictive of their behaviors or properties.</p>
</div>
<div class="readable-text" id="p127">
<h4 class="readable-text-h4">Neighborhood aggregation in PyG</h4>
</div>
<div class="readable-text" id="p128">
<p>In the PyG layers GCN (<code>GCNconv</code>) and GraphSAGE (<code>SAGEConv</code>), neighborhood aggregation is implemented in different ways. In GCN, a weighted average aggregation is built into the layer; if you want to tweak it, you must create a customized version of this layer. In this section, we’ll mostly focus on GraphSAGE, which allows you to set an aggregation via a parameter. An upcoming section will examine a layer-wise aggregation used in GCN. </p>
</div>
<div class="readable-text intended-text" id="p129">
<p>In <code>SAGEConv</code>, the <code>aggr</code> parameter specifies the type of aggregation. The options include, but are not limited to the following:</p>
</div>
<ul>
<li class="readable-text" id="p130"> <em>Sum aggregation</em>—A simple aggregation that sums up all neighbor node features. </li>
<li class="readable-text" id="p131"> <em>Mean aggregation</em>—Computes the mean of the neighbor node features. This is often used for its simplicity and effectiveness in averaging feature information, helping to smooth out anomalies in the data. </li>
<li class="readable-text" id="p132"> <em>Max aggregation</em>—Takes the maximum feature value among all neighbors for each feature dimension. This can help when the most prominent features are more informative than average features, capturing the most significant signals from the neighbors. </li>
<li class="readable-text" id="p133"> <em>LSTM aggregation</em>—A relatively compute- and memory-intensive method that uses an LSTM network to process features of the ordered sequence of neighbor nodes. It considers the sequence of nodes, which can be crucial for tasks where the order of node processing affects the results. As such, special care must be taken to arrange a dataset’s nodes and edges for training. </li>
</ul>
<div class="readable-text" id="p134">
<p>Choosing among these types will depend on the characteristics of a given graph, and the prediction goals. If you don’t have a good feel for which method will be more effective for your graph and your use case, trial and error can suffice to choose the aggregation method. In addition, while some of the aggregation options can be applied out of the box, others—such as LSTM aggregation, which relies on a trained LSTM network—require some thought to be put into data preparation.</p>
</div>
<div class="readable-text intended-text" id="p135">
<p>To see the effect of different aggregations, we add the <code>aggr</code> parameter to our model class and then proceed to train as in section 3.1, swapping out the mean, sum, and max aggregations. It should be noted that the mean aggregation is the default for the <code>SAGEConv</code> layer, so it’s equivalent to our GraphSAGE baseline model. Creating a <em>GraphSAGE</em> class with aggregations would look like the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p136">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.5</span> GraphSAGE class with aggregation parameter</h5>
<div class="code-area-container">
<pre class="code-area">class GraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, \
hidden_channels, out_channels, agg_func='mean'): <span class="aframe-location"/> #1
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_channels, \
hidden_channels, aggr=agg_func)  <span class="aframe-location"/> #2
        self.conv2 = SAGEConv(hidden_channels, \
out_channels, aggr=agg_func)  <span class="aframe-location"/> #3

    def forward(self, x, edge_index):
         x = self.conv1(x, edge_index)
         x = F.relu(x)
         x = self.conv2(x, edge_index)

    return F.log_softmax(x, dim=1)</pre>
<div class="code-annotations-overlay-container">
     #1 Sets keyword parameter for aggregation
     <br/>#2 First GraphSAGE layer with specified aggregation
     <br/>#3 Second GraphSAGE layer with specified aggregation
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p137">
<h4 class="readable-text-h4">Results of using mean, max, and sum aggregations</h4>
</div>
<div class="readable-text" id="p138">
<p>Table 3.3 compares the models using F1 score and log loss as performance metrics. The table shows that the model using max aggregation is the best under both measures. The results for the model using max aggregation shows the highest F1 score of 0.7449 and the lowest log loss of 2.1039, suggesting that max aggregation is a little more capable at identifying and using the most influential features in the prediction task. The model that uses mean aggregation is equivalent to the model trained in section 3.1. We observe that the max aggregation out-performs the other two. Overall, the performance using different aggregations is very similar to that of our baseline GraphSAGE model.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p139">
<h5 class="browsable-container-h5"><span class="num-string">Table 3.3</span> Classification performance for GraphSAGE models with different settings for neighborhood aggregation</h5>
<table>
<thead>
<tr>
<th>
<div>
         Aggregation Type 
       </div></th>
<th>
<div>
         F1 Score 
       </div></th>
<th>
<div>
         Log Loss 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Mean (default) <br/></td>
<td>  0.7406 <br/></td>
<td>  2.1214 <br/></td>
</tr>
<tr>
<td>  Sum <br/></td>
<td>  0.7384 <br/></td>
<td>  2.2496 <br/></td>
</tr>
<tr>
<td>  Max <br/></td>
<td>  0.7449 <br/></td>
<td>  2.1039 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p140">
<p>What model should be chosen if separate models had the highest F1 score and log loss? For example, what if the max aggregation model scored highest for F1, but the mean aggregation model took the highest for log loss? This will depend on the context of your application, your requirements for prediction, and the consequences of potential errors. </p>
</div>
<div class="readable-text intended-text" id="p141">
<p>In a healthcare situation, such as predicting patient readmissions within 30 days of discharge, for example, the choice of model can significantly affect patient outcomes and resource allocation. A model with a high F1 score would have a more balanced precision and recall, making it better in situations where missing a readmission could be costly or dangerous. It would be expected to identify more patients at risk, allowing for timely interventions. However, this could also result in higher false positives, leading to unnecessary treatments and increased costs.</p>
</div>
<div class="readable-text intended-text" id="p142">
<p>A model that exhibits low log loss, on the other hand, offers high confidence in its predictions, prioritizing the accuracy of each prediction over the number of positive cases detected. This model is useful when resource allocation needs to be precise or when treatments have substantial side effects. </p>
</div>
<div class="readable-text intended-text" id="p143">
<p>Coming back to our product manager who is deciding which products and product bundles to allocate marketing dollars to, having more confident predictions would be desirable in preventing wasted marketing efforts. The lower likelihood of false positives helps in efficiently using resources, but at the risk of missing some revenue-generating bundle configurations due to conservative predictions. </p>
</div>
<div class="readable-text intended-text" id="p144">
<p>In this section, we used a simple string argument for the <code>aggr</code> parameter. PyG, however, has a wide set of tools to incorporate a variety of aggregation methods into your models. We explore these in the next section.</p>
</div>
<div class="readable-text" id="p145">
<h3 class="readable-text-h3"><span class="num-string">3.2.2</span> Advanced aggregation tools</h3>
</div>
<div class="readable-text" id="p146">
<p>This section explores more advanced aggregation tools within PyG. We begin by assigning distinct aggregation methods to different layers within a multilayer architecture. Next, we explore the combination of various aggregation strategies—such as <code>'mean'</code>, <code>'max'</code>, and <code>'sum'</code>—within a single layer. Finally, we revisit GCNs to examine the jumping knowledge (JK) method.</p>
</div>
<div class="readable-text" id="p147">
<h4 class="readable-text-h4">Using multiple aggregations across layers</h4>
</div>
<div class="readable-text" id="p148">
<p>In a multilayer GraphSAGE model, you can of course adjust the aggregation function at each layer independently. For example, you might use mean aggregation at the first layer to smooth features but switch to max aggregation at a subsequent layer to highlight the most significant of the resulting neighbor features.</p>
</div>
<div class="readable-text intended-text" id="p149">
<p>As a first exercise in our exploration, let’s apply a couple of permutations of aggregations to two layers and see if these configurations outperform our previous results. We use the code from before, swapping out the <code>aggr</code> settings for <code>conv1</code> and <code>conv2</code>. For one model, we use <em>mean</em> for the first layer and <em>max</em> at the second. For the other model, we use <em>sum</em> for the first layer and <em>max</em> at the second. Table 3.4 summarizes the results.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p150">
<h5 class="browsable-container-h5"><span class="num-string">Table 3.4</span> Classification performance for GraphSAGE models with different settings for neighborhood aggregation</h5>
<table>
<thead>
<tr>
<th>
<div>
         Aggregation Type 
       </div></th>
<th>
<div>
         F1 Score 
       </div></th>
<th>
<div>
         Log Loss 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Mean (default) <br/></td>
<td>  0.7406 <br/></td>
<td>  2.1214 <br/></td>
</tr>
<tr>
<td>  Sum <br/></td>
<td>  0.7384 <br/></td>
<td>  2.2496 <br/></td>
</tr>
<tr>
<td>  Max <br/></td>
<td>  0.7449 <br/></td>
<td>  2.1039 <br/></td>
</tr>
<tr>
<td>  Layered: Mean <span class="regular-symbol">→</span> Max <br/></td>
<td>  0.7316 <br/></td>
<td>  2.2041 <br/></td>
</tr>
<tr>
<td>  Layered: Sum <span class="regular-symbol">→</span> Max <br/></td>
<td>  0.7344 <br/></td>
<td>  2.345 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p151">
<p>We have middling results at best for our dataset. The model with only max aggregation outperforms the newer models. Let’s move on to combining several aggregations for each layer.</p>
</div>
<div class="readable-text" id="p152">
<h4 class="readable-text-h4">List aggregations and aggregation functions</h4>
</div>
<div class="readable-text" id="p153">
<p>In PyG, the concept of using a list for specifying aggregation functions allows you to customize your models with multiple aggregation strategies simultaneously. This feature is significant as it enables the model to use different aspects of graph data, potentially enhancing model performance by capturing various properties of the graph. In a way, you’re aggregating your aggregations. For instance, you could combine <code>'mean'</code>, <code>'max'</code>, and <code>'sum'</code> aggregations in a single layer to capture average, most significant, and summed structural properties of the neighborhood. </p>
</div>
<div class="readable-text intended-text" id="p154">
<p>This works in PyG by passing a list of aggregation functions, either as strings or as <code>Aggregation</code> module instances, into the <code>MessagePassing</code> class. PyG resolves these strings against a predefined set of aggregation functions or can directly use an aggregation function as the <code>aggr</code> argument. For example, using the keyword <code>'mean'</code> invokes the <code>MeanAggregation()</code> function.</p>
</div>
<div class="readable-text intended-text" id="p155">
<p>There are a universe of combinations to try, but let’s try two examples to demonstrate, mixing familiar aggregations, <code>'</code>max<code>'</code>, <code>'</code>sum<code>'</code>, and <code>'</code>mean<code>'</code>; and a set of more exotic aggregations, <code>SoftmaxAggregation</code> and <code>StdAggregation</code> [3]. They can be applied to our <code>conv1</code> layer as follows (table 3.5 compares these results with previous results): </p>
</div>
<div class="browsable-container listing-container" id="p156">
<div class="code-area-container">
<pre class="code-area">       self.conv1 = SAGEConv(in_channels,\
 hidden_channels, aggr=['max', 'sum', 'mean'])

       self.conv1 = SAGEConv(in_channels,\
 hidden_channels, aggr=[SoftmaxAggregation(),\
 StdAggregation() ])</pre>
</div>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p157">
<h5 class="browsable-container-h5"><span class="num-string">Table 3.5</span> Classification performance for GraphSAGE models with list aggregations added</h5>
<table>
<thead>
<tr>
<th>
<div>
         Aggregation type 
       </div></th>
<th>
<div>
         F1 score 
       </div></th>
<th>
<div>
         Log loss 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Mean (default) <br/></td>
<td>  0.7406 <br/></td>
<td>  2.1214 <br/></td>
</tr>
<tr>
<td>  Sum <br/></td>
<td>  0.7384 <br/></td>
<td>  2.2496 <br/></td>
</tr>
<tr>
<td>  Max <br/></td>
<td>  0.7449 <br/></td>
<td>  2.1039 <br/></td>
</tr>
<tr>
<td>  Layered: Mean <span class="regular-symbol">→</span>Max <br/></td>
<td>  0.7316 <br/></td>
<td>  2.2041 <br/></td>
</tr>
<tr>
<td>  Layered: Sum <span class="regular-symbol">→</span>Max <br/></td>
<td>  0.7344 <br/></td>
<td>  2.345 <br/></td>
</tr>
<tr>
<td>  List (standard) <br/></td>
<td>  0.7484 <br/></td>
<td>  2.622 <br/></td>
</tr>
<tr>
<td>  List (exotic) <br/></td>
<td>  0.745 <br/></td>
<td>  2.156 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p158">
<p>Figure 3.5 visualizes the performance comparison from table 3.5. While the F1 scores are very similar, there is a slight performance boost in F1 score from the “standard” list aggregation, though with the drawback of a much higher log loss.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p159">
<img alt="figure" height="679" src="../Images/3-5.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.5</span> Performance comparison visualized from table 3.5. While the F1 scores are very similar, the standard list aggregation performs slightly better with respect to log loss.</h5>
</div>
<div class="readable-text" id="p160">
<p>Given the results of our quick survey of these aggregation methods applied to the GraphSAGE layer, you might conclude that sticking with the default setting is often the best option. However, the potential for performance improvements through tailored aggregation strategies suggests that further exploration could be beneficial. </p>
</div>
<div class="readable-text intended-text" id="p161">
<p>In the upcoming section 3.2.3, we’ll review some considerations in applying these aggregation methods. Before that, we’ll come back to the GCN layer to examine the JK aggregation method.</p>
</div>
<div class="readable-text" id="p162">
<h4 class="readable-text-h4">Jumping knowledge networks</h4>
</div>
<div class="readable-text" id="p163">
<p><em>Jumping knowledge </em>(JK) is a novel approach for node representation learning on graphs that addresses limitations of existing models such as GCNs and GraphSAGE [4]. It focuses on overcoming a problem of neighborhood aggregation models in which models are sensitive to the graph’s structure, causing inconsistent learning quality across different graph parts.</p>
</div>
<div class="readable-text intended-text" id="p164">
<p>Jumping knowledge networks (JK-Nets) allow flexible usage of different neighborhood ranges for each node, thereby adapting to local neighborhood properties and task-specific requirements. This adaptation results in improved node representations by enabling the model to selectively use information from various neighborhood depths based on the node and the subgraph’s context. JK has been implemented for the GCN layer in PyG, as shown in listing 3.6. </p>
</div>
<div class="readable-text intended-text" id="p165">
<p>Its main parameter, <code>mode</code>, specifies the aggregation scheme used to combine outputs from different layers. The options are listed here:</p>
</div>
<ul>
<li class="readable-text" id="p166"> <code>'cat'</code>—Concatenates the outputs from all layers along the feature dimension. This approach preserves all information from each layer but increases the dimensionality of the output. </li>
<li class="readable-text" id="p167"> <code>'max'</code>—Applies max pooling across the layer outputs. This method takes the maximum value across all layers for each feature, which can help in capturing the most significant features from the graph while being robust against less informative signals. </li>
<li class="readable-text" id="p168"> <code>'lstm'</code>—Uses a bidirectional LSTM to learn attention scores for each layer’s output. The outputs are then combined based on these learned attention weights, allowing the model to focus on the most relevant layers dynamically based on the input graph structure. </li>
</ul>
<div class="browsable-container listing-container" id="p169">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.6</span> GCN class with <code>JumpingKnowledge</code> layer</h5>
<div class="code-area-container">
<pre class="code-area">class CustomGCN(torch.nn.Module):
   def __init__(self, in_channels, hidden_channels, out_channels):
       super(CustomGCN, self).__init__()
       self.conv1 = GCNConv(in_channels, hidden_channels)
       self.conv2 = GCNConv(hidden_channels, out_channels)

       self.jk = JumpingKnowledge(mode='cat') <span class="aframe-location"/> #1

   def forward(self, x, edge_index):
       layer_outputs = [] <span class="aframe-location"/> #2

       x1 = self.conv1(x, edge_index)
       x1 = F.relu(x1)
       layer_outputs.append(x1) <span class="aframe-location"/> #3

       x2 = self.conv2(x1, edge_index)
       layer_outputs.append(x2) 


       x = self.jk(layer_outputs) <span class="aframe-location"/> #4

       return x</pre>
<div class="code-annotations-overlay-container">
     #1 Initializes JK with concatenation mode
     <br/>#2 List to save outputs from each layer for JK
     <br/>#3 Appends the layer outputs list 
     <br/>#4 Applies JK aggregation to the collected layer outputs
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p170">
<p>In the listing, for the initialization, a <code>JumpingKnowledge</code> layer is initialized with the mode set to <code>'cat'</code> (concatenate), indicating that the features from each layer will be concatenated to form the final node representations. </p>
</div>
<div class="readable-text intended-text" id="p171">
<p>In the forward pass, <code>layer_outputs</code> is initialized as an empty list to store the outputs from each convolutional layer. This list will be used by the <code>JumpingKnowledge</code> layer.</p>
</div>
<ul>
<li class="readable-text" id="p172"> The first convolutional layer processes the input <code>x</code> and the graph structure <code>edge_index</code>, and applies a ReLU activation function to introduce nonlinearity. </li>
<li class="readable-text" id="p173"> The output of the first layer (<code>x1</code>) is then added to the <code>layer_outputs</code> list. </li>
<li class="readable-text" id="p174"> After the second convolutional layer, the second output (<code>x2</code>) is also added to the <code>layer_outputs</code> list. </li>
<li class="readable-text" id="p175"> Then, the <code>JumpingKnowledge</code> layer takes the list of outputs from all the previous layers and aggregates them according to the specified mode (<code>'cat'</code>). In concatenation mode, the feature vectors from each layer are concatenated along the feature dimension. </li>
</ul>
<div class="readable-text" id="p176">
<p>Table 3.6 compares the classification performance for GCN models. The baseline GCN model from section 3.1 is compared to a version using the <code>JumpingKnowledge</code> aggregation method. The baseline model has a better F1 score, while the JK model outperforms in log loss.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p177">
<h5 class="browsable-container-h5"><span class="num-string">Table 3.6</span> Classification performance for GCN models</h5>
<table>
<thead>
<tr>
<th>
<div>
         Model 
       </div></th>
<th>
<div>
         F1 Score 
       </div></th>
<th>
<div>
         Log Loss 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Baseline GCN <br/></td>
<td>  0.781 <br/></td>
<td>  1.42 <br/></td>
</tr>
<tr>
<td>  JK (GCN) <br/></td>
<td>  0.699 <br/></td>
<td>  1.36 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p178">
<p>The results show that choosing between the baseline and JK versions involves a tradeoff between higher recall/precision and higher prediction certainty. This tradeoff should be carefully considered based on the specific requirements and goals of the task at hand. Further exploration in section 3.2.3 will review some considerations in applying these aggregation methods effectively.</p>
</div>
<div class="readable-text" id="p179">
<h3 class="readable-text-h3"><span class="num-string">3.2.3</span> Practical considerations in applying aggregation</h3>
</div>
<div class="readable-text" id="p180">
<p>Choosing the right aggregation method is a technical decision that should be informed by the specific characteristics and needs of the dataset at hand as well as the use case. For datasets where the local neighborhood structure is crucial, using mean or sum aggregations could potentially blur essential features. In contrast, max aggregation could help highlight critical attributes. For example, in a social network graph where influencer detection is key, max aggregation might be more effective. On the other hand, if what we want to do is represent typical features, max aggregation may overemphasize outliers. In a dataset of financial transactions, where we want to understand typical user behavior, a max aggregation could distort the common behavioral features in favor of one or two large but uncommon transactions. </p>
</div>
<div class="readable-text intended-text" id="p181">
<p>The task itself can dictate the choice of aggregation method. Tasks that require capturing the most influential features might benefit from max aggregation, while those needing a general representation may find mean aggregation sufficient. In a recommendation system for products, max aggregation could help identify the most important product features that drive purchases. Additionally, the nature of the graph’s topology should guide the aggregation method. Densely connected graphs might require different strategies compared to sparsely connected graphs to avoid over-smoothing or underrepresentation of node features. For instance, a transportation network graph with varying node connectivity might need different aggregations at different layers.</p>
</div>
<div class="readable-text intended-text" id="p182">
<p>Given the dataset’s complexity, empirical testing of different aggregation methods is essential. Experimentation can help identify which methods best capture the relational dynamics and feature distributions of the dataset. This is particularly important for more exotic aggregation methods, where intuition alone may not suffice to determine their effectiveness. The scalability of the chosen aggregation method to handle millions of nodes and edges efficiently is also crucial. It’s important to balance computational efficiency with the sophistication of the method, especially for real-time applications such as recommendation systems. </p>
</div>
<div class="readable-text intended-text" id="p183">
<p>Aggregation methods should be considered alongside other model enhancements such as feature engineering, node embedding techniques, and regularization strategies to address overfitting and improve model generalization. For instance, combining effective aggregation methods with advanced embedding techniques (e.g., Node2Vec) or incorporating dropout for regularization could significantly boost model performance.</p>
</div>
<div class="readable-text intended-text" id="p184">
<p>While there is no one-size-fits-all aggregation method, a thoughtful combination of techniques, backed by empirical validation, can significantly enhance model performance and applicability. This strategic approach not only aids in accurate product categorization but also in crafting effective recommendation systems that are crucial in e-commerce settings.</p>
</div>
<div class="readable-text intended-text" id="p185">
<p>This section explored and applied different aggregations to our models. The next section will round out our exploration of convolutional GNNs by applying regularization and adjusting the depth of our models. We’ll consolidate our improvements into a final model, from which we’ll generate another product bundle based on the Marceline figurine to see if there is improvement.</p>
</div>
<div class="readable-text" id="p186">
<h2 class="readable-text-h2"><span class="num-string">3.3</span> Further optimizations and refinements</h2>
</div>
<div class="readable-text" id="p187">
<p>Up to now, the GCN and GraphSAGE layers have been introduced via a product management example. We established a baseline using the default settings in section 3.1. In section 3.2, we examined the use of neighborhood and layer aggregation. In this section, we’ll consider other ways we can refine and improve our model. In the first subsections, we’ll introduce two other adjustments: the use of dropout and model depth. Dropout is a well-known regularization technique that can reduce overfitting, and model depth is an adjustment that has a unique meaning for GNNs. </p>
</div>
<div class="readable-text intended-text" id="p188">
<p>In section 3.3.3, we synthesize these insights to develop a model that incorporates multiple improvements and observe the cumulative performance uplift. Finally, in section 3.3.4, we revisit our product bundle problem. We create a new product bundle using the refined model of section 3.3.3 and compare its performance to the bundle created in section 3.1.</p>
</div>
<div class="readable-text" id="p189">
<h3 class="readable-text-h3"><span class="num-string">3.3.1</span> Dropout</h3>
</div>
<div class="readable-text" id="p190">
<p><em>Dropout</em> is a regularization technique used to prevent overfitting in neural networks by randomly dropping units during training. This helps the model generalize better by reducing its dependency on specific neurons. </p>
</div>
<div class="readable-text intended-text" id="p191">
<p>In PyG, the <code>dropout</code> function works similarly to the standard PyTorch dropout, meaning it randomly sets some elements of the input tensor and the hidden-layer activations to <code>0</code> during the training process. During each forward pass in training, inputs and activations are set to <code>0</code> according to the specified dropout rate. This helps prevent overfitting by ensuring that the model doesn’t rely too heavily on any particular input or activation. </p>
</div>
<div class="readable-text intended-text" id="p192">
<p>The structure of the graph, including its vertices (nodes) and edges, remains unchanged during dropout. The graph’s topology is preserved, and only the neural network’s activations are affected. This distinction is crucial as it maintains the integrity of the graph while still using dropout to improve model robustness. PyG does have functions that can drop nodes or drop edges during training, but the dropout built into <code>GCNConv</code> and <code>SAGEConv</code> refers to the traditional deep learning dropout.</p>
</div>
<div class="readable-text intended-text" id="p193">
<p>In PyG, both the GraphSAGE and GCN layers use <code>dropout</code> rate as a parameter, with a default of 0. Figure 3.6 illustrates the performance of GCN models with varying dropout rates (0%, 50%, and 85%). As shown, higher dropout rates can help mitigate overfitting, as indicated by the reduced gap between training and validation losses. For the 85% case, the higher dropout rate could be causing the model to converge more slowly, or it could be a sign of overfitting. More testing is warranted to find out.</p>
</div>
<div class="readable-text intended-text" id="p194">
<p>Next, let’s examine model depth and how it’s implemented for convolutional GNNs. </p>
</div>
<div class="readable-text" id="p195">
<h3 class="readable-text-h3"><span class="num-string">3.3.2</span> Model depth</h3>
</div>
<div class="readable-text" id="p196">
<p>In GNNs, a <em>layer</em> refers to the number of hops or message-passing steps. Each layer allows nodes to aggregate information from their immediate neighbors, effectively increasing the receptive field by one hop per layer. A three-layer model, for example, would interrogate the neighborhood three hops away from each node. The <em>depth</em> of a GNN, then, refers to the number of layers in the network, analogous to the depth in traditional deep learning models but with key differences due to graph-structured data. </p>
</div>
<div class="readable-text intended-text" id="p197">
<p>If a GNN has too few layers, it may not capture sufficient information from the graph, leading to poor representation learning, as each node can only aggregate information from a limited neighborhood. Conversely, increasing the number of layers can lead to <em>over-smoothing</em>, where node features become too similar, making it difficult to distinguish between different nodes. With each additional layer, nodes aggregate information from a larger neighborhood, diluting the unique features of individual nodes. Various metrics and methods have been proposed to measure and mitigate this effect. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p198">
<img alt="figure" height="1004" src="../Images/3-6.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.6</span> Training curve comparisons of three models with different levels of dropout. The left has a dropout of 0%, the middle a dropout of 50%, and the right a dropout of 85%. For our model and dataset, adding dropout indeed ameliorates overfitting. The model with 85% dropout could show signs of either underfitting or slowly converging, requiring more experimentation.</h5>
</div>
<div class="readable-text intended-text" id="p199">
<p>The performance of GNNs with different depths can vary significantly. Typically, GNNs with 2 or 3 layers perform competitively on many tasks, balancing the need for sufficient neighborhood information without causing over-smoothing. While deeper GNNs can theoretically capture more complex patterns, they often suffer from over-smoothing and increased computational complexity. Very deep GNNs, such as those with 50 or more layers, can lead to higher validation loss, indicating over-fitting and/or over-smoothing.</p>
</div>
<div class="readable-text intended-text" id="p200">
<p>Figure 3.7 compares the performance of GNNs with different depths (e.g., 2 layers, 10 layers, and 50 layers). <span class="aframe-location"/>We see that the 2-layer model achieves a good balance between training and validation loss. In the 10-layer GNNs, we see some improvement in training loss but also signs of over-smoothing from the higher validation loss. The 50-layer model shows degraded training and validation loss, which indicates severe over-smoothing or over-fitting.</p>
</div>
<div class="browsable-container figure-container" id="p201">
<img alt="figure" height="1785" src="../Images/3-7.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.7</span> Training curves for trained models of different depths: 2 layers (top), 10 layers (middle), 50 layers (bottom). The 2-layer model has the best profile with no signs of overfitting or performance degradation. </h5>
</div>
<div class="readable-text intended-text" id="p202">
<p>Balancing the depth of the model is critical to achieving optimal performance in GNNs. Too few layers may result in weak representation learning, while too many layers can lead to over-smoothing, where node features become indistinguishable. In the next section, we apply what we’ve learned about tuning our model in this chapter, resulting in a refined model that will outperform our baseline.</p>
</div>
<div class="readable-text" id="p203">
<h3 class="readable-text-h3"><span class="num-string">3.3.3</span> Improving the baseline model’s performance</h3>
</div>
<div class="readable-text" id="p204">
<p>Given all the insights gained in this chapter, let’s train models that synthesize these learnings and compare them against the baseline. Some key takeaways from the previous sections that we’ll incorporate are listed here:</p>
</div>
<ul>
<li class="readable-text" id="p205"> <em>Model depth</em>—We’ll keep it low, at two layers. </li>
<li class="readable-text" id="p206"> <em>Neighborhood aggregation</em>—We’ll use max aggregation and experiment with two list aggregations. The same aggregation will be used on both layers. </li>
<li class="readable-text" id="p207"> <em>Dropout</em>—We’ll use 50% dropout on both layers. </li>
</ul>
<div class="readable-text" id="p208">
<p>The following listing shows a GraphSAGE class with adjustable dropout, layer depth, and aggregations.</p>
</div>
<div class="browsable-container listing-container" id="p209">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.7</span> GraphSAGE class</h5>
<div class="code-area-container">
<pre class="code-area">class GraphSAGEWithCustomDropout(torch.nn.Module):
   def __init__(self, in_channels, \
hidden_channels, out_channels, num_layers, \
dropout_rate=0.5, aggr='mean'): <span class="aframe-location"/> #1
       super(GraphSAGEWithCustomDropout, self).__init__()
       self.layers = torch.nn.ModuleList\
([SAGEConv(in_channels, hidden_channels, aggr=aggr)])
       for _ in range(1, num_layers-1): <span class="aframe-location"/> #2
           self.layers.append(SAGEConv\
(hidden_channels, hidden_channels, aggr=aggr))
       self.layers.append(SAGEConv\
(hidden_channels, out_channels, aggr=aggr))
       self.dropout_rate = dropout_rate

   def forward(self, x, edge_index):
       for layer in self.layers[:-1]:
           x = F.relu(layer(x, edge_index))
           x = F.dropout(x, p=self.dropout_rate, training=self.training)
       x = self.layers[-1](x, edge_index)
       return F.log_softmax(x, dim=1)</pre>
<div class="code-annotations-overlay-container">
     #1 The layer is initialized with number of layers, dropout rate, and aggregation type.
     <br/>#2 The loop applies the aggregation to each layer.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p210">
<p>We trained three models using the preceding class:</p>
</div>
<div class="browsable-container listing-container" id="p211">
<div class="code-area-container">
<pre class="code-area">model_1 = GraphSAGEWithCustomDropout\
(subset_graph.num_features, 64, \
dataset.num_classes, 2, dropout_rate=.5, \
aggr= ‘max’).to(device)

model_2 = GraphSAGEWithCustomDropout\
(subset_graph.num_features, 64, \
dataset.num_classes, 2, dropout_rate=0.5, \
aggr=['max', 'sum', 'mean']).to(device)

model_3 = GraphSAGEWithCustomDropout\
(subset_graph.num_features, 64, \
dataset.num_classes, 2, dropout_rate=0.50,\
 aggr=[SoftmaxAggregation(), \
StdAggregation() ] ).to(device)</pre>
</div>
</div>
<div class="readable-text" id="p212">
<p>Table 3.7 summarizes the performance of different GraphSAGE models with various aggregation methods and a baseline model using the default mean aggregation. The results indicate that all the improved models outperform the baseline in both F1 score and log loss. Notably, Model 2, which uses a combination of <code>'max'</code>, <code>'sum'</code>, and <code>'mean'</code> aggregations, achieved the highest F1 score of 0.8828. Model 3, with a combination of <code>SoftmaxAggregation()</code> and <code>StdAggregation()</code>, shows the best log loss at 0.5764, suggesting it has the highest prediction certainty among the tested configurations.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p213">
<h5 class="browsable-container-h5"><span class="num-string">Table 3.7</span> Two-layer GraphSAGE models using 50% dropout and different aggregation types </h5>
<table>
<thead>
<tr>
<th>
<div>
         GraphSAGE Model 
       </div></th>
<th>
<div>
         Aggregation Type 
       </div></th>
<th>
<div>
         F1 Score 
       </div></th>
<th>
<div>
         Log Loss 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Model 1 <br/></td>
<td> <code>'max'</code> <br/></td>
<td>  0.8674 <br/></td>
<td>  0.594 <br/></td>
</tr>
<tr>
<td>  Model 2 <br/></td>
<td> <code>['max', 'sum', 'mean']</code> <br/></td>
<td>  0.8876 <br/></td>
<td>  0.660 <br/></td>
</tr>
<tr>
<td>  Model 3 <br/></td>
<td> <code>[SoftmaxAggregation(), StdAggregation()]</code> <br/></td>
<td>  0.8829 <br/></td>
<td>  0.574 <br/></td>
</tr>
<tr>
<td>  Baseline model <br/></td>
<td>  Mean (default) <br/></td>
<td>  0.7406 <br/></td>
<td>  2.1214 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p214">
<p>The confusion matrix in figure 3.8 visualizes the classification performance of Model 1 with max aggregation. The majority of values are along the diagonal, indicating that the model is correctly classifying most instances. However, there are off-diagonal elements, representing misclassifications, for example, instances of class 0 being classified as class 1 or vice versa. The frequency and spread of these misclassifications highlight areas where the model struggles. Additionally, using the bar on the side indicating the count of members per class, the confusion matrix shows how these different classes are distributed. Some classes have higher counts, while others have significantly lower counts, suggesting class imbalance in the dataset.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p215">
<img alt="figure" height="865" src="../Images/3-8.png" width="1050"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.8</span> Confusion matrix from the two-layer GraphSAGE model with 50% dropout and max aggregation. The strong diagonal pattern indicates good classification performance. The sidebar gives a distribution of the classes, highlighting a class imbalance.</h5>
</div>
<div class="readable-text" id="p216">
<p>Note that, in all of this, we’ve been using less than 1% of the dataset’s nodes, arbitrarily chosen by index order. Increasing the number of nodes would improve the performance of our models. Additionally, selecting a subgraph in a more meaningful way while keeping the number of nodes the same could also increase performance.</p>
</div>
<div class="readable-text intended-text" id="p217">
<p>While the current models show significant improvement, several other strategies can be considered to further enhance performance. Increasing the dataset size by using a larger subset can provide more training data, potentially improving model generalization. Refining subgraph selection based on domain knowledge or using graph sampling techniques can ensure that more meaningful data is used for training. Hyperparameter optimization, systematically tuning hyperparameters using tools such as Hyperopt, can help find the optimal settings for the model. Hyperopt allows for efficient searching of the hyperparameter space using algorithms such as Bayesian optimization. Exploring more sophisticated aggregation functions or custom aggregations tailored to the specific characteristics of the dataset can also yield improvements. Additionally, implementing regularization methods such as L2 regularization or gradient clipping can stabilize training and prevent over-fitting. Graph preprocessing techniques, such as normalization, feature engineering, and dimensionality reduction on graph features, can enhance the quality of input data, further <span class="aframe-location"/>boosting model performance. Next, we’ll select the model that performs highest on log loss to generate another product bundle.</p>
</div>
<div class="readable-text" id="p218">
<h3 class="readable-text-h3"><span class="num-string">3.3.4</span> Revisiting the Marcelina product bundle</h3>
</div>
<div class="readable-text" id="p219">
<p>The models have markedly improved from our baselines in section 3.1. Let’s revisit the product bundling problem and recommend one for our product manager based on our refined GraphSAGE model from the earlier section. Using the process from section 3.1.5 results in the bundle in figure 3.9, which is displayed with the original bundle for comparison. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p220">
<img alt="figure" height="584" src="../Images/3-9.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.9</span> Product bundles centered on the Marceline product. The upper bundle is from the improved model from section 3.3.3, while the lower bundle is from the baseline model of section 3.1.5. The new recommendations are members of the Toy &amp; Games, Books, and Movies &amp; TV categories. </h5>
</div>
<div class="readable-text" id="p221">
<p>What do you think of this new bundle? Is it an improvement, that is, more likely to drive purchases than the former bundle? This new bundle incorporates items from Toys &amp; Games, Books, and Movies &amp; TV categories, which is a diverse product selection. The introduction of the <em>Wild Kratts: Wildest Animal Adventures</em> DVD alongside the adventure book <em>The Sword of Shannara</em> and action figures reflects a pivot toward a more family-oriented and child-friendly product mix.</p>
</div>
<div class="readable-text intended-text" id="p222">
<p>This new bundle’s potential for driving purchases is grounded in the enhanced understanding of customer purchase behaviors and preferences captured by the updated model. The bundle seems well-suited for gifting purposes, catering to both the collectors of pop culture memorabilia (e.g., the Marceline figure and related collectibles) and young fans of fantasy and adventure narratives.</p>
</div>
<div class="readable-text intended-text" id="p223">
<p>The shift from a more generic collection of toys to a focused, theme-oriented bundle could likely increase its attractiveness as a purchase. The inclusion of both entertainment (<em>Wild Kratts: Wildest Animal Adventures</em> DVD) and literary (<em>The Sword of Shannara</em>) elements, in addition to the collectible figures, provides a more comprehensive entertainment experience centered around popular themes of adventure and exploration. This could appeal to parents looking for engaging and themed gifts that also offer educational value, such as the animal- and nature-related content of Wild Kratts.</p>
</div>
<div class="readable-text intended-text" id="p224">
<p>It’s crucial to consider the psychological effect of a well-curated bundle. By aligning the products more closely with identified customer interests and cross-selling patterns, the bundle not only caters to existing demand but also encourages additional purchases by enhancing perceived value by how the bundled items complement each other.</p>
</div>
<div class="readable-text intended-text" id="p225">
<p>Ultimately, the decision on whether this new bundle is an improvement over the original should be validated through customer feedback and sales data. Tracking the sales performance of both bundles (as well as bundles suggested by human product managers) and gathering direct customer insights through surveys or A/B testing would be beneficial to quantitatively assess which bundle performs better in terms of sales and customer satisfaction. This data-driven approach will confirm the theoretical benefits of the advanced modeling techniques used in the new bundle’s creation.</p>
</div>
<div class="readable-text intended-text" id="p226">
<p>With this, we conclude the hands-on product example of this chapter. The next two sections are optional as they dive deeper into the theory of convolutional GNNs and take a closer look at the Amazon Products dataset. </p>
</div>
<div class="readable-text" id="p227">
<h2 class="readable-text-h2"><span class="num-string">3.4</span> Under the hood</h2>
</div>
<div class="readable-text" id="p228">
<p>Now that we’ve created and refined a working convolutional GNN, let’s dig deeper into the elements of a GNN to better understand how they work. Such knowledge can help when we want to design new GNNs or troubleshoot a GNN.</p>
</div>
<div class="readable-text intended-text" id="p229">
<p>In chapter 2, we introduced the idea of using GNN layers to produce a prediction or create embeddings using message passing. Here’s that architecture diagram again, reproduced in figure 3.10. </p>
</div>
<div class="readable-text intended-text" id="p230">
<p>Let’s get below the surface of a GNN layer and examine its elements. Then, we’ll tie this to the concept of aggregation functions.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p231">
<img alt="figure" height="1533" src="../Images/3-10.png" width="450"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.10</span> Node embedding architecture diagram from chapter 2</h5>
</div>
<div class="readable-text" id="p232">
<h3 class="readable-text-h3"><span class="num-string">3.4.1</span> Convolution methods</h3>
</div>
<div class="readable-text" id="p233">
<p>Let’s first consider one of the most popular architectures for deep learning, the convolutional neural network (CNN). CNNs are typically used for computer vision tasks such as segmentation or classification. A CNN layer can be thought of as having a sequence of operations that are applied to input data:</p>
</div>
<div class="readable-text indented-paragraph equation-paragraph" id="p234">
<p>Layer: Filter <span class="regular-symbol">→</span> Activation function <span class="regular-symbol">→</span> Pooling</p>
</div>
<div class="readable-text" id="p235">
<p>The output of each entire layer is some transformed data that makes some downstream task easier or more successful. These transformation operations include the following:</p>
</div>
<ul>
<li class="readable-text" id="p236"> <em>Filter (or kernel operation)</em>—A process that transforms the input data. The filter is used to highlight some specific features of the input data and consists of learnable weights that are optimized by an objective or loss function. </li>
<li class="readable-text" id="p237"> <em>Activation Function</em>—A nonlinear transformation applied to the filter output. </li>
<li class="readable-text" id="p238"> <em>Pooling</em>—An operation that reduces the size of the filter output for subsequent learning tasks. </li>
</ul>
<div class="readable-text" id="p239">
<p>CNNs and many GNNs share a common foundation: the concept of convolution. You read about the concept of convolution when we discussed the three operations used in a CNN. Convolution in both CNNs and GNNs is all about learning by establishing <em>hierarchies of localized patterns</em> in the data. For CNNs this might be used for image classification, whereas a convolutional GNN, such as a GCN, might use convolution to predict features of nodes. To emphasize this point, CNNs apply convolution to a fixed grid of pixels to identify patterns in the grid. GCN models apply convolution to graphs of nodes to identify patterns in the graph. </p>
</div>
<div class="readable-text intended-text" id="p240">
<p>I referred to the <em>concept</em> of convolution in the previous paragraph because the convolution can be implemented in different ways. Theoretically, convolution relates to the mathematical convolution operator, which we’ll be discussing in more detail shortly. For GNNs, convolution can be separated into spatial and spectral methods [1, 5, 6]:</p>
</div>
<ul>
<li class="readable-text" id="p241"> <em>Spatial</em>—Sliding a window (filter) across a graph. </li>
<li class="readable-text" id="p242"> <em>Spectral</em>—Filtering a graph signal using spectral methods. </li>
</ul>
<div class="readable-text" id="p243">
<h4 class="readable-text-h4">Spatial methods</h4>
</div>
<div class="readable-text" id="p244">
<p>In traditional deep learning, convolutional processes learn data representations by applying a special filter called a <em>convolutional kernel</em> to input data. This kernel is smaller in size than the input data and is applied by moving it across the input data. This is shown in figure 3.11, where we apply our convolutional kernel (the matrix in the center) to an image of a lion. The resulting image has been inverted due to our convolutional kernel, which has negative values for all non-center elements. We can see that some of the features have been emphasized, such as the outline of the lion. This highlights the filtering aspect of convolutions.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p245">
<img alt="figure" height="296" src="../Images/3-11.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.11</span> A convolution of an input image (left). The kernel (middle) is passed over the image of an animal, resulting in a distinct representation (right) of the input image. In a deep learning process, the parameters of the filter (the numbers in the matrix) are learned parameters.</h5>
</div>
<div class="readable-text" id="p246">
<p>This use of convolutional networks is particularly common in the computer vision domain. For example, when learning on 2D images, we can apply a simple CNN of a few layers. In each layer, we pass a 2D filter (kernel) over each image. The 3 × 3 filter works on an image many times its size. We can produce learned representations of the input image by doing this over successive layers.</p>
</div>
<div class="readable-text intended-text" id="p247">
<p>For graphs, we want to apply this same idea of moving a window across our data, but now we need to make adjustments to account for the relational and non-Euclidian topology of our data. For images, we’re dealing with rigid 2D grids; for graphs, we’re dealing with data that has no fixed shape or order. Without a predefined ordering of the nodes in a graph, we use the concept of a <em>neighborhood</em>, consisting of a starting node, and all of its one-hop neighbors (i.e., all nodes within one hop from the central node). Then, our sliding window moves across a graph by moving across its node neighborhoods. </p>
</div>
<div class="readable-text intended-text" id="p248">
<p>In figure 3.12, we see an illustration comparing convolution applied to grid data and applied to graph data. In the grid case, pixel values are filtered around the nine pixels immediately surrounding the central pixel (marked with a gray dot). However, for a graph, node attributes are filtered based on all nodes that can be connected by one edge. Once we have the nodes that we’ll be considering, we then need to perform some operation on the nodes. This is known as the <em>aggregation operation</em>; for example, all the node weights in a neighborhood might be averaged or summed, or we might take the max value. What is important for graphs is that this operation is <em>permutation invariant</em>. The order of the nodes shouldn’t matter.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p249">
<img alt="figure" height="362" src="../Images/3-12.png" width="859"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.12</span> A comparison of convolution over grid data (left; e.g., a 2D image) and over a graph (right).</h5>
</div>
<div class="readable-text" id="p250">
<h4 class="readable-text-h4">Spectral methods</h4>
</div>
<div class="readable-text" id="p251">
<p>To introduce the second method of convolution, let’s examine the concept of a graph signal [6]. In the field of information processing, <em>signals</em> are sequences that can be examined in either time or frequency domains. When studying a signal in the time domain, we consider its dynamics, namely how it changes over time. From the frequency domain, we consider how much of the signal lies within each frequency band.</p>
</div>
<div class="readable-text intended-text" id="p252">
<p>We can also study the signal of a graph in an analogous way. To do this, we define a graph signal as a vector of node features. Thus, for a given graph, its set of node weights can be used to construct its signal. As a visual example, in figure 3.13, we have a graph with values associated with each node, where the height of each respective bar represents some node feature.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p253">
<img alt="figure" height="329" src="../Images/3-13.png" width="362"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.13</span> A random positive graph signal on the vertices of the graph. The height of each vertical bar represents the signal value at the node where the bar originates.</h5>
</div>
<div class="readable-text" id="p254">
<p>To operate on this graph signal, we represent the graph signal as a matrix, where each row is a set of features associated with a particular node. We can then apply operations from signal processing on the graph matrix. One critical operation is that of the Fourier transform. The Fourier transform can express a graph signal, its set of node features, into a frequency representation. Conversely, an inverse Fourier transform will revert the frequency representation into a graph signal.</p>
</div>
<div class="readable-text" id="p255">
<h4 class="readable-text-h4">Above and beyond: Limitations of traditional deep learning methods to graphs</h4>
</div>
<div class="readable-text" id="p256">
<p>Why can’t we apply CNNs directly to a graph structure? The reason is because graph representations have an ambiguity that image representations don’t. CNNs, and traditional deep learning tools in general, can’t resolve this ambiguity. A neural network that can deal with this ambiguity is said to be <em>permutation equivariant</em> or <em>permutation invariant</em>.</p>
</div>
<div class="readable-text intended-text" id="p257">
<p>Let’s illustrate the ambiguity of a graph versus an image by considering the image of the lion shown earlier. A simple representation of this set of pixels is as a 2D matrix (with dimensions for height and width). This representation will be unique: if we swap out two rows of the image, or two columns, we don’t have an equivalent image. Similarly, if we swap out two columns or rows of the matrix representation of the image (as shown in figure 3.14), we don’t have an equivalent matrix.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p258">
<img alt="figure" height="424" src="../Images/3-14.png" width="1005"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.14</span> The image of a lion is unique (left). If we swap out two columns (right), we end up with a distinct photo with respect to the original.</h5>
</div>
<div class="readable-text intended-text" id="p259">
<p>This isn’t the case with a graph. Graphs can be represented by adjacency matrices (described in chapter1 and appendix A), such that each row and column element stands for the relation between two nodes. If an element is nonzero, it means that the row node and column node are linked. Given such a matrix, we can repeat our previous experiment and swap out two rows as we did with the image. Unlike the case of the image, we end up with a matrix that represents the graph we started with. We can do any number of permutations or rows and columns and end up with a matrix that represents the same graph.</p>
</div>
<div class="readable-text intended-text" id="p260">
<p>Returning to the convolution operation, to successfully apply a convolutional filter or a CNN to the graph’s matrix representation, such an operation or layer would have to yield the same result no matter the ordering of the adjacency matrix (because every ordering describes the same thing). CNNs fail in this respect.</p>
</div>
<div class="readable-text intended-text" id="p261">
<p>Finding convolutional filters that can be applied to graphs has been solved in a variety of ways. In this chapter, we examined two ways this has been done: spatial and spectral methods. (For a deeper discussion and derivation of convolutional filters applied to graphs, see [7].)</p>
</div>
<div class="readable-text" id="p262">
<h3 class="readable-text-h3"><span class="num-string">3.4.2</span> Message passing</h3>
</div>
<div class="readable-text" id="p263">
<p>Both spatial and spectral approaches describe how we can combine data on our graph. Spatial methods look at the graph structure and combine data across spatial neighborhoods. Spectral methods look at the graph signal and use methods from signal processing, such as the Fourier transform to combine data across the graph. Implicit to both these methods is the idea of message passing.</p>
</div>
<div class="readable-text intended-text" id="p264">
<p>In chapter 3, we introduced message passing as a way to extract more information from our graphs. Let’s go step-by-step and consider what message passing does. First, the messages from each node or edge are collected from neighboring nodes. Second, we transform these messages to encode the data as feature vectors. Finally, we update the node or edge data to include these messages. The result is that each node or edge ends up containing individual data as well as data from the rest of the graph. The amount of data that becomes encoded in these nodes is reflected by the number of hops or message-passing steps. This is the same as the number of layers in a GNN. In figure 3.15, we show a mental model for message passing. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p265">
<img alt="figure" height="235" src="../Images/3-15.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.15</span> Elements of our message-passing layer. Each message-passing layer consists of an aggregation, a transformation, and an update step.</h5>
</div>
<div class="readable-text intended-text" id="p266">
<p>The output of each message-passing layer is a set of embeddings or features. In the aggregation step, we gather the messages from the graph neighborhoods. In the transformation step, we apply a neural network to the aggregated messages. Finally, in the update step, we alter the features of the nodes or edges to include the message passing data. </p>
</div>
<div class="readable-text intended-text" id="p267">
<p>In this way, a GNN layer is similar to a CNN layer. It can be interpreted as a sequence of operations that are applied to input data:</p>
</div>
<div class="readable-text indented-paragraph equation-paragraph" id="p268">
<p>Layer: Aggregate <span class="regular-symbol">→</span> Transform <span class="regular-symbol">→</span> Update</p>
</div>
<div class="readable-text" id="p269">
<p>As we explore different GNNs in this book, we’ll return to this set of operations, as most types of GNNs can be seen as modifications of these elements. For example, in this chapter, you’re learning about GCNs as a specific type of aggregation. In the next chapter, you’ll learn about GATs, which combine both transformation and aggregation steps by learning how to aggregate messages using an attention mechanism. </p>
</div>
<div class="readable-text intended-text" id="p270">
<p>To build this message-passing step, let’s work through the preceding process, adding more detail. The first two steps can be understood as a type of filter, similar to the first step of a conventional neural network. First, we aggregate node or edge data<em> </em>using our<em> aggregation operator</em>. For example, we might sum the features, average the features, or choose the maximum values. The most important thing is that the order of the nodes shouldn’t matter for the final representation. The reason that the order shouldn’t matter is that we want our models to be permutation equivariant, which means that subtraction or division wouldn’t be suitable. </p>
</div>
<div class="readable-text intended-text" id="p271">
<p>Once we’ve aggregated information from all node or collected all the messages, we then transform them into <em>embeddings</em> by passing the new messages through a neural network and an activation function. Once we have these transformed embeddings, we apply an activation function and then combine them with the node or edge data and the previous embeddings. </p>
</div>
<div class="readable-text intended-text" id="p272">
<p>The activation function is a nonlinear transformation that is applied to the transformed and aggregated messages. We need the function to be nonlinear; otherwise, the model would be linear, regardless of how many layers it has, similar to a linear (or logistic, in our case) regression model. These are standard activation functions used in artificial neural networks, such as the ReLU, which is the maximum value between zero and the input value. The pooling step then reduces the overall size of the filter output for any graph-level learning tasks. For node prediction, this can be omitted, which we’ll do here.</p>
</div>
<div class="readable-text intended-text" id="p273">
<p>We can combine the previous description into a single expression for the message-passing operation. First, let’s assume that we’re working with node embeddings as we’ll do in this chapter. We want to transform the data for node <em>n</em> into a node embedding. We can do so using the following formula:</p>
</div>
<div class="browsable-container equation-container" id="p274">
<h5 class="browsable-container-h5"><span class="num-string">(3.1)</span></h5>
<img alt="figure" height="25" src="../Images/Equation-3-1.png" width="394"/>
</div>
<div class="readable-text" id="p275">
<p>Here, <em>u</em> represents the nodes. The learnable weights are given by <em>W</em><sub>a</sub>, which will be tuned based on the loss function, and <em>σ</em> is the activation function. To build the embeddings, we need to take all the node data and combine it into a single vector. This is where the aggregation function comes in. For GCNs, the aggregation operator is summation. Therefore,</p>
</div>
<div class="browsable-container equation-container" id="p276">
<h5 class="browsable-container-h5"><span class="num-string">(3.2)</span></h5>
<img alt="figure" height="60" src="../Images/Equation-3-2.png" width="345"/>
</div>
<div class="readable-text" id="p277">
<p>where, for node <em>u, </em><em>h</em><sub>v</sub><em> </em>is data from node <em>v</em> in the neighborhood of node <em>u</em>, <em>N(u)</em>. Combining both equations, we can construct a general formula for constructing node embeddings:</p>
</div>
<div class="browsable-container equation-container" id="p278">
<h5 class="browsable-container-h5"><span class="num-string">(3.3)</span> </h5>
<img alt="figure" height="85" src="../Images/Equation-3-3.png" width="360"/>
</div>
<div class="readable-text" id="p279">
<p>For the preceding formula, we see that a node and its neighborhood play a central part. Indeed, this is one of the main reasons that GNNs have proven to be so successful. We also see that we need to make a choice on both our activation function and our aggregation function. Finally, these are updated to include the previous data at each node: </p>
</div>
<div class="browsable-container equation-container" id="p280">
<h5 class="browsable-container-h5"><span class="num-string">(3.4)</span></h5>
<img alt="figure" height="25" src="../Images/Equation-3-4.png" width="447"/>
</div>
<div class="readable-text" id="p281">
<p>Here, we’re concatenating the messages together. It’s also possible to use other methods to update the message information, and the choice depends on the architecture used. </p>
</div>
<div class="readable-text intended-text" id="p282">
<p>This update equation is the essence of message passing. For each layer, we’re <em>updating</em> all the node data using the <em>transformed</em> data that contains all <em>aggregated</em> messages. If we have only one layer, we perform this operation only once, we’re aggregating the information from the neighbors one hop away from our starting node. If we run these operations for multiple iterations, we aggregate nodes within two hops of our central node into the node feature data. Thus, the number of GNN layers is directly tied to the size of the neighborhoods we’re interrogating with our model.</p>
</div>
<div class="readable-text intended-text" id="p283">
<p>These are the fundamentals for what operations are being performed during a message-passing step. The variations of things such as aggregation or activation functions highlight key differences in the architecture of a GNN. </p>
</div>
<div class="readable-text" id="p284">
<h3 class="readable-text-h3"><span class="num-string">3.4.3</span> GCN aggregation function</h3>
</div>
<div class="readable-text" id="p285">
<p>The key distinction between GCN and GraphSAGE is that they perform different aggregation operations. GCN is a spectral-based GNN, whereas GraphSAGE is a spatial method. To better understand the difference between the two, let’s look at implementing them both. </p>
</div>
<div class="readable-text intended-text" id="p286">
<p>First, we need to understand how to apply convolution to graphs. Mathematically, the convolutional operation can be expressed as the combination of two functions that produces a third function as</p>
</div>
<div class="browsable-container equation-container" id="p287">
<h5 class="browsable-container-h5"><span class="num-string">(3.5)</span></h5>
<img alt="figure" height="25" src="../Images/Equation-3-5.png" width="342"/>
</div>
<div class="readable-text" id="p288">
<p>where <em>f</em>(<em>x</em>) and <em>h</em>(<em>x</em>) are functions, and the operator represents element-wise multiplication. In the context of CNNs, the image and the kernel matrices are the functions in equation 3.6:</p>
</div>
<div class="browsable-container equation-container" id="p289">
<h5 class="browsable-container-h5"><span class="num-string">(3.6)</span></h5>
<img alt="figure" height="25" src="../Images/Equation-3-6.png" width="322"/>
</div>
<div class="readable-text" id="p290">
<p>This mathematical operation is interpreted as the kernel sliding over the image, as in the sliding window method. We can convert the preceding description to matrices or tensors describing our data. To apply the convolution of equation 3.7 to graphs, we use the following ingredients:</p>
</div>
<ul>
<li class="readable-text buletless-item" id="p291"> Matrix representations of the graph: 
    <ul>
<li> Vector <strong>x</strong> as the graph signal </li>
<li> Adjacency matrix <strong>A</strong> </li>
<li> Laplacian matrix <strong>L</strong> </li>
<li> A matrix of eigenvectors of the Laplacian <strong>U</strong> </li>
</ul></li>
<li class="readable-text" id="p292"> A parameterized matrix for the weights, <strong>H</strong> </li>
<li class="readable-text" id="p293"> Fourier transform based on the matrix operations: <strong>U</strong><sup>T</sup><strong>x</strong> </li>
</ul>
<div class="readable-text" id="p294">
<p>This leads to the expression for spectral convolution over a graph:</p>
</div>
<div class="browsable-container equation-container" id="p295">
<h5 class="browsable-container-h5"><span class="num-string">(3.7)</span></h5>
<img alt="figure" height="29" src="../Images/Equation-3-7.png" width="477"/>
</div>
<div class="readable-text" id="p296">
<p>Because this operation isn’t a simple element-wise multiplication, we’re using the symbol *<sub>G</sub> to express this operation. Several convolutional-based GNNs build on equation 3.8; next, we’ll examine the GCN version.</p>
</div>
<div class="readable-text intended-text" id="p297">
<p>GCNs introduced changes to the convolution equation (3.8) to simplify operations and to reduce computational cost. These changes include using a filter based on a polynomial rather than a set of matrices and limiting the number of hops to one. This reduces the computational complexity from quadratic to linear, which is significant. However, the key thing to note is that GCNs updated the aggregation function that we described earlier. This will still use a summation but includes a normalization term. </p>
</div>
<div class="readable-text intended-text" id="p298">
<p>Previously, the aggregation operator was summation. This can lead to problems in graphs where the degree of nodes can have high variance. If a graph contains nodes whose degrees are high, those nodes will dominate. To solve this, one method is to replace summation with averaging. The aggregation function is then expressed as</p>
</div>
<div class="browsable-container equation-container" id="p299">
<h5 class="browsable-container-h5"><span class="num-string">(3.8)</span></h5>
<img alt="figure" height="70" src="../Images/Equation-3-8.png" width="307"/>
</div>
<div class="readable-text" id="p300">
<p>Therefore, for GCN message passing, we have</p>
</div>
<div class="browsable-container equation-container" id="p301">
<h5 class="browsable-container-h5"><span class="num-string">(3.9)</span></h5>
<img alt="figure" height="85" src="../Images/Equation-3-9.png" width="752"/>
</div>
<div class="readable-text" id="p302">
<p>where</p>
</div>
<ul>
<li class="readable-text" id="p303"> <em>h</em> is the updated node embedding. </li>
<li class="readable-text" id="p304"> sigma, <em>σ</em>, is a nonlinearity (i.e., activation function) applied to every element. </li>
<li class="readable-text" id="p305"> W is a trained weight matrix. </li>
<li class="readable-text" id="p306"> |<em>N </em>| denotes the count of the elements in the set of graph nodes </li>
</ul>
<div class="readable-text" id="p307">
<p>The summed factor,</p>
</div>
<div class="browsable-container equation-container" id="p308">
<h5 class="browsable-container-h5"><span class="num-string">(3.10)</span></h5>
<img alt="figure" height="69" src="../Images/Equation-3-10.png" width="219"/>
</div>
<div class="readable-text" id="p309">
<p>is a special normalization called <em>symmetric normalization</em>. Additionally, GCNs include self-loops such that node embeddings include both neighborhood data and data from the starting node. So, to implement a GCN, the following operations must occur:</p>
</div>
<ul>
<li class="readable-text" id="p310"> Graph nodes adjusted to contain self-loops </li>
<li class="readable-text" id="p311"> Matrix multiplication of the trained weight matrix and the node embeddings </li>
<li class="readable-text" id="p312"> Normalization operations summing the terms of the symmetric normalization </li>
</ul>
<div class="readable-text" id="p313">
<p>In figure 3.16, we explain each of the terms used in a message-passing step in detail.</p>
</div>
<div class="readable-text intended-text" id="p314">
<p>So far this has all been theoretical. Let’s next look at how we implement this in PyG. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p315">
<img alt="figure" height="368" src="../Images/3-16.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.16</span> Mapping of key computational operations in the GCN embedding formula</h5>
</div>
<div class="readable-text" id="p316">
<h3 class="readable-text-h3"><span class="num-string">3.4.4</span> GCN in PyTorch Geometric</h3>
</div>
<div class="readable-text" id="p317">
<p>In the PyG documentation, you can find source code that implements the GCN layer, as well as a simplified implementation of the GCN layer. In the following, we’ll point out how the source code implements the preceding key operations.</p>
</div>
<div class="readable-text intended-text" id="p318">
<p>In table 3.8, we break down key steps in the computation of the GCN embeddings and tie them to functions in the source code. These operations are implemented using a class and a function:</p>
</div>
<ul>
<li class="readable-text" id="p319"> Function <code>gcn_norm</code> performs normalization and add self-loops to the graph. </li>
<li class="readable-text" id="p320"> Class <code>GCNConv</code> instantiates the GNN layer and performs matrix operations. </li>
</ul>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p321">
<h5 class="browsable-container-h5"><span class="num-string">Table 3.8</span> Mapping key computational operations in the GCN embedding formula</h5>
<table>
<thead>
<tr>
<th>
<div>
         Operation 
       </div></th>
<th>
<div>
         Function/Method 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Add self-loops to nodes <br/></td>
<td> <code>gcn_norm()</code>, annotated in listing 3.8 <br/></td>
</tr>
<tr>
<td>  Multiply weights and embeddings <em>W</em><sup>(</sup><sup><em>k</em></sup><sup>)</sup><em>h</em><sub>u</sub> <br/></td>
<td> <code>GCNConv.__init__</code>; <code>GCNConv.forward</code> <br/></td>
</tr>
<tr>
<td>  Symmetric normalization <br/></td>
<td> <code>gcn_norm()</code>, annotated in listing 3.8 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p322">
<p>In listing 3.8, we show the code in detail for the <code>gcn_norm</code> function and class and use annotation to highlight the key operations. This normalization function is a key aspect for the GCN architecture. The <code>gcn_norm</code> arguments are as follows:</p>
</div>
<ul>
<li class="readable-text" id="p323"> <code>edge_index</code>—The node representations are in a tensor or sparse tensor form. </li>
<li class="readable-text" id="p324"> <code>edge_weight</code>—An array of one-dimensional edge weights is optional. </li>
<li class="readable-text" id="p325"> <code>num_nodes</code>—This is a dimension of the input graph. </li>
<li class="readable-text" id="p326"> <code>improved</code>—This introduces an alternative method to add self-loops from the Graph U-Nets paper [8]. </li>
<li class="readable-text" id="p327"> <code>Add_self_loops</code>—Adding self-loops is the default, but it’s optional. </li>
</ul>
<div class="browsable-container listing-container" id="p328">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.8</span> The <code>gcn_norm</code> function</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area"><strong>def</strong> gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False,
             add_self_loops=True, dtype=None): <span class="aframe-location"/> #1

   fill_value = 2. <strong>if</strong> improved <strong>else</strong> 1. <span class="aframe-location"/> #2

   <strong>if</strong> isinstance(edge_index, SparseTensor): <span class="aframe-location"/> #3
       adj_t = edge_index
       <strong>if</strong> not adj_t.has_value():
           adj_t = adj_t.fill_value(1., dtype=dtype)
       <strong>if</strong> add_self_loops:
           adj_t = fill_diag(adj_t, fill_value)
       deg = sparsesum(adj_t, dim=1)
       deg_inv_sqrt = deg.pow_(-0.5) 
       deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0.)
       adj_t = mul(adj_t, deg_inv_sqrt.view(-1, 1))
       adj_t = mul(adj_t, deg_inv_sqrt.view(1, -1))
       <strong>return</strong> adj_t

   <strong>else</strong>: 
       num_nodes = maybe_num_nodes(edge_index, num_nodes)

       <strong>if</strong> edge_weight is None:
           edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,
                                    device=edge_index.device)

       <strong>if</strong> add_self_loops:
           edge_index, tmp_edge_weight = add_remaining_self_loops(
               edge_index, edge_weight, fill_value, num_nodes)
           <strong>assert</strong> tmp_edge_weight is not None
           edge_weight = tmp_edge_weight

       row, col = edge_index[0], edge_index[1]
       deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)
       deg_inv_sqrt = deg.pow_(-0.5)
       deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)
       <strong>return</strong> edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]</pre>
<div class="code-annotations-overlay-container">
     #1 Performs symmetric normalization of the input graph and adds a self-loop to the input graph
     <br/>#2 The fill_value parameter is used in the alternative self-loop operation. 
     <br/>#3 If the graph input is a sparse tensor, the first block of code in the if-statement will apply. Otherwise, the second will apply.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p329">
<p>In practice, we can simplify the implementation of the normalization considerably by using some functions from PyTorch and PyG. In listing 3.9, we show a shortened version of normalizing the adjacency matrix. First, we compute the in-degree for each node and then calculate the inverse square root. We then use this to create a new edge weighting and apply the degree-based inverse square root to this weighting. Finally, we create a sparse tensor that represents the adjacency matrix and assign this to our data.</p>
</div>
<div class="browsable-container listing-container" id="p330">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.9</span> Normalizing using PyTorch and PyG</h5>
<div class="code-area-container">
<pre class="code-area">    edge_index = data.edge_index
    num_nodes = edge_index.max().item() + 1 <span class="aframe-location"/> #1

    deg = torch.zeros(num_nodes, \
    dtype=torch.float).to(edge_index.device)      <span class="aframe-location"/> #2
    deg.scatter_add_(0, edge_index[1],            
                 torch.ones(edge_index.size(1))\
.to(edge_index.device))                           

    deg_inv_sqrt = deg.pow(-0.5) <span class="aframe-location"/> #3
    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0   #3

    edge_weight = torch.ones(edge_index.size(1))\
    .to(edge_index.devic) <span class="aframe-location"/> #4
    edge_weight = deg_inv_sqrt[edge_index[0]]*edge_weight*\
    deg_inv_sqrt[edge_index[1]] <span class="aframe-location"/> #5

    num_nodes = edge_index.max().item() + 1  <span class="aframe-location"/> #6

    adj_t = torch.sparse_coo_tensor(indices=edge_index,\
    values=edge_weight, size=(num_nodes, num_nodes))   <span class="aframe-location"/>   #7
    data.adj_t = adj_t.coalesce()                         #7</pre>
<div class="code-annotations-overlay-container">
     #1 Assumes node indices start from 0
     <br/>#2 Computes in-degree for each node
     <br/>#3 Computes the degree-based inverse square
     <br/>#4 Creates a new edge_weight tensor
     <br/>#5 Applies deg_inv_sqrt to edge weights
     <br/>#6 Assumes node indices start from 0
     <br/>#7 Creates a sparse tensor and assigns to data
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p331">
<p>In the following listing, we have excerpts of the <code>GCNConv</code> class, which calls on the <code>gcn_norm</code> function as well as the matrix operations.</p>
</div>
<div class="browsable-container listing-container" id="p332">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.10</span> The <code>GCNConv</code> class </h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area"><strong>class</strong> GCNConv(MessagePassing):    

    <strong>def</strong> __init__(self, in_channels: int, out_channels: int,
improved: bool = False, cached: bool = False,
        add_self_loops: bool = True, normalize: bool = True,
        bias: bool = True, **kwargs):    

        self.lin = Linear(in_channels, out_channels, bias=False,
                         weight_initializer='glorot')
    <strong>def</strong> forward(self, x: Tensor, edge_index: Adj,
                edge_weight: OptTensor = None) -&gt; Tensor:

if self.normalize: <span class="aframe-location"/> #1
    if isinstance(edge_index, Tensor):
        cache = self._cached_edge_index
            if cache is None:
                edge_index, edge_weight = gcn_norm( 
                edge_index, edge_weight, x.size(self.node_dim),
                self.improved, self.add_self_loops)
                if self.cached:
                    self._cached_edge_index = (edge_index, edge_weight)
                else:
                    edge_index, edge_weight = cache[0], cache[1]

        x = self.lin(x) <span class="aframe-location"/> #2

        out = self.propagate(edge_index, x=x,\
 edge_weight=edge_weight, size=None) <span class="aframe-location"/> #3

        <strong>if</strong> self.bias is not None: <span class="aframe-location"/> #4
            out += self.bias

        <strong>return</strong> out</pre>
<div class="code-annotations-overlay-container">
     #1 The forward propagation function performs symmetric normalization given one of two options: that the input graph is a tensor or a sparse tensor. The source code for a tensor input is included here.
     <br/>#2 Linear transformation of the node feature matrix
     <br/>#3 Message propagation
     <br/>#4 There is an optional additive bias to the output.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p333">
<h3 class="readable-text-h3"><span class="num-string">3.4.5</span> Spectral vs. spatial convolution</h3>
</div>
<div class="readable-text" id="p334">
<p>In the previous section, we talked about interpreting convolution in two ways: (1) via a thought experiment of sliding a window filter across part of a graph consisting of a local neighborhood of linked nodes, and (2) by processing graph signal data through a filter. We also discussed how these two interpretations highlight two branches of convolutional GNNs: the spatial method and the spectral method. Sliding window and other spatial methods rely on a graph’s geometrical structure to perform convolution. Spectral methods instead use graph signal filters.</p>
</div>
<div class="readable-text intended-text" id="p335">
<p>There is no clear demarcation between the spectral and spatial methods, and often one type can be interpreted as the other. For example, one contribution of GCN is the demonstration that its spectral derivation could be interpreted in a spatial way. However, at the time of writing, spatial methods are preferred because they have fewer restrictions and, in general, offer less computational complexity. We’ve highlighted additional aspects of both spectral and spatial methods in table 3.9.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p336">
<h5 class="browsable-container-h5"><span class="num-string">Table 3.9</span> A comparison of spectral and spatial convolutional methods</h5>
<table>
<thead>
<tr>
<th>
<div>
         Spectral 
       </div></th>
<th>
<div>
         Spatial 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Operation: performing a convolution using a graph’s eigenvalues <br/></td>
<td>  Operation: aggregation of node features in node neighborhoods <br/></td>
</tr>
<tr>
<td>  • Must be undirected <br/>  • Operation dependent on node features <br/>  • Generally less computationally efficient <br/></td>
<td>  • Not required to be undirected <br/>  • Operation not dependent on node features <br/>  • Generally more computationally efficient <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p337">
<h3 class="readable-text-h3"><span class="num-string">3.4.6</span> GraphSAGE aggregation function</h3>
</div>
<div class="readable-text" id="p338">
<p>GraphSAGE improved on the computational cost of GCNs by limiting the number of neighboring nodes used in the aggregation operation. Instead, GraphSAGE aggregates from a randomly selected sample of the neighborhood. The aggregation operator is more flexible (e.g., it can be a summation or an average), but the messages that are considered are now only a subset of all messages. Mathematically, we can write this as</p>
</div>
<div class="browsable-container equation-container" id="p339">
<h5 class="browsable-container-h5"><span class="num-string">(3.11)</span></h5>
<img alt="figure" height="29" src="../Images/Equation-3-11.png" width="519"/>
</div>
<div class="readable-text" id="p340">
<p>where Ɐ<em>u</em> ϵ <em>S</em> denotes that the neighborhood is picked from a random sample, <em>S, </em>of the total neighborhood. From the GraphSAGE paper [2], we have the general embedding updating process, which the paper introduces as Algorithm 1, reproduced here in figure 3.17.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p341">
<img alt="figure" height="574" src="../Images/3-17.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.17</span> Algorithm 1, the GraphSAGE embedding generation algorithm from the GraphSAGE paper [2] </h5>
</div>
<div class="readable-text" id="p342">
<p>The basics of this algorithm can be described as follows:</p>
</div>
<ol>
<li class="readable-text" id="p343"> For every layer/iteration <em>and</em> for every node: 
    <ol style="list-style: lower-alpha">
<li> Aggregate the embeddings of the neighbors. </li>
<li> Concatenate neighbor embeddings with the central node. </li>
<li> Matrix multiply that concatenation with the Weights matrix. </li>
<li> Multiply that result with an activation function. </li>
<li> Apply a normalization. </li>
</ol></li>
<li class="readable-text" id="p344"> Update node features, z, with node embeddings, h. </li>
</ol>
<div class="readable-text" id="p345">
<p>Let’s take a closer look at what this means for the message-passing step. We’ve defined message passing for GraphSAGE as follows: </p>
</div>
<div class="browsable-container equation-container" id="p346">
<h5 class="browsable-container-h5"><span class="num-string">(3.12)</span></h5>
<img alt="figure" height="64" src="../Images/Equation-3-13.png" width="752"/>
</div>
<div class="readable-text" id="p347">
<p>If we choose the mean as the aggregation function, this becomes</p>
</div>
<div class="browsable-container equation-container" id="p348">
<h5 class="browsable-container-h5"><span class="num-string">(3.13)</span></h5>
<img alt="figure" height="29" src="../Images/Equation-3-14.png" width="597"/>
</div>
<div class="readable-text" id="p349">
<p>For implementation, we can further reduce this to</p>
</div>
<div class="browsable-container equation-container" id="p350">
<h5 class="browsable-container-h5"><span class="num-string">(3.14)</span></h5>
<img alt="figure" height="29" src="../Images/Equation-3-15.png" width="327"/>
</div>
<div class="readable-text" id="p351">
<p>where <em>x'</em><sub>i</sub> denotes the generated central node embeddings, and <em>x</em><sub>i</sub> and <em>x</em><sub>j</sub> are the input features of the central and neighboring nodes, respectively. The weight matrices are applied to both central nodes and neighboring nodes, as shown in figure 3.18, but only the neighboring nodes have an aggregation operator (in this case, the mean). <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p352">
<img alt="figure" height="220" src="../Images/3-18.png" width="467"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.18</span> Mapping key computational operations in the GraphSAGE embedding formula</h5>
</div>
<div class="readable-text" id="p353">
<p>We’ve now seen all the main features of the GraphSAGE algorithm. Let’s next look at how we implement this in PyG. </p>
</div>
<div class="readable-text" id="p354">
<h3 class="readable-text-h3"><span class="num-string">3.4.7</span> GraphSAGE in PyTorch Geometric</h3>
</div>
<div class="readable-text" id="p355">
<p>In table 3.10, we break down the key operations and where they occur in PyG’s GraphSAGE class. The key operations are aggregation of the neighbor embeddings, concatenation of a node’s neighbors’ embeddings with that node’s embeddings, multiplication of weights with the concatenation, and application of an activation function.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p356">
<h5 class="browsable-container-h5"><span class="num-string">Table 3.10</span> Mapping key computational operations in the GCN embedding formula</h5>
<table>
<thead>
<tr>
<th>
<div>
         Operation 
       </div></th>
<th>
<div>
         Function/Method 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Aggregate the embeddings of the neighbors (sum, mean, or other). <br/></td>
<td> <code>SAGEConv.message_and_aggregate</code> <br/></td>
</tr>
<tr>
<td>  Concatenate neighbor embeddings with that of the central node. <br/></td>
<td> <code>SAGEConv.forward</code> <br/></td>
</tr>
<tr>
<td>  Matrix multiply that concatenation with the Weights matrix. <br/></td>
<td> <code>SAGEConv.message_and_aggregate</code> <br/></td>
</tr>
<tr>
<td>  Apply an activation function. <br/></td>
<td>  If the <code>project</code> parameter is set to <code>True</code>, done in <code>SAGEConv.forward</code> <br/></td>
</tr>
<tr>
<td>  Apply a normalization. <br/></td>
<td> <code>SAGEConv.forward</code> <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p357">
<p>For GraphSAGE, PyG also has source code to implement this layer in the <code>SAGEConv</code> class, excerpts of which are shown in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p358">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.11</span> The GraphSAGE class</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area"><strong>class</strong> SAGEConv(MessagePassing):
…
   <strong>def</strong> forward(self, x, edge_index, size):

        <strong>if</strong> isinstance(x, Tensor):
            x: OptPairTensor = (x, x)

       <strong>if</strong> self.project and hasattr(self, 'lin'): <span class="aframe-location"/> #1
           x = (self.lin(x[0]).relu(), x[1])

       out = self.propagate(edge_index, x=x, size=size) <span class="aframe-location"/> #2
       out = self.lin_l(out)  #2
       x_r = x[1] <span class="aframe-location"/> #3

       <strong>if</strong> self.root_weight and x_r is not None: <span class="aframe-location"/> #4
           out += self.lin_r(x_r)  #4

       <strong>if</strong> self.normalize: <span class="aframe-location"/> #5
           out = F.normalize(out, p=2., dim=-1)  #5
       <strong>return</strong> out

   <strong>def</strong> message(self, x_j):
       <strong>return</strong> x_j

   <strong>def</strong> message_and_aggregate(self, adj_t, x):
       adj_t = adj_t.set_value(None, layout=None) <span class="aframe-location"/> #6
       <strong>return</strong> matmul(adj_t, x[0], reduce=self.aggr)  #6
…</pre>
<div class="code-annotations-overlay-container">
     #1 If the project parameter is set to True, this applies a linear transformation with an activation function (ReLU, in this case) to the neighbor nodes.
     <br/>#2 Propagates messages and applies a linear transformation
     <br/>#3 Assigns the root node to a variable
     <br/>#4 If the root_weight parameter is set to True and a root node exists, this will add (concatenate) the transformed root node features to the output.
     <br/>#5 If the normalize parameter is set to True, L2 normalization will be applied to the output features.
     <br/>#6 Matrix multiplication with an aggregation. Setting the aggr parameter establishes the aggregation scheme (e.g., mean, max, lstm; default is add). adj_t is the sparse matrix representation of the input; using such a representation speeds up calculations.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p359">
<h2 class="readable-text-h2"><span class="num-string">3.5</span> Amazon Products dataset</h2>
</div>
<div class="readable-text" id="p360">
<p>In both this chapter and chapter 5, we use the Amazon Products dataset [9]. This dataset explores product relationships, particularly co-purchases, which are products purchased in the same transaction. This co-purchase data is a great dataset for benchmarking methods for predicting both nodes and edges. We give a bit more information about the dataset in this section.</p>
</div>
<div class="readable-text intended-text" id="p361">
<p>To illustrate the concept of co-purchases, in figure 3.19, we show six example co-purchase images for an online customer. For each product, we include a picture, a plain text product label, and a bold text category label.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p362">
<img alt="figure" height="394" src="../Images/3-19.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.19</span> Examples of co-purchases on Amazon.com. Each product is represented by a picture, a plain text product title, and a bold text product category. We see that some co-purchases feature products that are obvious complements of one another, while other groupings are less so.</h5>
</div>
<div class="readable-text" id="p363">
<p>Some of these co-purchase groups seem to fit together well, such as the book purchases or the clothing purchases. Other co-purchases are less explainable, such as an Apple iPod being purchased with instant meals, or beans being purchased with a wireless speaker. In those less obvious groupings, there may be some latent product relationship, or maybe it’s just mere coincidence. Examining the data at scale can provide clues.</p>
</div>
<div class="readable-text intended-text" id="p364">
<p>To show how the co-purchasing graph would appear at a small scale, figure 3.20 takes one of the images from the previous figure and represents the products as nodes, with the edges between them representing each co-purchase. For one customer and one purchase, this is a small graph, with only four nodes and six edges. But for the same customer over time, for a larger set of customers with the same tastes in food, or even all the customers, it’s easy to imagine how this graph can scale with more products and product connections branching from these few products. </p>
</div>
<div class="readable-text intended-text" id="p365">
<p>The construction of this dataset is a long journey in itself, which is very much of interest to graph construction and the decisions that have to be made to get a meaningful and useful dataset. Put simply, this dataset was derived from purchasing log data from Amazon, which directly showed co-purchases, and from text data from product reviews, which was used to indirectly show product relationships. (For the in-depth story, see [8]).<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p366">
<img alt="figure" height="636" src="../Images/3-20.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.20</span> A graph representation of one of the co-purchases from figure 3.19. Each product’s picture is a node, and the co-purchases are the edges (shown as lines) between the products. For the four products shown here, this graph is only the co-purchasing graph of one customer. If we show the corresponding graph for all customers of Amazon, the number of products and edges could feature tens of thousands of product nodes and millions of co-purchasing edges.</h5>
</div>
<div class="readable-text intended-text" id="p367">
<p>To explore the product relationships, we can use the Amazon Products co-purchasing graph, a dataset of products that have been bought together in the same transaction (defined as a co-purchase). In this dataset, products are represented by nodes with both the type of product that was bought, which are the category labels, and some feature information. The feature information takes the product description and first applies a natural language processing (NLP) method, the bag-of-words algorithm, to convert the strings into numerical values. Then, to convert this into the same fixed length, the creators of the dataset used principal component analysis (PCA) to convert this into a vector of length 100. </p>
</div>
<div class="readable-text intended-text" id="p368">
<p>Meanwhile, co-purchases are represented by edges, which refers to two products that were bought together. In total, the dataset, <code>ogbn-products</code>, consists of 2.5 million nodes (products) and 61.9 million edges (co-purchases). The dataset is provided through the Open Graph Benchmark (OGB) dataset, mentioned at the beginning of the chapter, with a usage license from Amazon. Each node has 100 features. There are 47 categories that are used as targets in a classification task. We note that the edges here are undirected and unweighted. </p>
</div>
<div class="readable-text intended-text" id="p369">
<p>In figure 3.21, we see that the categories with the highest counts of nodes are Books (668,950 nodes), CDs &amp; Vinyl (172,199 nodes), and Toys &amp; Games (158,771 nodes). The lowest are Furniture and Decor (9 nodes), Digital Music (6 nodes), and an unknown category (#508510) with 1 node. </p>
</div>
<div class="browsable-container figure-container" id="p371">
<img alt="figure" height="985" src="../Images/3-21.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.21</span> Distribution of node labels in the Amazon Products dataset</h5>
</div>
<div class="readable-text intended-text" id="p370">
<p>We also observe that many categories have very low proportions in the dataset. The mean count of nodes per label/category is 52,107; the median count is 3,653. This highlights that there is a strong class imbalance in our dataset. This can pose a challenge for typical tabular results. <span class="aframe-location"/></p>
</div>
<div class="readable-text" id="p372">
<p>In this chapter, we explored the fundamentals of graph convolutional networks (GCNs) and GraphSAGE, two powerful architectures for learning on graph-structured data. We applied these models to a practical product categorization problem using the Amazon Products dataset, demonstrating how to implement, train, and refine GNNs. We also delved into the theoretical underpinnings of these models, examining concepts like neighborhood aggregation, message passing, and the distinctions between spectral and spatial convolution methods. By combining hands-on implementation with theoretical insights, this chapter has provided a comprehensive foundation for understanding and applying convolutional GNNs to real-world graph learning tasks. In the next chapter, we study a special convolutional GNN that uses the attention mechanism, the Graph Attention Network (GAT).</p>
</div>
<div class="readable-text" id="p373">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p374"> GCNs and GraphSAGE are GNNs that use convolution, done by spatial and spectral methods, respectively. </li>
<li class="readable-text" id="p375"> These GNNs can be used in supervised and semi-supervised learning problems. We applied them to the semi-supervised problem of predicting product categories. </li>
<li class="readable-text" id="p376"> The Amazon Products dataset, <code>ogbn-products</code>, consists of a set of products (nodes) linked by being purchased in the same transaction (co-purchases). Each product node has a set of features, including its product-category. This dataset is a popular benchmark for graph classification problems. We can also study how it was constructed to get insights on graph creation methodology. </li>
<li class="readable-text" id="p377"> Selecting subgraphs based on domain knowledge or using graph sampling techniques ensures more meaningful data is used for training. This can improve the performance of the models by focusing on relevant parts of the graph. </li>
<li class="readable-text" id="p378"> Different aggregation methods, such as mean, max, and sum, have varied effects on model performance. Experimenting with multiple aggregation strategies can help capture various properties of the graph data, potentially enhancing model performance. </li>
<li class="readable-text" id="p379"> Exploring more sophisticated aggregation functions or custom aggregations tailored to the specific characteristics of the dataset can yield performance improvements. Examples include <code>SoftmaxAggregation</code> and <code>StdAggregation</code>. </li>
<li class="readable-text" id="p380"> Depth in GNNs is analogous to the number of hops or message-passing steps. While deeper models can theoretically capture more complex patterns, they often suffer from over-smoothing, where node features become too similar, making it difficult to distinguish between different nodes. </li>
<li class="readable-text" id="p381"> Empirical testing of different aggregation methods and model configurations is essential. Experimentation helps determine which methods best capture the relational dynamics and feature distributions of the dataset. </li>
</ul>
</div></body></html>