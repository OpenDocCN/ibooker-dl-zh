["```py\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom pydantic import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\n# --- Create an index of documents ---\n\nurls = [\n    \"https://blog.langchain.dev/top-5-langgraph-agents-in-production-2024/\",\n    \"https://blog.langchain.dev/langchain-state-of-ai-2024/\",\n    \"https://blog.langchain.dev/introducing-ambient-agents/\",\n]\n\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=250, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorDB\nvectorstore = InMemoryVectorStore.from_documents(\n    documents=doc_splits,\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n\n# Retrieve the relevant documents\nresults = retriever.invoke(\n    \"What are 2 LangGraph agents used in production in 2024?\")\n\nprint(\"Results: \\n\", results)\n```", "```py\nimport { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';\nimport {\n  CheerioWebBaseLoader\n} from \"@langchain/community/document_loaders/web/cheerio\";\nimport { \n  InMemoryVectorStore \n} from '@langchain/community/vectorstores/in_memory';\nimport { OpenAIEmbeddings } from '@langchain/openai';\nimport { ChatPromptTemplate } from '@langchain/core/prompts';\nimport { z } from 'zod';\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst urls = [\n  'https://blog.langchain.dev/top-5-langgraph-agents-in-production-2024/',\n  'https://blog.langchain.dev/langchain-state-of-ai-2024/',\n  'https://blog.langchain.dev/introducing-ambient-agents/',\n];\n\n// Load documents from URLs\nconst loadDocs = async (urls) => {\n  const docs = [];\n  for (const url of urls) {\n    const loader = new CheerioWebBaseLoader(url);\n    const loadedDocs = await loader.load();\n    docs.push(...loadedDocs);\n  }\n  return docs;\n};\n\nconst docsList = await loadDocs(urls);\n\n// Initialize the text splitter\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 250,\n  chunkOverlap: 0,\n});\n\n// Split the documents into smaller chunks\nconst docSplits = textSplitter.splitDocuments(docsList);\n\n// Add to vector database\nconst vectorstore = await InMemoryVectorStore.fromDocuments(\n  docSplits,\n  new OpenAIEmbeddings()\n);\n\n// The `retriever` object can now be used for querying\nconst retriever = vectorstore.asRetriever(); \n\nconst question = 'What are 2 LangGraph agents used in production in 2024?';\n\nconst docs = retriever.invoke(question);\n\nconsole.log('Retrieved documents: \\n', docs[0].page_content);\n```", "```py\n### Retrieval Grader\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\n# Data model\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\n\n# Prompt\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a\n user question. \n If the document contains keyword(s) or semantic meaning related to the \n question, grade it as relevant. \n Give a binary score 'yes' or 'no' to indicate whether the document is \n relevant to the question.\"\"\"\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"\"\"Retrieved document: \\n\\n {document} \\n\\n User question: \n            {question}\"\"\"),\n    ]\n)\n\nretrieval_grader = grade_prompt | structured_llm_grader\nquestion = \"agent memory\"\ndocs = retriever.get_relevant_documents(question)\ndoc_txt = docs[0].page_content # as an example\nretrieval_grader`.`invoke({\"question\": question, \"document\": doc_txt})\n```", "```py\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { z } from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// Define the schema using Zod\nconst GradeDocumentsSchema = z.object({\n  binary_score: z.string().describe(`Documents are relevant to the question, \n 'yes' or 'no'`),\n});\n\n// Initialize LLM with structured output using Zod schema\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 });\nconst structuredLLMGrader = llm.withStructuredOutput(GradeDocumentsSchema);\n\n// System and prompt template\nconst systemMessage = `You are a grader assessing relevance of a retrieved \n document to a user question. \nIf the document contains keyword(s) or semantic meaning related to the \n question, grade it as relevant.\nGive a binary score 'yes' or 'no' to indicate whether the document is relevant \n to the question.`;\n\nconst gradePrompt = ChatPromptTemplate.fromMessages([\n  { role: \"system\", content: systemMessage },\n  {\n    role: \"human\",\n    content: \"Retrieved document: \\n\\n {document} \\n\\n \n User question: {question}\",\n  },\n]);\n\n// Combine prompt with the structured output\nconst retrievalGrader = gradePrompt.pipe(structuredLLMGrader);\n\nconst question = \"agent memory\";\nconst docs = await retriever.getRelevantDocuments(question);\n\nawait retrievalGrader.invoke({\n  question,\n  document: docs[1].pageContent,\n});\n```", "```py\nbinary_score='yes'\n```", "```py\nfrom langsmith import Client\n\nclient = Client()\n\n# Create a dataset\nexamples = [\n    (\"Which country's customers spent the most? And how much did they spend?\",\n        \"\"\"The country whose customers spent the most is the USA, with a total \n expenditure of $523.06\"\"\"),\n    (\"What was the most purchased track of 2013?\", \n        \"The most purchased track of 2013 was Hot Girl.\"),\n    (\"How many albums does the artist Led Zeppelin have?\",\n        \"Led Zeppelin has 14 albums\"),\n    (\"What is the total price for the album “Big Ones”?\",\n        \"The total price for the album 'Big Ones' is 14.85\"),\n    (\"Which sales agent made the most in sales in 2009?\", \n        \"Steve Johnson made the most sales in 2009\"),\n]\n\ndataset_name = \"SQL Agent Response\"\nif not client.has_dataset(dataset_name=dataset_name):\n    dataset = client.create_dataset(dataset_name=dataset_name)\n    inputs, outputs = zip(\n        *[({\"input\": text}, {\"output\": label}) for text, label in examples]\n    )\n    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n\n## chain\ndef predict_sql_agent_answer(example: dict):\n    \"\"\"Use this for answer evaluation\"\"\"\n    msg = {\"messages\": (\"user\", example[\"input\"])}\n    messages = graph.invoke(msg, config)\n    return {\"response\": messages['messages'][-1].content}\n```", "```py\nimport { Client } from 'langsmith';\n\nconst client = new Client();\n\n// Create a dataset\nconst examples = [\n  [\"Which country's customers spent the most? And how much did they spend?\", \n    `The country whose customers spent the most is the USA, with a total \n expenditure of $523.06`],\n  [\"What was the most purchased track of 2013?\", \n    \"The most purchased track of 2013 was Hot Girl.\"],\n  [\"How many albums does the artist Led Zeppelin have?\", \n    \"Led Zeppelin has 14 albums\"],\n  [\"What is the total price for the album 'Big Ones'?\", \n    \"The total price for the album 'Big Ones' is 14.85\"],\n  [\"Which sales agent made the most in sales in 2009?\", \n    \"Steve Johnson made the most sales in 2009\"],\n];\n\nconst datasetName = \"SQL Agent Response\";\n\nasync function createDataset() {\n  const hasDataset = await client.hasDataset({ datasetName });\n\n  if (!hasDataset) {\n    const dataset = await client.createDataset(datasetName);\n    const inputs = examples.map(([text]) => ({ input: text }));\n    const outputs = examples.map(([, label]) => ({ output: label }));\n\n    await client.createExamples({ inputs, outputs, datasetId: dataset.id });\n  }\n}\n\ncreateDataset();\n\n// Chain function\nasync function predictSqlAgentAnswer(example) {\n  // Use this for answer evaluation\n  const msg = { messages: [{ role: \"user\", content: example.input }] };\n  const output = await graph.invoke(msg, config);\n  return { response: output.messages[output.messages.length - 1].content };\n}\n```", "```py\nfrom langchain import hub\nfrom langchain_openai import ChatOpenAI\nfrom langsmith.evaluation import evaluate\n\n# Grade prompt\ngrade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n\ndef answer_evaluator(run, example) -> dict:\n    \"\"\"\n A simple evaluator for RAG answer accuracy\n \"\"\"\n\n    # Get question, ground truth answer, RAG chain answer\n    input_question = example.inputs[\"input\"]\n    reference = example.outputs[\"output\"]\n    prediction = run.outputs[\"response\"]\n\n    # LLM grader\n    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n    # Structured prompt\n    answer_grader = grade_prompt_answer_accuracy | llm\n\n    # Run evaluator\n    score = answer_grader.invoke({\"question\": input_question,\n                                  \"correct_answer\": reference,\n                                  \"student_answer\": prediction})\n    score = score[\"Score\"]\n\n    return {\"key\": \"answer_v_reference_score\", \"score\": score}\n\n## Run evaluation\nexperiment_results = evaluate(\n    predict_sql_agent_answer,\n    data=dataset_name,\n    evaluators=[answer_evaluator],\n    num_repetitions=3,\n)\n```", "```py\nimport { pull } from \"langchain/hub\";\nimport { ChatOpenAI } from \"langchain_openai\";\nimport { evaluate } from \"langsmith/evaluation\";\n\nasync function answerEvaluator(run, example) {\n  /**\n * A simple evaluator for RAG answer accuracy\n */\n\n  // Get question, ground truth answer, RAG chain answer\n  const inputQuestion = example.inputs[\"input\"];\n  const reference = example.outputs[\"output\"];\n  const prediction = run.outputs[\"response\"];\n\n  // LLM grader\n  const llm = new ChatOpenAI({ model: \"gpt-4o\", temperature: 0 });\n\n  // Grade prompt \n  const gradePromptAnswerAccuracy = pull(\n    \"langchain-ai/rag-answer-vs-reference\"\n  );\n\n  // Structured prompt\n  const answerGrader = gradePromptAnswerAccuracy.pipe(llm);\n\n  // Run evaluator\n  const scoreResult = await answerGrader.invoke({\n    question: inputQuestion,\n    correct_answer: reference,\n    student_answer: prediction\n  });\n\n  const score = scoreResult[\"Score\"];\n\n  return { key: \"answer_v_reference_score\", score: score };\n}\n\n// Run evaluation\nconst experimentResults = evaluate(predictSqlAgentAnswer, {\n  data: datasetName,\n  evaluators: [answerEvaluator],\n  numRepetitions: 3,\n});\n```", "```py\nfrom langsmith.schemas import Example, Run\n\ndef predict_assistant(example: dict):\n    \"\"\"Invoke assistant for single tool call evaluation\"\"\"\n    msg = [ (\"user\", example[\"input\"]) ]\n    result = assistant_runnable.invoke({\"messages\":msg})\n    return {\"response\": result}\n\ndef check_specific_tool_call(root_run: Run, example: Example) -> dict:\n    \"\"\"\n Check if the first tool call in the response matches the expected tool call.\n \"\"\"\n    # Expected tool call\n    expected_tool_call = 'sql_db_list_tables'\n\n    # Run\n    response = root_run.outputs[\"response\"]\n\n    # Get tool call\n    try:\n        tool_call = getattr(response, 'tool_calls', [])[0]['name']\n    except (IndexError, KeyError):\n        tool_call = None\n\n    score = 1 if tool_call == expected_tool_call else 0\n    return {\"score\": score, \"key\": \"single_tool_call\"}\n\nexperiment_results = evaluate(\n    predict_assistant,\n    data=dataset_name,\n    evaluators=[check_specific_tool_call],\n    num_repetitions=3,\n    metadata={\"version\": metadata},\n)\n```", "```py\nimport {evaluate} from 'langsmith/evaluation';\n\n// Predict Assistant\nfunction predictAssistant(example) {\n    /**\n * Invoke assistant for single tool call evaluation\n */\n    const msg = [{ role: \"user\", content: example.input }];\n    const result = assistantRunnable.invoke({ messages: msg });\n    return { response: result };\n}\n\n// Check Specific Tool Call\nfunction checkSpecificToolCall(rootRun, example) {\n    /**\n * Check if the first tool call in the response matches the expected \n * tool call.\n */\n\n    // Expected tool call\n    const expectedToolCall = \"sql_db_list_tables\";\n\n    // Run\n    const response = rootRun.outputs.response;\n\n    // Get tool call\n    let toolCall;\n    try {\n        toolCall = response.tool_calls?.[0]?.name;\n    } catch (error) {\n        toolCall = null;\n    }\n\n    const score = toolCall === expectedToolCall ? 1 : 0;\n    return { score, key: \"single_tool_call\" };\n}\n\n// Experiment Results\nconst experimentResults = evaluate(predictAssistant, {\n    data: datasetName,\n    evaluators: [checkSpecificToolCall],\n    numRepetitions: 3,\n});\n```", "```py\ndef predict_sql_agent_messages(example: dict):\n    \"\"\"Use this for answer evaluation\"\"\"\n    msg = {\"messages\": (\"user\", example[\"input\"])}\n    messages = graph.invoke(msg, config)\n    return {\"response\": messages}\n\ndef find_tool_calls(messages):\n    \"\"\"\n Find all tool calls in the messages returned\n \"\"\"\n    tool_calls = [\n        tc['name']\n        for m in messages['messages'] for tc in getattr(m, 'tool_calls', [])\n    ]\n    return tool_calls\n\ndef contains_all_tool_calls_any_order(\n    root_run: Run, example: Example\n) -> dict:\n    \"\"\"\n Check if all expected tools are called in any order.\n \"\"\"\n    expected = [\n        'sql_db_list_tables',\n        'sql_db_schema',\n        'sql_db_query_checker',\n        'sql_db_query',\n        'check_result'\n    ]\n    messages = root_run.outputs[\"response\"]\n    tool_calls = find_tool_calls(messages)\n    # Optionally, log the tool calls -\n    #print(\"Here are my tool calls:\")\n    #print(tool_calls)\n    if set(expected) <= set(tool_calls):\n        score = 1\n    else:\n        score = 0\n    return {\"score\": int(score), \"key\": \"multi_tool_call_any_order\"}\n\ndef contains_all_tool_calls_in_order(root_run: Run, example: Example) -> dict:\n    \"\"\"\n Check if all expected tools are called in exact order.\n \"\"\"\n    messages = root_run.outputs[\"response\"]\n    tool_calls = find_tool_calls(messages)\n    # Optionally, log the tool calls -\n    #print(\"Here are my tool calls:\")\n    #print(tool_calls)\n    it = iter(tool_calls)\n    expected = [\n        'sql_db_list_tables', \n        'sql_db_schema', \n        'sql_db_query_checker',\n        'sql_db_query', \n        'check_result'\n    ]\n    if all(elem in it for elem in expected):\n        score = 1\n    else:\n        score = 0\n    return {\"score\": int(score), \"key\": \"multi_tool_call_in_order\"}\n\ndef contains_all_tool_calls_in_order_exact_match(\n    root_run: Run, example: Example\n) -> dict:\n    \"\"\"\n Check if all expected tools are called in exact order and without any \n additional tool calls.\n \"\"\"\n    expected = [\n        'sql_db_list_tables',\n        'sql_db_schema',\n        'sql_db_query_checker',\n        'sql_db_query',\n        'check_result'\n    ]\n    messages = root_run.outputs[\"response\"]\n    tool_calls = find_tool_calls(messages)\n    # Optionally, log the tool calls -\n    #print(\"Here are my tool calls:\")\n    #print(tool_calls)\n    if tool_calls == expected:\n        score = 1\n    else:\n        score = 0\n\n    return {\"score\": int(score), \"key\": \"multi_tool_call_in_exact_order\"}\n\nexperiment_results = evaluate(\n    predict_sql_agent_messages,\n    data=dataset_name,\n    evaluators=[\n        contains_all_tool_calls_any_order,\n        contains_all_tool_calls_in_order,\n        contains_all_tool_calls_in_order_exact_match\n    ],\n    num_repetitions=3,\n)\n```", "```py\nimport {evaluate} from 'langsmith/evaluation';\n\n// Predict SQL Agent Messages\nfunction predictSqlAgentMessages(example) {\n  /**\n * Use this for answer evaluation\n */\n  const msg = { messages: [{ role: \"user\", content: example.input }] };\n  // Replace with your graph and config\n  const messages = graph.invoke(msg, config); \n  return { response: messages };\n}\n\n// Find Tool Calls\nfunction findToolCalls({messages}) {\n  /**\n * Find all tool calls in the messages returned\n */\n  return messages.flatMap(m => m.tool_calls?.map(tc => tc.name) || []);\n}\n\n// Contains All Tool Calls (Any Order)\nfunction containsAllToolCallsAnyOrder(rootRun, example) {\n  /**\n * Check if all expected tools are called in any order.\n */\n  const expected = [\n    \"sql_db_list_tables\",\n    \"sql_db_schema\",\n    \"sql_db_query_checker\",\n    \"sql_db_query\",\n    \"check_result\"\n  ];\n  const messages = rootRun.outputs.response;\n  const toolCalls = findToolCalls(messages);\n\n  const score = expected.every(tool => toolCalls.includes(tool)) ? 1 : 0;\n  return { score, key: \"multi_tool_call_any_order\" };\n}\n\n// Contains All Tool Calls (In Order)\nfunction containsAllToolCallsInOrder(rootRun, example) {\n  /**\n * Check if all expected tools are called in exact order.\n */\n  const messages = rootRun.outputs.response;\n  const toolCalls = findToolCalls(messages);\n\n  const expected = [\n    \"sql_db_list_tables\",\n    \"sql_db_schema\",\n    \"sql_db_query_checker\",\n    \"sql_db_query\",\n    \"check_result\"\n  ];\n\n  const score = expected.every(tool => {\n    let found = false;\n    for (let call of toolCalls) {\n      if (call === tool) {\n          found = true;\n          break;\n      }\n    }\n    return found;\n  }) ? 1 : 0;\n\n  return { score, key: \"multi_tool_call_in_order\" };\n}\n\n// Contains All Tool Calls (Exact Order, Exact Match)\nfunction containsAllToolCallsInOrderExactMatch(rootRun, example) {\n  /**\n * Check if all expected tools are called in exact order and without any \n * additional tool calls.\n */\n  const expected = [\n    \"sql_db_list_tables\",\n    \"sql_db_schema\",\n    \"sql_db_query_checker\",\n    \"sql_db_query\",\n    \"check_result\"\n  ];\n  const messages = rootRun.outputs.response;\n  const toolCalls = findToolCalls(messages);\n\n  const score = JSON.stringify(toolCalls) === JSON.stringify(expected) \n    ? 1 \n    : 0;\n  return { score, key: \"multi_tool_call_in_exact_order\" };\n}\n\n// Experiment Results\nconst experimentResults = evaluate(predictSqlAgentMessages, {\n  data: datasetName,\n  evaluators: [\n    containsAllToolCallsAnyOrder,\n    containsAllToolCallsInOrder,\n    containsAllToolCallsInOrderExactMatch\n  ],\n  numRepetitions: 3,\n});\n```", "```py\nexport LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_API_KEY=<your-api-key>\n\n# The below examples use the OpenAI API, though you can use other LLM providers\n\nexport OPENAI_API_KEY=<your-openai-api-key>\n```"]