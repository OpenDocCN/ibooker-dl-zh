<html><head></head><body>
<div id="book-content" class="calibre2">
<div id="sbo-rt-content" class="calibre3"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Operationalizing Generative AI Implementations" class="calibre5"><div class="preface" id="operationalizing_generative_ai_implementations">
      <h1 class="calibre4"><span class="keep-together">Chapter 5. </span>Operationalizing Generative AI Implementations</h1>
      <p class="subtitle">At this point, we have explored the evolution<a contenteditable="false" data-type="indexterm" data-primary="deployment of generative AI" id="xi_deploymentofgenerativeAI5355" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="deployment considerations" id="xi_generativeAIdeploymentconsiderations5355" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> of generative AI and Azure OpenAI Service, the main approaches for cloud native generative AI app development, and AI architectures and building blocks for LLM-enabled applications with Azure.</p>
      <p class="subtitle">In this chapter, we will explore the main considerations for going from implementation to production-level deployments. For this purpose, we will talk about advanced prompt engineering topics, related operations, security, and responsible AI considerations. All of these will contribute to a proper enterprise-grade implementation of cloud native, generative AI–enabled applications.</p>
      <section data-type="sect1" data-pdf-bookmark="The Art of Prompt Engineering" class="calibre5"><div class="preface" id="the_art_of_prompt_engineering">
        <h1 class="calibre4">The Art of Prompt Engineering</h1>
        <p class="subtitle"><em class="hyperlink">Prompt engineering</em> is one of those disciplines<a contenteditable="false" data-type="indexterm" data-primary="deployment of generative AI" data-secondary="prompt engineering" id="xi_deploymentofgenerativeAIpromptengineering5767" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" id="xi_promptengineering5767" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> that has taken existing AI skills frameworks by surprise. Before OpenAI’s ChatGPT, no one could imagine that the ability to interact with AI models by using just natural written language would be one of the most precious skills for companies trying to adopt, test, and deploy their generative AI systems. If there is an equivalent of the famous <a href="https://oreil.ly/0ZFLS" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">“Data Scientist: The Sexiest Job of the 21st Century”</a>, it is prompt engineering, with powerful examples such as the <a href="https://oreil.ly/kNLkR" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">prompt engineer job at Anthropic in the US</a>, with a base salary of $300K+.</p>
        <p class="subtitle">It is also a highly evolving area. What started as a simple way to send instructions to models is becoming a sort of “art” that allows you to also contextualize, secure, and operationalize LLMs. It has a mix of technical and creative skills. Some people see similarities between prompt engineer and QA (quality assurance) skills, as they both include empathy, creativity, technical testing, planning, etc. The lingo is also new. Similar to the call-response dynamics of traditional APIs, here we talk about prompt<a contenteditable="false" data-type="indexterm" data-primary="prompt" id="id724" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> (request) and completion<a contenteditable="false" data-type="indexterm" data-primary="completion" id="id725" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> (answer from the model).</p>
        <p class="subtitle"><a href="https://oreil.ly/T7PuQ" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Microsoft describes</a> prompt engineering as a key element to obtaining the best performance from GPT-enabled models, as models are very sensitive to the quality or shape of the prompts<a contenteditable="false" data-type="indexterm" data-primary="Azure OpenAI Service" data-secondary="prompting techniques" id="xi_AzureOpenAIServicepromptingtechniques59231" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. Here is the <a href="https://oreil.ly/jJkyA" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official guidance for prompting techniques with Azure OpenAI Service</a>, both for <a href="https://oreil.ly/8SMeX" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">chat</a> or <a href="https://oreil.ly/1VuzR" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">completion</a> scenarios. <a data-type="xref" href="#table-5-1" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Table 5-1</a> shows the recommended techniques in general terms.</p>
        <table id="table-5-1" class="calibre25">
          <caption class="calibre26"><span class="keep-together">Table 5-1. </span>Recommended prompting techniques</caption>
          <thead class="calibre27">
          <tr class="calibre28">
            <th class="calibre29">Recommendation</th>
            <th class="calibre29">Example</th>
          </tr>
        </thead>
        <tbody class="calibre30">
          <tr class="calibre28">
            <td class="calibre31"><p class="calibre46"><em class="hyperlink">Leverage both system messages</em> (at the beginning of the prompt to set context, instructions, etc.) <em class="hyperlink">and few-shot learning</em> (for examples of the desired input and output) as a way to improve performance.</p></td>
            <td class="calibre31">
              <p class="calibre46"><em class="hyperlink">Meta-prompt or system message:</em></p>
              <p class="calibre46">“You are an AI assistant for finance topics for company X, if anyone asks about something else, please say you cannot answer.”</p> 
              <p class="calibre46"><em class="hyperlink">Few-shot examples:</em></p>
              <p class="calibre46">“If anyone asks about the price of product A, redirect to this URL.”</p>
              <p class="calibre46">“If you get a question about company services, enumerate A, B, C, and D. Then ask the client to choose.”</p>
            </td>
          </tr>
          <tr class="calibre32">
            <td class="calibre31"><p class="calibre46"><em class="hyperlink">Use clear instructions</em>, define the expected format, and leverage both positive and negative examples.</p></td>
            <td class="calibre31">
              <p class="calibre46">“Provide answers in two paragraphs, max 1,000 tokens.”</p>
              <p class="calibre46">“Avoid talking about specific stock prices as they may be outdated. Instead, focus on enumerating trusted sources where clients can find those prices.”</p>
            </td>
          </tr>
          <tr class="calibre28">
            <td class="calibre31"><p class="calibre46"><em class="hyperlink">Adapt the prompts</em> to multiple scenarios or subtasks, depending on the context or user input, and <em class="hyperlink">use variables</em> as a technique to represent dynamic or unknown values in the input or output (for example, $name for username or $date for current date).</p></td>
            <td class="calibre31">
              <p class="calibre46"><em class="hyperlink">Passing the parameters as variables for the string:</em></p>  
              <p class="calibre46">“Provide recommendations to a user who is $age years old, from $location, adapting the language to their local context. Use their name $name when providing an answer.”</p>
            </td>
          </tr>
          <tr class="calibre32">
            <td class="calibre31"><p class="calibre46"><em class="hyperlink">Apply conditional logic</em>, as a way of using if-then statements or other logical operators to control the flow and content of the output, such as changing the tone, format, or information based on certain conditions or criteria.</p></td>
            <td class="calibre31">
              <p class="calibre46">“If the sentiment from the user prompts is mostly negative, use a kind, explicative, step-by-step approach.”</p>
              <p class="calibre46">“If the user prompts have a friendly tone, go directly to the point. One paragraph max”</p>
            </td>
          </tr>
          <tr class="calibre28">
            <td class="calibre31"><p class="calibre46"><em class="hyperlink">Use feedback loops</em> by adding the model’s output as part of the input for the next iteration, such as appending the output to the prompt or using it to generate new questions or instructions.</p></td>
            <td class="calibre31">
              <p class="calibre46"><em class="hyperlink">Meta-prompt or system message:</em></p>
              <p class="calibre46">“Answer user questions for company X, and keep in mind…&lt;output from previous discussion&gt; while answering. Explain the why of your reasoning.”</p>
            </td>
          </tr>
        </tbody>
        </table>
        <p class="subtitle"><a href="https://oreil.ly/gtxFR" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">OpenAI defines</a> their own set of best practices as well to optimize prompting and get the best model performance:</p>
        <dl class="stafflist">
          <dt class="calibre10">Always leverage the most recent model.</dt>
          <dd class="calibre11">
            <p class="subtitle">This allows you to take advantage of the latest advancements and updates. Ensure you are working with the most recent iteration of the model. This does not mean to use the most powerful model, but the most recent version of each model. You can always obtain the most recent version from the <a href="https://oreil.ly/BI5Ue" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official Azure OpenAI model page</a>.</p>
          </dd>
          <dt class="calibre10">Incorporate instructions at the outset.</dt>
          <dd class="calibre11">
            <p class="subtitle">Position your instructions at the start of your prompt. Use markers like ### or “"” to distinctly separate these instructions from the context.</p>
          </dd>
          <dt class="calibre10">Aim for specificity and detail.</dt>
          <dd class="calibre11">
            <p class="subtitle">Avoid vagueness when defining the desired context, outcome, length, format, and style.</p>
          </dd>
          <dt class="calibre10">Provide a clear output format via examples.</dt>
          <dd class="calibre11">
            <p class="subtitle">Examples can help guide the model toward your preferred output. For instance, Example 1, Example 2, etc.</p>
          </dd>
          <dt class="calibre10">Follow a progression by starting with a zero-shot approach<a contenteditable="false" data-type="indexterm" data-primary="zero-shot approach to prompt engineering" id="id726" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</dt>
          <dd class="calibre11">
            <p class="subtitle">This implies testing the model for specific questions without providing any illustrative examples. Then, proceed to few-shot scenarios, in which you provide one or several examples to the model, as LLMs can learn from their content and shape. If neither of these strategies yields the desired results, consider fine-tuning or grounding the model.</p>
          </dd>
          <dt class="calibre10">Eliminate fluffy descriptions. </dt>
          <dd class="calibre11">
            <p class="subtitle">Favor precision and brevity over vague, overcomplicated language to streamline your prompt and improve the model’s understanding.</p>
          </dd>
          <dt class="calibre10">Specify what to do. </dt>
          <dd class="calibre11">
            <p class="subtitle">Rather than merely pointing out what should be avoided, clearly articulate the desired action. This positively guides the model to perform as intended.</p>
          </dd>
          <dt class="calibre10">Nudge the model with leading words in code generation. </dt>
          <dd class="calibre11">
            <p class="subtitle">When your task is related to code generation, “leading words” can be instrumental in guiding the model toward a specific pattern.</p>
          </dd>
        </dl>
        <p class="subtitle">All of these recommendations and <a href="https://oreil.ly/Xfw5-" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">further recommendations</a> are oriented<a contenteditable="false" data-type="indexterm" data-primary="hallucination, AI model" id="id727" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to reduce <em class="hyperlink">generative AI model hallucination</em>, which is the ability (or limitation) of LLMs to create nonfactual information based on their creative ability. This is a recurring topic for all generative AI technologies, and most advanced architectures are created so that LLMs don’t deliver imaginary or incorrect results. Besides Microsoft’s and OpenAI’s best practices, this four-step framework can help with this problem by leveraging the best prompt engineering practices:</p>
        <dl class="stafflist">
          <dt class="calibre10">1. Include.</dt>
          <dd class="calibre11">
            <p class="subtitle">This strategy suggests including specific instructions in the prompt, such as requesting that the model not make stuff up and stick to facts. By providing clear guidelines, the AI model is more likely to generate accurate and factual content.</p>
          </dd>
          <dt class="calibre10">2. Restrict.</dt>
          <dd class="calibre11">
            <p class="subtitle">This approach involves limiting the output of the AI model. For example, you can choose from a confined list of options instead of allowing the model to generate free-form strings. By restricting the output, you can ensure that the generated text stays within the desired boundaries and is less likely to be based on hallucination.</p>
          </dd>
          <dt class="calibre10">3. Add chain of thought (CoT).</dt>
          <dd class="calibre11">
            <p class="subtitle">This strategy recommends incorporating a “chain of thought” style of instruction<a contenteditable="false" data-type="indexterm" data-primary="chain of thought (CoT) prompt instruction" id="id728" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="CoT (chain of thought) prompt instruction" id="id729" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, such as “Solve the problem step by step.” By guiding the AI model to follow a logical and structured thought process, it is more likely to produce coherent and accurate text.</p>
          </dd>
          <dt class="calibre10">4. Repeat and position.</dt>
          <dd class="calibre11">
            <p class="subtitle">This technique involves repeating the most important instructions in the prompt a couple of times and positioning them at the end of the prompt. This makes use of the latency effect, which means that the AI model is more likely to remember and follow the instructions that are presented last.</p>
          </dd>
        </dl>
        <p class="subtitle">By implementing these strategies in prompt engineering, you can improve the quality of AI-generated text and reduce the chances of hallucination, leading to more accurate and reliable content. This is fundamental for the operationalization of generative AI in the enterprise<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_AzureOpenAIServicepromptingtechniques59231" id="id730" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
        <p class="subtitle">As prompt engineering is a highly evolving area, I recommend you expand your knowledge with other fabulous external resources from community pros:</p>
        <dl class="stafflist">
          <dt class="calibre10">
            <a href="https://oreil.ly/ZmTev" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
              PromptsLab’s Awesome-Prompt-Engineering repo
            </a>
          </dt>
          <dd class="calibre11">
            <p class="subtitle">This repository<a contenteditable="false" data-type="indexterm" data-primary="PromptsLab, Awesome-Prompt-Engineering repo" id="id731" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Awesome-Prompt-Engineering repo" id="id732" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> contains hand-curated resources for prompt engineering with a focus on generative pre-trained transformers (GPT), ChatGPT, PaLM, etc. It includes papers, tutorials, blogs, videos, courses, and tools related to prompt engineering.</p>
          </dd>
          <dt class="calibre10">
            <a href="https://oreil.ly/lIbm7" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
              Lilian Weng’s blog
            </a>
          </dt>
          <dd class="calibre11">
            <p class="subtitle">Lilian<a contenteditable="false" data-type="indexterm" data-primary="Weng, Lilian" id="id733" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> is Head of Safety Systems at OpenAI. Her blog introduces the concept of prompt engineering, the challenges and opportunities it poses, and some examples of how to design effective prompts for different tasks.</p>
          </dd>
          <dt class="calibre10">
            <a href="https://oreil.ly/yQ7rt" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
              Chip Huyen’s blog
            </a>
          </dt>
          <dd class="calibre11">
            <p class="subtitle">Chip<a contenteditable="false" data-type="indexterm" data-primary="Huyen, Chip" id="id734" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Designing ML Systems (Huyen)" id="id735" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> is a well-known industry expert, cofounder of Claypot AI, and the author of <a class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2" href="https://oreil.ly/7UWAL"><em class="hyperlink">Designing Machine Learning Systems</em> (O’Reilly)</a>. She shares some best practices and tips for building LLM applications for production, such as how to choose the right model, how to optimize the inference speed, and how to monitor the quality and reliability of the outputs.</p>
          </dd>
          <dt class="calibre10">
            <a href="https://oreil.ly/3jvie" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
              Xavier Amatriain’s blog
            </a>
          </dt>
          <dd class="calibre11">
            <p class="subtitle">Xavier<a contenteditable="false" data-type="indexterm" data-primary="Amatriain, Xavier" id="id736" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> shares his incredible wealth of knowledge with <a href="https://oreil.ly/ZT8WB" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">101 (introduction and resources)</a> and <a href="https://oreil.ly/n_unu" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">201 (advanced methods and toolkits)</a> articles, as well as <a href="https://oreil.ly/jIbNt" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">online intro level prompt engineering training</a>.</p>
          </dd>
          <dt class="calibre10">DAIR.AI’s <a href="https://oreil.ly/JBB67" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Prompt Engineering Guide</a> and its <a href="https://oreil.ly/QzS_D" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">related GitHub repository</a></dt>
          <dd class="calibre11">
            <p class="subtitle">Guides, papers, lectures, notebooks, and resources for prompt engineering<a contenteditable="false" data-type="indexterm" data-primary="DAIR.AI" id="id737" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, including a series of <a href="https://oreil.ly/0sWII" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">examples for advanced prompt engineering scenarios</a>.</p>
          </dd>
        </dl>
        <p class="subtitle">Prompt engineering is just one step in operationalizing our generative AI implementations. We will now explore other operations related to Azure OpenAI and LLMs<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_deploymentofgenerativeAIpromptengineering5767" id="id738" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_promptengineering5767" id="id739" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Generative AI and LLMOps" class="calibre5"><div class="preface" id="generative_ai_and_llmops">
        <h1 class="calibre4">Generative AI and LLMOps</h1>
        <p class="subtitle">If we take all the architectural, model, and prompting considerations<a contenteditable="false" data-type="indexterm" data-primary="deployment of generative AI" data-secondary="LLMOps" id="xi_deploymentofgenerativeAILLMOps516181" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="LLMOps" id="xi_LLMOps516181" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, and we explore the envisioned workflow for an end-to-end LLM implementation, we come to the notion of <em class="hyperlink">LLMOps</em>, a new term used to define all LLM-related operations in the enterprise. LLMOps<a contenteditable="false" data-type="indexterm" data-primary="MLOps" id="xi_MLOps5161280" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> is similar to <a href="https://oreil.ly/VLSZA" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">MLOps (machine learning operations)</a>, which is a set of tools and best practices to manage the lifecycle of ML-powered applications.</p>
        <p class="subtitle"><a href="https://oreil.ly/dgVl1" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">LLMOps</a> is a discipline that combines several techniques for the development, deployment, and maintenance of LLM and generative AI applications. This includes prompt engineering, but also deployment and observability topics. The operations related to AI topics are not new, but have evolved exponentially in recent years, as you can see in <a data-type="xref" href="#fig_1_evolution_of_llmops" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 5-1</a>.</p>
        <figure class="calibre18"><div id="fig_1_evolution_of_llmops" class="figure">
          <img src="assets/aoas_0501.png" alt="" class="calibre19"/>
          <h6 class="calibre20"><span class="keep-together">Figure 5-1. </span>Evolution of LLMOps</h6>
        </div></figure>
        <p class="subtitle">This was an evolution<a contenteditable="false" data-type="indexterm" data-primary="scalability of generative AI" data-secondary="with LLMOps" data-secondary-sortas="LLMOps" id="id740" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> in terms of operations complexity, but also the scalability of the methods, and the availability of commercial platforms to make them simpler:</p>
        <dl class="stafflist">
          <dt class="calibre10">Up to 2015</dt>
          <dd class="calibre11">
            <p class="subtitle">This period represents the time before the development of modern MLOps practices. During this period, proprietary tools were used for modeling and inference, but there was also a rise of open source data science tools such as Python and R. These tools allowed for more flexibility and accessibility in data science and machine learning.</p>
          </dd>
          <dt class="calibre10">2015+</dt>
          <dd class="calibre11">
            <p class="subtitle">The inclusion of cloud native<a contenteditable="false" data-type="indexterm" data-primary="cloud native architectures and applications" data-seealso="implementation of generative AI" id="id741" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> and containerization<a contenteditable="false" data-type="indexterm" data-primary="containers and containerization" data-secondary="evolution of" id="id742" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> made it a bit easier to put models into production, and to scale in a robust and more efficient manner. This period witnessed the growth of MLOps platforms, which use dockerized ML stacks and deploy them both on premises or in the cloud via Kubernetes, including manageability and monitoring features.</p>
          </dd>
          <dt class="calibre10">2023+</dt>
          <dd class="calibre11">
            <p class="subtitle">The beginning of the LLMOps solutions market. A promising area, but still a very new one (we will explore one of the first LLMOps tools in this chapter), focused on specific functionalities that were not part of the traditional MLOps tools. That said, existing MLOps and new LLMOps approaches share some similarities, as you can see in <a data-type="xref" href="#fig_2_comparison_of_mlops_llmops_and_ai_provider_adopter" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 5-2</a>.</p>
          </dd>
        </dl>
        <figure class="calibre18"><div id="fig_2_comparison_of_mlops_llmops_and_ai_provider_adopter" class="figure">
          <img src="assets/aoas_0502.png" alt="" class="calibre19"/>
          <h6 class="calibre20"><span class="keep-together">Figure 5-2. </span>Comparison of MLOps/LLMOps and AI provider/adopter scope of activity</h6>
        </div></figure>
        <p class="subtitle">Summarizing, LLMOps bring a completely different split of AI model provider versus adopter activities as compared to MLOps, given the role of the pre-trained foundation models and their massive datasets. There are also clear differences in data needs (format, volume), the creation of pipelines and flows, and the methods to evaluate and monitor the results of the models. Also, some engineering tasks traditionally focused on preparing and testing ML models are evolving toward prompt <span class="keep-together">engineering.</span></p>
        <p class="subtitle">Additionally, companies<a contenteditable="false" data-type="indexterm" data-primary="Databricks" id="id743" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> such as Databricks <a href="https://oreil.ly/Rle9o" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">compare LLMOps to traditional MLOps</a> based on these concepts:</p>
        <ul class="stafflist">
          <li class="calibre8">
            <p class="calibre24">LLMs can be <em class="hyperlink">fine-tuned with new data</em> to adapt<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning GPT models" data-secondary="and LLMOps" data-secondary-sortas="LLMOps" id="id744" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="fine-tuning GPT models" id="id745" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to specific domains or tasks, which reduces the amount of data and resources needed compared to training from scratch.</p>
          </li>
          <li class="calibre8">
            <p class="calibre24">LLMs can benefit<a contenteditable="false" data-type="indexterm" data-primary="reinforcement learning from human feedback (RLHF)" id="id746" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning (ML)" data-secondary="RLHF" id="id747" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="RLHF (reinforcement learning from human feedback)" id="id748" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> from <em class="hyperlink">reinforcement learning from human feedback</em>, which helps in improving their performance and evaluating their outputs in open-ended tasks.</p>
          </li>
          <li class="calibre8">
            <p class="calibre24">LLMs have <em class="hyperlink">different performance metrics</em> than traditional ML models<a contenteditable="false" data-type="indexterm" data-primary="performance evaluation methods" id="id749" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="metrics for performance evaluation" id="id750" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="metrics versus ML" id="id751" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, such as <a href="https://oreil.ly/dWQ8D" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">BLEU</a> and <a href="https://oreil.ly/SRoZf" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">ROUGE</a>, or any of the <a href="https://oreil.ly/gfK28" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">built-in evaluation metrics</a> from Azure AI Studio that we discussed in <a data-type="xref" href="ch03.html#implementing_cloud_native_generative_ai_with_azure" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 3</a>.</p>
          </li>
          <li class="calibre8">
            <p class="calibre24">LLMs can be <em class="hyperlink">combined with other systems</em>, such as web search or vector databases<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_MLOps5161280" id="id752" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, to create pipelines that can handle complex tasks like knowledge base Q&amp;A, or even combined<a contenteditable="false" data-type="indexterm" data-primary="multi-agent systems (MAS)" id="id753" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="MAS (multi-agent systems)" id="id754" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> in more complex <a href="https://oreil.ly/CXcJI" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">multi-agent systems (MAS)</a>.</p>
          </li>
        </ul>
        <p class="subtitle">Coming back to generative AI activities, it is already clear that they go way beyond simply prompt engineering activities and include additional considerations at the system and application levels. Technical LLM and prompt-related questions can be explored from many different perspectives. For example:</p>
        <ul class="stafflist">
          <li class="calibre8">
            <p class="calibre24">From a <em class="hyperlink">user experience</em> perspective, by anticipating customer questions and assessing model responses, and including UX designers during the generative AI app development process</p>
          </li>
          <li class="calibre8">
            <p class="calibre24">Keeping in mind <em class="hyperlink">application capabilities</em>, including:</p>
            <ul class="calibre33">
              <li class="calibre8">
                <p class="calibre24">Cost, latency, and token length limitations (e.g., splitting tasks into smaller chunks)</p>
              </li>
              <li class="calibre8">
                <p class="calibre24">Articulating instructions, orchestrating prompt flows, and shifting things back and forth between assistant and system roles</p>
              </li>
              <li class="calibre8">
                <p class="calibre24">Adjusting model parameters such as temperature, output formats, etc.</p>
              </li>
            </ul>
          </li>
          <li class="calibre8">
            <p class="calibre24">Combining core LLM and prompt activities with the overall <em class="hyperlink">architecture design</em>:</p>
            <ul class="calibre33">
              <li class="calibre8">
                <p class="calibre24">Working with architects to address complex system requirements</p>
              </li>
              <li class="calibre8">
                <p class="calibre24">Employing advanced patterns like RAG and rounding AI responses in enterprise data to obtain accurate answers</p>
              </li>
            </ul>
          </li>
          <li class="calibre8">
            <p class="calibre24">Including <em class="hyperlink">compliance, security, and responsibility</em> questions from the design <span class="keep-together">phase—</span>for example, choosing the best Azure region to guarantee data residency in EU countries, or choosing the best filtering/moderation settings at both the prompt and completion levels</p>
          </li>
        </ul>
        <p class="subtitle">In the following sections, we will explore some of these topics, and the considerations and options available from an Azure and Azure OpenAI perspective. Let’s start by talking about prompt flows and pipelines.</p>
        <section data-type="sect2" data-pdf-bookmark="Prompt Flow and Azure ML" class="calibre5"><div class="preface" id="prompt_flow_and_azure_ml">
          <h2 class="calibre21">Prompt Flow and Azure ML</h2>
          <p class="subtitle">Azure AI Studio and <a href="https://oreil.ly/JyodR" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure ML</a> are enterprise-grade AI services<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="Azure ML and prompt flow" id="xi_promptengineeringAzureMLandpromptflow5239118" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Azure ML and prompt flow" id="xi_AzureMLandpromptflow5239118" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning (ML)" data-secondary="Azure ML and prompt flow" id="xi_machinelearningMLAzureMLandpromptflow5239118" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="LLMOps" data-secondary="Azure ML and prompt flow" id="xi_LLMOpsAzureMLandpromptflow5239118" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> for the end-to-end machine learning lifecycle, which includes building, testing, deploying, and managing machine learning models<a contenteditable="false" data-type="indexterm" data-primary="AutoML" id="id755" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. They are PaaSs that include <a href="https://oreil.ly/i0jAV" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">AutoML functionalities</a> to leverage existing pre-built classification, regression, forecasting, computer vision, and NLP models.</p>
          <p class="subtitle">With the arrival of Azure OpenAI Service to the family of Azure AI solutions, Azure ML has incorporated a new functionality called <em class="hyperlink">prompt flow</em>. A prompt flow <a contenteditable="false" data-type="indexterm" data-primary="prompt flow" id="xi_promptflow5240181" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>is a graphical representation of the data flow and processing logic of your AI application (it offers a <a href="https://oreil.ly/_JxYW" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Python library</a> and a <a href="https://oreil.ly/FjBbh" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Visual Studio extension</a> as well); this Azure ML feature is a development tool designed to streamline the entire development cycle of LLM-enabled applications. </p>
          <p class="subtitle">Microsoft <a href="https://oreil.ly/AI5N-" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">defines flows</a> as executable workflows that streamline the development of your LLM-based AI application, with a comprehensive framework for managing and processing data flows. Prompt flow includes three different <a href="https://oreil.ly/BZRXb" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">types of flows</a>:</p>
          <dl class="stafflist">
            <dt class="calibre10">Standard flow</dt>
            <dd class="calibre11">
              <p class="subtitle">This is the <a href="https://oreil.ly/3yWpf" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">default flow type</a> for general application development, for instruction (not chat) scenarios. You can use a variety of built-in tools to create a flow that connects LLMs, prompts, and Python tools. You can also customize and debug your flow using a notebook-like interface.</p>
            </dd>
            <dt class="calibre10">Chat flow</dt>
            <dd class="calibre11">
              <p class="subtitle">This is a <a href="https://oreil.ly/XlxTr" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">specialized flow type</a> for conversational applications. You can use the same tools as the standard flow, but with additional features for chat inputs/outputs and chat history management. You can also test and debug your flow in a native conversation mode.</p>
            </dd>
            <dt class="calibre10">Evaluation flow</dt>
            <dd class="calibre11">
              <p class="subtitle">This is a <a href="https://oreil.ly/IGZNA" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">dedicated flow type for evaluation scenarios</a>. You can use this flow to measure the quality and effectiveness of your prompts and flows using built-in or custom evaluation flows. You can also compare the results of different prompt variants using charts and tables.</p>
            </dd>
          </dl>
          <p class="subtitle">Regardless of the prompt type, the <a href="https://oreil.ly/jZEeX" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">prompt flow platform</a> focuses on the different implementation phases of Azure OpenAI and other LLMs in Azure, including the four-stage process that you can see in <a data-type="xref" href="#fig_3_llm_prompt_flow_steps_source_microsoft" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 5-3</a>.</p>
          <figure class="calibre18"><div id="fig_3_llm_prompt_flow_steps_source_microsoft" class="figure">
            <img src="assets/aoas_0503.png" alt="" class="calibre19"/>
            <h6 class="calibre20"><span class="keep-together">Figure 5-3. </span>LLM prompt flow steps (source: adapted from an image by Microsoft)</h6>
          </div></figure>
          <p class="subtitle">Let’s walk through each step:</p>
          <dl class="stafflist">
            <dt class="calibre10">1. Initialization (or creation)</dt>
            <dd class="calibre11">
              <p class="subtitle">Use the prompt flow authoring canvas to design and develop your prompt flow. It connects LLMs, prompts, and Python tools in one prompt flow, and it can generate multiple <a href="https://oreil.ly/MWjlS" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">prompt variants</a> to adjust the LLM outputs. It also allows <a href="https://oreil.ly/7GqOL" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">integration with LangChain functionalities</a>.</p>
            </dd>
            <dt class="calibre10">2. Experimentation (or testing)</dt>
            <dd class="calibre11">
              <p class="subtitle">In this stage, the prompt flow testing panel helps you run and debug your prompt flow. You can see the input and output of each node in your prompt flow and their variants. </p>
            </dd>
            <dt class="calibre10">3. Evaluation and refinement</dt>
            <dd class="calibre11">
              <p class="subtitle">In this stage, you can use the <a href="https://oreil.ly/4NrIz" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">prompt flow evaluation panel</a> to assess the quality and effectiveness of your prompt <a href="https://oreil.ly/htoB5" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">flow versions/variants</a>. You can use built-in evaluation flows or create your own custom evaluation flows to measure different metrics, such as accuracy, fluency, diversity, and relevance. You can also see the results of your evaluation flows in charts and tables.</p>
            </dd>
            <dt class="calibre10">4. Production (or deployment)</dt>
            <dd class="calibre11">
              <p class="subtitle">In this stage, you can use the prompt flow deployment panel to <a href="https://oreil.ly/FPZDN" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">deploy your prompt flow</a> as a real-time endpoint, for example via Azure Kubernetes Service (AKS). You can also <a href="https://oreil.ly/0GS-u" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">monitor the endpoints via Azure Monitor</a>, <a href="https://oreil.ly/XuSsH" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">troubleshoot</a>, and manage them using Azure AI/ML Studio’s <a href="https://oreil.ly/6UWqf" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">prompt flow runtimes</a>.</p>
            </dd>
          </dl>
          <p class="subtitle">Prompt flow is a very powerful (and evolving) tool with the capability to plan and deploy prompt-based implementations. The next step is to plan security requirements for these generative AI applications.</p>
          <div data-type="note" epub:type="note" class="calibre15"><h6 class="calibre16">Note</h6>
            <p class="subtitle">At the time of writing this book, Microsoft released <a href="https://oreil.ly/QdI37" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">a series of functionalities</a> that are relevant to this and the next section, as they include performance, safety, and security capabilities:</p>
            <dl class="stafflist">
              <dt class="calibre10"><a href="https://oreil.ly/GcCSo" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">AI-assisted safety evaluations</a></dt>
              <dd class="calibre11">
                <p class="subtitle">This powerful feature<a contenteditable="false" data-type="indexterm" data-primary="AI-assisted safety evaluations" id="id756" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> will help you create automated evaluations to systematically assess and improve your generative AI applications before deploying to production. You can check the <a href="https://oreil.ly/XKzEs" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">transparency note</a> to understand how and when to use them.</p>
              </dd>
              <dt class="calibre10"><a href="https://oreil.ly/KwA_D" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Prompt Shield</a></dt>
              <dd class="calibre11">
                <p class="subtitle">This functionality protects generative AI development<a contenteditable="false" data-type="indexterm" data-primary="Prompt Shield" id="id757" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="security" data-secondary="Prompt Shield" id="id758" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> against direct and indirect attacks. Direct attacks are those included directly in the prompt, while indirect attacks happen when the application processes information that wasn’t directly authored by either the developer of the application or the user. You can learn more about Prompt Shields from the <a href="https://oreil.ly/_J-J9" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official <span class="keep-together">documentation</span></a>.</p>
              </dd>
              <dt class="calibre10"><a href="https://oreil.ly/P_Oz6" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Spotlighting</a></dt>
              <dd class="calibre11">
                <p class="subtitle">A technique from Microsoft Research<a contenteditable="false" data-type="indexterm" data-primary="Spotlighting" id="id759" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="security" data-secondary="Spotlighting" id="id760" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> that leverages the <a href="https://oreil.ly/Wbc7V" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">system prompt</a> to protect against indirect attacks<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_promptengineeringAzureMLandpromptflow5239118" id="id761" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_AzureMLandpromptflow5239118" id="id762" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_machinelearningMLAzureMLandpromptflow5239118" id="id763" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_LLMOpsAzureMLandpromptflow5239118" id="id764" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_promptflow5240181" id="id765" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
              </dd>
            </dl>
          </div>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Securing LLMs" class="calibre5"><div class="preface" id="securing_llms">
          <h2 class="calibre21">Securing LLMs</h2>
          <p class="subtitle">Creating efficient prompts<a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="securing" id="xi_LLMsLargeLanguageModelssecuring530140" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> and managing all required flows<a contenteditable="false" data-type="indexterm" data-primary="LLMOps" data-secondary="securing LLMs" id="xi_LLMOpssecuringLLMs530172" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="security" data-secondary="LLMOps" id="xi_securityLLMOps530172" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> is key to reaching a high level of performance for enterprise-level implementations. However, companies developing generative AI applications have high security requirements to reduce any potential risk. As you can see in <a data-type="xref" href="#fig_4_layered_approach_to_securing_llms" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 5-4</a>, there are several levels of security for any LLM development with Microsoft’s Azure Cloud and Azure OpenAI Service.</p>
          <figure class="calibre18"><div id="fig_4_layered_approach_to_securing_llms" class="figure">
            <img src="assets/aoas_0504.png" alt="" class="calibre19"/>
            <h6 class="calibre20"><span class="keep-together">Figure 5-4. </span>Layered approach to securing LLMs</h6>
          </div></figure>
          <p class="subtitle">This approach includes:</p>
          <dl class="stafflist">
            <dt class="calibre10">Service-level measures</dt>
            <dd class="calibre11">
              <p class="subtitle">Securing a generative AI implementation with Azure OpenAI starts by managing all service model-related topics, including core model performance, but also the protection of prompts, endpoints, and the APIs. Here are some ways to implement these:</p>
            
            <ul class="calibre22">
              <li class="calibre23">
                <p class="calibre24">For interaction with the model, use contextualization methods<a contenteditable="false" data-type="indexterm" data-primary="grounding techniques" id="id766" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="system message" id="id767" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="meta-prompts" id="id768" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="contextualization grounding technique" id="id769" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="meta-prompts" id="id770" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> via <em class="hyperlink">system message/meta-prompts</em> to define and reduce the topic scope. This allows you to programmatically avoid prompts that are not desired by design.</p>
              </li>
              <li class="calibre23">
                <p class="calibre24">For the prompt templates we define as reusable text strings, store and protect them via databases in Azure. Regardless of the format, those databases<a contenteditable="false" data-type="indexterm" data-primary="Azure Monitor" id="id771" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> can be securely consumed by implementing <a href="https://oreil.ly/mp1Ls" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">monitoring activities with Azure <span class="keep-together">Monitor</span></a>.</p>
              </li>
              <li class="calibre23">
                <p class="calibre24">For Azure OpenAI endpoints, Azure Application Gateway<a contenteditable="false" data-type="indexterm" data-primary="Azure Application Gateway" id="id772" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> provides a <a href="https://oreil.ly/6plOw" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">single point of entry and load balancing</a> to get the responses in a fast and reliable way. An Application Gateway can function as a Web Application Firewall (WAF)<a contenteditable="false" data-type="indexterm" data-primary="Web Application Firewall (WAF)" id="id773" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, providing protection against common web-based attacks, configured with a custom set of rules that match the requirements of your OpenAI application to ensure only authorized access. That said, load balancing is not supported for stateful operations like model fine-tuning, deployments, and inference of fine-tuned models. </p>
              </li>
              <li class="calibre23">
                <p class="calibre24">You can<a contenteditable="false" data-type="indexterm" data-primary="RBAC (role-based access control)" id="id774" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="role-based access control (RBAC)" id="id775" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> also leverage <a href="https://oreil.ly/YJOZv" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">RBAC (role-based access control) with Azure OpenAI</a>, to decide who can access what, depending on their rights to access specific information via generative AI applications. This is useful if you want to develop internal copilots for different departments that should access different information.</p>
              </li>
              <li class="calibre8">
                <p class="calibre24">For additional security controls, such as <a href="https://oreil.ly/4uV20" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">model auditing and monitoring</a>, Azure API Management<a contenteditable="false" data-type="indexterm" data-primary="Azure API Management" id="id776" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Microsoft Entra ID (Azure Active Directory)" id="id777" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Azure Active Directory (Microsoft Entra ID)" id="id778" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> helps grant <a href="https://oreil.ly/sQuG_" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">access to the model APIs</a>, leveraging <a href="https://oreil.ly/EXO6A" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Microsoft Entra ID</a> (Azure Active Directory) groups with subscription-based permissions, enabling request logging with Azure Monitor, and providing detailed usage metrics and key performance indicators for your models.</p>
              </li>
            </ul>
            </dd>
          
            <dt class="calibre10">Other cloud-level measures</dt>
            <dd class="calibre11">
              <p class="subtitle">Besides the core model measures, there are other security and networking best practices that will help secure the rest of the cloud native architecture<a contenteditable="false" data-type="indexterm" data-primary="cloud native architectures and applications" data-secondary="securing" id="id779" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>:</p>
            
            <ul class="calibre22">
              <li class="calibre23">
                <p class="calibre24">Use <em class="hyperlink">Azure Private Link</em> to connect API Management<a contenteditable="false" data-type="indexterm" data-primary="Azure Private Link" id="id780" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to your Azure OpenAI instances and <a href="https://oreil.ly/iBwcQ" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">other Azure resources such as AI Search</a>. This can help <a href="https://oreil.ly/jcELV" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">protect data and traffic</a> from external exposure and keep them within the private network. You can use <a href="https://oreil.ly/q0MUT" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">private endpoints</a> to connect<a contenteditable="false" data-type="indexterm" data-primary="private endpoints" id="id781" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> between different virtual networks.</p>
              </li>
              <li class="calibre23">
                <p class="calibre24"><a contenteditable="false" data-type="indexterm" data-primary="Azure Key Vault" id="id782" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>Enable <em class="hyperlink">Azure Key Vault</em> to <a href="https://oreil.ly/aTyqt" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">store the security keys and secrets</a> that are used by the generative AI applications. This can help prevent unauthorized access to your data and models. Alternatively, tools<a contenteditable="false" data-type="indexterm" data-primary="Databricks MLflow AI Gateway" id="id783" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> like <a href="https://oreil.ly/owP_k" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Databricks MLflow AI Gateway</a> can also help centralize management of LLM credentials and deployments, especially for cases that combine Azure OpenAI Service and other non-OpenAI LLMs.</p>
              </li>
              <li class="calibre23">
                <p class="calibre24">Deploy <em class="hyperlink">Azure Storage</em> to store<a contenteditable="false" data-type="indexterm" data-primary="Azure Storage" id="id784" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> model training artifacts and data<a contenteditable="false" data-type="indexterm" data-primary="Defender for Storage" id="id785" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, and <em class="hyperlink">Defender for Storage</em> to add an Azure-native <a href="https://oreil.ly/cAjUN" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">layer of security intelligence</a> that detects potential threats to storage accounts. This helps prevent malicious file uploads, sensitive data exfiltration, and data corruption. Additionally, you can leverage services such as Microsoft Sentinel<a contenteditable="false" data-type="indexterm" data-primary="Microsoft Sentinel" id="id786" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Cloud Defender for Databases" id="id787" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> and <em class="hyperlink">Cloud Defender for Databases</em>, a security service that protects databases with <a href="https://oreil.ly/Xk9Q-" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">attack detection and threat response</a>, or <a href="https://oreil.ly/FPScz" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Defender for APIs</a>, a service<a contenteditable="false" data-type="indexterm" data-primary="Defender for APIs" id="id788" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> that offers protection, detection, and response capabilities for your APIs. All this can help ensure that your data is accessible and secure.</p>
              </li>
              <li class="calibre23">
                <p class="calibre24">Leverage<a contenteditable="false" data-type="indexterm" data-primary="encryption mechanisms" id="id789" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> by-default <a href="https://oreil.ly/n9bqC" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">encryption mechanisms</a> in <a href="https://oreil.ly/zwa8V" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure</a> as a way to protect data natively at rest and in transit. More specifically, Azure OpenAI Service includes automatic ways to <a href="https://oreil.ly/aTCqC" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">encrypt your data</a> when it’s persisted to the cloud, in order to meet organizational security and compliance commitments.</p>
              </li>
              <li class="calibre23">
                <p class="calibre24"><em class="hyperlink">Govern<a contenteditable="false" data-type="indexterm" data-primary="governance, data" id="id790" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="data quality, managing" id="id791" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> data and manage data quality</em> by using <a href="https://oreil.ly/KFNn6" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Microsoft Purview (Microsoft’s unified data governance solution)</a> and third-party tools<a contenteditable="false" data-type="indexterm" data-primary="Microsoft Purview" id="id792" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="CluedIn" id="id793" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Profisee" id="id794" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="master data management (MDM)" id="id795" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> such as <a href="https://oreil.ly/thi-L" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">CluedIn</a> or <a href="https://oreil.ly/Y4IPx" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Profisee</a> for master data management (MDM) and data quality. You will learn more about this topic in <a data-type="xref" href="ch07.html#exploring_the_big_picture" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 7</a> within the book’s expert <span class="keep-together">interviews.</span></p>
              </li>
              <li class="calibre8">
                <p class="calibre24">Last but not least, there are other building blocks<a contenteditable="false" data-type="indexterm" data-primary="Well-Architected Framework" id="id796" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="end-to-end landing zone" id="id797" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> based on the <a href="https://oreil.ly/jHwtt" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Well-Architected Framework</a> that help build an <a href="https://oreil.ly/wQkFc" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">end-to-end landing zone</a> for highly secured Azure OpenAI implementations.</p>
              </li>
            </ul>
            </dd>
         
            <dt class="calibre10">General company-level governance measures</dt>
            <dd class="calibre11">
              <p class="subtitle">These may include measures such as the following:</p>
            <ul class="calibre22">
              <li class="calibre23">
                <p class="calibre24">From a general <em class="hyperlink">security management</em> perspective<a contenteditable="false" data-type="indexterm" data-primary="AI Security Risk Assessment" id="id798" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, the <a href="https://oreil.ly/YzOaS" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">AI Security Risk Assessment at Microsoft</a> is a process of evaluating the potential risks and vulnerabilities of AI systems, such as machine learning models, data pipelines, and deployment environments. Microsoft developed a framework and a tool to help organizations conduct AI security risk assessments and improve the security of their AI systems, and it can be leveraged for Azure OpenAI and generative AI implementations.</p>
              </li>
              <li class="calibre8">
                <p class="calibre24">From a <em class="hyperlink">security testing and risk mitigation</em> point of view<a contenteditable="false" data-type="indexterm" data-primary="red teaming" id="id799" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, the notion of <a href="https://oreil.ly/JLZeB" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">red teaming</a> defines systematic adversarial attacks for testing security vulnerabilities. <a href="https://oreil.ly/oDBO_" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Red teaming for Azure OpenAI and other LLMs</a> is a practice of testing the security and robustness of generative AI systems. It involves simulating adversarial attacks on AI systems and identifying potential harms or vulnerabilities that could affect their quality, reliability, and trustworthiness. Red teaming is an important part of the responsible development and deployment of AI systems that use LLMs. Testing is done at both the LLM and application/UI levels.</p>
              </li>
            </ul>
          </dd>
          </dl>
          <p class="subtitle">Even if this three-level approach can help secure and avoid most security risks, this new area of development requires continued analysis and improvement. As with any other generative AI topics, the industry keeps updating the list of potential risks related to LLMs, and being aware of them can help reinforce your security initiatives. </p>
          <p class="subtitle">The <a href="https://oreil.ly/WnMkm" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">OWASP Foundation</a> has elaborated a comprehensive list<a contenteditable="false" data-type="indexterm" data-primary="OWASP Foundation" id="id800" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> of the main risks and vulnerabilities often seen in LLM applications, highlighting their potential impact, ease of exploitation, and prevalence in real-world applications:</p>
          <dl class="stafflist">
            <dt class="calibre10">Prompt injection</dt>
            <dd class="calibre11">
              <p class="subtitle">This is a way of tricking<a contenteditable="false" data-type="indexterm" data-primary="prompt injection" id="id801" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> an LLM by giving it clever inputs that change its behavior (for example, imagine an HR application for automated CV analysis that <span class="keep-together">leverages</span> LLMs where someone inserts a prompt in hidden text that alters the backend of the AI-enabled tool). In general, the inputs can overwrite the system prompts that guide the LLM, or even manipulate data from other sources that the LLM uses. This includes jailbreaking, a technique that exploits prompt manipulation to bypass usage policy measures in LLM chatbots, enabling the generation of responses and malicious content that violate the policies of the chatbot. All these issues can come from any part of the generative AI code, including development with pieces such as LangChain and Semantic Kernel.</p>
            </dd>
            <dt class="calibre10">Insecure output handling</dt>
            <dd class="calibre11">
              <p class="subtitle">This is a problem<a contenteditable="false" data-type="indexterm" data-primary="output handling, insecure" id="id802" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> that occurs when an LLM output is not checked carefully before using it, exposing other systems to risks. The output may contain harmful content that can cause different kinds of attacks. For example, this could occur in RAG scenarios with LLMs connecting and sending insecure queries to databases.</p>
            </dd>
            <dt class="calibre10">Training data poisoning</dt>
            <dd class="calibre11">
              <p class="subtitle">Someone messes<a contenteditable="false" data-type="indexterm" data-primary="training data poisoning" id="id803" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> with the data that is used to train an LLM, making it vulnerable or biased and affecting its security, performance, or ethics. </p>
            </dd>
            <dt class="calibre10">Model denial of service</dt>
            <dd class="calibre11">
              <p class="subtitle">Attackers<a contenteditable="false" data-type="indexterm" data-primary="denial of service attacks, model" id="id804" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> make an LLM do a lot of work that uses up its resources, making it slow or expensive. The problem is worse because LLMs need a lot of resources to run, and the user inputs are hard to predict.</p>
            </dd>
            <dt class="calibre10">Supply chain vulnerabilities</dt>
            <dd class="calibre11">
              <p class="subtitle">An LLM application<a contenteditable="false" data-type="indexterm" data-primary="supply chain vulnerabilities, as targets of attack" id="id805" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> can be compromised by using components or services that have weaknesses, leading to security attacks. The components or services may include third-party datasets, pre-trained models, and plug-ins.</p>
            </dd>
            <dt class="calibre10">Sensitive information disclosure</dt>
            <dd class="calibre11">
              <p class="subtitle">This can<a contenteditable="false" data-type="indexterm" data-primary="sensitive information disclosures" id="id806" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> occur when an LLM accidentally reveals private data in its responses, allowing unauthorized access, privacy violations, and security breaches. It is important to clean the data and enact strict user policies to prevent this. This can also apply to meta-prompt leakage, revealing key performance information to external users.</p>
            </dd>
            <dt class="calibre10">Insecure plug-in design</dt>
            <dd class="calibre11">
              <p class="subtitle">This becomes<a contenteditable="false" data-type="indexterm" data-primary="plug-ins" data-secondary="insecure design vulnerability" id="id807" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> an issue when LLM plug-ins have unsafe inputs and poor access control. This lack of application control makes them easy to exploit and can result in consequences like remote code execution.</p>
            </dd>
            <dt class="calibre10">Excessive agency</dt>
            <dd class="calibre11">
              <p class="subtitle">LLM-based systems<a contenteditable="false" data-type="indexterm" data-primary="excessive agency" id="id808" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> may do things that have unintended consequences. For example, if the LLM can interface and control other systems (i.e., an AI copilot controlling some software-based functionalities for an autonomous car), it can increase the attack surface. The issue comes from giving too much functionality, permissions, or autonomy to LLM-based systems, and can impact not only the AI piece, but also the rest of the connected systems.</p>
            </dd>
            <dt class="calibre10">Overreliance</dt>
            <dd class="calibre11">
              <p class="subtitle">This occurs when systems or people depend too much on LLMs without supervision<a contenteditable="false" data-type="indexterm" data-primary="overreliance on LLMs without supervision" id="id809" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. They may face problems like misinformation, miscommunication, legal issues, and security vulnerabilities due to incorrect or inappropriate content generated by LLMs. It can also generate shadow IT issues, where company employees may be using LLM-enabled systems that are not part of the approved list of applications.</p>
            </dd>
            <dt class="calibre10">Model theft</dt>
            <dd class="calibre11">
              <p class="subtitle">This occurs when someone<a contenteditable="false" data-type="indexterm" data-primary="model theft" id="id810" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> accesses, copies, or steals proprietary LLM models without permission. The impacts include economic losses, compromised competitive advantage, and potential access to sensitive information. Research has shown that it is even possible to <a href="https://oreil.ly/gvG-Q" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">re-create part of the training sets of an LLM</a>.</p>
            </dd>
          </dl>
          <p class="subtitle">Additionally, there are other organizations already <a href="https://oreil.ly/8vcyo" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">exploring risks related to generative AI open source software (OSS)</a>, due to its special nature<a contenteditable="false" data-type="indexterm" data-primary="open source software (OSS)" data-secondary="security risks of" id="id811" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. That said, securing both closed and open models will continue to be an important area of study. Let’s now analyze other legal considerations<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_LLMOpssecuringLLMs530172" id="id812" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_securityLLMOps530172" id="id813" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_LLMsLargeLanguageModelssecuring530140" id="id814" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Managing Privacy and Compliance" class="calibre5"><div class="preface" id="managing_privacy_and_compliance">
          <h2 class="calibre21">Managing Privacy and Compliance</h2>
          <p class="subtitle">Securing generative AI developments<a contenteditable="false" data-type="indexterm" data-primary="LLMOps" data-secondary="privacy and compliance management" id="xi_LLMOpsprivacyandcompliancemanagement541749" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="privacy and compliance management" id="xi_privacyandcompliancemanagement541749" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="compliance for data privacy, LLMOps" id="xi_compliancefordataprivacyLLMOps541749" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="governance, data" id="xi_governancedata541749" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> is a must, but it is just one of the key elements for company-level implementations. There are additional compliance and data privacy requirements that will impact the technology choice, including considerations such as data residency, model availability by geographic region, etc.</p>
          <p class="subtitle">For that purpose, there are core features related to Microsoft Azure and the managed Azure OpenAI Service that help achieve compliance and facilitate any legal and auditing activity:</p>
          <ul class="stafflist">
            <li class="calibre8">
              <p class="calibre24">General <em class="hyperlink">data protection</em> mechanisms<a contenteditable="false" data-type="indexterm" data-primary="data protection mechanisms" id="id815" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> for Microsoft Azure services, which focus on the <a href="https://oreil.ly/SfBXe" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">key principle</a> of “giving you control over the data you put in the cloud. In other words, you control your data.” This is important to leverage key security and data protection features, while keeping control of the data.</p>
            </li>
            <li class="calibre8">
              <p class="calibre24"><a href="https://oreil.ly/lKBjQ" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Compliance information</a> related to all Azure-related services<a contenteditable="false" data-type="indexterm" data-primary="regulation of generative AI" id="id816" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. This includes international regulations such as <a href="https://oreil.ly/hiTwi" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">GDPR</a>, <a href="https://oreil.ly/2btT8" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">CCPA</a>, <a href="https://oreil.ly/Uy8hS" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">HIPAA</a>, etc. This guarantees that any implementation with Microsoft Azure (including Azure OpenAI) is aligned with all regulatory requirements.</p>
            </li>
            <li class="calibre8">
              <p class="calibre24">Personally identifiable information (PII)<a contenteditable="false" data-type="indexterm" data-primary="Personally identifiable information (PII)" id="id817" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> <a href="https://oreil.ly/kFlHF" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">detection and document redaction</a> via <a href="https://oreil.ly/b191_" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure AI Language</a>, which can enable your generative AI scenarios with a preliminary filtering of any sensitive data before creating your RAG-enabled scenario with your knowledge base. For example, this is very relevant for personal information in healthcare or finance scenarios.</p>
            </li>
            <li class="calibre8">
              <p class="calibre24">Specific advantages<a contenteditable="false" data-type="indexterm" data-primary="Azure OpenAI Service" data-secondary="data privacy and security" id="xi_AzureOpenAIServicedataprivacyandsecurity543037" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="security" data-secondary="Azure OpenAI Service capabilities" id="xi_securityAzureOpenAIServicecapabilities543037" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> of <em class="hyperlink">Azure OpenAI as a managed service</em>, when compared to other non-Azure options. Specifically:</p>
              <ul class="calibre33">
                <li class="calibre8">
                  <p class="calibre24"><em class="hyperlink">Data privacy and security</em>: The data sent to Azure OpenAI Service stays within Microsoft Azure and is not passed to OpenAI (the company) for predictions. Azure OpenAI Service automatically encrypts any data that is persisted in the cloud, including training data and fine-tuned models. It includes specific information about how data and prompts are handled. Refer to the <a href="https://oreil.ly/1hpAO" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official Microsoft documentation</a> for any updates to this information:</p>
<blockquote class="calibre36 pcalibre5 pcalibre6">
  <p class="calibre37">Your prompts (inputs) and completions (outputs), your embeddings, and your training data:</p>
  <ul class="calibre33">
    <li class="calibre8">are NOT available to other customers.</li>
    <li class="calibre8">are NOT available to OpenAI.</li>
    <li class="calibre8">are NOT used to improve OpenAI models.</li>
    <li class="calibre8">are NOT used to improve any Microsoft or 3rd party products or services.</li>
    <li class="calibre8">are NOT used for automatically improving Azure OpenAI models for your use in your resource (The models are stateless, unless you explicitly fine-tune models with your training data).</li>
    <li class="calibre8">Your fine-tuned Azure OpenAI models are available exclusively for your use.</li>
  </ul>
  <p class="calibre37">The Azure OpenAI Service is fully controlled by Microsoft; Microsoft hosts the OpenAI models in Microsoft’s Azure environment and the Service does NOT interact with any services operated by OpenAI (e.g. ChatGPT, or the OpenAI API).</p>
</blockquote>
                </li>
              <li class="calibre8">
                <p class="calibre24"><em class="hyperlink">Regional availability and private networks</em>: Azure OpenAI Service<a contenteditable="false" data-type="indexterm" data-primary="regional availability of data, control over" id="id818" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="private networks" id="id819" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> allows you to define the location of the models (based on specific <a href="https://oreil.ly/BI5Ue" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">model region availability</a>) data processing and storage for your training data, which can be important for meeting local regulations or customer <span class="keep-together">preferences.</span> </p>
              </li>
              <li class="calibre8">
                <p class="calibre24"><em class="hyperlink">Responsible content filtering:</em> Azure OpenAI Service<a contenteditable="false" data-type="indexterm" data-primary="content filters" id="id820" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> provides an <a href="https://oreil.ly/SzGoi" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">additional layer of content filtering</a> to prevent models from generating inappropriate or offensive content. At the API level, this means that the response may include <a href="https://oreil.ly/dQJ34" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"><code class="calibre12">finish_reason = content_filter</code></a> when the content is filtered.</p>
              </li>
              <li class="calibre8">
                <p class="calibre24"><em class="hyperlink">Other AI content safety features:</em> These included <a href="https://oreil.ly/_J-J9" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">jailbreak detection (now called Prompt Shields)</a>, <a href="https://oreil.ly/mbVoM" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">protected material detection</a>, and <a href="https://oreil.ly/hIfnG" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">service abuse monitoring</a>. These advantages<a contenteditable="false" data-type="indexterm" data-primary="jailbreak detection (Prompt Shields)" id="id821" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="protected material detection" id="id822" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="service abuse monitoring" id="id823" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, plus the content filtering, help improve the quality and safety of applications that use Azure OpenAI Service.</p>
              </li>
              <li class="calibre8">
                <p class="calibre24"><em class="hyperlink">Support and SLAs for reliability</em>: Azure OpenAI Service offers more comprehensive technical support<a contenteditable="false" data-type="indexterm" data-primary="service level agreements (SLAs)" id="id824" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> and a <a href="https://oreil.ly/stFaz" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">service level agreement (SLA)</a> that guarantees high availability of the service. This can provide more confidence and peace of mind to customers who use Azure OpenAI Service for their critical <span class="keep-together">applications.</span></p>
              </li>
              <li class="calibre8">
                <p class="calibre24"><em class="hyperlink">Specific </em><a href="https://oreil.ly/9LTNj" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"><em class="hyperlink">Azure OpenAI product terms</em></a> with data, intended use, intellectual property, and other details. This documents relevant conditions and Microsoft commitments for enterprise-grade implementations. </p>
              </li>
              <li class="calibre8">
                <p class="calibre24">Last but not least, Azure OpenAI includes custom data management options at both the data and prompt levels (which are equally considered private customer data), such as DELETE API operations, and the <a href="https://oreil.ly/66UHN" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">option to opt out</a> of automated prompt monitoring and filtering for harmful topics.</p>
              </li>
            </ul>
            </li>
          </ul>
            
            <p class="subtitle">Now, let’s continue with the last item of our generative AI operationalization topics, which focuses on existing and future regulations, as well as responsible AI practices for implementations with Azure OpenAI Service<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_deploymentofgenerativeAILLMOps516181" id="id825" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_LLMOps516181" id="id826" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_LLMOpsprivacyandcompliancemanagement541749" id="id827" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_privacyandcompliancemanagement541749" id="id828" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_compliancefordataprivacyLLMOps541749" id="id829" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_AzureOpenAIServicedataprivacyandsecurity543037" id="id830" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_governancedata541749" id="id831" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_securityAzureOpenAIServicecapabilities543037" id="id832" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. </p>
         
        </div></section>
      </div></section>
      <section data-type="sect1" class="calibre5" data-pdf-bookmark="Responsible AI and New Regulations"><div class="preface" id="responsible_ai_and_new_regulations">
        <h1 class="calibre4">Responsible AI and New Regulations</h1>
        <p class="subtitle">One of the direct consequences of the new generative<a contenteditable="false" data-type="indexterm" data-primary="deployment of generative AI" data-secondary="responsibility in and regulation of" id="xi_deploymentofgenerativeAIresponsibilityinandregulationof547064" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI (RAI)" id="xi_responsibleAIRAI547064" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="regulation of generative AI" id="xi_regulationofgenerativeAI547064" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="RAI (responsible AI)" id="xi_RAIresponsibleAI547064" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> AI era was the general awareness from all society actors of the potential advantages and risks of artificial intelligence. The “AI Ethics” movements are not new, but they were mainly related to academics, AI observatories, and international associations trying to make sense of the principles that should guide what a “good AI” would be, as well as the potential negative outcomes of AI-enabled systems. Now, with the arrival of generative AI and ChatGPT, regulatory initiatives are accelerating and including new considerations for LLMs, etc. From a platform point of view, Azure OpenAI Service and Azure AI Studio have evolved and incorporated several responsible AI (RAI) measures.</p>
        <p class="subtitle">This section includes contextual information (e.g., international regulations) that will be important to keep in mind while designing generative AI solutions, plus several resources that facilitate the implementation of generative AI with responsible AI approaches, including several Microsoft resources for RAI and LLMs, including Azure OpenAI Service models.</p>
        <section data-type="sect2" data-pdf-bookmark="Relevant Regulatory Context for Generative AI Systems" class="calibre5"><div class="preface" id="relevant_regulatory_context_for_generative_ai_syst">
          <h2 class="calibre21">Relevant Regulatory Context for Generative AI Systems</h2>
          <p class="subtitle">Even if AI regulations<a contenteditable="false" data-type="indexterm" data-primary="regulatory context for generative AI" id="xi_regulatorycontextforgenerativeAI547436" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> are still a work-in-progress at the international level (at least in 2024), there are some key initiatives that will help you understand what regulators will be focusing on, especially for your generative AI development:</p>
          <dl class="stafflist">
            <dt class="calibre10">The European Union (EU) AI Act</dt>
            <dd class="calibre11">
              <p class="subtitle">The first example<a contenteditable="false" data-type="indexterm" data-primary="European Union (EU) AI Act" id="xi_EuropeanUnionEUAIAct547835" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> of comprehensive <a href="https://oreil.ly/JGDQh" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">regulation for AI systems</a>, and a key reference for <a href="https://oreil.ly/x0Hi6" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">other international regulations</a> (e.g., Canada’s AI and Data Act, China’s AI regulation). It is mainly based on several levels of risks, with specific obligations for both providers and adopters (in this case, Microsoft is the provider of your Azure OpenAI models, and you or your company are the adopters). It also includes <em class="hyperlink">specific requirements for generative AI systems</em>. There are several levels of AI risk:</p>
            
              <dl class="stafflist">
                <dt class="calibre10">Unacceptable risk</dt>
                <dd class="calibre11"><p class="subtitle">These are AI systems that pose a clear threat to people’s safety, dignity, or rights, such as those that manipulate human behavior, exploit vulnerabilities, or enable social scoring or mass surveillance. Some exceptions may be allowed for law enforcement purposes under strict conditions and oversight. Most of your applications will never be at this level, but it is important to be aware of the “forbidden” kind of systems.</p></dd>
                <dt class="calibre10">High-risk systems</dt>
                <dd class="calibre11"><p class="subtitle">These are AI systems that have a significant impact on people’s lives or the functioning of society, such as those used in health, education, employment, justice, or transport. These systems will have to meet strict requirements before and after being deployed, such as ensuring data quality, human oversight, accuracy, security, and transparency. They will also have to be registered in an EU database.</p>
                <p class="subtitle">Depending on your industry and area of activity, it will be important to align to this sort of requirement. In general, it will be a way to provide information about the details of the system, at both a performance and maintenance level.</p></dd>
                <dt class="calibre10">Generative AI systems</dt>
                <dd class="calibre11"><p class="subtitle">These are specific requirements for generative AI systems, all of them relatively simple to implement with Azure and Azure OpenAI:</p>
                  <dl class="stafflist">
                    <dt class="calibre10">Disclosing content that is generated by AI</dt>
                    <dd class="calibre11"><p class="subtitle">This can be easily achieved by providing a watermark for the generated content, at both the UI level and when the user copies answers from the Azure OpenAI–enabled tool.</p></dd>
                    <dt class="calibre10">Designing the model to prevent it from generating illegal content</dt>
                    <dd class="calibre11"><p class="subtitle">This is directly related to the ability to filter inputs and outputs to avoid any kind of negative content. We will deep dive into this later in this chapter.</p></dd>
                    <dt class="calibre10">Publishing summaries of copyrighted data used for training</dt>
                    <dd class="calibre11"><p class="subtitle">This will include the initial provider obligations (directly related to the baseline LLM), and your obligations in the case of fine-tuning or grounding with other copyrighted data.</p></dd>
                  </dl>   
                </dd>
                <dt class="calibre10">Limited-risk systems</dt>
                <dd class="calibre11"><p class="subtitle">These are AI systems that pose little or no risk to people or society, such as those used for entertainment, leisure, or personal use. These systems will be mostly free from regulation, but will still have to comply with existing laws and ethical principles<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_EuropeanUnionEUAIAct547835" id="id833" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p></dd>
              </dl>
            </dd>
          
            <dt class="calibre10">The AI Risk Management Framework</dt>
            <dd class="calibre11">
              <p class="subtitle">From the National Institute of Standards and Technology (NIST)<a contenteditable="false" data-type="indexterm" data-primary="AI Risk Management Framework" id="id834" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="National Institute of Standards and Technology (NIST)" id="id835" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="NIST (National Institute of Standards and Technology)" id="id836" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> in the United States, <a href="https://oreil.ly/o3Y12" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">this framework</a> is not an AI regulation per se, but it sets the field for a definition of what a trustworthy AI should be, including generative AI applications. The framework says that AI systems need to be valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. NIST has launched a specific <a href="https://oreil.ly/HRZ4D" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">working group</a> for generative AI topics to catch up with latest developments. This framework is part of <a href="https://oreil.ly/T23SA" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Microsoft’s commitment</a> to adopt best practices in their products.</p>
            </dd>
            <dt class="calibre10">Other generative AI development regulatory resources</dt>
            <dd class="calibre11">
                <p class="subtitle">For example<a contenteditable="false" data-type="indexterm" data-primary="Association for Computing Machinery (ACM)" id="id837" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="ACM (Association for Computing Machinery)" id="id838" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, the <em class="hyperlink">Association for Computing Machinery (ACM)</em>’s <a href="https://oreil.ly/81nar" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">generative AI principles</a> include considerations for generative AI models, including limits and usage, personal data, correctability, and system ownership questions.</p>
                <p class="subtitle">Meanwhile<a contenteditable="false" data-type="indexterm" data-primary="Global Partnership on AI (GPAI)" id="id839" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="GPAI (Global Partnership on AI)" id="id840" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, the <em class="hyperlink">Global Partnership on AI (GPAI)</em>’s 2023 report on <a href="https://oreil.ly/LZaot" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Detection Mechanisms for Foundation Models</a> focuses on the detection side of AI-generated content, and complements the transparency requirements of international regulations and frameworks. There is a similar initiative<a contenteditable="false" data-type="indexterm" data-primary="Partnership on AI" id="id841" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> from the <a href="https://oreil.ly/WAEgA" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Partnership on AI</a> for generative AI and responsible practices for synthetic media.</p>
          </dd>
          </dl>
          <p class="subtitle">This list of regulations, frameworks, and recommendations will continue evolving in upcoming years, but all of them converge and include transparency and accountability questions that should be considered when developing an Azure OpenAI system. For that purpose, the next two sections include organization- and technical-level resources that you can apply to your generative AI development<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_regulatorycontextforgenerativeAI547436" id="id842" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Company-Level AI Governance Resources" class="calibre5"><div class="preface" id="company_level_ai_governance_resources">
          <h2 class="calibre21">Company-Level AI Governance Resources</h2>
          <p class="subtitle">Microsoft has released a series of resources<a contenteditable="false" data-type="indexterm" data-primary="governance, data" id="xi_governancedata551658" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to guide the responsible implementation of AI systems, including generative AI, which can serve as baseline or inspiration to adapt generative AI development with Azure OpenAI to responsible approaches:</p>
          <ul class="stafflist">
            <li class="calibre8">
              <p class="calibre24">Microsoft’s <a href="https://oreil.ly/fqvjp" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Responsible AI Standard (version 2)</a>, which includes RAI principles<a contenteditable="false" data-type="indexterm" data-primary="Responsible AI Standard v2 (Microsoft)" id="id843" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, and a comprehensive document with <a href="https://oreil.ly/ACiW4" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">requirements to adopt those principles</a>. This is the approach used at Microsoft to achieve fairness, reliability and safety, privacy and security, inclusiveness, transparency, and accountability. These principles are highly related to the regulations and frameworks we have previously analyzed, so they represent a good baseline for your generative AI implementations for the enterprise. If you want an alternative version, also oriented to generative AI, here is a list of <a href="https://oreil.ly/cmghy" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">RAI principles from LinkedIn</a>.</p>
            </li>
            <li class="calibre8">
              <p class="calibre24">The <a href="https://oreil.ly/LV_Ei" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Responsible AI Maturity Model</a>, a way to analyze<a contenteditable="false" data-type="indexterm" data-primary="Responsible AI Maturity Model" id="id844" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> and evaluate the level of responsible AI maturity at your company. It includes 5 levels and 24 empirically derived dimensions. This is a good way to make sure we are setting the foundations for a generative AI aligned with future regulations.</p>
            </li>
            <li class="calibre8">
              <p class="calibre24">Specific LLM and Azure OpenAI best practices and requirements to guarantee RAI approaches, including:</p>
              <ul class="calibre33">
                <li class="calibre8">
                  <p class="calibre24">A <em class="hyperlink">four-stage methodology for responsible AI and Azure OpenAI</em>, adapted from the general Microsoft RAI Standard, which <a href="https://oreil.ly/uJiId" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">includes measures</a> to:</p>
                  <ul class="calibre33">
                    <li class="calibre8">
                      <p class="calibre24"><em class="hyperlink">Identify</em> and prioritize potential harms that could result from your AI system through iterative red teaming, stress testing, and analysis.</p>
                    </li>
                    <li class="calibre8">
                      <p class="calibre24"><em class="hyperlink">Measure</em> the frequency and severity of those harms by establishing clear metrics, creating measurement test sets, and completing iterative, systematic testing (both manual and automated).</p>
                    </li>
                    <li class="calibre8">
                      <p class="calibre24"><em class="hyperlink">Mitigate</em> harm by implementing tools and strategies such as prompt engineering and content filters. Repeat measurement to test effectiveness after implementing mitigations.</p>
                    </li>
                    <li class="calibre8"><p class="calibre24"><em class="hyperlink">Define and execute</em> a deployment and operational readiness plan.</p></li>
                  </ul>
                </li>
                <li class="calibre8">
                  <p class="calibre24">An <a href="https://oreil.ly/l3zSt" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"><em class="hyperlink">eight-step approach to responsible AI for LLMs</em></a>, including Azure OpenAI and other open source options in Azure, such as Meta’s LLaMA2. It focuses on risk mitigation, user-centric design, and additional safety measures.</p>
                </li>
                <li class="calibre8">
                  <p class="calibre24">Specific <em class="hyperlink">requirements for adopting Azure OpenAI</em>, which includes a <a href="https://oreil.ly/PHCsq" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">code of conduct with forbidden use cases</a>, including violence, exploitation, harmful content, etc. and a <a href="https://oreil.ly/nCF82" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">transparency note with intended use cases</a> and adoption considerations.</p>
                </li>
              </ul>
            </li>
            <li class="calibre8">
              <p class="calibre24">The <a href="https://oreil.ly/XPSKw" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">HAX Toolkit</a>, which is a very good resource<a contenteditable="false" data-type="indexterm" data-primary="HAX Toolkit" id="id845" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> for your user-facing AI solutions, to support the design process and anticipate how the AI-enabled system will work and behave. </p>
            </li>
          </ul>
          <p class="subtitle">Organizational-level measures will help you align with regulations and international requirements. However, the actual implementation of countermeasures at the model level requires technical RAI tools<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_governancedata551658" id="id846" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Technical-Level Responsible AI Tools" class="calibre5"><div class="preface" id="technical_level_responsible_ai_tools">
          <h2 class="calibre21">Technical-Level Responsible AI Tools</h2>
          <p class="subtitle">These are the main tools and features you can use to guarantee that your Azure OpenAI implementations are aligned with RAI principles:</p>
          <ul class="stafflist">
            <li class="calibre8">
              <p class="calibre24">The general RAI <a href="https://oreil.ly/AP5-N" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Dashboard</a> and <a href="https://oreil.ly/Z8187" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Toolbox</a>, which Microsoft defines as the way to assess, develop, and deploy AI systems in a safe, trustworthy, and ethical manner, by using a collection of integrated tools and functionalities to help operationalize responsible AI in practice. The <a href="https://oreil.ly/gnM8L" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official repository</a> includes tools to evaluate errors, analyze fairness, understand data dimensions, interpret models, etc.</p>
            </li>
            <li class="calibre8">
              <p class="calibre24"><em class="hyperlink">Azure AI Content Safety</em>, which adds<a contenteditable="false" data-type="indexterm" data-primary="Azure AI Content Safety" id="id847" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> an <a href="https://oreil.ly/6LWOF" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">extra layer of protection</a> to filter out harmful inputs and outputs from the model. This can help prevent intentional abuse by your users and mistakes by the model. This safety system works by checking both the prompt and completion for your model with a group of classification models that aim to detect and stop the output of harmful content in four categories (hate, sexual, violence, and self-harm) and four severity levels (safe, low, medium, and high). The default setting is to filter content at the medium severity level for all four harm categories for both prompts and completions. You can access it:</p>
              <ul class="calibre33">
                <li class="calibre8">
                  <p class="calibre24">Directly from <a href="https://oreil.ly/C0iVp" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">AI Content Safety Studio</a>, which allows text, image, and multimodal moderation, as well as customization and online activity monitorization. It also includes Prompt Shields for your LLM-enabled deployments.</p>
                </li>
                <li class="calibre8">
                  <p class="calibre24">Via <a href="https://oreil.ly/9W9J9" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure OpenAI Studio’s content filter</a> for responsible AI moderation. Each filter can be applied to Azure OpenAI “deployments,” and those deployments will include the content filter for each chat or completion implementation<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_deploymentofgenerativeAIresponsibilityinandregulationof547064" id="id848" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_responsibleAIRAI547064" id="id849" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_regulationofgenerativeAI547064" id="id850" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_RAIresponsibleAI547064" id="id851" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
                </li>
              </ul>
            </li>
          </ul>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Conclusion" class="calibre5"><div class="preface" id="conclusion_4">
        <h1 class="calibre4">Conclusion</h1>
        <p class="subtitle">This chapter covered the final set of technical considerations for generative AI applications with Azure OpenAI Service. You have explored all relevant operational questions related to deploying, securing, protecting, and responsibly adopting generative AI. Remember, designing and architecting solutions with Azure OpenAI is “just” the first step (as we discussed in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch02.html#designing_cloud_native_architectures_for_generativ" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">2</a>, <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.html#implementing_cloud_native_generative_ai_with_azure" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">3</a>, and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#additional_cloud_and_ai_capabilities" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">4</a>). The operationalization of these generative AI applications is key for company-level implementations, where security, performance, and privacy, as well as regulations and AI ethics, are key aspects for sustainable project implementations.</p>
        <p class="subtitle">Now, we will continue with a key business-related aspect of generative AI: elaborating realistic and financially sustainable business cases<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_deploymentofgenerativeAI5355" id="id852" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_generativeAIdeploymentconsiderations5355" id="id853" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. This means analyzing potential projects and their expected benefit and justifying the human and technical cost by discussing ROI (return on investment) scenarios. These aspects are as relevant as the technical details we’ve explored thus far to successfully implementing generative AI applications in your company.</p>
      </div></section>
    </div></section></div>
</div>
</body></html>