<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">6 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/>CycleGAN: Converting blond hair to black hair</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-123"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">The idea behind CycleGAN and cycle consistency loss</li>
<li class="co-summary-bullet">Building a CycleGAN model to translate images from one domain to another</li>
<li class="co-summary-bullet">Training a CycleGAN by using any dataset with two domains of images</li>
<li class="co-summary-bullet">Converting black hair to blond hair and vice versa</li>
</ul>
<p class="body">The generative adversarial networks (GAN) models we have discussed in the last three chapters are all trying to produce images that are indistinguishable from those in the training set.</p>
<p class="body">You may be wondering: Can we translate images from one domain to another, such as transforming horses into zebras, converting black hair to blond hair or blond hair to black, adding or removing eyeglasses in images, turning photographs into paintings, or converting winter scenes to summer scenes? It turns out you can, and you’ll acquire such skills in this chapter through CycleGAN!</p>
<p class="body">CycleGAN was introduced in 2017.<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">1</a></sup> The key innovation of CycleGAN is its ability to learn to translate between domains without paired examples. CycleGAN has a variety of interesting and useful applications, such as simulating the aging or rejuvenation process on faces to assist digital identity verification or visualizing clothing in different colors or patterns without physically creating each variant to streamline the design process.</p>
<p class="body">CycleGAN uses a cycle consistency loss function to ensure the original image can be reconstructed from the transformed image, encouraging the preservation of key features. The idea behind cycle consistency loss is truly ingenious and deserves to be highlighted here. The CycleGAN in this chapter has two generators: let’s call them the black hair generator and the blond hair generator, respectively. The black hair generator takes in an image with blond hair (instead of a random noise vector as you have seen before) and converts it to one with black hair, while the blond hair generator takes in an image with black hair and converts it to one with blond hair.</p>
<p class="body">To train the model, we’ll give a real image with black hair to the blond hair generator to produce a fake image with blond hair. We’ll then give the fake blond hair image to the black hair generator to convert it back to an image with black hair. If both generators work well, there is little difference between the original image with black hair and the fake one after a round-trip conversion. To train the CycleGAN, we adjust the model parameters to minimize the sum of adversarial losses and cycle consistency losses. As in chapters 3 and 4, adversarial losses are used to quantify how well the generator can fool the discriminator and how well the discriminator can differentiate between real and fake samples. Cycle consistency loss, a unique concept in CycleGANs, measures the difference between the original image and the fake image after a round-trip conversion. The inclusion of the cycle consistency loss in the total loss function is the key innovation in CycleGANs.</p>
<p class="body">We’ll use black and blond hair images as examples of two domains when training CycleGAN. However, the model can be applied to any two domains of images. To drive home the message, I’ll ask you to train the same CycleGAN model by using images with and without eyeglasses that you used in chapter 5. The solution is provided in the book’s GitHub repository (<a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>), and you’ll see that the trained model can indeed add or remove eyeglasses from human face images.</p>
<h2 class="fm-head" id="heading_id_3">6.1 CycleGAN and cycle consistency loss</h2>
<p class="body"><a id="marker-124"/>CycleGAN extends the basic GAN architecture to include two generators and two discriminators. Each generator-discriminator pair is responsible for learning the mapping between two distinct domains. It aims to translate images from one domain to another (e.g., horses to zebras, summer to winter scenes, and so on) while retaining the key characteristics of the original images. It uses a cycle consistency loss that ensures the original image can be reconstructed from the transformed image, encouraging the preservation of key features.<a id="idIndexMarker001"/><a id="idIndexMarker002"/></p>
<p class="body">In this section, we’ll first discuss the architecture of CycleGAN. We’ll emphasize the key innovation of CycleGANs: cycle consistency loss.</p>
<h3 class="fm-head1" id="heading_id_4">6.1.1 What is CycleGAN?</h3>
<p class="body">CycleGAN consists of two generators and two discriminator<a id="idTextAnchor006"/>s. The generators translate images from one domain to another, while the discriminators determine the authenticity of the images in their respective domains. These networks are capable of transforming photographs into artworks mimicking the style of famous painters or specific art movements, thereby bridging the gap between art and technology. They can also be used in healthcare for tasks like converting MRI images to CT scans or vice versa, which can be helpful in situations where one type of imaging is unavailable or too costly. <a id="idIndexMarker003"/><a id="marker-125"/></p>
<p class="body">For our project in this chapter, we’ll convert between images with black hair and blond hair. We therefore use them as an example when explaining how CycleGAN works. Figure 6.1 is a diagram of the CycleGAN architecture.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="590" src="../../OEBPS/Images/CH06_F01_Liu.png" width="726"/></p>
<p class="figurecaption">Figure 6.1 The architecture of a CycleGAN to convert images with black hair to ones with blond hair and to convert images with blond hair to ones with black hair. The diagram also outlines the training steps to minimize adversarial losses. How the model minimizes cycle consistency losses is explained in figure 6.2.</p>
</div>
<p class="body">To train CycleGAN, we use unpaired datasets from the two domains we wish to translate between. We’ll use 48,472 celebrity face images with black hair and 29,980 images with blond hair. We adjust the model parameters to minimize the sum of adversarial losses and cycle consistency losses. For ease of explanation, we’ll explain only adversarial losses in figure 6.1. I’ll explain how the model minimizes cycle consistency losses in the next subsection.</p>
<p class="body">In each iteration of training, we feed real black hair images (top left in figure 6.1) to the blond hair generator to obtain fake blond hair images. We then feed the fake blond hair images, along with real blond hair images, to the blond hair discriminator (top middle). The blond hair discriminator produces a probability that each one is a real blond hair image. We then compare the predictions with the ground truth (whether an image is a true image with blond hair) and calculate the loss to the discriminator (<code class="fm-code-in-text">Loss_D_Blond</code>) as well as the loss to the generator (<code class="fm-code-in-text">Loss_G_Blond</code>).</p>
<p class="body">At the same time, in each iteration of training, we feed real blond hair images (middle left) to the black hair generator (bottom left) to create fake black hair images. We present the fake black hair images, along with real ones, to the black hair discriminator (middle bottom) to obtain predictions that they are real. We compare the predictions from the black hair discriminator with the ground truth and calculate the loss to the discriminator (<code class="fm-code-in-text">Loss_D_Black</code>) and the loss to the generator (<code class="fm-code-in-text">Loss_G_Black</code>). We train the generators and discriminators simultaneously. To train the two discriminators, we adjust the model parameters to minimize the discriminator loss, which is the sum of <code class="fm-code-in-text">Loss_D_Black</code> and <code class="fm-code-in-text">Loss_D_Blond</code>.</p>
<h3 class="fm-head1" id="heading_id_5">6.1.2 Cycle consistency loss</h3>
<p class="body"><a id="marker-126"/>To train the two generators, we adjust the model parameters to minimize the sum of the adversarial loss and cycle consistency loss. The adversarial loss is the sum of <code class="fm-code-in-text">Loss_G_Black</code> and <code class="fm-code-in-text">Loss_G_Blond</code> that we discussed in the previous subsection. To explain cycle consistency loss, let’s look at figure 6.2.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="478" src="../../OEBPS/Images/CH06_F02_Liu.png" width="464"/></p>
<p class="figurecaption">Figure 6.2 How CycleGAN minimizes cycle consistency losses between original black hair images and fake ones after round trips and cycle consistency losses between original blond hair images and fake ones after round trips</p>
</div>
<p class="body">The loss function for the generators in CycleGAN consists of two parts. The first part, the adversarial loss, ensures that generated images are indistinguishable from real images in the target domain. For example, <code class="fm-code-in-text">Loss_G_Blond</code> (defined in the previous subsection) ensures that fake blond images produced by the blond hair generator resemble real images with blond hair in the training set. The second part, the cycle consistency loss, ensures that an image translated from one domain to another can be translated back to the original domain.</p>
<p class="body">The cycle consistency loss is a crucial component of CycleGANs, ensuring that the original input image can be recovered after a round-trip translation. The idea is that if you translate a real black hair image (top left in figure 6.2) to a fake blond hair image and convert it back to a fake black hair image (top right), you should end up with an image close to the original black hair image. The cycle consistency loss for black hair images is the mean absolute error, at the pixel level, between the fake image and the original real one. Let’s call this loss <code class="fm-code-in-text">Loss_Cycle_Black</code>. The same applies to translating blond hair to black hair and then back to blond hair, and we call this loss <code class="fm-code-in-text">Loss_Cycle_Blond</code>. The total cycle consistency loss is the sum of <code class="fm-code-in-text">Loss_Cycle_Black</code> and <code class="fm-code-in-text">Loss_Cycle_Blond</code>. <a id="idIndexMarker004"/><a id="marker-127"/><a id="idIndexMarker005"/></p>
<h2 class="fm-head" id="heading_id_6">6.2 The celebrity faces dataset</h2>
<p class="body">We’ll use celebrity face images with black hair and blond hair as the two domains. You’ll first download the data in this section. You’ll then process the images to get them ready for training later in this chapter. <a id="idIndexMarker006"/><a id="idIndexMarker007"/></p>
<p class="body">You’ll use two new Python libraries in this chapter: <code class="fm-code-in-text">pandas</code> and <code class="fm-code-in-text">albumentations</code>. To install these libraries, execute the following line of code in a new cell in your Jupyter Notebook application on your computer:<a id="idIndexMarker008"/><a id="idIndexMarker009"/></p>
<pre class="programlisting">!pip install pandas albumentations</pre>
<p class="body">Follow the on-screen instructions to finish the installation.</p>
<h3 class="fm-head1" id="heading_id_7">6.2.1 Downloading the celebrity faces dataset</h3>
<p class="body">To download the celebrity faces dataset, log into Kaggle and go to the link <a class="url" href="https://mng.bz/Ompo">https://mng.bz/Ompo</a>. Unzip the dataset after downloading and place all image files inside the folder /files/img_align_celeba/img_align_celeba/ on your computer (note there is a subfolder with the same name in the folder itself). There are about 200,000 images in the folder. Also download the file <code class="fm-code-in-text">list_attr_celeba.csv</code> from Kaggle and place it in the /files/ folder on your computer. The CSV file specifies various attributes of each image.<a id="idIndexMarker010"/><a id="idIndexMarker011"/></p>
<p class="body">The celebrity faces dataset contains images with many different hair colors: brown, gray, black, blond, and so on. We’ll select images with black or blond hair as our training set because these two types are the most abundant in the celebrity faces dataset. Run the code in the following listing to select all images with black or blond hair.</p>
<p class="fm-code-listing-caption">Listing 6.1 Selecting images with black or blond hair</p>
<pre class="programlisting">import pandas as pd
import os, shutil
  
df=pd.read_csv("files/list_attr_celeba.csv")         <span class="fm-combinumeral">①</span>
os.makedirs("files/black", exist_ok=True)  
os.makedirs("files/blond", exist_ok=True)            <span class="fm-combinumeral">②</span>
folder="files/img_align_celeba/img_align_celeba"
for i in range(len(df)):
    dfi=df.iloc[i]
    if dfi['Black_Hair']==1:                         <span class="fm-combinumeral">③</span>
        try:
            oldpath=f"{folder}/{dfi['image_id']}"
            newpath=f"files/black/{dfi['image_id']}"
            shutil.move(oldpath, newpath)
        except:
            pass
    elif dfi['Blond_Hair']==1:                       <span class="fm-combinumeral">④</span>
        try:
            oldpath=f"{folder}/{dfi['image_id']}"
            newpath=f"files/blond/{dfi['image_id']}"
            shutil.move(oldpath, newpath)
        except:
            pass</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Loads the CSV file that contains image attributes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates two folders to store images with black and blond hair</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> If the attribute Black_Hair is 1, moves the image to the black folder.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> If the attribute Blond_Hair is 1, moves the image to the blond folder.</p>
<p class="body"><a id="marker-128"/>We first use the <code class="fm-code-in-text">pandas</code> library to load the file <code class="fm-code-in-text">list_attr_celeba.csv</code> so that we know whether each image has black or blond hair in it. We then create two folders locally, /files/black/ and /files/blond/, to store images with black and blond hair, respectively. Listing 6.1 then iterates through all images in the dataset. If an image’s attribute <code class="fm-code-in-text">Black_Hair</code> is 1, we move it to the folder /files/black/; if an image’s attribute <code class="fm-code-in-text">Blond_Hair</code> is 1, we move it to the folder /files/blond/. You’ll see 48,472 images with black hair and 29,980 images with blond hair. Figure 6.3 shows some examples of the images. <a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="idIndexMarker015"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="173" src="../../OEBPS/Images/CH06_F03_Liu.png" width="562"/></p>
<p class="figurecaption">Figure 6.3 Sample images of celebrity faces with black or blond hair</p>
</div>
<p class="body">Images in the top row of figure 6.3 have black hair while images in the bottom row have blond hair. Further, the image quality is high: all faces are front and center, and hair colors are easy to identify. The quantity and quality of the training data will help the training of the CycleGAN model.</p>
<h3 class="fm-head1" id="heading_id_8">6.2.2 Process the black and blond hair image data</h3>
<p class="body">We’ll generalize the CycleGAN model so that it can be trained on any dataset with two domains of images. We’ll also define a <code class="fm-code-in-text">LoadData()</code> class to process the training dataset for the CycleGAN model. The function can be applied to any dataset with two domains, whether human face images with different hair colors, images with or without eyeglasses, or images with summer and winter scenes. <a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="marker-129"/></p>
<p class="body">To that end, we have created a local module <code class="fm-code-in-text">ch06util</code>. Download the files <code class="fm-code-in-text">ch06util.py</code> and <code class="fm-code-in-text">__init__.py</code> from the book’s GitHub repository (<a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>) and place them in the folder /utils/ on your computer. In the local module, we have defined the following <code class="fm-code-in-text">LoadData()</code> class.<a id="idIndexMarker018"/><a id="idIndexMarker019"/><a id="idIndexMarker020"/><a id="idIndexMarker021"/></p>
<p class="fm-code-listing-caption">Listing 6.2 The <code class="fm-code-in-text">LoadData()</code> class to process the training data in CycleGAN</p>
<pre class="programlisting">class LoadData(Dataset):
    def __init__(self, root_A, root_B, transform=None):    <span class="fm-combinumeral">①</span>
        super().__init__()
        self.root_A = root_A
        self.root_B = root_B
        self.transform = transform
        self.A_images = []
        for r in root_A:
            files=os.listdir(r)
            self.A_images += [r+i for i in files]
        self.B_images = []
        for r in root_B:                                   <span class="fm-combinumeral">②</span>
            files=os.listdir(r)
            self.B_images += [r+i for i in files]
        self.len_data = max(len(self.A_images),
                            len(self.B_images))
        self.A_len = len(self.A_images)
        self.B_len = len(self.B_images)
    def __len__(self):                                     <span class="fm-combinumeral">③</span>
        return self.len_data
    def __getitem__(self, index):                          <span class="fm-combinumeral">④</span>
        A_img = self.A_images[index % self.A_len]
        B_img = self.B_images[index % self.B_len]
        A_img = np.array(Image.open(A_img).convert("RGB"))
        B_img = np.array(Image.open(B_img).convert("RGB"))
        if self.transform:
            augmentations = self.transform(image=B_img,
                                           image0=A_img)
            B_img = augmentations["image"]
            A_img = augmentations["image0"]
        return A_img, B_img</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The two folders root_A and root_B are where the images in the two domains are stored</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Loads all images in each domain</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Defines a method to count the length of the dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines a method to access individual elements in each domain</p>
<p class="body">The <code class="fm-code-in-text">LoadData()</code> class is inherited from the <code class="fm-code-in-text">Dataset</code> class in PyTorch. The two lists <code class="fm-code-in-text">root_A</code> and <code class="fm-code-in-text">root_B</code> contain folders of images in domains A and B, respectively. The class loads up images in the two domains and produces a pair of images, one from domain A and one from domain B so that we can use the pair to train the CycleGAN model later.<a id="idIndexMarker022"/><a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="idIndexMarker025"/></p>
<p class="body">As we did in previous chapters, we create a data iterator with batches to improve computational efficiency, memory usage, and optimization dynamics in the training process.</p>
<p class="fm-code-listing-caption">Listing 6.3 Processing the black and blond hair images for training</p>
<pre class="programlisting">transforms = albumentations.Compose(
    [albumentations.Resize(width=256, height=256),        <span class="fm-combinumeral">①</span>
        albumentations.HorizontalFlip(p=0.5),
        albumentations.Normalize(mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5],max_pixel_value=255),         <span class="fm-combinumeral">②</span>
        ToTensorV2()],
    additional_targets={"image0": "image"}) 
dataset = LoadData(root_A=["files/black/"],
    root_B=["files/blond/"],
    transform=transforms)                                 <span class="fm-combinumeral">③</span>
loader=DataLoader(dataset,batch_size=1,
    shuffle=True, pin_memory=True)                        <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Resizes the images to 256 by 256 pixels</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Normalizes the images to the range of -1 to 1</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Applies the LoadData() class on the images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates a data iterator for training</p>
<p class="body"><a id="marker-130"/>We first define an instance of the <code class="fm-code-in-text">Compose()</code> class in the <code class="fm-code-in-text">albumentations</code> library (which is famous for fast and flexible image augmentations) and call it <code class="fm-code-in-text">transforms</code>. The class transforms the images in several ways: it resizes images to 256 by 256 pixels and normalizes t<a id="idTextAnchor007"/>he values to the range –1 to 1. The <code class="fm-code-in-text">HorizontalFlip()</code> argument in listing 6.3 creates a mirror image of the original image in the training set. Horizontal flipping is a simple yet powerful augmentation technique that enhances the diversity of training data, helping models generalize better and become more robust. The augmentations and increase in size boost the performance of the CycleGAN model and make the generated images realistic. <a id="idIndexMarker026"/><a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="idIndexMarker029"/></p>
<p class="body">We then apply the <code class="fm-code-in-text">LoadData()</code> class to the black and blond hair images. We set the batch size to 1 since the images have a large file size, and we use a pair of images to train the model in each iteration. Setting the batch size to more than 1 may result in your machine running out of memory. <a id="idIndexMarker030"/><a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/></p>
<h2 class="fm-head" id="heading_id_9">6.3 Building a CycleGAN model</h2>
<p class="body">We’ll build a CycleGAN model from scratch in this section. We’ll take great care to make our CycleGAN model general so that it can be trained using any dataset with two domains of images. As a result, we’ll use A and B to denote the two domains instead of, for example, black and blond hair images. As an exercise, you’ll train the same CycleGAN model by using the eyeglasses dataset that you used in chapter 5. This helps you apply the skills you learned in this chapter to other real-world applications by using a different dataset.<a id="idIndexMarker034"/></p>
<h3 class="fm-head1" id="heading_id_10">6.3.1 Creating two discriminators</h3>
<p class="body">Even though CycleGAN has two discriminators, they are identical ex ante. Therefore, we’ll create one single <code class="fm-code-in-text">Discriminator()</code> class and then instantiate the class twice: one instance is discriminator A and the other discriminator B. The two domains in CycleGAN are symmetric, and it doesn’t matter which domain we call domain A: images with black hair or images with blond hair. <a id="idIndexMarker035"/><a id="idIndexMarker036"/><a id="marker-131"/><a id="idIndexMarker037"/></p>
<p class="body">Open the file <code class="fm-code-in-text">ch06util.py</code> you just downloaded. In it, I have defined the <code class="fm-code-in-text">Discriminator()</code> class.<a id="idIndexMarker038"/><a id="idIndexMarker039"/></p>
<p class="fm-code-listing-caption">Listing 6.4 Defining the <code class="fm-code-in-text">Discriminator()</code> class in CycleGAN</p>
<pre class="programlisting">class Discriminator(nn.Module):
    def __init__(self, in_channels=3, features=[64,128,256,512]):
        super().__init__()
        self.initial = nn.Sequential(
            nn.Conv2d(in_channels,features[0],                  <span class="fm-combinumeral">①</span>
                kernel_size=4,stride=2,padding=1,
                padding_mode="reflect"),
            nn.LeakyReLU(0.2, inplace=True))
        layers = []
        in_channels = features[0]
        for feature in features[1:]:                            <span class="fm-combinumeral">②</span>
            layers.append(Block(in_channels, feature, 
                stride=1 if feature == features[-1] else 2))
            in_channels = feature
        layers.append(nn.Conv2d(in_channels,1,kernel_size=4,    <span class="fm-combinumeral">③</span>
                stride=1,padding=1,padding_mode="reflect"))
        self.model = nn.Sequential(*layers)
    def forward(self, x):
        out = self.model(self.initial(x))
        return torch.sigmoid(out)                               <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The first Conv2d layer has 3 input channels and 64 output channels.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Three more Conv2d layers with 126, 256, and 512 output channels, respectively</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The last Conv2d layer has 512 input channels and 1 output channel.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Applies the sigmoid activation function on the output so it can be interpreted as a probability</p>
<p class="body">The previous code listing defines the discriminator network. The architecture is similar to the discriminator network in chapter 4 and the critic network in chapter 5. The main components are five <code class="fm-code-in-text">Conv2d</code> layers. We apply the sigmoid activation function on the last layer because the discriminator performs a binary classification problem. The discriminator takes a three-channel color image as input and produces a single number between 0 and 1, which can be interpreted as the probability that the input image is a real image in the domain.</p>
<p class="body">The <code class="fm-code-in-text">padding_mode="reflect"</code> argument we used in listing 6.4 means the padding added to the input tensor is a reflection of the input tensor itself. Reflect padding helps in preserving the edge information by not introducing artificial zero values at the borders. It creates smoother transitions at the boundaries of the input tensor, which is beneficial for differentiating images in different domains in our setting.</p>
<p class="body">We then create two instances of the class and call them <code class="fm-code-in-text">disc_A</code> and <code class="fm-code-in-text">disc_B</code>, respectively:</p>
<pre class="programlisting">from utils.ch06util import Discriminator, weights_init    <span class="fm-combinumeral">①</span>
import torch
  
device = "cuda" if torch.cuda.is_available() else "cpu"
disc_A = Discriminator().to(device)
disc_B = Discriminator().to(device)                       <span class="fm-combinumeral">②</span>
weights_init(disc_A)
weights_init(disc_B)                                      <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports the Discriminator class from the local module</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates two instances of the Discriminator class</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Initializes weights</p>
<p class="body">In the local module <code class="fm-code-in-text">ch06util</code>, we also defined a <code class="fm-code-in-text">weights_init()</code> function to initialize model weights. The function is defined similarly to the one in chapter 5. We then initialize weights in the two newly created discriminators, <code class="fm-code-in-text">disc_A</code> and <code class="fm-code-in-text">disc_B</code>. <a id="idIndexMarker040"/><a id="marker-132"/><a id="idIndexMarker041"/></p>
<p class="body">Now that we have two discriminators, we’ll create two generators next.</p>
<h3 class="fm-head1" id="heading_id_11">6.3.2 Creating two generators</h3>
<p class="body">Similarly, we define a single <code class="fm-code-in-text">Generator()</code> class in the local module and instantiate the class twice: one instance is generator A, and the other is generator B. In the file <code class="fm-code-in-text">ch06util.py</code> you just downloaded, we have defined the <code class="fm-code-in-text">Generator()</code> class.<a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="idIndexMarker046"/></p>
<p class="fm-code-listing-caption">Listing 6.5 The <code class="fm-code-in-text">Generator()</code> class in CycleGAN</p>
<pre class="programlisting">class Generator(nn.Module):
    def __init__(self, img_channels, num_features=64,
                 num_residuals=9):
        super().__init__()     
        self.initial = nn.Sequential(
            nn.Conv2d(img_channels,num_features,kernel_size=7,
                stride=1,padding=3,padding_mode="reflect",),
            nn.InstanceNorm2d(num_features),
            nn.ReLU(inplace=True))
        self.down_blocks = nn.ModuleList(
            [ConvBlock(num_features,num_features*2,kernel_size=3,
                       stride=2, padding=1),
            ConvBlock(num_features*2,num_features*4,kernel_size=3, <span class="fm-combinumeral">①</span>
                stride=2,padding=1)])
        self.res_blocks = nn.Sequential(                           <span class="fm-combinumeral">②</span>
            *[ResidualBlock(num_features * 4) 
            for _ in range(num_residuals)])
        self.up_blocks = nn.ModuleList(
            [ConvBlock(num_features * 4, num_features * 2,
                    down=False, kernel_size=3, stride=2,
                    padding=1, output_padding=1),
                ConvBlock(num_features * 2, num_features * 1,      <span class="fm-combinumeral">③</span>
                    down=False,kernel_size=3, stride=2,
                    padding=1, output_padding=1)])
        self.last = nn.Conv2d(num_features * 1, img_channels,
            kernel_size=7, stride=1,
            padding=3, padding_mode="reflect")
        
    def forward(self, x):
        x = self.initial(x)
        for layer in self.down_blocks:
            x = layer(x)
        x = self.res_blocks(x)
        for layer in self.up_blocks:
            x = layer(x)
        return torch.tanh(self.last(x))                            <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Three Conv2d layers</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Nine residual blocks</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Two upsampling blocks</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Applies tanh activation on the output</p>
<p class="body">The generator network consists of several <code class="fm-code-in-text">Conv2d</code> layers, followed by nine residual blocks (which I’ll explain in detail later). After that, the network has two upsampling blocks that consist of a <code class="fm-code-in-text">ConvTranspose2d</code> layer, an <code class="fm-code-in-text">InstanceNorm2d</code> layer, and a <code class="fm-code-in-text">ReLU</code> activation. As we have done in previous chapters, we use the tanh activation function at the output layer, so the output pixels are all in the range of –1 to 1, the same as the images in the training set.<a id="idIndexMarker047"/><a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="idIndexMarker050"/><a id="marker-133"/></p>
<p class="body">The residual block in the generator is defined in the local module as follows:</p>
<pre class="programlisting">class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, 
                 down=True, use_act=True, **kwargs):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 
                      padding_mode="reflect", **kwargs)
            if down
            else nn.ConvTranspose2d(in_channels, 
                                    out_channels, **kwargs),
            nn.InstanceNorm2d(out_channels),
            nn.ReLU(inplace=True) if use_act else nn.Identity())
    def forward(self, x):
        return self.conv(x)
  
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.block = nn.Sequential(
            ConvBlock(channels,channels,kernel_size=3,padding=1),
            ConvBlock(channels,channels,
                      use_act=False, kernel_size=3, padding=1))
    def forward(self, x):
        return x + self.block(x)</pre>
<p class="body">A residual connection is a concept in deep learning, particularly in the design of deep neural networks. You’ll see it quite often later in this book. It’s a technique used to address the problem of vanishing gradients, which often occurs in very deep networks. In a residual block, which is the basic unit of a network with residual connections, the input is passed through a series of transformations (like convolution, activation, and batch or instance normalization) and then added back to the output of these transformations. Figure 6.4 provides a diagram of the architecture of the residual block defined previously. <a id="idIndexMarker051"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="479" src="../../OEBPS/Images/CH06_F04_Liu.png" width="425"/></p>
<p class="figurecaption">Figure 6.4 The architecture of a residual block. The input x is passed through a series of transformations (two sets of Conv2d layer and InstanceNorm2d layer and a ReLU activation). The input x is then added back to the output of these transformations, f(x). The output of the residual block is therefore x + f(x).</p>
</div>
<p class="body"><a id="marker-134"/>The transformations in each residual block are different. In this example, the input x is passed through two sets of <code class="fm-code-in-text">Conv2d</code> layer and <code class="fm-code-in-text">InstanceNorm2d</code> layer and a ReLU activation in between. The input x is then added back to the output of these transfo<a id="idTextAnchor008"/>rmations, f(x), to form the final output, x+f(x)—hence the name residual connection. <a id="idIndexMarker052"/><a id="idIndexMarker053"/><a id="idIndexMarker054"/></p>
<p class="body">Next, we create two instances of the <code class="fm-code-in-text">Generator()</code> class and call one of them <code class="fm-code-in-text">gen_A</code> and the other <code class="fm-code-in-text">gen_B</code>:<a id="idIndexMarker055"/></p>
<pre class="programlisting">from utils.ch06util import Generator
  
gen_A = Generator(img_channels=3, num_residuals=9).to(device)
gen_B = Generator(img_channels=3, num_residuals=9).to(device)
weights_init(gen_A)
weights_init(gen_B)</pre>
<p class="body">When training the model, we’ll use the mean absolute error (i.e., L1 loss) to measure the cycle consistency loss. We’ll use the mean squared error (i.e., L2 loss) to gauge the adversarial loss. L1 loss is often used if the data are noisy and have many outliers since it punishes extreme values less than the L2 loss. Therefore, we import the following loss functions:</p>
<pre class="programlisting">import torch.nn as nn
  
l1 = nn.L1Loss()
mse = nn.MSELoss()
g_scaler = torch.cuda.amp.GradScaler()
d_scaler = torch.cuda.amp.GradScaler()</pre>
<p class="body">Both L1 and L2 losses are calculated at the pixel level. The original image has a shape of <span class="times">(3, 256, 256)</span> and so is the fake image. To calculate the losses, we first calculate the difference (absolute value of this difference for L1 loss and the squared value of this difference for L2 loss) between the corresponding pixel values between two images at each of the <span class="times">3 <span class="cambria">×</span> 256 <span class="cambria">×</span> 256 = 196608</span> positions and average them over the positions.</p>
<p class="body">We’ll use PyTorch’s automatic mixed precision package <code class="fm-code-in-text">torch.cuda.amp</code> to speed up training. The default data type in PyTorch tensors is <code class="fm-code-in-text">float32</code>, a 32-bit floating-point number, which takes up twice as much memory as a 16-bit floating number, <code class="fm-code-in-text">float16</code>. Operations on the former are slower than those on the latter. There is a trade-off between precision and computational costs. Which data type to use depends on the task at hand. <code class="fm-code-in-text">torch.cuda.amp</code> provides an automatic mixed precision, where some operations use <code class="fm-code-in-text">float32</code> and others <code class="fm-code-in-text">float16</code>. Mixed precision tries to match each operation to its appropriate data type to speed up training. <a id="idIndexMarker056"/><a id="idIndexMarker057"/><a id="idIndexMarker058"/></p>
<p class="body">As we have done in chapter 4, we’ll use the Adam optimizer for both the discriminators and the generators:<a id="idIndexMarker059"/></p>
<pre class="programlisting">lr = 0.00001
opt_disc = torch.optim.Adam(list(disc_A.parameters()) + 
  list(disc_B.parameters()),lr=lr,betas=(0.5, 0.999))
opt_gen = torch.optim.Adam(list(gen_A.parameters()) + 
  list(gen_B.parameters()),lr=lr,betas=(0.5, 0.999))</pre>
<p class="body">Next, we’ll train the CycleGAN model by using images with black or blond hair.<a id="idIndexMarker060"/><a id="idIndexMarker061"/><a id="idIndexMarker062"/><a id="marker-135"/></p>
<h2 class="fm-head" id="heading_id_12">6.4 Using CycleGAN to translate between black and blond hair</h2>
<p class="body">Now that we have the training data and the CycleGAN model, we’ll train the model by using images with black or blond hair. As with all GAN models, we’ll discard the discriminators after training. We’ll use the two trained generators to convert black hair images to blond hair ones and convert blond hair images to black hair ones.<a id="idIndexMarker063"/><a id="idIndexMarker064"/><a id="idIndexMarker065"/><a id="idIndexMarker066"/></p>
<h3 class="fm-head1" id="heading_id_13">6.4.1 Training a CycleGAN to translate between black and blond hair</h3>
<p class="body">As we explained in chapter 4, we’ll use visual inspections to determine when to stop training. To that end, we create a function to test what the real images look like and what the corresponding generated images look like so that we can compare the two to visually inspect the effectiveness of the model. In the local module <code class="fm-code-in-text">ch06util</code>, we define a <code class="fm-code-in-text">test()</code> function:<a id="idIndexMarker067"/><a id="idIndexMarker068"/><a id="idIndexMarker069"/><a id="idIndexMarker070"/><a id="idIndexMarker071"/><a id="idIndexMarker072"/><a id="idIndexMarker073"/></p>
<pre class="programlisting">def test(i,A,B,fake_A,fake_B):
    save_image(A*0.5+0.5,f"files/A{i}.png")
    save_image(B*0.5+0.5,f"files/B{i}.png")               <span class="fm-combinumeral">①</span>
    save_image(fake_A*0.5+0.5,f"files/fakeA{i}.png")
    save_image(fake_B*0.5+0.5,f"files/fakeB{i}.png")      <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Real images in domains A and B, saved in a local folder</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The corresponding fake images in domains A and B, created by the generators in batch i</p>
<p class="body">We save four images after every 100 batches of training. We save real images and the corresponding fake images in the two domains in the local folder so we can periodically check the generated images and compare them with the real ones to assess the progress of training. We made the function general so that it can be applied to images from any two domains.</p>
<p class="body">Further, we define a <code class="fm-code-in-text">train_epoch()</code> function in the local module <code class="fm-code-in-text">ch06util</code> to train the discriminators and the generators for an epoch. The following listing highlights the code we use to train the two discriminators.<a id="idIndexMarker074"/><a id="marker-136"/><a id="idIndexMarker075"/></p>
<p class="fm-code-listing-caption">Listing 6.6 Training the two discriminators in CycleGAN</p>
<pre class="programlisting">def train_epoch(disc_A, disc_B, gen_A, gen_B, loader, opt_disc,
        opt_gen, l1, mse, d_scaler, g_scaler,device):
    loop = tqdm(loader, leave=True)
    for i, (A,B) in enumerate(loop):                       <span class="fm-combinumeral">①</span>
        A=A.to(device)
        B=B.to(device)
        with torch.cuda.amp.autocast():                    <span class="fm-combinumeral">②</span>
            fake_A = gen_A(B)
            D_A_real = disc_A(A)
            D_A_fake = disc_A(fake_A.detach())
            D_A_real_loss = mse(D_A_real, 
                                torch.ones_like(D_A_real))
            D_A_fake_loss = mse(D_A_fake,
                                torch.zeros_like(D_A_fake))
            D_A_loss = D_A_real_loss + D_A_fake_loss
            fake_B = gen_B(A)
            D_B_real = disc_B(B)
            D_B_fake = disc_B(fake_B.detach())
            D_B_real_loss = mse(D_B_real,
                                torch.ones_like(D_B_real))
            D_B_fake_loss = mse(D_B_fake,
                                torch.zeros_like(D_B_fake))
            D_B_loss = D_B_real_loss + D_B_fake_loss
            D_loss = (D_A_loss + D_B_loss) / 2             <span class="fm-combinumeral">③</span>
        opt_disc.zero_grad()
        d_scaler.scale(D_loss).backward()
        d_scaler.step(opt_disc)
        d_scaler.update()
        …</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Iterates through all pairs of images in the two domains</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses PyTorch automatic mixed precision package to speed up training</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The total loss for the two discriminators is t<a id="idTextAnchor009"/>he simple average of the adversarial losses to the two discriminators.</p>
<p class="body">We use the <code class="fm-code-in-text">detach()</code> method here to remove gradients in tensors <code class="fm-code-in-text">fake_A</code> and <code class="fm-code-in-text">fake_B</code> to reduce memory and speed up computations. The training for the two discriminators is similar to what we have done in chapter 4, with a couple of differences. First, instead of having just one discriminator, we have two discriminators here: one for images in domain A and one for images in domain B. The total loss for the two discriminators is the simple average of the adversarial losses of the two discriminators. Second, we use the PyTorch automatic mixed precision package to speed up training, reducing the training time by more than 50%. <a id="idIndexMarker076"/><a id="idIndexMarker077"/><a id="idIndexMarker078"/></p>
<p class="body">We simultaneously train the two generators in the same iteration. The following listing highlights the code we use to train the two generators.</p>
<p class="fm-code-listing-caption">Listing 6.7 Training the two generators in CycleGAN</p>
<pre class="programlisting">def train_epoch(disc_A, disc_B, gen_A, gen_B, loader, opt_disc,
        opt_gen, l1, mse, d_scaler, g_scaler,device):
        …
        with torch.cuda.amp.autocast():
            D_A_fake = disc_A(fake_A)
            D_B_fake = disc_B(fake_B)
            loss_G_A = mse(D_A_fake, torch.ones_like(D_A_fake))
            loss_G_B = mse(D_B_fake, torch.ones_like(D_B_fake))      <span class="fm-combinumeral">①</span>
            cycle_B = gen_B(fake_A)
            cycle_A = gen_A(fake_B)
            cycle_B_loss = l1(B, cycle_B)
            cycle_A_loss = l1(A, cycle_A)                            <span class="fm-combinumeral">②</span>
            G_loss=loss_G_A+loss_G_B+cycle_A_loss*10+cycle_B_loss*10 <span class="fm-combinumeral">③</span>
        opt_gen.zero_grad()
        g_scaler.scale(G_loss).backward()
        g_scaler.step(opt_gen)
        g_scaler.update()
        if i % 100 == 0:
            test(i,A,B,fake_A,fake_B)                                <span class="fm-combinumeral">④</span>
        loop.set_postfix(D_loss=D_loss.item(),G_loss=G_loss.item())</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Adversarial losses to the two generators</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Cycle consistency losses for the two generators</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The total loss for the two generators is the weighted sum of adversarial losses and cycle consistency losses.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Generates images for visual inspection after every 100 batches of training</p>
<p class="body"><a id="marker-137"/>The training for the two generators is different from what we have done in chapter 4 in two important ways. First, instead of having just one generator, we train two generators simultaneously here. Second, the total loss for the two generators is the weighted sum of adversarial losses and cycle consistency losses, and we weigh the latter 10 times more than the former loss. However, if you change the value 10 to other numbers such as 9 or 12, you’ll get similar results.</p>
<p class="body">The cycle consistency loss is the mean absolute error between the original image and the fake image that’s translated back to the original domain.</p>
<p class="body">Now that we have everything ready, we’ll start the training loop:</p>
<pre class="programlisting">from utils.ch06util import train_epoch
  
for epoch in range(1):
    train_epoch(disc_A, disc_B, gen_A, gen_B, loader, opt_disc,
    opt_gen, l1, mse, d_scaler, g_scaler, device)                   <span class="fm-combinumeral">①</span>
torch.save(gen_A.state_dict(), "files/gen_black.pth")
torch.save(gen_B.state_dict(), "files/gen_blond.pth")               <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Trains the CycleGAN for one epoch using the black and blond hair images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Saves the trained model weights</p>
<p class="body">The preceding training takes a couple of hours if you use GPU training. It may take a whole day otherwise. If you don’t have the computing resources to train the model, download the pretrained generators from my website: <a class="url" href="https://gattonweb.uky.edu/faculty/lium/ml/hair.zip">https://gattonweb.uky.edu/faculty/lium/ml/hair.zip</a>. Unzip the file and place the files <code class="fm-code-in-text">gen_black.pth</code> and <code class="fm-code-in-text">gen_blond.pth</code> in the folder /files/ on your computer. You’ll be able to convert between black hair images and blond hair ones in the next subsection.<a id="idIndexMarker079"/><a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="idIndexMarker082"/><a id="idIndexMarker083"/><a id="idIndexMarker084"/><a id="idIndexMarker085"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 6.1</p>
<p class="fm-sidebar-text">When training the CycleGAN model, we assume that domain A contains images with black hair and domain B contains images with blond hair. Modify the code in listing 6.2 so that domain A contains images with blond hair and domain B contains images with black hair.</p>
</div>
<h3 class="fm-head1" id="heading_id_14">6.4.2 Round-trip conversions of black hair images and blond hair images</h3>
<p class="body"><a id="marker-138"/>Due to the high quality and the abundant quantity of the training dataset, we have trained the CycleGAN with great success. We’ll not only convert between images with black hair and images with blond hair, but we’ll also conduct round-trip conversions. For example, we’ll convert images with black hair to images with blond hair and then convert them back to images with black hair. That way, we can compare the original images with the generated images in the same domain after a round trip and see the difference. <a id="idIndexMarker086"/><a id="idIndexMarker087"/><a id="idIndexMarker088"/><a id="idIndexMarker089"/><a id="idIndexMarker090"/></p>
<p class="body">The following listing performs conversions of images between the two domains as well as round-trip conversions of images in each domain.</p>
<p class="fm-code-listing-caption">Listing 6.8 Round-trip conversions of images with black or blond hair</p>
<pre class="programlisting">gen_A.load_state_dict(torch.load("files/gen_black.pth",
    map_location=device))
gen_B.load_state_dict(torch.load("files/gen_blond.pth",
    map_location=device))
i=1
for black,blond in loader:
    fake_blond=gen_B(black.to(device))
    save_image(black*0.5+0.5,f"files/black{i}.png")             <span class="fm-combinumeral">①</span>
    save_image(fake_blond*0.5+0.5,f"files/fakeblond{i}.png") 
    fake2black=gen_A(fake_blond)
    save_image(fake2black*0.5+0.5,
        f"files/fake2black{i}.png")                             <span class="fm-combinumeral">②</span>
    fake_black=gen_A(blond.to(device))
    save_image(blond*0.5+0.5,f"files/blond{i}.png")             <span class="fm-combinumeral">③</span>
    save_image(fake_black*0.5+0.5,f"files/fakeblack{i}.png")
    fake2blond=gen_B(fake_black)
    save_image(fake2blond*0.5+0.5,
        f"files/fake2blond{i}.png")                             <span class="fm-combinumeral">④</span>
    i=i+1
    if i&gt;10:
        break</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Original image with black hair</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A fake image with black hair after a round trip</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Original image with blond hair</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> A fake image with blond hair after a round trip</p>
<p class="body">We have saved six sets of images in your local folder /files/. The first set is the original images with black hair. The second set is the fake blond images produced by the trained blond hair generator: the images are saved as <code class="fm-code-in-text">fakeblond0.png</code>, <code class="fm-code-in-text">fakeblond1.png</code>, and so on. The third set is the fake images with black hair after a round trip: we feed the fake images we just created to the trained black hair generator to obtain fake images with black hair. They are saved as <code class="fm-code-in-text">fake2black0.png</code>, <code class="fm-code-in-text">fake2black1.png</code>, and so on. Figure 6.5 shows the three sets of images.<a id="marker-139"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="393" src="../../OEBPS/Images/CH06_F05_Liu.png" width="600"/></p>
<p class="figurecaption">Figure 6.5 A round-trip conversion of images with black hair. Images in the top row are the original images with black hair from the training set. Images in the middle row are the corresponding fake images with blond hair, produced by the trained blond hair generator. Images in the bottom row are fake images with black hair after a round trip: we feed the images in the middle row to the trained black hair generator to create fake images with black hair.</p>
</div>
<p class="body">There are three rows of images in figure 6.5. The top row displays original images with black hair from the training set. The middle row displays fake blond hair images produced by the trained blond hair. The bottom row contains fake black hair images after a round-trip conversion: the images look almost identical to the ones in the top row! Our trained CycleGAN model works extremely well.</p>
<p class="body">The fourth set of images in the local folder /files/ are the original images with blond hair. The fifth set is the fake image produced by the trained black hair generator. Finally, the sixth set contains fake images with blond hair after a round trip. Figure 6.6 compares these three sets of images.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="393" src="../../OEBPS/Images/CH06_F06_Liu.png" width="600"/></p>
<p class="figurecaption">Figure 6.6 A round-trip conversion of images with blond hair. Images in the top row are the original images with blond hair from the training set. Images in the middle row are the corresponding fake images with black hair, produced by the trained black hair generator. Images in the bottom row are fake images with blond hair after a round-trip conversion: we feed the images in the middle row to the trained blond hair generator to create fake images with blond hair.</p>
</div>
<p class="body">In figure 6.6, fake black hair images produced by the trained black hair generator are shown in the middle row: they have black hair on the same human faces as the top row. Fake blond hair images after a round trip are shown in the bottom row: they look almost identical to the original blond hair images in the top row.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 6.2</p>
<p class="fm-sidebar-text">The CycleGAN model is general and can be applied to any training dataset with two domains of images. Train the CycleGAN model using the eyeglasses images that you downloaded in chapter 5. Use images with glasses as domain A and images without glasses as domain B. Then use the trained CycleGAN to add and remove eyeglasses from images (i.e., translating images between the two domains). An example implementation and results are in the book’s GitHub repository.</p>
</div>
<p class="body">So far, we have focused on one type of generative model, GANs. In the next chapter, you’ll learn to use another type of generative model, variational autoencoders (VAEs), to generate high-resolution images. You’ll learn the advantages and disadvantages of VAEs compared to GANs. More importantly, you’ll learn the encoder-decoder architecture in VAEs. The architecture is widely used in generative models, including Transformers, which we’ll study later in the book. <a id="idIndexMarker091"/><a id="idIndexMarker092"/><a id="idIndexMarker093"/><a id="idIndexMarker094"/><a id="idIndexMarker095"/><a id="idIndexMarker096"/><a id="idIndexMarker097"/><a id="idIndexMarker098"/><a id="idIndexMarker099"/><a id="marker-140"/><a id="idIndexMarker100"/></p>
<h2 class="fm-head" id="heading_id_15">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">CycleGAN can translate images between two domains without paired examples. It consists of two discriminators and two generators. One generator converts images in domain A to domain B while the other generator converts images in domain B to domain A. The two discriminators classify if a given image is from a specific domain.</p>
</li>
<li class="fm-list-bullet">
<p class="list">CycleGAN uses a cycle consistency loss function to ensure the original image can be reconstructed from the transformed image, encouraging the preservation of key features.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A properly constructed CycleGAN model can be applied to any dataset with images from two domains. The same model can be trained with different datasets and be used to translate images in different domains.</p>
</li>
<li class="fm-list-bullet">
<p class="list">When we have abundant high-quality training data, the trained CycleGAN can convert images in one domain to another and convert them back to the original domain. The images after a round-trip conversion can potentially look almost identical to the original images.<a id="marker-141"/></p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">1</a></sup>  Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexie Efros, 2017, “Unpaired Image-to-Image Translation Using Cycle Consistent Adversarial Networks.” <a class="url" href="https://arxiv.org/abs/1703.10593">https://arxiv.org/abs/1703.10593</a>.</p>
</div></body></html>