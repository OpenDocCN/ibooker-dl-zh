<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="chp__hci"> <span class="chapter-title-numbering"><span class="num-string">8</span></span> <span class="title-text"> Designing solutions with large language models</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Using retrieval augmented generation to reduce errors</li> 
    <li class="readable-text" id="p3">How LLMs can supervise humans to mitigate automation bias</li> 
    <li class="readable-text" id="p4">Enabling classic machine learning tools with embeddings</li> 
    <li class="readable-text" id="p5">Ways to present LLMs that are mutually beneficial to companies and users</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>By now you should have a strong understanding of LLMs and their capabilities. They produce text that is very similar to human text because they are trained on hundreds of millions of human text documents. The content they produce is valuable but also subject to errors. And, as you know, you can mitigate these errors by incorporating domain knowledge or tools like parsers for computer source code.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Now you are ready to design a solution using an LLM. How do you consider everything we have discussed thus far and convert it into an effective implementation plan? This chapter will walk you through the process, trade-offs, and considerations in designing that plan. To do so, we will use a running example that we can all relate to: contacting tech support when help is needed.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>First, we will consider the obvious path: building a chatbot. Chatbots are the vehicle that introduced many people to LLMs because generally, they can do an excellent job of generating output interactively. We’ll evaluate the risks of deploying an LLM-powered chatbot in a customer service scenario. Through this discussion, you’ll see that using an LLM can increase risk compared to other options. However, a simple chatbot may be a valid option if the risks are sufficiently minimal.</p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>Next, we will explore ways to manage the risks by using application designs that improve how customers interact with the LLM. We’ll discuss how having a person check each output produced by an LLM is fraught with problems due to a phenomenon known as automation bias. We’ll discuss how automation bias can be somewhat counterintuitively avoided by having the LLM supervise the person instead. We’ll explore how an LLM’s embeddings, the semantic representation of text encoded as numbers, can be combined with classical machine learning algorithms to address this risk and handle tasks that an LLM can’t perform independently.</p> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>Finally, we’ll investigate how technology is presented to users and plays a vital role in establishing trust and conveying an understanding of its inner workings. We’ll discuss the area of “explainable AI,” where a machine learning algorithm produces output that describes or explains how it arrived at a specific output. Explainable AI is often the approach adopted to handle situations where people need to understand how an LLM works, but studies show that although explainability may shed some light on the inner workings of LLMs by describing the behavior of these models in human terms, it does not tend to help for its own sake. Instead, we’ll describe the benefits of focusing on transparency, aligning incentives with customers, and creating feedback cycles to design solutions that better meet the needs of both the companies that employ them and the customers that interact with them by providing accurate output and creating efficiencies in business processes.</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <h2 class=" readable-text-h2" id="just-make-a-chatbot"><span class="num-string browsable-reference-id">8.1</span> Just make a chatbot?</h2> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Unsurprisingly, many people are building chatbots using LLMs based on transformer architectures, the same technology that underpins ChatGPT. It’s an obvious and seemingly reasonable first step. ChatGPT’s fantastic ability to interact with people, adapt to conversations, and retrieve and present information demonstrates how well LLM technology supports customer interaction applications. With the advent and availability of LLMs, it would likely be short-sighted to attempt to implement a customer service agent using any other approach, such as using an expert system trained to use a decision tree of canned responses. When an unhappy customer has some technical problem, instead of searching an online Frequently Asked Questions (FAQ) document, sending an email into the black hole of a trouble ticket system, or calling a phone number with an automated interactive voice response system, they can start directly interacting with an AI-powered tool and make progress on getting their problems solved. This sounds wonderful on paper, and if you draw a little diagram like figure <a href="#fig__hci_chatbot">8.1</a>, it sure looks like we are simplifying life.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p13">  
   <img alt="figure" src="../Images/CH08_F01_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__hci_chatbot"><span class="num-string">Figure <span class="browsable-reference-id">8.1</span></span> When looking at the process diagram, it would seem like replacing FAQs, email tickets, and support numbers could be simplified and streamlined with an LLM-based chatbot. However, the folly of this view is that the process is incomplete. The potential errors and remediation processes required to ensure an LLM will perform accurately are hidden and create more complexity.</h5>
  </div> 
  <div class="readable-text" id="p14"> 
   <p>There are certainly cases where a chatbot is a good idea. But surprisingly, an online LLM-based chatbot that handles support probably is not at the top of the list of customer support tools for most companies because of the effort required to build a system that will be accurate and reliable in many cases and not create unexpected output when confronted with unexpected input. Ultimately, the decision to use an LLM to implement a customer support chatbot comes down to our ongoing discussion of the errors an LLM might make when generating customer responses. We know that LLMs are not error-free, and while machine learning is sometimes practical, the expense of those potential errors is the primary decision criterion when considering deploying this technology. Fundamentally, using an LLM potentially increases the cost of those errors. The bottom line is that in their current form, LLMs can provide incorrect answers, and the liability for these falls on the shoulders of the companies or individuals who deploy and maintain them.</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <p>Executives or product managers might consider the cost of errors in the context of a few classic business key performance indicators. For example, customer retention rates might decrease if they entrust support to chatbots. Perhaps the retention rate would be higher than if the customer relations functions were outsourced to a call center in another country. Indeed, these considerations are important to evaluate, and you should probably do a trial deployment to see what customers think before replacing your customer support function with an LLM wholesale.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p16"> 
   <p> <span class="print-book-callout-head">Note</span> We almost always recommend trial deployments of any machine learning system. The investing adage “Past performance is not a guarantee of future returns” is true of any AI. One way to do this is through <em>phantom deployments</em>, where you run your new AI system alongside the existing process for some weeks or months. You may choose to ignore its outcomes while the existing business processes are in place. This gives you time to observe the discrepancies between your current and new processes, identify and address problems, and determine whether the performance of the machine learning system degrades over time. </p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>Most critically, your LLM can give advice that causes harm to your users. Since an LLM is not a person who can be held legally liable for their actions, you and your company will be held liable instead. This has already happened with an airline that deployed a chatbot that gave errant policy statements. A court decided that the company had to abide by the policy incorrectly generated and shared by their chatbot [1].</p> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>We recommend always considering an <em>adversarial</em> mindset when deploying an LLM. Asking “What could a motivated bad actor do if they knew how this worked?” will help you identify and mitigate significant risks and is often the best way to determine whether your intended LLM application is a good or bad idea. For example, a car company integrated an LLM into their website to help sell cars and answer questions. After realizing this, it took less than a day for users to convince the website to sell them a car for just $1 [2].</p> 
  </div> 
  <div class="readable-text" id="p19"> 
   <p>If the potential cost or risk of errors is low, you can feel comfortable deploying an LLM chatbot if you so choose. But for the sake of this chapter, let us assume that this technical support agent we are hypothesizing is very important, and the mistakes it makes could cost the company a lot of money. The question now becomes: How do we design a solution that gives us benefits in productivity and efficiency yet limits users’ direct access to an LLM? If you are new to AI/ML and a chatbot is your primary exposure to the field, this might sound like a contradiction, but there are some easy, repeatable design patterns you can apply to do this.</p> 
  </div> 
  <div class="readable-text" id="p20"> 
   <h2 class=" readable-text-h2" id="automation-bias"><span class="num-string browsable-reference-id">8.2</span> Automation bias</h2> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>A common approach to addressing the risk of using LLMs for direct customer interactions is to have the LLM interact with support staff or technicians instead. This is often referred to as “human in the loop” because there’s a person who is reviewing the feedback loop between the LLM and the customer, providing a critical assessment of the automated system’s output, and intervening and adjusting the output when they detect an error. The technician will still be employed, but we will increase their efficiency by having the LLM generate an initial response to each question from a user and a technician curating those responses to ensure that they are accurate and relevant. If the LLM generates a potentially costly or incorrect response, our trusty technicians will intervene and reply with something more appropriate. In this context, it is ultimately up to the technician to choose the proper authoritative response.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>The clever reader who remembers our discussion about retrieval augmentedgeneration (RAG) from chapter <a href="../Text/chapter-5.html">5</a> might even identify ways to improve upon this idea. You’ll say, “Ah, we can put all our training manuals and documentation inside a database, and then we can use RAG so that the LLM can retrieve the most relevant information to a user’s question.”. This approach is outlined in figure <a href="#fig__human_in_loop_naive_rag">8.2</a>, which shows a process where a user’s questions are first sent to the LLM to focus output generation using a collection of known answers.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p23">  
   <img alt="figure" src="../Images/CH08_F02_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__human_in_loop_naive_rag"><span class="num-string">Figure <span class="browsable-reference-id">8.2</span></span> A naive approach toward implementing a “human in the loop” system that uses an LLM paired with a database of relevant information to produce output that is ultimately reviewed by and possibly corrected by a human worker</h5>
  </div> 
  <div class="readable-text" id="p24"> 
   <p>The RAG approach will likely mitigate a lot of risk, but it also has the potential to hit the pitfall of <em>automation bias</em>. Automation bias refers to the fact that people, in general, tend to pick automated or default choices presented by a system because it is easier than applying critical thinking to determine which choice is most appropriate to the situation at hand. If a system works well and does not need you to intervene often, it becomes incredibly challenging to remain hypervigilant and detect the occasional error. The paradox is that if the system is so inaccurate in its suggestions that you can maintain your vigilance, the chances are good that the system is slowing you down when compared to directly answering questions using no automation.</p> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>This is where trial or phantom deployments become incredibly important. If your system is so accurate that automation bias is the real source of risk, you have two options that do not require deviating from the “human in the loop” design:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p26">Add an “escape to a human” path to the pipeline</li> 
   <li class="readable-text" id="p27">Mitigate the risk of errors externally via process changes</li> 
  </ul> 
  <div class="readable-text" id="p28"> 
   <p>The first point is pretty straightforward. Eventually, a novel situation will occur that the LLM cannot answer. In this case, it would be best to provide a way for a customer to “escape” from an infinite loop with a computer to get to a higher tier of support. This could be a maximum conversation length measured in the number of messages exchanged or the amount of time spent chatting, an option to contact a human representative that appears based on multiple failed attempts to communicate, or other possible designs.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p29"> 
   <p> <span class="print-book-callout-head">Note</span> Suppose you are going to do the work to create an RLHF or SFT dataset to fine-tune your LLM to your situation as we discussed in chapter <a href="../Text/chapter-5.html">5</a>. In that case, you can even add training examples where the LLM’s expected response is “I’m sorry, this situation sounds more complex than what I can assist with; allow me to get a human to help.” </p> 
  </div> 
  <div class="readable-text" id="p30"> 
   <h3 class=" readable-text-h3" id="changing-the-process"><span class="num-string browsable-reference-id">8.2.1</span> Changing the process</h3> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>The second suggestion, changing the process, is not as difficult as it may sound. If one of your bosses has an MBA, they are (allegedly) trained to think in these terms. (One of the authors has an MBA, so it is OK for us to say that.) For example, interactions with the chatbot could include a caveat about any outcome requiring “a human’s final approval.” In this case, having the entire conversation reviewed by a person is far less of an automation bias risk than requiring someone to maintain constant vigilance throughout a continuous conversation. Ultimately, adversarial users know a human is going to check and so are demotivated from trying to game the system.</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>Depending on the context, preventing adversarial use of an LLM can be achieved by requiring the user to provide collateral to ensure they act in good faith. For example, you could take actions equivalent to putting a hold on the user’s credit card as a kind of insurance against bad-faith interactions. Such a hold would be released when the transaction is completed successfully. You could also limit how much of the process is automated, require authentication, or randomize how often people are routed to a human versus an AI so that it becomes unpredictable when a situation that could be exploited will arise.</p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>All of these actions will depend on your specific application, the risks, the tolerance of those risks, and the nature of your users. Some customers might be turned off by a credit hold and be upset. Or maybe you frame it as an optional method in which the user gets $2 off their bill if an AI system successfully helped them with their problem, presuming that it is less than what the old system would have cost per call. Either way, it is case by case and will depend on your creativity to manage the risk.</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <h3 class=" readable-text-h3" id="when-things-are-too-risky-for-autonomous-llms"><span class="num-string browsable-reference-id">8.2.2</span> When things are too risky for autonomous LLMs</h3> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>So now you have done a trial deployment, evaluated the risks and your users’ adversarial proclivities, and concluded that it is too risky for LLMs to provide the initial answers. How could an LLM still provide some level of efficiency?</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>An unintuitive approach is to have the LLM check the person rather than the person check the LLM. This may sound strange. Why would we let the LLM supervise if we cannot trust it to act alone? To consider this further, imagine you have an LLM system in this supervisory role, checking each response, as shown in figure <a href="#fig__HCI_LLM_supervise_human">8.3</a>.</p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>If the LLM and the person are correct, action will be taken, and the message will be relayed to the customer. It will be as if the user is chatting with the technician. But if the technician and the LLM disagree on the answer, we can prompt the technician to double-check their response before sending it to the user.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p38">  
   <img alt="figure" src="../Images/CH08_F03_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__HCI_LLM_supervise_human"><span class="num-string">Figure <span class="browsable-reference-id">8.3</span></span> Notice that the direction of the arrows in this diagram has changed from figure <a href="#fig__human_in_loop_naive_rag">8.2</a>. Everything goes to a human first, and we use LLMs to catch mistakes before they happen.</h5>
  </div> 
  <div class="readable-text" id="p39"> 
   <p>This double-check could be as simple as telling the technician, “Hey, this looks like it may be abnormal for a solution; please confirm before sending.” You could try having the LLM produce its own suggested alternative. Or you could keep the LLM out of the process and use it to notify a more experienced technician to join the process and assist. Regardless of how this is structured, the purpose is to signal that there may be a risk of a negative customer interaction, such as an incorrect answer. While this risk existed previously, we now have a chance to mitigate it.</p> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>Additionally, because we are considering human-initiated customer support errors, we are generally not taking on any new risk because a support representative acting alone could just as easily make a mistake. So if the LLM and human are both wrong simultaneously, you were already doomed to make that process error anyway. Such is life. Technically, we could argue that technicians could question their responses too much based on an LLM’s assessment of their interactions, thus reducing efficiency. Additionally, an overly sensitive LLM may ask technicians to double-check their work too often, which would cause alert fatigue that could lead to technicians ignoring the LLM suggestions entirely. If your use case is prone to these sorts of problems, that fact will be uncovered during trial deployments that provide context-specific feedback on how an LLM should be tuned to address this problem. The general caveat that applies to all machine learning is especially important here: always test; do not assume.</p> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>Employing an LLM to double-check human performance can reduce errors in the process as a whole. It may not seem like this approach makes anything faster because humans are still generating the initial response. However, this approach still creates opportunities for increased efficiency:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p42">It can reduce the conversation length by helping to catch errors and reach a solution faster.</li> 
   <li class="readable-text" id="p43">It can identify staff who need more training or information to answer customer questions or recognize when specific error situations occur.</li> 
   <li class="readable-text" id="p44">It may help avoid escalation to more costly levels of support or managers,reducing the frequency and cost of troublesome customers.</li> 
  </ul> 
  <div class="readable-text" id="p45"> 
   <h2 class=" readable-text-h2" id="using-more-than-llms-to-reduce-risk"><span class="num-string browsable-reference-id">8.3</span> Using more than LLMs to reduce risk</h2> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>Everything we have discussed has involved a “fight fire with fire” approach in which, although there are risks to using LLMs, we have considered different ways to use LLMs to mitigate those risks. While we’ve changed how we use the LLM, the LLM is still the primary component. Alternatively, we can consider using tools other than LLMs to address our design challenges. Other approaches in the scope of generative AI, such as text-to-speech and speech-to-text, can be used to build more accessible or simply convenient user experiences. For example, users with arthritis or low vision may greatly prefer a phone call over typing responses into a chatbot prompt window.</p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>If we think about our customer service problem and when LLMs work well, we will discover that the ingredients for a broader class of tools are also available. LLMs work best when there is repetition in scenarios where problems reoccur and formulaic solutions and responses can be given. LLMs are very flexible in recognizing broad patterns in the fuzzy nature of language. If the LLM can correctly interpret a user’s problem, and there is a known solution, it can potentially walk a user through that solution. This might sound much like an unsupervised chatbot, but the critical distinction is that in the cases where the LLM takes a subordinate role in the solution, the output was ultimately generated by customer support technicians, as described in figure <a href="#fig__HCI_LLM_supervise_human">8.3</a>.</p> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>This section will also discuss how we can use classic machine learning techniques, such as classification, to tackle existing problems. We can do this by using the knowl-edge within LLMs to enable machine learning techniques by producing embeddings of the user’s text.</p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <h3 class=" readable-text-h3" id="combining-llm-embeddings-with-other-tools"><span class="num-string browsable-reference-id">8.3.1</span> Combining LLM embeddings with other tools</h3> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>In chapter <a href="../Text/chapter-3.html">3</a>, we described how an LLM transforms tokens into embeddings, which are vectors that encode a semantic representation of the meaning of each token as a series of numbers. These vector embeddings are useful in other ways outside the context of LLM’s transformer architecture. While vector embeddings are essential for making the LLM operate, they are themselves an extraordinarily useful tool.</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>The semantic nature of the vectors produced by LLMs is important because hundreds of other practical machine learning algorithms operate on vector representations. LLMs are essentially a very powerful way of converting complex human language text into a form compatible with the rest of the machine learning field. Utilizing the vector outputs of LLMs with other algorithms has been such an extraordinarily useful strategy that practitioners will describe it as “creating embeddings.” The description comes from the idea that the LLM is taking one representation (human text) and embedding it into another representation (a mathematical vector). Because these numbers encode information about the original text, you can plot them like numbers and see that similar texts end up in similar locations on the plot, as shown in figure <a href="#fig__embeddings_are_useful">8.4</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p52">  
   <img alt="figure" src="../Images/CH08_F04_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__embeddings_are_useful"><span class="num-string">Figure <span class="browsable-reference-id">8.4</span></span> LLMs produce numeric vectors known as embeddings as an intrinsic part of their function-ing. The utility of these embeddings is dependent on the fact that these numbers only change a little bit when given similar text. The two example tests here will have similar embeddings, and thus, their plots look similar, even though they don’t share any of the same words. This is a powerful feature that was present in older machine learning techniques.</h5>
  </div> 
  <div class="readable-text" id="p53"> 
   <p>Let’s look at a quick description of four types of machine learning algorithms you can use once you have embeddings. We consider each type of machine learning to be particularly useful for most real-world use with LLMs; we will also note some popular algorithms you can find that are relatively reliable and easy to use. The critical takeaway is that if you break out of the mindset that only an LLM can solve a problem, a more extensive set of tools becomes available to you. This list is your starting map for some of those tools:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p54"><em>Clustering algorithms</em>—Grouping texts by similarity to each other that are distinct from the larger amount of text available (e.g., used for market segment analysis). Popular algorithms include k-means and HDBSCAN.</li> 
   <li class="readable-text" id="p55"><em>Outlier detection</em>—Finding texts that are dissimilar from essentially all other texts available (i.e., finding contrarian customers or novel problems). Popular algorithms include Isolation Forests and Local Outlier Factor (LoF).</li> 
   <li class="readable-text" id="p56"><em>Information visualization</em>—Creating a 2D plot of your data to allow visual inspection/exploration, especially when combined with interactive tools (i.e., data exploration). Popular algorithms include UMAP and PCA.</li> 
   <li class="readable-text" id="p57"><em>Classification and regression</em>—If you label your old texts with known outcomes (e.g., net promoter score rating), you can use classification (i.e., pick one of A, B, or C) or regression (i.e., predict a continuous number like 3.14 or 42) to predict what the score would be on a new text (i.e., data categorization and value prediction). Using embeddings as input for simple algorithms like logistic regression and linear regression works well for classification or regression, respectively.</li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p58"> 
   <p> <span class="print-book-callout-head">Note</span> Embeddings are not something new that was invented as a part of LLMs. An algorithm known as Word2Vec, which could embed single words, popularized embeddings as a go-to strategy for representing the meaning in text back in 2013. Despite this, LLMs tend to produce embeddings with greater utility than other older algorithms. However, an LLM is far more computationally demanding than older algorithms like Word2Vec. For this reason, you may want to use an older or faster algorithm for this task. The existence of generative AI methods in images, video, and speech means you can also use embeddings for domains such as images, video, and speech in addition to text. </p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <h3 class=" readable-text-h3" id="designing-a-solution-that-uses-embeddings"><span class="num-string browsable-reference-id">8.3.2</span> Designing a solution that uses embeddings</h3> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>Now that we have described the concept of embeddings and how they offer us more tools, let’s build an enhanced tech-support call center solution. We will continue to use LLMs for their text-generating capability and their embeddings and incorporate other machine learning techniques to enable the voice interaction that people are accustomed to, reduce wait times, and increase efficiency.</p> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>First, to support voice interaction, we will use speech-to-text to convert the words spoken by a user into text that is used as input into an LLM. It would be reasonable to think, “I’ve used some really horrible voice-controlled systems before,” and yes, you likely have. This is why adding a “bail-out” mechanism is essential to escape the automated system (e.g., max tries, times, or opt-out) for cases where the system can’t understand a user’s speech. In addition to speech-to-text, we will also use text-to-speech to give the LLM a way to convert the text output it produces into something that a user should be able to hear and understand.</p> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>Second, to reduce wait times, we can implement a system where, if a queue of callers has formed due to the number of support requests incoming, we will ask the customer to describe their problem so that they can be routed to the most appropriate analyst. Assuming that customers may have novel problems, we do not attempt to use the LLM to address their problems outright. Instead, we will use a customer’s problem description to call the LLM’s embedding API to produce a representation of their problem. Once we have that problem description embedding, we can use clustering to group the users in the queue. Users with similar problems can be assigned to the same team of analysts to help analysts solve problems faster. That alone is a win.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>We can use this problem grouping to gain further efficiency. Say an analyst has identified that a user has a common problem for which there is a consistent, predefined solution. Instead of relying on the LLM to dynamically generate a hypothetical solution, your human analyst can share the predefined solution that has already been vetted by real users. Additionally, you can push that solution out to the users who are waiting in the queue via the LLM. You will be able to inform the users: “An automated solution has been developed that we believe will solve your problem. While you wait, let us try to solve this with our automated AI.” This approach is summarized in figure <a href="#fig__HCI_MoreApproaches">8.5</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p64">  
   <img alt="figure" src="../Images/CH08_F05_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__HCI_MoreApproaches"><span class="num-string">Figure <span class="browsable-reference-id">8.5</span></span> This diagram describes our “better solution” to customer support requests, where customers describe their problem while waiting to talk to someone. The LLM uses an embedding representation of the problem to compare similar problems with known solutions. While the user waits, an automated system can provide information that may help them solve their problem without support personnel intervention. If that fails, there’s always the possibility to “bail out” and talk to a real person. The model used to generate the embeddings does not necessarily have to be the same as the LLM that walks the user through the solution.</h5>
  </div> 
  <div class="readable-text" id="p65"> 
   <p>It’s entirely possible to combine the solutions we have described so far. For example, the analyst-to-customer interaction loop in the top-right of figure <a href="#fig__HCI_MoreApproaches">8.5</a> could involve two people talking through the problem, or it could be the LLM-supervised validation solution we designed in figure <a href="#fig__HCI_LLM_supervise_human">8.3</a>. Depending on what problems need to be solved, there are many opportunities to extend these solutions now that we have embeddings. For example, if analysts saved information about how angry or upset a customer is, you could train a regression model to predict how angry a customer may be from their embedding. Then, you could distribute the angry customers evenly amongst analysts to avoid someone being overwhelmed or try to route angry customers away from new analysts who are still learning how to help customers solve their problems.</p> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>To be clear, we are not saying that all customer service tech support systems will be better if they use this approach. The goal is to show you that there are ways to build solutions with LLMs that work around their shortcomings, such as their tendency to hallucinate and their inability to incorporate new knowledge dynamically. In summary, we present two basic strategies:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p67">Use LLMs as a second set of eyes on what is happening. If the LLM agrees, all is good. If it disagrees, you perform a double-check that could be simple or complex, depending on the nature of the problem.</li> 
   <li class="readable-text" id="p68">Use embeddings to apply classic machine learning to the problem. Clustering (grouping similar things) and outlier detection (finding unique or unusual things) will be particularly useful for many real-world applications.</li> 
  </ul> 
  <div class="readable-text" id="p69"> 
   <p>We don’t solely rely on the LLM to create output at any point in these solutions because LLMs can generate incorrect or inappropriate output. However, we can still use the LLMs to reduce workload, errors, and the time to resolution by being careful in how we design the system as a whole.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <h2 class=" readable-text-h2" id="technology-presentation-matters"><span class="num-string browsable-reference-id">8.4</span> Technology presentation matters</h2> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Some of you may be incredulous after reading through this example of how we would design a tech support system that uses LLMs. We often hear folks who fully believe in LLM technology say, “If you have the LLM explain its reasoning, the user or analyst can figure out if it makes sense, and all of the problems related to hallucinations and errors will be solved.” We often receive similar requests to create “explainable AI” from those on the more skeptical end of the spectrum who are concerned about the errors LLMs produce and who don’t understand what is happening. Thus, there is a perception on both sides that explanations will provide the means to establish trust in the technology and believe that the LLM (or any machine learning algorithm) is working properly and effectively.</p> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>In this section, we want to discuss some points that support the notion that explain-ability is not the solution to these problems. Explainability is not the single solution that will help catch errors or make a system more transparent and trustworthy. The unfortunate truth is that our assumptions about how an LLM will work with people are often wrong and must be carefully evaluated. In fact, recent research has shown that when explainable AI techniques are employed by a system, people erroneously trust the AI to be correct solely based on the fact that an explanation is present, regardless of its accuracy. This is true even when the user could perform the task independently without an AI’s support, and the user has been taught about how the AI systems actually work [3]. The bottom line is that explanations can be harmful to the very goals that they attempt to advance.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p73"> 
    <h5 class=" callout-container-h5 readable-text-h5">Why use explainable AI at all?</h5> 
   </div> 
   <div class="readable-text" id="p74"> 
    <p> In our professional experience, many requests for explainable AI come from a place of fear or anxiety. Ideally, explainable AI would not be the way to calm such fears because it is counterproductive to the actual goals being solved. So why would anyone do any explainable AI of any form?</p> 
   </div> 
   <div class="readable-text" id="p75"> 
    <p>Two key things make explainable AI useful from a practical perspective:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p76">Answering the question, explainable <em>to whom</em>?</li> 
    <li class="readable-text" id="p77">Reaching explainable AI from the problem statement</li> 
   </ul> 
   <div class="readable-text" id="p78"> 
    <p>For example, a real-world problem statement may describe the need to develop a scientific understanding of a physical or chemical process. With this goal, a useful explanation from the algorithm may be to generate an equation that produces the answers rather than producing the answers directly. With the equation, a physicist or chemist can inspect it for logical consistency and use it as a starting point for further scientific exploration.</p> 
   </div> 
   <div class="readable-text" id="p79"> 
    <p>In this case, the solution is explainable only to someone with significant expertise, but that is the only person who needs the explanation. The explanation in the form of an equation also directly tackles the problem of scientific understanding rather than merely understanding the inner workings of the AI algorithm. We do not have any explanation of how the AI came up with the equation itself, and the equation is (hopefully) a logically consistent form that explains the physical or chemical process.</p> 
   </div> 
   <div class="readable-text" id="p80"> 
    <p>This example reflects the general situation in which we find explainable AI the most helpful: when it is used to aid a narrow and specific audience of potentially expert users in performing a very specific goal. For example, it is indeed common for data scientists to use explainable AI to help them figure out why a particular model is making a particular set of errors, even if the tools they use are not comprehensible to a nondata scientist audience.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>So if explainable AI is not a solution for building trust in an AI system or solution, what is? Unfortunately, there is no agreed-upon generic and rigorously evaluated way to build trust in AI. Our unoriginal suggestion is to focus on transparency, user evaluation, and the specifics of the use cases involved.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <h3 class=" readable-text-h3" id="how-can-you-be-transparent"><span class="num-string browsable-reference-id">8.4.1</span> How can you be transparent?</h3> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Transparency can be as simple as informing users about the AI system that is being used: Which model was it designed with and, at a high level, how was it modified? If the system is meant to mimic a specific person (“Get tutored by Albert A.I. Einstein”) or a type of credentialed person (“Ask Dr. GPT about that mole on your back”), has that person or similarly credentialed person consented to this or approved its efficacy? How can the consumer verify this information?</p> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Essentially, enumerating these kinds of reasonable questions and their answers that an auditor or skeptical user might want to know will put you far ahead of the average in making your system more transparent. These do not need to be presented in detail to every user, but having a way for users to discover this information is helpful. It not only helps sophisticated users understand what is happening but also helps set the expectations of users in general about what is and is not possible with a given system. Furthermore, it is essential to inform users when they are interacting with a system that is generating automated responses. There is a big difference between trying to pretend a human is in control and thus should be able to solve any reasonable challenge versus an automated AI that you inform the customer has limited capability.</p> 
  </div> 
  <div class="readable-text" id="p85"> 
   <h3 class=" readable-text-h3" id="aligning-incentives-with-users"><span class="num-string browsable-reference-id">8.4.2</span> Aligning incentives with users</h3> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>Part of transparency and system presentation involves aligning the incentives involved. This isn’t just a feel-good statement about management practices but a practical unit of advice. Remember from chapter <a href="../Text/chapter-4.html">4</a> that AI algorithms are greedy machines that optimize for what you ask, not what you intend. If you start building an LLM system where the incentives of the system are not well aligned with your broader goals, you risk overfitting to what you asked, not what both you and your users need.</p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>With aligned incentives (e.g., our example of “try out the LLM and get $2 off your bill if it worked”), you are much more likely to have a positive outcome. They also give you more ways to advertise using an LLM as a mechanism for providing value to your customers instead of coming across as the evil people trying to outsource all the jobs. Presenting and discussing the aligned incentives between a business and its customers and how you are using LLMs to achieve those goals describes what needs to be said without any need for hiding the information.</p> 
  </div> 
  <div class="readable-text" id="p88"> 
   <h3 class=" readable-text-h3" id="incorporating-feedback-cycles"><span class="num-string browsable-reference-id">8.4.3</span> Incorporating feedback cycles</h3> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>The world is not a static place. Things change, and what works today may not work tomorrow. This is one reason why you should have regular and continuous auditing of any automated AI/ML system: because they do not improve or adapt independently with experience.</p> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>But it will also help you catch potentially negative feedback cycles, something you want to try to think about in advance. Negative feedback cycles are not always possible to predict. To help you catch these, try to think about which users will or won’t find the most benefit with a new system and what happens as that repeats over and over again.</p> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>For example, we mentioned that speech-to-text and text-to-speech can be helpful for older customers or any hearing or movement-impaired customer. If we did not include such an option, we might alienate those customers over time, because every time they have a problem, they must use a physically difficult system.</p> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>Imagine you were a cell phone company that relied on family plans for some of your revenue. Your previously middle-aged customers who first bought your family plans are getting frustrated with your support system, so they move their entire family plan over to a new provider who puts in the extra work to ensure that the customer support process is accurate and efficient. Now you’re losing both your older and younger customers at once!</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>The point here is to think things through and train yourself to do these thought experiments. You will not catch every case, but you will improve. Regular auditing and testing then help you catch the failure cases, document them, and improve how you think about future situations and repeat problems.</p> 
  </div> 
  <div class="readable-text" id="p94"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p95">LLMs will have errors, and you first need to determine the risk and potential cost of errors to design an appropriate solution. If the risk and cost of errors are low, you can potentially use a normal chatbot-style LLM.</li> 
   <li class="readable-text" id="p96">It is possible to control the risk of using an LLM by changing how users interact with the system or shifting automation to a different part of the business process.</li> 
   <li class="readable-text" id="p97">Including a “human in the loop” to supervise an LLM creates automation bias risk, even when using techniques such as RAG to reduce the risk of errors.</li> 
   <li class="readable-text" id="p98">LLMs can convert text into embeddings, numeric representations where similar sentences receive similar values. This allows you to use additional machine learning approaches, including classic techniques like clustering and outlier detection.</li> 
   <li class="readable-text" id="p99">While LLMs can explain their decisions, their explanations are often ineffective because people become dependent on them. Instead, focus on producing explanations to satisfy a specific need or use case rather than generic “needing to explain.”</li> 
   <li class="readable-text" id="p100">Design your system’s incentives to align with your user’s incentives. This is both a good way to avoid mistakes from an LLM optimizing for what you asked instead of what you intended and a good way to communicate and present your LLM to users.</li> 
  </ul>
 </body></html>