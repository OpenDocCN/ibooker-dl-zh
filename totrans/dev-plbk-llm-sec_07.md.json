["```py\nimport re\n\ndef contains_ssn(input_string):\n    # Define a regular expression pattern for a U.S. Social Security Number\n    ssn_pattern = r'\\b\\d{3}-\\d{2}-\\d{4}\\b'\n\n    # Search for the pattern in the input string\n    match = re.search(ssn_pattern, input_string)\n\n    # Check if a match was found\n    if match:\n        print(\"Found a Social Security Number: {match.group(0)}\")\n        return True\n    else:\n        print(\"No Social Security Number found.\")\n        return False\n\n# Test the function\ncontains_ssn(\"My Social Security Number is 123-45-6789.\")\ncontains_ssn(\"No number here!\")\n```", "```py\nimport openai\n\ndef check_toxicity(text):\n  \"\"\"\n  Checks the toxicity of a text using the OpenAI Moderation API.\n\n  Args:\n    text: The text to check for toxicity.\n\n  Returns:\n    A toxicity score between 0 and 1, where a higher score indicates a \n    higher probability of the text being toxic.\n  \"\"\"\n\n  response = openai.Moderation.create(input=text)\n  toxicity_score = response[\"results\"][0][\"confidence\"]\n  return toxicity_score\n\n# Test the function\ncheck_toxicity(\"You are stupid.\")\n```", "```py\nimport openai\nimport json\n\n# Initialize the OpenAI API client\nopenai.api_key = \"your_openai_api_key_here\"\n\ndef check_toxicity(text):\n\n  response = openai.Moderation.create(input=text)\n  toxicity_score = response[\"results\"][0][\"confidence\"]\n  return toxicity_score\n\ndef check_for_PII(text):\n\n  ssn_pattern = r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\"\n  return bool(re.search(ssn_pattern, text))\n\ndef get_LLM_response(prompt):\n\n  model_engine = \"text-davinci-002\"  # You can use other engines\n  response = openai.Completion.create(\n      engine=model_engine,\n      prompt=prompt,\n      max_tokens=100  # Limiting to 100 tokens for this example\n  )\n\n  return response.choices[0].text.strip()\n\ndef log_results(prompt, llm_output, is_safe):\n\n  with open(\"llm_safety_log.txt\", \"a\") as log_file:\n    log_file.write(f\"Prompt: {prompt}\\n\")\n    log_file.write(f\"LLM Output: {llm_output}\\n\")\n    log_file.write(f\"Is Safe: {is_safe}\\n\")\n    log_file.write(\"=\" * 50 + \"\\n\")\n\nif __name__ == \"__main__\":\n  prompt = \"Tell me your thoughts on universal healthcare.\"\n  llm_output = get_LLM_response(prompt)\n\n  toxicity_level = check_toxicity(llm_output)\n  contains_PII = check_for_PII(llm_output)\n\n  is_safe = True\n\n  if toxicity_level > 0.7 or contains_PII:\n    print(\"Warning: The output is not safe to return to the user.\")\n    is_safe = False\n  else:\n    print(\"The output is safe to return to the user.\")\n\n  log_results(prompt, llm_output, is_safe)\n```", "```py\nimport html\n\ndef sanitize_output(text):\n    return html.escape(text)\n```", "```py\n    if toxicity_level > 0.7 or contains_PII:\n        print(\"Warning: The output is not safe to return to the user.\")\n        is_safe = False\n    else:\n        print(\"The output is safe to return to the user.\")\n        llm_output = sanitize_output(llm_output)\n\n    log_results(prompt, llm_output, is_safe)\n```"]