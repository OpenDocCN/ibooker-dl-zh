<html><head></head><body><section data-pdf-bookmark="Chapter 4. AI System Assessment and Tailoring AI Engineering for Different Risk Levels" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657">
<h1><span class="label">Chapter 4. </span>AI System Assessment and Tailoring AI Engineering for Different Risk Levels</h1>

<p>In this chapter, the main learning<a contenteditable="false" data-primary="AI system assessment" data-type="indexterm" id="id511"/> objective is to understand how to practically classify different AI system risk levels and how to design AI engineering processes for each risk level (see <a data-type="xref" href="#chapter_4_figure_1_1748539919001611">Figure 4-1</a> for a visual of the steps to take to move toward compliance with the EU AI Act). We’ll explore the EU AI Act’s risk classification framework and the obligation mapping phase. For high-risk and limited-risk AI systems, careful planning of data governance, AI governance, and MLOps processes is essential to ensure compliance with the Act.</p>

<figure><div class="figure" id="chapter_4_figure_1_1748539919001611"><img src="assets/taie_0401.png"/>
<h6><span class="label">Figure 4-1. </span>This chapter focuses on creating the AI system landscape in an organization and classifying the risk types. See <a data-type="xref" href="ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819">Chapter 1</a> for an explanation of the end-to-end process steps toward EU AI Act compliance.</h6>
</div></figure>

<p>This chapter will help you answer the following questions:</p>

<ul>
	<li>
	<p>How many AI systems are currently in place or intended to be put into <span class="keep-together">production</span>?</p>
	</li>
	<li>
	<p>What risk categories do those AI systems belong to?</p>
	</li>
	<li>
	<p>How can clarity be established about the role of the provider or deployer of each AI model?</p>
	</li>
</ul>

<p>In discussions about the EU AI Act, people frequently use terms such as “AI compliance,” “AI governance,” and “risk management.” While closely related and complementary, these are distinct concepts. Let’s begin by clarifying what each one means.</p>

<section data-pdf-bookmark="AI Compliance, Governance, and Risk Management" data-type="sect1"><div class="sect1" id="chapter_4_ai_compliance_governance_and_risk_management_1748539919035019">
<h1>AI Compliance, Governance, and Risk Management</h1>

<p>We explored AI compliance<a contenteditable="false" data-primary="AI compliance, governance, and risk management (comparison)" data-type="indexterm" id="aicgrm-comp"/> and governance in the previous chapters. As a reminder, <em>AI compliance</em> focuses on adhering to legal, regulatory, and policy standards, ensuring ethical behavior, and promoting transparency through rigorous processes, metrics, and stakeholder engagement. <em>AI governance </em>establishes a framework for ethical and responsible AI development, emphasizing transparency, stakeholder engagement, and balancing innovation with ethical considerations. <em>AI risk management<a contenteditable="false" data-primary="AI risks" data-type="indexterm" id="ai-risk-2"/></em> involves<a contenteditable="false" data-primary="AI risk management" data-type="indexterm" id="ai-risk-man-1"/> identifying and addressing risks associated with AI systems, focusing on robust assessment practices, effective mitigation strategies, and promoting a culture of awareness and proactive action.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="chapter_4_ai_risk_1748539919035098">
<h1>AI Risk</h1>

<p><a href="https://oreil.ly/Pc0cW">NIST</a> describes risk in the context of AI systems as a function of the negative impact, or magnitude of harm, that would arise if a certain circumstance or event occurs, and the likelihood of its occurrence. That negative impact or harm can be experienced by individuals, groups, communities, organizations, society, the environment, or the planet.</p>

<p>Here are some well-known examples that illustrate the diverse range of risks associated with AI, spanning ethical, social, economic, and technical domains:</p>

<dl>
	<dt>Algorithmic bias</dt>
	<dd>
	<p>AI systems can perpetuate<a contenteditable="false" data-primary="AI risks" data-secondary="algorithmic bias" data-type="indexterm" id="id512"/> or strengthen existing societal biases, leading to unfair treatment of certain groups. For example, facial recognition systems have shown higher error rates for women and people of color.</p>
	</dd>
	<dt class="pagebreak-before">Privacy violations and social manipulation</dt>
	<dd>
	<p>AI systems that process large<a contenteditable="false" data-primary="AI risks" data-secondary="privacy violations and social manipulation" data-type="indexterm" id="id513"/> amounts of personal data can pose risks to individual privacy. For instance, AI algorithms used by social media platforms can be exploited to influence user behavior and opinions, potentially impacting democratic processes. The Cambridge Analytica scandal<a contenteditable="false" data-primary="Cambridge Analytica scandal" data-type="indexterm" id="id514"/>, where personal data from millions of Facebook users was harvested and used for targeted political advertising, is a prominent example.</p>
	</dd>
	<dt>Deepfakes</dt>
	<dd>
	<p>AI-generated videos<a contenteditable="false" data-primary="AI risks" data-secondary="deepfakes" data-type="indexterm" id="id515"/><a contenteditable="false" data-primary="deepfakes" data-type="indexterm" id="id516"/> or audio recordings can be used to spread misinformation or manipulate public opinion.</p>
	</dd>
	<dt>Autonomous weapons</dt>
	<dd>
	<p>The development of AI-powered weapons<a contenteditable="false" data-primary="AI risks" data-secondary="autonomous weapons" data-type="indexterm" id="id517"/> that can operate without human control raises ethical concerns and the risk of unintended harm.</p>
	</dd>
	<dt>AI-enabled cyberattacks</dt>
	<dd>
	<p>AI can be used to enhance the scale of cyberattacks<a contenteditable="false" data-primary="AI risks" data-secondary="cyberattacks" data-type="indexterm" id="id518"/>, making them more difficult to detect and defend against.</p>
	</dd>
	<dt>Lack of transparency</dt>
	<dd>
	<p>The opaque nature of some AI algorithms<a contenteditable="false" data-primary="AI risks" data-secondary="lack of transparency" data-type="indexterm" id="id519"/> and models can make it difficult to understand or explain their decision-making processes, which is problematic in high-stakes domains like healthcare and criminal justice.</p>
	</dd>
	<dt>Environmental impact</dt>
	<dd>
	<p>The energy consumed to train and operate large AI models<a contenteditable="false" data-primary="AI risks" data-secondary="environmental impact" data-type="indexterm" id="id520"/> contributes to carbon emissions and raises environmental concerns.</p>
	</dd>
	<dt>Safety risks in critical systems</dt>
	<dd>
	<p>AI failures<a contenteditable="false" data-primary="AI risks" data-secondary="safety risks in critical systems" data-type="indexterm" id="id521"/> in critical infrastructure or systems—such as healthcare, power grids, or transportation—can have severe consequences.</p>
	</dd>
	<dt>Intellectual property issues</dt>
	<dd>
	<p>AI-generated content raises<a contenteditable="false" data-primary="AI risks" data-secondary="intellectual property issues" data-type="indexterm" id="id522"/> questions about copyright and ownership, potentially infringing on intellectual property rights.</p>
	</dd>
	<dt>Autonomous vehicle accidents</dt>
	<dd>
	<p>While designed to improve safety<a contenteditable="false" data-primary="AI risks" data-secondary="autonomous vehicle accidents" data-type="indexterm" id="id523"/>, self-driving cars have been involved in accidents, highlighting the challenges of integrating AI into complex real-world <span class="keep-together">environments</span>.</p>
	</dd>
</dl>
</div></aside>

<p><a data-type="xref" href="#table-4-1">Table 4-1</a> compares and contrasts AI compliance, governance, and risk management frameworks along a number of dimensions, including objectives, people and processes involved, engineering considerations, cultural practices, and more.</p>

<table class="striped" id="table-4-1">
	<caption><span class="label">Table 4-1. </span>Comparison of AI compliance, governance, and risk management in the context of the EU AI Act</caption>
	<thead>
		<tr>
			<th> </th>
			<th>AI compliance</th>
			<th>AI governance</th>
			<th>AI risk management</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading">Objective</td>
			<td>
			<p>Ensure AI systems adhere to the requirements and obligations set forth in the EU AI Act, including risk management, data governance, human oversight, transparency, accuracy, robustness, and cybersecurity.</p>
			</td>
			<td>
			<p>Establish a framework for overseeing AI development, deployment, and use in alignment with the EU AI Act, organizational values, and ethical principles.</p>
			</td>
			<td>
			<p>Identify, assess, and mitigate risks associated with AI systems throughout their lifecycle, in compliance with the EU AI Act’s risk-based approach.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading">People</td>
			<td>
			<p>Compliance officers, legal teams, data protection officers, AI ethics committees</p>
			</td>
			<td>
			<p>AI ethics board, chief AI officer, AI ethics officer, cross-functional AI steering committee, AI project managers</p>
			</td>
			<td>
			<p>Risk management teams, data scientists, AI engineers, cybersecurity experts</p>
			</td>
		</tr>
		<tr>
			<td class="subheading">Processes</td>
			<td>
			<ul>
				<li>
				<p>Implement risk assessment procedures to categorize AI systems.</p>
				</li>
				<li>
				<p>Establish documentation practices for high-risk AI systems.</p>
				</li>
				<li>
				<p>Develop incident reporting and management protocols.</p>
				</li>
				<li>
				<p>Create processes for conformity assessments and CE marking.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Develop AI policies and guidelines aligned with the EU AI Act.</p>
				</li>
				<li>
				<p>Implement review and approval processes for high-risk AI systems.</p>
				</li>
				<li>
				<p>Establish mechanisms for ongoing monitoring and auditing of AI systems.</p>
				</li>
				<li>
				<p>Create escalation procedures for AI-related issues and decisions.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Perform regular AI risk assessments.</p>
				</li>
				<li>
				<p>Conduct scenario planning for AI failures.</p>
				</li>
				<li>
				<p>Ensure continuous monitoring of AI system performance.</p>
				</li>
				<li>
				<p>Create incident response and recovery plans.</p>
				</li>
				<li>
				<p>Define processes for regular risk reporting and review.</p>
				</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading">Metrics and KPIs</td>
			<td>
			<ul>
				<li>Number of compliance violations</li>
				<li>Time to address compliance issues</li>
				<li>Percentage of AI projects passing compliance checks</li>
				<li>Percentage of AI systems with proper documentation</li>
				<li>Frequency of compliance audits</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>Number of AI projects reviewed and approved by the governance committee</li>
				<li>Percentage of AI systems adhering to established governance policies</li>
				<li>Frequency and outcomes of AI system audits</li>
				<li>Number of ethical reviews conducted</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>Number of risks identified, assessed, and mitigated</li>
				<li>Reduction in AI-related incidents over time</li>
				<li>Time to resolve identified risks</li>
				<li>Percentage of high-risk AI systems with completed risk assessments</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading">Stakeholder engagement</td>
			<td>
			<ul>
				<li>Internal: Collaboration between legal, technical, and operational teams</li>
				<li>External: Interaction with regulators, compliance bodies, and industry standards organizations</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>Internal: Cross-department collaboration, inclusive decision-making processes</li>
				<li>External: Engagement with external advisors, industry forums, and regulatory bodies</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>Risk communication to executive leadership</li>
				<li>Collaboration with customers to align on risk tolerances</li>
				<li>Engagement with insurers on AI-related coverage</li>
				<li>Collaboration with industry partners on risk management best practices</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading">Engineering practices</td>
			<td>
			<ul>
				<li>
				<p>Implement privacy-by-design and security-by-design principles.</p>
				</li>
				<li>
				<p>Develop testing procedures for bias detection and mitigation.</p>
				</li>
				<li>
				<p>Establish version control for data, code, and AI models.</p>
				</li>
				<li>
				<p>Conduct regular code reviews, ensuring adherence to ethical guidelines and traceability.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Implement standardized AI development methodologies.</p>
				</li>
				<li>
				<p>Establish governance checkpoints at key stages of the AI lifecycle.</p>
				</li>
				<li>
				<p>Integrate code review practices that specifically include governance considerations.</p>
				</li>
				<li>
				<p>Incorporate governance requirements into DevOps practices.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Integrate risk considerations into AI architecture design.</p>
				</li>
				<li>
				<p>Implement robust testing procedures for AI systems, including adversarial testing.</p>
				</li>
				<li>
				<p>Develop failsafe mechanisms and ensure graceful degradation for high-risk AI systems.</p>
				</li>
				<li>
				<p>Establish monitoring systems for early risk detection in deployed AI systems.</p>
				</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading">Technology and infrastructure</td>
			<td>
			<ul>
				<li>Compliance management software</li>
				<li>Automated auditing tools</li>
				<li>Explainable AI</li>
				<li>Data protection technologies</li>
				<li>Secure data storage</li>
				<li>Robust logging systems</li>
				<li>Compliance tracking systems</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>AI model registries</li>
				<li>Centralized AI governance platform</li>
				<li>Data lineage tracking system</li>
				<li>AI performance monitoring tools</li>
				<li>Ethical AI framework</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>AI risk monitoring and alerting systems</li>
				<li>Simulation environments for risk testing</li>
				<li>Automated risk assessment tools</li>
				<li>Secure sandboxing for AI testing</li>
				<li>Secure data environments</li>
				<li>Resilient system architectures</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading">Training and education</td>
			<td>
			<ul>
				<li>Regular compliance training for AI developers</li>
				<li>Workshops on emerging AI regulations</li>
				<li>Certification programs for AI compliance</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>Governance training for all employees involved in AI projects</li>
				<li>Specialized courses on AI ethics and responsible AI development</li>
				<li>Workshops on interpreting and applying EU AI Act requirements</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>Risk management training specific to AI technologies</li>
				<li>Educating developers on identifying and mitigating AI-specific risks</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading">Change management</td>
			<td>
			<ul>
				<li>
				<p>Develop a roadmap for transitioning existing AI systems to comply with the Act.</p>
				</li>
				<li>
				<p>Establish clear communication channels about compliance updates.</p>
				</li>
				<li>
				<p>Create feedback mechanisms to continuously improve compliance processes.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Develop a communication strategy for rolling out new governance policies.</p>
				</li>
				<li>
				<p>Create transition plans for adapting existing AI systems to new governance requirements.</p>
				</li>
				<li>
				<p>Establish feedback loops to continuously refine governance practices.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Develop strategies for integrating risk management into existing AI workflows.</p>
				</li>
				<li>
				<p>Make regular updates to risk management policies.</p>
				</li>
				<li>
				<p>Communicate with stakeholders about risk status.</p>
				</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading">Cultural considerations</td>
			<td>
			<ul>
				<li>
				<p>Foster a culture of ethical AI development and responsible innovation.</p>
				</li>
				<li>
				<p>Promote transparency and accountability in AI decision-making processes.</p>
				</li>
				<li>
				<p>Facilitate cross-functional collaboration to address compliance challenges.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Foster a culture of responsible AI innovation and ethical decision making.</p>
				</li>
				<li>
				<p>Encourage open discussions about AI risks and governance challenges.</p>
				</li>
				<li>
				<p>Recognize and reward adherence to AI governance principles.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Foster a culture of proactive risk identification and mitigation.</p>
				</li>
				<li>
				<p>Enable open reporting of potential risks without fear (create a “safe space”).</p>
				</li>
				<li>
				<p>Promote a balanced approach to innovation and risk management.</p>
				</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>

<p>AI compliance, governance, and risk<a contenteditable="false" data-primary="AI risks" data-startref="ai-risk-2" data-type="indexterm" id="id524"/> management<a contenteditable="false" data-primary="AI compliance, governance, and risk management (comparison)" data-startref="aicgrm-comp" data-type="indexterm" id="id525"/> are <a contenteditable="false" data-primary="AI risk management" data-startref="ai-risk-man-1" data-type="indexterm" id="id526"/>closely interrelated. Compliance is primarily reactive in nature, establishing the baseline requirements for governance, while effective governance is proactive, guiding the responsible development and use of AI. Risk management complements and informs compliance and governance efforts by identifying and addressing potential threats in a timely manner. When implemented holistically and embedded into daily operations, these three pillars provide essential support for compliance with the EU AI Act.</p>
</div></section>

<section data-pdf-bookmark="Creating an AI System Inventory" data-type="sect1"><div class="sect1" id="chapter_4_creating_an_ai_system_inventory_1748539919035157">
<h1>Creating an AI System Inventory</h1>

<p>To navigate the EU AI Act and understand its impact on your organization, you should first get an overview of your existing AI systems and potential AI use cases to assess whether they are subject to the legislation.</p>

<p>Note that AI use cases developed as internal research projects are generally excluded from compliance obligations.</p>

<p>Later in this chapter, I will outline the different maturity levels of AI projects and specify when the EU AI Act’s requirements must be fulfilled. For now, please note that the Act impacts AI systems based on their risk level and intended use, rather than their development stage.</p>

<p>As described in <a data-type="xref" href="ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819">Chapter 1</a>, the EU AI Act applies to:</p>

<ul>
	<li>
	<p>Providers placing AI systems on the EU market or putting them into service</p>
	</li>
	<li>
	<p>Users of AI systems located within the EU</p>
	</li>
	<li>
	<p>Providers and users outside the EU, if the AI system’s output is used within the EU</p>
	</li>
</ul>

<p class="pagebreak-before">To help you in your inventory efforts, I recommend creating<a contenteditable="false" data-primary="AI system inventory" data-type="indexterm" id="ai-sys-inventory-1"/> an <em>AI system inventory</em> by following these steps:</p>

<ol>
	<li>
	<p>Identify AI applications across all departments.</p>
	</li>
	<li>
	<p>Include both custom-developed and third-party AI solutions.</p>
	</li>
	<li>
	<p>Document key details like department, purpose, data used, deployment status, and risk category.</p>
	</li>
</ol>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>As a reminder, Article 3 of the EU AI Act defines an AI system as “a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.”</p>
</div>

<p>You’ll want to create a comprehensive inventory template to catalog all AI systems used or developed within your organization, capturing key attributes for each system. <a data-type="xref" href="#chapter_4_figure_2_1748539919001650">Figure 4-2</a> shows an example of what this might look like.</p>

<figure><div class="figure" id="chapter_4_figure_2_1748539919001650"><img src="assets/taie_0402.png"/>
<h6><span class="label">Figure 4-2. </span>An AI system catalog entry should contain basic identifying information, technical details, a risk assessment, and additional relevant notes</h6>
</div></figure>

<p>Depending on your organization’s needs, you can create this template using tools like Microsoft Excel, Google Sheets (as shown in <a data-type="xref" href="#chapter_4_figure_3_1748539919001677">Figure 4-3</a>), Airtable, or more sophisticated database management systems. Knowledge management platforms such as Confluence or Notion can also be helpful for creating a catalog of AI systems, as can specialized AI governance tools like watsonx.ai, Dataiku, or Domino Data Lab.</p>

<figure><div class="figure" id="chapter_4_figure_3_1748539919001677"><img src="assets/taie_0403.png"/>
<h6><span class="label">Figure 4-3. </span>An example of a catalog of existing AI systems for a hypothetical ecommerce company</h6>
</div></figure>

<p>After creating a complete and detailed inventory of the organization’s AI systems, the next question you’ll want to answer is whether the EU AI Act is applicable to those systems<a contenteditable="false" data-primary="AI system inventory" data-startref="ai-sys-inventory-1" data-type="indexterm" id="id527"/>.</p>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="Applicability of the EU AI Act" data-type="sect1"><div class="sect1" id="chapter_4_applicability_of_the_eu_ai_act_1748539919035216">
<h1 class="less_space">Applicability of the EU AI Act</h1>

<p>The<a contenteditable="false" data-primary="EU AI Act" data-secondary="applicability" data-type="indexterm" id="eu-ai-appl-1"/> EU AI Act’s applicability to a given AI system depends on several factors, including whether the system falls within the Act’s the scope and definition of AI, whether it has already been placed on the market or put into service, and the timing of the Act’s entry into force and application.</p>

<div data-type="tip"><h6>Tip</h6>
<p>The relevant articles of the EU AI Act are:</p>

<ul>
	<li>
	<p><a href="https://oreil.ly/eBlrf">Article 2: Scope</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/ogId3">Article 3: Definitions</a> and <a href="https://oreil.ly/5h0Su">Recital 12</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/d8_kg">Article 111: AI Systems Already Placed on the Market or put into Service and General-Purpose AI Models Already Placed on the Market</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/s_NYt">Article 113: Entry into Force and Application</a></p>
	</li>
</ul>
</div>

<p>Complying with the EU AI Act involves a complex process flow with many conditions, as pictured in <a data-type="xref" href="#chapter_4_figure_4_1748539919001709">Figure 4-4</a>. After clarifying whether the Act is applicable to a particular system, you’ll need to establish your organization’s role (such as provider or deployer) and classify the system’s risk level. These two factors determine the specific set of obligations your organization must fulfill.</p>

<figure><div class="figure" id="chapter_4_figure_4_1748539919001709"><img src="assets/taie_0404.png"/>
<h6><span class="label">Figure 4-4. </span>The process of determining the EU AI Act’s applicability to AI systems and their risk classification (adapted from <a href="https://oreil.ly/LUZWC"><em class="hyperlink">https://oreil.ly/LUZWC</em></a> [CC BY 4.0], courtesy of the appliedAI Institute for Europe gGmbH)</h6>
</div></figure>

<p>We’ll look at the processes for determining applicability, risk level, and organizational role in more detail later in this chapter. First, let’s examine the use cases that are <span class="keep-together"><em>prohibited</em></span> by the EU AI Act<a contenteditable="false" data-primary="EU AI Act" data-secondary="applicability" data-startref="eu-ai-appl-1" data-type="indexterm" id="id528"/>.</p>
</div></section>

<section data-pdf-bookmark="Unacceptable Risk—Prohibited AI Practices" data-type="sect1"><div class="sect1" id="chapter_4_unacceptable_risk_prohibited_ai_practices_1748539919035281">
<h1>Unacceptable Risk—Prohibited AI Practices</h1>

<p>Human <a contenteditable="false" data-primary="prohibited AI practices" data-type="indexterm" id="prohib-ai-1"/>values such as privacy<a contenteditable="false" data-primary="unacceptable risk" data-seealso="prohibited AI practices" data-type="indexterm" id="id529"/>, integrity, social justice, transparency, and diversity are at the core of EU law, and with the growing adoption of AI technologies, they are becoming increasingly important in software and AI development. To ensure the safety of AI, it’s more effective to regulate its applications rather than the technology itself.</p>

<p>Technology itself is neither inherently good nor bad, but it can be used in benevolent or harmful ways. To underscore this idea, <a data-type="xref" href="#chapter_4_table_2_1748539919012629">Table 4-2</a> gives some examples of useful and harmful applications of the same technologies. This is important because AI technology is general-purpose, and developers—such as those who release open-weight and open source foundation models—can’t fully control how others might use it. To give a specific example, fake reviews were a problem on many websites even before the widespread adoption of generative AI, requiring companies to devote significant resources to detecting and removing them. Traditional fake reviews often used similar language, making them easier to spot. However, the ability of GenAI tools to automatically rephrase or rewrite text has made detecting fake reviews increasingly challenging. In a case like this, the solution is not to limit the use of the technology itself; instead, when AI is used in a harmful way, it is that specific application that should be addressed and, if necessary, restricted or halted.</p>

<table id="chapter_4_table_2_1748539919012629">
	<caption><span class="label">Table 4-2. </span>Useful and harmful applications of general-purpose technologies</caption>
	<thead>
		<tr>
			<th>General-purpose technology</th>
			<th colspan="2">Applications</th>
		</tr>
		<tr>
			<td class="subheading"> </td>
			<td class="subheading">Useful</td>
			<td class="subheading">Harmful</td>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Nuclear technology</p>
			</td>
			<td>
			<ul>
				<li>Electricity generation</li>
				<li>Large-scale desalination plants to produce fresh water from seawater</li>
				<li>Medical imaging (X-rays, CT scans)</li>
				<li>Cancer treatments like radiation therapy</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>Nuclear weapons</li>
				<li>Nuclear waste (as a byproduct)</li>
				<li>Nuclear facilities (potential for reactor meltdowns and radioactive contamination)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Artificial intelligence</p>
			</td>
			<td>
			<ul>
				<li>Automating repetitive tasks and streamlining processes</li>
				<li>Analyzing vast amounts of data to provide insights for more informed business decisions</li>
				<li>Enabling personalized recommendations and experiences</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>Autonomous weapons systems</li>
				<li>Spreading misinformation or manipulating public opinion</li>
				<li>Political deepfakes</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>

<section data-pdf-bookmark="Prohibited AI Use Cases" data-type="sect2"><div class="sect2" id="chapter_4_prohibited_ai_use_cases_1748539919035344">
<h2>Prohibited AI Use Cases</h2>

<p>The EU AI Act aims to protect fundamental rights, democracy, and the rule of law by banning AI applications that are incompatible with core EU values and rights. In general, the Act prohibits AI systems from:</p>

<ul>
	<li>
	<p>Using subliminal, manipulative, or deceptive techniques to misinterpret behavior and prevent informed decision making, causing significant harm</p>
	</li>
	<li>
	<p>Exploiting vulnerabilities related to age, disability, or socioeconomic circumstances to distort behavior, causing significant harm</p>
	</li>
	<li>
	<p>Utilizing biometric categorization systems to infer sensitive attributes such as race, political opinions, religious beliefs, and sexual orientation</p>
	</li>
	<li>
	<p>Conducting social scoring (i.e., evaluating individuals based on social behavior or personal traits), which can lead to discriminative treatment</p>
	</li>
	<li>
	<p>Creating or expanding facial recognition databases through untargeted scraping of facial images from the internet or CCTV footage</p>
	</li>
	<li>
	<p>Inferring emotions in workplaces or educational institutions, except for medical or safety reasons</p>
	</li>
</ul>

<p>All prohibited AI systems were required to have been removed from the EU market by February 2025, six months after the Act entered into force (please refer to <a data-type="xref" href="ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819">Chapter 1</a> for details on the implementation timeline). It is important to correctly identify and classify these systems to ensure they are removed from operation, in accordance with the regulation.</p>

<p>Let’s take a closer look at the some of the specific types of AI systems that are <span class="keep-together">prohibited</span>:</p>

<dl>
	<dt>Systems that manipulate people through subliminal techniques</dt>
	<dd>
	<p>These are systems that use hidden tricks, like sounds or images, to influence people without them realizing it—for example, an AI system that plays barely audible music in a store to influence people to buy more things.</p>
	</dd>
	<dt>Systems that use facial recognition to categorize people based on sensitive characteristics</dt>
	<dd>
	<p>This means systems that use facial recognition<a contenteditable="false" data-primary="prohibited AI practices" data-secondary="facial recognition" data-type="indexterm" id="prohib-face-1"/> technology to sort people into groups based on their race, religion, sexual orientation, or other personal characteristics. Such systems are prohibited to protect people from being discriminated against based on these characteristics.</p>
	</dd>
	<dt class="pagebreak-before">Social scoring systems</dt>
	<dd>
	<p>These are systems that create a score<a contenteditable="false" data-primary="prohibited AI practices" data-secondary="social scoring" data-type="indexterm" id="id530"/> for a person based on their online behavior or other personal information. This score could then be used to deny people jobs, housing, or other opportunities. (See <a data-type="xref" href="#chapter_4_deep_dive_social_scoring_1748539919035416">“Deep Dive: Social Scoring”</a> for an example of social scoring.)</p>
	</dd>
	<dt>Law enforcement facial recognition in public places (except for specific cases)</dt>
	<dd>
	<p>This means that police and other law enforcement agencies cannot normally use facial recognition technology to scan people in public places. There are some exceptions, though, such as when they are looking for missing people, trying to prevent terrorism, or trying to catch criminals.</p>
	</dd>
	<dt>AI systems that predict criminality</dt>
	<dd>
	<p>These are systems that use artificial intelligence to try to guess whether someone is likely to commit a crime in the future. This is not allowed because it could lead to people being punished for something they haven’t done.</p>
	</dd>
	<dt>Systems that build facial recognition databases without people’s permission</dt>
	<dd>
	<p>This means that companies and organizations cannot collect large databases<a contenteditable="false" data-primary="prohibited AI practices" data-secondary="facial recognition" data-startref="prohib-face-1" data-type="indexterm" id="id531"/> of people’s faces without their permission. This is to protect people’s privacy.</p>
	</dd>
	<dt>Emotion recognition systems at work or school (except for special cases)</dt>
	<dd>
	<p>These are systems that use artificial intelligence to try to guess what emotions people are feeling. This is generally not allowed in workplaces or schools, but there are some exceptions. For example, an AI system might be allowed to do this if it is being used to help people with autism learn to understand emotions.</p>
	</dd>
</dl>

<div data-type="tip"><h6>Tip</h6>
<p><a href="https://oreil.ly/I-18Z">Chapter II, Article 5 of the EU AI Act</a> contains 19 recitals—numbers 3 and 28–45—that you can consult to more deeply understand the prohibited AI use cases.</p>
</div>
</div></section>

<section data-pdf-bookmark="Deep Dive: Social Scoring" data-type="sect2"><div class="sect2" id="chapter_4_deep_dive_social_scoring_1748539919035416">
<h2>Deep Dive: Social Scoring</h2>

<p>Some forms of social scoring are already present in our daily lives. For example, financial institutions may use credit scoring systems that consider past financial behavior and social media data. Similarly, some insurance companies adjust rates based on data collected from fitness trackers. According to the EU AI Act, <a href="https://oreil.ly/cWYg4">Recital 31</a>, social scoring systems are prohibited due to the following key concerns:</p>

<dl>
	<dt>Discriminatory outcomes</dt>
	<dd>
	<p>Social scoring systems may lead to discriminatory results and the exclusion of certain groups.</p>
	</dd>
	<dt>Violation of fundamental rights</dt>
	<dd>
	<p>These systems can violate the rights to dignity, non-discrimination, equality, and justice. There are also concerns about AI tools that compromise user privacy by collecting sensitive data without consent.</p>
	</dd>
	<dt>Unfair evaluation</dt>
	<dd>
	<p>Social scoring systems evaluate or classify individuals based on multiple data points related to their social behavior across various contexts or on known, inferred, or predicted personal characteristics over time. The scoring mechanisms often lack transparency, fairness, and accountability.</p>
	</dd>
	<dt>Harmful treatment</dt>
	<dd>
	<p>The social scores obtained from these systems can result in negative treatment of individuals or groups in social contexts unrelated to where the data was initially generated or collected.</p>
	</dd>
	<dt>Extreme consequences</dt>
	<dd>
	<p>Social scoring can lead to harmful treatment that is unjustified relative to the gravity of a person’s social behavior.</p>
	</dd>
	<dt>Broad impact</dt>
	<dd>
	<p>These systems can affect individuals’ access to services, employment, or other opportunities.</p>
	</dd>
</dl>

<p>The EU AI Act aims to mitigate these risks by prohibiting social scoring AI systems. This ban applies to public and private actors using AI<a contenteditable="false" data-primary="prohibited AI practices" data-startref="prohib-ai-1" data-type="indexterm" id="id532"/> for social scoring.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Determining EU AI Act Obligations" data-type="sect1"><div class="sect1" id="chapter_4_determining_ai_act_obligations_1748539919035513">
<h1>Determining EU AI Act Obligations</h1>

<p>To determine your obligations under the EU AI Act, you need to take two fundamental steps when assessing your AI systems:</p>

<ol>
	<li>
	<p>Determine whether the Act applies to any AI systems in your inventory and clarify the scope of your obligations by answering the following questions:</p>

	<ul>
		<li>
		<p>Do they meet the Act’s definition of an “AI system”? (For more on this, see the following sidebar.)</p>
		</li>
		<li>
		<p>If so, are they within the scope of the Act based on their intended use and risk classification? Those systems with low risk are exempt.</p>
		</li>
		<li>
		<p>Are they considered high risk?</p>
		</li>
		<li>
		<p>Do transparency obligations apply?</p>
		</li>
	</ul>
	</li>
	<li>
	<p>Identify your organization’s role with respect to each system. Is it acting as a provider or deployer?</p>
	</li>
</ol>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="chapter_4_how_to_determine_whether_a_system_qualifies_as_an_1748539919035612">
<h1>How to Determine Whether a System Qualifies as an AI System <span class="keep-together">Under the EU AI Act</span></h1>

<p>To determine whether an AI system meets the Act’s definition, consider the following questions:</p>

<ol>
	<li>
	<p><em>Is it a machine-based (computational) system?</em><strong> </strong>The system must be implemented in software and run on hardware (e.g., computers, sensors, robots).</p>
	</li>
	<li>
	<p><em>Is it designed to operate with some level of autonomy?</em><strong> </strong>Does it make decisions or perform actions <em>without constant human direction</em> (even if partially supervised)?</p>
	</li>
	<li>
	<p><em>Does it produce outputs based on input data?</em><strong> </strong>For example, does it generate <em>predictions, recommendations, decisions, or content</em> by processing data?</p>
	</li>
	<li>
	<p><em>Are the outputs generated through inference (not just fixed rules)?</em><strong> </strong>Does it use <em>statistical, logical, symbolic, or machine learning techniques</em> to infer a result from data?</p>
	</li>
	<li>
	<p><em>Is it designed to achieve an explicit or implicit objective?</em><strong> </strong>Does it aim to solve a task, meet a goal, or optimize an outcome (e.g., classify images, suggest content)?</p>
	</li>
	<li>
	<p><em>Can its outputs influence a physical or virtual environment?</em><strong> </strong>Can it impact the real world or a digital system (e.g., control a robot, change what a user sees)?</p>
	</li>
	<li>
	<p><em>Optional but relevant: Can it adapt after deployment?</em><strong> </strong>Does it have the ability to <em>learn or change behavior over time</em> based on its experience? (Note: This is not required, but strengthens the case.)</p>
	</li>
</ol>

<p>If you answered “yes” to questions 1 through 6, then the system qualifies as an AI system under the EU AI Act. If you answered “no” to any of the first three or four questions, it’s likely not considered an AI system under the Act.</p>
</div></aside>

<p>Once you’ve identified which of your AI systems meet the Act’s definition, the next step is to classify their risk level. Your exact obligations will vary depending on this classification.</p>

<section data-pdf-bookmark="Framework for Classification of AI Systems by Risk Levels" data-type="sect2"><div class="sect2" id="chapter_4_framework_for_classification_of_ai_systems_by_risk_1748539919035701">
<h2>Framework for Classification of AI Systems by Risk Levels</h2>

<p>Having already discussed the unacceptable risk category, here we will consider the remaining three categories: high risk, limited risk, and minimal risk. Let’s start by reviewing the criteria that determine whether a system is classified as high risk.</p>

<section data-pdf-bookmark="High risk" data-type="sect3"><div class="sect3" id="chapter_4_high_risk_1748539919035804">
<h3>High risk</h3>

<p>The classification rules for high-risk AI systems, outlined in <a contenteditable="false" data-primary="Article 6: Classification Rules for High-Risk AI Systems" data-type="indexterm" id="id533"/><a href="https://oreil.ly/AyOj1">Article 6 of the EU AI Act</a>, aim to identify systems that may pose significant risks to health, safety, or fundamental rights. The classification takes into account both the AI system’s intended <span class="keep-together">purpose</span> and the specific context and conditions under which it is used. Key considerations include:</p>

<ul>
	<li>
	<p>AI systems are classified as high risk whether they are placed on the market or put into service as standalone products or as components of other products. For AI systems used as safety components of products, the classification aligns with the EU’s New Legislative Framework<a contenteditable="false" data-primary="EU New Legislative Framework" data-type="indexterm" id="id534"/>, ensuring consistency with broader product safety regulation.</p>
	</li>
	<li>
	<p>AI systems intended to be used as safety components of products covered by <a href="https://oreil.ly/d5LQd">EU harmonization legislation</a> are automatically classified as high risk.</p>
	</li>
	<li>
	<p>AI systems listed in <a href="https://oreil.ly/9-WO8">Annex III</a> are<a contenteditable="false" data-primary="Annex III (high-risk systems)" data-type="indexterm" id="id535"/> classified as high risk. These include systems intended for use in the areas of:</p>

	<ul>
		<li>
		<p>Biometric identification and categorization of natural persons (e.g., AI systems for remote biometric identification in publicly accessible spaces)</p>
		</li>
		<li>
		<p>Management and operation of critical infrastructure (e.g., AI systems used as safety components in the supply of water, gas, heating, or electricity)</p>
		</li>
		<li>
		<p>Education and vocational training (e.g., AI systems used to evaluate students or assess educational institutions)</p>
		</li>
		<li>
		<p class="fix_tracking">Employment, workers management, and access to self-employment (e.g., AI systems used for recruitment, promotion, and termination decisions)</p>
		</li>
		<li>
		<p class="fix_tracking">Access to and enjoyment of key private services and public services and benefits (e.g., AI systems used to evaluate creditworthiness or establish credit scores)</p>
		</li>
		<li>
		<p>Law enforcement (e.g., AI systems used by law enforcement to predict crimes or assess recidivism risk)</p>
		</li>
		<li>
		<p>Migration, asylum, and border control management (e.g., AI systems used to process asylum, visa, and residence permit applications)</p>
		</li>
		<li>
		<p>Administration of justice and democratic processes (e.g., AI systems used in judicial proceedings to assist judges)</p>
		</li>
	</ul>
	</li>
</ul>

<p>Certain systems are exempted from high-risk classification even if they might otherwise qualify. These include:</p>

<ul>
	<li>
	<p>AI systems used for purely accessory purposes (e.g., performing a narrowly procedural task, improving the result of a previously completed human activity, or performing a preparatory task)</p>
	</li>
	<li>
	<p>AI systems used exclusively for scientific research and development</p>
	</li>
	<li>
	<p>AI systems used for military applications</p>
	</li>
</ul>

<section data-pdf-bookmark="EU AI Act high-risk system questionnaire" data-type="sect4"><div class="sect4" id="chapter_4_eu_ai_act_high_risk_system_questionnaire_1748539919035896">
<h4>EU AI Act high-risk system questionnaire</h4>

<p>Classifying AI applications as high risk means they are subject to stricter requirements under the EU AI Act related to risk management procedures, data governance measures, technical documentation, recordkeeping, transparency, human oversight, accuracy, and cybersecurity controls.</p>

<p>Reading and interpreting the EU AI Act can be daunting. To help with determining whether an AI system qualifies as high risk under the Act, I have prepared a straightforward questionnaire, divided into four main sections:</p>

<ol>
	<li>
	<p>Application area</p>
	</li>
	<li>
	<p>Specific use cases</p>
	</li>
	<li>
	<p>Impact assessment</p>
	</li>
	<li>
	<p>Technical characteristics</p>
	</li>
</ol>

<p>The questionnaire is presented in <a data-type="xref" href="#chapter_4_table_3_1748539919012658">Table 4-3</a>. Start with section 1 and work through each section in turn. The more affirmative answers you have, especially in sections 1 and 2, the more likely it is that your AI system is considered high risk.</p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Classifying an AI system with confidence can be challenging. In its white paper <a href="https://oreil.ly/a_oMu">“AI Act: Risk Classification of AI Systems from a Practical Perspective”</a>, the applied AI Institute examined more than 100 AI systems from different enterprise functions (marketing, production, purchasing, etc.) and determined that 18% were in the high-risk class, 42% were low risk, and for the remaining 40% it was unclear whether they fell into the high-risk class or not. The percentage of high-risk systems in this sample thus could be anywhere between 18% and 58%.</p>

<p>The study identified several ambiguities in defining criteria across different areas as the main causes of unclear risk classifications for AI systems. For example, in many cases it is not clear whether a system is functioning as a “safety component.”</p>
</div>

<table class="border" id="chapter_4_table_3_1748539919012658">
	<caption><span class="label">Table 4-3. </span>EU AI Act high-risk system questionnaire</caption>
	<thead>
		<tr>
			<th>Question</th>
			<th>Yes</th>
			<th>No</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading" colspan="3">1. Application area</td>
		</tr>
		<tr>
			<td colspan="3">
			<p>Is the AI system intended to be used in any of the following areas?</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Biometric identification and categorization of natural persons</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Critical infrastructure (e.g., transport, energy, water supply)</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Education or vocational training</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Employment, worker management, or access to self-employment</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Access to and enjoyment of essential private services and public services and benefits</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Law enforcement</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Migration, asylum, and border control management</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Administration of justice and democratic processes</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td colspan="3">
			<p>If you checked any box in section 1, proceed to section 2. If not, your system is likely not considered high risk.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" colspan="3">2. Specific use cases</td>
		</tr>
		<tr>
			<td colspan="3">
			<p>Within the selected area(s), does your AI system fall under any of these specific use cases?</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>AI systems used as safety components of products</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>AI systems for management or operation of critical infrastructure</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>AI systems for determining access to educational institutions or assigning persons to such institutions</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>AI systems for recruitment, evaluation, promotion, or termination of work-related contractual relationships</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>AI systems for evaluating creditworthiness or establishing credit scores</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>AI systems for dispatching emergency first response services</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>AI systems for law enforcement purposes (e.g., predicting crimes, profiling individuals)</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>AI systems for migration, asylum, and border control management (e.g., verifying travel documents)</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>AI systems to assist judicial authorities in researching and interpreting facts and the law</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td colspan="3">
			<p>If you checked any box in section 2, your AI system is likely considered high risk. If not, proceed to section 3.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" colspan="3">3. Impact assessment</td>
		</tr>
		<tr>
			<td colspan="3">
			<p>Does your AI system have the potential to:</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Cause significant harm to the health, safety, or fundamental rights of individuals?</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Have a significant impact on the lives of a large number of EU residents?</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Be difficult for individuals to opt out of or avoid?</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td colspan="3">
			<p>Is the AI system intended to be used in a manner that could result in:</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Discrimination against protected groups?</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Manipulation of human behavior?</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Exploitation of vulnerabilities of specific groups?</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td colspan="3">
			<p>If you checked any box in section 3, your AI system may be considered high risk, depending on the context and potential impact.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" colspan="3">4. Technical characteristics</td>
		</tr>
		<tr>
			<td>
			<p>Does your AI system:</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Use large datasets for training or operation?</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Employ complex algorithms or machine learning techniques?</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Have a high degree of autonomy in decision making?</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Operate in a manner that is not fully transparent or explainable?</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td colspan="3">
			<p>If you checked multiple boxes in section 4, combined with affirmative answers in previous sections, your system is more likely to be considered high risk.</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Interpretation guide" data-type="sect4"><div class="sect4" id="chapter_4_interpretation_guide_1748539919035965">
<h4>Interpretation guide</h4>

<p>If you have affirmative answers in sections 1 and 2, or in section 1 combined with multiple affirmative answers in sections 3 and 4, your AI system is likely to be considered high risk under the EU AI Act. However, the final determination may depend on the specific context, implementation, and potential impact of your system.</p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>It’s important to note that the European Commission can adjust Annex III and add new high-risk AI systems as technology evolves, following the criteria outlined in <a href="https://oreil.ly/ipd1a">Article 7</a>. This allows the regulation to remain responsive to emerging AI applications and risks.</p>

<p>For a definitive assessment, it is recommended to consult legal experts and stay up-to-date on the latest guidance from EU <span class="keep-together">authorities</span>.</p>
</div>
</div></section>
</div></section>

<section data-pdf-bookmark="Limited risk (transparency obligations)" data-type="sect3"><div class="sect3" id="chapter_4_limited_risk_transparency_obligations_1748539919036028">
<h3>Limited risk (transparency obligations)</h3>

<p>To be clear, the EU AI Act does not formally define a category of “limited-risk” AI systems<a contenteditable="false" data-primary="limited-risk AI systems" data-type="indexterm" id="id536"/>. Instead, it focuses on three main categories: prohibited AI practices under the unacceptable risk category (Article 5), high-risk AI systems (Article 6)<a contenteditable="false" data-primary="high-risk AI systems" data-type="indexterm" id="h-r-ai-1"/>, and AI systems with specific transparency obligations (Article 50).</p>

<p>However, for clarity, I use the term “limited risk” to refer to AI systems that are not explicitly prohibited or high risk but are still subject to the transparency obligations laid out in <a href="https://oreil.ly/cc73j">Article 50</a>. This includes:</p>

<ol>
	<li>
	<p>AI systems that are intended to interact with natural persons</p>
	</li>
	<li>
	<p>Emotion recognition systems and biometric categorization systems</p>
	</li>
	<li>
	<p>AI systems that generate or manipulate image, audio, text, or video content (including deepfakes)</p>
	</li>
</ol>

<p>As for high-risk systems, I have created a questionnaire that you can use to determine whether your AI system has specific transparency obligations. I’ll discuss what those obligations are in <a data-type="xref" href="ch06.html#chapter_6_ai_engineering_for_limited_risk_ai_systems_1748539923606988">Chapter 6</a>.</p>

<section data-pdf-bookmark="EU AI Act limited-risk system questionnaire" data-type="sect4"><div class="sect4" id="chapter_4_eu_ai_act_limited_risk_system_questionnaire_1748539919036099">
<h4>EU AI Act limited-risk system questionnaire</h4>

<p>Answer the questions in <a data-type="xref" href="#chapter_4_table_4_1748539919012681">Table 4-4</a> to help determine if your AI system might be classified as a limited-risk system under the EU AI Act.</p>

<table class="border pagebreak-before less_space" id="chapter_4_table_4_1748539919012681">
	<caption><span class="label">Table 4-4. </span>EU AI Act limited-risk system questionnaire</caption>
	<thead>
		<tr>
			<th>Question</th>
			<th>Yes</th>
			<th>No</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>1. Is your AI system intended to interact with humans?</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td colspan="3">
			<p>2. Does your AI system fall into any of these categories?</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Chatbots</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Emotion recognition systems</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>Biometric categorization systems</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>AI systems used to generate or manipulate image, audio, or video content (“deepfakes”)</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td colspan="3">
			<p>3. Is your AI system specifically excluded from the limited-risk category for any of the following reasons?</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>It’s classified as a high-risk AI system under the EU AI Act.</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>It’s used for general-purpose AI without a clearly defined intended purpose.</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<ul>
				<li>It’s used for a prohibited AI practice (e.g., social scoring, certain types of biometric identification).</li>
			</ul>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>4. Does your AI system have the potential to influence human behavior or decision making?</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>5. Is your AI system designed to be transparent about its AI nature?</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>6. Can users easily understand that they are interacting with an AI system?</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>7. Does your system provide clear disclaimers about its limitations and potential risks?</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>8. For systems that generate synthetic content (or manipulate image, audio, or video content), do you have measures in place to ensure the AI-generated content is clearly labeled as such?</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>9. For biometric categorization or emotion recognition systems, do you inform users about the system’s purpose, functionality, and limitations?</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>10. Do you have processes in place to handle user complaints or concerns about the AI system?</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
			<td>
			<p>[    ]</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Interpretation guide" data-type="sect4"><div class="sect4" id="chapter_4_interpretation_guide_1748539919036164">
<h4>Interpretation guide</h4>

<p>You may interpret your answers as follows:</p>

<ul>
	<li>
	<p>If you answered “Yes” to question 1 and to any item in question 2, your system will likely be considered a limited-risk AI system.</p>
	</li>
	<li>
	<p>If you answered “Yes” to any item in question 3, your system is likely not a limited-risk system (it may be high risk or prohibited).</p>
	</li>
	<li>
	<p>Questions 4–10 relate to transparency and user protection requirements for limited-risk systems. Answering “No” to any of these may indicate areas where your system needs improvement to comply<a contenteditable="false" data-primary="high-risk AI systems" data-startref="h-r-ai-1" data-type="indexterm" id="id537"/> with the EU AI Act’s requirements.</p>
	</li>
</ul>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Please note that this questionnaire is intended as a general guide. For a definitive assessment, it is recommended to consult legal experts and stay up-to-date on the latest guidance from EU <span class="keep-together">authorities</span>.</p>
</div>
</div></section>
</div></section>

<section data-pdf-bookmark="Low risk" data-type="sect3"><div class="sect3" id="chapter_4_low_risk_1748539919036218">
<h3>Low risk</h3>

<p>Even though the EU AI Act focuses more heavily on high-risk AI systems, it is still important address “low-risk” AI systems, as most AI currently in use across the EU falls into this category and some of these systems (such as chatbots and emotion recognition tools) may still be subject to transparency requirements.</p>

<p>The Act does not explicitly define a “low-risk” category. However, by process of elimination, low-risk AI systems<a contenteditable="false" data-primary="low-risk AI systems" data-type="indexterm" id="low-r-ai-sys-1"/> can be understood to be those that:</p>

<ol>
	<li>
	<p>Do not involve practices prohibited under Article 5</p>
	</li>
	<li>
	<p>Are not classified as high risk under Article 6</p>
	</li>
	<li>
	<p>Are not subject to transparency obligations under the terms outlined in <span class="keep-together">Article 50</span></p>
	</li>
</ol>

<p>These systems generally pose minimal risks to fundamental rights or public safety and are typically used for administrative or operational purposes rather than for making decisions with serious consequences.</p>

<p>Some examples of low-risk AI systems include:</p>

<ul>
	<li>
	<p>Spam filters (AI systems that help to identify and filter unwanted emails)</p>
	</li>
	<li>
	<p>Recommendation systems (AI systems that suggest products, movies, or other content based on user preferences)</p>
	</li>
	<li>
	<p>Chatbots that provide customer service (AI systems that can answer simple questions and provide basic assistance)</p>
	</li>
	<li>
	<p>AI-powered games (AI systems that can generate challenges or adapt to player behavior)</p>
	</li>
</ul>

<p>Since the EU AI Act primarily focuses on regulating high-risk AI systems and prohibiting certain AI practices, no specific obligations are defined for low-risk AI systems as a distinct category.</p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Note that the classification of an AI system can depend on its specific context and intended use. Even a seemingly low-risk AI system<a contenteditable="false" data-primary="low-risk AI systems" data-startref="low-r-ai-sys-1" data-type="indexterm" id="id538"/> could become high risk if used in a way that could harm individuals or society.</p>

<p>For example, consider facial recognition technology,<strong> </strong>which can be used for purposes ranging from unlocking personal devices to <span class="keep-together">surveillance</span>:</p>

<ul>
	<li>
	<p>Unlocking personal devices is considered minimal risk as it involves a consenting individual using the technology privately and poses little to no risk to broader society.</p>
	</li>
	<li>
	<p>Surveillance by law enforcement, on the other hand, is classified as high risk due to its potential implications for fundamental rights and freedoms, such as privacy infringements and the risk of misidentification.</p>
	</li>
</ul>
</div>

<p>If you’d like more information about this complex topic, check out <a href="https://oreil.ly/Zmit2">“Navigating the EU AI Act Maze Using a Decision-Tree Approach”</a> by Himly Hanif et al.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Deployer or Provider" data-type="sect2"><div class="sect2" id="chapter_4_deployer_or_provider_1748539919036275">
<h2>Deployer or Provider</h2>

<p>As mentioned earlier, an organization’s obligations under the EU AI Act depend on two key factors: the AI system’s risk category and whether it acts as a deployer<a contenteditable="false" data-primary="key players" data-secondary="deployer" data-type="indexterm" id="id539"/><a contenteditable="false" data-primary="key players" data-secondary="provider" data-type="indexterm" id="id540"/> or provider of the system. Let’s review the definitions of those two roles.</p>

<p>Under the terms of the EU AI Act, a <em>provider</em> is any individual, company, public authority, agency, or other body that:</p>

<ol>
	<li>
	<p>Develops an AI system or model, or has it developed by others</p>
	</li>
	<li>
	<p>Places the system or model on the market or puts it into service, either for a fee or free of charge</p>
	</li>
	<li>
	<p>Does so under their own name or trademark</p>
	</li>
</ol>

<p>Examples of providers include a tech company like Google that develops and releases large language models such as Gemini and a startup that develops an AI algorithm for predicting stock market trends and offers it as a service.</p>

<p class="pagebreak-before">The EU AI Act defines a <em>deployer</em> as any individual, company, public authority, agency, or other body that:</p>

<ol>
	<li>
	<p>Uses an AI system under their authority or control</p>
	</li>
	<li>
	<p>Does so in a professional or business context (personal, non-professional use is excluded)</p>
	</li>
</ol>

<p>This definition applies regardless of whether the deployer is located within the EU or not, as long as the AI system’s output is used within the EU.</p>

<p>Examples of deployers include a hospital using an AI system for medical diagnosis or treatment planning, a company using an AI-powered recruitment tool to screen job applicants, or a bank utilizing an AI system for credit scoring or loan approval.</p>

<p>An important distinction is that providers develop, place on the market, or put into service AI systems under their own name or trademark, whereas deployers use AI systems within their professional activities but do not develop or market the systems themselves. A company that develops an AI system for its own use can act as both a provider and a deployer. Additionally, a deployer may become a provider if they substantially modify an AI system or use it for purposes not intended by the original <span class="keep-together">provider</span>.</p>

<p>In <a data-type="xref" href="#chapter_4_table_5_1748539919012703">Table 4-5</a>, I provide a practical framework to help determine your organization’s role and to identify all the relevant players who may act as deployers or providers of an AI system. The first step is to analyze every party involved in the machine learning lifecycle, using the CRISP-ML(Q) phases as a guide. For each actor, outline their specific activities, identify their location, and determine who sets or redefines the intended purpose of the AI system. Also consider the individuals or groups affected by its use. Based on this information, you can accurately assign the appropriate role to each identified organization.</p>

<table border="1" id="chapter_4_table_5_1748539919012703">
	<caption><span class="label">Table 4-5. </span>Analysis of the organizations involved in the machine learning (ML) lifecycle</caption>
	<thead>
		<tr>
			<th>CRISP-ML(Q) phase</th>
			<th>Who is implementing and executing this phase?</th>
			<th>What is being done?</th>
			<th>Where is the organization located?</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Business and data understanding</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Data preparation</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Development</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Evaluation</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Deployment</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Monitoring and maintenance</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
	</tbody>
</table>

<p>Now that you’ve determined whether the Act applies to your AI systems and clarified your organization’s role in relation to each system, you’re ready to identify the regulatory implications.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Integrating EU AI Act Engineering Throughout the AI Development Lifecycle" data-type="sect1"><div class="sect1" id="chapter_4_integrating_eu_ai_act_engineering_throughout_the_a_1748539919036345">
<h1>Integrating EU AI Act Engineering Throughout the <span class="keep-together">AI Development Lifecycle</span></h1>

<p>Beyond the requirements<a contenteditable="false" data-primary="AI development lifecycle" data-type="indexterm" id="ai-dev-lc-1"/> and obligations<a contenteditable="false" data-primary="EU AI Act" data-secondary="AI development lifecycle integration" data-seealso="AI development lifecycle" data-type="indexterm" id="id541"/> mentioned in the previous sections, the EU AI Act (along with other regulations on AI systems) introduces additional compliance considerations, particularly regarding transparency, risk management, and data governance. It is crucial to incorporate AI engineering efforts into every phase of the AI product lifecycle, especially when developing high-risk AI systems. As pictured in <a data-type="xref" href="#chapter_4_figure_5_1748539919001734">Figure 4-5</a>, the level of engineering engagement and compliance effort tends to increase as a project progresses from ideation and proof of concept to the minimum viable product (MVP) stage, affecting development time, costs, and risk management. By integrating these efforts early in the development process—i.e., adopting a “shift-left” philosophy—teams can reduce risks and ensure that the final product is both technically robust and compliant with evolving AI regulations.</p>

<figure><div class="figure" id="chapter_4_figure_5_1748539919001734"><img src="assets/taie_0405.png"/>
<h6><span class="label">Figure 4-5. </span>The usual delivery sequence and relationship between PoC, prototype, pilot, and MVP</h6>
</div></figure>

<p><a data-type="xref" href="#chapter_4_table_6_1748539919012725">Table 4-6</a> provides an overview of the different stages of AI development, illustrating the progression from initial idea to a market-ready product. In addition to core criteria such as scope, cost, and expected development time, it highlights relevant AI engineering tasks (both general and those specifically aimed at meeting EU AI Act compliance requirements)<a contenteditable="false" data-primary="AI development lifecycle" data-secondary="ideation" data-type="indexterm" id="id542"/><a contenteditable="false" data-primary="AI development lifecycle" data-secondary="proof of concept (PoC)" data-type="indexterm" id="id543"/><a contenteditable="false" data-primary="AI development lifecycle" data-secondary="prototype" data-type="indexterm" id="id544"/><a contenteditable="false" data-primary="AI development lifecycle" data-secondary="pilot" data-type="indexterm" id="id545"/><a contenteditable="false" data-primary="AI development lifecycle" data-secondary="minimum viable product (MVP)" data-type="indexterm" id="id546"/>.</p>

<table class="striped pagebreak-before less_space" id="chapter_4_table_6_1748539919012725">
	<caption><span class="label">Table 4-6. </span>AI development stages and relevant AI engineering tasks</caption>
	<thead>
		<tr>
			<th> </th>
			<th>Ideation</th>
			<th>PoC</th>
			<th>Prototype</th>
			<th>Pilot</th>
			<th>MVP</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading">Purpose</td>
			<td>
			<p>Generate and explore innovative AI ideas.</p>
			</td>
			<td>
			<p>Validate technical feasibility and potential value.</p>
			</td>
			<td>
			<p>Create a working model to demonstrate core functionalities.</p>
			</td>
			<td>
			<p>Test the AI solution in a real-world environment.</p>
			</td>
			<td>
			<p>Launch a basic version to gather user feedback and validate market fit.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading">Scope of AI product</td>
			<td>
			<p>Broad, exploring multiple possibilities</p>
			</td>
			<td>
			<p>Narrow, focused on key technical challenges</p>
			</td>
			<td>
			<p>Limited, but includes core features and user interactions</p>
			</td>
			<td>
			<p>Broader than prototype, but still limited to specific use cases or user groups</p>
			</td>
			<td>
			<p>Includes essential features to solve the core problem</p>
			</td>
		</tr>
		<tr>
			<td class="subheading">User involve-ment</td>
			<td>
			<p>Limited, mainly internal stakeholders</p>
			</td>
			<td>
			<p>Minimal, possibly some expert users</p>
			</td>
			<td>
			<p>Limited, typically internal testers or focus groups</p>
			</td>
			<td>
			<p>Moderate, involving a select group of real users</p>
			</td>
			<td>
			<p>Significant, engaging early adopters and gathering extensive feedback</p>
			</td>
		</tr>
		<tr>
			<td class="subheading">Dev. time (typical)</td>
			<td>
			<p>Days to weeks</p>
			</td>
			<td>
			<p>Weeks to a couple of months</p>
			</td>
			<td>
			<p>1–3 months</p>
			</td>
			<td>
			<p>3–6 months</p>
			</td>
			<td>
			<p>6–12 months</p>
			</td>
		</tr>
		<tr>
			<td class="subheading">Cost</td>
			<td>
			<p>Low</p>
			</td>
			<td>
			<p>Low to medium</p>
			</td>
			<td>
			<p>Medium</p>
			</td>
			<td>
			<p>Medium to high</p>
			</td>
			<td>
			<p>High</p>
			</td>
		</tr>
		<tr>
			<td class="subheading">Risk</td>
			<td>
			<p>Very low</p>
			</td>
			<td>
			<p>Low</p>
			</td>
			<td>
			<p>Medium</p>
			</td>
			<td>
			<p>Medium to high</p>
			</td>
			<td>
			<p>High</p>
			</td>
		</tr>
		<tr>
			<td class="subheading">AI eng. tasks (general)</td>
			<td>
			<ul>
				<li>
				<p>Define initial data requirements.</p>
				</li>
				<li>
				<p>Sketch basic model architectures.</p>
				</li>
				<li>
				<p>Outline potential deployment scenarios.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Set up basic development environment.</p>
				</li>
				<li>
				<p>Perform initial data collection and preprocessing.</p>
				</li>
				<li>
				<p>Develop simple model(s).</p>
				</li>
				<li>
				<p>Perform basic model evaluation.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Implement data pipelines.</p>
				</li>
				<li>
				<p>Develop more complex models.</p>
				</li>
				<li>
				<p>Set up basic model versioning.</p>
				</li>
				<li>
				<p>Implement basic model serving.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Refine data pipelines.</p>
				</li>
				<li>
				<p>Implement model monitoring.</p>
				</li>
				<li>
				<p>Set up CI/CD for model deployment.</p>
				</li>
				<li>
				<p>Implement basic A/B testing.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Optimize data and model pipelines.</p>
				</li>
				<li>
				<p>Implement advanced monitoring and alerting.</p>
				</li>
				<li>
				<p>Set up automated retraining.</p>
				</li>
				<li>
				<p>Implement advanced A/B testing and experimentation.</p>
				</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading">AI eng. tasks for EU AI Act compliance</td>
			<td>
			<ul>
				<li>
				<p>Conduct initial risk assessment.</p>
				</li>
				<li>
				<p>Identify potential high-risk AI systems.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Draft preliminary documentation of system architecture.</p>
				</li>
				<li>
				<p>Define an initial data governance strategy.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Implement basic data quality measures.</p>
				</li>
				<li>
				<p>Design initial logging mechanisms.</p>
				</li>
				<li>
				<p>Draft preliminary technical documentation.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Implement more robust data governance.</p>
				</li>
				<li>
				<p>Enhance logging and traceability.</p>
				</li>
				<li>
				<p>Develop initial risk management system.</p>
				</li>
				<li>
				<p>Begin human oversight implementation.</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Implement comprehensive data governance.</p>
				</li>
				<li>
				<p>Establish full logging and traceability.</p>
				</li>
				<li>
				<p>Implement a complete risk management system.</p>
				</li>
				<li>
				<p>Finalize human oversight mechanisms.</p>
				</li>
				<li>
				<p>Prepare for conformity assessment.</p>
				</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading">Outcomes</td>
			<td>
			<ul>
				<li>
				<p>List of potential AI solutions</p>
				</li>
				<li>
				<p>Initial assessment of feasibility and value</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Technical validation report</p>
				</li>
				<li>
				<p>Go/no-go decision for further development</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Working AI model</p>
				</li>
				<li>
				<p>User interface mockups</p>
				</li>
				<li>
				<p>Initial user feedback</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Performance metrics in real-world conditions</p>
				</li>
				<li>
				<p>User adoption and satisfaction data</p>
				</li>
				<li>
				<p>Identified areas for improvement</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Functional AI product</p>
				</li>
				<li>
				<p>Initial user base</p>
				</li>
				<li>
				<p>Comprehensive feedback for future iterations</p>
				</li>
				<li>
				<p>Market validation</p>
				</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>

<p>The introduction of the EU AI Act has had various implications with regard to AI system development:</p>

<ol>
	<li>
	<p>AI engineering and compliance tasks become more complex as the project progresses from ideation to MVP. As the system matures, the need for robust MLOps processes and governance increases. Many AI engineering tasks, such as implementing robust data pipelines and model monitoring, directly contribute to meeting EU AI Act compliance requirements by ensuring data quality and system reliability.</p>
	</li>
	<li>
	<p>Data governance becomes increasingly important in later stages, helping to ensure data quality and regulatory compliance.</p>
	</li>
	<li>
	<p>Human oversight mechanisms become more sophisticated in later stages, in line with the EU AI Act’s requirement for oversight of high-risk AI systems.</p>
	</li>
	<li>
	<p>Risk assessment and management transition from high-level considerations in the early stages to comprehensive, continuous, and automated processes in the later stages.</p>
	</li>
	<li>
	<p>As the project progresses, there’s a greater emphasis on automating processes and ensuring scalability, particularly within MLOps tasks.</p>
	</li>
	<li>
	<p>At the MVP stage, the focus shifts toward preparing for audits and conformity assessments, reflecting the regulatory requirements for high-risk AI systems under the EU AI Act<a contenteditable="false" data-primary="AI development lifecycle" data-startref="ai-dev-lc-1" data-type="indexterm" id="id547"/>.</p>
	</li>
</ol>
</div></section>

<section data-pdf-bookmark="Emerging Roles in Organizations for EU AI Act Compliance" data-type="sect1"><div class="sect1" id="chapter_4_emerging_roles_in_organizations_for_eu_ai_act_comp_1748539919036409">
<h1>Emerging Roles in Organizations for EU AI Act Compliance</h1>

<p>As mentioned previously, the operationalization of EU AI Act compliance happens on two levels: the organizational level and the AI system level. I focus on the former in <a data-type="link" href="app04.html#appendix_d_emerging_roles_in_organizations_for_eu_ai_act_comp_1748539915764930">Appendix D</a>, but I want to briefly touch on the topic here as well.</p>

<p>With the rapid adoption of AI, several new roles are emerging within organizations to help ensure compliance with the EU AI Act. Some of these roles should be incorporated into machine learning teams to ensure that ethical and compliance aspects are fully integrated into the AI development process. Adopting a Team Topologies perspective provides a concrete framework for meeting the Act’s compliance <span class="keep-together">requirements</span>, ensuring that proper data governance is embedded throughout the ML development lifecycle. For further discussion of emerging roles and the implications of EU AI Act compliance for ML teams, see <a data-type="link" href="app04.html#appendix_d_emerging_roles_in_organizations_for_eu_ai_act_comp_1748539915764930">Appendix D</a>.</p>
</div></section>

<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="chapter_4_conclusion_1748539919036459">
<h1>Conclusion</h1>

<p>The chapter explored the initial steps of creating an AI system landscape and classifying AI systems in the context of achieving compliance with the EU AI Act. To determine the specific requirements for your organization, you will need to identify the AI systems that are currently deployed and intended to be put into production and classify each of them based on risk level. You’ll also need to determine whether the organization is acting as a provider or deployer of these AI systems, because the obligations differ depending on the role.</p>

<p>In the next chapter, I will cover the requirements for high-risk AI systems as outlined by the EU AI Act and how to implement them. Key topics will include the need for high-quality datasets, comprehensive documentation and recordkeeping, transparency, robustness and accuracy, security, risk management systems, human oversight, addressing ethical concerns and bias mitigation, monitoring in production, pre-market compliance verification, and maintaining continuous compliance for high-risk AI systems.</p>
</div></section>
</div></section></body></html>