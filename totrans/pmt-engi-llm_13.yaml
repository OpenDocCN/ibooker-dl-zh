- en: Chapter 10\. Evaluating LLM Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章\. 评估LLM应用
- en: GitHub Copilot is arguably the first industrial-scale application using LLMs.
    The curse of going first is that some of the choices you make will seem silly
    in hindsight, laughably flying in the face of what (by now) everyone knows.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Copilot可以说是第一个使用LLM的工业规模应用程序。作为第一个的诅咒是，您做出的某些选择在事后看起来可能很愚蠢，可笑地违背了现在众所周知的事实。
- en: 'But one of the things we got absolutely right was how we got started. The oldest
    part of Copilot’s codebase is not the proxy, or the prompts, or the UI, or even
    the boilerplate setting up the application as an IDE extension. The very first
    bit of code we wrote was the *evaluation*, and it’s only thanks to this that we
    were able to move so fast and so successfully with the rest. That’s because, for
    every change we made, we could check directly whether it was a step in the right
    direction, a mistake, or a good attempt that just didn’t have much of an impact.
    And that’s the main advantage of an evaluation framework for your LLM application:
    it will guide all future development.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们做得绝对正确的一件事是我们是如何开始的。Copilot代码库中最古老的部分不是代理，也不是提示，也不是UI，甚至不是作为IDE扩展设置应用程序的样板设置。我们写的第一段代码就是*评估*，也正是因为这一点，我们才能如此快速且成功地完成其他工作。那是因为，对于我们所做的每一个改变，我们都可以直接检查它是否是正确的方向，一个错误，或者是一个没有太大影响的良好尝试。这就是评估框架对您的LLM应用程序的主要优势：它将指导所有未来的开发。
- en: Depending on your application and your project’s position in its lifecycle,
    different types of evaluation may be available and appropriate. The two big categories
    here are offline and online evaluation. *Offline evaluation* is evaluation of
    example cases that are independent of any live runs of your application. Since
    it doesn’t require real users or even, in many cases, an end-to-end working app,
    it will typically be the evaluation you implement first in your project’s lifecycle.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的应用和项目在其生命周期中的位置，可能存在不同类型的评估，并且这些评估是合适和可行的。这里主要有两大类评估：离线评估和在线评估。*离线评估*是对与您应用程序的任何实际运行无关的示例案例的评估。由于它不需要真实用户，甚至在许多情况下，甚至不需要一个端到端工作的应用程序，因此它通常将是您在项目生命周期中首先实施的评估。
- en: 'Offline evaluation, however, is somewhat theoretical and possibly a bit disconnected
    from the real world. But once you deploy your app to the real world, you unlock
    *online evaluation*, which tests your ideas directly on your users. Being live
    raises the stakes for online evaluation compared to offline evaluation: you’d
    better be sure your ideas aren’t so terrible as to totally ruin the user experience,
    and you also need enough users to get sufficiently clear feedback in the first
    place. But if you overcome these hurdles, then the data you gather will be extremely
    valid for your use case in a way you can’t be sure of with offline evaluation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，离线评估在某种程度上是理论性的，并且可能与现实世界有些脱节。但一旦您将应用程序部署到现实世界，您就可以解锁*在线评估*，它直接在您的用户上测试您的想法。由于是实时进行的，与离线评估相比，在线评估的风险更高：您最好确保您的想法不会如此糟糕以至于完全破坏用户体验，并且您还需要足够的用户以获得足够清晰的反馈。但如果你克服了这些障碍，那么您收集到的数据将非常适用于您的用例，这是您在离线评估中无法确定的。
- en: Both offline and online evaluations are important, but before we delve into
    them, let’s zoom out for a second and ask ourselves a primary question.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 离线评估和在线评估都很重要，但在我们深入探讨它们之前，让我们先退一步，问自己一个主要问题。
- en: What Are We Even Testing?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们到底在测试什么？
- en: 'Evaluation can assess three things:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 评估可以评估三个方面：
- en: The model you use
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您使用的模型
- en: Your individual interactions with the model (i.e., your prompts)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您与模型的个人交互（即您的提示）
- en: The way many such interactions fit together in your overall application
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多此类交互如何在您的整体应用程序中组合在一起
- en: Think about the loop that represents one run of your application, which we discussed
    in [Chapter 4](ch04.html#ch04_designing_llm_applications_1728407230643376). As
    in traditional software testing, there’s a benefit in trying to test both the
    whole interaction (think regression tests) and the smallest building blocks, which
    in this case correspond to one pass of the model (think unit tests).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下代表您应用程序一次运行的循环，我们在[第4章](ch04.html#ch04_designing_llm_applications_1728407230643376)中讨论过。在传统的软件测试中，尝试测试整个交互（想想回归测试）和最小的构建块（在这种情况下对应于模型的一次遍历，想想单元测试）是有好处的。
- en: Many application workflows have only a single call to the model, so the distinction
    isn’t very meaningful. But for those applications that have large loops using
    iterated calls, you’ll design a test harness by carving out particular parts of
    the loop and declaring “This is what I’m testing now!” You’re not free in that
    choice; particular parts will be hard to test, but the ideal would be to have
    some regression tests that cover as large a part of the feedforward pass of the
    loop as possible and to also have unit tests for every interaction you deem critical
    (i.e., interactions that are hard and important).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序工作流程只对模型有一个调用，所以这种区别并不是很有意义。但对于那些使用迭代调用的大循环应用程序，你会通过划分循环的特定部分并声明“我现在正在测试这个！”来设计一个测试框架。在这个选择上，你没有自由；某些部分很难测试，但理想的情况是，有一些回归测试尽可能覆盖循环的前馈部分的大部分，并且对于你认为是关键的每个交互（即困难和重要的交互）都有单元测试。
- en: Tip
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: In all your tests, record total latency and token consumption statistics. While
    usually not the main focus of evaluation, they are easy to assess, and you’ll
    want to know of any big effects here.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在你所有的测试中，记录总延迟和令牌消耗统计信息。虽然通常不是评估的主要焦点，但它们很容易评估，你将希望了解这里是否有任何重大影响。
- en: 'If you have such a suite of tests, you can use them to assess the different
    components of your app as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一套这样的测试，你可以使用它们来评估应用程序的不同组件，如下所示：
- en: If you’re thinking of swapping out the model or upgrading it, you’ll probably
    want to capture as large a part of the app as possible. You can test each unit
    individually, but going for regression tests that cover a large section of the
    loop is a bit more natural—unless you’re thinking of mixing and matching models
    (e.g., for cost or latency reasons). In that case, looking at each pass in isolation
    makes more sense.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在考虑更换模型或升级它，你可能希望尽可能多地捕获应用程序的一部分。你可以单独测试每个单元，但进行覆盖大量循环部分的回归测试会更为自然——除非你打算混合匹配模型（例如，出于成本或延迟原因）。在这种情况下，单独查看每个循环的通过部分更有意义。
- en: If you want to optimize your prompts or other API parameters like temperature
    or completion length, your main focus should probably be on the small unit tests
    that capture a single pass to the model. After all, that’s what’s affected directly
    by a single prompt change. If your regression tests are powerful enough, you can
    use them as well, but it’s easier for statistical noise to drown out the individual
    effects that would be apparent at the unit level.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想要优化提示或其他API参数，如温度或完成长度，你的主要焦点可能应该是捕获模型单次通过的小型单元测试。毕竟，这是直接受到单个提示变化影响的。如果你的回归测试足够强大，你也可以使用它们，但统计噪声淹没在单元级别可能显现的个别效应更容易。
- en: If you’re tinkering with the overarching architecture of the whole app (e.g.,
    considering changing the overall shape of the loop), then by definition, regression
    tests are what you need to compare different approaches.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在调整整个应用程序的总体架构（例如，考虑改变循环的整体形状），那么根据定义，回归测试是你需要比较不同方法的东西。
- en: In sum, all test setups are useful, but if you have to choose one as the most
    important starting point, it’s probably best to have something that tests the
    whole loop. After all, testing should mirror reality, and in reality, it’s the
    performance of your whole system that you want to optimize for. Once you have
    a harness that covers (close to) the whole loop, you can still add specific tests
    for particularly critical parts of the loop.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，所有测试设置都是有用的，但如果你必须选择一个最重要的起点，可能最好是有一个测试整个循环的东西。毕竟，测试应该反映现实，而在现实中，你想要优化的是整个系统的性能。一旦你有一个覆盖（接近）整个循环的框架，你仍然可以添加针对循环特别关键部分的特定测试。
- en: Offline Evaluation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离线评估
- en: There’s a large range of complexity your offline evaluation suites can have.
    We found it useful to start out with something simple.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你的离线评估套件可以具有很大的复杂性范围。我们发现从简单的东西开始是有用的。
- en: Example Suites
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例套件
- en: 'When you write version 0 of your prompts, you’ll probably have a window open
    with an LLM chat, or you may have a completion playground environment, where you
    try out an example or two. That’s not scalable, but there’s a scalable version
    of it that’s immensely useful: the example suite. An *example suite* has a simple
    setup made out of three components:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当你编写提示的版本0时，你可能会有一个打开的LLM聊天窗口，或者你可能有一个完成游乐场环境，在那里你尝试一个或两个示例。这并不具有可扩展性，但有一个可扩展的版本，它非常有用：示例套件。一个*示例套件*由三个简单组件组成：
- en: A set of 5 to 20 examples of inputs to your application or one of its central
    steps. If possible, these should span the range of scenarios you’ll expect to
    encounter in reality.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组5到20个输入示例，这些示例来自你的应用程序或其核心步骤之一。如果可能的话，这些示例应该涵盖你预期在现实中可能遇到的所有场景。
- en: A script that applies your application’s prompt-making to each of the examples
    and asks the model for the completion, outputting both the assembled prompts and
    the completion as files.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个脚本，它将你的应用程序的提示生成应用于每个示例，并请求模型完成，将组装好的提示和完成内容作为文件输出。
- en: A way to eyeball differences among such files, for example, by committing them
    to your repo and looking at `git diff`s.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，可以通过将它们提交到你的仓库并查看`git diff`来直观地比较这些文件之间的差异。
- en: An example suite is not like a test suite in the software testing sense (although
    it could later evolve into one). You’ll have no automated way of knowing whether
    any change is an improvement or a regression. Instead, you’ll have to go through
    the differences yourself and decide whether you consider them improvements or
    regressions. That’s more of an investment than running a test suite and checking
    the headline result.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 示例套件在软件测试意义上并不像测试套件（尽管它可能后来会演变成为测试套件）。你将没有自动化的方式知道任何变化是改进还是回归。相反，你必须亲自检查差异，并决定你是否认为它们是改进或回归。这比运行测试套件并检查主要结果要投入得多。
- en: But there are two big advantages to this setup. The first is that you can start
    it the moment you codify your very first prompts, before you have any way of assessing
    your output at all. The second is that as you become familiar with these examples,
    you’ll not only see whether a new prompting scheme works or doesn’t work, but
    you’ll also be able to see typical shortcomings in the completion and decide to
    adjust your prompts to address them specifically.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种设置有两个主要优势。第一个优势是，你可以在编码你的第一个提示的瞬间就开始它，在你有任何方式评估输出之前。第二个优势是，随着你对这些示例的熟悉，你不仅会看到新的提示方案是否有效，而且你还能看到完成中的典型不足，并决定调整你的提示以专门解决这些问题。
- en: 'For example, we were working on a project at GitHub regarding [pull request
    (PR) summarization](https://oreil.ly/nIJ1B). PRs are common elements in software
    development in which a developer proposes code changes that a reviewer is supposed
    to check, and we wanted to give them a leg up by summarizing the changes in a
    small number of bullet points. So we took a set of tens of example PRs (mined
    from GitHub), and by eyeballing the summaries, we could see the typical problems
    of our summarizer with different formulations of the prompt. If we thought it
    was too terse, we could quickly add the word detailed to the prompt and immediately
    observe the effect. If we thought it was too verbose, we could ask it to limit
    itself to one or two paragraphs. If it made wild assumptions about the reasons
    motivating the PR, we could ask it to limit itself to describing the functionality.
    In fact, we’d ask it for a paragraph about the functionality and a second paragraph
    about putting the functionality in the context of the project goals, and we’d
    just not surface that second paragraph, using the trick we discussed when talking
    about fluff in [Chapter 7](ch07.html#ch07_taming_the_model_1728407187651669).
    By allowing us to easily compare the effects of different prompts, the example
    suite proved to be an incredibly useful combination: it was systematic enough
    to alert us to the consequences of changes while being flexible enough to provide
    value even before we came up with stringent quality criteria.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们当时在GitHub上有一个关于[拉取请求（PR）摘要](https://oreil.ly/nIJ1B)的项目。PR是软件开发中的常见元素，其中开发者提出代码更改，审阅者应检查这些更改，我们希望通过摘要这些更改为用户提供便利。因此，我们选取了一组数十个示例PR（从GitHub中挖掘而来），通过直观地查看摘要，我们可以看到我们的摘要器在不同提示方案下的典型问题。如果我们认为它太简略，我们可以迅速在提示中添加“详细”这个词，并立即观察到效果。如果我们认为它太冗长，我们可以要求它限制在一到两段之内。如果它对推动PR的原因做出了荒谬的假设，我们可以要求它仅描述功能。实际上，我们会要求它写一段关于功能的内容，另一段关于将功能置于项目目标背景中的内容，我们只是不会展示第二段，使用我们在[第7章](ch07.html#ch07_taming_the_model_1728407187651669)讨论的关于冗余内容的技巧。通过允许我们轻松比较不同提示的效果，示例套件证明是一个极其有用的组合：它足够系统，可以让我们意识到变化的影响，同时足够灵活，即使在制定严格的质量标准之前也能提供价值。
- en: 'Example suites are great for directed exploration, but their scale is limited
    by the number of examples you’re willing to eyeball each time you make a change.
    For subtle effects, however, you’ll want many hundreds of examples, maybe thousands.
    [Figure 10-1](#ch10_figure_1_1728407085447416) illustrates that if you want to
    unlock the statistical power that comes with such a harness, you need to solve
    two problems:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例子套件非常适合有针对性的探索，但它们的规模受限于每次你做出更改时愿意检查的例子数量。然而，对于微妙的影响，你可能需要数百个甚至数千个例子。[图10-1](#ch10_figure_1_1728407085447416)说明了如果你想要解锁这种结合带来的统计力量，你需要解决两个问题：
- en: Where do you get the example problems?
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你从哪里获得例子问题？
- en: How do you assess your app’s solutions to those problems?
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何评估你应用针对那些问题的解决方案？
- en: You can upgrade from playground tinkering to an example suite once you have
    your first code implementation written out. To graduate to an evaluation harness,
    you need lots more examples and a way to automatically assess suggestions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你写出了第一个代码实现，你就可以从游乐场尝试升级到例子套件。要过渡到评估工具，你需要更多的例子以及自动评估建议的方法。
- en: With the word *example*, we’re referring to a particular situation in which
    you might run your app. For simple loops, that’s pretty straightforward—if you
    call the LLM once, then one instance of all the context that could theoretically
    go into the prompt for that one call is an example problem, and what you could
    hope to get out of it (after processing) is the example solution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提到“例子”时，我们指的是你可能运行你的应用的一个特定情况。对于简单的循环，这相当直接——如果你调用LLM一次，那么所有可能进入该单个调用提示的上下文的一个实例就是一个例子问题，而你希望从中获得的结果（经过处理后）就是例子解决方案。
- en: '![A diagram of a diagram of a structure  Description automatically generated
    with medium confidence](assets/pefl_1001.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![一个结构的图解图  描述自动生成，置信度中等](assets/pefl_1001.png)'
- en: Figure 10-1\. The tech tree of offline evaluations
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1\. 离线评估的技术树
- en: 'But by now, we know of more complex, interactive architectures where the LLM
    is called several times, and those calls depend on each other. The most complex
    case probably occurs when a conversation between user and LLM occurs. There are
    two options for evaluating such cases:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 但到目前为止，我们已经知道有更复杂、交互式的架构，其中LLM被调用多次，这些调用相互依赖。最复杂的情况可能发生在用户和LLM之间的对话中。评估此类情况有两种选择：
- en: You give up on evaluating the whole loop, and instead, you evaluate individual
    passes of the conversation. For example, you can use so-called *canned conversations*,
    in which a whole script is written out, and you can evaluate the model on each
    of its conversation passes on how well it performs at that pass. Then, *regardless
    of what the model actually answered*, you can move on to test the next step in
    the conversation by assuming the model had used the answer from the canned conversation
    instead (see [Figure 10-2](#ch10_figure_2_1728407085447448)).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你放弃评估整个循环，而是评估对话的单独轮次。例如，你可以使用所谓的*预定义对话*，其中写出了整个脚本，你可以评估模型在每次对话轮次上的表现如何。然后，*无论模型实际回答了什么*，你都可以通过假设模型使用了预定义对话中的答案来测试对话的下一步（参见[图10-2](#ch10_figure_2_1728407085447448)）。
- en: You can use the model to mock out the user’s side of the conversation. In this
    case, the example consists of a profile of the user, which is a bit like the instructions
    in improv theater. The model will use that profile to emulate that real user.
    This allows you to test the whole loop, at the price of possible model shortfalls
    being baked in—in particular, shortfalls like misunderstandings of the domain
    or prejudices regarding how users are likely to behave. It’s not a perfect method,
    but often, it’s the best you can do.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用模型来模拟用户的对话部分。在这种情况下，例子包括用户的个人资料，这有点像即兴戏剧中的指示。模型将使用该个人资料来模拟真实用户。这允许你测试整个循环，但代价是模型可能存在的缺陷被内化——特别是对领域理解的不准确或对用户可能行为的偏见。这不是一个完美的方法，但通常是你能做的最好的。
- en: '![A screenshot of a chat  Description automatically generated](assets/pefl_1002.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![聊天截图  描述自动生成](assets/pefl_1002.png)'
- en: Figure 10-2\. Canned conversations
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2\. 预定义对话
- en: Finding Samples
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找样本
- en: 'There are three main sources for the many examples you’ll need to find:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要找到许多例子，有三个主要来源：
- en: They already exist, and you just have to find them.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们已经存在，你只需要找到它们。
- en: They’re being created by your project, and you’ll collect them.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是由你的项目创建的，你将收集它们。
- en: You’ll have to make them up entirely.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将不得不完全自己创造它们。
- en: We’ll address each source in turn, starting with what already exists.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依次解决每个来源，首先从已经存在的开始。
- en: Each LLM app solves a specific problem, and that problem (or one of its subproblems)
    may be similar to one that can be mined, because there are many example/solution
    pairs out there. If you’re lucky, the app you have in mind will solve a problem
    that users have solved themselves (without AI assistance) thousands of times and
    that generated records. I recently came across AI assistance in prefilling a summary
    field for an online form. Whoever programmed that feature most probably had records
    of tens of thousands of forms where humans had filled out that summary field for
    themselves. These would have provided a rich source of samples for any evaluation
    harness.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个LLM应用都解决一个特定的问题，这个问题（或其子问题）可能与可以挖掘的问题相似，因为有许多示例/解决方案对。如果你很幸运，你心中的应用程序将解决用户自己（没有AI辅助）成千上万次解决的问题，并生成了记录。我最近遇到了AI辅助在预填充在线表单的摘要字段的情况。编写那个功能的程序员最有可能有数万份记录，其中人类自己填写了那个摘要字段。这些将为任何评估工具提供丰富的样本来源。
- en: 'But very often, what you can mine is only similar, not identical, to the problem
    your app wants to solve. In that case, your source of samples needs to strike
    a balance: find a source of samples that’s ubiquitous enough in real world corpora
    to scale but similar enough to your application problem to allow valid conclusions.
    It’s a stepping stone between the lab and reality.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但非常常见的情况是，你可以挖掘的只是与你的应用想要解决的问题相似，而不是完全相同。在这种情况下，你的样本源需要找到一个平衡点：找到一个在现实世界语料库中普遍存在且足够规模化的样本源，同时与你的应用问题足够相似，以便得出有效的结论。这是实验室和现实之间的一个垫脚石。
- en: For example, the original GitHub Copilot problem is “What will the user want
    to type next?” If GitHub Copilot knows the answer to that question, it can then
    suggest it as grayed-out text while the user is typing. But there are no large-scale
    open source corpora for exactly this question. What does exist, however, is a
    large-scale corpus of open source code in the form of all the repositories on
    GitHub.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，原始的GitHub Copilot问题是“用户接下来会想输入什么？”如果GitHub Copilot知道这个问题的答案，它就可以在用户输入时将其作为灰色文本建议。但是，对于这个问题，并没有大规模的开源语料库。然而，存在的是GitHub上所有仓库中的大规模开源代码语料库。
- en: 'So we chose to generate samples by performing these steps:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们选择通过以下步骤生成样本：
- en: Take an open source repository, and from it, take one code file, and from that
    file, take one function.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个开源仓库，并从中提取一个代码文件，然后从该文件中提取一个函数。
- en: Remove the body of that function, imagining that the user was just writing that
    file and had almost finished writing up everything except the actual implementation
    for this one function, but their cursor is where the implementation would go.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除该函数的主体，想象一下用户刚刚在编写该文件，并且几乎完成了除了这个函数的实际实现之外的所有内容，但他们的光标就在实现应该放置的位置。
- en: Ask GitHub Copilot what to type next.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 询问GitHub Copilot下一步应该输入什么。
- en: 'This is not identical to the actual, real-world problem, for several reasons.
    For one, the distribution is skewed: whole function bodies are longer than the
    typical block suggested by Copilot. For another, any changes to the rest of the
    file that depend on the body (e.g., imports added to the preamble) have already
    happened. None of that is ideal, but it balances with the fact that this is a
    near-infinite well of samples.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不完全等同于实际的、现实世界的问题，原因有几个。首先，分布是偏斜的：整个函数体比Copilot建议的典型块要长。其次，任何依赖于主体的文件其他部分的更改（例如，添加到序言中的导入）已经发生。这些都不是理想的，但与这样一个几乎无限的样本库的事实相平衡。
- en: 'But maybe you have thought about it for a long time and have found no existing
    source of data sufficient for your purpose, neither direct instances of the problem
    the app addresses nor similar cases. So you need a new data source. Here’s the
    good news: you’re currently writing a source of data. The app that you’re building
    is a creator of example cases for its own problem, of course, with new samples
    accumulating as users use the application. That sort of data is as realistic as
    possible, of course, but there are also significant drawbacks:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但也许你已经思考了很长时间，但还没有找到足够满足你目的的现有数据源，既没有该应用解决的问题的直接实例，也没有类似的情况。因此，你需要一个新的数据源。好消息是：你目前正在编写一个数据源。你正在构建的应用程序是其自身问题的示例案例的创造者，当然，随着用户使用应用程序，新的样本会不断积累。当然，这种数据尽可能真实，但也有一些显著的缺点：
- en: Data only starts rolling in once your first prototype has rolled out.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有当您的第一个原型推出后，数据才会开始滚动。
- en: Whenever you make significant updates to your app, there’s a good chance your
    earlier data has become obsolete.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每当您对应用程序进行重大更新时，您的早期数据很可能已经过时。
- en: Recording extensive user telemetry requires very high standards in obtaining
    consent, handling, and safeguarding of the data.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录大量的用户遥测数据需要非常高的标准来获取同意、处理和保护数据。
- en: Application interaction is a source of great example problems (inputs) but not
    necessarily of great example solutions (outputs). Even if you can record what
    action the user actually ended up taking, that will be very heavily influenced
    by the action suggested by your app.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序交互是大量示例问题（输入）的来源，但不一定是大量示例解决方案（输出）的来源。即使您可以记录用户最终采取的实际操作，这也会受到您的应用程序建议的操作的强烈影响。
- en: We’ll see in the following paragraphs that not all evaluations rely on knowing
    which solution is the single right one (also known as *the gold standard solution*).
    In this case, gathering data from application interactions can be worthwhile.
    Otherwise, we’d recommend that you leave telemetry from the app for online evaluation,
    which avoids a few of the problems surrounding data handling and adds some extra
    advantages.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的段落中，我们将看到并非所有的评估都依赖于知道哪个解决方案是唯一正确的（也称为*金标准解决方案*）。在这种情况下，从应用程序交互中收集数据可能是值得的。否则，我们建议您将应用程序的遥测数据保留用于在线评估，这样可以避免一些围绕数据处理的问题，并增加一些额外的优势。
- en: 'So what can you do instead? Well, you can always make stuff up—probably not
    by hand and probably not all by yourself (this whole section is about scale, after
    all), but by now, you’re an experienced AI developer and can ask the LLM to generate
    samples. In some cases, this can work amazingly well. That’s particularly true
    in those cases where you can start with the solution and make up the problem from
    there. Alternatively, if you don’t need a gold standard solution at all, just
    generating situations is something that LLMs excels at. If you go down that route,
    it’s a good idea to go hierarchically, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，您能做些什么呢？好吧，您总是可以编造一些东西——可能不是手动编造，也可能不是完全自己编造（毕竟，这一节是关于规模的），但到目前为止，您已经是一位经验丰富的AI开发者，可以要求LLM生成样本。在某些情况下，这可以非常有效地工作。这在那些可以从解决方案开始并从那里编造问题的案例中尤其如此。或者，如果您根本不需要金标准解决方案，那么生成情况正是LLM擅长的事情。如果您选择这条路，按照以下层次结构进行操作是个好主意：
- en: Either ask the LLM to come up with a list of topics or present one yourself.
    If your problems have several aspects that can be combined, you can make good
    use of the fact that if you have *n* options for aspect A, *m* options for aspect
    B, *l* options for aspect C, and *k* options for aspect D, there are *n* × *m*
    × *l × k* possible combinations. Exploiting combinatorial explosions like that
    can easily give you a very large amount of topics that are well distributed over
    a large space.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要么让LLM提出一个主题列表，要么自己提出一个。如果您的问题有几个可以组合的方面，您就可以充分利用以下事实：如果您有* n *个方面A的选项，* m *个方面B的选项，*
    l *个方面C的选项，以及* k *个方面D的选项，那么就有* n * × * m * × * l × k *种可能的组合。利用这种组合爆炸可以轻松地为您提供大量在广阔空间中分布良好的主题。
- en: If you want more samples than topics, you can still ask the LLM to come up with
    several samples per topic. Provided your context window is long enough to output
    them all, asking for several examples in one go usually leads to a wider variety
    than just asking the LLM repeatedly with a temperature setting greater than 0
    to get several options.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想要的样本比主题多，您仍然可以要求LLM为每个主题生成几个样本。只要您的上下文窗口足够长，可以输出所有这些样本，一次性请求几个示例通常比反复请求LLM（温度设置大于0）以获得多个选项得到更广泛的多样性。
- en: If you’re not sure the LLM has complete command over the problem space, the
    generated examples may well be overly simplistic and exaggerated tropes, may rely
    on popular misunderstandings, or may simply be incorrect. Even more dangerous
    is the incestuous relationship between the LLM that tests and the LLM that comes
    up with the tests—if those two LLMs are one and the same, it biases the outcome.
    For example, if you’re using your test harness to decide whether to switch from
    model A to model B, if all the samples were made up by model A, then chances are
    model A will have a leg up on model B.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定LLM是否完全掌握了问题空间，生成的示例可能过于简单化，可能是夸张的陈词滥调，可能依赖于流行的误解，或者可能只是不正确。更危险的是，测试LLM和提出测试的LLM之间可能存在裙带关系——如果这两个LLM是同一个，那么它将影响结果。例如，如果你正在使用测试框架来决定是否从模型A切换到模型B，如果所有样本都是由模型A生成的，那么模型A可能比模型B有优势。
- en: Each of these approaches to finding samples has advantages and drawbacks, and
    depending on your particular situation, you may settle on one or more of them.
    That will give you lots of samples—maybe with gold standard solutions and maybe
    not. You can run your app on them and get a candidate solution for each of them,
    but now what?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 每种寻找样本的方法都有其优势和劣势，根据你特定的情境，你可能会选择其中一种或多种。这将为你提供大量的样本——可能包含黄金标准解决方案，也可能没有。你可以在这些样本上运行你的应用程序，并为每个样本得到一个候选解决方案，但接下来该怎么办呢？
- en: Evaluating Solutions
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估解决方案
- en: If you want to evaluate possible solutions at scale, there are three main approaches.
    Ordered by difficulty, they are matching the gold standard (whether exact or partial),
    functional testing, and LLM assessment.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要大规模地评估可能的解决方案，主要有三种方法。按照难度排序，它们是匹配黄金标准（无论是完全匹配还是部分匹配）、功能测试和LLM评估。
- en: Gold standard
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 黄金标准
- en: The easiest way, if you can manage it, is matching the gold standard (i.e.,
    an example solution for your example problem that you have some confidence in).
    For example, if you’ve mined historical records, it could be what the human without
    LLM assistance did. Depending on what kind of solution your app offers, this might
    be all you’ll ever need, especially if the solution can be expressed very simply.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能做到，最简单的方法是匹配黄金标准（即，对于你的示例问题的一个示例解决方案，你对它有一定的信心）。例如，如果你挖掘了历史记录，那可能是没有LLM辅助的人类所做的工作。根据你的应用程序提供的解决方案类型，这可能就是你所需要的全部，尤其是如果解决方案可以非常简单地表达。
- en: In the easiest case, your LLM application is supposed to arrive at one single
    yes/no answer in the end, and you have some gold standard data of good decisions.
    Then, all you need to evaluate is to check how often your app’s decision matches
    the gold standard. For example, Albert once worked on an application for unit
    test generation, and the first step in that app’s loop was to ask itself, “Do
    I even need unit tests for this piece of code?” That’s a question with a yes/no
    answer, and it was easy to validate the app’s performance on this step by checking
    how well it matched gold standard solutions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，你的LLM应用程序最终应该得出一个单一的肯定/否定答案，并且你有一些关于良好决策的黄金标准数据。那么，你需要评估的就是检查你的应用程序的决定与黄金标准匹配的频率。例如，Albert曾经为一个单元测试生成应用程序工作，该应用程序的循环中的第一步是询问自己，“我是否需要为这段代码编写单元测试？”这是一个肯定/否定的答案，通过检查它与黄金标准解决方案的匹配程度，可以很容易地验证应用程序在这个步骤上的性能。
- en: Tip
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Evaluation of binary decisions or multilabel classification using gold standards
    can just be counting how often the model gets it right. But if you crave more
    statistical power, you can use logprobs as discussed in [Chapter 7](ch07.html#ch07_taming_the_model_1728407187651669).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用黄金标准对二元决策或多标签分类进行评估，只需统计模型正确率出现的频率即可。但如果你渴望更多的统计能力，可以使用第7章中讨论的logprobs。
- en: 'However, very often, the output of an LLM is more or less free-form text. You
    can use exact match counts here, tallying how often your app produces a candidate
    solution that is verbatim the same as the gold standard. But the greater the degree
    of freedom you have, and in particular, the longer the model’s answer is, the
    rarer exact matches will be, even for great models. At some point, the chance
    to get an exact match is so low that the metric becomes more or less meaningless.
    Even before that point, it raises a question: what are you optimizing for, correct
    solutions or solutions that are formulated in a particular style?'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，非常常见的是，一个LLM的输出或多或少是自由形式的文本。你可以在这里使用精确匹配计数，统计你的应用程序产生与黄金标准完全相同的候选解决方案的频率。但随着自由度的增加，尤其是模型答案的长度增加，精确匹配将变得更加罕见，即使是对于优秀的模型也是如此。在某个点上，获得精确匹配的机会变得如此之低，以至于这个指标几乎失去了意义。甚至在那个点之前，它就提出了一个问题：你是在优化正确答案，还是优化以特定风格表述的答案？
- en: 'That’s where partial match metrics can be useful. They work by picking out
    one particularly important aspect of the solution and matching only on that. For
    example, if the LLM is supposed to write source code for you, you may want to
    ignore comments, blank lines, or (depending on the language) even all whitespace.
    Therefore, you choose the partial match metric “exact match after deleting all
    comment lines and removing all whitespace.” If the LLM is supposed to suggest
    travel destinations, you might want to match on the country of destination but
    ignore all other details the model gives you: that’s another partial match metric.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是部分匹配指标可以发挥作用的地方。它们通过挑选出解决方案的一个特别重要的方面，并仅在该方面进行匹配来实现。例如，如果LLM应该为你编写源代码，你可能希望忽略注释、空白行，或者（根据语言）甚至所有空白字符。因此，你选择部分匹配指标“删除所有注释行并移除所有空白字符后的精确匹配”。如果LLM应该建议旅行目的地，你可能希望匹配目的地国家，但忽略模型给出的所有其他细节：这也是另一个部分匹配指标。
- en: 'All partial match metrics come with a hard choice: you need to figure out which
    aspect of the solution you really care about. That’s easier said than done, because
    in most applications, a catastrophic failure in any aspect of the solution can
    theoretically invalidate the whole thing. But some modes of failure are more likely,
    so you can guard against those.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所有部分匹配指标都伴随着一个艰难的选择：你需要确定你真正关心解决方案的哪个方面。这说起来容易做起来难，因为在大多数应用中，解决方案任何方面的灾难性失败理论上都可能使整个解决方案无效。但某些失败模式更有可能发生，因此你可以防范这些情况。
- en: 'Let’s work through an example. Imagine you’re writing a smart home manager.
    You’re looking at a situation where the user says, “I’m chilly” (see [Figure 10-3](#ch10_figure_3_1728407085447471)).
    You’ve already determined a gold standard solution for this: the system could
    set the temperature to 77ºF, and that would be perfect. Checking for a partial
    match might consist of checking for only whether the manager regulates the right
    system (in this case, heating). It’s sensible to check for that because there’s
    (probably) a real chance the manager doesn’t react by adjusting the heating system,
    and that is likely to be a real failure on its part. On the other hand, if the
    manager does adjust the heating system, it’s likely that it will adjust it to
    something sensible, whether it’s exactly 77ºF or not. The manager could set the
    temperature to 0ºF, of course, but that’s a less likely failure case when compared
    with the possibility of the system not understanding at all that it’s supposed
    to regulate heating or not knowing how exactly to regulate heating. So, it makes
    sense to test for setting *any* temperature, rather than testing for the exact
    temperature the model should set.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来分析。想象一下你正在编写一个智能家居管理器。你面对的情况是用户说，“我感到冷”（见[图10-3](#ch10_figure_3_1728407085447471)）。你已经为这种情况确定了一个黄金标准解决方案：系统可以将温度设置为77°F，这将非常完美。检查部分匹配可能只包括检查管理器是否调节了正确的系统（在这种情况下，加热）。检查这一点是有道理的，因为（可能）管理器没有通过调整加热系统来做出反应，这很可能是它的一部分真正失败。另一方面，如果管理器确实调整了加热系统，那么它很可能会将其调整到合理的温度，无论是正好77°F还是其他温度。当然，管理器可以将温度设置为0°F，但与系统完全不理解它应该调节加热，或者不知道如何确切地调节加热的可能性相比，这更不太可能是一个真正的失败案例。因此，测试设置任何温度，而不是测试模型应该设置的精确温度，是有意义的。
- en: 'Generally, it’s best to evaluate on an aspect that meets these two criteria:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，最好在满足以下两个标准的一个方面进行评估：
- en: The aspect is good at distinguishing between breaking and benign divergence
    from the gold standard solution. This makes the evaluation meaningful or valid.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方面擅长区分从黄金标准解决方案中出现的破坏性和良性分歧。这使得评估变得有意义或有效。
- en: The aspect isn’t too specific; if it were, the LLM would have little chance
    of getting it right. The aspect also isn’t too general; if it were, the evaluation
    would be meaningless.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方面并不太具体；如果太具体，LLM正确获取它的机会就很小。该方面也不太泛泛；如果太泛泛，评估就没有意义。
- en: Both criteria require you to play around with the model a bit to see where some
    typical mistake patterns lie and just how bad those mistakes are. Unfortunately,
    there’s some circularity being introduced here because you choose the test based
    on what your LLM or setup is *currently* good at, and you’ll use that evaluation
    to guide its *future* development. But that’s still much better than choosing
    a weak or misleading aspect for your evaluation framework.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个标准都需要你与模型玩一会儿，看看一些典型的错误模式在哪里，以及这些错误有多严重。不幸的是，这里引入了一些循环性，因为你根据你的LLM或当前设置选择测试，然后使用那个评估来指导其未来的发展。但这仍然比选择一个弱或误导性的方面作为你的评估框架要好得多。
- en: 'If the LLM doesn’t return purely free-form completions, testing an especially
    critical one out of the several fields it contains is often a good aspect to focus
    on for partial match metrics. In particular, that holds for applications that
    are heavy on tool use: you can check on whether the right tool is used and maybe
    whether it’s called with the right syntax (see [Figure 10-3](#ch10_figure_3_1728407085447471)).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果LLM不返回纯自由形式的完成，那么在它包含的几个字段中测试特别关键的一个，通常对于部分匹配指标来说是一个值得关注的方面。特别是对于工具使用较多的应用来说，这一点尤为重要：你可以检查是否使用了正确的工具，也许还可以检查是否使用了正确的语法（参见[图10-3](#ch10_figure_3_1728407085447471)）。
- en: 'Checking on whether the model uses the right tool is also an instance of following
    another bit of general advice: when the model makes several decisions in a row
    while outputting its tokens one by one, it makes sense to evaluate the first decision
    that’s got a real chance of going wrong (and to invalidate later decision points).
    In [Figure 10-3](#ch10_figure_3_1728407085447471), the model first decides to
    use any tool at all by beginning with `to=functions.`, then commits to the specific
    tool `set_room_temp`, and then settles on the particular values `{“temp”: 77}`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '检查模型是否使用了正确的工具，这也是遵循另一条一般性建议的一个例子：当模型在输出其标记的同时连续做出几个决策时，评估第一个有真正出错可能性的决策（以及使后续决策点无效）是有意义的。在[图10-3](#ch10_figure_3_1728407085447471)中，模型首先通过以`to=functions.`开始来决定是否使用任何工具，然后承诺使用特定的工具`set_room_temp`，然后确定特定的值`{"temp":
    77}`。'
- en: '![A screenshot of a computer  Description automatically generated](assets/pefl_1003.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  自动生成的描述](assets/pefl_1003.png)'
- en: Figure 10-3\. Testing that the right tool is called with the right syntax
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3\. 检查是否以正确的语法调用了正确的工具
- en: In the figure, note that if the `set_room_temp` tool isn’t called correctly
    (as on the left), the suggestion is most likely useless. If the tool is called
    correctly but in a different way than in the gold standard solution (as on the
    right), the chance that the suggestion is reasonable (as on the upper right) is
    still substantial.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，请注意，如果`set_room_temp`工具调用不正确（如图左侧所示），建议很可能毫无用处。如果工具调用正确，但与黄金标准解决方案中的方式不同（如图右侧所示），那么建议合理（如图右上角所示）的机会仍然很大。
- en: Functional testing
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 功能测试
- en: 'What if you don’t have a gold standard solution or can’t easily compare it
    to your app’s solution? One option is *functional testing*: taking the completion
    and confirming that certain things “work” with it. For example, you can count
    how often the LLM gives you a completion that you can parse, calls only the functions
    and tools you have available (and with arguments of the right types), etc. In
    most applications, that is too weak, but occasionally, you can go pretty far with
    functional testing.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有黄金标准解决方案或者无法轻松地将它与你的应用程序解决方案进行比较，怎么办？一个选择是*功能测试*：获取完成内容并确认某些事情“工作”得如何。例如，你可以计算LLM给你提供可解析的完成次数，只调用你拥有的函数和工具（以及正确的类型参数）等。在大多数应用中，这可能太弱了，但偶尔，你可以通过功能测试走得很远。
- en: 'Let’s look again at the Copilot evaluation framework as an example of such
    functional testing. The evaluation framework would simulate cases where Copilot
    was used to reimplement a function from an open source repository, and then it
    would check whether the unit test suite from that repository still passes with
    the Copilot-suggested alternative source code. (A weaker version would check that
    linters agree with the code.) The idea is to use a particularity of code: there
    are (often) unit tests that you can execute, and the code comes with its own functional
    test already present. On the other hand, in some domains, you might not be able
    to construct any functional tests you can execute programmatically. But there
    is one last arrow a prompt engineer such as yourself always has in their quiver:
    the model itself.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次以Copilot评估框架为例，看看这种功能测试。评估框架将模拟使用Copilot从开源仓库重新实现一个函数的情况，然后它会检查该仓库的单元测试套件是否仍然可以通过Copilot建议的替代源代码。
    (较弱的版本会检查代码检查器是否与代码一致。) 这个想法是利用代码的一个特性：通常有可以执行的单元测试，代码本身也自带了功能测试。另一方面，在某些领域，你可能无法构建任何可以程序执行的函数测试。但作为像你这样的提示工程师，你最后还有一支箭在你的箭袋中：模型本身。
- en: LLM assessment
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM评估
- en: The quality of a natural language response to a problem is often a wooly, hard-to-pin-down
    affair. If the LLM outputs a number, you can easily compare it with the gold standard;
    if the LLM outputs a classification, you can directly compare strings to determine
    accuracy; and if the LLM outputs a program, you can run unit tests.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对问题的自然语言回答的质量通常是一个模糊的、难以确定的事情。如果LLM输出一个数字，你可以很容易地将其与黄金标准进行比较；如果LLM输出一个分类，你可以直接比较字符串以确定准确性；如果LLM输出一个程序，你可以运行单元测试。
- en: But if the LLM outputs a textual answer to a question, how do you measure how
    *friendly* and how *helpful the response is*? Fortunately, such evaluations are
    where LLMs shine, and you can use them to assess the response. On the other hand,
    maybe this isn’t a good idea—after all, it was probably the very same LLM that
    produced the response that we’re evaluating, and now, you’re asking it to grade
    its own work to see how good it is. Isn’t that a bit like giving a high schooler
    an assignment to write an essay and then asking them to grade their own work?
    The answer is *no*—at least not if you do it right.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果LLM对一个问题的回答是文本形式，你如何衡量这个回答的*友好性*和*帮助性*呢？幸运的是，这种评估正是LLM擅长的领域，你可以用它来评估回答。另一方面，也许这不是一个好主意——毕竟，可能正是产生我们正在评估的回答的同一个LLM，现在你要求它对自己的工作进行评分，看看它有多好。这难道不是有点像给高中生布置一篇论文写作的作业，然后让他们自己评分一样吗？答案是*不*——至少如果你做得正确的话。
- en: Warning
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Even though the questions to the LLM are often formulated as absolute quality
    questions (e.g., “Is this correct?”), an LLM assessment a priori serves only as
    a relative quality judgement (e.g., “Version A is considered right more often
    than version B.”). You may get assessments like “The LLM judges the application
    to be correct in 81% of cases,” and on their own, these carry little meaning.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对LLM的问题通常被表述为绝对质量问题（例如，“这是否正确？”），但LLM的评估在先验上仅作为相对质量判断（例如，“版本A比版本B更常被认为是正确的。”）。你可能会得到这样的评估，“LLM在81%的情况下判断应用程序是正确的”，单独来看，这些评估几乎没有意义。
- en: If you want to correctly use an LLM to assess its own work, then you shouldn’t
    let the LLM think it’s grading its own work. Assessments are a kind of advice
    conversation, and as you already know from [Chapter 6](ch06.html#ch06a_assembling_the_prompt_1728442733857948),
    advice conversations work best when the model thinks it’s grading a third party.
    In fact, while models get a bit less accurate when they think they’re being asked
    to grade the user compared with when they think they’re being asked to grade a
    third party, they usually get much worse when they think they’re being asked to
    grade themselves, because they’re suddenly subject to a host of conflicting biases.
    Most models’ training data includes a good chunk of forum discussions (or even
    comments), which aren’t exactly known for objective self-reflection. On the other
    hand, if the model is subject to RLHF, then to please its human evaluators, it
    often learns to veer toward the other extreme, falling over itself to correct
    its output on even the slightest expression of user doubt. Even if a model manages
    to strike a balance on average, being pulled in different directions isn’t conducive
    to it providing an objective analysis.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要正确使用大型语言模型来评估其自身的工作，那么你不应该让大型语言模型认为它在给自己打分。评估是一种建议性的对话，正如你从[第6章](ch06.html#ch06a_assembling_the_prompt_1728442733857948)中已经了解到的，建议性的对话在模型认为它在给第三方打分时效果最好。事实上，当模型认为它被要求评估用户而不是第三方时，它们的准确性会略有下降，但它们认为自己在给自己打分时通常会变得更差，因为它们突然受到众多相互冲突的偏差的影响。大多数模型的训练数据包括大量的论坛讨论（甚至评论），而这些讨论并不以客观的自我反思而闻名。另一方面，如果模型受到强化学习与人类反馈的强化（RLHF）的影响，那么为了取悦其人类评估者，它通常会学会偏向另一个极端，甚至在用户对输出的最轻微怀疑下就过度纠正其输出。即使模型设法在平均意义上达到平衡，被拉向不同方向并不利于它提供客观的分析。
- en: SOMA Assessment
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SOMA评估
- en: Another good way to optimize the LLM for assessment is to try to use what we’ll
    call a *SOMA assessment*, which consists of specific questions (S), ordinal scaled
    answers (O), and multiaspect coverage (MA). Let’s talk about each of the parts
    of SOMA assessment.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 优化大型语言模型进行评估的另一种好方法是尝试使用我们所说的*SOMA评估*，它由具体问题（S）、序列缩放答案（O）和多方面覆盖（MA）组成。让我们来谈谈SOMA评估的各个部分。
- en: Specific questions
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 具体问题
- en: 'There are tasks in which verifying a solution is much easier than coming up
    with one. For example, it’s hard to invent a limerick on the spot, but it’s easy
    to confirm whether a given poem fulfills the criteria for being a limerick. If
    your application task happens to be one of them, you might get away with asking
    “Is this right?” But in most cases, there’s little information gained from a generic
    assessment like that. In [Figure 10-3](#ch10_figure_3_1728407085447471), we had
    the example of a smart home system reacting to the user voicing “I’m a little
    chilly,” with `to=functions.set_room_temp {“temp”: 77}.` Answering the question,
    “Is the completion right?” is not much easier than coming up with the completion
    in the first place. In fact, the answer to the assessment might well be worse
    than the original generation, because there are several ways to interpret it.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '有一些任务，验证解决方案比提出解决方案容易得多。例如，即兴创作一首打油诗很难，但确认一首给定的诗是否符合打油诗的标准却很容易。如果你的应用任务恰好属于这类任务，你可能只需问“这是正确的吗？”就能过关。但在大多数情况下，这种通用的评估几乎得不到任何信息。在[图10-3](#ch10_figure_3_1728407085447471)中，我们有一个智能家居系统对用户说出“我有点冷”的反应的例子，`to=functions.set_room_temp
    {"temp": 77}`。回答“这个完成度是否正确？”并不比最初提出完成度更容易。事实上，评估的答案可能比原始生成的内容还要差，因为有多种方式可以解释它。'
- en: Ordinal scaled answers
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列缩放答案
- en: One such ambiguity is that it’s unclear how good a completion would need to
    be to be “right.” It’s no good if the standards that an individual answer is held
    to depend on the model’s capriciousness and the next answer is held to a different
    standard. It’s even worse if, instead of a random effect, there’s a systematic
    bias, such as the model holding answers trying for more accuracy to higher standards
    or accepting generally OK answers (that are more than 50% correct) while rejecting
    almost perfect answers (that are not *completely* right).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种歧义是，不清楚一个完成度需要多好才能被认为是“正确”的。如果对个体答案的标准取决于模型的反复无常，而下一个答案则有不同的标准，那就没什么好处。如果，不是随机效应，而是存在系统性偏差，例如模型对试图提高准确度的答案持有更高的标准，或者接受一般可接受的答案（正确率超过50%），而拒绝几乎完美的答案（不是*完全*正确），那就更糟了。
- en: The solution is to ditch yes/no answers in the first place and ask the model
    to rate the completion on an ordinal scale, where it’s not only easier to convey
    nuance but also to obtain consistent measurements by communicating the meaning
    of these numbers. For example, if you ask the model to rate on a scale from 1
    to 5,^([1](ch10.html#id1159)) you can add a description or examples for each of
    these levels, as shown in [Example 10-1](#ch10_example_1_1728407085466768).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是首先放弃是/否的答案，并要求模型在序数尺度上对完成情况进行评级，这样不仅更容易传达细微差别，而且通过传达这些数字的意义可以获得一致的测量。例如，如果你要求模型在1到5的尺度上进行评级，^([1](ch10.html#id1159))
    你可以为每个级别添加描述或示例，如[示例10-1](#ch10_example_1_1728407085466768)所示。
- en: Multi-aspect coverage
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多方面覆盖
- en: But “How good is good?” is far from the only source of ambiguity in getting
    an answer to a question like “Is the completion right?” If you think of different
    completions for “I’m a little chilly,” the model might focus sometimes on the
    question of whether the suggested room temperature is correct, sometimes on the
    question of whether the assistant should ask before changing the temperature,
    and sometimes on whether `set_room_temp` is the right function to use. Such inconsistencies
    are pretty bad if you want to use the model assessment in a systematic way.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 但“多好才算好？”并不是回答像“完成是否正确？”这样的问题中模糊性的唯一来源。如果你考虑“我有点冷”的不同完成方式，模型有时可能会关注建议的房间温度是否正确的问题，有时会关注助手在改变温度之前是否应该询问的问题，有时会关注是否应该使用`set_room_temp`函数。如果你想要系统地使用模型评估，这些不一致性是非常糟糕的。
- en: 'The remedy here is to control these multiple aspects explicitly: instead of
    asking the model how good a suggestion is and praying it’s always using the same
    criterion to judge goodness, you can prepare in advance a couple of categories
    to judge the model on and ask the model to rate the suggestion in each category.
    For the smart home assistant above, the categories could be as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是明确控制这些多个方面：而不是询问模型一个建议有多好，并祈祷它总是使用相同的标准来判断好坏，你可以提前准备几个类别来评估模型，并要求模型在每个类别中对建议进行评级。对于上述智能家居助手，这些类别可能如下所示：
- en: Whether the completion succeeded in implementing the action the model intended
    (making the correct choice of tool called with correct syntax)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否成功实现了模型意图中的动作（使用正确的语法调用正确的工具）
- en: Whether that action remedied the user’s problem at hand (being chilly)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该行为是否解决了用户当前的问题（感到寒冷）
- en: Whether the model was sufficiently restrained from doing something crazy without
    asking and sufficiently assertive to not need too much hand-holding
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否在没有询问的情况下足够克制地避免做一些疯狂的事情，并且足够自信，不需要过多的指导
- en: Then, instead of asking one question, you’re asking three, and you’re either
    adding up the scores or looking for more complex patterns.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你不再只问一个问题，而是问三个问题，并且要么是将分数相加，要么是在寻找更复杂的模式。
- en: Tip
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Remember to spell out the fact that you’re doing an assessment and which aspects
    to grade on, before showing the example to the model. After all, the LLM can’t
    backtrack and can read through the text only once. If the question precedes the
    example to evaluate, then when the LLM reads through the example, it does so with
    the evaluation framework already in mind, and it can focus on the right aspects.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在向模型展示示例之前，记得说明你正在进行评估以及要评估哪些方面。毕竟，大型语言模型无法回溯，并且只能一次性阅读文本。如果问题在评估示例之前，那么当LLM阅读示例时，它已经带着评估框架在心中，并且可以专注于正确的方面。
- en: 'When you’re choosing these aspects for grading an application, it’s important
    to choose the right ones. One common approach is to focus on the aspects of intent
    and execution:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当你选择这些方面来评估一个应用程序时，选择正确的是非常重要的。一种常见的方法是关注意图和执行方面的内容：
- en: Did the model have the right intent? For example, is turning up the heat to
    77ºF really the solution to the user’s problem?
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否有正确的意图？例如，将温度调至77°F真的是解决用户问题的方案吗？
- en: Did the model correctly execute upon that intent? For example, did the model
    use the correct tools and tool-calling syntax?
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否正确地执行了该意图？例如，模型是否使用了正确的工具和工具调用语法？
- en: For example, you can ask chat applications offering advice to the user whether
    the advice addressed the right things. If the user asked for things not to miss
    when visiting Morocco, you can ask the app whether it actually provided the intended
    sightseeing information (rather than telling the user not to miss their flight)
    in a complete way (rather than only listing the best cafes). Also, you can ask
    the application whether the advice was actually correct. These aspects form the
    basis for the relevance-truth-completeness (RTC)^([2](ch10.html#id1160)) system
    that was originally developed for scoring chat conversations by GitHub Copilot.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以询问为用户提供建议的聊天应用，该建议是否解决了正确的问题。如果用户询问访问摩洛哥时不要错过的事情，你可以询问该应用是否实际上以完整的方式提供了预期的观光信息（而不是告诉用户不要错过航班），而不是只列出最好的咖啡馆。此外，你可以询问该应用是否提供的建议实际上是正确的。这些方面构成了最初为
    GitHub Copilot 评分聊天对话而开发的关联-真实性-完整性（RTC）^([2](ch10.html#id1160)) 系统的基础。
- en: Warning
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'You should break apart any Goldilocks questions that ask whether a completion
    was “just right.” Those questions really capture two aspects: it was enough, and
    it wasn’t too much. You typically get cleaner results if you ask these questions
    separately.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该分解任何询问完成度是否“恰到好处”的 Goldilocks 问题。这些问题实际上捕捉了两个方面的内容：它足够了，而且不是太多。如果你单独提出这些问题，通常会得到更干净的结果。
- en: SOMA mastery
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SOMA 精通
- en: Taking it all together, a SOMA assessment asks specific questions on an ordinal
    scale covering multiple aspects, like in [Example 10-1](#ch10_example_1_1728407085466768).
    SOMA acts like a guardrail by defining the evaluation task so precisely that the
    model has no choice but to be objective in its assessments…at least we hope. But
    how can you be sure that it works? How do you choose your questions, aspects,
    and descriptions of the ordinal options correctly, and how can you be sure they
    didn’t just go over the model’s head?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 综合考虑，SOMA 评估在多个方面上对有序尺度上的具体问题进行提问，就像在 [示例 10-1](#ch10_example_1_1728407085466768)
    中那样。SOMA 通过精确地定义评估任务，就像一个护栏一样，使得模型在评估时没有选择但必须是客观的……至少我们希望如此。但你怎么能确定它有效呢？你如何正确选择你的问题、方面和有序选项的描述，以及如何确保它们没有只是从模型头上飘过？
- en: Example 10-1\. Asking the LLM to rate one of the chosen aspects
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-1\. 要求 LLM 对选定的某个方面进行评级
- en: '[PRE0]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The answer is that you should ground your model evaluation in human evaluation.
    The good thing about the model is that it scales, while people don’t (Elastigirl
    notwithstanding). Therefore, using LLMs to assess their own performance is basically
    a replacement for using human annotators, and you want to make sure that you suffer
    no substantial regression by doing that. You could let a human annotate some cases
    and compare, but all you’ll find out is that there’s some amount of disagreement
    between the human and the model—and that’s normal. Humans disagree, too, so what
    you actually need to do is let *several* humans answer the questions. Then, you
    need to confirm that the disagreement among this pool of human assessors (measured
    through some standard method like [Kendall’s Tau](https://oreil.ly/y0Lvm)) remains
    stable if you add the model (queried once, at temperature 0) to the pool.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，你应该将你的模型评估建立在人工评估的基础上。模型的好处是它可以扩展，而人不行（不考虑 Elastigirl）。因此，使用 LLM 来评估其自身性能基本上是使用人工标注员的替代品，你想要确保通过这样做不会出现实质性的退化。你可以让一个人标注一些案例并比较，但你只会发现人和模型之间有一些不一致——这是正常的。人类也会产生分歧，所以你实际上需要做的是让
    *多个* 人类回答问题。然后，你需要确认，如果你将模型（一次查询，温度为 0）添加到这个评估者群体中，通过某种标准方法（如 [Kendall 的 Tau](https://oreil.ly/y0Lvm)）测量的这种群体中的不一致性保持稳定。
- en: The following lists summarize the offline evaluation choices. Note that for
    offline evaluation, you need a source for inputs and a test for outputs. These
    lists include the main kinds with what we consider the most critical question;
    if you can’t find a way to answer with yes, then you can’t use that row.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表总结了离线评估选项。请注意，对于离线评估，你需要一个输入源和一个输出测试。这些列表包括我们认为最关键的几个主要种类；如果你找不到用“是”来回答的方法，那么你无法使用那一行。
- en: 'Pick one source:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个来源：
- en: Existing records
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现有记录
- en: Can you find plenty of them?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你能找到很多吗？
- en: App usage
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 应用使用
- en: Is the data trickle fast enough (also considering old data invalidation through
    app changes)?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流动是否足够快（同时考虑通过应用更改使旧数据无效）？
- en: Synthetic examples
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 合成示例
- en: Are you willing to spend the time crafting the synthesis procedure?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你愿意花时间精心制作合成过程吗？
- en: 'Pick one test:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个测试：
- en: Ground truth match
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 真实匹配
- en: Is a (complete or partial) match realistic and meaningful?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: （完整或部分）匹配是否现实且有意义？
- en: Functional test
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 功能测试
- en: Can you isolate a critical aspect that can be automatedly assessed?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你能否隔离出一个可以自动评估的关键方面？
- en: LLM assessment
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型评估
- en: Are good and bad outputs recognizably different (by people, say)?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 良好和不良的输出是否可以明显区分（例如，通过人）？
- en: Online Evaluation
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线评估
- en: 'All of the methods in the previous section are at least a bit artificial. They
    test the model’s performance in the lab, not in real life. There are three advantages
    of assessing your app in the lab:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节中介绍的所有方法至少在某种程度上都是人为的。它们测试的是模型在实验室中的性能，而不是在实际生活中的表现。在实验室中评估你的应用程序有三个优点：
- en: The lab is safe, and if you mess up in there, no one will know.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验室是安全的，如果你在那里出了差错，没有人会知道。
- en: The lab scales much better, so you can try out more ideas more quickly.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验室的可扩展性更好，因此你可以更快地尝试更多想法。
- en: The lab exists before your app ever ships, so you can start assessing earlier.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验室在你应用程序发布之前就已经存在，因此你可以更早地开始评估。
- en: But, as Opus told us in their famous song, “Life is live,” and it’s hard to
    beat that. If you run an app in real life, then you’ve got actual users in the
    loop, and application performance with users is the ultimate test of whether the
    application has true merit.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，正如 Opus 在他们著名的歌曲中所说的，“生活是真实的”，很难打败这一点。如果你在实际生活中运行应用程序，那么你就有实际的用户参与其中，并且应用程序在用户中的表现是衡量应用程序是否真正有价值的终极测试。
- en: A/B Testing
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A/B 测试
- en: 'The standard way to learn from users is through *A/B testing*: you ship two
    (or another small number) of alternatives—let’s call them A and B—to see which
    performs best. Normally, one of those alternatives will be the status quo, and
    the other will be your modification you want to assess. Hopefully, you’ve already
    performed offline evaluation of the alternatives, to whittle down the number of
    possibilities to test and also to avoid putting some real stinkers in front of
    your users. You define in advance which metrics to look for that you want to optimize;
    often, they are proxies for user satisfaction (e.g., average rating, acceptance
    rate). You may also define some guardrail metrics that you don’t want to increase;
    often, they are proxies for catastrophic failures (e.g., errors, complaints).
    Then, a random selection of users gets the app running in mode A, and the rest
    gets the app running in mode B. You let the experiment run for a while, collect
    the metrics you’ve decided on, and see whether A or B is better. Then, you roll
    out the winning alternative to all users.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户那里学习标准的途径是通过 *A/B 测试*：你发布两个（或另一个小数量）的替代方案——让我们称它们为 A 和 B——以查看哪个表现最好。通常，其中之一将是现状，另一个将是你要评估的修改。希望你已经对替代方案进行了离线评估，以减少要测试的可能性数量，并避免将一些真正的糟糕产品展示给用户。你事先定义你想要优化的指标；通常，它们是用户满意度的代理（例如，平均评分，接受率）。你还可以定义一些你不想增加的指标；通常，它们是灾难性失败的代理（例如，错误，投诉）。然后，随机选择的一组用户运行模式
    A 的应用程序，其余的用户运行模式 B 的应用程序。你让实验运行一段时间，收集你决定的指标，并查看 A 或 B 哪个更好。然后，你将获胜的替代方案推广给所有用户。
- en: Tip
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Online evaluation typically has less bandwidth than offline evaluation. You
    have only a finite number of users, and getting a signal can take some time, so
    be deliberate in which ideas you test online.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在线评估通常比离线评估的带宽要少。你只有有限数量的用户，获取信号可能需要一些时间，因此在在线测试哪些想法时要谨慎。
- en: 'A/B tests aren’t unique to LLM applications, and there are plenty of established
    solutions for taking care of assigning experiment groups to users or sessions
    as well as taking care of statistical analysis. These solutions include Optimizely,
    VWO, and AB Tasty, and they all rely on your app being able to run in two modes:
    alternative A and alternative B. For example, if A is your current prompt-engineering
    logic and B is a new prompting idea you want to try out, then you need your app
    to be able to perform either, depending on some flag being set by the A/B testing
    setup. If your app runs client-side, that means that you need to roll out an update
    with the new prompting idea to all (most^([3](ch10.html#id1168))) users before
    you can even start testing it. That rollout time introduces another significant
    reason why A/B experimentation often moves slower than offline evaluation.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: A/B 测试并不仅限于 LLM 应用，有许多成熟的解决方案用于处理将实验组分配给用户或会话以及进行统计分析。这些解决方案包括 Optimizely、VWO
    和 AB Tasty，它们都依赖于你的应用程序能够在两种模式下运行：替代方案 A 和替代方案 B。例如，如果 A 是你当前的提示工程逻辑，而 B 是你想要尝试的新提示想法，那么你需要你的应用程序能够根据
    A/B 测试设置的某些标志执行其中之一。如果你的应用程序在客户端运行，这意味着你需要向所有（大多数^([3](ch10.html#id1168))) 用户推出带有新提示想法的更新，然后你才能开始测试它。这种推出时间引入了另一个重要的原因，为什么
    A/B 实验通常比离线评估慢得多。
- en: To achieve successful online evaluation, your most important initial goal must
    be to determine which metric(s) you want to optimize for. That’s what determines
    how you decide which alternative is “better.” Let’s review an earlier example
    about an application that suggests travel destinations. A user in group A gets
    the suggestion “Monaco,” and a user in group B gets the suggestion “Chicago.”
    What signal should you listen for to be able to say whether those were great suggestions
    or bad ones? To answer this question, let’s get an overview of the possible metrics.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现成功的在线评估，你最重要的初始目标必须是确定你想要优化哪些指标。这决定了你如何决定哪个替代方案是“更好的”。让我们回顾一个关于建议旅行目的地的应用的早期例子。A
    组的用户收到建议“摩纳哥”，而 B 组的用户收到建议“芝加哥”。你应该如何倾听哪些信号来判断这些建议是好是坏？为了回答这个问题，让我们先了解一下可能的指标。
- en: Metrics
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指标
- en: 'There are five main kinds of metrics. From most to least straightforward, they
    are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 主要有五种类型的指标。从最直接到最不直接，它们如下：
- en: 'Direct feedback: what does the user say to the suggestion?'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接反馈：用户对建议说了什么？
- en: 'Functional correctness: does the suggestion work?'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 功能正确性：建议是否有效？
- en: 'User acceptance: does the user follow the suggestion?'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户接受度：用户是否遵循建议？
- en: 'Achieved impact: how much does the user benefit?'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 达成的效果：用户得到了多少好处？
- en: 'Incidental metrics: what are the measurements “around” the suggestion?'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 伴随指标：建议“周围”的测量是什么？
- en: We’ll explain them all in turn, starting with direct feedback (see [Figure 10-4](#ch10_figure_4_1728407085447490)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐一解释它们，从直接反馈（见图 10-4）开始。
- en: '![A screenshot of a chat  Description automatically generated](assets/pefl_1004.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![聊天截图  描述自动生成](assets/pefl_1004.png)'
- en: Figure 10-4\. Two different ways in which ChatGPT elicits direct feedback
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. ChatGPT 获取直接反馈的两种不同方式
- en: 'Have you noticed how regularly ChatGPT asks its users for their opinion? It
    makes sense: conversations are hard to evaluate, and who better than the user
    to assess them? A small thumbs-up or thumbs-down button next to answers gives
    the user a quick way to voice their delight—or an outlet for their frustration.
    The latter is more common, and typically, it’s also a reliable signal: thumbs-ups,
    if optional, are normally not given for solid performance, but only for particular
    brilliance, and that dilutes the signal. (Maybe that’s why OpenAI stopped displaying
    thumbs-up buttons for single conversation items.)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有注意到 ChatGPT 多次向用户征求他们的意见？这是有道理的：对话很难评估，谁比用户更能评估它们？在答案旁边的小点赞或点踩按钮为用户提供了一种快速表达满意或不满的方式。后者更为常见，而且通常也是一个可靠的信号：如果点赞是可选的，通常不会因为表现良好而点赞，而只会因为特别出色，这会稀释信号。（也许这就是为什么
    OpenAI 停止显示单个对话项的点赞按钮。）
- en: ChatGPT occasionally goes one step further than classical A/B testing toward
    *contrastive A/B testing*, asking, “Which one of these two suggestions is better?”
    That leads to possibly the clearest signal, but it’s also extra intrusive—and
    all requests for direct feedback are pretty intrusive already. If your app is
    an assistant (like ChatGPT) that people seek out and communicate with very deliberately,
    you can get away with that, but no one wants their smart home system to constantly
    ask, “And how was it for you?” each time it adjusts one of the lights.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT偶尔会比经典的 A/B 测试更进一步，进行*对比 A/B 测试*，问：“这两个建议中哪一个更好？”这可能导致可能最清晰的信号，但它也更为侵入性——而所有直接反馈的请求都已经相当侵入性了。如果你的应用程序是一个人们有意识地寻求并与之交流的助手（如ChatGPT），你可以这样做，但没有人希望他们的智能家居系统每次调整灯光时都不断问：“你觉得怎么样？”
- en: In many applications, feedback is more valuable if it’s delayed—it’s one thing
    for the user to appreciate the vacation suggestion of Chicago and be skeptical
    of the idea of going to Monaco, but if you’re aiming to provide proper value to
    the user, it’s even more valuable to learn, after the fact, that the suggested
    trip to Chicago kicked ass and the trip to Monaco sucked.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用程序中，如果反馈延迟，则更有价值——用户可能会欣赏芝加哥的度假建议，对去摩纳哥的想法持怀疑态度，但如果你旨在为用户提供适当的价值，那么在事后了解建议的芝加哥之旅很成功，而摩纳哥之旅很糟糕，就更有价值了。
- en: Tip
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The data you gather using direct feedback is usually very high quality—in addition
    to using it for evaluation, you can use it as training data for model fine-tuning.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用直接反馈收集的数据通常质量很高——除了用于评估之外，你还可以将其用作模型微调的训练数据。
- en: 'Metrics testing for *functional correctness* emphasizes the more objective
    parts of an LLM application: the app tried to do something, but did it work? Sometimes,
    you can easily check at least a partial aspect: the code compiles—and that’s good
    (although the code might not actually do the right thing); you get a ticket confirmation—and
    that’s good (although you might not have booked the right destination). At other
    times, signals for functional correctness are much more concrete and certain,
    especially for smaller subtasks in a greater routine. You wanted to open a program,
    but is it running? You wanted to send an email, but is it in your outbox?'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 功能正确性测试的指标强调了一个 LLM 应用程序中更客观的部分：应用程序试图做某事，但它是否成功了？有时，你可以轻松地检查至少部分方面：代码编译了——这是好的（尽管代码可能实际上没有做正确的事情）；你得到了机票确认——这是好的（尽管你可能没有预订正确的目的地）。在其他时候，功能正确性的信号更加具体和确定，尤其是在更大常规中的较小子任务中。你想要打开一个程序，但它是否正在运行？你想要发送一封电子邮件，但它是否在你的发件箱中？
- en: 'If you can’t assess the suggestion directly, most applications can check whether
    the *user accepts* them or at least takes steps to accept them—for example, did
    the user end up booking a trip to Chicago? Sometimes, that’s as direct as click-through
    rate: if your suggestion contains a link, how often do users click on it? That
    affirms only that a suggestion looked promising, not that it was actually useful,
    but quite often, a reasonable first start is actually the most important thing.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不能直接评估建议，大多数应用程序可以检查用户是否接受它们，或者至少采取了接受它们的步骤——例如，用户是否最终预订了去芝加哥的旅行？有时，这就像点击率一样直接：如果你的建议包含一个链接，用户点击它的频率是多少？这只能证实建议看起来很有希望，并不能证明它实际上是有用的，但很多时候，一个合理的起点实际上是最重要的。
- en: 'That turned out to be [our finding](https://oreil.ly/qwR21) for Copilot, when
    we found that acceptance metrics correlated more strongly with the productivity
    gains reported by the user than with more sophisticated *measurements of impact*.
    Those are related signals that try to assess the same question: did the user find
    the suggestion helpful? But they come from the other direction by looking at the
    final outcome. Here, you find metrics like “In the end, how much of the email
    was written by the assistant?” or “When the user clicked on the suggested travel
    destination, did they actually end up buying a ticket?”'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证明，这是我们对 Copilot 的[发现](https://oreil.ly/qwR21)，当我们发现接受度指标与用户报告的生产力提升相关性更强，而不是与更复杂的*影响测量*。这些是相关的信号，试图评估相同的问题：用户是否觉得建议有帮助？但它们从另一个方向来看，关注最终结果。在这里，你会找到像“最终，助手写了多少电子邮件？”或“当用户点击建议的旅行目的地时，他们实际上是否真的购买了机票？”这样的指标。
- en: Finally, each application comes with a gaggle of *incidental metrics* that measure
    relevant aspects, but not necessarily with a unique relationship to “goodness.”
    In interactive scenarios, the most important one of these will be latency, even
    though a lightning-fast suggestion can still be worthless and a more measured
    one can still be worthwhile. Conversational assistants typically also track conversation
    time, even though it’s far from clear in general whether a short conversation
    is good (i.e., the problem is solved instantly, and the user is completely satisfied)
    or bad (i.e., the assistant showed its incompetence from the start, user rage-quits).
    Typically, it’s better to track more incidental metrics than fewer, both as rough
    indicators of quality (e.g., you might have some idea that long conversations
    are often better) and to prompt an investigation into any unexpected changes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，每个应用都附带一系列*偶然性指标*，这些指标衡量相关方面，但并不一定与“良好”有独特的关系。在交互场景中，这些中最重要的是延迟，尽管闪电般的建议仍然可能毫无价值，而更加谨慎的建议仍然可能是有价值的。对话助手通常也会跟踪对话时间，尽管在一般情况下，是否短的对话是好的（即问题立即解决，用户完全满意）或坏的（即助手从一开始就表现出无能，用户愤怒退出）并不总是很清楚。通常，跟踪更多的偶然性指标比跟踪更少的指标更好，这不仅作为质量的大致指标（例如，你可能有一些想法，认为长对话通常更好），而且可以促使调查任何意外的变化。
- en: 'And there you have it: lots of different ideas to choose from. It’s worth spending
    some time investigating which kinds of metrics you can collect for your use case
    and how confident you are in their value. The most likely case, and where you
    should start looking first, will be an acceptance or impact metric. If you don’t
    find one that you can be confident of, you’ll have to ask for direct feedback.
    But even then, you’ll probably keep some acceptance or impact metrics as guardrails,
    monitoring that they don’t regress, and you’ll likely keep some functional correctness
    and incidental metrics (particularly latency and errors) as well.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，你就有很多不同的想法可以选择。花些时间去调查你可以为你的用例收集哪些类型的指标，以及你对这些指标价值的信心程度是值得的。最可能的情况，也是你应该首先开始寻找的情况，将是一个接受度或影响指标。如果你找不到一个你可以确信的，你将不得不要求直接反馈。但即使如此，你可能会保留一些接受度或影响指标作为安全线，监控它们不要退步，并且你可能会保留一些功能正确性和偶然性指标（尤其是延迟和错误）。
- en: Conclusion
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Evaluation is an important subject, but it’s difficult, due to the many knobs
    you can twiddle. Does your offline evaluation get its examples through existing
    records and historical app usage, or does it make them up synthetically? Do you
    test them by comparing them with a gold standard, do you automatically check their
    functionality, or do you assess them using the LLM itself? Does your online evaluation
    track user feedback, functional correctness, acceptance rates, or impact? And
    which incidental metrics do you add?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 评估是一个重要的主题，但由于你可以调整的许多旋钮，它很难。你的离线评估是通过现有的记录和历史应用使用来获取示例，还是通过合成它们？你是通过将它们与黄金标准进行比较来测试它们，自动检查它们的功能，还是使用LLM本身来评估它们？你的在线评估跟踪用户反馈、功能正确性、接受率或影响吗？你添加了哪些偶然性指标？
- en: The perfect choice is different for each application. But what is always true
    is that evaluation is essential for the continued development of your app, and
    any time spent on this area is time well spent.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 完美的选择因应用而异。但始终不变的是，评估对于你应用的持续发展至关重要，在这个领域花费的时间都是值得的。
- en: ^([1](ch10.html#id1159-marker)) [Research from psychometrics](https://oreil.ly/quHu8)
    indicates that 5 is a pretty good default, in fact.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#id1159-marker)) [心理测量学的研究](https://oreil.ly/quHu8)表明，5是一个相当好的默认值，事实上。
- en: '^([2](ch10.html#id1160-marker)) Lizzie Redford, [“Machine Psychometrics: Design
    & Validation Principles for LLM Self-Evaluation”](https://oreil.ly/vR9ud)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#id1160-marker)) Lizzie Redford，[“机器心理测量学：LLM自我评估的设计与验证原则”](https://oreil.ly/vR9ud)
- en: ^([3](ch10.html#id1168-marker)) You can’t just say, “Let’s put the users who
    already updated in group B (‘the new thing’) and the others in group A (‘the status
    quo’),” because users who update quickly usually behave differently from users
    who update less regularly. What you can do is say, “Let’s test only users who
    have already updated, and those who haven’t updated are members of neither group
    A or nor group B; they simply don’t take part in the analysis.”
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.html#id1168-marker)) 你不能只是说，“让我们把已经更新的用户放在B组（‘新事物’）中，而其他用户放在A组（‘现状’）中，”因为快速更新的用户通常与更新不那么频繁的用户行为不同。你可以做的是说，“让我们只测试已经更新的用户，而那些尚未更新的用户既不属于A组也不属于B组；他们根本不参与分析。”
