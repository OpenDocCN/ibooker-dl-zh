- en: Chapter 6\. AI Engineering for Limited-Risk AI Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapter translated Articles 9 to 15 of the EU AI Act into actionable
    specifications for high-risk AI systems within the CRISP-ML(Q) lifecycle, providing
    practical checklists to support responsible development and management. It also
    introduced the concept of AI engineering in the context of EU AI Act compliance.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter shifts focus to the Act’s requirements for limited-risk AI systems,
    with particular attention to transparency obligations. It explores how organizations
    can address these requirements proactively through MLOps. See [Figure 6-1](#chapter_6_figure_1_1748539923579859)
    for a visual of the steps to take to move toward compliance with the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. This chapter focuses on the requirements for limited-risk AI systems
    and the operationalization of compliance for such systems. See [Chapter 1](ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819)
    for an explanation of the end-to-end process steps toward EU AI Act compliance.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The two guiding questions for this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: To comply with the EU AI Act, what requirements must limited-risk AI systems
    fulfill?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What processes, structures, and AI engineering practices need to be established
    to comply with the Act with regard to transparency obligations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you may have noticed, I mention both compliance and transparency obligations
    here. Let’s examine the difference between these two concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Compliance Assessment Versus Transparency Obligation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A compliance or conformity assessment is a mandatory process for high-risk AI
    systems to ensure they meet the EU AI Act’s requirements, often involving third-party
    evaluation. Transparency obligations are less strict and are a broader requirement
    for all AI systems interacting directly with humans. Let’s quickly take a closer
    look at each before diving deeper into the transparency obligations laid out in
    the Act.
  prefs: []
  type: TYPE_NORMAL
- en: A conformity assessment is a formal process by which a provider demonstrates
    that an AI system meets the requirements outlined in Chapter III, Section 2 of
    the EU AI Act. This process ensures compliance with legal obligations and may
    involve a third-party evaluation by a notified body (an independent organization
    designated by Member States to assess high-risk AI systems). The primary objective
    is to verify that the system conforms to EU regulatory standards before it is
    placed on the market.
  prefs: []
  type: TYPE_NORMAL
- en: '[Article 50 of the EU AI Act](https://oreil.ly/cc73j) introduces transparency
    obligations that apply to all AI systems intended to interact directly with humans,
    regardless of their risk classification. These obligations do not require a formal
    assessment but instead aim to prevent deception or harm by imposing specific duties
    on providers and deployers to inform individuals that they are interacting with
    an AI system.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 6-1](#chapter_6_table_1_1748539923588699) compares conformity assessments
    and transparency obligations under the EU AI Act.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Key differences between conformity assessments and transparency
    obligations
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | Conformity assessment | Transparency obligation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Applicability scope | High-risk AI systems only | AI systems interacting
    with humans, broadly applicable |'
  prefs: []
  type: TYPE_TB
- en: '| Main goal | Ensure compliance with technical and legal requirements | Promote
    user awareness and prevent deception |'
  prefs: []
  type: TYPE_TB
- en: '| Nature | Formal process, which requires third-party assessment | Informative
    and clarity-focused |'
  prefs: []
  type: TYPE_TB
- en: '| Process | Technical evaluations and compliance checks | Labeling, information
    disclosure |'
  prefs: []
  type: TYPE_TB
- en: '| Responsibility | Providers | Providers and deployers |'
  prefs: []
  type: TYPE_TB
- en: '| Outcome | Certificate and EU declaration of conformity | Awareness and understanding
    for users |'
  prefs: []
  type: TYPE_TB
- en: '| Relation to other laws | Part of EU harmonization law, when applicable |
    In addition to other national or EU transparency laws |'
  prefs: []
  type: TYPE_TB
- en: In sum, a conformity assessment is a rigorous, technical process aimed at ensuring
    that high-risk AI systems are safe and fully compliant with the EU AI Act, while
    transparency obligations are designed to ensure that users are aware they are
    interacting with an AI system, regardless of its risk classification. In this
    chapter, we’ll explore what those obligations entail and how they can be addressed
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Transparency Obligations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As I explained in [Chapter 4](ch04.html#chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657),
    the term “limited risk” is not explicitly defined in the EU AI Act. Instead, the
    Act outlines three main categories: prohibited AI practices under the unacceptable
    risk category (Article 5), high-risk AI systems (Article 6), and AI systems with
    specific transparency obligations (Article 50).'
  prefs: []
  type: TYPE_NORMAL
- en: “Limited-risk” AI systems are generally understood to be those that are not
    explicitly prohibited or high risk but are still subject to transparency requirements
    when they interact directly with humans. Article 50 sets out the transparency
    obligations for both providers and deployers of AI systems. These obligations
    apply to *all AI systems intended to interact directly with natural persons*,
    regardless of their risk level. [Table 6-2](#chapter_6_table_2_1748539923588741)
    summarizes the key provisions.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. Overview of transparency obligations for providers and deployers
    of AI systems according to Article 50 of the EU AI Act
  prefs: []
  type: TYPE_NORMAL
- en: '| Transparency obligation | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Informing users of AI interaction | Providers must inform users when they
    are interacting with an AI system, unless it’s reasonably obvious or the system
    is used for law enforcement purposes (e.g., detecting, preventing, investigating,
    or prosecuting crimes). This applies to chatbots and content-generating tools.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Marking synthetic content | Providers of AI systems that generate synthetic
    content (audio, images, video, text) must clearly mark these outputs as artificially
    generated or manipulated. The labels should be machine-readable and easily detectable,
    signaling the content’s non-authentic nature. This requirement does not apply
    to assistive editing tools or systems that do not significantly alter the original
    input. |'
  prefs: []
  type: TYPE_TB
- en: '| Disclosing deepfakes | Providers of AI systems that generate deepfakes (described
    by Article 3(60) as “AI-generated or manipulated image, audio or video content
    that resembles existing persons, objects, places, entities or events and would
    falsely appear to a person to be authentic or truthful”) must clearly label these
    outputs as artificial. The labels should be machine-readable and easily detectable.
    This obligation does not apply to systems that provide standard editing functions,
    do not significantly alter the original data, or are used for law enforcement
    purposes. |'
  prefs: []
  type: TYPE_TB
- en: '| Additional transparency measures for emotion recognition and biometric categorization
    | Deployers of emotion recognition and biometric systems must clearly inform users
    about how these technologies work and how their data is used, with particular
    attention to accessibility for individuals with disabilities. Note that meeting
    the Act’s transparency requirements does not ensure compliance with other legal
    standards, such as the GDPR. |'
  prefs: []
  type: TYPE_TB
- en: Article 50 requires that AI systems intended to interact directly with natural
    persons be properly designed to support that interaction, as well as mandating
    that individuals be clearly informed that they are engaging with an AI system.
    This information should be provided at the start of the interaction. For instance,
    an AI system with a chat interface triggers this transparency obligation, and
    users should be informed of its artificial nature the first time they engage with
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some additional examples of systems that require disclosure:'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems like Jasper AI or Voice.ai that create or modify content (audio,
    images, videos, or text) must clearly label their outputs as artificially created
    or altered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI systems that generate deepfakes must clearly state that the content is artificially
    created or manipulated. This is in addition to the general requirement to mark
    all synthetic content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emotion recognition and biometric categorization systems like Affectiva (by
    Smart Eye), which analyze facial expressions or biometric data to infer emotions
    or classify individuals, require deployers to inform users about the system’s
    operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI systems that generate or manipulate text, such as social media content bots,
    AI-powered news writing systems like Narrative Science, and large language models
    (e.g., OpenAI’s GPT models) that produce news articles, financial updates, weather
    reports, and other public-interest content must disclose that the content was
    artificially generated or manipulated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to Article 50, all disclosures of this type must be clear, easily
    noticeable, and accessible to all users. General site-wide disclaimers are insufficient.
    The disclosure must be made before the actual exposure to the content in question.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have an idea of what the key transparency obligations look like,
    let’s turn our attention to how AI engineering practices can support effective
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Aligning AI Engineering with SMACTR and CRISP-ML(Q) for Transparency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed earlier, a conformity assessment is a formal process that requires
    a third-party audit. However, this audit is typically conducted after deployment,
    when the system might already have negatively impacted users. Inioluwa Deborah
    Raji et al. address this in their widely cited paper [“Closing the AI Accountability
    Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing”](https://oreil.ly/9RVsg),
    in which they propose an internal audit framework designed to guide the practical
    implementation of ethical AI development: SMACTR (Scoping, Mapping, Artifact Collection,
    Testing, and Reflection). SMACTR provides a structured approach for organizations
    to ensure that their AI systems align with their stated ethical principles and
    values. The framework is intended to be used throughout the entire AI system development
    lifecycle, not just at a single point in time, and it aids organizations in preparing
    effectively for third-party conformity audits.'
  prefs: []
  type: TYPE_NORMAL
- en: The Five Stages of the SMACTR Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by examining the five stages of the SMACTR framework:'
  prefs: []
  type: TYPE_NORMAL
- en: Scoping
  prefs: []
  type: TYPE_NORMAL
- en: Clarifies the objectives of the audit by reviewing the motivations and intended
    impact of the system and confirming the principles meant to guide system development.
    This stage also involves mapping out use cases and identifying analogous deployments
    to anticipate potential harm.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping
  prefs: []
  type: TYPE_NORMAL
- en: Connects the documents produced during the Scoping phase to the stakeholders
    that will be involved in testing and reviewing. FMEA should begin in this stage
    (for more on FMEA, see [“Failure Mode and Effects Analysis”](ch05.html#chapter_5_failure_mode_and_effects_analysis_1748539922577786)).
  prefs: []
  type: TYPE_NORMAL
- en: Artifact Collection
  prefs: []
  type: TYPE_NORMAL
- en: Gathers key documents and evidence needed to support ethical and technical evaluation,
    including an ethical review of the system’s use case and a social impact assessment.
    Prerequisite documents from system and engineering teams typically include a declaration
    of ethical objectives and a product requirements document (PRD).
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs: []
  type: TYPE_NORMAL
- en: Involves a thorough audit of the model’s performance, considering both technical
    and ethical aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Reflection
  prefs: []
  type: TYPE_NORMAL
- en: Focuses on reviewing the entire development process and identifying opportunities
    for improvement in the future.
  prefs: []
  type: TYPE_NORMAL
- en: As visualized in [Figure 6-2](#chapter_6_figure_2_1748539923579896), each stage
    has its own set of documentation requirements, which help ensure the framework
    supports accountability and transparency.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-2\. Overview of the SMACTR framework (source: [*https://oreil.ly/9RVsg*](https://oreil.ly/9RVsg))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Table 6-3](#chapter_6_table_3_1748539923588765) summarizes the purpose, key
    activities, and expected outcomes of each stage, as outlined in the paper introducing
    the framework.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-3\. The stages of the SMACTR framework
  prefs: []
  type: TYPE_NORMAL
- en: '| SMACTR stage | Purpose | Activities | Outcomes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Scoping | Clarify the audit’s objective by reviewing the AI system’s motivations
    and intended impact. Define the project’s scope and the ethical principles that
    should guide its development. |'
  prefs: []
  type: TYPE_TB
- en: Review the product or request document specifying the AI system’s requirements
    and expectations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the intended use cases and potential areas of harm or social impact.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Map out analogous deployments either within the organization or from competitors
    to anticipate potential issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confirm the ethical principles and values that are meant to guide product development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a risk analysis centered on the failure to achieve the ethical principles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| A clear understanding of the audit’s objectives, the intended impact of the
    AI system, and the ethical guidelines that should be followed. Key documentation
    to create includes AI system (project) scope documents, initial risk assessments,
    and ethical review reports. |'
  prefs: []
  type: TYPE_TB
- en: '| Mapping | Identify all relevant stakeholders to ensure that the audit considers
    the perspectives and interests of all parties involved, including users, developers,
    and other affected groups. |'
  prefs: []
  type: TYPE_TB
- en: Identify all relevant stakeholders who may be affected by the AI system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the potential impact of the system on each stakeholder group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| A comprehensive stakeholder map and a clear understanding of the potential
    impacts of the AI system on different groups. |'
  prefs: []
  type: TYPE_TB
- en: '| Artifact Collection | Gather all relevant documentation and data related
    to the development and deployment of the AI system. This information is essential
    for the auditors to thoroughly evaluate the system and identify potential risks
    and ethical concerns. |'
  prefs: []
  type: TYPE_TB
- en: Collect project-related documents, technical specifications, and relevant data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gather information about data sources, data preparation steps, model training
    details, and evaluation results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gather documentation related to the system’s development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| A complete set of documents, data, and other artifacts related to the AI
    system, which facilitates analysis. This documentation may include data quality
    assessments, model cards, training logs, and performance reports. |'
  prefs: []
  type: TYPE_TB
- en: '| Testing | Evaluate the AI system’s performance and identify any potential
    issues or risks that may arise during its operation. This goes beyond standard
    technical testing to include ethical implications and biases. |'
  prefs: []
  type: TYPE_TB
- en: Conduct technical tests to assess the model’s accuracy, robustness, and reliability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the model’s fairness and identify any potential biases that may lead
    to discriminatory outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess the model’s performance under various conditions, including edge cases
    and potential adversarial scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use both technical metrics and ethical considerations for evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| A thorough understanding of the system’s performance, limitations, and potential
    risks, focusing on both technical and ethical aspects. This includes performance
    reports and bias detection analysis. |'
  prefs: []
  type: TYPE_TB
- en: '| Reflection | Analyze the results from the previous phase and evaluate them
    in light of the original scoping goals. This step focuses on summarizing findings,
    assessing how well ethical risks were addressed, and proposing improvements for
    the future. |'
  prefs: []
  type: TYPE_TB
- en: Evaluate the outcomes of the Testing stage with regard to the project’s original
    scoping goals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify any ethical principles that may be jeopardized when the AI system is
    deployed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the test results, formulate recommendations for further mitigating
    identified risks and ensuring responsible use of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop a risk mitigation plan in collaboration with engineering teams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| An algorithmic use-related risk analysis, including a risk mitigation plan
    that addresses the identified failures and potential risks. This will include
    a reflection on the ethical implications of deploying the AI system. |'
  prefs: []
  type: TYPE_TB
- en: How the SMACTR Framework Aligns with the EU AI Act
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SMACTR framework is intended for pre-deployment auditing and can be used
    to support compliance with the EU AI Act. Its emphasis on embedding ethical considerations
    throughout the AI development lifecycle directly aligns with the Act’s objectives
    and requirements. Let’s look at some of the key areas of alignment:'
  prefs: []
  type: TYPE_NORMAL
- en: Proactive and preventative auditing
  prefs: []
  type: TYPE_NORMAL
- en: 'The SMACTR framework promotes a proactive approach to ethical AI development
    by integrating audit processes into the design and development phases. This is
    in contrast with traditional external audits, which often occur after deployment,
    when risks may have already caused harm. This preventive orientation aligns well
    with the EU AI Act’s requirements for risk assessment and mitigation: by using
    SMACTR, AI engineers can anticipate and address potential risks before they become
    problems. While the framework wasn’t developed specifically for the EU AI Act,
    its focus on “urgent governance,” which emphasizes auditing not only for system
    reliability but also for societal harm, makes it a good fit for the Act’s goal
    of protecting fundamental rights.'
  prefs: []
  type: TYPE_NORMAL
- en: Emphasis on transparency and documentation
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR produces structured documentation at each stage, forming a comprehensive
    audit trail. This aligns with the EU AI Act’s requirements for comprehensive technical
    documentation and traceability, particularly for high-risk AI systems. The documentation
    that is generated, including artifacts such as model cards and datasheets, supports
    information sharing along the AI value chain and can serve as evidence of due
    diligence in addressing ethical concerns and meeting regulatory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Alignment with ethical principles and values
  prefs: []
  type: TYPE_NORMAL
- en: The EU AI Act is grounded in the concept of trustworthy AI and builds on the
    ethical guidelines developed by the High-Level Expert Group on AI. The SMACTR
    framework is designed to help organizations align their AI systems with their
    stated ethical principles and values and enables them to evaluate how well the
    systems uphold those principles in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Risk assessment integration
  prefs: []
  type: TYPE_NORMAL
- en: The Scoping stage of the framework involves a structured risk analysis that
    goes beyond technical reliability, mapping use cases and identifying potential
    sources of harm and social impact. Tools such as FMEA and anticipatory “What if…?”
    questions support this process. This broader risk-based approach, which incorporates
    ethical and societal considerations, aligns well with the EU AI Act’s emphasis
    on comprehensive risk management.
  prefs: []
  type: TYPE_NORMAL
- en: Accountability and responsibility
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR explicitly promotes accountability by defining roles and responsibilities
    throughout the AI development lifecycle. This aligns with the EU AI Act’s emphasis
    on traceability, transparency, and explainability and defined obligations for
    providers, deployers, and other stakeholders. The Mapping stage of the framework,
    in particular, can help establish an internal record of individual accountability.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the SMACTR framework is not only a tool for ethical AI development
    but also a practical mechanism for supporting compliance with the EU AI Act. Its
    focus on proactive auditing, detailed documentation, alignment with ethical principles,
    integrated risk assessment, and accountability makes it especially suitable for
    guiding AI engineering teams in meeting the Act’s regulatory requirements. The
    framework can be implemented in full or in a streamlined form, depending on the
    desired level of rigor. It can also be adapted to different risk levels and specific
    system requirements, making it applicable across a wide range of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Crucially, SMACTR can be integrated with AI engineering practices to ensure
    that ethical and transparency considerations are addressed throughout the AI system
    lifecycle. By integrating SMACTR with the CRISP-ML(Q) methodology, AI engineers
    can embed transparency and compliance measures in all stages of development.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections outline how to integrate SMACTR into the CRISP-ML(Q)
    lifecycle and provide a step-by-step engineering guide to help organizations meet
    transparency obligations under the EU AI Act. For each phase of the lifecycle,
    I’ll start by examining the relevant transparency requirements, then describe
    the corresponding CRISP-ML(Q) activities, show how SMACTR can be applied, and
    identify the key artifacts that support compliance and traceability.
  prefs: []
  type: TYPE_NORMAL
- en: Business and Data Understanding Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the business and data understanding phase, several transparency requirements
    need to be addressed to ensure that users are aware when they are interacting
    with an AI system and that synthetic content is clearly identified. Key requirements
    in this phase include:'
  prefs: []
  type: TYPE_NORMAL
- en: Document the intended purpose of the AI system
  prefs: []
  type: TYPE_NORMAL
- en: Clearly document the intended purpose of the AI system, specifying what it is
    designed to do and the problem it is meant to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Define interaction points with humans
  prefs: []
  type: TYPE_NORMAL
- en: Identify all points where the AI system will interact directly with humans.
    This includes user interfaces, chatbots, or any other mechanism where a natural
    person will engage with the system.
  prefs: []
  type: TYPE_NORMAL
- en: Identify synthetic content generation capabilities
  prefs: []
  type: TYPE_NORMAL
- en: Determine whether the AI system can generate or manipulate audio, image, video,
    or text content. If so, document this capability, as any synthetic output must
    be clearly marked.
  prefs: []
  type: TYPE_NORMAL
- en: Plan notification mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: Develop a plan for notifying users that they are interacting with an AI system.
    This may involve implementing visible notifications, clear labeling, and other
    accessible methods.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) activities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the business and data understanding phase, the following CRISP-ML(Q) activities
    are crucial:'
  prefs: []
  type: TYPE_NORMAL
- en: Define scope and success criteria
  prefs: []
  type: TYPE_NORMAL
- en: Clearly define the project’s goals and the criteria for success. This includes
    identifying measurable outcomes and establishing KPIs for the AI system.
  prefs: []
  type: TYPE_NORMAL
- en: Assess data availability and quality
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the data available for training and validation, considering its quantity,
    relevance, accuracy, and completeness.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate feasibility
  prefs: []
  type: TYPE_NORMAL
- en: Assess the technical and operational feasibility of the project to determine
    whether its goals are achievable with the available resources and technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Design data collection strategy
  prefs: []
  type: TYPE_NORMAL
- en: If there’s not enough existing data, design a strategy for collecting additional
    data. Specify how new data will be sourced, stored, and managed.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The SMACTR framework enhances the CRISP-ML(Q) process by integrating ethical
    considerations and auditability from the beginning. The Scoping, Mapping, and
    Artifact Collection stages are relevant to the business and data understanding
    phase. Here are the key tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Scoping (ethical review of system use case)
  prefs: []
  type: TYPE_NORMAL
- en: Conduct an ethical review of the AI system’s intended use. This involves considering
    who might be affected by the system and what the potential social impacts might
    be.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping (stakeholder identification)
  prefs: []
  type: TYPE_NORMAL
- en: Identify all internal and external stakeholders who will be involved in or affected
    by the AI system. This could include developers, product managers, end users,
    and individuals whose rights may be impacted.
  prefs: []
  type: TYPE_NORMAL
- en: Artifact Collection (initial documentation gathering)
  prefs: []
  type: TYPE_NORMAL
- en: Begin gathering initial documentation for the AI system. This includes project
    proposals, requirements documents, and statements of ethical objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Key artifacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By the end of the business and data understanding phase, the following key
    artifacts should be produced to meet the requirements of the EU AI Act:'
  prefs: []
  type: TYPE_NORMAL
- en: Project scope document
  prefs: []
  type: TYPE_NORMAL
- en: A document that clearly defines the project’s objectives, scope, and success
    criteria. This should include details about what the AI system aims to achieve
    and the boundaries of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical review report
  prefs: []
  type: TYPE_NORMAL
- en: A report that outlines the ethical implications of the AI system, including
    potential risks and harms, and documents whether the system aligns with a set
    of ethical values or principles.
  prefs: []
  type: TYPE_NORMAL
- en: Stakeholder map
  prefs: []
  type: TYPE_NORMAL
- en: A visual representation of all stakeholders, showing their roles and relationships
    to the AI system. This map should clarify participant dynamics and provide context
    for interpreting the final audit report.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality assessment
  prefs: []
  type: TYPE_NORMAL
- en: A report assessing the availability, quality, and suitability of the data for
    the project. This document should identify any data gaps or potential biases that
    need to be addressed in the next phase.
  prefs: []
  type: TYPE_NORMAL
- en: Initial transparency requirements document
  prefs: []
  type: TYPE_NORMAL
- en: A document that outlines the preliminary transparency requirements based on
    Article 50\. This includes interaction points with humans, any synthetic content
    generation, and planned notification mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 6-4](#chapter_6_table_4_1748539923588788) summarizes the content of
    these artifacts.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-4\. Summary of key artifacts produced during the business and data understanding
    phase
  prefs: []
  type: TYPE_NORMAL
- en: '| Artifact | Content |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Project scope document |'
  prefs: []
  type: TYPE_TB
- en: Project objectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Success criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ethical review report |'
  prefs: []
  type: TYPE_TB
- en: Impact assessment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risk analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigation strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethics board recommendations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Stakeholder map |'
  prefs: []
  type: TYPE_TB
- en: Involved parties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roles and responsibilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communication paths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision authority levels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data quality assessment (assessment criteria) |'
  prefs: []
  type: TYPE_TB
- en: Completeness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timeliness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Initial transparency requirements document |'
  prefs: []
  type: TYPE_TB
- en: System purpose declaration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human interaction points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthetic content capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notification mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance checklist
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the data preparation phase, the focus is on guaranteeing that the data used
    to train the AI system is well documented and auditable. This is crucial for transparency,
    accountability, and enabling deployers and users to understand how the AI system
    makes decisions. Here’s how the transparency requirements of Article 50 translate
    into practical steps in this phase:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement data lineage tracking
  prefs: []
  type: TYPE_NORMAL
- en: Establish a system for tracking the origin and data flows. This includes recording
    where the data comes from, any transformations it undergoes, and how it is used.
    This is important to ensure that the data used by the system can be traced back
    to its original source, which can help to identify and correct potential issues.
    Technically, this requires components such as source system tracking, transformation
    history, and version control integration.
  prefs: []
  type: TYPE_NORMAL
- en: Document data transformations
  prefs: []
  type: TYPE_NORMAL
- en: All transformations applied to the data, such as cleaning, normalization, or
    aggregation, must be clearly documented. The documentation should describe what
    transformations were applied, why they were applied, and their impact on the data.
  prefs: []
  type: TYPE_NORMAL
- en: Establish audit trail
  prefs: []
  type: TYPE_NORMAL
- en: Create a robust audit trail by logging all data preparation activities, including
    who made the changes, when they were made, and what specific changes were implemented.
    An audit trail ensures that there is a record of all data preparation steps, which
    is essential for maintaining accountability and demonstrating compliance with
    transparency requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Create metadata management system
  prefs: []
  type: TYPE_NORMAL
- en: Develop a system for managing metadata about the data, such as its format, type,
    source, and any associated quality metrics. A well-structured metadata management
    system is key for understanding the data and its characteristics and enables better
    data governance. It also supports the traceability and reusability of ML assets,
    such as data, features, and models.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) activities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The key activities for a structured approach to data preparation within machine
    learning projects during this phase include:'
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning and validation
  prefs: []
  type: TYPE_NORMAL
- en: Identify and correct any errors or inconsistencies in the data. This includes
    handling missing values, correcting typos, and ensuring data conforms to the defined
    standards.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: Create new features from the existing data that are relevant to the modeling
    task. This might involve transforming existing variables, creating interaction
    terms, or extracting other meaningful information to enhance model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation
  prefs: []
  type: TYPE_NORMAL
- en: Convert the data from its raw format into a suitable format for machine learning
    algorithms. This may involve normalizing numerical data or encoding categorical
    data, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Data documentation
  prefs: []
  type: TYPE_NORMAL
- en: Record the characteristics of the data including its sources, data types, and
    limitations, and document any issues or biases present in the data.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Artifact Collection, Testing, and Reflection stages of the SMACTR framework
    can be integrated into all the remaining CRISP-ML(Q) phases. Here are the key
    activities for the data preparation phase:'
  prefs: []
  type: TYPE_NORMAL
- en: Artifact Collection (data documentation)
  prefs: []
  type: TYPE_NORMAL
- en: Create detailed documentation about the data and all processing steps. This
    should go beyond basic recordkeeping.
  prefs: []
  type: TYPE_NORMAL
- en: Testing (initial data quality verification)
  prefs: []
  type: TYPE_NORMAL
- en: Perform rigorous validation of data quality and transformation effectiveness
    to ensure that the data preparation process meets both technical requirements
    and compliance standards and that it is aligned with transparency and ethical
    goals.
  prefs: []
  type: TYPE_NORMAL
- en: Reflection (data preparation impact analysis)
  prefs: []
  type: TYPE_NORMAL
- en: Consider the broader implications of the data preparation process. Analyze how
    each transformation might affect different groups or communities, and reflect
    on potential ethical concerns such as bias or unfair outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Key artifacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By the end of the data preparation phase, several key artifacts should be produced
    to meet the requirements of the EU AI Act. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: Data quality reports
  prefs: []
  type: TYPE_NORMAL
- en: Detailed reports on the quality of the data, highlighting any issues or limitations.
    These reports should specify what metrics were used, what problems were found,
    and how they were resolved.
  prefs: []
  type: TYPE_NORMAL
- en: Feature documentation
  prefs: []
  type: TYPE_NORMAL
- en: Clear descriptions of all features used in the model, including how they were
    engineered and what they represent. This documentation should also explain the
    rationale behind the feature engineering choices.
  prefs: []
  type: TYPE_NORMAL
- en: Data lineage documentation
  prefs: []
  type: TYPE_NORMAL
- en: Records that trace the origin of the data and any transformations applied. This
    documentation should show the entire path the data takes, from its source to its
    final use in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation logs
  prefs: []
  type: TYPE_NORMAL
- en: Detailed logs of all data transformations, including the date, time, user, and
    specifics of each change. These logs serve as a verifiable record of the data
    transformation process.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata schema
  prefs: []
  type: TYPE_NORMAL
- en: A defined structure and format for metadata, ensuring consistency in how data
    is documented. The metadata schema provides a clear framework for classifying
    and tracking data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 6-5](#chapter_6_table_5_1748539923588811) provides an overview of the
    content of each of these artifacts.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-5\. Summary of key artifacts produced during the data preparation phase
  prefs: []
  type: TYPE_NORMAL
- en: '| Artifact | Content |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data quality reports | Validation checks:'
  prefs: []
  type: TYPE_NORMAL
- en: Schema validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business rule compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constraint verification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Format consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data quality assessments, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Completeness: (metric: null percentage; threshold: <0.05; validation result:
    pass/fail)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accuracy: (metric: error rate; threshold: <0.01; validation result: pass/fail)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature documentation |'
  prefs: []
  type: TYPE_TB
- en: Feature name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description (business meaning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source (original data source)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformations (list of applied transformations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependencies (parent features)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation rules (business constraints)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data lineage documentation |'
  prefs: []
  type: TYPE_TB
- en: Operation ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timestamp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input data: source (input location), version'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output data: destination (output location), version'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Transformation logs |'
  prefs: []
  type: TYPE_TB
- en: Transform ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Method: transformation function, input parameters, validation results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Metadata: timestamp, responsible user, code version'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metadata schema |'
  prefs: []
  type: TYPE_TB
- en: 'Technical metadata: data types, schema version, storage format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Business metadata: owners, update frequency, sensitivity level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Operational metadata: processing history, quality metrics, dependency map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The core focus of the CRISP-ML(Q) modeling phase is to select and train models,
    optimize hyperparameters, and evaluate model performance. The following actions
    support compliance with Article 50’s transparency requirements in this phase,
    ensuring that an AI system’s decision-making process is interpretable, traceable,
    and reproducible:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement version control for models
  prefs: []
  type: TYPE_NORMAL
- en: Use a version control system (such as Git) to track all changes made to the
    model’s code, parameters, and configurations. This not only enables reproducibility
    but also provides the ability to revert to previous versions when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Document model behavior and intent
  prefs: []
  type: TYPE_NORMAL
- en: Develop comprehensive documentation that clearly explains how the model works,
    its intended applications, and its limitations. This should include details about
    the model’s architecture, the training process, the datasets used, and evaluation
    methodology. In the previous chapter, I gave a detailed overview of the types
    of documentation that can be created for AI systems. For example, model cards
    can be used to provide a concise summary of the model’s key characteristics (intended
    use, performance metrics, limitations, ethical considerations, and an overview
    of the evaluation data, scope, and associated risks).
  prefs: []
  type: TYPE_NORMAL
- en: Establish explainability mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: Incorporate explainability tools or techniques to enhance understanding of the
    model’s decision-making process. For example, you might use feature importance
    analysis or surrogate models to provide insights into how the model arrives at
    its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) activities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The key activities during the modeling phase include:'
  prefs: []
  type: TYPE_NORMAL
- en: Model selection
  prefs: []
  type: TYPE_NORMAL
- en: Select the most appropriate model architecture based on the problem definition
    and requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Training and validation
  prefs: []
  type: TYPE_NORMAL
- en: Train the selected model using the prepared data, and use a hold-out validation
    set to fine-tune hyperparameters and evaluate its performance. Document the entire
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: Performance evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Assess the model’s performance using appropriate metrics, focusing on both technical
    accuracy and fairness across different subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: Model optimization
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune the model to improve performance, considering factors such as accuracy,
    fairness, and robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to thoroughly document all of these processes, along with the rationale
    behind all of your decisions.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following activities show how the SMACTR stages of Artifact Collection,
    Testing, and Reflection can be integrated into the modeling phase:'
  prefs: []
  type: TYPE_NORMAL
- en: Artifact Collection
  prefs: []
  type: TYPE_NORMAL
- en: Document all relevant information about the model, including its architecture,
    training details, and performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs: []
  type: TYPE_NORMAL
- en: Thoroughly evaluate the model’s performance, considering both technical and
    ethical aspects. Include tests for fairness, accuracy, and robustness across different
    subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: Reflection
  prefs: []
  type: TYPE_NORMAL
- en: Continuously assess the broader ethical implications of the model, including
    potential impacts on fundamental rights and societal outcomes. Document the model
    selection rationale and the ethical considerations behind modeling choices.
  prefs: []
  type: TYPE_NORMAL
- en: Key artifacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 6-6](#chapter_6_table_6_1748539923588831) lists the artifacts that should
    be generated and maintained during the modeling phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-6\. Summary of key artifacts produced during the modeling phase
  prefs: []
  type: TYPE_NORMAL
- en: '| Artifact | Content |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model cards | Concise documents summarizing the model’s intended use, performance
    metrics, limitations, and ethical considerations. (See the [Model Card Toolkit](https://oreil.ly/5fcMl).)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Training logs | Comprehensive records of the training process, including
    hyperparameters, data used, and performance metrics at each stage. |'
  prefs: []
  type: TYPE_TB
- en: '| Performance reports | Detailed analyses of the model’s performance, including
    accuracy, precision, recall, and other relevant metrics. Also include results
    from fairness testing and bias detection and mitigation strategies. |'
  prefs: []
  type: TYPE_TB
- en: '| Explainability documentation | Structured summaries of the methods used for
    model interpretation and the insights gained from them. |'
  prefs: []
  type: TYPE_TB
- en: '| Version control records | Logs from the version control system showing all
    changes made to the model’s code and configurations. This ensures full traceability
    and enables reproducibility. |'
  prefs: []
  type: TYPE_TB
- en: Technical implementation guide
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following infrastructure components enable teams to implement the practices
    outlined in the previous sections and support the transparency requirements of
    Article 50:'
  prefs: []
  type: TYPE_NORMAL
- en: Version control
  prefs: []
  type: TYPE_NORMAL
- en: Use a version control system such as Git to track changes to data, models, and
    code, to ensure transparency and facilitate rollbacks. You may want to adopt a
    dual tracking approach, using DVC for data versioning and MLflow for model versioning.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata tracking system
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement a robust metadata tracking system to record model versions, training
    data, evaluation results, and explainability information. The components of this
    system will include:'
  prefs: []
  type: TYPE_NORMAL
- en: A scalable database (e.g., PostgreSQL) for structured metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A document store (e.g., MongoDB) for unstructured data and logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A searchable index (e.g., Elasticsearch) for quick retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLOps platform
  prefs: []
  type: TYPE_NORMAL
- en: Use an MLOps platform that supports versioning, experiment tracking, and pipeline
    automation. Options include Managed MLflow and Unity Catalog (Databricks), Weights
    & Biases, and Metaflow, to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The CRISP-ML(Q) evaluation phase focuses on validating the AI model’s performance
    on unseen data and refining the model as needed prior to deployment. To meet the
    transparency obligations outlined in Article 50 of the EU AI Act, the following
    practical steps should be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: Record and track model predictions and confidence levels
  prefs: []
  type: TYPE_NORMAL
- en: Establish a system that logs all model predictions along with their associated
    confidence scores. This system should support easy retrieval and analysis to help
    users understand how the AI system generates its outputs. In addition to the predictions
    themselves, log any intermediate steps or features that contribute to these outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Verify notification mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate whether the system clearly informs users that they are interacting
    with an AI. The notification should describe the system’s purpose, data sources,
    and known limitations. Test the effectiveness and visibility of the disclosure
    in the user interface, and make sure it is accessible to all users.
  prefs: []
  type: TYPE_NORMAL
- en: Test content marking processes
  prefs: []
  type: TYPE_NORMAL
- en: If the AI system generates synthetic content (audio, images, videos, or text),
    it must have the capability to mark the content as artificial in a machine-readable
    format. Verify that these markers are detectable and compliant with deepfake disclosure
    requirements, if applicable, to enhance transparency and promote accountability.
  prefs: []
  type: TYPE_NORMAL
- en: Validate documentation completeness
  prefs: []
  type: TYPE_NORMAL
- en: Ensure all documentation is complete and up-to-date, including information on
    the intended use of the AI system, data sources, model cards, training logs, and
    performance reports. The documentation should be crafted in a way that is easily
    understandable for both deployers and end users, facilitating smooth communication
    and comprehension of the system’s functionality and purpose.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) activities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main activities of the evaluation phase are:'
  prefs: []
  type: TYPE_NORMAL
- en: Model validation
  prefs: []
  type: TYPE_NORMAL
- en: Assess the model’s accuracy and generalization ability by using use case–specific
    metrics on unseen data. Employing techniques such as cross-validation can improve
    the robustness of the model, enabling a more reliable evaluation of its capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Performance assessment
  prefs: []
  type: TYPE_NORMAL
- en: 'When evaluating a model’s performance, it’s essential to consider not just
    accuracy but also fairness, robustness, and explainability. It’s also important
    to track the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive performance across different continuous-training executions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata and artifacts generated throughout the pipeline to support debugging,
    reproducibility, and lineage analysis (the ability to track a trained model back
    to its training dataset, including all intermediate artifacts and metadata)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameters used during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All evaluations performed by the pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processed data snapshots after transformation steps, if feasible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data summaries such as descriptive statistics, schemas, and feature distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documenting these metrics in the evaluation report and model card enhances transparency.
    Aligning them with the model’s intended use is crucial for producing relevant
    and meaningful assessments.
  prefs: []
  type: TYPE_NORMAL
- en: Business goal alignment
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the model’s performance aligns with the original business goals
    and success criteria. Confirm that the model’s outputs are relevant and valuable
    for the intended use case.
  prefs: []
  type: TYPE_NORMAL
- en: Compliance verification
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the system’s functionalities and documentation comply with the EU
    AI Act and any other applicable laws or industry standards.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are the main SMACTR integration activities for the evaluation phase:'
  prefs: []
  type: TYPE_NORMAL
- en: Artifact Collection (test results)
  prefs: []
  type: TYPE_NORMAL
- en: Gather all test results and performance data to be used in the impact assessment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure all AI system artifacts are well documented and clearly presented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collect comprehensive data on the model’s behavior and performance under a variety
    of conditions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing (comprehensive system testing)
  prefs: []
  type: TYPE_NORMAL
- en: Conduct thorough testing of the entire AI system, including the model, data
    pipelines, and user interfaces. The tests should cover a range of scenarios, including
    edge cases and potential adversarial inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure all tests are documented and results are properly recorded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflection (system impact analysis)
  prefs: []
  type: TYPE_NORMAL
- en: Document potential risks and limitations, including sources of bias and fairness
    concerns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess whether the AI system aligns with the organization’s defined ethical
    principles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key artifacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 6-7](#chapter_6_table_7_1748539923588851) provides a summary of the
    artifacts that should be produced during this phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-7\. Summary of key artifacts produced during the evaluation phase
  prefs: []
  type: TYPE_NORMAL
- en: '| Artifact | Content |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation reports | Document all evaluation activities, including model
    validation, performance metrics, and business goal alignment. Include a summary
    of the model’s strengths, weaknesses, and limitations. |'
  prefs: []
  type: TYPE_TB
- en: '| Compliance verification documents | Provide evidence of compliance with the
    transparency requirements of Article 50 and other relevant regulations. Clearly
    demonstrate how the system meets legal and ethical obligations. |'
  prefs: []
  type: TYPE_TB
- en: '| Test results | Compile the findings of model and system tests, including
    inputs, outputs, and any errors found. Include performance data and error analysis.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Impact assessments | Analyze the system’s effects on stakeholders, considering
    potential ethical and societal implications. Document any identified biases and
    risks. |'
  prefs: []
  type: TYPE_TB
- en: '| Audit reports | Summarize findings from the evaluation phase, including compliance
    checks and ethical assessments. Provide recommendations for improvements, and
    include a social impact analysis. |'
  prefs: []
  type: TYPE_TB
- en: Available tools and technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Integrating transparency into MLOps workflows requires robust tools for explainability,
    monitoring, and validation. The following tools and technologies can be used to
    implement best practices and support compliance with the EU AI Act:'
  prefs: []
  type: TYPE_NORMAL
- en: Explainability tools
  prefs: []
  type: TYPE_NORMAL
- en: 'These help make the model’s decision-making process more interpretable and
    understandable. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: SHAP, for understanding feature importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIME, for explaining individual predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainable Boosting Machines (EBMs), for creating inherently interpretable
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and logging tools
  prefs: []
  type: TYPE_NORMAL
- en: 'These are essential for tracking system performance in real time and monitoring
    for fairness, and potential biases. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus, for time-series data and alerts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grafana, for data visualization and dashboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic Stack, for log management and analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Card Toolkit, for standardizing and documenting key information about
    AI models via structured model cards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing frameworks
  prefs: []
  type: TYPE_NORMAL
- en: 'These facilitate thorough testing of the entire AI system. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: PyTest, for writing and running tests of Python-based AI applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Testing, for testing TensorFlow models and pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The deployment phase marks the transition of the AI system into a live production
    environment. The following actions are essential for meeting transparency requirements
    under Article 50 during this lifecycle phase:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy notification systems
  prefs: []
  type: TYPE_NORMAL
- en: Users must be informed that they are interacting with an AI system before they
    engage with the system (unless it is obvious). These disclosures must be clearly
    visible within the system interface and include an explanation of the system’s
    purpose, data sources, and any known limitations. User disclosures must be clear,
    concise, and accessible to everyone. Test the notification system thoroughly to
    ensure it works properly in the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Implement content marking
  prefs: []
  type: TYPE_NORMAL
- en: Use content marking mechanisms that clearly identify synthetic content (audio,
    images, video, or text) as artificially generated or manipulated. For systems
    that produce deepfakes, explicit disclosure is required to inform users that the
    content is synthetic.
  prefs: []
  type: TYPE_NORMAL
- en: Enable monitoring
  prefs: []
  type: TYPE_NORMAL
- en: Activate monitoring tools to observe the system’s real-time performance. Monitor
    for data drift, model performance degradation, and the emergence of biases. The
    monitoring system should also track user interactions, content generation events,
    and model deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Establish audit logging
  prefs: []
  type: TYPE_NORMAL
- en: Implement a comprehensive audit logging framework to record all relevant system
    activities (user interactions, content generation or manipulation events, errors,
    etc.). The logs should include model versioning details, transparency notifications,
    and compliance verification results. The audit logging system should be robust
    and secure while adhering to data protection regulations.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) activities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This phase typically involves the following key activities:'
  prefs: []
  type: TYPE_NORMAL
- en: System integration
  prefs: []
  type: TYPE_NORMAL
- en: Integrate the AI model with the rest of the production environment. This requires
    end-to-end testing to ensure that all components are correctly connected and functioning
    as intended.
  prefs: []
  type: TYPE_NORMAL
- en: Production deployment
  prefs: []
  type: TYPE_NORMAL
- en: Deploy the AI model into the production environment using established CI/CD
    pipelines. Validate that the deployed system is functioning as expected by running
    tests in the live environment.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring setup
  prefs: []
  type: TYPE_NORMAL
- en: Configure monitoring tools to track key metrics and performance indicators (defined
    in the earlier CRISP-ML(Q) phases) such as accuracy, fairness, robustness, and
    explainability. Set up alerting mechanisms to notify stakeholders of any issues
    or anomalies that arise.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation finalization
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that all relevant documentation, including user manuals, system architecture
    diagrams, dataset documentation, and model cards, is complete and accurately reflects
    the current state of the deployed system. Finalizing documentation is an often
    overlooked step that is essential for transparency and providing end users and
    stakeholders with a clear understanding of the deployed system.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The deployment phase incorporates SMACTR’s Artifact Collection, Testing, and
    Reflection stages through the following activities:'
  prefs: []
  type: TYPE_NORMAL
- en: Artifact Collection (final documentation)
  prefs: []
  type: TYPE_NORMAL
- en: Gather final versions of all documentation, including system architecture diagrams,
    data flow schematics, dataset and model cards, and compliance records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confirm that the documentation reflects the complete process, from development
    to deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing (production verification)
  prefs: []
  type: TYPE_NORMAL
- en: Perform a final verification of the system’s performance in the live environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that the system continues to operate as expected and that all transparency
    mechanisms are functioning correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuously monitor the model for performance degradation, fairness issues,
    and potential bias drift.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflection (deployment readiness assessment)
  prefs: []
  type: TYPE_NORMAL
- en: Ensure all necessary safeguards are in place to protect users and support responsible
    AI deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct a final review of the entire system to confirm it meets all technical,
    ethical, and legal requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key artifacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By the end of the deployment phase, several key artifacts should be available
    to meet the transparency obligations of the EU AI Act. As usual, I’ve summarized
    them in [Table 6-8](#chapter_6_table_8_1748539923588877).
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-8\. Summary of key artifacts produced during the deployment phase
  prefs: []
  type: TYPE_NORMAL
- en: '| Artifact | Content |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Deployment configurations | Document the configurations and settings used
    for deploying the AI system into production. Include infrastructure setup, software
    versions, and any dependencies. |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring dashboards | Set up monitoring dashboards to provide real-time
    visibility into the system’s performance. Dashboards should display key metrics
    and include alerting mechanisms for potential issues and anomalies. |'
  prefs: []
  type: TYPE_TB
- en: '| System documentation | Compile all documentation related to the deployed
    system, including architecture diagrams, deployment details, API specifications,
    and user guides. Make sure this documentation is accessible to all stakeholders.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Compliance records | Maintain records demonstrating compliance with Article
    50 transparency obligations and any other applicable regulations. Ensure these
    records are accessible for audit purposes. |'
  prefs: []
  type: TYPE_TB
- en: '| Incident response plans | Establish clear procedures for identifying, containing,
    and resolving incidents in the production environment. Include communication protocols
    for notifying relevant stakeholders. |'
  prefs: []
  type: TYPE_TB
- en: Available tools and technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are some tools and technologies that can support the implementation of
    transparency during the deployment phase:'
  prefs: []
  type: TYPE_NORMAL
- en: Metadata tracking systems
  prefs: []
  type: TYPE_NORMAL
- en: 'These enable traceability by tracking user interactions, content generation
    events, model deployments, and compliance verification results. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Databases: PostgreSQL (for structured metadata), MongoDB (for unstructured
    data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Searchable index: Elasticsearch (for quick retrieval)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLOps platforms
  prefs: []
  type: TYPE_NORMAL
- en: 'These support model deployment, versioning, and lifecycle management. Popular
    examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow, an open source platform for deploying and managing machine learning
    workflows on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow, for managing the complete ML lifecycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertex AI, Google Cloud’s managed ML platform for building, deploying, and scaling
    ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CI/CD tools
  prefs: []
  type: TYPE_NORMAL
- en: 'These automate the deployment pipeline, ensuring consistency and traceability.
    They include:'
  prefs: []
  type: TYPE_NORMAL
- en: GitLab CI/CD, an integrated solution for version control and automated deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub Actions, for automating ML workflows within the GitHub ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and logging tools
  prefs: []
  type: TYPE_NORMAL
- en: 'These provide real-time observability into system behavior, supporting transparency
    and bias detection. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus, for time-series data and alerts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grafana, for data visualization and dashboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic Stack, for log management and analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainability tools
  prefs: []
  type: TYPE_NORMAL
- en: 'These help make the AI system’s decisions interpretable in a live environment.
    They include:'
  prefs: []
  type: TYPE_NORMAL
- en: SHAP, for understanding feature importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIME, for explaining individual predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and Maintenance Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The monitoring and maintenance phase is crucial for ensuring continued compliance
    with the EU AI Act and the reliability and ethical performance of the AI system
    in the long term. Relevant transparency requirements include:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitor notification delivery
  prefs: []
  type: TYPE_NORMAL
- en: Regularly verify that user-facing transparency notifications are clear, accessible,
    and delivered at the appropriate moments. Track delivery frequency and success
    rates to identify gaps and areas for improvement, and use these insights to improve
    the user experience and build trust in the AI system.
  prefs: []
  type: TYPE_NORMAL
- en: Track content marking effectiveness
  prefs: []
  type: TYPE_NORMAL
- en: Continuously validate that synthetic content is consistently and accurately
    marked in a machine-readable, detectable format and that this marking persists
    even after any modifications to the content. Monitor for instances when content
    is not marked correctly, and take corrective actions to address these issues.
  prefs: []
  type: TYPE_NORMAL
- en: Log compliance status
  prefs: []
  type: TYPE_NORMAL
- en: Record any system configuration changes or behaviors that could impact compliance.
    Ensure that compliance logs are stored securely and are readily available for
    audits.
  prefs: []
  type: TYPE_NORMAL
- en: Maintain an audit trail
  prefs: []
  type: TYPE_NORMAL
- en: Keep a detailed record of all system activities, including user actions, content
    creation events, model updates, and configuration changes. The audit logs should
    include timestamps, user IDs, and clear descriptions of what actions were taken.
    Review these logs regularly to detect any unusual activity or potential problems,
    to maintain the system’s reliability and performance and guide necessary improvements.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) activities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Core activities in this phase of the lifecycle include:'
  prefs: []
  type: TYPE_NORMAL
- en: Data drift and model decay detection
  prefs: []
  type: TYPE_NORMAL
- en: Implement mechanisms for detecting data drift, where the characteristics of
    the input data change over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor for changes in the distribution of input features and flag any significant
    shifts that could impact the model’s performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance monitoring
  prefs: []
  type: TYPE_NORMAL
- en: Continuously track key performance metrics such as accuracy, precision, recall,
    and F1 score, fairness indicators, feature attributions, and error rates. Also
    include serving efficiency metrics such as latency (responsiveness and health
    of the model service), throughput (volume of predictions handled within a given
    time), and resource utilization (CPU, GPU, and memory usage, for cost and performance
    optimization).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor these metrics over time to identify performance degradation or drift.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish thresholds for acceptable performance and set up alerts to notify
    stakeholders of any significant deviations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model updates
  prefs: []
  type: TYPE_NORMAL
- en: Establish a structured process for updating models in response to performance
    degradation, data drift, or changing business requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use version control to track model updates and roll back to previous versions
    if necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that all model updates are thoroughly tested and validated before deployment
    to production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System maintenance
  prefs: []
  type: TYPE_NORMAL
- en: Perform regular maintenance to address bugs, security vulnerabilities, and performance
    issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schedule downtime as necessary and communicate maintenance activities to users
    in advance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep detailed records of all monitoring and maintenance activities and ensure
    that system documentation remains up-to-date.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are the main SMACTR integration activities for the monitoring and maintenance
    phase:'
  prefs: []
  type: TYPE_NORMAL
- en: Artifact Collection (ongoing documentation)
  prefs: []
  type: TYPE_NORMAL
- en: Continuously collect and update all relevant documentation, including monitoring
    reports, maintenance logs, and update records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing (continuous monitoring)
  prefs: []
  type: TYPE_NORMAL
- en: Implement continuous monitoring of the AI system, with a focus on ethical considerations
    including fairness, bias, and accountability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularly evaluate the system’s impact on different user groups to ensure it
    does not perpetuate or amplify existing biases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up mechanisms for reporting and addressing any ethical concerns that arise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflection (regular assessments)
  prefs: []
  type: TYPE_NORMAL
- en: Conduct regular assessments of the system’s performance and compliance with
    transparency obligations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review the ethical and social implications of the system while it is in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use insights from these assessments to identify areas for improvement and guide
    updates to the design, deployment, or governance strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key artifacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 6-9](#chapter_6_table_9_1748539923588898) summarizes the artifacts that
    should be produced and maintained during this phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-9\. Summary of key artifacts produced during the monitoring and maintenance
    phase
  prefs: []
  type: TYPE_NORMAL
- en: '| Artifact | Content |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring reports |'
  prefs: []
  type: TYPE_TB
- en: Generate regular reports summarizing system performance and providing details
    on any identified issues or anomalies, such as data drift.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include visualizations and metrics to illustrate the system’s overall health
    and performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Maintenance logs |'
  prefs: []
  type: TYPE_TB
- en: Maintain detailed logs of all maintenance activities, including changes to system
    configurations, data, or code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Record the purpose of each activity, along with the steps taken and the outcome.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure the logs are securely stored and available for audits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Update records |'
  prefs: []
  type: TYPE_TB
- en: Maintain version control for all model and system updates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that you can roll back to previous AI model versions if needed or have
    a rule-based fallback mechanism in place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Compliance reports |'
  prefs: []
  type: TYPE_TB
- en: Produce periodic reports documenting adherence to Article 50 and other applicable
    regulations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include metrics related to notification delivery, content marking effectiveness,
    and audit trail maintenance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highlight any areas of noncompliance and actions taken to address these.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Audit trails |'
  prefs: []
  type: TYPE_TB
- en: Maintain a complete audit trail of all system activities, including user interactions,
    content generation events, model deployments, and changes to system configurations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure audit logs are securely stored and accessible to relevant stakeholders
    for audit and compliance verification purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Available tools and technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to the tools mentioned in the previous sections, supporting SMACTR
    integration during the monitoring and maintenance phase requires the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Tools for data drift detection
  prefs: []
  type: TYPE_NORMAL
- en: 'These monitor input data and model predictions over time to detect shifts that
    could impact performance or fairness. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Data Validation, for automatically detecting data anomalies and drift
    in TensorFlow pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evidently AI, an open source tool for monitoring data quality and model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NannyML, a library for detecting data drift and model degradation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance and auditing tools
  prefs: []
  type: TYPE_NORMAL
- en: 'These help organizations define, enforce, and monitor compliance policies while
    maintaining secure audit trails. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Policy Agent (OPA), for defining and enforcing governance and compliance
    controls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Security Command Center, for compliance monitoring and security checks
    in cloud-based environments like Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Congratulations! You’ve just walked through the complete process of integrating
    the SMACTR framework into the CRISP-ML(Q) AI system development lifecycle. This
    integration extends the technical workflow with an internal algorithmic audit
    process, enabling teams to proactively identify and mitigate potential harms prior
    to deployment. It ensures transparency and supports compliance throughout every
    phase, from business and data understanding to ongoing monitoring and maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: For reference, [Table 6-10](#chapter_6_table_10_1748539923588918) maps the SMACTR
    stages to the CRISP-ML(Q) phases.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-10\. SMACTR and CRISP-ML(Q) integration
  prefs: []
  type: TYPE_NORMAL
- en: '| CRISP-ML(Q) phase | SMACTR stage |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|   | Scoping | Mapping | Artifact Collection | Testing | Reflection |'
  prefs: []
  type: TYPE_TB
- en: '| Business and data understanding | ✓ | ✓ | ✓ |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Data preparation |   |   | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Modeling |   |   | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation |   |   | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Deployment |   |   | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring and maintenance |   |   | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'Technology Trend: AI Governance Platforms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An AI governance platform is a comprehensive, distributed system designed to
    manage complex workflows, handle large volumes of data, and integrate with diverse
    AI systems. It comprises a set of rules, processes, frameworks, and tools that
    organizations can use to support the ethical and responsible development and deployment
    of AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: The primary function of an AI governance platform is to align AI initiatives
    with organizational principles, legal requirements (such as the EU AI Act), and
    ethical standards. You can think of it as a central control system that helps
    organizations track and manage all their AI-related activities—much like how a
    financial management system tracks monetary transactions and ensures regulatory
    compliance.
  prefs: []
  type: TYPE_NORMAL
- en: These platforms typically provide a centralized inventory of all AI systems
    in use across the organization and assist in classifying them according to the
    EU AI Act’s risk levels. This is crucial because different risk classifications
    require different compliance measures.
  prefs: []
  type: TYPE_NORMAL
- en: Governance platforms offer built-in risk assessment tools that help identify
    potential compliance gaps and suggest remediation measures. For instance, if an
    AI system is used for hiring and classified as high risk, the platform can help
    track whether it’s being regularly tested for bias and fairness as required by
    the legislation.
  prefs: []
  type: TYPE_NORMAL
- en: AI governance platforms also provide tools for documenting and monitoring AI
    systems throughout their lifecycle. This includes tracking how AI models are developed,
    trained, and deployed, what data they use, and how they perform in production.
    They often include features for automatically generating technical documentation
    and maintaining audit trails as well, which is helpful as the EU AI Act requires
    extensive documentation, particularly for high-risk AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Governance workflows are another critical feature. These help manage the approvals
    required before deploying AI systems, ensure regular reviews are conducted, and
    maintain records of key decisions and changes. To give a practical example, imagine
    a large bank using AI for credit scoring. Its AI governance platform might support
    the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Document how the credit scoring model was developed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track the data used during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor performance for accuracy and fairness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate required compliance reports.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alert relevant stakeholders if the system starts showing signs of bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain records of all model updates and approval processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of the EU AI Act, these platforms are becoming increasingly valuable.
    The Act introduces strict requirements for documentation, risk management, human
    oversight, and accountability. While it doesn’t explicitly require organizations
    to use an AI governance platform, having one in place can significantly ease the
    compliance burden, especially for organizations managing multiple AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Leading Platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Several notable AI governance platforms are available at the time of writing,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Credo AI
  prefs: []
  type: TYPE_NORMAL
- en: Specializes in model risk management and compliance assessments, with a strong
    focus on generative AI
  prefs: []
  type: TYPE_NORMAL
- en: Monitaur
  prefs: []
  type: TYPE_NORMAL
- en: Provides an ML Assurance platform for building scalable AI governance programs
  prefs: []
  type: TYPE_NORMAL
- en: Fairly AI
  prefs: []
  type: TYPE_NORMAL
- en: Offers tools for real-time monitoring, auditing, and automated compliance management
  prefs: []
  type: TYPE_NORMAL
- en: Modulos
  prefs: []
  type: TYPE_NORMAL
- en: Features an agentic AI governance platform that automates compliance with trustworthy
    AI standards
  prefs: []
  type: TYPE_NORMAL
- en: Holistic AI
  prefs: []
  type: TYPE_NORMAL
- en: Delivers enterprise-level oversight of AI projects and inventory management
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To learn more about AI governance platforms, check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Domo’s [Top 8 AI Governance Platforms for 2025](https://oreil.ly/tuYhZ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“What Is an AI Alignment Platform?”](https://oreil.ly/FtsGt) by Andrew Burt,
    Mike Schiller, and Ben Lorica'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edwin Wenink’s [Ethical Tools Landscape](https://oreil.ly/rn8_p)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transparency obligations under the EU AI Act focus on user awareness and preventing
    deception. Unlike the conformity assessments required for high-risk AI systems,
    which are formal processes that verify legal compliance, transparency obligations
    apply to all AI systems that interact directly with humans, regardless of their
    risk classification. Their primary goal is to ensure that individuals are clearly
    informed when they are interacting with an AI system and understand its purpose
    and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has provided a practical, detailed guide for AI engineers seeking
    to achieve proactive compliance with the transparency requirements set out in
    Article 50 of the Act. As you’ve seen, integrating the SMACTR framework with the
    CRISP-ML(Q) methodology provides a robust and auditable process for responsibly
    developing and deploying AI systems and supports transparency throughout the AI
    lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will discuss the implications of the EU AI Act on developing
    and deploying general-purpose AI models and systems. We’ll examine the specific
    rules and obligations for GPAI and explore the conditions under which these systems
    might be classified as high risk, with corresponding stringent requirements.
  prefs: []
  type: TYPE_NORMAL
