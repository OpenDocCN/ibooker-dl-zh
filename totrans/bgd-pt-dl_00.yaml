- en: Preface
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 前言
- en: Deep Learning in the World Today
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当今世界的深度学习
- en: Hello and welcome! This book will introduce you to deep learning via PyTorch,
    an open source library released by Facebook in 2017\. Unless you’ve had your head
    stuck in the ground in a very good impression of an ostrich the past few years,
    you can’t have helped but notice that neural networks are everywhere these days.
    They’ve gone from being the really cool bit of computer science that people learn
    about and then do nothing with to being carried around with us in our phones every
    day to improve our pictures or listen to our voice commands. Our email software
    reads our email and produces context-sensitive replies, our speakers listen out
    for us, cars drive by themselves, and the computer has finally bested humans at
    Go. We’re also seeing the technology being used for more nefarious ends in authoritarian
    countries, where neural network–backed sentinels can pick faces out of crowds
    and make a decision on whether they should be apprehended.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你好，欢迎！本书将通过Facebook于2017年发布的开源库PyTorch向您介绍深度学习。除非您过去几年一直把头埋在地里，否则您一定会注意到神经网络如今无处不在。它们已经从计算机科学中人们学习后什么都不做的非常酷的部分，变成了我们每天随身携带的手机中，用于改善我们的图片或听取我们的语音命令。我们的电子邮件软件阅读我们的电子邮件并生成上下文相关的回复，我们的扬声器倾听我们，汽车自动驾驶，计算机最终在围棋上战胜了人类。我们还看到这项技术被用于更邪恶的目的，在威权国家，神经网络支持的哨兵可以从人群中识别出面孔，并决定是否应该逮捕他们。
- en: And yet, despite the feeling that this has all happened so fast, the concepts
    of neural networks and deep learning go back a long way. The proof that such a
    network could function as a way of replacing any mathematical function in an approximate
    way, which underpins the idea that neural networks can be trained for many different
    tasks, dates back to 1989,^([1](index_split_012.html#filepos23553)) and convolutional
    neural networks were being used to recognize digits on check in the late ’90s.
    There’s been a solid foundation building up all this time, so why does it feel
    like an explosion occurred in the last 10 years?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管感觉这一切发生得如此迅速，神经网络和深度学习的概念已经存在很长时间。证明这样一个网络可以作为替代任何数学函数的一种近似方式运行的证据，这是神经网络可以被训练用于许多不同任务的基础，可以追溯到1989年，而卷积神经网络在90年代末就被用于识别支票上的数字。这些基础一直在不断积累，那么为什么感觉在过去的10年里发生了一场爆炸呢？
- en: There are many reasons, but prime among them has to be the surge in graphical
    processing units (GPUs) performance and their increasing affordability. Designed
    originally for gaming, GPUs need to perform countless millions of matrix operations
    per second in order to render all the polygons for the driving or shooting game
    you’re playing on your console or PC, operations that a standard CPU just isn’t
    optimized for. A 2009 paper, “Large-Scale Deep Unsupervised Learning Using Graphics
    Processors” by Rajat Raina et al., pointed out that training neural networks was
    also based on performing lots of matrix operations, and so these add-on graphics
    cards could be used to speed up training as well as make larger, deeper neural
    network architectures feasible for the first time. Other important techniques
    such as Dropout (which we will look at in [Chapter 3](index_split_052.html#filepos201082))
    were also introduced in the last decade as ways to not just speed up training
    but make training more generalized (so that the network doesn’t just learn to
    recognize the training data, a problem called overfitting that we’ll encounter
    in the next chapter). In the last couple of years, companies have taken this GPU-based
    approach to the next level, with Google creating what it describes as tensor processing
    units (TPUs), which are devices custom-built for performing deep learning as fast
    as possible, and are even available to the general public as part of their Google
    Cloud ecosystem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多原因，但其中最主要的原因必须是图形处理单元（GPU）性能的激增以及它们价格的逐渐降低。最初设计用于游戏的GPU需要每秒执行数以百万计的矩阵运算，以便在您的游戏机或PC上渲染驾驶或射击游戏中的所有多边形，这些操作标准CPU并不优化。2009年的一篇论文《使用图形处理器进行大规模深度无监督学习》由Rajat
    Raina等人指出，训练神经网络也是基于执行大量矩阵运算，因此这些附加的图形卡可以用于加速训练，同时也使更大、更深的神经网络架构首次变得可行。其他重要的技术，如Dropout（我们将在第3章中讨论），也在过去的十年中被引入，以不仅加速训练，而且使训练更加泛化（使网络不仅学会识别训练数据，这是我们将在下一章中遇到的过拟合问题）。在过去几年中，公司已经将这种基于GPU的方法提升到了一个新的水平，谷歌创建了他们所描述的张量处理单元（TPUs），这些设备专门用于尽可能快地执行深度学习，并且甚至作为谷歌云生态系统的一部分向普通公众提供。
- en: Another way to chart deep learning’s progress over the past decade is through
    the ImageNet competition. A massive database of over 14 million pictures, manually
    labeled into 20,000 categories, ImageNet is a treasure trove of labeled data for
    machine learning purposes. Since 2010, the yearly ImageNet Large Scale Visual
    Recognition Challenge has sought to test all comers against a 1,000-category subset
    of the database, and until 2012, error rates for tackling the challenge rested
    around 25%. That year, however, a deep convolutional neural network won the competition
    with an error of 16%, massively outperforming all other entrants. In the years
    that followed, that error rate got pushed down further and further, to the point
    that in 2015, the ResNet architecture obtained a result of 3.6%, which beat the
    average human performance on ImageNet (5%). We had been outclassed.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种追踪过去十年深度学习进展的方法是通过ImageNet竞赛。ImageNet是一个包含超过1400万张图片的庞大数据库，手动标记为20000个类别，是用于机器学习目的的标记数据的宝库。自2010年以来，每年的ImageNet大规模视觉识别挑战赛旨在测试所有参与者对数据库的1000个类别子集，直到2012年，挑战的错误率约为25%。然而，那一年，一个深度卷积神经网络以16%的错误率赢得了比赛，远远超过了所有其他参赛者。随后的几年中，错误率不断下降，直到2015年，ResNet架构获得了3.6%的结果，超过了ImageNet上的平均人类表现（5%）。我们被超越了。
- en: But What Is Deep Learning Exactly, and Do I Need a PhD to Understand It?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但深度学习究竟是什么，我需要博士学位才能理解吗？
- en: Deep learning’s definition often is more confusing than enlightening. A way
    of defining it is to say that deep learning is a machine learning technique that
    uses multiple and numerous layers of nonlinear transforms to progressively extract
    features from raw input. Which is true, but it doesn’t really help, does it? I
    prefer to describe it as a technique to solve problems by providing the inputs
    and desired outputs and letting the computer find the solution, normally using
    a neural network.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的定义通常比启发性更令人困惑。一种定义深度学习的方式是说，深度学习是一种利用多个和众多的非线性变换层逐渐从原始输入中提取特征的机器学习技术。这是正确的，但并没有真正帮助，对吧？我更喜欢将其描述为一种通过提供输入和期望输出并让计算机找到解决方案的技术，通常使用神经网络。
- en: 'One thing about deep learning that scares off a lot of people is the mathematics.
    Look at just about any paper in the field and you’ll be subjected to almost impenetrable
    amounts of notation with Greek letters all over the place, and you’ll likely run
    screaming for the hills. Here’s the thing: for the most part, you don’t need to
    be a math genius to use deep learning techniques. In fact, for most day-to-day
    basic uses of the technology, you don’t need to know much at all, and to really
    understand what’s going on (as you’ll see in [Chapter 2](index_split_033.html#filepos97807)),
    you only have to stretch a little to understand concepts that you probably learned
    in high school. So don’t be too scared about the math. By the end of [Chapter 3](index_split_052.html#filepos201082),
    you’ll be able to put together an image classifier that rivals what the best minds
    in 2015 could offer with just a few lines of code.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习让很多人望而却步的一件事是数学。看看这个领域的任何论文，你将会看到几乎无法理解的符号大量出现，到处都是希腊字母，你可能会吓得跑到山上去。事实是：在大多数情况下，你不需要成为数学天才来使用深度学习技术。实际上，对于技术的大多数日常基本用途，你根本不需要了解太多，而要真正理解正在发生的事情（正如您将在[第2章](index_split_033.html#filepos97807)中看到的那样），您只需要稍微努力一下，就能理解您可能在高中学到的概念。所以不要太害怕数学。到[第3章](index_split_052.html#filepos201082)结束时，您将能够用几行代码组建一个图像分类器，这与2015年最优秀的头脑所能提供的相媲美。
- en: PyTorch
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch
- en: As I mentioned back at the start, PyTorch is an open source offering from Facebook
    that facilitates writing deep learning code in Python. It has two lineages. First,
    and perhaps not entirely surprisingly given its name, it derives many features
    and concepts from Torch, which was a Lua-based neural network library that dates
    back to 2002\. Its other major parent is Chainer, created in Japan in 2015\. Chainer
    was one of the first neural network libraries to offer an eager approach to differentiation
    instead of defining static graphs, allowing for greater flexibility in the way
    networks are created, trained, and operated. The combination of the Torch legacy
    plus the ideas from Chainer has made PyTorch popular over the past couple of years.^([2](index_split_012.html#filepos23981))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在开头提到的，PyTorch是Facebook提供的一个开源工具，可以在Python中编写深度学习代码。它有两个来源。首先，也许并不完全令人惊讶，鉴于其名称，它从Torch中获得了许多功能和概念，Torch是一个基于Lua的神经网络库，可以追溯到2002年。它的另一个主要来源是Chainer，于2015年在日本创建。Chainer是最早提供渴望式差异化方法而不是定义静态图的神经网络库之一，这种方法允许以更灵活的方式创建、训练和操作网络。Torch的遗产加上Chainer的思想使得PyTorch在过去几年中变得流行。
- en: The library also comes with modules that help with manipulating text, images,
    and audio (`torchtext`, `torchvision`, and `torchaudio`), along with built-in
    variants of popular architectures such as ResNet (with weights that can be downloaded
    to provide assistance with techniques like transfer learning, which you’ll see
    in [Chapter 4](index_split_070.html#filepos357125)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图书馆还配备了一些模块，可以帮助处理文本、图像和音频（`torchtext`、`torchvision`和`torchaudio`），以及流行架构的内置变体，如ResNet（可以下载权重以提供诸如迁移学习等技术的帮助，您将在[第4章](index_split_070.html#filepos357125)中看到）。
- en: Aside from Facebook, PyTorch has seen quick acceptance by industry, with companies
    such as Twitter, Salesforce, Uber, and NVIDIA using it in various ways for their
    deep learning work. Ah, but I sense a question coming….
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Facebook，PyTorch还受到工业界的迅速接受，Twitter、Salesforce、Uber和NVIDIA等公司以各种方式使用它进行深度学习工作。啊，但我感觉到有一个问题要来了...。
- en: What About TensorFlow?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 关于TensorFlow呢？
- en: Yes, let’s address the rather large, Google-branded elephant in the corner.
    What does PyTorch offer that TensorFlow doesn’t? Why should you learn PyTorch
    instead?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，让我们来谈谈角落里那只相当大的、带有谷歌品牌的大象。PyTorch提供了什么，TensorFlow没有？为什么应该学习PyTorch呢？
- en: The answer is that traditional TensorFlow works in a different way than PyTorch
    that has major implications for code and debugging. In TensorFlow, you use the
    library to build up a graph representation of the neural network architecture
    and then you execute operations on that graph, which happens within the TensorFlow
    library. This method of declarative programming is somewhat at odds with Python’s
    more imperative paradigm, meaning that Python TensorFlow programs can look and
    feel somewhat odd and difficult to understand. The other issue is that the static
    graph declaration can make dynamically altering the architecture during training
    and inference time a lot more complicated and stuffed with boilerplate than with
    PyTorch’s approach.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，传统的TensorFlow与PyTorch的工作方式不同，这对代码和调试有重大影响。在TensorFlow中，您使用库来构建神经网络架构的图形表示，然后在该图形上执行操作，这发生在TensorFlow库内部。这种声明式编程方法与Python更为命令式的范式有些不符，这意味着Python
    TensorFlow程序可能看起来和感觉有些奇怪和难以理解。另一个问题是，静态图声明可能会使在训练和推断时动态修改架构变得更加复杂，并且比PyTorch的方法更加充斥着样板代码。
- en: For these reasons, PyTorch has become popular in research-oriented communities.
    The number of papers submitted to the International Conference on Learning Representations
    that mention PyTorch has jumped 200% in the past year, and the number of papers
    mentioning TensorFlow has increased almost equally. PyTorch is definitely here
    to stay.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PyTorch在面向研究的社区中变得流行。在过去一年中，提交到国际学习表示会议的提及PyTorch的论文数量增加了200%，提及TensorFlow的论文数量也几乎同样增加。PyTorch绝对会持续存在。
- en: However, things are changing in more recent versions of TensorFlow. A new feature
    called eager execution has been recently added to the library that allows it to
    work similarly to PyTorch and will be the paradigm promoted in TensorFlow 2.0\.
    But as it’s new resources outside of Google that help you learn this new method
    of working with TensorFlow are thin on the ground, plus you’d need years of work
    out there to understand the other paradigm in order to get the most out of the
    library.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在最近的TensorFlow版本中，一些事情正在发生变化。一个名为急切执行的新功能最近被添加到该库中，使其能够类似于PyTorch工作，并且将是TensorFlow
    2.0中推广的范式。但是由于在Google之外的新资源帮助你学习这种新的TensorFlow工作方法的机会很少，而且你需要花费数年的时间来理解其他范式，以便充分利用该库。
- en: But none of this should make you think poorly of TensorFlow; it remains an industry-proven
    library with support from one of the biggest companies on the planet. PyTorch
    (backed, of course, by a different biggest company on the planet) is, I would
    say, a more streamlined and focused approach to deep learning and differential
    programming. Because it doesn’t have to continue supporting older, crustier APIs,
    it is easier to teach and become productive in PyTorch than in TensorFlow.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这一切都不应该让你对TensorFlow产生负面看法；它仍然是一个经过行业验证的库，得到了全球最大公司之一的支持。PyTorch（当然，由全球另一家最大公司支持）是我会说，对于深度学习和微分编程，是一种更简化和专注的方法。因为它不必继续支持旧的、陈旧的API，所以在PyTorch中教学和变得高效比在TensorFlow中更容易。
- en: Where does Keras fit in with this? So many good questions! Keras is a high-level
    deep learning library that originally supported Theano and TensorFlow, and now
    also supports certain other frames such as Apache MXNet. It provides certain features
    such as training, validation, and test loops that the lower-level frameworks leave
    as an exercise for the developer, as well as simple methods of building up neural
    network architectures. It has contributed hugely to the take-up of TensorFlow,
    and is now part of TensorFlow itself (as `tf.keras`) as well as continuing to
    be a separate project. PyTorch, in comparison, is something of a middle ground
    between the low level of raw TensorFlow and Keras; we will have to write our own
    training and inference routines, but creating neural networks is almost as straightforward
    (and I would say that PyTorch’s approach to making and reusing architectures is
    much more logical to a Python developer than some of Keras’s magic).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Keras在这方面如何？有很多好问题！Keras是一个高级深度学习库，最初支持Theano和TensorFlow，现在也支持某些其他框架，如Apache
    MXNet。它提供了一些功能，如训练、验证和测试循环，这些功能在低级框架中留给开发人员自己去实现，以及构建神经网络架构的简单方法。它对TensorFlow的普及做出了巨大贡献，并且现在已经成为TensorFlow本身的一部分（作为`tf.keras`），同时仍然是一个独立的项目。相比之下，PyTorch在原始TensorFlow和Keras之间是一种中间地带；我们将不得不编写自己的训练和推理例程，但创建神经网络几乎和Keras一样简单（我会说PyTorch的创建和重用架构方法对于Python开发人员来说比某些Keras的魔法更合乎逻辑）。
- en: As you’ll see in this book, although PyTorch is common in more research-oriented
    positions, with the advent of PyTorch 1.0, it’s perfectly suited to production
    use cases.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本书中所看到的，尽管PyTorch在更多面向研究的职位中很常见，但随着PyTorch 1.0的出现，它非常适合生产用例。
- en: Conventions Used in This Book
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用以下排版约定：
- en: Italic
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 斜体
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`常量宽度`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用于程序清单，以及在段落中引用程序元素，如变量或函数名、数据库、数据类型、环境变量、语句和关键字。
- en: '`Constant width bold`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`常量宽度粗体`'
- en: Shows commands or other text that should be typed literally by the user.
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 显示用户应该按照字面意义输入的命令或其他文本。
- en: '`Constant width italic`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`常量宽度斜体`'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 显示应该用用户提供的值或上下文确定的值替换的文本。
- en: TIP
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提示
- en: This element signifies a tip or suggestion.
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个元素表示一个提示或建议。
- en: NOTE
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个元素表示一般说明。
- en: WARNING
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 警告
- en: This element indicates a warning or caution.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个元素表示警告或注意事项。
- en: Using Code Examples
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用代码示例
- en: Supplemental material (including code examples and exercises) is available for
    download at [https://oreil.ly/pytorch-github](https://oreil.ly/pytorch-github).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可下载补充材料（包括代码示例和练习）位于[https://oreil.ly/pytorch-github](https://oreil.ly/pytorch-github)。
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing a CD-ROM
    of examples from O’Reilly books does require permission. Answering a question
    by citing this book and quoting example code does not require permission. Incorporating
    a significant amount of example code from this book into your product’s documentation
    does require permission.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书旨在帮助您完成工作。一般来说，如果本书提供示例代码，您可以在程序和文档中使用它。除非您复制了代码的大部分，否则无需征得我们的许可。例如，编写一个使用本书中几个代码块的程序不需要许可。出售或分发O'Reilly图书示例的CD-ROM需要许可。通过引用本书回答问题并引用示例代码不需要许可。将本书中大量示例代码合并到产品文档中需要许可。
- en: 'We appreciate, but do not require, attribution. An attribution usually includes
    the title, author, publisher, and ISBN. For example: “Programming PyTorch for
    Deep Learning by Ian Pointer (O’Reilly). Copyright 2019 Ian Pointer, 978-1-492-04535-9.”'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢，但不要求署名。署名通常包括标题，作者，出版商和ISBN。例如：“Ian Pointer（O'Reilly）的《深度学习PyTorch编程》。版权所有2019年Ian
    Pointer，978-1-492-04535-9。”
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [permissions@oreilly.com](mailto:permissions@oreilly.com).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为您使用的代码示例超出了合理使用范围或上述许可，请随时通过[permissions@oreilly.com](mailto:permissions@oreilly.com)与我们联系。
- en: O’Reilly Online Learning
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: O'Reilly在线学习
- en: NOTE
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: For almost 40 years, [O’Reilly Media](http://oreilly.com) has provided technology
    and business training, knowledge, and insight to help companies succeed.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 近40年来，[O'Reilly Media](http://oreilly.com)为公司提供技术和商业培训，知识和见解，帮助公司取得成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, conferences, and our online learning platform. O’Reilly’s
    online learning platform gives you on-demand access to live training courses,
    in-depth learning paths, interactive coding environments, and a vast collection
    of text and video from O’Reilly and 200+ other publishers. For more information,
    please visit [http://oreilly.com](http://oreilly.com).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍，文章，会议和我们的在线学习平台分享他们的知识和专长。O'Reilly的在线学习平台为您提供按需访问实时培训课程，深入学习路径，交互式编码环境以及来自O'Reilly和其他200多家出版商的大量文本和视频。有关更多信息，请访问[http://oreilly.com](http://oreilly.com)。
- en: How to Contact Us
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关此书的评论和问题发送至出版商：
- en: O’Reilly Media, Inc.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O'Reilly Media，Inc。
- en: 1005 Gravenstein Highway North
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastopol，CA 95472
- en: 800-998-9938 (in the United States or Canada)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-998-9938（在美国或加拿大）
- en: 707-829-0515 (international or local)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0515（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104（传真）
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [https://oreil.ly/prgrming-pytorch-for-dl](https://oreil.ly/prgrming-pytorch-for-dl).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书创建了一个网页，列出勘误表，示例和任何其他信息。您可以访问此页面：[https://oreil.ly/prgrming-pytorch-for-dl](https://oreil.ly/prgrming-pytorch-for-dl)。
- en: Email [bookquestions@oreilly.com](mailto:bookquestions@oreilly.com) to comment
    or ask technical questions about this book.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过电子邮件[bookquestions@oreilly.com](mailto:bookquestions@oreilly.com)发表评论或提出有关此书的技术问题。
- en: For more information about our books, courses, conferences, and news, see our
    website at [http://www.oreilly.com](http://www.oreilly.com).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有关我们的图书，课程，会议和新闻的更多信息，请访问我们的网站：[http://www.oreilly.com](http://www.oreilly.com)。
- en: 'Find us on Facebook: [http://facebook.com/oreilly](http://facebook.com/oreilly)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在Facebook上找到我们：[http://facebook.com/oreilly](http://facebook.com/oreilly)
- en: 'Follow us on Twitter: [http://twitter.com/oreillymedia](http://twitter.com/oreillymedia)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在Twitter上关注我们：[http://twitter.com/oreillymedia](http://twitter.com/oreillymedia)
- en: 'Watch us on YouTube: [http://www.youtube.com/oreillymedia](http://www.youtube.com/oreillymedia)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上关注我们：[http://www.youtube.com/oreillymedia](http://www.youtube.com/oreillymedia)
- en: Acknowledgments
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢
- en: A big thank you to my editor, Melissa Potter, my family, and Tammy Edlund for
    all their help in making this book possible. Thank you, also, to the technical
    reviewers who provided valuable feedback throughout the writing process, including
    Phil Rhodes, David Mertz, Charles Givre, Dominic Monn, Ankur Patel, and Sarah
    Nagy.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢我的编辑Melissa Potter，我的家人和Tammy Edlund在使这本书成为可能的过程中所提供的所有帮助。同时也感谢在写作过程中提供宝贵反馈的技术审阅人员，包括Phil
    Rhodes，David Mertz，Charles Givre，Dominic Monn，Ankur Patel和Sarah Nagy。
- en: '[1](index_split_004.html#filepos5741)'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[1](index_split_004.html#filepos5741)'
- en: See [“Approximation by Superpositions of Sigmoidal Functions”](https://oreil.ly/BQ8-9),
    by George Cybenko (1989).
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见George Cybenko（1989）的“由Sigmoidal函数的超级位置逼近”。
- en: '[2](index_split_006.html#filepos11364)'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[2](index_split_006.html#filepos11364)'
- en: Note that PyTorch borrows ideas from Chainer, but not actual code.
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，PyTorch借鉴了Chainer的想法，但没有实际代码。
