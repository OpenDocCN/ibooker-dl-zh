- en: Chapter 6\. Do Language Models Dream of Electric Sheep?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。语言模型会梦见电子羊吗？
- en: Among all the excitement about advances in LLMs, few phenomena captivate and
    perplex like their so-called *hallucinations*. It’s almost as if these computational
    entities, deep within their myriad layers, occasionally drift into a dreamlike
    state, creating wondrous and bewildering narratives. Like a human’s dreams, these
    hallucinations can be reflective, absurd, or even prophetic, providing insights
    into the complex interplay between training data and the model’s learned interpretations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有关于LLM进步的兴奋中，所谓的*幻觉*现象最能吸引和困惑人。几乎就像这些计算实体在其众多层中偶尔会进入一种梦幻般的状态，创造出奇妙而令人困惑的叙述。就像人类的梦境一样，这些幻觉可以是反思性的、荒谬的，甚至是预言性的，为训练数据和模型学习解释之间的复杂相互作用提供了见解。
- en: In the world of LLMs, the term “hallucination” might evoke images of vivid and
    whimsical creations, but in reality, it signifies a more mundane statistical anomaly.
    At its core, a hallucination is the model’s attempt to bridge gaps in its knowledge
    using the patterns it has gleaned from its training data. While it might be termed
    “imaginative,” it’s essentially the LLM making an educated guess when faced with
    unfamiliar input or scenarios. However, these guesses can manifest as confident
    yet unfounded assertions, revealing the model’s struggle to differentiate between
    well-learned facts and the statistical noise within its training data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的世界里，“幻觉”这个术语可能会让人联想到生动而异想天开的创造，但现实中，它指的是一种更常见的统计异常。本质上，幻觉是模型试图利用从其训练数据中汲取的规律来弥合其知识差距。虽然这可能被称为“富有想象力的”，但当面对不熟悉的信息或场景时，LLM实际上是在做出一个有根据的猜测。然而，这些猜测可能表现为自信但无根据的断言，揭示了模型在区分所学事实和训练数据中的统计噪声方面的努力。
- en: LLMs do not provide easily usable probability scores like some other “predictive”
    AI algorithms. For example, a vision classifier algorithm may return a probability
    as a percent. It might show a 79% chance that a particular image depicts a monkey.
    Thus, a user of that model gets a sense of how strongly the model “feels” about
    the prediction. LLMs simply predict the next token or tokens in a sequence. While
    the LLM uses a complex statistical model to do this, a certainty score for the
    overall response to a prompt isn’t typically part of the output. This can leave
    the end user unsure whether the LLM has returned a well-grounded reaction to the
    prompt or a weak, statistical extrapolation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: LLM不提供像某些其他“预测”AI算法那样易于使用的概率分数。例如，一个视觉分类器算法可能会以百分比的形式返回一个概率。它可能会显示有79%的可能性，某个特定的图像描绘了一只猴子。因此，该模型的用户可以感受到模型对预测的“感觉”有多强烈。LLM只是预测序列中的下一个或几个标记。虽然LLM使用复杂的统计模型来完成这项任务，但通常整个响应的确定性分数不是输出的一部分。这可能会让最终用户不确定LLM返回的是对提示的稳固反应还是薄弱的、基于统计的推断。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The term “hallucination” is unpopular with some because it personifies the LLM
    and makes its flaws seem less critical. Some literature now refers to this phenomenon
    as *confabulation*. However, hallucination is far more prevalent, so we’ll use
    it in this book.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些人来说，“幻觉”这个术语不受欢迎，因为它将LLM人格化，使得其缺陷看起来不那么关键。现在的一些文献将这种现象称为*虚构*。然而，幻觉更为普遍，所以在这本书中我们将使用它。
- en: 'This nuanced dance between fact and fiction in LLM outputs brings us to the
    crux of the challenge: *overreliance*. As humans, we are naturally inclined to
    trust results that are presented confidently, especially when they emanate from
    sophisticated computer software. Yet, it’s this very trust that can steer us astray.
    When LLMs hallucinate, they often don’t waver in their confidence, making it hard
    to discern genuine knowledge from imperfect statistical artifacts. The danger
    lies in the hallucination and also in our propensity to take these dreamlike utterances
    at face value, potentially leading to misinformation, missteps, and broader implications
    in real-world applications.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM输出中，事实与虚构之间的这种微妙舞蹈将我们引向挑战的核心：*过度依赖*。作为人类，我们天生倾向于信任那些自信呈现的结果，尤其是当它们来自复杂的计算机软件时。然而，正是这种信任可能会将我们引入歧途。当LLM产生幻觉时，它们通常不会在自信上动摇，这使得很难区分真正的知识和不完美的统计伪象。危险在于幻觉，也在于我们倾向于将这些梦幻般的陈述视为理所当然，这可能导致错误信息、失误，并在现实世界的应用中产生更广泛的影响。
- en: Note
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Overreliance refers to the excessive trust in the capabilities and exactness
    of LLM elaborations. Excessive confidence in LLM output, especially when hallucinations,
    errors, or biased data input are present, can lead to damaging outputs, particularly
    in professional or safety-critical environments. A significant example is trusting
    an LLM to provide medical advice without sufficient testing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 过度依赖指的是对LLM阐述的能力和精确度的过度信任。在LLM输出中，尤其是当存在幻觉、错误或偏数据输入时，过度自信可能导致有害的输出，尤其是在专业或安全关键的环境中。一个显著的例子是信任LLM提供医疗建议，而没有足够的测试。
- en: Why Do LLMs Hallucinate?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么LLM会幻觉？
- en: The core reason for hallucinations lies in the LLM’s operational mechanism,
    which is geared toward pattern matching and statistical extrapolation rather than
    factual verification. While they acquire knowledge through training on vast training
    datasets, LLMs often lack specific, actual knowledge. Their operation is rooted
    in identifying patterns in the input data and attempting to match these patterns
    with those learned during training. This pattern matching occurs without a real-world
    understanding, which can lead to the generation of hallucinated text, especially
    when faced with ambiguous or novel input prompts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉的核心原因在于LLM（大型语言模型）的操作机制，该机制侧重于模式匹配和统计外推，而不是事实验证。虽然它们通过在大规模训练数据集上进行训练来获取知识，但LLM通常缺乏具体、实际的知识。它们的操作根植于识别输入数据中的模式，并试图将这些模式与训练期间学习到的模式相匹配。这种模式匹配在没有现实世界理解的情况下发生，可能导致生成幻觉文本，尤其是在面对模糊或新颖的输入提示时。
- en: The quality and nature of the training data significantly impact the likelihood
    and extent of hallucinations. Biases, inaccuracies, or noise in the training data
    can mislead the model into generating biased or incorrect text.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的质量和性质显著影响幻觉的可能性和程度。训练数据中的偏差、不准确或噪声可能导致模型生成有偏见或不正确的文本。
- en: Hallucinations present a substantial challenge in using LLMs for critical or
    sensitive applications. They underline AI development’s inherent intricacies and
    challenges, spotlighting the gap between statistical pattern matching and real-world,
    factual understanding. The hallucination phenomenon in LLMs opens a window into
    the broader discourse on the limitations and the ethical implications of deploying
    large-scale AI models in real-world scenarios without a robust mechanism for factual
    verification or contextual understanding.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉在使用LLM进行关键或敏感应用时提出了重大挑战。它们突显了AI发展的固有复杂性和挑战，聚焦于统计模式匹配与现实世界、事实理解之间的差距。LLM中的幻觉现象为更广泛地讨论在现实场景中部署大规模AI模型（没有强大的事实验证或上下文理解机制）的限制和伦理影响打开了窗口。
- en: Types of Hallucinations
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幻觉的类型
- en: 'As we dig further into this, let’s look at some of the types of hallucinations
    we’ll likely experience. Doing so will help us understand the implications and
    mitigations:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进一步探讨这个问题，让我们看看我们可能会遇到的一些幻觉类型。这样做将帮助我们理解其影响和缓解措施：
- en: Factual inaccuracies
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 事实不准确
- en: LLMs may produce factually incorrect statements due to the model’s lack of specific
    knowledge or to misinterpreting the training data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型缺乏具体知识或对训练数据的误解，LLM可能会产生事实不准确的陈述。
- en: Unsupported claims
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无依据的断言
- en: Similar to factual inaccuracies, LLMs might generate baseless claims, which
    can be detrimental, especially in sensitive or critical contexts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与事实不准确类似，LLM可能会生成无根据的断言，这在敏感或关键环境中可能是有害的。
- en: Misrepresentation of abilities
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 能力误代表
- en: LLMs might give the illusion of understanding advanced topics such as chemistry,
    even when they don’t. They can convincingly double-talk about a topic, misleading
    users about their level of understanding.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 即使LLM并不真正理解高级主题，如化学，它们也可能给人以理解这些主题的错觉。它们可以令人信服地就某个主题进行双向讨论，误导用户对其理解程度的认识。
- en: Contradictory statements
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 矛盾的陈述
- en: LLMs might generate sentences contradicting previous statements or the user’s
    prompt. For instance, they might first state, “Cats are afraid of water,” and
    later claim, “Cats love to swim in water.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可能会生成与先前陈述或用户提示相矛盾的句子。例如，它们可能会首先声明，“猫害怕水”，然后又声称，“猫喜欢在水中游泳”。
- en: With these in mind, let’s look at real-world examples and their impact on application
    providers and customers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些因素，让我们看看现实世界的例子及其对应用提供商和客户的影响。
- en: Examples
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: In this section, we’ll examine four cases where hallucinations intersected with
    overreliance and caused harm. These should help drive home the need to address
    these issues in your LLM applications.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考察四个案例，其中幻觉与过度依赖相交并造成了损害。这些案例应该有助于强调在您的LLM应用中解决这些问题的必要性。
- en: Imaginary Legal Precedents
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚构的法律先例
- en: In 2023, in a US federal court, a judge levied fines on two lawyers and their
    law firm for negligent oversight in legal practice. The lawyers had submitted
    fictitious legal research in an aviation injury case. The fabricated case law,
    as it turned out, was generated by ChatGPT.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年，在美国联邦法院，一位法官对两位律师及其律师事务所因在法律实践中的疏忽监管进行了罚款。律师在航空伤害案件中提交了虚构的法律研究。结果证明，这些虚构的案件法是由ChatGPT生成的。
- en: The issue came to light during a routine legal proceeding when the opposition
    discovered that the legal citations provided by the lawyers were not merely erroneous
    but entirely fabricated. The lawyers used a general-purpose LLM, which did not
    have specific legal training or data access, for their research. Their unverified
    reliance on the AI output led to the submission of six fictitious case citations
    in a legal brief. The judge later judged this action as an act of bad faith. The
    repercussions of this act were not confined to the courtroom but resonated across
    the legal and tech communities, marking a significant incident in the discourse
    surrounding AI’s role in legal practice.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题是在一次常规法律程序中暴露出来的，当时反对派发现律师提供的法律引用不仅错误，而且完全是虚构的。律师在研究时使用了通用的LLM，该LLM没有特定的法律培训或数据访问权限。他们对AI输出的未经验证的依赖导致了在法律简报中提交了六个虚构的案件引用。法官后来判定这一行为是不诚实的行为。这一行为的后果不仅限于法庭，而且在法律和技术社区中产生了共鸣，标志着围绕AI在法律实践中的作用的讨论中的一个重大事件。
- en: Because the judge imposed substantial fines on the lawyers and their firm, the
    incident emerged as a cautionary tale about overreliance on AI in critical domains.
    It showcased the necessity of human verification and due diligence, especially
    in a field where accuracy and authenticity are paramount.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于法官对律师及其公司施加了巨额罚款，这一事件成为了一个关于在关键领域过度依赖AI的警示故事。它展示了人类验证和尽职调查的必要性，特别是在准确性和真实性至关重要的领域。
- en: 'Let’s look at the impacts this incident had on several different parties to
    ensure we can see the full scope of the problems caused:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这一事件对几个不同方面产生的影响，以确保我们能全面了解由此引发的问题：
- en: On the LLM provider
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LLM提供商
- en: The incident spotlighted the potential risks of using OpenAI’s products in critical
    and formal domains like legal practice. It raised questions about the reliability
    and safe usage of ChatGPT and potentially impacted OpenAI’s reputation. The misuse
    of ChatGPT in a legal setting could prompt further scrutiny and demands by legislators
    for stricter regulation on the use and deployment of OpenAI’s products in critical
    domains.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这一事件突出了在关键和正式领域，如法律实践中使用OpenAI产品的潜在风险。它提出了关于ChatGPT的可靠性和安全使用的疑问，并可能影响了OpenAI的声誉。在法律环境中滥用ChatGPT可能会促使立法者进一步审查，并要求对OpenAI产品在关键领域的使用和部署实施更严格的监管。
- en: On LLM customers
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LLM客户
- en: The repercussions were immediate and severe for the lawyers involved. They faced
    financial penalties, and their professional reputation was significantly tarnished.
    This incident is a deterrent for other legal professionals, making them wary of
    relying on AI tools for critical tasks without thorough verification.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于涉及的律师来说，后果是立即且严重的。他们面临了罚款，并且他们的专业声誉受到了严重影响。这一事件对其他法律专业人士来说是一个威慑，使他们警惕在没有彻底验证的情况下依赖AI工具进行关键任务。
- en: On the legal profession
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 关于法律职业
- en: The event echoed across the legal profession, emphasizing the importance of
    human verification and the dangers of unquestioningly trusting AI-generated content.
    It highlighted a pressing need for educating and alerting legal professionals
    about the limitations and correct usage of AI tools in legal practice.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这一事件在法律职业中产生了回响，强调了人类验证的重要性以及无条件的信任AI生成内容的风险。它突显了教育并警告法律专业人士关于AI工具在法律实践中的局限性和正确使用的一个迫切需要。
- en: At its core, this event underscores the indispensable value of verification.
    Legal professionals, and indeed all users of AI, should invest in verifying the
    information generated by AI tools. Further, the incident brings to light the necessity
    for robust guidelines that govern the use of AI in legal practice and other critical
    domains. Establishing such policies, including verification procedures to ensure
    the accuracy and reliability of AI-generated information, will act as a bulwark
    against similar incidents. The story also underscores the need to promote the
    ethical use of AI tools. Creating awareness about potential misuse and stressing
    the importance of adhering to professional standards when employing AI for critical
    tasks emerges as a pivotal lesson.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，这一事件强调了验证的不可或缺价值。法律专业人士，以及所有AI工具的使用者，都应投资于验证AI工具生成的信息。此外，这一事件突显了制定规范AI在法律实践和其他关键领域使用的稳健指南的必要性。建立此类政策，包括确保AI生成信息准确性和可靠性的验证程序，将作为防止类似事件的防御工事。这个故事还强调了推广AI工具道德使用的必要性。提高对潜在滥用的认识，并强调在执行关键任务时遵守专业标准的重要性，成为一项关键教训。
- en: LLM providers such as OpenAI should provide better guidelines, warnings, and
    education about their AI tools’ proper use and limitations to prevent misuse and
    ensure users are fully informed about the capabilities and potential risks. Lastly,
    the incident highlights the need for continuous improvement, urging AI software
    developers and the legal profession to learn from their mistakes and enhance their
    tools’ safety and reliability in critical applications. Through such a reflective
    lens, the incident offers a roadmap toward fostering a responsible AI usage culture
    anchored in verification, education, and ethical practice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如OpenAI之类的LLM（大型语言模型）提供商应提供更好的指南、警告和教育，关于其AI工具的正确使用和限制，以防止滥用并确保用户充分了解其功能和潜在风险。最后，这一事件突显了持续改进的必要性，敦促AI软件开发人员和法律界从他们的错误中学习，并提高他们在关键应用中工具的安全性和可靠性。通过这样的反思视角，这一事件为培养基于验证、教育和道德实践的负责任AI使用文化提供了路线图。
- en: Airline Chatbot Lawsuit
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 航空公司聊天机器人诉讼
- en: In a landmark decision in 2024, Canada’s largest airline, Air Canada, was ordered
    to compensate a customer after a chatbot provided incorrect information regarding
    fares. In this case, Jake Moffatt, a resident of British Columbia, sought information
    from Air Canada’s chatbot about the documents necessary for a bereavement fare
    and the possibility of obtaining a retroactive refund. Based on the information
    provided by the chatbot, Moffatt purchased a full-price ticket, believing he could
    secure a refund later. However, when he applied for the refund, Air Canada denied
    it, stating that bereavement rates did not apply to completed travel, contrary
    to the chatbot’s guidance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在2024年的一项里程碑式裁决中，加拿大最大的航空公司加拿大航空公司（Air Canada）被命令赔偿一位客户，因为聊天机器人提供了关于票价的不正确信息。在此案中，居住在不列颠哥伦比亚省的贾克·摩夫特（Jake
    Moffatt）向加拿大航空公司的聊天机器人咨询有关丧葬票价所需文件以及能否获得追溯性退款的可能性。根据聊天机器人提供的信息，摩夫特购买了一张全价票，相信他可以在以后获得退款。然而，当他申请退款时，加拿大航空公司拒绝了他的请求，声称丧葬票价不适用于已完成旅行的行程，这与聊天机器人的指导相反。
- en: Moffatt initiated legal action against Air Canada to recover the fare difference
    after the airline failed to honor the chatbot’s information. Air Canada’s defense
    claimed the chatbot was a “separate legal entity” and responsible for its own
    actions, a stance that was dismissed by the judge as illogical and irresponsible.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 摩夫特（Moffatt）在加拿大航空公司未能履行聊天机器人的信息后，对其提起了法律诉讼，以追回票价差额。加拿大航空公司的辩护称聊天机器人是一个“独立的法律实体”，应对其自身行为负责，但这一立场被法官判定为不合逻辑且不负责任。
- en: The judge ordered Air Canada to pay Moffatt the difference between the full
    fare and the bereavement fare, along with interest and fees. The judge emphasized
    that all information provided on Air Canada’s website, whether through a chatbot
    or a static page, was the airline’s responsibility.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 法官命令加拿大航空公司支付摩夫特全价票和丧葬票价之间的差额，以及利息和费用。法官强调，无论通过聊天机器人还是静态页面，在加拿大航空公司的网站上提供的信息都是航空公司的责任。
- en: 'Let’s examine the impacts of this case from several different angles:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从几个不同的角度来分析这一案例的影响：
- en: On Air Canada
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 关于加拿大航空公司
- en: The incident brought significant public and legal scrutiny, challenging the
    airline’s approach to AI in customer interactions. It highlighted the need for
    accurate AI-generated communications and the potential reputational damage from
    AI errors.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这起事件引发了公众和法律的广泛关注，挑战了航空公司对客户互动中人工智能的运用方法。它突显了准确的人工智能生成通信的必要性以及人工智能错误可能带来的声誉损害。
- en: On AI and legal precedents
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 关于人工智能和法律先例
- en: The case set a precedent regarding the legal accountability of AI communications
    in business operations. It raised questions about the extent to which companies
    can or should be held liable for AI-generated content.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此案在商业运营中人工智能通信的法律责任方面树立了先例。它提出了关于公司可以或应该对人工智能生成内容承担多大或何种责任的问题。
- en: On consumers and AI
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 关于消费者和人工智能
- en: The ruling reinforced consumer rights in the digital age, emphasizing that companies
    cannot absolve themselves of accountability for AI-generated errors.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这项裁决强化了数字时代的消费者权利，强调公司不能免除对人工智能生成错误的责任。
- en: The case emphasizes the critical importance of accuracy in LLM-generated content
    and the growing legal precedence that inaccuracies can lead to substantial financial
    and reputational penalties for companies. This ruling reinforces the notion that
    businesses cannot disown the outputs of their LLM applications and must treat
    AI communications with the same scrutiny as any other official corporate communication.
    Companies must ensure rigorous testing and continuous monitoring of their AI tools
    to avoid potential legal liabilities and uphold consumer trust. Moreover, the
    financial repercussions highlighted by this case serve as a reminder of the direct
    costs associated with such misinformation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此案强调了在LLM生成内容中准确性至关重要的意义，以及不准确可能导致公司遭受重大财务和声誉惩罚的法律先例。这一裁决强化了企业不能否认其LLM应用程序输出的观点，并且必须像对待任何其他官方企业通信一样仔细审查人工智能通信。公司必须确保对其人工智能工具进行严格的测试和持续的监控，以避免潜在的法律责任并维护消费者信任。此外，此案凸显的财务影响提醒我们，此类错误信息直接相关的成本。
- en: Unintentional Character Assassination
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无意中的人格诽谤
- en: In 2023, Brian Hood, mayor of Hepburn Shire in Australia, threatened legal action
    against OpenAI for a defamatory claim generated by the LLM. ChatGPT falsely asserted
    that Hood, then a whistleblower in a foreign bribery scandal, had served jail
    time. According to the suit, this fabricated information, presented as factual
    by the AI, significantly impacted Hood’s reputation and caused distress.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年，澳大利亚赫本郡市长布莱恩·霍德威胁要对OpenAI提起诉讼，因为LLM生成的一项诽谤性指控。ChatGPT错误地断言，霍德当时是外国贿赂丑闻中的告密者，曾服过刑。根据诉讼，AI呈现的这项虚构信息，作为事实呈现，对霍德的声誉造成了重大影响，并给他带来了痛苦。
- en: The issue may have stemmed from ChatGPT’s limited training data in this area.
    Without the LLM having access to strongly correlated data related to a user’s
    query, the LLM could have conflated unrelated snippets of information, resulting
    in the demonstrably false claim about Hood. The incident underscored the potential
    dangers of relying on AI-generated information uncritically, especially in sensitive
    domains like public reputation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 问题可能源于ChatGPT在此领域有限的训练数据。如果没有LLM能够访问与用户查询强相关数据，LLM可能会将不相关的信息片段混淆在一起，从而导致了关于霍德的明显错误指控。这一事件强调了不加批判地依赖人工智能生成信息的潜在危险，尤其是在像公共声誉这样的敏感领域。
- en: 'We can better understand the impacts of this case by looking at it from both
    plaintiff’s and defendant’s point of view:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从原告和被告的角度来更好地理解此案的影响：
- en: Hood
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 霍德
- en: The false claim caused Hood mental anguish and threatened his political career.
    The incident highlighted individuals’ vulnerability to AI-generated misinformation
    and the potential for reputational damage.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这项虚假声明给霍德带来了心理痛苦，并威胁到他的政治生涯。这一事件突显了个人容易受到人工智能生成错误信息的影响以及声誉损害的可能性。
- en: OpenAI
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI
- en: The company has opened itself to expensive and time-consuming litigation. In
    this case, the plaintiff indicated at the time of filing that he might be seeking
    over $200,000 in damages.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 公司已经使自己面临昂贵且耗时的诉讼。在此案中，原告在提起诉讼时表示，他可能寻求超过20万美元的赔偿。
- en: 'Understanding those impacts leads us to three lessons you can apply in your
    projects:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些影响使我们得出以下三个可以在项目中应用的教训：
- en: Verification
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 验证
- en: Robust verification mechanisms are crucial, whether through fact-checking tools,
    human oversight, or a combination. Users must develop a healthy skepticism toward
    AI-generated information.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是通过事实核查工具、人工监督还是两者的结合，强大的验证机制至关重要。用户必须对AI生成信息保持健康的怀疑态度。
- en: Education
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 教育
- en: Educating users about the capabilities and limitations of LLMs is critical to
    promoting responsible and ethical usage.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 教育用户关于LLM的能力和局限性对于促进负责任和道德的使用至关重要。
- en: Regulation
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 监管
- en: Regulatory frameworks may be necessary to govern the use of LLMs in critical
    domains, ensuring data privacy, algorithmic accountability, and user protection.
    The Hood case highlights the potential need for legal clarification around AI
    responsibility and liability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键领域使用LLM可能需要监管框架，以确保数据隐私、算法问责制和用户保护。《休德案》突出了在AI责任和责任方面进行法律澄清的潜在需求。
- en: The Brian Hood case exemplifies the potential pitfalls of hallucination and
    overreliance in LLMs. It calls for more robust safeguards, user education, and
    responsible application of this powerful technology. Only through a multipronged
    approach can we prevent future harm and ensure AI’s beneficial integration into
    society.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 布莱恩·休德案是LLM中幻觉和过度依赖潜在陷阱的例证。它呼吁采取更稳健的安全措施、用户教育和负责任地应用这项强大的技术。只有通过多管齐下的方法，我们才能防止未来的伤害并确保AI有益地融入社会。
- en: Open Source Package Hallucinations
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源包幻觉
- en: This incident centers around using LLMs as coding assistants. It has become
    commonplace now for developers to use LLMs to assist them while writing code.
    Developers might use general-purpose chatbots, such as ChatGPT, or dedicated copilots,
    such as GitHub Copilot. A [survey by GitHub](https://oreil.ly/tcy1y) in June 2023
    showed that 92% of developers working in large companies were using LLMs to help
    them code. This section will look at a notable example of the risks of hallucination
    and overreliance using these code generation tools.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这起事件围绕使用LLM作为编码助手展开。现在，开发者使用LLM来辅助他们编写代码已经变得司空见惯。开发者可能会使用通用聊天机器人，如ChatGPT，或者专门的副驾驶，如GitHub
    Copilot。2023年6月，GitHub进行的一项[调查](https://oreil.ly/tcy1y)显示，92%在大型公司工作的开发者正在使用LLM来帮助他们编码。本节将探讨使用这些代码生成工具的风险，包括幻觉和过度依赖的显著例子。
- en: These days, a substantial portion of code written uses open source libraries.
    This includes code written by AI coding assistants, which may leverage existing
    open source libraries to make code more compact or efficient. Usually, this works
    fine, but in some cases, these assistants have been shown to hallucinate about
    the existence of various open source libraries. They imagine a useful library
    to solve problems and generate code that uses the imaginary library. This may
    seem harmless enough, but in 2023, the research team at Vulcan Cyber demonstrated
    how [hackers could use this flaw to insert malicious code into applications](https://oreil.ly/oULNb).
    They dubbed the issue simply “AI package hallucination.”
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些天，大量编写的代码使用了开源库。这包括由AI编码助手编写的代码，这些助手可能会利用现有的开源库来使代码更加紧凑或高效。通常情况下，这没问题，但有些情况下，这些助手已被证明对各种开源库的存在产生了幻觉。他们想象出一个有用的库来解决问题，并生成使用这个想象中的库的代码。这看起来可能无害，但在2023年，Vulcan
    Cyber的研究团队展示了[黑客如何利用这个漏洞将恶意代码插入应用程序](https://oreil.ly/oULNb)。他们将这个问题简单地称为“AI包幻觉”。
- en: In this case, the research team crafted the attack by searching through popular
    Stack Overflow questions and asking ChatGPT to solve them. They quickly found
    over 100 hallucinated packages suggested by an assistant bot that were not published
    on any popular code repository. Because these were based on popular questions,
    many other developers will likely ask their AI assistants to generate similar
    code, which may include the same hallucination.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，研究团队通过搜索流行的Stack Overflow问题并要求ChatGPT解决这些问题来构建攻击。他们很快发现了一个由助手机器人建议的超过100个幻觉包，这些包尚未在任何流行的代码仓库中发布。因为这些是基于流行问题的，许多其他开发者可能会要求他们的AI助手生成类似的代码，这可能包括相同的幻觉。
- en: To exploit this hallucination, an attacker needs only to create malicious versions
    of the hallucinated packages, upload them to popular code repositories, and then
    wait for an unsuspecting developer to download and run this code based on AI suggesting
    the package.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要利用这种幻觉，攻击者只需要创建幻觉包的恶意版本，将它们上传到流行的代码仓库，然后等待一个毫无戒心的开发者根据AI的建议下载并运行此代码。
- en: Warning
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: In March 2024, the team at Lasso Security followed up on this study and found
    that up to 30% of the coding questions they asked a popular model resulted in
    at least one hallucinated package!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年3月，Lasso Security团队对此研究进行了跟进，发现他们向一个流行的模型提出的编码问题中，高达30%的结果至少包含一个幻觉包！
- en: The shift by developers from searching for coding solutions online to asking
    AI platforms like ChatGPT for answers created a lucrative opportunity for attackers.
    This scenario signifies a severe security concern as it showcases a novel pathway
    for attackers to exploit AI technologies to propagate malicious code, thereby
    compromising the integrity and security of software applications. While this vulnerability
    has been widely reported, it’s unclear how much this has been exploited in the
    wild. Nonetheless, it’s an essential example of another domain where hallucination
    and overreliance can combine to put an organization at risk.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者从在线搜索编码解决方案转向向ChatGPT等AI平台寻求答案，这为攻击者创造了有利可图的机遇。这种情况表明了一个严重的安全问题，因为它展示了攻击者利用AI技术传播恶意代码的新途径，从而损害了软件应用的完整性和安全性。尽管这种漏洞已被广泛报道，但其在野外被利用的程度尚不清楚。然而，它是一个重要的例子，说明了幻觉和过度依赖如何结合在一起，使组织面临风险。
- en: This incident sheds light on several critical lessons. Firstly, it underlines
    the necessity for rigorous validation of AI-generated outputs, particularly when
    such results can potentially influence software development or other mission-critical
    operations. It’s imperative to have mechanisms to verify the authenticity and
    safety of AI-recommended packages. Secondly, it highlights the importance of continuously
    monitoring and updating AI systems to mitigate the risks associated with outdated
    or inaccurate training data. Lastly, it calls for a collective effort within the
    AI and cybersecurity communities to devise strategies for detecting and preventing
    such exploitation avenues in the future. By learning from such incidents, stakeholders
    can work toward building more robust and secure AI-driven platforms that are resilient
    against evolving threat landscapes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这起事件揭示了几个关键教训。首先，它强调了严格验证AI生成输出的必要性，尤其是当这些结果可能影响软件开发或其他关键任务操作时。必须要有机制来验证AI推荐包的真实性和安全性。其次，它强调了持续监控和更新AI系统的重要性，以减轻与过时或不准确训练数据相关的风险。最后，它呼吁AI和网络安全社区共同努力，制定检测和预防未来此类利用途径的策略。通过从这些事件中学习，利益相关者可以共同努力构建更强大、更安全的AI驱动平台，以抵御不断发展的威胁环境。
- en: Who’s Responsible?
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谁负责？
- en: Development teams working with LLMs sometimes perceive the damage caused by
    hallucinations as a “people problem,” where they blame the user for misinterpreting
    or misusing the information provided. There’s no question that user education
    is important. Just as people learned that they can’t trust all the information
    they find on the web, people will grow more sophisticated in examining erroneous
    information given to them by a chatbot or copilot.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与大型语言模型（LLMs）合作的开发团队有时会将幻觉造成的损害视为“人为问题”，他们指责用户误解或误用了提供的信息。毫无疑问，用户教育非常重要。正如人们学会了他们不能信任在网络上找到的所有信息一样，人们将变得更加成熟，以检查聊天机器人或辅助驾驶员给他们提供的错误信息。
- en: However, as developers, we are responsible for ensuring the information provided
    by our software is as accurate as possible. The ripple effect of such misinformation
    can be profound, especially in critical domains such as the health care, legal,
    or financial sectors where the stakes are high. This accentuates the need for
    developers to invest in mechanisms to identify and rectify hallucinations or erroneous
    information before they reach the user.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作为开发者，我们负责确保我们的软件提供的信息尽可能准确。这种错误信息的涟漪效应可能非常深远，尤其是在医疗保健、法律或金融等高风险领域。这强调了开发者投资于识别和纠正幻觉或错误信息，在它们到达用户之前进行纠正的必要性。
- en: 'Our duty as developers extends beyond merely creating sophisticated AI systems.
    It encompasses fostering a safe and reliable ecosystem where users can interact
    with AI with a reasonable assurance of accuracy and reliability. This responsibility
    calls for a multifaceted approach: improving the system to reduce hallucinations,
    implementing robust output filtering mechanisms to catch and correct errors, and
    fostering a culture of continuous improvement and learning from past mistakes.
    Additionally, educating users about the potential limitations and the degree of
    reliability of LLMs is crucial. It helps nurture an informed user base that can
    engage with AI systems judiciously, while being mindful of the risks.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发者，我们的职责不仅限于创建复杂的AI系统。它包括培养一个安全可靠的环境，让用户可以与AI互动，并有一定程度的准确性和可靠性保证。这项责任需要多方面的方法：改进系统以减少幻觉，实施强大的输出过滤机制以捕捉和纠正错误，以及培养持续改进和从过去的错误中学习的文化。此外，教育用户关于LLM的潜在局限性和可靠性的程度至关重要。这有助于培养一个了解的用户群体，他们可以明智地与AI系统互动，同时注意风险。
- en: The case studies discussed in this chapter illustrate the differing legal responsibilities.
    In the instance involving lawyers using fictitious legal precedents generated
    by ChatGPT, the court placed the responsibility squarely on the professionals.
    As sophisticated users, the lawyers were expected to verify the authenticity of
    the information before its submission in legal documents. Their failure to do
    so led to significant repercussions, highlighting the critical importance of professional
    diligence in using AI tools.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的案例研究说明了不同的法律责任。在涉及律师使用ChatGPT生成的虚构法律先例的案例中，法院将责任明确地放在了专业人士身上。作为高级用户，律师被期望在提交法律文件之前验证信息的真实性。他们未能做到这一点导致了重大的后果，突显了在利用AI工具时专业审慎的至关重要性。
- en: In contrast, the Air Canada chatbot scenario resulted in the company being held
    liable for the misleading information provided to the consumer. This case underscores
    that corporations, especially in consumer-facing roles, must ensure their outputs
    are accurate and reliable. The tribunal’s decision reflects a growing legal consensus
    that companies cannot deflect responsibility for AI-generated content, reinforcing
    the expectation that businesses must safeguard consumer interactions with their
    systems. These cases collectively stress the need for clear guidelines and accountability
    in using AI, irrespective of the user’s sophistication level.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，加拿大航空聊天机器人的案例导致公司因向消费者提供误导性信息而承担责任。这个案例强调了公司，尤其是在面向消费者的角色中，必须确保其输出是准确可靠的。仲裁庭的决定反映了日益增长的司法共识，即公司不能推卸对AI生成内容的责任，强化了企业必须保护其系统与消费者互动的期望。这些案例共同强调了在使用AI时需要明确的指南和问责制，无论用户的复杂程度如何。
- en: Mitigation Best Practices
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓解最佳实践
- en: Hallucinations are going to happen. It’s an inherent property of current LLM
    technology. Our job as application developers is twofold. First, we should work
    to minimize the likelihood of hallucinations by our application, and second, we
    want to reduce the damage when they occur. Let’s look at options.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉是不可避免的。这是当前LLM技术的固有属性。作为应用开发者，我们的任务有两方面。首先，我们应该努力通过我们的应用来最小化幻觉的可能性，其次，我们希望在幻觉发生时减少其造成的损害。让我们看看有哪些选择。
- en: Expanded Domain-Specific Knowledge
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展特定领域的知识
- en: In the world of LLMs, domain-specific knowledge isn’t just a nice-to-have; it’s
    often essential for maximizing utility and minimizing the risk of hallucinations.
    When we focus an LLM on a specific domain—whether that’s health care, law, finance,
    or any other field—it has the potential to provide more accurate and contextually
    relevant information. This specialized focus can drastically reduce the chances
    of the model making incorrect or misleading statements, hallmarks of hallucinations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的世界里，特定领域的知识不仅仅是锦上添花；它对于最大化效用和最小化幻觉风险通常是至关重要的。当我们将LLM聚焦于特定领域——无论是医疗保健、法律、金融还是其他任何领域——它都有潜力提供更准确和上下文相关的信息。这种专业化的关注可以极大地减少模型做出错误或误导性陈述的可能性，这是幻觉的标志。
- en: In a previous chapter, we discussed the risks of arming your LLM with dangerous,
    biased, or privileged information. While that chapter emphasized avoiding these
    pitfalls by minimizing data exposure, you must give your model access to more
    domain-specific, factual knowledge to reduce hallucinations.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们讨论了为你的大型语言模型（LLM）配备危险、有偏见或特权信息的风险。虽然那一章强调了通过最小化数据暴露来避免这些陷阱，但你必须让你的模型访问更多特定领域的、事实性的知识，以减少幻觉。
- en: Model fine-tuning for specialization
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型微调以实现专业化
- en: Fine-tuning is a powerful tool for LLM applications to leverage the extensive
    knowledge encapsulated in foundation models while adding a layer of specialization
    for your specific use case. You can achieve this balance of general and specialized
    expertise at a relatively low computational and financial cost compared to training
    a model from scratch. The primary benefit? You obtain a more reliable and domain-specific
    LLM, tailor-made to your application’s unique needs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是LLM应用的一个强大工具，可以在利用基础模型中封装的广泛知识的同时，为你的特定用例添加一层专业化。与从头开始训练模型相比，你可以在相对较低的计算和财务成本下实现这种通用和专业化知识的平衡。主要好处是？你获得了一个更可靠、更特定领域的LLM，量身定制以满足你应用的独特需求。
- en: The process of fine-tuning helps narrow the LLM’s scope to be more in line with
    your domain-specific objectives. Fine-tuning optimizes the model’s utility and
    is a critical mitigating strategy against hallucinations. The more specialized
    a model is, the lower the probability of generating incorrect or out-of-context
    responses in the form of hallucinations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 微调过程有助于缩小LLM的范围，使其更符合你的特定领域目标。微调优化了模型的效用，是减轻幻觉的关键缓解策略。模型越专业化，生成错误或不合时宜的幻觉形式的不正确或离题响应的概率就越低。
- en: By fine-tuning your foundation model, you essentially transform it into a specialist.
    This higher level of specialization makes the LLM more trustworthy in critical
    operations, be it medical diagnoses, legal interpretations, or financial analyses.
    Fine-tuning is an important tactic in achieving the dual objectives of mitigating
    the risk of hallucinations and reducing their impact, thereby making your LLM
    application more robust and reliable.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过微调你的基础模型，你实际上将其转变为一个专家。这种更高层次的专业化使LLM在关键操作中更加值得信赖，无论是医疗诊断、法律解释还是财务分析。微调是实现减轻幻觉风险和减少其影响的双重目标的重要策略，从而使你的LLM应用更加稳健和可靠。
- en: RAG for enhanced domain expertise
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RAG以增强领域专业知识
- en: RAG introduces a new layer of sophistication to the capabilities of LLMs. It
    combines the strengths of retrieval-based models and sequence-to-sequence generative
    models. A developer uses a well-established, reliable information retrieval technology,
    such as a search engine or database, to collect information relevant to the user’s
    needs. This information can then be fed to the LLM as part of a prompt. The effect
    is similar to allowing the AI to “look up” information from a database or a set
    of documents during the generation process. This hybrid approach enhances the
    model’s contextual awareness, improves accuracy, and provides a mechanism for
    sourcing the generated content, thus contributing to increased trustworthiness.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: RAG为LLM的能力引入了新的复杂层次。它结合了基于检索的模型和序列到序列生成模型的优点。开发者使用一个经过验证、可靠的检索信息技术，如搜索引擎或数据库，来收集与用户需求相关的信息。然后，这些信息可以作为提示的一部分被输入到LLM中。这种混合方法增强了模型的环境意识，提高了准确性，并为生成内容的来源提供了一个机制，从而有助于提高可信度。
- en: When you’ve fine-tuned your LLM to be a domain-specific expert, the next logical
    step is to equip it with the best available reference materials, much like a real-world
    professional. Doctors, lawyers, and other experts seldom rely solely on their
    memory; they have a rich library of books, journals, and databases to consult
    for the most up-to-date and accurate information.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将你的LLM微调为特定领域的专家后，下一步合乎逻辑的步骤是为它配备最佳可用参考材料，就像现实世界中的专业人士一样。医生、律师和其他专家很少仅依赖他们的记忆；他们拥有丰富的图书、期刊和数据库，以获取最新和最准确的信息。
- en: Implementing RAG in your domain-specific LLM application is akin to giving it
    a virtual library filled with specialized knowledge. This curated resource can
    include textbooks, research papers, guidelines, or other credible material that
    can guide the model’s responses. RAG, combined with fine-tuning, amplifies the
    utility and reliability of your application and minimizes the risks associated
    with hallucinations and overreliance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在你特定领域的LLM应用中实施RAG（检索增强生成）相当于给它提供了一个虚拟图书馆，里面充满了专业知识。这个精选资源可以包括教科书、研究论文、指南或其他可信材料，这些材料可以指导模型生成响应。RAG与微调相结合，增强了应用的效用和可靠性，并最大限度地减少了与幻觉和过度依赖相关的风险。
- en: Note
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Not all incorrect statements by an LLM should be classified as hallucinations.
    The core definitions most experts use for hallucinations involve an LLM’s low-confidence
    token sequence prediction being stated in a high-confidence fashion. However,
    incorrect statements from an LLM could also result from false training data or
    faulty data retrieved from a database or web page during RAG. It could even result
    from other, more traditional, coding errors.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有LLM的错误陈述都应该被归类为幻觉。大多数专家用于定义幻觉的核心定义涉及LLM的低置信度标记序列预测以一种高置信度的形式被陈述。然而，LLM的错误陈述也可能源于错误的数据或从数据库或网页检索到的错误数据，甚至在RAG过程中也可能源于其他更传统的编码错误。
- en: Chain of Thought Prompting for Increased Accuracy
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链提示以增加准确性
- en: After fine-tuning your model and enhancing it with RAG for domain-specific expertise,
    another option for reducing hallucinations and bolstering reliability is *chain
    of thought* (CoT) reasoning. As we’ve established, hallucinations can lead to
    misleading or dangerous outputs, and CoT reasoning offers a structured approach
    to counteract this problem by enhancing the LLM’s logical reasoning capabilities.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调你的模型并使用RAG增强其特定领域专业知识之后，减少幻觉并提高可靠性的另一个选项是*思维链*（CoT）推理。正如我们已经建立的，幻觉可能导致误导或危险的结果，而思维链推理通过增强LLM的逻辑推理能力，提供了一种结构化的方法来对抗这个问题。
- en: CoT reasoning encourages an LLM to follow a logical sequence of steps or a reasoning
    pathway. Instead of generating a response based solely on the immediate input,
    the developer prompts the LLM to consider intermediate reasoning steps, breaking
    down complex problems into subproblems and addressing them systematically. CoT
    is particularly beneficial in complex tasks, such as medical diagnoses, legal
    reasoning, or intricate technical troubleshooting, where a misstep can have serious
    consequences.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链推理鼓励LLM遵循逻辑步骤序列或推理路径。开发者不是仅基于即时输入生成响应，而是提示LLM考虑中间推理步骤，将复杂问题分解为子问题并系统地解决它们。思维链在复杂任务中特别有益，例如医疗诊断、法律推理或复杂的技术故障排除，在这些任务中，任何失误都可能产生严重后果。
- en: 'The benefits of CoT reasoning include:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链（CoT）推理的好处包括：
- en: Reduced hallucinations
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 减少幻觉
- en: A structured approach to reasoning can significantly mitigate the risks associated
    with hallucinations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 采用结构化的推理方法可以显著降低与幻觉相关的风险。
- en: Enhanced accuracy
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 提高准确性
- en: When an LLM reasons through problems step by step, the likelihood of arriving
    at an accurate solution is higher.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个大型语言模型（LLM）逐步推理问题时，得出准确解决方案的可能性更高。
- en: Self-evaluation
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 自我评估
- en: Chain of thought reasoning enables an LLM to assess its own reasoning process,
    identifying and correcting errors along the way. This act of self-evaluation increases
    the reliability of the generated content, thus reducing the risks associated with
    overreliance on the model’s outputs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链推理使LLM能够评估其推理过程，在过程中识别和纠正错误。这种自我评估行为增加了生成内容的可靠性，从而降低了过度依赖模型输出的风险。
- en: Let’s look at a simple example to help illustrate the concept.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来帮助说明这个概念。
- en: '[PRE0]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A model might incorrectly add up the numbers without considering the quantities
    and prices for each item, leading to an inaccurate answer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型可能会错误地加总数字，而没有考虑每个项目的数量和价格，从而导致答案不准确。
- en: '[PRE1]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By breaking down the problem into sequential steps and explicitly guiding the
    model through each part of the calculation, the CoT prompt helps ensure that the
    model considers all parts of the problem and how they interact, leading to a more
    accurate response. The model is more likely to apply multiplication for the quantities
    of each item correctly and then add the totals together in the final step.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将问题分解为连续步骤并明确指导模型通过计算的每一部分，CoT提示有助于确保模型考虑问题的所有部分及其相互作用，从而得出更准确的响应。模型更有可能正确应用乘法来计算每个项目的数量，然后在最后一步将总数相加。
- en: Tip
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: There are increasingly more sophisticated examples of how to use CoT. These
    include “zero-shot” techniques that ask the LLM to create its own detailed steps
    to solve a complex problem. Research is ongoing and fast-paced, so check the current
    literature for advances in this promising area for reducing hallucinations and
    increasing accuracy.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用CoT的复杂示例越来越多。这包括“零样本”技术，要求LLM创建解决复杂问题的详细步骤。研究正在进行且进展迅速，因此请查阅当前文献，了解这一有前景领域的最新进展，以减少幻觉并提高准确性。
- en: CoT reasoning complements fine-tuning and RAG as a multipronged strategy for
    minimizing hallucinations and maximizing reliability. By layering these techniques,
    developers can significantly improve the robustness of LLM applications, ensuring
    they are better suited for complex and critical tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: CoT推理作为减少幻觉和最大化可靠性的多方面策略，补充了微调和RAG。通过分层这些技术，开发者可以显著提高LLM应用的鲁棒性，确保它们更适合复杂和关键任务。
- en: 'Feedback Loops: The Power of User Input in Mitigating Risks'
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反馈循环：用户输入在减轻风险中的力量
- en: 'While implementing various technological solutions like fine-tuning, RAG, and
    CoT reasoning can significantly improve the reliability of your LLM application,
    it’s crucial to remember that the end users often provide the most valuable insights
    into the system’s performance. Establishing a feedback loop allows users to flag
    problematic or misleading outputs, creating an additional layer of safety and
    quality assurance. There are several ways to collect feedback:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实施各种技术解决方案，如微调、RAG和CoT推理可以显著提高LLM应用的可靠性，但记住，最终用户往往提供了对系统性能最有价值的见解。建立反馈循环允许用户标记问题性或误导性输出，增加一层安全性和质量保证。收集反馈的方法有几种：
- en: Flagging system
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 标记系统
- en: Integrate a simple interface where users can flag inaccurate, biased, or problematic
    responses. The easier you make this process, the more likely users will participate.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 集成一个简单的界面，用户可以在其中标记不准确、有偏见或问题性回答。您使此过程越简单，用户参与的可能性就越大。
- en: Rating scale
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 评分量表
- en: Along with flagging, offer a rating scale for users to gauge the accuracy or
    helpfulness of the response. This quantitative data will assist in your ongoing
    model evaluation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标记之外，还提供评分量表，让用户评估回答的准确性或有用性。这些定量数据将有助于您对模型进行持续评估。
- en: Comment box
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 评论框
- en: Provide an optional comment box for users willing to give more detailed feedback
    describing what they found misleading or problematic about the output.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为愿意提供更详细反馈的用户提供可选的评论框，描述他们发现输出中哪些部分具有误导性或问题性。
- en: 'Once feedback is collected, it needs to be systematically analyzed to understand:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦收集到反馈，就需要系统地分析以了解：
- en: Recurring issues
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 重复性问题
- en: Are there patterns of hallucinations or inaccuracies in specific domains or
    types of queries?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定领域或查询类型中，是否存在幻觉或不准确性的模式？
- en: Severity
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 严重性
- en: Is the error a minor inconvenience, or could it potentially lead to severe consequences?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 错误是一个小的不便，还是可能引发严重后果？
- en: Underlying causes
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在原因
- en: What might be causing these issues? Is it a lack of domain-specific knowledge,
    or is the reasoning process flawed?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题可能是什么原因造成的？是缺乏特定领域的知识，还是推理过程有缺陷？
- en: 'Based on this analysis, the development team can then:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一分析，开发团队可以：
- en: Fine-tune further
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步微调
- en: Use the feedback to improve the model’s domain-specific performance or general
    reasoning capabilities.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 利用反馈来提高模型在特定领域的性能或通用推理能力。
- en: Enhance CoT reasoning
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 增强CoT推理
- en: If feedback suggests the model fails at logical reasoning, consider more targeted
    CoT prompting or supervised reasoning enhancements.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果反馈表明模型在逻辑推理方面失败，请考虑使用更针对性的CoT提示或监督推理增强。
- en: Enhance reference material in RAG
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 增强RAG中的参考资料
- en: If the model’s answers are consistently inaccurate in a particular domain, perhaps
    the RAG reference material must be updated or expanded.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型在特定领域的答案持续不准确，可能需要更新或扩展RAG（检索增强生成）参考材料。
- en: The feedback loop is not a one-off solution, but rather an ongoing process.
    Continually engaging with your user base and adapting your model based on its
    feedback ensures a continuously improving system. This adaptive approach enhances
    your application’s reliability and helps maintain user trust.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈循环不是一个一次性解决方案，而是一个持续的过程。持续与用户群体互动并根据其反馈调整你的模型确保系统持续改进。这种适应性方法提高了应用程序的可靠性，并有助于维持用户信任。
- en: Clear Communication of Intended Use and Limitations
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清晰传达预期用途和限制
- en: As we navigate the complexities of mitigating hallucinations and refining LLMs’
    capabilities, we must recognize the importance of transparency in application
    development. An LLM might be a marvel of technology, but it’s far from perfect.
    Clear, upfront communication about its intended uses, strengths, and limitations
    is not just ethical—it’s an essential aspect of building trust and managing the
    expectations of your user base.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们应对缓解幻觉和改进LLM（大型语言模型）能力复杂性的过程中，我们必须认识到在应用程序开发中透明度的重要性。一个LLM可能是一项技术的奇迹，但它远非完美。关于其预期用途、优势和局限性的清晰、坦率的沟通不仅符合道德规范，而且是建立信任和管理用户群体期望的必要方面。
- en: 'First, let’s review the areas where intended use documentation can be important:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下预期用途文档可能重要的领域：
- en: Intended use
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 预期用途
- en: Clearly outline what you designed your application to accomplish. Is it a specialized
    tool for legal professionals or a general-purpose assistant? Understanding the
    scope of the application helps users make informed decisions on how best to use
    it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 明确说明你设计应用程序要实现的目标。它是为法律专业人士设计的专用工具，还是一款通用助手？了解应用程序的范围有助于用户做出明智的决定，以最佳方式使用它。
- en: Limitations
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性
- en: Acknowledge the LLM’s constraints, including areas where it might not have domain-specific
    expertise or where the risk of hallucination is higher. Be explicit about what
    you exclude from the application’s intended field of use.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 承认LLM的局限性，包括它可能没有特定领域专业知识或幻觉风险较高的领域。明确说明你从应用程序的预期使用范围内排除的内容。
- en: Data handling
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理
- en: Share your data protection and privacy protocols. Make it clear how user data
    will be stored, processed, and protected.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 分享你的数据保护和隐私协议。明确说明用户数据将如何存储、处理和保护。
- en: Feedback mechanisms
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈机制
- en: Inform users that you have a feedback loop for continuous improvement and explain
    how they can contribute to this process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通知用户你有一个用于持续改进的反馈循环，并解释他们如何参与这一过程。
- en: 'Once you’ve decided on the items you wish to communicate with the user, here
    are some good options for how to communicate these:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你决定要向用户传达的项目，以下是一些有效的沟通方式：
- en: User interface
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 用户界面
- en: Use tooltips, pop-ups, or an FAQ within the application to provide quick reminders
    or explanations about the model’s intended use and limitations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用应用内的工具提示、弹出窗口或常见问题解答（FAQ）提供关于模型预期用途和限制的快速提醒或解释。
- en: Documentation
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 文档
- en: Create detailed guides or manuals that users can refer to for more information
    on what the system can and cannot do.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 创建详细的指南或手册，用户可以参考以获取有关系统可以做什么和不能做什么的更多信息。
- en: Introductory tutorials
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 初级教程
- en: Offer walk-throughs or tutorials when a user first engages with the application,
    focusing on illustrating both its capabilities and constraints.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户首次与应用程序互动时，提供操作演示或教程，重点关注展示其功能和限制。
- en: Update logs
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 更新日志
- en: Maintain a version history or update log where users can see what improvements
    have been made and what issues are being worked on.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 维护一个版本历史或更新日志，让用户可以看到已进行的改进和正在解决的问题。
- en: Transparency is more than just a one-and-done affair. As your model evolves—improving
    its capabilities, expanding its domain-specific knowledge, enhancing its reasoning
    abilities—it’s crucial to update the user community on these developments. Likewise,
    if new limitations or vulnerabilities are discovered, these should be communicated
    as promptly and transparently as possible.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度不仅仅是单次事件。随着你的模型的发展——提高其能力、扩展其特定领域的知识、增强其推理能力——及时向用户社区通报这些进展至关重要。同样，如果发现新的限制或漏洞，应尽可能及时和透明地沟通。
- en: Being transparent benefits users and boosts the development team by fostering
    a more engaged and forgiving user base. When people understand a tool’s limitations,
    they are less likely to misuse it and more likely to provide constructive feedback
    that can be used for further refinement. Transparency is an ethical obligation
    and the cornerstone of a mutually beneficial relationship between application
    developers and their users.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度对用户和开发团队都有益，因为它可以培养一个更加投入和宽容的用户群体。当人们了解工具的局限性时，他们不太可能滥用它，更有可能提供可以用于进一步改进的建设性反馈。透明度是一项道德义务，也是应用开发者和用户之间互惠互利关系的基石。
- en: 'User Education: Empowering Users Through Knowledge'
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户教育：通过知识赋权用户
- en: Much like how advanced anti-phishing software alone can’t entirely prevent phishing
    attacks, technical mitigations can only minimize the risks of LLM hallucinations
    and overreliance. Human awareness and education are crucial additional layers
    of defense. Corporate security teams train employees to recognize phishing attempts,
    double-check URLs, and be skeptical of unsolicited communications. Similarly,
    while we strive to minimize overreliance on LLMs, we must also cultivate an informed
    and vigilant user base. Educating users about the real trust issues and equipping
    them with cross-verification strategies is vital to ensuring they understand the
    limitations and best practices associated with using LLMs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 就像高级反钓鱼软件本身无法完全防止钓鱼攻击一样，技术缓解措施只能最小化LLM幻觉和过度依赖的风险。人类意识和教育是至关重要的额外防御层。企业安全团队培训员工识别钓鱼尝试、双重检查URL并对未经请求的通信持怀疑态度。同样，虽然我们努力减少对LLM的过度依赖，但我们还必须培养一个知情和警觉的用户群体。教育用户关于真实信任问题并为他们提供交叉验证策略是确保他们理解与使用LLM相关的局限性和最佳实践的关键。
- en: 'As you build out your education plan, here are some suggested topics to cover:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建您的教育计划时，以下是一些建议的主题：
- en: Understanding trust issues
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 理解信任问题
- en: Make users aware that while LLMs are advanced and often accurate, they are not
    infallible. Hallucinations can happen, and overreliance without verification can
    have significant consequences.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让用户知道，虽然大型语言模型（LLM）先进且通常准确，但它们并非完美无缺。幻觉现象可能发生，未经验证的过度依赖可能会产生重大后果。
- en: Cross-checking mechanisms
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉检查机制
- en: Educate users to cross-reference the information the LLM provides. Depending
    on the domain, this might include checking multiple trusted sources, consulting
    experts, or running empirical tests.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 教育用户交叉验证LLM提供的信息。根据领域不同，这可能包括检查多个可信来源、咨询专家或进行实证测试。
- en: Situational awareness
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 情境意识
- en: Encourage users to assess the information’s criticality. A higher level of trust
    might be acceptable for routine or noncritical tasks. However, you should encourage
    more rigorous verification for critical safety, finance, or legal jobs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励用户评估信息的紧迫性。对于常规或非关键任务，可能可以接受较高水平的信任。然而，您应该鼓励对关键安全、金融或法律工作进行更严格的验证。
- en: Feedback options
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈选项
- en: Make users aware of the feedback loop feature in your application. Their active
    participation in reporting anomalies can contribute to the system’s ongoing improvement.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让用户了解您应用程序中的反馈循环功能。他们积极参与报告异常情况可以帮助系统持续改进。
- en: 'Here are some suggested methods you can use to deliver educational content
    to your users:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些您可以用来向用户传递教育内容的建议方法：
- en: In-app guides
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 应用内指南
- en: Short, interactive guides or videos can introduce these concepts to users as
    they use the application.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 简短、互动的指南或视频可以在用户使用应用程序时向他们介绍这些概念。
- en: Resource library
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 资源库
- en: Create a repository of articles, FAQs, and how-to guides that detail these topics.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含文章、常见问题解答和如何指南的存储库，详细说明这些主题。
- en: Community forums
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 社区论坛
- en: An active user forum can help to quickly disseminate best practices and news,
    providing an extra layer of education and awareness.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃的用户论坛可以帮助快速传播最佳实践和新闻，提供额外的教育和意识层。
- en: Email campaigns
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 邮件营销
- en: Regular updates can be sent to users outlining new features, limitations, or
    educational material, ensuring that even infrequent users stay informed.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 定期向用户发送更新，概述新功能、限制或教育材料，确保即使是不常使用的用户也能保持信息畅通。
- en: While the development team focuses on technical mitigations like fine-tuning,
    RAG, and CoT reasoning, it’s important to remember that a well-educated user base
    is also a robust line of defense against the risks posed by LLMs. Thus, a balanced,
    comprehensive approach that combines technological advancements with ongoing user
    education is the optimal strategy for mitigating risks and enhancing reliability.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然开发团队专注于技术缓解措施，如微调、RAG 和 CoT 推理，但记住，一个受过良好教育的用户基础也是抵御 LLM 带来的风险的一个强大防线。因此，结合技术进步和持续用户教育的平衡、综合方法是减轻风险和增强可靠性的最佳策略。
- en: Warning
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'In a final twist of irony for this chapter, it seems that the lack of a sense
    of humor in LLMs is now a risk factor you must account for as well. Recent examples
    have highlighted this quirk: Google’s LLM-enhanced Search feature has offered
    dubious advice, such as recommending glue as a pizza topping, suggesting eating
    rocks as a nutritional tip, and even advising jumping off a bridge to cure depression.
    These bizarre recommendations were traced to nonauthoritative but popular websites
    like Reddit and The Onion. Unfortunately, without a sense of humor, the LLMs pass
    along these joke punchlines as if they were facts. This is just one more edge
    condition for you to consider.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章最后的讽刺转折中，似乎 LLM 缺乏幽默感现在也成为你必须考虑的风险因素。最近的例子突出了这一特点：谷歌的 LLM 增强搜索功能提供了可疑的建议，例如建议将胶水作为披萨配料，建议吃石头作为营养建议，甚至建议跳桥来治疗抑郁症。这些奇怪的建议追溯到非权威但流行的网站，如
    Reddit 和 The Onion。不幸的是，由于缺乏幽默感，LLM 将这些玩笑的结尾当作事实传递。这又是一个你需要考虑的边缘情况。
- en: Conclusion
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Addressing the risks of damage due to overreliance on hallucination-prone LLMs
    requires a comprehensive, multilayered approach. This challenge is best met through
    technological advancements, active user involvement, transparent communication,
    and thorough user education.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 解决过度依赖幻觉倾向的 LLM 带来的损害风险需要全面、多层次的方法。这个挑战最好通过技术进步、积极的用户参与、透明沟通和彻底的用户教育来解决。
- en: The first step is acknowledging the issue. Your first line of defense must be
    to reduce hallucinations to a minimum. Consider narrowing your application’s field
    of use to a specific domain, and then equip your LLM to become a world-class expert
    using techniques such as fine-tuning, RAG, and CoT.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是承认问题。你的第一道防线必须是将幻觉降至最低。考虑将你的应用程序的使用范围缩小到特定领域，然后利用微调、RAG 和 CoT 等技术，让你的 LLM
    成为世界级的专家。
- en: By combining technological safeguards, user feedback loops, transparent communication,
    and robust user education, the strategy for mitigating the risks associated with
    overreliance on LLMs becomes well rounded. Each of these elements contributes
    individually to reducing the risks of hallucinations and synergistically helps
    build a more resilient, transparent, and user-friendly system.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合技术保障、用户反馈循环、透明沟通和强大的用户教育，减轻过度依赖 LLM 的风险的策略变得全面。这些元素各自有助于降低幻觉的风险，并协同帮助构建一个更具弹性、透明和用户友好的系统。
