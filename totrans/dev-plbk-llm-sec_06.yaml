- en: Chapter 6\. Do Language Models Dream of Electric Sheep?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Among all the excitement about advances in LLMs, few phenomena captivate and
    perplex like their so-called *hallucinations*. It’s almost as if these computational
    entities, deep within their myriad layers, occasionally drift into a dreamlike
    state, creating wondrous and bewildering narratives. Like a human’s dreams, these
    hallucinations can be reflective, absurd, or even prophetic, providing insights
    into the complex interplay between training data and the model’s learned interpretations.
  prefs: []
  type: TYPE_NORMAL
- en: In the world of LLMs, the term “hallucination” might evoke images of vivid and
    whimsical creations, but in reality, it signifies a more mundane statistical anomaly.
    At its core, a hallucination is the model’s attempt to bridge gaps in its knowledge
    using the patterns it has gleaned from its training data. While it might be termed
    “imaginative,” it’s essentially the LLM making an educated guess when faced with
    unfamiliar input or scenarios. However, these guesses can manifest as confident
    yet unfounded assertions, revealing the model’s struggle to differentiate between
    well-learned facts and the statistical noise within its training data.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs do not provide easily usable probability scores like some other “predictive”
    AI algorithms. For example, a vision classifier algorithm may return a probability
    as a percent. It might show a 79% chance that a particular image depicts a monkey.
    Thus, a user of that model gets a sense of how strongly the model “feels” about
    the prediction. LLMs simply predict the next token or tokens in a sequence. While
    the LLM uses a complex statistical model to do this, a certainty score for the
    overall response to a prompt isn’t typically part of the output. This can leave
    the end user unsure whether the LLM has returned a well-grounded reaction to the
    prompt or a weak, statistical extrapolation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The term “hallucination” is unpopular with some because it personifies the LLM
    and makes its flaws seem less critical. Some literature now refers to this phenomenon
    as *confabulation*. However, hallucination is far more prevalent, so we’ll use
    it in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'This nuanced dance between fact and fiction in LLM outputs brings us to the
    crux of the challenge: *overreliance*. As humans, we are naturally inclined to
    trust results that are presented confidently, especially when they emanate from
    sophisticated computer software. Yet, it’s this very trust that can steer us astray.
    When LLMs hallucinate, they often don’t waver in their confidence, making it hard
    to discern genuine knowledge from imperfect statistical artifacts. The danger
    lies in the hallucination and also in our propensity to take these dreamlike utterances
    at face value, potentially leading to misinformation, missteps, and broader implications
    in real-world applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Overreliance refers to the excessive trust in the capabilities and exactness
    of LLM elaborations. Excessive confidence in LLM output, especially when hallucinations,
    errors, or biased data input are present, can lead to damaging outputs, particularly
    in professional or safety-critical environments. A significant example is trusting
    an LLM to provide medical advice without sufficient testing.
  prefs: []
  type: TYPE_NORMAL
- en: Why Do LLMs Hallucinate?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core reason for hallucinations lies in the LLM’s operational mechanism,
    which is geared toward pattern matching and statistical extrapolation rather than
    factual verification. While they acquire knowledge through training on vast training
    datasets, LLMs often lack specific, actual knowledge. Their operation is rooted
    in identifying patterns in the input data and attempting to match these patterns
    with those learned during training. This pattern matching occurs without a real-world
    understanding, which can lead to the generation of hallucinated text, especially
    when faced with ambiguous or novel input prompts.
  prefs: []
  type: TYPE_NORMAL
- en: The quality and nature of the training data significantly impact the likelihood
    and extent of hallucinations. Biases, inaccuracies, or noise in the training data
    can mislead the model into generating biased or incorrect text.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations present a substantial challenge in using LLMs for critical or
    sensitive applications. They underline AI development’s inherent intricacies and
    challenges, spotlighting the gap between statistical pattern matching and real-world,
    factual understanding. The hallucination phenomenon in LLMs opens a window into
    the broader discourse on the limitations and the ethical implications of deploying
    large-scale AI models in real-world scenarios without a robust mechanism for factual
    verification or contextual understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Hallucinations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we dig further into this, let’s look at some of the types of hallucinations
    we’ll likely experience. Doing so will help us understand the implications and
    mitigations:'
  prefs: []
  type: TYPE_NORMAL
- en: Factual inaccuracies
  prefs: []
  type: TYPE_NORMAL
- en: LLMs may produce factually incorrect statements due to the model’s lack of specific
    knowledge or to misinterpreting the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupported claims
  prefs: []
  type: TYPE_NORMAL
- en: Similar to factual inaccuracies, LLMs might generate baseless claims, which
    can be detrimental, especially in sensitive or critical contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Misrepresentation of abilities
  prefs: []
  type: TYPE_NORMAL
- en: LLMs might give the illusion of understanding advanced topics such as chemistry,
    even when they don’t. They can convincingly double-talk about a topic, misleading
    users about their level of understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Contradictory statements
  prefs: []
  type: TYPE_NORMAL
- en: LLMs might generate sentences contradicting previous statements or the user’s
    prompt. For instance, they might first state, “Cats are afraid of water,” and
    later claim, “Cats love to swim in water.”
  prefs: []
  type: TYPE_NORMAL
- en: With these in mind, let’s look at real-world examples and their impact on application
    providers and customers.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll examine four cases where hallucinations intersected with
    overreliance and caused harm. These should help drive home the need to address
    these issues in your LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: Imaginary Legal Precedents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2023, in a US federal court, a judge levied fines on two lawyers and their
    law firm for negligent oversight in legal practice. The lawyers had submitted
    fictitious legal research in an aviation injury case. The fabricated case law,
    as it turned out, was generated by ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: The issue came to light during a routine legal proceeding when the opposition
    discovered that the legal citations provided by the lawyers were not merely erroneous
    but entirely fabricated. The lawyers used a general-purpose LLM, which did not
    have specific legal training or data access, for their research. Their unverified
    reliance on the AI output led to the submission of six fictitious case citations
    in a legal brief. The judge later judged this action as an act of bad faith. The
    repercussions of this act were not confined to the courtroom but resonated across
    the legal and tech communities, marking a significant incident in the discourse
    surrounding AI’s role in legal practice.
  prefs: []
  type: TYPE_NORMAL
- en: Because the judge imposed substantial fines on the lawyers and their firm, the
    incident emerged as a cautionary tale about overreliance on AI in critical domains.
    It showcased the necessity of human verification and due diligence, especially
    in a field where accuracy and authenticity are paramount.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the impacts this incident had on several different parties to
    ensure we can see the full scope of the problems caused:'
  prefs: []
  type: TYPE_NORMAL
- en: On the LLM provider
  prefs: []
  type: TYPE_NORMAL
- en: The incident spotlighted the potential risks of using OpenAI’s products in critical
    and formal domains like legal practice. It raised questions about the reliability
    and safe usage of ChatGPT and potentially impacted OpenAI’s reputation. The misuse
    of ChatGPT in a legal setting could prompt further scrutiny and demands by legislators
    for stricter regulation on the use and deployment of OpenAI’s products in critical
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: On LLM customers
  prefs: []
  type: TYPE_NORMAL
- en: The repercussions were immediate and severe for the lawyers involved. They faced
    financial penalties, and their professional reputation was significantly tarnished.
    This incident is a deterrent for other legal professionals, making them wary of
    relying on AI tools for critical tasks without thorough verification.
  prefs: []
  type: TYPE_NORMAL
- en: On the legal profession
  prefs: []
  type: TYPE_NORMAL
- en: The event echoed across the legal profession, emphasizing the importance of
    human verification and the dangers of unquestioningly trusting AI-generated content.
    It highlighted a pressing need for educating and alerting legal professionals
    about the limitations and correct usage of AI tools in legal practice.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, this event underscores the indispensable value of verification.
    Legal professionals, and indeed all users of AI, should invest in verifying the
    information generated by AI tools. Further, the incident brings to light the necessity
    for robust guidelines that govern the use of AI in legal practice and other critical
    domains. Establishing such policies, including verification procedures to ensure
    the accuracy and reliability of AI-generated information, will act as a bulwark
    against similar incidents. The story also underscores the need to promote the
    ethical use of AI tools. Creating awareness about potential misuse and stressing
    the importance of adhering to professional standards when employing AI for critical
    tasks emerges as a pivotal lesson.
  prefs: []
  type: TYPE_NORMAL
- en: LLM providers such as OpenAI should provide better guidelines, warnings, and
    education about their AI tools’ proper use and limitations to prevent misuse and
    ensure users are fully informed about the capabilities and potential risks. Lastly,
    the incident highlights the need for continuous improvement, urging AI software
    developers and the legal profession to learn from their mistakes and enhance their
    tools’ safety and reliability in critical applications. Through such a reflective
    lens, the incident offers a roadmap toward fostering a responsible AI usage culture
    anchored in verification, education, and ethical practice.
  prefs: []
  type: TYPE_NORMAL
- en: Airline Chatbot Lawsuit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a landmark decision in 2024, Canada’s largest airline, Air Canada, was ordered
    to compensate a customer after a chatbot provided incorrect information regarding
    fares. In this case, Jake Moffatt, a resident of British Columbia, sought information
    from Air Canada’s chatbot about the documents necessary for a bereavement fare
    and the possibility of obtaining a retroactive refund. Based on the information
    provided by the chatbot, Moffatt purchased a full-price ticket, believing he could
    secure a refund later. However, when he applied for the refund, Air Canada denied
    it, stating that bereavement rates did not apply to completed travel, contrary
    to the chatbot’s guidance.
  prefs: []
  type: TYPE_NORMAL
- en: Moffatt initiated legal action against Air Canada to recover the fare difference
    after the airline failed to honor the chatbot’s information. Air Canada’s defense
    claimed the chatbot was a “separate legal entity” and responsible for its own
    actions, a stance that was dismissed by the judge as illogical and irresponsible.
  prefs: []
  type: TYPE_NORMAL
- en: The judge ordered Air Canada to pay Moffatt the difference between the full
    fare and the bereavement fare, along with interest and fees. The judge emphasized
    that all information provided on Air Canada’s website, whether through a chatbot
    or a static page, was the airline’s responsibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the impacts of this case from several different angles:'
  prefs: []
  type: TYPE_NORMAL
- en: On Air Canada
  prefs: []
  type: TYPE_NORMAL
- en: The incident brought significant public and legal scrutiny, challenging the
    airline’s approach to AI in customer interactions. It highlighted the need for
    accurate AI-generated communications and the potential reputational damage from
    AI errors.
  prefs: []
  type: TYPE_NORMAL
- en: On AI and legal precedents
  prefs: []
  type: TYPE_NORMAL
- en: The case set a precedent regarding the legal accountability of AI communications
    in business operations. It raised questions about the extent to which companies
    can or should be held liable for AI-generated content.
  prefs: []
  type: TYPE_NORMAL
- en: On consumers and AI
  prefs: []
  type: TYPE_NORMAL
- en: The ruling reinforced consumer rights in the digital age, emphasizing that companies
    cannot absolve themselves of accountability for AI-generated errors.
  prefs: []
  type: TYPE_NORMAL
- en: The case emphasizes the critical importance of accuracy in LLM-generated content
    and the growing legal precedence that inaccuracies can lead to substantial financial
    and reputational penalties for companies. This ruling reinforces the notion that
    businesses cannot disown the outputs of their LLM applications and must treat
    AI communications with the same scrutiny as any other official corporate communication.
    Companies must ensure rigorous testing and continuous monitoring of their AI tools
    to avoid potential legal liabilities and uphold consumer trust. Moreover, the
    financial repercussions highlighted by this case serve as a reminder of the direct
    costs associated with such misinformation.
  prefs: []
  type: TYPE_NORMAL
- en: Unintentional Character Assassination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2023, Brian Hood, mayor of Hepburn Shire in Australia, threatened legal action
    against OpenAI for a defamatory claim generated by the LLM. ChatGPT falsely asserted
    that Hood, then a whistleblower in a foreign bribery scandal, had served jail
    time. According to the suit, this fabricated information, presented as factual
    by the AI, significantly impacted Hood’s reputation and caused distress.
  prefs: []
  type: TYPE_NORMAL
- en: The issue may have stemmed from ChatGPT’s limited training data in this area.
    Without the LLM having access to strongly correlated data related to a user’s
    query, the LLM could have conflated unrelated snippets of information, resulting
    in the demonstrably false claim about Hood. The incident underscored the potential
    dangers of relying on AI-generated information uncritically, especially in sensitive
    domains like public reputation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can better understand the impacts of this case by looking at it from both
    plaintiff’s and defendant’s point of view:'
  prefs: []
  type: TYPE_NORMAL
- en: Hood
  prefs: []
  type: TYPE_NORMAL
- en: The false claim caused Hood mental anguish and threatened his political career.
    The incident highlighted individuals’ vulnerability to AI-generated misinformation
    and the potential for reputational damage.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI
  prefs: []
  type: TYPE_NORMAL
- en: The company has opened itself to expensive and time-consuming litigation. In
    this case, the plaintiff indicated at the time of filing that he might be seeking
    over $200,000 in damages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding those impacts leads us to three lessons you can apply in your
    projects:'
  prefs: []
  type: TYPE_NORMAL
- en: Verification
  prefs: []
  type: TYPE_NORMAL
- en: Robust verification mechanisms are crucial, whether through fact-checking tools,
    human oversight, or a combination. Users must develop a healthy skepticism toward
    AI-generated information.
  prefs: []
  type: TYPE_NORMAL
- en: Education
  prefs: []
  type: TYPE_NORMAL
- en: Educating users about the capabilities and limitations of LLMs is critical to
    promoting responsible and ethical usage.
  prefs: []
  type: TYPE_NORMAL
- en: Regulation
  prefs: []
  type: TYPE_NORMAL
- en: Regulatory frameworks may be necessary to govern the use of LLMs in critical
    domains, ensuring data privacy, algorithmic accountability, and user protection.
    The Hood case highlights the potential need for legal clarification around AI
    responsibility and liability.
  prefs: []
  type: TYPE_NORMAL
- en: The Brian Hood case exemplifies the potential pitfalls of hallucination and
    overreliance in LLMs. It calls for more robust safeguards, user education, and
    responsible application of this powerful technology. Only through a multipronged
    approach can we prevent future harm and ensure AI’s beneficial integration into
    society.
  prefs: []
  type: TYPE_NORMAL
- en: Open Source Package Hallucinations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This incident centers around using LLMs as coding assistants. It has become
    commonplace now for developers to use LLMs to assist them while writing code.
    Developers might use general-purpose chatbots, such as ChatGPT, or dedicated copilots,
    such as GitHub Copilot. A [survey by GitHub](https://oreil.ly/tcy1y) in June 2023
    showed that 92% of developers working in large companies were using LLMs to help
    them code. This section will look at a notable example of the risks of hallucination
    and overreliance using these code generation tools.
  prefs: []
  type: TYPE_NORMAL
- en: These days, a substantial portion of code written uses open source libraries.
    This includes code written by AI coding assistants, which may leverage existing
    open source libraries to make code more compact or efficient. Usually, this works
    fine, but in some cases, these assistants have been shown to hallucinate about
    the existence of various open source libraries. They imagine a useful library
    to solve problems and generate code that uses the imaginary library. This may
    seem harmless enough, but in 2023, the research team at Vulcan Cyber demonstrated
    how [hackers could use this flaw to insert malicious code into applications](https://oreil.ly/oULNb).
    They dubbed the issue simply “AI package hallucination.”
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the research team crafted the attack by searching through popular
    Stack Overflow questions and asking ChatGPT to solve them. They quickly found
    over 100 hallucinated packages suggested by an assistant bot that were not published
    on any popular code repository. Because these were based on popular questions,
    many other developers will likely ask their AI assistants to generate similar
    code, which may include the same hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: To exploit this hallucination, an attacker needs only to create malicious versions
    of the hallucinated packages, upload them to popular code repositories, and then
    wait for an unsuspecting developer to download and run this code based on AI suggesting
    the package.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In March 2024, the team at Lasso Security followed up on this study and found
    that up to 30% of the coding questions they asked a popular model resulted in
    at least one hallucinated package!
  prefs: []
  type: TYPE_NORMAL
- en: The shift by developers from searching for coding solutions online to asking
    AI platforms like ChatGPT for answers created a lucrative opportunity for attackers.
    This scenario signifies a severe security concern as it showcases a novel pathway
    for attackers to exploit AI technologies to propagate malicious code, thereby
    compromising the integrity and security of software applications. While this vulnerability
    has been widely reported, it’s unclear how much this has been exploited in the
    wild. Nonetheless, it’s an essential example of another domain where hallucination
    and overreliance can combine to put an organization at risk.
  prefs: []
  type: TYPE_NORMAL
- en: This incident sheds light on several critical lessons. Firstly, it underlines
    the necessity for rigorous validation of AI-generated outputs, particularly when
    such results can potentially influence software development or other mission-critical
    operations. It’s imperative to have mechanisms to verify the authenticity and
    safety of AI-recommended packages. Secondly, it highlights the importance of continuously
    monitoring and updating AI systems to mitigate the risks associated with outdated
    or inaccurate training data. Lastly, it calls for a collective effort within the
    AI and cybersecurity communities to devise strategies for detecting and preventing
    such exploitation avenues in the future. By learning from such incidents, stakeholders
    can work toward building more robust and secure AI-driven platforms that are resilient
    against evolving threat landscapes.
  prefs: []
  type: TYPE_NORMAL
- en: Who’s Responsible?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Development teams working with LLMs sometimes perceive the damage caused by
    hallucinations as a “people problem,” where they blame the user for misinterpreting
    or misusing the information provided. There’s no question that user education
    is important. Just as people learned that they can’t trust all the information
    they find on the web, people will grow more sophisticated in examining erroneous
    information given to them by a chatbot or copilot.
  prefs: []
  type: TYPE_NORMAL
- en: However, as developers, we are responsible for ensuring the information provided
    by our software is as accurate as possible. The ripple effect of such misinformation
    can be profound, especially in critical domains such as the health care, legal,
    or financial sectors where the stakes are high. This accentuates the need for
    developers to invest in mechanisms to identify and rectify hallucinations or erroneous
    information before they reach the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our duty as developers extends beyond merely creating sophisticated AI systems.
    It encompasses fostering a safe and reliable ecosystem where users can interact
    with AI with a reasonable assurance of accuracy and reliability. This responsibility
    calls for a multifaceted approach: improving the system to reduce hallucinations,
    implementing robust output filtering mechanisms to catch and correct errors, and
    fostering a culture of continuous improvement and learning from past mistakes.
    Additionally, educating users about the potential limitations and the degree of
    reliability of LLMs is crucial. It helps nurture an informed user base that can
    engage with AI systems judiciously, while being mindful of the risks.'
  prefs: []
  type: TYPE_NORMAL
- en: The case studies discussed in this chapter illustrate the differing legal responsibilities.
    In the instance involving lawyers using fictitious legal precedents generated
    by ChatGPT, the court placed the responsibility squarely on the professionals.
    As sophisticated users, the lawyers were expected to verify the authenticity of
    the information before its submission in legal documents. Their failure to do
    so led to significant repercussions, highlighting the critical importance of professional
    diligence in using AI tools.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the Air Canada chatbot scenario resulted in the company being held
    liable for the misleading information provided to the consumer. This case underscores
    that corporations, especially in consumer-facing roles, must ensure their outputs
    are accurate and reliable. The tribunal’s decision reflects a growing legal consensus
    that companies cannot deflect responsibility for AI-generated content, reinforcing
    the expectation that businesses must safeguard consumer interactions with their
    systems. These cases collectively stress the need for clear guidelines and accountability
    in using AI, irrespective of the user’s sophistication level.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigation Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hallucinations are going to happen. It’s an inherent property of current LLM
    technology. Our job as application developers is twofold. First, we should work
    to minimize the likelihood of hallucinations by our application, and second, we
    want to reduce the damage when they occur. Let’s look at options.
  prefs: []
  type: TYPE_NORMAL
- en: Expanded Domain-Specific Knowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the world of LLMs, domain-specific knowledge isn’t just a nice-to-have; it’s
    often essential for maximizing utility and minimizing the risk of hallucinations.
    When we focus an LLM on a specific domain—whether that’s health care, law, finance,
    or any other field—it has the potential to provide more accurate and contextually
    relevant information. This specialized focus can drastically reduce the chances
    of the model making incorrect or misleading statements, hallmarks of hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: In a previous chapter, we discussed the risks of arming your LLM with dangerous,
    biased, or privileged information. While that chapter emphasized avoiding these
    pitfalls by minimizing data exposure, you must give your model access to more
    domain-specific, factual knowledge to reduce hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: Model fine-tuning for specialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning is a powerful tool for LLM applications to leverage the extensive
    knowledge encapsulated in foundation models while adding a layer of specialization
    for your specific use case. You can achieve this balance of general and specialized
    expertise at a relatively low computational and financial cost compared to training
    a model from scratch. The primary benefit? You obtain a more reliable and domain-specific
    LLM, tailor-made to your application’s unique needs.
  prefs: []
  type: TYPE_NORMAL
- en: The process of fine-tuning helps narrow the LLM’s scope to be more in line with
    your domain-specific objectives. Fine-tuning optimizes the model’s utility and
    is a critical mitigating strategy against hallucinations. The more specialized
    a model is, the lower the probability of generating incorrect or out-of-context
    responses in the form of hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: By fine-tuning your foundation model, you essentially transform it into a specialist.
    This higher level of specialization makes the LLM more trustworthy in critical
    operations, be it medical diagnoses, legal interpretations, or financial analyses.
    Fine-tuning is an important tactic in achieving the dual objectives of mitigating
    the risk of hallucinations and reducing their impact, thereby making your LLM
    application more robust and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: RAG for enhanced domain expertise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG introduces a new layer of sophistication to the capabilities of LLMs. It
    combines the strengths of retrieval-based models and sequence-to-sequence generative
    models. A developer uses a well-established, reliable information retrieval technology,
    such as a search engine or database, to collect information relevant to the user’s
    needs. This information can then be fed to the LLM as part of a prompt. The effect
    is similar to allowing the AI to “look up” information from a database or a set
    of documents during the generation process. This hybrid approach enhances the
    model’s contextual awareness, improves accuracy, and provides a mechanism for
    sourcing the generated content, thus contributing to increased trustworthiness.
  prefs: []
  type: TYPE_NORMAL
- en: When you’ve fine-tuned your LLM to be a domain-specific expert, the next logical
    step is to equip it with the best available reference materials, much like a real-world
    professional. Doctors, lawyers, and other experts seldom rely solely on their
    memory; they have a rich library of books, journals, and databases to consult
    for the most up-to-date and accurate information.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing RAG in your domain-specific LLM application is akin to giving it
    a virtual library filled with specialized knowledge. This curated resource can
    include textbooks, research papers, guidelines, or other credible material that
    can guide the model’s responses. RAG, combined with fine-tuning, amplifies the
    utility and reliability of your application and minimizes the risks associated
    with hallucinations and overreliance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Not all incorrect statements by an LLM should be classified as hallucinations.
    The core definitions most experts use for hallucinations involve an LLM’s low-confidence
    token sequence prediction being stated in a high-confidence fashion. However,
    incorrect statements from an LLM could also result from false training data or
    faulty data retrieved from a database or web page during RAG. It could even result
    from other, more traditional, coding errors.
  prefs: []
  type: TYPE_NORMAL
- en: Chain of Thought Prompting for Increased Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After fine-tuning your model and enhancing it with RAG for domain-specific expertise,
    another option for reducing hallucinations and bolstering reliability is *chain
    of thought* (CoT) reasoning. As we’ve established, hallucinations can lead to
    misleading or dangerous outputs, and CoT reasoning offers a structured approach
    to counteract this problem by enhancing the LLM’s logical reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: CoT reasoning encourages an LLM to follow a logical sequence of steps or a reasoning
    pathway. Instead of generating a response based solely on the immediate input,
    the developer prompts the LLM to consider intermediate reasoning steps, breaking
    down complex problems into subproblems and addressing them systematically. CoT
    is particularly beneficial in complex tasks, such as medical diagnoses, legal
    reasoning, or intricate technical troubleshooting, where a misstep can have serious
    consequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of CoT reasoning include:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduced hallucinations
  prefs: []
  type: TYPE_NORMAL
- en: A structured approach to reasoning can significantly mitigate the risks associated
    with hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: Enhanced accuracy
  prefs: []
  type: TYPE_NORMAL
- en: When an LLM reasons through problems step by step, the likelihood of arriving
    at an accurate solution is higher.
  prefs: []
  type: TYPE_NORMAL
- en: Self-evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Chain of thought reasoning enables an LLM to assess its own reasoning process,
    identifying and correcting errors along the way. This act of self-evaluation increases
    the reliability of the generated content, thus reducing the risks associated with
    overreliance on the model’s outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a simple example to help illustrate the concept.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A model might incorrectly add up the numbers without considering the quantities
    and prices for each item, leading to an inaccurate answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: By breaking down the problem into sequential steps and explicitly guiding the
    model through each part of the calculation, the CoT prompt helps ensure that the
    model considers all parts of the problem and how they interact, leading to a more
    accurate response. The model is more likely to apply multiplication for the quantities
    of each item correctly and then add the totals together in the final step.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are increasingly more sophisticated examples of how to use CoT. These
    include “zero-shot” techniques that ask the LLM to create its own detailed steps
    to solve a complex problem. Research is ongoing and fast-paced, so check the current
    literature for advances in this promising area for reducing hallucinations and
    increasing accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: CoT reasoning complements fine-tuning and RAG as a multipronged strategy for
    minimizing hallucinations and maximizing reliability. By layering these techniques,
    developers can significantly improve the robustness of LLM applications, ensuring
    they are better suited for complex and critical tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feedback Loops: The Power of User Input in Mitigating Risks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While implementing various technological solutions like fine-tuning, RAG, and
    CoT reasoning can significantly improve the reliability of your LLM application,
    it’s crucial to remember that the end users often provide the most valuable insights
    into the system’s performance. Establishing a feedback loop allows users to flag
    problematic or misleading outputs, creating an additional layer of safety and
    quality assurance. There are several ways to collect feedback:'
  prefs: []
  type: TYPE_NORMAL
- en: Flagging system
  prefs: []
  type: TYPE_NORMAL
- en: Integrate a simple interface where users can flag inaccurate, biased, or problematic
    responses. The easier you make this process, the more likely users will participate.
  prefs: []
  type: TYPE_NORMAL
- en: Rating scale
  prefs: []
  type: TYPE_NORMAL
- en: Along with flagging, offer a rating scale for users to gauge the accuracy or
    helpfulness of the response. This quantitative data will assist in your ongoing
    model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Comment box
  prefs: []
  type: TYPE_NORMAL
- en: Provide an optional comment box for users willing to give more detailed feedback
    describing what they found misleading or problematic about the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once feedback is collected, it needs to be systematically analyzed to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: Recurring issues
  prefs: []
  type: TYPE_NORMAL
- en: Are there patterns of hallucinations or inaccuracies in specific domains or
    types of queries?
  prefs: []
  type: TYPE_NORMAL
- en: Severity
  prefs: []
  type: TYPE_NORMAL
- en: Is the error a minor inconvenience, or could it potentially lead to severe consequences?
  prefs: []
  type: TYPE_NORMAL
- en: Underlying causes
  prefs: []
  type: TYPE_NORMAL
- en: What might be causing these issues? Is it a lack of domain-specific knowledge,
    or is the reasoning process flawed?
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this analysis, the development team can then:'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune further
  prefs: []
  type: TYPE_NORMAL
- en: Use the feedback to improve the model’s domain-specific performance or general
    reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Enhance CoT reasoning
  prefs: []
  type: TYPE_NORMAL
- en: If feedback suggests the model fails at logical reasoning, consider more targeted
    CoT prompting or supervised reasoning enhancements.
  prefs: []
  type: TYPE_NORMAL
- en: Enhance reference material in RAG
  prefs: []
  type: TYPE_NORMAL
- en: If the model’s answers are consistently inaccurate in a particular domain, perhaps
    the RAG reference material must be updated or expanded.
  prefs: []
  type: TYPE_NORMAL
- en: The feedback loop is not a one-off solution, but rather an ongoing process.
    Continually engaging with your user base and adapting your model based on its
    feedback ensures a continuously improving system. This adaptive approach enhances
    your application’s reliability and helps maintain user trust.
  prefs: []
  type: TYPE_NORMAL
- en: Clear Communication of Intended Use and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we navigate the complexities of mitigating hallucinations and refining LLMs’
    capabilities, we must recognize the importance of transparency in application
    development. An LLM might be a marvel of technology, but it’s far from perfect.
    Clear, upfront communication about its intended uses, strengths, and limitations
    is not just ethical—it’s an essential aspect of building trust and managing the
    expectations of your user base.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s review the areas where intended use documentation can be important:'
  prefs: []
  type: TYPE_NORMAL
- en: Intended use
  prefs: []
  type: TYPE_NORMAL
- en: Clearly outline what you designed your application to accomplish. Is it a specialized
    tool for legal professionals or a general-purpose assistant? Understanding the
    scope of the application helps users make informed decisions on how best to use
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledge the LLM’s constraints, including areas where it might not have domain-specific
    expertise or where the risk of hallucination is higher. Be explicit about what
    you exclude from the application’s intended field of use.
  prefs: []
  type: TYPE_NORMAL
- en: Data handling
  prefs: []
  type: TYPE_NORMAL
- en: Share your data protection and privacy protocols. Make it clear how user data
    will be stored, processed, and protected.
  prefs: []
  type: TYPE_NORMAL
- en: Feedback mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: Inform users that you have a feedback loop for continuous improvement and explain
    how they can contribute to this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve decided on the items you wish to communicate with the user, here
    are some good options for how to communicate these:'
  prefs: []
  type: TYPE_NORMAL
- en: User interface
  prefs: []
  type: TYPE_NORMAL
- en: Use tooltips, pop-ups, or an FAQ within the application to provide quick reminders
    or explanations about the model’s intended use and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  prefs: []
  type: TYPE_NORMAL
- en: Create detailed guides or manuals that users can refer to for more information
    on what the system can and cannot do.
  prefs: []
  type: TYPE_NORMAL
- en: Introductory tutorials
  prefs: []
  type: TYPE_NORMAL
- en: Offer walk-throughs or tutorials when a user first engages with the application,
    focusing on illustrating both its capabilities and constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Update logs
  prefs: []
  type: TYPE_NORMAL
- en: Maintain a version history or update log where users can see what improvements
    have been made and what issues are being worked on.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency is more than just a one-and-done affair. As your model evolves—improving
    its capabilities, expanding its domain-specific knowledge, enhancing its reasoning
    abilities—it’s crucial to update the user community on these developments. Likewise,
    if new limitations or vulnerabilities are discovered, these should be communicated
    as promptly and transparently as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Being transparent benefits users and boosts the development team by fostering
    a more engaged and forgiving user base. When people understand a tool’s limitations,
    they are less likely to misuse it and more likely to provide constructive feedback
    that can be used for further refinement. Transparency is an ethical obligation
    and the cornerstone of a mutually beneficial relationship between application
    developers and their users.
  prefs: []
  type: TYPE_NORMAL
- en: 'User Education: Empowering Users Through Knowledge'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much like how advanced anti-phishing software alone can’t entirely prevent phishing
    attacks, technical mitigations can only minimize the risks of LLM hallucinations
    and overreliance. Human awareness and education are crucial additional layers
    of defense. Corporate security teams train employees to recognize phishing attempts,
    double-check URLs, and be skeptical of unsolicited communications. Similarly,
    while we strive to minimize overreliance on LLMs, we must also cultivate an informed
    and vigilant user base. Educating users about the real trust issues and equipping
    them with cross-verification strategies is vital to ensuring they understand the
    limitations and best practices associated with using LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you build out your education plan, here are some suggested topics to cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding trust issues
  prefs: []
  type: TYPE_NORMAL
- en: Make users aware that while LLMs are advanced and often accurate, they are not
    infallible. Hallucinations can happen, and overreliance without verification can
    have significant consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-checking mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: Educate users to cross-reference the information the LLM provides. Depending
    on the domain, this might include checking multiple trusted sources, consulting
    experts, or running empirical tests.
  prefs: []
  type: TYPE_NORMAL
- en: Situational awareness
  prefs: []
  type: TYPE_NORMAL
- en: Encourage users to assess the information’s criticality. A higher level of trust
    might be acceptable for routine or noncritical tasks. However, you should encourage
    more rigorous verification for critical safety, finance, or legal jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Feedback options
  prefs: []
  type: TYPE_NORMAL
- en: Make users aware of the feedback loop feature in your application. Their active
    participation in reporting anomalies can contribute to the system’s ongoing improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some suggested methods you can use to deliver educational content
    to your users:'
  prefs: []
  type: TYPE_NORMAL
- en: In-app guides
  prefs: []
  type: TYPE_NORMAL
- en: Short, interactive guides or videos can introduce these concepts to users as
    they use the application.
  prefs: []
  type: TYPE_NORMAL
- en: Resource library
  prefs: []
  type: TYPE_NORMAL
- en: Create a repository of articles, FAQs, and how-to guides that detail these topics.
  prefs: []
  type: TYPE_NORMAL
- en: Community forums
  prefs: []
  type: TYPE_NORMAL
- en: An active user forum can help to quickly disseminate best practices and news,
    providing an extra layer of education and awareness.
  prefs: []
  type: TYPE_NORMAL
- en: Email campaigns
  prefs: []
  type: TYPE_NORMAL
- en: Regular updates can be sent to users outlining new features, limitations, or
    educational material, ensuring that even infrequent users stay informed.
  prefs: []
  type: TYPE_NORMAL
- en: While the development team focuses on technical mitigations like fine-tuning,
    RAG, and CoT reasoning, it’s important to remember that a well-educated user base
    is also a robust line of defense against the risks posed by LLMs. Thus, a balanced,
    comprehensive approach that combines technological advancements with ongoing user
    education is the optimal strategy for mitigating risks and enhancing reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In a final twist of irony for this chapter, it seems that the lack of a sense
    of humor in LLMs is now a risk factor you must account for as well. Recent examples
    have highlighted this quirk: Google’s LLM-enhanced Search feature has offered
    dubious advice, such as recommending glue as a pizza topping, suggesting eating
    rocks as a nutritional tip, and even advising jumping off a bridge to cure depression.
    These bizarre recommendations were traced to nonauthoritative but popular websites
    like Reddit and The Onion. Unfortunately, without a sense of humor, the LLMs pass
    along these joke punchlines as if they were facts. This is just one more edge
    condition for you to consider.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Addressing the risks of damage due to overreliance on hallucination-prone LLMs
    requires a comprehensive, multilayered approach. This challenge is best met through
    technological advancements, active user involvement, transparent communication,
    and thorough user education.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is acknowledging the issue. Your first line of defense must be
    to reduce hallucinations to a minimum. Consider narrowing your application’s field
    of use to a specific domain, and then equip your LLM to become a world-class expert
    using techniques such as fine-tuning, RAG, and CoT.
  prefs: []
  type: TYPE_NORMAL
- en: By combining technological safeguards, user feedback loops, transparent communication,
    and robust user education, the strategy for mitigating the risks associated with
    overreliance on LLMs becomes well rounded. Each of these elements contributes
    individually to reducing the risks of hallucinations and synergistically helps
    build a more resilient, transparent, and user-friendly system.
  prefs: []
  type: TYPE_NORMAL
