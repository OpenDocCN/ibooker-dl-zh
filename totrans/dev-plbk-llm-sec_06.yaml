- en: Chapter 6\. Do Language Models Dream of Electric Sheep?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Among all the excitement about advances in LLMs, few phenomena captivate and
    perplex like their so-called *hallucinations*. It’s almost as if these computational
    entities, deep within their myriad layers, occasionally drift into a dreamlike
    state, creating wondrous and bewildering narratives. Like a human’s dreams, these
    hallucinations can be reflective, absurd, or even prophetic, providing insights
    into the complex interplay between training data and the model’s learned interpretations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In the world of LLMs, the term “hallucination” might evoke images of vivid and
    whimsical creations, but in reality, it signifies a more mundane statistical anomaly.
    At its core, a hallucination is the model’s attempt to bridge gaps in its knowledge
    using the patterns it has gleaned from its training data. While it might be termed
    “imaginative,” it’s essentially the LLM making an educated guess when faced with
    unfamiliar input or scenarios. However, these guesses can manifest as confident
    yet unfounded assertions, revealing the model’s struggle to differentiate between
    well-learned facts and the statistical noise within its training data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: LLMs do not provide easily usable probability scores like some other “predictive”
    AI algorithms. For example, a vision classifier algorithm may return a probability
    as a percent. It might show a 79% chance that a particular image depicts a monkey.
    Thus, a user of that model gets a sense of how strongly the model “feels” about
    the prediction. LLMs simply predict the next token or tokens in a sequence. While
    the LLM uses a complex statistical model to do this, a certainty score for the
    overall response to a prompt isn’t typically part of the output. This can leave
    the end user unsure whether the LLM has returned a well-grounded reaction to the
    prompt or a weak, statistical extrapolation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The term “hallucination” is unpopular with some because it personifies the LLM
    and makes its flaws seem less critical. Some literature now refers to this phenomenon
    as *confabulation*. However, hallucination is far more prevalent, so we’ll use
    it in this book.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'This nuanced dance between fact and fiction in LLM outputs brings us to the
    crux of the challenge: *overreliance*. As humans, we are naturally inclined to
    trust results that are presented confidently, especially when they emanate from
    sophisticated computer software. Yet, it’s this very trust that can steer us astray.
    When LLMs hallucinate, they often don’t waver in their confidence, making it hard
    to discern genuine knowledge from imperfect statistical artifacts. The danger
    lies in the hallucination and also in our propensity to take these dreamlike utterances
    at face value, potentially leading to misinformation, missteps, and broader implications
    in real-world applications.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Overreliance refers to the excessive trust in the capabilities and exactness
    of LLM elaborations. Excessive confidence in LLM output, especially when hallucinations,
    errors, or biased data input are present, can lead to damaging outputs, particularly
    in professional or safety-critical environments. A significant example is trusting
    an LLM to provide medical advice without sufficient testing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Why Do LLMs Hallucinate?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core reason for hallucinations lies in the LLM’s operational mechanism,
    which is geared toward pattern matching and statistical extrapolation rather than
    factual verification. While they acquire knowledge through training on vast training
    datasets, LLMs often lack specific, actual knowledge. Their operation is rooted
    in identifying patterns in the input data and attempting to match these patterns
    with those learned during training. This pattern matching occurs without a real-world
    understanding, which can lead to the generation of hallucinated text, especially
    when faced with ambiguous or novel input prompts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The quality and nature of the training data significantly impact the likelihood
    and extent of hallucinations. Biases, inaccuracies, or noise in the training data
    can mislead the model into generating biased or incorrect text.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations present a substantial challenge in using LLMs for critical or
    sensitive applications. They underline AI development’s inherent intricacies and
    challenges, spotlighting the gap between statistical pattern matching and real-world,
    factual understanding. The hallucination phenomenon in LLMs opens a window into
    the broader discourse on the limitations and the ethical implications of deploying
    large-scale AI models in real-world scenarios without a robust mechanism for factual
    verification or contextual understanding.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Types of Hallucinations
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we dig further into this, let’s look at some of the types of hallucinations
    we’ll likely experience. Doing so will help us understand the implications and
    mitigations:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Factual inaccuracies
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: LLMs may produce factually incorrect statements due to the model’s lack of specific
    knowledge or to misinterpreting the training data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Unsupported claims
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Similar to factual inaccuracies, LLMs might generate baseless claims, which
    can be detrimental, especially in sensitive or critical contexts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Misrepresentation of abilities
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: LLMs might give the illusion of understanding advanced topics such as chemistry,
    even when they don’t. They can convincingly double-talk about a topic, misleading
    users about their level of understanding.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Contradictory statements
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: LLMs might generate sentences contradicting previous statements or the user’s
    prompt. For instance, they might first state, “Cats are afraid of water,” and
    later claim, “Cats love to swim in water.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: With these in mind, let’s look at real-world examples and their impact on application
    providers and customers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll examine four cases where hallucinations intersected with
    overreliance and caused harm. These should help drive home the need to address
    these issues in your LLM applications.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Imaginary Legal Precedents
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2023, in a US federal court, a judge levied fines on two lawyers and their
    law firm for negligent oversight in legal practice. The lawyers had submitted
    fictitious legal research in an aviation injury case. The fabricated case law,
    as it turned out, was generated by ChatGPT.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The issue came to light during a routine legal proceeding when the opposition
    discovered that the legal citations provided by the lawyers were not merely erroneous
    but entirely fabricated. The lawyers used a general-purpose LLM, which did not
    have specific legal training or data access, for their research. Their unverified
    reliance on the AI output led to the submission of six fictitious case citations
    in a legal brief. The judge later judged this action as an act of bad faith. The
    repercussions of this act were not confined to the courtroom but resonated across
    the legal and tech communities, marking a significant incident in the discourse
    surrounding AI’s role in legal practice.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Because the judge imposed substantial fines on the lawyers and their firm, the
    incident emerged as a cautionary tale about overreliance on AI in critical domains.
    It showcased the necessity of human verification and due diligence, especially
    in a field where accuracy and authenticity are paramount.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the impacts this incident had on several different parties to
    ensure we can see the full scope of the problems caused:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: On the LLM provider
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The incident spotlighted the potential risks of using OpenAI’s products in critical
    and formal domains like legal practice. It raised questions about the reliability
    and safe usage of ChatGPT and potentially impacted OpenAI’s reputation. The misuse
    of ChatGPT in a legal setting could prompt further scrutiny and demands by legislators
    for stricter regulation on the use and deployment of OpenAI’s products in critical
    domains.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: On LLM customers
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The repercussions were immediate and severe for the lawyers involved. They faced
    financial penalties, and their professional reputation was significantly tarnished.
    This incident is a deterrent for other legal professionals, making them wary of
    relying on AI tools for critical tasks without thorough verification.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: On the legal profession
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The event echoed across the legal profession, emphasizing the importance of
    human verification and the dangers of unquestioningly trusting AI-generated content.
    It highlighted a pressing need for educating and alerting legal professionals
    about the limitations and correct usage of AI tools in legal practice.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: At its core, this event underscores the indispensable value of verification.
    Legal professionals, and indeed all users of AI, should invest in verifying the
    information generated by AI tools. Further, the incident brings to light the necessity
    for robust guidelines that govern the use of AI in legal practice and other critical
    domains. Establishing such policies, including verification procedures to ensure
    the accuracy and reliability of AI-generated information, will act as a bulwark
    against similar incidents. The story also underscores the need to promote the
    ethical use of AI tools. Creating awareness about potential misuse and stressing
    the importance of adhering to professional standards when employing AI for critical
    tasks emerges as a pivotal lesson.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: LLM providers such as OpenAI should provide better guidelines, warnings, and
    education about their AI tools’ proper use and limitations to prevent misuse and
    ensure users are fully informed about the capabilities and potential risks. Lastly,
    the incident highlights the need for continuous improvement, urging AI software
    developers and the legal profession to learn from their mistakes and enhance their
    tools’ safety and reliability in critical applications. Through such a reflective
    lens, the incident offers a roadmap toward fostering a responsible AI usage culture
    anchored in verification, education, and ethical practice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Airline Chatbot Lawsuit
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a landmark decision in 2024, Canada’s largest airline, Air Canada, was ordered
    to compensate a customer after a chatbot provided incorrect information regarding
    fares. In this case, Jake Moffatt, a resident of British Columbia, sought information
    from Air Canada’s chatbot about the documents necessary for a bereavement fare
    and the possibility of obtaining a retroactive refund. Based on the information
    provided by the chatbot, Moffatt purchased a full-price ticket, believing he could
    secure a refund later. However, when he applied for the refund, Air Canada denied
    it, stating that bereavement rates did not apply to completed travel, contrary
    to the chatbot’s guidance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Moffatt initiated legal action against Air Canada to recover the fare difference
    after the airline failed to honor the chatbot’s information. Air Canada’s defense
    claimed the chatbot was a “separate legal entity” and responsible for its own
    actions, a stance that was dismissed by the judge as illogical and irresponsible.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The judge ordered Air Canada to pay Moffatt the difference between the full
    fare and the bereavement fare, along with interest and fees. The judge emphasized
    that all information provided on Air Canada’s website, whether through a chatbot
    or a static page, was the airline’s responsibility.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the impacts of this case from several different angles:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: On Air Canada
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: The incident brought significant public and legal scrutiny, challenging the
    airline’s approach to AI in customer interactions. It highlighted the need for
    accurate AI-generated communications and the potential reputational damage from
    AI errors.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: On AI and legal precedents
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The case set a precedent regarding the legal accountability of AI communications
    in business operations. It raised questions about the extent to which companies
    can or should be held liable for AI-generated content.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: On consumers and AI
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The ruling reinforced consumer rights in the digital age, emphasizing that companies
    cannot absolve themselves of accountability for AI-generated errors.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The case emphasizes the critical importance of accuracy in LLM-generated content
    and the growing legal precedence that inaccuracies can lead to substantial financial
    and reputational penalties for companies. This ruling reinforces the notion that
    businesses cannot disown the outputs of their LLM applications and must treat
    AI communications with the same scrutiny as any other official corporate communication.
    Companies must ensure rigorous testing and continuous monitoring of their AI tools
    to avoid potential legal liabilities and uphold consumer trust. Moreover, the
    financial repercussions highlighted by this case serve as a reminder of the direct
    costs associated with such misinformation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Unintentional Character Assassination
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2023, Brian Hood, mayor of Hepburn Shire in Australia, threatened legal action
    against OpenAI for a defamatory claim generated by the LLM. ChatGPT falsely asserted
    that Hood, then a whistleblower in a foreign bribery scandal, had served jail
    time. According to the suit, this fabricated information, presented as factual
    by the AI, significantly impacted Hood’s reputation and caused distress.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The issue may have stemmed from ChatGPT’s limited training data in this area.
    Without the LLM having access to strongly correlated data related to a user’s
    query, the LLM could have conflated unrelated snippets of information, resulting
    in the demonstrably false claim about Hood. The incident underscored the potential
    dangers of relying on AI-generated information uncritically, especially in sensitive
    domains like public reputation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'We can better understand the impacts of this case by looking at it from both
    plaintiff’s and defendant’s point of view:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Hood
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: The false claim caused Hood mental anguish and threatened his political career.
    The incident highlighted individuals’ vulnerability to AI-generated misinformation
    and the potential for reputational damage.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: The company has opened itself to expensive and time-consuming litigation. In
    this case, the plaintiff indicated at the time of filing that he might be seeking
    over $200,000 in damages.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding those impacts leads us to three lessons you can apply in your
    projects:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Verification
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Robust verification mechanisms are crucial, whether through fact-checking tools,
    human oversight, or a combination. Users must develop a healthy skepticism toward
    AI-generated information.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Education
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Educating users about the capabilities and limitations of LLMs is critical to
    promoting responsible and ethical usage.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Regulation
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Regulatory frameworks may be necessary to govern the use of LLMs in critical
    domains, ensuring data privacy, algorithmic accountability, and user protection.
    The Hood case highlights the potential need for legal clarification around AI
    responsibility and liability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The Brian Hood case exemplifies the potential pitfalls of hallucination and
    overreliance in LLMs. It calls for more robust safeguards, user education, and
    responsible application of this powerful technology. Only through a multipronged
    approach can we prevent future harm and ensure AI’s beneficial integration into
    society.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Open Source Package Hallucinations
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This incident centers around using LLMs as coding assistants. It has become
    commonplace now for developers to use LLMs to assist them while writing code.
    Developers might use general-purpose chatbots, such as ChatGPT, or dedicated copilots,
    such as GitHub Copilot. A [survey by GitHub](https://oreil.ly/tcy1y) in June 2023
    showed that 92% of developers working in large companies were using LLMs to help
    them code. This section will look at a notable example of the risks of hallucination
    and overreliance using these code generation tools.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: These days, a substantial portion of code written uses open source libraries.
    This includes code written by AI coding assistants, which may leverage existing
    open source libraries to make code more compact or efficient. Usually, this works
    fine, but in some cases, these assistants have been shown to hallucinate about
    the existence of various open source libraries. They imagine a useful library
    to solve problems and generate code that uses the imaginary library. This may
    seem harmless enough, but in 2023, the research team at Vulcan Cyber demonstrated
    how [hackers could use this flaw to insert malicious code into applications](https://oreil.ly/oULNb).
    They dubbed the issue simply “AI package hallucination.”
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the research team crafted the attack by searching through popular
    Stack Overflow questions and asking ChatGPT to solve them. They quickly found
    over 100 hallucinated packages suggested by an assistant bot that were not published
    on any popular code repository. Because these were based on popular questions,
    many other developers will likely ask their AI assistants to generate similar
    code, which may include the same hallucination.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: To exploit this hallucination, an attacker needs only to create malicious versions
    of the hallucinated packages, upload them to popular code repositories, and then
    wait for an unsuspecting developer to download and run this code based on AI suggesting
    the package.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In March 2024, the team at Lasso Security followed up on this study and found
    that up to 30% of the coding questions they asked a popular model resulted in
    at least one hallucinated package!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年3月，Lasso Security团队对此研究进行了跟进，发现他们向一个流行的模型提出的编码问题中，高达30%的结果至少包含一个幻觉包！
- en: The shift by developers from searching for coding solutions online to asking
    AI platforms like ChatGPT for answers created a lucrative opportunity for attackers.
    This scenario signifies a severe security concern as it showcases a novel pathway
    for attackers to exploit AI technologies to propagate malicious code, thereby
    compromising the integrity and security of software applications. While this vulnerability
    has been widely reported, it’s unclear how much this has been exploited in the
    wild. Nonetheless, it’s an essential example of another domain where hallucination
    and overreliance can combine to put an organization at risk.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者从在线搜索编码解决方案转向向ChatGPT等AI平台寻求答案，这为攻击者创造了有利可图的机遇。这种情况表明了一个严重的安全问题，因为它展示了攻击者利用AI技术传播恶意代码的新途径，从而损害了软件应用的完整性和安全性。尽管这种漏洞已被广泛报道，但其在野外被利用的程度尚不清楚。然而，它是一个重要的例子，说明了幻觉和过度依赖如何结合在一起，使组织面临风险。
- en: This incident sheds light on several critical lessons. Firstly, it underlines
    the necessity for rigorous validation of AI-generated outputs, particularly when
    such results can potentially influence software development or other mission-critical
    operations. It’s imperative to have mechanisms to verify the authenticity and
    safety of AI-recommended packages. Secondly, it highlights the importance of continuously
    monitoring and updating AI systems to mitigate the risks associated with outdated
    or inaccurate training data. Lastly, it calls for a collective effort within the
    AI and cybersecurity communities to devise strategies for detecting and preventing
    such exploitation avenues in the future. By learning from such incidents, stakeholders
    can work toward building more robust and secure AI-driven platforms that are resilient
    against evolving threat landscapes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这起事件揭示了几个关键教训。首先，它强调了严格验证AI生成输出的必要性，尤其是当这些结果可能影响软件开发或其他关键任务操作时。必须要有机制来验证AI推荐包的真实性和安全性。其次，它强调了持续监控和更新AI系统的重要性，以减轻与过时或不准确训练数据相关的风险。最后，它呼吁AI和网络安全社区共同努力，制定检测和预防未来此类利用途径的策略。通过从这些事件中学习，利益相关者可以共同努力构建更强大、更安全的AI驱动平台，以抵御不断发展的威胁环境。
- en: Who’s Responsible?
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谁负责？
- en: Development teams working with LLMs sometimes perceive the damage caused by
    hallucinations as a “people problem,” where they blame the user for misinterpreting
    or misusing the information provided. There’s no question that user education
    is important. Just as people learned that they can’t trust all the information
    they find on the web, people will grow more sophisticated in examining erroneous
    information given to them by a chatbot or copilot.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与大型语言模型（LLMs）合作的开发团队有时会将幻觉造成的损害视为“人为问题”，他们指责用户误解或误用了提供的信息。毫无疑问，用户教育非常重要。正如人们学会了他们不能信任在网络上找到的所有信息一样，人们将变得更加成熟，以检查聊天机器人或辅助驾驶员给他们提供的错误信息。
- en: However, as developers, we are responsible for ensuring the information provided
    by our software is as accurate as possible. The ripple effect of such misinformation
    can be profound, especially in critical domains such as the health care, legal,
    or financial sectors where the stakes are high. This accentuates the need for
    developers to invest in mechanisms to identify and rectify hallucinations or erroneous
    information before they reach the user.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作为开发者，我们负责确保我们的软件提供的信息尽可能准确。这种错误信息的涟漪效应可能非常深远，尤其是在医疗保健、法律或金融等高风险领域。这强调了开发者投资于识别和纠正幻觉或错误信息，在它们到达用户之前进行纠正的必要性。
- en: 'Our duty as developers extends beyond merely creating sophisticated AI systems.
    It encompasses fostering a safe and reliable ecosystem where users can interact
    with AI with a reasonable assurance of accuracy and reliability. This responsibility
    calls for a multifaceted approach: improving the system to reduce hallucinations,
    implementing robust output filtering mechanisms to catch and correct errors, and
    fostering a culture of continuous improvement and learning from past mistakes.
    Additionally, educating users about the potential limitations and the degree of
    reliability of LLMs is crucial. It helps nurture an informed user base that can
    engage with AI systems judiciously, while being mindful of the risks.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: The case studies discussed in this chapter illustrate the differing legal responsibilities.
    In the instance involving lawyers using fictitious legal precedents generated
    by ChatGPT, the court placed the responsibility squarely on the professionals.
    As sophisticated users, the lawyers were expected to verify the authenticity of
    the information before its submission in legal documents. Their failure to do
    so led to significant repercussions, highlighting the critical importance of professional
    diligence in using AI tools.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the Air Canada chatbot scenario resulted in the company being held
    liable for the misleading information provided to the consumer. This case underscores
    that corporations, especially in consumer-facing roles, must ensure their outputs
    are accurate and reliable. The tribunal’s decision reflects a growing legal consensus
    that companies cannot deflect responsibility for AI-generated content, reinforcing
    the expectation that businesses must safeguard consumer interactions with their
    systems. These cases collectively stress the need for clear guidelines and accountability
    in using AI, irrespective of the user’s sophistication level.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Mitigation Best Practices
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hallucinations are going to happen. It’s an inherent property of current LLM
    technology. Our job as application developers is twofold. First, we should work
    to minimize the likelihood of hallucinations by our application, and second, we
    want to reduce the damage when they occur. Let’s look at options.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Expanded Domain-Specific Knowledge
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the world of LLMs, domain-specific knowledge isn’t just a nice-to-have; it’s
    often essential for maximizing utility and minimizing the risk of hallucinations.
    When we focus an LLM on a specific domain—whether that’s health care, law, finance,
    or any other field—it has the potential to provide more accurate and contextually
    relevant information. This specialized focus can drastically reduce the chances
    of the model making incorrect or misleading statements, hallmarks of hallucinations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: In a previous chapter, we discussed the risks of arming your LLM with dangerous,
    biased, or privileged information. While that chapter emphasized avoiding these
    pitfalls by minimizing data exposure, you must give your model access to more
    domain-specific, factual knowledge to reduce hallucinations.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Model fine-tuning for specialization
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning is a powerful tool for LLM applications to leverage the extensive
    knowledge encapsulated in foundation models while adding a layer of specialization
    for your specific use case. You can achieve this balance of general and specialized
    expertise at a relatively low computational and financial cost compared to training
    a model from scratch. The primary benefit? You obtain a more reliable and domain-specific
    LLM, tailor-made to your application’s unique needs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: The process of fine-tuning helps narrow the LLM’s scope to be more in line with
    your domain-specific objectives. Fine-tuning optimizes the model’s utility and
    is a critical mitigating strategy against hallucinations. The more specialized
    a model is, the lower the probability of generating incorrect or out-of-context
    responses in the form of hallucinations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: By fine-tuning your foundation model, you essentially transform it into a specialist.
    This higher level of specialization makes the LLM more trustworthy in critical
    operations, be it medical diagnoses, legal interpretations, or financial analyses.
    Fine-tuning is an important tactic in achieving the dual objectives of mitigating
    the risk of hallucinations and reducing their impact, thereby making your LLM
    application more robust and reliable.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: RAG for enhanced domain expertise
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG introduces a new layer of sophistication to the capabilities of LLMs. It
    combines the strengths of retrieval-based models and sequence-to-sequence generative
    models. A developer uses a well-established, reliable information retrieval technology,
    such as a search engine or database, to collect information relevant to the user’s
    needs. This information can then be fed to the LLM as part of a prompt. The effect
    is similar to allowing the AI to “look up” information from a database or a set
    of documents during the generation process. This hybrid approach enhances the
    model’s contextual awareness, improves accuracy, and provides a mechanism for
    sourcing the generated content, thus contributing to increased trustworthiness.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: When you’ve fine-tuned your LLM to be a domain-specific expert, the next logical
    step is to equip it with the best available reference materials, much like a real-world
    professional. Doctors, lawyers, and other experts seldom rely solely on their
    memory; they have a rich library of books, journals, and databases to consult
    for the most up-to-date and accurate information.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Implementing RAG in your domain-specific LLM application is akin to giving it
    a virtual library filled with specialized knowledge. This curated resource can
    include textbooks, research papers, guidelines, or other credible material that
    can guide the model’s responses. RAG, combined with fine-tuning, amplifies the
    utility and reliability of your application and minimizes the risks associated
    with hallucinations and overreliance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Not all incorrect statements by an LLM should be classified as hallucinations.
    The core definitions most experts use for hallucinations involve an LLM’s low-confidence
    token sequence prediction being stated in a high-confidence fashion. However,
    incorrect statements from an LLM could also result from false training data or
    faulty data retrieved from a database or web page during RAG. It could even result
    from other, more traditional, coding errors.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Chain of Thought Prompting for Increased Accuracy
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After fine-tuning your model and enhancing it with RAG for domain-specific expertise,
    another option for reducing hallucinations and bolstering reliability is *chain
    of thought* (CoT) reasoning. As we’ve established, hallucinations can lead to
    misleading or dangerous outputs, and CoT reasoning offers a structured approach
    to counteract this problem by enhancing the LLM’s logical reasoning capabilities.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: CoT reasoning encourages an LLM to follow a logical sequence of steps or a reasoning
    pathway. Instead of generating a response based solely on the immediate input,
    the developer prompts the LLM to consider intermediate reasoning steps, breaking
    down complex problems into subproblems and addressing them systematically. CoT
    is particularly beneficial in complex tasks, such as medical diagnoses, legal
    reasoning, or intricate technical troubleshooting, where a misstep can have serious
    consequences.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of CoT reasoning include:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Reduced hallucinations
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: A structured approach to reasoning can significantly mitigate the risks associated
    with hallucinations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Enhanced accuracy
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: When an LLM reasons through problems step by step, the likelihood of arriving
    at an accurate solution is higher.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Self-evaluation
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Chain of thought reasoning enables an LLM to assess its own reasoning process,
    identifying and correcting errors along the way. This act of self-evaluation increases
    the reliability of the generated content, thus reducing the risks associated with
    overreliance on the model’s outputs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a simple example to help illustrate the concept.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A model might incorrectly add up the numbers without considering the quantities
    and prices for each item, leading to an inaccurate answer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By breaking down the problem into sequential steps and explicitly guiding the
    model through each part of the calculation, the CoT prompt helps ensure that the
    model considers all parts of the problem and how they interact, leading to a more
    accurate response. The model is more likely to apply multiplication for the quantities
    of each item correctly and then add the totals together in the final step.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are increasingly more sophisticated examples of how to use CoT. These
    include “zero-shot” techniques that ask the LLM to create its own detailed steps
    to solve a complex problem. Research is ongoing and fast-paced, so check the current
    literature for advances in this promising area for reducing hallucinations and
    increasing accuracy.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: CoT reasoning complements fine-tuning and RAG as a multipronged strategy for
    minimizing hallucinations and maximizing reliability. By layering these techniques,
    developers can significantly improve the robustness of LLM applications, ensuring
    they are better suited for complex and critical tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Feedback Loops: The Power of User Input in Mitigating Risks'
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While implementing various technological solutions like fine-tuning, RAG, and
    CoT reasoning can significantly improve the reliability of your LLM application,
    it’s crucial to remember that the end users often provide the most valuable insights
    into the system’s performance. Establishing a feedback loop allows users to flag
    problematic or misleading outputs, creating an additional layer of safety and
    quality assurance. There are several ways to collect feedback:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Flagging system
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Integrate a simple interface where users can flag inaccurate, biased, or problematic
    responses. The easier you make this process, the more likely users will participate.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Rating scale
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Along with flagging, offer a rating scale for users to gauge the accuracy or
    helpfulness of the response. This quantitative data will assist in your ongoing
    model evaluation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Comment box
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Provide an optional comment box for users willing to give more detailed feedback
    describing what they found misleading or problematic about the output.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Once feedback is collected, it needs to be systematically analyzed to understand:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Recurring issues
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Are there patterns of hallucinations or inaccuracies in specific domains or
    types of queries?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Severity
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Is the error a minor inconvenience, or could it potentially lead to severe consequences?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Underlying causes
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: What might be causing these issues? Is it a lack of domain-specific knowledge,
    or is the reasoning process flawed?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this analysis, the development team can then:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune further
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Use the feedback to improve the model’s domain-specific performance or general
    reasoning capabilities.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Enhance CoT reasoning
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: If feedback suggests the model fails at logical reasoning, consider more targeted
    CoT prompting or supervised reasoning enhancements.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Enhance reference material in RAG
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: If the model’s answers are consistently inaccurate in a particular domain, perhaps
    the RAG reference material must be updated or expanded.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: The feedback loop is not a one-off solution, but rather an ongoing process.
    Continually engaging with your user base and adapting your model based on its
    feedback ensures a continuously improving system. This adaptive approach enhances
    your application’s reliability and helps maintain user trust.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Clear Communication of Intended Use and Limitations
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we navigate the complexities of mitigating hallucinations and refining LLMs’
    capabilities, we must recognize the importance of transparency in application
    development. An LLM might be a marvel of technology, but it’s far from perfect.
    Clear, upfront communication about its intended uses, strengths, and limitations
    is not just ethical—it’s an essential aspect of building trust and managing the
    expectations of your user base.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s review the areas where intended use documentation can be important:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Intended use
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Clearly outline what you designed your application to accomplish. Is it a specialized
    tool for legal professionals or a general-purpose assistant? Understanding the
    scope of the application helps users make informed decisions on how best to use
    it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledge the LLM’s constraints, including areas where it might not have domain-specific
    expertise or where the risk of hallucination is higher. Be explicit about what
    you exclude from the application’s intended field of use.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Data handling
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Share your data protection and privacy protocols. Make it clear how user data
    will be stored, processed, and protected.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Feedback mechanisms
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Inform users that you have a feedback loop for continuous improvement and explain
    how they can contribute to this process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve decided on the items you wish to communicate with the user, here
    are some good options for how to communicate these:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: User interface
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Use tooltips, pop-ups, or an FAQ within the application to provide quick reminders
    or explanations about the model’s intended use and limitations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Create detailed guides or manuals that users can refer to for more information
    on what the system can and cannot do.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Introductory tutorials
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Offer walk-throughs or tutorials when a user first engages with the application,
    focusing on illustrating both its capabilities and constraints.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Update logs
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Maintain a version history or update log where users can see what improvements
    have been made and what issues are being worked on.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Transparency is more than just a one-and-done affair. As your model evolves—improving
    its capabilities, expanding its domain-specific knowledge, enhancing its reasoning
    abilities—it’s crucial to update the user community on these developments. Likewise,
    if new limitations or vulnerabilities are discovered, these should be communicated
    as promptly and transparently as possible.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Being transparent benefits users and boosts the development team by fostering
    a more engaged and forgiving user base. When people understand a tool’s limitations,
    they are less likely to misuse it and more likely to provide constructive feedback
    that can be used for further refinement. Transparency is an ethical obligation
    and the cornerstone of a mutually beneficial relationship between application
    developers and their users.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'User Education: Empowering Users Through Knowledge'
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much like how advanced anti-phishing software alone can’t entirely prevent phishing
    attacks, technical mitigations can only minimize the risks of LLM hallucinations
    and overreliance. Human awareness and education are crucial additional layers
    of defense. Corporate security teams train employees to recognize phishing attempts,
    double-check URLs, and be skeptical of unsolicited communications. Similarly,
    while we strive to minimize overreliance on LLMs, we must also cultivate an informed
    and vigilant user base. Educating users about the real trust issues and equipping
    them with cross-verification strategies is vital to ensuring they understand the
    limitations and best practices associated with using LLMs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'As you build out your education plan, here are some suggested topics to cover:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Understanding trust issues
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Make users aware that while LLMs are advanced and often accurate, they are not
    infallible. Hallucinations can happen, and overreliance without verification can
    have significant consequences.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Cross-checking mechanisms
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Educate users to cross-reference the information the LLM provides. Depending
    on the domain, this might include checking multiple trusted sources, consulting
    experts, or running empirical tests.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Situational awareness
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Encourage users to assess the information’s criticality. A higher level of trust
    might be acceptable for routine or noncritical tasks. However, you should encourage
    more rigorous verification for critical safety, finance, or legal jobs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Feedback options
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Make users aware of the feedback loop feature in your application. Their active
    participation in reporting anomalies can contribute to the system’s ongoing improvement.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some suggested methods you can use to deliver educational content
    to your users:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: In-app guides
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Short, interactive guides or videos can introduce these concepts to users as
    they use the application.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Resource library
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Create a repository of articles, FAQs, and how-to guides that detail these topics.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Community forums
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: An active user forum can help to quickly disseminate best practices and news,
    providing an extra layer of education and awareness.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Email campaigns
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Regular updates can be sent to users outlining new features, limitations, or
    educational material, ensuring that even infrequent users stay informed.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: While the development team focuses on technical mitigations like fine-tuning,
    RAG, and CoT reasoning, it’s important to remember that a well-educated user base
    is also a robust line of defense against the risks posed by LLMs. Thus, a balanced,
    comprehensive approach that combines technological advancements with ongoing user
    education is the optimal strategy for mitigating risks and enhancing reliability.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In a final twist of irony for this chapter, it seems that the lack of a sense
    of humor in LLMs is now a risk factor you must account for as well. Recent examples
    have highlighted this quirk: Google’s LLM-enhanced Search feature has offered
    dubious advice, such as recommending glue as a pizza topping, suggesting eating
    rocks as a nutritional tip, and even advising jumping off a bridge to cure depression.
    These bizarre recommendations were traced to nonauthoritative but popular websites
    like Reddit and The Onion. Unfortunately, without a sense of humor, the LLMs pass
    along these joke punchlines as if they were facts. This is just one more edge
    condition for you to consider.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Addressing the risks of damage due to overreliance on hallucination-prone LLMs
    requires a comprehensive, multilayered approach. This challenge is best met through
    technological advancements, active user involvement, transparent communication,
    and thorough user education.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The first step is acknowledging the issue. Your first line of defense must be
    to reduce hallucinations to a minimum. Consider narrowing your application’s field
    of use to a specific domain, and then equip your LLM to become a world-class expert
    using techniques such as fine-tuning, RAG, and CoT.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: By combining technological safeguards, user feedback loops, transparent communication,
    and robust user education, the strategy for mitigating the risks associated with
    overreliance on LLMs becomes well rounded. Each of these elements contributes
    individually to reducing the risks of hallucinations and synergistically helps
    build a more resilient, transparent, and user-friendly system.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
