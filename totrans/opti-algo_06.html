<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">5 <a id="idTextAnchor000"/>Simulated annealing<a id="idIndexMarker000"/></h1>

  <p class="co-summary-head">This chapter covers<a id="marker-157"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">Introducing trajectory-based optimization algorithms</li>

    <li class="co-summary-bullet">Understanding the simulated annealing algorithm</li>

    <li class="co-summary-bullet">Solving function optimization as an example of continuous optimization problems</li>

    <li class="co-summary-bullet">Solving puzzle game problems like Sudoku as an example of constraint-satisfaction problems</li>

    <li class="co-summary-bullet">Solving permutation problems like TSP as an example of discrete problems</li>

    <li class="co-summary-bullet">Solving a real-world delivery semi-truck routing problem</li>
  </ul>

  <p class="body">In this chapter, we’ll look at simulated annealing as a trajectory-based metaheuristic optimization technique. We’ll discuss different elements of this algorithm and its adaptation aspects. A number of case studies will be presented to show the ability of this metaheuristic algorithm to solve continuous and discrete optimization problems.</p>

  <h2 class="fm-head" id="heading_id_3">5.1 Introducing trajectory-based optimization</h2>

  <p class="body"><a id="marker-158"/>Imagine yourself on a hiking trip looking for the lowest valley in a rugged landscape that has many valleys and hills. You don’t have access to global information or a map that shows the location of the lowest valley. You start your hiking journey by randomly choosing a direction. You keep moving step by step until you get stuck in a local valley surrounded by hills. You are not highly satisfied with the location, as you believe that there is a lower valley in the area that may be behind the hills. Your curiosity drives you to move up one of the hills to find the lowest valley. <a id="idIndexMarker001"/><a id="idIndexMarker002"/></p>

  <p class="body">This is exactly what simulated annealing (SA) does. The basic idea of the SA algorithm is to use a stochastic search that follows a trial-and-error approach, accepting changes that improve the objective function and also keeping some changes that are not ideal. In a minimization problem, for example, any better moves or changes that decrease the value of the objective function will be accepted. However, some moves that increase the objective function will also be accepted with a certain probability. SA is a trajectory-based metaheuristic algorithm that can be used to find the global optimum solution for complex optimization problems. <a id="idIndexMarker003"/></p>

  <p class="body">Generally speaking, <i class="fm-italics">metaheuristic algorithms</i> can be classified into <i class="fm-italics">trajectory-based</i> and <i class="fm-italics">population-based</i> algorithms, as shown in figure 5.1.<a id="idIndexMarker004"/><a id="idIndexMarker005"/><a id="idIndexMarker006"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F01_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.1 Exploration and exploitation of optimization algorithms</p>
  </div>

  <p class="body"><i class="fm-italics">Trajectory-based metaheuristic algorithms</i> or <i class="fm-italics">S-metaheuristics</i>, such as SA or tabu search, use a single search agent that moves through the search space in a piecewise style. A better move or solution is always accepted, while a not-so-good move can be accepted with a certain probability. The steps, or moves, trace a trajectory in the search space, with a nonzero probability that this trajectory can reach the global optimum. <a id="idIndexMarker007"/></p>

  <p class="body">In contrast, <i class="fm-italics">population-based algorithms</i> or <i class="fm-italics">P-metaheuristics</i>, such as genetic algorithms, particle swarm optimization, and ant colony optimization, use multiple agents to search for an optimal or near-optimal global solution. <a id="idIndexMarker008"/></p>

  <p class="body">Due to the large diversity of initial populations, population-based algorithms are naturally more exploration-based, whereas single or trajectory-based algorithms are more exploitation-based. The following section explains the SA algorithm in more detail.</p>

  <h2 class="fm-head" id="heading_id_4">5.2 The simulated annealing algorithm</h2>

  <p class="body"><a id="marker-159"/>Whether you need to solve a complex nonlinear nondifferential function optimization problem, a puzzle game like Sudoku, an academic course scheduling problem, a travelling salesman problem (TSP), a network design problem, a task allocation problem, a circuit partitioning and placement problem, a production planning and scheduling problem, or even a tennis tournament planning problem, SA can be used as a generic solver for these different continuous and discrete optimization problems. <a id="idIndexMarker009"/></p>

  <p class="body">Let’s first look at the details of this solver before we use it to solve different problems. We’ll start by shedding some light on the physical annealing process, which was the inspiration for SA.</p>

  <h3 class="fm-head1" id="heading_id_5">5.2.1 Physical annealing</h3>

  <p class="body">Annealing, as a heat treatment process, has been used for centuries across various industries, including metallurgy, glassmaking, and ceramics. For example, in the context of making glass bottles, annealing removes the stresses and strains in the glass resulting from shaping. This is an important step, and if it’s not done, the glass may shatter due to the buildup of tension caused by uneven cooling. After the bottles have cooled to room temperature, they are inspected and finally packaged.<a id="idIndexMarker010"/><a id="idIndexMarker011"/></p>

  <p class="body">Annealing alters a material, causing changes in its properties, such as strength and hardness. This process heats the material to above the recrystallization temperature, maintains a suitable temperature, and then cools the material. As the temperature reduces, the mobility of molecules reduces, with the tendency that molecules will align themselves in a crystalline structure (figure 5.2).</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F02_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.2 The effect of temperature on the mobility of the molecules</p>
  </div>

  <p class="body"><a id="marker-160"/>The aligned structure is the minimum energy state for the system. To ensure that this alignment is obtained, cooling must occur at a sufficiently slow rate. If the substance is cooled too rapidly, a noncrystalline state with irregular three-dimensional patterns may be reached, as illustrated in figure 5.3. Quartz, sodium chloride, diamond, and sugar are examples of crystalline solids that have a regular order for the arrangement of constituent particles, atoms, ions, or molecules. Glass, rubber, pitch, and many plastics are examples of noncrystalline amorphous solids. As you may know, quartz crystals are harder than glass thanks to their symmetrical molecular structure.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F03_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.3 Physical annealing. Left: a metal with a crystalline structure. Right: an amorphous metal with a disordered atomic-scale structure.</p>
  </div>

  <p class="fm-callout"><span class="fm-callout-head">Note</span> The annealing process involves the careful control of the temperature and cooling rate, often called the annealing or cooling schedule. The annealing time should be long enough for the material to undergo the required transformation. If the difference in the temperature rate of change between the outside and inside of a material is too big, this may cause defects and cracks.</p>

  <p class="body">The fact that the aligned structure represents the minimum energy state for the system inspired scientists to think about mimicking this process to solve optimization problems. <i class="fm-italics">Simulated</i> annealing is a computational model that mimics the physical annealing process. In the context of mathematical optimization, the minimum of an objective function represents the minimum energy of the system. SA is an algorithmic implementation of the cooling process, used to find the optimum of an objective function. Table 5.1 outlines the analogy between SA and the physical annealing process.</p>

  <p class="fm-table-caption">Table 5.1 The physical annealing and simulated annealing analogy</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="50%"/>
      <col class="contenttable-0-col" span="1" width="50%"/>
    </colgroup>

    <thead class="calibre6">
      <tr class="contenttable-0-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Physical annealing</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Simulated annealing</p>
        </th>
      </tr>
    </thead>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">State of a material</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Solution of an optimization problem</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">The energy of a state</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">The cost of a solution</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Temperature</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Control parameter (temperature)</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">High temperature makes molecules move freely</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">High temperature favors search space exploration</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Low temperature restricts molecules’ motion</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Low temperature leads to exploiting the search space</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Gradual cooling helps to reduce stress and increase homogeneity and structural stability.</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Gradual cooling helps to avoid getting stuck in suboptimal local minima and to find the globally optimal or near-optimal solution.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">In 1953, the first computational model that replicated the physical process of annealing was introduced. This model was presented as a universal method for computing the properties of substances that can be considered collections of individual molecules interacting with each other. S. Kirkpatrick et al. were trailblazers in utilizing SA for optimization, as described in their paper "Optimization by Simulated Annealing" [1]. The following subsection explains the steps involved in the SA algorithm.</p>

  <h3 class="fm-head1" id="heading_id_6">5.2.2 SA pseudocode</h3>

  <p class="body"><a id="marker-161"/>SA employs a Markov chain-based random search approach, which not only accepts new solutions that decrease the objective function (assuming a minimization problem) but can also accept probabilistic solutions that increase objective function values.<a id="idIndexMarker012"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Markov chain</p>

    <p class="fm-sidebar-text">The Markov property, named after Russian mathematician Andrey Markov (1856–1922), is a memoryless random process. This means that the next state depends only on the current state and not on the sequence of events that preceded it. A Markov chain (MC) is a stochastic or probabilistic model that describes a sequence of possible moves in which the probability of each move depends only on the state attained in the previous move. This means that the transition from one state to another depends only on the current fully observable state and a transition probability.<a id="idIndexMarker013"/><a id="idIndexMarker014"/></p>

    <p class="fm-sidebar-text">Following this memoryless random process, the transition between the current known state A, for example, to a next neighboring state B is governed by a transition probability as illustrated in the following figure. Markov chains are used in different domains such as stochastic optimization, economy, speech recognition, weather prediction, and control systems. It's also worth mentioning that Google’s PageRank algorithm uses a Markov chain to model the behavior of users navigating the web. SymPy provides a Python implementation for a finite discrete time-homogeneous Markov chain through the class <code class="fm-code-in-text">sympy.stats.DiscreteMarkovChain</code>.<a id="idIndexMarker015"/><a id="marker-162"/></p>

    <p class="sidebarafigures"><img alt="" class="calibre2" src="../Images/CH05_F03_UN01_Khamis.png"/></p>

    <p class="sidebaracaptions">Markov chain—<i class="timesitalic">p<sub class="fm-subscript">AB</sub></i>, <i class="timesitalic">p<sub class="fm-subscript">BA</sub></i>, <i class="timesitalic">p<sub class="fm-subscript">BC</sub></i>, and <i class="timesitalic">p<sub class="fm-subscript">CB</sub></i> are transition probabilities between the states A, B, and C.</p>
  </div>

  <p class="body">As illustrated in figure 5.4, a new neighboring solution or state <i class="timesitalic">x<sub class="fm-subscript">k</sub></i> is always accepted if it’s an improving solution (i.e., <span class="times"><i class="fm-italics">f</i>(<i class="fm-italics">x<sub class="fm-subscript">k</sub></i>) &lt; <i class="fm-italics">f</i>(<i class="fm-italics">x<sub class="fm-subscript">i</sub></i>))</span>. An improving solution is a solution that gives a lower value for the objective function if we’re dealing with a minimization problem or gives a higher value in the case of a maximization problem. In the case of non-improving solutions, such as <i class="timesitalic">x<sub class="fm-subscript">j</sub></i>, the solution can still be probabilistically accepted as a way to avoid the risk of getting trapped in a local minimum. This contrasts with a greedy algorithm’s tendency to accept only improving solutions, making greedy algorithms more susceptible to getting stuck in local minima.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F04_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.4 Transition probability, assuming a minimization problem. As solution <i class="timesitalic">x<sub class="fm-subscript">k</sub></i> is an improving move, it is always accepted, and as solution <i class="timesitalic">x<sub class="fm-subscript">j</sub></i> is non-improving, it may be probabilistically accepted based on the transition probability.</p>
  </div>

  <p class="body"><a id="marker-163"/>Temperature <i class="timesitalic">T</i> appears in the transition probability and controls the exploration and exploitation in the search space. At high temperatures, non-improving moves will have a good chance of being accepted, but as the temperature decreases, the probability of accepting worse moves decreases. We’ll discuss this in more detail in the following subsections.</p>

  <p class="body">The steps in SA can be summarized in the following pseudocode.</p>

  <p class="fm-code-listing-caption">Algorithm 5.1 The SA algorithm</p>
  <pre class="programlisting">Objective function f(x), x = (x_1, . . . , x_p)^T
Initialize initial temperature T_o, initial guess x_o, iteration counter n=0 and iteration per temperature counter k=0
Set final temperature T_f, kmax maximum number of iterations per temperature and max number of iterations N
Define cooling schedule
Begin
While T &gt; T_f and n &lt; N do
      While k&lt;k_{max}
             Move randomly to a new location/state x_n + 1
             Calculate Δf = f_{n+1}(x_{n+1}) – f_n(x_n)
             If the new solution if better then
                 Accept the new solution
             Else 
                 Generate a random number r
                 Accept if exp(−Δf/T)&gt;r
             k=k+1
        End
        Update T according to the cooling schedule
        n = n + 1
End
Return the final solution</pre>

  <p class="body">SA has the advantages of ease of use and the ability to provide optimal or near-optimal solutions for a wide range of continuous and discrete problems. The main drawbacks of this algorithm are the need to tune many parameters and the occasional slow convergence of the algorithm to the optimal or near-optimal solutions.</p>

  <p class="body">Aside from this original SA algorithm—classical simulated annealing (CSA)—various variants have been proposed to improve the algorithm's performance. For example, fast simulated annealing (FSA) is a semi-local search and consists of occasional long jumps. Dual annealing is a stochastic global optimization algorithm that is useful for dealing with complex nonlinear optimization problems. It is based on the combined classical simulated annealing and fast simulated annealing algorithms. The generalized simulated annealing (GSA) algorithm uses a distorted Cauchy-Lorentz visiting distribution [2].<a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/><a id="marker-164"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Quantum annealing (QA)</p>

    <p class="fm-sidebar-text">In quantum mechanics, a quantum particle is treated as an electromagnetic wave that can penetrate with a certain probability through a potential barrier. Due to the wave nature of matter on the quantum level, there is indeed some probability that a quantum particle can traverse such a barrier if the barrier is thin enough. This phenomenon is known as quantum tunneling. The quantum tunneling effect is a phenomenon whereby wave functions or particles can penetrate through a supposedly impassable barrier even if the total energy of the particle is less than the barrier height. <a id="idIndexMarker019"/></p>

    <p class="fm-sidebar-text">As illustrated in the following figure, SA uses a thermal jump to push the search particle out of the local valley to avoid getting trapped in local minima. QA, on the other hand, searches an energy landscape to find an optimal or near-optimal solution by applying quantum effects. Instead of just walking through the landscape of the function, quantum annealing can tunnel through. This allows the algorithm to escape from local minima using quantum tunneling (tunnel effect) instead of the thermal jumps used in SA.</p>

    <p class="sidebarafigures"><img alt="" class="calibre2" src="../Images/CH05_F04_UN02_Khamis.png"/></p>

    <p class="sidebaracaptions">Simulated annealing versus quantum annealing</p>

    <p class="fm-sidebar-text">In QA, a number of candidate states are initialized with equal weights. Quantum-mechanical probability is used to change adiabatically and gradually the amplitudes of all states in parallel. For more information and an example of quantum annealers, see the D-Wave implementation: <a class="url" href="https://docs.dwavesys.com/docs/latest/c_gs_2.html">https://docs.dwavesys.com/docs/latest/c_gs_2.html</a>.</p>
  </div>

  <p class="body">The following subsections explain the different components of the SA algorithm, starting with the transition probability that allows SA to accept or reject non-improving moves.<a id="idIndexMarker020"/><a id="marker-165"/></p>

  <h3 class="fm-head1" id="heading_id_7">5.2.3 Acceptance probability</h3>

  <p class="body">Unlike hill climbing (see section 4.3.1), SA probabilistically allows downward steps, controlled by the current temperature and how bad the move is. In SA, better moves are always accepted. As shown in figure 5.4, non-improving moves can be probabilistically accepted based on the Boltzmann-Gibbs distribution. <a id="idIndexMarker021"/><a id="idIndexMarker022"/></p>

  <p class="body">In thermodynamics, a state at a temperature <i class="timesitalic">t</i> has a probability of an increase in the energy magnitude <span class="times">Δ<i class="fm-italics">E</i></span> given by the Boltzmann-Gibbs distribution as in equation 5.1:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F04_UN02_Khamis-EQ01.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.1</p>
        </td>
      </tr>
    </tbody>
  </table><!--<p class="Equation"><img src="05-web-resources/image/khamis-ch5-eqs-1x.png" alt="" /><span class="Eq-capt">5.1</span></p>-->

  <p class="body">where <i class="timesitalic">k</i> is the Boltzmann constant, which is the proportionality factor that relates the average relative kinetic energy of particles in a gas with the thermodynamic temperature of the gas, and which has the value of <span class="times">1.380,649 × 10<sup class="fm-superscript">-23</sup> m<sup class="fm-superscript">2</sup> kg s<sup class="fm-superscript">-2</sup> K<sup class="fm-superscript">-1</sup></span>. However, there’s no need to use this constant in a computational model that mimics the physical annealing process, so it’s replaced by 1.</p>

  <p class="body">Moreover, the change in the energy can be replaced by the change in the objective function as a way to quantify the search progress toward the optimal or near-optimal state. So <span class="times">Δ<i class="fm-italics">E</i></span> can be linked with the change of the objective function using equation 5.2:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F04_UN02_Khamis-EQ02.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.2</p>
        </td>
      </tr>
    </tbody>
  </table><!--<p class="Equation"><img src="05-web-resources/image/khamis-ch5-eqs-2x.png" alt="" /><span class="Eq-capt">5.2</span></p>-->

  <p class="body">where <i class="timesitalic">γ</i> is a real constant. For simplicity, and without altering the core meaning, we can use <span class="times"><i class="fm-italics">k</i> = 1</span> and <span class="times"><i class="fm-italics">γ</i> = 1</span>. Thus, the transition probability <i class="timesitalic">p</i> simply becomes</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F04_UN02_Khamis-EQ03.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.3</p>
        </td>
      </tr>
    </tbody>
  </table><!--<p class="Equation"><img src="05-web-resources/image/khamis-ch5-eqs-3x.png" alt="" /><span class="Eq-capt">5.3</span></p>-->

  <p class="body">where <i class="timesitalic">T</i> is the temperature of the system. To determine whether or not we accept a change, we usually use a random number <i class="timesitalic">r</i> in the interval [0,1] as a threshold. Thus, if <span class="times"><i class="fm-italics">p</i> &gt; <i class="fm-italics">r</i></span>, or <span class="times"><i class="fm-italics">p</i> = <i class="fm-italics">e</i><sup class="fm-superscript">(–Δ</sup><i class="fm-italics"><sup class="fm-superscript">f</sup></i> <sup class="fm-superscript">/</sup><i class="fm-italics"><sup class="fm-superscript">T</sup></i><sup class="fm-superscript">)</sup> &gt; <i class="fm-italics">r</i></span>, the move is accepted. Otherwise, the move is rejected.</p>

  <p class="body"><a id="marker-166"/>If <i class="timesitalic">P<sub class="fm-subscript">ij</sub></i> is the probability of moving from point <i class="timesitalic">x<sub class="fm-subscript">i</sub></i> to <i class="timesitalic">x<sub class="fm-subscript">j</sub></i>, then <i class="timesitalic">P<sub class="fm-subscript">ij</sub></i> is calculated using</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F04_UN02_Khamis-EQ04a.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.4</p>
        </td>
      </tr>
    </tbody>
  </table><!--<p class="Equation"><img src="05-web-resources/image/khamis-ch5-eqs-4x.png" alt="" /><span class="Eq-capt">5.4</span></p>-->

  <p class="body">The probability <i class="timesitalic">P<sub class="fm-subscript">ij</sub></i> is called transition or acceptance probability. Accepting non-improving moves probabilistically makes the algorithm able to avoid getting trapped in some local minima. If the acceptance probability is set to 0, SA behaves similarly to hill climbing, as it will only accept solutions that are better than the current one. Conversely, if the acceptance probability is set to 1, SA becomes more exploratory, as it will always accept worse solutions, making it more akin to a random search.</p>

  <p class="body">The probability of accepting a worse state is a function of both the temperature of the system and the change in the cost function. As the temperature decreases, the probability of accepting worse moves decreases. Temperature can be seen as a parameter to balance the exploration and the exploitation in the search space. At high temperatures, the acceptance probability is high, which means that the algorithm accepts most of the moves to explore the parameter space. On the other hand, when the temperature is low, the acceptance probability is low, meaning that the algorithm restricts exploration. As shown in figure 5.5, if <span class="times"><i class="fm-italics">T</i> = 0</span>, no non-improving moves are accepted. In this case, SA is converted into hill climbing. As can be seen, the cooling process has an important effect on the search progress. The next section will present the different components of the cooling schedules used in SA.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F05_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.5 Change of acceptance probability with the temperature and the change in the objective function. The objective function change is the difference in the objective function’s value between the current solution and a candidate solution. In minimization problems, a positive objective function change indicates that the candidate solution is worse than the current solution. The acceptance probability gets lower as the objective function change increases. At high temperatures, SA tends to explore more by accepting non-improving moves. As the temperature gets lower, the algorithm restricts exploration, favoring exploitation.</p>
  </div>

  <p class="body"><a id="marker-167"/>Given that the Boltzmann-based acceptance probability takes significant computational time (~1/3 of the SA computations), lookup tables or non-exponential probability formulas can be used instead. A lookup table can be generated by performing the exponential calculations offline only once for a range of values for changes in <i class="timesitalic">f</i> and <i class="timesitalic">T</i>. Other non-exponential probability formulas, such as <span class="times"><i class="fm-italics">p</i>(Δ<i class="fm-italics">f</i>) = 1 – Δ<i class="fm-italics">f</i>/<i class="fm-italics">T</i></span>, can be used as an acceptance probability. This formula should be normalized to make sure that the maximum value is 1 and the minimum value is 0. <a id="idIndexMarker023"/><a id="idIndexMarker024"/></p>

  <p class="body">In a computational model like SA, there is no need to strictly mimic the thermodynamic models that govern the physical annealing process. Figure 5.6 shows the difference between exponential and non-exponential acceptance probability functions. The code is available in the book’s GitHub repo. The difference between exponential and non-exponential acceptance probability functions is small for small changes in the objective function—you can experiment with this using the provided code.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F06_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.6 Exponential versus non-exponential acceptance probability</p>
  </div>

  <p class="body">As temperature is part of the acceptance probability, it plays an important role in controlling the behavior of SA. The following subsection looks at how we can control the temperature to achieve a trade-off between exploration and exploitation.</p>

  <h3 class="fm-head1" id="heading_id_8">5.2.4 The annealing process</h3>

  <p class="body">The annealing process in SA involves the careful control of temperature and the cooling rate, often called the <i class="fm-italics">annealing schedule</i>. This process involves defining the following parameters: <a id="idIndexMarker025"/><a id="idIndexMarker026"/><a id="marker-168"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Starting temperature</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Temperature decrement following a cooling schedule</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Number of iterations at each temperature</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Final temperature</p>
    </li>
  </ul>

  <p class="body">This is shown in figure 5.7.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F07_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.7 Annealing process parameters</p>
  </div>

  <p class="body">The following subsections provide in-depth information about each of these parameters.</p>

  <p class="fm-head2">Initial temperature</p>

  <p class="body">The choice of the right initial temperature is crucially important. As shown in equation 5.4, for a given change <span class="times">Δ<i class="fm-italics">f</i></span><a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="marker-169"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">If <i class="timesitalic">T</i> is too high (<span class="times"><i class="fm-italics">T</i> <span class="cambria">→</span> ∞</span>), then <span class="times"><i class="fm-italics">p</i> <span class="cambria">→</span> 1</span>, which means almost all the changes will be accepted and the algorithm will behave like a random search algorithm.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">If <i class="timesitalic">T</i> is too low (<span class="times"><i class="fm-italics">T</i> <span class="cambria">→</span> 0</span>), then any <span class="times">Δ<i class="fm-italics">f</i> &gt; 0</span> (worse solution assuming a minimization problem) will rarely be accepted as <span class="times"><i class="fm-italics">p</i> <span class="cambria">→</span> 0</span>, and thus the diversity of the solution is limited, but any improvement (i.e., any <span class="times">Δ<i class="fm-italics">f</i> &lt; 0</span> in the case of a minimization problem) will almost always be accepted. In this case, SA behaves like a local search and may easily become trapped in local minima.</p>
    </li>
  </ul>

  <p class="body">To find a suitable starting temperature, we can use any available information about the objective function. If we know the maximum change <span class="times">max(Δ<i class="fm-italics">f</i>)</span> of the objective function, we can use this to estimate an initial temperature <i class="timesitalic">T<sub class="fm-subscript">o</sub></i> for a given acceptance probability <i class="timesitalic">p<sub class="fm-subscript">o</sub></i> using equation 5.5:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F07_Khamis-EQ05.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.5</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">If the potential maximum alteration of the objective function is unknown, we can use the following heuristic approach:</p>

  <ol class="calibre7">
    <li class="fm-list-bullet">
      <p class="list">Initiate evaluations at a very high temperature to allow for nearly all changes to be accepted.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Reduce the temperature quickly until roughly 50% to 60% of the inferior moves are accepted.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Use this temperature as the new initial temperature <i class="timesitalic">T<sub class="fm-subscript">o</sub></i> for proper and relatively slow cooling processing.</p>
    </li>
  </ol>

  <p class="fm-head2">Temperature decrement</p>

  <p class="body"><a id="marker-170"/>The cooling schedule is the rate at which the temperature is systematically decreased as the algorithm proceeds. This schedule is among the tunable parameters of SA. The following cooling schedules are commonly used:<a id="idIndexMarker029"/><a id="idIndexMarker030"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Linear cooling schedule</i>—The temperature is decremented linearly using equation 5.6:</p>
    </li>
  </ul>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F07_Khamis-EQ06.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.6</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body-ind">where <i class="fm-italics">T<sub class="fm-subscript">o</sub></i> is the initial temperature, <i class="timesitalic">i</i> is the pseudo time for iterations, and <i class="timesitalic">β</i> is the cooling rate, which should be chosen in such a way that <span class="times"><i class="fm-italics">T</i> <span class="cambria">→</span> 0</span> when <span class="times"><i class="fm-italics">i</i> <span class="cambria">→</span> <i class="fm-italics">i<sub class="fm-subscript">f</sub></i></span> (or the maximum number <i class="timesitalic">N</i> of iterations). This usually gives</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F07_Khamis-EQ07.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.7</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body-ind">This cooling schedule is simple and easy to implement, but may not be the best choice for all types of problems. Moreover, it requires prior knowledge or assumptions about the maximum number of iterations.</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Linear-inverse cooling schedule</i>—In linear-inverse cooling, the temperature decreases quickly at high temperatures and more gradually at low temperatures, as per equation 5.8. In this equation, <i class="timesitalic">α</i> is the cooling factor and should be between 0 and 1:<a id="idIndexMarker031"/></p>
    </li>
  </ul>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F07_Khamis-EQ08.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.8</p>
        </td>
      </tr>
    </tbody>
  </table>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Geometric cooling schedule</i>—A geometric cooling schedule essentially decreases the temperature by a cooling factor <span class="times">0 &lt; <i class="fm-italics">α</i> &lt; 1</span> following equation 5.9:</p>
    </li>
  </ul>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F07_Khamis-EQ09.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.9</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body-ind">The cooling process should be slow enough to allow the system to stabilize easily. In practice, <span class="times"><i class="fm-italics">α</i> = 0.7 ~ 0.95</span> is commonly used. The higher the value of <i class="timesitalic">α</i>, the longer it will take to reach the final (low) temperature. The main advantage of the geometric method is that <span class="times"><i class="fm-italics">T</i> <span class="cambria">→</span> 0</span> when <span class="times"><i class="fm-italics">i</i> <span class="cambria">→</span> ∞</span>, and thus there is no need to specify the maximum number of iterations. Moreover, the geometric annealing schedule provides more gradual cooling, as shown in figure 5.8.<a id="idIndexMarker032"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Logarithmic cooling schedule</i>—In this cooling schedule, the temperature is decreased logarithmically according to equation 5.10:<a id="idIndexMarker033"/><a id="marker-171"/></p>
    </li>
  </ul>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F07_Khamis-EQ10.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.10</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body-ind">where <span class="times"><i class="fm-italics">α</i> &gt; 1</span>. Theoretically, this cooling process asymptotically converges toward the global minimum. However, it requires prohibitive computing time.</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Exponential cooling schedule</i>—In this cooling schedule, the temperature is decreased exponentially according to equation 5.11:</p>
    </li>
  </ul>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F07_Khamis-EQ11.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.11</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body-ind">where <i class="timesitalic">α</i> is the cooling factor and <i class="timesitalic">n</i> is the dimensionality of the model space. In this cooling process, the temperature is decreased very quickly during the first iterations, but the speed of the exponential decay is slowed down later and can be controlled using the cooling factor.<a id="idIndexMarker034"/></p>

  <div class="figure">
    <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F08_Khamis.png"/></p>

    <p class="figurecaptiond">Figure 5.8 Different SA cooling schedules</p>
  </div>

  <p class="body">As you can see, these cooling schedules are all monotonically decreasing functions that don’t explicitly take into consideration how the search is progressing. In section 5.2.5, we’ll look at a nonmonotonic adaptive cooling schedule.</p>

  <p class="fm-head2">Iterations at each temperature</p>

  <p class="body">Before applying the cooling schedule (i.e., decreasing the temperature), it is important to allow a sufficient number of iterations at each temperature level to stabilize the system at that temperature. Typically, this is achieved by using a constant value. For example, the number of iterations at each temperature might be exponential to the problem size (e.g., the number of cities in TSP as a discrete problem or the dimensionality of a mathematical function in the case of continuous problems). However, this value can be altered dynamically. <a id="idIndexMarker035"/><a id="idIndexMarker036"/><a id="marker-172"/></p>

  <p class="body">One way to accomplish this is by limiting the number of iterations during the exploration phase of the search at the beginning, when the temperature is high. For example, when the temperature is high, we could perform a small number of iterations at each temperature and then implement the cooling process. As the search continues and the temperature decreases, we can shift toward exploitation by conducting a larger number of iterations at lower temperatures.</p>

  <p class="fm-head2">Final temperature</p>

  <p class="body">It is usual to let the temperature decrease until it reaches zero. However, this can make the algorithm run a lot longer, especially when certain cooling schedules, such as geometric cooling, are used. In reality, it is not necessary to let the temperature reach zero if the chances of accepting a non-improving move at the current temperature are almost the same as if the temperature were zero. Therefore, the stopping criteria can be either of the following:<a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="idIndexMarker039"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">A suitably low temperature <span class="times">(<i class="fm-italics">T<sub class="fm-subscript">f</sub></i> = 10<sup class="fm-superscript">–10</sup> ~ 10<sup class="fm-superscript">–5</sup>)</span></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">When the system reaches a “frozen” or minimum energy state (assuming a minimization problem), where neither better nor worse moves are accepted</p>
    </li>
  </ul>

  <h3 class="fm-head1" id="heading_id_9">5.2.5 Adaptation in SA</h3>

  <p class="body">Several parameters in SA can be used to make the algorithm more adaptive to the search’s progress. The initial temperature, the cooling schedule, and the number of iterations per temperature are the most critical of these parameters. Other components include the cost function, the method of generating neighborhood solutions, and the acceptance probability.<a id="idIndexMarker040"/><a id="idIndexMarker041"/></p>

  <p class="body">As illustrated in figure 5.9, the initial temperature can be used to control the exploration and exploitation behavior of SA. A high temperature leads to a high level of exploration, and a low temperature results in exploitative behavior (i.e., restricting the search around neighbors).</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F09_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.9 Effect of temperature in SA. High temperatures result in more exploration, whereas a low temperature restricts the exploration and leads to more exploitation in the search space.</p>
  </div>

  <p class="body"><a id="marker-173"/>You can think about it in terms of the movement of molecules. Assume that the molecule is the search agent. At high temperatures, the molecule moves freely in the search space, exploring different solutions. At low temperatures, the movement of the molecule becomes limited, so the exploration is restricted, and the search agent focuses on a specific part of the search space. With high temperatures at the beginning of the search, SA oscillates due to the exploration behavior that makes the algorithm accept non-improving moves with high probability. As the search progresses and the temperature gets lower, the algorithm starts to stabilize due to the exploitation behavior that makes the algorithm accept fewer non-improving moves and instead focus on the elite improving solutions.</p>

  <p class="body">It is always recommended that you start with a high temperature and gradually decrease it as the search progresses. However, the right initial temperature is problem-dependent. You can try different values and see which leads to better solutions. Some researchers suggest doing this adaptively, using other search methods or metaheuristics, such as a genetic algorithm.</p>

  <p class="body">Cooling schedules can also be used to make the algorithm more adaptive. Different cooling schedules can be used in different phases of the search, taking into consideration that most useful work is done in the middle of the schedule. Reheating can also be tried if no progress is observed. Cooling may take place every time a move (or a specific number of moves) is accepted. A nonmonotonic adaptive cooling schedule can be tried, where an adaptive factor is used, based on the difference between the current solution objective and the best objective achieved by the algorithm up to that moment, according to the following formula:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F09_Khamis-EQ12.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">5.12</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">where <i class="timesitalic">T</i> is the system temperature at each state transition, <span class="times"><i class="fm-italics">T</i>(<i class="fm-italics">i</i>)</span> is the current temperature, <i class="timesitalic">f<sub class="fm-subscript">i</sub></i> is the value of the objective function at iteration <i class="timesitalic">i</i>, and <span class="times"><i class="fm-italics">f</i><sup class="fm-superscript">*</sup></span> is the best value of the objective function obtained so far.</p>

  <p class="body"><a id="marker-174"/>Another adaptation parameter is the number of iterations per temperature. This number can be adaptively changed by allowing a small number of iterations at high temperatures and allowing a large number of iterations at low temperatures to fully explore the local optimum.</p>

  <p class="body">The adaptation ability of the SA algorithm can be also influenced by the objective function and the representation of problem constraints utilized. As a general rule, it is advisable to steer clear of cost functions that yield the same result for multiple states (e.g., the number of edges incorporated in a TSP route). This type of function does not guide the search because it may not change in the objective function from one state to another.</p>

  <p class="body">For example, imagine two feasible routes with the same number of edges (i.e., <span class="times">Δ<i class="fm-italics">f</i> = 0)</span>. Counting on the number of edges as a cost function wouldn’t be a good idea. However, many problems have constraints that can be represented in the cost function using reward or penalty terms. One way to make the algorithm more adaptive is to dynamically change the weighting of the reward and penalty terms. In the initial phase, the constraints can be relaxed more than in the advanced phases of the search.</p>

  <p class="body">There have been numerous efforts to make the selection and control of SA parameters totally adaptive. One example of such an effort was proposed by Ingber in “Adaptive simulated annealing (ASA): Lessons learned” [3]. ASA automatically adjusts the algorithm parameters that control the temperature schedule, requiring the user to only specify the cooling rate. The method uses a linear random combination of previously accepted steps and parameters to estimate new steps and parameters.</p>

  <p class="body">An ASA algorithm with greedy search (ASA-GS) is proposed by Geng et al. to solve the TSP [4]. ASA-GS is based on the classical SA algorithm and utilizes a greedy search technique to speed up the convergence rate. ASA utilizes dynamic adjustments in parameters like the temperature cooling coefficient, greedy search iterations, compulsive accept instances, and probability of accepting a new solution. These adaptive parameter controls aim to enhance the trade-off between quality and time efficiency.<a id="idIndexMarker042"/></p>

  <p class="body">SA finds extensive application across various domains. Its utility extends to solving diverse optimization problems encompassing nonlinear function optimization, TSP, academic course scheduling, network design, task allocation, circuit partitioning and placement, robot motion planning, and vehicle routing, as well as resource allocation and scheduling. The following sections show how you can use SA to solve continuous and discrete optimization problems in different domains.<a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="idIndexMarker045"/></p>

  <h2 class="fm-head" id="heading_id_10">5.3 Function optimization</h2>

  <p class="body"><a id="marker-175"/>As an example of continuous optimization problems, let’s consider the following simple function optimization problem: find <i class="timesitalic">x</i> which minimizes <span class="times"><i class="fm-italics">f</i>(<i class="fm-italics">x</i>) = (<i class="fm-italics">x</i> – 6)<sup class="fm-superscript">2</sup></span>, subject to the constraint <span class="times">0 ≤ <i class="fm-italics">x</i> ≤ 31</span>. <a id="idIndexMarker046"/><a id="idIndexMarker047"/><a id="idIndexMarker048"/></p>

  <p class="body">We can start with an initial random solution from the range of <i class="timesitalic">x</i>. Different neighboring solutions can be generated by adding a random floating value chosen from a Gaussian or normal distribution with a given mean and standard deviation. The <code class="fm-code-in-text">random.gauss()</code> function in Python can be used as follows:<a id="idIndexMarker049"/></p>
  <pre class="programlisting">import random
mu, sigma = 0, 1 # mean and standard deviation
print(random.gauss(mu, sigma))</pre>

  <p class="body">Assume that the initial temperature <span class="times"><i class="fm-italics">T</i><sub class="fm-subscript">0</sub> = 5</span>, the number of iterations per temperature is 2, and the geometric cooling factor <span class="times"><i class="fm-italics">α</i> = 0.85</span>. Let’s carry out a few hand iterations to show how SA can solve this problem:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Initialization</i>—An initial solution is randomly generated, and its cost is evaluated as follows: <span class="times"><i class="fm-italics">x</i> = 2</span> and <span class="times"><i class="fm-italics">f</i>(2) = 16</span>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Iteration 1</i>—A new solution <span class="times"><i class="fm-italics">x</i> = 2.25</span> is generated by adding a random value from a Gaussian distribution using the following code:</p>
    </li>
  </ul>
  <pre class="programlistingd">import numpy as np
x=x+np.random.normal(mu, sigma, 1)</pre>

  <p class="body-ind"><span class="times"><i class="fm-italics">f</i>(2.25) = 14.06</span> is an improving solution and so is accepted.</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Iteration 2</i>—A new solution is generated by adding a random value to the last accepted solution from the previous iteration. The new solution <span class="times"><i class="fm-italics">x</i> =2.25 – 1.07 = 1.18</span>, <span class="times"><i class="fm-italics">f</i>(1.18) = 23.23</span> is a non-improving solution, so the acceptance probability must be calculated: <span class="times"><i class="fm-italics">p</i> = <i class="fm-italics">e</i><sup class="fm-superscript">–Δf /T</sup> = <i class="fm-italics">e</i><sup class="fm-superscript">–(23.23 – 14.06)/5</sup> = 0.1597</span>. We generate a random number <span class="times"><i class="fm-italics">r</i> between (0,1)</span>, and let’s say it was <span class="times"><i class="fm-italics">r</i> = 0.37</span>. As <span class="times"><i class="fm-italics">p</i> <span class="cambria">≯</span> <i class="fm-italics">r</i></span>, we reject this solution.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Iteration 3</i>—We update the temperature because we have used the initial temperature <span class="times"><i class="fm-italics">T<sub class="fm-subscript">0</sub></i> = 5</span> for two iterations so far. Following geometric cooling, the new temperature is <span class="times"><i class="fm-italics">T<sub class="fm-subscript">1</sub></i> = <i class="fm-italics">T<sub class="fm-subscript">o</sub> α<sup class="fm-superscript">i</sup></i> = 5*0.85<sup class="fm-superscript">1</sup> = 4.25</span>. We’ll use this value for two iterations starting with this iteration. We’ll now generate a new solution based on the last accepted solution by adding a random value from a Gaussian distribution. The new solution is <span class="times"><i class="fm-italics">x</i> = 2.25 + 1.57 = 3.82</span> with <span class="times"><i class="fm-italics">f</i>(3.82) = 4.75</span>. This is an improving solution, so it’s accepted, and the search continues.</p>
    </li>
  </ul>

  <p class="body">SciPy provides Python implementations for SA algorithms and other algorithms to handle mathematical optimization problems. <code class="fm-code-in-text">scipy.optimize.anneal</code> is deprecated in SciPy, and a <code class="fm-code-in-text">dual_annealing()</code> function is available instead. The following listing shows the Bohachevsky function (which has the formula <span class="times"><i class="fm-italics">f</i>(<i class="fm-italics">x<sub class="fm-subscript">1</sub></i>,<i class="fm-italics">x<sub class="fm-subscript">2</sub></i>) = <i class="fm-italics">x<sub class="fm-subscript">1</sub></i><sup class="fm-superscript">2</sup> + 2<i class="fm-italics">x<sub class="fm-subscript">2</sub></i><sup class="fm-superscript">2</sup> – 0.3cos(3<i class="fm-italics">πx<sub class="fm-subscript">1</sub></i>) – 0.4cos(3<i class="fm-italics">πx<sub class="fm-subscript">2</sub></i>) + 0.7</span>) solution using the SciPy dual annealing algorithm. The complete listing is available in the book’s GitHub repo.<a id="idIndexMarker050"/><a id="idIndexMarker051"/><a id="marker-176"/></p>

  <p class="fm-code-listing-caption">Listing 5.1 Function optimization using <code class="fm-code-in-text">scipy.optimize.dual_annealing</code></p>
  <pre class="programlisting">#!pip install scipy
import numpy as np
from scipy.optimize import dual_annealing
  
def objective_function(solution):                                                 <span class="fm-combinumeral">①</span>
    return solution[0]**2 +2*(solution[1]**2) - 0.3*np.cos(3*np.pi*solution[0]) –
<span class="fm-code-continuation-arrow">➥</span> 0.4*np.cos(4*np.pi*solution[1]) + 0.7
  
bounds = np.asarray([[-100, 100], [-100, 100]])                                   <span class="fm-combinumeral">②</span>
  
res_dual = dual_annealing(objective_function, bounds=bounds, maxiter = 100)       <span class="fm-combinumeral">③</span>
  
print('Dual Annealing Solution: f(%s) = %.5f' % (res_dual['x'], res_dual['fun'])) <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Define the objective function or functions (e.g., Bohachevsky function).</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Define the boundary constraints of the decision variables.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Perform the dual annealing search.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Print the dual annealing solution.</p>

  <p class="body">MEALPY is another Python library that provides implementations of different nature-inspired metaheuristic algorithms (see appendix A for more details). As a continuation, the following code shows the Bohachevsky function solution using MEALPY SA (the complete version of listing 5.1 is available in the book’s GitHub repo):<a id="idIndexMarker052"/></p>
  <pre class="programlisting">#!pip install mealpy
from numpy import exp, arange
import matplotlib.pyplot as plt
from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show
from mealpy.physics_based.SA import OriginalSA 
  
problem = {"fit_func": objective_function,"lb": [bounds[0][0], bounds[1][0]], "ub":
<span class="fm-code-continuation-arrow">➥</span> [bounds[0][1], bounds[1][1]], "minmax": "min", "obj_weights": [1, 1]}    <span class="fm-combinumeral">①</span>
  
epoch = 100                                                                 <span class="fm-combinumeral">②</span>
pop_size = 10                                                               <span class="fm-combinumeral">②</span>
max_sub_iter = 2                                                            <span class="fm-combinumeral">②</span>
t0 = 1000                                                                   <span class="fm-combinumeral">②</span>
t1 = 1                                                                      <span class="fm-combinumeral">②</span>
move_count = 5                                                              <span class="fm-combinumeral">②</span>
mutation_rate = 0.1                                                         <span class="fm-combinumeral">②</span>
mutation_step_size = 0.1                                                    <span class="fm-combinumeral">②</span>
mutation_step_size_damp = 0.99                                              <span class="fm-combinumeral">②</span>
  
model = OriginalSA(epoch, pop_size, max_sub_iter, t0, t1, move_count, mutation_rate,
<span class="fm-code-continuation-arrow">➥</span> mutation_step_size, mutation_step_size_damp)                             <span class="fm-combinumeral">③</span>
  
mealpy_solution, mealpy_value = model.solve(problem)                        <span class="fm-combinumeral">④</span>
  
print('MEALPY SA Solution: f(%s) = %.5f' % (mealpy_solution, mealpy_value)) <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Define the problem</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Define the MEALPY algorithm parameters to perform an SA search using MEALPY.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Define a MEALPY SA solver.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Solve the problem using a defined solver.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Print the MEALPY SA solution.</p>

  <p class="body"><a id="marker-177"/>Figure 5.10 shows the Bohachevsky function’s solution. The performance of the algorithm depends mainly on its parameter tuning and stopping criteria. MEALPY runs a parallel version of SA and exposes many parameters to be tuned.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F10_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.10 Solution of a continuous function optimization problem using SA. The cross in the center is the optimal solution. The triangle is the solution obtained by MEALPY SA. The dot is the SciPy dual annealing solution.</p>
  </div>

  <p class="fm-callout"><span class="fm-callout-head">Note</span> Appendix A shows how to use SA in other Python packages to solve mathematical optimization problems.</p>

  <p class="body">Let’s implement an SA algorithm from scratch so we can gain more control and better handle different types of continuous and discrete optimization problems. In our implementation in the optalgotools package, we decouple the problem definition from the solver so we can use the solvers to handle different problems.</p>

  <p class="body">Let’s apply our implementation to find the global minimum of the aforementioned simple function optimization problem and of more complex function optimization problems as well. There are several complex mathematical functions in multidimensional space, such as Rosenbrock’s function, the Ackley function, the Rastrigin function, the Schaffer function, the Schwefel function, the Langermann function, the Levy function, the Bukin function, the Eggholder function, the cross-in-tray function, the drop wave function, and the Griewank function. Examples of function optimization test problems and datasets can be found in appendix B. <a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="idIndexMarker057"/><a id="idIndexMarker058"/><a id="idIndexMarker059"/><a id="idIndexMarker060"/><a id="idIndexMarker061"/><a id="idIndexMarker062"/><a id="idIndexMarker063"/><a id="idIndexMarker064"/><a id="marker-178"/></p>

  <p class="body">Listing 5.2 shows how SA can be used to solve the following mathematical functions:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">A simple quadratic equation</i>—This is what we used in the hand iterations.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">The Bohachevsky function (global minimum 0)</i>—This is a 2D unimodal function with a bowl shape. This function is known to be continuous, convex, separable, differentiable, nonmultimodal, nonrandom, and nonparametric, so derivate-based solvers can efficiently handle it. Note that a function whose variables can be separated is known as a <i class="fm-italics">separable function</i>. Nonrandom functions contain no random variables. Nonparametric functions assume that the data distribution cannot be defined in terms of a finite set of parameters.<a id="idIndexMarker065"/><a id="idIndexMarker066"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">The Bukin function</i>—This function has many local minima, all of which lie on a ridge, and one global minimum <span class="times"><i class="fm-italics">f</i>(<i class="fm-italics">x<sub class="fm-subscript">0</sub></i>) = 0</span> at <span class="times"><i class="fm-italics">x<sub class="fm-subscript">0</sub></i> = <i class="fm-italics">f</i>(−10,1)</span>. This function is continuous, convex, nonseparable, nondifferentiable, multimodal, nonrandom, and nonparametric. This requires a derivative-free solver (also known as a black-box solver) such as SA.<a id="idIndexMarker067"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">The Gramacy &amp; Lee function</i>—This is a 1D function with multiple local minima and local and global trends. This function is continuous, nonconvex, separable, differentiable, nonmultimodal, nonrandom, and nonparametric.<a id="idIndexMarker068"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">The Griewank 1D, 2D, and 3D functions</i>—These have many widespread local minima. These functions are continuous, nonconvex, separable, differentiable, multimodal, nonrandom, and nonparametric.<a id="idIndexMarker069"/></p>
    </li>
  </ul>

  <p class="body">In our implementation, these are the SA parameters:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">A maximum number of iterations: <code class="fm-code-in-text">max_iter=1000</code><code class="fm-code-in-text"><a class="calibre" id="idIndexMarker070"/></code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Maximum iterations per temperature: <code class="fm-code-in-text">max_iter_per_temp=100</code><code class="fm-code-in-text"><a class="calibre" id="idIndexMarker071"/></code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">An initial temperature: <code class="fm-code-in-text">initial_temp=1000</code><code class="fm-code-in-text"><a class="calibre" id="idIndexMarker072"/></code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">A final temperature: <code class="fm-code-in-text">final_temp=0.0001</code><code class="fm-code-in-text"><a class="calibre" id="idIndexMarker073"/></code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">A cooling schedule: <code class="fm-code-in-text">cooling_schedule='geometric'</code> (available options: <code class="fm-code-in-text">'linear'</code>, <code class="fm-code-in-text">'geometric'</code>, <code class="fm-code-in-text">'logarithmic'</code>, <code class="fm-code-in-text">'exponential'</code>, <code class="fm-code-in-text">'linear_inverse'</code>)<a id="idIndexMarker074"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">A cooling factor: <code class="fm-code-in-text">cooling_alpha=0.9</code><code class="fm-code-in-text"><a class="calibre" id="idIndexMarker075"/></code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">A debug option: <code class="fm-code-in-text">debug=1</code> (<code class="fm-code-in-text">debug=1</code> prints the initial and final solution; <code class="fm-code-in-text">debug=2</code> provides hand-iteration tracing)<a id="idIndexMarker076"/><a id="marker-179"/></p>
    </li>
  </ul>

  <p class="body">Feel free to change these settings and observe their effect on the performance of the algorithm.</p>

  <p class="fm-code-listing-caption">Listing 5.2 Continuous function optimization using SA</p>
  <pre class="programlisting">import random
import math
import numpy as np
from optalgotools.algorithms import SimulatedAnnealing
from optalgotools.problems import ProblemBase, ContinuousFunctionBase
  
def simple_example(x):                                                       <span class="fm-combinumeral">①</span>
    return (x-6)**2                                                          <span class="fm-combinumeral">①</span>
  
simple_example_bounds = np.asarray([[0, 31]])                                <span class="fm-combinumeral">①</span>
simple_example_obj = ContinuousFunctionBase(simple_example, simple_example_  <span class="fm-combinumeral">①</span>
<span class="fm-code-continuation-arrow">➥</span> simple_example_bounds)                                                    <span class="fm-combinumeral">①</span>
sa = SimulatedAnnealing(max_iter=1000, max_iter_per_temp=100,                <span class="fm-combinumeral">①</span>
<span class="fm-code-continuation-arrow">➥</span> initial_ temp=1000, final_temp=0.0001, cooling_schedule='geometric',      <span class="fm-combinumeral">①</span>
<span class="fm-code-continuation-arrow">➥</span> cooling_alpha=0.9, debug=1)                                               <span class="fm-combinumeral">①</span>
sa.run(simple_example_obj)                                                   <span class="fm-combinumeral">①</span>
  
def Bohachevsky(x_1, x_2):                                                   <span class="fm-combinumeral">②</span>
    return x_1**2 +2*(x_2**2)-0.3*np.cos(3*np.pi*x_1)-0.4*np.cos(4*np.       <span class="fm-combinumeral">②</span> 
pi*x_2)+0.7                                                                  <span class="fm-combinumeral">②</span>
  
Bohachevsky_bounds = np.asarray([[-100, 100], [-100, 100]])                  <span class="fm-combinumeral">②</span>
Bohachevsky_obj = ContinuousFunctionBase(Bohachevsky, Bohachevsky_bounds, 5) <span class="fm-combinumeral">②</span>
sa.run(Bohachevsky_obj)                                                      <span class="fm-combinumeral">②</span>
  
  
def bukin(x_1, x_2):                                                         <span class="fm-combinumeral">③</span>
    return 100*math.sqrt(abs(x_2-0.01*x_1**2)) + 0.01 * abs(x_1 + 10)        <span class="fm-combinumeral">③</span>
  
bukin_bounds = np.asarray([[-15, -5], [-3, 3]])                              <span class="fm-combinumeral">③</span>
bukin_obj = ContinuousFunctionBase(bukin, bukin_bounds, 5)                   <span class="fm-combinumeral">③</span>
sa.run(bukin_obj)                                                            <span class="fm-combinumeral">③</span>
  
def gramacy_and_lee(x):                                                      <span class="fm-combinumeral">④</span>
    return math.sin(10*pi*x)/(2*x) + (x-1)**4                                <span class="fm-combinumeral">④</span>
  
gramacy_and_lee_bounds = np.asarray([[0.5, 2.5]])                            <span class="fm-combinumeral">④</span>
gramacy_and_lee_obj = ContinuousFunctionBase(gramacy_and_lee, gramacy_and_   <span class="fm-combinumeral">④</span>
<span class="fm-code-continuation-arrow">➥</span> lee_bounds, .1)                                                           <span class="fm-combinumeral">④</span>
sa.run(gramacy_and_lee_obj)                                                  <span class="fm-combinumeral">④</span>
  
def griewank(*x):                                                            <span class="fm-combinumeral">⑤</span>
    x = np.asarray(x)                                                        <span class="fm-combinumeral">⑤</span>
    return np.sum(x**2/4000) - np.prod(np.cos(x/np.sqrt(np.asarray(range(1,  <span class="fm-combinumeral">⑤</span>
    <span class="fm-code-continuation-arrow">➥</span> len(x)+1))))) + 1                                                     <span class="fm-combinumeral">⑤</span>
  
griewank_bounds = np.asarray([[-600, 600]])                                  <span class="fm-combinumeral">⑤</span>
griewank_1d=ContinuousFunctionBase(griewank, griewank_bounds, 10)            <span class="fm-combinumeral">⑤</span>
sa.run(griewank_1d)                                                          <span class="fm-combinumeral">⑤</span>
  
griewank_bounds_2d = np.asarray([[-600, 600]]*2)                             <span class="fm-combinumeral">⑥</span>
griewank_2d=ContinuousFunctionBase(griewank, griewank_bounds_2d,             <span class="fm-combinumeral">⑥</span>
<span class="fm-code-continuation-arrow">➥</span> (griewank_bounds_2d[:, 1] - griewank_bounds_2d[:, 0])/10)                 <span class="fm-combinumeral">⑥</span>
sa.run(griewank_2d)                                                          <span class="fm-combinumeral">⑥</span>
  
  
griewank_bounds_3d = np.asarray([[-600, 600]]*3)                             <span class="fm-combinumeral">⑦</span>
griewank_3d=ContinuousFunctionBase(griewank, griewank_bounds_3d,             <span class="fm-combinumeral">⑦</span>
<span class="fm-code-continuation-arrow">➥</span> (griewank_bounds_3d[:, 1] - griewank_bounds_3d[:, 0])/10)                 <span class="fm-combinumeral">⑦</span>
sa.run(griewank_3d)                                                          <span class="fm-combinumeral">⑦</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Quadratic function SA-based solution</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Bohachevsky SA-based solution</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Bukin SA-based solution</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Gramacy &amp; Lee SA-based solution</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Griewank 1D SA-based solution</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Griewank 2D SA-based solution</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Griewank 3D SA-based solution</p>

  <p class="body">This is an example of the output for the Bukin function:</p>
  <pre class="programlisting">Simulated annealing is initialized:
current value = 60.73784664253138, current temp=1000
Simulated Annealing is done: 
curr iter: 154, curr best value: 0.6093437608551259, curr temp:9.97938882337113e-05, curr best: sol: [-14.63282848   2.14122839]
global minimum: x = -14.6328, 2.1412, f(x) = 0.6093</pre>

  <p class="body">As can be seen, SA is able to handle different multidimensional, nonlinear function optimization problems. This stochastic global optimization algorithm is able to adapt to the landscape of the objective function and avoid getting trapped in local minima. However, in the case of multidimensional functions such as Griewank 2D and 3D, SA takes time to converge. The following sections show how SA can handle discrete problems such as Sudoku and TSP.<a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="idIndexMarker079"/><a id="marker-180"/></p>

  <h2 class="fm-head" id="heading_id_11">5.4 Solving Sudoku</h2>

  <p class="body">Sudoku, also known as Su Doku, is one of the most popular number puzzles of all time. This game is adapted from a mathematical concept called a <i class="fm-italics">Latin square</i>. The first version of the Sudoku puzzle was created by a retired architect named Howard Garns and appeared in the late 1970s as a puzzle in <i class="fm-italics">Dell Pencil Puzzles and Word Games</i>. The game subsequently showed up in Japan in 1984 under the name “Sudoku,” which is abbreviated from the Japanese “Sūji wa dokushin ni kagiru,” which means the numbers (or digits) must remain single. Nowadays, Sudoku games are popular around the globe and are published in game websites, puzzle booklets, and newspapers. <a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="idIndexMarker082"/><a id="idIndexMarker083"/></p>

  <p class="body">The Sudoku game can be seen as a constraint-satisfaction problem (CSP) that is solved by correctly filling a <span class="times">9 × 9</span> grid with digits so that each column, each row, and each of the nine <span class="times">3 × 3</span> subgrids (aka “boxes,” “blocks,” or “regions”) contain all of the digits from 1 to 9. Any row or column or <span class="times">3 × 3</span> subgrid shouldn't contain more than one of the same number from 1 to 9. <a id="idIndexMarker084"/></p>

  <p class="body">Aside from entertainment, Sudoku is used in real-life applications such as developmental psychology and steganography. For example, several studies have showed that solving Sudoku or crosswords or other brain games may help in keeping your brain 10 years younger and can slow down the progression of conditions such as Alzheimer’s. Sudoku can also be used as a tool to improve problem-solving skills, critical thinking, and attention. Finally, steganography is the technique of hiding images, messages, files, or other secret data within something that isn’t a secret. In secret data delivery applications, digital images can be used to conceal secret data. The Sudoku puzzle is then used to modify selected pixel pairs in the cover image, based on a specially designed reference matrix, to insert secret digits.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Latin square</p>

    <p class="fm-sidebar-text">Latin squares were devised in the 10th century by Arabic numerologists who dealt with the mystical power of numbers. Islamic amulets, known as wafq majazi, from the 13th century have been found, and they were sketched in the margins of a 16th century Arabic medical text. The name “Latin” was inspired by the famous Swiss mathematician Leonhard Euler (1707–1783) who used Latin letters as symbols in the squares. <a id="idIndexMarker085"/></p>

    <p class="fm-sidebar-text">A <i class="fm-italics">Latin square</i> is an <span class="times"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> array filled with <i class="timesitalic">n</i> different numbers, symbols, or colors arranged in such a way that no orthogonal (row or column) contains the same number, symbol, or color twice. An example of a <span class="times">4 × 4</span> Latin square is shown here:</p>

    <p class="sidebarafigures"><img alt="" class="calibre2" src="../Images/CH05_F10_UN03_Khamis.png"/></p>

    <p class="fm-sidebar-text">Latin squares are different from <i class="fm-italics">magic squares</i>. A magic square is a square array of positive integers <span class="times">1, 2, ..., <i class="fm-italics">n</i><sup class="fm-superscript">2</sup></span> arranged such that the sum of the <i class="timesitalic">n</i> numbers in any horizontal, vertical, or main diagonal line is always the same number. Sudoku is based on Latin squares. In fact, any solution to a Sudoku puzzle is a Latin square. KenKen and KenDoku are other number puzzles based on an enhanced version of Latin squares and require some degree of arithmetic skills.<a id="idIndexMarker086"/><a id="marker-181"/></p>
  </div>

  <p class="body">Generally speaking, the search space of Sudoku is huge. There are <span class="times">6.671 × 10<sup class="fm-superscript">21</sup></span> possible solvable Sudoku grids that yield a unique result [5]. According to Encyclopedia Britannica, if each human on earth solves one Sudoku puzzle every second, they wouldn’t get through all of them until about the year 30,992. However, taking out symmetries, such as rotations, reflections, permuting columns and rows, and swapping digits, the number of essentially different Sudoku grids is reduced to <span class="times">5,472,730,538 ≈5.473 × 10<sup class="fm-superscript">9</sup> [6]</span>. The generalized <span class="times"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> Sudoku problem is an NP-complete problem. However, some instances, such as standard <span class="times">9 × 9</span> Sudoku, are not NP-complete. Constant-time algorithms exist to solve some instances of <span class="cambria">9 × 9</span> Sudoku, in <span class="times"><i class="fm-italics">O</i>(1)</span> time, as each and every <span class="times">9 × 9</span> Sudoku can be listed, enumerated, and indexed in a finite dictionary or a lookup table used to find a solution. However, these algorithms cannot handle arbitrary generalized <span class="times"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> Sudoku problems.</p>

  <p class="body">Backtracking, dancing links, and Crook’s pencil-and-paper are common algorithms for solving Sudoku, especially if the size of the problem is small. <i class="fm-italics">Backtracking</i> is mainly a classical depth-first search that tests a whole branch until that branch violates the rules or returns a solution.</p>

  <p class="body"><i class="fm-italics">Dancing links</i> (DLX), invented by Donald Knuth in 2000, uses algorithm X to solve Sudoku puzzles, handled as exact cover problems. In the exact cover problem, given a binary matrix (i.e., a matrix composed only of 0 and 1), it is necessary to find a set of rows containing exactly one 1 in each column. Algorithm X, a recursive search algorithm, is applied to solve the exact cover problem using the backtracking method. <a id="idIndexMarker087"/><a id="marker-182"/></p>

  <p class="body">In <i class="fm-italics">Crook’s pencil-and-paper algorithm</i>, all possible numbers in each cell are listed. This list of numbers is called marking-up of the cell. We then try to find out if there is a row, column, or block with only one possible value throughout the row, column, or block. Once found, we fill in this cell with this number and update the markups in any affected row, column, or box. The next step is to find preemptive sets. As described in Crook’s paper, a preemptive set is composed of numbers from the set <span class="times">[1,2,…,9]</span> and is a set of size <span class="times"><i class="fm-italics">m</i>, 2 ≤ <i class="fm-italics">m</i> ≤ 9</span>, whose numbers are potential occupants of <i class="timesitalic">m</i> cells exclusively, where exclusively means that no other numbers in the set <span class="times">[1,2,…,9]</span>, other than the members of the preemptive set, are potential occupants of those <i class="timesitalic">m</i> cells. The last step is to eliminate possible numbers outside preemptive sets.<a id="idIndexMarker088"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Backtracking</p>

    <p class="fm-sidebar-text">Backtracking algorithms are commonly used to solve search and optimization problems by recursion. The backtracking algorithm builds a feasible solution or a set of feasible solutions incrementally. Given a <span class="times">9 × 9</span> Sudoku board, the algorithm visits all the empty cells following depth-first traversal order, filling the digits incrementally, and it backtracks when a number is not found to be valid. The following figure illustrates the backtracking algorithm steps for a <span class="times">9 × 9</span> Sudoku puzzle.<a id="idIndexMarker089"/><a id="marker-183"/></p>

    <p class="sidebarafigures"><img alt="" class="calibre2" src="../Images/CH05_F10_UN04_Khamis.png"/></p>

    <p class="sidebaracaptions">Backtracking steps for a <span class="times">9 × 9</span> Sudoku puzzle</p>
  </div>

  <p class="body">The next listing shows how a <span class="times">9 × 9</span> Sudoku is solved using the SA algorithm.</p>

  <p class="fm-code-listing-caption">Listing 5.3 Solving Sudoku using SA</p>
  <pre class="programlisting">from optalgotools.algorithms import SimulatedAnnealing                 <span class="fm-combinumeral">①</span>
from optalgotools.problems import Sudoku                               <span class="fm-combinumeral">②</span>
  
sa = SimulatedAnnealing(max_iter=100000, max_iter_per_temp=1000, 
<span class="fm-code-continuation-arrow">➥</span> initial_temp=500, final_temp=0.001, cooling_schedule='geometric',
<span class="fm-code-continuation-arrow">➥</span> cooling_alpha=0.9, debug=1)                                         <span class="fm-combinumeral">③</span>
  
  
Sudoku_hard = [
    [9, 0, 0, 1, 0, 0, 0, 5, 4],
    [0, 0, 0, 0, 8, 0, 0, 0, 0],
    [0, 0, 5, 0, 0, 9, 0, 0, 3],
    [0, 9, 0, 0, 3, 5, 0, 4, 1],
    [0, 0, 0, 0, 1, 0, 0, 0, 0],
    [4, 1, 0, 2, 6, 0, 0, 8, 0],
    [7, 0, 0, 3, 0, 0, 1, 0, 0],
    [0, 0, 0, 0, 4, 0, 0, 0, 0],
    [3, 5, 0, 0, 0, 1, 0, 0, 6],
]
sudoku_prob = Sudoku(Sudoku_hard)    
sudoku_prob.print()                                                   <span class="fm-combinumeral">④</span>
                                                                      <span class="fm-combinumeral">④</span>
sudoku_prob.solve_backtrack()                                         <span class="fm-combinumeral">⑤</span>
sa.run(sudoku_prob, 0)                                                <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Import the SA solver.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Import a Sudoku problem.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Create a SA solver with the selected parameters.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Create a hard <span class="times">9 × 9</span> Sudoku (available variants include trivial, easy, medium, hard, and evil).</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Solve the Sudoku using the backtracking algorithm.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Solve the Sudoku using SA.</p>

  <p class="body">You can try different variants of Sudoku by changing the puzzle configuration. In easy Sudoku problems, cells contain more prefilled numbers than medium or hard ones. Evil Sudoku is the highest level of puzzle difficulty. Table 5.2 compares the time it takes for SA, backtracking, and the Python Linear Programming (PuLP) library to solve different instances of <span class="times">9 × 9</span> Sudoku puzzles. PuLP provides linear and mixed programming solvers. The default solver used in PuLP is Cbc (COIN-OR branch and cut), which is an open source solver for mixed integer linear programming problems. More information about PuLP is available in appendix A. <a id="idIndexMarker090"/><a id="idIndexMarker091"/><a id="marker-184"/></p>

  <p class="fm-table-caption">Table 5.2 SA versus backtracking versus PuLP in solving a <span class="times">9 × 9</span> Sudoku puzzle</p>

  <table border="1" class="contenttable-1-table" id="table002" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="30%"/>
      <col class="contenttable-0-col" span="1" width="12.5%"/>
      <col class="contenttable-0-col" span="1" width="12.5%"/>
      <col class="contenttable-0-col" span="1" width="12.5%"/>
      <col class="contenttable-0-col" span="1" width="12.5%"/>
      <col class="contenttable-0-col" span="1" width="20%"/>
    </colgroup>

    <thead class="calibre6">
      <tr class="contenttable-0-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Time to find the solution(s)</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Trivial</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Easy</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Medium</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Hard</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Evil</p>
        </th>
      </tr>
    </thead>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-1-th1">
          <p class="fm-table-head">Backtracking</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.01</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.01</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.11</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.69</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">1.58</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-th1">
          <p class="fm-table-head">PuLP</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.69</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.12</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.11</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.13</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.12</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-th1">
          <p class="fm-table-head">Classical SA</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.10</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.07</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.01</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">3:17</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Suboptimal in 3:16</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">As you can see, classical SA does not outperform the backtracking approach and is much slower in the case of hard and evil instances of Sudoku problems. PuLP is efficiently able to handle different variants of Sudoku in a consistent time, compared to backtracking and SA.</p>

  <p class="body">In the case of evil Sudoku, SA converges to a suboptimal solution despite trying different parameter settings. Given that this is a constraint-satisfaction problem, the notion of suboptimality is not valid, as a suboptimal solution is an invalid solution. This means that SA doesn’t manage to solve the evil instance of Sudoku. Being a well-structured problem, <span class="times">9 × 9</span> Sudoku with different levels of difficulty can be easily solved using a backtracking algorithm. Generally speaking, if the problem is well-structured with a well-known algorithm solution, metaheuristics approaches don’t usually outperform these classical and more deterministic approaches.<a id="idIndexMarker092"/><a id="idIndexMarker093"/><a id="idIndexMarker094"/></p>

  <h2 class="fm-head" id="heading_id_12">5.5 Solving TSP</h2>

  <p class="body">As described in section 2.1.1, the traveling salesman problem (TSP) is used as a platform for the study of general methods that can be applied to a wide range of discrete optimization problems. Consider solving the instance of TSP shown in figure 5.11 using SA. In this TSP, a traveling salesman must visit five cities and return home, making a loop (a round trip). <a id="idIndexMarker095"/><a id="idIndexMarker096"/><a id="idIndexMarker097"/><a id="marker-185"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F11_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.11 TSP for five cities—there are 5!/2 = 60 possible tours, assuming symmetric TSP. The weights on the edges of the graph represent the travel distances between the cities.</p>
  </div>

  <p class="body">Assume the following values: initial temperature = 500, final temperature = 50, a linear decrement rate of 50, and one iteration at each temperature. A TSP solution takes the form of a permutation as follows: Solution = [ 1, 3, 4, 2, 5]. The objective function is the total distance of the route. Swapping is a suitable operator that can be used to generate neighboring solutions:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Iteration 0</i>—The initial solution is Solution = [ 1, 3, 4, 2, 5], cost = 2 + 4 + 5 + 5 + 12 = 28, as shown in figure 5.12.</p>
    </li>
  </ul>

  <div class="figure">
    <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F12_Khamis.png"/></p>

    <p class="figurecaptiond">Figure 5.12 SA iteration 0 of a 5-city TSP</p>
  </div>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Iteration 1</i>—To generate a candidate solution, select two random cities (e.g., 2 and 3), and swap them. This results in a new solution [ 1, 2, 4, 3, 5] with a cost of 35 (figure 5.13).</p>
    </li>
  </ul>

  <div class="figure">
    <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F13_Khamis.png"/></p>

    <p class="figurecaptiond">Figure 5.13 SA iteration 1 of a 5-city TSP</p>
  </div>

  <p class="body"><a id="marker-186"/>Since the new solution has a longer tour length, it will be conditionally accepted, according to a probability of <span class="times"><i class="fm-italics">p</i> = <i class="fm-italics">e</i><sup class="fm-superscript">–</sup><sup class="fm-superscript">Δ</sup><i class="fm-italics"><sup class="fm-superscript">f</sup></i> <sup class="fm-superscript">/</sup><i class="fm-italics"><sup class="fm-superscript">T</sup></i> = <i class="fm-italics">e</i><sup class="fm-superscript">–(35–28) /</sup><i class="fm-italics"><sup class="fm-superscript">T</sup></i> = <i class="fm-italics">e</i><sup class="fm-superscript">–7 /</sup><i class="fm-italics"><sup class="fm-superscript">T</sup></i></span> (at higher temperatures, there’s a higher probability of acceptance). We pick a random value <i class="timesitalic">r</i> within 0 and 1. If <span class="times"><i class="fm-italics">P</i> &gt; <i class="fm-italics">r</i></span>, we accept this solution. Otherwise, we reject this solution. Assuming the new solution was not accepted, we generate a different one, starting from the initial solution:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Iteration 2</i>—A solution is generated by swapping cities 2 and 5 in the initial solution. The tour length of the candidate solution is 18 (figure 5.14).</p>
    </li>
  </ul>

  <div class="figure">
    <p class="figured"><img alt="" class="calibre4" src="../Images/CH05_F14_Khamis.png"/></p>

    <p class="figurecaptiond">Figure 5.14 SA iteration 2 of a 5-city TSP</p>
  </div>

  <p class="body">Since this solution has a shorter tour length, it will be accepted, and the search continues until the termination criteria are met. The following listing shows an SA solution for this simple TSP problem.</p>

  <p class="fm-code-listing-caption">Listing 5.4 Solving TSP using SA</p>
  <pre class="programlisting">from optalgotools.algorithms import SimulatedAnnealing
from optalgotools.problems import TSP
  
dists = [ [0] * 5 for _ in range(5)]
dists[0][1] = dists[1][0] = 4
dists[0][2] = dists[2][0] = 2
dists[0][3] = dists[3][0] = 9
dists[0][4] = dists[4][0] = 12
dists[1][2] = dists[2][1] = 7
dists[1][3] = dists[3][1] = 5
dists[1][4] = dists[4][1] = 5
dists[2][3] = dists[3][2] = 4
dists[2][4] = dists[4][2] = 10
dists[3][4] = dists[4][3] = 3
  
tsp_sample = TSP(dists, 'random_swap')                                           <span class="fm-combinumeral">①</span>
  
sa = SimulatedAnnealing(max_iter=10000, max_iter_per_temp=1, initial_temp=500,
<span class="fm-code-continuation-arrow">➥</span> final_temp=50, cooling_schedule='linear_inverse', cooling_alpha=0.9, debug=2) <span class="fm-combinumeral">②</span>
sa.run(tsp_sample)                                                               <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Create an instance of a TSP problem.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Create an instance of the SA solver.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Run the SA solver, and show the results in each iteration.</p>

  <p class="body"><a id="marker-187"/>Let’s now consider some benchmark instances of TSP, such as Berlin52 from TSPLIB (<a class="url" href="http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/">http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/</a>). This dataset contains 52 locations in the city of Berlin. The shortest route obtained for the Berlin52 dataset is 7,542. The next listing shows how we can solve this TSP instance using our SA implementation.</p>

  <p class="fm-code-listing-caption">Listing 5.5 Solving the Berlin52 TSP using SA</p>
  <pre class="programlisting">from optalgotools.problems import TSP
from optalgotools.algorithms import SimulatedAnnealing
import matplotlib.pyplot as plt
  
berlin52_tsp_url = 'https://raw.githubusercontent.com/coin-or/
jorlib/b3a41ce773e9b3b5b73c149d4c06097ea1511680/jorlib-core/src/test/resources/
tspLib/tsp/berlin52.tsp'                                                  <span class="fm-combinumeral">①</span>
  
berlin52_tsp = TSP(load_tsp_url=berlin52_tsp_url, gen_method='mutate',
<span class="fm-code-continuation-arrow">➥</span> rand_len=True, init_method='random')                                   <span class="fm-combinumeral">②</span>
  
sa = SimulatedAnnealing(max_iter=1200, max_iter_per_temp=500, initial_temp=150,
<span class="fm-code-continuation-arrow">➥</span> final_temp=0.01, cooling_schedule='linear', debug=1)                   <span class="fm-combinumeral">③</span>
  
sa.run(berlin52_tsp, repetition=1)                                        <span class="fm-combinumeral">④</span>
print(sa.val_allbest)                                                     <span class="fm-combinumeral">④</span>
  
berlin52_tsp.plot(sa.s_best)                                              <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Permanent URL for the Berlin52 dataset</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Create a TSP object for Berlin52.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Create an SA model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Run SA, and evaluate the best solution distance.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Plot the route.</p>

  <p class="body">Here is an example of the output:</p>
  <pre class="programlisting">sol: [0, 48, 34, 35, 33, 43, 45, 36, 37, 47, 23, 4, 14, 5, 3, 24, 11, 50, 10, 
51, 13, 12, 26, 27, 25, 46, 28, 15, 49, 19, 22, 29, 1, 6, 41, 20, 30, 17, 16, 
2, 44, 18, 40, 7, 8, 9, 32, 42, 39, 38, 31, 21, 0]
8106.88</pre>

  <p class="body">Figure 5.15 shows the route generated by SA for Belin52 TSP.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F15_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.15 Solution of Belin52 using SA</p>
  </div>

  <p class="body"><a id="marker-188"/>As you can see, the near-optimal solution found by SA is 8,106.88. This value is a bit higher than the best-known solution for the Berlin52 TSP, which is 7,542. Parameter tuning and algorithm adaptation can help improve the results. For example, Geng et al.’s “Solving the traveling salesman problem based on an adaptive simulated annealing algorithm with greedy search” paper discusses using three kinds of mutations (vertex insert mutation, block insert mutation, and block reverse mutation) with different probabilities during the search to improve the accuracy of SA in solving TSP problems. Moreover, parameters such as the cooling coefficient of the temperature, the times of greedy search used to speed up the convergence rate, the times of compulsive accept, and the probability of accepting a new solution, can be adapted according to the size of the TSP instances. An implementation of this adaptive algorithm is available from this GitHub repo: <a class="url" href="https://github.com/ildoonet/simulated-annealing-for-tsp">https://github.com/ildoonet/simulated-annealing-for-tsp</a>.</p>

  <p class="body">The effect of the algorithm’s parameters can be studied, such as the initial temperature, the cooling schedule, the number of iterations per temperature, and the final temperature. For example, SA was applied for the Berlin52 TSP instance with the following settings:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Maximum number of iterations = 1200</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Number of iterations per each temperature <span class="times">T = 500</span></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="times">T<i class="fm-italics"><sub class="fm-subscript">initial</sub></i> = 150</span></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="times">T<i class="fm-italics"><sub class="fm-subscript">final</sub></i> = 0.01</span></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Linear cooling</p>
    </li>
  </ul>

  <p class="body">Our implementation supports the following methods for mutating a new solution from an old one:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">random_swap</code>—Swap two cities in the path. This can be done multiple times for the same solution by using <code class="fm-code-in-text">num_swaps</code>. Also, the swap can be done in a smaller window of the whole path using <code class="fm-code-in-text">swap_wind = [1 - n]</code>. For example, suppose the route is [A, B, C, D, F]. Swapping two random cities, like B and F, will result in a new route [A, F, C, D, B].</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">reverse</code>—Reverse the order of a subset of the cities with either a random length, using <code class="fm-code-in-text">rand_len</code>, or using <code class="fm-code-in-text">rev_len</code>, which has a default of 2. For example, starting with the solution [A, B, C, D, F], if we apply <code class="fm-code-in-text">reverse</code> with length 3, we can get a new solution [A, D, C, B, F].</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">insert</code>—Pick a random city, remove it from the path, and reinsert it before a different random city. For example, starting from the solution [A, B, C, D, F], we could pick city B and insert it before city F so we get a new solution [A, C, D, B, F].</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">mutate</code>—Randomly pick a number of consecutive cities from the current solution, and shuffle them. For example, starting from the solution [A, B, C, D, F], we may pick C, D, F and shuffle them so we get a new solution [A, B, F, C, D].</p>
    </li>
  </ul>

  <p class="body"><a id="marker-189"/>The implementation also supports two methods of initializing the path:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">random</code>—This means the path is generated completely randomly.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">greedy</code>—This tries to select a possibly suboptimal initial path by selecting the pairwise shortest distances between cities. This will not lead to the shortest path, but it may be better than the random initialization.</p>
    </li>
  </ul>

  <p class="body">It is worth noting that the results of the SA algorithm may not be exactly repeatable. Due to the randomness included in the algorithm, each time you run the algorithm, you may get slightly different results. To avoid this, the <code class="fm-code-in-text">run</code> function included in the <code class="fm-code-in-text">SimulatedAnnealing</code> class contains a <code class="fm-code-in-text">repetition</code> argument that allows you to report the best solution generated out of multiple runs, as follows:<a id="idIndexMarker098"/><a id="idIndexMarker099"/><a id="idIndexMarker100"/><a id="idIndexMarker101"/></p>
  <pre class="programlisting">run(self, problem_obj=None, stoping_val=None, init=None, repetition=1)</pre>

  <p class="body">You can set repetition to be 10, so the algorithm reports the best solution generated out of 10 runs.</p>

  <h2 class="fm-head" id="heading_id_13">5.6 Solving a delivery semi-truck routing problem<a id="idTextAnchor001"/></h2>

  <p class="body"><a id="marker-190"/>Let’s consider a more real-life example of TSP. Assume that Walmart Supercenters are points of interest (POIs) to be visited by a delivery semi-truck. The vehicle will start from Walmart Supercenter number 3001, located at 270 Kingston Rd. E in Ajax, Ontario. It is required to find the shortest possible route the truck can follow to visit each POI only once and get back to the home location. There are 18 Walmart Supercenters in the selected part of the Greater Toronto Area (GTA), as shown in figure 5.16. This results in 18! possible routes to visit these stores located in Durham Region, York Region, and Toronto, Ontario.<a id="idIndexMarker102"/><a id="idIndexMarker103"/><a id="idIndexMarker104"/><a id="idIndexMarker105"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F16_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.16 Selected Walmart Supercenters in the Greater Toronto Area (GTA)</p>
  </div>

  <p class="body">The GPS coordinates (longitude and latitude) of each POI and the addresses are available on the POI Factory website (<a class="url" href="http://www.poi-factory.com/node/25560">www.poi-factory.com/node/25560</a>) and are included in the Walmart_United States&amp;Canada.csv file that can be downloaded for free (for noncommercial use) after registration. Google Places API, Here Places API, and SafeGraph on ArcGIS Marketplace can also be used to get data about points of interest such as hospitals, restaurants, retail stores, and grocery stores. Appendix B provides more details about open data sources.</p>

  <p class="body">The OSMnx library is used in this example to create a NetworkX graph that represents the supercenter locations. Pyrosm can also be used instead of OSMnx. The shortest distances between these locations are computed using the NetworkX built-in function <code class="fm-code-in-text">shortest_path</code>, which uses Dijkstra’s algorithm as a default method (see section 3.4.1). Supercenter locations are rendered on OpenStreetMap based on their GPS coordinates using the folium library. Appendix A provides more details about these libraries.<a id="idIndexMarker106"/></p>

  <p class="body">In our implementation, the problem solver is decoupled from the problem object. We start by creating a TSP object for this discrete problem. We then create an SA object to solve the TSP problem. An initial solution is generated using the <code class="fm-code-in-text">mutate</code> method. As shown in figure 5.17, this initial solution is far from optimal. The total length of the initial route is 593.88 km, and this route is not convenient or easy to follow in practice.<a id="idIndexMarker107"/><a id="marker-191"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F17_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.17 Initial solution for the Walmart delivery semi-truck route with a total distance of 593.88 km</p>
  </div>

  <p class="body">Let’s run SA with the following parameters:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Maximum number of iterations = 10000</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Maximum interaction per temperature = 100</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Initial temperature = 85</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Final temperature = 0.0001</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Linear cooling schedule</p>
    </li>
  </ul>

  <p class="body">Other cooling schedules could also be used. For example, geometric cooling can generate consistent, superior-quality, and timely solutions compared to other schemes. However, this is one of the algorithm parameters that can be tuned, as it sometimes depends on the nature of the problem. Figure 5.18 shows the shortest route with a total distance of 227.17 km.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F18_Khamis.png"/></p>

    <p class="figurecaption">Figure 5.18 SA solution for the Walmart delivery semi-truck route with a total distance of 227.17 km</p>
  </div>

  <p class="body"><a id="marker-192"/>The next listing is a snippet of the code used to generate the shortest route for the delivery semi-truck using SA. The complete code is available in the book’s GitHub repo.</p>

  <p class="fm-code-listing-caption">Listing 5.6 Generating a Walmart delivery semi-truck route using SA</p>
  <pre class="programlisting">from optalgotools.algorithms import SimulatedAnnealing
from optalgotools.problems import TSP
import numpy as np
import pandas as pd
import osmnx as ox
import networkx as nx
import folium
import folium.plugins
  
wal_df = pd.read_csv("https://raw.githubusercontent.com/Optimization-Algorithms-
<span class="fm-code-continuation-arrow">➥</span>Book/Code-Listings/main/Appendix%20B/data/TSP/Walmart_ON.csv")                     <span class="fm-combinumeral">①</span>
  
cities_list = [city for city, region in cityToRegion.items() if city in
<span class="fm-code-continuation-arrow">➥</span>wal_df.city.unique() and region in ['Durham Region', 'York Region', 'Toronto']]    <span class="fm-combinumeral">②</span>
  
gta_part = wal_df[wal_df.store_number.str.startswith('Walmart Supercentre') &amp;
<span class="fm-code-continuation-arrow">➥</span> wal_df.city.isin(cities_list)].reset_index(drop=True)                             <span class="fm-combinumeral">③</span>
wal_gta_count = gta_part.shape[0]
  
gta_part_loc = gta_part[['latitude', 'longitude']]                                   <span class="fm-combinumeral">④</span>
  
G = ox.graph_from_point(tuple(gta_part_loc.mean().to_list()), dist=42000,            <span class="fm-combinumeral">④</span>
<span class="fm-code-continuation-arrow">➥</span> dist_type='network', network_type='drive', clean_periphery=True, simplify=True,   <span class="fm-combinumeral">④</span>
<span class="fm-code-continuation-arrow">➥</span> retain_all=True, truncate_by_edge=True)                                           <span class="fm-combinumeral">④</span>
  
gta_part['osmid'], gta_part['osmid_dist_m'] = zip(*gta_part.apply(lambda row:
<span class="fm-code-continuation-arrow">➥</span>ox.nearest_nodes(G, row.longitude, row.latitude, return_dist=True), axis = 1)) 
  
gta_part_dists = np.zeros([wal_gta_count, wal_gta_count])                            <span class="fm-combinumeral">⑤</span>
gta_part_pathes = [[[] for i in range(wal_gta_count)] for j in range(wal_gta_count)] <span class="fm-combinumeral">⑤</span>
for i in range(wal_gta_count):                                                       <span class="fm-combinumeral">⑤</span>
    for j in range(wal_gta_count):                                                   <span class="fm-combinumeral">⑤</span>
        if i==j:                                                                     <span class="fm-combinumeral">⑤</span>
            continue                                                                 <span class="fm-combinumeral">⑤</span>
        gta_part_pathes[i][j] = nx.shortest_path(G=G, source=gta_part.osmid[i],      <span class="fm-combinumeral">⑤</span>
<span class="fm-code-continuation-arrow">➥</span> target=gta_part.osmid[j], weight='length', method='dijkstra')                     <span class="fm-combinumeral">⑤</span>
        gta_part_dists[i][j] = nx.shortest_path_length(G=G,                          <span class="fm-combinumeral">⑤</span>
<span class="fm-code-continuation-arrow">➥</span>source=gta_part.osmid[i], target=gta_part.osmid[j], weight='length',               <span class="fm-combinumeral">⑤</span>
<span class="fm-code-continuation-arrow">➥</span> method='dijkstra')/1000                                                           <span class="fm-combinumeral">⑤</span>
  
gta_part_tsp = TSP(dists=gta_part_dists, gen_method='mutate')                        <span class="fm-combinumeral">⑥</span>
  
sa = SimulatedAnnealing(max_iter=1000, max_iter_per_temp=100, initial_temp=85,
<span class="fm-code-continuation-arrow">➥</span> final_temp=0.0001, cooling_schedule='linear')                                     <span class="fm-combinumeral">⑦</span>
  
sa.init_annealing(gta_part_tsp)                                                      <span class="fm-combinumeral">⑧</span>
sa.run(gta_part_tsp)                                                                 <span class="fm-combinumeral">⑨</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Load a list of all Walmart locations in Ontario.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Select cities that are in Durham Region, York Region, or Toronto.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Select Walmart stores that are in the preceding list and are Supercenters.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Get the lat. and long. locations of the preceding set of Walmarts, and create a graph of roads that connects them and that is within 42 km.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculate the distances between the Walmart locations using the graph.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Create a TSP object for the problem.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Create an SA object to help solve the TSP problem.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Get an initial random solution, and check its length</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Run SA, and evaluate the best solution distance.</p>

  <p class="body"><a id="marker-193"/>As you can see, we separated the solver class from the problem object in optalgotools. The solver is imported from <code class="fm-code-in-text">algorithms</code>, and the problem is an instance of the TSP problem in the <code class="fm-code-in-text">problems</code> class. This implementation allows you to change the problem instances and tune the parameters of the algorithm to reach an optimal or near-optimal solution. You may consider trying the adaptation aspects of SA explained in section 5.2.5 to figure out their effect on the algorithm’s performance in terms of the length of the obtained route and the run time (CPU time and wall-clock time).<a id="idIndexMarker108"/></p>

  <p class="body">A metaheuristic algorithm like SA seeks optimal or near-optimal solutions at a reasonable computational cost, but it cannot guarantee either their feasibility or degree of optimality. With proper parameter tuning, the algorithm can provide acceptable solutions without further postprocessing. In the next chapter, we will discuss tabu search as another trajectory-based optimization algorithm. <a id="idIndexMarker109"/><a id="idIndexMarker110"/></p>

  <h2 class="fm-head" id="heading_id_14">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Metaheuristic algorithms can be broadly classified into trajectory-based algorithms and population-based algorithms. A trajectory-based metaheuristic algorithm, or S-metaheuristic, uses a single search agent that moves through the design or search space in a piecewise style. Population-based algorithms, or P-metaheuristics, use multiple agents to search for an optimal or near-optimal global solution. Simulated annealing is a trajectory-based metaheuristic algorithm.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Simulated annealing mimics the annealing process in material processing, where a metal cools and freezes into a crystalline state with the minimum energy and larger crystal size so as to reduce the defects in metallic structures. The annealing process involves the careful control of temperature and cooling rate, often called the annealing schedule.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Simulated annealing runs a series of moves under different thermodynamic conditions and always accepts improving moves and can probabilistically accept non-improving moves.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The acceptance probability is proportional to the temperature. A high temperature increases the chance of accepting non-improving moves to favor exploration of the search space at the beginning of the search. As the search progresses, the temperature is decremented to restrict exploration and favor exploitation.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">As the temperature goes to zero, SA acts greedily like hill climbing, and as the temperature goes to infinity, SA behaves like a random walk. The temperature should decrease gradually to achieve the best trade-off between exploration and exploitation.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Simulated annealing is a stochastic search algorithm and a derivative-free solver that can be used when derivative information is unavailable, unreliable, or prohibitively expensive. SA seeks optimal or near-optimal solutions at a reasonable computational cost but it cannot guarantee either the feasibility or degree of optimality.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Adaptive simulated annealing can dynamically change its parameters with the search progress to control the exploration and exploitation behavior.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Simulated annealing is an easy-to-implement probabilistic approximation algorithm that can be used to solve continuous and discrete problems in different domains.<a id="marker-194"/></p>
    </li>
  </ul>
</body></html>