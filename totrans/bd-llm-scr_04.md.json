["```py\nimport torch\nfrom chapter04 import GPTModel\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,\n    \"context_length\": 256,    #1\n    \"emb_dim\": 768,\n    \"n_heads\": 12,\n    \"n_layers\": 12, \n    \"drop_rate\": 0.1,       #2\n    \"qkv_bias\": False\n}\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval()\n```", "```py\nimport tiktoken\nfrom chapter04 import generate_text_simple\n\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0)    #1\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0)                #2\n    return tokenizer.decode(flat.tolist())\n\nstart_context = \"Every effort moves you\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=10,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n```", "```py\nOutput text:\n Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n```", "```py\ninputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n                       [40,    1107, 588]])   #  \"I really like\"]\n```", "```py\ntargets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n                        [1107, 588, 11311]])  #  \" really like chocolate\"]\n```", "```py\nwith torch.no_grad():     #1\n    logits = model(inputs)\nprobas = torch.softmax(logits, dim=-1)     #2\nprint(probas.shape)\n```", "```py\ntorch.Size([2, 3, 50257])\n```", "```py\ntoken_ids = torch.argmax(probas, dim=-1, keepdim=True)\nprint(\"Token IDs:\\n\", token_ids)\n```", "```py\nToken IDs:\n tensor([[[16657],       #1\n         [  339],\n         [42826]],\n        [[49906],        #2\n         [29669],\n         [41751]]])\n```", "```py\nprint(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\nprint(f\"Outputs batch 1:\"\n      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n```", "```py\nTargets batch 1:  effort moves you\nOutputs batch 1:  Armed heNetflix\n```", "```py\ntext_idx = 0\ntarget_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\nprint(\"Text 1:\", target_probas_1)\n\ntext_idx = 1\ntarget_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\nprint(\"Text 2:\", target_probas_2)\n```", "```py\nText 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\nText 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n```", "```py\nlog_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\nprint(log_probas)\n```", "```py\ntensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n```", "```py\navg_log_probas = torch.mean(log_probas)\nprint(avg_log_probas)\n```", "```py\ntensor(-10.7940)\n```", "```py\nneg_avg_log_probas = avg_log_probas * -1\nprint(neg_avg_log_probas)\n```", "```py\nprint(\"Logits shape:\", logits.shape)\nprint(\"Targets shape:\", targets.shape)\n```", "```py\nLogits shape: torch.Size([2, 3, 50257])\nTargets shape: torch.Size([2, 3])\n```", "```py\nlogits_flat = logits.flatten(0, 1)\ntargets_flat = targets.flatten()\nprint(\"Flattened logits:\", logits_flat.shape)\nprint(\"Flattened targets:\", targets_flat.shape)\n```", "```py\nFlattened logits: torch.Size([6, 50257])\nFlattened targets: torch.Size([6])\n```", "```py\nloss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\nprint(loss)\n```", "```py\ntensor(10.7940)\n```", "```py\nfile_path = \"the-verdict.txt\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    text_data = file.read()\n```", "```py\ntotal_characters = len(text_data)\ntotal_tokens = len(tokenizer.encode(text_data))\nprint(\"Characters:\", total_characters)\nprint(\"Tokens:\", total_tokens)\n```", "```py\nCharacters: 20479\nTokens: 5145\n```", "```py\ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntrain_data = text_data[:split_idx]\nval_data = text_data[split_idx:]\n```", "```py\nfrom chapter02 import create_dataloader_v1\ntorch.manual_seed(123)\n\ntrain_loader = create_dataloader_v1(\n    train_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=True,\n    shuffle=True,\n    num_workers=0\n)\nval_loader = create_dataloader_v1(\n    val_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=False,\n    shuffle=False,\n    num_workers=0\n)\n```", "```py\nprint(\"Train loader:\")\nfor x, y in train_loader:\n    print(x.shape, y.shape)\n\nprint(\"\\nValidation loader:\")\nfor x, y in val_loader:\n    print(x.shape, y.shape)\n```", "```py\nTrain loader:\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\n\nValidation loader:\ntorch.Size([2, 256]) torch.Size([2, 256])\n```", "```py\ndef calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch = input_batch.to(device)         #1\n    target_batch = target_batch.to(device)      \n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(\n        logits.flatten(0, 1), target_batch.flatten()\n    )\n    return loss\n```", "```py\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)     #1\n    else:\n        num_batches = min(num_batches, len(data_loader))   #2\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(\n                input_batch, target_batch, model, device\n            )\n            total_loss += loss.item()    #3\n        else:\n            break\n    return total_loss / num_batches    #4\n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)   #1\nwith torch.no_grad():                                        #2\n    train_loss = calc_loss_loader(train_loader, model, device)    #3\n    val_loss = calc_loss_loader(val_loader, model, device)\nprint(\"Training loss:\", train_loss)\nprint(\"Validation loss:\", val_loss)\n```", "```py\nTraining loss: 10.98758347829183\nValidation loss: 10.98110580444336\n```", "```py\ndef train_model_simple(model, train_loader, val_loader,\n                       optimizer, device, num_epochs,\n                       eval_freq, eval_iter, start_context, tokenizer):\n    train_losses, val_losses, track_tokens_seen = [], [], []    #1\n    tokens_seen, global_step = 0, -1\n\n    for epoch in range(num_epochs):    #2\n        model.train()\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad()   #3\n            loss = calc_loss_batch(\n                input_batch, target_batch, model, device\n            )\n            loss.backward()                     #4\n            optimizer.step()                    #5\n            tokens_seen += input_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:    #6\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader, device, eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, \"\n                      f\"Val loss {val_loss:.3f}\"\n                )\n\n        generate_and_print_sample(                      #7\n            model, tokenizer, device, start_context\n        )\n    return train_losses, val_losses, track_tokens_seen\n```", "```py\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval()  #1\n    with torch.no_grad():                              #2\n        train_loss = calc_loss_loader(\n            train_loader, model, device, num_batches=eval_iter\n        )\n        val_loss = calc_loss_loader(\n            val_loader, model, device, num_batches=eval_iter\n        )\n    model.train()\n    return train_loss, val_loss\n```", "```py\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n    context_size = model.pos_emb.weight.shape[0]\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n    with torch.no_grad():\n        token_ids = generate_text_simple(\n            model=model, idx=encoded,\n            max_new_tokens=50, context_size=context_size\n        )\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\n    print(decoded_text.replace(\"\\n\", \" \"))      #1\n    model.train()\n```", "```py\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\noptimizer = torch.optim.AdamW(\n     model.parameters(),           #1\n    lr=0.0004, weight_decay=0.1\n)\nnum_epochs = 10\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model, train_loader, val_loader, optimizer, device,\n    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n    start_context=\"Every effort moves you\", tokenizer=tokenizer\n)\n```", "```py\nEp 1 (Step 000000): Train loss 9.781, Val loss 9.933\nEp 1 (Step 000005): Train loss 8.111, Val loss 8.339\nEvery effort moves you,,,,,,,,,,,,.                                     \nEp 2 (Step 000010): Train loss 6.661, Val loss 7.048\nEp 2 (Step 000015): Train loss 5.961, Val loss 6.616\nEvery effort moves you, and, and, and, and, and, and, and, and, and, and,\n and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n[...]                                                   #1\nEp 9 (Step 000080): Train loss 0.541, Val loss 6.393\nEvery effort moves you?\"  \"Yes--quite insensible to the irony. She wanted\nhim vindicated--and by me!\"  He laughed again, and threw back the \nwindow-curtains, I had the donkey. \"There were days when I\nEp 10 (Step 000085): Train loss 0.391, Val loss 6.452\nEvery effort moves you know,\" was one of the axioms he laid down across the\nSevres and silver of an exquisitely appointed luncheon-table, when, on a\nlater day, I had again run over from Monte Carlo; and Mrs. Gis\n```", "```py\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\ndef plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n    ax1.plot(\n        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n    )\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax2 = ax1.twiny()                   #1\n    ax2.plot(tokens_seen, train_losses, alpha=0)     #2\n    ax2.set_xlabel(\"Tokens seen\")\n    fig.tight_layout()\n    plt.show()\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n```", "```py\nmodel.to(\"cpu\")\nmodel.eval()\n```", "```py\ntokenizer = tiktoken.get_encoding(\"gpt2\")\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=25,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n```", "```py\nOutput text:\nEvery effort moves you know,\" was one of the axioms he laid down across the\nSevres and silver of an exquisitely appointed lun\n```", "```py\nvocab = { \n    \"closer\": 0,\n    \"every\": 1, \n    \"effort\": 2, \n    \"forward\": 3,\n    \"inches\": 4,\n    \"moves\": 5, \n    \"pizza\": 6,\n    \"toward\": 7,\n    \"you\": 8,\n} \ninverse_vocab = {v: k for k, v in vocab.items()}\n```", "```py\nnext_token_logits = torch.tensor(\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n)\n```", "```py\nprobas = torch.softmax(next_token_logits, dim=0)\nnext_token_id = torch.argmax(probas).item()\nprint(inverse_vocab[next_token_id])\n```", "```py\ntorch.manual_seed(123) \nnext_token_id = torch.multinomial(probas, num_samples=1).item()\nprint(inverse_vocab[next_token_id])\n```", "```py\ndef print_sampled_tokens(probas):\n    torch.manual_seed(123)\n    sample = [torch.multinomial(probas, num_samples=1).item()\n             for i in range(1_000)]\n    sampled_ids = torch.bincount(torch.tensor(sample))\n    for i, freq in enumerate(sampled_ids):\n        print(f\"{freq} x {inverse_vocab[i]}\")\n\nprint_sampled_tokens(probas)\n```", "```py\n73 x closer\n0 x every\n0 x effort\n582 x forward\n2 x inches\n0 x moves\n0 x pizza\n343 x toward\n```", "```py\ndef softmax_with_temperature(logits, temperature):\n    scaled_logits = logits / temperature\n    return torch.softmax(scaled_logits, dim=0)\n```", "```py\ntemperatures = [1, 0.1, 5]                                     #1\nscaled_probas = [softmax_with_temperature(next_token_logits, T)\n                for T in temperatures]\nx = torch.arange(len(vocab))\nbar_width = 0.15\nfig, ax = plt.subplots(figsize=(5, 3))\nfor i, T in enumerate(temperatures):\n    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n                   bar_width, label=f'Temperature = {T}')\nax.set_ylabel('Probability')\nax.set_xticks(x)\nax.set_xticklabels(vocab.keys(), rotation=90)\nax.legend()\nplt.tight_layout()\nplt.show()\n```", "```py\ntop_k = 3\ntop_logits, top_pos = torch.topk(next_token_logits, top_k)\nprint(\"Top logits:\", top_logits)\nprint(\"Top positions:\", top_pos)\n```", "```py\nTop logits: tensor([6.7500, 6.2800, 4.5100])\nTop positions: tensor([3, 7, 0])\n```", "```py\nnew_logits = torch.where(\n    condition=next_token_logits < top_logits[-1],    #1\n    input=torch.tensor(float('-inf')),     #2\n    other=next_token_logits     #3\n)\nprint(new_logits)\n```", "```py\ntensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,\n     -inf])\n```", "```py\ntopk_probas = torch.softmax(new_logits, dim=0)\nprint(topk_probas)\n```", "```py\ntensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610,\n        0.0000])\n```", "```py\ndef generate(model, idx, max_new_tokens, context_size,\n             temperature=0.0, top_k=None, eos_id=None):\n    for _ in range(max_new_tokens):            #1\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:, -1, :]\n        if top_k is not None:                #2\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature > 0.0:                  #3\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:    #4\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n        if idx_next == eos_id:              #5\n            break\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx\n```", "```py\ntorch.manual_seed(123)\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=15,\n    context_size=GPT_CONFIG_124M[\"context_length\"],\n    top_k=25,\n    temperature=1.4\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n```", "```py\nOutput text:\n Every effort moves you stand to work on surprise, a one of us had gone\n with random-\n```", "```py\ntorch.save(model.state_dict(), \"model.pth\")\n```", "```py\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(torch.load(\"model.pth\", map_location=device))\nmodel.eval()\n```", "```py\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict(),\n    }, \n    \"model_and_optimizer.pth\"\n)\n```", "```py\ncheckpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\nmodel.train();\n```", "```py\npip install tensorflow>=2.15.0  tqdm>=4.66\n```", "```py\nimport urllib.request\nurl = (\n    \"https://raw.githubusercontent.com/rasbt/\"\n    \"LLMs-from-scratch/main/ch05/\"\n    \"01_main-chapter-code/gpt_download.py\"\n)\nfilename = url.split('/')[-1]\nurllib.request.urlretrieve(url, filename)\n```", "```py\nfrom gpt_download import download_and_load_gpt2\nsettings, params = download_and_load_gpt2(\n    model_size=\"124M\", models_dir=\"gpt2\"\n)\n```", "```py\ncheckpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, \n                                                         63.9kiB/s]\nencoder.json: 100%|█████████████████████████| 1.04M/1.04M [00:00<00:00,\n                                                           2.20MiB/s]\nhprams.json: 100%|██████████████████████████| 90.0/90.0 [00:00<00:00,\n                                                         78.3kiB/s]\nmodel.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [01:09<00:00,\n                                                         7.16MiB/s]\nmodel.ckpt.index: 100%|█████████████████████| 5.21k/5.21k [00:00<00:00,\n                                                           3.24MiB/s]\nmodel.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, \n                                                         2.46MiB/s]\nvocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00,\n                                                         1.70MiB/s]\n```", "```py\nprint(\"Settings:\", settings)\nprint(\"Parameter dictionary keys:\", params.keys())\n```", "```py\nSettings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12,\n           'n_layer': 12}\nParameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n```", "```py\nprint(params[\"wte\"])\nprint(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)\n```", "```py\n[[-0.11010301 ... -0.1363697   0.01506208   0.04531523]\n [ 0.04034033 ...  0.08605453  0.00253983   0.04318958]\n [-0.12746179  ...  0.08991534 -0.12972379 -0.08785918]\n ...\n [-0.04453601 ...   0.10435229  0.09783269 -0.06952604]\n [ 0.1860082  ...  -0.09625227  0.07847701 -0.02245961]\n [ 0.05135201 ...   0.00704835  0.15519823  0.12067825]]\nToken embedding weight tensor dimensions: (50257, 768)\n```", "```py\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n```", "```py\nmodel_name = \"gpt2-small (124M)\"\nNEW_CONFIG = GPT_CONFIG_124M.copy()\nNEW_CONFIG.update(model_configs[model_name])\n```", "```py\nNEW_CONFIG.update({\"context_length\": 1024})\n```", "```py\nNEW_CONFIG.update({\"qkv_bias\": True})\n```", "```py\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval()\n```", "```py\ndef assign(left, right):\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n                          \"Right: {right.shape}\"\n        )\n    return torch.nn.Parameter(torch.tensor(right))\n```", "```py\nimport numpy as np\n\ndef load_weights_into_gpt(gpt, params):           #1\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n\n    for b in range(len(params[\"blocks\"])):     #2\n        q_w, k_w, v_w = np.split(                            #3\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.weight = assign(\n            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n        gpt.trf_blocks[b].att.W_key.weight = assign(\n            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n        gpt.trf_blocks[b].att.W_value.weight = assign(\n            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n\n        q_b, k_b, v_b = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.bias = assign(\n            gpt.trf_blocks[b].att.W_query.bias, q_b)\n        gpt.trf_blocks[b].att.W_key.bias = assign(\n            gpt.trf_blocks[b].att.W_key.bias, k_b)\n        gpt.trf_blocks[b].att.W_value.bias = assign(\n            gpt.trf_blocks[b].att.W_value.bias, v_b)\n\n        gpt.trf_blocks[b].att.out_proj.weight = assign(\n            gpt.trf_blocks[b].att.out_proj.weight, \n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n        gpt.trf_blocks[b].att.out_proj.bias = assign(\n            gpt.trf_blocks[b].att.out_proj.bias, \n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n            gpt.trf_blocks[b].ff.layers[0].weight, \n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n            gpt.trf_blocks[b].ff.layers[0].bias, \n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n            gpt.trf_blocks[b].ff.layers[2].weight, \n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n            gpt.trf_blocks[b].ff.layers[2].bias, \n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n\n        gpt.trf_blocks[b].norm1.scale = assign(\n            gpt.trf_blocks[b].norm1.scale, \n            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n        gpt.trf_blocks[b].norm1.shift = assign(\n            gpt.trf_blocks[b].norm1.shift, \n            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n        gpt.trf_blocks[b].norm2.scale = assign(\n            gpt.trf_blocks[b].norm2.scale, \n            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n        gpt.trf_blocks[b].norm2.shift = assign(\n            gpt.trf_blocks[b].norm2.shift, \n            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n\n    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])    #4\n```", "```py\nload_weights_into_gpt(gpt, params)\ngpt.to(device)\n```", "```py\ntorch.manual_seed(123)\ntoken_ids = generate(\n    model=gpt,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n    max_new_tokens=25,\n    context_size=NEW_CONFIG[\"context_length\"],\n    top_k=50,\n    temperature=1.5\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n```", "```py\nOutput text:\n Every effort moves you toward finding an ideal new way to practice something!\nWhat makes us want to be on top of that?\n```"]