- en: Chapter 5\. AI Engineering for High-Risk AI Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter explores the comprehensive requirements for high-risk AI systems
    and how to meet them proactively by implementing AI engineering practices to ensure
    compliance with the EU AI Act (see [Figure 5-1](#chapter_5_figure_1_1748539922504826)
    for a visual of the steps to take to move toward compliance). In the previous
    chapter, I introduced the foundational steps for compliance: creating an inventory
    of current and planned AI systems, identifying the risk category of each, and
    determining whether your organization acts as a provider or deployer of these
    systems. These steps are essential for understanding which systems fall under
    the high-risk category and what compliance obligations need to be addressed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. This chapter focuses on the requirements for high-risk AI systems
    and the operationalization of compliance for such systems. See [Chapter 1](ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819)
    for an explanation of the end-to-end process steps toward EU AI Act compliance.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Two guiding questions to consider in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: To comply with the EU AI Act, what requirements must high-risk AI systems fulfill?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What processes, structures, and AI engineering practices need to be established
    to comply with the EU AI Act?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by exploring the requirements for high-risk AI systems, translating
    them into AI engineering practices, and defining the notion of AI engineering
    for the EU AI Act in this way.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As mentioned previously, the author is not a lawyer, and this book does not
    provide legal advice. The intersection of law and artificial intelligence is a
    complex subject that requires expertise beyond the scope of AI, data science,
    and machine learning. Legal considerations surrounding AI systems can be complex
    and far-reaching. If you have any legal concerns related to the AI systems you
    are working on, seek professional legal advice from qualified experts in the field.
  prefs: []
  type: TYPE_NORMAL
- en: AI Engineering for the EU AI Act
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The EU AI Act regulates the development, deployment, and use of AI systems within
    the European Union. It aims to promote trustworthy AI by mitigating risks and
    protecting fundamental rights. As depicted in [Figure 5-2](#chapter_5_figure_2_1748539922504869),
    Articles 9–15 provide requirements for providers of high-risk AI systems. However,
    these articles do not provide technical guidance for implementing compliance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter suggests a method for technical implementation of compliance with
    the EU AI Act by aligning the Act’s articles with the quality model for safety-critical
    AI systems outlined in the paper [“Navigating the EU AI Act: A Methodological
    Approach to Compliance for Safety-Critical Products”](https://oreil.ly/2J89P)
    by Jessica Kelly et al., presented at the 2024 IEEE Conference on Artificial Intelligence
    (CAI). The authors propose an expanded product quality model for AI systems, drawing
    from established standards such as ISO/IEC 25059\. This enhanced model includes
    additional attributes relevant to the EU AI Act, such as ethical integrity, human
    oversight, and fairness.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-2\. EU AI Act Requirements for High-Risk AI Systems (source: [*https://oreil.ly/7h6xE*](https://oreil.ly/7h6xE))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Mapping EU AI Act requirements to quality attributes for AI systems is a helpful
    approach for several reasons, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Operationalization of regulatory requirements
  prefs: []
  type: TYPE_NORMAL
- en: The EU AI Act provides high-level regulatory guidelines, which can be abstract
    and challenging to implement directly. By mapping these requirements to specific
    quality attributes, we create a more concrete, actionable framework for compliance.
    This helps translate legal language into technical specifications that engineers
    and data scientists can work with.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with existing practices
  prefs: []
  type: TYPE_NORMAL
- en: Quality attributes are well-established software and systems engineering concepts.
    Mapping EU AI Act requirements to these attributes facilitates the integration
    of regulatory compliance into existing development processes and quality assurance
    practices. This allows organizations to leverage familiar tools, methodologies,
    and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Alignment with standards
  prefs: []
  type: TYPE_NORMAL
- en: Many quality attributes are defined by international standards (e.g., ISO/IEC
    25010). Mapping EU AI Act requirements to these attributes helps align regulatory
    compliance with established standards, potentially simplifying overall compliance
    efforts.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the mapping of EU AI Act articles to the quality attributes provides
    technical guidance for achieving compliance. [Figure 5-3](#chapter_5_figure_3_1748539922504896)
    provides a high-level overview of this mapping.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. Mapping of EU AI Act requirements to quality attributes for AI
    systems
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Goals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we discuss the method, let’s explore what AI engineering for the EU
    AI Act involves. This multidisciplinary field focuses on designing, developing,
    deploying, and maintaining artificial intelligence systems in full compliance
    with the requirements of the Act. It combines expertise in AI technology, software
    engineering, data science, law, and ethics in order to achieve the following goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement technical measures ensuring compliance with the Act’s requirements,
    including risk assessment, transparency, human oversight, and robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop methodologies for continuous monitoring and auditing of AI systems throughout
    their lifecycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create documentation and traceability mechanisms to demonstrate compliance and
    facilitate regulatory inspections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate privacy-enhancing technologies and data governance practices aligned
    with EU data protection laws.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design user interfaces and operational protocols that enable effective human
    oversight and intervention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborate with legal and policy experts to interpret and apply evolving AI
    regulations across diverse application domains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alignment with CRISP-ML(Q) Phases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To establish effective engineering processes, we’ll align the quality attributes
    of safety-critical (i.e., high-risk) AI systems with the phases of the CRISP-ML(Q)
    development process model introduced in [Chapter 2](ch02.html#chapter_2_ai_engineering_a_proactive_compliance_catalyst_1748539917637495).
  prefs: []
  type: TYPE_NORMAL
- en: A core principle of CRISP-ML(Q) is the integration of quality assurance practices
    into each phase of the machine learning lifecycle. This includes defining system
    requirements, identifying potential risks, and applying risk mitigation strategies
    based on established best practices. As such, this model provides a suitable framework
    for integrating the quality attributes essential to safety-critical AI systems.
    As discussed previously, these attributes can be mapped directly to regulatory
    requirements such as those defined in the EU AI Act. Addressing them throughout
    the development lifecycle supports *continuous compliance*, rather than treating
    compliance as a one-time task.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you already know, CRISP-ML(Q) defines six key phases in the ML development
    process: business and data understanding, data preparation, modeling, evaluation,
    deployment, and monitoring and maintenance. It’s an iterative framework, where
    each phase consists of specific tasks (such as dataset cleaning or model training)
    and outputs (such as datasets or model artifacts). Many quality attributes relate
    directly to potential risks. By considering these attributes in each development
    phase, you can identify and address risks early, when changes are less costly
    and more feasible to implement. In this chapter, I align the CRISP-ML(Q) phases
    with the quality attributes linked to EU AI Act requirements to establish AI engineering
    best practices that support robust implementation and sustained compliance.'
  prefs: []
  type: TYPE_NORMAL
- en: AI Engineering Practices for Achieving Compliance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The requirements for providers of high-risk AI systems are distributed across
    several articles in the EU AI Act. For your convenience, I’ve included them here
    as condensed action points:'
  prefs: []
  type: TYPE_NORMAL
- en: Establish a risk management system that spans the entire AI system lifecycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure strong data governance by validating and testing datasets to guarantee
    their relevance, representativeness, and accuracy in relation to the system’s
    intended purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create comprehensive technical documentation to demonstrate compliance and provide
    authorities with the necessary information for assessment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable automatic logging capabilities in the AI system to track serious incidents
    and substantial modifications throughout its lifecycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide detailed instructions for downstream deployers to support compliance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporate human oversight mechanisms into the system’s design for downstream
    deployers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure the system meets standards for accuracy, robustness, and cybersecurity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish a quality management system to support and maintain compliance. The
    requirements for this system are laid out in [Article 17](https://oreil.ly/zAKkT).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the remainder of this chapter we will explore Articles 9–15 of the EU AI
    Act, which lay out the requirements for providers of high-risk AI systems, and
    see how these legal requirements translate into actionable AI engineering practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article 9: Risk Management System'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To meet the requirements of Article 9 and ensure that high-risk AI systems
    are developed responsibly, organizations need to implement AI engineering processes
    that address three quality attributes throughout the AI system lifecycle. As identified
    by the paper I referenced earlier, [“Navigating the EU AI Act: A Methodological
    Approach to Compliance for Safety-Critical Products”](https://oreil.ly/2J89P)
    by Jessica Kelly et al., these are:'
  prefs: []
  type: TYPE_NORMAL
- en: Risk identification
  prefs: []
  type: TYPE_NORMAL
- en: This involves proactively recognizing potential issues that could arise from
    the AI system’s development, deployment, and operation. Implementing robust risk
    identification processes ensures that risks are managed before they become critical
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Testability
  prefs: []
  type: TYPE_NORMAL
- en: This ensures that the AI system can be thoroughly evaluated for performance,
    reliability, and safety before deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Value alignment
  prefs: []
  type: TYPE_NORMAL
- en: This ensures that the AI system’s objectives and behaviors are consistent with
    human values, ethical principles, and societal norms.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can translate these quality attributes into AI engineering
    practices. I’ll use the CRISP-ML(Q) phases as a framework to tackle the complexity
    of the end-to-end process, considering each phase separately.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Business and data understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The initial phase of the AI project focuses on establishing clear business
    goals and defining success criteria to ensure the application delivers business
    value. This involves pinpointing the specific objectives of the AI solution, identifying
    relevant data sources, and assembling an initial dataset. Practically speaking,
    conducting an AI system risk assessment during this phase involves three steps:
    identifying potential risks, assessing the impact and likelihood of those risks,
    and assigning appropriate mitigation strategies to responsible parties.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider conducting workshops with stakeholders to identify possible risks.
    You might also want to use established AI risk frameworks, such as the IEEE AI
    Ethics Guidelines. Questions to ask include:'
  prefs: []
  type: TYPE_NORMAL
- en: What could go wrong with the data (e.g., bias, privacy breaches)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What operational risks could occur (e.g., concept drift, service downtime, poor
    model performance)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What regulatory risks are relevant (e.g., GDPR or EU AI Act noncompliance)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Failure Mode and Effects Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As part of a proactive risk management strategy, you should conduct *Failure
    Mode and Effects Analysis* (FMEA) tailored to AI systems. This systematic process
    used to identify, assess, and mitigate risks associated with potential failure
    points in a system is especially valuable in safety-critical environments. It
    enables teams to anticipate and prioritize risks before they impact users or system
    performance. Incorporating FMEA into your continuous delivery pipelines and model
    monitoring workflows during the later stages of CRISP-ML(Q) supports ongoing compliance
    and minimizes operational risk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conducting FMEA for AI systems involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Identify potential failure modes.
  prefs: []
  type: TYPE_NORMAL
- en: Consider AI-specific failures such as data quality issues (e.g., missing values,
    outliers), model underfitting or overfitting, concept drift or data drift, adversarial
    attacks, bias in predictions, explainability failures, performance degradation
    over time, resource constraints (e.g., memory, computation time), and integration
    failures with other systems.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Determine potential effects.
  prefs: []
  type: TYPE_NORMAL
- en: For each failure mode, identify potential consequences, such as incorrect predictions
    leading to poor decision making, biased outcomes affecting specific groups, privacy
    breaches, financial losses, reputational damage, and regulatory noncompliance.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Identify potential causes.
  prefs: []
  type: TYPE_NORMAL
- en: For each failure mode, determine possible root causes, such as insufficient
    or biased training data, poor feature selection, suboptimal hyperparameter tuning,
    inadequate model architecture, errors in data preprocessing, use of personal data
    without appropriate consent, and changes in the deployment environment.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Assess current controls.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate existing measures to prevent or detect each failure mode. These might
    include data validation checks, model performance monitoring, automated testing
    procedures, human-in-the-loop oversight, and explainability techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Create a template to use for FMEA. [Table 5-1](#chapter_5_table_1_1748539922529349)
    shows an example of what this might look like and the kind of information it might
    contain.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-1\. Example FMEA specific to an AI system for proactive risk management
    in alignment with Article 9 of the EU AI Act
  prefs: []
  type: TYPE_NORMAL
- en: '| Failure mode | Failure cause | Failure effect | S | O | D | RPN | Mitigation
    plan | Owner |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data bias | Poor data sampling | Biased predictions | 9 | 8 | 5 | 360 | Use
    fairness audits | Data team |'
  prefs: []
  type: TYPE_TB
- en: '| Model overfitting | Model complexity | Poor generalization in production
    | 7 | 6 | 7 | 294 | Conduct regular retraining | ML team |'
  prefs: []
  type: TYPE_TB
- en: '| Model drift | Evolving user behavior | Decreasing prediction quality | 8
    | 7 | 6 | 336 | Monitor metrics | MLOps team |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s walk through each of the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: The *failure modes* are the way in which the ML models that are part of the
    AI system could fail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *failure causes* are the root causes that might lead to the failures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The*failure effects* are the consequences of the failures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The*severity* (*S*) can be assigned on a scale from 1 to 10, where 1 = minimal
    impact and 10 = catastrophic failure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *frequency of occurrence* (*O*) can be assigned on a scale from 1 to 10,
    where 1 = unlikely and 10 = very frequent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The*detection rate* (*D*) can be assigned on a scale from 1 to 10, where 1 =
    easily detectable and 10 = hard to detect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *risk priority number* (*RPN*) is equal to*S * O * D*; a higher RPN means
    a higher priority for mitigation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *mitigation plan* is the strategy and concrete steps to mitigate the identified
    risk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *owner* is the team accountable for mitigating the identified risk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use tools like Jira, Confluence, Google Sheets, or Excel for tracking
    FMEA.
  prefs: []
  type: TYPE_NORMAL
- en: Testability and value alignment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to risk identification and mitigation, it’s essential to establish
    clear, measurable objectives and key performance indicators (KPIs) that the AI
    system must meet. This includes creating a comprehensive plan outlining how each
    component and the system as a whole will be tested.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to clearly outline the ethical guidelines and principles
    that the AI system must adhere to, such as fairness and transparency. One practical
    tool that you can use for this is the [Values Canvas](https://oreil.ly/c79Xu),
    a template for developing ethical AI strategies and documenting existing ethics
    efforts.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to involve internal and external stakeholders in these processes, to
    understand their values, expectations, and needs. Internal stakeholders may include
    data scientists, MLOps engineers, legal teams, product managers, and executives,
    while external stakeholders can include regulators, customers, partners, and auditors.
    To categorize and prioritize stakeholders based on their level of influence and
    interest in the AI system, use the [Power–Interest Grid](https://oreil.ly/CGtNw).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When conducting risk identification, it’s vital to assess the quality of the
    data being used. This includes identifying risks related to data quality, such
    as missing values, inconsistencies, and potential biases. Typical MLOps practices
    involve automating data validation and profiling using frameworks like Great Expectations
    or Deequ that can automatically detect issues like these.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality pipelines should also incorporate unit tests to monitor drift and
    data integrity in real time. Make sure to write unit tests for the data extraction,
    transformation, and loading (ETL) processes in the data pipelines to ensure any
    issues with data handling are caught early. To ensure effective testability, implement
    scripts to automatically validate the integrity and quality of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding value alignment, when curating data, it’s important to ensure that
    the dataset represents the necessary diversity to prevent biased outcomes. MLOps
    practices should integrate ethical checks, such as ensuring that no discriminatory
    proxies are used in feature engineering. To implement ethical checks, integrate
    tools like SHAP, Fairlearn, Great Expectations, and EvidentlyAI into your MLOps
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the data pipeline might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quality checks (Great Expectations)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Correlation analysis for proxy detection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, privacy preservation techniques such as data minimization or anonymization
    via advanced techniques like differential privacy should be applied to align with
    ethical standards.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Risk identification in the modeling phase involves evaluating the risks associated
    with different modeling approaches, such as overfitting and underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: To incorporate testability into your model code, consider using automated testing
    frameworks such as pytest for unit tests. Implementing cross-validation techniques
    can help ensure that your model generalizes well across different datasets and
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to value alignment, it’s important to select algorithms that align
    with ethical considerations—for example, you should use interpretable models for
    high-stakes decisions. Additionally, metrics should be incorporated into the model
    to assess and address biases in model predictions. Collaborate with stakeholders
    to determine the values you want to uphold (e.g., fairness, robustness, interpretability)
    and define specific metrics, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Fairness
  prefs: []
  type: TYPE_NORMAL
- en: Demographic parity, equal opportunity
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  prefs: []
  type: TYPE_NORMAL
- en: Tolerance to noisy inputs, resilience to adversarial examples
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrating relevant tools into the pipeline can help ensure continuous alignment
    with ethical values throughout the product lifecycle and compliance with the requirements
    of the EU AI Act. Here are some recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'MLOps tools: MLflow, Airflow, DVC, Kubeflow, TensorFlow Extended (TFX)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fairness tools: Fairlearn, AI Fairness 360'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monitoring tools: EvidentlyAI for drift detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let’s consider how the quality attributes that Article 9 maps to can be
    implemented in the evaluation phase of the CRISP-ML(Q) model.
  prefs: []
  type: TYPE_NORMAL
- en: Risk identification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Performing model interpretability analysis during the evaluation phase helps
    identify unexpected or undesirable model behaviors. You can use tools like SHAP
    (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations),
    or InterpretML to generate explanations.
  prefs: []
  type: TYPE_NORMAL
- en: When analyzing the results, look for features that are unexpectedly high in
    importance and counterintuitive feature interactions. Check for proxy variables
    that might inadvertently introduce bias. Document your findings in a structured
    report detailing unexpected behaviors and their potential impacts, hypotheses
    for the causes of these behaviors, and proposed mitigations. Also include key
    features influencing the model in this report.
  prefs: []
  type: TYPE_NORMAL
- en: To further uncover risks and ensure testability, conduct adversarial testing
    using libraries such as the Adversarial Robustness Toolbox (ART), PyRIT, or CleverHans.
    Tailor your approach to the model type, deployment mode, and model exposure strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Relevant types of adversarial attacks might include:'
  prefs: []
  type: TYPE_NORMAL
- en: Evasion attacks (e.g., Fast Gradient Sign Method, Carlini & Wagner)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poisoning attacks (e.g., label flipping, backdoor insertion)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model extraction attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document your findings regarding the model’s vulnerability to different types
    of attacks and classify them based on severity and likelihood. You can use this
    not only for recordkeeping but as a “thinking tool” to develop mitigation strategies
    such as adversarial training or input preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Testability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'AI systems are always embedded in larger software systems, which are often
    legacy systems with complex interdependencies. Therefore, comprehensive test suites
    are needed to ensure that the AI components integrate reliably and function as
    expected. These should include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unit tests* to validate individual components of the ML pipeline (e.g., data
    preprocessing, feature engineering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Integration tests* to ensure that the AI system component interacts correctly
    with other parts of the system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*System tests* to evaluate the AI system in a production-like environment (including
    performance testing, load testing, and error handling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A/B testing* in controlled environments to validate performance, demonstrate
    the business impact of changes to the model, enable incremental rollouts, and
    detect issues early'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a compliance perspective, A/B testing can help verify that a new model
    maintains or improves fairness metrics across different user groups in a real-world
    setting. It aids in demonstrating due diligence in model deployment, which is
    crucial for regulatory compliance under the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: Value alignment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Be sure to evaluate model performance against ethical guidelines defined earlier,
    such as fairness, transparency, privacy, and accountability, by using libraries
    like Fairlearn or AI Fairness 360 to implement fairness metrics.
  prefs: []
  type: TYPE_NORMAL
- en: This comprehensive evaluation helps ensure compliance with Article 9 and promotes
    the development of responsible and trustworthy AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next stage is to deploy your AI system in production. Let’s look at deployment
    through the lens of the three quality attributes that are relevant to Article
    9.
  prefs: []
  type: TYPE_NORMAL
- en: Risk identification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You’ll first want to implement vulnerability scanning to identify security
    risks. The [OWASP Machine Learning Security Top 10](https://oreil.ly/Ks7Jt) is
    a good reference for understanding common security issues, vulnerabilities, and
    risks associated with machine learning systems. You might also use tools like
    OWASP ZAP or Nessus to scan for common vulnerabilities such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Input manipulation attacks (ML01:2023)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data poisoning attacks (ML02:2023)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model inversion attacks (ML03:2023)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Membership inference attacks (ML04:2023)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model theft (ML05:2023)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI supply chain attacks (ML06:2023)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning attacks (ML07:2023)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model skewing (ML08:2023)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output integrity attacks (ML09:2023)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model poisoning (ML10:2023)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, the following AI incident and risk trackers can help you stay
    informed about the current developments and keep your AI security strategy up-to-date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[AI Risk Repository](https://oreil.ly/4vrn_)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI Incident Database](https://oreil.ly/8dyrm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OECD AI Incidents and Hazards Monitor](https://oreil.ly/tVVZ1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important pre-deployment activity is manual or automated penetration
    testing. Penetration testing helps identify ML vulnerabilities before malicious
    actors can exploit them. It’s essential for AI systems handling sensitive data
    or intellectual property. Tools like Metasploit and Burp Suite can be used to
    test various attack vectors. Compliance with the EU AI Act requires demonstrable
    security measures, and regular penetration testing, continuous monitoring, and
    incident response planning should be part of a comprehensive security strategy
    for AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Perform a security-focused code review of the deployment scripts and infrastructure-as-code,
    and assess operational risks in the production environment. You can evaluate the
    reliability and scalability of your cloud or on-premises infrastructure using
    tools like AWS Trusted Advisor or Azure Advisor. To mitigate supply chain risks,
    analyze all dependencies for known vulnerabilities in the [National Vulnerability
    Database](https://oreil.ly/euMDH). Tools like OWASP Dependency-Check and GitHub
    Dependabot are good starting points.
  prefs: []
  type: TYPE_NORMAL
- en: During deployment, you should also plan for disaster recovery. Develop and test
    backup and restore procedures for data, models, and pipeline artifacts, and implement
    and validate failover mechanisms for critical components in the ML system.
  prefs: []
  type: TYPE_NORMAL
- en: Testability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Testability in the deployment phase hinges on AI system observability, including
    implementing continuous monitoring and alerting systems. The initial step is to
    establish key metrics that offer a holistic view of the AI system. Your metrics
    categories should include:'
  prefs: []
  type: TYPE_NORMAL
- en: Model performance metrics (e.g., accuracy, latency)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System health metrics (e.g., CPU usage, memory usage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business metrics (e.g., number of predictions, error rates)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the metrics are defined, set up alerting by defining appropriate thresholds.
    You can use tools like Prometheus with Alertmanager or CloudWatch Alarms to implement
    alerting based on predefined conditions.
  prefs: []
  type: TYPE_NORMAL
- en: You might also want to implement API testing, automated regression testing,
    and data and model validation tests to detect drift. You can use tools like Deepchecks,
    a holistic open source solution for AI and ML validation.
  prefs: []
  type: TYPE_NORMAL
- en: Value alignment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To implement continuous value alignment checks, regularly assess model outputs
    against defined ethical guidelines (examples of ethical metrics are demographic
    parity, equal opportunity, and representation balance). A useful way to identify
    these ethical values is to apply the Foundational Value Finder Framework, as outlined
    in Olivia Gambelin’s book [*Responsible AI*](https://oreil.ly/hmXoe). It unites
    the government, industry, and organizational ethical value sets.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Monitoring and maintenance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To implement the quality attributes associated with Article 9 during the monitoring
    and maintenance phase, several key tasks need to be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Risk identification involves continuously monitoring for data drift and model
    performance degradation using automated tools. Update the AI system’s risk register
    regularly to reflect new insights gathered from operational usage. Risks should
    be categorized across technical, operational, ethical, and regulatory domains.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure testability, periodic audits of model performance, system behavior,
    and fairness metrics are important. Introduce chaos engineering techniques, such
    as simulating data corruption, resource exhaustion, or network failures, to identify
    system vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining value alignment requires regular reviews and updates of ethical
    guidelines and periodic stakeholder reviews to assess alignment between stakeholder
    expectations and system behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 5-2](#chapter_5_table_2_1748539922529399) summarizes the AI engineering
    practices you should implement across the CRISP-ML(Q) phases to support the quality
    attributes associated with Article 9\. Following the steps outlined here will
    help ensure ongoing compliance with the EU AI Act while maintaining system reliability
    and ethical alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-2\. Quality attributes relevant to Article 9 and corresponding AI engineering
    practices and tools
  prefs: []
  type: TYPE_NORMAL
- en: '| CRISP-ML(Q) phase | Quality attributes | AI engineering practices and tools
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Business and data understanding | Risk identification | Stakeholder interviews,
    initial risk assessment, creation of risk register, FMEA |'
  prefs: []
  type: TYPE_TB
- en: '| Value alignment | Ethical impact assessments aligned with project goals |'
  prefs: []
  type: TYPE_TB
- en: '| Data preparation | Risk identification | Data quality assessment, data privacy/security
    risks |'
  prefs: []
  type: TYPE_TB
- en: '| Testability | Data validation tests, versioning/lineage |'
  prefs: []
  type: TYPE_TB
- en: '| Value alignment | Ethical checks, fairness audits |'
  prefs: []
  type: TYPE_TB
- en: '| Modeling | Risk identification | Model vulnerability assessments, failure
    modes |'
  prefs: []
  type: TYPE_TB
- en: '| Testability | Unit tests, integration tests |'
  prefs: []
  type: TYPE_TB
- en: '| Value alignment | Define metrics, ethical guidelines |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation | Risk identification | Model interpretability analysis, adversarial
    testing |'
  prefs: []
  type: TYPE_TB
- en: '| Testability | Comprehensive test suites, A/B testing |'
  prefs: []
  type: TYPE_TB
- en: '| Value alignment | Evaluate against ethical guidelines |'
  prefs: []
  type: TYPE_TB
- en: '| Deployment | Risk identification | Pre-deployment security audits, operational
    risks |'
  prefs: []
  type: TYPE_TB
- en: '| Testability | Continuous monitoring/alerting, regression testing |'
  prefs: []
  type: TYPE_TB
- en: '| Value alignment | Feedback mechanisms |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring and maintenance | Risk identification | Continuous monitoring,
    risk register updates |'
  prefs: []
  type: TYPE_TB
- en: '| Testability | Model/system audits, chaos engineering |'
  prefs: []
  type: TYPE_TB
- en: '| Value alignment | Regular reviews and updates of ethical guidelines, stakeholder
    reviews |'
  prefs: []
  type: TYPE_TB
- en: Checklist for compliance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is a practical checklist for aligning AI engineering practices
    with Article 9 of the EU AI Act across the CRISP-ML(Q) lifecycle. This framework
    supports regulatory compliance by integrating risk management, testability, and
    value alignment throughout each phase of the development process:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-development (not a part of CRISP-ML(Q))
  prefs: []
  type: TYPE_NORMAL
- en: Define risk assessment methodologies and tools to identify critical risks based
    on potential harms (e.g., safety, privacy, bias).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a risk management plan aligned with EU AI Act requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish a centralized risk register and documentation strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business and data understanding
  prefs: []
  type: TYPE_NORMAL
- en: Conduct an initial risk identification workshop with key stakeholders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document potential AI risks in the risk register.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform an ethical impact assessment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Align project goals with organizational values and EU AI Act compliance requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs: []
  type: TYPE_NORMAL
- en: Assess data quality, completeness, and representativeness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure datasets are free from discriminatory bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify and document data-related risks (e.g., bias, privacy, drift).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement data versioning and lineage tracking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design and implement data validation tests, incorporating quality checks for
    completeness and correctness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling
  prefs: []
  type: TYPE_NORMAL
- en: Conduct model vulnerability assessments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate the model architecture for explainability and robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement version control for model code and artifacts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design and implement unit tests for model components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document model architecture and design decisions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement integration tests for the full ML pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulate edge cases and stress-test model behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Execute comprehensive test suites (unit, integration, system).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform adversarial testing and document results (vulnerabilities, mitigation
    strategies).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate performance with regard to business and ethical goals and the intended
    purpose of the system; include value alignment metrics such as fairness, explainability,
    robustness, safety, and accountability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the risk register based on evaluation results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct A/B testing in controlled environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engage domain experts to validate evaluation metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment
  prefs: []
  type: TYPE_NORMAL
- en: Conduct a pre-deployment security audit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a continuous monitoring and alerting system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up automated regression testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish feedback mechanisms for end users and stakeholders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that risk mitigation measures (e.g., rollback mechanisms) are operational.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate quality gate checks into the deployment pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and maintenance
  prefs: []
  type: TYPE_NORMAL
- en: Implement automated monitoring for data drift, concept drift, and model degradation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define thresholds and set up alerting for critical metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct regular internal audits of model and system performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform periodic chaos engineering tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularly update the risk register based on operational insights and regulatory
    guidance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schedule periodic reviews of ethical guidelines and value alignment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As further reading, I recommend the [NIST AI Risk Management Framework](https://oreil.ly/ht2NM)
    (AI RMF). This is important in the context of Article 9 because it offers a lifecycle-based
    approach to identifying, assessing, and mitigating AI risks, in line with the
    EU AI Act’s requirements for continuous risk management in high-risk AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article 10: Data and Data Governance'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you may recall from [Chapter 3](ch03.html#chapter_3_data_and_ai_governance_and_ai_engineering_1748539918115723),
    at its core, data governance ensures high data quality throughout the machine
    learning lifecycle, supports business objectives, and enables organization-wide
    data use by focusing on availability, usability, consistency, integrity, security,
    and compliance. That chapter also outlined data governance processes for each
    stage of the CRISP-ML(Q) framework, providing a practical approach to managing
    data responsibly across the development lifecycle that supports the creation of
    AI systems that are trustworthy, compliant, and sustainable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Article 10 of the EU AI Act provides only a high-level explanation of
    the requirements regarding data and data governance, let’s break down the quality
    attributes that support a comprehensive framework for implementing trustworthy
    AI through MLOps practices. In “Navigating the EU AI Act,” Kelly et al. map Article
    10 to 13 quality attributes  for high-risk AI systems. These are:'
  prefs: []
  type: TYPE_NORMAL
- en: Independence
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which datasets used for training, validation, and testing are
    isolated and unaffected by each other, preventing data leakage across sets.
  prefs: []
  type: TYPE_NORMAL
- en: Data completeness
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which the dataset contains all necessary data points and variables
    to support accurate predictions, covering the full range of use cases and features
    relevant to the AI system’s purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Currentness
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which the data is up-to-date and temporally relevant for its intended
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: Data fairness
  prefs: []
  type: TYPE_NORMAL
- en: The absence of bias, prejudice, or favoritism toward individuals or groups in
    the data, particularly in relation to protected characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs: []
  type: TYPE_NORMAL
- en: The level of exactness or granularity in the data measurements and representations.
  prefs: []
  type: TYPE_NORMAL
- en: Representativeness
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which the data and its distribution reflects the characteristics
    of the real-world population or scenario it is intended to model.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency
  prefs: []
  type: TYPE_NORMAL
- en: The uniformity of data (the degree to which it maintains its format, structure,
    and values) across the entire dataset and over time.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs: []
  type: TYPE_NORMAL
- en: The closeness of data values to their true or accepted real-world values.
  prefs: []
  type: TYPE_NORMAL
- en: Credibility
  prefs: []
  type: TYPE_NORMAL
- en: The trustworthiness of the data sources and collection methods.
  prefs: []
  type: TYPE_NORMAL
- en: Temporality
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which the temporal characteristics of the data, including timeliness,
    aging, versioning, and lifecycle management, ensure its ongoing relevance and
    appropriateness. This is important for understanding how data changes over time
    and for making decisions based on current information.
  prefs: []
  type: TYPE_NORMAL
- en: Confidentiality
  prefs: []
  type: TYPE_NORMAL
- en: The protection of sensitive information in accordance with privacy requirements
    and data protection regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Compliance
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which data practices conform to applicable legal, ethical, and
    organizational standards and regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Data traceability
  prefs: []
  type: TYPE_NORMAL
- en: The ability to track data from its origin through its entire lifecycle, including
    all transformations, access points, and uses, supporting transparency, accountability,
    and auditability.
  prefs: []
  type: TYPE_NORMAL
- en: The quality attributes associated with Article 10 are distributed throughout
    the CRISP-ML(Q) lifecycle. [Table 5-3](#chapter_5_table_3_1748539922529427) maps
    these attributes to each development phase and highlights practical AI engineering
    practices for implementing them.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-3\. Quality attributes relevant to Article 10 and corresponding AI engineering
    practices and tools
  prefs: []
  type: TYPE_NORMAL
- en: '| CRISP-ML(Q) phase | Quality attributes | AI engineering practices and tools
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Business and data understanding | Independence | Establish clear data governance
    policies that outline data acquisition procedures and criteria to ensure data
    independence. |'
  prefs: []
  type: TYPE_TB
- en: '| Data completeness | Perform thorough data exploration and profiling to identify
    missing data points, outliers, or insufficient representation and define strategies
    for addressing them. |'
  prefs: []
  type: TYPE_TB
- en: '| Currentness | Document data update frequencies and establish refresh procedures
    to maintain currentness. |'
  prefs: []
  type: TYPE_TB
- en: '| Data fairness | Analyze data for potential biases using fairness metrics
    and visualization tools; document and mitigate identified biases. |'
  prefs: []
  type: TYPE_TB
- en: '| Data traceability | Implement data versioning and logging practices from
    the outset to enable traceability of data provenance, transformations, and access
    across the lifecycle. |'
  prefs: []
  type: TYPE_TB
- en: '| Data preparation | Precision | Define and enforce data quality rules for
    input data, including data types, ranges, and formats, and use data validation
    tools to ensure data precision. |'
  prefs: []
  type: TYPE_TB
- en: '| Consistency | Apply data cleaning techniques to address inconsistencies,
    such as duplicate entries and conflicting data formats. |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | Implement data validation checks to identify and correct inaccuracies
    and errors in the data. |'
  prefs: []
  type: TYPE_TB
- en: '| Data traceability | Continue documenting data manipulation steps, including
    transformations, feature engineering and data cleaning procedures, to maintain
    traceability throughout the pipeline. |'
  prefs: []
  type: TYPE_TB
- en: '| Modeling | Representativeness | Ensure the training data represents the target
    population or scenario accurately, using sampling techniques to address imbalances
    and improve representativeness. |'
  prefs: []
  type: TYPE_TB
- en: '| Data traceability | Log and version model training parameters, hyperparameters,
    datasets, and code to support full traceability of the development process. |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation | Accuracy | Evaluate model performance using domain-relevant
    metrics on a strictly independent hold-out test dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| Robustness | Conduct robustness testing by introducing variations in input
    data and assessing model performance under different conditions. |'
  prefs: []
  type: TYPE_TB
- en: '| Deployment | Confidentiality | Implement security measures such as data encryption,
    authentication, and access control to protect sensitive data during model deployment
    and prediction serving. |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring and maintenance | Currentness | Monitor incoming data for changes
    in distribution or characteristics that might signal data staleness or concept
    drift, and define thresholds and triggers for automated model retraining. |'
  prefs: []
  type: TYPE_TB
- en: '| Compliance | Regularly audit model performance and data usage to verify compliance
    with relevant regulations and ethical guidelines. |'
  prefs: []
  type: TYPE_TB
- en: Key tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to ensuring data quality, several key tools and technologies stand
    out. Among them are Great Expectations, Apache Griffin, Deequ, and TensorFlow
    Data Validation (TFDV), all of which play crucial roles in maintaining data integrity
    and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of monitoring data and machine learning processes, tools like Prometheus,
    Grafana, MLflow, and Weights & Biases are invaluable. They help track performance
    and visualize trends effectively.
  prefs: []
  type: TYPE_NORMAL
- en: For compliance purposes, OpenLineage, Atlas, Collibra, and DataHub are go-to
    tools that assist organizations in managing and safeguarding their data assets,
    ensuring they adhere to industry regulations and standards.
  prefs: []
  type: TYPE_NORMAL
- en: Checklist for compliance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following checklist can be used to ensure compliance with Article 10 throughout
    the CRISP-ML(Q) lifecycle. The key is to implement these practices systematically,
    maintain thorough documentation in all phases, and view compliance engineering
    as an ongoing process:'
  prefs: []
  type: TYPE_NORMAL
- en: Business and data understanding
  prefs: []
  type: TYPE_NORMAL
- en: Establish a data governance framework aligned with EU AI Act requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Organizational) Define roles and responsibilities for data management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define and document data quality metrics and standards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain documentation of all data sources, including assessments of their credibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify and record data licensing terms and usage rights for all datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess data protection requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate data source reliability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profile existing datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document known data quality issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess data completeness requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate data representativeness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check for potential biases in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs: []
  type: TYPE_NORMAL
- en: Specify data validation requirements (e.g., schema validation, data type checks,
    range checks, functional dependency validation, consistency checks).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement data quality checks (e.g., completeness, accuracy, consistency).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up data validation pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement data cleaning procedures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement data lineage tracking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate processed data quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document transformation rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate transformed data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement data anonymization where required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up data versioning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement privacy-preserving measures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up secure data storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement access controls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document security measures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up audit logging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling
  prefs: []
  type: TYPE_NORMAL
- en: Validate training data quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document the data splitting methodology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a cross-validation strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track data versions used for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track model versions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track model lineage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor data drift.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement reproducibility controls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track experiment results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement model validation procedures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test model robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate model fairness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess model fairness and check for biases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate model robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct a risk assessment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate security measures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment
  prefs: []
  type: TYPE_NORMAL
- en: Establish service level agreements for data quality attributes such as completeness,
    accuracy, timeliness, and consistency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate quality gates into the deployment pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure automated alerting mechanisms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop a structured incident response plan.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable continuous monitoring of model performance in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design and test rollback procedures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and maintenance
  prefs: []
  type: TYPE_NORMAL
- en: Continuously monitor data quality metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track model performance indicators such as processing or inference time and
    error rates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularly assess fairness metrics across user groups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor system health (e.g., uptime, resource utilization).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track business-critical metrics tied to revenue, customer experience, and compliance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement structured update procedures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systematically log all changes and updates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain an internal audit trail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we’ll delve further into the requirements for technical documentation
    and the importance of keeping logs and records that document the system’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article 11: Technical Documentation and Article 12: Record-Keeping'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to Kelly et al., Article 11 maps to one key quality attribute: traceability.
    Documenting AI models is a central requirement of the EU AI Act, particularly
    as a means of assessing legal compliance. Both technical and quality management
    system documentation are critical components in the conformity assessment process.'
  prefs: []
  type: TYPE_NORMAL
- en: For downstream providers incorporating a preexisting AI model into an AI system,
    a comprehensive understanding of the model and its functionalities is essential,
    both to facilitate the integration into their offerings and to ensure they fulfill
    their responsibilities under the EU AI Act and other applicable regulations ([Recital
    101](https://oreil.ly/kB5Pj)). As such, the creator of the AI model is required
    to provide comprehensive technical documentation to downstream providers ([Article
    53(1b)](https://oreil.ly/Xn8iw)).
  prefs: []
  type: TYPE_NORMAL
- en: MLOps and documentation are closely related, as documentation plays a critical
    role in the lifecycle of machine learning (ML) projects and is essential for operationalizing
    models effectively. For example, documentation captures details about data preprocessing,
    feature engineering, model configurations, hyperparameters, and environment settings,
    enabling others (or the same team at a later time) to reproduce experiments and
    results reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, MLOps involves tracking all stages of the ML workflow—data preparation,
    model training, testing, deployment, and monitoring. Documenting each stage, including
    data sources, code versions, and model changes, supports traceability and accountability,
    which are vital for auditability, especially in regulated sectors. More importantly,
    robust documentation is essential for meeting regulatory standards, including
    the data protection and transparency requirements outlined in the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: What’s more, according to DORA’s [2023 Accelerate State of DevOps Report](https://oreil.ly/sjTX3),
    high-quality documentation has been linked to a 25% increase in team performance.
  prefs: []
  type: TYPE_NORMAL
- en: Technical documentation requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Article 11 outlines the requirements for the technical documentation of high-risk
    AI systems, although related obligations are found throughout Articles 8–15.^([1](ch05.html#id564))
    These requirements apply to both datasets and AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technical documentation serves two primary purposes: providing instructions
    for users of the system and demonstrating regulatory compliance to authorities.
    As such, the intended audience spans technical teams, compliance teams (AI auditors
    and conformity assessment bodies), and end users. [Table 5-4](#chapter_5_table_4_1748539922529451)
    outlines the relevant technical information elements under the EU AI Act. It provides
    an overview of key documentation types, their intended audiences, and the essential
    details each should contain.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-4\. Overview of technical documentation requirements for datasets and
    AI systems
  prefs: []
  type: TYPE_NORMAL
- en: '| Document type | Target audience | Description | Key information elements
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data source documentation | Compliance teams | Documentation of data origins
    and collection methods | Sources, collection methods, and aggregation approaches
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset characteristics | All stakeholders | Documentation of dataset contents,
    coverage, and known limitations | Data types, scope, and limitations |'
  prefs: []
  type: TYPE_TB
- en: '| Data processing documentation | Technical teams | Documentation of data handling
    procedures | Processing steps, quality controls, and preparation methods |'
  prefs: []
  type: TYPE_TB
- en: '| Data quality metrics | Compliance teams | Documentation of quality assessment
    procedures | Quality metrics, validation results, and completeness checks |'
  prefs: []
  type: TYPE_TB
- en: '| Population coverage analysis | All stakeholders | Documentation of dataset
    representativeness | Coverage metrics, demographic analysis, and usage context
    |'
  prefs: []
  type: TYPE_TB
- en: '| Privacy protection documentation | Compliance teams | Documentation of privacy
    safeguards | Privacy measures, data protection controls |'
  prefs: []
  type: TYPE_TB
- en: '| System purpose and scope | All stakeholders | Documentation of intended use
    and limitations | Use cases, constraints, and misuse prevention measures |'
  prefs: []
  type: TYPE_TB
- en: '| Risk assessment documentation | All stakeholders | Documentation of risk
    analysis and controls | Risk assessment, mitigation measures |'
  prefs: []
  type: TYPE_TB
- en: '| System operation documentation | All stakeholders | Documentation of system
    behavior and oversight | Operation procedures, human oversight measures |'
  prefs: []
  type: TYPE_TB
- en: '| Technical architecture documentation | Technical teams | Documentation of
    system design and implementation | Architecture, components, development process
    |'
  prefs: []
  type: TYPE_TB
- en: '| Performance documentation | All stakeholders | Documentation of system capabilities
    and limitations | Accuracy metrics, robustness measures, test results |'
  prefs: []
  type: TYPE_TB
- en: '| Security documentation | All stakeholders | Documentation of security measures
    | Security controls, vulnerability management |'
  prefs: []
  type: TYPE_TB
- en: '| Maintenance documentation | Technical teams | Documentation of system lifecycle
    management | Change procedures, version control, updates |'
  prefs: []
  type: TYPE_TB
- en: Technical documentation for high-risk AI systems is a legal requirement and
    must be created prior to placing the systems on the market or putting them into
    service. It is essential to keep this documentation updated, as it serves to demonstrate
    compliance to regulatory authorities. Furthermore, it plays a crucial role in
    conformity assessment procedures, ensuring that all requirements are met. The
    documentation must be detailed enough to show compliance with all applicable regulations.
    Ultimately, it serves the needs of both users and regulatory authorities, providing
    transparency and assurance in the use of high-risk AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Managing documentation debt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Documentation debt* refers to the accumulated costs and risks associated with
    incomplete, outdated, inconsistent, or missing documentation across the dataset,
    data processing, and AI system lifecycle. Organizations can minimize this debt
    by automating documentation generation as part of ML pipeline execution, taking
    care to capture the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: Data quality metrics and validation results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data lineage and transformation steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature definitions and characteristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline metadata, including execution parameters, data statistics and profiling,
    validation results, transformed datasets, evaluation metrics, and training logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model versions, hyperparameters, and training outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another essential aspect of operationalizing documentation is applying version
    control to key documentation assets such as feature definitions and schemas, model
    documentation (such as model cards), ML pipeline configurations and parameters,
    training datasets, feature stores, data transformations, and preprocessing steps.
    Documentation versions should be tracked alongside code versions using Git-based
    version control systems, artifact repositories, model registries, and feature
    stores with versioning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: By automating these processes and utilizing appropriate tools and platforms,
    organizations can establish a robust documentation system that is integrated into
    their MLOps practices. This reduces manual effort, improves consistency, and helps
    ensure the AI system’s ongoing compliance with the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: Existing frameworks for documenting data and AI systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Documentation has long been a challenge in software engineering, and several
    mature approaches have emerged to address it. [Table 5-5](#chapter_5_table_5_1748539922529474)
    outlines state-of-the-art documentation tools and methods for data and AI systems,
    highlighting widely adopted practices.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-5\. Popular documentation approaches for data and AI systems
  prefs: []
  type: TYPE_NORMAL
- en: '| Approach | Origin | Format | Purpose | Coverage |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Datasheets for datasets | Black in AI, Microsoft, and academia (2018) | Questionnaire-based
    documentation | Documents datasets, including collection procedures, intended
    uses, content, distribution, and maintenance | Strong on data provenance, scope,
    representation, and privacy considerations. |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset Nutrition Label | Harvard and MIT (2018, updated 2022) | Visual template
    with standardized fields | Provides standardized dataset information to drive
    higher data quality | Good for data quality, fairness, and completeness checks.
    |'
  prefs: []
  type: TYPE_TB
- en: '| AI Factsheets | IBM (2019) | Customizable questionnaire templates for different
    stakeholders | Documents AI systems comprehensively through their lifecycle |
    Most comprehensive overall coverage of AI system documentation needs. Covers technical,
    performance, risk, and operational aspects. |'
  prefs: []
  type: TYPE_TB
- en: '| Model cards | Google (2019) | Standardized information sheet | Documents
    model specifications and intended uses | Suitable for model performance, limitations,
    and use cases.Focus: Model-specific documentation and transparency. |'
  prefs: []
  type: TYPE_TB
- en: '| OECD AI Classification Framework | OECD (2022) | Structured questionnaire
    | Standardized AI system classification and documentation | Comprehensive AI system
    classification framework. Institutional backing: International standard-setting
    organization. |'
  prefs: []
  type: TYPE_TB
- en: '| Use case cards | European Commission, Joint Research Centre (2024) | Information
    sheet | Documents intended use cases of AI systems | Provides a high-level overview
    of use cases without technical details.Focus: Risk assessment under the EU AI
    Act. |'
  prefs: []
  type: TYPE_TB
- en: '| AI Cards framework | European Commission, Joint Research Centre (2024) |
    Dual representation: visual human-readable and machine-readable specifications
    | Provides holistic documentation of AI systems and their risks | Covers technical
    specifications, context of use, risk management, and compliance. Recent development
    specifically for EU AI Act compliance. |'
  prefs: []
  type: TYPE_TB
- en: When selecting a documentation framework, it is essential to choose one that
    aligns with your organization’s specific needs and regulatory obligations. In
    practice, the most effective approach often involves combining multiple complementary
    documentation methods to ensure thorough coverage while maintaining usability.
    AI Factsheets and AI Cards currently offer the most comprehensive support, but
    organizations may benefit from supplementing them with more specialized tools,
    such as datasheets for datasets and model cards. Taking a “layered documentation”
    approach is a useful strategy, where the AI Cards framework is used for high-level
    system documentation, component-specific details are captured through datasheets
    and model cards, and greater technical depth is provided by using AI Factsheets.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing machine-readable specifications that integrate with existing metadata
    management systems is crucial. This enables automated documentation generation,
    streamlining the process and improving consistency and efficiency. Finally, it’s
    important to adapt the level of detail in documentation to suit different audiences
    (technical teams, compliance teams, and end users).
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s turn to the recordkeeping requirements outlined in Article 12, which
    closely relate to the technical documentation obligations in Article 11.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article 12: Recordkeeping requirements for high-risk AI systems'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In “Navigating the EU AI Act,” Kelly et al. map Article 12 to eight quality
    attributes for high-risk AI systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Operability
  prefs: []
  type: TYPE_NORMAL
- en: The ease with which an AI system can be operated, managed, and maintained effectively,
    including usability, reliability, and the ability to perform functions under stated
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Non-repudiation
  prefs: []
  type: TYPE_NORMAL
- en: The ability to ensure that actions or events within an AI system cannot be denied
    after they have occurred, preventing any party from falsely denying responsibility.
  prefs: []
  type: TYPE_NORMAL
- en: Traceability
  prefs: []
  type: TYPE_NORMAL
- en: The ability to track and document every aspect of the AI system’s data and decision-making
    processes, including data sources, transformations, model training, and predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Self-descriptiveness
  prefs: []
  type: TYPE_NORMAL
- en: The AI system’s capacity to explain its structure, functionalities, and behaviors
    in understandable terms, including comprehensive documentation and the use of
    explainable AI techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Accountability
  prefs: []
  type: TYPE_NORMAL
- en: The obligation of individuals or organizations to accept responsibility for
    the AI system’s actions and outcomes, involving clear role assignments and ensuring
    accountability for errors or negative impacts.
  prefs: []
  type: TYPE_NORMAL
- en: Self-monitoring
  prefs: []
  type: TYPE_NORMAL
- en: The AI system’s ability to autonomously observe and assess its performance and
    behavior, detecting anomalies, errors, or deviations from expected operations
    without external prompts.
  prefs: []
  type: TYPE_NORMAL
- en: User engagement
  prefs: []
  type: TYPE_NORMAL
- en: The active participation of users with the AI system, involving designing systems
    that encourage user interaction, feedback, and collaboration.
  prefs: []
  type: TYPE_NORMAL
- en: Monitorability
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which the AI system’s operations and performance can be observed
    and measured in real time or retrospectively, involving tools and processes to
    track system behavior, performance metrics, and compliance.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5-6](#chapter_5_table_6_1748539922529501) maps these quality attributes
    to the CRISP-ML(Q) phases with specific AI engineering processes and tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-6\. Quality attributes relevant to Article 12 and corresponding AI engineering
    practices and tools
  prefs: []
  type: TYPE_NORMAL
- en: '| CRISP-ML(Q) phase | Quality attributes | AI engineering practices and tools
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Business and data understanding | Accountability |'
  prefs: []
  type: TYPE_TB
- en: Defined roles and responsibilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Established governance frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| User engagement |'
  prefs: []
  type: TYPE_TB
- en: Stakeholder involvement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedback mechanisms (surveys, focus groups)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Self-descriptiveness |'
  prefs: []
  type: TYPE_TB
- en: Comprehensive documentation (data catalog)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Traceability |'
  prefs: []
  type: TYPE_TB
- en: Metadata recording (data sources, collection methods)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version control for documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data preparation | Traceability |'
  prefs: []
  type: TYPE_TB
- en: Data versioning (DVC, LakeFS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data lineage tracking (Apache Atlas, OpenLineage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Accountability |'
  prefs: []
  type: TYPE_TB
- en: Assigned data stewardship (escalation paths)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audit trails (Apache Ranger)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Operability |'
  prefs: []
  type: TYPE_TB
- en: Automated data pipelines (Airflow, Luigi, Prefect)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data validation (Great Expectations, TensorFlow Data Validation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Self-descriptiveness |'
  prefs: []
  type: TYPE_TB
- en: Data documentation (dictionaries, schemas)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Descriptive metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Modeling | Traceability |'
  prefs: []
  type: TYPE_TB
- en: Model versioning (MLflow, DVC, Git LFS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment tracking (Weights & Biases, Neptune.ai, Comet.ml)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model documentation (diagrams, explanations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Self-descriptiveness |'
  prefs: []
  type: TYPE_TB
- en: Explainable AI techniques (SHAP, LIME, Integrated Gradients)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Accountability |'
  prefs: []
  type: TYPE_TB
- en: Assigned modeling responsibilities (peer review)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Governance policies (performance, ethics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Operability |'
  prefs: []
  type: TYPE_TB
- en: Automated training pipelines (Kubeflow pipelines, TFX)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Non-repudiation |'
  prefs: []
  type: TYPE_TB
- en: Immutable logs (append-only databases)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure model artifacts (digital signatures, checksums)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluation | Traceability |'
  prefs: []
  type: TYPE_TB
- en: Evaluation logging (metrics, datasets, conditions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version control (evaluation code, documentation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Accountability |'
  prefs: []
  type: TYPE_TB
- en: Assigned evaluation roles (sign-off procedures)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approval processes (performance, ethics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Non-repudiation |'
  prefs: []
  type: TYPE_TB
- en: Immutable evaluation records (timestamped)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Digital signatures (evaluation reports)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Self-descriptiveness |'
  prefs: []
  type: TYPE_TB
- en: Evaluation documentation (metrics, visualizations, explainable AI)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Operability |'
  prefs: []
  type: TYPE_TB
- en: Automated evaluation pipelines (CI/CD tools)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Deployment | Operability |'
  prefs: []
  type: TYPE_TB
- en: Continuous deployment (Jenkins, GitLab CI/CD, CircleCI)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerization and orchestration (Docker, Kubernetes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Non-repudiation |'
  prefs: []
  type: TYPE_TB
- en: Deployment logs (immutable, access-controlled)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artifact signatures (cryptographic methods)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Traceability |'
  prefs: []
  type: TYPE_TB
- en: Deployment documentation (architecture diagrams, artifact repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Accountability |'
  prefs: []
  type: TYPE_TB
- en: Deployment approvals (change management systems)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Monitorability |'
  prefs: []
  type: TYPE_TB
- en: Monitoring setup (Prometheus, Grafana, Datadog)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging practices (structured logs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Monitoring and maintenance | Self-monitoring |'
  prefs: []
  type: TYPE_TB
- en: Automated alerts (Alertmanager, PagerDuty)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Health checks (APIs, scripts, Kubernetes liveness/readiness probes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Monitorability |'
  prefs: []
  type: TYPE_TB
- en: Centralized monitoring systems (observability practices)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Operability |'
  prefs: []
  type: TYPE_TB
- en: Log aggregation (Elastic Stack, Splunk)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable infrastructure (cloud autoscaling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Accountability |'
  prefs: []
  type: TYPE_TB
- en: Assigned maintenance roles (ticketing systems)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incident response plans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Traceability |'
  prefs: []
  type: TYPE_TB
- en: Maintenance logs (change management logs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| User engagement |'
  prefs: []
  type: TYPE_TB
- en: Feedback collection (support portals, in-app forms)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User support (documentation, FAQs, help centers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Interdependence of Articles 11 and 12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Articles 11 and 12 of the EU AI Act are closely connected and interdependent.
    As you’ve seen, Article 11 outlines comprehensive documentation requirements covering
    overall system design, technical specifications, development processes, and risk
    and quality management measures. Article 12 focuses specifically on the automatic
    recording of events and activities, capturing operational logs, behavioral data,
    and runtime information. In the context of compliance, technical documentation
    provides the framework for what needs to be recorded, while recordkeeping provides
    the operational evidence that the AI system performs in accordance with that documentation.
    [Table 5-7](#chapter_5_table_7_1748539922529522) shows the correspondence between
    technical documentation and runtime recordkeeping elements.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-7\. Correspondence between technical documentation and recordkeeping
    obligations
  prefs: []
  type: TYPE_NORMAL
- en: '| Article 11: Technical Documentation | Article 12: Record-Keeping |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| System specifications | Runtime behavior logs |'
  prefs: []
  type: TYPE_TB
- en: '| Expected behaviors | Actual behaviors |'
  prefs: []
  type: TYPE_TB
- en: '| Risk assessments | Incident logs |'
  prefs: []
  type: TYPE_TB
- en: '| Design decisions | Operational metrics |'
  prefs: []
  type: TYPE_TB
- en: 'Considering the technical requirements of Articles 11 and 12, one aspect that
    becomes apparent is that they share a common technical component: the metadata
    store.'
  prefs: []
  type: TYPE_NORMAL
- en: Data and AI system metadata
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Capturing and managing metadata is fundamental to meeting the documentation
    and recordkeeping obligations under Articles 11 and 12 of the EU AI Act. Metadata
    provides the foundation for systematic, auditable, and maintainable documentation
    by enabling traceability, demonstrating compliance, supporting collaboration,
    and ensuring documentation quality over time.
  prefs: []
  type: TYPE_NORMAL
- en: A robust metadata management system is key to maintaining the level of oversight,
    quality, and governance required for compliance, particularly as AI systems evolve
    and scale over time. [Table 5-8](#chapter_5_table_8_1748539922529568) outlines
    the types of data and AI system metadata required for each of the documentation
    types listed in [Table 5-4](#chapter_5_table_4_1748539922529451).
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-8\. Essential metadata for meeting compliance demands for Articles 9–15
    of the EU AI Act
  prefs: []
  type: TYPE_NORMAL
- en: '| Document type | Data and AI system metadata |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data source documentation | Source identification:'
  prefs: []
  type: TYPE_NORMAL
- en: Source name/identifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source type (database, API, sensor data, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source owner/maintainer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source access method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source version/timestamp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data licensing information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terms of use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Data collection methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Collection protocol identifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collection time frame (start/end dates)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collection frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collection tools/software used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collection validation procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling methodology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample size calculations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Aggregation approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merge procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deduplication methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data harmonization steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source reconciliation procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data lineage tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset characteristics | Content description:'
  prefs: []
  type: TYPE_NORMAL
- en: Data types (numerical, categorical, text, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature descriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schema definition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding standards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Coverage information:'
  prefs: []
  type: TYPE_NORMAL
- en: Temporal coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geographical coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demographic coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing data patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Known biases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Known limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Gaps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coverage limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usage restrictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data processing documentation | Preprocessing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outlier handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing value treatment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Quality controls:'
  prefs: []
  type: TYPE_NORMAL
- en: Validation rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data consistency checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrity constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error handling procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exception management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Data quality metrics | Statistical measures:'
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribution analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data completeness rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Validation results:'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance indicators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmark comparisons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Population coverage analysis | Demographic metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Population distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representation ratios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coverage gaps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fairness indicators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Usage context:'
  prefs: []
  type: TYPE_NORMAL
- en: Target population description
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Privacy protection documentation | Privacy measures:'
  prefs: []
  type: TYPE_NORMAL
- en: Anonymization methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pseudonymization techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data masking rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access controls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consent management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deletion requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data retention policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Data protection controls:'
  prefs: []
  type: TYPE_NORMAL
- en: Security protocols
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encryption methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data segregation rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy impact assessments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| System purpose and scope | Definition:'
  prefs: []
  type: TYPE_NORMAL
- en: Primary objectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Target users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case descriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Success criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance targets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: Operational limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regulatory restrictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Misuse prevention:'
  prefs: []
  type: TYPE_NORMAL
- en: Usage restrictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prevention mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detection methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Risk assessment documentation | Risk analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Risk categories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impact assessments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Severity ratings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risk matrices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risk evolution tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Control measures:'
  prefs: []
  type: TYPE_NORMAL
- en: Mitigation strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response plans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recovery procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review cycles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| System operation documentation | Operational procedures:'
  prefs: []
  type: TYPE_NORMAL
- en: Startup procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shutdown procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintenance routines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recovery procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backup protocols
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Human oversight:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervision roles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision authorities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intervention protocols
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Override mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audit procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Technical architecture documentation | Architecture specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: Component diagrams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interface definitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data flows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Technology stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libraries and frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API specifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource specifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Performance documentation | Accuracy metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Robustness measures:'
  prefs: []
  type: TYPE_NORMAL
- en: Stability metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resilience tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge case handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Failure modes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recovery capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Security documentation | Security controls:'
  prefs: []
  type: TYPE_NORMAL
- en: Access controls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authorization rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data protection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Vulnerability management:'
  prefs: []
  type: TYPE_NORMAL
- en: Security assessments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threat models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patch management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incident response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recovery procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audit logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Maintenance documentation | Change management:'
  prefs: []
  type: TYPE_NORMAL
- en: Version control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Release procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update protocols
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rollback procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Lifecycle management:'
  prefs: []
  type: TYPE_NORMAL
- en: Maintenance schedules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecation plans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrade paths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-of-life procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Archive requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: Metadata systems are often overlooked or deprioritized, but they are key components
    of any data and ML platform for operationalizing documentation requirements and
    maintaining compliance with the EU AI Act. A well-designed metadata system provides
    the foundation for systematic documentation management, supports automation and
    quality control, and enables organizations to demonstrate compliance to regulatory
    authorities. To ensure effectiveness, organizations should treat metadata systems
    as integral to their data and AI governance infrastructure and periodically conduct
    internal test audits to verify that key information is easily accessible when
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: Checklist for compliance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following checklist defines a structured process you can follow to implement
    the technical documentation and recordkeeping requirements of Articles 11 and
    12 of the EU AI Act. It uses a compressed version of the CRISP-ML(Q) lifecycle
    to streamline implementation across the business and data understanding, model
    development, and operational phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Business and data understanding
  prefs: []
  type: TYPE_NORMAL
- en: Document intended purpose and use cases (business objectives and constraints,
    expected performance metrics).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish recordkeeping infrastructure (data collection scope and methods, metadata
    tracking systems, logging requirements).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document data sources and specifications (data provenance, quality criteria,
    privacy and security controls, governance procedures).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training and operationalization
  prefs: []
  type: TYPE_NORMAL
- en: Document model development environment (hardware specifications, software dependencies,
    tools and versioning).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document training methodology (model architecture, hyperparameters, algorithms,
    feature engineering steps).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement pipeline logging (infrastructure usage, execution records, resource
    utilization).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement training logs (metadata on training runs, performance metrics, data
    versioning, feature extraction).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model deployment and serving
  prefs: []
  type: TYPE_NORMAL
- en: Document deployment procedures (testing protocols, validation methods, release
    criteria).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document deployment architecture (infrastructure design, scaling, security measures).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement deployment logging (event logs, configuration changes, versioning,
    access control).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document operational procedures (maintenance protocols, update routines, emergency
    response plans).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document serving infrastructure (API specifications, performance benchmarks,
    resource allocations, scaling policies).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement prediction logging (inference logs, latency, errors, user feedback).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document monitoring strategy (metrics tracked, alerting thresholds, response
    procedures, maintenance schedules).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document monitoring system (tools used, automated alerts, performance metrics,
    health checks).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teams should adjust this checklist based on their specific high-risk AI system
    and organizational context, while ensuring all regulatory requirements are addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more in-depth information about the importance of metadata and documentation
    in software development, see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Crafting Docs for Success: An End-to-End Approach to Developer Documentation*](https://oreil.ly/8IY2w)
    by Diana Lakatos (Apress)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Fundamentals of Metadata Management*](https://oreil.ly/W5xiK) by Ole Olesen-Bagneux
    (O’Reilly)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“ML Lineage for Trustworthy Machine Learning Systems”](https://oreil.ly/nlNfU)
    by Mikko Raatikainen et al.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article 13: Transparency and Provision of Information to Deployers and Article
    14: Human Oversight'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Articles 13 and 14 of the EU AI Act are closely connected, as they both deal
    with transparency and human oversight requirements for high-risk AI systems. However,
    they focus on different aspects, as depicted in [Table 5-9](#chapter_5_table_9_1748539922529591).
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-9\. Similarities and differences between Articles 13 and 14
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | Article 13 | Article 14 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Primary focus | Transparency and user information | Human oversight and control
    |'
  prefs: []
  type: TYPE_TB
- en: '| Scope | System capabilities and limitations | Technical design for oversight
    |'
  prefs: []
  type: TYPE_TB
- en: '| Implementation | Documentation and communication | Technical measures and
    procedures |'
  prefs: []
  type: TYPE_TB
- en: '| Key requirements | Transparency about AI system outputs and interpretationsInformation
    about accuracy and performance limitsDocumentation of AI system capabilities |
    Human oversight measures to monitor and control the AI systemDesign for human
    interpretability and understandingDocumentation of oversight procedures and assessments
    |'
  prefs: []
  type: TYPE_TB
- en: '| Relationship to risk management | Focus on transparency about potential risks
    | Focus on human oversight as a risk mitigation measure |'
  prefs: []
  type: TYPE_TB
- en: '| Overall goal | Ensure users understand the system’s capabilities and limitations
    | Enable effective human monitoring and control of high-risk AI systems |'
  prefs: []
  type: TYPE_TB
- en: Article 13 quality attributes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kelly et al. associate Article 13 with to the following quality attributes,
    along with self-descriptiveness:'
  prefs: []
  type: TYPE_NORMAL
- en: User engagement
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which users are actively involved with and informed about the
    AI system, such as being able to opt in/out, challenge outputs, or provide feedback.
  prefs: []
  type: TYPE_NORMAL
- en: User transparency
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which the functionalities, capabilities, and limitations of the
    AI system are understandably communicated to users.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which the reasoning and decision-making processes of the AI system
    can be understood and explained. An interpretable system allows users and stakeholders
    to comprehend how inputs are transformed into outputs, supporting trust and accountability.
  prefs: []
  type: TYPE_NORMAL
- en: Documentability
  prefs: []
  type: TYPE_NORMAL
- en: The quality, completeness, and accessibility of documentation describing the
    AI system design, development, and operation. This includes information on data
    sources, model architectures, training processes, evaluation metrics, and system
    updates, supporting transparency and reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Appropriateness recognizability
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which users can discern that they are interacting with an AI system
    rather than a human and assess whether it is appropriate for a particular context
    or task. This ensures that users can identify the system’s limitations and intended
    use cases, preventing misuse or overreliance.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5-10](#chapter_5_table_10_1748539922529615) maps these attributes to
    the CRISP-ML(Q) phases and outlines core AI engineering practices associated with
    each phase. This structure helps ensure systematic implementation of the requirements
    of Article 13 throughout the entire AI system lifecycle.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-10\. Quality attributes relevant to Article 13 and corresponding AI
    engineering practices and tools
  prefs: []
  type: TYPE_NORMAL
- en: '| CRISP-ML(Q) phase | Quality attributes | AI engineering practices and tools
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Business and data understanding | User engagement |'
  prefs: []
  type: TYPE_TB
- en: Involve users early through interviews, workshops, or cocreation exercises.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Appropriateness recognizability |'
  prefs: []
  type: TYPE_TB
- en: Conduct feasibility studies and risk assessments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design mock interfaces to gather user feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data preparation | Documentability |'
  prefs: []
  type: TYPE_TB
- en: Maintain data lineage with tools like DataHub or Great Expectations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| User transparency |'
  prefs: []
  type: TYPE_TB
- en: Annotate datasets clearly and provide metadata descriptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Modeling | Interpretability |'
  prefs: []
  type: TYPE_TB
- en: Use interpretable models (like decision trees).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Documentability |'
  prefs: []
  type: TYPE_TB
- en: Implement versioning for models and scripts using Git and DVC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use techniques like feature attribution (SHAP, LIME).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test explanations with users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automate documentation updates using pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a centralized repository for project documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluation | Appropriateness recognizability |'
  prefs: []
  type: TYPE_TB
- en: Ensure evaluation metrics are domain-appropriate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| User engagement |'
  prefs: []
  type: TYPE_TB
- en: Test models with representative user groups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Deployment | User transparency |'
  prefs: []
  type: TYPE_TB
- en: Deploy user-facing documentation alongside APIs or systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use plain language in user-facing documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Documentability |'
  prefs: []
  type: TYPE_TB
- en: Utilize CI/CD pipelines (e.g., GitHub Actions, MLflow).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish ethical guidelines and disclaimers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Monitoring and maintenance | User transparency |'
  prefs: []
  type: TYPE_TB
- en: Provide users access to monitoring dashboards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Interpretability |'
  prefs: []
  type: TYPE_TB
- en: Monitor explainability metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Article 14 quality attributes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to documentability, value alignment, accountability, and interpretability,
    Article 14 is associated with the following quality attributes for safety-critical
    AI systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Learnability
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which human operators can understand and learn an AI system’s
    behavior, outputs, and decision-making processes. Learnability depends on the
    clarity and comprehensibility of system documentation, interfaces, and training
    materials; it reflects how quickly human operators can achieve competency in overseeing
    the system.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which the system produces outcomes that are unbiased and do not
    result in unjustified discrimination against any individual or group. Fairness
    ensures equitable treatment and decision making across different demographic groups.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which the internal logic and behavior of the system can be understood
    and interpreted by humans. Explainability enables stakeholders to comprehend how
    inputs are transformed into outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Intervenability
  prefs: []
  type: TYPE_NORMAL
- en: The capacity for human operators to influence or override the system’s behavior
    and decisions, or halt the system if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Monitorability
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which the system’s operations and performance can be continuously
    observed, measured, and analyzed. Monitorability facilitates early detection of
    issues such as drift, anomalies, and system failures.
  prefs: []
  type: TYPE_NORMAL
- en: User error protection
  prefs: []
  type: TYPE_NORMAL
- en: The inclusion of design features that help prevent or mitigate errors caused
    by user interactions. This includes safeguards against incorrect inputs, misinterpretations,
    and unintended uses of the system.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5-11](#chapter_5_table_11_1748539922529637) maps these quality attributes
    to the relevant AI engineering practices to fulfill the technical requirements
    of Article 14.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-11\. Quality attributes relevant to Article 14 and corresponding AI
    engineering practices and tools
  prefs: []
  type: TYPE_NORMAL
- en: '| CRISP-ML(Q) phase | Quality attributes | AI engineering practices and tools
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Business and data understanding | Fairness |'
  prefs: []
  type: TYPE_TB
- en: Assess potential biases in datasets using fairness auditing tools like Aequitas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define fairness metrics (e.g., demographic parity) as project requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learnability |'
  prefs: []
  type: TYPE_TB
- en: Conduct user research to understand user learning capabilities and needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define metrics like System Usability Scale (SUS) for usability and ease of learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data preparation | Fairness |'
  prefs: []
  type: TYPE_TB
- en: Ensure representative and balanced datasets by applying techniques such as oversampling
    or undersampling. Use bias mitigation tools like Fairlearn or IBM’s AI Fairness
    360 to evaluate and reduce bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a data dictionary to promote transparency about feature definitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Monitorability |'
  prefs: []
  type: TYPE_TB
- en: Use metadata tracking tools like DVC to document data transformations, ensuring
    traceability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Modeling | Explainability |'
  prefs: []
  type: TYPE_TB
- en: Choose models that balance accuracy with interpretability, such as decision
    trees or linear models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Intervenability |'
  prefs: []
  type: TYPE_TB
- en: Introduce model checkpoints during training for human-in-the-loop interventions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate tooling like MLflow for easy rollback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluation | Explainability |'
  prefs: []
  type: TYPE_TB
- en: Utilize tools like SHAP or LIME to evaluate model predictions and make them
    understandable to nontechnical stakeholders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Fairness |'
  prefs: []
  type: TYPE_TB
- en: Evaluate the model on the fairness metrics defined earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement subgroup performance tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| User error protection |'
  prefs: []
  type: TYPE_TB
- en: Simulate potential user interaction scenarios and evaluate how errors can be
    mitigated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create fallback strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Deployment | Monitorability |'
  prefs: []
  type: TYPE_TB
- en: Simulate monitoring scenarios such as data drift or model decay.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate model monitoring tools like Prometheus, Grafana, or AWS SageMaker
    Model Monitor to track model drift, bias drift, and other performance indicators.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Intervenability |'
  prefs: []
  type: TYPE_TB
- en: Set up mechanisms for human intervention in case anomalies are detected, such
    as alert systems and override mechanisms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Monitoring and maintenance | Learnability |'
  prefs: []
  type: TYPE_TB
- en: Provide regular training and clear documentation for users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish a feedback loop to gather user input continuously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Monitorability |'
  prefs: []
  type: TYPE_TB
- en: Conduct regular audits to validate model behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement automated retraining pipelines to address performance drift and ensure
    compliance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Checklist for compliance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since Articles 13 and 14 of the EU AI Act are closely related, as shown in
    [Table 5-9](#chapter_5_table_9_1748539922529591), I’ve created a comprehensive
    joint checklist for integrating AI engineering best practices that align with
    the quality attributes associated with both of them. It defines a structured process
    you can follow to implement the requirements of Articles 13 and 14 throughout
    all phases of the development lifecycle:'
  prefs: []
  type: TYPE_NORMAL
- en: Business and data understanding
  prefs: []
  type: TYPE_NORMAL
- en: 'User requirement gathering: Engage end users in defining AI system requirements
    through interviews, surveys, or workshops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Expectation alignment: Ensure user expectations are documented and understood,
    along with all stakeholders’ fairness expectations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transparency requirements: Define user-specific transparency needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'User personas and goals: Create representative user personas to guide the design
    of user-facing transparency and oversight features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Explainability objectives: Establish clear interpretability goals aligned with
    user needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation standards: Set organization-wide standards for capturing system
    requirements, data sources, design decisions, and rationales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Requirement documentation: Use structured templates to document all information
    related to system requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scope definition: Clearly define the AI system’s intended use cases and limitations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case documentation: Develop detailed descriptions of appropriate and inappropriate
    use cases to prevent misuse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs: []
  type: TYPE_NORMAL
- en: 'Data relevance validation: Involve users or domain experts in validating dataset
    relevance and quality. Analyze the dataset distribution, complete a data bias
    assessment, and log the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fairness-aware preprocessing: Apply preprocessing techniques to address bias
    or imbalances where necessary, and document decisions and justifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data catalog accessibility: Provide users with access to data catalogs, including
    metadata, schema descriptions, and data quality indicators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature explainability: Validate that data features are understandable and
    meaningful to users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature selection documentation: Record the rationale behind feature selection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data source documentation: Maintain detailed records of data sources, collection
    methods, and preprocessing steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data versioning: Implement a version control system to track changes to the
    data over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contextual data labeling: Label and tag data to indicate the context in which
    it is appropriate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling
  prefs: []
  type: TYPE_NORMAL
- en: 'Model selection input: Involve users or domain experts in model selection decisions
    to ensure they meet users’ needs and expectations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model architecture disclosure: Provide users with understandable information
    about the model architecture and its components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use of interpretable models: Prefer inherently interpretable model types where
    possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Explainability techniques: Implement techniques to explain complex or opaque
    models, if these are used. Validate explainability outputs with SHAP, LIME, or
    similar tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model documentation: Maintain complete documentation of model architecture,
    hyperparameters, and training processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model cards: Create model cards summarizing key information about each model
    (intended use, ethical considerations, limitations, evaluation results).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Human override mechanisms: Implement mechanisms that allow human operators
    to override or intervene in model decisions when necessary, and document when
    and how they should be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation
  prefs: []
  type: TYPE_NORMAL
- en: 'User testing sessions: Involve users in testing and evaluating model outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evaluation results sharing: Provide stakeholders with accessible reports on
    evaluation outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Explain evaluation metrics: Use understandable metrics and explain their implications
    to users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evaluation procedures documentation: Thoroughly document evaluation methods
    and rationales, along with datasets used and results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Experiment tracking: Use tools such as MLflow or Weights & Biases to track
    experiments and results systematically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contextual performance analysis: Evaluate model performance across different
    contexts (user groups, usage scenarios), and document findings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model applicability testing: Validate that the models used are appropriate
    for the intended deployment contexts and use cases. Document limitations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stress testing: Perform stress tests to assess model performance in various
    scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'User control options: Implement mechanisms that allow users to control AI interactions
    (e.g., opt-in/opt-out features, adjustable system behavior settings).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AI interaction disclosure: Clearly inform users when they are interacting with
    an AI system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Information accessibility: Provide easy access to information about how the
    AI system works.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decision explanations: Offer understandable explanations for AI decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deployment documentation: Record details on deployment configurations, environments,
    and versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AI system indicators: Use visual cues or labels to indicate AI-generated content
    or decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage limitations display: Clearly communicate the intended use cases and known
    limitations of the AI system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alerting systems: Configure alerting mechanisms for data drift, model performance
    degradation, and other issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and maintenance
  prefs: []
  type: TYPE_NORMAL
- en: 'Feedback mechanisms: Provide channels for users to submit feedback and report
    issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Update notifications: Inform stakeholders about system updates, significant
    changes, and issues that arise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Continuous explainability: Ensure that explanations remain accurate and relevant
    over time, and implement real-time explainability pipelines if necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maintenance logs: Keep detailed records of maintenance activities, updates,
    and system changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation updates: Regularly revise technical and user-facing documentation
    to reflect the system’s current state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Context drift monitoring: Monitor for changes in the operating environment
    that could affect the appropriateness or effectiveness of the system. Conduct
    regular fairness and performance audits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Misuse alerts: Set up alerting mechanisms for potential misuse or operation
    outside intended contexts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feedback loops: Implement mechanisms for integrating user feedback into future
    system improvements, retraining, or governance decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This checklist provides a structured way to ensure compliance with the joint
    requirements of Articles 13 and 14\. Again, teams should revise the checklist
    based on their specific high-risk AI system and organizational context, while
    ensuring compliance with all regulatory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article 15: Accuracy, Robustness, and Cybersecurity'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to Kelly et al., to guarantee the implementation of accurate, reliable,
    and secure AI systems, the requirements stated in Article 15 can be mapped to
    the following quality attributes, as well as appropriateness recognizability and
    self-descriptiveness:'
  prefs: []
  type: TYPE_NORMAL
- en: Functional correctness
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which the system produces outputs that accurately reflect its
    intended functionality and specified behavior for given inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Faultlessness
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which the system operates without defects, malfunctions, or unintended
    behavior throughout its lifecycle, ensuring reliability and consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  prefs: []
  type: TYPE_NORMAL
- en: The system’s ability to maintain stable performance and functionality under
    a wide range of conditions, including adversarial inputs, edge cases, or unexpected
    environmental changes.
  prefs: []
  type: TYPE_NORMAL
- en: Functional adaptability
  prefs: []
  type: TYPE_NORMAL
- en: The capacity of the system to adapt to changing requirements, environmental
    conditions, or use contexts without significant performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance
  prefs: []
  type: TYPE_NORMAL
- en: The system’s ability to continue functioning correctly (or in a degraded mode)
    even when faults or failures occur.
  prefs: []
  type: TYPE_NORMAL
- en: Integrity
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which the system’s data and operations are protected against unauthorized
    access or alterations, ensuring trust in its behavior and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Resistance
  prefs: []
  type: TYPE_NORMAL
- en: The ability of the system to defend against attacks, tampering, or unauthorized
    manipulations, maintaining secure and reliable operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5-12](#chapter_5_table_12_1748539922529659) summarizes the AI engineering
    practices you should implement throughout the development lifecycle to support
    these attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-12\. Quality attributes relevant to Article 15 and corresponding AI
    engineering practices and tools
  prefs: []
  type: TYPE_NORMAL
- en: '| CRISP-ML(Q) phase | Quality attributes | AI engineering practices and tools
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Business and data understanding | Functional correctness | Define measurable
    goals using JIRA. |'
  prefs: []
  type: TYPE_TB
- en: '| Self-descriptiveness | Document assumptions using Confluence or GitHub wikis.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Data preparation | Faultlessness | Implement data validation pipelines with
    Great Expectations. |'
  prefs: []
  type: TYPE_TB
- en: '| Integrity | Secure data with encryption (AWS KMS, GCP KMS) and access controls.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Robustness | Include data augmentation techniques for edge cases. |'
  prefs: []
  type: TYPE_TB
- en: '| Modeling | Robustness | Use adversarial training frameworks like CleverHans
    or IBM ART. |'
  prefs: []
  type: TYPE_TB
- en: '| Functional adaptability | Utilize AutoML tools (H2O.ai, DataRobot) for adaptability.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fault tolerance | Design models with redundancy (ensemble methods) for fault
    tolerance. |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation | Functional correctness | Use metrics like precision, recall,
    F1 score, and confusion matrix. |'
  prefs: []
  type: TYPE_TB
- en: '| Robustness | Evaluate models with stress testing and adversarial scenarios.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Appropriateness recognizability | Assess model transparency with explainability
    tools (SHAP, LIME). |'
  prefs: []
  type: TYPE_TB
- en: '| Deployment | Faultlessness | Use CI/CD pipelines (Jenkins, GitHub Actions,
    ArgoCD) for bug-free deployments. |'
  prefs: []
  type: TYPE_TB
- en: '| Integrity | Sign model artifacts with secure hash algorithms (SHA-256). |'
  prefs: []
  type: TYPE_TB
- en: '| Resistance | Apply NIST Cybersecurity Framework principles: identify vulnerabilities,
    secure environments, detect threats. |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring and maintenance | Functional correctness | Implement real-time
    performance monitoring with Grafana dashboards. |'
  prefs: []
  type: TYPE_TB
- en: '| Self-descriptiveness | Log decisions and anomalies with Elastic Stack. |'
  prefs: []
  type: TYPE_TB
- en: '| Fault tolerance | Design failover mechanisms with Kubernetes health probes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Resistance | Apply active threat detection with AWS GuardDuty or Azure Security
    Center. |'
  prefs: []
  type: TYPE_TB
- en: Checklist for compliance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Building an AI system that complies with Article 15 of the EU AI Act requires
    a comprehensive approach that addresses the related dimensions of accuracy, robustness,
    and cybersecurity. The following checklist (which can be adapted according to
    context) outlines practical steps that can be taken to support compliance throughout
    the system’s lifecycle. It integrates AI engineering best practices aligned with
    the quality attributes described in the previous section into each development
    phase:'
  prefs: []
  type: TYPE_NORMAL
- en: Business and data understanding
  prefs: []
  type: TYPE_NORMAL
- en: Identify potential data quality issues through profiling and exploratory analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document known data limitations and risks that could impact system reliability
    and correctness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure training data represents diverse operational scenarios, including potential
    edge cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define robustness requirements based on system objectives and identified risks
    (e.g., resistance to common attacks such as feature perturbation or injection
    of adversarial examples, performance degradation thresholds under stress conditions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs: []
  type: TYPE_NORMAL
- en: Validate datasets for accuracy, duplicates, outliers, and completeness using
    tools such as Great Expectations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain end-to-end data lineage and document all preprocessing steps to support
    traceability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply data augmentation techniques to simulate edge cases and stress-test model
    generalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use adversarial testing to ensure robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encrypt data in transit and at rest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enforce strict access controls to ensure secure handling of sensitive data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling
  prefs: []
  type: TYPE_NORMAL
- en: Define and track performance metrics to comprehensively assess functional correctness,
    robustness, and security (e.g., precision, recall, F1 score, mean absolute error,
    adversarial robustness score, attack success rate, system uptime, and throughput).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement unit and integration tests for model components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use exception handling mechanisms in code to manage unexpected inputs or processing
    errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct code reviews and apply static code analysis tools (e.g., SonarQube)
    to detect defects early.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use adversarial training techniques to improve robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use interpretable models or apply explainability tools (e.g., SHAP, LIME) to
    clarify model behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use metadata management tools (e.g., MLflow, Neptune.ai) to track model lineage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sign model artifacts with secure hash algorithms (e.g., SHA-256) to ensure their
    integrity and authenticity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protect model training environments from unauthorized access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct security assessments and penetration tests during development to identify
    and address potential vulnerabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate system behavior under adversarial attack scenarios and mitigate high-level
    threats (e.g., gradient masking).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Validate model outputs against ground truth data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess the model’s accuracy using holdout datasets and cross-validation techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct regression testing to ensure updates do not introduce new errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate model performance against adversarial inputs and out-of-distribution
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform stress testing to determine performance limits and evaluate system behavior
    in extreme or unexpected conditions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment
  prefs: []
  type: TYPE_NORMAL
- en: Implement CI/CD pipelines to automate testing and validation prior to deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor real-time performance to ensure stability and consistency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply redundancy mechanisms to mitigate failures in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy redundant models or ensemble models for fault tolerance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure deployment pipelines using role-based access controls and encrypted communications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor for cybersecurity threats using tools like AWS GuardDuty.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop and periodically test incident response plans for security breaches
    and failures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and maintenance
  prefs: []
  type: TYPE_NORMAL
- en: Monitor edge case behavior and retrain models as new data becomes available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Periodically reassess the system’s robustness to ensure it can handle evolving
    operational or environmental conditions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct regular security audits to verify the integrity of deployed systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encrypt backups and secure access to sensitive data and systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuously monitor for emerging threats and implement mitigations to maintain
    system security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use intrusion detection systems to detect unauthorized access or anomalous activity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By integrating these practices into the AI lifecycle, documenting them thoroughly,
    and continuously evaluating performance against accuracy, robustness, and security
    requirements, AI engineering teams can support compliance with Article 15\. Close
    collaboration between AI engineers, information security teams, and risk and compliance
    stakeholders will be key.
  prefs: []
  type: TYPE_NORMAL
- en: Further information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following frameworks and resources offer practical guidance for implementing
    the technical requirements of Article 15 and building trustworthy, reliable, and
    secure AI systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[NIST Cybersecurity Framework](https://oreil.ly/2mQel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[NIST Privacy Framework](https://oreil.ly/kTfwZ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Machine Learning for High-Risk Applications*](https://oreil.ly/dRyWk) by
    Patrick Hall, James Curtis, and Parul Pandey (O’Reilly)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has explored the intersection of the EU AI Act and AI engineering,
    presenting a framework for achieving and demonstrating compliance through robust
    engineering practices. As you have seen, data and AI governance, monitoring, alerting,
    and documentation are not merely best practices but foundational elements for
    compliance with the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: We examined Articles 9 through 15 of the Act here, translating their high-level
    requirements for high-risk AI systems into actionable technical specifications
    aligned with the CRISP-ML(Q) lifecycle. By mapping these requirements to relevant
    quality attributes for safety-critical AI systems, this chapter provides a practical
    roadmap for organizations to develop, deploy, and manage high-risk AI systems
    responsibly and transparently. (See [Table 5-13](#chapter_5_table_13_1748539922529689)
    for the complete mapping.) A key takeaway is the central role of documentation
    in assessing and demonstrating compliance. A robust metadata management system
    is also essential, both for meeting the documentation requirements of Article
    11 and for ensuring traceability, as mandated by Article 12.
  prefs: []
  type: TYPE_NORMAL
- en: AI governance is an emerging discipline, and its implementation varies across
    industries. Consequently, achieving compliance with the EU AI Act presents real
    challenges, and there is a need for an engineering-level approach to meet the
    Act’s requirements. This chapter, along with the previous ones, represents an
    initial effort to establish principles for proactive compliance through comprehensive
    AI engineering practices—practices that serve as enablers of trustworthy AI, which
    is the core motivation behind the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of AI engineering for the EU AI Act introduced here blends law,
    ethics, and MLOps, with an emphasis on continuous compliance through monitoring
    and documentation. In the next chapter, we will examine the requirements for limited-risk
    AI systems and explore strategies for proactive compliance through AI engineering
    in that context.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-13\. Mapping the EU AI Act requirements to quality attributes for high-risk
    AI systems
  prefs: []
  type: TYPE_NORMAL
- en: '| Quality attribute | Article 9 | Article 10 | Article 11 | Article 12 | Article
    13 | Article 14 | Article 15 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Risk identification | ✓ |   |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Testability | ✓ |   |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Value alignment | ✓ |   |   |   |   | ✓ |   |'
  prefs: []
  type: TYPE_TB
- en: '| Independence |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Data completeness |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Currentness |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Data fairness |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Precision |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Representativeness |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Consistency |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Credibility |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Temporality |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Confidentiality |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Compliance |   | ✓ |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Data traceability |   | ✓ |   | ✓ |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Traceability |   |   | ✓ | ✓ |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Operability |   |   |   | ✓ |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Non-repudiation |   |   |   | ✓ |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Self-descriptiveness |   |   |   | ✓ | ✓ |   | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Accountability |   |   |   | ✓ |   | ✓ |   |'
  prefs: []
  type: TYPE_TB
- en: '| Self-monitoring |   |   |   | ✓ |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Monitorability |   |   |   | ✓ |   | ✓ |   |'
  prefs: []
  type: TYPE_TB
- en: '| User error protection |   |   |   | ✓ |   | ✓ |   |'
  prefs: []
  type: TYPE_TB
- en: '| User engagement |   |   |   |   | ✓ |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| User transparency |   |   |   |   | ✓ |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Interpretability |   |   |   |   | ✓ | ✓ |   |'
  prefs: []
  type: TYPE_TB
- en: '| Documentability |   |   |   |   | ✓ | ✓ |   |'
  prefs: []
  type: TYPE_TB
- en: '| Appropriateness recognizability |   |   |   |   | ✓ |   | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Learnability |   |   |   |   |   | ✓ |   |'
  prefs: []
  type: TYPE_TB
- en: '| Fairness |   |   |   |   |   | ✓ |   |'
  prefs: []
  type: TYPE_TB
- en: '| Explainability |   |   |   |   |   | ✓ |   |'
  prefs: []
  type: TYPE_TB
- en: '| Intervenability |   |   |   |   |   | ✓ |   |'
  prefs: []
  type: TYPE_TB
- en: '| Functional correctness |   |   |   |   |   |   | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Faultlessness |   |   |   |   |   |   | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Robustness |   |   |   |   |   |   | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Functional adaptability |   |   |   |   |   |   | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Fault tolerance |   |   |   |   |   |   | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Integrity |   |   |   |   |   |   | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Resistance |   |   |   |   |   |   | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '^([1](ch05.html#id564-marker)) To learn more about technical documentation,
    refer to the paper [“Documenting High-Risk AI: A European Regulatory Perspective”](https://oreil.ly/AoUII)
    by Isabelle Hupont et al.'
  prefs: []
  type: TYPE_NORMAL
