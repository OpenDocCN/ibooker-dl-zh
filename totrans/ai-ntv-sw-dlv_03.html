<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. The Build and Pre-Deployment Testing Steps of Continuous Integration"><div class="chapter" id="chapter_3_the_build_and_pre_deployment_testing_steps_of_cont_1749354010266208">
      <h1><span class="label">Chapter 3. </span>The Build and Pre-Deployment <span class="keep-together">Testing Steps of Continuous Integration</span></h1>
      <p>Simply put, our modern software delivery practices provide a structure to help us plan, write, build, test, and deploy software. In <a data-type="xref" href="ch02.html#chapter_2_source_control_management_1749354010078326">Chapter 2</a>, we looked at how SCM systems help track and manage changes as we write code. </p>
      <p>In this chapter, we turn our attention to continuous integration<a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" id="xi_CICDpipeline3474"/>. <a data-type="xref" href="#chapter_3_figure_1_1749354010256769">Figure 3-1</a> shows a CI/CD pipeline that we’ll look at shortly and return to in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#chapter_4_deploying_to_test_environments_1749354010445896">4</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch08.html#chapter_8_feature_management_and_experimentation_1749354011197288">8</a>.</p>
      <figure><div id="chapter_3_figure_1_1749354010256769" class="figure">
        <img src="assets/ansd_0301.png" width="600" height="254"/>
        <h6><span class="label">Figure 3-1. </span>A CI/CD pipeline</h6>
      </div></figure>
      <p>We’ll explore the continuous integration pipeline with emphasis on build processes and pre-deployment testing (static scans, unit tests, and integration tests). We’ll demonstrate how an AI-native approach can accelerate CI through GenAI, agentic AI, and open standards such as MCP implementations. These technologies enable automated processes, predictive optimization, standardized context management, and intelligent testing strategies throughout the build, cache, and testing phases.</p>
      <p>In addition to the key continuous integration steps, we’ll review continuous integration tools and discuss factors to consider when selecting one. You will come away with an understanding of how to improve efficiency, quality, and security in your build pipeline.</p>
      <section data-type="sect1" data-pdf-bookmark="A Short History of Building and Testing Software"><div class="sect1" id="chapter_3_a_short_history_of_building_and_testing_software_1749354010266349">
        <h1>A Short History of Building and Testing Software</h1>
        <p>This is a familiar story<a contenteditable="false" data-type="indexterm" data-primary="software, history of building and testing" id="xi_softwarehistoryofbuildingandtesting31336"/><a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="building and testing software (history of CI)" id="xi_CICDpipelinebuildingandtestingsoftwarehistoryofCI31336"/>. In 1947, while working on the Harvard Mark II computer, a team of engineers discovered a moth trapped in a relay, causing the machine to malfunction. They removed the moth and taped it into their logbook with the note “First actual case of bug being found,” thus solidifying the association of “bug” with software errors. Finding the bug in the machine accurately characterizes testing in the early days of software development. Developers would write code independently and integrate it. Testing<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="historical development of software" id="id448"/> was typically done manually and ad hoc. Teams focused on finding the bugs, ridding machines of “the moths” when errors were discovered. Bugs were typically found in production, resulting in delays and unreliable software.</p>
        <p>As software development evolved, testing became more formalized and rigorous, with a focus on trying to “break” the software to uncover defects. Formal testing methodologies and standards began to emerge, such as the IEEE 829 Standard for Software and System Test Documentation (1983)<a contenteditable="false" data-type="indexterm" data-primary="IEEE 829 Standard for Software and System Test Documentation (1983)" id="id449"/>.</p>
        <section data-type="sect2" data-pdf-bookmark="Structured Software Development and Waterfall Methodologies"><div class="sect2" id="chapter_3_structured_software_development_and_waterfall_meth_1749354010266409">
          <h2>Structured Software Development and Waterfall Methodologies</h2>
          <p>Waterfall methodologies<a contenteditable="false" data-type="indexterm" data-primary="waterfall methodologies, building software" id="id450"/><a contenteditable="false" data-type="indexterm" data-primary="structured software development" id="id451"/><a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="in waterfall methodologies" data-secondary-sortas="waterfall methodologies" id="id452"/> introduced a structured approach to software development, where testing became a distinct phase. Acceptance criteria, defined during requirements gathering, outlined the conditions the software must meet. Test cases were then developed and executed at the end of development to validate these criteria. Defects were documented and resolved until the software met all requirements. This formal approach, however, often resulted in a considerable delay between coding and testing, making early issue detection and resolution challenging and eventually resulting in a slower time-to-market for new products and features.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Agile and Test-Driven Development"><div class="sect2" id="chapter_3_agile_and_test_driven_development_1749354010266461">
          <h2>Agile and Test-Driven Development</h2>
          <p>In <a data-type="xref" href="ch01.html#chapter_1_the_road_to_ai_native_devops_1749354009875299">Chapter 1</a>, we discussed the emergence of Agile methodologies<a contenteditable="false" data-type="indexterm" data-primary="Agile practices, development of" id="id453"/><a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="in Agile methodologies" data-secondary-sortas="Agile methodologies" id="id454"/> in software development, motivated by the inefficiencies and limitations of the waterfall development. Agile methodologies’ more flexible and responsive development model emphasized frequent feedback and iterative development, necessitating new testing approaches that could keep pace with the rapid development cycles. This led to new testing approaches.</p>
          <p>Extreme Programming (XP)<a contenteditable="false" data-type="indexterm" data-primary="Extreme Programming (XP)" id="id455"/><a contenteditable="false" data-type="indexterm" data-primary="XP (Extreme Programming)" id="id456"/>, developed by Kent Beck<a contenteditable="false" data-type="indexterm" data-primary="Beck, Kent" id="id457"/><a contenteditable="false" data-type="indexterm" data-primary="Cunningham, Ward" id="id458"/><a contenteditable="false" data-type="indexterm" data-primary="Jeffries, Ron" id="id459"/>, Ward Cunningham, and Ron Jeffries, was a specific Agile methodology defined by a set of best practices. One fundamental XP practice is test-driven development (TDD)<a contenteditable="false" data-type="indexterm" data-primary="test-driven development (TDD)" id="id460"/>. In TDD, you write tests before writing the associated code. Beck’s influential book <em>Extreme Programming Explained</em><em> </em>(Addison-Wesley), first published in 1999, popularized TDD to a wide audience, and early tools like JUnit<a contenteditable="false" data-type="indexterm" data-primary="JUnit framework" id="id461"/> (for Java) and NUnit<a contenteditable="false" data-type="indexterm" data-primary="NUnit framework" id="id462"/> (for .NET) provided developers with frameworks to easily write these types of tests before writing corresponding code. </p>
          <p>Writing tests before code encourages developers to think deeply about desired code behavior, leading to better design and fewer defects. While this concept existed previously, TDD’s specific approach of writing failing tests first and then coding to pass them aligned well with Agile’s focus on short cycles and frequent delivery of working software. This practice redefined the notion of completeness: <em>A feature isn’t done when the code is working, but when the automated tests are complete and passing.</em></p>
          <p>The automated tests<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="and TDD" data-secondary-sortas="TDD " id="id463"/> created during TDD provide a safety net, allowing developers to refactor code with confidence, knowing that any regressions will be quickly caught by the tests. This enables faster iteration and more frequent releases, which in turn allows for quicker feedback from customers and stakeholders. The tests themselves also serve as a form of documentation, clearly articulating the expected behavior of the system.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Enter Continuous Integration"><div class="sect2" id="chapter_3_enter_continuous_integration_1749354010266508">
          <h2>Enter Continuous Integration</h2>
          <p>As we introduced in <a data-type="xref" href="ch01.html#chapter_1_the_road_to_ai_native_devops_1749354009875299">Chapter 1</a>, CI is the practice of automating the integration of code changes from multiple contributors into a shared repository, frequently triggering automated builds and tests to ensure the software remains in a working state. This complemented TDD.</p>
          <p>The <a href="https://oreil.ly/neqmf">roots of CI</a> trace back to the 1990s. Grady Booch<a contenteditable="false" data-type="indexterm" data-primary="Booch, Grady" id="id464"/> first coined the term “continuous integration” in 1991, but it was Kent Beck and Ron Jeffries who truly put it into practice while collaborating on a project in 1997. Their goal was to address the “integration hell” that arose from infrequent code merges, where conflicts and errors would pile up and become increasingly difficult to resolve.</p>
          <p>Early CI systems were often custom-built and tailored to specific projects. One notable example was CruiseControl<a contenteditable="false" data-type="indexterm" data-primary="CruiseControl" id="id465"/>, created in 2001 by ThoughtWorks. It was one of the first open source CI servers, allowing teams to automate the building and testing of software with every code commit. However, it lacked a user-friendly interface and flexible job scheduling, leading to the development of Hudson in 2005 by Kohsuke Kawaguchi<a contenteditable="false" data-type="indexterm" data-primary="Kawaguchi, Kohsuke" id="id466"/>. Hudson quickly gained popularity due to its ease of use and powerful features.</p>
          <p>In 2011, a dispute with Oracle led to <a href="https://oreil.ly/MF9WD">Hudson being forked into Jenkins</a>, which has since become one of the most widely used tools<a contenteditable="false" data-type="indexterm" data-primary="Jenkins" id="id467"/> for not only continuous integration, but also continuous delivery and deployment. The popularity of Jenkins can be attributed to its flexibility, extensibility, and vast plug-in ecosystem, allowing it to integrate with various tools and adapt to different <span class="keep-together">workflows</span>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Continuous Integration Today"><div class="sect2" id="chapter_3_continuous_integration_today_1749354010266564">
          <h2>Continuous Integration Today</h2>
          <p>Continuous integration has evolved into a foundational practice in modern software development, and CI/CD systems are the backbone of any delivery pipeline. Through the continuous integration of code changes, teams have come to depend on the following advantages:</p>
          <dl>
            <dt>Reduced integration problems</dt>
            <dd>
              <p>CI eliminates the dreaded “integration hell” by ensuring developers merge their code changes frequently, minimizing conflicts and making them easier to resolve.</p>
            </dd>
            <dt>Faster feedback</dt>
            <dd>
              <p>CI’s automated build and test processes provide developers with rapid feedback on their code changes, allowing them to catch and fix errors quickly, thus maintaining a stable and deployable codebase.</p>
            </dd>
            <dt>Increased efficiency and reliability</dt>
            <dd>
              <p>By automating the build and testing process, CI eliminates manual errors and inconsistencies, leading to more reliable and predictable builds.</p>
            </dd>
            <dt>Improved transparency</dt>
            <dd>
              <p>CI dashboards and notifications provide real-time visibility into the build and test status, allowing everyone on the team to track progress, identify potential issues, and collaborate more effectively.</p>
            </dd>
            <dt>Accelerated releases</dt>
            <dd>
              <p>By streamlining and automating the build, test, and integration processes, CI enables faster and more frequent releases, allowing businesses to respond more rapidly to customer feedback and market changes.</p>
            </dd>
          </dl>
          <p>In <a data-type="xref" data-xrefstyle="select:nopage" href="#chapter_3_continuous_integration_in_the_ci_cd_pipeline_1749354010266620">“Continuous Integration in the CI/CD Pipeline”</a>, we’ll look at the function of CI in the delivery pipeline and explore the landscape of CI tools<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_softwarehistoryofbuildingandtesting31336" id="id468"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_CICDpipelinebuildingandtestingsoftwarehistoryofCI31336" id="id469"/>.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Continuous Integration in the CI/CD Pipeline"><div class="sect1" id="chapter_3_continuous_integration_in_the_ci_cd_pipeline_1749354010266620">
        <h1>Continuous Integration in the CI/CD Pipeline</h1>
        <p>In <a data-type="xref" href="ch02.html#chapter_2_source_control_management_1749354010078326">Chapter 2</a>, we introduced a CI/CD pipeline, focusing on the relationship between the code repository and code integration. Let’s return to this pipeline<a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="CI process" id="xi_CICDpipelineCIprocess363295"/> and focus on the continuous integration, that is, the build step and the steps to execute pre-deployment test types, including static analysis, unit tests, and integration tests.</p>
        <p class="pagebreak-before">The pipeline in <a data-type="xref" href="#chapter_3_figure_2_1749354010256797">Figure 3-2</a> shows a typical CI process. </p>
        <figure><div id="chapter_3_figure_2_1749354010256797" class="figure">
          <img src="assets/ansd_0302.png" width="600" height="224"/>
          <h6><span class="label">Figure 3-2. </span>CI pipeline triggered by opening a Git PR</h6>
        </div></figure>
        <p>This example is triggered when a developer opens a pull request. The goal of this pipeline is to validate the changes proposed in the PR <em>before the changes are merged into the main branch.</em> Let’s go through the steps:</p>
        <dl>
          <dt>1. Code trigger</dt>
          <dd>
            <p>A developer or an AI agent opens<a contenteditable="false" data-type="indexterm" data-primary="code trigger step, CI process" id="id470"/> a pull request on the hosted repository (e.g., GitHub, GitLab, Bitbucket), which triggers the pipeline.</p>
          </dd>
          <dt>2. Checkout</dt>
          <dd>
            <p>The pipeline checks out<a contenteditable="false" data-type="indexterm" data-primary="checkout step, CI process" id="id471"/> the source code from the branch specified in the PR.</p>
          </dd>
          <dt>3. Build</dt>
          <dd>
            <p>The code is compiled<a contenteditable="false" data-type="indexterm" data-primary="build step, CI process" id="id472"/> (if necessary) and built into an executable or deployable artifact.</p>
          </dd>
          <dt>4. Static analysis</dt>
          <dd>
            <p>Tools like linters and code analyzers<a contenteditable="false" data-type="indexterm" data-primary="static analysis, CI process" id="id473"/> scan the code for style violations, potential bugs, and security issues. </p>
          </dd>
          <dt>5. Unit tests</dt>
          <dd>
            <p class="fix_tracking">Automated tests<a contenteditable="false" data-type="indexterm" data-primary="unit tests" id="id474"/><a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="unit tests in CI/CD pipeline" id="id475"/><a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="unit tests" id="id476"/> that verify the functionality of individual code units are executed.</p>
          </dd>
          <dt>6. Integration tests</dt>
          <dd>
            <p>Relatively fast tests<a contenteditable="false" data-type="indexterm" data-primary="integration tests" id="id477"/> may be run to verify the interaction between different components of the code.</p>
          </dd>
          <dt>7. Feedback</dt>
          <dd>
            <p>The pipeline provides feedback<a contenteditable="false" data-type="indexterm" data-primary="feedback from pipeline step, CI process" id="id478"/> to the developer about the PR’s status (success/failure) and any issues found. This feedback is displayed directly in the PR on the hosted repository.</p>
          </dd>
        </dl>
        <p>This pipeline detects and notifies developers of any issues within their code. The build step determines whether the code changes have broken the build. The test steps answer the following questions: Does this code do what is intended? Does this code include security vulnerabilities, unsafe operations, potential bugs, bad practices, deprecated features, or even inconsistent formatting?</p>
        <p>The code pipeline provides developers with near-real-time feedback by detecting issues and running fast tests when pull requests are opened or updated. It answers critical questions about the code’s functionality, security, and quality. Developers can then quickly address problems, refine the PR, or confidently merge it when all checks pass, accelerating development and ensuring a robust codebase.</p>
        <p>(In <a data-type="xref" href="ch04.html#chapter_4_deploying_to_test_environments_1749354010445896">Chapter 4</a>, we’ll explore a complementary CI pipeline triggered when a PR is merged. This pipeline deploys new code to test environments and executes longer-running test suites.)</p>
        <p>Note that while our sample pipeline uses a code change trigger, CI/CD systems typically offer other trigger options, like scheduled and manual triggers, for more flexibility.</p>
        <section data-type="sect2" data-pdf-bookmark="The Essential Build Step"><div class="sect2" id="chapter_3_the_essential_build_step_1749354010266676">
          <h2>The Essential Build Step</h2>
          <p>The build step<a contenteditable="false" data-type="indexterm" data-primary="build step, CI process" id="xi_buildstepCIprocess310628"/> involves packaging code into a deployable artifact<a contenteditable="false" data-type="indexterm" data-primary="deployable artifacts, packaging code into" id="id479"/>. Examples of deployable artifacts include container images (used to deploy in Kubernetes/serverless environments), language-specific packages (such as JAR, npm, NuGet, etc.), and mobile application packages (such as APK or IPA), among others. For example, code written in a compiled language, like C++, is first compiled and then linked to create machine code. Interpreted languages often require a build step to package code into an intermediate format, such as a Java Archive (JAR) file, for compilation at runtime. Other interpreted languages, including JavaScript, can be transpiled or minified to optimize for execution.</p>
          <p>Depending on the type of code, this step or series of steps relies on build automation tools, task runners, or build scripts. </p>
          <p>Build automation tools<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="CI/CD pipeline" id="xi_automationCICDpipeline310836"/><a contenteditable="false" data-type="indexterm" data-primary="build automation tools, CI process" id="xi_buildautomationtoolsCIprocess310836"/> orchestrate the entire build process. Popular examples of automation tools include the following:</p>
          <dl>
            <dt>Make and CMake</dt>
            <dd>
              <p>Make is one of the oldest and most fundamental build tools<a contenteditable="false" data-type="indexterm" data-primary="Make build tool" id="id480"/>. It uses a Makefile to define dependencies between files and the commands needed to build them. CMake<a contenteditable="false" data-type="indexterm" data-primary="CMake build tool" id="id481"/> is a newer cross-platform build system generator that can generate Makefiles, Visual Studio projects, and other build scripts. It’s widely used for C and C++ projects<a contenteditable="false" data-type="indexterm" data-primary="C and C++ projects, CMake build tool" id="id482"/>.</p>
            </dd>
            <dt>Ant</dt>
            <dd>
              <p>An early Java-based<a contenteditable="false" data-type="indexterm" data-primary="Java" data-secondary="build tools for" id="id483"/> build tool that uses XML to describe the build process<a contenteditable="false" data-type="indexterm" data-primary="Ant build tool" id="id484"/>. It’s known for its flexibility and cross-platform compatibility.</p>
            </dd>
            <dt>Maven</dt>
            <dd>
              <p>Another popular Java build tool<a contenteditable="false" data-type="indexterm" data-primary="Maven build tool" id="id485"/> that goes beyond just compilation. It manages dependencies, builds, tests, and packages projects.</p>
            </dd>
            <dt>Gradle</dt>
            <dd>
              <p>A newer build tool that combines the best of Ant and Maven<a contenteditable="false" data-type="indexterm" data-primary="Gradle build tool" id="id486"/>. It uses a Groovy-based DSL<a contenteditable="false" data-type="indexterm" data-primary="Groovy-based DSL" id="id487"/> to define builds and offers a more flexible and concise syntax.</p>
            </dd>
            <dt>Bazel</dt>
            <dd>
              <p>Developed by Google, Bazel<a contenteditable="false" data-type="indexterm" data-primary="Bazel build tool" id="id488"/> is a powerful build system designed for large-scale projects. It’s known for its speed, scalability, and support for multiple languages.</p>
            </dd>
            <dt>MSBuild</dt>
            <dd>
              <p>A build automation platform<a contenteditable="false" data-type="indexterm" data-primary="MSBuild build tool" id="id489"/> commonly used with .NET frameworks<a contenteditable="false" data-type="indexterm" data-primary=".NET frameworks" data-primary-sortas="NET frameworks" id="id490"/> and languages like C#, Visual Basic .NET, and F#. </p>
            </dd>
            <dt>Cargo</dt>
            <dd>
              <p>Cargo<a contenteditable="false" data-type="indexterm" data-primary="Cargo build tool" id="id491"/> is a package manager for the Rust programming language, used to build, compile, and manage Rust projects.</p>
            </dd>
          </dl>
          <p>Task runners automate repetitive tasks in the development workflow, such as minification, concatenation, and transpilation. Widely used task runners for JavaScript<a contenteditable="false" data-type="indexterm" data-primary="JavaScript" data-secondary="task runners for" id="id492"/> include the following:</p>
          <dl>
            <dt>npm scripts</dt>
            <dd>
              <p>Part of the Node Package Manager (npm)<a contenteditable="false" data-type="indexterm" data-primary="Node Package Manager (NPM)" id="id493"/>, npm scripts<a contenteditable="false" data-type="indexterm" data-primary="npm scripts" id="id494"/> are simple scripts defined in the <em>package.json</em> file that can automate common tasks like starting a development server, running tests, and building for production.</p>
            </dd>
            <dt>Gulp</dt>
            <dd>
              <p>A streaming build system that uses JavaScript<a contenteditable="false" data-type="indexterm" data-primary="Gulp task runner" id="id495"/> code to define tasks. It’s known for its speed and efficiency in processing files.</p>
            </dd>
            <dt>Grunt</dt>
            <dd>
              <p>Another task runner<a contenteditable="false" data-type="indexterm" data-primary="Grunt task runner" id="id496"/> for JavaScript projects, Grunt uses configuration files to define tasks. It’s known for its vast ecosystem of plug-ins.</p>
            </dd>
            <dt>Webpack</dt>
            <dd>
              <p>A module bundler<a contenteditable="false" data-type="indexterm" data-primary="Webpack module bundler" id="id497"/> primarily used for JavaScript applications. It can bundle JavaScript, CSS, and other assets into optimized files for production.</p>
            </dd>
            <dt>Rollup</dt>
            <dd>
              <p>Another module bundler<a contenteditable="false" data-type="indexterm" data-primary="Rollup module bundler" id="id498"/> that’s known for its focus on generating smaller and more efficient bundles than Webpack.</p>
            </dd>
          </dl>
          <p>Lastly, build scripts<a contenteditable="false" data-type="indexterm" data-primary="build scripts" id="id499"/> are custom scripts (often written in Bash, Python, or other scripting languages) that define the specific steps and commands needed to build a project. These can be used in conjunction with build automation tools or task runners<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_automationCICDpipeline310836" id="id500"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_buildstepCIprocess310628" id="id501"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_buildautomationtoolsCIprocess310836" id="id502"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Prioritizing Quality and Security with Static Analysis"><div class="sect2" id="chapter_3_prioritizing_quality_and_security_with_static_anal_1749354010266727">
          <h2>Prioritizing Quality and Security with Static Analysis</h2>
          <p>Immediately after we build our code, we run static analysis tool<a contenteditable="false" data-type="indexterm" data-primary="static analysis, CI process" id="id503"/>s, which may include a linter. Linters<a contenteditable="false" data-type="indexterm" data-primary="linters, CI static analysis" id="id504"/> are a specific type of static analysis tool used to check coding style (ensuring, for example, consistent formatting and naming patterns); for interpreted languages like JavaScript, linters check for typos, missing semicolons, or incorrect language usage. These tools examine source code without executing it, similar to proofreading a document before publishing it. They help identify potential issues early in the development process. Static code analysis encompasses a range of techniques to evaluate code for:</p>
          <dl>
            <dt>Potential bugs</dt>
            <dd>
              <p>Identifies common programming errors<a contenteditable="false" data-type="indexterm" data-primary="bug detection, static analysis" id="id505"/>, like null pointer dereferences, resource leaks, or logic flaws</p>
            </dd>
            <dt>Security vulnerabilities</dt>
            <dd>
              <p>Detects insecure<a contenteditable="false" data-type="indexterm" data-primary="security considerations" data-secondary="vulnerability scanning" id="id506"/> coding practices that could lead to SQL injections, cross-site scripting (XSS), or other exploits</p>
            </dd>
            <dt>Code smells</dt>
            <dd>
              <p>Flags maintainability issues<a contenteditable="false" data-type="indexterm" data-primary="code smells, static analysis" id="id507"/>, like duplicate code, excessive complexity, or unused variables, suggesting areas for refactoring</p>
            </dd>
            <dt>Adherence to standards</dt>
            <dd>
              <p>Enforces coding guidelines<a contenteditable="false" data-type="indexterm" data-primary="standards adherence" data-secondary="static analysis for" id="id508"/> and, sometimes, best practices specific to a language or project, ensuring consistency and readability</p>
            </dd>
          </dl>
          <p>By integrating these static analysis tools into the early stages of the development process, we not only ensure code quality but also implement a best practice referred to as shift-left security<a contenteditable="false" data-type="indexterm" data-primary="shift-left security" data-secondary="static analysis" id="id509"/>. Shift-left security<strong> </strong>refers to the strategy of implementing security practices in the earliest stages of development. We’ll dig into shift-left security and also explore how AI can help remediate security issues quickly in <a data-type="xref" href="ch05.html#chapter_5_securing_applications_and_the_software_supply_chai_1749354010735711">Chapter 5</a>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Automated Testing: Test Early, Test Often"><div class="sect2" id="chapter_3_automated_testing_test_early_test_often_1749354010266776">
          <h2>Automated Testing: Test Early, Test Often</h2>
          <p>Automated testing<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="CI/CD pipeline’s automated" id="xi_testingCICDpipelinesautomated318931"/><a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="CI/CD pipeline" id="xi_automationCICDpipeline318931"/><a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="of testing" data-secondary-sortas="testing" id="xi_automationoftesting318931"/> is fundamental to the CI/CD pipeline. After our example pipeline runs static analysis checks, it executes unit and integration tests against new code. Let’s look at these test types:</p>
          <dl>
            <dt>Unit tests</dt>
            <dd>
              <p>These tests<a contenteditable="false" data-type="indexterm" data-primary="unit tests" id="id510"/><a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="unit tests" id="id511"/> validate the smallest isolated pieces of code (units), such as functions or methods, to verify that they behave as expected in isolation. Imagine a simple weather application that fetches weather data from an external API, processes it, and displays it to the user. Unit tests might test functions that process raw weather data, validating that they correctly convert the data into the desired formats. The tests validate the conversion logic alone.</p>
            </dd>
            <dt>Integration tests</dt>
            <dd>
              <p>These tests<a contenteditable="false" data-type="indexterm" data-primary="integration tests" id="id512"/><a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="integration tests" id="id513"/><a contenteditable="false" data-type="indexterm" data-primary="security considerations" data-secondary="testing" id="id514"/> focus on verifying the interactions between software modules, ensuring proper communication and data exchange. Integration tests are relatively fast, often conducted after unit testing, and, like unit tests, help identify issues early. An integration test for the same weather app might focus on how the data fetching and processing modules interact. These tests could verify that the app correctly retrieves and handles weather data from the API, including error scenarios, using partial mocking to simulate real-world API responses. Unlike unit tests, which isolate components, integration tests assess how multiple components work together. Integration tests that are used early in the pipeline, such as in our example pipeline, should avoid slow operations such as accessing a database, file system, or other external systems.</p>
            </dd>
          </dl>
          <p>Unit and integration test frameworks are numerous and vary by language, for <span class="keep-together">example</span>:</p>
          <dl>
            <dt>Java</dt>
            <dd>
              <p>JUnit 5 and TestNG are frameworks<a contenteditable="false" data-type="indexterm" data-primary="Java" data-secondary="automated testing" id="id515"/><a contenteditable="false" data-type="indexterm" data-primary="JUnit 5 and TestNG frameworks, Java" id="id516"/><a contenteditable="false" data-type="indexterm" data-primary="TestNG framework, Java" id="id517"/> for unit testing. Mockito and Spring are used for Java integration testing.</p>
            </dd>
            <dt>JavaScript</dt>
            <dd>
              <p>Jest and Mocha for JavaScript<a contenteditable="false" data-type="indexterm" data-primary="Jest and Mocha, JavaScript testing" id="id518"/><a contenteditable="false" data-type="indexterm" data-primary="JavaScript" data-secondary="Jest and Mocha for testing" id="id519"/> are widely used for unit testing. Jest also supports integration testing.</p>
            </dd>
            <dt>Python</dt>
            <dd>
              <p>pyTest and pyUnit (UnitTest)<a contenteditable="false" data-type="indexterm" data-primary="pyTest and pyUnit (UnitTest), Python" id="id520"/> are options for both unit and integration testing.</p>
            </dd>
            <dt>.NET</dt>
            <dd>
              <p>NUnit and xUnit for .NET<a contenteditable="false" data-type="indexterm" data-primary="NUnit and xUnit, .NET framework" id="id521"/><a contenteditable="false" data-type="indexterm" data-primary=".NET frameworks" data-primary-sortas="NET frameworks" id="id522"/> are options for unit testing, whereas Moq and <span class="keep-together">NSubstitute</span> are commonly used for integration testing.</p>
            </dd>
            <dt>Ruby</dt>
            <dd>
              <p>RSpec<a contenteditable="false" data-type="indexterm" data-primary="RSpec, Ruby testing" id="id523"/><a contenteditable="false" data-type="indexterm" data-primary="Ruby, RSpec for testing" id="id524"/> supports both unit and integration testing for Ruby.</p>
            </dd>
            <dt>Mobile (iOS/Android)</dt>
            <dd>
              <p>XCTest for iOS<a contenteditable="false" data-type="indexterm" data-primary="iOS, XCTest for" id="id525"/><a contenteditable="false" data-type="indexterm" data-primary="mobile testing tools" id="id526"/> and Espresso for Android<a contenteditable="false" data-type="indexterm" data-primary="Android, Expresso for testing" id="id527"/><a contenteditable="false" data-type="indexterm" data-primary="Expresso for testing Android" id="id528"/> are standard bearers for mobile unit and integration testing.</p>
            </dd>
          </dl>
          <p>Unit and integration tests act as a first line of defense, alerting developers to potential bugs or regressions in their code. These quick, automated checks are just the beginning of our testing strategy. In <a data-type="xref" href="ch04.html#chapter_4_deploying_to_test_environments_1749354010445896">Chapter 4</a>, we’ll look at a subsequent pipeline that is triggered when the PR is closed and merged.</p>
          <p>Thoroughly testing each unit of code, including all possible scenarios, results in a large but crucial suite of tests—even for seemingly simple code. However, since unit tests are isolated and don’t rely on external resources, they execute rapidly.</p>
          <p>Our pipeline prioritizes these speedy unit tests as the foundation, followed by integration tests that verify how different components work together, and finally, a smaller number of comprehensive end-to-end tests that simulate real-world usage.</p>
          <p>In <a data-type="xref" data-xrefstyle="select:nopage" href="#chapter_3_the_test_pyramid_1749354010266825">“The Test Pyramid”</a>, we’ll look at the Test Pyramid framework, which illustrates how to balance different test types for optimal software quality<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_testingCICDpipelinesautomated318931" id="id529"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_automationCICDpipeline318931" id="id530"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_automationoftesting318931" id="id531"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="The Test Pyramid"><div class="sect2" id="chapter_3_the_test_pyramid_1749354010266825">
          <h2>The Test Pyramid</h2>
          <p>The Test Pyramid<strong> </strong>provides a model<a contenteditable="false" data-type="indexterm" data-primary="Test Pyramid" id="id532"/> for structuring our tests strategically, prioritizing different types based on their scope and speed. While the Test Pyramid is sometimes depicted with specific test types at each layer, we prefer to conceptualize layers that encompass broad classes of tests, as shown in <a data-type="xref" href="#chapter_3_figure_3_1749354010256819">Figure 3-3</a>.</p>
          <figure><div id="chapter_3_figure_3_1749354010256819" class="figure">
            <img src="assets/ansd_0303.png" width="600" height="331"/>
            <h6><span class="label">Figure 3-3. </span>Large sets of fast tests make up the base of the Test Pyramid; smaller sets of slower tests form the higher layers</h6>
          </div></figure>
          <p>At the base of our pyramid are pre-deployment tests, which include types like unit tests, integration tests, and static scans. These tests are small and execute quickly. Integration testing<a contenteditable="false" data-type="indexterm" data-primary="integration tests" id="id533"/> can refer to a range of test strategies. Integration tests that don’t interact with external systems like databases and network services are fast and are included at this level. The wide pyramid base reflects that the suite of these types of tests should be large and, ideally, cover the complete codebase. Tests should be designed to provide fast feedback to the developer.</p>
          <p>Moving up the pyramid, we depict the middle layer as including any type of tests that we execute against deployed code in a pre-production, test environment.<strong> </strong>Generally, these tests are typically slower than the ones mentioned above but provide valuable insights into how the system functions as a whole.</p>
          <p>At the peak of the pyramid, we find manual tests.<strong> </strong>These are slow and labor-intensive and occur after the code has been vetted by layers of automated testing. </p>
          <p>Embracing the pyramid approach allows teams to balance speed, cost, and effectiveness in their testing efforts. By focusing on a solid foundation of small and fast tests and supplementing them with strategic testing against deployed code, we can achieve comprehensive test coverage while minimizing the time and resources required.</p>
          <p>A robust testing strategy is key to a streamlined pipeline, accelerating the delivery of high-quality releases. In <a data-type="xref" data-xrefstyle="select:nopage" href="#chapter_3_continuous_integration_tools_1749354010266880">“Continuous Integration Tools”</a> we’ll consider how the CI tool choice can prioritize that factor<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_CICDpipelineCIprocess363295" id="id534"/>.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Continuous Integration Tools"><div class="sect1" id="chapter_3_continuous_integration_tools_1749354010266880">
        <h1>Continuous Integration Tools</h1>
        <p>Effective CI processes<a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="CI tools" id="xi_CICDpipelineCItools324834"/> are essential for modern development teams. In this section, we’ll look at legacy CI tools and the features that characterize modern tools.</p>
        <p>A major national retailer—a client of ours—anticipating a surge in digital demand found itself at a crossroads. Its legacy CI/CD tools, including Jenkins, were fragmented across client web, mobile, and backend service teams, causing long build times that cost the company a staggering $500,000 annually in idle developer time. These tools not only stifled innovation but also posed significant security risks, further exacerbated by the $800,000 spent yearly on maintenance and custom scripts. This substantial investment diverted resources away from enhancing the customer experience. Faced with mounting challenges and escalating costs, the retailer sought a unified CI/CD platform to streamline operations, accelerate innovation, and fortify security.</p>
        <p>The company’s compounding challenges shed light on the inherent limitations of Jenkins, especially as organizations scale and digital demands intensify. Let’s look at some of those limitations.</p>
        <section data-type="sect2" data-pdf-bookmark="Jenkins Considered"><div class="sect2" id="chapter_3_jenkins_considered_1749354010266933">
          <h2>Jenkins Considered</h2>
          <p>Jenkins<a contenteditable="false" data-type="indexterm" data-primary="Jenkins" id="xi_Jenkins325321"/> deserves credit for bringing continuous integration into the mainstream. An open source automation server, Jenkins leverages a vast ecosystem of plug-ins that extend its functionality and features and give users the ability to customize their pipelines endlessly. The Jenkins plug-in marketplace is a central repository where users can find and install thousands of these community-developed plug-ins. The Jenkins community is large and its documentation is extensive. It is an adaptable solution for diverse development environments.</p>
          <p>While Jenkins remains valuable for legacy systems due to its specialized plug-ins (e.g., mainframes), modern CI pipelines demand more. Today’s development environments require CI tools that deliver speed, security, collaborative workflows, and native integration with cloud technologies across multiple providers, Kubernetes orchestration, and containerized applications. The following sections explore specific challenges that make Jenkins less suitable for these modern requirements.</p>
          <section data-type="sect3" data-pdf-bookmark="Plug-in complexity"><div class="sect3" id="chapter_3_plug_in_complexity_1749354010266980">
            <h3>Plug-in complexity</h3>
            <p>The flexibility and extensive plug-in<a contenteditable="false" data-type="indexterm" data-primary="plug-ins, complexity of Jenkins" id="xi_pluginscomplexityofJenkins325759"/> ecosystem  of Jenkins often leads to a complex and fragmented architecture, hindering maintainability and increasing developer toil. The reliance on Groovy scripts<a contenteditable="false" data-type="indexterm" data-primary="Groovy scripts" id="id535"/> for pipeline customization can make troubleshooting and updates cumbersome, especially as the number of pipelines and their complexity grows. </p>
            <p>In addition, modern CI/CD solutions often embrace the “pipeline-as-code” paradigm<a contenteditable="false" data-type="indexterm" data-primary="“pipeline-as-code” paradigm" data-primary-sortas="pipeline-as-code paradigm" id="id536"/>, using declarative languages like YAML to define pipelines. This approach is generally considered more straightforward and maintainable than the scripting-heavy approach of Jenkins. YAML-based pipelines<a contenteditable="false" data-type="indexterm" data-primary="YAML-based pipelines" id="id537"/> are generally more human-readable and easier to maintain (there might be exceptions) than Groovy scripts, which can become complex and harder to debug as pipelines grow in size and complexity. Defining pipelines as code allows them to be stored in VCSs alongside the application code. This ensures that pipeline changes are tracked, reviewed, and auditable, enabling better collaboration among team members. Thus, the pipeline-as-code approach allows for better version control, collaboration, and easier troubleshooting.</p>
            <p>Lastly, the need to manage a multitude of plug-ins, each with its own configuration, introduces maintenance overhead. Team members find themselves spending valuable time on mundane tasks like resolving plug-in conflicts, updating dependencies, and deciphering cryptic error messages. This detracts from the focus on innovation and core development, slowing down innovation and delivering features.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Scalability challenges"><div class="sect3" id="chapter_3_scalability_challenges_1749354010267024">
            <h3>Scalability challenges</h3>
            <p>The architecture of Jenkins, primarily designed for single-server setups, often struggles to scale<a contenteditable="false" data-type="indexterm" data-primary="scalability" data-secondary="challenges for Jenkins" id="id538"/> efficiently as the number of jobs, pipelines, and users increases. This can lead to performance bottlenecks, slower build times, and overall system instability. While Jenkins offers distributed builds and clustering options, setting up and maintaining these solutions can be complex and resource-intensive, requiring specialized expertise and significant overhead. As a result, <a href="https://oreil.ly/6qFLO">scaling Jenkins horizontally</a> to meet the demands of large organizations or high-throughput CI/CD workflows often becomes a major challenge.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Security concerns"><div class="sect3" id="chapter_3_security_concerns_1749354010267069">
            <h3>Security concerns</h3>
            <p>While Jenkins plug-ins provide extensibility, they also introduce potential vulnerabilities<a contenteditable="false" data-type="indexterm" data-primary="security considerations" data-secondary="for Jenkins plug-ins" data-secondary-sortas="Jenkins plug-ins" id="id539"/>. Each plug-in, with its own codebase and dependencies, expands the attack surface of a Jenkins instance. Monitoring these plug-ins for vulnerabilities and ensuring timely updates becomes ongoing overhead for administrators. Furthermore, configuring Jenkins security, including user permissions, access controls, and network configurations, can be intricate. Misconfigurations can expose the system to unauthorized access or malicious activities. The dynamic nature of the plug-in ecosystem and the potential for misconfigurations mean you must be vigilant in monitoring risks and proactive in mitigating risks within your Jenkins environment.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Resource usage and efficiency concerns"><div class="sect3" id="chapter_3_resource_usage_and_efficiency_concerns_1749354010267115">
            <h3>Resource usage and efficiency concerns</h3>
            <p>Jenkins’s resource<a contenteditable="false" data-type="indexterm" data-primary="resource usage challenge for Jenkins" id="id540"/> consumption can be a significant drawback, especially as the number of jobs and plug-ins increases. The Java-based architecture<a contenteditable="false" data-type="indexterm" data-primary="Java-based architecture, Jenkins’s efficiency concerns" id="id541"/> (JVM’s runtime requirements, garbage collection behavior, and framework abstractions) often leads to high memory usage, and managing numerous concurrent builds can put a strain on CPU and disk resources. This can result in slower build times, increased infrastructure costs, and potential performance issues. In larger environments, scaling Jenkins horizontally can become complex and resource-intensive, requiring additional hardware and careful configuration.</p>
            <p>In addition, building Docker images<a contenteditable="false" data-type="indexterm" data-primary="Docker" id="id542"/> in CI pipelines can quickly become resource-intensive and expensive, particularly when dealing with large codebases or frequent commits that trigger numerous parallel builds. Each image requires computational resources, storage space, and network bandwidth—costs that multiply across environments and branches. Similarly, while comprehensive observability<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="Jenkins in CI pipelines" id="id543"/><a contenteditable="false" data-type="indexterm" data-primary="logs and logging" data-secondary="Jenkins challenges in CI pipelines" id="id544"/> provides valuable system insights, implementing excessive logging can create its own problems: storage costs surge, signal-to-noise ratios decrease, and processing overhead increases. Finding the right balance between comprehensive coverage and resource efficiency remains a critical challenge<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_pluginscomplexityofJenkins325759" id="id545"/>.</p>
          </div></section>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Beyond Jenkins"><div class="sect2" id="chapter_3_beyond_jenkins_1749354010267161">
          <h2>Beyond Jenkins</h2>
          <p>Due to the limitations of Jenkins, companies like our national retailer often outgrow it and seek modern, fully managed solutions<a contenteditable="false" data-type="indexterm" data-primary="fully-managed solutions, CI/CD" id="xi_fullymanagedsolutionsCICD3277138"/> that offer:</p>
          <dl>
            <dt>Built-in, fully supported building blocks</dt>
            <dd>
              <p>Modern CI/CD tools offer extensive libraries of built-in, fully supported building blocks<a contenteditable="false" data-type="indexterm" data-primary="building blocks, built-in for CI/CD pipelines" id="id546"/> that streamline pipeline setup. This eliminates reliance on community-maintained plug-ins, ensuring reliability and stability. However, recognizing the need for customization, most solutions still support extensibility through custom plug-ins. This empowers teams to automate unique workflows and tailor the CI/CD environment to their specific needs.</p>
            </dd>
            <dt>Pipelines define declaratively</dt>
            <dd>
              <p>Modern CI/CD tools streamline pipeline<a contenteditable="false" data-type="indexterm" data-primary="pipelines" data-seealso="CI/CD pipeline" id="id547"/> definition using declarative code like YAML, making them more accessible and easier to maintain than the Groovy scripts  for Jenkins. This accelerates setup and minimizes errors associated with manual scripting.</p>
            </dd>
            <dt>Native support for containerization and orchestration</dt>
            <dd>
              <p>Jenkins predates the widespread adoption<a contenteditable="false" data-type="indexterm" data-primary="containers and containerization" data-secondary="tools to support" id="id548"/> of Docker<a contenteditable="false" data-type="indexterm" data-primary="Docker" id="id549"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" id="id550"/> and Kubernetes, and while Jenkins pipelines can use plug-ins to work with and orchestrate containers, the lack of native support often results in cumbersome configurations. Newer tools, in contrast, seamlessly incorporate containerization and orchestration features, simplifying the deployment and management of applications in containerized environments.</p>
            </dd>
          </dl>
          <p>In the next sections, we’ll look at additional modern features that tools newer than Jenkins offer. Before we turn our attention to these features, let’s consider a fundamental question when considering CI/CD tools: whether to host and manage tools yourself or select a fully managed solution. The decision will impact everything from development velocity and cost-effectiveness to maintenance requirements. Given the importance of mobile, it’s essential to select a CI/CD setup that handles the complexities of building and deploying mobile applications and we’ll look at the factors specific to mobile app development<a contenteditable="false" data-type="indexterm" data-primary="mobile app development, CI/CD pipeline" id="xi_mobileappdevelopmentCICDpipeline3292630"/> to consider.</p>
          <section data-type="sect3" data-pdf-bookmark="Hosting options"><div class="sect3" id="chapter_3_hosting_options_1749354010267208">
            <h3>Hosting options</h3>
            <p>Organizations have three primary build infrastructure choices for their CI/CD systems: self-hosted on-premises, self-hosted cloud, and vendor-hosted (cloud). Each option presents unique benefits and drawbacks that should be carefully considered:</p>
            <dl>
              <dt>Self-hosted, on-prem solutions</dt>
              <dd>
                <p>Self-hosting a CI/CD system on-premises<a contenteditable="false" data-type="indexterm" data-primary="self-hosted, on-premises solutions, CI/CD pipelines" id="id551"/> gives you complete control and ownership over its infrastructure and data. This approach allows for maximum customization, enabling tailoring to specific security protocols and organizational needs. Additionally, some organizations may prefer the one-time payment model associated with on-prem solutions. However, this approach comes with several drawbacks. It necessitates substantial up-front investment in hardware and software, as well as time and effort to maintain and update. The demand for ongoing maintenance and potential scalability challenges can strain resources, particularly for smaller organizations.</p>
              </dd>
              <dt>Self-hosted, cloud solutions </dt>
              <dd>
                <p>The self-managed, cloud-hosted model<a contenteditable="false" data-type="indexterm" data-primary="self-managed, cloud-hosted solutions, CI/CD pipelines" id="id552"/><a contenteditable="false" data-type="indexterm" data-primary="cloud-hosted solution, CI/CD pipelines" id="id553"/> strikes a balance between control and scalability. Organizations maintain control over their CI/CD software while leveraging the cloud’s flexibility and scalability. This approach reduces the need for physical hardware and simplifies scaling compared to on-prem solutions. </p>
              </dd>
              <dd>
                <p>Cloud-hosted applications run within virtualized environments called hypervisors<a contenteditable="false" data-type="indexterm" data-primary="hypervisors" id="id554"/>, and when considering cloud hosting, the type of hypervisor you select will impact simplicity and performance. The two types of hypervisors to understand are:</p>
                <dl>
                  <dt>Type 1 bare-metal hypervisor</dt>
                  <dd>
                    <p>These run directly on the hardware<a contenteditable="false" data-type="indexterm" data-primary="bare-metal hypervisors" id="id555"/>, offering superior performance and isolation but requiring dedicated hardware. </p>
                  </dd>
                  <dt>Type 2, embedded hypervisors</dt>
                  <dd>
                    <p>These run on top of an operating system<a contenteditable="false" data-type="indexterm" data-primary="embedded hypervisors" id="id556"/>, providing easier setup and flexibility but potentially with lower performance. </p>
                  </dd>
                </dl>
              </dd>
            
              <dd>
                <p>Bare metal might be better for demanding, high-security setups, while embedded could be suitable for less intensive needs and budget constraints.</p>
              </dd>
              <dd>
                <p>Any cloud-hosted toolset will require ongoing maintenance and updates, and your organization will remain responsible for managing the cloud infrastructure. This can lead to challenges similar to those of on-prem solutions, albeit with potentially reduced up-front costs.</p>
              </dd>
              <dt>Fully managed, vendor-hosted solutions</dt>
              <dd>
                <p>Vendor-hosted CI/CD solutions offer a fully managed service<a contenteditable="false" data-type="indexterm" data-primary="vendor-hosted solutions, CI/CD pipelines" id="id557"/> where the vendor handles infrastructure, maintenance, and updates. Your organization focuses on development rather than infrastructure management. These solutions are highly scalable, easy to use, and often follow a pay-as-you-go model, making them cost-effective. However, they may offer less customization than self-hosted options and potentially limit your organization’s ability to tailor the system to your specific needs. Additionally, concerns about data security and potential vendor lock-in can arise with this approach.</p>
              </dd>
            </dl>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Mobile app development–specific challenges"><div class="sect3" id="chapter_3_mobile_app_development_specific_challenges_1749354010267259">
            <h3>Mobile app development–specific challenges</h3>
            <p>Having a robust and efficient CI/CD solution is crucial to keep pace with the fast release cycles and high-quality apps that mobile users expect. Developing for mobile brings unique challenges: your processes and your CI/CD tools must be able to manage device fragmentation and frequent mobile OS updates.</p>
            <p>When choosing between self-hosted and fully managed CI/CD solutions, consider that self-hosted solutions, while offering control and customization, can lead to challenges like physical hardware constraints. In addition, your team will be responsible for constant maintenance and updates to build environments. These complexities can lead to unexpected costs. The frequent release cycles of tools like Xcode for iOS development necessitate regular hardware updates, which can be a significant time and resource drain for any team.</p>
            <p>Fully managed CI/CD solutions, on the other hand, alleviate these pain points by providing automatic updates to build environments and predictable costs. This allows your team to focus on building features and improving their apps rather than managing infrastructure. Moreover, fully managed CI/CD solutions specifically optimized for mobile development offer mobile-specific integrations and features that streamline the development process. Many of these platforms fully manage challenges of mobile development, such as device fragmentation and OS updates, for you<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_Jenkins325321" id="id558"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_fullymanagedsolutionsCICD3277138" id="id559"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_mobileappdevelopmentCICDpipeline3292630" id="id560"/>.</p>
          </div></section>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Modern Features to Accelerate Software Builds"><div class="sect2" id="chapter_3_modern_features_to_accelerate_software_builds_1749354010267311">
          <h2>Modern Features to Accelerate Software Builds</h2>
          <p>Returning to our retailer: it researched newer options and decided to move on from Jenkins and the set of plug-ins and tools pieced together to work with it. The company selected a unified platform that simplified its toolset while providing the scalability and cost savings that it required. It was able to consolidate CI/CD processes for services, client web, and mobile teams onto this single platform. The new platform eliminated the need for extensive scripting, saving developers time and enabling them to focus on innovation. It also leveraged AI/ML for testing, resulting in further cost savings and much faster builds. Furthermore, a unified platform improved security by supporting security testing early in the pipeline, enabling faster detection and remediation of vulnerabilities. The efficiency, security, and reliability of the new platform enabled the retailer to easily handle its digital growth.</p>
          <p>In the next sections, we will look at features in modern systems that enable faster, cost-effective, and secure pipelines.</p>
          <section data-type="sect3" data-pdf-bookmark="Accelerate builds with caching"><div class="sect3" id="chapter_3_accelerate_builds_with_caching_1749354010267359">
            <h3>Accelerate builds with caching</h3>
            <p>Modern build environments are ephemeral, enhancing agility by providing isolated, cost-effective, and scalable setups that accelerate development cycles while maintaining consistency across stages of the CI/CD pipeline. However, ephemeral environments require setting up the entire build process from scratch each time, including downloading dependencies, compiling code, and generating artifacts. This is time-consuming.</p>
            <p>Caching<a contenteditable="false" data-type="indexterm" data-primary="caching" id="id561"/> is a technique used in CI/CD to store and reuse build artifacts, dependencies, Docker layers, and intermediate results. This significantly reduces build times by avoiding redundant operations and focusing on building only what has changed, which not only speeds up development cycles but also conserves computational resources and energy. Modern CI/CD systems intelligently manage this caching process, optimizing builds without manual intervention. Caching can be done at different stages—caching software dependencies, caching Docker layers, and caching build outputs from tools like Bazel, Gradle, and Maven.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Streamline building, caching, and testing with AI"><div class="sect3" id="chapter_3_streamline_building_caching_and_testing_with_ai_1749354010267410">
            <h3>Streamline building, caching, and testing with AI</h3>
            <p>An AI-native CI solution<a contenteditable="false" data-type="indexterm" data-primary="AI-native software delivery" data-secondary="GenAI/agentic AI/MCP integration" id="xi_AInativesoftwaredeliveryGenAIagenticAIMCPintegration334940"/><a contenteditable="false" data-type="indexterm" data-primary="agentic AI" data-secondary="integration with GenAI and MCP for CI/CD pipeline" id="xi_agenticAIintegrationwithGenAIandMCPforCICDpipeline334940"/> will seamlessly integrate GenAI, agentic AI, and MCP to enhance building the software, caching required components, and testing each build. Let’s look at these enhancements in more detail.</p>
            <section data-type="sect4" data-pdf-bookmark="Build phase enhancements"><div class="sect4" id="chapter_3_build_phase_enhancements_1749354010267458">
              <h4>Build phase enhancements</h4>
              <p>GenAI can automate boilerplate code creation<a contenteditable="false" data-type="indexterm" data-primary="build phase, AI enhancements for CI/CD pipeline" id="id562"/> for repetitive tasks (e.g., Dockerfile templates, CI configuration files), reducing manual effort. It can also analyze historical build data to predict dependency conflicts and suggest optimal versions, minimizing build failures. Another interesting use case for GenAI is generating optimized CI pipeline YAML configurations<a contenteditable="false" data-type="indexterm" data-primary="YAML-based pipelines" id="id563"/> based on project structure, reducing trial-and-error setups.</p>
              <p>Agentic AI<a contenteditable="false" data-type="indexterm" data-primary="agentic AI" data-secondary="build failure detection and resource scaling for CI/CD pipelines" id="id564"/><a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="AI’s role in" id="id565"/><a contenteditable="false" data-type="indexterm" data-primary="scalability" data-secondary="resource scaling for CI/CD pipelines" id="id566"/> can detect build failures (e.g., missing dependencies), and can then automatically retry with corrected configurations and log root causes. It can also dynamically scale build resources (e.g., cloud instances) based on workload demands, balancing speed and cost, and can dynamically split monolithic builds into parallelizable tasks, reducing execution time.</p>
              <p>MCP<a contenteditable="false" data-type="indexterm" data-primary="Model Context Protocol (MCP)" id="id567"/><a contenteditable="false" data-type="indexterm" data-primary="MCP (Model Context Protocol)" id="id568"/> can standardize environment variables, build flags, and toolchain versions across distributed teams, ensuring consistency and sharing prebuilt artifacts, such as compiled libraries, between related projects via MCP’s centralized cache, avoiding redundant builds.</p>
            </div></section>
            <section data-type="sect4" data-pdf-bookmark="Cache phase enhancements"><div class="sect4" id="chapter_3_cache_phase_enhancements_1749354010267503">
              <h4>Cache phase enhancements</h4>
              <p>GenAI can be used to make the caching<a contenteditable="false" data-type="indexterm" data-primary="generative AI (GenAI)" data-secondary="caching techniques for CI/CD pipeline" id="id569"/> techniques more intelligent. It can predict which dependencies (e.g., <em>node_modules</em>, <em>.m2</em> artifacts) will be needed based on code changes, precaching them before builds start. ML models can be used to identify stale caches by analyzing code diff patterns, ensuring only relevant artifacts are retained. Agentic AI<a contenteditable="false" data-type="indexterm" data-primary="agentic AI" data-secondary="caching enhancements" id="id570"/> can flag and purge poisoned caches<a contenteditable="false" data-type="indexterm" data-primary="poisoned caches" id="id571"/> (e.g., corrupted artifacts) in real time, preventing failed builds.</p>
              <p>Using MCP in scalable infrastructure has many advantages, including enabling secure, low-latency cache sharing across CI pipelines via standardized APIs, and reducing redundant data transfers by caching intermediate build outputs (e.g., Docker layers) between CI runs. MCP can enable secure cache sharing between parallel CI jobs through standardized APIs, eliminating redundant builds in monorepo architectures<a contenteditable="false" data-type="indexterm" data-primary="monorepo architectures" id="id572"/>.</p>
            </div></section>
            <section data-type="sect4" data-pdf-bookmark="Test phase enhancements"><div class="sect4" id="chapter_3_test_phase_enhancements_1749354010267554">
              <h4>Test phase enhancements</h4>
              <p>Consider a scenario<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="CI/CD pipeline’s automated" id="id573"/> where a developer modifies a single line of code in a seldom-used component within a large application. We have high code coverage with our large and robust set of unit tests; these are the foundation of our test strategy, the base of our Test Pyramid. Yet, when little code has changed, executing the entire test suite results in lengthy, resource-intensive, and very inefficient test cycles.</p>
              <p>Modern tools can mitigate these issues with AI tooling that intelligently selects and executes only the tests directly relevant to the modified code. This approach significantly reduces the time and resources required for testing, leading to faster feedback loops and more efficient development processes.</p>
              <p><a href="https://oreil.ly/_-jPi">Harness Test Intelligence (TI)</a> is an example<a contenteditable="false" data-type="indexterm" data-primary="Harness" data-secondary="Test Intelligence (TI)" id="id574"/> of this approach. Let’s look at how TI works under the hood. Three components work together to enable Harness TI:</p>
              <dl>
                <dt>TI service</dt>
                <dd>
                  <p>This service uses AI and understands your repository, Git commits, and unit tests and uses this data to dynamically build a graph that maps the relationships between code methods and their corresponding unit tests. This graph is continuously updated to reflect changes in the codebase.</p>
                </dd>
                <dt>A test runner agent</dt>
                <dd>
                  <p>This component communicates with the service and executes tests.</p>
                </dd>
                <dt>A test step</dt>
                <dd>
                  <p>This is the step you add to your CI pipeline to integrate TI into your workflow. </p>
                </dd>
              </dl>
              <p>The TI workflow begins when a developer initiates a pull request and triggers the pipeline. The TI service analyzes the code changes and compares them to its graph to identify the tests that need to be executed. It considers not only the code modifications but also any changes or additions to the tests themselves. This ensures that all relevant aspects of the codebase are thoroughly tested while avoiding redundant test runs.</p>
              <p>Thus, by focusing on the impacted tests, intelligent testing approaches can significantly reduce the testing time, especially in large projects with extensive test suites. This translates to faster builds and faster feedback for developers, allowing them to identify and address issues more quickly<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_AInativesoftwaredeliveryGenAIagenticAIMCPintegration334940" id="id575"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_agenticAIintegrationwithGenAIandMCPforCICDpipeline334940" id="id576"/>. </p>
            </div></section>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="AI-powered build and test insights"><div class="sect3" id="chapter_3_ai_powered_build_and_test_insights_1749354010267602">
            <h3>AI-powered build and test insights</h3>
            <p>Modern CI/CD tools also leverage GenAI<a contenteditable="false" data-type="indexterm" data-primary="generative AI (GenAI)" data-secondary="providing insights in build and test phases" id="id577"/> to automate tedious tasks and provide insights when things go wrong. For example, a tool can autogenerate your pipelines, analyze code for potential issues, and troubleshoot build and deployment failures in real time. If a CI build fails, GenAI can analyze log files, pinpoint the error, and even suggest potential fixes. This saves your time, reduces downtime, and accelerates the software delivery process. </p>
            <p>Agentic AI<a contenteditable="false" data-type="indexterm" data-primary="agentic AI" data-secondary="optimization recommendations for CI/CD pipelines" id="id578"/> can also be used to come up with recommendations to optimize existing pipelines based on your organization’s golden standards. This feature would be extremely valuable since organizations, more often than not, optimize their current pipelines rather than create new pipelines.</p>
            <p>Another excellent use case for GenAI is writing intent-based tests<a contenteditable="false" data-type="indexterm" data-primary="intent-based tests" id="id579"/><a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="intent-based tests" id="id580"/><a contenteditable="false" data-type="indexterm" data-primary="security considerations" data-secondary="testing" id="id581"/>. Testing, especially UI testing, can be extremely manual and flaky if the UI changes. By using GenAI, developers and QA engineers can simply state the intent of a test and let GenAI figure out the steps. We will discuss intent-based testing in detail in <a data-type="xref" href="ch04.html#chapter_4_deploying_to_test_environments_1749354010445896">Chapter 4</a>.</p>
            <p>Finally, AI can also be used to generate data for tests<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="AI generating ethical and responsible data for" id="id582"/> ethically and responsibly. Some examples include ensuring compliance with GDPR and other regulations when using production data for model training, maintaining data privacy and security throughout the data generation process, and using proper algorithms to generate synthetic data. </p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Unify CI/CD metrics with enterprise observability"><div class="sect3" id="chapter_3_unify_ci_cd_metrics_with_enterprise_observability_1749354010267650">
            <h3>Unify CI/CD metrics with enterprise observability</h3>
            <p>A modern CI/CD solution should be a team player, working with the other key platforms in your corporate ecosystem, particularly the observability<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="unifying CI/CD metrics with" id="id583"/><a contenteditable="false" data-type="indexterm" data-primary="metrics" data-secondary="and observability" data-secondary-sortas="observability" id="id584"/> platform that your organization relies on to understand system behavior, identify performance bottlenecks, and proactively detect and resolve issues before they impact users or business operations. Observability platforms include Elastic with Logstash<a contenteditable="false" data-type="indexterm" data-primary="Logstash" id="id585"/> and Kibana<a contenteditable="false" data-type="indexterm" data-primary="Kibana" id="id586"/>, a popular open source platform, and Datadog<a contenteditable="false" data-type="indexterm" data-primary="Datadog" id="id587"/> and Splunk<a contenteditable="false" data-type="indexterm" data-primary="Splunk" id="id588"/>, well-known commercial options. </p>
            <p>Modern continuous integration tools provide telemetry data<a contenteditable="false" data-type="indexterm" data-primary="telemetry data, CI/CD pipeline" id="id589"/><a contenteditable="false" data-type="indexterm" data-primary="OpenTelemetry" id="id590"/> to these platforms by implementing OpenTelemetry, an open source framework. This brings in CI/CD metrics to enable observability and dashboards that can help you understand what’s happening and improve build performance and reliability.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Modern CI/CD support for monorepos"><div class="sect3" id="chapter_3_modern_ci_cd_support_for_monorepos_1749354010267696">
            <h3>Modern CI/CD support for monorepos</h3>
            <p>Versioning and dependency management<a contenteditable="false" data-type="indexterm" data-primary="versioning and dependency management" id="id591"/> become very challenging when managing complex codebases across several repositories. Monorepos<a contenteditable="false" data-type="indexterm" data-primary="monorepo architectures" id="id592"/> are single repositories that contain all the code for a project or organization, providing a centralized approach to managing complex codebases. A single repository simplifies dependency <span class="keep-together">management</span> by keeping a single copy of any shared library or component, and simplifies code sharing and reuse across different projects. While monorepos increase the risk of merge conflicts and require careful design to avoid tightly coupled code, many large companies have successfully adopted them for massive codebases, demonstrating that an effectively managed monorepo can provide a very scalable approach.</p>
            <p>When adopting a monorepo strategy, it’s important to understand the unique requirements that monorepos make of code repositories<a contenteditable="false" data-type="indexterm" data-primary="code repositories" data-secondary="and monorepos" data-secondary-sortas="monorepos" id="id593"/> and CI tools. With potentially hundreds of developers contributing to a large monorepo, managing changes and pull requests efficiently becomes critical. Teams must be able to define appropriate access by subdirectories, in part to ensure that only relevant reviewers are notified for each change. Repositories should support subdirectory-specific ownership.</p>
            <p>Monorepos require CI systems that enable selective building and testing of changed components and that support advanced dependency management, caching, and parallel execution. Tools like Harness CI<a contenteditable="false" data-type="indexterm" data-primary="Harness" data-secondary="CI" id="id594"/> support these needs through features like path-based triggers<a contenteditable="false" data-type="indexterm" data-primary="path-based triggers, CI systems for monorepos" id="id595"/>, which run pipelines only when specific directories in the repository change (e.g., triggering service A’s pipeline for changes to <em>serviceA/</em>), and sparse checkout<a contenteditable="false" data-type="indexterm" data-primary="sparse checkout, CI systems for monorepos" id="id596"/>, which clones a subdirectory instead of the entire repository. This optimizes resource usage and speeds up feedback loops while maintaining dependency <span class="keep-together">integrity</span><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_CICDpipelineCItools324834" id="id597"/>.</p>
          </div></section>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="chapter_3_summary_1749354010267741">
        <h1>Summary</h1>
        <p>CI has become an indispensable practice, reducing integration issues, providing faster feedback, and improving overall efficiency. In this chapter, we looked at modern, fully managed CI/CD tool features, contrasting the trade-offs with the costs and challenges of self-hosting. We looked at the importance of prioritizing faster, smaller unit tests for quick feedback, followed by slower test types for comprehensive coverage. The continuous integration pipeline we looked at exemplified this practice: in the context of opening a PR, we build, complete static scans, and then run quick tests to ensure our code does what it should and doesn’t introduce regressions<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_CICDpipeline3474" id="id598"/>. We also explored various ways in which an AI-native CI tool could use GenAI, agentic AI, and MCP to enhance the build, cache, and test phases of CI.</p>
        <p>In <a data-type="xref" href="ch04.html#chapter_4_deploying_to_test_environments_1749354010445896">Chapter 4</a>, we’ll continue with CI/CD and focus on deploying to test environments and executing the slower tests that evaluate the system’s performance, resiliency, and end-to-end behavior.</p>
      </div></section>
    </div></section></div></div></body></html>