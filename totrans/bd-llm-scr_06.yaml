- en: 7 Fine-tuning to follow instructions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The instruction fine-tuning process of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a dataset for supervised instruction fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing instruction data in training batches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading a pretrained LLM and fine-tuning it to follow human instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting LLM-generated instruction responses for evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating an instruction-fine-tuned LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Previously, we implemented the LLM architecture, carried out pretraining, and
    imported pretrained weights from external sources into our model. Then, we focused
    on fine-tuning our LLM for a specific classification task: distinguishing between
    spam and non-spam text messages. Now we’ll implement the process for fine-tuning
    an LLM to follow human instructions, as illustrated in figure 7.1\. Instruction
    fine-tuning is one of the main techniques behind developing LLMs for chatbot applications,
    personal assistants, and other conversational tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1 The three main stages of coding an LLM. This chapter focuses on
    step 9 of stage 3: fine-tuning a pretrained LLM to follow human instructions.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Figure 7.1 shows two main ways of fine-tuning an LLM: fine-tuning for classification
    (step 8) and fine-tuning an LLM to follow instructions (step 9). We implemented
    step 8 in chapter 6\. Now we will fine-tune an LLM using an *instruction dataset*.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Introduction to instruction fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now know that pretraining an LLM involves a training procedure where it learns
    to generate one word at a time. The resulting pretrained LLM is capable of *text
    completion*, meaning it can finish sentences or write text paragraphs given a
    fragment as input. However, pretrained LLMs often struggle with specific instructions,
    such as “Fix the grammar in this text” or “Convert this text into passive voice.”
    Later, we will examine a concrete example where we load the pretrained LLM as
    the basis for *instruction fine-tuning*, also known as *supervised instruction
    fine-tuning*.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we focus on improving the LLM’s ability to follow such instructions and
    generate a desired response, as illustrated in figure 7.2\. Preparing the dataset
    is a key aspect of instruction fine-tuning. Then we’ll complete all the steps
    in the three stages of the instruction fine-tuning process, beginning with the
    dataset preparation, as shown in figure 7.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 Examples of instructions that are processed by an LLM to generate
    desired responses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/7-3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3 The three-stage process for instruction fine-tuning an LLM. Stage
    1 involves dataset preparation, stage 2 focuses on model setup and fine-tuning,
    and stage 3 covers the evaluation of the model. We will begin with step 1 of stage
    1: downloading and formatting the dataset.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 7.2 Preparing a dataset for supervised instruction fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s download and format the instruction dataset for instruction fine-tuning
    a pretrained LLM. The dataset consists of 1,100 *instruction–response pairs* similar
    to those in figure 7.2\. This dataset was created specifically for this book,
    but interested readers can find alternative, publicly available instruction datasets
    in appendix B.
  prefs: []
  type: TYPE_NORMAL
- en: The following code implements and executes a function to download this dataset,
    which is a relatively small file (only 204 KB) in JSON format. JSON, or JavaScript
    Object Notation, mirrors the structure of Python dictionaries, providing a simple
    structure for data interchange that is both human readable and machine friendly.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.1 Downloading the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The output of executing the preceding code is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `data` list that we loaded from the JSON file contains the 1,100 entries
    of the instruction dataset. Let’s print one of the entries to see how each entry
    is structured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The content of the example entry is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the example entries are Python dictionary objects containing
    an `''instruction''`, `''input''`, and `''output''`. Let’s take a look at another
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the contents of this entry, the `''input''` field may occasionally
    be empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Instruction fine-tuning involves training a model on a dataset where the input-output
    pairs, like those we extracted from the JSON file, are explicitly provided. There
    are various methods to format these entries for LLMs. Figure 7.4 illustrates two
    different example formats, often referred to as *prompt styles*, used in the training
    of notable LLMs such as Alpaca and Phi-3\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 Comparison of prompt styles for instruction fine-tuning in LLMs.
    The Alpaca style (left) uses a structured format with defined sections for instruction,
    input, and response, while the Phi-3 style (right) employs a simpler format with
    designated `<|user|>` and `<|assistant|>` tokens.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Alpaca was one of the early LLMs to publicly detail its instruction fine-tuning
    process. Phi-3, developed by Microsoft, is included to demonstrate the diversity
    in prompt styles. The rest of this chapter uses the Alpaca prompt style since
    it is one of the most popular ones, largely because it helped define the original
    approach to fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.1 Changing prompt styles
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: After fine-tuning the model with the Alpaca prompt style, try the Phi-3 prompt
    style shown in figure 7.4 and observe whether it affects the response quality
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define a `format_input` function that we can use to convert the entries
    in the `data` list into the Alpaca-style input format.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.2 Implementing the prompt formatting function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This `format_input` function takes a dictionary `entry` as input and constructs
    a formatted string. Let’s test it to dataset entry `data[50]`, which we looked
    at earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The formatted input looks like as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `format_input` skips the optional `###` `Input:` section if the
    `''input''` field is empty, which we can test out by applying the `format_input`
    function to entry `data[999]` that we inspected earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that entries with an empty `''input''` field don’t contain
    an `###` `Input:` section in the formatted input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Before we move on to setting up the PyTorch data loaders in the next section,
    let’s divide the dataset into training, validation, and test sets analogous to
    what we have done with the spam classification dataset in the previous chapter.
    The following listing shows how we calculate the portions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.3 Partitioning the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Use 85% of the data for training'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Use 10% for testing'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Use remaining 5% for validation'
  prefs: []
  type: TYPE_NORMAL
- en: 'This partitioning results in the following dataset sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Having successfully downloaded and partitioned the dataset and gained a clear
    understanding of the dataset prompt formatting, we are now ready for the core
    implementation of the instruction fine-tuning process. Next, we focus on developing
    the method for constructing the training batches for fine-tuning the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Organizing data into training batches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we progress into the implementation phase of our instruction fine-tuning
    process, the next step, illustrated in figure 7.5, focuses on constructing the
    training batches effectively. This involves defining a method that will ensure
    our model receives the formatted training data during the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5 The three-stage process for instruction fine-tuning an LLM. Next,
    we look at step 2 of stage 1: assembling the training batches.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the previous chapter, the training batches were created automatically by
    the PyTorch `DataLoader` class, which employs a default *collate* function to
    combine lists of samples into batches. A collate function is responsible for taking
    a list of individual data samples and merging them into a single batch that can
    be processed efficiently by the model during training.
  prefs: []
  type: TYPE_NORMAL
- en: However, the batching process for instruction fine-tuning is a bit more involved
    and requires us to create our own custom collate function that we will later plug
    into the `DataLoader`. We implement this custom collate function to handle the
    specific requirements and formatting of our instruction fine-tuning dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s tackle the *batching process* in several steps, including coding the custom
    collate function, as illustrated in figure 7.6\. First, to implement steps 2.1
    and 2.2, we code an `InstructionDataset` class that applies `format_input` and
    *pretokenizes* all inputs in the dataset, similar to the `SpamDataset` in chapter
    6\. This two-step process, detailed in figure 7.7, is implemented in the `__init__`
    constructor method of the `InstructionDataset`.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6 The five substeps involved in implementing the batching process:
    (2.1) applying the prompt template, (2.2) using tokenization from previous chapters,
    (2.3) adding padding tokens, (2.4) creating target token IDs, and (2.5) replacing
    `-100` placeholder tokens to mask padding tokens in the loss function.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/7-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 The first two steps involved in implementing the batching process.
    Entries are first formatted using a specific prompt template (2.1) and then tokenized
    (2.2), resulting in a sequence of token IDs that the model can process.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 7.4 Implementing an instruction dataset class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Pretokenizes texts'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the approach used for classification fine-tuning, we want to accelerate
    training by collecting multiple training examples in a batch, which necessitates
    padding all inputs to a similar length. As with classification fine-tuning, we
    use the `<|endoftext|>` token as a padding token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of appending the `<|endoftext|>` tokens to the text inputs, we can
    append the token ID corresponding to `<|endoftext|>` to the pretokenized inputs
    directly. We can use the tokenizer’s `.encode` method on an `<|endoftext|>` token
    to remind us which token ID we should use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The resulting token ID is `50256`.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to step 2.3 of the process (see figure 7.6), we adopt a more sophisticated
    approach by developing a custom collate function that we can pass to the data
    loader. This custom collate function pads the training examples in each batch
    to the same length while allowing different batches to have different lengths,
    as demonstrated in figure 7.8\. This approach minimizes unnecessary padding by
    only extending sequences to match the longest one in each batch, not the whole
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 The padding of training examples in batches using token ID `50256`
    to ensure uniform length within each batch. Each batch may have different lengths,
    as shown by the first and second.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can implement the padding process with a custom collate function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Finds the longest sequence in the batch'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Pads and prepares inputs'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Removes extra padded token added earlier'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Converts the list of inputs to a tensor and transfers it to the target device'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `custom_collate_draft_1` we implemented is designed to be integrated into
    a PyTorch `DataLoader`, but it can also function as a standalone tool. Here, we
    use it independently to test and verify that it operates as intended. Let’s try
    it on three different inputs that we want to assemble into a batch, where each
    example gets padded to the same length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting batch looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This output shows all inputs have been padded to the length of the longest input
    list, `inputs_1`, containing five token IDs.
  prefs: []
  type: TYPE_NORMAL
- en: We have just implemented our first custom collate function to create batches
    from lists of inputs. However, as we previously learned, we also need to create
    batches with the target token IDs corresponding to the batch of input IDs. These
    target IDs, as shown in figure 7.9, are crucial because they represent what we
    want the model to generate and what we need during training to calculate the loss
    for the weight updates. That is, we modify our custom collate function to return
    the target token IDs in addition to the input token IDs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 The five substeps involved in implementing the batching process.
    We are now focusing on step 2.4, the creation of target token IDs. This step is
    essential as it enables the model to learn and predict the tokens it needs to
    generate.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similar to the process we used to pretrain an LLM, the target token IDs match
    the input token IDs but are shifted one position to the right. This setup, as
    shown in figure 7.10, allows the LLM to learn how to predict the next token in
    a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 The input and target token alignment used in the instruction fine-tuning
    process of an LLM. For each input sequence, the corresponding target sequence
    is created by shifting the token IDs one position to the right, omitting the first
    token of the input, and appending an end-of-text token.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following updated collate function generates the target token IDs from
    the input token IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Truncates the last token for inputs'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Shifts +1 to the right for targets'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied to the example `batch` consisting of three input lists we defined earlier,
    the new `custom_collate_draft_2` function now returns the input and the target
    batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The first tensor represents inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The second tensor represents the targets.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we assign a `-100` placeholder value to all padding tokens,
    as highlighted in figure 7.11\. This special value allows us to exclude these
    padding tokens from contributing to the training loss calculation, ensuring that
    only meaningful data influences model learning. We will discuss this process in
    more detail after we implement this modification. (When fine-tuning for classification,
    we did not have to worry about this since we only trained the model based on the
    last output token.)
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 The five substeps involved in implementing the batching process.
    After creating the target sequence by shifting token IDs one position to the right
    and appending an end-of-text token, in step 2.5, we replace the end-of-text padding
    tokens with a placeholder value (`-100`).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, note that we retain one end-of-text token, ID `50256`, in the target
    list, as depicted in figure 7.12\. Retaining it allows the LLM to learn when to
    generate an end-of-text token in response to instructions, which we use as an
    indicator that the generated response is complete.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 Step 2.4 in the token replacement process in the target batch for
    the training data preparation. We replace all but the first instance of the end-of-text
    token, which we use as padding, with the placeholder value `-100`, while keeping
    the initial end-of-text token in each target sequence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the following listing, we modify our custom collate function to replace tokens
    with ID `50256` with `-100` in the target lists. Additionally, we introduce an
    `allowed_ max_length` parameter to optionally limit the length of the samples.
    This adjustment will be useful if you plan to work with your own datasets that
    exceed the 1,024-token context size supported by the GPT-2 model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.5 Implementing a custom batch collate function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Pads sequences to max_length'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Truncates the last token for inputs'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Shifts +1 to the right for targets'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Replaces all but the first padding tokens in targets by ignore_index'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Optionally truncates to the maximum sequence length'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, let’s try the collate function on the sample batch that we created earlier
    to check that it works as intended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows, where the first tensor represents the inputs and
    the second tensor represents the targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The modified collate function works as expected, altering the target list by
    inserting the token ID `-100`. What is the logic behind this adjustment? Let’s
    explore the underlying purpose of this modification.
  prefs: []
  type: TYPE_NORMAL
- en: 'For demonstration purposes, consider the following simple and self-contained
    example where each output logit corresponds to a potential token from the model’s
    vocabulary. Here’s how we might calculate the cross entropy loss(introduced in
    chapter 5) during training when the model predicts a sequence of tokens, which
    is similar to what we did when we pretrained the model and fine-tuned it for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#1 predictions for 1st token'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 predictions for 2nd token'
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss value calculated by the previous code is `1.1269`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As we would expect, adding an additional token ID affects the loss calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New third token ID prediction'
  prefs: []
  type: TYPE_NORMAL
- en: After adding the third token, the loss value is `0.7936`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have carried out some more or less obvious example calculations
    using the cross entropy loss function in PyTorch, the same loss function we used
    in the training functions for pretraining and fine-tuning for classification.
    Now let’s get to the interesting part and see what happens if we replace the third
    target token ID with `-100`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The resulting output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The resulting loss on these three training examples is identical to the loss
    we calculated from the two training examples earlier. In other words, the cross
    entropy loss function ignored the third entry in the `targets_3` vector, the token
    ID corresponding to `-100`. (Interested readers can try to replace the `-100`
    value with another token ID that is not `0` or `1`; it will result in an error.)
  prefs: []
  type: TYPE_NORMAL
- en: So what’s so special about `-100` that it’s ignored by the cross entropy loss?
    The default setting of the cross entropy function in PyTorch is `cross_entropy(...,`
    `ignore_index=-100)`. This means that it ignores targets labeled with `-100`.
    We take advantage of this `ignore_index` to ignore the additional end-of-text
    (padding) tokens that we used to pad the training examples to have the same length
    in each batch. However, we want to keep one `50256` (end-of-text) token ID in
    the targets because it helps the LLM to learn to generate end-of-text tokens,
    which we can use as an indicator that a response is complete.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to masking out padding tokens, it is also common to mask out the
    target token IDs that correspond to the instruction, as illustrated in figure
    7.13\. By masking out the LLM’s target token IDs corresponding to the instruction,
    the cross entropy loss is only computed for the generated response target IDs.
    Thus, the model is trained to focus on generating accurate responses rather than
    memorizing instructions, which can help reduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13 Left: The formatted input text we tokenize and then feed to the
    LLM during training. Right: The target text we prepare for the LLM where we can
    optionally mask out the instruction section, which means replacing the corresponding
    token IDs with the `-100` `ignore_index` value.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As of this writing, researchers are divided on whether masking the instructions
    is universally beneficial during instruction fine-tuning. For instance, the 2024
    paper by Shi et al., “Instruction Tuning With Loss Over Instructions” ([https://arxiv.org/abs/2405.14394](https://arxiv.org/abs/2405.14394)),
    demonstrated that not masking the instructions benefits the LLM performance (see
    appendix B for more details). Here, we will not apply masking and leave it as
    an optional exercise for interested readers.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.2 Instruction and input masking
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: After completing the chapter and fine-tuning the model with `InstructionDataset`,
    replace the instruction and input tokens with the `-100` mask to use the instruction
    masking method illustrated in figure 7.13\. Then evaluate whether this has a positive
    effect on model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Creating data loaders for an instruction dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have completed several stages to implement an `InstructionDataset` class
    and a `custom_collate_fn` function for the instruction dataset. As shown in figure
    7.14, we are ready to reap the fruits of our labor by simply plugging both `InstructionDataset`
    objects and the `custom_collate_fn` function into PyTorch data loaders. These
    loaders will automatically shuffle and organize the batches for the LLM instruction
    fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 The three-stage process for instruction fine-tuning an LLM. Thus
    far, we have prepared the dataset and implemented a custom collate function to
    batch the instruction dataset. Now, we can create and apply the data loaders to
    the training, validation, and test sets needed for the LLM instruction fine-tuning
    and evaluation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Before we implement the data loader creation step, we have to briefly talk about
    the `device` setting of the `custom_collate_fn`. The `custom_collate_fn` includes
    code to move the input and target tensors (for example, `torch.stack(inputs_lst).to
    (device)`) to a specified device, which can be either `"cpu"` or `"cuda"` (for
    NVIDIA GPUs) or, optionally, `"mps"` for Macs with Apple Silicon chips.
  prefs: []
  type: TYPE_NORMAL
- en: Note  Using an `"mps"` device may result in numerical differences compared to
    the contents of this chapter, as Apple Silicon support in PyTorch is still experimental.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we moved the data onto the target device (for example, the GPU memory
    when `device="cuda"`) in the main training loop. Having this as part of the collate
    function offers the advantage of performing this device transfer process as a
    background process outside the training loop, preventing it from blocking the
    GPU during model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code initializes the `device` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Uncomments these two lines to use the GPU on an Apple Silicon chip'
  prefs: []
  type: TYPE_NORMAL
- en: This will either print `"Device:` `cpu"` or `"Device:` `cuda"`, depending on
    your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, to reuse the chosen device setting in `custom_collate_fn` when we plug
    it into the PyTorch `DataLoader` class, we use the `partial` function from Python’s
    `functools` standard library to create a new version of the function with the
    device argument prefilled. Additionally, we set the `allowed_max_length` to `1024`,
    which truncates the data to the maximum context length supported by the GPT-2
    model, which we will fine-tune later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can set up the data loaders as we did previously, but this time, we
    will use our custom collate function for the batching process.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.6 Initializing the data loaders
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '#1 You can try to increase this number if parallel Python processes are supported
    by your operating system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the dimensions of the input and target batches generated by the
    training loader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (truncated to conserve space):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This output shows that the first input and target batch have dimensions 8 ×
    61, where 8 represents the batch size and 61 is the number of tokens in each training
    example in this batch. The second input and target batch have a different number
    of tokens—for instance, 76\. Thanks to our custom collate function, the data loader
    is able to create batches of different lengths. In the next section, we load a
    pretrained LLM that we can then fine-tune with this data loader.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Loading a pretrained LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have spent a lot of time preparing the dataset for instruction fine-tuning,
    which is a key aspect of the supervised fine-tuning process. Many other aspects
    are the same as in pretraining, allowing us to reuse much of the code from earlier
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Before beginning instruction fine-tuning, we must first load a pretrained GPT
    model that we want to fine-tune (see figure 7.15), a process we have undertaken
    previously. However, instead of using the smallest 124-million-parameter model
    as before, we load the medium-sized model with 355 million parameters. The reason
    for this choice is that the 124-million-parameter model is too limited in capacity
    to achieve satisfactory results via instruction fine-tuning. Specifically, smaller
    models lack the necessary capacity to learn and retain the intricate patterns
    and nuanced behaviors required for high-quality instruction-following tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 The three-stage process for instruction fine-tuning an LLM. After
    the dataset preparation, the process of fine-tuning an LLM for instruction-following
    begins with loading a pretrained LLM, which serves as the foundation for subsequent
    training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Loading our pretrained models requires the same code as when we pretrained the
    data (section 5.5) and fine-tuned it for classification (section 6.4), except
    that we now specify `"gpt2-medium` `(355M)"` instead of `"gpt2-small` `(124M)"`.
  prefs: []
  type: TYPE_NORMAL
- en: Note  Executing this code will initiate the download of the medium-sized GPT
    model, which has a storage requirement of approximately 1.42 gigabytes. This is
    roughly three times larger than the storage space needed for the small model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.7 Loading the pretrained model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing the code, several files will be downloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s take a moment to assess the pretrained LLM’s performance on one
    of the validation tasks by comparing its output to the expected response. This
    will give us a baseline understanding of how well the model performs on an instruction-following
    task right out of the box, prior to fine-tuning, and will help us appreciate the
    effect of fine-tuning later on. We will use the first example from the validation
    set for this assessment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The content of the instruction is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we generate the model’s response using the same `generate` function we
    used to pretrain the model in chapter 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `generate` function returns the combined input and output text. This behavior
    was previously convenient since pretrained LLMs are primarily designed as text-completion
    models, where the input and output are concatenated to create coherent and legible
    text. However, when evaluating the model’s performance on a specific task, we
    often want to focus solely on the model’s generated response.
  prefs: []
  type: TYPE_NORMAL
- en: 'To isolate the model’s response text, we need to subtract the length of the
    input instruction from the start of the `generated_text`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This code removes the input text from the beginning of the `generated_text`,
    leaving us with only the model’s generated response. The `strip()` function is
    then applied to remove any leading or trailing whitespace characters. The output
    is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This output shows that the pretrained model is not yet capable of correctly
    following the given instruction. While it does create a Response section, it simply
    repeats the original input sentence and part of the instruction, failing to convert
    the active sentence to passive voice as requested. So, let’s now implement the
    fine-tuning process to improve the model’s ability to comprehend and appropriately
    respond to such requests.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Fine-tuning the LLM on instruction data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s time to fine-tune the LLM for instructions (figure 7.16). We will take
    the loaded pretrained model in the previous section and further train it using
    the previously prepared instruction dataset prepared earlier in this chapter.
    We already did all the hard work when we implemented the instruction dataset processing
    at the beginning of this chapter. For the fine-tuning process itself, we can reuse
    the loss calculation and training functions implemented in chapter 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 The three-stage process for instruction fine-tuning an LLM. In step
    5, we train the pretrained model we previously loaded on the instruction dataset
    we prepared earlier.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we begin training, let’s calculate the initial loss for the training
    and validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The initial loss values are as follows; as previously, our goal is to minimize
    the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Dealing with hardware limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Using and training a larger model like GPT-2 medium (355 million parameters)
    is more computationally intensive than the smaller GPT-2 model (124 million parameters).
    If you encounter problems due to hardware limitations, you can switch to the smaller
    model by changing `CHOOSE_MODEL` `=` `"gpt2-medium` `(355M)"` to `CHOOSE_MODEL`
    `=` `"gpt2-small` `(124M)"` (see section 7.5). Alternatively, to speed up the
    model training, consider using a GPU. The following supplementary section in this
    book’s code repository lists several options for using cloud GPUs: [https://mng.bz/EOEq](https://mng.bz/EOEq).'
  prefs: []
  type: TYPE_NORMAL
- en: The following table provides reference run times for training each model on
    various devices, including CPUs and GPUs, for GPT-2\. Running this code on a compatible
    GPU requires no code changes and can significantly speed up training. For the
    results shown in this chapter, I used the GPT-2 medium model and trained it on
    an A100 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model name | Device | Run time for two epochs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| gpt2-medium (355M)  | CPU (M3 MacBook Air)  | 15.78 minutes  |'
  prefs: []
  type: TYPE_TB
- en: '| gpt2-medium (355M)  | GPU (NVIDIA L4)  | 1.83 minutes  |'
  prefs: []
  type: TYPE_TB
- en: '| gpt2-medium (355M)  | GPU (NVIDIA A100)  | 0.86 minutes  |'
  prefs: []
  type: TYPE_TB
- en: '| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74 minutes  |'
  prefs: []
  type: TYPE_TB
- en: '| gpt2-small (124M)  | GPU (NVIDIA L4)  | 0.69 minutes  |'
  prefs: []
  type: TYPE_TB
- en: '| gpt2-small (124M)  | GPU (NVIDIA A100)  | 0.39 minutes  |'
  prefs: []
  type: TYPE_TB
- en: With the model and data loaders prepared, we can now proceed to train the model.
    The code in listing 7.8 sets up the training process, including initializing the
    optimizer, setting the number of epochs, and defining the evaluation frequency
    and starting context to evaluate generated LLM responses during training based
    on the first validation set instruction (`val_data[0]`) we looked at in section
    7.5.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8 Instruction fine-tuning the pretrained LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output displays the training progress over two epochs, where
    a steady decrease in losses indicates improving ability to follow instructions
    and generate appropriate responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The training output shows that the model is learning effectively, as we can
    tell based on the consistently decreasing training and validation loss values
    over the two epochs. This result suggests that the model is gradually improving
    its ability to understand and follow the provided instructions. (Since the model
    demonstrated effective learning within these two epochs, extending the training
    to a third epoch or more is not essential and may even be counterproductive as
    it could lead to increased overfitting.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the generated responses at the end of each epoch let us inspect the
    model’s progress in correctly executing the given task in the validation set example.
    In this case, the model successfully converts the active sentence `"The` `chef`
    `cooks` `the` `meal` `every` `day."` into its passive voice counterpart: `"The`
    `meal` `is` `cooked` `every` `day` `by` `the` `chef."`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will revisit and evaluate the response quality of the model in more detail
    later. For now, let’s examine the training and validation loss curves to gain
    additional insights into the model’s learning process. For this, we use the same
    `plot_losses` function we used for pretraining:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: From the loss plot shown in figure 7.17, we can see that the model’s performance
    on both the training and validation sets improves substantially over the course
    of training. The rapid decrease in losses during the initial phase indicates that
    the model quickly learns meaningful patterns and representations from the data.
    Then, as training progresses to the second epoch, the losses continue to decrease
    but at a slower rate, suggesting that the model is fine-tuning its learned representations
    and converging to a stable solution.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 The training and validation loss trends over two epochs. The solid
    line represents the training loss, showing a sharp decrease before stabilizing,
    while the dotted line represents the validation loss, which follows a similar
    pattern.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While the loss plot in figure 7.17 indicates that the model is training effectively,
    the most crucial aspect is its performance in terms of response quality and correctness.
    So, next, let’s extract the responses and store them in a format that allows us
    to evaluate and quantify the response quality.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.3 Fine-tuning on the original Alpaca dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Alpaca dataset, by researchers at Stanford, is one of the earliest and most
    popular openly shared instruction datasets, consisting of 52,002 entries. As an
    alternative to the `instruction-data.json` file we use here, consider fine-tuning
    an LLM on this dataset. The dataset is available at [https://mng.bz/NBnE](https://mng.bz/NBnE).
  prefs: []
  type: TYPE_NORMAL
- en: This dataset contains 52,002 entries, which is approximately 50 times more than
    those we used here, and most entries are longer. Thus, I highly recommend using
    a GPU to conduct the training, which will accelerate the fine-tuning process.
    If you encounter out-of-memory errors, consider reducing the `batch_size` from
    8 to 4, 2, or even 1\. Lowering the `allowed_max_length` from 1,024 to 512 or
    256 can also help manage memory problems.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7 Extracting and saving responses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having fine-tuned the LLM on the training portion of the instruction dataset,
    we are now ready to evaluate its performance on the held-out test set. First,
    we extract the model-generated responses for each input in the test dataset and
    collect them for manual analysis, and then we evaluate the LLM to quantify the
    quality of the responses, as highlighted in figure 7.18.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 The three-stage process for instruction fine-tuning the LLM. In
    the first two steps of stage 3, we extract and collect the model responses on
    the held-out test dataset for further analysis and then evaluate the model to
    quantify the performance of the instruction-fine-tuned LLM.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To complete the response instruction step, we use the `generate` function.
    We then print the model responses alongside the expected test set answers for
    the first three test set entries, presenting them side by side for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Iterates over the first three test set samples'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses the generate function imported in section 7.5'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, the `generate` function returns the combined input and
    output text, so we use slicing and the `.replace()` method on the `generated_text`
    contents to extract the model’s response. The instructions, followed by the given
    test set response and model response, are shown next.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7_prompt.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see based on the test set instructions, given responses, and the model’s
    responses, the model performs relatively well. The answers to the first and last
    instructions are clearly correct, while the second answer is close but not entirely
    accurate. The model answers with “cumulus cloud” instead of “cumulonimbus,” although
    it’s worth noting that cumulus clouds can develop into cumulonimbus clouds, which
    are capable of producing thunderstorms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most importantly, model evaluation is not as straightforward as it is for classification
    fine-tuning, where we simply calculate the percentage of correct spam/non-spam
    class labels to obtain the classification’s accuracy. In practice, instruction-fine-tuned
    LLMs such as chatbots are evaluated via multiple approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Short-answer and multiple-choice benchmarks, such as Measuring Massive Multitask
    Language Understanding (MMLU; [https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)),
    which test the general knowledge of a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human preference comparison to other LLMs, such as LMSYS chatbot arena ([https://arena.lmsys.org](https://arena.lmsys.org)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated conversational benchmarks, where another LLM like GPT-4 is used to
    evaluate the responses, such as AlpacaEval ([https://tatsu-lab.github.io/alpaca_eval/](https://tatsu-lab.github.io/alpaca_eval/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, it can be useful to consider all three types of evaluation methods:
    multiple-choice question answering, human evaluation, and automated metrics that
    measure conversational performance. However, since we are primarily interested
    in assessing conversational performance rather than just the ability to answer
    multiple-choice questions, human evaluation and automated metrics may be more
    relevant.'
  prefs: []
  type: TYPE_NORMAL
- en: Conversational performance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Conversational performance of LLMs refers to their ability to engage in human-like
    communication by understanding context, nuance, and intent. It encompasses skills
    such as providing relevant and coherent responses, maintaining consistency, and
    adapting to different topics and styles of interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluation, while providing valuable insights, can be relatively laborious
    and time-consuming, especially when dealing with a large number of responses.
    For instance, reading and assigning ratings to all 1,100 responses would require
    a significant amount of effort.
  prefs: []
  type: TYPE_NORMAL
- en: So, considering the scale of the task at hand, we will implement an approach
    similar to automated conversational benchmarks, which involves evaluating the
    responses automatically using another LLM. This method will allow us to efficiently
    assess the quality of the generated responses without the need for extensive human
    involvement, thereby saving time and resources while still obtaining meaningful
    performance indicators.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s employ an approach inspired by AlpacaEval, using another LLM to evaluate
    our fine-tuned model’s responses. However, instead of relying on a publicly available
    benchmark dataset, we use our own custom test set. This customization allows for
    a more targeted and relevant assessment of the model’s performance within the
    context of our intended use cases, represented in our instruction dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To prepare the responses for this evaluation process, we append the generated
    model responses to the `test_set` dictionary and save the updated data as an `"instruction-data-with-response.json"`
    file for record keeping. Additionally, by saving this file, we can easily load
    and analyze the responses in separate Python sessions later on if needed.
  prefs: []
  type: TYPE_NORMAL
- en: The following code listing uses the `generate` method in the same manner as
    before; however, we now iterate over the entire `test_set`. Also, instead of printing
    the model responses, we add them to the `test_set` dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9 Generating test set responses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '#1 indent for pretty-printing'
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing the dataset takes about 1 minute on an A100 GPU and 6 minutes on
    an M3 MacBook Air:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s verify that the responses have been correctly added to the `test_set`
    dictionary by examining one of the entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that the `model_response` has been added correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we save the model as `gpt2-medium355M-sft.pth` file to be able to
    reuse it in future projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Removes white spaces and parentheses from file name'
  prefs: []
  type: TYPE_NORMAL
- en: The saved model can then be loaded via `model.load_state_dict(torch.load("gpt2
    -medium355M-sft.pth")`).
  prefs: []
  type: TYPE_NORMAL
- en: 7.8 Evaluating the fine-tuned LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously, we judged the performance of an instruction-fine-tuned model by
    looking at its responses on three examples of the test set. While this gives us
    a rough idea of how well the model performs, this method does not scale well to
    larger amounts of responses. So, we implement a method to automate the response
    evaluation of the fine-tuned LLM using another, larger LLM, as highlighted in
    figure 7.19.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-19.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 The three-stage process for instruction fine-tuning the LLM. In
    this last step of the instruction-fine-tuning pipeline, we implement a method
    to quantify the performance of the fine-tuned model by scoring the responses it
    generated for the test.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To evaluate test set responses in an automated fashion, we utilize an existing
    instruction-fine-tuned 8-billion-parameter Llama 3 model developed by Meta AI.
    This model can be run locally using the open source Ollama application ([https://ollama.com](https://ollama.com)).
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Ollama is an efficient application for running LLMs on a laptop. It serves
    as a wrapper around the open source llama.cpp library ([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)),
    which implements LLMs in pure C/C++ to maximize efficiency. However, Ollama is
    only a tool for generating text using LLMs (inference) and does not support training
    or fine-tuning LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Using larger LLMs via web APIs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The 8-billion-parameter Llama 3 model is a very capable LLM that runs locally.
    However, it’s not as capable as large proprietary LLMs such as GPT-4 offered by
    OpenAI. For readers interested in exploring how to utilize GPT-4 through the OpenAI
    API to assess generated model responses, an optional code notebook is available
    within the supplementary materials accompanying this book at [https://mng.bz/BgEv](https://mng.bz/BgEv).
  prefs: []
  type: TYPE_NORMAL
- en: 'To execute the following code, install Ollama by visiting [https://ollama.com](https://ollama.com)and
    follow the provided instructions for your operating system:'
  prefs: []
  type: TYPE_NORMAL
- en: '*For macOS and Windows users*—Open the downloaded Ollama application. If prompted
    to install command-line usage, select Yes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*For Linux users*—Use the installation command available on the Ollama website.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before implementing the model evaluation code, let’s first download the Llama
    3 model and verify that Ollama is functioning correctly by using it from the command-line
    terminal. To use Ollama from the command line, you must either start the Ollama
    application or run `ollama` `serve` in a separate terminal, as shown in figure
    7.20.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-20.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 Two options for running Ollama. The left panel illustrates starting
    Ollama using `ollama` `serve`. The right panel shows a second option in macOS,
    running the Ollama application in the background instead of using the `ollama`
    `serve` command to start the application.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'With the Ollama application or `ollama` `serve` running in a different terminal,
    execute the following command on the command line (not in a Python session) to
    try out the 8-billion-parameter Llama 3 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The first time you execute this command, this model, which takes up 4.7 GB
    of storage space, will be automatically downloaded. The output looks like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Alternative Ollama models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `llama3` in the `ollama` `run` `llama3` command refers to the instruction-fine-tuned
    8-billion-parameter Llama 3 model. Using Ollama with the `llama3` model requires
    approximately 16 GB of RAM. If your machine does not have sufficient RAM, you
    can try using a smaller model, such as the 3.8-billion-parameter `phi3` model
    via `ollama` `run` `llama3`, which only requires around 8 GB of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: For more powerful computers, you can also use the larger 70-billion-parameter
    Llama 3 model by replacing `llama3` with `llama3:70b`. However, this model requires
    significantly more computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model download is complete, we are presented with a command-line interface
    that allows us to interact with the model. For example, try asking the model,
    “What do llamas eat?”
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Note that the response you see might differ since Ollama is not deterministic
    as of this writing.
  prefs: []
  type: TYPE_NORMAL
- en: You can end this `ollama` `run` `llama3` session using the input `/bye`. However,
    make sure to keep the `ollama` `serve` command or the Ollama application running
    for the remainder of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code verifies that the Ollama session is running properly before
    we use Ollama to evaluate the test set responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Ensure that the output from executing the previous code displays `Ollama` `running:`
    `True`. If it shows `False`, verify that the `ollama` `serve` command or the Ollama
    application is actively running.
  prefs: []
  type: TYPE_NORMAL
- en: Running the code in a new Python session
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'If you already closed your Python session or if you prefer to execute the remaining
    code in a different Python session, use the following code, which loads the instruction
    and response data file we previously created and redefines the `format_input`
    function we used earlier (the `tqdm` progress bar utility is used later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: An alternative to the `ollama` `run` command for interacting with the model
    is through its REST API using Python. The `query_model` function shown in the
    following listing demonstrates how to use the API.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.10 Querying a local Ollama model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates the data payload as a dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Settings for deterministic responses'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Converts the dictionary to a JSON-formatted string and encodes it to bytes'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Creates a request object, setting the method to POST and adding necessary
    headers'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Sends the request and captures the response'
  prefs: []
  type: TYPE_NORMAL
- en: Before running the subsequent code cells in this notebook, ensure that Ollama
    is still running. The previous code cells should print `"Ollama` `running:` `True"`
    to confirm that the model is active and ready to receive requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of how to use the `query_model` function we just
    implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting response is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Using the `query_model` function defined earlier, we can evaluate the responses
    generated by our fine-tuned model that prompts the Llama 3 model to rate our fine-tuned
    model’s responses on a scale from 0 to 100 based on the given test set response
    as reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we apply this approach to the first three examples from the test set
    that we previously examined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This code prints outputs similar to the following (as of this writing, Ollama
    is not fully deterministic, so the generated texts may vary):'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7_prompt2.png)'
  prefs: []
  type: TYPE_IMG
- en: The generated responses show that the Llama 3 model provides reasonable evaluations
    and is capable of assigning partial points when a model’s answer is not entirely
    correct. For instance, if we consider the evaluation of the “cumulus cloud” answer,
    the model acknowledges the partial correctness of the response.
  prefs: []
  type: TYPE_NORMAL
- en: The previous prompt returns highly detailed evaluations in addition to the score.
    We can modify the prompt to just generate integer scores ranging from 0 to 100,
    where 100 represents the best possible score. This modification allows us to calculate
    an average score for our model, which serves as a more concise and quantitative
    assessment of its performance. The `generate_model_scores` function shown in the
    following listing uses a modified prompt telling the model to `"Respond` `with`
    `the` `integer` `number` `only."`
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.11 Evaluating the instruction fine-tuning LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Modified instruction line to only return the score'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now apply the `generate_model_scores` function to the entire `test_data`
    set, which takes about 1 minute on a M3 Macbook Air:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The evaluation output shows that our fine-tuned model achieves an average score
    above 50, which provides a useful benchmark for comparison against other models
    or for experimenting with different training configurations to improve the model’s
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that Ollama is not entirely deterministic across operating
    systems at the time of this writing, which means that the scores you obtain might
    vary slightly from the previous scores. To obtain more robust results, you can
    repeat the evaluation multiple times and average the resulting scores.
  prefs: []
  type: TYPE_NORMAL
- en: To further improve our model’s performance, we can explore various strategies,
    such as
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the hyperparameters during fine-tuning, such as the learning rate,
    batch size, or number of epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the size of the training dataset or diversifying the examples to
    cover a broader range of topics and styles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with different prompts or instruction formats to guide the model’s
    responses more effectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a larger pretrained model, which may have greater capacity to capture
    complex patterns and generate more accurate responses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE  For reference, when using the methodology described herein, the Llama
    3 8B base model, without any fine-tuning, achieves an average score of 58.51 on
    the test set. The Llama 3 8B instruct model, which has been fine-tuned on a general
    instruction-following dataset, achieves an impressive average score of 82.6.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.4 Parameter-efficient fine-tuning with LoRA
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To instruction fine-tune an LLM more efficiently, modify the code in this chapter
    to use the low-rank adaptation method (LoRA) from appendix E. Compare the training
    run time and model performance before and after the modification.
  prefs: []
  type: TYPE_NORMAL
- en: 7.9 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter marks the conclusion of our journey through the LLM development
    cycle. We have covered all the essential steps, including implementing an LLM
    architecture, pretraining an LLM, and fine-tuning it for specific tasks, as summarized
    in figure 7.21\. Let’s discuss some ideas for what to look into next.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/7-21.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 The three main stages of coding an LLM.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 7.9.1 What’s next?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While we covered the most essential steps, there is an optional step that can
    be performed after instruction fine-tuning: preference fine-tuning. Preference
    fine-tuning is particularly useful for customizing a model to better align with
    specific user preferences. If you are interested in exploring this further, see
    the `04_preference-tuning-with-dpo` folder in this book’s supplementary GitHub
    repository at [https://mng.bz/dZwD](https://mng.bz/dZwD).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the main content covered in this book, the GitHub repository
    also contains a large selection of bonus material that you may find valuable.
    To learn more about these additional resources, visit the Bonus Material section
    on the repository’s README page: [https://mng.bz/r12g](https://mng.bz/r12g).'
  prefs: []
  type: TYPE_NORMAL
- en: 7.9.2 Staying up to date in a fast-moving field
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The fields of AI and LLM research are evolving at a rapid (and, depending on
    who you ask, exciting) pace. One way to keep up with the latest advancements is
    to explore recent research papers on arXiv at [https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent).
    Additionally, many researchers and practitioners are very active in sharing and
    discussing the latest developments on social media platforms like X (formerly
    Twitter) and Reddit. The subreddit r/LocalLLaMA, in particular, is a good resource
    for connecting with the community and staying informed about the latest tools
    and trends. I also regularly share insights and write about the latest in LLM
    research on my blog, available at [https://magazine.sebastianraschka.com](https://magazine.sebastianraschka.com)
    and [https://sebastianraschka.com/blog/](https://sebastianraschka.com/blog/).
  prefs: []
  type: TYPE_NORMAL
- en: 7.9.3 Final words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I hope you have enjoyed this journey of implementing an LLM from the ground
    up and coding the pretraining and fine-tuning functions from scratch. In my opinion,
    building an LLM from scratch is the most effective way to gain a deep understanding
    of how LLMs work. I hope that this hands-on approach has provided you with valuable
    insights and a solid foundation in LLM development.
  prefs: []
  type: TYPE_NORMAL
- en: While the primary purpose of this book is educational, you may be interested
    in utilizing different and more powerful LLMs for real-world applications. For
    this, I recommend exploring popular tools such as Axolotl ([https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl))
    or LitGPT ([https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt)),
    which I am actively involved in developing.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for joining me on this learning journey, and I wish you all the best
    in your future endeavors in the exciting field of LLMs and AI!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The instruction-fine-tuning process adapts a pretrained LLM to follow human
    instructions and generate desired responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the dataset involves downloading an instruction-response dataset,
    formatting the entries, and splitting it into train, validation, and test sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training batches are constructed using a custom collate function that pads sequences,
    creates target token IDs, and masks padding tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load a pretrained GPT-2 medium model with 355 million parameters to serve
    as the starting point for instruction fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pretrained model is fine-tuned on the instruction dataset using a training
    loop similar to pretraining.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation involves extracting model responses on a test set and scoring them
    (for example, using another LLM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Ollama application with an 8-billion-parameter Llama model can be used to
    automatically score the fine-tuned model’s responses on the test set, providing
    an average score to quantify performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
