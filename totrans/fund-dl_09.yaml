- en: Chapter 9\. Models for Sequence Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。序列分析模型
- en: Surya Bhupatiraju
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Surya Bhupatiraju
- en: Analyzing Variable-Length Inputs
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析可变长度输入
- en: 'Up until now, we’ve worked only with data with fixed sizes: images from MNIST,
    CIFAR-10, and ImageNet. These models are incredibly powerful, but there are many
    situations in which fixed-length models are insufficient. The vast majority of
    interactions in our daily lives require a deep understanding of sequences—whether
    it’s reading the morning newspaper, making a bowl of cereal, listening to the
    radio, watching a presentation, or deciding to execute a trade on the stock market.
    To adapt to variable-length inputs, we’ll have to be a little bit more clever
    about how we approach designing deep learning models.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只处理了具有固定大小的数据：来自MNIST、CIFAR-10和ImageNet的图像。这些模型非常强大，但在许多情况下，固定长度模型是不够的。我们日常生活中绝大多数的互动都需要对序列有深入的理解——无论是阅读早报、准备一碗麦片、听收音机、观看演示还是决定在股市上执行交易。为了适应可变长度的输入，我们必须更加聪明地设计深度学习模型的方法。
- en: '[Figure 9-1](#feedforward_networks_thrive) illustrates how our feed-forward
    neural networks break when analyzing sequences. If the sequence is the same size
    as the input layer, the model can perform as expected. It’s even possible to deal
    with smaller inputs by padding zeros to the end of the input until it’s the appropriate
    length. However, the moment the input exceeds the size of the input layer, naively
    using the feed-forward network no longer works.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-1](#feedforward_networks_thrive)说明了我们的前馈神经网络在分析序列时会出现问题。如果序列与输入层大小相同，模型可以按预期执行。甚至可以通过在输入末尾填充零直到达到适当长度来处理较小的输入。然而，一旦输入超过输入层的大小，朴素地使用前馈网络就不再起作用。'
- en: Feed-forward networks thrive on fixed input size problems. Zero padding can
    address the handling of smaller inputs, but when naively utilized, these models
    break when inputs exceed the fixed input size.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络在固定输入大小的问题上表现出色。零填充可以解决处理较小输入的问题，但是当朴素地使用时，这些模型在输入超过固定输入大小时会出现问题。
- en: '![](Images/fdl2_0901.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0901.png)'
- en: Figure 9-1\. Broken feed-forward network
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1。损坏的前馈网络
- en: Not all hope is lost, however. In the next couple of sections, we’ll explore
    several strategies we can leverage to “hack” feed-forward networks to handle sequences.
    Later in the chapter, we’ll analyze the limitations of these hacks and discuss
    new architectures to address them. We will conclude the chapter by discussing
    some of the most advanced architectures explored to date to tackle some of the
    most difficult challenges in replicating human-level logical reasoning and cognition
    over sequences.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非一切希望都已失去。在接下来的几节中，我们将探讨几种策略，可以利用“黑客”前馈网络来处理序列。在本章后面，我们将分析这些黑客的局限性，并讨论新的架构来解决这些问题。我们将通过讨论迄今为止探索的一些最先进的架构来结束本章，以解决复制人类级别逻辑推理和认知的一些最困难挑战。
- en: Tackling seq2seq with Neural N-Grams
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经N-Grams解决seq2seq
- en: In this section, we’ll begin exploring a feed-forward neural network architecture
    that can process a body of text and produce a sequence of part-of-speech (POS)
    tags. In other words, we want to appropriately label each word in the input text
    as a noun, verb, preposition, and so on. An example of this is shown in [Figure 9-2](#example_of_an_accurate_pos).
    While it’s not the same complexity as building an AI that can answer questions
    after reading a story, it’s a solid first step toward developing an algorithm
    that can understand the meaning of how words are used in a sentence. This problem
    is also interesting because it is an instance of a class of problems known as
    *seq2seq*, where the goal is to transform an input sequence into a corresponding
    output sequence. Other famous seq2seq problems include translating text between
    languages (which we will tackle later in this chapter),  text summarization, and
    transcribing speech to text.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开始探索一个前馈神经网络架构，可以处理一段文本并生成一系列词性（POS）标签。换句话说，我们希望适当地标记输入文本中的每个单词，如名词、动词、介词等。这在[图9-2](#example_of_an_accurate_pos)中有一个示例。虽然构建一个可以阅读故事后回答问题的人工智能的复杂性不同，但这是朝着开发一个能够理解单词在句子中如何使用含义的算法的坚实第一步。这个问题也很有趣，因为它是一类问题的一个实例，被称为*seq2seq*，目标是将输入序列转换为相应的输出序列。其他著名的seq2seq问题包括在语言之间翻译文本（我们将在本章后面处理）、文本摘要和将语音转录为文本。
- en: '![](Images/fdl2_0902.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0902.png)'
- en: Figure 9-2\. An example of an accurate POS parse of an English sentence
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。英语句子准确的POS解析示例
- en: 'As we discussed, it’s not obvious how we might take a body of text all at once
    to predict the full sequence of POS tags. Instead, we leverage a trick that is
    akin to the way we developed distributed vector representations of words in the
    previous chapter. The key observation is this: *it is not necessary to take into
    account long-term dependencies to predict the POS of any given word*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们讨论过的，如何一次性处理一段文本以预测完整的POS标签序列并不明显。相反，我们利用了一种类似于我们在上一章中开发单词的分布式向量表示的技巧。关键观察是：*不需要考虑长期依赖性来预测任何给定单词的POS*。 '
- en: The implication of this observation is that instead of using the whole sequence
    to predict all of the POS tags simultaneously, we can predict each POS tag one
    at a time by using a fixed-length subsequence. In particular, we utilize the subsequence
    starting from the word of interest and extending *n* words into the past. This
    *neural n-gram strategy* is depicted in [Figure 9-3](#perform_seq2seq).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这一观察的含义是，我们可以通过使用固定长度的子序列，而不是使用整个序列同时预测所有POS标签，逐个预测每个POS标签。特别是，我们利用从感兴趣的单词开始并向过去扩展n个单词的子序列。这种*神经n-gram策略*在[图9-3](#perform_seq2seq)中有所描述。
- en: '![](Images/fdl2_0903.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0903.png)'
- en: Figure 9-3\. Using a feed-forward network to perform seq2seq when we can ignore
    long-term dependencies
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3。在我们可以忽略长期依赖性时使用前馈网络执行seq2seq
- en: Specifically, when we predict the POS tag for the  <math alttext="i Superscript
    t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>  word in
    the input, we use the the  <math alttext="i minus n plus 1 Superscript s t Baseline
    comma i minus n plus 2 Superscript n d Baseline comma ellipsis comma i Superscript
    t h"><mrow><mi>i</mi> <mo>-</mo> <mi>n</mi> <mo>+</mo> <msup><mn>1</mn> <mrow><mi>s</mi><mi>t</mi></mrow></msup>
    <mo>,</mo> <mi>i</mi> <mo>-</mo> <mi>n</mi> <mo>+</mo> <msup><mn>2</mn> <mrow><mi>n</mi><mi>d</mi></mrow></msup>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow></math>
    words as the input. We’ll refer to this subsequence as the *context window*. In
    order to process the entire text, we’ll start by positioning the network at the
    beginning of the text. We’ll then proceed to move the network’s context window
    one word at a time, predicting the POS tag of the rightmost word, until we reach
    the end of the input.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，当我们预测输入中第i个单词的词性标签时，我们使用第i-n+1到第i个单词作为输入。我们将这个子序列称为*上下文窗口*。为了处理整个文本，我们将首先将网络定位在文本的开头。然后，我们将继续将网络的上下文窗口每次移动一个单词，预测最右边单词的词性标签，直到达到输入的末尾。
- en: Leveraging the word embedding strategy from last chapter, we’ll also use condensed
    representations of the words instead of one-hot vectors. This will allow us to
    reduce the number of parameters in our model and make learning faster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 利用上一章的词嵌入策略，我们将使用单词的压缩表示，而不是独热向量。这将使我们能够减少模型中的参数数量，并加快学习速度。
- en: Implementing a Part-of-Speech Tagger
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现词性标注器
- en: Now that we have a strong understanding of the POS network architecture, we
    can dive into the implementation. On a high level, the network consists of an
    input layer that leverages a three-gram context window. We’ll use word embeddings
    that are 300-dimensional, resulting in a context window of size 900\. The feed-forward
    network will have two hidden layers of size 512 neurons and 256 neurons, respectively.
    Then, the output layer will be a softmax calculating the probability distribution
    of the POS tag output over a space of 44 possible tags. As usual, we’ll use the
    Adam optimizer with our default hyperparameter settings, train for a total of
    1,000 epochs, and leverage batch-normalization for regularization.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对词性网络架构有了深入的理解，我们可以深入实现。在高层次上，网络由一个利用三元上下文窗口的输入层组成。我们将使用300维的词嵌入，从而得到一个大小为900的上下文窗口。前馈网络将有两个隐藏层，分别为512个神经元和256个神经元。然后，输出层将是一个softmax，计算POS标签输出在44个可能标签空间上的概率分布。像往常一样，我们将使用Adam优化器和默认的超参数设置，总共训练1000个时代，并利用批量归一化进行正则化。
- en: 'The actual network is extremely similar to networks we’ve implemented in the
    past. Rather, the tricky part of building the POS tagger is in preparing the dataset.
    We’ll leverage pretrained word embeddings generated from [Google News](https://oreil.ly/Rsu9A).
    It includes vectors for 3 million words and phrases and was trained on roughly
    100 billion words. We can use the `gensim` Python package to read the dataset.
    Google Colab already has `gensim` preinstalled. If you are using another machine,
    you can use `pip` to install the package. You will also need to download the Google
    News data file:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的网络与我们过去实现的网络非常相似。相反，构建词性标注器的难点在于准备数据集。我们将利用从[Google News](https://oreil.ly/Rsu9A)生成的预训练词嵌入。它包括了300万个单词和短语的向量，并在大约1000亿个单词上进行了训练。我们可以使用`gensim`
    Python包来读取数据集。Google Colab已经预先安装了`gensim`。如果您使用另一台机器，可以使用`pip`来安装该包。您还需要下载Google
    News数据文件：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can subsequently load these vectors into memory using the following command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用以下命令将这些向量加载到内存中：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The issue with this operation, however, is that it’s incredibly slow (it can
    take up to an hour, depending on the specs of your machine). To avoid loading
    the full dataset into memory every single time we run our program, especially
    while debugging code or experimenting with different hyperparameters, we cache
    the relevant subset of the vectors to disk using a lightweight database known
    as [LevelDB](http://leveldb.org). To build the appropriate Python bindings (which
    allow us to interact with a LevelDB instance from Python), we simply use the following
    command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种操作的问题在于它非常慢（根据您的机器规格，可能需要长达一小时）。为了避免每次运行程序时都将整个数据集加载到内存中，特别是在调试代码或尝试不同超参数时，我们使用轻量级数据库[LevelDB](http://leveldb.org)将相关子集的向量缓存到磁盘上。为了构建适当的Python绑定（允许我们从Python与LevelDB实例交互），我们只需使用以下命令：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we mentioned, the `gensim` model contains three million words, which is larger
    than our dataset. For the sake of efficiency, we’ll selectively cache word vectors
    for words in our dataset and discard everything else. To figure out which words
    we’d like to cache, let’s download the POS dataset from the [CoNLL-2000 task](https://oreil.ly/8qJeZ).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，`gensim`模型包含三百万个单词，比我们的数据集要大。为了提高效率，我们将有选择地缓存数据集中的单词向量，并丢弃其他所有内容。为了找出我们想要缓存的单词，让我们从[CoNLL-2000任务](https://oreil.ly/8qJeZ)下载POS数据集。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The dataset consists of contiguous text that is formatted as a sequence of
    rows, where the first element is a word and the second element is the corresponding
    part of speech. Here are the first several lines of the training dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由格式化为一系列行的连续文本组成，其中第一个元素是一个单词，第二个元素是相应的词性。以下是训练数据集的前几行：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To match the formatting of the dataset to the `gensim` model, we’ll have to
    do some preprocessing. For example, the model replaces digits with ''#'' characters,
    combines separate words into entities where appropriate (e.g., considering “New_York”
    as a single token instead of two separate words), and utilizes underscores where
    the raw data uses dashes. We preprocess the dataset to conform to this model schema
    with the following code (analogous code is used to process the training data):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据集的格式与`gensim`模型匹配，我们需要进行一些预处理。例如，该模型用'#'字符替换数字，将适当的单词组合成实体（例如，将“New_York”视为一个单词而不是两个单独的单词），并在原始数据使用破折号时使用下划线。我们对数据集进行预处理以符合这个模型模式，使用以下代码（类似的代码用于处理训练数据）：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we’ve appropriately processed the datasets for use, we can load the
    words in LevelDB. If the word or phrase is present in the `gensim` model, we can
    cache that in the LevelDB instance. If not, we randomly select a vector to represent
    to the token, and cache it so that we remember to use the same vector in case
    we encounter it again:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经适当处理了用于加载的数据集，我们可以加载LevelDB中的单词。如果`gensim`模型中存在单词或短语，我们可以将其缓存到LevelDB实例中。如果没有，我们会随机选择一个向量来表示该标记，并将其缓存，以便在再次遇到时记得使用相同的向量：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After running the script for the first time, we can just load our data straight
    from the database if it already exists:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次运行脚本后，如果数据已经存在，我们可以直接从数据库加载数据：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we build dataset objects for both training and test datasets, which we
    can use to generate minibatches for training and testing purposes. Building the
    dataset object requires access to the LevelDB `db`, the `dataset`, a dictionary
    `tags_to_index` that maps POS tags to indices in the output vector, and a boolean
    flat `get_all` that determines whether getting the minibatch should retrieve the
    full set by default:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为训练和测试数据集构建数据集对象，我们可以使用这些对象生成用于训练和测试的小批量数据。构建数据集对象需要访问LevelDB `db`，`dataset`，将POS标记映射到输出向量中的索引的字典`tags_to_index`，以及一个布尔值`get_all`，用于确定获取小批量数据时是否应默认检索完整集合：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Finally, we design our feed-forward network similarly to our approaches in previous
    chapters. We omit a discussion of the code and refer to the file *Ch09_01_POS_Tagger.ipynb*
    in the [book’s repository](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们设计我们的前馈网络与之前章节中的方法类似。我们省略了代码的讨论，并参考文件*Ch09_01_POS_Tagger.ipynb*在[书的存储库](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book)中。
- en: 'Every epoch, we manually inspect the model by parsing the sentence: “The woman,
    after grabbing her umbrella, went to the bank to deposit her cash.” Within 100
    epochs of training, the algorithm achieves over 96% accuracy and nearly perfectly
    parses the validation sentence (it makes the understandable mistake of confusing
    the possessive pronoun and personal pronoun tags for the first appearance of the
    word “her”). We’ll conclude this by including the visualizations of our model’s
    performance using TensorBoard in [Figure 9-4](#tensorboard_viz_of_feedforward_pos).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时代，我们通过解析句子“这个女人，在拿起她的伞后，去了银行存钱。”来手动检查模型。在训练100个时代后，算法达到了超过96%的准确率，并几乎完美地解析了验证句子（它犯了一个可以理解的错误，混淆了第一次出现“her”一词的所有格代词和人称代词标记）。我们将通过在TensorBoard中包含我们模型性能的可视化来结束这一部分[图9-4](#tensorboard_viz_of_feedforward_pos)。
- en: The POS tagging model was a great exercise, but it was mostly rinsing and repeating
    concepts we’ve learned in previous chapters. In the rest of the chapter, we’ll
    start to think about much more complicated sequence-related learning tasks. To
    tackle these more difficult problems, we’ll need to broach brand-new concepts,
    develop new architectures, and start to explore the cutting edge of modern deep
    learning research. We’ll start by tackling the problem of dependency parsing next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: POS标记模型是一个很好的练习，但它主要是重复我们在之前章节学到的概念。在本章的其余部分，我们将开始思考更复杂的与序列相关的学习任务。为了解决这些更困难的问题，我们需要涉及全新的概念，开发新的架构，并开始探索现代深度学习研究的前沿。我们将从下一个依存句法分析问题开始。
- en: '![](Images/fdl2_0904.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0904.png)'
- en: Figure 9-4\. TensorBoard visualization of our feed-forward POS tagging model
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4。我们前馈POS标记模型的TensorBoard可视化
- en: Dependency Parsing and SyntaxNet
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依存句法分析和SyntaxNet
- en: The framework we used to solve the POS tagging task was rather simple. Sometimes
    we need to be much more creative about how we tackle seq2seq problems, especially
    as the complexity of the problem increases. In this section, we’ll explore strategies
    that employ creative data structures to tackle difficult seq2seq problems. As
    an illustrative example, we’ll explore the problem of dependency parsing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来解决POS标记任务的框架相当简单。有时候，我们需要更有创意地解决seq2seq问题，特别是在问题复杂性增加时。在本节中，我们将探讨采用创造性数据结构来解决困难的seq2seq问题的策略。作为一个说明性例子，我们将探讨依存句法分析问题。
- en: The idea behind building a dependency parse tree is to map the relationships
    between words in a sentence. Take, for example, the dependency in [Figure 9-5](#example_of_a_dependency_parse).
    The words “I” and “taxi” are children of the word “took,” specifically as the
    subject and direct object of the verb, respectively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 构建依存句法分析树的想法是映射句子中单词之间的关系。例如，看一下[图9-5](#example_of_a_dependency_parse)中的依存关系。单词“I”和“taxi”是单词“took”的子节点，具体来说是动词的主语和直接宾语。
- en: '![](Images/fdl2_0905.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0905.png)'
- en: Figure 9-5\. An example of a dependency parse, which generates a tree of relationships
    between words in a sentence
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5。一个依存句法分析的示例，生成句子中单词之间关系的树
- en: One way to express a tree as a sequence is by linearizing it. Let’s consider
    the examples in [Figure 9-6](#linearize_two_example_trees). Essentially, if you
    have a graph with a root `R`, and children `A` (connected by edge `r_a`), `B`
    (connected by edge `r_b`), and `C` (connected by edge `r_c`), we can linearize
    the representation as `(R, r_a, A, r_b, B, r_c, C)`. We can even represent more
    complex graphs. Let’s assume, for example, that node `B` actually has two more
    children named `D` (connected by edge `b_d`) and `E` (connected by edge `b_e`).
    We can represent this new graph as `(R, r_a, A, r_b, [B, b_d, D, b_e, E], r_c,` `C)`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0906.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-6\. We linearize two example trees: the diagrams omit edge labels
    for the sake of visual clarity'
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using this paradigm, we can take our example dependency parse and linearize
    it, as shown in [Figure 9-7](#linearization_of_dependency_parse_tree).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0907.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. Linearization of the dependency parse tree example
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One interpretation of this seq2seq problem would be to read the input sentence
    and produce a sequence of tokens as an output that represents the linearization
    of the input’s dependency parse. It’s not particularly clear, however, how we
    might port our strategy from the previous section, where there was a clear one-to-one
    mapping between words and their POS tags. Moreover, we could easily make decisions
    about a POS tag by looking at the nearby context. For dependency parsing, there’s
    no clear relationship between how words are ordered in the sentence and how tokens
    in the linearization are ordered. It also seems like dependency parsing tasks
    us with identifying edges that may span a significantly large number of words.
    Therefore, at first glance, it seems like this setup directly violates our assumption
    that we need not take into account any long-term dependencies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: To make the problem more approachable, we instead reconsider the dependency
    parsing task as finding a sequence of valid “actions” that generates the correct
    dependency parse. This technique, known as the *arc-standard* system, was first
    described by Nivre in 2004 and later leveraged in a neural context by Chen and
    Manning in 2014.^([1](ch09.xhtml#idm45934164188240)) In the arc-standard system,
    we start by putting the first two words of the sentence in the stack and maintaining
    the remaining words in the buffer, as shown in [Figure 9-8](#we_have_three_options).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0908.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-8\. Three options in the arc-standard system: shift a word from the
    buffer to the stack, draw an arc from the right element to the left element (left
    arc), or draw an arc from the left element to the right element (right arc)'
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'At any step, we can take one of three possible classes of actions:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Shift
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Move a word from the buffer to the front of the stack.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Left arc
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Combine the two elements at the front of the stack into a single unit where
    the root of the rightmost element is the parent node and the root of leftmost
    element is the child node.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Right arc
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Combine the two elements at the front of the stack into a single unit where
    the root of the left element is the parent node, and the root of right element
    is the child node.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: We note that while there is only one way to perform a shift, the arc actions
    can be of many flavors, each differentiated by the dependency label assigned to
    the arc that is generated. That being said, we’ll simplify our discussions and
    illustrations in this section by considering each decision as a choice among three
    actions (rather than tens of actions).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: We terminate this process when the buffer is empty and the stack has one element
    in it (which represents the full dependency parse). To illustrate this process
    in its entirety, we illustrate a sequence of actions that generates the dependency
    parse for our example input sentence in [Figure 9-9](#sequence_of_actions).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0909.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Figure 9-9\. A sequence of actions that results in the correct dependency parse;
    we omit labels
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s not too difficult to reformulate this decision-making framework as a learning
    problem. At every step, we take the current configuration, and we vectorize the
    configuration by extracting a large number of features that describe the configuration
    (words in specific locations of the stack/buffer, specific children of the words
    in these locations, part of speech tags, etc.). During train time, we can feed
    this vector into a feed-forward network and compare its prediction of the next
    action to take to a gold-standard decision made by a human linguist. To use this
    model in the wild, we can take the action that the network recommends, apply it
    to the configuration, and use this new configuration as the starting point for
    the next step (feature extraction, action prediction, and action application).
    This process is shown in [Figure 9-10](#neural_framework_for_arc_standard_dependency).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0910.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: Figure 9-10\. A neural framework for arc-standard dependency parsing
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Taken together, these ideas form the core for Google’s SyntaxNet, the state-of-the-art
    open source implementation for dependency parsing. Delving into the nitty-gritty
    aspects of implementation is beyond the scope of this text, but we refer you to
    the [open source repository](https://oreil.ly/UT1ga), which contains an implementation
    of Parsey McParseface, the most accurate publicly reported English language parser
    as of the publication of this text.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Beam Search and Global Normalization
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we described a naive strategy for deploying SyntaxNet
    in practice. The strategy was purely *greedy*; that is, we selected prediction
    with the highest probability without being concerned that we might potentially
    paint ourselves into a corner by making an early mistake. In the POS example,
    making an incorrect prediction was largely inconsequential. This is because each
    prediction could be considered a purely independent subproblem (the results of
    a given prediction do not affect the inputs of the next step).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: This assumption no longer holds in SyntaxNet, because our prediction at step *n* affects
    the input we use at step  <math alttext="n plus 1"><mrow><mi>n</mi> <mo>+</mo>
    <mn>1</mn></mrow></math> . This implies that any mistake we make will influence
    all later decisions. Moreover, there’s no good way of “going backward” and fixing
    mistakes when they become apparent.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '*Garden path sentences* are an extreme case of where this is important. Consider
    the following sentence: “The complex houses married and single soldiers and their
    families.” The first glance pass-through is confusing. Most people interpret “complex”
    as an adjective “houses” as a noun, and “married” as a past tense verb. This makes
    little semantic sense though, and starts to break down as the rest of the sentence
    is read. Instead, we realize that “complex” is a noun (as in a military complex)
    and that “houses” is a verb. In other words, the sentence implies that the military
    complex contains soldiers (who may be single or married) and their families. A
    *greedy* version of SyntaxNet would fail to correct the early parse mistake of
    considering “complex” as an adjective describing the “houses,” and therefore would
    fail on the full version of the sentence.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'To remedy this shortcoming, we use a strategy known as *beam search*, illustrated
    in [Figure 9-11](#illustration_of_using_beam_search). We generally leverage beam
    searches in situations like SyntaxNet, where the output of our network at a particular
    step influences the inputs used in future steps. The basic idea behind beam search
    is that instead of greedily selecting the most probable prediction at each step,
    we maintain a *beam* of the most likely hypothesis (up to a fixed *beam size b*)
    for the sequence of the first *k* actions and their associated probabilities.
    Beam searching can be broken up into two major phases: expansion and pruning.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0911.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: Figure 9-11\. Using beam search (with beam size 2) while deploying a trained
    SyntaxNet model
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: During the *expansion* step, we take each hypothesis and consider it as a possible
    input to SyntaxNet. Assume SyntaxNet produces a probability distribution over
    a space of  <math alttext="StartAbsoluteValue upper A EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>A</mi> <mo>|</mo></mrow></math>  total actions. We then compute the probability
    of each of the  <math alttext="b StartAbsoluteValue upper A EndAbsoluteValue"><mrow><mi>b</mi>
    <mo>|</mo> <mi>A</mi> <mo>|</mo></mrow></math>  possible hypotheses for the sequence
    of the first  <math alttext="k plus 1"><mrow><mi>k</mi> <mo>+</mo> <mn>1</mn></mrow></math>
     actions. Then, during the *pruning *step, we keep only the *b* hypothesis out
    of the  <math alttext="b StartAbsoluteValue upper A EndAbsoluteValue"><mrow><mi>b</mi>
    <mo>|</mo> <mi>A</mi> <mo>|</mo></mrow></math>  total options with the largest
    probabilities. As [Figure 9-11](#illustration_of_using_beam_search) illustrates,
    beam searching enables SyntaxNet to correct incorrect predictions post facto by
    entertaining less probable hypotheses early that might turn out to be more fruitful
    later in the sentence. In fact, digging deeper into the illustrated example, a
    greedy approach would have suggested that the correct sequence of moves would
    have been a shift followed by a left arc. In reality, the best (highest probability)
    option would have been to use a left arc followed by a right arc. Beam searching
    with beam size 2 surfaces this result.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The full open source version takes this a full step further and attempts to
    bring the concept of beam searching to the process of training the network. As
    Andor et al. described in 2016,^([2](ch09.xhtml#idm45934164127728)) this process
    of *global normalization* provides both strong theoretical guarantees and clear
    performance gains relative to *local normalization* in practice. In a locally
    normalized network, our network is tasked with selecting the best action given
    a configuration. The network outputs a score that is normalized using a softmax
    layer. This is meant to model a probability distribution over all possible actions,
    provided the actions performed thus far. Our loss function attempts to force the
    probability distribution to the ideal output (i.e., probability 1 for the correct
    action and 0 for all other actions). The cross-entropy loss does a spectacular
    job of ensuring this for us.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: In a globally normalized network, our interpretation of the scores is slightly
    different. Instead of putting the scores through a softmax to generate a per-action
    probability distribution, we instead add up all the scores for a hypothesis action
    sequence. One way of ensuring that we select the correct hypothesis sequence is
    by computing this sum over all possible hypotheses and then applying a softmax
    layer to generate a probability distribution. We could theoretically use the same
    cross-entropy loss function as we used in the locally normalized network. The
    problem with this strategy, however, is that there is an intractably large number
    of possible hypothesis sequences. Even considering an average sentence length
    of 10 and a conservative total number of 15 possible actions—1 shift and 7 labels
    for each of the left and right arcs—this corresponds to 1,000,000,000,000,000
    possible hypotheses.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: To make this problem tractable, as shown in [Figure 9-12](#make_global_normalization_in_syntaxnet_tractable),
    we apply a beam search, with a fixed beam size, until we either (1) reach the
    end of the sentence, or (2) the correct sequence of actions is no longer contained
    on the beam. We then construct a loss function that tries to push the “gold standard”
    action sequence (highlighted in blue) as high as possible on the beam by maximizing
    its score relative to the other hypotheses. While we won’t dive into the details
    of how we might construct this loss function here, we refer you to the original
    paper by Andor et al. in 2016.^([3](ch09.xhtml#idm45934164119456)) The paper also
    describes a more sophisticated POS tagger that uses global normalization and beam
    search to significantly increase accuracy (compared to the POS tagger we built
    earlier in the chapter).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个问题可解，如[图9-12](#make_global_normalization_in_syntaxnet_tractable)所示，我们应用一个固定大小的束搜索，直到我们要么（1）到达句子的结尾，要么（2）正确的动作序列不再包含在束中。然后，我们构建一个损失函数，试图通过最大化相对于其他假设的分数来尽可能将“黄金标准”动作序列（用蓝色突出显示）推到束的顶部。虽然我们不会在这里深入讨论如何构建这个损失函数的细节，但我们建议您参考2016年Andor等人的原始论文。^([3](ch09.xhtml#idm45934164119456))
    该论文还描述了一个更复杂的词性标注器，它使用全局归一化和束搜索来显著提高准确性（与我们在本章前面构建的词性标注器相比）。
- en: '![](Images/fdl2_0912.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0912.png)'
- en: Figure 9-12\. Coupling training and beam search can make global normalization
    in SyntaxNet tractable
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-12. 训练和束搜索的耦合可以使SyntaxNet中的全局归一化可解
- en: A Case for Stateful Deep Learning Models
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有关有状态深度学习模型的案例
- en: While we’ve explored several tricks to adapt feed-forward networks to sequence
    analysis, we’ve yet to truly find an elegant solution to sequence analysis. In
    the POS tagger example, we made the explicit assumption that we can ignore long-term
    dependencies. We were able to overcome some of the limitations of this assumption
    by introducing the concepts of beam searching and global normalization, but even
    still, the problem space was constrained to situations in which there was a one-to-one
    mapping between elements in the input sequence to elements in the output sequence.
    For example, even in the dependency parsing model, we had to reformulate the problem
    to discover a one-to-one mapping between a sequence of input configurations while
    constructing the parse tree and arc-standard actions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经探索了几种将前馈网络适应序列分析的技巧，但我们尚未真正找到一个优雅的解决方案来进行序列分析。在词性标注器示例中，我们明确假设可以忽略长期依赖关系。通过引入束搜索和全局归一化的概念，我们能够克服这种假设的一些局限性，但即使如此，问题空间仍受限于输入序列中的元素与输出序列中的元素之间存在一对一的映射的情况。例如，即使在依赖解析模型中，我们也必须重新制定问题，以发现在构建解析树和弧标准动作时，输入配置序列之间存在一对一的映射。
- en: Sometimes, however, the task is far more complicated than finding a one-to-one
    mapping between input and output sequences. For example, we might want to develop
    a model that can consume an entire input sequence at once and then conclude if
    the sentiment of the entire input was positive or negative. We’ll build a simple
    model to perform this task later in the chapter. We may want an algorithm that
    consumes a complex input (such as an image) and generate a sentence, one word
    at a time, describing the input. We may event want to translate sentences from
    one language to another (e.g., from English to French). In all of these instances,
    there’s no obvious mapping between input tokens and output tokens. Instead, the
    process is more like the situation in [Figure 9-13](#ideal_model_for_sequence_analysis).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时任务比找到输入和输出序列之间的一对一映射要复杂得多。例如，我们可能希望开发一个模型，可以一次消耗整个输入序列，然后得出整个输入的情感是积极的还是消极的。我们将在本章后面构建一个简单的模型来执行这个任务。我们可能希望一个算法可以消耗一个复杂的输入（比如一幅图像），然后逐字生成一个描述输入的句子。我们甚至可能希望将句子从一种语言翻译成另一种语言（例如，从英语到法语）。在所有这些情况下，输入标记和输出标记之间没有明显的映射。相反，这个过程更像是[图9-13](#ideal_model_for_sequence_analysis)中的情况。
- en: '![](Images/fdl2_0913.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0913.png)'
- en: Figure 9-13\. The ideal model for sequence analysis can store information in
    memory over long periods of time, leading to a coherent “thought” vector that
    it can use to generate an answer
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-13. 用于序列分析的理想模型可以在长时间内存储信息，从而产生一个连贯的“思考”向量，可以用来生成答案
- en: The idea is simple. We want our model to maintain some sort of memory over the
    span of reading the input sequence. As it reads the input, the model should be
    able to modify this memory bank, taking into account the information that it observes.
    By the time it has reached the end of the input sequence, the internal memory
    contains a “thought” that represents the key pieces of information, that is, the
    meaning, of the original input. We should then, as shown in [Figure 9-13](#ideal_model_for_sequence_analysis),
    be able to use this thought vector to either produce a label for the original
    sequence or produce an appropriate output sequence (translation, description,
    abstractive summary, etc.).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单。我们希望我们的模型在阅读输入序列的过程中保持某种记忆。当它阅读输入时，模型应该能够修改这个记忆库，考虑到它观察到的信息。当它到达输入序列的末尾时，内部记忆包含一个代表原始输入的关键信息，即意义的“思考”。然后，如[图9-13](#ideal_model_for_sequence_analysis)所示，我们应该能够使用这个思考向量来为原始序列产生一个标签，或者产生一个适当的输出序列（翻译、描述、抽象摘要等）。
- en: The concept here isn’t something we’ve explored in any of the previous chapters.
    Feed-forward networks are inherently “stateless.” After it’s been trained, the
    feed-forward network is a static structure. It isn’t able to maintain memories
    between inputs, or change how it processes an input based on inputs it has seen
    in the past. To execute this strategy, we’ll need to reconsider how we construct
    neural networks to create deep learning models that are “stateful.” To do this,
    we’ll have to return to how we think about networks on an individual neuron level.
    In the next section, we’ll explore how *recurrent connections* (as opposed to
    the feed-forward connections we have studied this far) enable models to maintain
    state as we describe a class of models known as *recurrent neural networks* (RNNs).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs were first introduced in the 1980s, but have regained popularity recently
    due to several intellectual and hardware breakthroughs that have made them tractable
    to train. RNNs are different from feed-forward networks because they leverage
    a special type of neural layer, known as recurrent layers, that enable the network
    to maintain state between uses of the network.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-14](#recurrent_layer_contains) illustrates the neural architecture
    of a recurrent layer. All of the neurons have both (1) incoming connections emanating
    from all of the neurons of the previous layer and (2) outgoing connections leading
    to all of the neurons to the subsequent layer. We notice here, however, that these
    aren’t the only connections that neurons of a recurrent layer have. Unlike a feed-forward
    layer, recurrent layers also have recurrent connections, which propagate information
    between neurons of the same layer. A fully connected recurrent layer has information
    flow from every neuron to every other neuron in its layer (including itself).
    Thus a recurrent layer with  <math alttext="r"><mi>r</mi></math>  neurons has
    a total of  <math alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math>
     recurrent connections.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0914.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: Figure 9-14\. A recurrent layer contains recurrent connections, that is to say,
    connections between neurons that are located in the same layer
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To better understand how RNNs work, let’s explore how one functions after it’s
    been appropriately trained. Every time we want to process a new sequence, we create
    a fresh instance of our model. We can reason about networks that contain recurrent
    layers by dividing the lifetime of the network instance into discrete time steps.
    At each time step, we feed the model the next element of the input. Feed-forward
    connections represent information flow from one neuron to another where the data
    being transferred is the computed neuronal activation from the current time step.
    Recurrent connections, however, represent information flow where the data is the
    stored neuronal activation from the *previous* time step. Thus, the activations
    of the neurons in a recurrent network represent the accumulating state of the
    network instance. The initial activations of neurons in the recurrent layer are
    parameters of our model, and we determine the optimal values for them just like
    we determine the optimal values for the weights of each connection during the
    process of training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that, given a fixed lifetime (say *t* time steps) of an RNN instance,
    we can actually express the instance as a feed-forward network (albeit irregularly
    structured). This clever transformation, illustrated in [Figure 9-15](#an_rnn_through_time),
    is often referred to as “unrolling” the RNN through time. Let’s consider the example
    RNN in the figure. We’d like to map a sequence of two inputs (each dimension 1)
    to a single output (also of dimension 1). We perform the transformation by taking
    the neurons of the single recurrent layer and replicating them it *t* times, once
    for each time step. We similarly replicate the neurons of the input and output
    layers. We redraw the feed-forward connections within each time replica just as
    they were in the original network. Then we draw the recurrent connections as feed-forward
    connections from each time replica to the next (since the recurrent connections
    carry the neuronal activation from the previous time step).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0915.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Figure 9-15\. We can run an RNN through time to express it as a feed-forward
    network that we can train using backpropagation
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can also now train the RNN by computing the gradient based on the unrolled
    version. This means that all of the backpropagation techniques that we used for
    feed-forward networks also apply to training RNNs. We do run into one issue, however.
    After every batch of training examples we use, we need to modify the weights based
    on the error derivatives we calculate. In our unrolled network, we have sets of
    connections that all correspond to the same connection in the original RNN. The
    error derivatives calculated for these unrolled connections, however, are not
    guaranteed to be (and, in practice, probably won’t be) equal. We can circumvent
    this issue by averaging or summing the error derivatives over all the connections
    that belong to the same set. This allows us to utilize an error derivative that
    considers all of the dynamics acting on the weight of a connection as we attempt
    to force the network to construct an accurate output.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The Challenges with Vanishing Gradients
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our motivation for using a stateful network model hinges on this idea of capturing
    long-term dependencies in the input sequence. It seems reasonable that an RNN
    with a large memory bank (i.e., a significantly sized recurrent layer) would be
    able to summarize these dependencies. In fact, from a theoretical perspective,
    Kilian and Siegelmann demonstrated in 1996 that the RNN is a universal functional
    representation.^([4](ch09.xhtml#idm45934164070368)) In other words, with enough
    neurons and the right parameter settings, an RNN can be used to represent any
    functional mapping between input and output sequences.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The theory is promising, but it doesn’t necessarily translate to practice. While
    it is nice to know that it is *possible* for an RNN to represent any arbitrary
    function, it is more useful to know whether it is *practical* to teach the RNN
    a realistic functional mapping from scratch by applying gradient descent algorithms.
    If it turns out to be impractical, we’ll be in hot water, so it will be useful
    for us to be rigorous in exploring this question. Let’s start our investigation
    by considering the simplest possible RNN, shown in [Figure 9-16](#single_neuron_fully_connected),
    with a single input neuron, a single output neuron, and a fully connected recurrent
    layer with one neuron.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0916.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Figure 9-16\. A single neuron, fully connected recurrent layer (both compressed
    and unrolled) for the sake of investigating gradient-based learning algorithms
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s start off simple. Given nonlinearity <math alttext="f"><mi>f</mi></math>
    , we can express the activation <math alttext="h Superscript left-parenthesis
    t right-parenthesis"><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></math>
     of the hidden neuron of the recurrent layer at time step *t* as follows, where
    <math alttext="i Superscript left-parenthesis t right-parenthesis"><msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></math>  is the incoming logit
    from the input neuron at time step *t*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="h Superscript left-parenthesis t right-parenthesis Baseline equals
    f left-parenthesis w Subscript i n Superscript left-parenthesis t right-parenthesis
    Baseline i Superscript left-parenthesis t right-parenthesis Baseline plus w Subscript
    r e c Superscript left-parenthesis t minus 1 right-parenthesis Baseline h Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline right-parenthesis"><mrow><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>=</mo> <mi>f</mi> <mfenced
    separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="h Superscript left-parenthesis t right-parenthesis Baseline equals
    f left-parenthesis w Subscript i n Superscript left-parenthesis t right-parenthesis
    Baseline i Superscript left-parenthesis t right-parenthesis Baseline plus w Subscript
    r e c Superscript left-parenthesis t minus 1 right-parenthesis Baseline h Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline right-parenthesis"><mrow><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>=</mo> <mi>f</mi> <mfenced
    separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: 'Let’s try to compute how the activation of the hidden neuron changes in response
    to changes to the input logit from *k* time steps in the past. In analyzing this
    component of the backpropagation gradient expressions, we can start to quantify
    how much “memory” is retained from past inputs. We start by taking the partial
    derivative and apply the chain rule:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction equals f prime left-parenthesis
    w Subscript i n Superscript left-parenthesis t right-parenthesis Baseline i Superscript
    left-parenthesis t right-parenthesis Baseline plus w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline h Superscript left-parenthesis
    t minus 1 right-parenthesis Baseline right-parenthesis StartFraction normal partial-differential
    Over normal partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction left-parenthesis w Subscript i n Superscript left-parenthesis
    t right-parenthesis Baseline i Superscript left-parenthesis t right-parenthesis
    Baseline plus w Subscript r e c Superscript left-parenthesis t minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus 1 right-parenthesis Baseline right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow> <mrow><mi>∂</mi><msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <mfrac><mi>∂</mi> <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction equals f prime left-parenthesis
    w Subscript i n Superscript left-parenthesis t right-parenthesis Baseline i Superscript
    left-parenthesis t right-parenthesis Baseline plus w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline h Superscript left-parenthesis
    t minus 1 right-parenthesis Baseline right-parenthesis StartFraction normal partial-differential
    Over normal partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction left-parenthesis w Subscript i n Superscript left-parenthesis
    t right-parenthesis Baseline i Superscript left-parenthesis t right-parenthesis
    Baseline plus w Subscript r e c Superscript left-parenthesis t minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus 1 right-parenthesis Baseline right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow> <mrow><mi>∂</mi><msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <mfrac><mi>∂</mi> <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: 'Because the values of the input and recurrent weights are independent of the
    input logit at time step  <math alttext="t minus k"><mrow><mi>t</mi> <mo>-</mo>
    <mi>k</mi></mrow></math> , we can further simplify this expression:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction equals f prime left-parenthesis
    w Subscript i n Superscript left-parenthesis t right-parenthesis Baseline i Superscript
    left-parenthesis t right-parenthesis Baseline plus w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline h Superscript left-parenthesis
    t minus 1 right-parenthesis Baseline right-parenthesis w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline StartFraction normal partial-differential
    h Superscript left-parenthesis t minus 1 right-parenthesis Baseline Over normal
    partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction equals f prime left-parenthesis
    w Subscript i n Superscript left-parenthesis t right-parenthesis Baseline i Superscript
    left-parenthesis t right-parenthesis Baseline plus w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline h Superscript left-parenthesis
    t minus 1 right-parenthesis Baseline right-parenthesis w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline StartFraction normal partial-differential
    h Superscript left-parenthesis t minus 1 right-parenthesis Baseline Over normal
    partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: 'Because we care about the magnitude of this derivative, we can take the absolute
    value of both sides. We also know that for all common nonlinearities (the tanh,
    logistic, and ReLU nonlinearities), the maximum value of  <math alttext="StartAbsoluteValue
    f prime EndAbsoluteValue"><mfenced separators="" open="|" close="|"><msup><mi>f</mi>
    <mo>''</mo></msup></mfenced></math>  is at most 1\. This leads to the following
    recursive inequality:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue
    StartFraction normal partial-differential h Superscript left-parenthesis t minus
    1 right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction EndAbsoluteValue"><mrow><mfenced
    separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced></mrow></math>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue
    StartFraction normal partial-differential h Superscript left-parenthesis t minus
    1 right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction EndAbsoluteValue"><mrow><mfenced
    separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced></mrow></math>
- en: 'We can continue to expand this inequality recursively until we reach the base
    case, at step  <math alttext="t minus k"><mrow><mi>t</mi> <mo>-</mo> <mi>k</mi></mrow></math>
    :'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot ellipsis
    dot StartAbsoluteValue w Subscript r e c Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue StartFraction
    normal partial-differential h Superscript left-parenthesis t minus k right-parenthesis
    Baseline Over normal partial-differential i Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndFraction EndAbsoluteValue"><mrow><mfenced separators=""
    open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mo>...</mo> <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced></mrow></math>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot ellipsis
    dot StartAbsoluteValue w Subscript r e c Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue StartFraction
    normal partial-differential h Superscript left-parenthesis t minus k right-parenthesis
    Baseline Over normal partial-differential i Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndFraction EndAbsoluteValue"><mrow><mfenced separators=""
    open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mo>...</mo> <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced></mrow></math>
- en: 'We can evaluate this partial derivative similarly to how we proceeded previously:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="h Superscript left-parenthesis t minus k right-parenthesis Baseline
    equals f left-parenthesis w Subscript i n Superscript left-parenthesis t minus
    k right-parenthesis Baseline i Superscript left-parenthesis t minus k right-parenthesis
    Baseline plus w Subscript r e c Superscript left-parenthesis t minus k minus 1
    right-parenthesis Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline right-parenthesis"><mrow><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <mi>f</mi> <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="h Superscript left-parenthesis t minus k right-parenthesis Baseline
    equals f left-parenthesis w Subscript i n Superscript left-parenthesis t minus
    k right-parenthesis Baseline i Superscript left-parenthesis t minus k right-parenthesis
    Baseline plus w Subscript r e c Superscript left-parenthesis t minus k minus 1
    right-parenthesis Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline right-parenthesis"><mrow><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <mi>f</mi> <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t minus k right-parenthesis Baseline Over normal partial-differential i Superscript
    left-parenthesis t minus k right-parenthesis Baseline EndFraction equals f prime
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis StartFraction normal partial-differential Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <mfrac><mi>∂</mi> <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t minus k right-parenthesis Baseline Over normal partial-differential i Superscript
    left-parenthesis t minus k right-parenthesis Baseline EndFraction equals f prime
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis StartFraction normal partial-differential Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <mfrac><mi>∂</mi> <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: 'In this expression, the hidden activation at time  <math alttext="t minus k
    minus 1"><mrow><mi>t</mi> <mo>-</mo> <mi>k</mi> <mo>-</mo> <mn>1</mn></mrow></math>
     is independent of the value of the input at <math alttext="t minus k"><mrow><mi>t</mi>
    <mo>-</mo> <mi>k</mi></mrow></math> . Thus we can rewrite this expression as:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t minus k right-parenthesis Baseline Over normal partial-differential i Superscript
    left-parenthesis t minus k right-parenthesis Baseline EndFraction equals f prime
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mrow></math>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t minus k right-parenthesis Baseline Over normal partial-differential i Superscript
    left-parenthesis t minus k right-parenthesis Baseline EndFraction equals f prime
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mrow></math>
- en: 'Finally, taking the absolute value on both sides and again applying the observation
    about the maximum value of  <math alttext="StartAbsoluteValue f prime EndAbsoluteValue"><mfenced
    separators="" open="|" close="|"><msup><mi>f</mi> <mo>''</mo></msup></mfenced></math>
    , we can write:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t minus k right-parenthesis Baseline Over normal
    partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue
    w Subscript i n Superscript left-parenthesis t minus k right-parenthesis Baseline
    EndAbsoluteValue"><mrow><mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced></mrow></math>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t minus k right-parenthesis Baseline Over normal
    partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue
    w Subscript i n Superscript left-parenthesis t minus k right-parenthesis Baseline
    EndAbsoluteValue"><mrow><mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced></mrow></math>
- en: 'This results in the final inequality (which we can simplify because we constrain
    the connections at different time steps to have equal value):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot ellipsis
    dot StartAbsoluteValue w Subscript r e c Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue w Subscript
    i n Superscript left-parenthesis t minus k right-parenthesis Baseline EndAbsoluteValue
    equals StartAbsoluteValue w Subscript r e c Baseline EndAbsoluteValue Superscript
    k Baseline dot w Subscript i n"><mrow><mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow> <mrow><mi>∂</mi><msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mo>...</mo> <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>=</mo> <msup><mfenced separators="" open="|" close="|"><msub><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow></msub></mfenced>
    <mi>k</mi></msup> <mo>·</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></math>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot ellipsis
    dot StartAbsoluteValue w Subscript r e c Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue w Subscript
    i n Superscript left-parenthesis t minus k right-parenthesis Baseline EndAbsoluteValue
    equals StartAbsoluteValue w Subscript r e c Baseline EndAbsoluteValue Superscript
    k Baseline dot w Subscript i n"><mrow><mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow> <mrow><mi>∂</mi><msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mo>...</mo> <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>=</mo> <msup><mfenced separators="" open="|" close="|"><msub><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow></msub></mfenced>
    <mi>k</mi></msup> <mo>·</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></math>
- en: This relationship places a strong upper bound on how much a change in the input
    at time  <math alttext="t minus k"><mrow><mi>t</mi> <mo>-</mo> <mi>k</mi></mrow></math>
     can impact the hidden state at time *t*. Because the weights of our model are
    initialized to small values at the beginning of training, the value of this derivative
    approaches zero as *k* increases. In other words, the gradient quickly diminishes
    when it’s computed with respect to inputs several time steps into the past, severely
    limiting our model’s ability to learn long-term dependencies. This issue is commonly
    referred to as the problem of *vanishing gradients*, and it severely impacts the
    learning capabilities of vanilla RNNs. In order to address this limitation, we
    will spend the next section exploring an extraordinarily influential twist on
    recurrent layers known as long short-term memory.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory Units
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To combat the problem of vanishing gradients, Sepp Hochreiter and Jürgen Schmidhuber
    introduced the *long short-term memory* (LSTM) architecture. The basic principle
    behind the architecture was that the network would be designed for the purpose
    of reliably transmitting important information many time steps into the future.
    The design considerations resulted in the architecture shown in [Figure 9-17](#architecture_of_an_lstm_unit).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0917.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: Figure 9-17\. The architecture of an LSTM unit, illustrated at a tensor (designated
    by arrows) and operation (designated by the inner blocks) level
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-17\. LSTM单元的架构，以张量（由箭头表示）和操作（由内部块表示）级别进行说明
- en: For the purposes of this discussion, we’ll take a step back from the individual
    neuron level and start talking about the network as collection tensors and operations
    on tensors. As the figure indicates, the LSTM unit is composed of several key
    components. One of the core components of the LSTM architecture is the *memory
    cell*, a tensor represented by the bolded loop in the center of the figure. The
    memory cell holds critical information that it has learned over time, and the
    network is designed to effectively maintain useful information in the memory cell
    over many time steps. At every time step, the LSTM unit modifies the memory cell
    with new information with three different phases. First, the unit must determine
    how much of the previous memory to keep. This is determined by the *keep gate*,
    shown in detail in [Figure 9-18](#architecture_of_the_keep_gate).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了讨论的目的，我们将从单个神经元级别退后一步，开始讨论网络作为张量集合和张量上的操作。正如图所示，LSTM单元由几个关键组件组成。 LSTM架构的一个核心组件是*内存单元*，在图中心的粗体循环表示的张量。内存单元保存了它随时间学到的关键信息，并且网络被设计为在许多时间步长上有效地保持内存单元中的有用信息。在每个时间步长，LSTM单元通过三个不同的阶段用新信息修改内存单元。首先，单元必须确定要保留多少先前的记忆。这由*保持门*确定，详细显示在[图9-18](#architecture_of_the_keep_gate)中。
- en: '![](Images/fdl2_0918.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0918.png)'
- en: Figure 9-18\. Architecture of the keep gate of an LSTM unit
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-18\. LSTM单元保持门的架构
- en: The basic idea of the keep gate is simple. The memory state tensor from the
    previous time step is rich with information, but some of that information may
    be stale (and therefore might need to be erased). We figure out which elements
    in the memory state tensor are still relevant and which elements are irrelevant
    by trying to compute a bit tensor (a tensor of zeros and ones) that we multiply
    with the previous state. If a particular location in the bit tensor holds a 1,
    it means that location in the memory cell is still relevant and ought to be kept.
    If that particular location instead holds a 0, it means that the location in the
    memory cell is no longer relevant and ought to be eased. We approximate this bit
    tensor by concatenating the input of this time step and the LSTM unit’s output
    from the previous time step and applying a sigmoid layer to the resulting tensor.
    A sigmoidal neuron, as you may recall, outputs a value that is either very close
    to 0 or very close to 1 most of the time (the only exception is when the input
    is close to 0). As a result, the output of the sigmoidal layer is a close approximation
    of a bit tensor, and we can use this to complete the keep gate.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 保持门的基本思想很简单。来自上一个时间步的内存状态张量充满信息，但其中一些信息可能已经过时（因此可能需要被擦除）。我们通过尝试计算一个比特张量（一个由零和一组成的张量）来弄清楚内存状态张量中哪些元素仍然相关，哪些元素是无关的。如果比特张量中的特定位置包含1，则表示内存单元中的该位置仍然相关且应该保留。如果该特定位置相反包含0，则表示内存单元中的该位置不再相关且应该被擦除。我们通过将本时间步的输入和上一个时间步的LSTM单元输出连接起来，并对结果张量应用sigmoid层来近似这个比特张量。正如您可能记得的那样，sigmoid神经元大部分时间输出接近0或接近1的值（唯一的例外是当输入接近0时）。因此，sigmoid层的输出是比特张量的一个接近近似，我们可以使用这个来完成保持门。
- en: Once we’ve figured out what information to keep in the old state and what to
    erase, we’re ready to think about what information we’d like to write into the
    memory state. This part of the LSTM unit is known as the *write gate*, and it’s
    depicted in [Figure 9-19](#architecture_of_the_write_gate). This is broken down
    into two major parts. The first component is figuring out what information we’d
    like to write into the state. This is computed by the tanh layer to create an
    intermediate tensor. The second component is figuring out which components of
    this computed tensor we actually want to include into the new state and which
    we want to toss before writing. We do this by approximating a bit vector of 0’s
    and 1’s using the same strategy (a sigmoidal layer) as we used in the keep gate.
    We multiply the bit vector with our intermediate tensor and then add the result
    to create the new state vector for the LSTM.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们弄清楚了要在旧状态中保留什么信息和要擦除什么信息，我们就准备考虑我们想要写入内存状态的信息。这部分LSTM单元称为*写入门*，在[图9-19](#architecture_of_the_write_gate)中描述。这可以分解为两个主要部分。第一个组件是弄清楚我们想要写入状态的信息。这通过tanh层计算以创建一个中间张量。第二个组件是弄清楚我们实际上想要包含到新状态中的计算张量的哪些组件，以及我们在写入之前想要丢弃哪些组件。我们通过使用与我们在保持门中使用的相同策略（一个sigmoid层）来近似一个由0和1组成的比特向量。我们将比特向量与我们的中间张量相乘，然后将结果相加以创建LSTM的新状态向量。
- en: '![](Images/fdl2_0919.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0919.png)'
- en: Figure 9-19\. Architecture of the write gate of an LSTM unit
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-19\. LSTM单元写入门的架构
- en: 'At every time step, we’d like the LSTM unit to provide an output. While we
    could treat the state vector as the output directly, the LSTM unit is engineered
    to provide more flexibility by emitting an output tensor that is an “interpretation”
    or external “communication” of what the state vector represents. The architecture
    of the output gate is shown in [Figure 9-20](#architecture_of_the_output_gate).
    We use a nearly identical structure as the write gate: (1) the tanh layer creates
    an intermediate tensor from the state vector, (2) the sigmoid layer produces a
    bit tensor mask using the current input and previous output, and (3) the intermediate
    tensor is multiplied with the bit tensor to produce the final output.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0920.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: Figure 9-20\. Architecture of the output gate of an LSTM unit
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So why is this better than using a raw RNN unit? The key observation is how
    information propagates through the network when we unroll the LSTM unit through
    time. The unrolled architecture is shown in [Figure 9-21](#unrolling_an_lstm_unit).
    At the top, we can observed the propagation of the state vector, whose interactions
    are primarily linear through time. The result is that the gradient that relates
    an input several time steps in the past to the current output does not attenuate
    as dramatically as in the vanilla RNN architecture. This means that the LSTM can
    learn long-term relationships much more effectively than our original formulation
    of the RNN.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0921.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: Figure 9-21\. Unrolling an LSTM unit through time
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, we want to understand how easy it is to generate arbitrary architectures
    with LSTM units. How “composable” are LSTMs? Do we need to sacrifice flexibility
    to use LSTM units instead of a vanilla RNN? Just as we can we can stack RNN layers
    to create more expressive models with more capacity, we can stack LSTM units,
    where the input of the second unit is the output of the first unit, the input
    of the third unit is the output of the second, and so on. [Figure 9-22](#composimg_lstm_units)
    shows how this works with a multicellular architecture made of two LSTM units.
    This means that anywhere we use a vanilla RNN layer, we can easily substitute
    an LSTM unit.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0922.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 9-22\. Composing LSTM units as one might stack recurrent layers in a
    neural network
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we have overcome the issue of vanishing gradients and understand the
    inner workings of LSTM units, we’re ready to dive into the implementation of our
    first RNN models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Primitives for RNN Models
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch provides seceral primitives that we can use out of the box in order
    to build RNN models. First, we have`torch.nn.RNNCell` objects that represent either
    an RNN layer or an LSTM unit:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `RNNCell` abstraction represents a vanilla recurrent neuron layer, while
    the `LSTMCell` represents an implementation of the LSTM unit. PyTorch also includes
    a variation of the LSTM unit known as the *Gated Recurrent Unit* (GRU), proposed
    in 2014 by Yoshua Bengio’s group. The critical initialization variable for all
    of these cells is the size of the hidden state vector or `hidden_size`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the primitives, PyTorch provides multilayer RNN and LSTM classes
    for stacking layers. If we want to stack recurrent units or layers, we can use
    the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can also use the `dropout` parameter to apply dropout to the inputs and
    outputs of an LSTM with specified keep probability. If the `dropout` parameter
    is nonzero, the model introduces a dropout layer on the outputs of each LSTM layer
    except the last layer, with dropout probability equal to `dropout`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As shown here, the multilayer RNN and LSTM classes also provide a `batch_first`
    parameter. If `batch_first` equals `True`, then the input and output tensors are
    provided as `(batch, seq, feature)` instead of `(seq, batch, feature)`. Note that
    this does not apply to hidden or cell states. The default value of `batch_first`
    is `False`. See the PyTorch documentation for details.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we instantiate an RNN by calling the PyTorch LSTM constructor:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The result of calling `rnn` is a tensor representing the outputs of the RNN,
    `output_n`, along with the final state vectors for each layer. The first tensor,
    `hn`, contains the hidden state vectors for each layer that holds the outputs
    of the Output Gates at time, `n`. The second tensor, `cn`, contains the state
    vectors for the memory cells of each layer, which is the output of the write gates.
    Both `hn` and `cn` are of size `(n_layers, batch_size, hidden_size)`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an understanding of the tools at our disposal in constructing
    RNNs in PyTorch, we’ll build our first LSTM in the next section, focused on the
    task of sentiment analysis.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Sentiment Analysis Model
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we attempt to analyze the sentiment of movie reviews taken
    from the Large Movie Review Dataset. This dataset consists of 50,000 reviews from
    IMDb, each of which is labeled as having positive or negative sentiment. We use
    a simple LSTM model leveraging dropout to learn how to classify the sentiment
    of movie reviews. The LSTM model will consume the movie review one word at a time.
    Once it has consumed the entire review, we’ll use its output as the basis of a
    binary classification to map the sentiment to be “positive” or “negative.”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start off by loading the dataset with the PyTorch library Torchtext,
    which comes preinstalled with Google Colab. If you’re running on another machine,
    you can install Torchtext by running the following command:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once we’ve installed the package, we can download the dataset and define a tokenizer.
    Torchtext provides many natural language processing (NLP) datasets and tokenizers
    through the `torchtext.datasets` and `torchtext.data.utils` submodules, respectively.
    We’ll use the built-in IMDb dataset and standard `'basic_english'` tokenizer provided
    by PyTorch.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Until now, we’ve been using map-style datasets from PyTorch. Torchtext returns
    NLP datasets as iterable-style datasets, which are more appropriate for streaming
    data. Next, we need to create a vocabulary based on the training dataset and prune
    the vocabulary to include only the 30,000 most common words. Then, we need to
    pad each input sequence up to a length of 500 words, and process the labels.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As shown, Torchtext provides a function, `build_vocab_from_iterator`, to create
    a vocabulary. However, this function expects a list of tokens as input, where
    `next(train_iter)`would return a tuple `(label_string, review_string)`. To satisfy
    this requirement, we define a function to yield tokens as the dataset is iterated.
    Finally, we add special tokens for unknown and padding, and set the default.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to actually prune the vocabulary and pad the review sequences,
    as well as convert the label strings, `''neg''` or `''pos''`, to numbers. We accomplish
    this by defining a pipeline function for both the labels and review strings:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `text_pipeline` function converts the inputs to 500-dimensional vectors.
    Each vector corresponds to a movie review where the <math alttext="i Superscript
    t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math> component
    of the vector corresponds to the index of the  <math alttext="i Superscript t
    h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>  word of the
    review in our global dictionary of 30,000 words. To complete the data preparation,
    we create a special Python class designed to serve minibatches of a desired size
    from the underlying dataset.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the built-in `DataLoader` class from PyTorch to sample the dataset
    in batches. Before we do so, we need to define a function, `collate_batch`, that
    will tell the `DataLoader` how to preprocess each batch:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `collate_batch` function simply runs the labels and review strings through
    each respective pipeline and returns the batch as a tuple of tensors `(labels_batch,
    reviews_batch)`. Once the `collate_fn` is defined, we simply load the dataset
    using the IMDb constructor, and configure the dataloaders using the `DataLoader`
    constructor:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We use the `torchtext.datasets.IMDB` Python class to serve both the training
    and validation sets we’ll use while training our sentiment analysis model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data is ready to go, we’ll begin to construct the sentiment analysis
    model, step by step. First, we’ll want to map each word in the input review to
    a word vector. To do this, we’ll utilize an embedding layer, which, as you may
    recall from [Chapter 8](ch08.xhtml#embedding_and_representing_learning), is a
    simple lookup table that stores an embedding vector that corresponds to each word.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike in previous examples, where we treated the learning of the word embeddings
    as a separate problem (i.e., by building a Skip-Gram model), we’ll learn the word
    embeddings jointly with the sentiment analysis problem by treating the embedding
    matrix as a matrix of parameters in the full problem. We accomplish this by using
    the PyTorch primitives for managing embeddings (remember that `input` represents
    one full minibatch at a time, not just one movie review vector):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We then take the result of the embedding layer and build an LSTM with dropout
    using the primitives we saw in the previous section. The implementation of the
    LSTM can be achieved as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We top it all off using a batch-normalized hidden layer, identical to the ones
    we’ve used time and time again in previous examples. Stringing all of these components
    together, we can build the model by calling `TextClassifier`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We omit the other boilerplate involved in setting up summary statistics, saving
    intermediate snapshots, and creating the session because it’s identical to the
    other models we’ve built in this book (see the [GitHub repository](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book)).
    We can then run and visualize the performance of our model using TensorBoard ([Figure 9-23](#fig0723)).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0923.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 9-23\. Training cost, validation cost, and accuracy of our movie review
    sentiment model
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the beginning of training, the model struggles slightly with stability, and
    toward the end of the training, the model clearly starts to overfit as training
    cost and validation cost significantly diverge. At its optimal performance, however,
    the model performs rather effectively and generalizes to approximately 86% accuracy
    on the test set. Congratulations! You’ve built your first RNN.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Solving seq2seq Tasks with Recurrent Neural Networks
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we’ve built a strong understanding of RNNs, we’re ready to revisit
    the problem of seq2seq. We started off this chapter with an example of a seq2seq
    task: mapping a sequence of words in a sentence to a sequence of POS tags. Tackling
    this problem was tractable because we didn’t need to take into account long-term
    dependencies to generate the appropriate tags. But there are several seq2seq problems,
    such as translating between languages or creating a summary for a video, where
    long-term dependencies are crucial to the success of the model. This is where
    RNNs come in.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The RNN approach to seq2seq looks a lot like the autoencoder we discussed in
    the previous chapter. The seq2seq model is composed of two separate networks.
    The first network is known as the *encoder* network. The encoder network is a
    recurrent network (usually one that uses LSTM units) that consumes the entire
    input sequence. The goal of the encoder network is to generate a condensed understanding
    of the input and summarize it into a singular thought represented by the final
    state of the encoder network. Then we use a *decoder* network, whose starting
    state is initialized with the final state of the encoder network, to produce the
    target output sequence token by token. At each step, the decoder network consumes
    its own output from the previous time step as the current time step’s input. The
    entire process is visualized in [Figure 9-24](#encoder_decoder_recurrent_network_schema).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0924.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 9-24\. How we use an encoder/decoder recurrent network schema to tackle
    seq2seq problems
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this this setup, we are attempting to translate an English sentence into
    French. We tokenize the input sentence and use an embedding (similar to our approach
    in the sentiment analysis model we built in the previous section), one word at
    a time as an input to the encoder network. At the end of the sentence, we use
    a special “end-of-sequence” (EOS) token to indicate the end of the input sequence
    to the encoder network. Then we take the hidden state of the encoder network and
    use that as the initialization of the decoder network. The first input to the
    decoder network is the EOS token, and the output is interpreted as the first word
    of the predicted French translation. From that point onward, we use the output
    of the decoder network as the input to itself at the next time step. We continue
    until the decoder network emits an EOS token as its output, at which point we
    know that the network has completed producing the translation of the original
    English sentence. We’ll dissect the practical, open source implementation of this
    network (with a couple of enhancements and tricks to improve accuracy) later in
    this chapter.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The seq2seq RNN architecture can also be reappropriated for the purpose of learning
    good embeddings of sequences. For example, Kiros et al. in 2015 invented the notion
    of a *skip-thought vector*,^([5](ch09.xhtml#idm45934164517408)) which borrowed
    architectural characteristics from both the autoencoder framework and the Skip-Gram
    model discussed in [Chapter 8](ch08.xhtml#embedding_and_representing_learning).
    The skip-thought vector was generated by dividing a passage into a set of triplets
    consisting of consecutive sentences. The authors utilized a single encoder network
    and two decoder networks, as shown in [Figure 9-25](#skip_thought_seq2seq_architecture).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0925.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 9-25\. The skip-thought seq2seq architecture to generate embedding representations
    of entire sentences
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The encoder network consumed the sentence for which we wanted to generate a
    condensed representation (which was stored in the final hidden state of the encoder
    network). Then came the decoding step. The first of the decoder networks would
    take that representation as the initialization of its own hidden state and attempt
    to reconstruct the sentence that appeared prior to the input sentence. The second
    decoder network would instead attempt the sentence that appeared immediately after
    the input sentence. The full system was trained end to end on these triplets,
    and once completed, could be used to generate seemingly cohesive passages of text
    in addition to improve performance on key sentence-level classification tasks.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of story generation, excerpted from the original paper:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that we’ve developed an understanding of how to leverage RNNs to tackle
    seq2seq problems, we’re almost ready to try to build our own. Before we get there,
    however, we’ve got one more major challenge to tackle, and we’ll address it head-on
    in the next section when we discuss the concept of attentions in seq2seq RNNs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting Recurrent Networks with Attention
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s think harder about the translation problem. If you’ve ever attempted to
    learn a foreign language, you’ll know that there are several helpful steps when
    trying to complete a translation. First, it’s helpful to read the full sentence
    to understand the concept you would like to convey. Then you write out the translation
    one word at a time, each word following logically from the word you wrote previously.
    But one important aspect of translation is that as you compose the new sentence,
    you often refer back to the original text, focusing on specific parts that are
    relevant to your current translation. At each step, you are paying attention to
    the most relevant parts of the original “input” so you can make the best decision
    about the next word to put on the page.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Recall our approach to seq2seq. By consuming the full input and summarizing
    it into a “thought” inside its hidden state, the encoder network effectively achieves
    the first part of the translation process. By using the previous output as its
    current input, the decoder network achieves the second part of the translation
    process. This phenomenon of *attention* has yet to be captured by our approach
    to seq2seq, and this is the final building block we’ll need to engineer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the sole input to the decoder network at a given time step *t* is
    its output at time step  <math alttext="t minus 1"><mrow><mi>t</mi> <mo>-</mo>
    <mn>1</mn></mrow></math> . One way to give the decoder network some vision into
    the original sentence is by giving the decoder access to all of the outputs from
    the encoder network (which we previously had completely ignored). These outputs
    are interesting to us because they represent how the encoder network’s internal
    state evolves after seeing each new token. A proposed implementation of this strategy
    is shown in [Figure 9-26](#attempt_at_engineering_attentional_abilities). This
    attempt falls short because it fails to dynamically select the most relevant parts
    of the input to focus on.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0926.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 9-26\. An attempt at engineering attentional abilities in a seq2seq architecture
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This approach has a critical flaw, however. The problem here is that at every
    time step, the decoder considers all of the outputs of the encoder network in
    the exact same way. However, this is clearly not the case for a human during the
    translation process. We focus on different aspects of the original text when working
    on different parts of the translation. The key realization here is that it’s not
    enough to merely give the decoder access to all the outputs. Instead, we must
    engineer a mechanism by which the decoder network can dynamically pay attention
    to a specific subset of the encoder’s outputs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: We can fix this problem by changing the inputs to the concatenation operation,
    using the proposal in Bahdanau et al. 2015 as inspiration.^([6](ch09.xhtml#idm45934164490960))
    Instead of directly using the raw outputs from the encoder network, we perform
    a weighting operation on the encoder’s outputs. We leverage the decoder network’s
    state at time  <math alttext="t minus 1"><mrow><mi>t</mi> <mo>-</mo> <mn>1</mn></mrow></math>
     as the basis for the weighting operation.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The weighting operation is illustrated in [Figure 9-27](#modification_to_our_original_proposal).
    First we create a scalar (a single number, not a tensor) relevance score for each
    of the encoder’s outputs. The score is generated by computing the dot product
    between each encoder output and the decoder’s state at time  <math alttext="t
    minus 1"><mrow><mi>t</mi> <mo>-</mo> <mn>1</mn></mrow></math> . We then normalize
    these scores using a softmax operation. Finally, we use these normalized scores
    to individually scale the encoder’s outputs before plugging them into the concatenation
    operation. The key here is that the relative scores computed for each encoder
    output signify how important that particular encoder output is to the decision
    for the decoder at time step *t*. In fact, as we’ll see later, we can visualize
    which parts of the input are most relevant to the translation at each time step
    by inspecting the output of the softmax.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0927.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 9-27\. A modification to our original proposal that enables a dynamic
    attentional mechanism based on the hidden state of the decoder network in the
    previous time step
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Armed with this strategy for engineering attention into seq2seq architectures,
    we’re finally ready to get our hands dirty with an RNN model for translating English
    sentences into French. But before we jump in, it’s worth noting that attentions
    are incredibly applicable in problems that extend beyond language translation.
    Attentions can be important in speech-to-text problems, where the algorithm learns
    to dynamically pay attention to corresponding parts of the audio while transcribing
    the audio into text. Similarly, attentions can be used to improve image captioning
    algorithms by helping the captioning algorithm focus on specific parts of the
    input image while writing out the caption. Anytime particular parts of the input
    are highly correlated to correctly producing corresponding segments of the output,
    attentions can dramatically improve performance.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Dissecting a Neural Translation Network
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: State-of-the-art neural translation networks use a number of different techniques
    and advancements that build on the basic seq2seq encoder-decoder architecture.
    Attention, as detailed in the previous section, is an important and critical architectural
    improvement. In this section, we will dissect a fully implemented neural machine
    translation system, complete with the data processing steps, building the model,
    training it, and eventually using it as a translation system to convert English
    phrases to French phrases.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline used in training and eventually using a neural machine translation
    system is similar to that of most machine learning pipelines: gather data, prepare
    the data, construct the model, train the model, evaluate the model’s progress,
    and eventually use the trained model to predict or infer something useful. We
    review each of these steps here.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: We first gather the data from the [International Workshop on Spoken Language
    Translation (IWSLT2016) repository](https://wit3.fbk.eu/2016-01), which houses
    large corpora used in training translation systems. For our use case, we’ll be
    using the English-to-French data. Note that if we want to be able to translate
    to or from different languages, we would have to train a model from scratch with
    the new data. We then preprocess our data into a format that is easily usable
    by our models during training and inference time. This will involve some amount
    of cleaning and tokenizing the sentences in each of the English and French phrases.
    What follows now is a set of techniques used in preparing the data, and later
    we will present the implementations of the techniques.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to parse sentences and phrases into formats that are more
    compatible with the model by *tokenization*. This is the process by which we discretize
    a particular English or French sentence into its constituent tokens. For instance,
    a simple word-level tokenizer will consume the sentence “I read.” to produce the
    array ["I”, “read”, “."], or it would consume the French sentence “Je lis.” to
    produce the array ["Je”, “lis”, “."].
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: A character-level tokenizer may break the sentence into individual characters
    or into pairs of characters like ["I”, " “, “r”, “e”, “a”, “d”, “."] and ["I “,
    “re”, “ad”, “."], respectively. One kind of tokenization may work better than
    the other, and each has its pros and cons. For instance, a word-level tokenizer
    will ensure that the model produces words that are from some dictionary, but the
    size of the dictionary may be too large to efficiently choose from during decoding.
    This is in fact a known issue and something that we’ll address in the coming discussions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the decoder using a character-level tokenization may not
    produce intelligible outputs, but the total dictionary that the decoder must choose
    from is much smaller, as it is simply the set of all printable ASCII characters.
    In this tutorial, we use a word-level tokenization, but we encourage you to experiment
    with different tokenizations to observe the effects this has. It is worth noting
    that we must also add a special EOS character, to the end of all output sequences
    because we need to provide a definitive way for the decoder to indicate that it
    has reached the end of its decoding. We can’t use regular punctuation because
    we cannot assume that we are translating full sentences. Note that we do not need
    EOS characters in our source sequences because we are feeding these in preformatted
    and do not need an EOS character for ourselves to denote the end of our source
    sequence.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: The next optimization involves further modifying how we represent each source
    and target sequence, and we introduce a concept called *bucketing*. This is a
    method employed primarily in sequence-to-sequence tasks, especially machine translation,
    that helps the model efficiently handle sentences or phrases of different lengths.
    We first describe the naive method of feeding in training data and illustrate
    the shortcomings of this approach. Normally, when feeding in encoder and decoder
    tokens, the length of the source sequence and the target sequence is not always
    equal between pairs of examples. For example, the source sequence may have length
    *X*, and the target sequence may have length *Y*. It may seem that we need different
    seq2seq networks to accommodate each (*X, Y*) pair, yet this immediately seems
    wasteful and inefficient. Instead, we can do a little better if we *pad* each
    sequence up to a certain length, as shown in [Figure 9-28](#naive_strategy_for_padding_sequences),
    assuming we use a word-level tokenization and that we’ve appended EOS tokens to
    our target sequences.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0928.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 9-28\. Naive strategy for padding sequences
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This step saves us the trouble of having to construct a different seq2seq model
    for each pair of source and target lengths. However, this introduces a different
    issue: if there were a very long sequence, it would mean that we would have to
    pad every other sequence *up to that length*. This would make a short sequence
    padded to the end take as much computational resources as a long one with few
    pad tokens, which is wasteful and could introduce a major performance hit to our
    model. We could consider breaking up every sentence in the corpus into phrases
    such that the length of each phrase does not exceed a certain maximum limit, but
    it’s not clear how to break the corresponding translations. This is where bucketing
    helps us.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Bucketing is the idea that we can place encoder and decoder pairs into buckets
    of similar size, and only pad up to the maximum length of sequences in each respective
    bucket. For instance, we can denote a set of buckets, [(5, 10), (10, 15), (20,
    25), (30, 40)], where each tuple in the list is the maximum length of the source
    sequence and target sequence, respectively. Borrowing the preceding example, we
    can place the pair of sequences (["I”, “read”, “."], ["Je”, “lis”, “.”, “EOS"])
    in the first bucket, as the source sequence is smaller than 5 tokens and the target
    sequence is smaller than 10 tokens. We would then place the (["See”, “you”, “in”,
    “a”, “little”, “while"], ["A”, “tout”, “a”, “l’heure”, “EOS"]) in the second bucket,
    and so on. This technique allows us to compromise between the two extremes, where
    we need to pad only as much as necessary, as shown in [Figure 9-29](#padding_sequences_with_buckets).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0929.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Figure 9-29\. Padding sequences with buckets
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using bucketing shows a considerable speedup during training and test time,
    and allows developers and frameworks to write very optimized code to leverage
    the fact that any sequence from a bucket will have the same size and pack the
    data together in ways that allow even further GPU efficiency.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'With the sequences properly padded, we need to add one additional token to
    the target sequences: *a GO token*. This GO token will signal to the decoder that
    decoding needs to begin, at which point it will take over and begin decoding.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: The last improvement we make in the data preparation side is to reverse the
    source sequences. Researchers found that doing so improved performance, and this
    has become a standard trick to try when training neural machine translation models.
    This is a bit of an engineering hack, but consider the fact that our fixed-size
    neural state can hold only so much information, and information encoded while
    processing the beginning of the sentence may be overwritten while encoding later
    parts of the sentence. In many language pairs, the beginning of sentences is harder
    to translate than the end of sentences, so this hack of reversing the sentence
    improves translation accuracy by giving the beginning of the sentence the last
    say on what final state is encoded. With these ideas in place, the final sequences
    look as they do in [Figure 9-30](#padding_scheme_reversing_the_inputs).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0930.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 9-30\. Final padding scheme with buckets, reversing the inputs, and adding
    the GO token
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With these techniques described, we can now detail the implementation. First,
    we load the dataset, then we define our tokenizers and vocabularies. We do not
    define the word embeddings here, as we will train our model to compute them. PyTorch’s
    Torchtext library supports IWSLT2016 in `torch.text.datasets`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The dataset constructor returns an iterable-style dataset that can retrieve
    English and French sentence pairs with `next(train_iter)`. We’ll use this iterable-style
    dataset to create bucketed datasets for batching later in our code.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let’s also define our tokenizers and vocabularies for each language.
    PyTorch offers a `get_tokenizer` function that operates on common tokenizers.
    Here, we’ll use the `spacy` tokenizer for each language. You may need to download
    the `spacy` language files first:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once we have the language files, we can create the tokenizers as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we will create our vocabularies for English and French using PyTorch’s
    `build_vocab_from_iterator` function. This function takes tokens from an iterable-style
    dataset from a single language and creates a vocabulary. Since our dataset has
    both English and French sentences, we create a `yield_tokens` function to return
    only the English or French tokens, and pass this into `build_vocab_from_iterator`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Notice that we need to reload the `train_iter` before building the French vocabulary
    to restart the iterable-style dataset. We also add in the special tokens and their
    indices.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the dataset, tokenizers, and vocabularies, we need to create
    functions to preprocess the tokens and generate batches of bucketed data. First
    let’s define a `process_tokens` function to apply the improvements we discussed
    earlier:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this function, we pass in the variable size lists of source and target tokens,
    plus a list of bucket sizes. First, we decide on the smallest bucket size that
    will fit both the source and target token lists. Then, we process the source tokens
    by padding and reversing the sequence as described earlier. For the target tokens,
    we add a `<go>` token to the beginning and add an `<eos>` token to the end, then
    pad to the bucket size. When determining the smallest bucket size, we accounted
    for the two added tokens `<go>` and `<eos>`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a function that takes lists of source and target tokens and prepares
    them appropriately. Next, we need to collect a single batch of data for our model
    and training loop. To do this, we will use the built-in PyTorch `Dataset` and
    `DataLoader` classes.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: We are going to separate `Dataset` and `DataLoader` for each bucket size. This
    approach will enable us to use the built-in feature of the `DataLoader` for random
    batching and parallel processing.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'First we create a `BucketedDataset` class by subclassing PyTorch’s `Dataset`
    class. Since this will be a map-style dataset, we’ll need to define the `__getitem__`
    and `__len__` methods for data access:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We create a list of `BucketedDataset` objects in `bucketed_datasets`, one for
    each bucket size. The `BucketedDataset` constructor also converts our vocabulary
    integers to PyTorch tensors so we can pass them into our model later.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we use the PyTorch’s `DataLoader` class to create dataloaders for each
    dataset in `bucketed_datasets`. Since we created `Dataset` objects, we get the
    batching capabilities of the `DataLoader` class without writing any additional
    code:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The dataloaders list hold a dataloader for each bucket size, so when we run
    our training or test loops, we will select a bucket size (randomly for training)
    and use the corresponding dataloader to pull a batch of encoders and decoder inputs:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We measure the loss incurred during prediction time, as well as keep track
    of other running metrics:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Lastly, every so often, as dictated by a global variable, we will carry out
    a number of tasks. First, we print statistics for the previous batch, such as
    the loss, the learning rate, and the perplexity. If we find that the loss is not
    decreasing, the model may have fallen into a local optima. To assist the model
    in escaping this, we anneal the learning rate so that it won’t make large leaps
    in any particular direction. At this point, we also save a copy of the model and
    its weights and activations to disk.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the high-level details of training and using the models. We have
    largely abstracted away the fine details of the model itself. For more, see the
    [book’s repository](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: With this, we’ve successfully completed a full tour of the implementation details
    of a fairly sophisticated neural machine translation system. Production systems
    have additional tricks that are not as generalizable, and these systems are trained
    on huge compute servers to ensure that state-of-the-art performance is met.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: For reference, this exact model was trained on eight NVIDIA Telsa M40 GPUs for
    four days. We show plots for the perplexity in Figures [9-31](#plot_of_perplexity_on_training_data)
    and [9-32](#plot_of_learning_rate_over_time), and show the learning rate anneal
    over time as well. In [Figure 9-31](#plot_of_perplexity_on_training_data), we
    see that after 50,000 epochs, the perplexity decreases from about 6 to 4, which
    is a reasonable score for a neural machine translation system. In [Figure 9-32](#plot_of_learning_rate_over_time),
    we observe that the learning rate almost smoothly declines to 0\. This means that
    by the time we stopped training, the model was approaching a stable state.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0931.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: Figure 9-31\. Plot of perplexity on training data over time
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0932.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: Figure 9-32\. Plot of learning rate over time
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To showcase the attentional model more explicitly, we can visualize the attention
    that the decoder LSTM computes while translating a sentence from English to French.
    In particular, we know that as the encoder LSTM is updating its cell state in
    order to compress the sentence into continuous vector representations, it also
    computes hidden states at every time step. We know that the decoder LSTM computes
    a convex sum over these hidden states, and we can think of this sum as the attention
    mechanism; when there is more weight on a particular hidden state, we can interpret
    that as the model is paying more attention to the token inputted at that time
    step.This is exactly what we visualize in [Figure 9-33](#explicitly_viz_the_weights).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The English sentence to be translated is on the top row, and the resulting French
    translation is on the first column. The lighter a square is, the more attention
    the decoder paid to that particular column when decoding that row element. That
    is, the *(i, j)^(th)* element in the attention map shows the amount of attention
    that was paid to the *j^(th)* token in the English sentence when translating the
    *i^(th)* token in the French sentence.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: We can immediately see that the attention mechanism seems to be working quite
    well. Large amounts of attention are generally being placed in the right areas,
    even though there is slight noise in the model’s prediction. It is possible that
    adding layers to the network would help produce crisper attention. One impressive
    aspect is that the phrase “the European Economic” is translated in reverse in
    French as the “zone économique européenne,” and as such, the attention weights
    reflect this flip. These kinds of attention patterns may be even more interesting
    when translating from English to a different language that does not parse smoothly
    from left to right.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0933.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: Figure 9-33\. Visualizing the weights of the convex sum when the decoder attends
    over hidden states in the encoder
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With one of the most fundamental architectures understood and implemented, we
    now move forward to study exciting new developments with RNNs and begin a foray
    into more sophisticated learning.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention and Transformers
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, we discussed a form of attention that was first presented in Bahdanau
    et al. in 2015\. Specifically, we used a simple, feed-forward neural network to
    calculate the alignment score of each encoder hidden state with the decoder state
    at the current time step. In this section, we’ll discuss a different form of attention
    called *scaled dot product attention,* its use in *self-attention,* and the *transformer,*
    a recent language modeling breakthrough. Transformer-based models have primarily
    replaced LSTM, and have been proven to be superior in quality for many sequence-to-sequence
    problems.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Dot product attention is really as simple as it sounds—this method calculates
    alignment scores as the dot product between each encoder hidden state  <math alttext="s
    Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math> . These weights are used
    in the calculation of the context vector, which is a convex sum (via softmax)
    of the encoder hidden states. Why use the dot product to measure alignment? As
    we learned in [Chapter 1](ch01.xhtml#fundamentals_of_linear_algebra_for_deep_learning),
    the dot product of two vectors can be expressed as a product of the norms of the
    two vectors and the cosine of the angle between them. As the angle between the
    two vectors goes to zero, the cosine goes to one. Also, recall from trigonometry
    that cosine has the range 1 to –1 when the input angle is between 0 degrees and
    180 degrees, which is the only part of the domain of the angle we need to consider.
    The dot product has the nice property that, as the angle between two vectors gets
    smaller, the dot product gets larger. This allows us to use the dot product as
    a natural measure of similarity.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2017, Vaswani et al.^([7](ch09.xhtml#idm45934165152352)) introduced a modification
    to the preexisting dot product attention framework via the inclusion of a scaling
    factor—the square root of the dimension of the hidden states. Vaswani et al. acknowledge
    the fact that, as hidden state representations get larger and larger in terms
    of dimension, we expect to see significantly more instances of high magnitude
    dot products. To understand the reasoning behind the inclusion of this scaling
    factor, assume, for the sake of argument, each index of  <math alttext="h Subscript
    i"><msub><mi>h</mi> <mi>i</mi></msub></math> is drawn independently and identically
    distributed from a mean zero, unit variance random variable. Let’s compute the
    expectation and variance of their dot product:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper E left-bracket s Subscript t Superscript
    upper T Baseline h Subscript i Baseline right-bracket equals sigma-summation Underscript
    j equals 1 Overscript k Endscripts double-struck upper E left-bracket s Subscript
    t comma j Baseline asterisk h Subscript i comma j Baseline right-bracket"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msubsup><mi>s</mi> <mi>t</mi> <mi>T</mi></msubsup> <msub><mi>h</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket s Subscript t Superscript
    upper T Baseline h Subscript i Baseline right-bracket equals sigma-summation Underscript
    j equals 1 Overscript k Endscripts double-struck upper E left-bracket s Subscript
    t comma j Baseline asterisk h Subscript i comma j Baseline right-bracket"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msubsup><mi>s</mi> <mi>t</mi> <mi>T</mi></msubsup> <msub><mi>h</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow></mrow></math>
- en: = <math alttext="sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket s Subscript t comma j Baseline right-bracket
    double-struck upper E left-bracket h Subscript i comma j Baseline right-bracket"><mrow><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>]</mo></mrow>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals 0"><mrow><mo>=</mo> <mn>0</mn></mrow></math>
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals 0"><mrow><mo>=</mo> <mn>0</mn></mrow></math>
- en: <math alttext="upper V a r left-parenthesis s Subscript t Superscript upper
    T Baseline h Subscript i Baseline right-parenthesis equals sigma-summation Underscript
    j equals 1 Overscript k Endscripts upper V a r left-parenthesis s Subscript t
    comma j Baseline asterisk h Subscript i comma j Baseline right-parenthesis"><mrow><mi>V</mi>
    <mi>a</mi> <mi>r</mi> <mrow><mo>(</mo> <msubsup><mi>s</mi> <mi>t</mi> <mi>T</mi></msubsup>
    <msub><mi>h</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup> <mi>V</mi> <mi>a</mi>
    <mi>r</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper V a r left-parenthesis s Subscript t Superscript upper
    T Baseline h Subscript i Baseline right-parenthesis equals sigma-summation Underscript
    j equals 1 Overscript k Endscripts upper V a r left-parenthesis s Subscript t
    comma j Baseline asterisk h Subscript i comma j Baseline right-parenthesis"><mrow><mi>V</mi>
    <mi>a</mi> <mi>r</mi> <mrow><mo>(</mo> <msubsup><mi>s</mi> <mi>t</mi> <mi>T</mi></msubsup>
    <msub><mi>h</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup> <mi>V</mi> <mi>a</mi>
    <mi>r</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket left-parenthesis s Subscript t comma j Superscript
    2 Baseline asterisk h Subscript i comma j Superscript 2 Baseline right-parenthesis
    right-bracket minus double-struck upper E left-bracket s Subscript t comma j Baseline
    asterisk h Subscript i comma j Baseline right-bracket squared"><mrow><mo>=</mo>
    <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup>
    <mi>𝔼</mi> <mrow><mo>[</mo> <mrow><mo>(</mo> <msubsup><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>*</mo> <msubsup><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo> <mi>𝔼</mi>
    <msup><mrow><mo>[</mo><msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo><msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket left-parenthesis s Subscript t comma j Superscript
    2 Baseline asterisk h Subscript i comma j Superscript 2 Baseline right-parenthesis
    right-bracket minus double-struck upper E left-bracket s Subscript t comma j Baseline
    asterisk h Subscript i comma j Baseline right-bracket squared"><mrow><mo>=</mo>
    <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup>
    <mi>𝔼</mi> <mrow><mo>[</mo> <mrow><mo>(</mo> <msubsup><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>*</mo> <msubsup><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo> <mi>𝔼</mi>
    <msup><mrow><mo>[</mo><msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo><msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket s Subscript t comma j Superscript 2 Baseline
    right-bracket double-struck upper E left-bracket h Subscript i comma j Superscript
    2 Baseline right-bracket"><mrow><mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo> <msubsup><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>]</mo></mrow> <mi>𝔼</mi> <mrow><mo>[</mo> <msubsup><mi>h</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> <mn>2</mn></msubsup> <mo>]</mo></mrow></mrow></math>
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket s Subscript t comma j Superscript 2 Baseline
    right-bracket double-struck upper E left-bracket h Subscript i comma j Superscript
    2 Baseline right-bracket"><mrow><mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo> <msubsup><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>]</mo></mrow> <mi>𝔼</mi> <mrow><mo>[</mo> <msubsup><mi>h</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> <mn>2</mn></msubsup> <mo>]</mo></mrow></mrow></math>
- en: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    1"><mrow><mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mn>1</mn></mrow></math>
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    1"><mrow><mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mn>1</mn></mrow></math>
- en: <math alttext="equals k"><mrow><mo>=</mo> <mi>k</mi></mrow></math>
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals k"><mrow><mo>=</mo> <mi>k</mi></mrow></math>
- en: Let’s review the steps that got us to these conclusions regarding the expectation
    and variance. The first equality in the expectation is due to the linearity of
    expectation, since the dot product can be expressed as a sum of the product of
    each index. The second equality comes from the fact that the two random variables
    in each expectation are independent, so we can separate the expectation of the
    product into a product of expectations. The final step follows directly from the
    fact that each of these individual expectations are zero.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The first equality in the variance is due to the linearity of variance when
    the individual terms are all independent. The second equality is just the definition
    of variance. The third equality uses a result from our calculation of the expectation
    of the dot product (we can separate out the square of the expectation into the
    product of squares of expectations, where each individual expectation is zero).
    Additionally, the expectation of the product of squares can be split up into a
    product of expectations of squares, since the square of each random variable is
    independent of the squares of all other random variables. The second to last equality
    comes from the fact that the expectation of the square of each random variable
    is just the variance of the random variable (since the expectation of each random
    variable is zero). The final equality follows directly.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: We see that the expectation of the dot product is zero, while its variance is
    k, the dimension of the hidden representation. Thus, as the dimension increases,
    the variance increases—this implies a higher probability of seeing high-magnitude
    dot products.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, with the presence of more high-magnitude dot products comes smaller
    gradients due to the softmax function. Although we won’t derive it here, this
    makes a lot of intuitive sense—think back to the use of softmax in neural networks
    for classification problems. As the neural network gets more and more confident
    in a correct prediction (i.e., a high logit value for the true index), the gradient
    gets smaller and smaller. The scaling factor introduced by Vaswani et al. reduces
    the magnitude of dot products, leading to larger gradients and better learning.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered scaled dot product attention, we turn our attention to
    self-attention. In the previous sections, we saw attention through the context
    of machine translation where we are given a training set of sentences that are
    in English and French, and the goal is to be able to translate unseen English
    sentences to French. In this specific class of problems, there exists a direct
    supervision through the target French sentences. However, attention can also be
    used in a completely self-contained manner. The intuition is that, given a sentence
    in English, we may be able to perform more insightful sentiment analysis, more
    effective machine reading, and better understanding via learning relationships
    between tokens within sentences or paragraphs.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: The transformer, our final topic for this section, utilizes both scaled dot
    product attention and self-attention. The transformer architecture (Vaswani et
    al. 2017) has both encoder and decoder architectures, where there exists self-attention
    within both the encoder and decoder, as well as standard attention between the
    encoder and decoder. The self-attention layers in the encoders and decoders allow
    each to attend to all positions prior to the current position in their respective
    architectures. The standard attention allows the decoder to attend to each encoder
    hidden state, as described earlier.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve delved deep into the world of sequence analysis. We’ve
    analyzed how we might hack feed-forward networks to process sequences, developed
    a strong understanding of RNNs, and explored how attentional mechanisms can enable
    incredible applications ranging from language translation to audio transcription.
    Sequence analysis is a field that ranges problems not only in natural language,
    but also topics in finance, such as time-series analysis of returns of financial
    assets. Any field that involves longitudinal analyses, or analyses across time,
    could use the applications of sequence analysis described in this chapter. We
    advise you to really deepen your understanding of sequence analysis via implementation
    across different fields and by comparing the results of the techniques presented
    for natural language with the state-of-the-art in each field. There are also situations
    in which the techniques presented here may not be the most appropriate modeling
    choice, and we advise you to think deeply about why the modeling assumptions made
    here may not apply broadly. Sequence analysis is a powerful tool that has a place
    in almost all technical applications, not just natural language.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch09.xhtml#idm45934164188240-marker)) Nivre, Joakim. “Incrementality
    in Deterministic Dependency Parsing.” *Proceedings of the Workshop on Incremental
    Parsing: Bringing Engineering and Cognition Together*. Association for Computational
    Linguistics, 2004; Chen, Danqi, and Christopher D. Manning. “A Fast and Accurate
    Dependency Parser Using Neural Networks.” *EMNLP*. 2014.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.xhtml#idm45934164127728-marker)) Andor, Daniel, et al. “Globally
    Normalized Transition-Based Neural Networks.” *arXiv preprint* *arXiv*:1603.06042
    (2016).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch09.xhtml#idm45934164119456-marker)) Ibid.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch09.xhtml#idm45934164070368-marker)) Kilian, Joe, and Hava T. Siegelmann.
    “The dynamic universality of sigmoidal neural networks.” *Information and computation*
    128.1 (1996): 48-56.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch09.xhtml#idm45934164517408-marker)) Kiros, Ryan, et al. “Skip-Thought
    Vectors.” *Advances in neural information processing systems*. 2015.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch09.xhtml#idm45934164490960-marker)) Bahdanau, Dzmitry, Kyunghyun Cho,
    and Yoshua Bengio. “Neural Machine Translation by Jointly Learning to Align and
    Translate.” *arXiv preprint arXiv*:1409.0473 (2014).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch09.xhtml#idm45934165152352-marker)) Vaswani et. al. “Attention Is All
    You Need.” *arXiv Preprint arXiv*:1706.03762 2017.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
