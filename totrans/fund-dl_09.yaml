- en: Chapter 9\. Models for Sequence Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。序列分析模型
- en: Surya Bhupatiraju
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Surya Bhupatiraju
- en: Analyzing Variable-Length Inputs
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析可变长度输入
- en: 'Up until now, we’ve worked only with data with fixed sizes: images from MNIST,
    CIFAR-10, and ImageNet. These models are incredibly powerful, but there are many
    situations in which fixed-length models are insufficient. The vast majority of
    interactions in our daily lives require a deep understanding of sequences—whether
    it’s reading the morning newspaper, making a bowl of cereal, listening to the
    radio, watching a presentation, or deciding to execute a trade on the stock market.
    To adapt to variable-length inputs, we’ll have to be a little bit more clever
    about how we approach designing deep learning models.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只处理了具有固定大小的数据：来自MNIST、CIFAR-10和ImageNet的图像。这些模型非常强大，但在许多情况下，固定长度模型是不够的。我们日常生活中绝大多数的互动都需要对序列有深入的理解——无论是阅读早报、准备一碗麦片、听收音机、观看演示还是决定在股市上执行交易。为了适应可变长度的输入，我们必须更加聪明地设计深度学习模型的方法。
- en: '[Figure 9-1](#feedforward_networks_thrive) illustrates how our feed-forward
    neural networks break when analyzing sequences. If the sequence is the same size
    as the input layer, the model can perform as expected. It’s even possible to deal
    with smaller inputs by padding zeros to the end of the input until it’s the appropriate
    length. However, the moment the input exceeds the size of the input layer, naively
    using the feed-forward network no longer works.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-1](#feedforward_networks_thrive)说明了我们的前馈神经网络在分析序列时会出现问题。如果序列与输入层大小相同，模型可以按预期执行。甚至可以通过在输入末尾填充零直到达到适当长度来处理较小的输入。然而，一旦输入超过输入层的大小，朴素地使用前馈网络就不再起作用。'
- en: Feed-forward networks thrive on fixed input size problems. Zero padding can
    address the handling of smaller inputs, but when naively utilized, these models
    break when inputs exceed the fixed input size.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络在固定输入大小的问题上表现出色。零填充可以解决处理较小输入的问题，但是当朴素地使用时，这些模型在输入超过固定输入大小时会出现问题。
- en: '![](Images/fdl2_0901.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0901.png)'
- en: Figure 9-1\. Broken feed-forward network
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1。损坏的前馈网络
- en: Not all hope is lost, however. In the next couple of sections, we’ll explore
    several strategies we can leverage to “hack” feed-forward networks to handle sequences.
    Later in the chapter, we’ll analyze the limitations of these hacks and discuss
    new architectures to address them. We will conclude the chapter by discussing
    some of the most advanced architectures explored to date to tackle some of the
    most difficult challenges in replicating human-level logical reasoning and cognition
    over sequences.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非一切希望都已失去。在接下来的几节中，我们将探讨几种策略，可以利用“黑客”前馈网络来处理序列。在本章后面，我们将分析这些黑客的局限性，并讨论新的架构来解决这些问题。我们将通过讨论迄今为止探索的一些最先进的架构来结束本章，以解决复制人类级别逻辑推理和认知的一些最困难挑战。
- en: Tackling seq2seq with Neural N-Grams
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经N-Grams解决seq2seq
- en: In this section, we’ll begin exploring a feed-forward neural network architecture
    that can process a body of text and produce a sequence of part-of-speech (POS)
    tags. In other words, we want to appropriately label each word in the input text
    as a noun, verb, preposition, and so on. An example of this is shown in [Figure 9-2](#example_of_an_accurate_pos).
    While it’s not the same complexity as building an AI that can answer questions
    after reading a story, it’s a solid first step toward developing an algorithm
    that can understand the meaning of how words are used in a sentence. This problem
    is also interesting because it is an instance of a class of problems known as
    *seq2seq*, where the goal is to transform an input sequence into a corresponding
    output sequence. Other famous seq2seq problems include translating text between
    languages (which we will tackle later in this chapter),  text summarization, and
    transcribing speech to text.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开始探索一个前馈神经网络架构，可以处理一段文本并生成一系列词性（POS）标签。换句话说，我们希望适当地标记输入文本中的每个单词，如名词、动词、介词等。这在[图9-2](#example_of_an_accurate_pos)中有一个示例。虽然构建一个可以阅读故事后回答问题的人工智能的复杂性不同，但这是朝着开发一个能够理解单词在句子中如何使用含义的算法的坚实第一步。这个问题也很有趣，因为它是一类问题的一个实例，被称为*seq2seq*，目标是将输入序列转换为相应的输出序列。其他著名的seq2seq问题包括在语言之间翻译文本（我们将在本章后面处理）、文本摘要和将语音转录为文本。
- en: '![](Images/fdl2_0902.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0902.png)'
- en: Figure 9-2\. An example of an accurate POS parse of an English sentence
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。英语句子准确的POS解析示例
- en: 'As we discussed, it’s not obvious how we might take a body of text all at once
    to predict the full sequence of POS tags. Instead, we leverage a trick that is
    akin to the way we developed distributed vector representations of words in the
    previous chapter. The key observation is this: *it is not necessary to take into
    account long-term dependencies to predict the POS of any given word*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们讨论过的，如何一次性处理一段文本以预测完整的POS标签序列并不明显。相反，我们利用了一种类似于我们在上一章中开发单词的分布式向量表示的技巧。关键观察是：*不需要考虑长期依赖性来预测任何给定单词的POS*。 '
- en: The implication of this observation is that instead of using the whole sequence
    to predict all of the POS tags simultaneously, we can predict each POS tag one
    at a time by using a fixed-length subsequence. In particular, we utilize the subsequence
    starting from the word of interest and extending *n* words into the past. This
    *neural n-gram strategy* is depicted in [Figure 9-3](#perform_seq2seq).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这一观察的含义是，我们可以通过使用固定长度的子序列，而不是使用整个序列同时预测所有POS标签，逐个预测每个POS标签。特别是，我们利用从感兴趣的单词开始并向过去扩展n个单词的子序列。这种*神经n-gram策略*在[图9-3](#perform_seq2seq)中有所描述。
- en: '![](Images/fdl2_0903.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0903.png)'
- en: Figure 9-3\. Using a feed-forward network to perform seq2seq when we can ignore
    long-term dependencies
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3。在我们可以忽略长期依赖性时使用前馈网络执行seq2seq
- en: Specifically, when we predict the POS tag for the  <math alttext="i Superscript
    t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>  word in
    the input, we use the the  <math alttext="i minus n plus 1 Superscript s t Baseline
    comma i minus n plus 2 Superscript n d Baseline comma ellipsis comma i Superscript
    t h"><mrow><mi>i</mi> <mo>-</mo> <mi>n</mi> <mo>+</mo> <msup><mn>1</mn> <mrow><mi>s</mi><mi>t</mi></mrow></msup>
    <mo>,</mo> <mi>i</mi> <mo>-</mo> <mi>n</mi> <mo>+</mo> <msup><mn>2</mn> <mrow><mi>n</mi><mi>d</mi></mrow></msup>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow></math>
    words as the input. We’ll refer to this subsequence as the *context window*. In
    order to process the entire text, we’ll start by positioning the network at the
    beginning of the text. We’ll then proceed to move the network’s context window
    one word at a time, predicting the POS tag of the rightmost word, until we reach
    the end of the input.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，当我们预测输入中第i个单词的词性标签时，我们使用第i-n+1到第i个单词作为输入。我们将这个子序列称为*上下文窗口*。为了处理整个文本，我们将首先将网络定位在文本的开头。然后，我们将继续将网络的上下文窗口每次移动一个单词，预测最右边单词的词性标签，直到达到输入的末尾。
- en: Leveraging the word embedding strategy from last chapter, we’ll also use condensed
    representations of the words instead of one-hot vectors. This will allow us to
    reduce the number of parameters in our model and make learning faster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 利用上一章的词嵌入策略，我们将使用单词的压缩表示，而不是独热向量。这将使我们能够减少模型中的参数数量，并加快学习速度。
- en: Implementing a Part-of-Speech Tagger
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现词性标注器
- en: Now that we have a strong understanding of the POS network architecture, we
    can dive into the implementation. On a high level, the network consists of an
    input layer that leverages a three-gram context window. We’ll use word embeddings
    that are 300-dimensional, resulting in a context window of size 900\. The feed-forward
    network will have two hidden layers of size 512 neurons and 256 neurons, respectively.
    Then, the output layer will be a softmax calculating the probability distribution
    of the POS tag output over a space of 44 possible tags. As usual, we’ll use the
    Adam optimizer with our default hyperparameter settings, train for a total of
    1,000 epochs, and leverage batch-normalization for regularization.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对词性网络架构有了深入的理解，我们可以深入实现。在高层次上，网络由一个利用三元上下文窗口的输入层组成。我们将使用300维的词嵌入，从而得到一个大小为900的上下文窗口。前馈网络将有两个隐藏层，分别为512个神经元和256个神经元。然后，输出层将是一个softmax，计算POS标签输出在44个可能标签空间上的概率分布。像往常一样，我们将使用Adam优化器和默认的超参数设置，总共训练1000个时代，并利用批量归一化进行正则化。
- en: 'The actual network is extremely similar to networks we’ve implemented in the
    past. Rather, the tricky part of building the POS tagger is in preparing the dataset.
    We’ll leverage pretrained word embeddings generated from [Google News](https://oreil.ly/Rsu9A).
    It includes vectors for 3 million words and phrases and was trained on roughly
    100 billion words. We can use the `gensim` Python package to read the dataset.
    Google Colab already has `gensim` preinstalled. If you are using another machine,
    you can use `pip` to install the package. You will also need to download the Google
    News data file:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的网络与我们过去实现的网络非常相似。相反，构建词性标注器的难点在于准备数据集。我们将利用从[Google News](https://oreil.ly/Rsu9A)生成的预训练词嵌入。它包括了300万个单词和短语的向量，并在大约1000亿个单词上进行了训练。我们可以使用`gensim`
    Python包来读取数据集。Google Colab已经预先安装了`gensim`。如果您使用另一台机器，可以使用`pip`来安装该包。您还需要下载Google
    News数据文件：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can subsequently load these vectors into memory using the following command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用以下命令将这些向量加载到内存中：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The issue with this operation, however, is that it’s incredibly slow (it can
    take up to an hour, depending on the specs of your machine). To avoid loading
    the full dataset into memory every single time we run our program, especially
    while debugging code or experimenting with different hyperparameters, we cache
    the relevant subset of the vectors to disk using a lightweight database known
    as [LevelDB](http://leveldb.org). To build the appropriate Python bindings (which
    allow us to interact with a LevelDB instance from Python), we simply use the following
    command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种操作的问题在于它非常慢（根据您的机器规格，可能需要长达一小时）。为了避免每次运行程序时都将整个数据集加载到内存中，特别是在调试代码或尝试不同超参数时，我们使用轻量级数据库[LevelDB](http://leveldb.org)将相关子集的向量缓存到磁盘上。为了构建适当的Python绑定（允许我们从Python与LevelDB实例交互），我们只需使用以下命令：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we mentioned, the `gensim` model contains three million words, which is larger
    than our dataset. For the sake of efficiency, we’ll selectively cache word vectors
    for words in our dataset and discard everything else. To figure out which words
    we’d like to cache, let’s download the POS dataset from the [CoNLL-2000 task](https://oreil.ly/8qJeZ).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，`gensim`模型包含三百万个单词，比我们的数据集要大。为了提高效率，我们将有选择地缓存数据集中的单词向量，并丢弃其他所有内容。为了找出我们想要缓存的单词，让我们从[CoNLL-2000任务](https://oreil.ly/8qJeZ)下载POS数据集。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The dataset consists of contiguous text that is formatted as a sequence of
    rows, where the first element is a word and the second element is the corresponding
    part of speech. Here are the first several lines of the training dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由格式化为一系列行的连续文本组成，其中第一个元素是一个单词，第二个元素是相应的词性。以下是训练数据集的前几行：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To match the formatting of the dataset to the `gensim` model, we’ll have to
    do some preprocessing. For example, the model replaces digits with ''#'' characters,
    combines separate words into entities where appropriate (e.g., considering “New_York”
    as a single token instead of two separate words), and utilizes underscores where
    the raw data uses dashes. We preprocess the dataset to conform to this model schema
    with the following code (analogous code is used to process the training data):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据集的格式与`gensim`模型匹配，我们需要进行一些预处理。例如，该模型用'#'字符替换数字，将适当的单词组合成实体（例如，将“New_York”视为一个单词而不是两个单独的单词），并在原始数据使用破折号时使用下划线。我们对数据集进行预处理以符合这个模型模式，使用以下代码（类似的代码用于处理训练数据）：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we’ve appropriately processed the datasets for use, we can load the
    words in LevelDB. If the word or phrase is present in the `gensim` model, we can
    cache that in the LevelDB instance. If not, we randomly select a vector to represent
    to the token, and cache it so that we remember to use the same vector in case
    we encounter it again:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经适当处理了用于加载的数据集，我们可以加载LevelDB中的单词。如果`gensim`模型中存在单词或短语，我们可以将其缓存到LevelDB实例中。如果没有，我们会随机选择一个向量来表示该标记，并将其缓存，以便在再次遇到时记得使用相同的向量：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After running the script for the first time, we can just load our data straight
    from the database if it already exists:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次运行脚本后，如果数据已经存在，我们可以直接从数据库加载数据：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we build dataset objects for both training and test datasets, which we
    can use to generate minibatches for training and testing purposes. Building the
    dataset object requires access to the LevelDB `db`, the `dataset`, a dictionary
    `tags_to_index` that maps POS tags to indices in the output vector, and a boolean
    flat `get_all` that determines whether getting the minibatch should retrieve the
    full set by default:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为训练和测试数据集构建数据集对象，我们可以使用这些对象生成用于训练和测试的小批量数据。构建数据集对象需要访问LevelDB `db`，`dataset`，将POS标记映射到输出向量中的索引的字典`tags_to_index`，以及一个布尔值`get_all`，用于确定获取小批量数据时是否应默认检索完整集合：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Finally, we design our feed-forward network similarly to our approaches in previous
    chapters. We omit a discussion of the code and refer to the file *Ch09_01_POS_Tagger.ipynb*
    in the [book’s repository](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们设计我们的前馈网络与之前章节中的方法类似。我们省略了代码的讨论，并参考文件*Ch09_01_POS_Tagger.ipynb*在[书的存储库](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book)中。
- en: 'Every epoch, we manually inspect the model by parsing the sentence: “The woman,
    after grabbing her umbrella, went to the bank to deposit her cash.” Within 100
    epochs of training, the algorithm achieves over 96% accuracy and nearly perfectly
    parses the validation sentence (it makes the understandable mistake of confusing
    the possessive pronoun and personal pronoun tags for the first appearance of the
    word “her”). We’ll conclude this by including the visualizations of our model’s
    performance using TensorBoard in [Figure 9-4](#tensorboard_viz_of_feedforward_pos).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时代，我们通过解析句子“这个女人，在拿起她的伞后，去了银行存钱。”来手动检查模型。在训练100个时代后，算法达到了超过96%的准确率，并几乎完美地解析了验证句子（它犯了一个可以理解的错误，混淆了第一次出现“her”一词的所有格代词和人称代词标记）。我们将通过在TensorBoard中包含我们模型性能的可视化来结束这一部分[图9-4](#tensorboard_viz_of_feedforward_pos)。
- en: The POS tagging model was a great exercise, but it was mostly rinsing and repeating
    concepts we’ve learned in previous chapters. In the rest of the chapter, we’ll
    start to think about much more complicated sequence-related learning tasks. To
    tackle these more difficult problems, we’ll need to broach brand-new concepts,
    develop new architectures, and start to explore the cutting edge of modern deep
    learning research. We’ll start by tackling the problem of dependency parsing next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: POS标记模型是一个很好的练习，但它主要是重复我们在之前章节学到的概念。在本章的其余部分，我们将开始思考更复杂的与序列相关的学习任务。为了解决这些更困难的问题，我们需要涉及全新的概念，开发新的架构，并开始探索现代深度学习研究的前沿。我们将从下一个依存句法分析问题开始。
- en: '![](Images/fdl2_0904.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0904.png)'
- en: Figure 9-4\. TensorBoard visualization of our feed-forward POS tagging model
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4。我们前馈POS标记模型的TensorBoard可视化
- en: Dependency Parsing and SyntaxNet
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依存句法分析和SyntaxNet
- en: The framework we used to solve the POS tagging task was rather simple. Sometimes
    we need to be much more creative about how we tackle seq2seq problems, especially
    as the complexity of the problem increases. In this section, we’ll explore strategies
    that employ creative data structures to tackle difficult seq2seq problems. As
    an illustrative example, we’ll explore the problem of dependency parsing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来解决POS标记任务的框架相当简单。有时候，我们需要更有创意地解决seq2seq问题，特别是在问题复杂性增加时。在本节中，我们将探讨采用创造性数据结构来解决困难的seq2seq问题的策略。作为一个说明性例子，我们将探讨依存句法分析问题。
- en: The idea behind building a dependency parse tree is to map the relationships
    between words in a sentence. Take, for example, the dependency in [Figure 9-5](#example_of_a_dependency_parse).
    The words “I” and “taxi” are children of the word “took,” specifically as the
    subject and direct object of the verb, respectively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 构建依存句法分析树的想法是映射句子中单词之间的关系。例如，看一下[图9-5](#example_of_a_dependency_parse)中的依存关系。单词“I”和“taxi”是单词“took”的子节点，具体来说是动词的主语和直接宾语。
- en: '![](Images/fdl2_0905.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0905.png)'
- en: Figure 9-5\. An example of a dependency parse, which generates a tree of relationships
    between words in a sentence
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5。一个依存句法分析的示例，生成句子中单词之间关系的树
- en: One way to express a tree as a sequence is by linearizing it. Let’s consider
    the examples in [Figure 9-6](#linearize_two_example_trees). Essentially, if you
    have a graph with a root `R`, and children `A` (connected by edge `r_a`), `B`
    (connected by edge `r_b`), and `C` (connected by edge `r_c`), we can linearize
    the representation as `(R, r_a, A, r_b, B, r_c, C)`. We can even represent more
    complex graphs. Let’s assume, for example, that node `B` actually has two more
    children named `D` (connected by edge `b_d`) and `E` (connected by edge `b_e`).
    We can represent this new graph as `(R, r_a, A, r_b, [B, b_d, D, b_e, E], r_c,` `C)`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将树表示为序列的一种方法是将其线性化。让我们考虑[图9-6](#linearize_two_example_trees)中的示例。基本上，如果您有一个以根`R`为根，子节点`A`（通过边`r_a`连接），`B`（通过边`r_b`连接）和`C`（通过边`r_c`连接）的图，我们可以将表示线性化为`(R,
    r_a, A, r_b, B, r_c, C)`。我们甚至可以表示更复杂的图。例如，假设节点`B`实际上有两个名为`D`（通过边`b_d`连接）和`E`（通过边`b_e`连接）的子节点。我们可以将这个新图表示为`(R,
    r_a, A, r_b, [B, b_d, D, b_e, E], r_c, C)`。
- en: '![](Images/fdl2_0906.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0906.png)'
- en: 'Figure 9-6\. We linearize two example trees: the diagrams omit edge labels
    for the sake of visual clarity'
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6。我们线性化了两个示例树：为了视觉清晰起见，图中省略了边标签
- en: Using this paradigm, we can take our example dependency parse and linearize
    it, as shown in [Figure 9-7](#linearization_of_dependency_parse_tree).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种范式，我们可以将我们的示例依赖解析线性化，如[图9-7](#linearization_of_dependency_parse_tree)所示。
- en: '![](Images/fdl2_0907.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0907.png)'
- en: Figure 9-7\. Linearization of the dependency parse tree example
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7。依赖解析树示例的线性化
- en: One interpretation of this seq2seq problem would be to read the input sentence
    and produce a sequence of tokens as an output that represents the linearization
    of the input’s dependency parse. It’s not particularly clear, however, how we
    might port our strategy from the previous section, where there was a clear one-to-one
    mapping between words and their POS tags. Moreover, we could easily make decisions
    about a POS tag by looking at the nearby context. For dependency parsing, there’s
    no clear relationship between how words are ordered in the sentence and how tokens
    in the linearization are ordered. It also seems like dependency parsing tasks
    us with identifying edges that may span a significantly large number of words.
    Therefore, at first glance, it seems like this setup directly violates our assumption
    that we need not take into account any long-term dependencies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个seq2seq问题的一个解释是读取输入句子并生成一个代表输入依赖解析线性化的输出令牌序列。然而，我们可能不太清楚如何将我们从前一节中的策略移植过来，那里单词和它们的POS标签之间有明确的一对一映射。此外，我们可以通过查看附近的上下文轻松地做出关于POS标签的决定。对于依赖解析，句子中单词的顺序与线性化中的令牌的顺序之间没有明确的关系。看起来依赖解析任务要求我们识别可能跨越大量单词的边缘。因此，乍一看，这种设置似乎直接违反了我们不需要考虑任何长期依赖的假设。
- en: To make the problem more approachable, we instead reconsider the dependency
    parsing task as finding a sequence of valid “actions” that generates the correct
    dependency parse. This technique, known as the *arc-standard* system, was first
    described by Nivre in 2004 and later leveraged in a neural context by Chen and
    Manning in 2014.^([1](ch09.xhtml#idm45934164188240)) In the arc-standard system,
    we start by putting the first two words of the sentence in the stack and maintaining
    the remaining words in the buffer, as shown in [Figure 9-8](#we_have_three_options).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使问题更容易解决，我们将依赖解析任务重新考虑为找到一系列有效的“动作”，以生成正确的依赖解析。这种技术，称为*弧-标准*系统，最初由Nivre在2004年描述，后来在2014年由Chen和Manning在神经环境中利用。[^1](ch09.xhtml#idm45934164188240)在弧-标准系统中，我们首先将句子的前两个词放入堆栈，并保留缓冲区中的剩余词，如[图9-8](#we_have_three_options)所示。
- en: '![](Images/fdl2_0908.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0908.png)'
- en: 'Figure 9-8\. Three options in the arc-standard system: shift a word from the
    buffer to the stack, draw an arc from the right element to the left element (left
    arc), or draw an arc from the left element to the right element (right arc)'
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8。弧-标准系统中的三个选项：将一个词从缓冲区移动到堆栈，从右元素到左元素绘制弧（左弧），或从左元素到右元素绘制弧（右弧）
- en: 'At any step, we can take one of three possible classes of actions:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何步骤中，我们可以采取三种可能的行动类别之一：
- en: Shift
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Shift
- en: Move a word from the buffer to the front of the stack.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个词从缓冲区移动到堆栈的前面。
- en: Left arc
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 左弧
- en: Combine the two elements at the front of the stack into a single unit where
    the root of the rightmost element is the parent node and the root of leftmost
    element is the child node.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将堆栈前面的两个元素合并为一个单元，其中最右边元素的根是父节点，最左边元素的根是子节点。
- en: Right arc
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 右弧
- en: Combine the two elements at the front of the stack into a single unit where
    the root of the left element is the parent node, and the root of right element
    is the child node.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将堆栈前面的两个元素合并为一个单元，其中左元素的根是父节点，右元素的根是子节点。
- en: We note that while there is only one way to perform a shift, the arc actions
    can be of many flavors, each differentiated by the dependency label assigned to
    the arc that is generated. That being said, we’ll simplify our discussions and
    illustrations in this section by considering each decision as a choice among three
    actions (rather than tens of actions).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，虽然执行shift只有一种方法，但弧动作可以有许多不同的风格，每种风格由分配给生成的弧的依赖标签区分。也就是说，在本节中，我们将简化我们的讨论和说明，将每个决策视为三种行动中的一种选择（而不是数十种行动）。
- en: We terminate this process when the buffer is empty and the stack has one element
    in it (which represents the full dependency parse). To illustrate this process
    in its entirety, we illustrate a sequence of actions that generates the dependency
    parse for our example input sentence in [Figure 9-9](#sequence_of_actions).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当缓冲区为空且堆栈中有一个元素时（表示完整的依赖解析），我们终止此过程。为了完整地说明这个过程，我们展示了一系列生成我们示例输入句子的依赖解析的动作，如[图9-9](#sequence_of_actions)所示。
- en: '![](Images/fdl2_0909.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0909.png)'
- en: Figure 9-9\. A sequence of actions that results in the correct dependency parse;
    we omit labels
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9。一系列动作的序列，导致正确的依赖解析；我们省略标签
- en: It’s not too difficult to reformulate this decision-making framework as a learning
    problem. At every step, we take the current configuration, and we vectorize the
    configuration by extracting a large number of features that describe the configuration
    (words in specific locations of the stack/buffer, specific children of the words
    in these locations, part of speech tags, etc.). During train time, we can feed
    this vector into a feed-forward network and compare its prediction of the next
    action to take to a gold-standard decision made by a human linguist. To use this
    model in the wild, we can take the action that the network recommends, apply it
    to the configuration, and use this new configuration as the starting point for
    the next step (feature extraction, action prediction, and action application).
    This process is shown in [Figure 9-10](#neural_framework_for_arc_standard_dependency).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个决策框架重新表述为一个学习问题并不太困难。在每一步，我们采取当前的配置，并通过提取描述配置的大量特征（堆栈/缓冲区中特定位置的单词，这些位置单词的特定子节点，词性标签等）对配置进行向量化。在训练时，我们可以将这个向量输入到一个前馈网络中，并将其对下一步要采取的行动的预测与人类语言学家做出的黄金标准决策进行比较。在实际应用中，我们可以采取网络推荐的行动，将其应用于配置，并将这个新配置作为下一步的起点（特征提取，行动预测和行动应用）。这个过程在[图9-10](#neural_framework_for_arc_standard_dependency)中显示。
- en: '![](Images/fdl2_0910.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0910.png)'
- en: Figure 9-10\. A neural framework for arc-standard dependency parsing
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10。用于arc-standard依赖解析的神经框架
- en: Taken together, these ideas form the core for Google’s SyntaxNet, the state-of-the-art
    open source implementation for dependency parsing. Delving into the nitty-gritty
    aspects of implementation is beyond the scope of this text, but we refer you to
    the [open source repository](https://oreil.ly/UT1ga), which contains an implementation
    of Parsey McParseface, the most accurate publicly reported English language parser
    as of the publication of this text.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 综上所述，这些想法构成了谷歌的SyntaxNet的核心，这是用于依赖解析的最先进的开源实现。深入讨论实现的细节超出了本文的范围，但我们建议您参考[开源存储库](https://oreil.ly/UT1ga)，其中包含Parsey
    McParseface的实现，这是截至本文发表时最准确的公开报告的英语解析器。
- en: Beam Search and Global Normalization
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 束搜索和全局归一化
- en: In the previous section, we described a naive strategy for deploying SyntaxNet
    in practice. The strategy was purely *greedy*; that is, we selected prediction
    with the highest probability without being concerned that we might potentially
    paint ourselves into a corner by making an early mistake. In the POS example,
    making an incorrect prediction was largely inconsequential. This is because each
    prediction could be considered a purely independent subproblem (the results of
    a given prediction do not affect the inputs of the next step).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们描述了在实践中部署SyntaxNet的一个天真策略。这个策略是纯粹*贪婪*的；也就是说，我们选择具有最高概率的预测，而不考虑可能会因为早期错误而陷入困境。在POS的例子中，做出错误的预测基本上是无关紧要的。这是因为每个预测都可以被视为一个纯粹独立的子问题（给定预测的结果不会影响下一步的输入）。
- en: This assumption no longer holds in SyntaxNet, because our prediction at step *n* affects
    the input we use at step  <math alttext="n plus 1"><mrow><mi>n</mi> <mo>+</mo>
    <mn>1</mn></mrow></math> . This implies that any mistake we make will influence
    all later decisions. Moreover, there’s no good way of “going backward” and fixing
    mistakes when they become apparent.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在SyntaxNet中，这个假设不再成立，因为我们在第*n*步的预测会影响我们在第*n+1*步使用的输入。这意味着我们所犯的任何错误都会影响所有后续决策。此外，一旦错误变得明显，就没有好的方法“回头”并纠正错误。
- en: '*Garden path sentences* are an extreme case of where this is important. Consider
    the following sentence: “The complex houses married and single soldiers and their
    families.” The first glance pass-through is confusing. Most people interpret “complex”
    as an adjective “houses” as a noun, and “married” as a past tense verb. This makes
    little semantic sense though, and starts to break down as the rest of the sentence
    is read. Instead, we realize that “complex” is a noun (as in a military complex)
    and that “houses” is a verb. In other words, the sentence implies that the military
    complex contains soldiers (who may be single or married) and their families. A
    *greedy* version of SyntaxNet would fail to correct the early parse mistake of
    considering “complex” as an adjective describing the “houses,” and therefore would
    fail on the full version of the sentence.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*花园路径句*是这一点很重要的一个极端案例。考虑以下句子：“The complex houses married and single soldiers
    and their families.”第一次浏览时令人困惑。大多数人将“complex”解释为形容词，“houses”解释为名词，“married”解释为过去时动词。尽管这在语义上几乎没有意义，并且在阅读句子的其余部分时开始出现问题。相反，我们意识到“complex”是一个名词（如军事基地），而“houses”是一个动词。换句话说，这个句子暗示着军事基地包含士兵（可能是单身或已婚）和他们的家人。一个*贪婪*版本的SyntaxNet会无法纠正早期解析错误，即将“complex”视为描述“houses”的形容词，因此无法正确解析整个句子。'
- en: 'To remedy this shortcoming, we use a strategy known as *beam search*, illustrated
    in [Figure 9-11](#illustration_of_using_beam_search). We generally leverage beam
    searches in situations like SyntaxNet, where the output of our network at a particular
    step influences the inputs used in future steps. The basic idea behind beam search
    is that instead of greedily selecting the most probable prediction at each step,
    we maintain a *beam* of the most likely hypothesis (up to a fixed *beam size b*)
    for the sequence of the first *k* actions and their associated probabilities.
    Beam searching can be broken up into two major phases: expansion and pruning.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这个缺点，我们使用一种称为*束搜索*的策略，如[图9-11](#illustration_of_using_beam_search)所示。我们通常在像SyntaxNet这样的情况下利用束搜索，其中我们网络在特定步骤的输出会影响未来步骤中使用的输入。束搜索的基本思想是，我们不是在每一步贪婪地选择最有可能的预测，而是维护一个*束*，其中包含前*k*个动作序列及其相关概率的最可能的假设（最多固定的*束大小b*）。束搜索可以分为两个主要阶段：扩展和修剪。
- en: '![](Images/fdl2_0911.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0911.png)'
- en: Figure 9-11\. Using beam search (with beam size 2) while deploying a trained
    SyntaxNet model
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-11。在部署经过训练的SyntaxNet模型时使用束搜索（束大小为2）
- en: During the *expansion* step, we take each hypothesis and consider it as a possible
    input to SyntaxNet. Assume SyntaxNet produces a probability distribution over
    a space of  <math alttext="StartAbsoluteValue upper A EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>A</mi> <mo>|</mo></mrow></math>  total actions. We then compute the probability
    of each of the  <math alttext="b StartAbsoluteValue upper A EndAbsoluteValue"><mrow><mi>b</mi>
    <mo>|</mo> <mi>A</mi> <mo>|</mo></mrow></math>  possible hypotheses for the sequence
    of the first  <math alttext="k plus 1"><mrow><mi>k</mi> <mo>+</mo> <mn>1</mn></mrow></math>
     actions. Then, during the *pruning *step, we keep only the *b* hypothesis out
    of the  <math alttext="b StartAbsoluteValue upper A EndAbsoluteValue"><mrow><mi>b</mi>
    <mo>|</mo> <mi>A</mi> <mo>|</mo></mrow></math>  total options with the largest
    probabilities. As [Figure 9-11](#illustration_of_using_beam_search) illustrates,
    beam searching enables SyntaxNet to correct incorrect predictions post facto by
    entertaining less probable hypotheses early that might turn out to be more fruitful
    later in the sentence. In fact, digging deeper into the illustrated example, a
    greedy approach would have suggested that the correct sequence of moves would
    have been a shift followed by a left arc. In reality, the best (highest probability)
    option would have been to use a left arc followed by a right arc. Beam searching
    with beam size 2 surfaces this result.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在*扩展*步骤中，我们将每个假设视为SyntaxNet的可能输入。假设SyntaxNet对总共 <math alttext="StartAbsoluteValue
    upper A EndAbsoluteValue"><mrow><mo>|</mo> <mi>A</mi> <mo>|</mo></mrow></math>
    个操作空间产生一个概率分布。然后，我们计算前 <math alttext="k plus 1"><mrow><mi>k</mi> <mo>+</mo> <mn>1</mn></mrow></math>
    个操作序列的每个可能假设的概率。然后，在*修剪*步骤中，我们仅保留具有最大概率的*b*个假设，而不是总共 <math alttext="b StartAbsoluteValue
    upper A EndAbsoluteValue"><mrow><mi>b</mi> <mo>|</mo> <mi>A</mi> <mo>|</mo></mrow></math>
    个选项。正如[图9-11](#illustration_of_using_beam_search)所示，束搜索使SyntaxNet能够通过在句子中早期考虑可能性较小的假设来纠正不正确的预测。事实上，深入研究所示例子，贪婪方法会建议正确的移动序列应该是一个移动，然后是一个左弧。实际上，最佳（最高概率）选项应该是使用左弧后跟右弧。束搜索与束大小为2呈现了这个结果。
- en: The full open source version takes this a full step further and attempts to
    bring the concept of beam searching to the process of training the network. As
    Andor et al. described in 2016,^([2](ch09.xhtml#idm45934164127728)) this process
    of *global normalization* provides both strong theoretical guarantees and clear
    performance gains relative to *local normalization* in practice. In a locally
    normalized network, our network is tasked with selecting the best action given
    a configuration. The network outputs a score that is normalized using a softmax
    layer. This is meant to model a probability distribution over all possible actions,
    provided the actions performed thus far. Our loss function attempts to force the
    probability distribution to the ideal output (i.e., probability 1 for the correct
    action and 0 for all other actions). The cross-entropy loss does a spectacular
    job of ensuring this for us.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的开源版本将这一步推进到更深层次，并尝试将束搜索的概念引入到网络训练的过程中。正如Andor等人在2016年所描述的那样，这个*全局归一化*的过程在实践中提供了强大的理论保证和明显的性能增益，相对于*局部归一化*。在局部归一化网络中，我们的网络被要求在给定配置的情况下选择最佳操作。网络输出一个通过softmax层归一化的分数。这旨在对所有可能的操作建模一个概率分布，考虑到迄今为止执行的操作。我们的损失函数试图将概率分布强制为理想输出（即，对于正确操作的概率为1，对于所有其他操作为0）。交叉熵损失出色地确保了这一点。
- en: In a globally normalized network, our interpretation of the scores is slightly
    different. Instead of putting the scores through a softmax to generate a per-action
    probability distribution, we instead add up all the scores for a hypothesis action
    sequence. One way of ensuring that we select the correct hypothesis sequence is
    by computing this sum over all possible hypotheses and then applying a softmax
    layer to generate a probability distribution. We could theoretically use the same
    cross-entropy loss function as we used in the locally normalized network. The
    problem with this strategy, however, is that there is an intractably large number
    of possible hypothesis sequences. Even considering an average sentence length
    of 10 and a conservative total number of 15 possible actions—1 shift and 7 labels
    for each of the left and right arcs—this corresponds to 1,000,000,000,000,000
    possible hypotheses.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在全局归一化网络中，我们对分数的解释略有不同。我们不是通过softmax将分数转换为每个操作的概率分布，而是将所有假设动作序列的分数相加。确保我们选择正确的假设序列的一种方法是计算所有可能假设的总和，然后应用softmax层生成概率分布。理论上，我们可以使用与局部归一化网络中相同的交叉熵损失函数。然而，这种策略的问题在于可能的假设序列数量太大，难以处理。即使考虑到平均句子长度为10，每个左右弧有1个移动和7个标签的保守总操作数为15，这对应于1,000,000,000,000,000个可能的假设。
- en: To make this problem tractable, as shown in [Figure 9-12](#make_global_normalization_in_syntaxnet_tractable),
    we apply a beam search, with a fixed beam size, until we either (1) reach the
    end of the sentence, or (2) the correct sequence of actions is no longer contained
    on the beam. We then construct a loss function that tries to push the “gold standard”
    action sequence (highlighted in blue) as high as possible on the beam by maximizing
    its score relative to the other hypotheses. While we won’t dive into the details
    of how we might construct this loss function here, we refer you to the original
    paper by Andor et al. in 2016.^([3](ch09.xhtml#idm45934164119456)) The paper also
    describes a more sophisticated POS tagger that uses global normalization and beam
    search to significantly increase accuracy (compared to the POS tagger we built
    earlier in the chapter).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个问题可解，如[图9-12](#make_global_normalization_in_syntaxnet_tractable)所示，我们应用一个固定大小的束搜索，直到我们要么（1）到达句子的结尾，要么（2）正确的动作序列不再包含在束中。然后，我们构建一个损失函数，试图通过最大化相对于其他假设的分数来尽可能将“黄金标准”动作序列（用蓝色突出显示）推到束的顶部。虽然我们不会在这里深入讨论如何构建这个损失函数的细节，但我们建议您参考2016年Andor等人的原始论文。^([3](ch09.xhtml#idm45934164119456))
    该论文还描述了一个更复杂的词性标注器，它使用全局归一化和束搜索来显著提高准确性（与我们在本章前面构建的词性标注器相比）。
- en: '![](Images/fdl2_0912.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0912.png)'
- en: Figure 9-12\. Coupling training and beam search can make global normalization
    in SyntaxNet tractable
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-12. 训练和束搜索的耦合可以使SyntaxNet中的全局归一化可解
- en: A Case for Stateful Deep Learning Models
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有关有状态深度学习模型的案例
- en: While we’ve explored several tricks to adapt feed-forward networks to sequence
    analysis, we’ve yet to truly find an elegant solution to sequence analysis. In
    the POS tagger example, we made the explicit assumption that we can ignore long-term
    dependencies. We were able to overcome some of the limitations of this assumption
    by introducing the concepts of beam searching and global normalization, but even
    still, the problem space was constrained to situations in which there was a one-to-one
    mapping between elements in the input sequence to elements in the output sequence.
    For example, even in the dependency parsing model, we had to reformulate the problem
    to discover a one-to-one mapping between a sequence of input configurations while
    constructing the parse tree and arc-standard actions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经探索了几种将前馈网络适应序列分析的技巧，但我们尚未真正找到一个优雅的解决方案来进行序列分析。在词性标注器示例中，我们明确假设可以忽略长期依赖关系。通过引入束搜索和全局归一化的概念，我们能够克服这种假设的一些局限性，但即使如此，问题空间仍受限于输入序列中的元素与输出序列中的元素之间存在一对一的映射的情况。例如，即使在依赖解析模型中，我们也必须重新制定问题，以发现在构建解析树和弧标准动作时，输入配置序列之间存在一对一的映射。
- en: Sometimes, however, the task is far more complicated than finding a one-to-one
    mapping between input and output sequences. For example, we might want to develop
    a model that can consume an entire input sequence at once and then conclude if
    the sentiment of the entire input was positive or negative. We’ll build a simple
    model to perform this task later in the chapter. We may want an algorithm that
    consumes a complex input (such as an image) and generate a sentence, one word
    at a time, describing the input. We may event want to translate sentences from
    one language to another (e.g., from English to French). In all of these instances,
    there’s no obvious mapping between input tokens and output tokens. Instead, the
    process is more like the situation in [Figure 9-13](#ideal_model_for_sequence_analysis).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时任务比找到输入和输出序列之间的一对一映射要复杂得多。例如，我们可能希望开发一个模型，可以一次消耗整个输入序列，然后得出整个输入的情感是积极的还是消极的。我们将在本章后面构建一个简单的模型来执行这个任务。我们可能希望一个算法可以消耗一个复杂的输入（比如一幅图像），然后逐字生成一个描述输入的句子。我们甚至可能希望将句子从一种语言翻译成另一种语言（例如，从英语到法语）。在所有这些情况下，输入标记和输出标记之间没有明显的映射。相反，这个过程更像是[图9-13](#ideal_model_for_sequence_analysis)中的情况。
- en: '![](Images/fdl2_0913.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0913.png)'
- en: Figure 9-13\. The ideal model for sequence analysis can store information in
    memory over long periods of time, leading to a coherent “thought” vector that
    it can use to generate an answer
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-13. 用于序列分析的理想模型可以在长时间内存储信息，从而产生一个连贯的“思考”向量，可以用来生成答案
- en: The idea is simple. We want our model to maintain some sort of memory over the
    span of reading the input sequence. As it reads the input, the model should be
    able to modify this memory bank, taking into account the information that it observes.
    By the time it has reached the end of the input sequence, the internal memory
    contains a “thought” that represents the key pieces of information, that is, the
    meaning, of the original input. We should then, as shown in [Figure 9-13](#ideal_model_for_sequence_analysis),
    be able to use this thought vector to either produce a label for the original
    sequence or produce an appropriate output sequence (translation, description,
    abstractive summary, etc.).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单。我们希望我们的模型在阅读输入序列的过程中保持某种记忆。当它阅读输入时，模型应该能够修改这个记忆库，考虑到它观察到的信息。当它到达输入序列的末尾时，内部记忆包含一个代表原始输入的关键信息，即意义的“思考”。然后，如[图9-13](#ideal_model_for_sequence_analysis)所示，我们应该能够使用这个思考向量来为原始序列产生一个标签，或者产生一个适当的输出序列（翻译、描述、抽象摘要等）。
- en: The concept here isn’t something we’ve explored in any of the previous chapters.
    Feed-forward networks are inherently “stateless.” After it’s been trained, the
    feed-forward network is a static structure. It isn’t able to maintain memories
    between inputs, or change how it processes an input based on inputs it has seen
    in the past. To execute this strategy, we’ll need to reconsider how we construct
    neural networks to create deep learning models that are “stateful.” To do this,
    we’ll have to return to how we think about networks on an individual neuron level.
    In the next section, we’ll explore how *recurrent connections* (as opposed to
    the feed-forward connections we have studied this far) enable models to maintain
    state as we describe a class of models known as *recurrent neural networks* (RNNs).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的概念是我们在之前的章节中没有探讨过的。前馈网络本质上是“无状态”的。在训练后，前馈网络是一个静态结构。它无法在输入之间保持记忆，或者根据过去看到的输入改变其处理输入的方式。为了执行这种策略，我们需要重新考虑如何构建神经网络，以创建“有状态”的深度学习模型。为此，我们将不得不回到如何在单个神经元级别思考网络。在下一节中，我们将探讨*循环连接*（与迄今为止我们研究的前馈连接相对）如何使模型保持状态，同时描述一类称为*循环神经网络*（RNNs）的模型。
- en: Recurrent Neural Networks
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: RNNs were first introduced in the 1980s, but have regained popularity recently
    due to several intellectual and hardware breakthroughs that have made them tractable
    to train. RNNs are different from feed-forward networks because they leverage
    a special type of neural layer, known as recurrent layers, that enable the network
    to maintain state between uses of the network.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs最早在1980年代引入，但最近由于几项智力和硬件突破，使得它们易于训练而重新流行起来。RNNs与前馈网络不同，因为它们利用一种特殊类型的神经层，称为循环层，使得网络能够在使用网络之间保持状态。
- en: '[Figure 9-14](#recurrent_layer_contains) illustrates the neural architecture
    of a recurrent layer. All of the neurons have both (1) incoming connections emanating
    from all of the neurons of the previous layer and (2) outgoing connections leading
    to all of the neurons to the subsequent layer. We notice here, however, that these
    aren’t the only connections that neurons of a recurrent layer have. Unlike a feed-forward
    layer, recurrent layers also have recurrent connections, which propagate information
    between neurons of the same layer. A fully connected recurrent layer has information
    flow from every neuron to every other neuron in its layer (including itself).
    Thus a recurrent layer with  <math alttext="r"><mi>r</mi></math>  neurons has
    a total of  <math alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math>
     recurrent connections.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-14](#recurrent_layer_contains)展示了循环层的神经结构。所有神经元都有(1)来自前一层所有神经元的传入连接和(2)指向后续层所有神经元的传出连接。然而，我们注意到循环层神经元并不只有这些连接。与前馈层不同，循环层还具有循环连接，用于在同一层神经元之间传播信息。一个完全连接的循环层使得每个神经元都与其层中的每个其他神经元(包括自身)之间有信息流。因此，一个具有 
    <math alttext="r"><mi>r</mi></math>  个神经元的循环层共有  <math alttext="r squared"><msup><mi>r</mi>
    <mn>2</mn></msup></math>  个循环连接。'
- en: '![](Images/fdl2_0914.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0914.png)'
- en: Figure 9-14\. A recurrent layer contains recurrent connections, that is to say,
    connections between neurons that are located in the same layer
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-14。循环层包含循环连接，即位于同一层的神经元之间的连接
- en: To better understand how RNNs work, let’s explore how one functions after it’s
    been appropriately trained. Every time we want to process a new sequence, we create
    a fresh instance of our model. We can reason about networks that contain recurrent
    layers by dividing the lifetime of the network instance into discrete time steps.
    At each time step, we feed the model the next element of the input. Feed-forward
    connections represent information flow from one neuron to another where the data
    being transferred is the computed neuronal activation from the current time step.
    Recurrent connections, however, represent information flow where the data is the
    stored neuronal activation from the *previous* time step. Thus, the activations
    of the neurons in a recurrent network represent the accumulating state of the
    network instance. The initial activations of neurons in the recurrent layer are
    parameters of our model, and we determine the optimal values for them just like
    we determine the optimal values for the weights of each connection during the
    process of training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解RNNs的工作原理，让我们探讨一下在适当训练后一个RNN是如何运作的。每当我们想要处理一个新序列时，我们会创建一个模型的新实例。我们可以通过将网络实例的生命周期划分为离散的时间步来推理包含循环层的网络。在每个时间步，我们将输入的下一个元素提供给模型。前馈连接表示从一个神经元到另一个神经元的信息流，传输的数据是当前时间步的计算神经元激活。然而，循环连接表示数据是*上一个*时间步存储的神经元激活的信息流。因此，循环网络中神经元的激活表示网络实例的累积状态。循环层中神经元的初始激活是我们模型的参数，我们像确定每个连接的权重的最佳值一样确定它们的最佳值在训练过程中。
- en: It turns out that, given a fixed lifetime (say *t* time steps) of an RNN instance,
    we can actually express the instance as a feed-forward network (albeit irregularly
    structured). This clever transformation, illustrated in [Figure 9-15](#an_rnn_through_time),
    is often referred to as “unrolling” the RNN through time. Let’s consider the example
    RNN in the figure. We’d like to map a sequence of two inputs (each dimension 1)
    to a single output (also of dimension 1). We perform the transformation by taking
    the neurons of the single recurrent layer and replicating them it *t* times, once
    for each time step. We similarly replicate the neurons of the input and output
    layers. We redraw the feed-forward connections within each time replica just as
    they were in the original network. Then we draw the recurrent connections as feed-forward
    connections from each time replica to the next (since the recurrent connections
    carry the neuronal activation from the previous time step).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，给定一个固定的RNN实例寿命（比如*t*个时间步），我们实际上可以将实例表示为一个前馈网络（尽管结构不规则）。这种巧妙的转换，如[图9-15](#an_rnn_through_time)所示，通常被称为“通过时间展开”RNN。让我们考虑图中的示例RNN。我们想要将两个输入序列（每个维度为1）映射到一个单一输出（也是维度为1）。我们通过将单个循环层的神经元复制*t*次来执行转换，每个时间步都复制一次。我们类似地复制输入和输出层的神经元。我们重新绘制每个时间副本内的前馈连接，就像在原始网络中一样。然后我们将循环连接绘制为从每个时间副本到下一个时间副本的前馈连接（因为循环连接携带前一个时间步的神经元激活）。
- en: '![](Images/fdl2_0915.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0915.png)'
- en: Figure 9-15\. We can run an RNN through time to express it as a feed-forward
    network that we can train using backpropagation
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-15. 我们可以通过时间运行RNN来将其表示为一个可以使用反向传播训练的前馈网络
- en: We can also now train the RNN by computing the gradient based on the unrolled
    version. This means that all of the backpropagation techniques that we used for
    feed-forward networks also apply to training RNNs. We do run into one issue, however.
    After every batch of training examples we use, we need to modify the weights based
    on the error derivatives we calculate. In our unrolled network, we have sets of
    connections that all correspond to the same connection in the original RNN. The
    error derivatives calculated for these unrolled connections, however, are not
    guaranteed to be (and, in practice, probably won’t be) equal. We can circumvent
    this issue by averaging or summing the error derivatives over all the connections
    that belong to the same set. This allows us to utilize an error derivative that
    considers all of the dynamics acting on the weight of a connection as we attempt
    to force the network to construct an accurate output.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在也可以通过计算展开版本的梯度来训练RNN。这意味着我们为前馈网络使用的所有反向传播技术也适用于训练RNN。然而，我们遇到了一个问题。在使用每批训练示例之后，我们需要根据计算的误差导数修改权重。在我们的展开网络中，我们有一组连接，它们都对应于原始RNN中的同一连接。然而，为这些展开连接计算的误差导数不能保证是相等的（实际上，可能不会相等）。我们可以通过对属于同一组的所有连接的误差导数进行平均或求和来规避这个问题。这使我们能够利用一个考虑到所有作用于连接权重的动态的误差导数，以便我们试图迫使网络构建准确的输出。
- en: The Challenges with Vanishing Gradients
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消失梯度的挑战
- en: Our motivation for using a stateful network model hinges on this idea of capturing
    long-term dependencies in the input sequence. It seems reasonable that an RNN
    with a large memory bank (i.e., a significantly sized recurrent layer) would be
    able to summarize these dependencies. In fact, from a theoretical perspective,
    Kilian and Siegelmann demonstrated in 1996 that the RNN is a universal functional
    representation.^([4](ch09.xhtml#idm45934164070368)) In other words, with enough
    neurons and the right parameter settings, an RNN can be used to represent any
    functional mapping between input and output sequences.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用有状态的网络模型的动机在于捕捉输入序列中的长期依赖关系。一个具有大内存库（即具有相当大的循环层）的RNN能够总结这些依赖关系似乎是合理的。事实上，从理论上讲，Kilian和Siegelmann在1996年证明了RNN是一个通用的功能表示。换句话说，通过足够的神经元和正确的参数设置，RNN可以用来表示输入和输出序列之间的任何功能映射。
- en: The theory is promising, but it doesn’t necessarily translate to practice. While
    it is nice to know that it is *possible* for an RNN to represent any arbitrary
    function, it is more useful to know whether it is *practical* to teach the RNN
    a realistic functional mapping from scratch by applying gradient descent algorithms.
    If it turns out to be impractical, we’ll be in hot water, so it will be useful
    for us to be rigorous in exploring this question. Let’s start our investigation
    by considering the simplest possible RNN, shown in [Figure 9-16](#single_neuron_fully_connected),
    with a single input neuron, a single output neuron, and a fully connected recurrent
    layer with one neuron.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个理论很有前途，但并不一定能转化为实践。知道RNN可以表示任意任意函数是很好的，但更有用的是知道是否可以通过应用梯度下降算法从头开始教会RNN一个现实的功能映射。如果发现这是不切实际的，我们将陷入困境，因此我们有必要严谨地探讨这个问题。让我们从考虑最简单的可能的RNN开始，如[图9-16](#single_neuron_fully_connected)所示，具有一个输入神经元，一个输出神经元和一个具有一个神经元的全连接循环层。
- en: '![](Images/fdl2_0916.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0916.png)'
- en: Figure 9-16\. A single neuron, fully connected recurrent layer (both compressed
    and unrolled) for the sake of investigating gradient-based learning algorithms
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-16. 一个单神经元，完全连接的循环层（压缩和展开），用于研究基于梯度的学习算法
- en: 'Let’s start off simple. Given nonlinearity <math alttext="f"><mi>f</mi></math>
    , we can express the activation <math alttext="h Superscript left-parenthesis
    t right-parenthesis"><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></math>
     of the hidden neuron of the recurrent layer at time step *t* as follows, where
    <math alttext="i Superscript left-parenthesis t right-parenthesis"><msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></math>  is the incoming logit
    from the input neuron at time step *t*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单的开始。给定非线性 <math alttext="f"><mi>f</mi></math>，我们可以将循环层中隐藏神经元在时间步 *t* 的激活 <math
    alttext="h Superscript left-parenthesis t right-parenthesis"><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></math> 表示为以下形式，其中 <math alttext="i
    Superscript left-parenthesis t right-parenthesis"><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></math> 是来自时间步*t*的输入神经元的传入逻辑：
- en: <math alttext="h Superscript left-parenthesis t right-parenthesis Baseline equals
    f left-parenthesis w Subscript i n Superscript left-parenthesis t right-parenthesis
    Baseline i Superscript left-parenthesis t right-parenthesis Baseline plus w Subscript
    r e c Superscript left-parenthesis t minus 1 right-parenthesis Baseline h Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline right-parenthesis"><mrow><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>=</mo> <mi>f</mi> <mfenced
    separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="h Superscript left-parenthesis t right-parenthesis Baseline equals
    f left-parenthesis w Subscript i n Superscript left-parenthesis t right-parenthesis
    Baseline i Superscript left-parenthesis t right-parenthesis Baseline plus w Subscript
    r e c Superscript left-parenthesis t minus 1 right-parenthesis Baseline h Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline right-parenthesis"><mrow><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>=</mo> <mi>f</mi> <mfenced
    separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: 'Let’s try to compute how the activation of the hidden neuron changes in response
    to changes to the input logit from *k* time steps in the past. In analyzing this
    component of the backpropagation gradient expressions, we can start to quantify
    how much “memory” is retained from past inputs. We start by taking the partial
    derivative and apply the chain rule:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试计算隐藏神经元的激活如何响应过去*k*个时间步的输入逻辑的变化。在分析反向传播梯度表达式的这个组件时，我们可以开始量化从过去输入中保留多少“记忆”。我们首先取偏导数并应用链式法则：
- en: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction equals f prime left-parenthesis
    w Subscript i n Superscript left-parenthesis t right-parenthesis Baseline i Superscript
    left-parenthesis t right-parenthesis Baseline plus w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline h Superscript left-parenthesis
    t minus 1 right-parenthesis Baseline right-parenthesis StartFraction normal partial-differential
    Over normal partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction left-parenthesis w Subscript i n Superscript left-parenthesis
    t right-parenthesis Baseline i Superscript left-parenthesis t right-parenthesis
    Baseline plus w Subscript r e c Superscript left-parenthesis t minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus 1 right-parenthesis Baseline right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow> <mrow><mi>∂</mi><msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <mfrac><mi>∂</mi> <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction equals f prime left-parenthesis
    w Subscript i n Superscript left-parenthesis t right-parenthesis Baseline i Superscript
    left-parenthesis t right-parenthesis Baseline plus w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline h Superscript left-parenthesis
    t minus 1 right-parenthesis Baseline right-parenthesis StartFraction normal partial-differential
    Over normal partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction left-parenthesis w Subscript i n Superscript left-parenthesis
    t right-parenthesis Baseline i Superscript left-parenthesis t right-parenthesis
    Baseline plus w Subscript r e c Superscript left-parenthesis t minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus 1 right-parenthesis Baseline right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow> <mrow><mi>∂</mi><msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <mfrac><mi>∂</mi> <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: 'Because the values of the input and recurrent weights are independent of the
    input logit at time step  <math alttext="t minus k"><mrow><mi>t</mi> <mo>-</mo>
    <mi>k</mi></mrow></math> , we can further simplify this expression:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因为输入和循环权重的值与时间步 <math alttext="t minus k"><mrow><mi>t</mi> <mo>-</mo> <mi>k</mi></mrow></math> 的输入逻辑无关，我们可以进一步简化这个表达式：
- en: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction equals f prime left-parenthesis
    w Subscript i n Superscript left-parenthesis t right-parenthesis Baseline i Superscript
    left-parenthesis t right-parenthesis Baseline plus w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline h Superscript left-parenthesis
    t minus 1 right-parenthesis Baseline right-parenthesis w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline StartFraction normal partial-differential
    h Superscript left-parenthesis t minus 1 right-parenthesis Baseline Over normal
    partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction equals f prime left-parenthesis
    w Subscript i n Superscript left-parenthesis t right-parenthesis Baseline i Superscript
    left-parenthesis t right-parenthesis Baseline plus w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline h Superscript left-parenthesis
    t minus 1 right-parenthesis Baseline right-parenthesis w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline StartFraction normal partial-differential
    h Superscript left-parenthesis t minus 1 right-parenthesis Baseline Over normal
    partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: 'Because we care about the magnitude of this derivative, we can take the absolute
    value of both sides. We also know that for all common nonlinearities (the tanh,
    logistic, and ReLU nonlinearities), the maximum value of  <math alttext="StartAbsoluteValue
    f prime EndAbsoluteValue"><mfenced separators="" open="|" close="|"><msup><mi>f</mi>
    <mo>''</mo></msup></mfenced></math>  is at most 1\. This leads to the following
    recursive inequality:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们关心这个导数的大小，我们可以对两边取绝对值。我们也知道对于所有常见的非线性（tanh、logistic和ReLU非线性）， <math alttext="StartAbsoluteValue
    f prime EndAbsoluteValue"><mfenced separators="" open="|" close="|"><msup><mi>f</mi>
    <mo>'</mo></msup></mfenced></math> 的最大值最多为1。这导致以下递归不等式：
- en: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue
    StartFraction normal partial-differential h Superscript left-parenthesis t minus
    1 right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction EndAbsoluteValue"><mrow><mfenced
    separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced></mrow></math>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue
    StartFraction normal partial-differential h Superscript left-parenthesis t minus
    1 right-parenthesis Baseline Over normal partial-differential i Superscript left-parenthesis
    t minus k right-parenthesis Baseline EndFraction EndAbsoluteValue"><mrow><mfenced
    separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced></mrow></math>
- en: 'We can continue to expand this inequality recursively until we reach the base
    case, at step  <math alttext="t minus k"><mrow><mi>t</mi> <mo>-</mo> <mi>k</mi></mrow></math>
    :'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续递归地扩展这个不等式，直到达到基本情况，在步骤 <math alttext="t minus k"><mrow><mi>t</mi> <mo>-</mo>
    <mi>k</mi></mrow></math> 处：
- en: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot ellipsis
    dot StartAbsoluteValue w Subscript r e c Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue StartFraction
    normal partial-differential h Superscript left-parenthesis t minus k right-parenthesis
    Baseline Over normal partial-differential i Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndFraction EndAbsoluteValue"><mrow><mfenced separators=""
    open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mo>...</mo> <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced></mrow></math>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot ellipsis
    dot StartAbsoluteValue w Subscript r e c Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue StartFraction
    normal partial-differential h Superscript left-parenthesis t minus k right-parenthesis
    Baseline Over normal partial-differential i Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndFraction EndAbsoluteValue"><mrow><mfenced separators=""
    open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mo>...</mo> <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced></mrow></math>
- en: 'We can evaluate this partial derivative similarly to how we proceeded previously:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以类似地评估这个偏导数：
- en: <math alttext="h Superscript left-parenthesis t minus k right-parenthesis Baseline
    equals f left-parenthesis w Subscript i n Superscript left-parenthesis t minus
    k right-parenthesis Baseline i Superscript left-parenthesis t minus k right-parenthesis
    Baseline plus w Subscript r e c Superscript left-parenthesis t minus k minus 1
    right-parenthesis Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline right-parenthesis"><mrow><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <mi>f</mi> <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="h Superscript left-parenthesis t minus k right-parenthesis Baseline
    equals f left-parenthesis w Subscript i n Superscript left-parenthesis t minus
    k right-parenthesis Baseline i Superscript left-parenthesis t minus k right-parenthesis
    Baseline plus w Subscript r e c Superscript left-parenthesis t minus k minus 1
    right-parenthesis Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline right-parenthesis"><mrow><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <mi>f</mi> <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t minus k right-parenthesis Baseline Over normal partial-differential i Superscript
    left-parenthesis t minus k right-parenthesis Baseline EndFraction equals f prime
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis StartFraction normal partial-differential Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <mfrac><mi>∂</mi> <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t minus k right-parenthesis Baseline Over normal partial-differential i Superscript
    left-parenthesis t minus k right-parenthesis Baseline EndFraction equals f prime
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis StartFraction normal partial-differential Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <mfrac><mi>∂</mi> <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mfenced separators="" open="(" close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup> <msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: 'In this expression, the hidden activation at time  <math alttext="t minus k
    minus 1"><mrow><mi>t</mi> <mo>-</mo> <mi>k</mi> <mo>-</mo> <mn>1</mn></mrow></math>
     is independent of the value of the input at <math alttext="t minus k"><mrow><mi>t</mi>
    <mo>-</mo> <mi>k</mi></mrow></math> . Thus we can rewrite this expression as:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表达式中，时间为 <math alttext="t minus k minus 1"><mrow><mi>t</mi> <mo>-</mo> <mi>k</mi>
    <mo>-</mo> <mn>1</mn></mrow></math> 的隐藏激活与时间为 <math alttext="t minus k"><mrow><mi>t</mi>
    <mo>-</mo> <mi>k</mi></mrow></math> 的输入值无关。因此我们可以将这个表达式重写为：
- en: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t minus k right-parenthesis Baseline Over normal partial-differential i Superscript
    left-parenthesis t minus k right-parenthesis Baseline EndFraction equals f prime
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mrow></math>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction normal partial-differential h Superscript left-parenthesis
    t minus k right-parenthesis Baseline Over normal partial-differential i Superscript
    left-parenthesis t minus k right-parenthesis Baseline EndFraction equals f prime
    left-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis
    Baseline i Superscript left-parenthesis t minus k right-parenthesis Baseline plus
    w Subscript r e c Superscript left-parenthesis t minus k minus 1 right-parenthesis
    Baseline h Superscript left-parenthesis t minus k minus 1 right-parenthesis Baseline
    right-parenthesis w Subscript i n Superscript left-parenthesis t minus k right-parenthesis"><mrow><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mfenced separators="" open="("
    close=")"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup>
    <msup><mi>h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mrow></math>
- en: 'Finally, taking the absolute value on both sides and again applying the observation
    about the maximum value of  <math alttext="StartAbsoluteValue f prime EndAbsoluteValue"><mfenced
    separators="" open="|" close="|"><msup><mi>f</mi> <mo>''</mo></msup></mfenced></math>
    , we can write:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对两边取绝对值，并再次应用关于 <math alttext="StartAbsoluteValue f prime EndAbsoluteValue"><mfenced
    separators="" open="|" close="|"><msup><mi>f</mi> <mo>'</mo></msup></mfenced></math> 最大值的观察，我们可以写成：
- en: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t minus k right-parenthesis Baseline Over normal
    partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue
    w Subscript i n Superscript left-parenthesis t minus k right-parenthesis Baseline
    EndAbsoluteValue"><mrow><mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced></mrow></math>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t minus k right-parenthesis Baseline Over normal
    partial-differential i Superscript left-parenthesis t minus k right-parenthesis
    Baseline EndFraction EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue
    w Subscript i n Superscript left-parenthesis t minus k right-parenthesis Baseline
    EndAbsoluteValue"><mrow><mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow>
    <mrow><mi>∂</mi><msup><mi>i</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced></mrow></math>
- en: 'This results in the final inequality (which we can simplify because we constrain
    the connections at different time steps to have equal value):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致最终的不等式（我们可以简化，因为我们约束不同时间步的连接具有相等的值）：
- en: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot ellipsis
    dot StartAbsoluteValue w Subscript r e c Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue w Subscript
    i n Superscript left-parenthesis t minus k right-parenthesis Baseline EndAbsoluteValue
    equals StartAbsoluteValue w Subscript r e c Baseline EndAbsoluteValue Superscript
    k Baseline dot w Subscript i n"><mrow><mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow> <mrow><mi>∂</mi><msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mo>...</mo> <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>=</mo> <msup><mfenced separators="" open="|" close="|"><msub><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow></msub></mfenced>
    <mi>k</mi></msup> <mo>·</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></math>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue StartFraction normal partial-differential
    h Superscript left-parenthesis t right-parenthesis Baseline Over normal partial-differential
    i Superscript left-parenthesis t minus k right-parenthesis Baseline EndFraction
    EndAbsoluteValue less-than-or-equal-to StartAbsoluteValue w Subscript r e c Superscript
    left-parenthesis t minus 1 right-parenthesis Baseline EndAbsoluteValue dot ellipsis
    dot StartAbsoluteValue w Subscript r e c Superscript left-parenthesis t minus
    k right-parenthesis Baseline EndAbsoluteValue dot StartAbsoluteValue w Subscript
    i n Superscript left-parenthesis t minus k right-parenthesis Baseline EndAbsoluteValue
    equals StartAbsoluteValue w Subscript r e c Baseline EndAbsoluteValue Superscript
    k Baseline dot w Subscript i n"><mrow><mfenced separators="" open="|" close="|"><mfrac><mrow><mi>∂</mi><msup><mi>h</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow> <mrow><mi>∂</mi><msup><mi>i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msup></mrow></mfrac></mfenced>
    <mo>≤</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mo>...</mo> <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>·</mo> <mfenced separators="" open="|" close="|"><msubsup><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mo>=</mo> <msup><mfenced separators="" open="|" close="|"><msub><mi>w</mi> <mrow><mi>r</mi><mi>e</mi><mi>c</mi></mrow></msub></mfenced>
    <mi>k</mi></msup> <mo>·</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></math>
- en: This relationship places a strong upper bound on how much a change in the input
    at time  <math alttext="t minus k"><mrow><mi>t</mi> <mo>-</mo> <mi>k</mi></mrow></math>
     can impact the hidden state at time *t*. Because the weights of our model are
    initialized to small values at the beginning of training, the value of this derivative
    approaches zero as *k* increases. In other words, the gradient quickly diminishes
    when it’s computed with respect to inputs several time steps into the past, severely
    limiting our model’s ability to learn long-term dependencies. This issue is commonly
    referred to as the problem of *vanishing gradients*, and it severely impacts the
    learning capabilities of vanilla RNNs. In order to address this limitation, we
    will spend the next section exploring an extraordinarily influential twist on
    recurrent layers known as long short-term memory.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关系在很大程度上限制了时间为 <math alttext="t minus k"><mrow><mi>t</mi> <mo>-</mo> <mi>k</mi></mrow></math> 的输入变化对时间为*t*的隐藏状态的影响。因为我们模型的权重在训练开始时被初始化为小值，所以这个导数的值随着*k*的增加而接近零。换句话说，当计算与过去几个时间步的输入相关时，梯度迅速减小，严重限制了我们模型学习长期依赖关系的能力。这个问题通常被称为*梯度消失*问题，严重影响了普通RNN的学习能力。为了解决这个限制，我们将在下一节中探讨对循环层的一个极具影响力的变化，即长短期记忆。
- en: Long Short-Term Memory Units
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆单元
- en: To combat the problem of vanishing gradients, Sepp Hochreiter and Jürgen Schmidhuber
    introduced the *long short-term memory* (LSTM) architecture. The basic principle
    behind the architecture was that the network would be designed for the purpose
    of reliably transmitting important information many time steps into the future.
    The design considerations resulted in the architecture shown in [Figure 9-17](#architecture_of_an_lstm_unit).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决梯度消失的问题，Sepp Hochreiter和Jürgen Schmidhuber引入了*长短期记忆*（LSTM）架构。该架构背后的基本原则是，网络将被设计用于可靠地传输重要信息到未来的许多时间步。设计考虑导致了[图9-17](#architecture_of_an_lstm_unit)中所示的架构。
- en: '![](Images/fdl2_0917.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0917.png)'
- en: Figure 9-17\. The architecture of an LSTM unit, illustrated at a tensor (designated
    by arrows) and operation (designated by the inner blocks) level
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-17\. LSTM单元的架构，以张量（由箭头表示）和操作（由内部块表示）级别进行说明
- en: For the purposes of this discussion, we’ll take a step back from the individual
    neuron level and start talking about the network as collection tensors and operations
    on tensors. As the figure indicates, the LSTM unit is composed of several key
    components. One of the core components of the LSTM architecture is the *memory
    cell*, a tensor represented by the bolded loop in the center of the figure. The
    memory cell holds critical information that it has learned over time, and the
    network is designed to effectively maintain useful information in the memory cell
    over many time steps. At every time step, the LSTM unit modifies the memory cell
    with new information with three different phases. First, the unit must determine
    how much of the previous memory to keep. This is determined by the *keep gate*,
    shown in detail in [Figure 9-18](#architecture_of_the_keep_gate).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了讨论的目的，我们将从单个神经元级别退后一步，开始讨论网络作为张量集合和张量上的操作。正如图所示，LSTM单元由几个关键组件组成。 LSTM架构的一个核心组件是*内存单元*，在图中心的粗体循环表示的张量。内存单元保存了它随时间学到的关键信息，并且网络被设计为在许多时间步长上有效地保持内存单元中的有用信息。在每个时间步长，LSTM单元通过三个不同的阶段用新信息修改内存单元。首先，单元必须确定要保留多少先前的记忆。这由*保持门*确定，详细显示在[图9-18](#architecture_of_the_keep_gate)中。
- en: '![](Images/fdl2_0918.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0918.png)'
- en: Figure 9-18\. Architecture of the keep gate of an LSTM unit
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-18\. LSTM单元保持门的架构
- en: The basic idea of the keep gate is simple. The memory state tensor from the
    previous time step is rich with information, but some of that information may
    be stale (and therefore might need to be erased). We figure out which elements
    in the memory state tensor are still relevant and which elements are irrelevant
    by trying to compute a bit tensor (a tensor of zeros and ones) that we multiply
    with the previous state. If a particular location in the bit tensor holds a 1,
    it means that location in the memory cell is still relevant and ought to be kept.
    If that particular location instead holds a 0, it means that the location in the
    memory cell is no longer relevant and ought to be eased. We approximate this bit
    tensor by concatenating the input of this time step and the LSTM unit’s output
    from the previous time step and applying a sigmoid layer to the resulting tensor.
    A sigmoidal neuron, as you may recall, outputs a value that is either very close
    to 0 or very close to 1 most of the time (the only exception is when the input
    is close to 0). As a result, the output of the sigmoidal layer is a close approximation
    of a bit tensor, and we can use this to complete the keep gate.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 保持门的基本思想很简单。来自上一个时间步的内存状态张量充满信息，但其中一些信息可能已经过时（因此可能需要被擦除）。我们通过尝试计算一个比特张量（一个由零和一组成的张量）来弄清楚内存状态张量中哪些元素仍然相关，哪些元素是无关的。如果比特张量中的特定位置包含1，则表示内存单元中的该位置仍然相关且应该保留。如果该特定位置相反包含0，则表示内存单元中的该位置不再相关且应该被擦除。我们通过将本时间步的输入和上一个时间步的LSTM单元输出连接起来，并对结果张量应用sigmoid层来近似这个比特张量。正如您可能记得的那样，sigmoid神经元大部分时间输出接近0或接近1的值（唯一的例外是当输入接近0时）。因此，sigmoid层的输出是比特张量的一个接近近似，我们可以使用这个来完成保持门。
- en: Once we’ve figured out what information to keep in the old state and what to
    erase, we’re ready to think about what information we’d like to write into the
    memory state. This part of the LSTM unit is known as the *write gate*, and it’s
    depicted in [Figure 9-19](#architecture_of_the_write_gate). This is broken down
    into two major parts. The first component is figuring out what information we’d
    like to write into the state. This is computed by the tanh layer to create an
    intermediate tensor. The second component is figuring out which components of
    this computed tensor we actually want to include into the new state and which
    we want to toss before writing. We do this by approximating a bit vector of 0’s
    and 1’s using the same strategy (a sigmoidal layer) as we used in the keep gate.
    We multiply the bit vector with our intermediate tensor and then add the result
    to create the new state vector for the LSTM.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们弄清楚了要在旧状态中保留什么信息和要擦除什么信息，我们就准备考虑我们想要写入内存状态的信息。这部分LSTM单元称为*写入门*，在[图9-19](#architecture_of_the_write_gate)中描述。这可以分解为两个主要部分。第一个组件是弄清楚我们想要写入状态的信息。这通过tanh层计算以创建一个中间张量。第二个组件是弄清楚我们实际上想要包含到新状态中的计算张量的哪些组件，以及我们在写入之前想要丢弃哪些组件。我们通过使用与我们在保持门中使用的相同策略（一个sigmoid层）来近似一个由0和1组成的比特向量。我们将比特向量与我们的中间张量相乘，然后将结果相加以创建LSTM的新状态向量。
- en: '![](Images/fdl2_0919.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0919.png)'
- en: Figure 9-19\. Architecture of the write gate of an LSTM unit
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-19\. LSTM单元写入门的架构
- en: 'At every time step, we’d like the LSTM unit to provide an output. While we
    could treat the state vector as the output directly, the LSTM unit is engineered
    to provide more flexibility by emitting an output tensor that is an “interpretation”
    or external “communication” of what the state vector represents. The architecture
    of the output gate is shown in [Figure 9-20](#architecture_of_the_output_gate).
    We use a nearly identical structure as the write gate: (1) the tanh layer creates
    an intermediate tensor from the state vector, (2) the sigmoid layer produces a
    bit tensor mask using the current input and previous output, and (3) the intermediate
    tensor is multiplied with the bit tensor to produce the final output.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，我们希望LSTM单元提供一个输出。虽然我们可以直接将状态向量视为输出，但LSTM单元经过设计，通过发出一个输出张量来提供更多灵活性，该输出张量是对状态向量表示的“解释”或外部“通信”。输出门的架构显示在[图9-20](#architecture_of_the_output_gate)中。我们使用几乎相同的结构作为写入门：（1）tanh层从状态向量创建一个中间张量，（2）sigmoid层使用当前输入和先前输出产生一个位张量掩码，（3）中间张量与位张量相乘以产生最终输出。
- en: '![](Images/fdl2_0920.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0920.png)'
- en: Figure 9-20\. Architecture of the output gate of an LSTM unit
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-20\. LSTM单元的输出门架构
- en: So why is this better than using a raw RNN unit? The key observation is how
    information propagates through the network when we unroll the LSTM unit through
    time. The unrolled architecture is shown in [Figure 9-21](#unrolling_an_lstm_unit).
    At the top, we can observed the propagation of the state vector, whose interactions
    are primarily linear through time. The result is that the gradient that relates
    an input several time steps in the past to the current output does not attenuate
    as dramatically as in the vanilla RNN architecture. This means that the LSTM can
    learn long-term relationships much more effectively than our original formulation
    of the RNN.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么这比使用原始RNN单元更好呢？关键观察是当我们将LSTM单元在时间上展开时，信息如何在网络中传播。展开的架构显示在[图9-21](#unrolling_an_lstm_unit)中。在顶部，我们可以观察到状态向量的传播，其相互作用主要是线性的。结果是，将过去几个时间步的输入与当前输出相关联的梯度不会像在普通RNN架构中那样急剧减弱。这意味着LSTM可以比我们最初的RNN公式更有效地学习长期关系。
- en: '![](Images/fdl2_0921.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0921.png)'
- en: Figure 9-21\. Unrolling an LSTM unit through time
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-21\. 通过时间展开LSTM单元
- en: Finally, we want to understand how easy it is to generate arbitrary architectures
    with LSTM units. How “composable” are LSTMs? Do we need to sacrifice flexibility
    to use LSTM units instead of a vanilla RNN? Just as we can we can stack RNN layers
    to create more expressive models with more capacity, we can stack LSTM units,
    where the input of the second unit is the output of the first unit, the input
    of the third unit is the output of the second, and so on. [Figure 9-22](#composimg_lstm_units)
    shows how this works with a multicellular architecture made of two LSTM units.
    This means that anywhere we use a vanilla RNN layer, we can easily substitute
    an LSTM unit.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想要了解使用LSTM单元生成任意架构有多容易。LSTM有多“可组合”？我们是否需要牺牲灵活性来使用LSTM单元而不是普通的RNN？就像我们可以堆叠RNN层以创建更具表现力和容量的模型一样，我们可以堆叠LSTM单元，其中第二个单元的输入是第一个单元的输出，第三个单元的输入是第二个单元的输出，依此类推。[图9-22](#composimg_lstm_units)展示了由两个LSTM单元组成的多细胞架构的工作原理。这意味着我们可以在任何使用普通RNN层的地方轻松替换为LSTM单元。
- en: '![](Images/fdl2_0922.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0922.png)'
- en: Figure 9-22\. Composing LSTM units as one might stack recurrent layers in a
    neural network
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-22\. 就像我们可以堆叠循环层一样，组合LSTM单元
- en: Now that we have overcome the issue of vanishing gradients and understand the
    inner workings of LSTM units, we’re ready to dive into the implementation of our
    first RNN models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经克服了梯度消失的问题，并了解了LSTM单元的内部工作原理，我们准备深入研究我们的第一个RNN模型的实现。
- en: PyTorch Primitives for RNN Models
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch用于RNN模型的原语
- en: 'PyTorch provides seceral primitives that we can use out of the box in order
    to build RNN models. First, we have`torch.nn.RNNCell` objects that represent either
    an RNN layer or an LSTM unit:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了几个原语，我们可以直接使用它们来构建RNN模型。首先，我们有代表RNN层或LSTM单元的`torch.nn.RNNCell`对象：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `RNNCell` abstraction represents a vanilla recurrent neuron layer, while
    the `LSTMCell` represents an implementation of the LSTM unit. PyTorch also includes
    a variation of the LSTM unit known as the *Gated Recurrent Unit* (GRU), proposed
    in 2014 by Yoshua Bengio’s group. The critical initialization variable for all
    of these cells is the size of the hidden state vector or `hidden_size`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`RNNCell`抽象表示普通的循环神经元层，而`LSTMCell`表示LSTM单元的实现。PyTorch还包括一种称为*门控循环单元*（GRU）的LSTM单元变体，由Yoshua
    Bengio的团队于2014年提出。所有这些单元的关键初始化变量是隐藏状态向量的大小或`hidden_size`。'
- en: 'In addition to the primitives, PyTorch provides multilayer RNN and LSTM classes
    for stacking layers. If we want to stack recurrent units or layers, we can use
    the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原语，PyTorch还提供了用于堆叠层的多层RNN和LSTM类。如果我们想要堆叠循环单元或层，我们可以使用以下内容：
- en: '[PRE10]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can also use the `dropout` parameter to apply dropout to the inputs and
    outputs of an LSTM with specified keep probability. If the `dropout` parameter
    is nonzero, the model introduces a dropout layer on the outputs of each LSTM layer
    except the last layer, with dropout probability equal to `dropout`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`dropout`参数来对LSTM的输入和输出应用指定保留概率的dropout。如果`dropout`参数不为零，则模型会在每个LSTM层的输出上引入一个dropout层，除了最后一层，dropout概率等于`dropout`：
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As shown here, the multilayer RNN and LSTM classes also provide a `batch_first`
    parameter. If `batch_first` equals `True`, then the input and output tensors are
    provided as `(batch, seq, feature)` instead of `(seq, batch, feature)`. Note that
    this does not apply to hidden or cell states. The default value of `batch_first`
    is `False`. See the PyTorch documentation for details.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，多层RNN和LSTM类还提供了一个`batch_first`参数。如果`batch_first`等于`True`，那么输入和输出张量将以`(batch,
    seq, feature)`的形式提供，而不是`(seq, batch, feature)`。请注意，这不适用于隐藏状态或单元状态。`batch_first`的默认值为`False`。有关详细信息，请参阅PyTorch文档。
- en: 'Finally, we instantiate an RNN by calling the PyTorch LSTM constructor:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过调用PyTorch LSTM构造函数来实例化一个RNN：
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The result of calling `rnn` is a tensor representing the outputs of the RNN,
    `output_n`, along with the final state vectors for each layer. The first tensor,
    `hn`, contains the hidden state vectors for each layer that holds the outputs
    of the Output Gates at time, `n`. The second tensor, `cn`, contains the state
    vectors for the memory cells of each layer, which is the output of the write gates.
    Both `hn` and `cn` are of size `(n_layers, batch_size, hidden_size)`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`rnn`的结果是表示RNN输出的张量`output_n`，以及每一层的最终状态向量。第一个张量`hn`包含每一层的隐藏状态向量，保存了时间`n`时刻的输出门的输出。第二个张量`cn`包含每一层的记忆单元的状态向量，即写门的输出。`hn`和`cn`的大小均为`(n_layers,
    batch_size, hidden_size)`。
- en: Now that we have an understanding of the tools at our disposal in constructing
    RNNs in PyTorch, we’ll build our first LSTM in the next section, focused on the
    task of sentiment analysis.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了在PyTorch中构建RNN的工具，接下来我们将在下一节中构建我们的第一个LSTM，重点是情感分析任务。
- en: Implementing a Sentiment Analysis Model
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现情感分析模型
- en: In this section, we attempt to analyze the sentiment of movie reviews taken
    from the Large Movie Review Dataset. This dataset consists of 50,000 reviews from
    IMDb, each of which is labeled as having positive or negative sentiment. We use
    a simple LSTM model leveraging dropout to learn how to classify the sentiment
    of movie reviews. The LSTM model will consume the movie review one word at a time.
    Once it has consumed the entire review, we’ll use its output as the basis of a
    binary classification to map the sentiment to be “positive” or “negative.”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们尝试分析来自大型电影评论数据集的电影评论的情感。该数据集包含来自IMDb的50,000条评论，每条评论都被标记为积极或消极情感。我们使用一个简单的LSTM模型利用辍学来学习如何对电影评论的情感进行分类。LSTM模型将逐个单词消耗电影评论。一旦它消耗完整个评论，我们将使用其输出作为二元分类的基础，将情感映射为“积极”或“消极”。
- en: 'Let’s start off by loading the dataset with the PyTorch library Torchtext,
    which comes preinstalled with Google Colab. If you’re running on another machine,
    you can install Torchtext by running the following command:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从PyTorch库Torchtext开始加载数据集，Torchtext已经预装在Google Colab中。如果您在另一台机器上运行，可以通过运行以下命令来安装Torchtext：
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once we’ve installed the package, we can download the dataset and define a tokenizer.
    Torchtext provides many natural language processing (NLP) datasets and tokenizers
    through the `torchtext.datasets` and `torchtext.data.utils` submodules, respectively.
    We’ll use the built-in IMDb dataset and standard `'basic_english'` tokenizer provided
    by PyTorch.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完包后，我们可以下载数据集并定义一个分词器。Torchtext通过`torchtext.datasets`和`torchtext.data.utils`子模块提供了许多自然语言处理（NLP）数据集和分词器。我们将使用内置的IMDb数据集和PyTorch提供的标准`'basic_english'`分词器。
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Until now, we’ve been using map-style datasets from PyTorch. Torchtext returns
    NLP datasets as iterable-style datasets, which are more appropriate for streaming
    data. Next, we need to create a vocabulary based on the training dataset and prune
    the vocabulary to include only the 30,000 most common words. Then, we need to
    pad each input sequence up to a length of 500 words, and process the labels.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用PyTorch的映射式数据集。Torchtext将NLP数据集返回为可迭代式数据集，这对于流式数据更为合适。接下来，我们需要基于训练数据集创建一个词汇表，并修剪词汇表，只包括最常见的30,000个单词。然后，我们需要将每个输入序列填充到500个单词的长度，并处理标签。
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As shown, Torchtext provides a function, `build_vocab_from_iterator`, to create
    a vocabulary. However, this function expects a list of tokens as input, where
    `next(train_iter)`would return a tuple `(label_string, review_string)`. To satisfy
    this requirement, we define a function to yield tokens as the dataset is iterated.
    Finally, we add special tokens for unknown and padding, and set the default.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，Torchtext提供了一个函数`build_vocab_from_iterator`来创建词汇表。但是，该函数期望以标记列表作为输入，其中`next(train_iter)`会返回一个元组`(label_string,
    review_string)`。为了满足这一要求，我们定义了一个函数，在数据集被迭代时产生标记。最后，我们添加了未知和填充的特殊标记，并设置了默认值。
- en: 'Next, we need to actually prune the vocabulary and pad the review sequences,
    as well as convert the label strings, `''neg''` or `''pos''`, to numbers. We accomplish
    this by defining a pipeline function for both the labels and review strings:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要实际修剪词汇表并填充评论序列，以及将标签字符串`'neg'`或`'pos'`转换为数字。我们通过为标签和评论字符串定义一个流水线函数来实现这一点：
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `text_pipeline` function converts the inputs to 500-dimensional vectors.
    Each vector corresponds to a movie review where the <math alttext="i Superscript
    t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math> component
    of the vector corresponds to the index of the  <math alttext="i Superscript t
    h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>  word of the
    review in our global dictionary of 30,000 words. To complete the data preparation,
    we create a special Python class designed to serve minibatches of a desired size
    from the underlying dataset.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`text_pipeline`函数将输入转换为500维向量。每个向量对应一个电影评论，其中向量的第i^th个分量对应于评论中第i^th个单词在我们的全局词典中的索引，该词典包含30,000个单词。为了完成数据准备，我们创建了一个特殊的Python类，用于从底层数据集中提供所需大小的小批量数据。'
- en: 'We can use the built-in `DataLoader` class from PyTorch to sample the dataset
    in batches. Before we do so, we need to define a function, `collate_batch`, that
    will tell the `DataLoader` how to preprocess each batch:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用PyTorch中内置的`DataLoader`类来对数据集进行批处理采样。在这样做之前，我们需要定义一个函数`collate_batch`，告诉`DataLoader`如何预处理每个批次：
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `collate_batch` function simply runs the labels and review strings through
    each respective pipeline and returns the batch as a tuple of tensors `(labels_batch,
    reviews_batch)`. Once the `collate_fn` is defined, we simply load the dataset
    using the IMDb constructor, and configure the dataloaders using the `DataLoader`
    constructor:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`collate_batch`函数只是通过每个相应的管道运行标签和评论字符串，并将批处理作为张量元组`(labels_batch, reviews_batch)`返回。一旦定义了`collate_fn`，我们只需使用IMDb构造函数加载数据集，并使用`DataLoader`构造函数配置数据加载器：'
- en: '[PRE18]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We use the `torchtext.datasets.IMDB` Python class to serve both the training
    and validation sets we’ll use while training our sentiment analysis model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`torchtext.datasets.IMDB` Python类来为我们在训练情感分析模型时使用的训练和验证集提供服务。
- en: Now that the data is ready to go, we’ll begin to construct the sentiment analysis
    model, step by step. First, we’ll want to map each word in the input review to
    a word vector. To do this, we’ll utilize an embedding layer, which, as you may
    recall from [Chapter 8](ch08.xhtml#embedding_and_representing_learning), is a
    simple lookup table that stores an embedding vector that corresponds to each word.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经准备就绪，我们将逐步构建情感分析模型。首先，我们将希望将输入评论中的每个单词映射到一个单词向量。为此，我们将利用一个嵌入层，正如您可能从[第8章](ch08.xhtml#embedding_and_representing_learning)中回忆的那样，这是一个简单的查找表，存储与每个单词对应的嵌入向量。
- en: 'Unlike in previous examples, where we treated the learning of the word embeddings
    as a separate problem (i.e., by building a Skip-Gram model), we’ll learn the word
    embeddings jointly with the sentiment analysis problem by treating the embedding
    matrix as a matrix of parameters in the full problem. We accomplish this by using
    the PyTorch primitives for managing embeddings (remember that `input` represents
    one full minibatch at a time, not just one movie review vector):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的例子不同，我们将学习词嵌入作为一个单独的问题（即通过构建Skip-Gram模型），我们将通过将嵌入矩阵视为完整问题中的参数矩阵来同时学习词嵌入和情感分析问题。我们通过使用PyTorch原语来管理嵌入来实现这一点（请记住，`input`代表一次完整的小批量，而不仅仅是一个电影评论向量）：
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We then take the result of the embedding layer and build an LSTM with dropout
    using the primitives we saw in the previous section. The implementation of the
    LSTM can be achieved as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将嵌入层的结果传递给使用我们在前一节中看到的原语构建带有丢失的LSTM。LSTM的实现可以如下实现：
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We top it all off using a batch-normalized hidden layer, identical to the ones
    we’ve used time and time again in previous examples. Stringing all of these components
    together, we can build the model by calling `TextClassifier`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后使用一个批归一化的隐藏层，与我们在以前的例子中一次又一次使用的隐藏层相同。将所有这些组件串联在一起，我们可以通过调用`TextClassifier`来构建模型：
- en: '[PRE21]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We omit the other boilerplate involved in setting up summary statistics, saving
    intermediate snapshots, and creating the session because it’s identical to the
    other models we’ve built in this book (see the [GitHub repository](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book)).
    We can then run and visualize the performance of our model using TensorBoard ([Figure 9-23](#fig0723)).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们省略了设置摘要统计信息、保存中间快照和创建会话所涉及的其他样板，因为它与本书中构建的其他模型相同（请参阅[GitHub存储库](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book)）。然后，我们可以使用TensorBoard运行和可视化我们模型的性能（参见[图9-23](#fig0723)）。
- en: '![](Images/fdl2_0923.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0923.png)'
- en: Figure 9-23\. Training cost, validation cost, and accuracy of our movie review
    sentiment model
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-23。我们电影评论情感模型的训练成本、验证成本和准确率
- en: At the beginning of training, the model struggles slightly with stability, and
    toward the end of the training, the model clearly starts to overfit as training
    cost and validation cost significantly diverge. At its optimal performance, however,
    the model performs rather effectively and generalizes to approximately 86% accuracy
    on the test set. Congratulations! You’ve built your first RNN.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始时，模型稍微稳定性不佳，而在训练结束时，模型明显开始过拟合，训练成本和验证成本明显分歧。然而，在其最佳性能时，模型表现相当有效，并且在测试集上大约达到86%的准确率。恭喜！您已经构建了您的第一个RNN。
- en: Solving seq2seq Tasks with Recurrent Neural Networks
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用循环神经网络解决seq2seq任务
- en: 'Now that we’ve built a strong understanding of RNNs, we’re ready to revisit
    the problem of seq2seq. We started off this chapter with an example of a seq2seq
    task: mapping a sequence of words in a sentence to a sequence of POS tags. Tackling
    this problem was tractable because we didn’t need to take into account long-term
    dependencies to generate the appropriate tags. But there are several seq2seq problems,
    such as translating between languages or creating a summary for a video, where
    long-term dependencies are crucial to the success of the model. This is where
    RNNs come in.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了对RNN的深入理解，我们准备重新审视seq2seq问题。我们在本章开始时以一个seq2seq任务的示例开始：将句子中的单词序列映射到POS标签序列。解决这个问题是可行的，因为我们不需要考虑长期依赖性来生成适当的标签。但是有几个seq2seq问题，例如语言之间的翻译或为视频创建摘要，其中长期依赖性对模型的成功至关重要。这就是RNN的用武之地。
- en: The RNN approach to seq2seq looks a lot like the autoencoder we discussed in
    the previous chapter. The seq2seq model is composed of two separate networks.
    The first network is known as the *encoder* network. The encoder network is a
    recurrent network (usually one that uses LSTM units) that consumes the entire
    input sequence. The goal of the encoder network is to generate a condensed understanding
    of the input and summarize it into a singular thought represented by the final
    state of the encoder network. Then we use a *decoder* network, whose starting
    state is initialized with the final state of the encoder network, to produce the
    target output sequence token by token. At each step, the decoder network consumes
    its own output from the previous time step as the current time step’s input. The
    entire process is visualized in [Figure 9-24](#encoder_decoder_recurrent_network_schema).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq的RNN方法看起来很像我们在上一章中讨论的自动编码器。seq2seq模型由两个独立的网络组成。第一个网络称为*编码器*网络。编码器网络是一个循环网络（通常使用LSTM单元），它消耗整个输入序列。编码器网络的目标是生成对输入的简洁理解，并将其总结为由编码器网络的最终状态表示的一个单一思想。然后我们使用一个*解码器*网络，其起始状态由编码器网络的最终状态初始化，逐个标记地生成目标输出序列。在每一步中，解码器网络将其上一个时间步的输出作为当前时间步的输入。整个过程在[图9-24](#encoder_decoder_recurrent_network_schema)中可视化。
- en: '![](Images/fdl2_0924.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/fdl2_0924.png)
- en: Figure 9-24\. How we use an encoder/decoder recurrent network schema to tackle
    seq2seq problems
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-24。我们如何使用编码器/解码器循环网络架构来解决seq2seq问题
- en: In this this setup, we are attempting to translate an English sentence into
    French. We tokenize the input sentence and use an embedding (similar to our approach
    in the sentiment analysis model we built in the previous section), one word at
    a time as an input to the encoder network. At the end of the sentence, we use
    a special “end-of-sequence” (EOS) token to indicate the end of the input sequence
    to the encoder network. Then we take the hidden state of the encoder network and
    use that as the initialization of the decoder network. The first input to the
    decoder network is the EOS token, and the output is interpreted as the first word
    of the predicted French translation. From that point onward, we use the output
    of the decoder network as the input to itself at the next time step. We continue
    until the decoder network emits an EOS token as its output, at which point we
    know that the network has completed producing the translation of the original
    English sentence. We’ll dissect the practical, open source implementation of this
    network (with a couple of enhancements and tricks to improve accuracy) later in
    this chapter.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，我们试图将一句英语句子翻译成法语。我们对输入句子进行标记，并使用一个嵌入（类似于我们在前一节构建的情感分析模型中的方法），逐个单词作为编码器网络的输入。在句子结束时，我们使用一个特殊的“序列结束”（EOS）标记来指示输入序列的结束。然后我们取编码器网络的隐藏状态，并将其用作解码器网络的初始化。解码器网络的第一个输入是EOS标记，输出被解释为预测的法语翻译的第一个单词。从那时起，我们使用解码器网络的输出作为下一个时间步的输入。直到解码器网络发出EOS标记作为其输出，此时我们知道网络已经完成了对原始英语句子的翻译。我们将在本章后面剖析这个网络的实际开源实现（通过一些增强和技巧来提高准确性）。
- en: The seq2seq RNN architecture can also be reappropriated for the purpose of learning
    good embeddings of sequences. For example, Kiros et al. in 2015 invented the notion
    of a *skip-thought vector*,^([5](ch09.xhtml#idm45934164517408)) which borrowed
    architectural characteristics from both the autoencoder framework and the Skip-Gram
    model discussed in [Chapter 8](ch08.xhtml#embedding_and_representing_learning).
    The skip-thought vector was generated by dividing a passage into a set of triplets
    consisting of consecutive sentences. The authors utilized a single encoder network
    and two decoder networks, as shown in [Figure 9-25](#skip_thought_seq2seq_architecture).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq RNN架构也可以重新用于学习序列的良好嵌入。例如，2015年Kiros等人发明了*skip-thought向量*的概念，借鉴了自动编码器框架和第8章中讨论的Skip-Gram模型的结构特征。skip-thought向量通过将一段话分成一组由连续句子组成的三元组来生成。作者使用了一个编码器网络和两个解码器网络，如[图9-25](#skip_thought_seq2seq_architecture)所示。
- en: '![](Images/fdl2_0925.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/fdl2_0925.png)
- en: Figure 9-25\. The skip-thought seq2seq architecture to generate embedding representations
    of entire sentences
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-25。skip-thought seq2seq架构用于生成整个句子的嵌入表示
- en: The encoder network consumed the sentence for which we wanted to generate a
    condensed representation (which was stored in the final hidden state of the encoder
    network). Then came the decoding step. The first of the decoder networks would
    take that representation as the initialization of its own hidden state and attempt
    to reconstruct the sentence that appeared prior to the input sentence. The second
    decoder network would instead attempt the sentence that appeared immediately after
    the input sentence. The full system was trained end to end on these triplets,
    and once completed, could be used to generate seemingly cohesive passages of text
    in addition to improve performance on key sentence-level classification tasks.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器网络消耗了我们想要生成简洁表示的句子（存储在编码器网络的最终隐藏状态中）。然后是解码步骤。第一个解码器网络将以该表示作为其自己隐藏状态的初始化，并尝试重建出出现在输入句子之前的句子。第二个解码器网络将尝试出现在输入句子之后的句子。整个系统在这些三元组上端到端地进行训练，一旦完成，就可以用来生成看似连贯的文本段落，同时提高关键句子级分类任务的性能。
- en: 'Here’s an example of story generation, excerpted from the original paper:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从原始论文中摘录的故事生成示例：
- en: '[PRE22]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that we’ve developed an understanding of how to leverage RNNs to tackle
    seq2seq problems, we’re almost ready to try to build our own. Before we get there,
    however, we’ve got one more major challenge to tackle, and we’ll address it head-on
    in the next section when we discuss the concept of attentions in seq2seq RNNs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何利用RNN来解决seq2seq问题，我们几乎准备好尝试构建自己的模型了。然而，在那之前，我们还有一个重要的挑战要解决，在下一节中我们将直面讨论seq2seq
    RNN中的注意力概念。
- en: Augmenting Recurrent Networks with Attention
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用注意力增强循环网络
- en: Let’s think harder about the translation problem. If you’ve ever attempted to
    learn a foreign language, you’ll know that there are several helpful steps when
    trying to complete a translation. First, it’s helpful to read the full sentence
    to understand the concept you would like to convey. Then you write out the translation
    one word at a time, each word following logically from the word you wrote previously.
    But one important aspect of translation is that as you compose the new sentence,
    you often refer back to the original text, focusing on specific parts that are
    relevant to your current translation. At each step, you are paying attention to
    the most relevant parts of the original “input” so you can make the best decision
    about the next word to put on the page.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地思考翻译问题。如果你曾尝试学习一门外语，你会知道在完成翻译时有几个有用的步骤。首先，阅读完整的句子以理解你想要传达的概念是有帮助的。然后，逐字逐句地写出翻译，每个词都逻辑地跟在你之前写的词后面。但翻译的一个重要方面是，当你构成新句子时，你经常会参考原始文本，关注与当前翻译相关的特定部分。在每一步中，你都在关注原始“输入”的最相关部分，以便能够做出关于下一个要写在页面上的词的最佳决定。
- en: Recall our approach to seq2seq. By consuming the full input and summarizing
    it into a “thought” inside its hidden state, the encoder network effectively achieves
    the first part of the translation process. By using the previous output as its
    current input, the decoder network achieves the second part of the translation
    process. This phenomenon of *attention* has yet to be captured by our approach
    to seq2seq, and this is the final building block we’ll need to engineer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们对seq2seq的方法。通过消耗完整的输入并将其总结为“思考”隐藏在其隐藏状态中，编码器网络有效地实现了翻译过程的第一部分。通过使用先前的输出作为当前输入，解码器网络实现了翻译过程的第二部分。然而，我们的seq2seq方法尚未捕捉到*注意力*这一现象，这是我们需要工程化的最后一个构建块。
- en: Currently, the sole input to the decoder network at a given time step *t* is
    its output at time step  <math alttext="t minus 1"><mrow><mi>t</mi> <mo>-</mo>
    <mn>1</mn></mrow></math> . One way to give the decoder network some vision into
    the original sentence is by giving the decoder access to all of the outputs from
    the encoder network (which we previously had completely ignored). These outputs
    are interesting to us because they represent how the encoder network’s internal
    state evolves after seeing each new token. A proposed implementation of this strategy
    is shown in [Figure 9-26](#attempt_at_engineering_attentional_abilities). This
    attempt falls short because it fails to dynamically select the most relevant parts
    of the input to focus on.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，在给定时间步骤*t*，解码器网络的唯一输入是在时间步骤<t-1>的输出。给解码器网络一些关于原始句子的视觉信息的一种方法是让解码器访问编码器网络的所有输出（我们之前完全忽略了）。这些输出对我们很有趣，因为它们代表编码器网络在看到每个新标记后内部状态的演变。这种策略的一个提议实现如[图9-26](#attempt_at_engineering_attentional_abilities)所示。这个尝试失败了，因为它未能动态选择要关注的输入的最相关部分。
- en: '![](Images/fdl2_0926.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0926.png)'
- en: Figure 9-26\. An attempt at engineering attentional abilities in a seq2seq architecture
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-26。在seq2seq架构中尝试工程化注意力能力
- en: This approach has a critical flaw, however. The problem here is that at every
    time step, the decoder considers all of the outputs of the encoder network in
    the exact same way. However, this is clearly not the case for a human during the
    translation process. We focus on different aspects of the original text when working
    on different parts of the translation. The key realization here is that it’s not
    enough to merely give the decoder access to all the outputs. Instead, we must
    engineer a mechanism by which the decoder network can dynamically pay attention
    to a specific subset of the encoder’s outputs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法存在一个关键缺陷。问题在于，在每个时间步骤，解码器以完全相同的方式考虑编码器网络的所有输出。然而，在翻译过程中，人类显然不是这样做的。在翻译不同部分时，我们会关注原始文本的不同方面。关键的认识在于，仅仅让解码器访问所有输出是不够的。相反，我们必须设计一种机制，使解码器网络能够动态关注编码器输出的特定子集。
- en: We can fix this problem by changing the inputs to the concatenation operation,
    using the proposal in Bahdanau et al. 2015 as inspiration.^([6](ch09.xhtml#idm45934164490960))
    Instead of directly using the raw outputs from the encoder network, we perform
    a weighting operation on the encoder’s outputs. We leverage the decoder network’s
    state at time  <math alttext="t minus 1"><mrow><mi>t</mi> <mo>-</mo> <mn>1</mn></mrow></math>
     as the basis for the weighting operation.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过改变连接操作的输入来解决这个问题，使用Bahdanau等人2015年的提议作为灵感。而不是直接使用编码器网络的原始输出，我们对编码器的输出进行加权操作。我们利用解码器网络在时间<t-1>的状态作为加权操作的基础。
- en: The weighting operation is illustrated in [Figure 9-27](#modification_to_our_original_proposal).
    First we create a scalar (a single number, not a tensor) relevance score for each
    of the encoder’s outputs. The score is generated by computing the dot product
    between each encoder output and the decoder’s state at time  <math alttext="t
    minus 1"><mrow><mi>t</mi> <mo>-</mo> <mn>1</mn></mrow></math> . We then normalize
    these scores using a softmax operation. Finally, we use these normalized scores
    to individually scale the encoder’s outputs before plugging them into the concatenation
    operation. The key here is that the relative scores computed for each encoder
    output signify how important that particular encoder output is to the decision
    for the decoder at time step *t*. In fact, as we’ll see later, we can visualize
    which parts of the input are most relevant to the translation at each time step
    by inspecting the output of the softmax.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 权重操作在[图9-27](#modification_to_our_original_proposal)中进行了说明。首先，我们为编码器的每个输出创建一个标量（一个单一的数字，而不是张量）相关性分数。该分数是通过计算每个编码器输出与时间<t减1>的解码器状态之间的点积来生成的。然后，我们使用softmax操作对这些分数进行归一化。最后，我们使用这些归一化的分数来分别缩放编码器的输出，然后将它们插入连接操作中。关键在于，为每个编码器输出计算的相对分数表示该特定编码器输出对于解码器在时间步*t*上的决策有多重要。事实上，正如我们将在后面看到的，我们可以通过检查softmax的输出来可视化哪些输入部分对于每个时间步的翻译最为相关。
- en: '![](Images/fdl2_0927.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0927.png)'
- en: Figure 9-27\. A modification to our original proposal that enables a dynamic
    attentional mechanism based on the hidden state of the decoder network in the
    previous time step
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-27. 根据上一个时间步的解码器网络的隐藏状态实现的动态注意力机制的我们原始提议的修改
- en: Armed with this strategy for engineering attention into seq2seq architectures,
    we’re finally ready to get our hands dirty with an RNN model for translating English
    sentences into French. But before we jump in, it’s worth noting that attentions
    are incredibly applicable in problems that extend beyond language translation.
    Attentions can be important in speech-to-text problems, where the algorithm learns
    to dynamically pay attention to corresponding parts of the audio while transcribing
    the audio into text. Similarly, attentions can be used to improve image captioning
    algorithms by helping the captioning algorithm focus on specific parts of the
    input image while writing out the caption. Anytime particular parts of the input
    are highly correlated to correctly producing corresponding segments of the output,
    attentions can dramatically improve performance.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握了将注意力引入seq2seq架构的策略后，我们终于准备好使用RNN模型将英语句子翻译成法语。但在我们开始之前，值得注意的是，注意力在超越语言翻译的问题中非常适用。在语音转文本问题中，注意力可以帮助算法动态关注音频的相应部分，同时将音频转录为文本。同样，注意力可以用于改进图像字幕算法，帮助字幕算法在撰写字幕时专注于输入图像的特定部分。每当输入的特定部分与正确生成相应输出的部分高度相关时，注意力都可以显著提高性能。
- en: Dissecting a Neural Translation Network
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 剖析神经翻译网络
- en: State-of-the-art neural translation networks use a number of different techniques
    and advancements that build on the basic seq2seq encoder-decoder architecture.
    Attention, as detailed in the previous section, is an important and critical architectural
    improvement. In this section, we will dissect a fully implemented neural machine
    translation system, complete with the data processing steps, building the model,
    training it, and eventually using it as a translation system to convert English
    phrases to French phrases.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的神经翻译网络使用了许多不同的技术和进展，这些技术和进展建立在基本的seq2seq编码器-解码器架构之上。注意力机制，如前一节所详细介绍的，是一个重要且关键的架构改进。在本节中，我们将剖析一个完全实现的神经机器翻译系统，包括数据处理步骤，构建模型，训练模型，最终将其用作翻译系统，将英语短语转换为法语短语。
- en: 'The pipeline used in training and eventually using a neural machine translation
    system is similar to that of most machine learning pipelines: gather data, prepare
    the data, construct the model, train the model, evaluate the model’s progress,
    and eventually use the trained model to predict or infer something useful. We
    review each of these steps here.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和最终使用神经机器翻译系统的流程与大多数机器学习流程类似：收集数据，准备数据，构建模型，训练模型，评估模型的进展，最终使用训练好的模型来预测或推断一些有用的东西。我们在这里回顾每个步骤。
- en: We first gather the data from the [International Workshop on Spoken Language
    Translation (IWSLT2016) repository](https://wit3.fbk.eu/2016-01), which houses
    large corpora used in training translation systems. For our use case, we’ll be
    using the English-to-French data. Note that if we want to be able to translate
    to or from different languages, we would have to train a model from scratch with
    the new data. We then preprocess our data into a format that is easily usable
    by our models during training and inference time. This will involve some amount
    of cleaning and tokenizing the sentences in each of the English and French phrases.
    What follows now is a set of techniques used in preparing the data, and later
    we will present the implementations of the techniques.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从[国际口语翻译研讨会（IWSLT2016）存储库](https://wit3.fbk.eu/2016-01)中收集数据，该存储库包含用于训练翻译系统的大型语料库。对于我们的用例，我们将使用英语到法语的数据。请注意，如果我们想要能够翻译不同语言，我们将不得不使用新数据从头开始训练模型。然后，我们将数据预处理为在训练和推断时我们的模型可以轻松使用的格式。这将涉及对英语和法语短语中的句子进行一定程度的清理和标记化。接下来是一系列用于准备数据的技术，稍后我们将介绍这些技术的实现。
- en: The first step is to parse sentences and phrases into formats that are more
    compatible with the model by *tokenization*. This is the process by which we discretize
    a particular English or French sentence into its constituent tokens. For instance,
    a simple word-level tokenizer will consume the sentence “I read.” to produce the
    array ["I”, “read”, “."], or it would consume the French sentence “Je lis.” to
    produce the array ["Je”, “lis”, “."].
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是通过*标记化*将句子和短语解析为更适合模型的格式。这是将特定的英语或法语句子离散化为其组成标记的过程。例如，一个简单的单词级标记器将消耗句子“I
    read.”来生成数组["I", "read", "."]，或者它将消耗法语句子“Je lis.”来生成数组["Je", "lis", "."]。
- en: A character-level tokenizer may break the sentence into individual characters
    or into pairs of characters like ["I”, " “, “r”, “e”, “a”, “d”, “."] and ["I “,
    “re”, “ad”, “."], respectively. One kind of tokenization may work better than
    the other, and each has its pros and cons. For instance, a word-level tokenizer
    will ensure that the model produces words that are from some dictionary, but the
    size of the dictionary may be too large to efficiently choose from during decoding.
    This is in fact a known issue and something that we’ll address in the coming discussions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 字符级标记化可能会将句子分解为单个字符或成对字符，如["I", " ", "r", "e", "a", "d", "."]和["I ", "re", "ad",
    "."]。一种标记化方式可能比另一种更好，每种都有其优缺点。例如，单词级标记化将确保模型生成来自某个字典的单词，但字典的大小可能太大，以至于在解码过程中难以有效地选择。这实际上是一个已知的问题，我们将在接下来的讨论中解决。
- en: On the other hand, the decoder using a character-level tokenization may not
    produce intelligible outputs, but the total dictionary that the decoder must choose
    from is much smaller, as it is simply the set of all printable ASCII characters.
    In this tutorial, we use a word-level tokenization, but we encourage you to experiment
    with different tokenizations to observe the effects this has. It is worth noting
    that we must also add a special EOS character, to the end of all output sequences
    because we need to provide a definitive way for the decoder to indicate that it
    has reached the end of its decoding. We can’t use regular punctuation because
    we cannot assume that we are translating full sentences. Note that we do not need
    EOS characters in our source sequences because we are feeding these in preformatted
    and do not need an EOS character for ourselves to denote the end of our source
    sequence.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使用字符级标记化的解码器可能不会产生可理解的输出，但解码器必须选择的总字典要小得多，因为它只是所有可打印ASCII字符的集合。在本教程中，我们使用单词级标记化，但我们鼓励您尝试不同的标记化以观察其影响。值得注意的是，我们还必须在所有输出序列的末尾添加一个特殊的EOS字符，因为我们需要提供一种明确的方式让解码器指示它已经解码结束。我们不能使用常规标点，因为我们不能假设我们正在翻译完整的句子。请注意，我们在源序列中不需要EOS字符，因为我们正在预先格式化地喂入这些数据，不需要一个EOS字符来表示我们源序列的结束。
- en: The next optimization involves further modifying how we represent each source
    and target sequence, and we introduce a concept called *bucketing*. This is a
    method employed primarily in sequence-to-sequence tasks, especially machine translation,
    that helps the model efficiently handle sentences or phrases of different lengths.
    We first describe the naive method of feeding in training data and illustrate
    the shortcomings of this approach. Normally, when feeding in encoder and decoder
    tokens, the length of the source sequence and the target sequence is not always
    equal between pairs of examples. For example, the source sequence may have length
    *X*, and the target sequence may have length *Y*. It may seem that we need different
    seq2seq networks to accommodate each (*X, Y*) pair, yet this immediately seems
    wasteful and inefficient. Instead, we can do a little better if we *pad* each
    sequence up to a certain length, as shown in [Figure 9-28](#naive_strategy_for_padding_sequences),
    assuming we use a word-level tokenization and that we’ve appended EOS tokens to
    our target sequences.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个优化涉及进一步修改我们如何表示每个源序列和目标序列的方式，并引入了一个称为*桶装*的概念。这是一种主要用于序列到序列任务，特别是机器翻译的方法，可以帮助模型有效地处理不同长度的句子或短语。我们首先描述了喂入训练数据的朴素方法，并说明了这种方法的缺点。通常，在喂入编码器和解码器标记时，源序列和目标序列的长度在不同示例对之间并不总是相等。例如，源序列可能长度为*X*，目标序列可能长度为*Y*。看起来我们需要不同的seq2seq网络来适应每个(*X,
    Y*)对，但这似乎立即显得浪费和低效。相反，我们可以稍微改进，如果我们将每个序列*填充*到一定长度，如[图9-28](#naive_strategy_for_padding_sequences)所示，假设我们使用单词级的标记化，并且我们已经在目标序列中添加了EOS标记。
- en: '![](Images/fdl2_0928.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0928.png)'
- en: Figure 9-28\. Naive strategy for padding sequences
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-28。填充序列的朴素策略
- en: 'This step saves us the trouble of having to construct a different seq2seq model
    for each pair of source and target lengths. However, this introduces a different
    issue: if there were a very long sequence, it would mean that we would have to
    pad every other sequence *up to that length*. This would make a short sequence
    padded to the end take as much computational resources as a long one with few
    pad tokens, which is wasteful and could introduce a major performance hit to our
    model. We could consider breaking up every sentence in the corpus into phrases
    such that the length of each phrase does not exceed a certain maximum limit, but
    it’s not clear how to break the corresponding translations. This is where bucketing
    helps us.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步省去了为每对源和目标长度构建不同的seq2seq模型的麻烦。然而，这引入了另一个问题：如果有一个非常长的序列，那意味着我们必须将每个其他序列*填充到该长度*。这将使一个短序列填充到末尾所需的计算资源与一个带有少量填充标记的长序列一样多，这是浪费的，可能会给我们的模型带来重大性能损失。我们可以考虑将语料库中的每个句子分解为短语，使得每个短语的长度不超过某个最大限制，但如何分解相应的翻译并不清楚。这就是桶装如何帮助我们的地方。
- en: Bucketing is the idea that we can place encoder and decoder pairs into buckets
    of similar size, and only pad up to the maximum length of sequences in each respective
    bucket. For instance, we can denote a set of buckets, [(5, 10), (10, 15), (20,
    25), (30, 40)], where each tuple in the list is the maximum length of the source
    sequence and target sequence, respectively. Borrowing the preceding example, we
    can place the pair of sequences (["I”, “read”, “."], ["Je”, “lis”, “.”, “EOS"])
    in the first bucket, as the source sequence is smaller than 5 tokens and the target
    sequence is smaller than 10 tokens. We would then place the (["See”, “you”, “in”,
    “a”, “little”, “while"], ["A”, “tout”, “a”, “l’heure”, “EOS"]) in the second bucket,
    and so on. This technique allows us to compromise between the two extremes, where
    we need to pad only as much as necessary, as shown in [Figure 9-29](#padding_sequences_with_buckets).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 分桶是一个想法，我们可以将编码器和解码器对放入相似大小的桶中，并且只填充到每个桶中序列的最大长度。例如，我们可以表示一组桶，[(5, 10), (10,
    15), (20, 25), (30, 40)]，列表中的每个元组分别是源序列和目标序列的最大长度。借用前面的例子，我们可以将序列对(["I", "read",
    "."], ["Je", "lis", ".", "EOS"])放入第一个桶中，因为源序列小于5个标记，目标序列小于10个标记。然后我们将(["See",
    "you", "in", "a", "little", "while"], ["A", "tout", "a", "l’heure", "EOS"])放入第二个桶，依此类推。这种技术允许我们在两个极端之间取得折衷，只填充必要的部分，如[图9-29](#padding_sequences_with_buckets)所示。
- en: '![](Images/fdl2_0929.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0929.png)'
- en: Figure 9-29\. Padding sequences with buckets
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-29。使用桶填充序列
- en: Using bucketing shows a considerable speedup during training and test time,
    and allows developers and frameworks to write very optimized code to leverage
    the fact that any sequence from a bucket will have the same size and pack the
    data together in ways that allow even further GPU efficiency.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分桶在训练和测试时显示出显著的加速，并且允许开发人员和框架编写非常优化的代码，以利用任何来自桶的序列将具有相同的大小，并以一种允许进一步GPU效率的方式打包数据。
- en: 'With the sequences properly padded, we need to add one additional token to
    the target sequences: *a GO token*. This GO token will signal to the decoder that
    decoding needs to begin, at which point it will take over and begin decoding.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在正确填充序列后，我们需要为目标序列添加一个额外的标记：*一个GO标记*。这个GO标记将向解码器发出信号，表示解码需要开始，在这一点上，解码器将接管并开始解码。
- en: The last improvement we make in the data preparation side is to reverse the
    source sequences. Researchers found that doing so improved performance, and this
    has become a standard trick to try when training neural machine translation models.
    This is a bit of an engineering hack, but consider the fact that our fixed-size
    neural state can hold only so much information, and information encoded while
    processing the beginning of the sentence may be overwritten while encoding later
    parts of the sentence. In many language pairs, the beginning of sentences is harder
    to translate than the end of sentences, so this hack of reversing the sentence
    improves translation accuracy by giving the beginning of the sentence the last
    say on what final state is encoded. With these ideas in place, the final sequences
    look as they do in [Figure 9-30](#padding_scheme_reversing_the_inputs).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据准备方面我们做的最后一个改进是反转源序列。研究人员发现这样做可以提高性能，这已经成为训练神经机器翻译模型时尝试的标准技巧。这有点工程上的技巧，但考虑到我们的固定大小神经状态只能容纳有限信息，处理句子开头时编码的信息可能会在处理句子后部时被覆盖。在许多语言对中，句子开头比句子结尾更难翻译，因此通过反转句子的这种技巧可以通过让句子开头最后发言来提高翻译准确性。有了这些想法，最终的序列看起来如[图9-30](#padding_scheme_reversing_the_inputs)所示。
- en: '![](Images/fdl2_0930.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0930.png)'
- en: Figure 9-30\. Final padding scheme with buckets, reversing the inputs, and adding
    the GO token
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-30。最终的填充方案，使用桶，反转输入，并添加GO标记
- en: 'With these techniques described, we can now detail the implementation. First,
    we load the dataset, then we define our tokenizers and vocabularies. We do not
    define the word embeddings here, as we will train our model to compute them. PyTorch’s
    Torchtext library supports IWSLT2016 in `torch.text.datasets`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些描述的技术，我们现在可以详细说明实现。首先，我们加载数据集，然后定义我们的分词器和词汇表。我们不在这里定义词嵌入，因为我们将训练我们的模型来计算它们。PyTorch的Torchtext库支持`torch.text.datasets`中的IWSLT2016：
- en: '[PRE23]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The dataset constructor returns an iterable-style dataset that can retrieve
    English and French sentence pairs with `next(train_iter)`. We’ll use this iterable-style
    dataset to create bucketed datasets for batching later in our code.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集构造函数返回一个可迭代样式数据集，可以使用`next(train_iter)`检索英语和法语句子对。我们将使用这种可迭代样式数据集在代码中稍后创建分桶数据集以进行批处理。
- en: 'For now, let’s also define our tokenizers and vocabularies for each language.
    PyTorch offers a `get_tokenizer` function that operates on common tokenizers.
    Here, we’ll use the `spacy` tokenizer for each language. You may need to download
    the `spacy` language files first:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为每种语言定义分词器和词汇表。PyTorch提供了一个`get_tokenizer`函数，可用于常见的分词器。在这里，我们将为每种语言使用`spacy`分词器。您可能需要先下载`spacy`语言文件：
- en: '[PRE24]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once we have the language files, we can create the tokenizers as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了语言文件，我们可以按以下方式创建分词器：
- en: '[PRE25]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we will create our vocabularies for English and French using PyTorch’s
    `build_vocab_from_iterator` function. This function takes tokens from an iterable-style
    dataset from a single language and creates a vocabulary. Since our dataset has
    both English and French sentences, we create a `yield_tokens` function to return
    only the English or French tokens, and pass this into `build_vocab_from_iterator`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用PyTorch的`build_vocab_from_iterator`函数为英语和法语创建词汇表。该函数从单一语言的可迭代样式数据集中获取标记并创建词汇表。由于我们的数据集包含英语和法语句子，我们创建一个`yield_tokens`函数，仅返回英语或法语标记，并将其传递给`build_vocab_from_iterator`：
- en: '[PRE26]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Notice that we need to reload the `train_iter` before building the French vocabulary
    to restart the iterable-style dataset. We also add in the special tokens and their
    indices.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在构建法语词汇表之前，我们需要重新加载`train_iter`以重新启动可迭代样式数据集。我们还添加了特殊标记及其索引。
- en: 'Now that we have the dataset, tokenizers, and vocabularies, we need to create
    functions to preprocess the tokens and generate batches of bucketed data. First
    let’s define a `process_tokens` function to apply the improvements we discussed
    earlier:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据集、分词器和词汇表，我们需要创建函数来预处理标记并生成分桶数据的批次。首先让我们定义一个`process_tokens`函数来应用我们之前讨论的改进：
- en: '[PRE27]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this function, we pass in the variable size lists of source and target tokens,
    plus a list of bucket sizes. First, we decide on the smallest bucket size that
    will fit both the source and target token lists. Then, we process the source tokens
    by padding and reversing the sequence as described earlier. For the target tokens,
    we add a `<go>` token to the beginning and add an `<eos>` token to the end, then
    pad to the bucket size. When determining the smallest bucket size, we accounted
    for the two added tokens `<go>` and `<eos>`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们传入变量大小的源标记和目标标记列表，以及一个桶大小列表。首先，我们决定适合源标记和目标标记列表的最小桶大小。然后，我们通过填充和反转序列来处理源标记，如前面所述。对于目标标记，我们在开头添加一个`<go>`标记，并在末尾添加一个`<eos>`标记，然后填充到桶大小。在确定最小桶大小时，我们考虑了两个添加的标记`<go>`和`<eos>`。
- en: Now we have a function that takes lists of source and target tokens and prepares
    them appropriately. Next, we need to collect a single batch of data for our model
    and training loop. To do this, we will use the built-in PyTorch `Dataset` and
    `DataLoader` classes.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个函数，它接受源标记和目标标记的列表，并适当地准备它们。接下来，我们需要为我们的模型和训练循环收集一批数据。为此，我们将使用内置的PyTorch`Dataset`和`DataLoader`类。
- en: We are going to separate `Dataset` and `DataLoader` for each bucket size. This
    approach will enable us to use the built-in feature of the `DataLoader` for random
    batching and parallel processing.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为每个桶大小分开`Dataset`和`DataLoader`。这种方法将使我们能够利用`DataLoader`的内置功能进行随机分批和并行处理。
- en: 'First we create a `BucketedDataset` class by subclassing PyTorch’s `Dataset`
    class. Since this will be a map-style dataset, we’ll need to define the `__getitem__`
    and `__len__` methods for data access:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过对PyTorch的`Dataset`类进行子类化来创建一个`BucketedDataset`类。由于这将是一个映射样式的数据集，我们需要为数据访问定义`__getitem__`和`__len__`方法：
- en: '[PRE28]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We create a list of `BucketedDataset` objects in `bucketed_datasets`, one for
    each bucket size. The `BucketedDataset` constructor also converts our vocabulary
    integers to PyTorch tensors so we can pass them into our model later.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`bucketed_datasets`中创建了一个`BucketedDataset`对象的列表，每个桶大小一个。`BucketedDataset`构造函数还将我们的词汇整数转换为PyTorch张量，以便我们稍后可以将它们传递给我们的模型。
- en: 'Next we use the PyTorch’s `DataLoader` class to create dataloaders for each
    dataset in `bucketed_datasets`. Since we created `Dataset` objects, we get the
    batching capabilities of the `DataLoader` class without writing any additional
    code:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用PyTorch的`DataLoader`类为`bucketed_datasets`中的每个数据集创建数据加载器。由于我们创建了`Dataset`对象，我们可以在不编写任何额外代码的情况下获得`DataLoader`类的批处理能力：
- en: '[PRE29]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The dataloaders list hold a dataloader for each bucket size, so when we run
    our training or test loops, we will select a bucket size (randomly for training)
    and use the corresponding dataloader to pull a batch of encoders and decoder inputs:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载器列表保存了每个桶大小的数据加载器，因此当我们运行训练或测试循环时，我们将选择一个桶大小（对于训练是随机的），并使用相应的数据加载器来获取一批编码器和解码器输入：
- en: '[PRE30]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We measure the loss incurred during prediction time, as well as keep track
    of other running metrics:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测量预测时间产生的损失，同时跟踪其他运行指标：
- en: '[PRE31]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Lastly, every so often, as dictated by a global variable, we will carry out
    a number of tasks. First, we print statistics for the previous batch, such as
    the loss, the learning rate, and the perplexity. If we find that the loss is not
    decreasing, the model may have fallen into a local optima. To assist the model
    in escaping this, we anneal the learning rate so that it won’t make large leaps
    in any particular direction. At this point, we also save a copy of the model and
    its weights and activations to disk.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，根据全局变量的指示，我们会定期执行一些任务。首先，我们打印出先前批次的统计信息，如损失、学习率和困惑度。如果发现损失没有减少，模型可能已经陷入局部最优解。为了帮助模型摆脱这种情况，我们会降低学习率，以便它不会朝任何特定方向大幅跳跃。在这一点上，我们还会将模型及其权重和激活保存到磁盘上。
- en: This concludes the high-level details of training and using the models. We have
    largely abstracted away the fine details of the model itself. For more, see the
    [book’s repository](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了关于训练和使用模型的高层细节。我们已经大大抽象了模型本身的细节。更多内容，请参阅[书籍的存储库](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book)。
- en: With this, we’ve successfully completed a full tour of the implementation details
    of a fairly sophisticated neural machine translation system. Production systems
    have additional tricks that are not as generalizable, and these systems are trained
    on huge compute servers to ensure that state-of-the-art performance is met.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们成功完成了一个相当复杂的神经机器翻译系统实现细节的完整介绍。生产系统有一些额外的技巧，这些技巧不太具有一般性，并且这些系统是在巨大的计算服务器上训练的，以确保达到最先进的性能。
- en: For reference, this exact model was trained on eight NVIDIA Telsa M40 GPUs for
    four days. We show plots for the perplexity in Figures [9-31](#plot_of_perplexity_on_training_data)
    and [9-32](#plot_of_learning_rate_over_time), and show the learning rate anneal
    over time as well. In [Figure 9-31](#plot_of_perplexity_on_training_data), we
    see that after 50,000 epochs, the perplexity decreases from about 6 to 4, which
    is a reasonable score for a neural machine translation system. In [Figure 9-32](#plot_of_learning_rate_over_time),
    we observe that the learning rate almost smoothly declines to 0\. This means that
    by the time we stopped training, the model was approaching a stable state.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 参考资料，这个确切的模型在八个NVIDIA Telsa M40 GPU上训练了四天。我们在图[9-31](#plot_of_perplexity_on_training_data)和[9-32](#plot_of_learning_rate_over_time)中展示了困惑度的图表，并展示了随时间变化的学习率。在[图9-31](#plot_of_perplexity_on_training_data)中，我们看到在50,000个epochs之后，困惑度从大约6下降到4，这对于神经机器翻译系统来说是一个合理的分数。在[图9-32](#plot_of_learning_rate_over_time)中，我们观察到学习率几乎平稳地下降到0。这意味着在我们停止训练时，模型正在接近稳定状态。
- en: '![](Images/fdl2_0931.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0931.png)'
- en: Figure 9-31\. Plot of perplexity on training data over time
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-31. 随时间变化的训练数据困惑度图
- en: '![](Images/fdl2_0932.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0932.png)'
- en: Figure 9-32\. Plot of learning rate over time
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-32. 随时间变化的学习率图
- en: To showcase the attentional model more explicitly, we can visualize the attention
    that the decoder LSTM computes while translating a sentence from English to French.
    In particular, we know that as the encoder LSTM is updating its cell state in
    order to compress the sentence into continuous vector representations, it also
    computes hidden states at every time step. We know that the decoder LSTM computes
    a convex sum over these hidden states, and we can think of this sum as the attention
    mechanism; when there is more weight on a particular hidden state, we can interpret
    that as the model is paying more attention to the token inputted at that time
    step.This is exactly what we visualize in [Figure 9-33](#explicitly_viz_the_weights).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更明确地展示注意力模型，我们可以可视化解码器LSTM在将句子从英语翻译成法语时计算的注意力。特别是，我们知道当编码器LSTM更新其单元状态以将句子压缩为连续的向量表示时，它还会在每个时间步计算隐藏状态。我们知道解码器LSTM对这些隐藏状态进行凸组求和，我们可以将这个求和看作是注意力机制；当某个隐藏状态上有更多的权重时，我们可以将其解释为模型更多地关注在该时间步输入的标记上。这正是我们在[图9-33](#explicitly_viz_the_weights)中可视化的内容。
- en: The English sentence to be translated is on the top row, and the resulting French
    translation is on the first column. The lighter a square is, the more attention
    the decoder paid to that particular column when decoding that row element. That
    is, the *(i, j)^(th)* element in the attention map shows the amount of attention
    that was paid to the *j^(th)* token in the English sentence when translating the
    *i^(th)* token in the French sentence.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要翻译的英语句子在顶行，结果的法语翻译在第一列。方块颜色越浅，解码器在解码该行元素时对该特定列付出的注意力越多。也就是说，注意力图中的*(i, j)^(th)*元素显示了在将法语句子的*i^(th)*标记翻译为英语句子的*j^(th)*标记时，对英语句子的*j^(th)*标记付出的注意力量。
- en: We can immediately see that the attention mechanism seems to be working quite
    well. Large amounts of attention are generally being placed in the right areas,
    even though there is slight noise in the model’s prediction. It is possible that
    adding layers to the network would help produce crisper attention. One impressive
    aspect is that the phrase “the European Economic” is translated in reverse in
    French as the “zone économique européenne,” and as such, the attention weights
    reflect this flip. These kinds of attention patterns may be even more interesting
    when translating from English to a different language that does not parse smoothly
    from left to right.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即看到注意力机制似乎运作得相当不错。尽管模型的预测中存在轻微噪音，但通常会在正确的区域放置大量注意力。添加网络层可能有助于产生更清晰的注意力。一个令人印象深刻的方面是，“the
    European Economic”这个短语在法语中被翻译为“zone économique européenne”，因此注意力权重反映了这种翻转。当从英语翻译到不从左到右顺畅解析的不同语言时，这种注意力模式可能会更有趣。
- en: '![](Images/fdl2_0933.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0933.png)'
- en: Figure 9-33\. Visualizing the weights of the convex sum when the decoder attends
    over hidden states in the encoder
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-33. 当解码器关注编码器中的隐藏状态时，可视化凸组的权重
- en: With one of the most fundamental architectures understood and implemented, we
    now move forward to study exciting new developments with RNNs and begin a foray
    into more sophisticated learning.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 随着最基本的架构被理解和实现，我们现在将继续研究RNN的令人兴奋的新发展，并开始探索更复杂的学习。
- en: Self-Attention and Transformers
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自注意力和变压器
- en: Earlier, we discussed a form of attention that was first presented in Bahdanau
    et al. in 2015\. Specifically, we used a simple, feed-forward neural network to
    calculate the alignment score of each encoder hidden state with the decoder state
    at the current time step. In this section, we’ll discuss a different form of attention
    called *scaled dot product attention,* its use in *self-attention,* and the *transformer,*
    a recent language modeling breakthrough. Transformer-based models have primarily
    replaced LSTM, and have been proven to be superior in quality for many sequence-to-sequence
    problems.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们讨论了一种注意力形式，这种形式首次在2015年由Bahdanau等人提出。具体来说，我们使用一个简单的前馈神经网络来计算每个编码器隐藏状态与当前时间步的解码器状态的对齐分数。在本节中，我们将讨论一种称为*缩放点积注意力*的不同形式的注意力，它在*自注意力*中的使用，以及*变压器*，这是一种最近的语言建模突破。基于变压器的模型主要取代了LSTM，并已被证明在许多序列到序列问题中具有更高的质量。
- en: Dot product attention is really as simple as it sounds—this method calculates
    alignment scores as the dot product between each encoder hidden state  <math alttext="s
    Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math> . These weights are used
    in the calculation of the context vector, which is a convex sum (via softmax)
    of the encoder hidden states. Why use the dot product to measure alignment? As
    we learned in [Chapter 1](ch01.xhtml#fundamentals_of_linear_algebra_for_deep_learning),
    the dot product of two vectors can be expressed as a product of the norms of the
    two vectors and the cosine of the angle between them. As the angle between the
    two vectors goes to zero, the cosine goes to one. Also, recall from trigonometry
    that cosine has the range 1 to –1 when the input angle is between 0 degrees and
    180 degrees, which is the only part of the domain of the angle we need to consider.
    The dot product has the nice property that, as the angle between two vectors gets
    smaller, the dot product gets larger. This allows us to use the dot product as
    a natural measure of similarity.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 点积注意力实际上就像听起来的那样简单——这种方法计算编码器隐藏状态之间的点积作为对齐分数。这些权重用于计算上下文向量，这是编码器隐藏状态的凸组合（通过softmax）。为什么使用点积来衡量对齐？正如我们在[第1章](ch01.xhtml#fundamentals_of_linear_algebra_for_deep_learning)中学到的，两个向量的点积可以表示为两个向量的范数和它们之间夹角的余弦的乘积。当两个向量之间的夹角趋近于零时，余弦趋近于一。此外，从三角学中我们知道，当输入角度在0度到180度之间时，余弦的范围是1到-1，这是我们需要考虑的角度域的唯一部分。点积具有一个很好的性质，即当两个向量之间的夹角变小时，点积变大。这使我们能够将点积作为相似性的自然度量。
- en: 'In 2017, Vaswani et al.^([7](ch09.xhtml#idm45934165152352)) introduced a modification
    to the preexisting dot product attention framework via the inclusion of a scaling
    factor—the square root of the dimension of the hidden states. Vaswani et al. acknowledge
    the fact that, as hidden state representations get larger and larger in terms
    of dimension, we expect to see significantly more instances of high magnitude
    dot products. To understand the reasoning behind the inclusion of this scaling
    factor, assume, for the sake of argument, each index of  <math alttext="h Subscript
    i"><msub><mi>h</mi> <mi>i</mi></msub></math> is drawn independently and identically
    distributed from a mean zero, unit variance random variable. Let’s compute the
    expectation and variance of their dot product:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，Vaswani等人^([7](ch09.xhtml#idm45934165152352))通过引入一个缩放因子——隐藏状态维度的平方根，对现有的点积注意力框架进行了修改。Vaswani等人承认，随着隐藏状态表示在维度上变得越来越大，我们预计会看到更多高幅度的点积。为了理解包含这个缩放因子的原因，假设每个 <math
    alttext="h Subscript i"><msub><mi>h</mi> <mi>i</mi></msub></math> 的索引都是从均值为零、单位方差的随机变量中独立且相同地抽取的。让我们计算它们的点积的期望和方差：
- en: <math alttext="double-struck upper E left-bracket s Subscript t Superscript
    upper T Baseline h Subscript i Baseline right-bracket equals sigma-summation Underscript
    j equals 1 Overscript k Endscripts double-struck upper E left-bracket s Subscript
    t comma j Baseline asterisk h Subscript i comma j Baseline right-bracket"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msubsup><mi>s</mi> <mi>t</mi> <mi>T</mi></msubsup> <msub><mi>h</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket s Subscript t Superscript
    upper T Baseline h Subscript i Baseline right-bracket equals sigma-summation Underscript
    j equals 1 Overscript k Endscripts double-struck upper E left-bracket s Subscript
    t comma j Baseline asterisk h Subscript i comma j Baseline right-bracket"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msubsup><mi>s</mi> <mi>t</mi> <mi>T</mi></msubsup> <msub><mi>h</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow></mrow></math>
- en: = <math alttext="sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket s Subscript t comma j Baseline right-bracket
    double-struck upper E left-bracket h Subscript i comma j Baseline right-bracket"><mrow><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>]</mo></mrow>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: = <math alttext="sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket s Subscript t comma j Baseline right-bracket
    double-struck upper E left-bracket h Subscript i comma j Baseline right-bracket"><mrow><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>]</mo></mrow>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow></mrow></math>
- en: <math alttext="equals 0"><mrow><mo>=</mo> <mn>0</mn></mrow></math>
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals 0"><mrow><mo>=</mo> <mn>0</mn></mrow></math>
- en: <math alttext="upper V a r left-parenthesis s Subscript t Superscript upper
    T Baseline h Subscript i Baseline right-parenthesis equals sigma-summation Underscript
    j equals 1 Overscript k Endscripts upper V a r left-parenthesis s Subscript t
    comma j Baseline asterisk h Subscript i comma j Baseline right-parenthesis"><mrow><mi>V</mi>
    <mi>a</mi> <mi>r</mi> <mrow><mo>(</mo> <msubsup><mi>s</mi> <mi>t</mi> <mi>T</mi></msubsup>
    <msub><mi>h</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup> <mi>V</mi> <mi>a</mi>
    <mi>r</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper V a r left-parenthesis s Subscript t Superscript upper
    T Baseline h Subscript i Baseline right-parenthesis equals sigma-summation Underscript
    j equals 1 Overscript k Endscripts upper V a r left-parenthesis s Subscript t
    comma j Baseline asterisk h Subscript i comma j Baseline right-parenthesis"><mrow><mi>V</mi>
    <mi>a</mi> <mi>r</mi> <mrow><mo>(</mo> <msubsup><mi>s</mi> <mi>t</mi> <mi>T</mi></msubsup>
    <msub><mi>h</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup> <mi>V</mi> <mi>a</mi>
    <mi>r</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket left-parenthesis s Subscript t comma j Superscript
    2 Baseline asterisk h Subscript i comma j Superscript 2 Baseline right-parenthesis
    right-bracket minus double-struck upper E left-bracket s Subscript t comma j Baseline
    asterisk h Subscript i comma j Baseline right-bracket squared"><mrow><mo>=</mo>
    <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup>
    <mi>𝔼</mi> <mrow><mo>[</mo> <mrow><mo>(</mo> <msubsup><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>*</mo> <msubsup><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo> <mi>𝔼</mi>
    <msup><mrow><mo>[</mo><msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo><msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket left-parenthesis s Subscript t comma j Superscript
    2 Baseline asterisk h Subscript i comma j Superscript 2 Baseline right-parenthesis
    right-bracket minus double-struck upper E left-bracket s Subscript t comma j Baseline
    asterisk h Subscript i comma j Baseline right-bracket squared"><mrow><mo>=</mo>
    <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi></msubsup>
    <mi>𝔼</mi> <mrow><mo>[</mo> <mrow><mo>(</mo> <msubsup><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>*</mo> <msubsup><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo> <mi>𝔼</mi>
    <msup><mrow><mo>[</mo><msub><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>*</mo><msub><mi>h</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>]</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket s Subscript t comma j Superscript 2 Baseline
    right-bracket double-struck upper E left-bracket h Subscript i comma j Superscript
    2 Baseline right-bracket"><mrow><mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo> <msubsup><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>]</mo></mrow> <mi>𝔼</mi> <mrow><mo>[</mo> <msubsup><mi>h</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> <mn>2</mn></msubsup> <mo>]</mo></mrow></mrow></math>
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    double-struck upper E left-bracket s Subscript t comma j Superscript 2 Baseline
    right-bracket double-struck upper E left-bracket h Subscript i comma j Superscript
    2 Baseline right-bracket"><mrow><mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mi>𝔼</mi> <mrow><mo>[</mo> <msubsup><mi>s</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>]</mo></mrow> <mi>𝔼</mi> <mrow><mo>[</mo> <msubsup><mi>h</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> <mn>2</mn></msubsup> <mo>]</mo></mrow></mrow></math>
- en: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    1"><mrow><mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mn>1</mn></mrow></math>
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript j equals 1 Overscript k Endscripts
    1"><mrow><mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></msubsup> <mn>1</mn></mrow></math>
- en: <math alttext="equals k"><mrow><mo>=</mo> <mi>k</mi></mrow></math>
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals k"><mrow><mo>=</mo> <mi>k</mi></mrow></math>
- en: Let’s review the steps that got us to these conclusions regarding the expectation
    and variance. The first equality in the expectation is due to the linearity of
    expectation, since the dot product can be expressed as a sum of the product of
    each index. The second equality comes from the fact that the two random variables
    in each expectation are independent, so we can separate the expectation of the
    product into a product of expectations. The final step follows directly from the
    fact that each of these individual expectations are zero.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下导致我们对期望和方差得出这些结论的步骤。期望中的第一个等式是由于期望的线性性，因为点积可以表示为每个索引的乘积之和。第二个等式来自于每个期望中的两个随机变量是独立的这一事实，因此我们可以将乘积的期望分离为期望的乘积。最后一步直接来自于这些单独期望都为零的事实。
- en: The first equality in the variance is due to the linearity of variance when
    the individual terms are all independent. The second equality is just the definition
    of variance. The third equality uses a result from our calculation of the expectation
    of the dot product (we can separate out the square of the expectation into the
    product of squares of expectations, where each individual expectation is zero).
    Additionally, the expectation of the product of squares can be split up into a
    product of expectations of squares, since the square of each random variable is
    independent of the squares of all other random variables. The second to last equality
    comes from the fact that the expectation of the square of each random variable
    is just the variance of the random variable (since the expectation of each random
    variable is zero). The final equality follows directly.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 方差中的第一个等式是由于当各个项都是独立时方差的线性性。第二个等式只是方差的定义。第三个等式使用了我们计算点积期望的结果（我们可以将期望的平方分离为期望的平方的乘积，其中每个单独的期望都为零）。此外，平方的乘积的期望可以分解为平方的期望的乘积，因为每个随机变量的平方与所有其他随机变量的平方是独立的。倒数第二个等式来自于每个随机变量的平方的期望就是随机变量的方差（因为每个随机变量的期望为零）。最后一个等式直接得出。
- en: We see that the expectation of the dot product is zero, while its variance is
    k, the dimension of the hidden representation. Thus, as the dimension increases,
    the variance increases—this implies a higher probability of seeing high-magnitude
    dot products.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到点积的期望值为零，而其方差为k，即隐藏表示的维度。因此，随着维度的增加，方差增加——这意味着更高概率看到高幅度的点积。
- en: Unfortunately, with the presence of more high-magnitude dot products comes smaller
    gradients due to the softmax function. Although we won’t derive it here, this
    makes a lot of intuitive sense—think back to the use of softmax in neural networks
    for classification problems. As the neural network gets more and more confident
    in a correct prediction (i.e., a high logit value for the true index), the gradient
    gets smaller and smaller. The scaling factor introduced by Vaswani et al. reduces
    the magnitude of dot products, leading to larger gradients and better learning.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，随着更多高幅度点积的存在，由于softmax函数，梯度变得更小。虽然我们不会在这里推导它，但这在直觉上是有意义的——回想一下在神经网络中用于分类问题的softmax的使用。随着神经网络对正确预测变得越来越自信（即对真实索引的高logit值），梯度变得越来越小。Vaswani等人引入的缩放因子减小了点积的幅度，导致更大的梯度和更好的学习。
- en: Now that we’ve covered scaled dot product attention, we turn our attention to
    self-attention. In the previous sections, we saw attention through the context
    of machine translation where we are given a training set of sentences that are
    in English and French, and the goal is to be able to translate unseen English
    sentences to French. In this specific class of problems, there exists a direct
    supervision through the target French sentences. However, attention can also be
    used in a completely self-contained manner. The intuition is that, given a sentence
    in English, we may be able to perform more insightful sentiment analysis, more
    effective machine reading, and better understanding via learning relationships
    between tokens within sentences or paragraphs.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了缩放的点积注意力，我们将注意力转向自注意力。在前面的章节中，我们通过机器翻译的背景看到了注意力，其中我们有一个包含英语和法语句子的训练集，目标是能够将看不见的英语句子翻译成法语。在这类特定问题中，通过目标法语句子存在直接监督。然而，注意力也可以完全自包含地使用。直觉是，给定一个英语句子，我们可能能够通过学习句子或段落中标记之间的关系来执行更深入的情感分析、更有效的机器阅读和更好的理解。
- en: The transformer, our final topic for this section, utilizes both scaled dot
    product attention and self-attention. The transformer architecture (Vaswani et
    al. 2017) has both encoder and decoder architectures, where there exists self-attention
    within both the encoder and decoder, as well as standard attention between the
    encoder and decoder. The self-attention layers in the encoders and decoders allow
    each to attend to all positions prior to the current position in their respective
    architectures. The standard attention allows the decoder to attend to each encoder
    hidden state, as described earlier.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器，我们本节的最后一个主题，利用了缩放的点积注意力和自注意力。转换器架构（Vaswani等人，2017）具有编码器和解码器架构，其中编码器和解码器内部都存在自注意力，以及编码器和解码器之间的标准注意力。编码器和解码器中的自注意力层允许每个位置在其各自架构中关注当前位置之前的所有位置。标准注意力允许解码器关注每个编码器隐藏状态，如前文所述。
- en: Summary
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we’ve delved deep into the world of sequence analysis. We’ve
    analyzed how we might hack feed-forward networks to process sequences, developed
    a strong understanding of RNNs, and explored how attentional mechanisms can enable
    incredible applications ranging from language translation to audio transcription.
    Sequence analysis is a field that ranges problems not only in natural language,
    but also topics in finance, such as time-series analysis of returns of financial
    assets. Any field that involves longitudinal analyses, or analyses across time,
    could use the applications of sequence analysis described in this chapter. We
    advise you to really deepen your understanding of sequence analysis via implementation
    across different fields and by comparing the results of the techniques presented
    for natural language with the state-of-the-art in each field. There are also situations
    in which the techniques presented here may not be the most appropriate modeling
    choice, and we advise you to think deeply about why the modeling assumptions made
    here may not apply broadly. Sequence analysis is a powerful tool that has a place
    in almost all technical applications, not just natural language.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了序列分析的世界。我们分析了如何修改前馈网络以处理序列，发展了对RNN的深刻理解，并探讨了注意力机制如何实现从语言翻译到音频转录等令人难以置信的应用。序列分析是一个领域，不仅涉及自然语言问题，还涉及金融领域的主题，比如对金融资产回报的时间序列分析。任何涉及纵向分析或跨时间分析的领域都可以使用本章描述的序列分析应用。我们建议您通过在不同领域实施序列分析来加深对其的理解，并通过将自然语言技术的结果与各领域的最新技术进行比较。在某些情况下，本文介绍的技术可能不是最合适的建模选择，我们建议您深入思考为什么这里所做的建模假设可能不适用于广泛应用。序列分析是一个强大的工具，在几乎所有技术应用中都有一席之地，不仅仅是自然语言。
- en: '^([1](ch09.xhtml#idm45934164188240-marker)) Nivre, Joakim. “Incrementality
    in Deterministic Dependency Parsing.” *Proceedings of the Workshop on Incremental
    Parsing: Bringing Engineering and Cognition Together*. Association for Computational
    Linguistics, 2004; Chen, Danqi, and Christopher D. Manning. “A Fast and Accurate
    Dependency Parser Using Neural Networks.” *EMNLP*. 2014.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch09.xhtml#idm45934164188240-marker)) Nivre, Joakim. “Incrementality
    in Deterministic Dependency Parsing.” *Proceedings of the Workshop on Incremental
    Parsing: Bringing Engineering and Cognition Together*. Association for Computational
    Linguistics, 2004; Chen, Danqi, and Christopher D. Manning. “A Fast and Accurate
    Dependency Parser Using Neural Networks.” *EMNLP*. 2014.'
- en: ^([2](ch09.xhtml#idm45934164127728-marker)) Andor, Daniel, et al. “Globally
    Normalized Transition-Based Neural Networks.” *arXiv preprint* *arXiv*:1603.06042
    (2016).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.xhtml#idm45934164127728-marker)) Andor, Daniel, et al. “Globally
    Normalized Transition-Based Neural Networks.” *arXiv preprint* *arXiv*:1603.06042
    (2016).
- en: ^([3](ch09.xhtml#idm45934164119456-marker)) Ibid.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch09.xhtml#idm45934164119456-marker)) 同上。
- en: '^([4](ch09.xhtml#idm45934164070368-marker)) Kilian, Joe, and Hava T. Siegelmann.
    “The dynamic universality of sigmoidal neural networks.” *Information and computation*
    128.1 (1996): 48-56.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch09.xhtml#idm45934164070368-marker)) Kilian, Joe, 和 Hava T. Siegelmann。“Sigmoid神经网络的动态普适性。”
    *信息与计算* 128.1 (1996): 48-56。'
- en: ^([5](ch09.xhtml#idm45934164517408-marker)) Kiros, Ryan, et al. “Skip-Thought
    Vectors.” *Advances in neural information processing systems*. 2015.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch09.xhtml#idm45934164517408-marker)) Kiros, Ryan, 等。“Skip-Thought Vectors。”
    *神经信息处理系统的进展*。2015年。
- en: ^([6](ch09.xhtml#idm45934164490960-marker)) Bahdanau, Dzmitry, Kyunghyun Cho,
    and Yoshua Bengio. “Neural Machine Translation by Jointly Learning to Align and
    Translate.” *arXiv preprint arXiv*:1409.0473 (2014).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch09.xhtml#idm45934164490960-marker)) Bahdanau, Dzmitry, Kyunghyun Cho,
    and Yoshua Bengio. “通过联合学习对齐和翻译的神经机器翻译。” *arXiv预印本arXiv*:1409.0473 (2014)。
- en: ^([7](ch09.xhtml#idm45934165152352-marker)) Vaswani et. al. “Attention Is All
    You Need.” *arXiv Preprint arXiv*:1706.03762 2017.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch09.xhtml#idm45934165152352-marker)) Vaswani等。“注意力就是一切。” *arXiv预印本arXiv*:1706.03762
    2017年。
