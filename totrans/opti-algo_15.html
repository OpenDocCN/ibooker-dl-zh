<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">11 <a id="idTextAnchor000"/>Supervised and unsupervised learning<a id="idIndexMarker000"/></h1>

  <p class="co-summary-head">This chapter covers<a id="marker-397"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">Reviewing the basics of artificial intelligence, machine learning, and deep learning</li>

    <li class="co-summary-bullet">Understanding graph machine learning, graph embedding, and graph convolutional networks</li>

    <li class="co-summary-bullet">Understanding attention mechanisms</li>

    <li class="co-summary-bullet">Understanding self-organizing maps</li>

    <li class="co-summary-bullet">Solving optimization problems using supervised and unsupervised machine learning</li>
  </ul>

  <p class="body">Artificial intelligence (AI) is one of the fastest growing fields of technology, driven by advancements in computing power, access to vast amounts of data, breakthroughs in algorithms, and increased investment from both public and private sectors. AI aims to create intelligent systems or machines that can exhibit intelligent behavior, often by mimicking or drawing inspiration from biological intelligence. These systems can be designed to function autonomously or with some human guidance, and ideally, they can adapt to environments with diverse structures, observability levels, and dynamics. AI augments our intelligence by empowering us to analyze vast amounts of multidimensional, multimodal data and identify hidden patterns that would be difficult for humans to recognize. AI also supports our learning and decision-making by providing relevant insights and potential courses of action. AI encompasses various subfields, such as situation awareness (comprising perception, comprehension, and projection), knowledge representation, cognitive reasoning, machine learning, data analytics (covering descriptive, diagnostic, predictive, and prescriptive analytics), problem solving (involving constraint satisfaction and problem-solving using search and optimization), as well as digital and physical automation (such as conversational AI and robotics). <a id="idIndexMarker001"/></p>

  <p class="body">In this last part of the book, we will explore the convergence of two branches of AI: machine learning and optimization. Our focus will be on showcasing the practical applications of machine learning in tackling optimization problems. This chapter provides an overview of machine learning fundamentals as essential background knowledge, and then it delves into applications of supervised and unsupervised machine learning in handling optimization problems. Reinforcement learning will be covered in the next chapter.</p>

  <h2 class="fm-head" id="heading_id_3">11.1 A day in the life of AI-empowered daily routines</h2>

  <p class="body">AI, and machine learning in particular, forms the foundation of many successful disruptive industries and has successfully delivered many commercial products that touch everybody’s life every day. Starting at home, voice assistants eagerly await your commands, effortlessly controlling smart appliances and adjusting the smart thermostat to ensure comfort and convenience. Smart meters intelligently manage energy consumption, optimizing efficiency and reducing costs.<a id="idIndexMarker002"/><a id="idIndexMarker003"/><a id="idIndexMarker004"/><a id="marker-398"/></p>

  <p class="body">On the route to school or work, navigation apps with location intelligence guide the way, considering real-time traffic updates to provide the fastest and most efficient route. Shared mobility services offer flexible transportation options on demand, while advanced driver assistance systems enhance safety and convenience if you decide to drive. In the not-too-distant future, we will enjoy safe and entertaining self-driving vehicles as a third living space, after our homes and workplaces, with consumer-centric products and services.</p>

  <p class="body">Once at school or at the workplace, AI becomes an invaluable tool for personalization and to boost productivity. Personalized learning platforms cater to individual needs, adapting teaching methods and content to maximize understanding and retention. Summarization and grammar-checking algorithms aid in crafting flawless documents, while translation tools bridge language barriers effortlessly. Excel AI formula generators streamline complex calculations, saving time and effort. Human-like text generation enables natural and coherent writing, while audio, image, and video generation from text unlock creative possibilities. Optimization algorithms ensure optimal resource allocation and scheduling, maximizing efficiency in various scenarios, and handle different design, planning, and control problems.</p>

  <p class="body">During shopping, AI enhances the experience in numerous ways. Voice search enables hands-free exploration, while searching by images allows for effortless discovery of desired items. Semantic search understands context and intent, providing more accurate results. Recommendation engines offer personalized suggestions based on individual preferences and online shopping behavior, while last-mile or door-to-door delivery services ensure timely, transparent, and convenient package arrival.</p>

  <p class="body">In the realm of health, AI revolutionizes personalized healthcare, assisting with diagnosis, treatment planning, and rehabilitation. Lab automation speeds up testing processes, improving accuracy and efficiency. AI-driven drug discovery and delivery enable the development of innovative treatments and targeted therapies, transforming lives.</p>

  <p class="body">During leisure time, AI contributes to physical and mental well-being. Fitness planning apps tailor workout routines to individual goals and capabilities, providing personalized guidance and motivation. Trip planning tools recommend exciting destinations and itineraries, ensuring memorable experiences. AI-powered meditation apps offer customized relaxation experiences, soothing the mind and promoting mindfulness.</p>

  <p class="body">Machine learning, a prominent subfield of artificial intelligence, has played a pivotal role in bringing AI from the confines of high-tech research labs to the convenience of our daily lives.</p>

  <h2 class="fm-head" id="heading_id_4">11.2 Demystifying machine learning</h2>

  <p class="body"><a id="marker-399"/>The goal of learning is to create an internal model or abstraction of the external world. More comprehensively, Stanislas Dehaene, in <i class="fm-italics">How We Learn</i> [1], introduced seven key definitions of learning that lie at the heart of present-day machine learning algorithms:<a id="idIndexMarker005"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Learning is adjusting the parameters of a mental model.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Learning is exploring a combinatorial explosion.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Learning is minimizing errors.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Learning is exploring the space of possibilities.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Learning is optimizing a reward function.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Learning is restricting search space.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Learning is projecting a priori hypotheses.</p>
    </li>
  </ul>

  <p class="body">Machine learning (ML) is a subfield of AI that endows an artificial system or process with the ability to learn from experience and observation without being explicitly programmed. Thomas Mitchell, in <i class="fm-italics">Machine Learning</i>, defines ML as follows: “A computer program is said to learn from experience <i class="timesitalic">E</i> with respect to some class of tasks <i class="timesitalic">T</i> and performance measure <i class="fm-italics">P</i>, if its performance at tasks in <i class="fm-italics">T</i>, as measured by <i class="fm-italics">P</i>, improves with experience <i class="fm-italics">E</i>” [2]. In his book <i class="fm-italics">The Master Algorithm</i>, Pedro Domingos summarizes the ML schools of thought into five main schools [3], illustrated in figure 11.1:<a id="idIndexMarker006"/><a id="idIndexMarker007"/><a id="idIndexMarker008"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Bayesians with probabilistic inference as the master algorithm</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Symbolists with rules and trees as the main core algorithm within this paradigm</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Connectionists who use neural networks with backpropagation as a master algorithm</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Evolutionaries who rely on the evolutionary computing paradigm</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Analogizers who use mathematical techniques like support vector machines with different kernels</p>
    </li>
  </ul>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F01_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.1 Different ML schools of thought according to Domingos’ <i class="fm-italics">The Master Algorithm</i> <a id="idIndexMarker009"/></p>
  </div>

  <p class="body"><a id="marker-400"/>Nowadays, connectionist learning approaches have attracted most of the attention, thanks to their perception and learning capabilities in several challenging domains. These statistical ML algorithms follow a bottom-up inductive reasoning paradigm (i.e., inferring general rules from a set of examples) to discover patterns from vast amounts of data.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">The unreasonable effectiveness of data</p>

    <p class="fm-sidebar-text">Simple models and a lot of data trump more elaborate models based on less data [4]. This means that having a large amount of data to train simple models is often more effective than using complex models with only a small amount of data. For example, in self-driving vehicles, a simple model that has been trained on millions of hours of driving data can often be more effective in recognizing and reacting to diverse road situations than a more complex model trained on a smaller dataset. This is because the massive amount of data helps the simple model learn a wide range of patterns and scenarios, including adversarial and edge cases it might encounter, making it more adaptable and reliable in real-world driving conditions.<a id="idIndexMarker010"/></p>
  </div>

  <p class="body">These connectionist learning or statistical ML approaches are based on the experimental findings that even very complex problems in artificial intelligence may be solved by simple statistical models trained on massive datasets [4]. Statistical ML is currently the most famous form of AI. The rapid advancement of this form of ML can be attributed primarily to the widespread availability of big data and open source tools, enhanced computational power such as AI accelerators, and substantial research and development funding from both public and private sectors.</p>

  <p class="body">Generally speaking, ML algorithms can be categorized into supervised, unsupervised, hybrid learning, and reinforcement learning algorithms, as illustrated in figure 11.2.<a id="marker-401"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F02_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.2 ML taxonomy as a subfield of AI</p>
  </div>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Supervised learning</i>—This approach uses inductive inference to approximate mapping functions between data and known labels or classes. This mapping is learned using already labeled training data. <i class="fm-italics">Classification</i> (predicting discrete or categorical values) and <i class="fm-italics">regression</i> (predicting continuous values) are common tasks in supervised learning. For example, classification seeks a scoring function <span class="times"><i class="fm-italics">f</i>:<i class="fm-italics">Χ</i>×<i class="fm-italics">C</i><span class="cambria">⟶</span><i class="fm-italics">R</i></span>, where <i class="timesitalic">X</i> represents the training data space and <i class="fm-italics">C</i> represents the label or class space. This mapping can be learned using <i class="timesitalic">N</i> training examples of the form <span class="times">{(<i class="fm-italics">x</i><sub class="fm-subscript">11</sub>, <i class="fm-italics">x</i><sub class="fm-subscript">21</sub>, …, <i class="fm-italics">x<sub class="fm-subscript">m</sub></i><sub class="fm-subscript">1</sub>, <i class="fm-italics">c</i><sub class="fm-subscript">1</sub>), (<i class="fm-italics">x</i><sub class="fm-subscript">12</sub>, <i class="fm-italics">x</i><sub class="fm-subscript">22</sub>, …, <i class="fm-italics">x<sub class="fm-subscript">m</sub></i><sub class="fm-subscript">2</sub>, <i class="fm-italics">c</i><sub class="fm-subscript">2</sub>), …, (<i class="fm-italics">x</i><sub class="fm-subscript">1</sub><i class="fm-italics"><sub class="fm-subscript">N</sub>, x</i><sub class="fm-subscript">2</sub><i class="fm-italics"><sub class="fm-subscript">N</sub></i>, …, <i class="fm-italics">x<sub class="fm-subscript">mN</sub>, c<sub class="fm-subscript">N</sub></i>)}</span>, where <i class="timesitalic">x<sub class="fm-subscript">i</sub></i> is the feature vector of the <i class="fm-italics">i</i>-th example, <i class="timesitalic">m</i> is number of features, and <i class="timesitalic">c<sub class="fm-subscript">i</sub></i> is the corresponding class. The predicted class is the class that gives the highest score of <span class="times"><i class="fm-italics">f</i>, i.e., <i class="fm-italics">c</i>(<i class="fm-italics">x</i>) = argmax<i class="fm-italics"><sub class="fm-subscript">c</sub>f</i>(<i class="fm-italics">x</i>,<i class="fm-italics">c</i>)</span>. In the context of self-driving vehicles, supervised learning might be used to train a model to recognize traffic signs. The input data would be images of various traffic signs, and the correct output (the labels) would be the type of each sign. The trained model could then identify traffic signs correctly when driving. Feedforward neural networks (FNNs) or multilayer perceptrons (MLPs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and sequence-to-sequence (Seq2Seq) models are examples of common neural network architectures that are typically trained using supervised learning. Examples of solving combinatorial problems using supervised ML are provided in sections 11.6, 11.7, and 11.9.<a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/><a id="idIndexMarker019"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Unsupervised learning</i>—This approach deals with unlabeled data through techniques like <i class="fm-italics">clustering</i> and <i class="fm-italics">dimensionality reduction</i>. In clustering, for example, <i class="timesitalic">n</i> objects (each could be a vector of <i class="timesitalic">d</i> features) are given, and the task is to group them based on certain similarity measures into <i class="fm-italics">c</i> groups (clusters) in such a way that all objects in a single group have a “natural” relation to one another, and objects not in the same group are somehow different. For instance, unsupervised learning might be used in self-driving vehicles to cluster similar driving scenarios or environments. Using unsupervised learning, the car might learn to identify different types of intersections or roundabouts, even if no one has explicitly labeled the data with these categories. Autoencoders, k-means, density-based spatial clustering (DBSCAN), principal component analysis (PCA), and self- organizing maps (SOMs) are examples of unsupervised learning methods. SOM is explained in section 11.4. An example of a combinatorial problem using SOM is provided in section 11.8. <a id="idIndexMarker020"/><a id="idIndexMarker021"/><a id="idIndexMarker022"/><a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="marker-402"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Hybrid learning</i>—This approach includes <i class="fm-italics">semi-supervised learning and self-supervised learning</i> techniques. Semi-supervised learning is a mix of supervised and unsupervised learning where only a fraction of the input data is labeled with corresponding outputs. In this case, the training process uses the small amount of labeled data available and pseudo-labels the rest of the dataset—for example, training a self-driving vehicle's perception system with a limited set of labeled driving scenarios, then using a vast collection of unlabeled driving data to improve its ability to recognize and respond to various road conditions and obstacles. Self-supervised learning is an ML process where a model learns meaningful representations of the input data by using the inherent structure or relationships within the data itself. This is achieved by creating supervised learning tasks from the unlabeled data. For instance, a self-supervised model might be trained to predict the next word in a sentence based on the previous words or to reconstruct an image from a scrambled version. These learned representations can then be used for various downstream tasks, such as image classification or object detection. In the context of self-driving vehicles, a perception system can be trained to identify essential features in unlabeled driving scenes, such as lane markings, pedestrians, and other vehicles. Then, the learned features are utilized as pseudo-labels to classify new driving scenes in a supervised manner, enabling the vehicle to make decisions based on its understanding of the road environment.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Reinforcement learning (RL)</i>—This approach learns from interactions through a feedback loop or by trial and error. A learning agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. For self-driving vehicles, reinforcement learning could be used in the decision-making process. For instance, the car might learn over time the best way to merge into traffic on a busy highway. It would receive positive rewards for successful merges and negative rewards for dangerous maneuvers or failed attempts. Over time, through trial and error and the desire to maximize the reward, the car would learn an optimal policy for merging into traffic. More details about RL are provided in the next chapter.<a id="idIndexMarker025"/></p>
    </li>
  </ul>

  <p class="body">Deep learning (DL) is a subfield of ML concerned with learning underlying features in data using neural networks with many layers (hence “deep”) enabling artificial systems to build complex concepts out of simpler concepts. DL enables learning discriminative features or representations and learning at different levels of abstraction. To achieve this, the network uses hierarchical feature learning and employs a handful of convolutional layers. DL revolutionizes the field of ML by reducing the need for extensive data preprocessing. DL models can automatically extract highly discriminative features from raw data, eliminating the need for hand-crafted feature engineering. This end-to-end learning process significantly reduces the reliance on human experts, as the model learns to extract meaningful representations and patterns directly from the input data. <a id="idIndexMarker026"/></p>

  <p class="body">Unlike traditional ML algorithms, DL models have the ability to directly consume and process various forms of structured and unstructured data, such as text, audio, images, video, and even graphs. Graph-structured data is particularly important in the field of combinatorial optimization due to its ability to capture and represent the relationships and constraints between elements in optimization problems. Geometric DL is a subfield of ML that combines graph theory with DL. <a id="idIndexMarker027"/></p>

  <p class="body"><a id="marker-403"/>The following two sections address graph machine learning and self-organizing maps in more detail. They are essential background knowledge to the use cases described later in this chapter.<a id="idIndexMarker028"/><a id="idIndexMarker029"/></p>

  <h2 class="fm-head" id="heading_id_5">11.3 Machine learning with graphs</h2>

  <p class="body">As explained in section 3.1, a graph is a nonlinear data structure composed of entities known as <i class="fm-italics">vertices</i> (or nodes) and the relationships between them, known as <i class="fm-italics">edges</i> (or <i class="fm-italics">arcs</i> or <i class="fm-italics">links</i>). Data coming from different domains can be nicely captured using a graph. Social media networks, for instance, employ graphs to depict connections between users and to analyze social interactions, which in turn drive content propagation and recommendations. Navigation applications use graphs to represent physical locations and the paths between them, enabling route calculations, real-time traffic updates, and estimated time of arrival (ETA) predictions. Recommender systems rely on graphs to model user–item interactions and preferences, thereby offering personalized recommendations. Search engines use web graphs, where web pages are nodes and hyperlinks are edges, to crawl and index the internet and facilitate efficient information retrieval. Knowledge graphs offer a structured representation of factual information, relationships, and entities, and they’re used in diverse fields from digital assistants to enterprise data integration. Question-answering engines use graphs to understand and decompose complex questions and search for relevant answers in structured datasets. In the realm of chemistry, molecular structures can be viewed as graphs, where atoms are nodes and bonds are edges, supporting tasks like discovering compounds and predicting properties.<a id="idIndexMarker030"/><a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="idIndexMarker039"/></p>

  <p class="body"><a id="marker-404"/>Graph-structured data is vital due to its power to model complex relationships and dependencies between entities in an intuitive, self-descriptive, intrinsically explainable, and natural way. Unlike traditional tabular data, graphs allow for the representation of networked relationships and complex interconnectedness between entities of interest, making them an excellent tool for modeling numerous real-world systems. Tabular data can be converted into graph-structured data—the specific definitions of nodes and edges would depend on what relationships you’re interested in examining within the data. For example, in the context of a FIFA dataset, we can define nodes and edges based on the information available in this dataset:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Nodes</i>—Nodes represent entities of interest and could be the players, the clubs they play for, or their nationalities. Each of these entities could be a separate node in the graph. For example, Lionel Messi, Inter Miami, and Argentina could all be individual nodes in the graph.<a id="idIndexMarker040"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Edges</i>—Edges represent the relationships between the nodes. For instance, an edge could connect a player to the club they play for, indicating that the player is part of that club. Another edge could connect a player to their nationality, showing that the player belongs to that country. So, for example, Lionel Messi could be connected to Inter Miami with an edge indicating that Messi plays for Inter Miami, and another edge could connect Lionel Messi to Argentina, indicating his nationality.<a id="idIndexMarker041"/></p>
    </li>
  </ul>

  <p class="body">The next listing shows how to convert tabular data for 10 selected soccer players into a graph using NetworkX.</p>

  <p class="fm-code-listing-caption">Listing 11.1 Converting tabular data to a graph</p>
  <pre class="programlisting">import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
  
data={'Player':['L. Messi','R. Lewandowski','C. Ronaldo','Neymar Jr','K. 
<span class="fm-code-continuation-arrow">➥</span> Mbappé','E.Haaland','H. Kane','Luka Modrić','L. Goretzka','M. Salah'],
<span class="fm-code-continuation-arrow">➥</span>    'Age':[36,34,38,22,24,35,29,37,28,31],
<span class="fm-code-continuation-arrow">➥</span>    'Nationality':['Argentina','Poland','Portugal','Brazil','France','Norway',
<span class="fm-code-continuation-arrow">➥</span>    'England','Croatia','Germany','Egypt'],
<span class="fm-code-continuation-arrow">➥</span>    'Club':['Inter Miami','Barcelona','Al-Nassr','Al-Hilal ','PSG','Manchester
<span class="fm-code-continuation-arrow">➥</span>    City','Tottenham Hotspur','Real Madrid','Bayern Munich','Liverpool'],
<span class="fm-code-continuation-arrow">➥</span>    'League':['Major League Soccer ','Spain Primera Division','Saudi Arabia
<span class="fm-code-continuation-arrow">➥</span> League','Saudi Arabia League','French Ligue 1','English Premier
<span class="fm-code-continuation-arrow">➥</span> League','English Premier League','Spain Primera Division','German 1.
<span class="fm-code-continuation-arrow">➥</span> Bundesliga','English Premier League']}
df=pd.DataFrame.from_dict(data)</pre>

  <p class="body">As a continuation of listing 11.1, we can create a NetworkX graph whose nodes represent the player name, club, and nationality and whose edges represent the semantic relationships between these nodes.</p>
  <pre class="programlisting">G = nx.Graph()                                                                      <span class="fm-combinumeral">①</span>
for index, row in df.iterrows():
    G.add_edge(row['Player'], row['Club'], relationship='plays_for')                <span class="fm-combinumeral">②</span>
for index, row in df.iterrows():
    G.add_edge(row['Player'], row['Nationality'], relationship='belongs_to')        <span class="fm-combinumeral">③</span>
pos = nx.kamada_kawai_layout(G)                                                     <span class="fm-combinumeral">④</span>
plt.figure(figsize=(20, 14))                                                        <span class="fm-combinumeral">⑤</span>
player_nodes = df['Player'].unique().tolist()                                       <span class="fm-combinumeral">⑥</span>
club_nodes = df['Club'].unique().tolist()                                           <span class="fm-combinumeral">⑥</span>
nationality_nodes = df['Nationality'].unique().tolist()                             <span class="fm-combinumeral">⑥</span>
nx.draw_networkx_nodes(G, pos, nodelist=player_nodes, node_color='blue',
<span class="fm-code-continuation-arrow">➥</span> label='Player Name', node_shape='o')                                             <span class="fm-combinumeral">⑦</span>
nx.draw_networkx_nodes(G, pos, nodelist=club_nodes, node_color='red', label='Club', <span class="fm-combinumeral">⑦</span>
<span class="fm-code-continuation-arrow">➥</span> node_shape='d')                                                                  <span class="fm-combinumeral">⑦</span>
nx.draw_networkx_nodes(G, pos, nodelist=nationality_nodes, node_color='gray',       <span class="fm-combinumeral">⑦</span>
<span class="fm-code-continuation-arrow">➥</span> label='Nationality', node_shape='v')                                             <span class="fm-combinumeral">⑦</span>
nx.draw_networkx_edges(G, pos)                                                      <span class="fm-combinumeral">⑧</span>
edge_labels = nx.get_edge_attributes(G, 'relationship')                             <span class="fm-combinumeral">⑨</span>
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)         <span class="fm-combinumeral">⑨</span>
nx.draw_networkx_labels(G, pos)                                                     <span class="fm-combinumeral">⑩</span>
plt.legend(fontsize=13, loc='upper right')
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Create a new graph.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Add nodes and edges for clubs.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Add nodes and edges for nationalities.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Create the layout</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Set the size of the figure.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Get lists of player, club, and nationality nodes.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Draw nodes in different colors.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Draw edges.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Draw edge labels.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Draw node labels.</p>

  <p class="body"><a id="marker-405"/>Figure 11.3 shows the data for the 10 selected soccer players in a graph. This graph shows the entities of interest (player, club, and nationality) and their relationships. For example, L. Messi is a player who plays for Inter Miami and is from Argentina.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F03_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.3 Graph-structured data for 10 selected soccer players</p>
  </div>

  <p class="body"><a id="marker-406"/>Graph data fundamentally differs from Euclidean data, as the concept of distance is not simply a matter of straight-line (Euclidean) distance between two points. In the case of a graph, what matters is the structure of the nodes and edges—whether two nodes are connected by an edge and how they are connected to other nodes in the graph. Table 11.1 summarizes the differences between Euclidean and non-Euclidean graph data.</p>

  <p class="fm-table-caption">Table 11.1 Euclidean data versus non-Euclidean graph data</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="calibre6">
      <tr class="contenttable-0-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Aspects</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Euclidean data</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Non-Euclidean graph data</p>
        </th>
      </tr>
    </thead>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Common data types</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Numerical, text, audio, images, videos</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Road networks, social networks, web pages, and molecular structures</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Dimensionality</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Can be 1D (e.g., numbers, text), 2D (e.g., images, heatmaps), or higher-dimensional (e.g., RGB-D images or depth maps, 3D point cloud data)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Large dimensionality (e.g., a Pinterest graph has 3 billion nodes and 18 billion edges)</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Structure</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Fixed structure (e.g., in the case of an image, the structure is embedded via pixel proximity)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Arbitrary structure (every node can have a different neural structure because the network neighborhood around it is different, as the model adapts to the data)</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Spatial locality</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Yes (i.e., data points that are close together in the input space are also likely to be close together in the output space).</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">No, “closeness” is determined by the graph structure, not spatial arrangement (i.e., two nodes that are “close” to each other might not necessarily have similar properties or features, such as in the case of a traffic light node and a crosswalk node).</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Shift-invariance</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Yes (i.e., data-inherent meaning is preserved when shifted; for instance, the concept of a cat in a picture does not change if the cat is in the top left corner or the bottom right corner of the image).</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">No (in a graph, there’s no inherent meaning to the “position” of a node that can be “shifted”).</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Ordinality or hierarchy</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Yes</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">No, graph data has “permutation invariance”—the specific ordering or labeling of nodes doesn’t usually affect the underlying relationships and properties of the graph.</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Shortest path between two points</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">A straight line</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Is not necessarily a straight line</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Examples of ML models</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Convolutional neural networks (CNNs), long short-term memory (LSTM), and recurrent neural networks (RNNs)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Graph neural networks (GNNs), graph convolutional networks (GCNs), temporal graph networks (TGNs), spatial-temporal graph neural networks (STGNNs)</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body"><i class="fm-italics">Geometric deep learning</i> (GDL) is an umbrella term for emerging techniques seeking to extend (structured) deep neural models to handle non-Euclidean data with underlying geometric structures, such as graphs (networks of connected entities), point clouds (collections of 3D data points), molecules (chemical structures), and manifolds (curved, high-dimensional surfaces). Graph machine learning (GML) is a subfield of ML that focuses on developing algorithms and models capable of learning from graph-structured data. Graph embedding or representation learning is the first step in performing ML tasks such as node classification (predicting a category for each node), link prediction (forecasting connections between nodes), and community detection (identifying groups of interconnected nodes). The next subsection describes different graph embedding techniques.<a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="marker-407"/></p>

  <h3 class="fm-head1" id="heading_id_6">11.3.1 Graph embedding</h3>

  <p class="body">Graph embedding is a task that aims to learn a mapping from a discrete high- dimensional graph domain to a low-dimensional continuous domain. Through the process of graph embedding, graph nodes, edges, and their features are transformed into continuous vectors while preserving the structural information of the graph. For example, as shown in figure 11.4, an encoder, <span class="times"><i class="fm-italics">ENC</i>(<i class="fm-italics">v</i>)</span>, maps node <i class="timesitalic">v</i> from the input graph space <i class="timesitalic">G</i> to a low-dimensional vector <i class="timesitalic">h<sub class="fm-subscript">v</sub></i> in the embedding or latent space <i class="timesitalic">H</i> based on the node’s position in the graph, its local neighborhood structure, or its features, or some combination of the three.<a id="idIndexMarker046"/><a id="idIndexMarker047"/><a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="idIndexMarker050"/><a id="idIndexMarker051"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F04_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.4 Graph embedding</p>
  </div>

  <p class="body">This encoder needs to be optimized to minimize the difference between the similarity of a pair of nodes in the graph and their similarity in the embedding space. Nodes that are connected or nearby in the graph should be close in the embedded space. Conversely, nodes that are not connected or are far apart in the graph should be far apart in the embedded space. In a more generalized encoder/decoder architecture, a decoder is added to extract user-specified information from the low-dimensional embedding [5]. By jointly optimizing the encoder and decoder, the system learns to compress information about the graph structure into the low-dimensional embedding space.</p>

  <p class="body">There are various methods for graph embedding which can be broadly classified into transductive (shallow) embedding and inductive embedding:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Transductive embedding</i>—In the transductive learning paradigm, the model learns embeddings only for the nodes present in the graph during the training phase. The learned embeddings are specific to these nodes, and the model cannot generate embeddings for new nodes that weren’t present during training. These methods are difficult to scale and are suitable for static graphs. Examples of transductive methods for graph embedding include random walk (e.g., node2vec and DeepWalk) and matrix factorization (e.g., graph factorization and HOPE).<a id="idIndexMarker052"/><a id="marker-408"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Inductive embedding</i>—Inductive learning methods can generalize to unseen nodes or entire graphs that were not present during training. They do this by learning a function that generates the embedding of a node based on its features and the structure of its local neighborhood, which can be applied to any node, regardless of whether it was present during training or not. These methods are suitable for evolving graphs. Examples of inductive methods for graph embedding are graph neural networks (GNN) and graph convolutional networks (GCNs).<a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="idIndexMarker055"/></p>
    </li>
  </ul>

  <p class="body">Appendix A contains examples of some of these methods. For more information, see Broadwater and Stillman’s <i class="fm-italics">Graph Neural Networks in Action</i> [6]. We’ll focus on GCN, as it is the most relevant approach to the combinatorial optimization application presented in this chapter.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Transductive versus inductive learning</p>

    <p class="fm-sidebar-text"><i class="fm-italics">Transductive learning</i> aims to learn from a specific set of data to a specific set of predictions without generalizing to new data. <i class="fm-italics">Inductive learning</i> aims to learn general rules from observed training cases. These general rules can then be applied to new, unseen data.<a id="idIndexMarker056"/><a id="idIndexMarker057"/><a id="marker-409"/></p>
  </div>

  <p class="body">The <i class="fm-italics">convolution operation</i> forms the basis of representation learning in many structured data scenarios, enabling the automatic learning of meaningful features from raw data, thereby obviating the need for manual feature engineering. Convolution is a mathematical operation that takes two functions (input data and a kernel, filter, or feature detector) and measures their overlap or merges the two sets of information to produce a feature map. One critical aspect of convolution is its ability to respect and utilize the known structural relationships among data points, such as the positional associations among pixels, the temporal order of time points, or the edges linking nodes in a network. In traditional ML, convolutional neural networks (CNNs) employ the convolution operator as a key tool for identifying spatial patterns within images. This is made possible by the inherent grid-like structure of image data, which allows the model to slide filters over the image, exploit the spatial regularities, and extract features in a manner akin to pattern recognition.<a id="idIndexMarker058"/><a id="idIndexMarker059"/></p>

  <p class="body">However, in the realm of graph machine learning (GML), the situation changes considerably. The data in this context is non-Euclidean, as explained previously in table 11.1, meaning that it isn’t arranged on a regular grid like pixels are in an image or points are on a 3D surface. Instead, it’s represented in the form of a network or graph, which can capture complex relationships. Moreover, this data exhibits order invariance, implying that the output does not change with the rearrangement of nodes.<a id="idIndexMarker060"/></p>

  <p class="body">Unlike CNNs, which operate on a regular grid, GCNs are designed to work with data that’s structured as a graph, which can represent a wide variety of irregular and complex structures. Each node is connected to its neighbors without any predefined pattern, and the convolution operation is applied to a node and its direct neighbors in the graph.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">How does Google DeepMind predict the estimated time of arrival?</p>

    <p class="fm-sidebar-text">Have you ever wondered how Google Maps predicts the estimated time of arrival (ETA) when you’re planning your trip? Google DeepMind uses a GML approach to do so. The traditional ML approach would be to break the route down into a number of road segments, predict the time to traverse each road segment using a feedforward neural network, and sum them up to get the ETA. However, the underlying assumption of feedforward NN is that the road segments are independent of each other. In reality, road segment traffic easily influences the ETA of neighboring road segments, so the samples are not independent.</p>

    <p class="fm-sidebar-text">For instance, consider the situation where congestion on a minor road influences the traffic flow on a main road. When the model encompasses multiple junctions, it naturally develops the capacity to predict slowdowns at intersections, delays due to converging traffic, and the total time taken in stop-and-go traffic conditions. A better approach is to use GML to take the influence of the neighboring road segments into consideration.</p>

    <p class="fm-sidebar-text">In this case, the road network will first be converted into a graph where each road segment is represented as a node. If two road segments are connected to each other, their corresponding nodes will be connected by an edge in the graph. Graph embedding is then generated by GNN to map the node features and graph structures from a high- dimensional discrete graph space to a low-dimensional continuous latent space. Information is propagated and aggregated across the graph through a technique called <i class="fm-italics">message passing</i>, where, at the end, the embedding vector for each node contains and encodes its own information as well as the network information from all its neighboring nodes, according to the degree of neighborhood. Adjacent nodes pass messages to each other. In the first pass, each node knows about its neighbor. In the second pass, every node knows about its neighbor’s neighbors, and this information is encoded into the embedding, and so on. This allows us to represent the influence of the traffic in each of the neighboring road segments. <a id="idIndexMarker061"/></p>

    <p class="fm-sidebar-text">The accuracy of real time ETAs was improved by up to 50% in places like Berlin, Jakarta, São Paulo, Sydney, Tokyo, and Washington DC using this approach [7].</p>
  </div>

  <p class="body"><a id="marker-410"/>As illustrated in figure 11.5, given an input graph, which includes node features <i class="fm-italics">x<sub class="fm-subscript">v</sub></i> and an adjacency matrix <i class="timesitalic">A</i>, a GCN transforms the features of each node into a latent or embedding space <i class="timesitalic">H</i>, while preserving the graph structure denoted by the adjacency matrix <i class="timesitalic">A</i>. These latent vectors provide a rich representation of each node, making it possible to perform node classification independently. <a id="idIndexMarker062"/><a id="idIndexMarker063"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F05_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.5 Graph embedding and node, link, and graph classification</p>
  </div>

  <p class="body">Moreover, GCNs are also capable of predicting characteristics related to edges, such as whether a link exists between two nodes. Once node embeddings are generated, the likelihood of an edge between two nodes <i class="timesitalic">v</i> and <i class="timesitalic">u</i> can be predicted based on their embeddings <i class="timesitalic">h<sub class="fm-subscript">v</sub></i>, <i class="timesitalic">h<sub class="fm-subscript">u</sub></i>. A common approach is to compute a similarity measure (e.g., a dot product) between the embeddings of two nodes. This similarity can then be passed through a sigmoid function to predict the probability of an edge. The errors (loss) on predictions will be backpropagated and update the weights in neural networks. <a id="idIndexMarker064"/><a id="idIndexMarker065"/></p>

  <p class="body">Finally, GCNs enable classification at the level of the entire graph. This can be achieved by aggregating all the latent or embedding vectors (<i class="timesitalic">H</i>) for all the nodes. The aggregation function used must be permutation invariant, meaning the output should remain the same regardless of the order of the nodes. Common examples of such functions are summation or averaging or maximizing. Once you’ve aggregated the latent vectors into a single representation, you can feed this representation into a module (e.g., a neural network layer) to predict an output for the whole graph. In essence, GCNs allow node-level, edge-level, and graph-level predictions.</p>

  <p class="body">To better understand how GCN works, let’s consider a graph with five nodes, as shown in figure 11.6. For each node in the graph, the first step is to find the neighboring nodes. Let’s assume we want to examine how the embedding for node 5 is generated. As you can see in the original graph (upper-left corner of figure 11.6), nodes 2 and 4 are neighbors of node 5. The second step is message-passing, which is the process of nodes sending, receiving, and aggregating messages from their neighbors to iteratively update their features. This allows GCNs to learn a representation for each node that captures both its own features and its context within the graph. The learned representations can then be used for downstream tasks like node classification, link prediction, or graph classification.<a id="idIndexMarker066"/><a id="marker-411"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F06_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.6 Message passing and updating in GCN</p>
  </div>

  <p class="body">The embedding of node <i class="timesitalic">v</i> after <i class="timesitalic">t</i> layers of neighborhood aggregation considering <span class="times"><i class="fm-italics">N</i>(<i class="fm-italics">v</i>)</span> neighboring nodes is based on the formula shown in figure 11.7. The initial <span class="times">0<sup class="fm-superscript">th</sup></span> layer embeddings <span class="times"><i class="fm-italics">h<sub class="fm-subscript">v</sub></i><sup class="fm-superscript">0</sup></span> are equal to node features <span class="times">x<sub class="fm-subscript">v</sub></span>.<a id="idIndexMarker067"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F07_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.7 Embedding function in GCN</p>
  </div>

  <p class="body">This formula is applied recursively to get another, better vector <i class="timesitalic">h</i> at each time step, where <i class="timesitalic">h</i> is the vector representation of the nodes in the latent space. The weight matrix is learned through training on given data. At the beginning, each node in the graph is aware only of its own initial features. In the first layer of the GCN, each node communicates with its immediate neighbors, aggregating its own features and receiving features from those neighbors. As we move to the second layer, each node again communicates with its neighbors. However, because the neighbors have already incorporated information from their own neighbors in the first layer, the original node now indirectly accesses information from two hops away in the graph—its neighbors’ neighbors. As this process repeats through more layers in the GCN, information is propagated and aggregated across the graph. At the end, the embedding vector for each node contains and encodes its own information as well as the network information from all its neighboring nodes according to the degree of neighborhood, or its <i class="timesitalic">k</i>-hop neighborhood, to create context embedding. The <i class="fm-italics">k-hop neighborhood</i>, or neighborhood of radius <i class="timesitalic">k</i>, of a node is a set of neighboring nodes at a distance less than or equal to <i class="timesitalic">k</i>.<a id="idIndexMarker068"/><a id="marker-412"/></p>

  <p class="body">Listing 11.2 shows how to generate node embedding for the Cora dataset using GCN. The Cora dataset consists of 2,708 scientific publications classified into one of seven classes. The citation network consists of 5,429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word in the dictionary. The dictionary consists of 1,433 unique words. <a id="idIndexMarker069"/></p>

  <p class="body">PyG (PyTorch Geometric) is used and can be installed as follows:</p>
  <pre class="programlisting">$conda install pytorch torchvision -c pytorch<a id="idIndexMarker070"/>
$conda install torch_scatter
$conda install torch_sparse
$conda install torch_cluster
$conda install torch-spline-conv
$conda install torch_geometric</pre>

  <p class="body">More information about PyG CUDA installation is available in the PyG documentation (<a class="url" href="https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html">https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html</a>).</p>

  <p class="body">We’ll start by importing the libraries we’ll use.</p>

  <p class="fm-code-listing-caption">Listing 11.2 Node embedding using GCN</p>
  <pre class="programlisting">import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
import torch
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GCNConv
from torch_geometric.utils import to_networkx</pre>

  <p class="body">PyG provides several datasets that can be loaded directly, such as KarateClub, Cora, Amazon, Reddit, etc. The Cora dataset is part of the Planetoid dataset and can be loaded as follows:</p>
  <pre class="programlisting">dataset = Planetoid(root='/tmp/Cora', name='Cora')</pre>

  <p class="body"><a id="marker-413"/>As you can see in the following code, the GCN model is defined with two <code class="fm-code-in-text">GCNConv</code> layers (<code class="fm-code-in-text">GCNConv</code>) and a <code class="fm-code-in-text">torch.nn.Dropout</code> layer. <code class="fm-code-in-text">GCNConv</code> is a graph convolution layer, and <code class="fm-code-in-text">torch.nn.Dropout</code> is a dropout layer, which randomly zeroes some of the elements of the input tensor with probability 0.5 during training as a simple way to prevent overfitting. <a id="idIndexMarker071"/><a id="idIndexMarker072"/><a id="idIndexMarker073"/><a id="idIndexMarker074"/><a id="idIndexMarker075"/></p>

  <p class="body">The <code class="fm-code-in-text">forward</code> function defines the forward pass of the model. It takes a data object as input, representing the graph, and the features of the nodes and the adjacency list of the graph are extracted from the input data. The node features (<code class="fm-code-in-text">x</code>) are passed through the first GCN layer <code class="fm-code-in-text">conv1</code>, a <code class="fm-code-in-text">relu</code> activation function, a dropout layer, and finally the second GCN layer <code class="fm-code-in-text">conv2</code>. The adjacency list, <code class="fm-code-in-text">edge_index</code>, is required for the convolution operation in the GCN layers. The output of the network is then returned:<a id="idIndexMarker076"/><a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="idIndexMarker079"/></p>
  <pre class="programlisting">class GCN(torch.nn.Module):
    def __init__(self):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(dataset.num_node_features, 16)
        self.conv2 = GCNConv(16, dataset.num_classes)
        self.dropout = torch.nn.Dropout(0.5)
 
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
  
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
  
        return x</pre>

  <p class="body">As a continuation of listing 11.2, the following code snippet trains the GCN model on a single graph and extracts the node embedding from the trained model. The <code class="fm-code-in-text">model</code> is trained for 200 epochs. Its gradients are first zeroed, then the forward pass is computed, and the negative log-likelihood loss is calculated on the training nodes (those marked by <code class="fm-code-in-text">data.train_mask</code>). The backward pass is then computed to get the gradients, and the optimizer performs a step to update the model parameters. The model is set to evaluation mode and is run on the graph again to obtain the final node embeddings: <a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="marker-414"/></p>
  <pre class="programlisting">device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')        <span class="fm-combinumeral">①</span>
model = GCN().to(device)                                                     <span class="fm-combinumeral">②</span>
data = dataset[0].to(device)                                                 <span class="fm-combinumeral">③</span>
  
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4) <span class="fm-combinumeral">④</span>
  
model.train()                                                                <span class="fm-combinumeral">⑤</span>
for epoch in range(200):                                                     <span class="fm-combinumeral">⑤</span>
    optimizer.zero_grad()                                                    <span class="fm-combinumeral">⑤</span>
    out = model(data)                                                        <span class="fm-combinumeral">⑤</span>
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])         <span class="fm-combinumeral">⑤</span>
    loss.backward()                                                          <span class="fm-combinumeral">⑤</span>
    optimizer.step()                                                         <span class="fm-combinumeral">⑤</span>
  
model.eval()                                                                 <span class="fm-combinumeral">⑥</span>
embeddings_pyg = model(data).detach().cpu().numpy()                          <span class="fm-combinumeral">⑦</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> If CUDA is available, the code uses the GPU; otherwise, it will use the CPU.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Create an instance of the GCN model, and move it to the chosen device.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Load the first graph in the dataset, and move it to the device.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Use the Adam optimizer with a learning rate of 0.01 and weight decay (a form of regularization) of 0.0005.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Train the model for 200 epochs.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Set evaluation mode.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Obtain the final node embeddings.</p>

  <p class="body">The <code class="fm-code-in-text">.detach()</code> function is used to detach the output from the computational graph and returns a new tensor that doesn’t require a gradient. The embeddings are then moved from the GPU (if they were on the GPU) to the CPU. This is done to make the data accessible for further processing, such as converting it to a NumPy array. The generated embedding has a size of (2708, 7), where the number of nodes is 2,708 and the number of classes or subjects is 7. Dimensionality reduction using principle component analysis (PCA) is applied to visualize the embedding in 2D as shown in figure 11.8. <a id="idIndexMarker082"/><a id="idIndexMarker083"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F08_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.8 Node embedding using GCN in PyG</p>
  </div>

  <p class="body">As you can see, the node embedding makes the nodes belonging to the same classes cluster together. This means increased discrimination power of the features, which results in more accurate predictions.</p>

  <p class="body"><a id="marker-415"/>The complete version of listing 11.2 available in the book’s GitHub repo also shows how to generate node embedding using the GCN available in StellarGraph. StellarGraph is a Python library for ML on graphs and networks.</p>

  <h3 class="fm-head1" id="heading_id_7">11.3.2 Attention mechanisms</h3>

  <p class="body">As you saw in figure 11.7, the embedding function in GCN consists of message passing, aggregation, and update functions. The message passing function mainly integrates messages from the node’s neighbors based on a learnable weight matrix <i class="timesitalic">W<sup class="fm-superscript">t</sup></i>. This weight matrix does not reflect the degree of importance of neighboring nodes. The convolution operation applies the same learned weights to all neighbors of a node as a linear transformation, without explicitly accounting for their importance or relevance. This might not be ideal because some segments may need more attention than others.<a id="idIndexMarker084"/><a id="idIndexMarker085"/><a id="idIndexMarker086"/><a id="idIndexMarker087"/></p>

  <p class="body">The concept of “attention” in DL essentially permits the model to selectively concentrate on specific segments of the input data as it produces the output sequence. This mechanism ensures that context is maintained and propagated from the initial stages to the end. It also allows the model to dynamically allocate its resources by focusing on the most important parts of the input at each time step. In a broad sense, attention in DL can be visualized as a vector consisting of importance or relevance scores. These scores help quantify the relationship or association between a node in a graph and all other nodes in the graph.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Attention is all you need</p>

    <p class="fm-sidebar-text">The groundbreaking paper “Attention Is All You Need” [8] proposes a new Transformer model for processing sequential data like text. In the world of language processing and translation, models usually read an entire sentence or document word by word, in order (like we do when we read a book), and then make predictions based on that. These models have some difficulties understanding long sentences and recalling information from far away in the text. In the case of long sequences, there is a high probability that the initial context will be lost by the end of the sequence. This is called the <i class="fm-italics">forgetting problem</i>. <a id="idIndexMarker088"/><a id="marker-416"/></p>

    <p class="fm-sidebar-text">The authors of the paper propose a different way of handling this task. Instead of reading everything in order, their model focuses on different parts of the input at different times, almost like it’s jumping around the text. This is what they refer to as “attention.” The attention mechanism allows the model to dynamically prioritize which parts of the input are most relevant for each word it’s trying to predict, making it more effective at understanding context and reducing confusion arising from long sentences or complex phrases. For more details, see “The Annotated Transformer” [9].</p>
  </div>

  <p class="body">Figure 11.9b shows a graph attention network (GAT), where a weighting factor or attention coefficient <i class="timesitalic">α</i> is added to the embedding equation to reflect the importance of the neighboring nodes. GAT uses a weighted adjacency matrix instead of nonweighted adjacency matrix used in case of GCN (figure 11.9a). An attentional mechanism <i class="timeitalic">a</i> is used to compute unnormalized coefficients <i class="timesitalic">e<sub class="fm-subscript">vu</sub></i> across pairs of nodes <i class="timesitalic">v</i> and <i class="timesitalic">u</i> based on their features:<a id="idIndexMarker089"/></p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH11_F08_Khamis-EQ01.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">11.1</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">An example of this attentional mechanism can be dot-product attention that measures the similarity or alignment between the features of the two nodes, providing a quantitative indication of how much attention node <i class="timesitalic">v</i> should give to node <i class="timesitalic">u</i>. Other mechanisms may involve learned attention weights, nonlinear transformations, or more complex interactions between node features. Following the graph structure, node <i class="timesitalic">v</i> can attend over nodes in its neighborhood only <span class="times"><i class="fm-italics">i</i> <span class="cambria">∈</span> <i class="fm-italics">N<sub class="fm-subscript">v</sub></i></span>.</p>

  <p class="body">Attention coefficients are typically normalized using the softmax function so that they are comparable, irrespective of the scale or distribution of raw scores in different neighborhoods or contexts. Note that in figure 11.9b, for simplicity, the attention coefficients <i class="timesitalic">α<sub class="fm-subscript">vu</sub></i> are denoted as <a id="marker-417"/><i class="timesitalic">α<sub class="fm-subscript">u</sub></i>.</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH11_F08_Khamis-EQ02.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">11.2</p>
        </td>
      </tr>
    </tbody>
  </table>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F09_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.9 Graph convolutional network (GCN) vs. graph attention network (GAT)</p>
  </div>

  <p class="body"><i class="fm-italics">Multi-head attention</i> is a key component in GATs and also in the Transformer model discussed in the “Attention Is All You Need” paper. In a multi-head attention mechanism, the model has multiple sets of attention weights. Each set (or “head”) can learn to pay attention to different parts of the input. Instead of having just one focus of attention, the model can have multiple focuses, allowing it to capture different types of relationships and patterns in the data. In the context of GATs, a multi-head attention mechanism allows each node in the graph to focus on different neighboring nodes in different ways, as shown in figure 11.10.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F10_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.10 Multi-head attention with <span class="times"><i class="fm-italics">H</i> = 3</span> heads by node 5. <span class="times"><i class="fm-italics">α</i><sub class="fm-subscript">52</sub></span>, <span class="times"><i class="fm-italics">α</i><sub class="fm-subscript">54</sub></span>, and <span class="times"><i class="fm-italics">α</i><sub class="fm-subscript">55</sub></span> are the attention coefficients between the nodes. The aggregated features from each head are averaged to obtain the final embedding of the node.</p>
  </div>

  <p class="body"><a id="marker-418"/>Once the multiple heads have performed their respective attention operations, their results are typically averaged. This process condenses the diverse perspectives captured by the multiple attention heads into a single output. After the results of the multi-head attention operation are combined, a final nonlinearity is then applied. This step typically involves the use of a softmax function or logistic sigmoid function, especially in classification problems. These functions serve to translate the model’s final outputs into probabilities, making the output easier to interpret and more useful for prediction tasks.</p>

  <h3 class="fm-head1" id="heading_id_8">11.3.3 Pointer networks</h3>

  <p class="body">Sequential ML involves dealing with data where the order of observations matters, such as time series data, sentences, or permutations. Sequential ML tasks can be classified based on the number of inputs and outputs, as shown in table 11.2. A <i class="fm-italics">sequence-to- sequence</i> (seq2seq) model takes a sequence of items and outputs another sequence of items. Recurrent neural networks (RNN) and long short-term memory (LSTM) have been established as state-of-the-art approaches in seq2seq modeling.<a id="idIndexMarker090"/><a id="idIndexMarker091"/><a id="idIndexMarker092"/><a id="idIndexMarker093"/><a id="idIndexMarker094"/><a id="idIndexMarker095"/></p>

  <p class="fm-table-caption">Table 11.2 Sequential ML</p>

  <table border="1" class="contenttable-1-table" id="table002" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="25%"/>
      <col class="contenttable-0-col" span="1" width="75%"/>
    </colgroup>

    <thead class="calibre6">
      <tr class="contenttable-0-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Task</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Example</p>
        </th>
      </tr>
    </thead>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">One-to-one</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Image classification. We provide a single image as input, and the model outputs the classification or category, like “dog” or “cat,” as a single output.</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">One-to-many</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Image captioning. We input a single image into the model, and it generates a sequence of words describing that image.</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Many-to-one</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Sentiment analysis. We input a sequence of words (like a sentence or a tweet), and the model outputs a single sentiment score (like “positive," “negative,” or “neutral”).</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Many-to-many (type 1)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Sequence input and sequence output, like in the case of named entity recognition (NER). We input a sentence (a sequence of words), and the model outputs the recognized entity, such as a person, organization, location, etc.</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Many-to-many (type 2), known as a synchronized sequence model</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Synced sequence input and output. The model takes a sequence of inputs but doesn’t output anything until the entire sequence has been read. Then it outputs a sequence. An example of this is video classification, where the model takes a sequence of video frames as input and then outputs a sequence of labels for those frames.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body"><a id="marker-419"/>In discrete combinatorial optimization problems like the travelling salesman problem, sorting tasks, or the convex hull problem, both the input and output data are sequential. However, traditional seq2seq models struggle to solve these problems effectively. This is primarily because the discrete categories of output elements are not predetermined. Instead, they are contingent on the variable size of the input (for instance, the output dictionary is dependent on the input length). The <i class="fm-italics">pointer network</i> (Ptr-Net) model [10] addresses this problem by utilizing attention as a mechanism to point to or select a member of the input sequence for the output. This model not only enhances performance over the conventional seq2seq model equipped with input attention, but it also enables us to generalize to output dictionaries of variable sizes.<a id="idIndexMarker096"/></p>

  <p class="body">While traditional attention mechanisms distribute attention over the input sequence to generate an output element, Ptr-Net instead uses attention as a pointer. This pointer is used to select an element from the input sequence to be included in the output sequence. Let’s consider the convex hull problem as an example of a discrete combinatorial optimization problem. A convex hull is a geometric shape, specifically a polygon, that fully encompasses a given set of points. It achieves this by optimizing two distinct parameters: it maximizes the area that the shape covers, while simultaneously minimizing the boundary or circumference of the shape, as illustrated in figure 11.11. To understand this concept, it can be useful to imagine stretching a rubber band around the extreme points or vertices of the set. When you release the rubber band, it automatically encompasses the entire set in the smallest perimeter possible, and this is essentially what a convex hull does.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F11_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.11 The convex hull problem. a) A valid convex hull that encloses all points while maximizing the area and minimizing the circumference. Note that the number of points included in the output sequence of the polygon may be smaller than the number of given points. b) An invalid convex hull, as the circumference is not minimized. c) An invalid convex hull, as not all the points are enclosed.</p>
  </div>

  <p class="body"><a id="marker-420"/>Convex hulls have a multitude of applications across a variety of disciplines. For example, in the field of image recognition, convex hulls can help determine the shape and boundary of objects within an image. Similarly, in robotics, they can assist in obstacle detection and navigation by defining the “reachable” space around a robot.</p>

  <p class="body">The problem of finding or computing a convex hull, given a set of points, has been addressed through various algorithms. For example, the Graham scan algorithm sorts the points according to their angle with the point at the bottom of the hull and then processes them to find the convex hull [11]. The Jarvis march (or the gift wrapping algorithm) starts with the leftmost point and wraps the remaining points like wrapping a gift [12]. The quickhull algorithm finds the convex hull of a point set by recursively dividing the set into subsets, selecting the point farthest from the line between two extreme points, and eliminating points within the formed triangles until the hull’s vertices are identified [13].</p>

  <p class="body">As shown in figure 11.12, Ptr-Net takes as input a planar set of points <span class="times"><i class="fm-italics">P</i> = {<i class="fm-italics">P</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">P</i><sub class="fm-subscript">2</sub>, …, <i class="fm-italics">P<sub class="fm-subscript">n</sub></i>}</span> with <i class="timesitalic">n</i> elements each, where <span class="times"><i class="fm-italics">P<sub class="fm-subscript">j</sub></i> = (<i class="fm-italics">x<sub class="fm-subscript">j</sub>, y<sub class="fm-subscript">j</sub></i>)</span> are the Cartesian coordinates of the points. The outputs <span class="times"><i class="fm-italics">C<sub class="fm-subscript">P</sub></i> = {<i class="fm-italics">C</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">C</i><sub class="fm-subscript">2</sub>,…, <i class="fm-italics">C<sub class="fm-subscript">m</sub></i><sub class="fm-subscript">(</sub><i class="fm-italics"><sub class="fm-subscript">P</sub></i><sub class="fm-subscript">)</sub>}</span> are sequences representing the solution associated with the point set <i class="timesitalic">P</i>. In this figure, Ptr-Net estimates the output sequence [1 4 2] from the input data points [1 2 3 4]. This output sequence represents the convex hull that includes all the input points with maximum area and minimum circumference. As can be seen, the convex hull is formed by connecting <span class="times"><i class="fm-italics">P</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">P</i><sub class="fm-subscript">2</sub></span>, and <span class="times"><i class="fm-italics">P</i><sub class="fm-subscript">4</sub></span>. The third point <span class="times"><i class="fm-italics">P</i><sub class="fm-subscript">3</sub></span> is inside this convex hull.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F12_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.12 Pointer network (Prt-Net) estimating the output sequence [1 4 2] from the input data points [1 2 3 4]</p>
  </div>

  <p class="body">Ptr-Net consists of three main components:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Encoder</i>—The encoder is a recurrent neural network (RNN), often implemented with long short-term memory (LSTM) units or gated recurrent units (GRUs). The encoder’s purpose is to process the input sequence, converting each input element into a corresponding hidden state. These hidden states (<span class="times"><i class="fm-italics">e</i><sub class="fm-subscript">1</sub>,…, <i class="fm-italics">e</i><sub class="fm-subscript">n</sub></span>) encapsulate the context-dependent representation of the elements in the input sequence.<a id="idIndexMarker097"/><a id="marker-421"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Decoder</i>—Like the encoder, the decoder is also an RNN. It’s responsible for generating the output sequence (<span class="times"><i class="fm-italics">d</i><sub class="fm-subscript">1</sub>,…, <i class="fm-italics">d</i><sub class="fm-subscript">m</sub></span>). For each output step, it takes the previous output and its own hidden state as inputs.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Attention mechanism (pointer)</i>—The attention mechanism in a Ptr-Net operates as a pointer. It computes a distribution over the hidden states output by the encoder, indicating where to “point” in the input sequence for each output step. Essentially, it decides which of the inputs should be the next output. The attention mechanism is a softmax function over the learned attention scores, which gives a probability distribution over the input sequence, signifying the likeliness of each element being pointed at.</p>
    </li>
  </ul>

  <p class="body">The attention vector at each output time <i class="timesitalic">i</i> is computed using the following equations:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH11_F12_Khamis-EQ03.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">11.3</p>
        </td>
      </tr>
    </tbody>
  </table>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH11_F12_Khamis-EQ04.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">11.4</p>
        </td>
      </tr>
    </tbody>
  </table>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH11_F12_Khamis-EQ05.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">11.5</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">where</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="timesitalic">u<sub class="fm-subscript">j</sub></i> is the attention vector or alignment score that represents the similarity between the decoder and encoder hidden states. <span class="times"><i class="fm-italics">v, W</i><sub class="fm-subscript">1</sub></span>, and <span class="times"><i class="fm-italics">W</i><sub class="fm-subscript">2</sub></span> are learnable parameters of the model. If the same hidden dimensionality is used for the encoder and decoder (typically 512), <i class="timesitalic">v</i> is a vector, and <span class="times"><i class="fm-italics">W</i><sub class="fm-subscript">1</sub></span> and <span class="times"><i class="fm-italics">W</i><sub class="fm-subscript">2</sub></span> are square matrices.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="timesitalic">a<sub class="fm-subscript">j</sub></i> is the attention mask over the input or weights computed by applying the softmax operation to the alignment scores.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="times"><i class="fm-italics">d<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">’</sup></span> is the context vector that is fed into the decoder at each time step. In other words, <i class="timesitalic">d<sub class="fm-subscript">i</sub></i> and <span class="times"><i class="fm-italics">d<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">’</sup></span> are concatenated and used as the hidden states from which the predictions are made. This weighted sum of all the encoder hidden states allows the decoder to flexibly focus the attention on the most relevant parts of the input sequence.</p>
    </li>
  </ul>

  <p class="body">Ptr-Net can process variable-length sequences and solve complex combinatorial problems, especially those involving sorting or ordering tasks, where the output is a permutation of the input, as you will see in section 11.9.</p>

  <h2 class="fm-head" id="heading_id_9">11.4 Self-organizing maps</h2>

  <p class="body"><a id="marker-422"/>The <i class="fm-italics">self-organizing map</i> (SOM), also known as a <i class="fm-italics">self-organizing feature map</i> (SOFM) or <i class="fm-italics">Kohonen map</i>, is a type of artificial neural network (ANN) that is trained with unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a <i class="fm-italics">map</i>. SOMs are distinguished from traditional ANNs by the nature of their learning process, known as <i class="fm-italics">competitive learning</i>. In such algorithms, processing elements or neurons compete for the right to respond to a subset of the input data. The degree to which an output neuron is activated is amplified as the similarity between the neuron’s weight vector and the input grows. The similarity between the weight vector and the input, leading to neuron activation, is commonly gauged through the calculation of Euclidean distance. The output unit that demonstrates the highest level of activation, or equivalently the shortest distance, in response to a specific input is deemed the best matching unit (BMU) or the “winning” neuron, as illustrated in figure 11.13. This winner is then drawn incrementally closer to the input data point by adjusting its weight.<a id="idIndexMarker098"/><a id="idIndexMarker099"/><a id="idIndexMarker100"/><a id="idIndexMarker101"/><a id="idIndexMarker102"/><a id="idIndexMarker103"/><a id="idIndexMarker104"/><a id="idIndexMarker105"/><a id="idIndexMarker106"/><a id="idIndexMarker107"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F13_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.13 Self-organizing map (SOM) with a Gaussian neighborhood function</p>
  </div>

  <p class="body">A key characteristic of SOM is the concept of a <i class="fm-italics">neighborhood function</i>, which ensures that not only the winning neuron but also its neighbors learn from each new input, creating clusters of similar data. This allows the network to preserve the topological properties of the input space. Equation 11.6 shows an example of a neighborhood function:<a id="idIndexMarker108"/><a id="marker-423"/></p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH11_F13_Khamis-EQ06.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">11.6</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">where <i class="timesitalic">v</i> is the index of the node in the map, <i class="timesitalic">u</i> is the index of the winning neuron, <span class="times"><i class="fm-italics">LDist</i>(<i class="fm-italics">u,v</i>)</span> represents the lattice distance between <i class="timesitalic">u</i> and <i class="timesitalic">v</i>, and <i class="timesitalic">σ</i> is the bandwidth of the Gaussian kernel. In SOMs, <i class="timesitalic">σ</i> represents the radius or width of the neighborhood and determines how far the influence of the winning neuron extends to its neighbors during the weight update phase. A large <i class="timesitalic">σ</i> means a broader neighborhood is affected. On the other hand, a small <i class="timesitalic">σ</i> means that fewer neighboring neurons are influenced. When <i class="timesitalic">σ</i> is set to an extremely small value, the neighborhood effectively shrinks to include only the winning neuron itself. This means that only the winning neuron’s weights are significantly updated in response to the input, while the weights of the other neurons are barely or not at all affected. This behavior, where only the winning neuron is updated, is referred to as “winner take all” learning.</p>

  <p class="body">Algorithm 11.1 shows the steps of SOM, assuming that <i class="timesitalic">D<sub class="fm-subscript">t</sub></i> is a target input data vector, <i class="timesitalic">W<sub class="fm-subscript">v</sub></i> is the current weight vector of node <span class="times"><i class="fm-italics">v, θ</i>(<i class="fm-italics">u</i>,<i class="fm-italics">v</i>,<i class="fm-italics">s</i>)</span> is the neighborhood function that represents the restraint due to the distance from the winning neuron, and <i class="timesitalic">α</i> is a learning rate where <span class="times"><i class="fm-italics">α</i> <span class="cambria">∈</span> (0,1)</span>.</p>

  <p class="fm-code-listing-caption">Algorithm 11.1 Self-organizing map (SOM)</p>
  <pre class="programlisting">Randomly initialize the weights of each neuron
For each step s=1 to iteration limit:
    Randomly pick an input vector from the dataset
        Traverse each node in the map
            Calculate Euclidean distance as a similarity measure
            Determine the node that produces the smallest distance (winning neuron)
        Adapt the weights of each neuron v according to the following rule
        W<sub class="fm-subscript">v</sub>(s+1)=W<sub class="fm-subscript">v</sub>(s)+ α(s).<span class="cambria">θ</span>(u,v,s).<span class="cambria">‖</span>D<sub class="fm-subscript">t</sub>-W<sub class="fm-subscript">v</sub>(s)<span class="cambria">‖</span></pre>

  <p class="body">SOMs were initially used as a dimensionality reduction method for data visualization and clustering tasks. For example, the neural phonetic typewriter was one of the early applications of Kohonen’s SOM algorithm. It was a system where spoken phonemes (the smallest unit of speech that can distinguish one word from another) were recognized and converted into symbols. When someone spoke into the system, the SOM would classify the input phoneme and type the corresponding symbol. SOMs can be applied to different problems such as feature extraction, adaptive control, and travelling salesman problems (see section 11.8).</p>

  <p class="body"><a id="marker-424"/>SOMs offer a significant advantage in that they preserve the relative distances between points as calculated within the input space. Points that are close in the input space are mapped onto neighboring units within the SOM, making SOMs effective tools for analyzing clusters within high-dimensional data. When using techniques like principal component analysis (PCA) to handle high-dimensional data, data loss may occur when reducing the dimensions to two. If the data contains numerous dimensions and if each dimension carries valuable information, then SOMs can be superior to PCA for dimensionality reduction purposes. Beyond this, SOMs also possess the ability to generalize. Through this process, the network can identify or categorize input data that it has not previously encountered. This new input is associated with a specific unit on the map and is thus mapped accordingly. <a id="idIndexMarker109"/><a id="idIndexMarker110"/></p>

  <p class="body">The previous sections have offered a fundamental foundation in ML, equipping you with essential background knowledge. The upcoming sections will delve deeply into the practical applications of supervised and unsupervised ML in tackling optimization problems.<a id="idIndexMarker111"/><a id="idIndexMarker112"/><a id="idIndexMarker113"/></p>

  <h2 class="fm-head" id="heading_id_10">11.5 Machine learning for optimization problems</h2>

  <p class="body">The utilization of ML techniques to tackle combinatorial optimization problems represents an emergent and exciting field of study. <i class="fm-italics">Neural combinatorial optimization</i> refers to the application of ML and neural network models, specifically seq2seq supervised models, unsupervised models, and reinforcement learning, to solve combinatorial optimization problems. Within this context, the application of ML to combinatorial optimization has been comprehensively described by Yoshua Bengio and his co-authors [14]. The authors depict three distinctive methods for harnessing ML for combinatorial optimization (see figure 11.14):<a id="idIndexMarker114"/><a id="idIndexMarker115"/><a id="idIndexMarker116"/><a id="idIndexMarker117"/><a id="idIndexMarker118"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F14_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.14 Machine learning (ML) for combinatorial optimization (CO) problems</p>
  </div>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">End-to-end learning</i>—To use ML to address optimization problems, we need to instruct the ML model to formulate solutions directly from the input instance. An example of this approach is Ptr-Net, which is trained on <i class="timesitalic">m</i> points and validated on <i class="timesitalic">n</i> points for a Euclidean planar symmetric TSP [10]. Examples of solving combinatorial optimization problems using end-to-end learning are provided in sections 11.6, 11.7, and 11.9.<a id="idIndexMarker119"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Learning to configure algorithms</i>—The second method involves applying an ML model to enhance a combinatorial optimization algorithm with pertinent information. In this regard, ML can offer a parameterization of the algorithm. Examples of such parameters comprise, but are not restricted to, learning rate or step size in gradient descent methodologies; initial temperature or cooling schedule in simulated annealing; standard deviation of Gaussian mutation or selective crossover in genetic algorithms; inertia weight or cognitive and social acceleration coefficients in particle swarm optimization (PSO); or rate of evaporation, influence of pheromone deposition, or influence of the desirability of state transition in ant colony optimization (ACO).<a id="idIndexMarker120"/><a id="marker-425"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">ML in conjunction with optimization algorithms</i>—The third method calls for a combinatorial optimization algorithm to repetitively consult the same ML model for decision-making purposes. The ML model accepts as input the current state of the algorithm, which could encompass the problem definition. The fundamental distinction between this approach and the other two lies in the repeated utilization of the same ML model by the combinatorial optimization algorithm to make identical kinds of decisions, approximately as many times as the total number of iterations of the algorithm. An example of this approach is DL-assisted heuristic tree search (DLTS), which consists of a heuristic tree search in which decisions about which branches to explore and how to bound nodes are made by deep neural networks (DNNs) [15].<a id="idIndexMarker121"/><a id="idIndexMarker122"/><a id="idIndexMarker123"/></p>
    </li>
  </ul>

  <p class="body">Another intriguing research paper by Vesselinova et al. delves into some pertinent questions concerning the intersection of ML and combinatorial optimization [16]. Specifically, the paper investigates the following questions:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Can ML techniques be utilized to automate the process of learning heuristics for combinatorial optimization tasks and, as a result, solve these problems more efficiently?</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">What essential ML methods have been employed to tackle these real-world problems?</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">How applicable are these methods to practical domains?</p>
    </li>
  </ul>

  <p class="body">This paper offers a thorough survey of various applications of supervised and reinforcement learning strategies in tackling optimization problems. The authors analyze these learning approaches by examining their application to a range of optimization problems:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The knapsack problem (KP), where the goal is to maximize the total value of items chosen without exceeding the capacity of the knapsack<a id="idIndexMarker124"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The maximal clique (MC) and maximal independent set (MIS) problems, which both involve identifying subsets of a graph with specific properties<a id="idIndexMarker125"/><a id="idIndexMarker126"/><a id="marker-426"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The maximum coverage problem (MCP), which requires selecting a subset of items to maximize coverage<a id="idIndexMarker127"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The maximum cut (MaxCut) and minimum vertex cover (MVC) problems, which involve partitioning a graph in particular ways<a id="idIndexMarker128"/><a id="idIndexMarker129"/></p>
    </li>
  </ul>

  <p class="body">In addition, the paper discusses the application of ML approaches to the satisfiability problem (SAT), which is a decision problem involving Boolean logic; the classic TSP, which requires finding the shortest possible route that visits a given set of cities and returns to the origin city; and the vehicular routing problem (VRP), which is a generalized version of TSP where multiple “salesmen” (vehicles) are allowed. More information about benchmark optimization problems is provided in appendix B. <a id="idIndexMarker130"/><a id="idIndexMarker131"/></p>

  <p class="body">Optimization by prompting (OPRO) is described in Chengrun et al.’s “Large Language Models as Optimizers” article as a simple and effective approach to using large language models (LLMs) as optimizers, where the optimization task is described in natural language [17]. Additional examples showcasing the use of ML in addressing optimization problems can be accessed through the AI for Smart Mobility publication hub (<a class="url" href="https://medium.com/ai4sm">https://medium.com/ai4sm</a>). To stimulate further exploration and draw more researchers into this emerging domain, a competition named Machine Learning for Combinatorial Optimization (ML4CO) was organized as part of the Neural Information Processing Systems (NeurIPS) conference. The competition posed a unique proposition for participants, requiring them to devise ML models or algorithms targeted at resolving three separate challenges. Each of these challenges mirrors a specific control task that commonly emerges in conventional optimization solvers. This competition provides a platform where researchers can explore and test novel ML strategies, contributing to the advancement of the field of combinatorial optimization.<a id="idIndexMarker132"/><a id="idIndexMarker133"/><a id="idIndexMarker134"/><a id="idIndexMarker135"/><a id="idIndexMarker136"/><a id="idIndexMarker137"/><a id="idIndexMarker138"/><a id="idIndexMarker139"/></p>

  <h2 class="fm-head" id="heading_id_11">11.6 Solving function optimization using supervised machine learning</h2>

  <p class="body"><i class="fm-italics">Amortized optimization</i>, or <i class="fm-italics">learning to optimize</i>, is an approach where ML models are used to rapidly predict the solutions to an optimization problem. Amortized optimization methods try to learn the mapping between the decision variable space and the optimal or near-optimal solution space. The learned model can be used to predict the optimal value of an objective function, enabling fast solvers. The computation cost of the optimization process is spread out between learning and inferencing. This is the reason for the name “amortized optimization,” as the word “amortization” generally refers to spreading out costs.<a id="idIndexMarker140"/><a id="idIndexMarker141"/><a id="idIndexMarker142"/><a id="idIndexMarker143"/><a id="idIndexMarker144"/><a id="idIndexMarker145"/><a id="marker-427"/></p>

  <p class="body">B. Amos shows several examples of how to use amortized optimization to solve optimization problems in his tutorial [18]. For example, a supervised ML approach can learn to solve optimization problems over spheres. Here the objective is to find the extreme values of a function defined on the earth or other space that can be approximated with a sphere of the form</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH11_F14_Khamis-EQ07.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">11.7</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">where <i class="fm-italics">S</i><sup class="fm-superscript">2</sup> is the surface of the unit 2-sphere embedded in real-number space <span class="times"><i class="fm-italics">R</i><sup class="fm-superscript">3</sup> as <i class="fm-italics">S</i><sup class="fm-superscript">2</sup>:= {<i class="fm-italics">y</i> <span class="cambria">∈</span> <i class="fm-italics">R</i><sup class="fm-superscript">3</sup> | ||<i class="fm-italics">y</i>||<sub class="fm-subscript">2</sub> =1}</span>, and <i class="timesitalic">x</i> is some parameterization of the function <span class="times"><i class="fm-italics">f</i> : <i class="fm-italics">S</i><sup class="fm-superscript">2</sup> × <i class="fm-italics">X</i> <span class="cambria">→</span> <i class="fm-italics">R</i>. ||<i class="fm-italics">y</i>||<sub class="fm-subscript">2</sub></span> refers to the Euclidean norm (also known as the <i class="timesitalic">L2 norm</i> or <i class="fm-italics">2-norm</i>) of a vector <i class="timesitalic">y</i>. More details about the amortization objective function are available in Amos’s “Tutorial on amortized optimization for learning to optimize over continuous domains” [18].<a id="idIndexMarker146"/><a id="idIndexMarker147"/></p>

  <p class="body">Listing 11.3 shows the steps for applying amortized optimization based on supervised learning to solve the problem of finding the extreme values of a function defined on the earth or other spaces. We’ll start by defining two conversion functions, <code class="fm-code-in-text">celestial_to_euclidean()</code> and <code class="fm-code-in-text">euclidean_to_celestial()</code>, that convert between celestial coordinates (right ascension, <code class="fm-code-in-text">ra</code>, and declination, <code class="fm-code-in-text">dec</code>) and Euclidean coordinates (<code class="fm-code-in-text">x, y, z</code>). <a id="idIndexMarker148"/><a id="idIndexMarker149"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">The celestial coordinate system</p>

    <p class="fm-sidebar-text">The <i class="fm-italics">astronomical</i> or <i class="fm-italics">celestial coordinate system</i> is a reference system used to specify the positions of objects in the sky, such as satellites, stars, planets, galaxies, and other celestial bodies. There are several celestial coordinate systems, with the most common being the equatorial system. In the equatorial system, right ascension (RA) and declination (Dec) are the two numbers used to fix the location of an object in the sky. These coordinates are analogous to the latitude and longitude used in earth’s geographic coordinate system. <a id="idIndexMarker150"/><a id="marker-428"/></p>

    <p class="fm-sidebar-text">As shown in the following figure, RA is measured in hours, minutes, and seconds (h:m:s), and it is analogous to longitude in earth’s coordinate system. RA is the angular distance of an object measured eastward along the celestial equator from the vernal equinox (the point where the sun crosses the celestial equator during the March equinox). The celestial equator is an imaginary great circle on the celestial sphere, lying in the same plane as earth’s equator. Dec is measured in degrees and represents the angular distance of an object north or south of the celestial equator. It is analogous to latitude in earth’s coordinate system.</p>

    <p class="sidebarafigures"><img alt="" class="calibre2" src="../Images/CH11_F14_UN01_Khamis.png"/></p>

    <p class="sidebaracaptions">Celestial coordinate system with an example point with a right ascension of 10 hours and a declination of 30 degrees</p>

    <p class="fm-sidebar-text">Positive declination is used for objects above the celestial equator, and negative declination is used for objects below the celestial equator.</p>
  </div>

  <p class="body">The <code class="fm-code-in-text">sphere_dist(x, y)</code> function calculates the Riemannian distance (the great- circle distance) between two points on the sphere in the Euclidean space. This distance represents the shortest (geodesic) path between two points on the surface of a sphere, measured along the surface rather than through the interior of the sphere. The function asserts that the input vectors are two-dimensional. Then it calculates the dot product of <i class="timesitalic">x</i> and <i class="timesitalic">y</i> and returns the arccosine of the result, which corresponds to the angle between <i class="timesitalic">x</i> and <i class="fm-italics">y</i>.<a id="idIndexMarker151"/></p>

  <p class="fm-code-listing-caption">Listing 11.3 Solving a function optimization problem using supervised learning</p>
  <pre class="programlisting">import torch
from torch import nn
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
  
def celestial_to_euclidean(ra, dec):             <span class="fm-combinumeral">①</span>
    x = np.cos(dec)*np.cos(ra)
    y = np.cos(dec)*np.sin(ra)
    z = np.sin(dec)
    return x, y, z
  
def euclidean_to_celestial(x, y, z):             <span class="fm-combinumeral">②</span>
    sindec = z
    cosdec = (x*x + y*y).sqrt()
    sinra = y / cosdec
    cosra = x / cosdec
    ra = torch.atan2(sinra, cosra)
    dec = torch.atan2(sindec, cosdec)
    return ra, dec
  
def sphere_dist(x,y):                            <span class="fm-combinumeral">③</span>
    if x.ndim == 1:
        x = x.unsqueeze(0)
    if y.ndim == 1:
        y = y.unsqueeze(0)
    assert x.ndim == y.ndim == 2
    inner = (x*y).sum(-1)
    return torch.arccos(inner)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Convert from celestial coordinates to Euclidean coordinates.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Convert from Euclidean coordinates to celestial coordinates.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculate the Riemannian distance between two points on the sphere.</p>

  <p class="body"><a id="marker-429"/>We then define a <code class="fm-code-in-text">c-convex</code> class as a subclass of <code class="fm-code-in-text">nn.Module</code>, which makes it a trainable model in PyTorch. Cohen and his co-authors defined <i class="fm-italics">c-convex</i> in their “Riemannian convex potential maps” article as a synthetic class of optimization problems defined on the sphere [19]. The <code class="fm-code-in-text">c-convex</code> class models a c-convex function on the sphere with <code class="fm-code-in-text">n_components</code> components that we can sample data from for training. The <code class="fm-code-in-text">gamma</code> parameter controls the aggregation of the components of the function, and <code class="fm-code-in-text">seed</code> is used to initialize the random number generator for reproducibility. It also generates random parameters <code class="fm-code-in-text">ys</code> (which are unit vectors in the 3D space) and <code class="fm-code-in-text">alphas</code> (which are scalars between 0 and 0.7) for each component of the c-convex function. The parameters are concatenated into a single <code class="fm-code-in-text">params</code> vector. The <code class="fm-code-in-text">forward(xyz)</code> method calculates the value of the c-convex function at the point <code class="fm-code-in-text">xyz</code>:<a id="idIndexMarker152"/><a id="idIndexMarker153"/></p>
  <pre class="programlisting">class c_convex(nn.Module):                                                <span class="fm-combinumeral">①</span>
    def __init__(self, n_components=4, gamma=0.5, seed=None):
        super().__init__()
        self.n_components = n_components
        self.gamma = gamma
        
        if seed is not None:                                              <span class="fm-combinumeral">②</span>
            torch.manual_seed(seed)                                       <span class="fm-combinumeral">②</span>
        self.ys = torch.randn(n_components, 3)                            <span class="fm-combinumeral">②</span>
        self.ys = self.ys / torch.norm(self.ys, 2, dim=-1, keepdim=True)  <span class="fm-combinumeral">②</span>
        self.alphas = .7*torch.rand(self.n_components)                    <span class="fm-combinumeral">②</span>
        self.params = torch.cat((self.ys.view(-1), self.alphas.view(-1))) <span class="fm-combinumeral">②</span>
  
    def forward(self, xyz):                                               <span class="fm-combinumeral">③</span>
        cs = []                                                           <span class="fm-combinumeral">③</span>
        for y, alpha in zip(self.ys, self.alphas):                        <span class="fm-combinumeral">③</span>
            ci = 0.5*sphere_dist(y, xyz)**2 + alpha                       <span class="fm-combinumeral">③</span>
            cs.append(ci)
        cs = torch.stack(cs)
        if self.gamma == None or self.gamma == 0.:
            z = cs.min(dim=0).values 
        else:
            z = -self.gamma*(-cs/self.gamma).logsumexp(dim=0) 
        return z</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Define a c-convex function.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sample random parameters.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Computes the output of the c-convex function given input coordinates xyz on the sphere.</p>

  <p class="body">As a continuation of the preceding code, we define an amortized model, which takes a parameter vector as input and outputs a 3D vector representing a point on the sphere. The amortized model uses a neural network to learn a mapping from the parameter space to the 3D space of points on the sphere. The code also initializes a list of <code class="fm-code-in-text">c_convex</code> objects with different seeds and sets the number of parameters for the amortized model:<a id="marker-430"/><a id="idIndexMarker154"/></p>
  <pre class="programlisting">seeds = [8,9,2,31,4,20,16,7]           <span class="fm-combinumeral">①</span>
fs = [c_convex(seed=i) for i in seeds] <span class="fm-combinumeral">②</span>
n_params = len(fs[0].params)           <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Create a list of integers representing different seeds.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Create an fs list that contains different instances of the c_convex class.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Set the number of parameters in the first c_convex object (fs[0]).</p>

  <p class="body">The amortized model is represented as <code class="fm-code-in-text">nn.Module</code> in the following code. The neural network is defined as a feedforward neural network or a multilayer perceptron that consists of three fully connected (linear) layers with ReLU activation functions:<a id="idIndexMarker155"/></p>
  <pre class="programlisting">class AmortizedModel(nn.Module):
    def __init__(self, n_params):            <span class="fm-combinumeral">①</span>
        super().__init__()
        self.base = nn.Sequential(
  
            nn.Linear(n_params, n_hidden),
            nn.ReLU(inplace=True),
            nn.Linear(n_hidden, n_hidden),
            nn.ReLU(inplace=True),
            nn.Linear(n_hidden, 3)
        )                                    <span class="fm-combinumeral">②</span>
  
    def forward(self, p):                    <span class="fm-combinumeral">③</span>
        squeeze = p.ndim == 1
        if squeeze:
            p = p.unsqueeze(0)
        assert p.ndim == 2
        z = self.base(p)
        z = z / z.norm(dim=-1, keepdim=True)
        if squeeze:
            z = z.squeeze(0)
        return z</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Number of parameters in the c-convex function that will be used as input to the neural network</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Define the layers of the neural network in sequence.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Define the forward pass of the amortized model, which maps the input p (parameter vector) to a point on the sphere.</p>

  <p class="body">We can now train the amortized model to learn a mapping from parameter vectors to points on the sphere. It uses a list of <span class="fm-code-in-text">c_convex</span> functions (<span class="fm-code-in-text">fs</span>) with different random seeds to generate training data. The amortized model is trained using an <span class="fm-code-in-text">Adam</span> optimizer, and its progress is visualized using a <span class="fm-code-in-text">tqdm</span> progress bar. The resulting output points on the sphere are stored in a tensor <span class="fm-code-in-text">xs</span>:<a id="idIndexMarker156"/><a id="marker-431"/><a id="idIndexMarker157"/><a id="idIndexMarker158"/></p>
  <pre class="programlisting">n_hidden = 128                                               <span class="fm-combinumeral">①</span>
torch.manual_seed(0)                                         <span class="fm-combinumeral">②</span>
model = AmortizedModel(n_params=n_params)                    <span class="fm-combinumeral">③</span>
opt = torch.optim.Adam(model.parameters(), lr=5e-4)          <span class="fm-combinumeral">④</span>
  
xs = []                                                      <span class="fm-combinumeral">⑤</span>
num_iterations = 100
  
pbar = tqdm(range(num_iterations), desc="Training Progress")
  
for i in pbar:                                               <span class="fm-combinumeral">⑥</span>
    losses = []                                              <span class="fm-combinumeral">⑦</span>
    xis = []                                                 <span class="fm-combinumeral">⑦</span>
    for f in fs:                                             <span class="fm-combinumeral">⑧</span>
        pred_opt = model(f.params)
        xis.append(pred_opt)
        losses.append(f(pred_opt))
    with torch.no_grad():                                    <span class="fm-combinumeral">⑧</span>
        xis = torch.stack(xis)
        xs.append(xis)
    loss = sum(losses)
  
    opt.zero_grad()
    loss.backward()
    opt.step()
  
    pbar.set_postfix({"Loss": loss.item()})
  
xs = torch.stack(xs, dim=1)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Set the number of hidden units for the AmortizedModel neural network.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Set the random seed to ensure the reproducibility of the training process.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Create an instance of the AmortizedModel.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Create an Adam optimizer to update the parameters with a learning rate of 0.0005.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Store the output points on the sphere for each iteration of training.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Training loop</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Store the losses for each c_convex function and the corresponding output points on the sphere (xis).</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Iterate over each c_convex function (f) in the list fs.</p>

  <p class="body">After training is complete, all the predicted output points on the sphere are stacked along a new dimension, resulting in a tensor <code class="fm-code-in-text">xs</code> with the following shape: number of iterations, number of <code class="fm-code-in-text">c_convex</code> functions, 3. Each element in this tensor represents a point on the sphere predicted by the amortized model at different stages of training. It generates a visual representation of the training progress for the amortized model and <code class="fm-code-in-text">c_convex</code> functions, as shown in figure 11.15.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F15_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.15 Examples of output from the trained amortized model</p>
  </div>

  <p class="body"><a id="marker-432"/>The complete version of listing 11.3 is available in the book’s GitHub repo. It creates a grid of celestial coordinates, evaluates the <code class="fm-code-in-text">c_convex</code> functions and the amortized model on this grid, and then plots contour maps of the functions, the predicted paths, and the optimal points on the sphere. The optimal points are the points that give minimum loss, given that supervised learning is used to train the amortized model.<a id="idIndexMarker159"/><a id="idIndexMarker160"/><a id="idIndexMarker161"/><a id="idIndexMarker162"/><a id="idIndexMarker163"/><a id="idIndexMarker164"/><a id="idIndexMarker165"/></p>

  <h2 class="fm-head" id="heading_id_12">11.7 Solving TSP using supervised graph machine learning</h2>

  <p class="body">Joshi, Laurent, and Bresson, in their “Graph Neural Networks for the Travelling Salesman Problem” article [20], proposed a generic end-to-end pipeline to tackle combinatorial optimization problems such as the traveling salesman problem (TSP), vehicle routing problem (VRP), satisfiability problem (SAT), maximum cut (MaxCut), and maximal independent set (MIS). Figure 11.16 shows the steps of solving TSP using ML. <a id="idIndexMarker166"/><a id="idIndexMarker167"/><a id="idIndexMarker168"/><a id="idIndexMarker169"/><a id="idIndexMarker170"/><a id="idIndexMarker171"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F16_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.16 End-to-end pipeline for combinatorial optimization problems<a id="marker-433"/></p>
  </div>

  <p class="body">Following this approach, we start by defining the graph problem in the form of node features and an adjacency matrix between the nodes. A low-dimensional graph embedding is then generated using GNN or GCN, based on the message-passing approach. The probability of nodes or edges belonging to the solution is predicted using multilayer perceptrons (MLPs). A graph search, such as beam search (see chapter 4), is then applied to search the graph with the probability distribution over the edge to find a feasible candidate solution. Learning by imitation (supervised learning) and learning by exploration (reinforcement learning) are applied. Supervised learning minimizes the loss between optimal solutions (obtained by a well-known solver such as Concorde in the case of TSP) and the model’s prediction. The reinforcement learning approach uses a policy gradient to minimize the length of the tour predicted by the model at the end of decoding. Reinforcement learning is discussed in the next chapter.</p>

  <p class="body">Training an ML model from scratch and applying it to solve TSP requires a substantial amount of code and data preprocessing. Listing 11.4 shows how you can use the pretrained models to solve different instances of TSP. We start by importing the libraries and modules we’ll use. These libraries provide functionality for handling data, performing computations, visualization, and optimization. The Gurobi library is used to eliminate subtours during optimization and to calculate the reduce costs for a set of points (see appendix A). We set the <code class="fm-code-in-text">CUDA_DEVICE_ORDER</code> and <code class="fm-code-in-text">CUDA_VISIBLE_DEVICES</code> environment variables to control the GPU device visibility.<a id="idIndexMarker172"/><a id="idIndexMarker173"/><a id="marker-434"/></p>

  <p class="fm-code-listing-caption">Listing 11.4 Solving TSP using supervised ML</p>
  <pre class="programlisting">import os
import math
import itertools
import numpy as np
import networkx as nx
from scipy.spatial.distance import pdist, squareform
import seaborn as sns
import matplotlib.pyplot as plt
  
import torch
from torch.utils.data import DataLoader
from torch.nn import DataParallel
  
from learning_tsp.problems.tsp.problem_tsp import TSP
from learning_tsp.utils import load_model, move_to
  
from gurobipy import *
  
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID" 
os.environ["CUDA_VISIBLE_DEVICES"] = "0" </pre>

  <p class="body">As a continuation, the following <code class="fm-code-in-text">opts</code> class contains several class-level attributes that define the following options and configurations:<a id="idIndexMarker174"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">dataset path</code>—The TSP dataset available in the book’s GitHub repo.<a id="idIndexMarker175"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">batch size</code>—This determines the number of TSP instances (problems) processed simultaneously during training or evaluation. It specifies how many TSP instances are grouped together and processed in parallel.<a id="idIndexMarker176"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">number of samples</code>—This is the number of samples per TSP size.<a id="idIndexMarker177"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">neighbors</code>—This is used in the TSP data processing pipeline to specify the proportion (percentage) of nearest neighbors to consider for graph sparsification. It controls the connectivity of the TSP graph by selecting a subset of the nearest neighbors for each node.<a id="idIndexMarker178"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">knn strategy</code>—This is the strategy used to determine the number of nearest neighbors when performing graph sparsification. In the code, the <code class="fm-code-in-text">'percentage'</code> value indicates that the number of nearest neighbors is determined by the <code class="fm-code-in-text">neighbors</code> parameter, which specifies the percentage of neighbors to consider.<a id="idIndexMarker179"/><a id="idIndexMarker180"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">model</code>—This is the path for the pretrained ML model. The model used is a pretrained GNN model available in the book’s GitHub repo. <a id="idIndexMarker181"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">use_cuda</code>—This checks if CUDA is available on the system. CUDA is a parallel computing platform and programming model that allows for efficient execution of computations on NVIDIA GPUs. <code class="fm-code-in-text">torch.cuda.is_available()</code> returns a Boolean value (true or false) indicating whether CUDA is available or not. If CUDA is available, that means a compatible NVIDIA GPU is present on the system and can be utilized for accelerated computations.<a id="idIndexMarker182"/><a id="marker-435"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">device</code>—This is the device to be used for computations:<a id="idIndexMarker183"/></p>
    </li>
  </ul>
  <pre class="programlisting">class opts:
    dataset_path = "learning_tsp/data/tsp20-50_concorde.txt"
    batch_size = 16
    num_samples = 1280 
    
    neighbors = 0.20
    knn_strat = 'percentage'
    
    model = 
<span class="fm-code-continuation-arrow">➥</span> "learning_tsp/pretrained/tspsl_20-50/sl-ar-var-20pnn-gnn-max_20200308T172931"
    
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda:0" if use_cuda else "cpu")</pre>

  <p class="body">The next step is to create a dataset object using the TSP class with the following parameters:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">filename</code>—The path or filename of the dataset to be used, specified by <code class="fm-code-in-text">opts .dataset_path</code><a id="idIndexMarker184"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">batch_size</code>—The number of samples to include in each batch, specified by <code class="fm-code-in-text">opts.batch_size</code><a id="idIndexMarker185"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">num_samples</code>—The total number of samples to include in the dataset, specified by <code class="fm-code-in-text">opts.num_samples</code><a id="idIndexMarker186"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">neighbors</code>—The value representing the number of nearest neighbors for graph sparsification, specified by <code class="fm-code-in-text">opts.neighbors</code><a id="idIndexMarker187"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">knn_strat</code>—The strategy for selecting nearest neighbors (<code class="fm-code-in-text">'percentage'</code> or <code class="fm-code-in-text">None</code>), specified by <code class="fm-code-in-text">opts.knn_strat</code><a id="idIndexMarker188"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">supervised</code>—A Boolean value indicating whether the dataset is used for supervised learning, set to <code class="fm-code-in-text">True</code><a id="idIndexMarker189"/></p>
    </li>
  </ul>

  <p class="body">The <code class="fm-code-in-text">make_dataset</code> method creates an instance of the TSP dataset class and initializes it with the provided arguments, returning the <code class="fm-code-in-text">dataset</code> object:<a id="idIndexMarker190"/></p>
  <pre class="programlisting">dataset = TSP.make_dataset(
    filename=opts.dataset_path, batch_size=opts.batch_size,
<span class="fm-code-continuation-arrow">➥</span> num_samples=opts.num_samples, 
<span class="fm-code-continuation-arrow">➥</span> neighbors=opts.neighbors, knn_strat=opts.knn_strat, supervised=True
)</pre>

  <p class="body">The following line creates a data loader object that enables convenient iteration over the dataset in batches, which is useful for processing the data during evaluation. The <code class="fm-code-in-text">dataset</code> object created in the previous line will be used as the source of the data. You can provide other optional arguments to customize the behavior of the data loader, such as <code class="fm-code-in-text">shuffle</code> (to shuffle the data) and <code class="fm-code-in-text">num_workers</code> (to specify the number of worker processes for data loading):<a id="idIndexMarker191"/><a id="idIndexMarker192"/><a id="marker-436"/></p>
  <pre class="programlisting">dataloader = DataLoader(dataset, batch_size=opts.batch_size, shuffle=False,
<span class="fm-code-continuation-arrow">➥</span> num_workers=0)</pre>

  <p class="body">We can now load the trained model and assign it to the <code class="fm-code-in-text">model</code> variable. If the model is wrapped in <code class="fm-code-in-text">torch.nn.DataParallel</code>, it extracts the underlying module by accessing <code class="fm-code-in-text">model.module</code>. <code class="fm-code-in-text">DataParallel</code> is a PyTorch wrapper that allows for parallel execution of models on multiple GPUs. If the model is indeed an instance of <code class="fm-code-in-text">DataParallel</code>, it extracts the underlying model module by accessing the <code class="fm-code-in-text">module</code> attribute. This step is necessary to ensure consistent behavior when accessing model attributes and methods. The decode type of the model is then set to <code class="fm-code-in-text">"greedy"</code>. This means that during inference or evaluation, the model should use a greedy decoding strategy to generate output predictions:<a id="idIndexMarker193"/><a id="idIndexMarker194"/><a id="idIndexMarker195"/><a id="idIndexMarker196"/></p>
  <pre class="programlisting">model, model_args = load_model(opts.model, extra_logging=True) <span class="fm-combinumeral">①</span>
model.to(opts.device)
  
if isinstance(model, DataParallel):                            <span class="fm-combinumeral">②</span>
    model = model.module                                       <span class="fm-combinumeral">②</span>
  
model.set_decode_type("greedy")                                <span class="fm-combinumeral">③</span>
  
model.eval()                                                   <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Load a pretrained model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Extract the underlying module.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Set the decoding type of the model to "greedy".</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Set the model’s mode to evaluation</p>

  <p class="body">The complete version of listing 11.4, including the visualization code, is available in the book’s GitHub repo. Figure 11.17 shows the output produced by the pretrained ML model for the TSP50 instance.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F17_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.17 The TSP50 solution using a pretrained ML model</p>
  </div>

  <p class="body"><a id="marker-437"/>The figure shows the following seven plots related to the TSP instance and the model’s predictions:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Concorde</i>—The plot in the upper-left corner shows the ground truth solution generated by the Concorde solver, which is an efficient implementation of the branch-and-cut algorithm for solving TSP instances to optimality. It shows the nodes of the TSP problem as circles connected by edges, representing the optimal tour calculated by Concorde. The title of the plot indicates the length (cost) of the tour obtained from Concorde.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">1 - (Reduce Costs)</i>—The second plot contains the shortest subtour and shows the reduced costs for the points in these subtours using the Gurobi optimization library. It displays the edges of the TSP as red lines, with the edge color indicating the reduced cost value.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Prediction Heatmap</i>—The third plot presents a heatmap visualization of the model’s predictions for the TSP problem. It uses a color scale to represent the prediction probabilities of edges, with higher probabilities shown in darker shades.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Greedy Solution</i>—The fourth plot illustrates the solution generated by the ML model using a greedy decoding strategy. It displays the nodes of the TSP problem connected by edges, representing the tour obtained from the model. The title of the plot shows the length (cost) of the tour calculated by the model.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Euclidean Distance (norm by max)</i>—The lower-left plot is a heatmap visualization of the Euclidean distances between nodes in the TSP problem. It uses a color scale to represent the distances, with lighter shades indicating smaller distances.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Reduce Costs</i>—The lower-middle plot is a heatmap representation of the reduced costs of edges in the TSP problem. It shows the reduced costs as a color scale, with lower values displayed in lighter shades.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">1 - (Model Predictions)</i>—The lower-right plot presents a heatmap visualization of the model’s predictions for the TSP problem, similar to the third plot. However, in this case, the heatmap displays “1 - (Model Predictions)” by subtracting the model’s prediction probabilities from 1. Darker shades represent lower probabilities, indicating stronger confidence in the edge selection.</p>
    </li>
  </ul>

  <p class="body">This example demonstrated how we can employ a pretrained GNN model for solving TSP. Figure 11.17 displays the model’s solution alongside the Concorde TSP solver’s results for a TSP instance comprising 50 points of interest. More information and complete code, including model training steps, are available in “Learning the Travelling Salesperson Problem Requires Rethinking Generalization” GitHub repo [21].<a id="idIndexMarker197"/><a id="idIndexMarker198"/><a id="idIndexMarker199"/><a id="idIndexMarker200"/><a id="idIndexMarker201"/><a id="idIndexMarker202"/></p>

  <h2 class="fm-head" id="heading_id_13">11.8 Solving TSP using unsupervised machine learning</h2>

  <p class="body"><a id="marker-438"/>As an example of an unsupervised ML approach, listing 11.5 shows how we can solve TSP using self-organizing maps (SOMs). We start by importing the libraries we’ll use. Some helper functions are imported from the som-tsp implementation described in Vicente’s blog post [22] to read the TSP instance, get the neighborhood, get the route, select the closest candidate, and calculate the route distance and plot the route. We read the TSP instance from the provided URL and obtain the cities and normalize their coordinates to a range of [0, 1].<a id="idIndexMarker203"/><a id="idIndexMarker204"/><a id="idIndexMarker205"/><a id="idIndexMarker206"/><a id="idIndexMarker207"/><a id="idIndexMarker208"/><a id="idIndexMarker209"/><a id="idIndexMarker210"/></p>

  <p class="fm-code-listing-caption">Listing 11.5 Solving TSP using unsupervised learning</p>
  <pre class="programlisting">import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML
import requests
import os
from tqdm import tqdm
  
from som_tsp.helper import read_tsp, normalize, get_neighborhood, get_route,
<span class="fm-code-continuation-arrow">➥</span> select_closest, route_distance, plot_network, plot_route
  
url = 'https://raw.githubusercontent.com/Optimization-Algorithms-Book/Code-
<span class="fm-code-continuation-arrow">➥</span>Listings/256207c4a8badc0977286c48a6e1cfd33237a51d/Appendix%20B/data/TSP/' <span class="fm-combinumeral">①</span>
  
tsp='qa194.tsp'                                                             <span class="fm-combinumeral">②</span>
  
response = requests.get(url+tsp)                                            <span class="fm-combinumeral">③</span>
response.raise_for_status()                                                 <span class="fm-combinumeral">③</span>
problem_text = response.text                                                <span class="fm-combinumeral">③</span>
with open(tsp, 'w') as file:                                                <span class="fm-combinumeral">③</span>
    file.write(problem_text)                                                <span class="fm-combinumeral">③</span>
  
problem = read_tsp(tsp)                                                     <span class="fm-combinumeral">④</span>
  
cities = problem.copy()                                                     <span class="fm-combinumeral">⑤</span>
cities[['x', 'y']] = normalize(cities[['x', 'y']])                          <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Define the URL where the TSP instances are located.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> TSP instance</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Download the file if it does not exist.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Read the TSP problem.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Obtain the normalized set of cities (with coordinates in [0,1]).</p>

  <p class="body"><a id="marker-439"/>We can now set up various parameters and initialize a network of neurons for the SOM:</p>
  <pre class="programlisting">number_of_neurons = cities.shape[0] * 8         <span class="fm-combinumeral">①</span>
  
iterations = 12000                              <span class="fm-combinumeral">②</span>
  
learning_rate=0.8                               <span class="fm-combinumeral">③</span>
network = np.random.rand(number_of_neurons, 2)  <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The population size is 8 times the number of cities.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Set the number of iterations.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Set the learning rate.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Generate an adequate network of neurons.</p>

  <p class="body">As a continuation, the following code snippet implements the training loop for SOM. This loop iterates over the specified number of training iterations using <code class="fm-code-in-text">tqdm</code> to show a progress bar:</p>
  <pre class="programlisting">route_lengths = []                                                        <span class="fm-combinumeral">①</span>
paths_x = []                                                              <span class="fm-combinumeral">②</span>
paths_y = []                                                              <span class="fm-combinumeral">②</span>
  
for i in tqdm(range(iterations)):                                         <span class="fm-combinumeral">③</span>
    if not i % 100:
        print('\t&gt; Iteration {}/{}'.format(i, iterations), end="\r")      <span class="fm-combinumeral">④</span>
  
    city = cities.sample(1)[['x', 'y']].values                            <span class="fm-combinumeral">⑤</span>
    winner_idx = select_closest(network, city)                            <span class="fm-combinumeral">⑥</span>
   
    gaussian = get_neighborhood(winner_idx, number_of_neurons // 10, 
<span class="fm-code-continuation-arrow">➥</span>  network.shape[0])                                                      <span class="fm-combinumeral">⑦</span>
  
    network += gaussian[:, np.newaxis] * learning_rate * (city - network) <span class="fm-combinumeral">⑧</span>
  
    paths_x.append(network[:, 0].copy())                                  <span class="fm-combinumeral">⑨</span>
    paths_y.append(network[:, 1].copy())                                  <span class="fm-combinumeral">⑨</span>
  
    learning_rate = learning_rate * 0.99997                               <span class="fm-combinumeral">⑩</span>
    number_of_neurons = number_of_neurons * 0.9997                        <span class="fm-combinumeral">⑩</span>
  
    if not i % 1000:                                                      <span class="fm-combinumeral">⑪</span>
        plot_network(cities, network, name='diagrams/{:05d}.png'.format(i))
  
    if number_of_neurons &lt; 1:                                             <span class="fm-combinumeral">⑫</span>
        print('Radius has completely decayed, finishing execution',
              <span class="fm-code-continuation-arrow">➥</span> 'at {} iterations'.format(i))
        break
    if learning_rate &lt; 0.001:
        print('Learning rate has completely decayed, finishing execution',
              <span class="fm-code-continuation-arrow">➥</span> 'at {} iterations'.format(i))
        break
    
    route = get_route(cities, network)                                    <span class="fm-combinumeral">⑬</span>
    problem = problem.reindex(route)                                      <span class="fm-combinumeral">⑬</span>
    distance = route_distance(problem)                                    <span class="fm-combinumeral">⑬</span>
    route_lengths.append(distance)                                        <span class="fm-combinumeral">⑬</span>
  
else:
    print('Completed {} iterations.'.format(iterations))                  <span class="fm-combinumeral">⑭</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Store the lengths of the TSP routes during the SOM training iterations.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Store the x and y coordinates of the neurons in the network during the training iterations.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Training loop</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Print only if the current iteration index is a multiple of 100.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Choose a random city.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Find the index of the neuron (winner) in the SOM network that is closest to the randomly chosen city.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Generate a filter that applies changes to the winner’s gaussian.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Update the network’s weights.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Append the current coordinates to the paths.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Decay the learning rate and the neighborhood radius n at each iteration to gradually reduce the influence of the Gaussian filter over time.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑪</span> Check for the plotting interval.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑫</span> Check if any parameter has completely decayed.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑬</span> Calculate distance, and store it in the route_lengths list.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑭</span> Indicate that the specified number of training iterations has been completed.</p>

  <p class="body">The following code snippet plots the route length in each iteration.</p>
  <pre class="programlisting">plt.figure(figsize=(8, 6))
plt.plot(range(len(route_lengths)), route_lengths, label='Route Length')
plt.xlabel('Iterations')
plt.ylabel('Route Length')
plt.title('Route Length per Iteration')
plt.grid(True)
plt.show()</pre>

  <p class="body"><a id="marker-440"/>Figure 11.18 shows the route length per iteration. The final route length is 9,816, and the optimal length for the Qatar TSP instance used, <code class="fm-code-in-text">qa194.tsp</code>, is 9,352. <a id="idIndexMarker211"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F18_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.18 Route length per iteration of SOM for the Qatar TSP. The final route length is 9,816, and the optimal solution is 9,352.</p>
  </div>

  <p class="body">The complete version of listing 11.5 is available in the book’s GitHub repo, and it contains an implementation based on MiniSom. MiniSom is a minimalistic and Numpy-based implementation of SOM. You can install this library using <code class="fm-code-in-text">!pip install minisom</code>. However, the route obtained by MiniSom is 11,844.47, which is far from the optimal length of 9,352 for this TSP instance. To improve the result, you can experiment with the provided code and try to tune SOM parameters such as the number of neurons, the sigma, the learning rate, and the number of iterations.<a id="idIndexMarker212"/><a id="idIndexMarker213"/><a id="idIndexMarker214"/><a id="idIndexMarker215"/><a id="idIndexMarker216"/><a id="idIndexMarker217"/><a id="idIndexMarker218"/><a id="idIndexMarker219"/></p>

  <h2 class="fm-head" id="heading_id_14">11.9 Finding a convex hull</h2>

  <p class="body"><a id="marker-441"/>Ptr-Net can be used to tackle the convex hull problem using a supervised learning approach, as described by Vinyals and his co-authors in their “Pointer networks” article [10]. Ptr-Net has two key components: an encoder and a decoder, as illustrated in figure 11.19. <a id="idIndexMarker220"/><a id="idIndexMarker221"/><a id="idIndexMarker222"/><a id="idIndexMarker223"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F19_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.19 Solving the convex hull problem using Ptr-Net. The output in each step is a pointer to the input that maximizes the probability distribution.</p>
  </div>

  <p class="body">The encoder, a recurrent neural network (RNN), converts the raw input sequence. In this case, it coordinates delineating the points for which we want to determine the convex hull into a more manageable representation. <a id="idIndexMarker224"/></p>

  <p class="body">This encoded vector is then passed on to the decoder. The vector acts as the modulator for a content-based attention mechanism, which is applied over the inputs. The content-based attention mechanism can be likened to a spotlight that highlights different segments of the input data at varying times, focusing on the most pertinent parts of the task at hand.</p>

  <p class="body">The output of this attention mechanism is a softmax distribution with a dictionary size equal to the length of the input. This softmax distribution gives probabilities to every point in the input sequence. This setup allows Ptr-Net to probabilistically decide at each step which point should be added next to the convex hull. This is determined based on the current state of the input and the network’s internal state. The training process is repeated until the network has made a decision for every point, yielding a complete resolution to the convex hull problem.</p>

  <p class="body">Listing 11.6 shows the steps for solving convex hull problem using pointer networks. We start by importing several necessary libraries and modules, such as torch, numpy, and matplotlib. The three helper classes <code class="fm-code-in-text">Data</code>, <code class="fm-code-in-text">ptr_net</code>, and <code class="fm-code-in-text">Disp</code> are imported based on the implementations provided in McGough’s “Pointer Networks with Transformers” article [23]. They contain functions for generating training and validation data, defining the pointer network architecture, and visualizing the results. This code generates two datasets for training and validation respectively. These datasets consist of random 2D points, where the number of points in each sample (the convex hull problem’s input) varies between <code class="fm-code-in-text">min_samples</code> and <code class="fm-code-in-text">max_samples</code>. <code class="fm-code-in-text">Scatter2DDataset</code> is a custom dataset class used to generate these random 2D point datasets.<a id="idIndexMarker225"/><a id="idIndexMarker226"/><a id="idIndexMarker227"/><a id="idIndexMarker228"/><a id="idIndexMarker229"/><a id="idIndexMarker230"/><a id="marker-442"/></p>

  <p class="fm-code-listing-caption">Listing 11.6 Solving a convex hull problem using pointer networks</p>
  <pre class="programlisting">import numpy as np
import torch
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from scipy.spatial import ConvexHull
  
from ptrnets.Data import display_points_with_hull, cyclic_permute, 
<span class="fm-code-continuation-arrow">➥</span> Scatter2DDataset,Disp_results
from ptrnets.ptr_net import ConvexNet, AverageMeter, masked_accuracy,
<span class="fm-code-continuation-arrow">➥</span> calculate_hull_overlap
  
min_samples = 5
max_samples = 50
n_rows_train = 100000
n_rows_val = 1000
  
torch.random.manual_seed(231)
train_dataset = Scatter2DDataset(n_rows_train, min_samples, max_samples)
val_dataset = Scatter2DDataset(n_rows_val, min_samples, max_samples)</pre>

  <p class="body">Running this code generates 100,000 training points and 1,000 validation points. We can then set the parameters of the pointer network. These parameters include a <code class="fm-code-in-text">TOKENS</code> dictionary containing the following tokens: <a id="idIndexMarker231"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">&lt;eos&gt;</code>—End-of-sequence token with the index 0</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">c_inputs</code>—Number of input features for the model</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">c_embed</code>—Number of embedding dimensions</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">c_hidden</code>—Number of hidden units in the model</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">n_heads</code>—Number of attention heads in the multi-head self-attention mechanism</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">n_layers</code>—Number of layers in the model</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">dropout</code>—Dropout probability, which is used for regularization</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">use_cuda</code>—A Boolean flag indicating whether to use CUDA (GPU) if available or CPU</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">n_workers</code>—Number of worker threads for data loading in DataLoader</p>
    </li>
  </ul>

  <p class="body">The training parameters include <code class="fm-code-in-text">n_epochs</code> (number of training epochs), <code class="fm-code-in-text">batch_size</code> (batch size used during training), <code class="fm-code-in-text">lr</code> (learning rate for the optimizer), and <code class="fm-code-in-text">log_interval</code> (interval for logging training progress). The code checks if CUDA (GPU) is available and sets the <code class="fm-code-in-text">device</code> variable accordingly:<a id="marker-443"/><a id="idIndexMarker232"/><a id="idIndexMarker233"/><a id="idIndexMarker234"/><a id="idIndexMarker235"/><a id="idIndexMarker236"/></p>
  <pre class="programlisting">TOKENS = {'&lt;eos&gt;': 0 } 
c_inputs = 2 + len(TOKENS)
c_embed = 16
c_hidden = 16
n_heads = 4
n_layers = 3
dropout = 0.0
use_cuda = True
n_workers = 2
n_epochs = 5
batch_size = 16
lr = 1e-3
log_interval = 500
device = torch.device("cuda" if torch.cuda.is_available() and use_cuda else "cpu")</pre>

  <p class="body">As a continuation, we load the training and validation data with the specified <code class="fm-code-in-text">batch_size</code> and <code class="fm-code-in-text">num_workers</code>:</p>
  <pre class="programlisting">train_loader = DataLoader(train_dataset, batch_size=batch_size,
<span class="fm-code-continuation-arrow">➥</span>  num_workers=n_workers)
  
val_loader = DataLoader(val_dataset, batch_size=batch_size,
<span class="fm-code-continuation-arrow">➥</span>  num_workers=n_workers)</pre>

  <p class="body">The <code class="fm-code-in-text">ConvexNet</code> model is a Ptr-Net model that is implemented as a transformer architecture with an encoder and decoder that use <code class="fm-code-in-text">nn.TransformerEncoderLayer</code> and apply multi-head self-attention. The complete code is available in the <code class="fm-code-in-text">ptr_net.py</code> class, available in the book’s GitHub repo. This model is initialized with the predefined hyperparameters. The <code class="fm-code-in-text">AverageMeter</code> class is used for keeping track of the average loss and accuracy during training and validation:<a id="idIndexMarker237"/><a id="idIndexMarker238"/><a id="idIndexMarker239"/><a id="idIndexMarker240"/></p>
  <pre class="programlisting">model = ConvexNet(c_inputs=c_inputs, c_embed=c_embed, n_heads=n_heads,
<span class="fm-code-continuation-arrow">➥</span>  n_layers=n_layers, dropout=dropout, c_hidden=c_hidden).to(device) <span class="fm-combinumeral">①</span>
  
optimizer = torch.optim.Adam(model.parameters(), lr=lr)               <span class="fm-combinumeral">②</span>
criterion = torch.nn.NLLLoss(ignore_index=TOKENS['&lt;eos&gt;'])            <span class="fm-combinumeral">③</span>
  
train_loss = AverageMeter()                                           <span class="fm-combinumeral">④</span>
train_accuracy = AverageMeter()                                       <span class="fm-combinumeral">④</span>
val_loss = AverageMeter()                                             <span class="fm-combinumeral">④</span>
val_accuracy = AverageMeter()                                         <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Create a ConvexNet model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Use the Adam optimizer for training the model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Use negative log-likelihood loss as the loss function.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Keep track of the average loss and accuracy during training and validation.</p>

  <p class="body"><a id="marker-444"/>We can now perform the training and evaluation loop for a model (<code class="fm-code-in-text">ConvexNet</code>) using PyTorch. The model is being trained on the <code class="fm-code-in-text">train_loader</code> dataset with known labels and evaluated on the <code class="fm-code-in-text">val_loader</code> dataset:<a id="idIndexMarker241"/><a id="idIndexMarker242"/><a id="idIndexMarker243"/></p>
  <pre class="programlisting">for epoch in range(n_epochs): 
  model.train()                                                         <span class="fm-combinumeral">①</span>
  for bat, (batch_data, batch_labels, batch_lengths) in enumerate(train_loader):                                                                <span class="fm-combinumeral">②</span>
    batch_data = batch_data.to(device)
    batch_labels = batch_labels.to(device)
    batch_lengths = batch_lengths.to(device)
  
  
    optimizer.zero_grad()                                               <span class="fm-combinumeral">③</span>
    log_pointer_scores, pointer_argmaxs = model(batch_data, batch_lengths, 
<span class="fm-code-continuation-arrow">➥</span>    batch_labels=batch_labels) 
    loss = criterion(log_pointer_scores.view(-1, log_pointer_scores.    <span class="fm-combinumeral">④</span>
<span class="fm-code-continuation-arrow">➥</span> shape[-1]), batch_labels.reshape(-1))
    
    assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'  <span class="fm-combinumeral">⑤</span>
  
    loss.backward()                                                     <span class="fm-combinumeral">⑥</span>
    optimizer.step()                                                    <span class="fm-combinumeral">⑥</span>
  
    train_loss.update(loss.item(), batch_data.size(0))                  <span class="fm-combinumeral">⑦</span>
    mask = batch_labels != TOKENS['&lt;eos&gt;']                              <span class="fm-combinumeral">⑦</span>
    acc = masked_accuracy(pointer_argmaxs, batch_labels, mask).item()   <span class="fm-combinumeral">⑦</span>
    train_accuracy.update(acc, mask.int().sum().item())                 <span class="fm-combinumeral">⑦</span>
  
    if bat % log_interval == 0:                                         <span class="fm-combinumeral">⑧</span>
      print(f'Epoch {epoch}: '
            f'Train [{bat * len(batch_data):9d}/{len(train_dataset):9d} '
            f'Loss: {train_loss.avg:.6f}\tAccuracy: {train_accuracy.avg:3.4%}')</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Train the model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Iterate over batches of training data.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Set the model’s parameters’ gradients to zero to avoid accumulation from previous batches.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Calculate the loss</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> A safeguard check to ensure that the loss value during the training process is not a NaN.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Perform a backward pass and optimization step.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Update training loss and accuracy.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Print the training progress.</p>

  <p class="body">As a continuation, the trained model (<span class="fm-code-in-text">model</span>) is evaluated on a validation dataset (<span class="fm-code-in-text">val_dataset</span>) to calculate the validation loss, accuracy, and overlap between the convex hull of the input data and the predicted pointer sequences. We start by setting the model to evaluation mode, where the model’s parameters are frozen and the batch normalization or dropout layers behave differently than during training. The code then iterates through the validation dataset using the <span class="fm-code-in-text">val_loader</span>, which provides batches of data (<span class="fm-code-in-text">batch_data</span>), ground truth labels (<span class="fm-code-in-text">batch_labels</span>), and the lengths of each sequence (<span class="fm-code-in-text">batch_lengths</span>):<a id="idIndexMarker244"/><a id="idIndexMarker245"/><a id="idIndexMarker246"/><a id="idIndexMarker247"/><a id="idIndexMarker248"/><a id="idIndexMarker249"/></p>
  <pre class="programlisting">model.eval()                                                         <span class="fm-combinumeral">①</span>
  hull_overlaps = []                                                 <span class="fm-combinumeral">②</span>
  for bat, (batch_data, batch_labels, batch_lengths) 
  <span class="fm-code-continuation-arrow">➥</span> in enumerate(val_loader):                                       <span class="fm-combinumeral">③</span>
    batch_data = batch_data.to(device)
    batch_labels = batch_labels.to(device)
    batch_lengths = batch_lengths.to(device)
    log_pointer_scores, pointer_argmaxs = model(batch_data, batch_lengths,
<span class="fm-code-continuation-arrow">➥</span>      batch_labels=batch_labels)                                   <span class="fm-combinumeral">④</span>
    loss = criterion(log_pointer_scores.view(-1, log_pointer_scores. <span class="fm-combinumeral">⑤</span>
<span class="fm-code-continuation-arrow">➥</span> shape[-1]),batch_labels.reshape(-1)) 
     
    assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'
    val_loss.update(loss.item(), batch_data.size(0))                  <span class="fm-combinumeral">⑥</span>
    mask = batch_labels != TOKENS['&lt;eos&gt;']                            <span class="fm-combinumeral">⑦</span>
    acc = masked_accuracy(pointer_argmaxs, batch_labels, mask).item() <span class="fm-combinumeral">⑧</span>
    val_accuracy.update(acc, mask.int().sum().item())                 <span class="fm-combinumeral">⑨</span>
    
for data, length, ptr in zip(batch_data.cpu(), batch_lengths.cpu(),
<span class="fm-code-continuation-arrow">➥</span>        pointer_argmaxs.cpu()):                                     <span class="fm-combinumeral">⑩</span>
      hull_overlaps.append(calculate_hull_overlap(data, length, ptr)) <span class="fm-combinumeral">⑪</span>
  print(f'Epoch {epoch}: Val\tLoss: {val_loss.avg:.6f} '
        f'\tAccuracy: {val_accuracy.avg:3.4%} '
        f'\tOverlap: {np.mean(hull_overlaps):3.4%}')                  <span class="fm-combinumeral">⑫</span>
  train_loss.reset()                                                  <span class="fm-combinumeral">⑬</span>
  train_accuracy.reset()                                              <span class="fm-combinumeral">⑬</span>
  val_loss.reset()                                                    <span class="fm-combinumeral">⑬</span>
  val_accuracy.reset()                                                <span class="fm-combinumeral">⑬</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Set the model to evaluation mode.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Initialize an empty list to store the overlap values between the convex hull of the input data and the predicted pointer sequences.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Iterate through the validation dataset.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Produce pointer scores and argmax predictions.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculate the validation loss.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Update the validation loss.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Ignore the loss contribution from positions where the &lt;eos&gt; token is present in batch_labels.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Calculate the masked accuracy.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Update the validation accuracy.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Iterate through each batch’s data, lengths, and pointer argmax predictions.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑪</span> Calculate the overlap between the convex hull of the input data and the predicted pointer sequences.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑫</span> Print the epoch-wise validation loss, accuracy, and mean overlap.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑬</span> Reset the metrics.</p>

  <p class="body"><a id="marker-445"/>You can display the results of training and validation losses and accuracies using the <code class="fm-code-in-text">Disp_results</code> helper function:<a id="idIndexMarker250"/></p>
  <pre class="programlisting">Disp_results(train_loss, train_accuracy, val_loss, val_accuracy, n_epochs)</pre>

  <p class="body">The preceding line of code will generate output like the following:</p>
  <pre class="programlisting">Best Scores:
train_loss: 0.0897 (ep: 9)
train_accuracy 96.61% (ep: 9)
val_loss: 0.0937 (ep: 7)
val_accuracy: 96.54% (ep: 7)</pre>

  <p class="body">After model training and validation, we can test the model. The following test function will evaluate a trained model (<code class="fm-code-in-text">model</code>) on a test dataset. The function evaluates the model’s accuracy and overlap with the convex hull for different test sample sizes. This test function takes as inputs the model, the number of test samples, and the number of points per sample. The code performs the test for different numbers of points per sample (<code class="fm-code-in-text">i</code>) by iterating from 5 to 45 in steps of 5. The <code class="fm-code-in-text">AverageMeter</code> class is used to keep track of average loss and accuracy during testing:<a id="idIndexMarker251"/><a id="idIndexMarker252"/><a id="marker-446"/></p>
  <pre class="programlisting">n_rows_test = 1000                                                            <span class="fm-combinumeral">①</span>
  
def test(model, n_rows_test, n_per_row):                                      <span class="fm-combinumeral">②</span>
  test_dataset = Scatter2DDataset(n_rows_test, n_per_row, n_per_row)          <span class="fm-combinumeral">③</span>
  test_loader = DataLoader(test_dataset, batch_size=batch_size,               <span class="fm-combinumeral">③</span>
   <span class="fm-code-continuation-arrow">➥</span> num_workers=n_workers)                                                  <span class="fm-combinumeral">③</span>
  
  test_accuracy = AverageMeter()
  hull_overlaps = []
  model.eval()
  
  for _, (batch_data, batch_labels, batch_lengths) in enumerate(test_loader): <span class="fm-combinumeral">④</span>
    batch_data = batch_data.to(device)
    batch_labels = batch_labels.to(device)
    batch_lengths = batch_lengths.to(device)
  
    _, pointer_argmaxs = model(batch_data, batch_lengths)
  
    val_loss.update(loss.item(), batch_data.size(0))                          <span class="fm-combinumeral">⑤</span>
    mask = batch_labels != TOKENS['&lt;eos&gt;']                                    <span class="fm-combinumeral">⑤</span>
    acc = masked_accuracy(pointer_argmaxs, batch_labels, mask).item()         <span class="fm-combinumeral">⑤</span>
    test_accuracy.update(acc, mask.int().sum().item())                        <span class="fm-combinumeral">⑤</span>
  
    for data, length, ptr in zip(batch_data.cpu(), batch_lengths.cpu(),       <span class="fm-combinumeral">⑥</span>
       <span class="fm-code-continuation-arrow">➥</span> pointer_argmaxs.cpu()):                                             <span class="fm-combinumeral">⑥</span>
      hull_overlaps.append(calculate_hull_overlap(data, length, ptr))         <span class="fm-combinumeral">⑥</span>
  
  print(f'# Test Samples: {n_per_row:3d}\t '                                  <span class="fm-combinumeral">⑦</span>
        f'\tAccuracy: {test_accuracy.avg:3.1%} '                              <span class="fm-combinumeral">⑦</span>
        f'\tOverlap: {np.mean(hull_overlaps):3.1%}')                          <span class="fm-combinumeral">⑦</span>
  
for i in range(5,50,5):                                                       <span class="fm-combinumeral">⑧</span>
  test(model, n_rows_test, i)                                                 <span class="fm-combinumeral">⑧</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Set the number of test samples to be generated for each test.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Test function</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Generate the test dataset.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Iterate through the batches of test data.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Track the loss and accuracy.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Update the overlap between the convex hull and predicted pointer sequences.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Print the accuracy and overlap.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Iterate and print results for different sample sizes.</p>

  <p class="body">This code will produce output like the following:</p>
  <pre class="programlisting"># Test Samples:   5     Accuracy: 54.8%     Overlap: 43.7%
# Test Samples:  10     Accuracy: 72.1%     Overlap: 79.1%
# Test Samples:  15     Accuracy: 79.0%     Overlap: 90.1%
# Test Samples:  20     Accuracy: 84.8%     Overlap: 92.7%
# Test Samples:  25     Accuracy: 80.6%     Overlap: 92.3%
# Test Samples:  30     Accuracy: 80.3%     Overlap: 91.6%
# Test Samples:  35     Accuracy: 77.8%     Overlap: 91.9%
# Test Samples:  40     Accuracy: 75.8%     Overlap: 92.1%
# Test Samples:  45     Accuracy: 72.4%     Overlap: 90.4%</pre>

  <p class="body">Let’s now test the trained model and see how well this model generalizes to new unseen data. We’ll use a dataset with 50 points to test the trained and validated model and calculate the convex hull overlap between the predicted hull and the ground truth hull obtained by SciPy. We pass the batch of input data and its lengths through the model to obtain the predicted scores (<code class="fm-code-in-text">log_pointer_scores</code>) and the argmax indices (<code class="fm-code-in-text">pointer_argmaxs</code>) of the pointer network. The ground truth is the convex hull obtained using the <code class="fm-code-in-text">ConvexHull</code> function from <code class="fm-code-in-text">scipy.spatial</code>:<a id="idIndexMarker253"/><a id="idIndexMarker254"/><a id="marker-447"/><a id="idIndexMarker255"/></p>
  <pre class="programlisting">idx = 0
n_per_row = 50                                                             <span class="fm-combinumeral">①</span>
  
test_dataset = Scatter2DDataset(n_rows_test, n_per_row, n_per_row)         <span class="fm-combinumeral">②</span>
test_loader = DataLoader(test_dataset, batch_size=batch_size,              <span class="fm-combinumeral">③</span>
<span class="fm-code-continuation-arrow">➥</span>  num_workers=n_workers)                                                 <span class="fm-combinumeral">③</span>
batch_data, batch_labels, batch_lengths = next(iter(test_loader))
print(batch_data.shape,batch_lengths.shape) 
log_pointer_scores, pointer_argmaxs = model(batch_data.to(device),
<span class="fm-code-continuation-arrow">➥</span>  batch_lengths.to(device))                                              <span class="fm-combinumeral">④</span>
pred_hull_idxs = pointer_argmaxs[idx].cpu()                                <span class="fm-combinumeral">⑤</span>
pred_hull_idxs = pred_hull_idxs[pred_hull_idxs &gt; 1] – 2                    <span class="fm-combinumeral">⑥</span>
points = batch_data[idx, 2:batch_lengths[idx], :2]                         <span class="fm-combinumeral">⑦</span>
points1 = batch_data[idx, 1:batch_lengths[idx], :2]                        <span class="fm-combinumeral">⑦</span>
print(points.shape,)                                                       <span class="fm-combinumeral">⑦</span>
true_hull_idxs = ConvexHull(points).vertices.tolist()                      <span class="fm-combinumeral">⑧</span>
true_hull_idxs = cyclic_permute(true_hull_idxs, np.argmin(true_hull_idxs)) <span class="fm-combinumeral">⑧</span>
  
overlap = calculate_hull_overlap(batch_data[idx].cpu(), batch_lengths[idx].cpu(),
<span class="fm-code-continuation-arrow">➥</span>  pointer_argmaxs[idx].cpu())                                             <span class="fm-combinumeral">⑨</span>
  
print(f'Predicted: {pred_hull_idxs.tolist()}')                             <span class="fm-combinumeral">⑩</span>
print(f'True:      {true_hull_idxs}')                                      <span class="fm-combinumeral">⑩</span>
print(f'Hull overlap: {overlap:3.2%}')                                     <span class="fm-combinumeral">⑩</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Set the number of points in each sample.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Create a test dataset.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Load the first batch of data from the test dataset.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Obtain the predicted scores and the argmax indices of the pointer network.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Extract the predicted argmax indices for the selected sample from the batch.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Filter out the special tokens (e.g., &lt;eos&gt;) and adjust the indices for indexing the points correctly.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Extract and print the 2D points for the selected sample from the batch.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Ground truth convex hull</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Calculate the convex hull overlap.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Print the list of predicted convex hull indices, convex hull indices, and overlap percentage.</p>

  <p class="body">Running the code will produce output like the following. You can run the preceding code snippets multiple times to get a high percentage of hull overlap:</p>
  <pre class="programlisting">torch.Size([16, 51, 3]) torch.Size([16])
torch.Size([49, 2])
Predicted: [0, 3, 5, 31, 45, 47, 48, 40, 10]
True:      [0, 3, 5, 31, 45, 47, 48, 40, 10]
Hull overlap: 100.00%</pre>

  <p class="body">The following code snippet can be used to visualize the convex hull generated by the pointer network (<code class="fm-code-in-text">ConvexNet</code>) in comparison with the convex hull generated by <code class="fm-code-in-text">scipy.spatial</code> as a ground truth:<a id="idIndexMarker256"/></p>
  <pre class="programlisting">plt.rcParams['figure.figsize'] = (10, 6)                <span class="fm-combinumeral">①</span>
plt.subplot(1, 2, 1)                                    <span class="fm-combinumeral">①</span>
  
true_hull_idxs = ConvexHull(points).vertices.tolist()   <span class="fm-combinumeral">②</span>
display_points_with_hull(points, true_hull_idxs)        <span class="fm-combinumeral">③</span>
_ = plt.title('SciPy Convex Hull')                      <span class="fm-combinumeral">③</span>
  
plt.subplot(1, 2, 2)                                    <span class="fm-combinumeral">④</span>
display_points_with_hull(points, pred_hull_idxs)        <span class="fm-combinumeral">⑤</span>
_ = plt.title('ConvexNet Convex Hull')                  <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Set the default figure size, and create the first subplot.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Compute the convex hull of a set of points (points) using the ConvexHull function from scipy.spatial.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Display the points and their convex hull in the first subplot.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Create a second subplot.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Display the points and the convex hull generated by ConvexNet.</p>

  <p class="body"><a id="marker-448"/>Figure 11.20 shows the convex hulls generated by SciPy and ConvexNet. These convex hulls are identical in some instances (i.e., hull overlap = 100.00%), yet achieving this consistency requires proper training and careful tuning of the ConvexNet parameters.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F20_Khamis.png"/></p>

    <p class="figurecaption">Figure 11.20 Convex hulls generated by SciPy and Ptr-Net for 50 points</p>
  </div>

  <p class="body">This chapter has offered a fundamental foundation in ML and discussed the applications of supervised and unsupervised ML in handling optimization problems. The next chapter will focus on reinforcement learning and will delve deeply into its practical applications in tackling optimization problems.</p>

  <h2 class="fm-head" id="heading_id_15">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Machine learning (ML), a branch of artificial intelligence (AI), grants an artificial system or process the capacity to learn from experiences and observations, rather than through explicit programming.<a id="idIndexMarker257"/><a id="idIndexMarker258"/><a id="idIndexMarker259"/><a id="idIndexMarker260"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Deep learning (DL) is a subset of ML that is focused on the detection of inherent features within data by employing deep neural networks. This allows artificial systems to form intricate concepts from simpler ones.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Geometric deep learning (GDL) extends (structured) deep neural models to handle non-Euclidean data with underlying geometric structures, such as graphs, point clouds, and manifolds. <a id="idIndexMarker261"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Graph machine learning (GML) is a subfield of ML that focuses on developing algorithms and models capable of learning from graph-structured data.<a id="idIndexMarker262"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Graph embedding represents the process of creating a conversion from the discrete, high-dimensional graph domain to a lower-dimensional continuous domain.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The attention mechanism allows a model to selectively focus on certain portions of the input data while it is in the process of generating the output sequence.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The pointer network (Ptr-Net) is a variation of the sequence-to-sequence model with attention designed to deal with variable-sized input data sequences.<a id="idIndexMarker263"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">A self-organizing map (SOM), also known as a Kohonen map, is a type of artificial neural network (ANN) used for unsupervised learning. SOMs differ from other types of ANNs, as they apply competitive learning rather than error-correction learning (such as backpropagation with gradient descent).<a id="idIndexMarker264"/><a id="idIndexMarker265"/><a id="marker-449"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Neural combinatorial optimization refers to the application of ML to solve combinatorial optimization problems.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Harnessing ML for combinatorial optimization can be achieved through three main methods: end-to-end learning where the model directly formulates solutions, using ML to configure and improve optimization algorithms, and integrating ML with optimization algorithms where the model continuously guides the optimization algorithm based on its current state.</p>
    </li>
  </ul>
</body></html>