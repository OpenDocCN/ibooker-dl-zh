<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">1 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/>Understanding large language models</h1>

  <p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">Introducing generative AI (specifically, large language models)</li>

    <li class="co-summary-bullet">Exploring the benefits of generative AI</li>

    <li class="co-summary-bullet">Determining when and when not to use generative AI</li>
  </ul>

  <p class="body"><a id="marker-3"/>Whether you realize it or not, and whether you want to admit it or not, you have quietly received a promotion. Every professional software engineer has. Almost overnight, we have gone from staff engineers to engineering managers. You now have the world’s smartest and most talented junior developer on your team—generative AI is your new coding partner. So, guiding, mentoring, and performing code reviews should become part of your daily routine. This chapter will provide you with an overview of a subset of generative AI called large language models (LLMs), specifically ChatGPT, GitHub Copilot, and AWS CodeWhisperer.<a id="idIndexMarker001"/></p>

  <p class="fm-callout"><span class="fm-callout-head">Note</span> This is not a traditional programming book. You will not be able to use it like you would a script. You are going to engage in a dialogue with LLMs, and like any conversation, the words and direction will change depending on the model and the prior context. The output you receive will very likely differ from what is printed in this book. This should not discourage you. Instead, you should explore. The journey is as rewarding as the destination. You may find yourself frustrated that you can’t follow along. Have patience. If you are disciplined (and somewhat adventurous), you can get GPT to cooperate with the general themes and aim of this book: learning how to use generative AI to make you a better programmer.<a id="idIndexMarker002"/></p>

  <h2 class="fm-head" id="heading_id_3">1.1 Accelerating your development</h2>

  <p class="body"><a id="marker-4"/>Welcome to a new era in software development in which your development team expands by one very talented engineer. Generative AI isn’t just a tool; it’s your next team member, poised to elevate your programming to new heights. Imagine designing intricate systems, coding with unprecedented speed, and testing with robustness you never thought possible—all with an intelligence that learns from the best. In this book, we’ll explore how generative AI will not only assist you in everyday coding tasks but also enable you to achieve feats previously beyond reach, ensuring faster development, enhanced quality, and the capacity to innovate like never before.<a id="idIndexMarker003"/></p>

  <p class="body">LLMs and generative AI can drastically accelerate your software development process. By automating the tedious creation of boilerplate code, you are freed to focus on creative problem-solving and value-adding tasks. Consider this example: you want to have the LLM set up a Flask application. You might use the following prompt:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">Generate a stubbed-out Flask application.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">The LLM might output the following Python code.</p>

  <p class="fm-code-listing-caption">Listing 1.1 Potential code generated by an LLM to stub out a Flask application</p>
  <pre class="programlisting">from flask import Flask
 
app = Flask(__name__)
 
@app.route('/')
def home():
    return "Hello, world!"
  
if __name__ == '__main__':
    app.run(debug=True)</pre>

  <p class="body">LLMs are deep learning models that are good at providing natural language responses to natural language prompts. You can imagine simply describing what you need in plain English and receiving ready-to-integrate code:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">Create a class for managing user sessions in Python.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">Here’s a possible response.</p>

  <p class="fm-code-listing-caption">Listing 1.2 Potential code generated to create a session management class</p>
  <pre class="programlisting">import uuid
from datetime import datetime, timedelta
  
class SessionManager:
    def __init__(self):
        self.sessions = {}
    
    def create_session(self, user_id):
        """Create a new session for a user with a unique session ID and expiration time."""
        session_id = str(uuid.uuid4())
        # Sessions expire after 1 hour
        expiration = datetime.now() + timedelta(hours=1)  
        self.sessions[session_id] = {'user_id': 
        user_id, 'expiration': expiration}
        return session_id
    
    def get_user_id(self, session_id):
        """Retrieve a user ID from a session ID 
               if the session is still valid."""
        session_info = self.sessions.get(session_id)
        if session_info and session_info['expiration'] &gt; datetime.now():
            return session_info['user_id']
        return None  # Session is expired or does not exist
    
    def delete_session(self, session_id):
        """Delete a session by its ID."""
        if session_id in self.sessions:
            del self.sessions[session_id]
    
    def is_session_valid(self, session_id):
        """Check if a session is still valid."""
        return session_id in self. sessions and self.sessions[session_id]['expiration'] &gt; datetime.now()</pre>

  <p class="body"><a id="marker-5"/>These tools also enhance your debugging efficiency by identifying patterns and suggesting fixes. Suppose you want your AI tool to analyze a block of code and flag potential memory leaks. It might produce the following code.</p>

  <p class="fm-code-listing-caption">Listing 1.3 Using an LLM to find a potential memory leak in Python</p>
  <pre class="programlisting">    def process_data():
    large_data = [x for x in range(1000000)]  # Large list of numbers
    result = sum(large_data)
    print("Result of processing:", result)
    # large_data not cleared or reassigned, 
    # potentially leading to memory retention
  
process_data()</pre>

  <p class="body">According to the explanation provided by ChatGPT, this Python code demonstrates a potential memory leak due to a large list not being cleared after use, which can keep its contents in memory longer than necessary if references are not properly managed.</p>

  <p class="body">In the improved code, the AI tool suggests explicitly clearing the list or reassigning <code class="fm-code-in-text">None</code> after its usage to help in releasing the memory sooner, especially in environments where garbage collection is not aggressive.</p>

  <p class="fm-code-listing-caption">Listing 1.4 “Improved” LLM code to address the potential memory leak</p>
  <pre class="programlisting">def process_data():
    large_data = [x for x in range(1000000)]  # Large list of numbers
    result = sum(large_data)
    print("Result of processing:", result)
    large_data = None  # Clear the reference to release memory
  
process_data()</pre>

  <p class="body"><a id="marker-6"/>Moreover, when it comes to refactoring, the AI can suggest optimizations that make your code cleaner and more efficient, as shown in the next two listings.</p>

  <p class="fm-code-listing-caption">Listing 1.5 Verbose code before the suggested refactoring</p>
  <pre class="programlisting">class DataProcessor:
    def __init__(self, data):
        self.data = data
 
    def process_data(self):
        if self. data is not None:
            if len(self.data) &gt; 0:
                processed_data = []
                for d in self.data:
                    if d is not None:
                        if d % 2 == 0:
                            processed_data.append(d)
                return processed_data
            else:
                return []
        else:
            return []
 
processor = DataProcessor([1, 2, 3, 4, None, 6])
result = processor.process_data()
print("Processed Data:", result)</pre>

  <p class="body">After the refactoring, the code is more readable, maintainable, and idiomatic.</p>

  <p class="fm-code-listing-caption">Listing 1.6 LLM refactored code that is more concise</p>
  <pre class="programlisting">class DataProcessor:
    def __init__(self, data):
        self. data = data or []
  
    def process_data(self):
        return [d for d in self.data if d is not None and d % 2 == 0]
  
processor = DataProcessor([1, 2, 3, 4, None, 6])
result = processor.process_data()
print("Processed Data:", result)</pre>

  <p class="body"><a id="marker-7"/>LLMs extend beyond mere code generation; they are sophisticated enough to assist in designing software architecture as well. This capability allows developers to engage with these models more creatively and strategically. For instance, rather than simply requesting specific snippets of code, a developer can describe the overall objectives or functional requirements of a system. The LLM can then propose various architectural designs, suggest design patterns, or outline an entire system’s structure. This approach not only saves significant time but also takes advantage of the AI’s extensive training to innovate and optimize solutions, potentially introducing efficiencies or ideas that the human developer may not have initially considered. This flexibility makes LLMs invaluable partners in the creative and iterative processes of software development. We will explore this in chapter 3.</p>

  <p class="body">In addition, by enhancing the quality and security of your deliverables—from code to documentation—these tools ensure that your outputs meet the highest standards. For instance, when integrating a new library, the AI can automatically generate secure, efficient implementation examples, helping you avoid common security pitfalls.</p>

  <p class="body">Finally, learning new programming languages or frameworks becomes significantly easier. The AI can provide real-time, context-aware guidance and documentation, helping you to not only understand but also apply new concepts practically. For example, are you transitioning to a new framework like Dash? Your AI assistant can instantly generate sample code snippets and detailed explanations tailored to your current project’s context.</p>

  <p class="fm-code-listing-caption">Listing 1.7 LLM-generated sample code demonstrating how to use a library</p>
  <pre class="programlisting">import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import pandas as pd
import plotly.express as px
  
# Sample data creation
dates = pd.date_range(start='1/1/2020', periods=100)
prices = pd.Series(range(100)) + pd.Series(range(100))/2  
# Just a simple series to mimic stock prices
data = pd.DataFrame({'Date': dates, 'Price': prices})
  
# Initialize the Dash app (typically in your main module)
app = dash.Dash(__name__)
  
# Define the layout of the app
app.layout = html.Div([
    html.H1("Stock Prices Dashboard"),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=data['Date'].min(),
        end_date=data['Date'].max(),
        display_format='MMM D, YYYY',
        start_date_placeholder_text='Start Period',
        end_date_placeholder_text='End Period'
    ),
    dcc.Graph(id='price-graph'),
])
 
# Callback to update the graph based on the date range picker input
@app.callback(
    Output('price-graph', 'figure'),
    Input('date-picker-range', 'start_date'),
    Input('date-picker-range', 'end_date')
)
def update_graph(start_date, end_date):
    filtered_data = data[(data['Date'] &gt;= 
            start_date) &amp; (data['Date'] &lt;= end_date)]
    figure = px.line(filtered_data, x='Date', 
            y='Price', title='Stock Prices Over Time')
    return figure
  
# Run the app
if __name__ == '__main__':
    app.run_server(debug=True)</pre>

  <p class="body"><a id="marker-8"/>We can see the output of this code in figure 1.1, which is the running Dash code.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH01_F01_Crocker2.png"/></p>

    <p class="figurecaption">Figure 1.1 The Stock Prices Dashboard created by ChatGPT in response to the prompt “<code class="fm-code-in-text">create a sample dashboard using dash</code>”</p>
  </div>

  <p class="body">The real power of LLMs unfolds in their integration in development environments. Tools like GitHub Copilot, developed by Microsoft, harness the capabilities of LLMs to provide real-time coding assistance directly in integrated development environments (IDEs) such as Visual Studio Code. We will unleash this power in chapter 4.<a id="idIndexMarker004"/><a id="idIndexMarker005"/></p>

  <p class="body">This book will not only explain these concepts but also demonstrate them through numerous examples, showing how you can use LLMs to improve your productivity and code quality dramatically. From setting up your environment to tackling complex coding challenges, you’ll learn how to make the most out of these intelligent tools in your everyday development.<a id="idIndexMarker006"/></p>

  <h2 class="fm-head" id="heading_id_4">1.2 A developer’s introduction to LLMs</h2>

  <p class="body"><a id="marker-9"/>Although this book is mainly a practitioner’s guide and therefore very light on theory, the following section will provide you with the most relevant material for you to get the most out of your new teammate. <a id="idIndexMarker007"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Yes, but I want to know more</p>

    <p class="fm-sidebar-text">If you are interested in diving deeper into the theory behind LLMs, neural networks, and all things generative AI, you should look at the following two books: the forthcoming <i class="fm-italics">Build a Large Language Model (From Scratch)</i> by Sebastian Raschka (Manning, 2024) and the amusingly titled <i class="fm-italics">The Complete Obsolete Guide to Generative AI</i> by David Clinton (Manning, 2024).<a id="idIndexMarker008"/><a id="idIndexMarker009"/><a id="idIndexMarker010"/><a id="idIndexMarker011"/></p>
  </div>

  <p class="body">Let’s start with a very simple definition of what an LLM is and what it can do for you; this way, you can properly pitch it to your boss and co-workers. A <i class="fm-italics">large language model</i> is a type of artificial intelligence model that processes, understands, and generates human-like text based on the data it has been trained on. These models are a subset of deep learning and are particularly advanced in handling various aspects of natural language processing (NLP).<a id="idIndexMarker012"/></p>

  <p class="body">As the name implies, these models are “large” not just in terms of the physical size of the data they are trained on but also in the complexity and number of parameters. Modern LLMs like OpenAI’s GPT-4 have up to hundreds of billions of parameters.</p>

  <p class="body">LLMs are trained on vast amounts of text data. This training involves reading and analyzing a wide range of internet texts, books, articles, and other forms of written communication to learn the structure, nuances, and complexities of human language.</p>

  <p class="body">Most LLMs use the Transformer architecture, a deep learning model that relies on self-attention mechanisms to weigh the importance of different words in a sentence regardless of their position. This allows LLMs to generate more contextually relevant text. A typical Transformer model consists of an encoder and a decoder, each composed of multiple layers.<a id="idIndexMarker013"/></p>

  <p class="body">Understanding the architecture of LLMs helps in using their capabilities more effectively as well as addressing their limitations in practical applications. As these models continue to evolve, they promise to offer even more sophisticated tools for developers to enhance their applications.</p>

  <h2 class="fm-head" id="heading_id_5">1.3 When to use and when to avoid generative AI</h2>

  <p class="body"><a id="marker-10"/>Generative AI (and by extension an LLM) is not a one-size-fits-all solution. Understanding when to employ these technologies, as well as recognizing situations where they may be less effective or even problematic, is crucial for maximizing their benefits while mitigating potential drawbacks. We will start with when it is appropriate for you to use an LLM: <a id="idIndexMarker014"/><a id="idIndexMarker015"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Enhancing productivity</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list"><i class="fm-italics">Example</i>—Use AI to automate boilerplate code, generate documentation, or provide coding suggestions within your IDE.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><i class="fm-italics">Discussed in chapters 3 and 4</i>—These chapters explore how tools like GitHub Copilot can boost coding efficiency.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Learning and exploration</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list"><i class="fm-italics">Example</i>—Employ AI to learn new programming languages or frameworks by generating example codes and explanations.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><i class="fm-italics">Covered in chapter 5</i>—Here, we examine how AI can accelerate the learning process and introduce you to new technologies.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Handling repetitive tasks</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list"><i class="fm-italics">Example</i>—Use AI to handle repetitive software testing or data entry tasks, freeing up time for more complex problems.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><i class="fm-italics">Explored in chapter 7</i>—Discusses automation in testing and maintenance tasks.</p>
        </li>
      </ul>
    </li>
  </ul>

  <p class="body">There are, however, situations in which you should avoid using LLMs and generative AI tools such as ChatGPT and GitHub Copilot, mainly those related to data security and privacy protection. Using AI in environments with sensitive or proprietary data can risk unintended data leaks. There are several reasons for this, one of which is that part or all of the code is sent to the model as context, meaning at least part of your proprietary code may find its way outside of your firewall. There is a question as to whether it may be included in the training data for the next round of training. But have no fear: we will examine a couple of methods to address this concern in chapter 9.</p>

  <p class="body">Another scenario in which you might limit your usage is when precision and expertise are required. Given that a feature of LLMs is their ability to add randomness to their output (sometimes referred to as <i class="fm-italics">hallucinations</i>), the output may contain subtle variations from the true and right answer. For this reason, you should always verify the output before including it in your codebase.</p>

  <p class="body">Although generative AI offers numerous advantages, it’s essential to apply it judiciously, considering both the context of its use and the specific needs of the project. By understanding when to use these powerful tools and when to proceed with caution, developers can maximize their effectiveness and ensure ethical and efficient use of technology.</p>

  <h2 class="fm-head" id="heading_id_6">Summary<a id="marker-11"/></h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Generative AI is both evolutionary and revolutionary. It’s evolutionary in the sense that it is just another iteration of the tools that we as developers use every day. It’s revolutionary in that it will transform how we do our jobs.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The future of development will involve managing generative AI. Even the mythical 10× developer will not have the productivity of a developer with an AI partner; an AI-powered developer will produce higher-quality code at a substantially faster rate, at a lower cost than one who is not. We will spend more of our time training our AI partner to do what we want and how we want it done than we do writing code without the AI.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Trust but verify the LLM’s output.<a id="idIndexMarker016"/></p>
    </li>
  </ul>
</body></html>