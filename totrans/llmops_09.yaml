- en: 'Chapter 9\. Scaling: Hardware, Infrastructure, and Resource Management'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章\. 规模化：硬件、基础设施和资源管理
- en: Deploying and managing LLMs presents unique challenges and opportunities in
    the realm of infrastructure and resource management. LLMs, as you’ve seen throughout
    this book, are computationally intensive, requiring substantial hardware, storage,
    and network resources to operate efficiently. Whether you’re leveraging LLMs as
    a cloud-based service, deploying pretrained models in on-premises data centers,
    or training your own models from scratch, your infrastructure decisions will influence
    their performance, scalability, and cost-effectiveness.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 部署和管理LLM在基础设施和资源管理领域带来了独特的挑战和机遇。正如您在本书中所见，LLM是计算密集型的，需要大量的硬件、存储和网络资源才能高效运行。无论您是将LLM作为基于云的服务利用、在本地数据中心部署预训练模型，还是从头开始训练自己的模型，您的基础设施决策将影响其性能、可扩展性和成本效益。
- en: Effective resource management for LLMs involves optimizing compute power, memory,
    and storage. In this chapter, we will explore the key components of infrastructure
    for LLMs, including hardware requirements and deployment strategies. We’ll also
    discuss best practices for optimizing resource use, managing costs, and maintaining
    reliability in production environments. This chapter will help you understand
    the trade-offs involved in managing resources for large-scale AI applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLMs（大型语言模型）的有效资源管理涉及优化计算能力、内存和存储。在本章中，我们将探讨LLM基础设施的关键组成部分，包括硬件要求和部署策略。我们还将讨论优化资源使用、管理成本和在生产环境中保持可靠性的最佳实践。本章将帮助您了解管理大规模AI应用资源所涉及到的权衡。
- en: Choosing the Right Approach
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择正确的方法
- en: Selecting the appropriate method for using LLMs depends on the requirements
    of the application that you want to use it for. For startups or small-scale applications,
    using models directly from the cloud may be the quickest and most cost-effective
    solution. For enterprises with specialized requirements or high workloads, deploying
    LLMs on cloud infrastructure can help you find an appropriate balance between
    flexibility and scalability. Finally, for organizations with strict data privacy
    or latency requirements, local deployment offers unmatched control and security,
    though at the cost of higher operational complexity.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 选择使用LLM的适当方法取决于您想要使用它的应用程序的需求。对于初创公司或小规模应用，直接从云端使用模型可能是最快且最具成本效益的解决方案。对于有特殊需求或高负载的企业，在云基础设施上部署LLM可以帮助您在灵活性和可扩展性之间找到适当的平衡。最后，对于有严格数据隐私或延迟要求的组织，本地部署提供了无与伦比的控制和安全，尽管代价是更高的运营复杂性。
- en: By carefully evaluating the trade-offs of each approach, your organization can
    align its LLM deployment strategy with its technical and business objectives,
    ensuring efficient and effective use of these transformative AI technologies.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细评估每种方法的权衡，您的组织可以将其LLM部署策略与其技术和业务目标对齐，确保这些变革性AI技术的有效和高效使用。
- en: Regardless of the solution you choose, my suggestion is to always begin with
    a third-party API-based approach; that is, start by using models directly from
    the cloud. One of the major issues I’ve observed in real-world deployments is
    figuring out whether the LLM is a good solution to a given problem. Using a third-party,
    API-based approach will allow you to answer that question in a prototype before
    committing a large number of resources to infrastructure.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择哪种解决方案，我的建议是始终从第三方API-based方法开始；也就是说，首先使用来自云端的模型。我在实际部署中观察到的一个主要问题是确定LLM是否是解决特定问题的良好解决方案。使用第三方、基于API的方法将允许您在投入大量资源到基础设施之前，在原型中回答这个问题。
- en: Scaling and Resource Allocation
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规模化和资源分配
- en: To maintain performance, cost-effectiveness, and reliability in your LLM-based
    application, you’ll have to manage your resources well. Overallocating resources,
    especially those in high demand, such as the required GPUs and memory bandwidth
    to run AI systems, will lead to unnecessary expenses. Underallocating resources
    will expose you to risks of system crashes and poor user experiences.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持基于LLM的应用的性能、成本效益和可靠性，您必须有效地管理您的资源。过度分配资源，尤其是那些需求量大的资源，如运行AI系统所需的GPU和内存带宽，将导致不必要的开支。资源分配不足将使您面临系统崩溃和用户体验不佳的风险。
- en: Most training failures come from running out of memory and not compute. I call
    this the “iceberg problem” where the visible tip is the failure, but the real
    hidden issue is memory inefficiency beneath. Most people don’t realize that the
    real miss-out is when the suboptimal memory use goes unnoticed and under-utilized.
    Thus, people leave a lot of performance on the table. If you’re hitting memory
    walls, don’t reach for more hardware just yet. When used correctly, methods like
    sharding, activation checkpointing, dynamic batching, model offloading, etc.,
    can easily make your 24 GB consumer GPU behave like a 48 GB A100.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数训练失败源于内存不足而非计算能力不足。我把这个问题称为“冰山问题”，其中可见的尖端是失败，但真正的隐藏问题是内存效率低下。大多数人没有意识到真正的问题在于当次优的内存使用未被注意到且未被充分利用时。因此，人们留下了很多性能上的潜力。如果你正在遇到内存墙，不要急于寻求更多的硬件。当正确使用时，像分片、激活检查点、动态批处理、模型卸载等方法，可以轻松让你的24
    GB消费级GPU表现得像48 GB的A100。
- en: The two main components of resource allocation are *monitoring* and *automating
    deployments*. You need to monitor in order to know when you are over- or underallocating
    resources. Then, once you have this information, you need to be able to react
    quickly. While it’s possible to live with manual deployments, the costs will likely
    become prohibitive over time. This is especially true if the demand for your service
    varies a lot, which could happen if your service achieves sudden success or expands
    to different geographical regions whose usage patterns reflect different time
    zones.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 资源分配的两个主要组成部分是*监控*和*自动化部署*。你需要监控以便知道何时资源分配过多或不足。一旦你有了这些信息，你需要能够快速反应。虽然可以忍受手动部署，但随着时间的推移，成本可能会变得难以承受。这尤其适用于你的服务需求变化很大时，这可能发生在你的服务突然成功或扩展到不同地理区域，其使用模式反映了不同的时区。
- en: Monitoring
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控
- en: Monitoring enables you to understand your application’s behavior, optimize resource
    usage, and maintain high availability and performance under varying workloads.
    A successful monitoring approach revolves around tracking key performance indicators
    (KPIs) using the appropriate monitoring tools, then developing appropriate procedures
    to implement changes when needed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 监控使你能够了解应用程序的行为，优化资源使用，并在不同的工作负载下保持高可用性和性能。一个成功的监控方法围绕使用适当的监控工具跟踪关键性能指标（KPIs），然后在需要时开发适当的程序来实施更改。
- en: 'Key metrics for monitoring include:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 监控的关键指标包括：
- en: Latency
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟
- en: '*Latency* measures the response time for user queries and is shown to directly
    impact user satisfaction. Your goal is to minimize latency.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*延迟*衡量用户查询的响应时间，并显示出它直接影响到用户满意度。你的目标是尽量减少延迟。'
- en: Throughput
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量
- en: '*Throughput*, or the number of requests processed in a period of time (usually
    per second), indicates the system’s capacity to handle demand and is critical
    to understand how your system is performing during peak loads.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*吞吐量*，或单位时间内（通常为每秒）处理请求数量，表明系统处理需求的能力，了解你的系统在高峰负载期间的表现至关重要。'
- en: Resource utilization metrics
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 资源利用率指标
- en: '*Resource utilization metrics* such as CPU, GPU, memory, disk I/O, and network
    bandwidth provide insights into which resources are and are not well allocated.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*资源利用率指标*，如CPU、GPU、内存、磁盘I/O和网络带宽，提供了关于哪些资源分配得很好，哪些没有的见解。'
- en: Error rates
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 错误率
- en: Monitoring error rates, including server errors and application-specific issues
    like exceeding token limits or LLM safety responses, can help you identify issues
    before they become big problems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 监控错误率，包括服务器错误和特定于应用程序的问题，如超过令牌限制或LLM安全响应，可以帮助你在问题变成大问题之前识别它们。
- en: Cost
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 成本
- en: Monitor cost to make sure your application is economically viable, especially
    for resource-intensive LLMs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 监控成本以确保你的应用程序在经济上是可行的，特别是对于资源密集型的LLM。
- en: Cloud environments offer numerous native monitoring tools for these metrics
    that are tailored to their respective platforms, like AWS CloudWatch, Azure Monitor,
    and Google Cloud Operations Suite. These comprehensive tools enable you to track
    both standard and custom metrics, such as model-specific data like token usage
    or inference times.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 云环境为这些指标提供了许多针对各自平台的本地监控工具，如AWS CloudWatch、Azure Monitor和Google Cloud Operations
    Suite。这些综合工具使你能够跟踪标准和自定义指标，例如特定于模型的如令牌使用或推理时间的数据。
- en: Application performance-monitoring platforms like Datadog, New Relic, and AppDynamics
    go a step further by visualizing application dependencies, providing detailed
    insights into bottlenecks and potential failures. Model-specific platforms like
    Weights & Biases and MLflow allow you to monitor LLM behavior, track fine-tuning
    iterations, and compare deployments.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Datadog、New Relic和AppDynamics这样的应用程序性能监控平台通过可视化应用程序依赖关系，提供了对瓶颈和潜在故障的详细洞察。像Weights
    & Biases和MLflow这样的特定平台允许您监控LLM行为，跟踪微调迭代，并比较部署。
- en: For logging, centralized systems like the ELK Stack or Fluentd are valuable
    for capturing detailed application logs, query specifics, and system warnings;
    distributed tracing tools like OpenTelemetry or Jaeger let you trace requests
    across services to pinpoint latency hotspots.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于日志记录，像ELK Stack或Fluentd这样的集中式系统对于捕获详细的应用程序日志、查询具体信息和系统警告非常有价值；像OpenTelemetry或Jaeger这样的分布式跟踪工具可以让您跨服务跟踪请求，以确定延迟热点。
- en: 'A good monitoring architecture will have at least three layers:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的监控架构至少应该有三个层级：
- en: Client layer
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端层
- en: The client layer allows you to capture user-side performance and satisfaction
    metrics, often by asking users to rate an answer using a thumbs-up or a thumbs-down.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端层允许您捕获用户端性能和满意度指标，通常是通过让用户使用点赞或点踩来对答案进行评分。
- en: Application layer
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序层
- en: The application layer can focus on API performance, tracking throughput, processing
    times, and error rates.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序层可以专注于API性能，跟踪吞吐量、处理时间和错误率。
- en: Infrastructure layer
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施层
- en: The infrastructure layer can monitor the underlying resources that host the
    LLM and your application, measuring CPU, GPU, memory, storage, and I/O performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施层可以监控承载LLM和您的应用程序的底层资源，测量CPU、GPU、内存、存储和I/O性能。
- en: Finally, you can treat the model as a separate fourth layer, depending on the
    kind of granularity you want. This is especially desirable for LLM-based applications.
    This *model layer* can track inference times, token usage, token caching, and
    other model-specific metrics, such as perplexity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以根据所需的粒度将模型视为一个独立的第四层。这对于基于LLM的应用程序尤其理想。这个*模型层*可以跟踪推理时间、令牌使用、令牌缓存以及其他模型特定指标，例如困惑度。
- en: Real-time alerting can help automate issue detection. By setting thresholds
    for metrics like latency, resource utilization, and error rates, you can receive
    alerts by email or SMS when a specific metric falls below expected levels. It’s
    also a good idea to implement *synthetic monitoring* by automatically sending
    your application some requests for which you know the expected answer and measuring
    the output.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 实时警报可以帮助自动化问题检测。通过为诸如延迟、资源利用率和错误率等指标设置阈值，当特定指标低于预期水平时，您可以通过电子邮件或短信接收警报。同时，通过自动向您的应用程序发送一些您已知预期答案的请求并测量输出，实施*合成监控*也是一个好主意。
- en: When a threshold fails, you can set scripts to trigger automatically; for example,
    starting a new virtual machine if a current one hits some CPU- or memory-level
    threshold. You can also automatically run scripts to reduce downtime during common
    issues, such as restarting services periodically or scaling resources up or down
    based on anticipated demand.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当阈值失败时，您可以设置脚本自动触发；例如，如果当前虚拟机达到某些CPU或内存级别的阈值，则启动一个新的虚拟机。您还可以自动运行脚本以减少常见问题（如定期重启服务或根据预期需求上下调整资源）的停机时间。
- en: The insights you derive from monitoring will be invaluable in optimizing your
    system. For instance, *autoscaling* mechanisms can adjust compute resources dynamically,
    based on workload demands. *Horizontal scaling* can accommodate more requests
    by adding instances, while *vertical scaling* increases the capacity of existing
    nodes. *Caching* frequently accessed responses reduces latency and lessens the
    workload on the model, while *batching* low-priority queries enhances efficiency.
    Furthermore, techniques like model distillation and quantization (discussed in
    [Chapter 5](ch05.html#ch05_model_domain_adaptation_for_llm_based_applications_1748896666813361))
    can optimize the model itself, balancing performance with resource consumption.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从监控中获得的见解对于优化您的系统将非常有价值。例如，*自动扩展*机制可以根据工作负载需求动态调整计算资源。*水平扩展*可以通过添加实例来容纳更多请求，而*垂直扩展*则增加现有节点的容量。*缓存*频繁访问的响应可以减少延迟并减轻模型的工作负载，而*批处理*低优先级查询可以提高效率。此外，模型蒸馏和量化（在第5章中讨论）等技术可以优化模型本身，在性能和资源消耗之间取得平衡。
- en: Monitoring is not a one-time setup but a continuous process of observability
    and refinement. Observability tools allow you to identify workload patterns, predict
    resource needs, and analyze trends in user interactions to refine both your infrastructure
    and your model’s performance. Advanced testing techniques, such as A/B and shadow
    testing, let you validate new deployments in a controlled manner, minimizing risks
    while introducing improvements. These are discussed in the ​next section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 监控不是一个一次性设置，而是一个持续的过程，涉及可观察性和细化。可观察性工具允许您识别工作负载模式，预测资源需求，并分析用户交互的趋势，以细化您的基础设施和模型性能。高级测试技术，如A/B测试和影子测试，允许您以受控的方式验证新部署，最小化风险同时引入改进。这些将在下一节中讨论。
- en: A/B Testing and Shadow Testing for LLMs
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的A/B测试和影子测试
- en: As described in [Chapter 7](ch07.html#ch07_evaluation_for_llms_1748896751667823),
    A/B testing is a widely used method for evaluating the performance of different
    versions of a system. In the LLM context, A/B testing involves deploying two versions
    of the LLM—often referred to as the “champion” (the existing model) and the “challenger”
    (the new model)—to determine which performs better under real-world conditions
    using the metrics described previously.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第7章](ch07.html#ch07_evaluation_for_llms_1748896751667823)所述，A/B测试是评估系统不同版本性能的一种广泛使用的方法。在LLM的背景下，A/B测试涉及部署LLM的两个版本——通常被称为“冠军”（现有模型）和“挑战者”（新模型）——以确定在现实世界条件下使用之前描述的指标哪个表现更好。
- en: In contrast, *shadow testing* provides a safer, less intrusive way to evaluate
    a new model without directly affecting users. In this approach, the challenger
    model runs in the background, “shadowing” the champion model by processing the
    same inputs (or a fraction of them, for cost savings) but without influencing
    the live application’s outputs. This allows teams to collect performance data,
    identify potential issues, and fine-tune the challenger model before making it
    available to users. Shadow testing is particularly useful for testing LLMs in
    high-stakes or sensitive applications, such as customer service or healthcare,
    where introducing a flawed model could lead to significant negative consequences.
    Again, the better defined your metrics are, the more accurately you can see whether
    the new model performs better or worse than the existing one.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*影子测试*提供了一种更安全、更不侵入的方式来评估新模型，而不会直接影响用户。在这种方法中，挑战者模型在后台运行，通过处理相同的输入（或其中的一部分，以节省成本）来“影子”冠军模型，但不会影响实时应用程序的输出。这允许团队收集性能数据，识别潜在问题，并在将模型提供给用户之前对其进行微调。影子测试特别适用于测试在风险较高或敏感的应用程序中使用的LLM，例如客户服务或医疗保健，引入有缺陷的模型可能导致重大负面后果。再次强调，您的指标定义得越明确，您就越能准确地看到新模型的表现是否优于现有模型。
- en: 'One caveat: in shadow testing, the users don’t see the output of the new model,
    so you can only collect their interactions with or feedback about the existing
    (champion) model. This makes A/B testing ideal for situations where user feedback
    is essential to evaluating performance, whereas shadow testing is better suited
    for testing infrastructure and ensuring a model’s reliability and safety *before*
    deployment.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个注意事项：在影子测试中，用户看不到新模型的输出，因此您只能收集他们对现有（冠军）模型的交互或反馈。这使得A/B测试非常适合那些用户反馈对于评估性能至关重要的场景，而影子测试则更适合在部署前测试基础设施，确保模型的可靠性和安全性*之前*。
- en: Automatic Infrastructure Provisioning and Management
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动基础设施配置与管理
- en: Deploying and managing infrastructure for LLMs requires significant resources,
    whether in cloud architectures or on premises. Automatic infrastructure provisioning
    can help you optimize resource utilization, ensure scalability, and reduce operational
    overhead by dynamically adjusting your infrastructure to meet your model’s computational
    demands during training, fine-tuning, and inference based on monitoring signals.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 部署和管理用于LLM的基础设施需要大量的资源，无论是在云架构中还是在本地。自动基础设施配置可以帮助您优化资源利用，确保可扩展性，并通过根据监控信号动态调整您的基础设施以满足模型在训练、微调和推理期间的计算需求来降低运营成本。
- en: Provisioning and Management in Cloud Architectures
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云架构中的配置与管理
- en: The major​ cloud platforms offer tools for automatic infrastructure provisioning
    and management, including scalable compute instances, GPU and TPU support, managed
    storage, and networking solutions tailored for AI workloads. Tools like AWS CloudFormation,
    Azure Resource Manager (ARM), and Google Cloud Deployment Manager allow you to
    deploy *infrastructure as code* (IaC) and define infrastructure requirements like
    products, versions, and features in declarative YAML or JSON templates. These
    templates automate resource provisioning to keep environments consistent across
    multiple deployments.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的云平台提供了自动基础设施配置和管理工具，包括可扩展的计算实例、GPU和TPU支持、托管存储以及针对AI工作负载定制的网络解决方案。像AWS CloudFormation、Azure资源管理器（ARM）和Google
    Cloud Deployment Manager这样的工具允许您部署*基础设施即代码*（IaC），并在声明性的YAML或JSON模板中定义基础设施需求，如产品、版本和功能。这些模板自动化资源配置，以保持多个部署之间的环境一致性。
- en: One of the most significant advantages of cloud architectures is their ability
    to scale resources automatically based on demand. Services like AWS Auto Scaling,
    Azure Virtual Machine Scale Sets (VMSS), and Google Cloud Platform (GCP) Autoscaler
    can dynamically increase or decrease the number of compute instances based on
    predefined metrics, such as CPU usage, memory consumption, and GPU utilization.
    Linking one of these tools to your monitoring setup can really help you manage
    costs and latency. This elasticity is particularly useful for LLM inference, which
    consumes expensive resources quickly. You can also use your monitoring metrics
    to automatically scale down resources that are not being used and quickly scale
    up when needed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 云架构最显著的优势之一是它们能够根据需求自动扩展资源。像AWS Auto Scaling、Azure虚拟机规模集（VMSS）和Google Cloud
    Platform（GCP）自动扩展这样的服务可以根据预定义的指标（如CPU使用率、内存消耗和GPU利用率）动态增加或减少计算实例的数量。将这些工具之一链接到您的监控设置中，真的可以帮助您管理成本和延迟。这种弹性对于LLM推理特别有用，因为它会快速消耗昂贵的资源。您还可以使用您的监控指标自动缩减未使用的资源，并在需要时快速扩展。
- en: Cloud providers also offer cost-saving options like AWS Spot Instances, Azure
    Spot VMs, and GCP preemptible VMs, which let you take advantage of unused capacity
    at a lower price. These are ideal for noncritical workloads, such as batch processing
    or distributed LLM training. However, because these instances can be interrupted,
    it’s critical to integrate their provisioning with your monitoring infrastructure
    to manage fault tolerance and job retries.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 云提供商还提供了成本节约选项，如AWS Spot实例、Azure Spot VM和GCP可抢占VM，这些选项允许您以较低的价格利用未使用的容量。这些非常适合非关键工作负载，如批处理或分布式LLM训练。然而，因为这些实例可能会被中断，所以将它们的配置与您的监控基础设施集成以管理容错性和作业重试是至关重要的。
- en: Finally, as we noted earlier, cloud-based monitoring tools like AWS CloudWatch,
    Azure Monitor, and GCP Operations Suite can track resource utilization, detect
    anomalies, and trigger automated actions. You can combine them with automation
    tools like AWS Lambda, Azure Functions, or GCP Cloud Functions to enable *self-healing*
    architectures. For example, if a GPU instance fails during an LLM training job,
    a function can automatically provision a replacement instance, then restart the
    job. These tools are very configurable. While you’re likely to use many preconfigured
    metrics as they are (such as those for CPU and memory usage), you should still
    configure custom metrics for your specific use case.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正如我们之前提到的，基于云的监控工具，如AWS CloudWatch、Azure Monitor和GCP Operations Suite，可以跟踪资源利用率、检测异常并触发自动化操作。您可以将它们与自动化工具（如AWS
    Lambda、Azure Functions或GCP Cloud Functions）结合使用，以实现*自愈*架构。例如，如果在LLM训练作业期间GPU实例失败，一个函数可以自动配置一个替换实例，然后重新启动作业。这些工具非常可配置。虽然您可能会使用许多预配置的指标（如CPU和内存使用率）作为它们是，但您仍然应该为您的特定用例配置自定义指标。
- en: Provisioning and Management on Owned Hardware
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在自有硬件上的配置和管理
- en: For organizations that choose to deploy LLMs on hardware they own themselves,
    whether on premises or in private clouds, automatic provisioning and management
    present unique challenges and opportunities. These setups often rely on virtualization
    technologies (like VMware, Proxmox, or Hyper-V) and containerization platforms
    (like Kubernetes or Docker Swarm) to orchestrate resources effectively.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于选择在自有的硬件上部署LLM的组织，无论是在本地还是在私有云中，自动配置和管理都带来了独特的挑战和机遇。这些设置通常依赖于虚拟化技术（如VMware、Proxmox或Hyper-V）和容器化平台（如Kubernetes或Docker
    Swarm）来有效编排资源。
- en: Deploying LLMs on owned hardware often involves deciding between “bare metal”
    servers and virtualized environments. Bare metal offers better performance and
    is well suited for resource-intensive tasks like LLM training or fine-tuning,
    especially when paired with high-end GPUs like NVIDIA A100s or H100s. However,
    virtualization provides greater flexibility, allowing multiple workloads to share
    resources. Tools like Kubernetes node pools can allocate GPU resources to pods
    dynamically, optimizing resource utilization across LLM workloads.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在自有硬件上部署LLM通常涉及在“裸金属”服务器和虚拟化环境之间做出选择。裸金属提供更好的性能，非常适合资源密集型任务，如LLM训练或微调，尤其是当与高端GPU如NVIDIA
    A100s或H100s搭配使用时。然而，虚拟化提供了更大的灵活性，允许多个工作负载共享资源。Kubernetes节点池等工具可以动态地将GPU资源分配给Pod，优化LLM工作负载的资源利用率。
- en: Just as in cloud environments, on-premises deployments can leverage IaC tools
    like Terraform, Ansible, and Chef to automate infrastructure provisioning. These
    tools enable the consistent configuration of servers, networking, and storage,
    ensuring reproducibility across environments. For example, you could use Terraform
    to define GPU-enabled nodes and Ansible to configure ML frameworks like PyTorch
    or TensorFlow on those nodes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在云环境中一样，本地部署可以利用Terraform、Ansible和Chef等IaC工具来自动化基础设施配置。这些工具能够确保服务器、网络和存储的一致配置，确保环境间的可重复性。例如，您可以使用Terraform定义启用GPU的节点，并使用Ansible在这些节点上配置机器学习框架，如PyTorch或TensorFlow。
- en: On-premises deployments require robust monitoring to track resource usage and
    performance. You can use open source tools like Prometheus and Grafana to visualize
    metrics, while workload schedulers like SLURM (Simple Linux Utility for Resource
    Management) and Kubernetes help allocate compute resources efficiently. For inference
    tasks, edge deployments may also benefit from low-latency scheduling algorithms
    to prioritize real-time requests.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本地部署需要强大的监控来跟踪资源使用和性能。您可以使用Prometheus和Grafana等开源工具来可视化指标，同时工作负载调度器如SLURM（简单Linux资源管理工具）和Kubernetes有助于高效地分配计算资源。对于推理任务，边缘部署也可能从低延迟调度算法中受益，以优先处理实时请求。
- en: Scaling on-premises infrastructure is more challenging than in the cloud, since
    it requires purchasing and provisioning additional hardware. Hybrid approaches—combining
    owned hardware with cloud resources—can address this limitation. For example,
    you might train your LLM using local GPUs but offload inference or testing workloads
    to the cloud during peak demand. However, hybrid architectures also present challenges;
    for example, your LLMOps engineer will need to configure the endpoints and parameters
    for when to send requests to different endpoints, as well as implement monitoring
    and automatic failure recovery. [Table 9-1](#ch09_table_1_1748896826210123) compares
    several aspects of managingLLMs in the cloud versus on premises.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本地基础设施的扩展比云中更具挑战性，因为它需要购买和配置额外的硬件。混合方法——结合自有硬件和云资源——可以解决这个问题。例如，您可能使用本地GPU训练LLM，但在高峰需求期间将推理或测试工作负载卸载到云上。然而，混合架构也带来了挑战；例如，您的LLMOps工程师需要配置端点以及何时向不同端点发送请求的参数，以及实现监控和自动故障恢复。[表9-1](#ch09_table_1_1748896826210123)比较了云和本地管理LLM的几个方面。
- en: Table 9-1\. Comparison of cloud and on-premises management
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-1. 云和本地管理的比较
- en: '| Aspect | Cloud architectures | Owned hardware |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 云架构 | 自有硬件 |'
- en: '| --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Scalability | Highly scalable with autoscaling tools | Limited by hardware
    availability |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性 | 可通过自动扩展工具高度可扩展 | 受硬件可用性限制 |'
- en: '| Up-front costs | Low up-front costs; pay-as-you-go model | High up-front
    costs for hardware acquisition |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 首期成本 | 首期成本较低；按需付费模式 | 硬件采购的首期成本较高 |'
- en: '| Operational costs | Variable costs based on usage | Fixed costs for power,
    cooling, and maintenance |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 运营成本 | 根据使用情况变化的可变成本 | 功耗、冷却和维护的固定成本 |'
- en: '| Performance | High for training/inference with cloud GPUs | High for specific
    workloads with bare metal |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 使用云GPU进行训练/推理时性能高 | 使用裸金属进行特定工作负载时性能高 |'
- en: '| Flexibility | Easy to provision and reconfigure resources | Requires manual
    or automated reconfiguration |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 灵活性 | 资源配置和重新配置简单 | 需要手动或自动重新配置 |'
- en: '| Control | Limited to cloud provider’s offerings | Full control over hardware
    and software stack |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 控制 | 限于云提供商的提供 | 对硬件和软件堆栈有完全控制 |'
- en: Best Practices for Automatic Infrastructure Management
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动基础设施管理的最佳实践
- en: 'Combining the flexibility of cloud platforms with the control of owned hardware
    allows organizations to leverage the best of both worlds. A few best practices
    to implement are:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 将云平台的灵活性与自有硬件的控制相结合，使组织能够利用两者的最佳优势。以下是实施时的一些最佳实践：
- en: Cloud bursting
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 云爆发
- en: With this common strategy, additional workloads are handled by the cloud during
    peak demand.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种共同策略，在高峰需求期间，额外的负载由云来处理。
- en: Using automation pipelines
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动化管道
- en: Use IaC and CI/CD pipelines to automate deployment and updates. For instance,
    Jenkins or GitHub Actions can automate resource provisioning, LLM deployment,
    and inference tasks.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基础设施即代码（IaC）和持续集成/持续部署（CI/CD）管道来自动化部署和更新。例如，Jenkins 或 GitHub Actions 可以自动化资源分配、LLM
    部署和推理任务。
- en: Optimizing for cost and performance
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 优化成本和性能
- en: Whether in the cloud or on premises, monitoring tools and scheduling algorithms
    can help you balance cost and performance. Use the cost simulators provided by
    your cloud platform or benchmark tests for owned hardware to plan your deployments.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是在云端还是本地，监控工具和调度算法可以帮助您平衡成本和性能。使用云平台提供的成本模拟器或自有硬件的基准测试来规划您的部署。
- en: Designing for high availability and redundancy
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 设计高可用性和冗余
- en: Ensure that critical LLM applications are fault tolerant by deploying resources
    across multiple zones (in the cloud) or using redundant hardware (on premises).
    Implement automated failover mechanisms to minimize downtime.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在多个区域（云端）部署资源或使用冗余硬件（本地）来确保关键 LLM 应用程序的容错性。实施自动故障转移机制以最小化停机时间。
- en: Scaling Law and the Compute-Optimal Argument
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规模定律与计算最优论证
- en: The [compute-optimal argument](https://oreil.ly/dLHHa) is a principle in ML
    model training that addresses the trade-off between model size (number of parameters)
    and the amount of data used for training, emphasizing finding a balance between
    these factors to optimize use of available computational power.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[计算最优论证](https://oreil.ly/dLHHa)是机器学习模型训练中的一个原则，它解决了模型大小（参数数量）与用于训练的数据量之间的权衡，强调在两者之间找到平衡以优化可用计算能力的使用。'
- en: This principle was formalized by DeepMind’s Chinchilla scaling law, discussed
    in [Chapter 5](ch05.html#ch05_model_domain_adaptation_for_llm_based_applications_1748896666813361),
    which revealed that many earlier LLMs, such as GPT-3, were undertrained relative
    to their size. These models used a disproportionate amount of compute to scale
    their number of parameters but didn’t implement a corresponding increase in the
    volume of training data. This imbalance resulted in suboptimal performance because
    the models had a vast number of parameters but weren’t exposed to enough training
    data to find the optimal weights for those parameters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个原则由 DeepMind 的 Chinchilla 规模定律正式化，这在[第 5 章](ch05.html#ch05_model_domain_adaptation_for_llm_based_applications_1748896666813361)中讨论，该定律揭示了许多早期的
    LLM，如 GPT-3，相对于其大小而言训练不足。这些模型使用了不成比例的计算量来扩展其参数数量，但没有实施相应的训练数据量增加。这种不平衡导致了次优性能，因为这些模型具有大量的参数，但没有足够多的训练数据来找到这些参数的最佳权重。
- en: The practical implication here is that, when allocating computational resources
    to train an LLM, you have to balance the model’s size with the amount of training
    data. That’s where the compute-optimal argument comes in. For instance, rather
    than building a massive model but training it with inadequate data, it may be
    a more effective use of resources to create a smaller model and train it more
    thoroughly on the same dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的实际意义是，在分配计算资源以训练一个 LLM 时，您必须平衡模型的大小与训练数据量。这就是计算最优论证发挥作用的地方。例如，与其构建一个庞大的模型但用不充分的数据进行训练，不如创建一个较小的模型并在同一数据集上对其进行更彻底的训练可能更有效地利用资源。
- en: A major benefit of training models with a compute-optimal balance is that they
    tend to require less retraining or fine-tuning for downstream tasks than overly
    large, undertrained models require. More modern default models like GPT-4 and
    newer versions of Claude and Gemini do apply the compute-optimal principle, making
    them better at general tasks and decreasing the need for custom fine-tuning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用计算最优平衡训练模型的主要好处是，与过度庞大且训练不足的模型相比，它们往往需要更少的再训练或微调来完成下游任务。更现代的默认模型，如 GPT-4 和
    Claude 及 Gemini 的新版本，都应用了计算最优原则，这使得它们在通用任务上表现更佳，并减少了定制微调的需求。
- en: Let’s work through a concrete example.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个具体的例子来分析。
- en: 'The Chinchilla scaling law suggests that for a compute budget *C*, the relationship
    between a model’s number of parameters *N* and the training data it uses *D* (measured
    in tokens) is:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Chinchilla缩放法则表明，对于计算预算*C*，模型参数数量*N*和它使用的训练数据*D*（以标记为单位）之间的关系是：
- en: $upper C proportional-to upper N times upper D$
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: $upper C proportional-to upper N \times upper D$
- en: Here, the $proportional-to$ symbol means to “proportional to.”
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，“$proportional-to$”符号表示“成比例于”。
- en: 'Additionally:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外：
- en: $upper D proportional-to upper N$
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: $upper D proportional-to upper N$
- en: This means that *D* should scale approximately linearly with *N*. The optimal
    proportion between tokens and parameters is between 15 and 25; that is, the number
    of tokens should be between 15 and 25 times the number of ​parameters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着*D*应该大约线性地与*N*成比例。标记和参数之间的最佳比例在15到25之间；也就是说，标记的数量应该是参数数量的15到25倍。
- en: Suppose you have a compute budget of $upper C equals 10 Superscript 23$ floating-point
    operations (FLOPs) and you want to train an LLM. Let’s explore two scenarios for
    that compute budget.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个10^(23)浮点运算（FLOPs）的计算预算，并且你想训练一个LLM。让我们探讨该计算预算的两个场景。
- en: 'Scenario 1: Overprioritize model size'
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 场景1：过度优先考虑模型大小
- en: 'Overprioritize model size by training a 200-billion-parameter model on 300
    billion tokens of data:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在3000亿个数据标记上训练一个2000亿参数的模型来过度优先考虑模型大小：
- en: $StartLayout 1st Row  upper N equals 200 times 10 Superscript 9 Baseline 2nd
    Row  upper D equals 300 times 10 Superscript 9 EndLayout$
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: $StartLayout 1st Row upper N = 200 \times 10^9 Baseline 2nd Row upper D = 300
    \times 10^9 EndLayout$
- en: Here, your *D*/*N* is 1.5 tokens per parameter, falling below the compute-optimal
    zone specified by the Chinchilla scaling law.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，你的*D*/*N*是每参数1.5个标记，低于Chinchilla缩放法则指定的计算最优区域。
- en: 'The compute required for training is proportional to *N × D*, and the exact
    proportionality *k* is unknown:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 训练所需的计算量与*N × D*成比例，而确切的成比例系数*k*是未知的：
- en: $upper C equals k times upper N times upper D$
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: $upper C = k \times upper N \times upper D$
- en: 'Substituting *N* and *D*, we obtain:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 代入*N*和*D*，我们得到：
- en: $upper C equals k times left-parenthesis 200 times 10 Superscript 9 Baseline
    right-parenthesis times left-parenthesis 300 times 10 Superscript 9 Baseline right-parenthesis
    equals k times 6 times 10 Superscript 4 Baseline times 10 Superscript 18 Baseline
    equals 6 times 10 Superscript 22$
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: $upper C = k \times (200 \times 10^9 Baseline) \times (300 \times 10^9 Baseline)
    = k \times 6 \times 10^4 Baseline \times 10^{18 Baseline} = 6 \times 10^{22}$
- en: This fits within the compute budget of 10^(23) FLOPS.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这符合10^(23) FLOPS的计算预算。
- en: 'Scenario 2: Compute-optimal strategy'
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 场景2：计算最优策略
- en: 'Use the compute-optimal strategy and train a smaller model of 50 billion parameters
    on 1 trillion (a thousand billion) tokens:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用计算最优策略，并训练一个50亿参数的小型模型，在1万亿（一千亿）个标记上：
- en: $StartLayout 1st Row  upper N equals 50 times 10 Superscript 9 Baseline 2nd
    Row  upper D equals 1 comma 000 times 10 Superscript 9 EndLayout$
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: $StartLayout 1st Row upper N = 50 \times 10^9 Baseline 2nd Row upper D = 1,000
    \times 10^9 EndLayout$
- en: Here, your *D*/*N* is 20.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，你的*D*/*N*是20。
- en: 'The compute required for training is:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 训练所需的计算量：
- en: $upper C equals k times left-parenthesis 50 times 10 Superscript 9 Baseline
    right-parenthesis times left-parenthesis 1 comma 000 times 10 Superscript 9 Baseline
    right-parenthesis equals k times 5 times 10 Superscript 4 Baseline times 10 Superscript
    18 Baseline equals 5 times 10 Superscript 22$
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: $upper C = k \times (50 \times 10^9 Baseline) \times (1,000 \times 10^9 Baseline)
    = k \times 5 \times 10^4 Baseline \times 10^{18 Baseline} = 5 \times 10^{22}$
- en: Not only does that fit within the compute budget of 10^(23) FLOPS, but the *D*/*N*
    of 20 tokens per parameter also falls within the compute-optimal zone. This finding
    indicates that training a smaller model with more data will lead to better performance
    per unit of compute.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅这符合10^(23) FLOPS的计算预算，而且每参数20个标记的*D*/*N*也位于计算最优区域内。这一发现表明，使用更多数据训练较小的模型将导致每单位计算的性能更好。
- en: Scenario 2 is a better solution, because it ensures that every parameter has
    sufficient exposure to training data, reducing overfitting and utilizing resources
    ​appropriately.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 场景2是一个更好的解决方案，因为它确保每个参数都有足够的训练数据曝光，减少过拟合并合理利用资源。
- en: Optimizing LLM Infrastructure
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化LLM基础设施
- en: Deploying and managing LLMs efficiently requires infrastructure, of course,
    but optimizing it also requires software that takes advantage of that infrastructure.
    To meet the demands of LLM training and inference, techniques such as compilers;
    parallel and distributed computing; and frameworks like CUDA (Nvidia’s Compute
    Unified Device Architecture), NCCL (NVIDIA Collective Communications Library),
    ZeRO (Zero Redundancy Optimizer), DeepSpeed, TF-Replicator, and Horovod play critical
    roles. Another key aspect of optimization is fault tolerance and backup systems.
    In an ideal situation, all your resources would go toward enhancing performance,
    but in practice, some need to be used to ensure that your system can continue
    to operate (overhead costs).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 高效部署和管理LLM需要基础设施，当然，但优化它还需要利用该基础设施的软件。为了满足LLM训练和推理的需求，编译器；并行和分布式计算；以及CUDA（Nvidia的Compute
    Unified Device Architecture）、NCCL（NVIDIA Collective Communications Library）、ZeRO（Zero
    Redundancy Optimizer）、DeepSpeed、TF-Replicator和Horovod等框架等技术发挥着关键作用。优化的另一个关键方面是容错和备份系统。在理想情况下，所有资源都应用于提高性能，但在实践中，一些资源需要用于确保系统可以继续运行（开销成本）。
- en: '*Compilers* translate high-level code into machine instructions optimized for
    specific hardware architectures. For LLM workloads, which demand high computational
    efficiency, you need specialized compilers such as NVIDIA’s NVCC (for CUDA), TensorFlow’s
    XLA, or PyTorch’s TorchScript. These compilers focus on achieving three types
    of optimizations: kernel fusion, precision scaling, and hardware utilization.
    Let’s look at each type in turn.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*编译器* 将高级代码转换为针对特定硬件架构优化的机器指令。对于需要高计算效率的LLM工作负载，您需要专门的编译器，如NVIDIA的NVCC（用于CUDA）、TensorFlow的XLA或PyTorch的TorchScript。这些编译器专注于实现三种类型的优化：内核融合、精确缩放和硬件利用。让我们依次看看每种类型。'
- en: Kernel Fusion
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内核融合
- en: '*Kernel fusion* is a technique where multiple computational operations are
    combined into a single GPU kernel to reduce memory traffic and execution overhead.
    In deep-learning workflows, operations like matrix multiplications, element-wise
    additions, and activations often occur sequentially. Without kernel fusion, these
    operations would each involve separate memory read/write operations, “going out
    of the core” to save intermediate results and then “going out of the core” again
    to read these intermediate results. The repeated need to access global memory
    leads to latency and inefficiency. Compilers thus identify opportunities to combine
    (or *fuse*) these operations. The benefits of fusing include:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*内核融合* 是一种技术，将多个计算操作组合成一个单独的GPU内核，以减少内存流量和执行开销。在深度学习工作流程中，矩阵乘法、逐元素加法和激活等操作通常按顺序发生。没有内核融合，这些操作将涉及单独的内存读写操作，“离开核心”以保存中间结果，然后再次“离开核心”以读取这些中间结果。重复访问全局内存会导致延迟和低效。因此，编译器会识别出合并（或
    *融合*）这些操作的机会。融合的好处包括：'
- en: Reduced memory access
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 减少内存访问
- en: Intermediate results are stored in faster, low-latency GPU registers or shared
    memory, rather than being written back to global memory.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 中间结果存储在更快、低延迟的GPU寄存器或共享内存中，而不是写回全局内存。
- en: Minimized kernel launch overhead
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化内核启动开销
- en: Each kernel launch has a computational overhead. Fused kernels require fewer
    launches, speeding up execution.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 每次内核启动都有计算开销。融合内核需要更少的启动次数，从而加快执行速度。
- en: Improved cache efficiency
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 提高缓存效率
- en: Fusion allows related operations to share data in memory more effectively, reducing
    cache misses.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 融合允许相关操作在内存中更有效地共享数据，减少缓存未命中。
- en: For example, a typical deep-learning evaluation sequence like `ReLU(Wx + b)`,
    where `W` and `b` are the weights and biases, can be fused into a single kernel
    that computes the matrix multiplication (`Wx`), adds the bias (`+b`), and applies
    the activation function (`ReLU`) without having to write each intermediate step
    in the global memory outside of the GPU.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个典型的深度学习评估序列如 `ReLU(Wx + b)`，其中 `W` 和 `b` 是权重和偏差，可以被融合成一个单独的内核，该内核计算矩阵乘法
    (`Wx`)，添加偏差 (`+b`)，并应用激活函数 (`ReLU`)，而无需在GPU外部全局内存中逐个写入每个中间步骤。
- en: Precision Scaling
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确缩放
- en: 'Deep-learning workloads often involve numerical computations that don’t require
    high precision. *Precision scaling* enables models to use lower-precision formats
    like 16-bit floating point (FP16) or brain floating point (BF16) instead of the
    traditional 32-bit floating point (FP32) format. Compilers help by:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习工作负载通常涉及不需要高精度的数值计算。*精度缩放*使模型能够使用16位浮点（FP16）或脑浮点（BF16）等低精度格式，而不是传统的32位浮点（FP32）格式。编译器通过以下方式帮助：
- en: Automating mixed-precision training
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化混合精度训练
- en: Compilers like NVIDIA’s APEX (for PyTorch) and TensorFlow’s Mixed Precision
    API automatically downscale certain operations to FP16 while maintaining critical
    operations (such as gradient accumulation) in FP32\. This ensures numerical stability
    while reducing memory usage and speeding computation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于NVIDIA的APEX（用于PyTorch）和TensorFlow的混合精度API的编译器会自动将某些操作下缩到FP16，同时保持关键操作（如梯度累积）在FP32中。这确保了数值稳定性，同时减少了内存使用并加快了计算速度。
- en: Leveraging specialized hardware
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 利用专用硬件
- en: Modern GPUs (like NVIDIA’s A100 or H100) include tensor cores optimized for
    lower precision. Compilers can translate high-level operations into instructions
    that specifically use these lower-precision cores, significantly speeding up matrix
    multiplications and other tensor operations while freeing up the higher-precision
    cores for operations that require them.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现代GPU（如NVIDIA的A100或H100）包括针对低精度优化的张量核心。编译器可以将高级操作转换为特定于这些低精度核心的指令，从而显著加快矩阵乘法和其他张量操作，同时为需要高精度的操作释放高精度核心。
- en: Enhancing memory efficiency
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 提高内存效率
- en: By reducing precision, models can consume less memory, which lets you use larger
    batch sizes or train on hardware with lower memory capacity.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过降低精度，模型可以消耗更少的内存，这让你可以使用更大的批量大小或在内存容量较低的硬件上进行训练。
- en: Hardware Utilization
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件利用率
- en: Efficient hardware utilization ensures that GPUs or other accelerators operate
    at their full potential, maximizing computational throughput. Modern hardware
    can include specialized units such as tensor cores, matrix multiplication units,
    and vector processors. Compilers map operations like general matrix multiplication
    to these specialized units, leveraging their high throughput and freeing more
    general resources for other tasks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的硬件利用率确保GPU或其他加速器在其全部潜力下运行，最大化计算吞吐量。现代硬件可以包括专门的单元，如张量核心、矩阵乘法单元和向量处理器。编译器将诸如通用矩阵乘法之类的操作映射到这些专用单元，利用它们的高吞吐量，并为其他任务释放更多通用资源。
- en: '*Instruction-level parallelism* is another way AI-specialized compilers can
    optimize hardware utilization. They can generate code that exploits parallelism
    at multiple levels, including at the thread level (using thousands of GPU threads)
    and the vector level.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*指令级并行性*是AI专用编译器优化硬件利用率的另一种方式。它们可以生成在多个级别上利用并行性的代码，包括线程级别（使用数千个GPU线程）和向量级别。'
- en: '*AI-specialized compilers* know how memory is organized in modern GPUs and
    AI servers, so their memory hierarchies are especially efficient. They optimize
    the code to use shared memory, registers, and caches effectively and reduce reliance
    on the slower global memory.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*AI专用编译器*了解现代GPU和AI服务器中内存的布局，因此它们的内存层次结构特别高效。它们优化代码以有效地使用共享内存、寄存器和缓存，并减少对较慢的全局内存的依赖。'
- en: Parallel and Distributed Computing for LLMs
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对大型语言模型（LLMs）的并行和分布式计算
- en: Large-scale LLMs require parallel and distributed computing to manage their
    immense computational and memory demands. Techniques like *data parallelism*,
    *model parallelism*, and *pipeline parallelism* distribute the workload across
    multiple processors or nodes to use hardware resources efficiently. The building
    blocks of these techniques are the CUDA and NCCL frameworks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模LLMs需要并行和分布式计算来管理其巨大的计算和内存需求。像*数据并行性*、*模型并行性*和*流水线并行性*等技术将工作负载分配到多个处理器或节点，以有效地使用硬件资源。这些技术的构建块是CUDA和NCCL框架。
- en: NVIDIA’s CUDA is the cornerstone of GPU-based acceleration, providing APIs for
    high-performance parallel computing. It lets developers write code that directly
    utilizes GPUs’ processing power, which is critical for LLM tasks like matrix multiplication,
    attention mechanisms, and gradient computations. Even very small language models
    depend on CUDA to run with acceptable performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA的CUDA是GPU加速的基础，提供了高性能并行计算的API。它允许开发者编写直接利用GPU处理能力的代码，这对于LLM任务（如矩阵乘法、注意力机制和梯度计算）至关重要。即使是非常小的语言模型也依赖于CUDA以实现可接受的性能。
- en: The NCCL complements CUDA by optimizing communication among multiple GPUs. It
    provides primitives for data movement, such as `all-reduce`, `all-gather`, and
    `broadcast`, ensuring minimal latency and high bandwidth. This is particularly
    important in distributed training, where model gradients frequently need to be
    synchronized between GPUs. As models grow, they tend to require multiple GPUs,
    and NCCL provides APIs that let different GPUs communicate.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: NCCL 通过优化多个 GPU 之间的通信来补充 CUDA。它提供了数据移动的原语，如 `all-reduce`、`all-gather` 和 `broadcast`，确保最小延迟和高带宽。这在分布式训练中尤为重要，因为模型梯度经常需要在
    GPU 之间同步。随着模型的增长，它们往往需要多个 GPU，NCCL 提供了允许不同 GPU 通信的 API。
- en: Data Parallelism
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据并行
- en: '*Data parallelism* involves splitting training dataset into chunks, each corresponding
    to a device (such as a GPU or TPU). Each device processes its own chunk in parallel
    during a training iteration. You then place an identical copy of the model on
    each device, which computes the gradients for its chunk of data. Next, the gradients
    are averaged and synchronized across devices, using communication primitives like
    `all-reduce`, and the averaged gradients are applied to update the model’s parameters
    on each device.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据并行* 涉及将训练数据集分割成块，每个块对应一个设备（如 GPU 或 TPU）。每个设备在训练迭代期间并行处理其自己的块。然后，在每个设备上放置模型的相同副本，该副本计算其数据块的梯度。接下来，使用通信原语如
    `all-reduce` 对梯度进行平均并同步到设备，然后将平均梯度应用于更新每个设备上的模型参数。'
- en: Model Parallelism
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型并行
- en: '*Model parallelism* divides the model itself across multiple devices, making
    each device responsible for a portion of the model, such as a few layers (or operations).
    This is useful when the model is too large to fit on a single device. You then
    pass the input through the model sequentially, moving intermediate outputs between
    devices as needed; this is called a *forward pass*. Next, in a *backward pass*,
    you compute the gradients for each layer in reverse order. This helps to synchronize
    across devices for gradient flow. Finally, parameters are updated, either independently
    on each device or via a central parameter server.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型并行* 将模型本身分割到多个设备上，使每个设备负责模型的一部分，例如几个层（或操作）。当模型太大而无法适应单个设备时，这很有用。然后，您按顺序通过模型传递输入，根据需要在不同设备之间移动中间输出；这被称为
    *正向传递*。接下来，在 *反向传递* 中，您按相反顺序计算每个层的梯度。这有助于同步设备以实现梯度流动。最后，更新参数，要么在每个设备上独立更新，要么通过中央参数服务器更新。'
- en: Model parallelism optimizes memory but at the cost of throughput; while one
    device works on an input, the other devices wait.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行优化了内存使用，但以吞吐量为代价；当一个设备处理输入时，其他设备处于等待状态。
- en: Pipeline Parallelism
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流水线并行
- en: '*Pipeline parallelism* also divides the model, assigning different layers to
    different devices, much like in model parallelism. However, in pipeline parallelism,
    the data batches are broken up into smaller pieces so that as many devices as
    possible are occupied at any given time. This requires additional communication
    but reduces idle compute time.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*流水线并行* 也将模型分割，将不同的层分配给不同的设备，类似于模型并行。然而，在流水线并行中，数据批次被分割成更小的块，以便尽可能多的设备在任何给定时间都被占用。这需要额外的通信，但可以减少空闲计算时间。'
- en: '[Figure 9-1](#ch09_figure_1_1748896826206032) shows an example of implementing
    pipeline parallelism with four devices and breaking the batch data into four micro-batches.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-1](#ch09_figure_1_1748896826206032) 展示了使用四个设备实现流水线并行并将批次数据分割成四个微批次的示例。'
- en: '![](assets/llmo_0901.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/llmo_0901.png)'
- en: Figure 9-1\. Implementing pipeline parallelism
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1\. 实现流水线并行
- en: Pipeline parallelism is very effective in speeding up the training of models
    with a smaller hardware footprint, but it used to be hard to implement. In 2022,
    Meta released Pipeline Parallelism for PyTorch, or [PiPPy](https://oreil.ly/g5EZ3).
    PiPPy was merged into the main PyTorch distribution as the `torch.distributed.pipelining`
    subpackage and no longer requires a separate installation.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线并行在加速具有较小硬件足迹的模型训练方面非常有效，但过去实现起来很困难。2022 年，Meta 发布了针对 PyTorch 的流水线并行，或称为
    [PiPPy](https://oreil.ly/g5EZ3)。PiPPy 已合并到 PyTorch 的主分布中，作为 `torch.distributed.pipelining`
    子包，不再需要单独安装。
- en: 'Advanced Frameworks: ZeRO and DeepSpeed'
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级框架：ZeRO 和 DeepSpeed
- en: Developed by Microsoft, ZeRO minimizes memory overhead during training by partitioning
    model states (like parameters, gradients, and optimizer states) across devices.
    This lets you train models with tens or even hundreds of billions of parameters
    without requiring GPUs with excessive memory capacities.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由微软开发的ZeRO通过在设备间划分模型状态（如参数、梯度和优化器状态）来最小化训练过程中的内存开销。这使得您能够在不需要具有过多内存容量的GPU的情况下，训练具有数十亿甚至数百亿参数的模型。
- en: Built on top of ZeRO, DeepSpeed is a deep-learning optimization library that
    makes training large models more efficient. It provides features like mixed-precision
    training, gradient accumulation, and memory optimization, significantly reducing
    training time and cost.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在ZeRO之上，DeepSpeed是一个深度学习优化库，它使训练大型模型更加高效。它提供了混合精度训练、梯度累积和内存优化等功能，显著减少了训练时间和成本。
- en: '[Table 9-2](#table0902) summarizes when to use each technique mentioned.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[表9-2](#table0902)总结了何时使用所提到的每种技术。'
- en: Table 9-2\. A comparison of different memory optimization techniques
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-2. 不同内存优化技术的比较
- en: '| Technique | The problem it solves | How it works | Trade-off |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 它解决的问题 | 它是如何工作的 | 折衷方案 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Sharding | Model too big for one GPU | Split model weights/layers across
    multiple GPUs | Increased complexity in syncing and communication |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 分片 | 模型太大，无法在一个GPU上运行 | 将模型权重/层分散到多个GPU上 | 同步和通信的复杂性增加 |'
- en: '| Activation checkpointing | High memory use during backprop | Save only key
    activations and recompute the rest later | Extra compute time |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 激活检查点 | 反向传播期间的内存使用量高 | 仅保存关键激活并稍后重新计算其余部分 | 额外的计算时间 |'
- en: '| Dynamic batching | Wasted compute on small requests | Group inputs on the
    fly to maximize GPU use | Slight response delay |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 动态批量 | 小请求上的浪费计算 | 飞行中分组输入以最大化GPU使用 | 略有响应延迟 |'
- en: '| Model offloading | GPU can’t hold entire model | Move unused parts to CPU
    or disk; fetch when needed | Slower due to transfer time |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 模型卸载 | GPU无法容纳整个模型 | 将未使用的部分移动到CPU或磁盘；需要时再获取 | 由于传输时间而变慢 |'
- en: '| Mixed precision training | Activations and weights take too much space |
    Use lower-precision (e.g., FP16) instead of FP32 | Slight loss in numerical accuracy
    (often negligible) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 混合精度训练 | 激活和权重占用太多空间 | 使用低精度（例如，FP16）而不是FP32 | 数值精度略有损失（通常可以忽略不计） |'
- en: '| Quantization | Models are too large for deployment | Compress weights to
    8-bit or lower | Potential accuracy loss if not careful |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 模型太大，无法部署 | 将权重压缩到8位或更低 | 如果不小心，可能会损失精度 |'
- en: '| Gradient accumulation | Batch size too big for GPU | Split one big batch
    into smaller chunks and accumulate gradients | Slower iteration time |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 梯度累积 | 批次大小太大，无法使用GPU | 将一个大批次分成更小的块并累积梯度 | 迭代时间较慢 |'
- en: '| Zero Redundancy Optimizer (ZeRO) | Redundant optimizer state across GPUs
    | Partition optimizer state and gradients across devices | Complexity and comm
    overhead |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 零冗余优化器（ZeRO） | GPU间的冗余优化器状态 | 在设备间划分优化器状态和梯度 | 复杂性和通信开销 |'
- en: '| Operator fusion | Too many small intermediate tensors | Combine multiple
    ops into one to reduce memory ops | Needs compiler/tooling support |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 操作融合 | 太多小的中间张量 | 将多个操作组合成一个以减少内存操作 | 需要编译器/工具支持 |'
- en: '| Paged attention (for inference) | Memory spikes from long contexts | Stream
    key–value cache in and out like virtual memory | Requires smart scheduling |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 分页注意力（用于推理） | 长上下文导致的内存峰值 | 类似虚拟内存的流式键值缓存进出 | 需要智能调度 |'
- en: Backup and Failsafe Processes for LLM Applications
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM应用的备份和故障安全流程
- en: In LLM applications, the LLMOps engineer is usually responsible for managing
    backups. Failures do happen, due to hardware malfunctions, software issues, or
    even malicious activity. LLM engineers can mitigate risk with robust backup and
    failsafe strategies to ensure continuity and minimize downtime.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM应用中，LLMOps工程师通常负责管理备份。由于硬件故障、软件问题或甚至恶意活动，故障确实会发生。LLM工程师可以通过强大的备份和故障安全策略来降低风险，确保连续性和最小化停机时间。
- en: The name of these activities can be misleading. Having a well-documented, regularly
    tested restore strategy is just as important as having good backups. It’s far
    too common for longtime practitioners to have “war stories” of occasions when
    backups were done for years but never tested and that, when actually required,
    did not work.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这些活动的名称可能会误导。拥有良好的文档记录和定期测试的恢复策略与拥有良好的备份一样重要。长期从业者有“战争故事”的情况很常见，即备份已经进行了多年但从未测试，而在实际需要时却不起作用。
- en: Which artifacts the LLMOps engineer backs up will vary, depending on the stage
    of the model and application lifecycle. During development, the most common artifacts
    to back up are the training data and intermediate model weights (model checkpoints),
    as well as the files describing the training as infrastructure as code. Datasets
    for training and model checkpoints tend to be very large, while the IaC files
    tend to be small.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps工程师备份哪些工件将根据模型和应用生命周期的阶段而变化。在开发阶段，最常备份的工件是训练数据和中间模型权重（模型检查点），以及描述训练的基础设施即代码的文件。训练数据和模型检查点往往非常大，而IaC文件则相对较小。
- en: As the application moves to production, the IaC files representing the production
    architecture should be backed up, as well as user data (such as query logs and
    personalizations) and performance metrics. LLMs depend on large datasets, and
    losing preprocessed or fine-tuning data can be extremely costly. Backups safeguard
    the data from corruption or accidental deletion. Training LLMs is also computationally
    expensive, so backups of model checkpoints can make a big difference in the event
    of a failure or data corruption, preserving progress. Furthermore, many industries
    and jurisdictions have compliance standards that require data to be backed up
    for auditability and accountability.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用迁移到生产环境，代表生产架构的IaC文件应该进行备份，以及用户数据（如查询日志和个人化设置）和性能指标。LLM依赖于大量数据集，丢失预处理或微调数据可能代价极高。备份可以保护数据免受损坏或意外删除。训练LLM也是计算密集型的，因此模型检查点的备份在发生故障或数据损坏的情况下可以发挥重要作用，从而保留进度。此外，许多行业和司法管辖区都有合规标准，要求数据备份以供审计和问责。
- en: Types of Backup Strategies
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 备份策略类型
- en: 'Backup strategies for LLM applications fall into three basic categories: full,
    incremental, and differential. Let’s examine these more closely:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: LLM应用的备份策略分为三个基本类别：完全备份、增量备份和差异备份。让我们更详细地考察这些：
- en: Full backups
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 完全备份
- en: Full backups capture an *entire* dataset or model at a specific point in time.
    While they require significant storage, they are comprehensive and straightforward
    to restore.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 完全备份在特定时间点捕获 *整个* 数据集或模型。虽然它们需要大量的存储空间，但它们是全面且恢复简单的。
- en: Incremental backups
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 增量备份
- en: '*Incremental backups* store only the changes made since the last (full or incremental)
    backup, to reduce storage requirements. To restore, you need the entire historical
    sequence of data; even a single missing data block will cause the restore to fail.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*增量备份* 仅存储自上次（完全或增量）备份以来所做的更改，以减少存储需求。要恢复，您需要整个历史数据序列；即使缺少一个数据块也会导致恢复失败。'
- en: Differential backups
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 差异备份
- en: '*Differential backups* save the changes made since the last *full* backup,
    balancing storage efficiency and recovery speed. To restore, you need the latest
    full backup and the latest differential backup.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*差异备份* 保存自上次 *完全备份* 以来所做的更改，平衡存储效率和恢复速度。要恢复，您需要最新的完全备份和最新的差异备份。'
- en: Your choice of backup strategy depends on a few factors. High-stakes applications
    require more frequent backups and redundancy and often less downtime as well.
    Restores for critical applications need to be fast and trouble-free, so frequent
    full backups are usually recommended.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您选择的备份策略取决于几个因素。高风险应用需要更频繁的备份和冗余，以及更少的停机时间。对于关键应用，恢复需要快速且无故障，因此通常建议进行频繁的完全备份。
- en: Data volume is also an important factor. Incremental or differential backups
    can help minimize storage overhead for large datasets like those used in LLM applications,
    since making full copies daily consumes expensive time and storage.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量也是一个重要因素。增量或差异备份可以帮助最小化LLM应用中使用的类似大数据集的存储开销，因为每天制作完整副本会消耗昂贵的时间和存储空间。
- en: In volatile systems where the data changes rapidly, such as active fine-tuning
    environments, frequent backups are a particularly good idea. If the data volume
    is small, these can even be full backups. For relatively static systems like deployed
    inference models, however, you can have a lower frequency of backups (for example,
    weekly).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据变化迅速的易变系统中，例如活跃的微调环境，频繁的备份尤其是个好主意。如果数据量小，这些甚至可以是完全备份。然而，对于相对静态的系统，例如部署的推理模型，备份的频率可以较低（例如，每周一次）。
- en: 'The Most Important Practice: Test Restores Regularly'
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最重要的实践：定期测试恢复
- en: Regardless of your backup strategy, it is *imperative* that you regularly test
    the restore process. For example, large backups are often placed in cold storage,
    which is a lot cheaper than hot storage. *Hot storage* is somewhat similar to
    having a folder on the cloud, in that you can access files immediately. *Cold
    storage* is more like keeping a disk in a warehouse—it takes a while to access
    the data, sometimes as long as a few days. An LLMOps engineer can go from hero
    to zero quickly by saying, “Don’t worry, I have all the data backed up; however,
    production will be down for two weeks while I retrieve it.”
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您的备份策略如何，定期测试恢复过程都是**必不可少的**。例如，大型备份通常存放在冷存储中，这比热存储便宜得多。*热存储*某种程度上类似于在云上有一个文件夹，您可以立即访问文件。*冷存储*更像是将磁盘存放在仓库中——访问数据需要一段时间，有时长达几天。一个LLMOps工程师可能会迅速从英雄变成零，他说：“别担心，我已经备份了所有数据；然而，由于我需要检索数据，生产将中断两周。”
- en: Conclusion
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Managing LLM infrastructure and resources requires different approaches depending
    on whether you’re running them on custom cloud infrastructure or owned hardware.
    Your choice of deployment strategy should consider cost, scalability, data privacy,
    and operational complexity. Regardless of the choice of infrastructure, LLMOps
    engineers have to monitor and evaluate performance to ensure that their deployments
    remain efficient and reliable.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您是在自定义云基础设施上运行LLM还是使用自有硬件，管理LLM基础设施和资源需要不同的方法。您的部署策略选择应考虑成本、可扩展性、数据隐私和运营复杂性。无论选择哪种基础设施，LLMOps工程师都必须监控和评估性能，以确保其部署保持高效和可靠。
- en: Scaling LLM infrastructure effectively requires advanced tools, like special
    compilers that optimize hardware usage, and techniques, such as balancing model
    size and training data to improve performance at a given cost. Understanding and
    implementing parallelism strategies lets you train and deploy even the largest
    models.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有效扩展LLM基础设施需要先进的工具，如优化硬件使用的特殊编译器，以及平衡模型大小和训练数据以在给定成本下提高性能的技术。理解和实施并行策略让您能够训练和部署甚至最大的模型。
- en: It’s crucial to have good backup strategies and to test restores regularly,
    identifying potential failure points. Integrating these best practices will help
    you deploy resilient, high-performance AI-driven applications that meet your customers’
    demands.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有良好的备份策略并定期测试恢复过程至关重要，以识别潜在的故障点。整合这些最佳实践将帮助您部署具有弹性和高性能、满足客户需求的AI驱动应用程序。
- en: References
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Hoffman, Jordan, et al. [“Training Compute-Optimal Large Language Models”](https://oreil.ly/dLHHa),
    arXiv, March 29, 2022.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Hoffman, Jordan, 等人. [“训练计算最优的大型语言模型”](https://oreil.ly/dLHHa), arXiv, 2022年3月29日。
- en: Mueller, Z. R. [PiPPy](https://oreil.ly/g5EZ3), PyTorch, September 2024.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Mueller, Z. R. [PiPPy](https://oreil.ly/g5EZ3), PyTorch, 九月 2024.
