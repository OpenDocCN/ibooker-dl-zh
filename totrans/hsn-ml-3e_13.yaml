- en: Chapter 11\. Training Deep Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。训练深度神经网络
- en: 'In [Chapter 10](ch10.html#ann_chapter) you built, trained, and fine-tuned your
    first artificial neural networks. But they were shallow nets, with just a few
    hidden layers. What if you need to tackle a complex problem, such as detecting
    hundreds of types of objects in high-resolution images? You may need to train
    a much deeper ANN, perhaps with 10 layers or many more, each containing hundreds
    of neurons, linked by hundreds of thousands of connections. Training a deep neural
    network isn’t a walk in the park. Here are some of the problems you could run
    into:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#ann_chapter)中，您构建、训练和微调了您的第一个人工神经网络。但它们是浅层网络，只有几个隐藏层。如果您需要解决一个复杂的问题，比如在高分辨率图像中检测数百种对象，您可能需要训练一个更深的人工神经网络，也许有10层或更多层，每一层包含数百个神经元，通过数十万个连接相连。训练深度神经网络并不是一件轻松的事情。以下是您可能遇到的一些问题：
- en: You may be faced with the problem of gradients growing ever smaller or larger,
    when flowing backward through the DNN during training. Both of these problems
    make lower layers very hard to train.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，当反向传播通过DNN向后流动时，您可能会面临梯度变得越来越小或越来越大的问题。这两个问题都会使得较低层非常难以训练。
- en: You might not have enough training data for such a large network, or it might
    be too costly to label.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能没有足够的训练数据来训练这样一个庞大的网络，或者标记成本太高。
- en: Training may be extremely slow.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练可能会非常缓慢。
- en: A model with millions of parameters would severely risk overfitting the training
    set, especially if there are not enough training instances or if they are too
    noisy.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个拥有数百万参数的模型会严重增加过拟合训练集的风险，特别是如果训练实例不足或者太嘈杂。
- en: In this chapter we will go through each of these problems and present techniques
    to solve them. We will start by exploring the vanishing and exploding gradients
    problems and some of their most popular solutions. Next, we will look at transfer
    learning and unsupervised pretraining, which can help you tackle complex tasks
    even when you have little labeled data. Then we will discuss various optimizers
    that can speed up training large models tremendously. Finally, we will cover a
    few popular regularization techniques for large neural networks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将逐个讨论这些问题，并提出解决方法。我们将首先探讨梯度消失和梯度爆炸问题以及它们最流行的解决方案。接下来，我们将看看迁移学习和无监督预训练，这可以帮助您解决复杂任务，即使您只有很少的标记数据。然后，我们将讨论各种优化器，可以极大地加快训练大型模型。最后，我们将介绍一些用于大型神经网络的流行正则化技术。
- en: With these tools, you will be able to train very deep nets. Welcome to deep
    learning!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些工具，您将能够训练非常深的网络。欢迎来到深度学习！
- en: The Vanishing/Exploding Gradients Problems
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度消失/爆炸问题
- en: As discussed in [Chapter 10](ch10.html#ann_chapter), the backpropagation algorithm’s
    second phase works by going from the output layer to the input layer, propagating
    the error gradient along the way. Once the algorithm has computed the gradient
    of the cost function with regard to each parameter in the network, it uses these
    gradients to update each parameter with a gradient descent step.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第10章](ch10.html#ann_chapter)中讨论的那样，反向传播算法的第二阶段是从输出层到输入层，沿途传播错误梯度。一旦算法计算出网络中每个参数相对于成本函数的梯度，它就会使用这些梯度来更新每个参数，进行梯度下降步骤。
- en: 'Unfortunately, gradients often get smaller and smaller as the algorithm progresses
    down to the lower layers. As a result, the gradient descent update leaves the
    lower layers’ connection weights virtually unchanged, and training never converges
    to a good solution. This is called the *vanishing gradients* problem. In some
    cases, the opposite can happen: the gradients can grow bigger and bigger until
    layers get insanely large weight updates and the algorithm diverges. This is the
    *exploding gradients* problem, which surfaces most often in recurrent neural networks
    (see [Chapter 15](ch15.html#rnn_chapter)). More generally, deep neural networks
    suffer from unstable gradients; different layers may learn at widely different
    speeds.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，随着算法向下进行到更低的层，梯度通常会变得越来越小。结果是，梯度下降更新几乎不会改变较低层的连接权重，训练永远不会收敛到一个好的解决方案。这被称为*梯度消失*问题。在某些情况下，相反的情况可能发生：梯度会变得越来越大，直到层的权重更新变得非常大，算法发散。这是*梯度爆炸*问题，最常出现在递归神经网络中（参见[第15章](ch15.html#rnn_chapter)）。更一般地说，深度神经网络受到不稳定梯度的困扰；不同层可能以非常不同的速度学习。
- en: This unfortunate behavior was empirically observed long ago, and it was one
    of the reasons deep neural networks were mostly abandoned in the early 2000s.
    It wasn’t clear what caused the gradients to be so unstable when training a DNN,
    but some light was shed in a [2010 paper](https://homl.info/47) by Xavier Glorot
    and Yoshua Bengio.⁠^([1](ch11.html#idm45720199815184)) The authors found a few
    suspects, including the combination of the popular sigmoid (logistic) activation
    function and the weight initialization technique that was most popular at the
    time (i.e., a normal distribution with a mean of 0 and a standard deviation of
    1). In short, they showed that with this activation function and this initialization
    scheme, the variance of the outputs of each layer is much greater than the variance
    of its inputs. Going forward in the network, the variance keeps increasing after
    each layer until the activation function saturates at the top layers. This saturation
    is actually made worse by the fact that the sigmoid function has a mean of 0.5,
    not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better
    than the sigmoid function in deep networks).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在-r和+r之间的均匀分布，r = sqrt(3 / fan_avg)
- en: Looking at the sigmoid activation function (see [Figure 11-1](#sigmoid_saturation_plot)),
    you can see that when inputs become large (negative or positive), the function
    saturates at 0 or 1, with a derivative extremely close to 0 (i.e., the curve is
    flat at both extremes). Thus, when backpropagation kicks in it has virtually no
    gradient to propagate back through the network, and what little gradient exists
    keeps getting diluted as backpropagation progresses down through the top layers,
    so there is really nothing left for the lower layers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文中，Glorot和Bengio提出了一种显著减轻不稳定梯度问题的方法。他们指出，我们需要信号在两个方向上正确地流动：在前向方向进行预测时，以及在反向方向进行反向传播梯度时。我们不希望信号消失，也不希望它爆炸和饱和。为了使信号正确地流动，作者认为每一层的输出方差应该等于其输入方差，并且在反向方向通过一层之后，梯度在前后具有相等的方差（如果您对数学细节感兴趣，请查看论文）。实际上，除非层具有相等数量的输入和输出（这些数字称为层的*fan-in*和*fan-out*），否则不可能保证两者都相等，但Glorot和Bengio提出了一个在实践中被证明非常有效的良好折衷方案：每层的连接权重必须随机初始化，如[方程11-1](#xavier_initialization_equation)所述，其中*fan*[avg]
    = (*fan*[in] + *fan*[out]) / 2。这种初始化策略称为*Xavier初始化*或*Glorot初始化*，以论文的第一作者命名。
- en: '![mls3 1101](assets/mls3_1101.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: 观察Sigmoid激活函数（参见[图11-1](#sigmoid_saturation_plot)），您会发现当输入变大（负或正）时，函数在0或1处饱和，导数非常接近0（即曲线在两个极端处平坦）。因此，当反向传播开始时，几乎没有梯度可以通过网络向后传播，存在的微小梯度会随着反向传播通过顶层逐渐稀释，因此对于较低层几乎没有剩余的梯度。
- en: Figure 11-1\. Sigmoid activation function saturation
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1。Sigmoid激活函数饱和
- en: Glorot and He Initialization
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Glorot和He初始化
- en: 'In their paper, Glorot and Bengio propose a way to significantly alleviate
    the unstable gradients problem. They point out that we need the signal to flow
    properly in both directions: in the forward direction when making predictions,
    and in the reverse direction when backpropagating gradients. We don’t want the
    signal to die out, nor do we want it to explode and saturate. For the signal to
    flow properly, the authors argue that we need the variance of the outputs of each
    layer to be equal to the variance of its inputs,⁠^([2](ch11.html#idm45720199802080))
    and we need the gradients to have equal variance before and after flowing through
    a layer in the reverse direction (please check out the paper if you are interested
    in the mathematical details). It is actually not possible to guarantee both unless
    the layer has an equal number of inputs and outputs (these numbers are called
    the *fan-in* and *fan-out* of the layer), but Glorot and Bengio proposed a good
    compromise that has proven to work very well in practice: the connection weights
    of each layer must be initialized randomly as described in [Equation 11-1](#xavier_initialization_equation),
    where *fan*[avg] = (*fan*[in] + *fan*[out]) / 2\. This initialization strategy
    is called *Xavier initialization* or *Glorot initialization*, after the paper’s
    first author.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不幸的行为早在很久以前就被经验性地观察到，这也是深度神经网络在2000年代初大多被放弃的原因之一。当训练DNN时，梯度不稳定的原因并不清楚，但在2010年的一篇论文中，Xavier
    Glorot和Yoshua Bengio揭示了一些端倪。作者发现了一些嫌疑人，包括当时最流行的Sigmoid（逻辑）激活函数和权重初始化技术的组合（即均值为0，标准差为1的正态分布）。简而言之，他们表明，使用这种激活函数和初始化方案，每一层的输出方差远大于其输入方差。在网络中前进，每一层的方差在每一层之后都会增加，直到激活函数在顶层饱和。实际上，这种饱和现象被sigmoid函数的均值为0.5而不是0所加剧（双曲正切函数的均值为0，在深度网络中的表现略好于sigmoid函数）。
- en: Equation 11-1\. Glorot initialization (when using the sigmoid activation function)
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-1。Glorot初始化（使用Sigmoid激活函数时）
- en: <math display="block"><mtable columnalign="left"><mtr><mtd><mtext>Normal distribution
    with mean 0 and variance </mtext><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><msub><mi
    mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></mtd></mtr><mtr><mtd><mtext>Or
    a uniform distribution between </mtext><mo>-</mo><mi>r</mi><mtext> and </mtext><mo>+</mo><mi>r</mi><mtext>,
    with </mtext><mi>r</mi><mo>=</mo><msqrt><mfrac><mn>3</mn><msub><mi mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></msqrt></mtd></mtr></mtable></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布，均值为0，方差为σ^2 = 1 / fan_avg
- en: 'If you replace *fan*[avg] with *fan*[in] in [Equation 11-1](#xavier_initialization_equation),
    you get an initialization strategy that Yann LeCun proposed in the 1990s. He called
    it *LeCun initialization*. Genevieve Orr and Klaus-Robert Müller even recommended
    it in their 1998 book *Neural Networks: Tricks of the Trade* (Springer). LeCun
    initialization is equivalent to Glorot initialization when *fan*[in] = *fan*[out].
    It took over a decade for researchers to realize how important this trick is.
    Using Glorot initialization can speed up training considerably, and it is one
    of the practices that led to the success of deep learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您在[方程式11-1](#xavier_initialization_equation)中用*fan*[in]替换*fan*[avg]，您将得到Yann
    LeCun在1990年代提出的初始化策略。他称之为*LeCun初始化*。Genevieve Orr和Klaus-Robert Müller甚至在他们1998年的书*Neural
    Networks: Tricks of the Trade*（Springer）中推荐了这种方法。当*fan*[in] = *fan*[out]时，LeCun初始化等同于Glorot初始化。研究人员花了十多年的时间才意识到这个技巧有多重要。使用Glorot初始化可以显著加快训练速度，这是深度学习成功的实践之一。'
- en: Some papers⁠^([3](ch11.html#idm45720199777104)) have provided similar strategies
    for different activation functions. These strategies differ only by the scale
    of the variance and whether they use *fan*[avg] or *fan*[in], as shown in [Table 11-1](#initialization_table)
    (for the uniform distribution, just use <math><mi>r</mi><mo>=</mo><msqrt><mn>3</mn><msup><mi>σ</mi><mn>2</mn></msup></msqrt></math>).
    The initialization strategy proposed for the ReLU activation function and its
    variants is called *He initialization* or *Kaiming initialization*, after [the
    paper’s first author](https://homl.info/48). For SELU, use Yann LeCun’s initialization
    method, preferably with a normal distribution. We will cover all these activation
    functions shortly.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些论文提供了不同激活函数的类似策略。这些策略仅在方差的规模和它们是否使用*fan*[avg]或*fan*[in]上有所不同，如[表11-1](#initialization_table)所示（对于均匀分布，只需使用<math><mi>r</mi><mo>=</mo><msqrt><mn>3</mn><msup><mi>σ</mi><mn>2</mn></msup></msqrt></math>）。为ReLU激活函数及其变体提出的初始化策略称为*He初始化*或*Kaiming初始化*，以[论文的第一作者](https://homl.info/48)命名。对于SELU，最好使用Yann
    LeCun的初始化方法，最好使用正态分布。我们将很快介绍所有这些激活函数。
- en: Table 11-1\. Initialization parameters for each type of activation function
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-1。每种激活函数的初始化参数
- en: '| Initialization | Activation functions | *σ*² (Normal) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: 初始化 | 激活函数 | *σ*²（正态）
- en: '| --- | --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Glorot | None, tanh, sigmoid, softmax | 1 / *fan*[avg] |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Glorot | 无，tanh，sigmoid，softmax | 1 / *fan*[avg]'
- en: '| He | ReLU, Leaky ReLU, ELU, GELU, Swish, Mish | 2 / *fan*[in] |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| He | ReLU，Leaky ReLU，ELU，GELU，Swish，Mish | 2 / *fan*[in]'
- en: '| LeCun | SELU | 1 / *fan*[in] |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| LeCun | SELU | 1 / *fan*[in]'
- en: 'By default, Keras uses Glorot initialization with a uniform distribution. When
    you create a layer, you can switch to He initialization by setting `kernel_initializer=​"he_uniform"`
    or `kernel_initializer="he_normal"` like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Keras使用均匀分布的Glorot初始化。当您创建一个层时，您可以通过设置`kernel_initializer="he_uniform"`或`kernel_initializer="he_normal"`来切换到He初始化。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Alternatively, you can obtain any of the initializations listed in [Table 11-1](#initialization_table)
    and more using the `VarianceScaling` initializer. For example, if you want He
    initialization with a uniform distribution and based on *fan*[avg] (rather than
    *fan*[in]), you can use the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用`VarianceScaling`初始化器获得[表11-1](#initialization_table)中列出的任何初始化方法，甚至更多。例如，如果您想要使用均匀分布并基于*fan*[avg]（而不是*fan*[in]）进行He初始化，您可以使用以下代码：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Better Activation Functions
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好的激活函数
- en: One of the insights in the 2010 paper by Glorot and Bengio was that the problems
    with unstable gradients were in part due to a poor choice of activation function.
    Until then most people had assumed that if Mother Nature had chosen to use roughly
    sigmoid activation functions in biological neurons, they must be an excellent
    choice. But it turns out that other activation functions behave much better in
    deep neural networks—in particular, the ReLU activation function, mostly because
    it does not saturate for positive values, and also because it is very fast to
    compute.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 2010年Glorot和Bengio的一篇论文中的一个见解是，不稳定梯度的问题在一定程度上是由于激活函数的选择不当。直到那时，大多数人都认为，如果自然界选择在生物神经元中使用大致为S形的激活函数，那么它们一定是一个很好的选择。但事实证明，其他激活函数在深度神经网络中表现得更好，特别是ReLU激活函数，主要是因为它对于正值不会饱和，而且计算速度非常快。
- en: 'Unfortunately, the ReLU activation function is not perfect. It suffers from
    a problem known as the *dying ReLUs*: during training, some neurons effectively
    “die”, meaning they stop outputting anything other than 0\. In some cases, you
    may find that half of your network’s neurons are dead, especially if you used
    a large learning rate. A neuron dies when its weights get tweaked in such a way
    that the input of the ReLU function (i.e., the weighted sum of the neuron’s inputs
    plus its bias term) is negative for all instances in the training set. When this
    happens, it just keeps outputting zeros, and gradient descent does not affect
    it anymore because the gradient of the ReLU function is zero when its input is
    negative.⁠^([4](ch11.html#idm45720199613584))'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，ReLU激活函数并不完美。它存在一个称为*dying ReLUs*的问题：在训练过程中，一些神经元实际上“死亡”，意味着它们停止输出除0以外的任何值。在某些情况下，您可能会发现您网络的一半神经元已经死亡，尤其是如果您使用了较大的学习率。当神经元的权重被微调得使得ReLU函数的输入（即神经元输入的加权和加上偏置项）在训练集中的所有实例中都为负时，神经元就会死亡。当这种情况发生时，它只会继续输出零，并且梯度下降不再影响它，因为当其输入为负时，ReLU函数的梯度为零。
- en: To solve this problem, you may want to use a variant of the ReLU function, such
    as the *leaky ReLU*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，您可能希望使用ReLU函数的变体，比如*leaky ReLU*。
- en: Leaky ReLU
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: 'The leaky ReLU activation function is defined as LeakyReLU[*α*](*z*) = max(*αz*,
    *z*) (see [Figure 11-2](#leaky_relu_plot)). The hyperparameter *α* defines how
    much the function “leaks”: it is the slope of the function for *z* < 0\. Having
    a slope for *z* < 0 ensures that leaky ReLUs never die; they can go into a long
    coma, but they have a chance to eventually wake up. A [2015 paper](https://homl.info/49)
    by Bing Xu et al.⁠^([5](ch11.html#idm45720199596240)) compared several variants
    of the ReLU activation function, and one of its conclusions was that the leaky
    variants always outperformed the strict ReLU activation function. In fact, setting
    *α* = 0.2 (a huge leak) seemed to result in better performance than *α* = 0.01
    (a small leak). The paper also evaluated the *randomized leaky ReLU* (RReLU),
    where *α* is picked randomly in a given range during training and is fixed to
    an average value during testing. RReLU also performed fairly well and seemed to
    act as a regularizer, reducing the risk of overfitting the training set. Finally,
    the paper evaluated the *parametric leaky ReLU* (PReLU), where *α* is authorized
    to be learned during training: instead of being a hyperparameter, it becomes a
    parameter that can be modified by backpropagation like any other parameter. PReLU
    was reported to strongly outperform ReLU on large image datasets, but on smaller
    datasets it runs the risk of overfitting the training set.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: leaky ReLU激活函数定义为LeakyReLU[*α*](*z*) = max(*αz*, *z*)（参见[图11-2](#leaky_relu_plot)）。超参数*α*定义了函数“泄漏”的程度：它是*z*
    < 0时函数的斜率。对于*z* < 0，具有斜率的leaky ReLU永远不会死亡；它们可能会陷入长时间的昏迷，但最终有机会苏醒。Bing Xu等人在2015年的一篇[论文](https://homl.info/49)比较了几种ReLU激活函数的变体，其中一个结论是，泄漏变体总是优于严格的ReLU激活函数。事实上，设置*α*=0.2（一个巨大的泄漏）似乎比*α*=0.01（一个小泄漏）表现更好。该论文还评估了*随机泄漏ReLU*（RReLU），其中*α*在训练期间在给定范围内随机选择，并在测试期间固定为平均值。RReLU表现也相当不错，并似乎作为正则化器，减少了过拟合训练集的风险。最后，该论文评估了*参数泄漏ReLU*（PReLU），其中*α*在训练期间被授权学习：它不再是一个超参数，而是一个可以像其他参数一样通过反向传播修改的参数。据报道，PReLU在大型图像数据集上明显优于ReLU，但在较小的数据集上存在过拟合训练集的风险。
- en: '![mls3 1102](assets/mls3_1102.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1102](assets/mls3_1102.png)'
- en: 'Figure 11-2\. Leaky ReLU: like ReLU, but with a small slope for negative values'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2\. Leaky ReLU：类似于ReLU，但对负值有一个小的斜率
- en: 'Keras includes the classes `LeakyReLU` and `PReLU` in the `tf.keras.layers`
    package. Just like for other ReLU variants, you should use He initialization with
    these. For example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Keras在`tf.keras.layers`包中包含了`LeakyReLU`和`PReLU`类。就像其他ReLU变体一样，您应该使用He初始化。例如：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you prefer, you can also use `LeakyReLU` as a separate layer in your model;
    it makes no difference for training and predictions:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您愿意，您也可以在模型中将`LeakyReLU`作为一个单独的层来使用；对于训练和预测没有任何影响：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For PReLU, replace `LeakyReLU` with `PReLU`. There is currently no official
    implementation of RReLU in Keras, but you can fairly easily implement your own
    (to learn how to do that, see the exercises at the end of [Chapter 12](ch12.html#tensorflow_chapter)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PReLU，将`LeakyReLU`替换为`PReLU`。目前在Keras中没有官方实现RReLU，但您可以相当容易地实现自己的（要了解如何做到这一点，请参见[第12章](ch12.html#tensorflow_chapter)末尾的练习）。
- en: 'ReLU, leaky ReLU, and PReLU all suffer from the fact that they are not smooth
    functions: their derivatives abruptly change (at *z* = 0). As we saw in [Chapter 4](ch04.html#linear_models_chapter)
    when we discussed lasso, this sort of discontinuity can make gradient descent
    bounce around the optimum, and slow down convergence. So now we will look at some
    smooth variants of the ReLU activation function, starting with ELU and SELU.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU、leaky ReLU和PReLU都存在一个问题，即它们不是平滑函数：它们的导数在*z*=0处突然变化。正如我们在[第4章](ch04.html#linear_models_chapter)中讨论lasso时看到的那样，这种不连续性会导致梯度下降在最优点周围反弹，并减慢收敛速度。因此，现在我们将看一些ReLU激活函数的平滑变体，从ELU和SELU开始。
- en: ELU and SELU
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ELU和SELU
- en: 'In 2015, a [paper](https://homl.info/50) by Djork-Arné Clevert et al.⁠^([6](ch11.html#idm45720199427712))
    proposed a new activation function, called the *exponential linear unit* (ELU),
    that outperformed all the ReLU variants in the authors’ experiments: training
    time was reduced, and the neural network performed better on the test set. [Equation
    11-2](#elu_activation_equation) shows this activation function’s definition.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，Djork-Arné Clevert等人提出了一篇[论文](https://homl.info/50)，提出了一种新的激活函数，称为*指数线性单元*（ELU），在作者的实验中表现优于所有ReLU变体：训练时间缩短，神经网络在测试集上表现更好。[方程式11-2](#elu_activation_equation)展示了这个激活函数的定义。
- en: Equation 11-2\. ELU activation function
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式11-2\. ELU激活函数
- en: <math display="block"><mrow><msub><mo form="prefix">ELU</mo> <mi>α</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>α</mi> <mo>(</mo>
    <mo form="prefix">exp</mo> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>-</mo> <mn>1</mn>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi>
    <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mi>z</mi></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mo form="prefix">ELU</mo> <mi>α</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>α</mi> <mo>(</mo>
    <mo form="prefix">exp</mo> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>-</mo> <mn>1</mn>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi>
    <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mi>z</mi></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'The ELU activation function looks a lot like the ReLU function (see [Figure 11-3](#elu_and_selu_activation_plot)),
    with a few major differences:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ELU激活函数看起来很像ReLU函数（参见[图11-3](#elu_and_selu_activation_plot)），但有一些主要区别：
- en: It takes on negative values when *z* < 0, which allows the unit to have an average
    output closer to 0 and helps alleviate the vanishing gradients problem. The hyperparameter
    *α* defines the opposite of the value that the ELU function approaches when *z*
    is a large negative number. It is usually set to 1, but you can tweak it like
    any other hyperparameter.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当*z* < 0时，它会取负值，这使得单元的平均输出更接近于0，并有助于缓解梯度消失问题。超参数*α*定义了当*z*是一个较大的负数时ELU函数接近的值的相反数。通常设置为1，但您可以像调整其他超参数一样进行调整。
- en: It has a nonzero gradient for *z* < 0, which avoids the dead neurons problem.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*z* < 0时具有非零梯度，避免了死神经元问题。
- en: If *α* is equal to 1 then the function is smooth everywhere, including around
    *z* = 0, which helps speed up gradient descent since it does not bounce as much
    to the left and right of *z* = 0.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*α*等于1，则该函数在任何地方都是平滑的，包括在*z* = 0附近，这有助于加快梯度下降的速度，因为它在*z* = 0的左右两侧不会反弹太多。
- en: Using ELU with Keras is as easy as setting `activation="elu"`, and like with
    other ReLU variants, you should use He initialization. The main drawback of the
    ELU activation function is that it is slower to compute than the ReLU function
    and its variants (due to the use of the exponential function). Its faster convergence
    rate during training may compensate for that slow computation, but still, at test
    time an ELU network will be a bit slower than a ReLU network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中使用ELU就像设置`activation="elu"`一样简单，与其他ReLU变体一样，应该使用He初始化。ELU激活函数的主要缺点是它的计算速度比ReLU函数及其变体慢（由于使用了指数函数）。在训练期间更快的收敛速度可能会弥补这种缓慢的计算，但是在测试时，ELU网络将比ReLU网络慢一点。
- en: '![mls3 1103](assets/mls3_1103.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1103](assets/mls3_1103.png)'
- en: Figure 11-3\. ELU and SELU activation functions
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-3\. ELU和SELU激活函数
- en: 'Not long after, a [2017 paper](https://homl.info/selu) by Günter Klambauer
    et al.⁠^([7](ch11.html#idm45720199386128)) introduced the *scaled ELU* (SELU)
    activation function: as its name suggests, it is a scaled variant of the ELU activation
    function (about 1.05 times ELU, using *α* ≈ 1.67). The authors showed that if
    you build a neural network composed exclusively of a stack of dense layers (i.e.,
    an MLP), and if all hidden layers use the SELU activation function, then the network
    will *self-normalize*: the output of each layer will tend to preserve a mean of
    0 and a standard deviation of 1 during training, which solves the vanishing/exploding
    gradients problem. As a result, the SELU activation function may outperform other
    activation functions for MLPs, especially deep ones. To use it with Keras, just
    set `activation="selu"`. There are, however, a few conditions for self-normalization
    to happen (see the paper for the mathematical justification):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不久之后，Günter Klambauer等人在[2017年的一篇论文](https://homl.info/selu)中介绍了*缩放ELU*（SELU）激活函数：正如其名称所示，它是ELU激活函数的缩放变体（大约是ELU的1.05倍，使用*α*
    ≈ 1.67）。作者们表明，如果构建一个仅由一堆稠密层（即MLP）组成的神经网络，并且所有隐藏层使用SELU激活函数，那么网络将*自标准化*：每一层的输出在训练过程中倾向于保持均值为0，标准差为1，从而解决了梯度消失/爆炸的问题。因此，SELU激活函数可能在MLP中胜过其他激活函数，尤其是深层网络。要在Keras中使用它，只需设置`activation="selu"`。然而，自标准化发生的条件有一些（请参阅论文进行数学证明）：
- en: 'The input features must be standardized: mean 0 and standard deviation 1.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入特征必须标准化：均值为0，标准差为1。
- en: Every hidden layer’s weights must be initialized using LeCun normal initialization.
    In Keras, this means setting `kernel_initializer="lecun_normal"`.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层的权重必须使用LeCun正态初始化。在Keras中，这意味着设置`kernel_initializer="lecun_normal"`。
- en: The self-normalizing property is only guaranteed with plain MLPs. If you try
    to use SELU in other architectures, like recurrent networks (see [Chapter 15](ch15.html#rnn_chapter))
    or networks with *skip connections* (i.e., connections that skip layers, such
    as in Wide & Deep nets), it will probably not outperform ELU.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有在普通MLP中才能保证自标准化属性。如果尝试在其他架构中使用SELU，如循环网络（参见[第15章](ch15.html#rnn_chapter)）或具有*跳跃连接*（即跳过层的连接，例如在Wide
    & Deep网络中），它可能不会胜过ELU。
- en: You cannot use regularization techniques like ℓ[1] or ℓ[2] regularization, max-norm,
    batch-norm, or regular dropout (these are discussed later in this chapter).
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不能使用正则化技术，如ℓ[1]或ℓ[2]正则化、最大范数、批量归一化或常规的dropout（这些将在本章后面讨论）。
- en: 'These are significant constraints, so despite its promises, SELU did not gain
    a lot of traction. Moreover, three more activation functions seem to outperform
    it quite consistently on most tasks: GELU, Swish, and Mish.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是重要的限制条件，因此尽管SELU有所承诺，但并没有获得很大的关注。此外，另外三种激活函数似乎在大多数任务上表现出色：GELU、Swish和Mish。
- en: GELU, Swish, and Mish
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GELU、Swish和Mish
- en: '*GELU* was introduced in a [2016 paper](https://homl.info/gelu) by Dan Hendrycks
    and Kevin Gimpel.^([8](ch11.html#idm45720199369856)) Once again, you can think
    of it as a smooth variant of the ReLU activation function. Its definition is given
    in [Equation 11-3](#gelu_activation_equation), where Φ is the standard Gaussian
    cumulative distribution function (CDF): Φ(*z*) corresponds to the probability
    that a value sampled randomly from a normal distribution of mean 0 and variance
    1 is lower than *z*.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*GELU*是由Dan Hendrycks和Kevin Gimpel在[2016年的一篇论文](https://homl.info/gelu)中引入的。再次，您可以将其视为ReLU激活函数的平滑变体。其定义在[方程11-3](#gelu_activation_equation)中给出，其中Φ是标准高斯累积分布函数（CDF）：Φ(*z*)对应于从均值为0、方差为1的正态分布中随机抽取的值低于*z*的概率。'
- en: Equation 11-3\. GELU activation function
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-3\. GELU激活函数
- en: <math display="block"><mrow><mi>GELU</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>=</mo><mi>z</mi><mi
    mathvariant="normal">Φ</mi><mo>(</mo><mi>z</mi><mo>)</mo></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>GELU</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>=</mo><mi>z</mi><mi
    mathvariant="normal">Φ</mi><mo>(</mo><mi>z</mi><mo>)</mo></math>
- en: 'As you can see in [Figure 11-4](#gelu_swish_mish_plot), GELU resembles ReLU:
    it approaches 0 when its input *z* is very negative, and it approaches *z* when
    *z* is very positive. However, whereas all the activation functions we’ve discussed
    so far were both convex and monotonic,^([9](ch11.html#idm45720199356368)) the
    GELU activation function is neither: from left to right, it starts by going straight,
    then it wiggles down, reaches a low point around –0.17 (near z ≈ –0.75), and finally
    bounces up and ends up going straight toward the top right. This fairly complex
    shape and the fact that it has a curvature at every point may explain why it works
    so well, especially for complex tasks: gradient descent may find it easier to
    fit complex patterns. In practice, it often outperforms every other activation
    function discussed so far. However, it is a bit more computationally intensive,
    and the performance boost it provides is not always sufficient to justify the
    extra cost. That said, it is possible to show that it is approximately equal to
    *z*σ(1.702 *z*), where σ is the sigmoid function: using this approximation also
    works very well, and it has the advantage of being much faster to compute.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图11-4](#gelu_swish_mish_plot)中所见，GELU类似于ReLU：当其输入*z*非常负时，它接近0，当*z*非常正时，它接近*z*。然而，到目前为止我们讨论的所有激活函数都是凸函数且单调递增的，而GELU激活函数则不是：从左到右，它开始直线上升，然后下降，达到大约-0.17的低点（接近z≈-0.75），最后反弹上升并最终向右上方直线前进。这种相当复杂的形状以及它在每个点上都有曲率的事实可能解释了为什么它效果如此好，尤其是对于复杂任务：梯度下降可能更容易拟合复杂模式。在实践中，它通常优于迄今讨论的任何其他激活函数。然而，它的计算成本稍高，提供的性能提升并不总是足以证明额外成本的必要性。尽管如此，可以证明它大致等于*z*σ(1.702
    *z*)，其中σ是sigmoid函数：使用这个近似也非常有效，并且计算速度更快。
- en: '![mls3 1104](assets/mls3_1104.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1104](assets/mls3_1104.png)'
- en: Figure 11-4\. GELU, Swish, parametrized Swish, and Mish activation functions
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-4. GELU、Swish、参数化Swish和Mish激活函数
- en: 'The GELU paper also introduced the *sigmoid linear unit* (SiLU) activation
    function, which is equal to *z*σ(*z*), but it was outperformed by GELU in the
    authors’ tests. Interestingly, a [2017 paper](https://homl.info/swish) by Prajit
    Ramachandran et al.^([10](ch11.html#idm45720199347968)) rediscovered the SiLU
    function by automatically searching for good activation functions. The authors
    named it *Swish*, and the name caught on. In their paper, Swish outperformed every
    other function, including GELU. Ramachandran et al. later generalized Swish by
    adding an extra hyperparameter *β* to scale the sigmoid function’s input. The
    generalized Swish function is Swish[*β*](*z*) = *z*σ(*βz*), so GELU is approximately
    equal to the generalized Swish function using *β* = 1.702\. You can tune *β* like
    any other hyperparameter. Alternatively, it’s also possible to make *β* trainable
    and let gradient descent optimize it: much like PReLU, this can make your model
    more powerful, but it also runs the risk of overfitting the data.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GELU论文还介绍了*sigmoid linear unit*（SiLU）激活函数，它等于*z*σ(*z*)，但在作者的测试中被GELU表现得更好。有趣的是，Prajit
    Ramachandran等人在[2017年的一篇论文](https://homl.info/swish)中重新发现了SiLU函数，通过自动搜索好的激活函数。作者将其命名为*Swish*，这个名字很受欢迎。在他们的论文中，Swish表现优于其他所有函数，包括GELU。Ramachandran等人后来通过添加额外的超参数*β*来推广Swish，用于缩放sigmoid函数的输入。推广后的Swish函数为Swish[*β*](*z*)
    = *z*σ(*βz*)，因此GELU大致等于使用*β* = 1.702的推广Swish函数。您可以像调整其他超参数一样调整*β*。另外，也可以将*β*设置为可训练的，让梯度下降来优化它：这样可以使您的模型更加强大，但也会有过拟合数据的风险。
- en: Another quite similar activation function is *Mish*, which was introduced in
    a [2019 paper](https://homl.info/mish) by Diganta Misra.^([11](ch11.html#idm45720199337776))
    It is defined as mish(*z*) = *z*tanh(softplus(*z*)), where softplus(*z*) = log(1
    + exp(*z*)). Just like GELU and Swish, it is a smooth, nonconvex, and nonmonotonic
    variant of ReLU, and once again the author ran many experiments and found that
    Mish generally outperformed other activation functions—even Swish and GELU, by
    a tiny margin. [Figure 11-4](#gelu_swish_mish_plot) shows GELU, Swish (both with
    the default *β* = 1 and with *β* = 0.6), and lastly Mish. As you can see, Mish
    overlaps almost perfectly with Swish when *z* is negative, and almost perfectly
    with GELU when *z* is positive.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相当相似的激活函数是*Mish*，它是由Diganta Misra在[2019年的一篇论文](https://homl.info/mish)中引入的。它被定义为mish(*z*)
    = *z*tanh(softplus(*z*))，其中softplus(*z*) = log(1 + exp(*z*))。就像GELU和Swish一样，它是ReLU的平滑、非凸、非单调变体，作者再次进行了许多实验，并发现Mish通常优于其他激活函数，甚至比Swish和GELU稍微好一点。[图11-4](#gelu_swish_mish_plot)展示了GELU、Swish（默认*β*
    = 1和*β* = 0.6）、最后是Mish。如您所见，当*z*为负时，Mish几乎完全重叠于Swish，当*z*为正时，几乎完全重叠于GELU。
- en: Tip
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'So, which activation function should you use for the hidden layers of your
    deep neural networks? ReLU remains a good default for simple tasks: it’s often
    just as good as the more sophisticated activation functions, plus it’s very fast
    to compute, and many libraries and hardware accelerators provide ReLU-specific
    optimizations. However, Swish is probably a better default for more complex tasks,
    and you can even try parametrized Swish with a learnable *β* parameter for the
    most complex tasks. Mish may give you slightly better results, but it requires
    a bit more compute. If you care a lot about runtime latency, then you may prefer
    leaky ReLU, or parametrized leaky ReLU for more complex tasks. For deep MLPs,
    give SELU a try, but make sure to respect the constraints listed earlier. If you
    have spare time and computing power, you can use cross-validation to evaluate
    other activation functions as well.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，对于深度神经网络的隐藏层，你应该使用哪种激活函数？对于简单任务，ReLU仍然是一个很好的默认选择：它通常和更复杂的激活函数一样好，而且计算速度非常快，许多库和硬件加速器提供了ReLU特定的优化。然而，对于更复杂的任务，Swish可能是更好的默认选择，甚至可以尝试带有可学习*β*参数的参数化Swish来处理最复杂的任务。Mish可能会给出稍微更好的结果，但需要更多的计算。如果你非常关心运行时延迟，那么你可能更喜欢leaky
    ReLU，或者对于更复杂的任务，可以使用参数化leaky ReLU。对于深度MLP，可以尝试使用SELU，但一定要遵守之前列出的约束条件。如果你有多余的时间和计算能力，也可以使用交叉验证来评估其他激活函数。
- en: Keras supports GELU and Swish out of the box; just use `activation="gelu"` or
    `activation="swish"`. However, it does not support Mish or the generalized Swish
    activation function yet (but see [Chapter 12](ch12.html#tensorflow_chapter) to
    see how to implement your own activation functions and layers).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Keras支持GELU和Swish，只需使用`activation="gelu"`或`activation="swish"`。然而，它目前不支持Mish或广义Swish激活函数（但请参阅[第12章](ch12.html#tensorflow_chapter)了解如何实现自己的激活函数和层）。
- en: 'That’s all for activation functions! Now, let’s look at a completely different
    way to solve the unstable gradients problem: batch normalization.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数就介绍到这里！现在，让我们看一种完全不同的解决不稳定梯度问题的方法：批量归一化。
- en: Batch Normalization
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化
- en: Although using He initialization along with ReLU (or any of its variants) can
    significantly reduce the danger of the vanishing/exploding gradients problems
    at the beginning of training, it doesn’t guarantee that they won’t come back during
    training.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用He初始化与ReLU（或其任何变体）可以显著减少训练开始时梯度消失/爆炸问题的危险，但并不能保证它们在训练过程中不会再次出现。
- en: 'In a [2015 paper](https://homl.info/51),⁠^([12](ch11.html#idm45720199311872))
    Sergey Ioffe and Christian Szegedy proposed a technique called *batch normalization*
    (BN) that addresses these problems. The technique consists of adding an operation
    in the model just before or after the activation function of each hidden layer.
    This operation simply zero-centers and normalizes each input, then scales and
    shifts the result using two new parameter vectors per layer: one for scaling,
    the other for shifting. In other words, the operation lets the model learn the
    optimal scale and mean of each of the layer’s inputs. In many cases, if you add
    a BN layer as the very first layer of your neural network, you do not need to
    standardize your training set. That is, there’s no need for `StandardScaler` or
    `Normalization`; the BN layer will do it for you (well, approximately, since it
    only looks at one batch at a time, and it can also rescale and shift each input
    feature).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇[2015年的论文](https://homl.info/51)中，Sergey Ioffe和Christian Szegedy提出了一种称为*批量归一化*（BN）的技术，解决了这些问题。该技术包括在模型中在每个隐藏层的激活函数之前或之后添加一个操作。这个操作简单地将每个输入零中心化和归一化，然后使用每层两个新的参数向量进行缩放和移位：一个用于缩放，另一个用于移位。换句话说，该操作让模型学习每个层输入的最佳缩放和均值。在许多情况下，如果将BN层作为神经网络的第一层，您就不需要标准化训练集。也就是说，不需要`StandardScaler`或`Normalization`；BN层会为您完成（大致上，因为它一次只看一个批次，并且还可以重新缩放和移位每个输入特征）。
- en: In order to zero-center and normalize the inputs, the algorithm needs to estimate
    each input’s mean and standard deviation. It does so by evaluating the mean and
    standard deviation of the input over the current mini-batch (hence the name “batch
    normalization”). The whole operation is summarized step by step in [Equation 11-4](#batch_normalization_algorithm).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将输入零中心化和归一化，算法需要估计每个输入的均值和标准差。它通过评估当前小批量输入的均值和标准差来实现这一点（因此称为“批量归一化”）。整个操作在[方程式11-4](#batch_normalization_algorithm)中逐步总结。
- en: Equation 11-4\. Batch normalization algorithm
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式11-4\. 批量归一化算法
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msub><mi mathvariant="bold">μ</mi>
    <mi>B</mi></msub> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn>
    <msub><mi>m</mi> <mi>B</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <msub><mi>m</mi> <mi>B</mi></msub></munderover> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mn>2</mn> <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msup><mrow><msub><mi
    mathvariant="bold">σ</mi> <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>m</mi>
    <mi>B</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <msub><mi>m</mi> <mi>B</mi></msub></munderover> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi
    mathvariant="bold">μ</mi> <mi>B</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>3</mn> <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msup><mover
    accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi
    mathvariant="bold">μ</mi> <mi>B</mi></msub></mrow> <msqrt><mrow><msup><mrow><msub><mi
    mathvariant="bold">σ</mi> <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>4</mn> <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msup><mi
    mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>=</mo>
    <mi mathvariant="bold">γ</mi> <mo>⊗</mo> <msup><mover accent="true"><mi mathvariant="bold">x</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <mi mathvariant="bold">β</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msub><mi mathvariant="bold">μ</mi>
    <mi>B</mi></msub> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn>
    <msub><mi>m</mi> <mi>B</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <msub><mi>m</mi> <mi>B</mi></msub></munderover> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mn>2</mn> <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msup><mrow><msub><mi
    mathvariant="bold">σ</mi> <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>m</mi>
    <mi>B</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <msub><mi>m</mi> <mi>B</mi></msub></munderover> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi
    mathvariant="bold">μ</mi> <mi>B</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>3</mn> <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msup><mover
    accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi
    mathvariant="bold">μ</mi> <mi>B</mi></msub></mrow> <msqrt><mrow><msup><mrow><msub><mi
    mathvariant="bold">σ</mi> <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>4</mn> <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msup><mi
    mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>=</mo>
    <mi mathvariant="bold">γ</mi> <mo>⊗</mo> <msup><mover accent="true"><mi mathvariant="bold">x</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <mi mathvariant="bold">β</mi></mrow></mtd></mtr></mtable></math>
- en: 'In this algorithm:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中：
- en: '**μ**[*B*] is the vector of input means, evaluated over the whole mini-batch
    *B* (it contains one mean per input).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**μ**[*B*] 是在整个小批量*B*上评估的输入均值向量（它包含每个输入的一个均值）。'
- en: '*m*[*B*] is the number of instances in the mini-batch.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m*[*B*] 是小批量中实例的数量。'
- en: '**σ**[*B*] is the vector of input standard deviations, also evaluated over
    the whole mini-batch (it contains one standard deviation per input).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**σ**[*B*] 是输入标准差的向量，也是在整个小批量上评估的（它包含每个输入的一个标准差）。'
- en: <math><mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover></math>
    ^((*i*)) is the vector of zero-centered and normalized inputs for instance *i*.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math><mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover></math>
    ^((*i*)) 是实例*i*的零中心化和归一化输入向量。
- en: '*ε* is a tiny number that avoids division by zero and ensures the gradients
    don’t grow too large (typically 10^(–5)). This is called a *smoothing term*.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ε* 是一个微小的数字，避免了除以零，并确保梯度不会增长太大（通常为10^（–5））。这被称为*平滑项*。'
- en: '**γ** is the output scale parameter vector for the layer (it contains one scale
    parameter per input).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**γ** 是该层的输出比例参数向量（它包含每个输入的一个比例参数）。'
- en: ⊗ represents element-wise multiplication (each input is multiplied by its corresponding
    output scale parameter).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ⊗ 表示逐元素乘法（每个输入都会乘以其对应的输出比例参数）。
- en: '**β** is the output shift (offset) parameter vector for the layer (it contains
    one offset parameter per input). Each input is offset by its corresponding shift
    parameter.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**β** 是该层的输出偏移参数向量（它包含每个输入的一个偏移参数）。每个输入都会被其对应的偏移参数偏移。'
- en: '**z**^((*i*)) is the output of the BN operation. It is a rescaled and shifted
    version of the inputs.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**z**^((*i*)) 是BN操作的输出。它是输入的重新缩放和偏移版本。'
- en: 'So during training, BN standardizes its inputs, then rescales and offsets them.
    Good! What about at test time? Well, it’s not that simple. Indeed, we may need
    to make predictions for individual instances rather than for batches of instances:
    in this case, we will have no way to compute each input’s mean and standard deviation.
    Moreover, even if we do have a batch of instances, it may be too small, or the
    instances may not be independent and identically distributed, so computing statistics
    over the batch instances would be unreliable. One solution could be to wait until
    the end of training, then run the whole training set through the neural network
    and compute the mean and standard deviation of each input of the BN layer. These
    “final” input means and standard deviations could then be used instead of the
    batch input means and standard deviations when making predictions. However, most
    implementations of batch normalization estimate these final statistics during
    training by using a moving average of the layer’s input means and standard deviations.
    This is what Keras does automatically when you use the `BatchNormalization` layer.
    To sum up, four parameter vectors are learned in each batch-normalized layer:
    **γ** (the output scale vector) and **β** (the output offset vector) are learned
    through regular backpropagation, and **μ** (the final input mean vector) and **σ**
    (the final input standard deviation vector) are estimated using an exponential
    moving average. Note that **μ** and **σ** are estimated during training, but they
    are used only after training (to replace the batch input means and standard deviations
    in [Equation 11-4](#batch_normalization_algorithm)).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练期间，BN会标准化其输入，然后重新缩放和偏移它们。很好！那么，在测试时呢？嗯，事情并不那么简单。实际上，我们可能需要为单个实例而不是一批实例进行预测：在这种情况下，我们将无法计算每个输入的均值和标准差。此外，即使我们有一批实例，它可能太小，或者实例可能不是独立且同分布的，因此在批次实例上计算统计数据将是不可靠的。一个解决方案可能是等到训练结束，然后通过神经网络运行整个训练集，并计算BN层每个输入的均值和标准差。这些“最终”输入均值和标准差可以在进行预测时代替批次输入均值和标准差。然而，大多数批次归一化的实现在训练期间通过使用该层输入均值和标准差的指数移动平均值来估计这些最终统计数据。这就是当您使用`BatchNormalization`层时Keras自动执行的操作。总之，在每个批次归一化的层中学习了四个参数向量：**γ**（输出缩放向量）和**β**（输出偏移向量）通过常规反向传播学习，而**μ**（最终输入均值向量）和**σ**（最终输入标准差向量）则使用指数移动平均值进行估计。请注意，**μ**和**σ**是在训练期间估计的，但仅在训练后使用（以替换[公式11-4](#batch_normalization_algorithm)中的批次输入均值和标准差）。
- en: 'Ioffe and Szegedy demonstrated that batch normalization considerably improved
    all the deep neural networks they experimented with, leading to a huge improvement
    in the ImageNet classification task (ImageNet is a large database of images classified
    into many classes, commonly used to evaluate computer vision systems). The vanishing
    gradients problem was strongly reduced, to the point that they could use saturating
    activation functions such as the tanh and even the sigmoid activation function.
    The networks were also much less sensitive to the weight initialization. The authors
    were able to use much larger learning rates, significantly speeding up the learning
    process. Specifically, they note that:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Ioffe和Szegedy证明了批次归一化显著改善了他们进行实验的所有深度神经网络，从而在ImageNet分类任务中取得了巨大的改进（ImageNet是一个大型图像数据库，被分类为许多类别，通常用于评估计算机视觉系统）。梯度消失问题得到了很大程度的减轻，以至于他们可以使用饱和激活函数，如tanh甚至sigmoid激活函数。网络对权重初始化也不那么敏感。作者能够使用更大的学习率，显著加快学习过程。具体来说，他们指出：
- en: 'Applied to a state-of-the-art image classification model, batch normalization
    achieves the same accuracy with 14 times fewer training steps, and beats the original
    model by a significant margin. […​] Using an ensemble of batch-normalized networks,
    we improve upon the best published result on ImageNet classification: reaching
    4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human
    raters.'
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 应用于最先进的图像分类模型，批次归一化在14倍更少的训练步骤下实现了相同的准确性，并且以显著的优势击败了原始模型。[...] 使用一组批次归一化的网络，我们在ImageNet分类上取得了最佳发布结果：达到4.9%的前5验证错误率（和4.8%的测试错误率），超过了人类评分者的准确性。
- en: Finally, like a gift that keeps on giving, batch normalization acts like a regularizer,
    reducing the need for other regularization techniques (such as dropout, described
    later in this chapter).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，就像一份源源不断的礼物，批次归一化就像一个正则化器，减少了对其他正则化技术（如本章后面描述的dropout）的需求。
- en: 'Batch normalization does, however, add some complexity to the model (although
    it can remove the need for normalizing the input data, as discussed earlier).
    Moreover, there is a runtime penalty: the neural network makes slower predictions
    due to the extra computations required at each layer. Fortunately, it’s often
    possible to fuse the BN layer with the previous layer after training, thereby
    avoiding the runtime penalty. This is done by updating the previous layer’s weights
    and biases so that it directly produces outputs of the appropriate scale and offset.
    For example, if the previous layer computes **XW** + **b**, then the BN layer
    will compute **γ** ⊗ (**XW** + **b** – **μ**) / **σ** + **β** (ignoring the smoothing
    term *ε* in the denominator). If we define **W**′ = **γ**⊗**W** / **σ** and **b**′
    = **γ** ⊗ (**b** – **μ**) / **σ** + **β**, the equation simplifies to **XW**′
    + **b**′. So, if we replace the previous layer’s weights and biases (**W** and
    **b**) with the updated weights and biases (**W**′ and **b**′), we can get rid
    of the BN layer (TFLite’s converter does this automatically; see [Chapter 19](ch19.html#deployment_chapter)).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，批量归一化确实给模型增加了一些复杂性（尽管它可以消除对输入数据进行归一化的需要，如前面讨论的）。此外，还存在运行时惩罚：由于每一层需要额外的计算，神经网络的预测速度变慢。幸运的是，通常可以在训练后将BN层与前一层融合在一起，从而避免运行时惩罚。这是通过更新前一层的权重和偏置，使其直接产生适当规模和偏移的输出来实现的。例如，如果前一层计算**XW**
    + **b**，那么BN层将计算**γ** ⊗ (**XW** + **b** - **μ**) / **σ** + **β**（忽略分母中的平滑项*ε*）。如果我们定义**W**′
    = **γ**⊗**W** / **σ**和**b**′ = **γ** ⊗ (**b** - **μ**) / **σ** + **β**，则方程简化为**XW**′
    + **b**′。因此，如果我们用更新后的权重和偏置（**W**′和**b**′）替换前一层的权重和偏置（**W**和**b**），我们可以摆脱BN层（TFLite的转换器会自动执行此操作；请参阅[第19章](ch19.html#deployment_chapter)）。
- en: Note
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You may find that training is rather slow, because each epoch takes much more
    time when you use batch normalization. This is usually counterbalanced by the
    fact that convergence is much faster with BN, so it will take fewer epochs to
    reach the same performance. All in all, *wall time* will usually be shorter (this
    is the time measured by the clock on your wall).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会发现训练速度相当慢，因为使用批量归一化时，每个时期需要更多的时间。通常，这通常会被BN的收敛速度更快所抵消，因此需要更少的时期才能达到相同的性能。总的来说，*墙上的时间*通常会更短（这是您墙上时钟上测量的时间）。
- en: Implementing batch normalization with Keras
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Keras实现批量归一化
- en: 'As with most things with Keras, implementing batch normalization is straightforward
    and intuitive. Just add a `BatchNormalization` layer before or after each hidden
    layer’s activation function. You may also add a BN layer as the first layer in
    your model, but a plain `Normalization` layer generally performs just as well
    in this location (its only drawback is that you must first call its `adapt()`
    method). For example, this model applies BN after every hidden layer and as the
    first layer in the model (after flattening the input images):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与Keras的大多数事物一样，实现批量归一化是简单直观的。只需在每个隐藏层的激活函数之前或之后添加一个`BatchNormalization`层。您还可以将BN层添加为模型中的第一层，但通常在此位置使用普通的`Normalization`层效果一样好（它的唯一缺点是您必须首先调用其`adapt()`方法）。例如，这个模型在每个隐藏层后应用BN，并将其作为模型中的第一层（在展平输入图像之后）：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That’s all! In this tiny example with just two hidden layers batch normalization
    is unlikely to have a large impact, but for deeper networks it can make a tremendous
    difference.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！在这个只有两个隐藏层的微小示例中，批量归一化不太可能产生很大的影响，但对于更深的网络，它可能产生巨大的差异。
- en: 'Let’s display the model summary:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示模型摘要：
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you can see, each BN layer adds four parameters per input: **γ**, **β**,
    **μ**, and **σ** (for example, the first BN layer adds 3,136 parameters, which
    is 4 × 784). The last two parameters, **μ** and **σ**, are the moving averages;
    they are not affected by backpropagation, so Keras calls them “non-trainable”⁠^([13](ch11.html#idm45720198989872))
    (if you count the total number of BN parameters, 3,136 + 1,200 + 400, and divide
    by 2, you get 2,368, which is the total number of non-trainable parameters in
    this model).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，每个BN层都会为每个输入添加四个参数：**γ**、**β**、**μ**和**σ**（例如，第一个BN层会添加3,136个参数，即4×784）。最后两个参数，**μ**和**σ**，是移动平均值；它们不受反向传播的影响，因此Keras将它们称为“不可训练”⁠^([13](ch11.html#idm45720198989872))（如果您计算BN参数的总数，3,136
    + 1,200 + 400，然后除以2，您将得到2,368，这是该模型中不可训练参数的总数）。
- en: 'Let’s look at the parameters of the first BN layer. Two are trainable (by backpropagation),
    and two are not:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第一个BN层的参数。其中两个是可训练的（通过反向传播），另外两个不是：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The authors of the BN paper argued in favor of adding the BN layers before
    the activation functions, rather than after (as we just did). There is some debate
    about this, as which is preferable seems to depend on the task—you can experiment
    with this too to see which option works best on your dataset. To add the BN layers
    before the activation function, you must remove the activation functions from
    the hidden layers and add them as separate layers after the BN layers. Moreover,
    since a batch normalization layer includes one offset parameter per input, you
    can remove the bias term from the previous layer by passing `use_bias=False` when
    creating it. Lastly, you can usually drop the first BN layer to avoid sandwiching
    the first hidden layer between two BN layers. The updated code looks like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: BN论文的作者主张在激活函数之前而不是之后添加BN层（就像我们刚刚做的那样）。关于这一点存在一些争论，因为哪种方式更可取似乎取决于任务-您也可以尝试这个来看看哪个选项在您的数据集上效果最好。要在激活函数之前添加BN层，您必须从隐藏层中删除激活函数，并在BN层之后作为单独的层添加它们。此外，由于批量归一化层包含每个输入的一个偏移参数，您可以在创建时通过传递`use_bias=False`来删除前一层的偏置项。最后，通常可以删除第一个BN层，以避免将第一个隐藏层夹在两个BN层之间。更新后的代码如下：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `BatchNormalization` class has quite a few hyperparameters you can tweak.
    The defaults will usually be fine, but you may occasionally need to tweak the
    `momentum`. This hyperparameter is used by the `BatchNormalization` layer when
    it updates the exponential moving averages; given a new value **v** (i.e., a new
    vector of input means or standard deviations computed over the current batch),
    the layer updates the running average <math><mover><mi mathvariant="bold">v</mi><mo>^</mo></mover></math>
    using the following equation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`BatchNormalization`类有很多可以调整的超参数。默认值通常是可以的，但偶尔您可能需要调整`momentum`。当`BatchNormalization`层更新指数移动平均值时，该超参数将被使用；给定一个新值**v**（即，在当前批次上计算的新的输入均值或标准差向量），该层使用以下方程更新运行平均值<math><mover><mi
    mathvariant="bold">v</mi><mo>^</mo></mover></math>：'
- en: <math display="block"><mrow><mover accent="true"><mi mathvariant="bold">v</mi>
    <mo>^</mo></mover> <mo>←</mo> <mover accent="true"><mi mathvariant="bold">v</mi>
    <mo>^</mo></mover> <mo>×</mo> <mtext>momentum</mtext> <mo>+</mo> <mi mathvariant="bold">v</mi>
    <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mtext>momentum</mtext> <mo>)</mo></mrow></mrow></math>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi mathvariant="bold">v</mi>
    <mo>^</mo></mover> <mo>←</mo> <mover accent="true"><mi mathvariant="bold">v</mi>
    <mo>^</mo></mover> <mo>×</mo> <mtext>momentum</mtext> <mo>+</mo> <mi mathvariant="bold">v</mi>
    <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mtext>momentum</mtext> <mo>)</mo></mrow></mrow></math>
- en: A good momentum value is typically close to 1; for example, 0.9, 0.99, or 0.999\.
    You want more 9s for larger datasets and for smaller mini-batches.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好的动量值通常接近于1；例如，0.9，0.99或0.999。对于更大的数据集和更小的小批量，您希望有更多的9。
- en: 'Another important hyperparameter is `axis`: it determines which axis should
    be normalized. It defaults to –1, meaning that by default it will normalize the
    last axis (using the means and standard deviations computed across the *other*
    axes). When the input batch is 2D (i.e., the batch shape is [*batch size, features*]),
    this means that each input feature will be normalized based on the mean and standard
    deviation computed across all the instances in the batch. For example, the first
    BN layer in the previous code example will independently normalize (and rescale
    and shift) each of the 784 input features. If we move the first BN layer before
    the `Flatten` layer, then the input batches will be 3D, with shape [*batch size,
    height, width*]; therefore, the BN layer will compute 28 means and 28 standard
    deviations (1 per column of pixels, computed across all instances in the batch
    and across all rows in the column), and it will normalize all pixels in a given
    column using the same mean and standard deviation. There will also be just 28
    scale parameters and 28 shift parameters. If instead you still want to treat each
    of the 784 pixels independently, then you should set `axis=[1, 2]`.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的超参数是“axis”：它确定应该对哪个轴进行归一化。默认为-1，这意味着默认情况下将归一化最后一个轴（使用在*其他*轴上计算的均值和标准差）。当输入批次为2D（即，批次形状为[*批次大小，特征*]）时，这意味着每个输入特征将基于在批次中所有实例上计算的均值和标准差进行归一化。例如，前面代码示例中的第一个BN层将独立地归一化（和重新缩放和移位）784个输入特征中的每一个。如果我们将第一个BN层移到`Flatten`层之前，那么输入批次将是3D，形状为[*批次大小，高度，宽度*]；因此，BN层将计算28个均值和28个标准差（每个像素列一个，跨批次中的所有实例和列中的所有行计算），并且将使用相同的均值和标准差归一化给定列中的所有像素。还将有28个比例参数和28个移位参数。如果您仍希望独立处理784个像素中的每一个，则应将`axis=[1,
    2]`。
- en: 'Batch normalization has become one of the most-used layers in deep neural networks,
    especially deep convolutional neural networks discussed in ([Chapter 14](ch14.html#cnn_chapter)),
    to the point that it is often omitted in the architecture diagrams: it is assumed
    that BN is added after every layer. Now let’s look at one last technique to stabilize
    gradients during training: gradient clipping.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化已经成为深度神经网络中最常用的层之一，特别是在深度卷积神经网络中讨论的（[第14章](ch14.html#cnn_chapter)），以至于在架构图中通常被省略：假定在每一层之后都添加了BN。现在让我们看看最后一种稳定梯度的技术：梯度裁剪。
- en: Gradient Clipping
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度裁剪
- en: Another technique to mitigate the exploding gradients problem is to clip the
    gradients during backpropagation so that they never exceed some threshold. This
    is called [*gradient clipping*](https://homl.info/52).⁠^([14](ch11.html#idm45720198748720))
    This technique is generally used in recurrent neural networks, where using batch
    normalization is tricky (as you will see in [Chapter 15](ch15.html#rnn_chapter)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种缓解梯度爆炸问题的技术是在反向传播过程中裁剪梯度，使其永远不超过某个阈值。这被称为[*梯度裁剪*](https://homl.info/52)。⁠^([14](ch11.html#idm45720198748720))
    这种技术通常用于循环神经网络中，其中使用批量归一化是棘手的（正如您将在[第15章](ch15.html#rnn_chapter)中看到的）。
- en: 'In Keras, implementing gradient clipping is just a matter of setting the `clipvalue`
    or `clipnorm` argument when creating an optimizer, like this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，实现梯度裁剪只需要在创建优化器时设置`clipvalue`或`clipnorm`参数，就像这样：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This optimizer will clip every component of the gradient vector to a value between
    –1.0 and 1.0\. This means that all the partial derivatives of the loss (with regard
    to each and every trainable parameter) will be clipped between –1.0 and 1.0\.
    The threshold is a hyperparameter you can tune. Note that it may change the orientation
    of the gradient vector. For instance, if the original gradient vector is [0.9,
    100.0], it points mostly in the direction of the second axis; but once you clip
    it by value, you get [0.9, 1.0], which points roughly at the diagonal between
    the two axes. In practice, this approach works well. If you want to ensure that
    gradient clipping does not change the direction of the gradient vector, you should
    clip by norm by setting `clipnorm` instead of `clipvalue`. This will clip the
    whole gradient if its ℓ[2] norm is greater than the threshold you picked. For
    example, if you set `clipnorm=1.0`, then the vector [0.9, 100.0] will be clipped
    to [0.00899964, 0.9999595], preserving its orientation but almost eliminating
    the first component. If you observe that the gradients explode during training
    (you can track the size of the gradients using TensorBoard), you may want to try
    clipping by value or clipping by norm, with different thresholds, and see which
    option performs best on the validation set.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个优化器将梯度向量的每个分量剪切到-1.0和1.0之间的值。这意味着损失的所有偏导数（对每个可训练参数）将在-1.0和1.0之间被剪切。阈值是您可以调整的超参数。请注意，这可能会改变梯度向量的方向。例如，如果原始梯度向量是[0.9,
    100.0]，它主要指向第二轴的方向；但是一旦您按值剪切它，您会得到[0.9, 1.0]，它大致指向两个轴之间的对角线。在实践中，这种方法效果很好。如果您希望确保梯度剪切不改变梯度向量的方向，您应该通过设置`clipnorm`而不是`clipvalue`来按范数剪切。如果其ℓ[2]范数大于您选择的阈值，则会剪切整个梯度。例如，如果设置`clipnorm=1.0`，那么向量[0.9,
    100.0]将被剪切为[0.00899964, 0.9999595]，保持其方向但几乎消除第一个分量。如果您观察到梯度在训练过程中爆炸（您可以使用TensorBoard跟踪梯度的大小），您可能希望尝试按值剪切或按范数剪切，使用不同的阈值，看看哪个选项在验证集上表现最好。
- en: Reusing Pretrained Layers
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重用预训练层
- en: It is generally not a good idea to train a very large DNN from scratch without
    first trying to find an existing neural network that accomplishes a similar task
    to the one you are trying to tackle (I will discuss how to find them in [Chapter 14](ch14.html#cnn_chapter)).
    If you find such as neural network, then you can generally reuse most of its layers,
    except for the top ones. This technique is called *transfer learning*. It will
    not only speed up training considerably, but also requires significantly less
    training data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常不建议从头开始训练一个非常大的DNN，而不是先尝试找到一个现有的神经网络，完成与您尝试解决的任务类似的任务（我将在[第14章](ch14.html#cnn_chapter)中讨论如何找到它们）。如果找到这样的神经网络，那么通常可以重用大部分层，除了顶部的层。这种技术称为*迁移学习*。它不仅会显著加快训练速度，而且需要的训练数据明显较少。
- en: Suppose you have access to a DNN that was trained to classify pictures into
    100 different categories, including animals, plants, vehicles, and everyday objects,
    and you now want to train a DNN to classify specific types of vehicles. These
    tasks are very similar, even partly overlapping, so you should try to reuse parts
    of the first network (see [Figure 11-5](#reuse_pretrained_diagram)).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您可以访问一个经过训练的DNN，用于将图片分类为100个不同的类别，包括动物、植物、车辆和日常物品，现在您想要训练一个DNN来分类特定类型的车辆。这些任务非常相似，甚至部分重叠，因此您应该尝试重用第一个网络的部分（参见[图11-5](#reuse_pretrained_diagram)）。
- en: Note
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If the input pictures for your new task don’t have the same size as the ones
    used in the original task, you will usually have to add a preprocessing step to
    resize them to the size expected by the original model. More generally, transfer
    learning will work best when the inputs have similar low-level features.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您新任务的输入图片与原始任务中使用的图片大小不同，通常需要添加一个预处理步骤，将它们调整为原始模型期望的大小。更一般地说，当输入具有相似的低级特征时，迁移学习效果最好。
- en: '![mls3 1105](assets/mls3_1105.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1105](assets/mls3_1105.png)'
- en: Figure 11-5\. Reusing pretrained layers
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-5。重用预训练层
- en: The output layer of the original model should usually be replaced because it
    is most likely not useful at all for the new task, and probably will not have
    the right number of outputs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常应该替换原始模型的输出层，因为它很可能对新任务没有用处，而且可能不会有正确数量的输出。
- en: Similarly, the upper hidden layers of the original model are less likely to
    be as useful as the lower layers, since the high-level features that are most
    useful for the new task may differ significantly from the ones that were most
    useful for the original task. You want to find the right number of layers to reuse.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，原始模型的上层隐藏层不太可能像下层那样有用，因为对于新任务最有用的高级特征可能与对原始任务最有用的特征有很大不同。您需要找到要重用的正确层数。
- en: Tip
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The more similar the tasks are, the more layers you will want to reuse (starting
    with the lower layers). For very similar tasks, try to keep all the hidden layers
    and just replace the output layer.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 任务越相似，您将希望重用的层次就越多（从较低层次开始）。对于非常相似的任务，尝试保留所有隐藏层，只替换输出层。
- en: 'Try freezing all the reused layers first (i.e., make their weights non-trainable
    so that gradient descent won’t modify them and they will remain fixed), then train
    your model and see how it performs. Then try unfreezing one or two of the top
    hidden layers to let backpropagation tweak them and see if performance improves.
    The more training data you have, the more layers you can unfreeze. It is also
    useful to reduce the learning rate when you unfreeze reused layers: this will
    avoid wrecking their fine-tuned weights.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先尝试冻结所有重用的层（即使它们的权重不可训练，以便梯度下降不会修改它们并保持固定），然后训练您的模型并查看其表现。然后尝试解冻顶部一两个隐藏层，让反向传播调整它们，看看性能是否提高。您拥有的训练数据越多，您可以解冻的层次就越多。解冻重用层时降低学习率也很有用：这将避免破坏它们微调的权重。
- en: If you still cannot get good performance, and you have little training data,
    try dropping the top hidden layer(s) and freezing all the remaining hidden layers
    again. You can iterate until you find the right number of layers to reuse. If
    you have plenty of training data, you may try replacing the top hidden layers
    instead of dropping them, and even adding more hidden layers.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仍然无法获得良好的性能，并且训练数据很少，尝试删除顶部隐藏层并再次冻结所有剩余的隐藏层。您可以迭代直到找到要重用的正确层数。如果您有大量训练数据，您可以尝试替换顶部隐藏层而不是删除它们，甚至添加更多隐藏层。
- en: Transfer Learning with Keras
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras进行迁移学习
- en: 'Let’s look at an example. Suppose the Fashion MNIST dataset only contained
    eight classes—for example, all the classes except for sandal and shirt. Someone
    built and trained a Keras model on that set and got reasonably good performance
    (>90% accuracy). Let’s call this model A. You now want to tackle a different task:
    you have images of T-shirts and pullovers, and you want to train a binary classifier:
    positive for T-shirts (and tops), negative for sandals. Your dataset is quite
    small; you only have 200 labeled images. When you train a new model for this task
    (let’s call it model B) with the same architecture as model A, you get 91.85%
    test accuracy. While drinking your morning coffee, you realize that your task
    is quite similar to task A, so perhaps transfer learning can help? Let’s find
    out!'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。假设时尚MNIST数据集仅包含八个类别，例如除凉鞋和衬衫之外的所有类别。有人在该数据集上构建并训练了一个Keras模型，并获得了相当不错的性能（>90%的准确率）。我们将这个模型称为A。现在您想要解决一个不同的任务：您有T恤和套头衫的图像，并且想要训练一个二元分类器：对于T恤（和上衣）为正，对于凉鞋为负。您的数据集非常小；您只有200张带标签的图像。当您为这个任务训练一个新模型（我们称之为模型B），其架构与模型A相同时，您获得了91.85%的测试准确率。在喝早晨咖啡时，您意识到您的任务与任务A非常相似，因此也许迁移学习可以帮助？让我们找出来！
- en: 'First, you need to load model A and create a new model based on that model’s
    layers. You decide to reuse all the layers except for the output layer:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要加载模型A并基于该模型的层创建一个新模型。您决定重用除输出层以外的所有层：
- en: '[PRE9]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that `model_A` and `model_B_on_A` now share some layers. When you train
    `model_B_on_A`, it will also affect `model_A`. If you want to avoid that, you
    need to *clone* `model_A` before you reuse its layers. To do this, you clone model
    A’s architecture with `clone_model()`, then copy its weights:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`model_A`和`model_B_on_A`现在共享一些层。当您训练`model_B_on_A`时，它也会影响`model_A`。如果您想避免这种情况，您需要在重用其层之前*克隆*`model_A`。为此，您可以使用`clone_model()`克隆模型A的架构，然后复制其权重：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Warning
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: '`tf.keras.models.clone_model()` only clones the architecture, not the weights.
    If you don’t copy them manually using `set_weights()`, they will be initialized
    randomly when the cloned model is first used.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.models.clone_model()`仅克隆架构，而不是权重。如果您不使用`set_weights()`手动复制它们，那么当首次使用克隆模型时，它们将被随机初始化。'
- en: 'Now you could train `model_B_on_A` for task B, but since the new output layer
    was initialized randomly it will make large errors (at least during the first
    few epochs), so there will be large error gradients that may wreck the reused
    weights. To avoid this, one approach is to freeze the reused layers during the
    first few epochs, giving the new layer some time to learn reasonable weights.
    To do this, set every layer’s `trainable` attribute to `False` and compile the
    model:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以为任务B训练`model_B_on_A`，但由于新的输出层是随机初始化的，它将产生大误差（至少在最初的几个时期），因此会产生大误差梯度，可能会破坏重用的权重。为了避免这种情况，一种方法是在最初的几个时期内冻结重用的层，让新层有时间学习合理的权重。为此，将每个层的`trainable`属性设置为`False`并编译模型：
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You must always compile your model after you freeze or unfreeze layers.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在冻结或解冻层之后，您必须始终编译您的模型。
- en: Now you can train the model for a few epochs, then unfreeze the reused layers
    (which requires compiling the model again) and continue training to fine-tune
    the reused layers for task B. After unfreezing the reused layers, it is usually
    a good idea to reduce the learning rate, once again to avoid damaging the reused
    weights.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以为模型训练几个时期，然后解冻重用的层（这需要重新编译模型）并继续训练以微调任务B的重用层。在解冻重用的层之后，通常最好降低学习率，再次避免损坏重用的权重。
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'So, what’s the final verdict? Well, this model’s test accuracy is 93.85%, up
    exactly two percentage points from 91.85%! This means that transfer learning reduced
    the error rate by almost 25%:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，最终的结论是什么？好吧，这个模型的测试准确率为93.85%，比91.85%高出两个百分点！这意味着迁移学习将错误率减少了近25%：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Are you convinced? You shouldn’t be: I cheated! I tried many configurations
    until I found one that demonstrated a strong improvement. If you try to change
    the classes or the random seed, you will see that the improvement generally drops,
    or even vanishes or reverses. What I did is called “torturing the data until it
    confesses”. When a paper just looks too positive, you should be suspicious: perhaps
    the flashy new technique does not actually help much (in fact, it may even degrade
    performance), but the authors tried many variants and reported only the best results
    (which may be due to sheer luck), without mentioning how many failures they encountered
    on the way. Most of the time, this is not malicious at all, but it is part of
    the reason so many results in science can never be reproduced.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您相信了吗？您不应该相信：我作弊了！我尝试了许多配置，直到找到一个表现出强烈改进的配置。如果您尝试更改类别或随机种子，您会发现改进通常会下降，甚至消失或反转。我所做的被称为“折磨数据直到它招认”。当一篇论文看起来过于积极时，您应该持怀疑态度：也许这种花哨的新技术实际上并没有太大帮助（事实上，它甚至可能降低性能），但作者尝试了许多变体并仅报告了最佳结果（这可能仅仅是纯粹的运气），而没有提及他们在过程中遇到了多少失败。大多数情况下，这并不是恶意的，但这是科学中许多结果永远无法重现的原因之一。
- en: Why did I cheat? It turns out that transfer learning does not work very well
    with small dense networks, presumably because small networks learn few patterns,
    and dense networks learn very specific patterns, which are unlikely to be useful
    in other tasks. Transfer learning works best with deep convolutional neural networks,
    which tend to learn feature detectors that are much more general (especially in
    the lower layers). We will revisit transfer learning in [Chapter 14](ch14.html#cnn_chapter),
    using the techniques we just discussed (and this time there will be no cheating,
    I promise!).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我作弊了？事实证明，迁移学习在小型密集网络上效果不佳，可能是因为小型网络学习的模式较少，而密集网络学习的是非常具体的模式，这些模式不太可能在其他任务中有用。迁移学习最适用于深度卷积神经网络，这些网络倾向于学习更通用的特征检测器（特别是在较低层）。我们将在[第14章](ch14.html#cnn_chapter)中重新讨论迁移学习，使用我们刚讨论的技术（这次不会作弊，我保证！）。
- en: Unsupervised Pretraining
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督预训练
- en: Suppose you want to tackle a complex task for which you don’t have much labeled
    training data, but unfortunately you cannot find a model trained on a similar
    task. Don’t lose hope! First, you should try to gather more labeled training data,
    but if you can’t, you may still be able to perform *unsupervised pretraining*
    (see [Figure 11-6](#unsupervised_pretraining_diagram)). Indeed, it is often cheap
    to gather unlabeled training examples, but expensive to label them. If you can
    gather plenty of unlabeled training data, you can try to use it to train an unsupervised
    model, such as an autoencoder or a generative adversarial network (GAN; see [Chapter 17](ch17.html#autoencoders_chapter)).
    Then you can reuse the lower layers of the autoencoder or the lower layers of
    the GAN’s discriminator, add the output layer for your task on top, and fine-tune
    the final network using supervised learning (i.e., with the labeled training examples).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要解决一个复杂的任务，但您没有太多标记的训练数据，而不幸的是，您找不到一个类似任务训练的模型。不要失去希望！首先，您应该尝试收集更多标记的训练数据，但如果您无法做到，您仍然可以执行*无监督预训练*（见[图11-6](#unsupervised_pretraining_diagram)）。事实上，收集未标记的训练示例通常很便宜，但标记它们却很昂贵。如果您可以收集大量未标记的训练数据，您可以尝试使用它们来训练一个无监督模型，例如自动编码器或生成对抗网络（GAN；见[第17章](ch17.html#autoencoders_chapter)）。然后，您可以重复使用自动编码器的较低层或GAN的鉴别器的较低层，添加顶部的输出层，然后使用监督学习（即使用标记的训练示例）微调最终网络。
- en: It is this technique that Geoffrey Hinton and his team used in 2006, and which
    led to the revival of neural networks and the success of deep learning. Until
    2010, unsupervised pretraining—typically with restricted Boltzmann machines (RBMs;
    see the notebook at [*https://homl.info/extra-anns*](https://homl.info/extra-anns))—was
    the norm for deep nets, and only after the vanishing gradients problem was alleviated
    did it become much more common to train DNNs purely using supervised learning.
    Unsupervised pretraining (today typically using autoencoders or GANs rather than
    RBMs) is still a good option when you have a complex task to solve, no similar
    model you can reuse, and little labeled training data but plenty of unlabeled
    training data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 正是这种技术在2006年由Geoffrey Hinton及其团队使用，导致了神经网络的复兴和深度学习的成功。直到2010年，无监督预训练（通常使用受限玻尔兹曼机（RBMs；请参阅[*https://homl.info/extra-anns*](https://homl.info/extra-anns)中的笔记本））是深度网络的标准，只有在消失梯度问题得到缓解后，纯粹使用监督学习训练DNN才变得更加普遍。无监督预训练（今天通常使用自动编码器或GAN，而不是RBMs）仍然是一个很好的选择，当您有一个复杂的任务需要解决，没有类似的可重用模型，但有大量未标记的训练数据时。
- en: 'Note that in the early days of deep learning it was difficult to train deep
    models, so people would use a technique called *greedy layer-wise pretraining*
    (depicted in [Figure 11-6](#unsupervised_pretraining_diagram)). They would first
    train an unsupervised model with a single layer, typically an RBM, then they would
    freeze that layer and add another one on top of it, then train the model again
    (effectively just training the new layer), then freeze the new layer and add another
    layer on top of it, train the model again, and so on. Nowadays, things are much
    simpler: people generally train the full unsupervised model in one shot and use
    autoencoders or GANs rather than RBMs.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在深度学习的早期阶段，训练深度模型是困难的，因此人们会使用一种称为*贪婪逐层预训练*的技术（在[图11-6](#unsupervised_pretraining_diagram)中描述）。他们首先使用单层训练一个无监督模型，通常是一个RBM，然后冻结该层并在其顶部添加另一层，然后再次训练模型（实际上只是训练新层），然后冻结新层并在其顶部添加另一层，再次训练模型，依此类推。如今，事情简单得多：人们通常一次性训练完整的无监督模型，并使用自动编码器或GAN，而不是RBMs。
- en: '![mls3 1106](assets/mls3_1106.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1106](assets/mls3_1106.png)'
- en: Figure 11-6\. In unsupervised training, a model is trained on all data, including
    the unlabeled data, using an unsupervised learning technique, then it is fine-tuned
    for the final task on just the labeled data using a supervised learning technique;
    the unsupervised part may train one layer at a time as shown here, or it may train
    the full model directly
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-6。在无监督训练中，模型使用无监督学习技术在所有数据上进行训练，包括未标记的数据，然后使用监督学习技术仅在标记的数据上对最终任务进行微调；无监督部分可以像这里所示一次训练一层，也可以直接训练整个模型
- en: Pretraining on an Auxiliary Task
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 辅助任务上的预训练
- en: If you do not have much labeled training data, one last option is to train a
    first neural network on an auxiliary task for which you can easily obtain or generate
    labeled training data, then reuse the lower layers of that network for your actual
    task. The first neural network’s lower layers will learn feature detectors that
    will likely be reusable by the second neural network.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有太多标记的训练数据，最后一个选择是在一个辅助任务上训练第一个神经网络，您可以轻松获取或生成标记的训练数据，然后重复使用该网络的较低层来执行实际任务。第一个神经网络的较低层将学习特征检测器，很可能可以被第二个神经网络重复使用。
- en: For example, if you want to build a system to recognize faces, you may only
    have a few pictures of each individual—clearly not enough to train a good classifier.
    Gathering hundreds of pictures of each person would not be practical. You could,
    however, gather a lot of pictures of random people on the web and train a first
    neural network to detect whether or not two different pictures feature the same
    person. Such a network would learn good feature detectors for faces, so reusing
    its lower layers would allow you to train a good face classifier that uses little
    training data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想构建一个识别人脸的系统，您可能只有每个个体的少量图片，显然不足以训练一个良好的分类器。收集每个人数百张照片是不现实的。但是，您可以在网络上收集大量随机人的照片，并训练第一个神经网络来检测两张不同图片是否展示了同一个人。这样的网络将学习良好的人脸特征检测器，因此重用其较低层将允许您训练一个使用很少训练数据的良好人脸分类器。
- en: For natural language processing (NLP) applications, you can download a corpus
    of millions of text documents and automatically generate labeled data from it.
    For example, you could randomly mask out some words and train a model to predict
    what the missing words are (e.g., it should predict that the missing word in the
    sentence “What ___ you saying?” is probably “are” or “were”). If you can train
    a model to reach good performance on this task, then it will already know quite
    a lot about language, and you can certainly reuse it for your actual task and
    fine-tune it on your labeled data (we will discuss more pretraining tasks in [Chapter 15](ch15.html#rnn_chapter)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自然语言处理（NLP）应用，您可以下载数百万个文本文档的语料库，并从中自动生成标记数据。例如，您可以随机屏蔽一些单词并训练模型来预测缺失的单词是什么（例如，它应该预测句子“What
    ___ you saying?”中缺失的单词可能是“are”或“were”）。如果您可以训练模型在这个任务上达到良好的性能，那么它将已经对语言有相当多的了解，您肯定可以在实际任务中重复使用它，并在标记数据上进行微调（我们将在[第15章](ch15.html#rnn_chapter)中讨论更多的预训练任务）。
- en: Note
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '*Self-supervised learning* is when you automatically generate the labels from
    the data itself, as in the text-masking example, then you train a model on the
    resulting “labeled” dataset using supervised learning techniques.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*自监督学习*是指从数据本身自动生成标签，例如文本屏蔽示例，然后使用监督学习技术在生成的“标记”数据集上训练模型。'
- en: Faster Optimizers
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更快的优化器
- en: 'Training a very large deep neural network can be painfully slow. So far we
    have seen four ways to speed up training (and reach a better solution): applying
    a good initialization strategy for the connection weights, using a good activation
    function, using batch normalization, and reusing parts of a pretrained network
    (possibly built for an auxiliary task or using unsupervised learning). Another
    huge speed boost comes from using a faster optimizer than the regular gradient
    descent optimizer. In this section we will present the most popular optimization
    algorithms: momentum, Nesterov accelerated gradient, AdaGrad, RMSProp, and finally
    Adam and its variants.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个非常庞大的深度神经网络可能会非常缓慢。到目前为止，我们已经看到了四种加速训练（并达到更好解决方案）的方法：应用良好的连接权重初始化策略，使用良好的激活函数，使用批量归一化，并重用预训练网络的部分（可能是为辅助任务构建的或使用无监督学习）。另一个巨大的加速来自使用比常规梯度下降优化器更快的优化器。在本节中，我们将介绍最流行的优化算法：动量、Nesterov加速梯度、AdaGrad、RMSProp，最后是Adam及其变体。
- en: Momentum
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动量
- en: 'Imagine a bowling ball rolling down a gentle slope on a smooth surface: it
    will start out slowly, but it will quickly pick up momentum until it eventually
    reaches terminal velocity (if there is some friction or air resistance). This
    is the core idea behind *momentum optimization*, [proposed by Boris Polyak in
    1964](https://homl.info/54).⁠^([15](ch11.html#idm45720198261376)) In contrast,
    regular gradient descent will take small steps when the slope is gentle and big
    steps when the slope is steep, but it will never pick up speed. As a result, regular
    gradient descent is generally much slower to reach the minimum than momentum optimization.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一颗保龄球在光滑表面上缓坡滚动：它会从慢慢开始，但很快会积累动量，直到最终达到终端速度（如果有一些摩擦或空气阻力）。这就是*动量优化*的核心思想，[由鲍里斯·波利亚克在1964年提出](https://homl.info/54)。与此相反，常规梯度下降在坡度平缓时会采取小步骤，在坡度陡峭时会采取大步骤，但它永远不会加速。因此，与动量优化相比，常规梯度下降通常要慢得多才能达到最小值。
- en: Recall that gradient descent updates the weights **θ** by directly subtracting
    the gradient of the cost function *J*(**θ**) with regard to the weights (∇[**θ**]*J*(**θ**))
    multiplied by the learning rate *η*. The equation is **θ** ← **θ** – *η*∇[**θ**]*J*(**θ**).
    It does not care about what the earlier gradients were. If the local gradient
    is tiny, it goes very slowly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，梯度下降通过直接减去成本函数*J*(**θ**)相对于权重的梯度（∇[**θ**]*J*(**θ**))乘以学习率*η*来更新权重**θ**。方程式为**θ**
    ← **θ** - *η*∇[**θ**]*J*(**θ**)。它不关心先前的梯度是什么。如果局部梯度很小，它会走得很慢。
- en: 'Momentum optimization cares a great deal about what previous gradients were:
    at each iteration, it subtracts the local gradient from the *momentum vector*
    **m** (multiplied by the learning rate *η*), and it updates the weights by adding
    this momentum vector (see [Equation 11-5](#momentum_equation)). In other words,
    the gradient is used as an acceleration, not as a speed. To simulate some sort
    of friction mechanism and prevent the momentum from growing too large, the algorithm
    introduces a new hyperparameter *β*, called the *momentum*, which must be set
    between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 动量优化非常关注先前梯度是什么：在每次迭代中，它从*动量向量* **m**（乘以学习率*η*）中减去局部梯度，然后通过添加这个动量向量来更新权重（参见[方程11-5](#momentum_equation)）。换句话说，梯度被用作加速度，而不是速度。为了模拟某种摩擦机制并防止动量增长过大，该算法引入了一个新的超参数*β*，称为*动量*，必须设置在0（高摩擦）和1（无摩擦）之间。典型的动量值为0.9。
- en: Equation 11-5\. Momentum algorithm
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-5. 动量算法
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">m</mi>
    <mo>←</mo> <mi>β</mi> <mi mathvariant="bold">m</mi> <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi>
    <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo> <mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">m</mi>
    <mo>←</mo> <mi>β</mi> <mi mathvariant="bold">m</mi> <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi>
    <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo> <mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable></math>
- en: You can verify that if the gradient remains constant, the terminal velocity
    (i.e., the maximum size of the weight updates) is equal to that gradient multiplied
    by the learning rate *η* multiplied by 1 / (1 – *β*) (ignoring the sign). For
    example, if *β* = 0.9, then the terminal velocity is equal to 10 times the gradient
    times the learning rate, so momentum optimization ends up going 10 times faster
    than gradient descent! This allows momentum optimization to escape from plateaus
    much faster than gradient descent. We saw in [Chapter 4](ch04.html#linear_models_chapter)
    that when the inputs have very different scales, the cost function will look like
    an elongated bowl (see [Figure 4-7](ch04.html#elongated_bowl_diagram)). Gradient
    descent goes down the steep slope quite fast, but then it takes a very long time
    to go down the valley. In contrast, momentum optimization will roll down the valley
    faster and faster until it reaches the bottom (the optimum). In deep neural networks
    that don’t use batch normalization, the upper layers will often end up having
    inputs with very different scales, so using momentum optimization helps a lot.
    It can also help roll past local optima.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以验证，如果梯度保持不变，则终端速度（即权重更新的最大大小）等于该梯度乘以学习率*η*乘以1 / (1 - *β*)（忽略符号）。例如，如果*β*
    = 0.9，则终端速度等于梯度乘以学习率的10倍，因此动量优化的速度比梯度下降快10倍！这使得动量优化比梯度下降更快地摆脱高原。我们在[第4章](ch04.html#linear_models_chapter)中看到，当输入具有非常不同的比例时，成本函数看起来像一个拉长的碗（参见[图4-7](ch04.html#elongated_bowl_diagram)）。梯度下降很快下降陡峭的斜坡，但然后需要很长时间才能下降到山谷。相比之下，动量优化将会越来越快地滚动到山谷，直到达到底部（最优解）。在不使用批量归一化的深度神经网络中，上层通常会出现具有非常不同比例的输入，因此使用动量优化会有很大帮助。它还可以帮助跳过局部最优解。
- en: Note
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot
    again, and oscillate like this many times before stabilizing at the minimum. This
    is one of the reasons it’s good to have a bit of friction in the system: it gets
    rid of these oscillations and thus speeds up convergence.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于动量的原因，优化器可能会稍微超调，然后返回，再次超调，并在稳定在最小值之前多次振荡。这是有摩擦力的好处之一：它消除了这些振荡，从而加快了收敛速度。
- en: 'Implementing momentum optimization in Keras is a no-brainer: just use the `SGD`
    optimizer and set its `momentum` hyperparameter, then lie back and profit!'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中实现动量优化是一件轻而易举的事情：只需使用`SGD`优化器并设置其`momentum`超参数，然后躺下来赚钱！
- en: '[PRE14]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The one drawback of momentum optimization is that it adds yet another hyperparameter
    to tune. However, the momentum value of 0.9 usually works well in practice and
    almost always goes faster than regular gradient descent.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 动量优化的一个缺点是它增加了另一个需要调整的超参数。然而，在实践中，动量值0.9通常效果很好，几乎总是比常规梯度下降更快。
- en: Nesterov Accelerated Gradient
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Nesterov加速梯度
- en: One small variant to momentum optimization, proposed by [Yurii Nesterov in 1983](https://homl.info/55),⁠^([16](ch11.html#idm45720198190768))
    is almost always faster than regular momentum optimization. The *Nesterov accelerated
    gradient* (NAG) method, also known as *Nesterov momentum optimization*, measures
    the gradient of the cost function not at the local position **θ** but slightly
    ahead in the direction of the momentum, at **θ** + *β***m** (see [Equation 11-6](#nesterov_momentum_equation)).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 动量优化的一个小变体，由[Yurii Nesterov于1983年提出](https://homl.info/55)，^([16](ch11.html#idm45720198190768))几乎总是比常规动量优化更快。*Nesterov加速梯度*（NAG）方法，也称为*Nesterov动量优化*，测量成本函数的梯度不是在本地位置**θ**处，而是稍微向前在动量方向，即**θ**
    + *β***m**（参见[方程11-6](#nesterov_momentum_equation)）。
- en: Equation 11-6\. Nesterov accelerated gradient algorithm
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第11-6方程。Nesterov加速梯度算法
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">m</mi>
    <mo>←</mo> <mi>β</mi> <mi mathvariant="bold">m</mi> <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>+</mo> <mi>β</mi> <mi mathvariant="bold">m</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo>
    <mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">m</mi>
    <mo>←</mo> <mi>β</mi> <mi mathvariant="bold">m</mi> <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>+</mo> <mi>β</mi> <mi mathvariant="bold">m</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo>
    <mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable></math>
- en: This small tweak works because in general the momentum vector will be pointing
    in the right direction (i.e., toward the optimum), so it will be slightly more
    accurate to use the gradient measured a bit farther in that direction rather than
    the gradient at the original position, as you can see in [Figure 11-7](#nesterov_momentum_diagram)
    (where ∇[1] represents the gradient of the cost function measured at the starting
    point **θ**, and ∇[2] represents the gradient at the point located at **θ** +
    *β***m**).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小调整有效是因为通常动量向量将指向正确的方向（即朝向最优解），因此使用稍微更准确的梯度测量更有利于使用稍微更远处的梯度，而不是原始位置处的梯度，如您在[图11-7](#nesterov_momentum_diagram)中所见（其中∇[1]表示在起始点**θ**处测量的成本函数的梯度，而∇[2]表示在位于**θ**
    + *β***m**的点处测量的梯度）。
- en: '![mls3 1107](assets/mls3_1107.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1107](assets/mls3_1107.png)'
- en: 'Figure 11-7\. Regular versus Nesterov momentum optimization: the former applies
    the gradients computed before the momentum step, while the latter applies the
    gradients computed after'
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-7。常规与Nesterov动量优化：前者应用动量步骤之前计算的梯度，而后者应用动量步骤之后计算的梯度
- en: As you can see, the Nesterov update ends up closer to the optimum. After a while,
    these small improvements add up and NAG ends up being significantly faster than
    regular momentum optimization. Moreover, note that when the momentum pushes the
    weights across a valley, ∇[1] continues to push farther across the valley, while
    ∇[2] pushes back toward the bottom of the valley. This helps reduce oscillations
    and thus NAG converges faster.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Nesterov更新最终更接近最优解。随着时间的推移，这些小的改进累积起来，NAG最终比常规动量优化快得多。此外，请注意，当动量将权重推过山谷时，∇[1]继续推动更远，而∇[2]则向山谷底部推回。这有助于减少振荡，因此NAG收敛更快。
- en: 'To use NAG, simply set `nesterov=True` when creating the `SGD` optimizer:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用NAG，只需在创建`SGD`优化器时设置`nesterov=True`：
- en: '[PRE15]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: AdaGrad
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaGrad
- en: 'Consider the elongated bowl problem again: gradient descent starts by quickly
    going down the steepest slope, which does not point straight toward the global
    optimum, then it very slowly goes down to the bottom of the valley. It would be
    nice if the algorithm could correct its direction earlier to point a bit more
    toward the global optimum. The [*AdaGrad* algorithm](https://homl.info/56)⁠^([17](ch11.html#idm45720198095888))
    achieves this correction by scaling down the gradient vector along the steepest
    dimensions (see [Equation 11-7](#adagrad_algorithm)).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑再次延长碗问题：梯度下降首先快速沿着最陡的斜坡下降，这并不直指全局最优解，然后它非常缓慢地下降到山谷底部。如果算法能够更早地纠正方向，使其更多地指向全局最优解，那将是很好的。[*AdaGrad*算法](https://homl.info/56)通过沿着最陡的维度缩小梯度向量来实现这种校正（参见[方程11-7](#adagrad_algorithm)）。
- en: Equation 11-7\. AdaGrad algorithm
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-7。AdaGrad算法
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi>
    <mo>←</mo> <mi mathvariant="bold">s</mi> <mo>+</mo> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi>
    <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow> <mo>⊘</mo> <msqrt><mrow><mi mathvariant="bold">s</mi> <mo>+</mo>
    <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable></math>
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi>
    <mo>←</mo> <mi mathvariant="bold">s</mi> <mo>+</mo> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi>
    <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow> <mo>⊘</mo> <msqrt><mrow><mi mathvariant="bold">s</mi> <mo>+</mo>
    <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable></math>
- en: The first step accumulates the square of the gradients into the vector **s**
    (recall that the ⊗ symbol represents the element-wise multiplication). This vectorized
    form is equivalent to computing *s*[*i*] ← *s*[*i*] + (∂ *J*(**θ**) / ∂ *θ*[*i*])²
    for each element *s*[*i*] of the vector **s**; in other words, each *s*[*i*] accumulates
    the squares of the partial derivative of the cost function with regard to parameter
    *θ*[*i*]. If the cost function is steep along the *i*^(th) dimension, then *s*[*i*]
    will get larger and larger at each iteration.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步将梯度的平方累积到向量**s**中（请记住，⊗符号表示逐元素乘法）。这种向量化形式等同于计算*s*[i] ← *s*[i] + (∂*J*(**θ**)/∂*θ*[i])²，对于向量**s**的每个元素*s*[i]来说，换句话说，每个*s*[i]累积了成本函数对参数*θ*[i]的偏导数的平方。如果成本函数沿第*i*维陡峭，那么在每次迭代中*s*[i]将变得越来越大。
- en: 'The second step is almost identical to gradient descent, but with one big difference:
    the gradient vector is scaled down by a factor of <math><msqrt><mrow><mi mathvariant="bold">s</mi><mo>+</mo><mi>ε</mi></mrow></msqrt></math>
    (the ⊘ symbol represents the element-wise division, and *ε* is a smoothing term
    to avoid division by zero, typically set to 10^(–10)). This vectorized form is
    equivalent to simultaneously computing <math><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>-</mo><mi>η</mi><mo>∂</mo><mi>J</mi><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo><mo>/</mo><mo>∂</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>/</mo><msqrt><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mi>ε</mi></msqrt></math>
    for all parameters *θ*[*i*].'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步几乎与梯度下降完全相同，但有一个重大区别：梯度向量被一个因子<math><msqrt><mrow><mi mathvariant="bold">s</mi><mo>+</mo><mi>ε</mi></mrow></msqrt></math>缩小（⊘符号表示逐元素除法，*ε*是一个平滑项，用于避免除以零，通常设置为10^(–10)）。这个向量化形式等价于同时计算所有参数*θ*[*i*]的<math><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>-</mo><mi>η</mi><mo>∂</mo><mi>J</mi><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo><mo>/</mo><mo>∂</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>/</mo><msqrt><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mi>ε</mi></msqrt></math>。
- en: In short, this algorithm decays the learning rate, but it does so faster for
    steep dimensions than for dimensions with gentler slopes. This is called an *adaptive
    learning rate*. It helps point the resulting updates more directly toward the
    global optimum (see [Figure 11-8](#adagrad_diagram)). One additional benefit is
    that it requires much less tuning of the learning rate hyperparameter *η*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这个算法会衰减学习率，但对于陡峭的维度比对于坡度较缓的维度衰减得更快。这被称为*自适应学习率*。它有助于更直接地指向全局最优（参见[图11-8](#adagrad_diagram)）。另一个好处是它需要更少的调整学习率超参数*η*。
- en: '![mls3 1108](assets/mls3_1108.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1108](assets/mls3_1108.png)'
- en: 'Figure 11-8\. AdaGrad versus gradient descent: the former can correct its direction
    earlier to point to the optimum'
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-8\. AdaGrad与梯度下降的比较：前者可以更早地纠正方向指向最优点
- en: 'AdaGrad frequently performs well for simple quadratic problems, but it often
    stops too early when training neural networks: the learning rate gets scaled down
    so much that the algorithm ends up stopping entirely before reaching the global
    optimum. So even though Keras has an `Adagrad` optimizer, you should not use it
    to train deep neural networks (it may be efficient for simpler tasks such as linear
    regression, though). Still, understanding AdaGrad is helpful to comprehend the
    other adaptive learning rate optimizers.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的二次问题上，AdaGrad通常表现良好，但在训练神经网络时经常会过早停止：学习率被缩小得太多，以至于算法最终在达到全局最优之前完全停止。因此，即使Keras有一个`Adagrad`优化器，你也不应该用它来训练深度神经网络（尽管对于简单任务如线性回归可能是有效的）。不过，理解AdaGrad有助于理解其他自适应学习率优化器。
- en: RMSProp
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RMSProp
- en: As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never
    converging to the global optimum. The *RMSProp* algorithm⁠^([18](ch11.html#idm45720198007744))
    fixes this by accumulating only the gradients from the most recent iterations,
    as opposed to all the gradients since the beginning of training. It does so by
    using exponential decay in the first step (see [Equation 11-8](#rmsprop_algorithm)).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，AdaGrad有减速得太快并且永远无法收敛到全局最优的风险。*RMSProp*算法⁠^([18](ch11.html#idm45720198007744))通过仅累积最近迭代的梯度来修复这个问题，而不是自训练开始以来的所有梯度。它通过在第一步中使用指数衰减来实现这一点（参见[方程11-8](#rmsprop_algorithm)）。
- en: Equation 11-8\. RMSProp algorithm
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-8\. RMSProp算法
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi>
    <mo>←</mo> <mi>ρ</mi> <mi mathvariant="bold">s</mi> <mo>+</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>ρ</mi> <mo>)</mo></mrow> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi>
    <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow> <mo>⊘</mo> <msqrt><mrow><mi mathvariant="bold">s</mi> <mo>+</mo>
    <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable></math>
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi>
    <mo>←</mo> <mi>ρ</mi> <mi mathvariant="bold">s</mi> <mo>+</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>ρ</mi> <mo>)</mo></mrow> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi>
    <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow> <mo>⊘</mo> <msqrt><mrow><mi mathvariant="bold">s</mi> <mo>+</mo>
    <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable></math>
- en: The decay rate *ρ* is typically set to 0.9.⁠^([19](ch11.html#idm45720197966960))
    Yes, it is once again a new hyperparameter, but this default value often works
    well, so you may not need to tune it at all.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 衰减率*ρ*通常设置为0.9。⁠^([19](ch11.html#idm45720197966960)) 是的，这又是一个新的超参数，但这个默认值通常效果很好，所以你可能根本不需要调整它。
- en: 'As you might expect, Keras has an `RMSprop` optimizer:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所期望的，Keras有一个`RMSprop`优化器：
- en: '[PRE16]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Except on very simple problems, this optimizer almost always performs much better
    than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers
    until Adam optimization came around.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在非常简单的问题上，这个优化器几乎总是比AdaGrad表现得更好。事实上，直到Adam优化算法出现之前，它一直是许多研究人员首选的优化算法。
- en: Adam
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚当
- en: '[*Adam*](https://homl.info/59),⁠^([20](ch11.html#idm45720197921776)) which
    stands for *adaptive moment estimation*, combines the ideas of momentum optimization
    and RMSProp: just like momentum optimization, it keeps track of an exponentially
    decaying average of past gradients; and just like RMSProp, it keeps track of an
    exponentially decaying average of past squared gradients (see [Equation 11-9](#adam_algorithm)).
    These are estimations of the mean and (uncentered) variance of the gradients.
    The mean is often called the *first moment* while the variance is often called
    the *second moment*, hence the name of the algorithm.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Equation 11-9\. Adam algorithm
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">m</mi>
    <mo>←</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi mathvariant="bold">m</mi> <mo>-</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi>
    <mo>←</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mi mathvariant="bold">s</mi> <mo>+</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo> <msub><mi>∇</mi> <mi
    mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>3</mn>
    <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mover accent="true"><mi
    mathvariant="bold">m</mi><mo>^</mo></mover> <mo>←</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mi mathvariant="bold">m</mi> <mrow><mn>1</mn> <mo>-</mo>
    <msup><mrow><msub><mi>β</mi> <mn>1</mn></msub></mrow> <mi>t</mi></msup></mrow></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>4</mn> <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mover
    accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover> <mo>←</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mi mathvariant="bold">s</mi> <mrow><mn>1</mn><mo>-</mo><msup><mrow><msub><mi>β</mi>
    <mn>2</mn></msub></mrow> <mi>t</mi></msup></mrow></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>5</mn> <mo>.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo>
    <mi>η</mi> <mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover>
    <mo>⊘</mo> <msqrt><mrow><mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover>
    <mo>+</mo> <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable></math>
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, *t* represents the iteration number (starting at 1).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'If you just look at steps 1, 2, and 5, you will notice Adam’s close similarity
    to both momentum optimization and RMSProp: *β*[1] corresponds to *β* in momentum
    optimization, and *β*[2] corresponds to *ρ* in RMSProp. The only difference is
    that step 1 computes an exponentially decaying average rather than an exponentially
    decaying sum, but these are actually equivalent except for a constant factor (the
    decaying average is just 1 – *β*[1] times the decaying sum). Steps 3 and 4 are
    somewhat of a technical detail: since **m** and **s** are initialized at 0, they
    will be biased toward 0 at the beginning of training, so these two steps will
    help boost **m** and **s** at the beginning of training.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The momentum decay hyperparameter *β*[1] is typically initialized to 0.9, while
    the scaling decay hyperparameter *β*[2] is often initialized to 0.999\. As earlier,
    the smoothing term *ε* is usually initialized to a tiny number such as 10^(–7).
    These are the default values for the `Adam` class. Here is how to create an Adam
    optimizer using Keras:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 动量衰减超参数*β*[1]通常初始化为0.9，而缩放衰减超参数*β*[2]通常初始化为0.999。与之前一样，平滑项*ε*通常初始化为一个非常小的数字，如10^(–7)。这些是`Adam`类的默认值。以下是如何在Keras中创建Adam优化器的方法：
- en: '[PRE17]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Since Adam is an adaptive learning rate algorithm, like AdaGrad and RMSProp,
    it requires less tuning of the learning rate hyperparameter *η*. You can often
    use the default value *η* = 0.001, making Adam even easier to use than gradient
    descent.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Adam是一种自适应学习率算法，类似于AdaGrad和RMSProp，它需要较少调整学习率超参数*η*。您通常可以使用默认值*η*=0.001，使得Adam比梯度下降更容易使用。
- en: Tip
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you are starting to feel overwhelmed by all these different techniques and
    are wondering how to choose the right ones for your task, don’t worry: some practical
    guidelines are provided at the end of this chapter.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您开始感到对所有这些不同技术感到不知所措，并想知道如何为您的任务选择合适的技术，不用担心：本章末尾提供了一些实用指南。
- en: 'Finally, three variants of Adam are worth mentioning: AdaMax, Nadam, and AdamW.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得一提的是Adam的三个变体：AdaMax、Nadam和AdamW。
- en: AdaMax
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaMax
- en: The Adam paper also introduced AdaMax. Notice that in step 2 of [Equation 11-9](#adam_algorithm),
    Adam accumulates the squares of the gradients in **s** (with a greater weight
    for more recent gradients). In step 5, if we ignore *ε* and steps 3 and 4 (which
    are technical details anyway), Adam scales down the parameter updates by the square
    root of **s**. In short, Adam scales down the parameter updates by the ℓ[2] norm
    of the time-decayed gradients (recall that the ℓ[2] norm is the square root of
    the sum of squares).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Adam论文还介绍了AdaMax。请注意，在[方程式11-9](#adam_algorithm)的第2步中，Adam在**s**中累积梯度的平方（对于最近的梯度有更大的权重）。在第5步中，如果我们忽略*ε*和步骤3和4（这些都是技术细节），Adam通过**s**的平方根缩小参数更新。简而言之，Adam通过时间衰减梯度的ℓ[2]范数缩小参数更新（回想一下，ℓ[2]范数是平方和的平方根）。
- en: AdaMax replaces the ℓ[2] norm with the ℓ[∞] norm (a fancy way of saying the
    max). Specifically, it replaces step 2 in [Equation 11-9](#adam_algorithm) with
    <math><mi mathvariant="bold">s</mi><mo>←</mo><mpadded lspace="-1px"><mi>max</mi><mo>(</mo><msub><mi>β</mi><mn>2</mn></msub><mi
    mathvariant="bold">s</mi><mo>,</mo> <mo>abs(</mo><msub><mo mathvariant="bold">∇</mo><mi
    mathvariant="bold">θ</mi></msub><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo><mo>)</mo><mo>)</mo></mpadded></math>,
    it drops step 4, and in step 5 it scales down the gradient updates by a factor
    of **s**, which is the max of the absolute value of the time-decayed gradients.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: AdaMax用ℓ[∞]范数（一种说法是最大值）替换了ℓ[2]范数。具体来说，它用<math><mi mathvariant="bold">s</mi><mo>←</mo><mpadded
    lspace="-1px"><mi>max</mi><mo>(</mo><msub><mi>β</mi><mn>2</mn></msub><mi mathvariant="bold">s</mi><mo>,</mo>
    <mo>abs(</mo><msub><mo mathvariant="bold">∇</mo><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo><mo>)</mo><mo>)</mo></mpadded></math>替换了方程式11-9的第2步，删除了第4步，在第5步中，它通过**s**的因子缩小梯度更新，**s**是时间衰减梯度的绝对值的最大值。
- en: In practice, this can make AdaMax more stable than Adam, but it really depends
    on the dataset, and in general Adam performs better. So, this is just one more
    optimizer you can try if you experience problems with Adam on some task.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这使得AdaMax比Adam更稳定，但这确实取决于数据集，总体上Adam表现更好。因此，如果您在某些任务上遇到Adam的问题，这只是另一个您可以尝试的优化器。
- en: Nadam
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Nadam
- en: Nadam optimization is Adam optimization plus the Nesterov trick, so it will
    often converge slightly faster than Adam. In [his report introducing this technique](https://homl.info/nadam),⁠^([21](ch11.html#idm45720197757088))
    the researcher Timothy Dozat compares many different optimizers on various tasks
    and finds that Nadam generally outperforms Adam but is sometimes outperformed
    by RMSProp.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Nadam优化是Adam优化加上Nesterov技巧，因此它通常会比Adam收敛速度稍快。在[介绍这种技术的研究报告](https://homl.info/nadam)中，研究员Timothy
    Dozat比较了许多不同的优化器在各种任务上的表现，发现Nadam通常优于Adam，但有时会被RMSProp超越。
- en: AdamW
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdamW
- en: '[AdamW](https://homl.info/adamw)⁠^([22](ch11.html#idm45720197753840)) is a
    variant of Adam that integrates a regularization technique called *weight decay*.
    Weight decay reduces the size of the model’s weights at each training iteration
    by multiplying them by a decay factor such as 0.99\. This may remind you of ℓ[2]
    regularization (introduced in [Chapter 4](ch04.html#linear_models_chapter)), which
    also aims to keep the weights small, and indeed it can be shown mathematically
    that ℓ[2] regularization is equivalent to weight decay when using SGD. However,
    when using Adam or its variants, ℓ[2] regularization and weight decay are *not*
    equivalent: in practice, combining Adam with ℓ[2] regularization results in models
    that often don’t generalize as well as those produced by SGD. AdamW fixes this
    issue by properly combining Adam with weight decay.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[AdamW](https://homl.info/adamw)是Adam的一个变体，它集成了一种称为*权重衰减*的正则化技术。权重衰减通过将模型的权重在每次训练迭代中乘以一个衰减因子，如0.99，来减小权重的大小。这可能让您想起ℓ[2]正则化（在[第4章](ch04.html#linear_models_chapter)介绍），它也旨在保持权重较小，事实上，可以在数学上证明，当使用SGD时，ℓ[2]正则化等效于权重衰减。然而，当使用Adam或其变体时，ℓ[2]正则化和权重衰减*不*等效：实际上，将Adam与ℓ[2]正则化结合使用会导致模型通常不如SGD产生的模型泛化能力好。AdamW通过正确地将Adam与权重衰减结合来解决这个问题。'
- en: Warning
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Adaptive optimization methods (including RMSProp, Adam, AdaMax, Nadam, and
    AdamW optimization) are often great, converging fast to a good solution. However,
    a [2017 paper](https://homl.info/60)⁠^([23](ch11.html#idm45720197744832)) by Ashia
    C. Wilson et al. showed that they can lead to solutions that generalize poorly
    on some datasets. So when you are disappointed by your model’s performance, try
    using NAG instead: your dataset may just be allergic to adaptive gradients. Also
    check out the latest research, because it’s moving fast.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应优化方法（包括RMSProp、Adam、AdaMax、Nadam和AdamW优化）通常很好，快速收敛到一个好的解决方案。然而，阿希亚·C·威尔逊等人在一篇[2017年的论文](https://homl.info/60)中表明，它们可能导致在某些数据集上泛化能力较差的解决方案。因此，当您对模型的性能感到失望时，请尝试使用NAG：您的数据集可能只是对自适应梯度过敏。还要关注最新的研究，因为它发展迅速。
- en: To use Nadam, AdaMax, or AdamW in Keras, replace `tf.keras.optimizers.Adam`
    with `tf.keras.optimizers.Nadam`, `tf.keras.optimizers.Adamax`, or `tf.keras.optimizers.experimental.AdamW`.
    For AdamW, you probably want to tune the `weight_decay` hyperparameter.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Keras中使用Nadam、AdaMax或AdamW，请将`tf.keras.optimizers.Adam`替换为`tf.keras.optimizers.Nadam`、`tf.keras.optimizers.Adamax`或`tf.keras.optimizers.experimental.AdamW`。对于AdamW，您可能需要调整`weight_decay`超参数。
- en: All the optimization techniques discussed so far only rely on the *first-order
    partial derivatives* (*Jacobians*). The optimization literature also contains
    amazing algorithms based on the *second-order partial derivatives* (the *Hessians*,
    which are the partial derivatives of the Jacobians). Unfortunately, these algorithms
    are very hard to apply to deep neural networks because there are *n*² Hessians
    per output (where *n* is the number of parameters), as opposed to just *n* Jacobians
    per output. Since DNNs typically have tens of thousands of parameters or more,
    the second-order optimization algorithms often don’t even fit in memory, and even
    when they do, computing the Hessians is just too slow.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的所有优化技术只依赖于*一阶偏导数*（*雅可比*）。优化文献中还包含基于*二阶偏导数*（*海森*，即雅可比的偏导数）的惊人算法。不幸的是，这些算法很难应用于深度神经网络，因为每个输出有*n*²个海森（其中*n*是参数的数量），而不是每个输出只有*n*个雅可比。由于DNN通常具有成千上万个参数甚至更多，第二阶优化算法通常甚至无法适应内存，即使能够适应，计算海森也太慢。
- en: '[Table 11-2](#optimizer_summary_table) compares all the optimizers we’ve discussed
    so far (* is bad, ** is average, and *** is good).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11-2](#optimizer_summary_table)比较了到目前为止我们讨论过的所有优化器（*是不好的，**是平均的，***是好的）。'
- en: Table 11-2\. Optimizer comparison
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-2。优化器比较
- en: '| Class | Convergence speed | Convergence quality |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 类 | 收敛速度 | 收敛质量 |'
- en: '| --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `SGD` | * | *** |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| `SGD` | * | *** |'
- en: '| `SGD(momentum=...)` | ** | *** |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| `SGD(momentum=...)` | ** | *** |'
- en: '| `SGD(momentum=..., nesterov=True)` | ** | *** |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| `SGD(momentum=..., nesterov=True)` | ** | *** |'
- en: '| `Adagrad` | *** | * (stops too early) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `Adagrad` | *** | *（过早停止） |'
- en: '| `RMSprop` | *** | ** or *** |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `RMSprop` | *** | ** or *** |'
- en: '| `Adam` | *** | ** or *** |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `Adam` | *** | ** or *** |'
- en: '| `AdaMax` | *** | ** or *** |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `AdaMax` | *** | ** or *** |'
- en: '| `Nadam` | *** | ** or *** |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `Nadam` | *** | ** or *** |'
- en: '| `AdamW` | *** | ** or *** |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `AdamW` | *** | ** or *** |'
- en: Learning Rate Scheduling
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率调度
- en: Finding a good learning rate is very important. If you set it much too high,
    training may diverge (as discussed in [“Gradient Descent”](ch04.html#gradientDescent4)).
    If you set it too low, training will eventually converge to the optimum, but it
    will take a very long time. If you set it slightly too high, it will make progress
    very quickly at first, but it will end up dancing around the optimum and never
    really settling down. If you have a limited computing budget, you may have to
    interrupt training before it has converged properly, yielding a suboptimal solution
    (see [Figure 11-9](#learning_schedule_diagram)).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个好的学习率非常重要。如果设置得太高，训练可能会发散（如[“梯度下降”](ch04.html#gradientDescent4)中讨论的）。如果设置得太低，训练最终会收敛到最优解，但需要很长时间。如果设置得稍微偏高，它会在一开始就非常快地取得进展，但最终会围绕最优解打转，从未真正稳定下来。如果你的计算预算有限，你可能需要在训练收敛之前中断训练，得到一个次优解（参见[图11-9](#learning_schedule_diagram)）。
- en: '![mls3 1109](assets/mls3_1109.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1109](assets/mls3_1109.png)'
- en: Figure 11-9\. Learning curves for various learning rates η
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-9。不同学习率η的学习曲线
- en: As discussed in [Chapter 10](ch10.html#ann_chapter), you can find a good learning
    rate by training the model for a few hundred iterations, exponentially increasing
    the learning rate from a very small value to a very large value, and then looking
    at the learning curve and picking a learning rate slightly lower than the one
    at which the learning curve starts shooting back up. You can then reinitialize
    your model and train it with that learning rate.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第10章](ch10.html#ann_chapter)中讨论的，您可以通过训练模型几百次，将学习率从一个非常小的值指数增加到一个非常大的值，然后查看学习曲线并选择一个略低于学习曲线开始迅速上升的学习率来找到一个好的学习率。然后，您可以重新初始化您的模型，并使用该学习率进行训练。
- en: 'But you can do better than a constant learning rate: if you start with a large
    learning rate and then reduce it once training stops making fast progress, you
    can reach a good solution faster than with the optimal constant learning rate.
    There are many different strategies to reduce the learning rate during training.
    It can also be beneficial to start with a low learning rate, increase it, then
    drop it again. These strategies are called *learning schedules* (I briefly introduced
    this concept in [Chapter 4](ch04.html#linear_models_chapter)). These are the most
    commonly used learning schedules:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你可以比恒定学习率做得更好：如果你从一个较大的学习率开始，然后在训练停止快速取得进展时降低它，你可以比使用最佳恒定学习率更快地达到一个好的解。有许多不同的策略可以在训练过程中降低学习率。从一个低学习率开始，增加它，然后再次降低它也可能是有益的。这些策略被称为*学习计划*（我在[第4章](ch04.html#linear_models_chapter)中简要介绍了这个概念）。这些是最常用的学习计划：
- en: '*Power scheduling*'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*幂调度*'
- en: 'Set the learning rate to a function of the iteration number *t*: *η*(*t*) =
    *η*[0] / (1 + *t*/*s*)^(*c*). The initial learning rate *η*[0], the power *c*
    (typically set to 1), and the steps *s* are hyperparameters. The learning rate
    drops at each step. After *s* steps, the learning rate is down to *η*[0] / 2.
    After *s* more steps it is down to *η*[0] / 3, then it goes down to *η*[0] / 4,
    then *η*[0] / 5, and so on. As you can see, this schedule first drops quickly,
    then more and more slowly. Of course, power scheduling requires tuning *η*[0]
    and *s* (and possibly *c*).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 将学习率设置为迭代次数*t*的函数：*η*(*t*) = *η*[0] / (1 + *t*/*s*)^(*c*)。初始学习率*η*[0]，幂*c*（通常设置为1）和步长*s*是超参数。学习率在每一步下降。经过*s*步，学习率降至*η*[0]的一半。再经过*s*步，它降至*η*[0]的1/3，然后降至*η*[0]的1/4，然后*η*[0]的1/5，依此类推。正如您所看到的，这个调度首先快速下降，然后变得越来越慢。当然，幂调度需要调整*η*[0]和*s*（可能还有*c*）。
- en: '*Exponential scheduling*'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '*指数调度*'
- en: Set the learning rate to *η*(*t*) = *η*[0] 0.1^(*t/s*). The learning rate will
    gradually drop by a factor of 10 every *s* steps. While power scheduling reduces
    the learning rate more and more slowly, exponential scheduling keeps slashing
    it by a factor of 10 every *s* steps.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 将学习率设置为*η*(*t*) = *η*[0] 0.1^(*t/s*)。学习率将每*s*步逐渐降低10倍。虽然幂调度使学习率降低得越来越慢，指数调度则每*s*步将其降低10倍。
- en: '*Piecewise constant scheduling*'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*分段常数调度*'
- en: Use a constant learning rate for a number of epochs (e.g., *η*[0] = 0.1 for
    5 epochs), then a smaller learning rate for another number of epochs (e.g., *η*[1]
    = 0.001 for 50 epochs), and so on. Although this solution can work very well,
    it requires fiddling around to figure out the right sequence of learning rates
    and how long to use each of them.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些时期内使用恒定的学习率（例如，*η*[0] = 0.1，持续5个时期），然后在另一些时期内使用较小的学习率（例如，*η*[1] = 0.001，持续50个时期），依此类推。尽管这种解决方案可能效果很好，但需要调整以找出正确的学习率序列以及每个学习率使用的时间长度。
- en: '*Performance scheduling*'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*性能调度*'
- en: Measure the validation error every *N* steps (just like for early stopping),
    and reduce the learning rate by a factor of *λ* when the error stops dropping.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 每*N*步测量验证错误（就像提前停止一样），当错误停止下降时，将学习率降低*λ*倍。
- en: '*1cycle scheduling*'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*1cycle调度*'
- en: 1cycle was introduced in a [2018 paper](https://homl.info/1cycle) by Leslie
    Smith.⁠^([24](ch11.html#idm45720197650432)) Contrary to the other approaches,
    it starts by increasing the initial learning rate *η*[0], growing linearly up
    to *η*[1] halfway through training. Then it decreases the learning rate linearly
    down to *η*[0] again during the second half of training, finishing the last few
    epochs by dropping the rate down by several orders of magnitude (still linearly).
    The maximum learning rate *η*[1] is chosen using the same approach we used to
    find the optimal learning rate, and the initial learning rate *η*[0] is usually
    10 times lower. When using a momentum, we start with a high momentum first (e.g.,
    0.95), then drop it down to a lower momentum during the first half of training
    (e.g., down to 0.85, linearly), and then bring it back up to the maximum value
    (e.g., 0.95) during the second half of training, finishing the last few epochs
    with that maximum value. Smith did many experiments showing that this approach
    was often able to speed up training considerably and reach better performance.
    For example, on the popular CIFAR10 image dataset, this approach reached 91.9%
    validation accuracy in just 100 epochs, compared to 90.3% accuracy in 800 epochs
    through a standard approach (with the same neural network architecture). This
    feat was dubbed *super-convergence*.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 1cycle是由Leslie Smith在[2018年的一篇论文](https://homl.info/1cycle)中提出的。与其他方法相反，它从增加初始学习率*η*[0]开始，线性增长到训练中途的*η*[1]。然后在训练的第二半部分线性降低学习率至*η*[0]，最后几个时期通过几个数量级的降低率（仍然是线性）来完成。最大学习率*η*[1]是使用我们用来找到最佳学习率的相同方法选择的，初始学习率*η*[0]通常低10倍。当使用动量时，我们首先使用高动量（例如0.95），然后在训练的前半部分将其降低到较低的动量（例如0.85，线性），然后在训练的后半部分将其提高到最大值（例如0.95），最后几个时期使用该最大值。Smith进行了许多实验，表明这种方法通常能够显著加快训练速度并达到更好的性能。例如，在流行的CIFAR10图像数据集上，这种方法仅在100个时期内达到了91.9%的验证准确率，而通过标准方法（使用相同的神经网络架构）在800个时期内仅达到了90.3%的准确率。这一壮举被称为*超级收敛*。
- en: A [2013 paper](https://homl.info/63) by Andrew Senior et al.⁠^([25](ch11.html#idm45720197641920))
    compared the performance of some of the most popular learning schedules when using
    momentum optimization to train deep neural networks for speech recognition. The
    authors concluded that, in this setting, both performance scheduling and exponential
    scheduling performed well. They favored exponential scheduling because it was
    easy to tune and it converged slightly faster to the optimal solution. They also
    mentioned that it was easier to implement than performance scheduling, but in
    Keras both options are easy. That said, the 1cycle approach seems to perform even
    better.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Andrew Senior等人在[2013年的一篇论文](https://homl.info/63)中比较了使用动量优化训练深度神经网络进行语音识别时一些最流行的学习调度的性能。作者得出结论，在这种情况下，性能调度和指数调度表现良好。他们更青睐指数调度，因为它易于调整，并且收敛到最佳解稍快。他们还提到，它比性能调度更容易实现，但在Keras中，这两个选项都很容易。也就是说，1cycle方法似乎表现得更好。
- en: 'Implementing power scheduling in Keras is the easiest option—just set the `decay`
    hyperparameter when creating an optimizer:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中实现幂调度是最简单的选择——只需在创建优化器时设置`衰减`超参数：
- en: '[PRE18]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `decay` is the inverse of *s* (the number of steps it takes to divide the
    learning rate by one more unit), and Keras assumes that *c* is equal to 1.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`衰减`是*s*的倒数（将学习率除以一个单位所需的步数），Keras假设*c*等于1。'
- en: 'Exponential scheduling and piecewise scheduling are quite simple too. You first
    need to define a function that takes the current epoch and returns the learning
    rate. For example, let’s implement exponential scheduling:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 指数调度和分段调度也很简单。您首先需要定义一个函数，该函数接受当前epoch并返回学习率。例如，让我们实现指数调度：
- en: '[PRE19]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If you do not want to hardcode *η*[0] and *s*, you can create a function that
    returns a configured function:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想硬编码 *η*[0] 和 *s*，您可以创建一个返回配置函数的函数：
- en: '[PRE20]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, create a `LearningRateScheduler` callback, giving it the schedule function,
    and pass this callback to the `fit()` method:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个 `LearningRateScheduler` 回调，将调度函数传递给它，并将此回调传递给 `fit()` 方法：
- en: '[PRE21]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `LearningRateScheduler` will update the optimizer’s `learning_rate` attribute
    at the beginning of each epoch. Updating the learning rate once per epoch is usually
    enough, but if you want it to be updated more often, for example at every step,
    you can always write your own callback (see the “Exponential Scheduling” section
    of this chapter’s notebook for an example). Updating the learning rate at every
    step may help if there are many steps per epoch. Alternatively, you can use the
    `tf.keras.​optimiz⁠ers.schedules` approach, described shortly.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`LearningRateScheduler` 将在每个epoch开始时更新优化器的 `learning_rate` 属性。通常每个epoch更新一次学习率就足够了，但是如果您希望更频繁地更新它，例如在每一步，您可以随时编写自己的回调（请参阅本章笔记本中“指数调度”部分的示例）。在每一步更新学习率可能有助于处理每个epoch中的许多步骤。或者，您可以使用
    `tf.keras.​optimiz⁠ers.schedules` 方法，稍后会进行描述。'
- en: Tip
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: After training, `history.history["lr"]` gives you access to the list of learning
    rates used during training.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，`history.history["lr"]` 可以让您访问训练过程中使用的学习率列表。
- en: 'The schedule function can optionally take the current learning rate as a second
    argument. For example, the following schedule function multiplies the previous
    learning rate by 0.1^(1/20), which results in the same exponential decay (except
    the decay now starts at the beginning of epoch 0 instead of 1):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 调度函数可以选择将当前学习率作为第二个参数。例如，以下调度函数将前一个学习率乘以0.1^(1/20)，这将导致相同的指数衰减（除了衰减现在从第0个epoch开始而不是第1个）：
- en: '[PRE22]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This implementation relies on the optimizer’s initial learning rate (contrary
    to the previous implementation), so make sure to set it appropriately.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现依赖于优化器的初始学习率（与之前的实现相反），所以请确保适当设置它。
- en: 'When you save a model, the optimizer and its learning rate get saved along
    with it. This means that with this new schedule function, you could just load
    a trained model and continue training where it left off, no problem. Things are
    not so simple if your schedule function uses the `epoch` argument, however: the
    epoch does not get saved, and it gets reset to 0 every time you call the `fit()`
    method. If you were to continue training a model where it left off, this could
    lead to a very large learning rate, which would likely damage your model’s weights.
    One solution is to manually set the `fit()` method’s `initial_epoch` argument
    so the `epoch` starts at the right value.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当您保存一个模型时，优化器及其学习率也会被保存。这意味着使用这个新的调度函数，您可以加载一个训练好的模型，并继续在离开的地方继续训练，没有问题。然而，如果您的调度函数使用
    `epoch` 参数，情况就不那么简单了：epoch 不会被保存，并且每次调用 `fit()` 方法时都会被重置为 0。如果您要继续训练一个模型，这可能会导致一个非常大的学习率，这可能会损坏模型的权重。一个解决方案是手动设置
    `fit()` 方法的 `initial_epoch` 参数，使 `epoch` 从正确的值开始。
- en: 'For piecewise constant scheduling, you can use a schedule function like the
    following one (as earlier, you can define a more general function if you want;
    see the “Piecewise Constant Scheduling” section of the notebook for an example),
    then create a `LearningRateScheduler` callback with this function and pass it
    to the `fit()` method, just like for exponential scheduling:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分段常数调度，您可以使用以下类似的调度函数（与之前一样，如果您愿意，您可以定义一个更通用的函数；请参阅笔记本中“分段常数调度”部分的示例），然后创建一个带有此函数的
    `LearningRateScheduler` 回调，并将其传递给 `fit()` 方法，就像对指数调度一样：
- en: '[PRE23]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For performance scheduling, use the `ReduceLROnPlateau` callback. For example,
    if you pass the following callback to the `fit()` method, it will multiply the
    learning rate by 0.5 whenever the best validation loss does not improve for five
    consecutive epochs (other options are available; please check the documentation
    for more details):'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 对于性能调度，请使用 `ReduceLROnPlateau` 回调。例如，如果您将以下回调传递给 `fit()` 方法，每当最佳验证损失连续五个epoch没有改善时，它将把学习率乘以0.5（还有其他选项可用；请查看文档以获取更多详细信息）：
- en: '[PRE24]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Lastly, Keras offers an alternative way to implement learning rate scheduling:
    you can define a scheduled learning rate using one of the classes available in
    `tf.keras.​opti⁠mizers.schedules`, then pass it to any optimizer. This approach
    updates the learning rate at each step rather than at each epoch. For example,
    here is how to implement the same exponential schedule as the `exponential_decay_fn()`
    function we defined earlier:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Keras提供了另一种实现学习率调度的方法：您可以使用 `tf.keras.​opti⁠mizers.schedules` 中可用的类之一定义一个调度学习率，然后将其传递给任何优化器。这种方法在每一步而不是每个epoch更新学习率。例如，以下是如何实现与我们之前定义的
    `exponential_decay_fn()` 函数相同的指数调度：
- en: '[PRE25]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This is nice and simple, plus when you save the model, the learning rate and
    its schedule (including its state) get saved as well.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单明了，而且当您保存模型时，学习率及其调度（包括其状态）也会被保存。
- en: As for 1cycle, Keras does not support it, but it’s possible to implement it
    in less than 30 lines of code by creating a custom callback that modifies the
    learning rate at each iteration. To update the optimizer’s learning rate from
    within the callback’s `on_batch_begin()` method, you need to call `tf.keras.​back⁠end.set_value(self.model.optimizer.learning_rate`,
    `new_learning_rate)`. See the “1Cycle Scheduling” section of the notebook for
    an example.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 至于1cycle，Keras 不支持它，但是可以通过创建一个自定义回调，在每次迭代时修改学习率来实现它，代码不到30行。要从回调的 `on_batch_begin()`
    方法中更新优化器的学习率，您需要调用 `tf.keras.​back⁠end.set_value(self.model.optimizer.learning_rate`,
    `new_learning_rate)`。请参阅笔记本中的“1Cycle Scheduling”部分以获取示例。
- en: To sum up, exponential decay, performance scheduling, and 1cycle can considerably
    speed up convergence, so give them a try!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，指数衰减、性能调度和1cycle可以显著加快收敛速度，所以试一试吧！
- en: Avoiding Overfitting Through Regularization
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过正则化避免过拟合
- en: With four parameters I can fit an elephant and with five I can make him wiggle
    his trunk.
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有了四个参数，我可以拟合一只大象，有了五个我可以让它摇动它的鼻子。
- en: ''
  id: totrans-285
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: John von Neumann, cited by Enrico Fermi in *Nature* 427
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 约翰·冯·诺伊曼，引用自恩里科·费米在《自然》427中
- en: With thousands of parameters, you can fit the whole zoo. Deep neural networks
    typically have tens of thousands of parameters, sometimes even millions. This
    gives them an incredible amount of freedom and means they can fit a huge variety
    of complex datasets. But this great flexibility also makes the network prone to
    overfitting the training set. Regularization is often needed to prevent this.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有成千上万个参数，你可以拟合整个动物园。深度神经网络通常有数万个参数，有时甚至有数百万个。这给予它们极大的自由度，意味着它们可以拟合各种复杂的数据集。但这种极大的灵活性也使得网络容易过拟合训练集。通常需要正则化来防止这种情况发生。
- en: 'We already implemented one of the best regularization techniques in [Chapter 10](ch10.html#ann_chapter):
    early stopping. Moreover, even though batch normalization was designed to solve
    the unstable gradients problems, it also acts like a pretty good regularizer.
    In this section we will examine other popular regularization techniques for neural
    networks: ℓ[1] and ℓ[2] regularization, dropout, and max-norm regularization.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第10章](ch10.html#ann_chapter)中实现了最好的正则化技术之一：提前停止。此外，即使批量归一化是为了解决不稳定梯度问题而设计的，它也像一个相当不错的正则化器。在本节中，我们将研究神经网络的其他流行正则化技术：ℓ[1]
    和 ℓ[2] 正则化、dropout 和最大范数正则化。
- en: ℓ[1] and ℓ[2] Regularization
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ℓ[1] 和 ℓ[2] 正则化
- en: 'Just like you did in [Chapter 4](ch04.html#linear_models_chapter) for simple
    linear models, you can use ℓ[2] regularization to constrain a neural network’s
    connection weights, and/or ℓ[1] regularization if you want a sparse model (with
    many weights equal to 0). Here is how to apply ℓ[2] regularization to a Keras
    layer’s connection weights, using a regularization factor of 0.01:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你在[第4章](ch04.html#linear_models_chapter)中为简单线性模型所做的那样，你可以使用 ℓ[2] 正则化来约束神经网络的连接权重，和/或者使用
    ℓ[1] 正则化如果你想要一个稀疏模型（其中许多权重等于0）。以下是如何将 ℓ[2] 正则化应用于Keras层的连接权重，使用正则化因子为0.01：
- en: '[PRE26]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `l2()` function returns a regularizer that will be called at each step during
    training to compute the regularization loss. This is then added to the final loss.
    As you might expect, you can just use `tf.keras.regularizers.l1()` if you want
    ℓ[1] regularization; if you want both ℓ[1] and ℓ[2] regularization, use `tf.keras.regularizers.l1_l2()`
    (specifying both regularization factors).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '`l2()` 函数返回一个正则化器，在训练过程中的每一步都会调用它来计算正则化损失。然后将其添加到最终损失中。正如你所期望的那样，如果你想要 ℓ[1]
    正则化，你可以简单地使用`tf.keras.regularizers.l1()`；如果你想要同时使用 ℓ[1] 和 ℓ[2] 正则化，可以使用`tf.keras.regularizers.l1_l2()`（指定两个正则化因子）。'
- en: 'Since you will typically want to apply the same regularizer to all layers in
    your network, as well as using the same activation function and the same initialization
    strategy in all hidden layers, you may find yourself repeating the same arguments.
    This makes the code ugly and error-prone. To avoid this, you can try refactoring
    your code to use loops. Another option is to use Python’s `functools.partial()`
    function, which lets you create a thin wrapper for any callable, with some default
    argument values:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 由于通常希望在网络的所有层中应用相同的正则化器，以及在所有隐藏层中使用相同的激活函数和相同的初始化策略，你可能会发现自己重复相同的参数。这会使代码变得丑陋且容易出错。为了避免这种情况，你可以尝试重构代码以使用循环。另一个选择是使用Python的`functools.partial()`函数，它允许你为任何可调用对象创建一个薄包装器，并设置一些默认参数值：
- en: '[PRE27]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Warning
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'As we saw earlier, ℓ[2] regularization is fine when using SGD, momentum optimization,
    and Nesterov momentum optimization, but not with Adam and its variants. If you
    want to use Adam with weight decay, then do not use ℓ[2] regularization: use AdamW
    instead.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，当使用 SGD、动量优化和 Nesterov 动量优化时，ℓ[2] 正则化是可以的，但在使用 Adam 及其变种时不行。如果你想要在使用
    Adam 时进行权重衰减，那么不要使用 ℓ[2] 正则化：使用 AdamW 替代。
- en: Dropout
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout
- en: '*Dropout* is one of the most popular regularization techniques for deep neural
    networks. It was [proposed in a paper](https://homl.info/64)⁠^([26](ch11.html#idm45720196986560))
    by Geoffrey Hinton et al. in 2012 and further detailed in a [2014 paper](https://homl.info/65)⁠^([27](ch11.html#idm45720196946560))
    by Nitish Srivastava et al., and it has proven to be highly successful: many state-of-the-art
    neural networks use dropout, as it gives them a 1%–2% accuracy boost. This may
    not sound like a lot, but when a model already has 95% accuracy, getting a 2%
    accuracy boost means dropping the error rate by almost 40% (going from 5% error
    to roughly 3%).'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout* 是深度神经网络中最流行的正则化技术之一。它是由 Geoffrey Hinton 等人在2012年的一篇论文中提出的，并在2014年由
    Nitish Srivastava 等人进一步详细阐述，已被证明非常成功：许多最先进的神经网络使用了dropout，因为它使它们的准确率提高了1%–2%。这听起来可能不多，但当一个模型已经有95%的准确率时，获得2%的准确率提升意味着将错误率减少了近40%（从5%的错误率降至大约3%）。'
- en: 'It is a fairly simple algorithm: at every training step, every neuron (including
    the input neurons, but always excluding the output neurons) has a probability
    *p* of being temporarily “dropped out”, meaning it will be entirely ignored during
    this training step, but it may be active during the next step (see [Figure 11-10](#dropout_diagram)).
    The hyperparameter *p* is called the *dropout rate*, and it is typically set between
    10% and 50%: closer to 20%–30% in recurrent neural nets (see [Chapter 15](ch15.html#rnn_chapter)),
    and closer to 40%–50% in convolutional neural networks (see [Chapter 14](ch14.html#cnn_chapter)).
    After training, neurons don’t get dropped anymore. And that’s all (except for
    a technical detail we will discuss momentarily).'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: It’s surprising at first that this destructive technique works at all. Would
    a company perform better if its employees were told to toss a coin every morning
    to decide whether or not to go to work? Well, who knows; perhaps it would! The
    company would be forced to adapt its organization; it could not rely on any single
    person to work the coffee machine or perform any other critical tasks, so this
    expertise would have to be spread across several people. Employees would have
    to learn to cooperate with many of their coworkers, not just a handful of them.
    The company would become much more resilient. If one person quit, it wouldn’t
    make much of a difference. It’s unclear whether this idea would actually work
    for companies, but it certainly does for neural networks. Neurons trained with
    dropout cannot co-adapt with their neighboring neurons; they have to be as useful
    as possible on their own. They also cannot rely excessively on just a few input
    neurons; they must pay attention to each of their input neurons. They end up being
    less sensitive to slight changes in the inputs. In the end, you get a more robust
    network that generalizes better.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1110](assets/mls3_1110.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
- en: Figure 11-10\. With dropout regularization, at each training iteration a random
    subset of all neurons in one or more layers—except the output layer—are “dropped
    out”; these neurons output 0 at this iteration (represented by the dashed arrows)
  id: totrans-302
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another way to understand the power of dropout is to realize that a unique neural
    network is generated at each training step. Since each neuron can be either present
    or absent, there are a total of 2^(*N*) possible networks (where *N* is the total
    number of droppable neurons). This is such a huge number that it is virtually
    impossible for the same neural network to be sampled twice. Once you have run
    10,000 training steps, you have essentially trained 10,000 different neural networks,
    each with just one training instance. These neural networks are obviously not
    independent because they share many of their weights, but they are nevertheless
    all different. The resulting neural network can be seen as an averaging ensemble
    of all these smaller neural networks.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-304
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In practice, you can usually apply dropout only to the neurons in the top one
    to three layers (excluding the output layer).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one small but important technical detail. Suppose *p* = 75%: on average
    only 25% of all neurons are active at each step during training. This means that
    after training, a neuron would be connected to four times as many input neurons
    as it would be during training. To compensate for this fact, we need to multiply
    each neuron’s input connection weights by four during training. If we don’t, the
    neural network will not perform well as it will see different data during and
    after training. More generally, we need to divide the connection weights by the
    *keep probability* (1 – *p*) during training.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement dropout using Keras, you can use the `tf.keras.layers.Dropout`
    layer. During training, it randomly drops some inputs (setting them to 0) and
    divides the remaining inputs by the keep probability. After training, it does
    nothing at all; it just passes the inputs to the next layer. The following code
    applies dropout regularization before every dense layer, using a dropout rate
    of 0.2:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras实现dropout，可以使用`tf.keras.layers.Dropout`层。在训练期间，它会随机丢弃一些输入（将它们设置为0），并将剩余的输入除以保留概率。训练结束后，它什么也不做；它只是将输入传递给下一层。以下代码在每个密集层之前应用了dropout正则化，使用了0.2的dropout率：
- en: '[PRE28]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Warning
  id: totrans-309
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Since dropout is only active during training, comparing the training loss and
    the validation loss can be misleading. In particular, a model may be overfitting
    the training set and yet have similar training and validation losses. So, make
    sure to evaluate the training loss without dropout (e.g., after training).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 由于dropout只在训练期间激活，比较训练损失和验证损失可能会产生误导。特别是，模型可能会过度拟合训练集，但训练和验证损失却相似。因此，请确保在没有dropout的情况下评估训练损失（例如，在训练后）。
- en: If you observe that the model is overfitting, you can increase the dropout rate.
    Conversely, you should try decreasing the dropout rate if the model underfits
    the training set. It can also help to increase the dropout rate for large layers,
    and reduce it for small ones. Moreover, many state-of-the-art architectures only
    use dropout after the last hidden layer, so you may want to try this if full dropout
    is too strong.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如果观察到模型过拟合，可以增加dropout率。相反，如果模型对训练集拟合不足，可以尝试减少dropout率。对于大型层，增加dropout率，对于小型层，减少dropout率也有帮助。此外，许多最先进的架构仅在最后一个隐藏层之后使用dropout，因此如果全局dropout太强，您可能想尝试这样做。
- en: Dropout does tend to significantly slow down convergence, but it often results
    in a better model when tuned properly. So, it is generally well worth the extra
    time and effort, especially for large models.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout确实会显著减慢收敛速度，但在适当调整后通常会得到更好的模型。因此，额外的时间和精力通常是值得的，特别是对于大型模型。
- en: Tip
  id: totrans-313
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you want to regularize a self-normalizing network based on the SELU activation
    function (as discussed earlier), you should use *alpha dropout*: this is a variant
    of dropout that preserves the mean and standard deviation of its inputs. It was
    introduced in the same paper as SELU, as regular dropout would break self-normalization.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要对基于SELU激活函数的自正则化网络进行正则化（如前面讨论的），应该使用*alpha dropout*：这是一种保留其输入均值和标准差的dropout变体。它是在与SELU一起引入的同一篇论文中提出的，因为常规dropout会破坏自正则化。
- en: Monte Carlo (MC) Dropout
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡洛（MC）Dropout
- en: 'In 2016, a [paper](https://homl.info/mcdropout)⁠^([28](ch11.html#idm45720196754832))
    by Yarin Gal and Zoubin Ghahramani added a few more good reasons to use dropout:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，Yarin Gal和Zoubin Ghahramani的一篇[论文](https://homl.info/mcdropout)建立了使用dropout的更多好理由：
- en: First, the paper established a profound connection between dropout networks
    (i.e., neural networks containing `Dropout` layers) and approximate Bayesian inference,⁠^([29](ch11.html#idm45720196752128))
    giving dropout a solid mathematical justification.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，该论文建立了dropout网络（即包含`Dropout`层的神经网络）与近似贝叶斯推断之间的深刻联系，为dropout提供了坚实的数学理论基础。
- en: Second, the authors introduced a powerful technique called *MC dropout*, which
    can boost the performance of any trained dropout model without having to retrain
    it or even modify it at all. It also provides a much better measure of the model’s
    uncertainty, and it can be implemented in just a few lines of code.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，作者引入了一种强大的技术称为*MC dropout*，它可以提升任何经过训练的dropout模型的性能，而无需重新训练它甚至修改它。它还提供了模型不确定性的更好度量，并且可以在几行代码中实现。
- en: 'If this all sounds like some “one weird trick” clickbait, then take a look
    at the following code. It is the full implementation of MC dropout, boosting the
    dropout model we trained earlier without retraining it:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一切听起来像某种“奇怪的技巧”点击诱饵，那么看看以下代码。这是MC dropout的完整实现，增强了我们之前训练的dropout模型而无需重新训练它：
- en: '[PRE29]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Note that `model(X)` is similar to `model.predict(X)` except it returns a tensor
    rather than a NumPy array, and it supports the `training` argument. In this code
    example, setting `training=True` ensures that the `Dropout` layer remains active,
    so all predictions will be a bit different. We just make 100 predictions over
    the test set, and we compute their average. More specifically, each call to the
    model returns a matrix with one row per instance and one column per class. Because
    there are 10,000 instances in the test set and 10 classes, this is a matrix of
    shape [10000, 10]. We stack 100 such matrices, so `y_probas` is a 3D array of
    shape [100, 10000, 10]. Once we average over the first dimension (`axis=0`) we
    get `y_proba`, an array of shape [10000, 10], like we would get with a single
    prediction. That’s all! Averaging over multiple predictions with dropout turned
    on gives us a Monte Carlo estimate that is generally more reliable than the result
    of a single prediction with dropout turned off. For example, let’s look at the
    model’s prediction for the first instance in the Fashion MNIST test set, with
    dropout turned off:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`model(X)`类似于`model.predict(X)`，只是它返回一个张量而不是NumPy数组，并支持`training`参数。在这个代码示例中，设置`training=True`确保`Dropout`层保持活动状态，因此所有预测都会有些不同。我们只对测试集进行100次预测，并计算它们的平均值。更具体地说，每次调用模型都会返回一个矩阵，每个实例一行，每个类别一列。因为测试集中有10,000个实例和10个类别，所以这是一个形状为[10000,
    10]的矩阵。我们堆叠了100个这样的矩阵，所以`y_probas`是一个形状为[100, 10000, 10]的3D数组。一旦我们在第一个维度上取平均值（`axis=0`），我们得到`y_proba`，一个形状为[10000,
    10]的数组，就像我们在单次预测中得到的一样。就是这样！在打开dropout的情况下对多次预测取平均值会给我们一个通常比关闭dropout的单次预测结果更可靠的蒙特卡洛估计。例如，让我们看看模型对Fashion
    MNIST测试集中第一个实例的预测，关闭dropout：
- en: '[PRE30]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The model is fairly confident (84.4%) that this image belongs to class 9 (ankle
    boot). Compare this with the MC dropout prediction:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 模型相当自信（84.4%）这张图片属于第9类（踝靴）。与MC dropout预测进行比较：
- en: '[PRE31]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The model still seems to prefer class 9, but its confidence dropped down to
    72.3%, and the estimated probabilities for classes 5 (sandal) and 7 (sneaker)
    have increased, which makes sense given they’re also footwear.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 模型似乎仍然更喜欢类别9，但其置信度降至72.3%，类别5（凉鞋）和7（运动鞋）的估计概率增加，这是有道理的，因为它们也是鞋类。
- en: 'MC dropout tends to improve the reliability of the model’s probability estimates.
    This means that it’s less likely to be confident but wrong, which can be dangerous:
    just imagine a self-driving car confidently ignoring a stop sign. It’s also useful
    to know exactly which other classes are most likely. Additionally, you can take
    a look at the [standard deviation of the probability estimates](https://xkcd.com/2110):'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: MC dropout倾向于提高模型概率估计的可靠性。这意味着它不太可能自信但错误，这可能是危险的：想象一下一个自动驾驶汽车自信地忽略一个停车标志。了解哪些其他类别最有可能也很有用。此外，您可以查看[概率估计的标准差](https://xkcd.com/2110)：
- en: '[PRE32]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Apparently there’s quite a lot of variance in the probability estimates for
    class 9: the standard deviation is 0.183, which should be compared to the estimated
    probability of 0.723: if you were building a risk-sensitive system (e.g., a medical
    or financial system), you would probably treat such an uncertain prediction with
    extreme caution. You would definitely not treat it like an 84.4% confident prediction.
    The model’s accuracy also got a (very) small boost from 87.0% to 87.2%:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，类别9的概率估计存在相当大的方差：标准差为0.183，应与估计的概率0.723进行比较：如果您正在构建一个风险敏感的系统（例如医疗或金融系统），您可能会对这种不确定的预测极为谨慎。您绝对不会将其视为84.4%的自信预测。模型的准确性也从87.0%略微提高到87.2%：
- en: '[PRE33]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-330
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The number of Monte Carlo samples you use (100 in this example) is a hyperparameter
    you can tweak. The higher it is, the more accurate the predictions and their uncertainty
    estimates will be. However, if you double it, inference time will also be doubled.
    Moreover, above a certain number of samples, you will notice little improvement.
    Your job is to find the right trade-off between latency and accuracy, depending
    on your application.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 您使用的蒙特卡洛样本数量（在此示例中为100）是一个可以调整的超参数。它越高，预测和不确定性估计就越准确。但是，如果您将其加倍，推断时间也将加倍。此外，在一定数量的样本之上，您将注意到改进很小。您的任务是根据您的应用程序找到延迟和准确性之间的正确权衡。
- en: If your model contains other layers that behave in a special way during training
    (such as `BatchNormalization` layers), then you should not force training mode
    like we just did. Instead, you should replace the `Dropout` layers with the following
    `MCDropout` class:⁠^([30](ch11.html#idm45720196529216))
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型包含在训练期间以特殊方式行为的其他层（例如`BatchNormalization`层），那么您不应该像我们刚刚做的那样强制训练模式。相反，您应该用以下`MCDropout`类替换`Dropout`层：⁠^([30](ch11.html#idm45720196529216))
- en: '[PRE34]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here, we just subclass the `Dropout` layer and override the `call()` method
    to force its `training` argument to `True` (see [Chapter 12](ch12.html#tensorflow_chapter)).
    Similarly, you could define an `MCAlphaDropout` class by subclassing `AlphaDropout`
    instead. If you are creating a model from scratch, it’s just a matter of using
    `MCDropout` rather than `Dropout`. But if you have a model that was already trained
    using `Dropout`, you need to create a new model that’s identical to the existing
    model except with `Dropout` instead of `MCDropout`, then copy the existing model’s
    weights to your new model.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是子类化`Dropout`层，并覆盖`call()`方法以强制其`training`参数为`True`（请参阅[第12章](ch12.html#tensorflow_chapter)）。类似地，您可以通过子类化`AlphaDropout`来定义一个`MCAlphaDropout`类。如果您从头开始创建一个模型，只需使用`MCDropout`而不是`Dropout`。但是，如果您已经使用`Dropout`训练了一个模型，您需要创建一个与现有模型相同但使用`Dropout`而不是`MCDropout`的新模型，然后将现有模型的权重复制到新模型中。
- en: In short, MC dropout is a great technique that boosts dropout models and provides
    better uncertainty estimates. And of course, since it is just regular dropout
    during training, it also acts like a regularizer.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，MC dropout是一种很棒的技术，可以提升dropout模型并提供更好的不确定性估计。当然，由于在训练期间只是常规的dropout，因此它也起到了正则化的作用。
- en: Max-Norm Regularization
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大范数正则化
- en: 'Another popular regularization technique for neural networks is called *max-norm
    regularization*: for each neuron, it constrains the weights **w** of the incoming
    connections such that ∥ **w** ∥[2] ≤ *r*, where *r* is the max-norm hyperparameter
    and ∥ · ∥[2] is the ℓ[2] norm.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的另一种流行的正则化技术称为*最大范数正则化*：对于每个神经元，它约束传入连接的权重**w**，使得∥ **w** ∥[2] ≤ *r*，其中*r*是最大范数超参数，∥
    · ∥[2]是ℓ[2]范数。
- en: Max-norm regularization does not add a regularization loss term to the overall
    loss function. Instead, it is typically implemented by computing ∥ **w** ∥[2]
    after each training step and rescaling **w** if needed (**w** ← **w** *r* / ∥ **w** ∥[2]).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 最大范数正则化不会向整体损失函数添加正则化损失项。相反，通常是在每个训练步骤之后计算∥ **w** ∥[2]，并在需要时重新缩放**w**（**w**
    ← **w** *r* / ∥ **w** ∥[2]）。
- en: Reducing *r* increases the amount of regularization and helps reduce overfitting.
    Max-norm regularization can also help alleviate the unstable gradients problems
    (if you are not using batch normalization).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 减小*r*会增加正则化的程度，并有助于减少过拟合。最大范数正则化还可以帮助缓解不稳定的梯度问题（如果您没有使用批量归一化）。
- en: 'To implement max-norm regularization in Keras, set the `kernel_constraint`
    argument of each hidden layer to a `max_norm()` constraint with the appropriate
    max value, like this:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中实现最大范数正则化，将每个隐藏层的`kernel_constraint`参数设置为具有适当最大值的`max_norm()`约束，如下所示：
- en: '[PRE35]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: After each training iteration, the model’s `fit()` method will call the object
    returned by `max_norm()`, passing it the layer’s weights and getting rescaled
    weights in return, which then replace the layer’s weights. As you’ll see in [Chapter 12](ch12.html#tensorflow_chapter),
    you can define your own custom constraint function if necessary and use it as
    the `kernel_​con⁠straint`. You can also constrain the bias terms by setting the
    `bias_constraint` argument.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练迭代之后，模型的`fit()`方法将调用`max_norm()`返回的对象，将该层的权重传递给它，并得到重新缩放的权重，然后替换该层的权重。正如您将在[第12章](ch12.html#tensorflow_chapter)中看到的，如果需要，您可以定义自己的自定义约束函数，并将其用作`kernel_constraint`。您还可以通过设置`bias_constraint`参数来约束偏置项。
- en: The `max_norm()` function has an `axis` argument that defaults to `0`. A `Dense`
    layer usually has weights of shape [*number of inputs*, *number of neurons*],
    so using `axis=0` means that the max-norm constraint will apply independently
    to each neuron’s weight vector. If you want to use max-norm with convolutional
    layers (see [Chapter 14](ch14.html#cnn_chapter)), make sure to set the `max_norm()`
    constraint’s `axis` argument appropriately (usually `axis=[0, 1, 2]`).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_norm()`函数有一个默认为`0`的`axis`参数。一个`Dense`层通常具有形状为[*输入数量*，*神经元数量*]的权重，因此使用`axis=0`意味着最大范数约束将独立应用于每个神经元的权重向量。如果您想在卷积层中使用最大范数（参见[第14章](ch14.html#cnn_chapter)），请确保适当设置`max_norm()`约束的`axis`参数（通常为`axis=[0,
    1, 2]`）。'
- en: Summary and Practical Guidelines
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和实用指南
- en: In this chapter we have covered a wide range of techniques, and you may be wondering
    which ones you should use. This depends on the task, and there is no clear consensus
    yet, but I have found the configuration in [Table 11-3](#default_deep_neural_network_config)
    to work fine in most cases, without requiring much hyperparameter tuning. That
    said, please do not consider these defaults as hard rules!
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了各种技术，您可能想知道应该使用哪些技术。这取决于任务，目前还没有明确的共识，但我发现[表11-3](#default_deep_neural_network_config)中的配置在大多数情况下都能很好地工作，而不需要太多的超参数调整。尽管如此，请不要将这些默认值视为硬性规则！
- en: Table 11-3\. Default DNN configuration
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-3. 默认DNN配置
- en: '| Hyperparameter | Default value |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 默认值 |'
- en: '| --- | --- |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Kernel initializer | He initialization |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 内核初始化器 | He初始化 |'
- en: '| Activation function | ReLU if shallow; Swish if deep |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | 如果是浅层则为ReLU；如果是深层则为Swish |'
- en: '| Normalization | None if shallow; batch norm if deep |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 归一化 | 如果是浅层则为无；如果是深层则为批量归一化 |'
- en: '| Regularization | Early stopping; weight decay if needed |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 正则化 | 提前停止；如果需要则使用权重衰减 |'
- en: '| Optimizer | Nesterov accelerated gradients or AdamW |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | Nesterov加速梯度或AdamW |'
- en: '| Learning rate schedule | Performance scheduling or 1cycle |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 学习率调度 | 性能调度或1cycle |'
- en: If the network is a simple stack of dense layers, then it can self-normalize,
    and you should use the configuration in [Table 11-4](#self_norm_deep_neural_network_config)
    instead.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络是简单的密集层堆叠，则它可以自我归一化，您应该使用[表11-4](#self_norm_deep_neural_network_config)中的配置。
- en: Table 11-4\. DNN configuration for a self-normalizing net
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-4. 自我归一化网络的DNN配置
- en: '| Hyperparameter | Default value |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 默认值 |'
- en: '| --- | --- |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Kernel initializer | LeCun initialization |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 内核初始化器 | LeCun初始化 |'
- en: '| Activation function | SELU |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | SELU |'
- en: '| Normalization | None (self-normalization) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 归一化 | 无（自我归一化） |'
- en: '| Regularization | Alpha dropout if needed |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 正则化 | 如果需要则使用Alpha dropout |'
- en: '| Optimizer | Nesterov accelerated gradients |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | Nesterov加速梯度 |'
- en: '| Learning rate schedule | Performance scheduling or 1cycle |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 学习率调度 | 性能调度或1cycle |'
- en: Don’t forget to normalize the input features! You should also try to reuse parts
    of a pretrained neural network if you can find one that solves a similar problem,
    or use unsupervised pretraining if you have a lot of unlabeled data, or use pretraining
    on an auxiliary task if you have a lot of labeled data for a similar task.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记对输入特征进行归一化！您还应尝试重用预训练神经网络的部分，如果您可以找到一个解决类似问题的模型，或者如果您有大量未标记数据，则使用无监督预训练，或者如果您有大量类似任务的标记数据，则使用辅助任务的预训练。
- en: 'While the previous guidelines should cover most cases, here are some exceptions:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的指南应该涵盖了大多数情况，但这里有一些例外情况：
- en: If you need a sparse model, you can use ℓ[1] regularization (and optionally
    zero out the tiny weights after training). If you need an even sparser model,
    you can use the TensorFlow Model Optimization Toolkit. This will break self-normalization,
    so you should use the default configuration in this case.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您需要一个稀疏模型，您可以使用ℓ[1]正则化（并在训练后选择性地将微小权重归零）。如果您需要一个更稀疏的模型，您可以使用TensorFlow模型优化工具包。这将破坏自我归一化，因此在这种情况下应使用默认配置。
- en: If you need a low-latency model (one that performs lightning-fast predictions),
    you may need to use fewer layers, use a fast activation function such as ReLU
    or leaky ReLU, and fold the batch normalization layers into the previous layers
    after training. Having a sparse model will also help. Finally, you may want to
    reduce the float precision from 32 bits to 16 or even 8 bits (see [“Deploying
    a Model to a Mobile or Embedded Device”](ch19.html#deployingModelMobileEmbedded)).
    Again, check out TF-MOT.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您需要一个低延迟模型（执行闪电般快速预测的模型），您可能需要使用更少的层，使用快速激活函数（如ReLU或leaky ReLU），并在训练后将批量归一化层折叠到前面的层中。拥有一个稀疏模型也会有所帮助。最后，您可能希望将浮点精度从32位减少到16位甚至8位（参见[“将模型部署到移动设备或嵌入式设备”](ch19.html#deployingModelMobileEmbedded)）。再次，查看TF-MOT。
- en: If you are building a risk-sensitive application, or inference latency is not
    very important in your application, you can use MC dropout to boost performance
    and get more reliable probability estimates, along with uncertainty estimates.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您正在构建一个风险敏感的应用程序，或者推断延迟在您的应用程序中并不是非常重要，您可以使用MC dropout来提高性能，并获得更可靠的概率估计，以及不确定性估计。
- en: With these guidelines, you are now ready to train very deep nets! I hope you
    are now convinced that you can go quite a long way using just the convenient Keras
    API. There may come a time, however, when you need to have even more control;
    for example, to write a custom loss function or to tweak the training algorithm.
    For such cases you will need to use TensorFlow’s lower-level API, as you will
    see in the next chapter.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the problem that Glorot initialization and He initialization aim to
    fix?
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it OK to initialize all the weights to the same value as long as that value
    is selected randomly using He initialization?
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it OK to initialize the bias terms to 0?
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In which cases would you want to use each of the activation functions we discussed
    in this chapter?
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What may happen if you set the `momentum` hyperparameter too close to 1 (e.g.,
    0.99999) when using an `SGD` optimizer?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name three ways you can produce a sparse model.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does dropout slow down training? Does it slow down inference (i.e., making predictions
    on new instances)? What about MC dropout?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Practice training a deep neural network on the CIFAR10 image dataset:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but
    it’s the point of this exercise). Use He initialization and the Swish activation
    function.
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Nadam optimization and early stopping, train the network on the CIFAR10
    dataset. You can load it with `tf.keras.datasets.cifar10.load_​data()`. The dataset
    is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000
    for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.
    Remember to search for the right learning rate each time you change the model’s
    architecture or hyperparameters.
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now try adding batch normalization and compare the learning curves: is it converging
    faster than before? Does it produce a better model? How does it affect training
    speed?'
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Try replacing batch normalization with SELU, and make the necessary adjustments
    to ensure the network self-normalizes (i.e., standardize the input features, use
    LeCun normal initialization, make sure the DNN contains only a sequence of dense
    layers, etc.).
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Try regularizing the model with alpha dropout. Then, without retraining your
    model, see if you can achieve better accuracy using MC dropout.
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrain your model using 1cycle scheduling and see if it improves training speed
    and model accuracy.
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch11.html#idm45720199815184-marker)) Xavier Glorot and Yoshua Bengio,
    “Understanding the Difficulty of Training Deep Feedforward Neural Networks”, *Proceedings
    of the 13th International Conference on Artificial Intelligence and Statistics*
    (2010): 249–256.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch11.html#idm45720199802080-marker)) Here’s an analogy: if you set a
    microphone amplifier’s knob too close to zero, people won’t hear your voice, but
    if you set it too close to the max, your voice will be saturated and people won’t
    understand what you are saying. Now imagine a chain of such amplifiers: they all
    need to be set properly in order for your voice to come out loud and clear at
    the end of the chain. Your voice has to come out of each amplifier at the same
    amplitude as it came in.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch11.html#idm45720199777104-marker)) E.g., Kaiming He et al., “Delving
    Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,”
    *Proceedings of the 2015 IEEE International Conference on Computer Vision* (2015):
    1026–1034.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch11.html#idm45720199613584-marker)) A dead neuron may come back to life
    if its inputs evolve over time and eventually return within a range where the
    ReLU activation function gets a positive input again. For example, this may happen
    if gradient descent tweaks the neurons in the layers below the dead neuron.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch11.html#idm45720199596240-marker)) Bing Xu et al., “Empirical Evaluation
    of Rectified Activations in Convolutional Network,” arXiv preprint arXiv:1505.00853
    (2015).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch11.html#idm45720199427712-marker)) Djork-Arné Clevert et al., “Fast
    and Accurate Deep Network Learning by Exponential Linear Units (ELUs),” *Proceedings
    of the International Conference on Learning Representations*, arXiv preprint (2015).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch11.html#idm45720199386128-marker)) Günter Klambauer et al., “Self-Normalizing
    Neural Networks”, *Proceedings of the 31st International Conference on Neural
    Information Processing Systems* (2017): 972–981.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch11.html#idm45720199369856-marker)) Dan Hendrycks and Kevin Gimpel, “Gaussian
    Error Linear Units (GELUs)”, arXiv preprint arXiv:1606.08415 (2016).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch11.html#idm45720199356368-marker)) A function is convex if the line
    segment between any two points on the curve never lies below the curve. A monotonic
    function only increases, or only decreases.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch11.html#idm45720199347968-marker)) Prajit Ramachandran et al., “Searching
    for Activation Functions”, arXiv preprint arXiv:1710.05941 (2017).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch11.html#idm45720199337776-marker)) Diganta Misra, “Mish: A Self Regularized
    Non-Monotonic Activation Function”, arXiv preprint arXiv:1908.08681 (2019).'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch11.html#idm45720199311872-marker)) Sergey Ioffe and Christian Szegedy,
    “Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift”, *Proceedings of the 32nd International Conference on Machine
    Learning* (2015): 448–456.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch11.html#idm45720198989872-marker)) However, they are estimated during
    training based on the training data, so arguably they *are* trainable. In Keras,
    “non-trainable” really means “untouched by backpropagation”.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '^([14](ch11.html#idm45720198748720-marker)) Razvan Pascanu et al., “On the
    Difficulty of Training Recurrent Neural Networks”, *Proceedings of the 30th International
    Conference on Machine Learning* (2013): 1310–1318.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '^([15](ch11.html#idm45720198261376-marker)) Boris T. Polyak, “Some Methods
    of Speeding Up the Convergence of Iteration Methods”, *USSR Computational Mathematics
    and Mathematical Physics* 4, no. 5 (1964): 1–17.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '^([16](ch11.html#idm45720198190768-marker)) Yurii Nesterov, “A Method for Unconstrained
    Convex Minimization Problem with the Rate of Convergence *O*(1/*k*²),” *Doklady
    AN USSR* 269 (1983): 543–547.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch11.html#idm45720198095888-marker)) John Duchi et al., “Adaptive Subgradient
    Methods for Online Learning and Stochastic Optimization”, *Journal of Machine
    Learning Research* 12 (2011): 2121–2159.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '^([18](ch11.html#idm45720198007744-marker)) This algorithm was created by Geoffrey
    Hinton and Tijmen Tieleman in 2012 and presented by Geoffrey Hinton in his Coursera
    class on neural networks (slides: [*https://homl.info/57*](https://homl.info/57);
    video: [*https://homl.info/58*](https://homl.info/58)). Amusingly, since the authors
    did not write a paper to describe the algorithm, researchers often cite “slide
    29 in lecture 6e” in their papers.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: ^([19](ch11.html#idm45720197966960-marker)) *ρ* is the Greek letter rho.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '^([20](ch11.html#idm45720197921776-marker)) Diederik P. Kingma and Jimmy Ba,
    “Adam: A Method for Stochastic Optimization”, arXiv preprint arXiv:1412.6980 (2014).'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch11.html#idm45720197757088-marker)) Timothy Dozat, “Incorporating Nesterov
    Momentum into Adam” (2016).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch11.html#idm45720197753840-marker)) Ilya Loshchilov, and Frank Hutter,
    “Decoupled Weight Decay Regularization”, arXiv preprint arXiv:1711.05101 (2017).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '^([23](ch11.html#idm45720197744832-marker)) Ashia C. Wilson et al., “The Marginal
    Value of Adaptive Gradient Methods in Machine Learning”, *Advances in Neural Information
    Processing Systems* 30 (2017): 4148–4158.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '^([24](ch11.html#idm45720197650432-marker)) Leslie N. Smith, “A Disciplined
    Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch Size,
    Momentum, and Weight Decay”, arXiv preprint arXiv:1803.09820 (2018).'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: Leslie N. Smith，“神经网络超参数的纪律性方法：第1部分—学习率、批量大小、动量和权重衰减”，arXiv预印本arXiv:1803.09820（2018）。
- en: '^([25](ch11.html#idm45720197641920-marker)) Andrew Senior et al., “An Empirical
    Study of Learning Rates in Deep Neural Networks for Speech Recognition”, *Proceedings
    of the IEEE International Conference on Acoustics, Speech, and Signal Processing*
    (2013): 6724–6728.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: Andrew Senior等人，“深度神经网络在语音识别中的学习率的实证研究”，*IEEE国际会议论文集*（2013）：6724–6728。
- en: ^([26](ch11.html#idm45720196986560-marker)) Geoffrey E. Hinton et al., “Improving
    Neural Networks by Preventing Co-Adaptation of Feature Detectors”, arXiv preprint
    arXiv:1207.0580 (2012).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: Geoffrey E. Hinton等人，“通过防止特征探测器的共适应来改进神经网络”，arXiv预印本arXiv:1207.0580（2012）。
- en: '^([27](ch11.html#idm45720196946560-marker)) Nitish Srivastava et al., “Dropout:
    A Simple Way to Prevent Neural Networks from Overfitting”, *Journal of Machine
    Learning Research* 15 (2014): 1929–1958.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: Nitish Srivastava等人，“Dropout：防止神经网络过拟合的简单方法”，*机器学习研究杂志* 15（2014）：1929–1958。
- en: '^([28](ch11.html#idm45720196754832-marker)) Yarin Gal and Zoubin Ghahramani,
    “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning”,
    *Proceedings of the 33rd International Conference on Machine Learning* (2016):
    1050–1059.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: Yarin Gal和Zoubin Ghahramani，“Dropout作为贝叶斯近似：在深度学习中表示模型不确定性”，*第33届国际机器学习会议论文集*（2016）：1050–1059。
- en: ^([29](ch11.html#idm45720196752128-marker)) Specifically, they show that training
    a dropout network is mathematically equivalent to approximate Bayesian inference
    in a specific type of probabilistic model called a *deep Gaussian process*.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，他们表明训练一个dropout网络在数学上等同于在一种特定类型的概率模型中进行近似贝叶斯推断，这种模型被称为*深高斯过程*。
- en: ^([30](ch11.html#idm45720196529216-marker)) This `MCDropout` class will work
    with all Keras APIs, including the sequential API. If you only care about the
    functional API or the subclassing API, you do not have to create an `MCDropout`
    class; you can create a regular `Dropout` layer and call it with `training=True`.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`MCDropout`类将与所有Keras API一起工作，包括顺序API。如果您只关心功能API或子类API，您不必创建一个`MCDropout`类；您可以创建一个常规的`Dropout`层，并使用`training=True`调用它。
