- en: Chapter 7\. Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we’ve introduced you to the use of deep learning to process
    various types of inputs. We started from simple linear and logistic regression
    on fixed dimensional feature vectors, and then followed up with a discussion of
    fully connected deep networks. These models take in arbitrary feature vectors
    with fixed, predetermined sizes. These models make no assumptions about the type
    of data encoded into these vectors. On the other hand, convolutional networks
    place strong assumptions upon the structure of their data. Inputs to convolutional
    networks have to satisfy a locality assumption that allows for the definition
    of a local receptive field.
  prefs: []
  type: TYPE_NORMAL
- en: How could we use the networks that we’ve described thus far to process data
    like sentences? Sentences do have some locality properties (nearby words are typically
    related), and it is indeed possible to use a one-dimensional convolutional network
    to process sentence data. That said, most practitioners resort to a different
    type of architecture, the recurrent neural network, in order to handle sequences
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks (RNNs) are designed natively to allow deep networks
    to process sequences of data. RNNs assume that incoming data takes the form of
    a sequence of vectors or tensors. If we transform each word in a sentence into
    a vector (more on how to do this later), sentences can be fed into RNNs. Similarly,
    video (considered as a sequence of images) can similarly be processed by an RNN.
    At each sequence position, an RNN applies an arbitrary nonlinear transformation
    to the input at that sequence location. This nonlinear transformation is shared
    for all sequence steps.
  prefs: []
  type: TYPE_NORMAL
- en: The description in the previous paragraph is a little abstract, but turns out
    to be immensely powerful. In this chapter, you will learn more details about how
    RNNs are structured and about how to implement RNNs in TensorFlow. We will also
    discuss how RNNs can be used in practice to perform tasks like sampling new sentences
    or generating text for applications such as chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: The case study for this chapter trains a recurrent neural network language model
    on the Penn Treebank corpus, a body of sentences extracted from *Wall Street Journal*
    articles. This tutorial was adapted from the TensorFlow official documentation
    tutorial on recurrent networks. (We encourage you to access the original tutorial
    on the TensorFlow website if you’re curious about the changes we’ve made.) As
    always, we recommend that you follow along with the code in the [GitHub repo associated
    with this book](https://github.com/matroid/dlwithtf).
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Recurrent Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent architectures are useful for modeling very complex time varying datasets.
    Time varying datasets are traditionally called *time-series*. [Figure 7-1](#ch7-timeseries)
    displays a number of time-series datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![time_series.png](assets/tfdl_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Some time-series datasets that we might be interested in modeling.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In time-series modeling, we design learning systems that are capable of learning
    the evolution rule that models how the future of the system at hand evolves depending
    on the past. Mathematically, let’s suppose that at each time step, we receive
    a datapoint <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    where *t* is the current time. Then, time-series methods seek to learn some function
    *f* such that
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>x</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that *f* encodes the underlying dynamics of the system well and
    learning it from data would enable a learning system to predict the future of
    the system at hand. In practice, it’s too unwieldy to learn a function that depends
    on all past inputs, so learning systems often assume that all information about
    last datapoints <math alttext="x 1 comma ellipsis comma x Subscript t minus 1
    Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
    can be encoded into some fixed vector <math alttext="h Subscript t"><msub><mi>h</mi>
    <mi>t</mi></msub></math> . Then, the update equation simplifies into the format
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>x</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>h</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>h</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we assume that the same function *f* here applies for all timesteps
    *t*. That is, we assume the time-series to be *stationary* ([Figure 7-2](#ch7-timeseriesmodel)).
    This assumption is broken for many systems, notably including the stock market
    where today’s rules need not hold tomorrow.
  prefs: []
  type: TYPE_NORMAL
- en: '![RNN-unrolled.png](assets/tfdl_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. A mathematical model of a time-series with a stationary evolution
    rule. Recall that a stationary system is one whose underlying dynamics don’t shift
    over time.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What does this equation have to do with recurrent neural nets? The basic answer
    derives from the universal approximation theorem that we introduced in [Chapter 4](ch04.html#fully_connected_networks).
    The function *f* can be arbitrarily complex, so using a fully connected deep network
    to learn *f* seems like a reasonable idea. This intuition essentially defines
    the RNN. A simple recurrent network can be viewed as a fully connected network
    that is applied repeatedly to each time step of the data.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, recurrent neural networks really become interesting only for complex
    high-dimensional time-series. For simpler systems, there are classical signal
    processing time-series methods that often do a good job of modeling time dynamics.
    However, for complex systems, such as speech (see the speech spectrogram in [Figure 7-3](#ch7-spect)),
    RNNs come into their own and offer capabilities that other methods can’t.
  prefs: []
  type: TYPE_NORMAL
- en: '![sepctrogram.GIF](assets/tfdl_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. A speech spectrogram representing the frequencies found in a speech
    sample.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recurrent Cells
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient Instability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent networks tend to degrade signal over time. Think of it as attenuating
    a signal by a multiplicative factor at each timestep. As a result, after 50 timesteps,
    the signal is quite degraded.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of this instability, it has been challenging to train recurrent
    neural networks on longer time-series. A number of methods have arisen to combat
    this instability, which we will discuss in the remainder of this section.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of elaborations on the concept of a simple recurrent neural
    network that have proven significantly more successful in practical applications.
    In this section, we will briefly review some of these variations.
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory (LSTM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Part of the challenge with the standard recurrent cell is that signals from
    the distant past attenuate rapidly. As a result, RNNs can fail to learn models
    of complex dependencies. This failure becomes particularly notable in applications
    such as language modeling, where words can have complex dependencies on earlier
    phrases.
  prefs: []
  type: TYPE_NORMAL
- en: One potential solution to this issue is to allow states from the past to pass
    through unmodified. The long short-term memory (LSTM) architecture proposes a
    mechanism to allow past state to pass through to the present with minimal modifications.
    Empirically using an LSTM “cell” (shown in [Figure 7-4](#ch7-lstm)) seems to offer
    superior learning performance when compared to simple recurrent neural networks
    using fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![colah_lstm.png](assets/tfdl_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. A long short-term memory (LSTM) cell. LSTMs perform better than
    standard recurrent neural networks at preserving long-range dependencies in inputs.
    As a result, LSTMs are often preferred for complex sequential data, such as natural
    language.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So Many Equations!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LSTM equations involve many sophisticated terms. If you are interested in
    understanding precisely the mathematical intuitions behind the LSTM, we encourage
    you to play with the equations with pencil and paper and trying to take derivatives
    of the LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: However, for other readers who are primarily interested in using recurrent architectures
    to solve practical problems, we believe it isn’t absolutely necessary to delve
    into the nitty-gritty details of how LSTMs work. Rather, keep the high-level intuition
    that past state is allowed to pass through, and work through the example code
    for this chapter in some depth.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Recurrent Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike fully connected networks or convolutional networks, LSTMs involve some
    sophisticated mathematical operations and control-flow operations. As a result,
    training large recurrent networks at scale has proven to be challenging, even
    with modern GPU hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Significant effort has been put into optimizing RNN implementations to run quickly
    on GPU hardware. In particular, Nvidia has incorporated RNNs into their CuDNN
    library that provides specially optimized code for training deep networks on GPUs.
    Luckily for TensorFlow users, integration with libraries such as CuDNN is performed
    within TensorFlow itself so you don’t need to worry too much about code optimization
    (unless of course, you’re working on very large-scale datasets). We will discuss
    hardware needs for deep neural networks at greater depth in [Chapter 9](ch09.html#training_large_deep_networks).
  prefs: []
  type: TYPE_NORMAL
- en: Gated Recurrent Units (GRU)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The complexity, both conceptual and computational, for LSTM cells has motivated
    a number of researchers to attempt to simplify the LSTM equations while retaining
    the performance gains and modeling capabilities of the original equations.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of contenders for LSTM replacement, but one of the frontrunners
    is the gated recurrent unit (GRU), shown in [Figure 7-5](#ch7-gru). The GRU removes
    one of the subcomponents of the LSTM but empirically seems to achieve performance
    similar to that of the LSTM. The GRU might be a suitable replacement for LSTM
    cells on sequence modeling projects.
  prefs: []
  type: TYPE_NORMAL
- en: '![lstm_gru.png](assets/tfdl_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. A gated recurrent unit (GRU) cell. GRUs preserve many of the benefits
    of LSTMs at lower computational cost.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Applications of Recurrent Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While recurrent neural networks are useful tools for modeling time-series datasets,
    there are a host of other applications of recurrent networks. These include applications
    such as natural language modeling, machine translation, chemical retrosynthesis,
    and arbitrary computation with Neural Turing machines. In this section, we provide
    a brief tour of some of these exciting applications.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling from Recurrent Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve taught you how recurrent networks can learn to model the time
    evolution of sequences of data. It stands to reason that if you understand the
    evolution rule for a set of sequences, you ought to be able to sample new sequences
    from the distribution of training sequences. And indeed, it turns out that that
    good sequences can be sampled from trained models. The most useful application
    thus far is in language modeling. Being able to generate realistic sentences is
    a very useful tool that underpins systems such as autocomplete and chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Why Don’t We Use GANs for Sequences?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 6](ch06.html#convolutional_neural_networks), we discussed the problem
    of generating new images. We discussed models such as variational autoencoders
    that produced only blurry images and introduced the technology of generative adversarial
    networks that proves capable of producing sharp images. The question remains,
    though: if we need GANs for good image samples, why don’t we use them for good
    sentences?'
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that today’s generative adversarial models are mediocre at sampling
    sequences. It’s not clear why this is the case. Theoretical understanding of GANs
    remains very weak (even by the standards of deep learning theory), but something
    about the game theoretic equilibrium discovery seems to perform worse for sequences
    than for images.
  prefs: []
  type: TYPE_NORMAL
- en: Seq2seq Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sequence-to-sequence (seq2seq) models are powerful tools that enable models
    to transform one sequence into another. The core idea of a sequence-to-sequence
    model is to use an encoding recurrent network that embeds input sequences into
    vector spaces alongside a decoding network that enables sampling of output sequences
    as described in previous sentences. [Figure 7-6](#ch7-seq2seq) illustrates a seq2seq
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '![seq2seq_colah.png](assets/tfdl_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Sequence-to-sequence models are powerful tools that can learn sequence
    transformations. They have been applied to machine translation (for example, transforming
    a sequence of English words to Mandarin) and chemical retrosynthesis (transforming
    a sequence of chemical products into a sequence of reactants).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Things get interesting since encoder and decoder layers can themselves be deep.
    (RNN layers can be stacked in a natural fashion.) The Google neural machine translation
    (GNMT) system has many stacked encoding and decoding layers. As a result of this
    powerful representational capacity, it is capable of performing state-of-the-art
    translations far beyond the capabilities of its nearest nondeep competitors. [Figure 7-7](#ch7-gnmt)
    illustrates the GNMT architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![google_nmt.png](assets/tfdl_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. The Google neural machine translation (GNMT) architecture is a
    deep seq2seq model that learns to perform machine translation.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While so far we’ve mainly discussed applications to natural language processing,
    the seq2seq architecture has myriad applications in other domains. One of the
    authors has used seq2seq architectures to perform chemical retrosynthesis, the
    act of deconstructing molecules into simpler constituents. [Figure 7-8](#ch7-seqret)
    illustrates.
  prefs: []
  type: TYPE_NORMAL
- en: '![seq2seq_retrosynthesis.png](assets/tfdl_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. A seq2seq model for chemical retrosynthesis transforms a sequence
    of chemical products into a sequence of chemical reactants.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Neural Turing Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dream of machine learning has been to move further up the abstraction stack:
    moving from learning short pattern-matching engines to learning to perform arbitrary
    computations. The Neural Turing machine is a powerful step in this evolution.'
  prefs: []
  type: TYPE_NORMAL
- en: The Turing machine was a seminal contribution to the mathematical theory of
    computation. It was the first mathematical model of a machine capable of performing
    any computation. The Turing machine maintains a “tape” that provides a memory
    of the performed computation. The second part of the machine is a “head” that
    performs transformations on single tape cells. The insight of the Turing machine
    was that the “head” didn’t need to be very complicated in order to perform arbitrarily
    complicated calculations.
  prefs: []
  type: TYPE_NORMAL
- en: The Neural Turing machine (NTM) is a very clever attempt to transmute a Turing
    machine itself into a neural network. The trick in this transmutation is to turn
    discrete actions into soft continuous functions (this is a trick that pops up
    in deep learning repeatedly, so take note!)
  prefs: []
  type: TYPE_NORMAL
- en: The Turing machine head is quite similar to the RNN cell! As a result, the NTM
    can be trained end-to-end to learn to perform arbitrary computations, in principle
    at least ([Figure 7-9](#ch7-neuraltm)). In practice, there are severe limitations
    to the set of computations that the NTM can perform. Gradient flow instabilities
    (as always) limit what can be learned. More research and experimentation will
    be needed to devise successors to NTMs capable of learning more useful functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![turing_machine.png](assets/tfdl_0709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. A Neural Turing machine (NTM) is a learnable version of a Turing
    machine. It maintains a tape where it can store the outputs of intermediate computations.
    While NTMs have many practical limitations, it’s possible that their intellectual
    descendants will be capable of learning powerful algorithms.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Turing Completeness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of Turing completeness is an important notion in computer science.
    A programming language is said to be Turing complete if it is capable of performing
    any computation that can be performed by a Turing machine. The Turing machine
    itself was invented to provide a mathematical model of what it means for a function
    to be “computable.” The machine provides the capability to read, write, and store
    in memory various instructions, abstract primitives that underlie all computing
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, a large body of work has shown that the Turing machine closely models
    the set of computations performable in the physical world. To a first approximation,
    if it can be shown that a Turing machine is incapable of performing a computation,
    no computing device is capable of it either. On the other side, if it can be shown
    that a computing system can perform the basic operations of a Turing machine,
    it is then “Turing complete” and capable of performing in principle any computation
    that can be performed at all. A number of surprising systems are Turing complete.
    We encourage you to read more about this topic if interested.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Networks Are Turing Complete
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps unsurprisingly, NTMs are capable of performing any computation a Turing
    machine can and are consequently Turing complete. However, a less known fact is
    that vanilla recurrent neural networks are themselves Turing complete! Put another
    way, in principle, a recurrent neural network is capable of learning to perform
    arbitrary computation.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is that the transition operator can learn to perform basic reading,
    writing, and storage operations. The unrolling of the recurrent network over time
    allows for the performance of complex computations. In some sense, this fact shouldn’t
    be too surprising. The universal approximation theorem already demonstrates that
    fully connected networks are capable of learning arbitrary functions. Chaining
    arbitrary functions together over time leads to arbitrary computations. (The technical
    details required to formally prove this are formidable, though.)
  prefs: []
  type: TYPE_NORMAL
- en: Working with Recurrent Neural Networks in Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn about the use of recurrent neural networks for
    language modeling on the Penn Treebank dataset, a natural language dataset built
    from *Wall Street Journal* articles. We will introduce the TensorFlow primitives
    needed to perform this modeling and will also walk you through the data handling
    and preprocessing steps needed to prepare data for training. We encourage you
    to follow along and try running the code in the [GitHub repo associated with the
    book](https://github.com/matroid/dlwithtf).
  prefs: []
  type: TYPE_NORMAL
- en: Processing the Penn Treebank Corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Penn Treebank contains a million-word corpus of *Wall Street Journal* articles.
    This corpus can be used for either character-level or word-level modeling (the
    tasks of predicting the next character or word in a sentence given those preceding).
    The efficacy of models is measured using the perplexity of trained models (more
    on this metric later).
  prefs: []
  type: TYPE_NORMAL
- en: The Penn Treebank corpus consists of sentences. How can we transform sentences
    into a form that can be fed to machine learning systems such as recurrent language
    models? Recall that machine learning models accept tensors (with recurrent models
    accepting sequences of tensors) as input. Consequently, we need to transform words
    into tensors for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest method of transforming words into vectors is to use “one-hot” encoding.
    In this encoding, let’s suppose that our language dataset uses a vocabulary that
    has <math><mrow><mo>|</mo> <mi>V</mi> <mo>|</mo></mrow></math> words. Then each
    word is transformed into a vector of shape <math><mrow><mo>(</mo> <mo>|</mo> <mi>V</mi>
    <mo>|</mo> <mo>)</mo></mrow></math> . All the entries of this vector are zero,
    except for one entry, at the index that corresponds to the current word. For an
    example of this embedding, see [Figure 7-10](#ch7-one-hot).
  prefs: []
  type: TYPE_NORMAL
- en: '![one-hot.jpg](assets/tfdl_0710.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. One-hot encodings transform words into vectors with only one nonzero
    entry (which is typically set to one). Different indices in the vector uniquely
    represent words in a language corpus.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s also possible to use more sophisticated embeddings. The basic idea is similar
    to that for the one-hot encoding. Each word is associated with a unique vector.
    However, the key difference is that it’s possible to learn this encoding vector
    directly from data to obtain a “word embedding” for the word in question that’s
    meaningful for the dataset at hand. We will show you how to learn word embeddings
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In order to process the Penn Treebank data, we need to find the vocabulary of
    words used in the corpus, then transform each word into its associated word vector.
    We will then show how to feed the processed data into a TensorFlow model.
  prefs: []
  type: TYPE_NORMAL
- en: Penn Treebank Limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Penn Treebank is a very useful dataset for language modeling, but it no
    longer poses a challenge for state-of-the-art language models; researchers have
    already overfit models on the peculiarities of this collection. State-of-the-art
    research would use larger datasets such as the billion-word-corpus language benchmark.
    However, for our exploratory purposes, the Penn Treebank easily suffices.
  prefs: []
  type: TYPE_NORMAL
- en: Code for Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The snippet of code in [Example 7-1](#ch7-readwords) reads in the raw files
    associated with the Penn Treebank corpus. The corpus is stored with one sentence
    per line. Some Python string handling is done to replace `"\n"` newline markers
    with fixed-token `"<eos>"` and then split the file into a list of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-1\. This function reads in the raw Penn Treebank file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With `_read_words` defined, we can build the vocabulary associated with a given
    file using function `_build_vocab` defined in [Example 7-2](#ch7-vocab). We simply
    read in the words in the file, and count the number of unique words in the file
    using Python’s `collections` library. For convenience, we construct a dictionary
    object mapping words to their unique integer identifiers (their positions in the
    vocabulary). Tying it all together, `_file_to_word_ids` transforms a file into
    a list of word identifiers ([Example 7-3](#ch7-file-to-words)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-2\. This function builds a vocabulary consisting of all words in the
    specified file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Example 7-3\. This function transforms words in a file into id numbers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With these utilities in place, we can process the Penn Treebank corpus with
    function `ptb_raw_data` ([Example 7-4](#ch7-ptb-raw)). Note that training, validation,
    and test datasets are pre-specified, so we need only read each file into a list
    of unique indices.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-4\. This function loads the Penn Treebank data from the specified
    location
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: tf.GFile and tf.Flags
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is a large project that contains many bits and pieces. While most
    of the library is devoted to machine learning, there’s also a large proportion
    that’s dedicated to loading and massaging data. Some of these functions provide
    useful capabilities that aren’t found elsewhere. Other parts of the loading functionality
    are less useful, however.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.GFile` and `tf.FLags` provide functionality that is more or less identical
    to standard Python file handling and `argparse`. The provenance of these tools
    is historical. With Google, custom file handlers and flag handling are required
    by internal code standards. For the rest of us, though, it’s better style to use
    standard Python tools whenever possible. It’s much better for readability and
    stability.'
  prefs: []
  type: TYPE_NORMAL
- en: Loading Data into TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we cover the code needed to load our processed indices into
    TensorFlow. To do so, we will introduce you to a new bit of TensorFlow machinery.
    Until now, we’ve used feed dictionaries to pass data into TensorFlow. While feed
    dictionaries are fine for small toy datasets, they are often not good choices
    for larger datasets, since large Python overheads involving packing and unpacking
    dictionaries are introduced. For more performant code, it’s better to use TensorFlow
    queues.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Queue` provides a way to load data asynchronously. This allows decoupling
    of the GPU compute thread from the CPU-bound data preprocessing thread. This decoupling
    is particularly useful for large datasets where we want to keep the GPU maximally
    active.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible to feed `tf.Queue` objects into TensorFlow placeholders to train
    models and achieve greater performance. We will demonstrate how to do so later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The function `ptb_producer` introduced in [Example 7-5](#ch7-ptb-producer) transforms
    raw lists of indices into `tf.Queues` that can pass data into a TensorFlow computational
    graph. Let’s start by introducing some of the computational primitives we use.
    `tf.train.range_input_producer` is a convenience operation that produces a `tf.Queue`
    from an input tensor. The method `tf.Queue.dequeue()` pulls a tensor from the
    queue for training. `tf.strided_slice` extracts the part of this tensor that corresponds
    to the data for the current minibatch.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-5\. This function loads the Penn Treebank data from the specified
    location
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: tf.data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow (from version 1.4 onward) supports a new module `tf.data` with a
    new class `tf.data.Dataset` that provides an explicit API for representing streams
    of data. It’s likely that `tf.data` will eventually supersede queues as the preferred
    input modality, especially since it has a well-thought-out functional API.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the `tf.data` module was just released and remained
    relatively immature compared with other parts of the API, so we decided to stick
    with queues for the examples. However, we encourage you to learn about `tf.data`
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Recurrent Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use an LSTM cell for modeling the Penn Treebank, since LSTMs often offer
    superior performance for language modeling challenges. The function `tf.contrib.rnn.BasicLSTMCell`
    implements the basic LSTM cell for us already, so no need to implement it ourselves
    ([Example 7-6](#ch7-lstm-wrap)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-6\. This function wraps an LSTM cell from tf.contrib
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Is Using TensorFlow Contrib Code OK?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that the LSTM implementation we use is drawn from `tf.contrib`. Is it acceptable
    to use code from `tf.contrib` for industrial-strength projects? The jury still
    appears to be out on this one. From our personal experience, code in `tf.contrib`
    tends to be a bit shakier than code in the core TensorFlow library, but is usually
    still pretty solid. There are often many useful libraries and utilities that are
    only available as part of `tf.contrib`. Our recommendation is to use pieces from
    `tf.contrib` as necessary, but make note of the pieces you use and replace them
    if an equivalent in the core TensorFlow library becomes available.
  prefs: []
  type: TYPE_NORMAL
- en: The snippet in [Example 7-7](#ch7-embed) instructs TensorFlow to learn a word
    embedding for each word in our vocabulary. The key function for us is `tf.nn.embedding_lookup`,
    which allows us to perform the correct tensorial lookup operation. Note that we
    need to manually define the embedding matrix as a TensorFlow variable.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-7\. Learn a word embedding for each word in the vocabulary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'With our word vectors in hand, we simply need to apply the LSTM cell (using
    function `lstm_cell`) to each word vector in our sequence. To do this, we simply
    use a Python `for`-loop to construct the needed set of calls to `cell()`. There’s
    only one trick here: we need to make sure we reuse the same variables at each
    timestep, since the LSTM cell should perform the same operation at each timestep.
    Luckily, the method `reuse_variables()` for variable scopes allows us to do so
    without much effort. See [Example 7-8](#ch7-embed2).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-8\. Apply LSTM cell to each word vector in input sequence
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: All that remains now is to define the loss associated with the graph in order
    to train it. Conveniently, TensorFlow offers a loss for training language models
    in `tf.contrib`. We need only make a call to `tf.contrib.seq2seq.sequence_loss`
    ([Example 7-9](#ch7-seqloss)). Underneath the hood, this loss turns out to be
    a form of perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-9\. Add the sequence loss
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Perplexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perplexity is often used for language modeling challenges. It is a variant of
    the binary cross-entropy that is useful for measuring how close the learned distribution
    is to the true distribution of data. Empirically, perplexity has proven useful
    for many language modeling challenges and we make use of it here in that capacity
    (since the `sequence_loss` just implements perplexity specialized to sequences
    inside).
  prefs: []
  type: TYPE_NORMAL
- en: We can then train this graph using a standard gradient descent method. We leave
    out some of the messy details of the underlying code, but suggest you check GitHub
    if curious. Evaluating the quality of the trained model turns out to be straightforward
    as well, since the perplexity is used both as the training loss and the evaluation
    metric. As a result, we can simply display `self._cost` to gauge how the model
    is training. We encourage you to train the model for yourself!
  prefs: []
  type: TYPE_NORMAL
- en: Challenge for the Reader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Try lowering perplexity on the Penn Treebank by experimenting with different
    model architectures. Note that these experiments might be time-consuming without
    a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced you to recurrent neural networks (RNNs), a powerful
    architecture for learning on sequential data. RNNs are capable of learning the
    underlying evolution rule that governs a sequence of data. While RNNs can be used
    for modeling simple time-series, they are most powerful when modeling complex
    sequential data such as speech and natural language.
  prefs: []
  type: TYPE_NORMAL
- en: We introduced you to a number of RNN variants such as LSTMs and GRUs, which
    perform better on data with complex long-range interactions, and also took a brief
    detour to discuss the exciting prospect of Neural Turing machines. We ended the
    chapter with an in-depth case study that applied LSTMs to model the Penn Treebank.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](ch08.html#reinforcement_learning), we will introduce you to reinforcement
    learning, a powerful technique for learning to play games.
  prefs: []
  type: TYPE_NORMAL
