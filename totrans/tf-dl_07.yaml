- en: Chapter 7\. Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。循环神经网络
- en: So far in this book, we’ve introduced you to the use of deep learning to process
    various types of inputs. We started from simple linear and logistic regression
    on fixed dimensional feature vectors, and then followed up with a discussion of
    fully connected deep networks. These models take in arbitrary feature vectors
    with fixed, predetermined sizes. These models make no assumptions about the type
    of data encoded into these vectors. On the other hand, convolutional networks
    place strong assumptions upon the structure of their data. Inputs to convolutional
    networks have to satisfy a locality assumption that allows for the definition
    of a local receptive field.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们向您介绍了使用深度学习处理各种类型输入的方法。我们从简单的线性和逻辑回归开始，这些回归是在固定维度的特征向量上进行的，然后讨论了全连接的深度网络。这些模型接受任意大小的特征向量，这些向量的大小是固定的，预先确定的。这些模型不对编码到这些向量中的数据类型做任何假设。另一方面，卷积网络对其数据的结构做出了强烈的假设。卷积网络的输入必须满足一个允许定义局部感受野的局部性假设。
- en: How could we use the networks that we’ve described thus far to process data
    like sentences? Sentences do have some locality properties (nearby words are typically
    related), and it is indeed possible to use a one-dimensional convolutional network
    to process sentence data. That said, most practitioners resort to a different
    type of architecture, the recurrent neural network, in order to handle sequences
    of data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们如何使用我们描述的网络来处理句子等数据？句子确实具有一些局部性质（附近的单词通常相关），因此确实可以使用一维卷积网络来处理句子数据。尽管如此，大多数从业者倾向于使用不同类型的架构，即循环神经网络，以处理数据序列。
- en: Recurrent neural networks (RNNs) are designed natively to allow deep networks
    to process sequences of data. RNNs assume that incoming data takes the form of
    a sequence of vectors or tensors. If we transform each word in a sentence into
    a vector (more on how to do this later), sentences can be fed into RNNs. Similarly,
    video (considered as a sequence of images) can similarly be processed by an RNN.
    At each sequence position, an RNN applies an arbitrary nonlinear transformation
    to the input at that sequence location. This nonlinear transformation is shared
    for all sequence steps.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）被设计成允许深度网络处理数据序列。RNNs假设传入的数据采用向量或张量序列的形式。如果我们将句子中的每个单词转换为向量（稍后会详细介绍如何做到这一点），那么句子可以被馈送到RNN中。同样，视频（被视为图像序列）也可以通过RNN进行处理。在每个序列位置，RNN对该序列位置的输入应用任意非线性转换。这种非线性转换对所有序列步骤都是共享的。
- en: The description in the previous paragraph is a little abstract, but turns out
    to be immensely powerful. In this chapter, you will learn more details about how
    RNNs are structured and about how to implement RNNs in TensorFlow. We will also
    discuss how RNNs can be used in practice to perform tasks like sampling new sentences
    or generating text for applications such as chatbots.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 前面段落中的描述有点抽象，但事实证明它非常强大。在本章中，您将了解有关RNN结构的更多细节，以及如何在TensorFlow中实现RNN。我们还将讨论RNN如何在实践中用于执行诸如抽样新句子或为诸如聊天机器人之类的应用生成文本的任务。
- en: The case study for this chapter trains a recurrent neural network language model
    on the Penn Treebank corpus, a body of sentences extracted from *Wall Street Journal*
    articles. This tutorial was adapted from the TensorFlow official documentation
    tutorial on recurrent networks. (We encourage you to access the original tutorial
    on the TensorFlow website if you’re curious about the changes we’ve made.) As
    always, we recommend that you follow along with the code in the [GitHub repo associated
    with this book](https://github.com/matroid/dlwithtf).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的案例研究在Penn Treebank语料库上训练了一个循环神经网络语言模型，这是从《华尔街日报》文章中提取的句子集合。本教程改编自TensorFlow官方文档关于循环网络的教程。（如果您对我们所做的更改感兴趣，我们鼓励您访问TensorFlow网站上的原始教程。）与往常一样，我们建议您跟着本书相关的[GitHub存储库](https://github.com/matroid/dlwithtf)中的代码进行学习。
- en: Overview of Recurrent Architectures
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环架构概述
- en: Recurrent architectures are useful for modeling very complex time varying datasets.
    Time varying datasets are traditionally called *time-series*. [Figure 7-1](#ch7-timeseries)
    displays a number of time-series datasets.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 循环架构对建模非常复杂的时变数据集非常有用。时变数据集传统上称为*时间序列*。[图7-1](#ch7-timeseries)显示了一些时间序列数据集。
- en: '![time_series.png](assets/tfdl_0701.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![time_series.png](assets/tfdl_0701.png)'
- en: Figure 7-1\. Some time-series datasets that we might be interested in modeling.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1。我们可能感兴趣建模的一些时间序列数据集。
- en: In time-series modeling, we design learning systems that are capable of learning
    the evolution rule that models how the future of the system at hand evolves depending
    on the past. Mathematically, let’s suppose that at each time step, we receive
    a datapoint <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    where *t* is the current time. Then, time-series methods seek to learn some function
    *f* such that
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列建模中，我们设计学习系统，该系统能够学习演化规则，即根据过去学习系统的未来如何演变。从数学上讲，假设在每个时间步骤，我们接收到一个数据点<math
    alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>，其中*t*是当前时间。然后，时间序列方法试图学习某个函数*f*，使得
- en: <math display="block"><mrow><msub><mi>x</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>x</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: The idea is that *f* encodes the underlying dynamics of the system well and
    learning it from data would enable a learning system to predict the future of
    the system at hand. In practice, it’s too unwieldy to learn a function that depends
    on all past inputs, so learning systems often assume that all information about
    last datapoints <math alttext="x 1 comma ellipsis comma x Subscript t minus 1
    Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
    can be encoded into some fixed vector <math alttext="h Subscript t"><msub><mi>h</mi>
    <mi>t</mi></msub></math> . Then, the update equation simplifies into the format
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>x</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>h</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>h</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we assume that the same function *f* here applies for all timesteps
    *t*. That is, we assume the time-series to be *stationary* ([Figure 7-2](#ch7-timeseriesmodel)).
    This assumption is broken for many systems, notably including the stock market
    where today’s rules need not hold tomorrow.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![RNN-unrolled.png](assets/tfdl_0702.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. A mathematical model of a time-series with a stationary evolution
    rule. Recall that a stationary system is one whose underlying dynamics don’t shift
    over time.
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What does this equation have to do with recurrent neural nets? The basic answer
    derives from the universal approximation theorem that we introduced in [Chapter 4](ch04.html#fully_connected_networks).
    The function *f* can be arbitrarily complex, so using a fully connected deep network
    to learn *f* seems like a reasonable idea. This intuition essentially defines
    the RNN. A simple recurrent network can be viewed as a fully connected network
    that is applied repeatedly to each time step of the data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: In fact, recurrent neural networks really become interesting only for complex
    high-dimensional time-series. For simpler systems, there are classical signal
    processing time-series methods that often do a good job of modeling time dynamics.
    However, for complex systems, such as speech (see the speech spectrogram in [Figure 7-3](#ch7-spect)),
    RNNs come into their own and offer capabilities that other methods can’t.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![sepctrogram.GIF](assets/tfdl_0703.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. A speech spectrogram representing the frequencies found in a speech
    sample.
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recurrent Cells
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient Instability
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent networks tend to degrade signal over time. Think of it as attenuating
    a signal by a multiplicative factor at each timestep. As a result, after 50 timesteps,
    the signal is quite degraded.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: As a result of this instability, it has been challenging to train recurrent
    neural networks on longer time-series. A number of methods have arisen to combat
    this instability, which we will discuss in the remainder of this section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of elaborations on the concept of a simple recurrent neural
    network that have proven significantly more successful in practical applications.
    In this section, we will briefly review some of these variations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory (LSTM)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Part of the challenge with the standard recurrent cell is that signals from
    the distant past attenuate rapidly. As a result, RNNs can fail to learn models
    of complex dependencies. This failure becomes particularly notable in applications
    such as language modeling, where words can have complex dependencies on earlier
    phrases.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: One potential solution to this issue is to allow states from the past to pass
    through unmodified. The long short-term memory (LSTM) architecture proposes a
    mechanism to allow past state to pass through to the present with minimal modifications.
    Empirically using an LSTM “cell” (shown in [Figure 7-4](#ch7-lstm)) seems to offer
    superior learning performance when compared to simple recurrent neural networks
    using fully connected layers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![colah_lstm.png](assets/tfdl_0704.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. A long short-term memory (LSTM) cell. LSTMs perform better than
    standard recurrent neural networks at preserving long-range dependencies in inputs.
    As a result, LSTMs are often preferred for complex sequential data, such as natural
    language.
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So Many Equations!
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LSTM equations involve many sophisticated terms. If you are interested in
    understanding precisely the mathematical intuitions behind the LSTM, we encourage
    you to play with the equations with pencil and paper and trying to take derivatives
    of the LSTM cell.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: However, for other readers who are primarily interested in using recurrent architectures
    to solve practical problems, we believe it isn’t absolutely necessary to delve
    into the nitty-gritty details of how LSTMs work. Rather, keep the high-level intuition
    that past state is allowed to pass through, and work through the example code
    for this chapter in some depth.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Recurrent Networks
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike fully connected networks or convolutional networks, LSTMs involve some
    sophisticated mathematical operations and control-flow operations. As a result,
    training large recurrent networks at scale has proven to be challenging, even
    with modern GPU hardware.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Significant effort has been put into optimizing RNN implementations to run quickly
    on GPU hardware. In particular, Nvidia has incorporated RNNs into their CuDNN
    library that provides specially optimized code for training deep networks on GPUs.
    Luckily for TensorFlow users, integration with libraries such as CuDNN is performed
    within TensorFlow itself so you don’t need to worry too much about code optimization
    (unless of course, you’re working on very large-scale datasets). We will discuss
    hardware needs for deep neural networks at greater depth in [Chapter 9](ch09.html#training_large_deep_networks).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Gated Recurrent Units (GRU)
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The complexity, both conceptual and computational, for LSTM cells has motivated
    a number of researchers to attempt to simplify the LSTM equations while retaining
    the performance gains and modeling capabilities of the original equations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of contenders for LSTM replacement, but one of the frontrunners
    is the gated recurrent unit (GRU), shown in [Figure 7-5](#ch7-gru). The GRU removes
    one of the subcomponents of the LSTM but empirically seems to achieve performance
    similar to that of the LSTM. The GRU might be a suitable replacement for LSTM
    cells on sequence modeling projects.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![lstm_gru.png](assets/tfdl_0705.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. A gated recurrent unit (GRU) cell. GRUs preserve many of the benefits
    of LSTMs at lower computational cost.
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Applications of Recurrent Models
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While recurrent neural networks are useful tools for modeling time-series datasets,
    there are a host of other applications of recurrent networks. These include applications
    such as natural language modeling, machine translation, chemical retrosynthesis,
    and arbitrary computation with Neural Turing machines. In this section, we provide
    a brief tour of some of these exciting applications.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Sampling from Recurrent Networks
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve taught you how recurrent networks can learn to model the time
    evolution of sequences of data. It stands to reason that if you understand the
    evolution rule for a set of sequences, you ought to be able to sample new sequences
    from the distribution of training sequences. And indeed, it turns out that that
    good sequences can be sampled from trained models. The most useful application
    thus far is in language modeling. Being able to generate realistic sentences is
    a very useful tool that underpins systems such as autocomplete and chatbots.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Why Don’t We Use GANs for Sequences?
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 6](ch06.html#convolutional_neural_networks), we discussed the problem
    of generating new images. We discussed models such as variational autoencoders
    that produced only blurry images and introduced the technology of generative adversarial
    networks that proves capable of producing sharp images. The question remains,
    though: if we need GANs for good image samples, why don’t we use them for good
    sentences?'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that today’s generative adversarial models are mediocre at sampling
    sequences. It’s not clear why this is the case. Theoretical understanding of GANs
    remains very weak (even by the standards of deep learning theory), but something
    about the game theoretic equilibrium discovery seems to perform worse for sequences
    than for images.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Seq2seq Models
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sequence-to-sequence (seq2seq) models are powerful tools that enable models
    to transform one sequence into another. The core idea of a sequence-to-sequence
    model is to use an encoding recurrent network that embeds input sequences into
    vector spaces alongside a decoding network that enables sampling of output sequences
    as described in previous sentences. [Figure 7-6](#ch7-seq2seq) illustrates a seq2seq
    model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![seq2seq_colah.png](assets/tfdl_0706.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Sequence-to-sequence models are powerful tools that can learn sequence
    transformations. They have been applied to machine translation (for example, transforming
    a sequence of English words to Mandarin) and chemical retrosynthesis (transforming
    a sequence of chemical products into a sequence of reactants).
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Things get interesting since encoder and decoder layers can themselves be deep.
    (RNN layers can be stacked in a natural fashion.) The Google neural machine translation
    (GNMT) system has many stacked encoding and decoding layers. As a result of this
    powerful representational capacity, it is capable of performing state-of-the-art
    translations far beyond the capabilities of its nearest nondeep competitors. [Figure 7-7](#ch7-gnmt)
    illustrates the GNMT architecture.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![google_nmt.png](assets/tfdl_0707.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. The Google neural machine translation (GNMT) architecture is a
    deep seq2seq model that learns to perform machine translation.
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While so far we’ve mainly discussed applications to natural language processing,
    the seq2seq architecture has myriad applications in other domains. One of the
    authors has used seq2seq architectures to perform chemical retrosynthesis, the
    act of deconstructing molecules into simpler constituents. [Figure 7-8](#ch7-seqret)
    illustrates.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![seq2seq_retrosynthesis.png](assets/tfdl_0708.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. A seq2seq model for chemical retrosynthesis transforms a sequence
    of chemical products into a sequence of chemical reactants.
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Neural Turing Machines
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dream of machine learning has been to move further up the abstraction stack:
    moving from learning short pattern-matching engines to learning to perform arbitrary
    computations. The Neural Turing machine is a powerful step in this evolution.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: The Turing machine was a seminal contribution to the mathematical theory of
    computation. It was the first mathematical model of a machine capable of performing
    any computation. The Turing machine maintains a “tape” that provides a memory
    of the performed computation. The second part of the machine is a “head” that
    performs transformations on single tape cells. The insight of the Turing machine
    was that the “head” didn’t need to be very complicated in order to perform arbitrarily
    complicated calculations.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The Neural Turing machine (NTM) is a very clever attempt to transmute a Turing
    machine itself into a neural network. The trick in this transmutation is to turn
    discrete actions into soft continuous functions (this is a trick that pops up
    in deep learning repeatedly, so take note!)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The Turing machine head is quite similar to the RNN cell! As a result, the NTM
    can be trained end-to-end to learn to perform arbitrary computations, in principle
    at least ([Figure 7-9](#ch7-neuraltm)). In practice, there are severe limitations
    to the set of computations that the NTM can perform. Gradient flow instabilities
    (as always) limit what can be learned. More research and experimentation will
    be needed to devise successors to NTMs capable of learning more useful functions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![turing_machine.png](assets/tfdl_0709.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. A Neural Turing machine (NTM) is a learnable version of a Turing
    machine. It maintains a tape where it can store the outputs of intermediate computations.
    While NTMs have many practical limitations, it’s possible that their intellectual
    descendants will be capable of learning powerful algorithms.
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Turing Completeness
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of Turing completeness is an important notion in computer science.
    A programming language is said to be Turing complete if it is capable of performing
    any computation that can be performed by a Turing machine. The Turing machine
    itself was invented to provide a mathematical model of what it means for a function
    to be “computable.” The machine provides the capability to read, write, and store
    in memory various instructions, abstract primitives that underlie all computing
    machines.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Over time, a large body of work has shown that the Turing machine closely models
    the set of computations performable in the physical world. To a first approximation,
    if it can be shown that a Turing machine is incapable of performing a computation,
    no computing device is capable of it either. On the other side, if it can be shown
    that a computing system can perform the basic operations of a Turing machine,
    it is then “Turing complete” and capable of performing in principle any computation
    that can be performed at all. A number of surprising systems are Turing complete.
    We encourage you to read more about this topic if interested.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Networks Are Turing Complete
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps unsurprisingly, NTMs are capable of performing any computation a Turing
    machine can and are consequently Turing complete. However, a less known fact is
    that vanilla recurrent neural networks are themselves Turing complete! Put another
    way, in principle, a recurrent neural network is capable of learning to perform
    arbitrary computation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is that the transition operator can learn to perform basic reading,
    writing, and storage operations. The unrolling of the recurrent network over time
    allows for the performance of complex computations. In some sense, this fact shouldn’t
    be too surprising. The universal approximation theorem already demonstrates that
    fully connected networks are capable of learning arbitrary functions. Chaining
    arbitrary functions together over time leads to arbitrary computations. (The technical
    details required to formally prove this are formidable, though.)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Working with Recurrent Neural Networks in Practice
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn about the use of recurrent neural networks for
    language modeling on the Penn Treebank dataset, a natural language dataset built
    from *Wall Street Journal* articles. We will introduce the TensorFlow primitives
    needed to perform this modeling and will also walk you through the data handling
    and preprocessing steps needed to prepare data for training. We encourage you
    to follow along and try running the code in the [GitHub repo associated with the
    book](https://github.com/matroid/dlwithtf).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解如何在Penn Treebank数据集上使用递归神经网络进行语言建模，这是一个由*华尔街日报*文章构建的自然语言数据集。 我们将介绍执行此建模所需的TensorFlow基元，并将指导您完成准备数据进行训练所需的数据处理和预处理步骤。
    我们鼓励您跟着尝试在与本书相关的[GitHub存储库](https://github.com/matroid/dlwithtf)中运行代码。
- en: Processing the Penn Treebank Corpus
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理Penn Treebank语料库
- en: The Penn Treebank contains a million-word corpus of *Wall Street Journal* articles.
    This corpus can be used for either character-level or word-level modeling (the
    tasks of predicting the next character or word in a sentence given those preceding).
    The efficacy of models is measured using the perplexity of trained models (more
    on this metric later).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Penn Treebank包含一个由*华尔街日报*文章组成的百万字语料库。 此语料库可用于字符级或单词级建模（预测给定前面的句子中的下一个字符或单词的任务）。
    使用训练模型的困惑度来衡量模型的有效性（稍后将详细介绍此指标）。
- en: The Penn Treebank corpus consists of sentences. How can we transform sentences
    into a form that can be fed to machine learning systems such as recurrent language
    models? Recall that machine learning models accept tensors (with recurrent models
    accepting sequences of tensors) as input. Consequently, we need to transform words
    into tensors for machine learning.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Penn Treebank语料库由句子组成。 我们如何将句子转换为可以馈送到机器学习系统（例如递归语言模型）的形式？ 请记住，机器学习模型接受张量（递归模型接受张量序列）作为输入。
    因此，我们需要将单词转换为机器学习的张量。
- en: The simplest method of transforming words into vectors is to use “one-hot” encoding.
    In this encoding, let’s suppose that our language dataset uses a vocabulary that
    has <math><mrow><mo>|</mo> <mi>V</mi> <mo>|</mo></mrow></math> words. Then each
    word is transformed into a vector of shape <math><mrow><mo>(</mo> <mo>|</mo> <mi>V</mi>
    <mo>|</mo> <mo>)</mo></mrow></math> . All the entries of this vector are zero,
    except for one entry, at the index that corresponds to the current word. For an
    example of this embedding, see [Figure 7-10](#ch7-one-hot).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词转换为向量的最简单方法是使用“一热”编码。 在此编码中，假设我们的语言数据集使用具有<math><mrow><mo>|</mo> <mi>V</mi>
    <mo>|</mo></mrow></math>个单词的词汇表。 然后，每个单词转换为形状为<math><mrow><mo>(</mo> <mo>|</mo>
    <mi>V</mi> <mo>|</mo> <mo>)</mo></mrow></math>的向量。 此向量的所有条目都为零，除了一个条目，在索引处，该索引对应于当前单词。
    有关此嵌入的示例，请参见[图7-10](#ch7-one-hot)。
- en: '![one-hot.jpg](assets/tfdl_0710.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![one-hot.jpg](assets/tfdl_0710.png)'
- en: Figure 7-10\. One-hot encodings transform words into vectors with only one nonzero
    entry (which is typically set to one). Different indices in the vector uniquely
    represent words in a language corpus.
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10。 一热编码将单词转换为只有一个非零条目的向量（通常设置为一）。 向量中的不同索引唯一表示语言语料库中的单词。
- en: It’s also possible to use more sophisticated embeddings. The basic idea is similar
    to that for the one-hot encoding. Each word is associated with a unique vector.
    However, the key difference is that it’s possible to learn this encoding vector
    directly from data to obtain a “word embedding” for the word in question that’s
    meaningful for the dataset at hand. We will show you how to learn word embeddings
    later in this chapter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用更复杂的嵌入。 基本思想类似于一热编码。 每个单词与唯一向量相关联。 但是，关键区别在于可以直接从数据中学习此编码向量，以获得对于当前数据集有意义的单词的“单词嵌入”。
    我们将在本章后面向您展示如何学习单词嵌入。
- en: In order to process the Penn Treebank data, we need to find the vocabulary of
    words used in the corpus, then transform each word into its associated word vector.
    We will then show how to feed the processed data into a TensorFlow model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理Penn Treebank数据，我们需要找到语料库中使用的单词的词汇表，然后将每个单词转换为其关联的单词向量。 然后，我们将展示如何将处理后的数据馈送到TensorFlow模型中。
- en: Penn Treebank Limitations
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Penn Treebank的限制
- en: The Penn Treebank is a very useful dataset for language modeling, but it no
    longer poses a challenge for state-of-the-art language models; researchers have
    already overfit models on the peculiarities of this collection. State-of-the-art
    research would use larger datasets such as the billion-word-corpus language benchmark.
    However, for our exploratory purposes, the Penn Treebank easily suffices.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Penn Treebank是语言建模的一个非常有用的数据集，但对于最先进的语言模型来说已经不再构成挑战； 研究人员已经在这个集合的特殊性上过拟合了模型。
    最先进的研究将使用更大的数据集，例如十亿字语料库语言基准。 但是，对于我们的探索目的，Penn Treebank已经足够。
- en: Code for Preprocessing
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理代码
- en: The snippet of code in [Example 7-1](#ch7-readwords) reads in the raw files
    associated with the Penn Treebank corpus. The corpus is stored with one sentence
    per line. Some Python string handling is done to replace `"\n"` newline markers
    with fixed-token `"<eos>"` and then split the file into a list of tokens.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例7-1](#ch7-readwords)中的代码片段读取了与Penn Treebank语料库相关的原始文件。 语料库存储在每行一个句子的形式中。
    通过一些Python字符串处理，将`"\n"`换行标记替换为固定标记`"<eos>"`，然后将文件拆分为标记列表。'
- en: Example 7-1\. This function reads in the raw Penn Treebank file
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-1。 此函数读取原始Penn Treebank文件
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With `_read_words` defined, we can build the vocabulary associated with a given
    file using function `_build_vocab` defined in [Example 7-2](#ch7-vocab). We simply
    read in the words in the file, and count the number of unique words in the file
    using Python’s `collections` library. For convenience, we construct a dictionary
    object mapping words to their unique integer identifiers (their positions in the
    vocabulary). Tying it all together, `_file_to_word_ids` transforms a file into
    a list of word identifiers ([Example 7-3](#ch7-file-to-words)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了`_read_words`后，我们可以使用[示例7-2](#ch7-vocab)中定义的`_build_vocab`函数构建与给定文件相关联的词汇表。我们简单地读取文件中的单词，并使用Python的`collections`库计算文件中唯一单词的数量。为方便起见，我们构建一个字典对象，将单词映射到它们的唯一整数标识符（在词汇表中的位置）。将所有这些联系在一起，`_file_to_word_ids`将文件转换为单词标识符列表（[示例7-3](#ch7-file-to-words)）。
- en: Example 7-2\. This function builds a vocabulary consisting of all words in the
    specified file
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-2。此函数构建一个由指定文件中所有单词组成的词汇表
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Example 7-3\. This function transforms words in a file into id numbers
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-3。此函数将文件中的单词转换为id号
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With these utilities in place, we can process the Penn Treebank corpus with
    function `ptb_raw_data` ([Example 7-4](#ch7-ptb-raw)). Note that training, validation,
    and test datasets are pre-specified, so we need only read each file into a list
    of unique indices.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些实用工具，我们可以使用函数`ptb_raw_data`（[示例7-4](#ch7-ptb-raw)）处理Penn Treebank语料库。请注意，训练、验证和测试数据集是预先指定的，因此我们只需要将每个文件读入一个唯一索引列表中。
- en: Example 7-4\. This function loads the Penn Treebank data from the specified
    location
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-4。此函数从指定位置加载Penn Treebank数据
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: tf.GFile and tf.Flags
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tf.GFile和tf.Flags
- en: TensorFlow is a large project that contains many bits and pieces. While most
    of the library is devoted to machine learning, there’s also a large proportion
    that’s dedicated to loading and massaging data. Some of these functions provide
    useful capabilities that aren’t found elsewhere. Other parts of the loading functionality
    are less useful, however.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是一个庞大的项目，包含许多部分。虽然大部分库专注于机器学习，但也有相当大比例的库专门用于加载和处理数据。其中一些功能提供了在其他地方找不到的有用功能。然而，加载功能的其他部分则不太有用。
- en: '`tf.GFile` and `tf.FLags` provide functionality that is more or less identical
    to standard Python file handling and `argparse`. The provenance of these tools
    is historical. With Google, custom file handlers and flag handling are required
    by internal code standards. For the rest of us, though, it’s better style to use
    standard Python tools whenever possible. It’s much better for readability and
    stability.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.GFile`和`tf.FLags`提供的功能与标准Python文件处理和`argparse`几乎相同。这些工具的来源是历史性的。对于Google来说，内部代码标准要求使用自定义文件处理程序和标志处理程序。然而，对于我们其他人来说，尽可能使用标准Python工具是更好的风格。这样做对于可读性和稳定性更有利。'
- en: Loading Data into TensorFlow
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据加载到TensorFlow中
- en: In this section, we cover the code needed to load our processed indices into
    TensorFlow. To do so, we will introduce you to a new bit of TensorFlow machinery.
    Until now, we’ve used feed dictionaries to pass data into TensorFlow. While feed
    dictionaries are fine for small toy datasets, they are often not good choices
    for larger datasets, since large Python overheads involving packing and unpacking
    dictionaries are introduced. For more performant code, it’s better to use TensorFlow
    queues.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍加载我们处理过的索引到TensorFlow所需的代码。为此，我们将向您介绍一些新的TensorFlow机制。到目前为止，我们已经使用feed字典将数据传递到TensorFlow中。虽然feed字典对于小型玩具数据集来说是可以接受的，但对于较大的数据集来说，它们通常不是一个好选择，因为引入了大量Python开销，涉及打包和解包字典。为了更高性能的代码，最好使用TensorFlow队列。
- en: '`tf.Queue` provides a way to load data asynchronously. This allows decoupling
    of the GPU compute thread from the CPU-bound data preprocessing thread. This decoupling
    is particularly useful for large datasets where we want to keep the GPU maximally
    active.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Queue`提供了一种异步加载数据的方式。这允许将GPU计算线程与CPU绑定的数据预处理线程解耦。这种解耦对于希望保持GPU最大活跃性的大型数据集特别有用。'
- en: It’s possible to feed `tf.Queue` objects into TensorFlow placeholders to train
    models and achieve greater performance. We will demonstrate how to do so later
    in this chapter.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将`tf.Queue`对象提供给TensorFlow占位符以训练模型并实现更高的性能。我们将在本章后面演示如何做到这一点。
- en: The function `ptb_producer` introduced in [Example 7-5](#ch7-ptb-producer) transforms
    raw lists of indices into `tf.Queues` that can pass data into a TensorFlow computational
    graph. Let’s start by introducing some of the computational primitives we use.
    `tf.train.range_input_producer` is a convenience operation that produces a `tf.Queue`
    from an input tensor. The method `tf.Queue.dequeue()` pulls a tensor from the
    queue for training. `tf.strided_slice` extracts the part of this tensor that corresponds
    to the data for the current minibatch.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例7-5](#ch7-ptb-producer)中介绍的`ptb_producer`函数将原始索引列表转换为可以将数据传递到TensorFlow计算图中的`tf.Queues`。让我们首先介绍一些我们使用的计算原语。`tf.train.range_input_producer`是一个方便的操作，从输入张量生成一个`tf.Queue`。方法`tf.Queue.dequeue()`从队列中提取一个张量进行训练。`tf.strided_slice`提取与当前小批量数据对应的部分张量。
- en: Example 7-5\. This function loads the Penn Treebank data from the specified
    location
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-5。此函数从指定位置加载Penn Treebank数据
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: tf.data
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tf.data
- en: TensorFlow (from version 1.4 onward) supports a new module `tf.data` with a
    new class `tf.data.Dataset` that provides an explicit API for representing streams
    of data. It’s likely that `tf.data` will eventually supersede queues as the preferred
    input modality, especially since it has a well-thought-out functional API.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow（从版本1.4开始）支持一个新模块`tf.data`，其中包含一个新类`tf.data.Dataset`，提供了一个明确的API来表示数据流。很可能`tf.data`最终会取代队列成为首选的输入模式，特别是因为它具有经过深思熟虑的功能API。
- en: At the time of writing, the `tf.data` module was just released and remained
    relatively immature compared with other parts of the API, so we decided to stick
    with queues for the examples. However, we encourage you to learn about `tf.data`
    yourself.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，`tf.data`模块刚刚发布，与API的其他部分相比仍然相对不成熟，因此我们决定在示例中继续使用队列。但是，我们鼓励您自己了解`tf.data`。
- en: The Basic Recurrent Architecture
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use an LSTM cell for modeling the Penn Treebank, since LSTMs often offer
    superior performance for language modeling challenges. The function `tf.contrib.rnn.BasicLSTMCell`
    implements the basic LSTM cell for us already, so no need to implement it ourselves
    ([Example 7-6](#ch7-lstm-wrap)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-6\. This function wraps an LSTM cell from tf.contrib
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Is Using TensorFlow Contrib Code OK?
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that the LSTM implementation we use is drawn from `tf.contrib`. Is it acceptable
    to use code from `tf.contrib` for industrial-strength projects? The jury still
    appears to be out on this one. From our personal experience, code in `tf.contrib`
    tends to be a bit shakier than code in the core TensorFlow library, but is usually
    still pretty solid. There are often many useful libraries and utilities that are
    only available as part of `tf.contrib`. Our recommendation is to use pieces from
    `tf.contrib` as necessary, but make note of the pieces you use and replace them
    if an equivalent in the core TensorFlow library becomes available.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The snippet in [Example 7-7](#ch7-embed) instructs TensorFlow to learn a word
    embedding for each word in our vocabulary. The key function for us is `tf.nn.embedding_lookup`,
    which allows us to perform the correct tensorial lookup operation. Note that we
    need to manually define the embedding matrix as a TensorFlow variable.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-7\. Learn a word embedding for each word in the vocabulary
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With our word vectors in hand, we simply need to apply the LSTM cell (using
    function `lstm_cell`) to each word vector in our sequence. To do this, we simply
    use a Python `for`-loop to construct the needed set of calls to `cell()`. There’s
    only one trick here: we need to make sure we reuse the same variables at each
    timestep, since the LSTM cell should perform the same operation at each timestep.
    Luckily, the method `reuse_variables()` for variable scopes allows us to do so
    without much effort. See [Example 7-8](#ch7-embed2).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-8\. Apply LSTM cell to each word vector in input sequence
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: All that remains now is to define the loss associated with the graph in order
    to train it. Conveniently, TensorFlow offers a loss for training language models
    in `tf.contrib`. We need only make a call to `tf.contrib.seq2seq.sequence_loss`
    ([Example 7-9](#ch7-seqloss)). Underneath the hood, this loss turns out to be
    a form of perplexity.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-9\. Add the sequence loss
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Perplexity
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perplexity is often used for language modeling challenges. It is a variant of
    the binary cross-entropy that is useful for measuring how close the learned distribution
    is to the true distribution of data. Empirically, perplexity has proven useful
    for many language modeling challenges and we make use of it here in that capacity
    (since the `sequence_loss` just implements perplexity specialized to sequences
    inside).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: We can then train this graph using a standard gradient descent method. We leave
    out some of the messy details of the underlying code, but suggest you check GitHub
    if curious. Evaluating the quality of the trained model turns out to be straightforward
    as well, since the perplexity is used both as the training loss and the evaluation
    metric. As a result, we can simply display `self._cost` to gauge how the model
    is training. We encourage you to train the model for yourself!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Challenge for the Reader
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Try lowering perplexity on the Penn Treebank by experimenting with different
    model architectures. Note that these experiments might be time-consuming without
    a GPU.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Review
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced you to recurrent neural networks (RNNs), a powerful
    architecture for learning on sequential data. RNNs are capable of learning the
    underlying evolution rule that governs a sequence of data. While RNNs can be used
    for modeling simple time-series, they are most powerful when modeling complex
    sequential data such as speech and natural language.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: We introduced you to a number of RNN variants such as LSTMs and GRUs, which
    perform better on data with complex long-range interactions, and also took a brief
    detour to discuss the exciting prospect of Neural Turing machines. We ended the
    chapter with an in-depth case study that applied LSTMs to model the Penn Treebank.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向您介绍了许多RNN变体，如LSTMs和GRUs，它们在具有复杂长程交互的数据上表现更好，并且还简要讨论了神经图灵机的令人兴奋的前景。我们以一个深入的案例研究结束了本章，该案例将LSTMs应用于对Penn
    Treebank进行建模。
- en: In [Chapter 8](ch08.html#reinforcement_learning), we will introduce you to reinforcement
    learning, a powerful technique for learning to play games.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.html#reinforcement_learning)中，我们将向您介绍强化学习，这是一种学习玩游戏的强大技术。
