<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Adapting LLMs to Your Use Case"><div class="chapter" id="chapter_utilizing_llms">
<h1><span class="label">Chapter 5. </span>Adapting LLMs to Your Use Case</h1>


<p>In this chapter, we will continue with our journey<a data-type="indexterm" data-primary="use cases" id="xi_usecases5451"/> through the LLM landscape, exploring the various LLMs available for commercial use and providing pointers on how to choose the right LLM for your task. We will also examine how to load LLMs of various sizes and run inference on them. We will then decipher various decoding strategies for text generation. We will also investigate how to interpret the outputs and intermediate results from language models, surveying interpretability tools like LIT-NLP.</p>






<section data-type="sect1" data-pdf-bookmark="Navigating the LLM Landscape"><div class="sect1" id="id363">
<h1>Navigating the LLM Landscape</h1>

<p>Seemingly a new LLM is being released every few days, many claiming to be state of the art. Most of these LLMs are not very different from each other, so you need not spend too much time tracking new LLM releases. This book’s <a href="https://oreil.ly/llm-playbooks">GitHub repository</a> attempts to keep track of the major releases, but I don’t promise it will be complete.</p>

<p>Nevertheless, it is a good idea to have a broad understanding of the different types of LLM providers out there, the kinds of LLMs being made available, and the copyright and licensing implications. Therefore, let’s now explore the LLM landscape through this lens and understand the choices at our disposal.</p>








<section data-type="sect2" data-pdf-bookmark="Who Are the LLM providers?"><div class="sect2" id="id66">
<h2>Who Are the LLM providers?</h2>

<p>LLM providers<a data-type="indexterm" data-primary="use cases" data-secondary="providers of LLMs" id="id861"/><a data-type="indexterm" data-primary="providers of LLMs" id="id862"/><a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="providers of" id="id863"/> can be broadly categorized into the following types:</p>
<dl>
<dt>Companies providing proprietary LLMs</dt>
<dd>
<p>These include companies<a data-type="indexterm" data-primary="proprietary LLM providers" id="id864"/> like OpenAI <a href="https://oreil.ly/r-lb1">(GPT)</a>, Google <a href="https://oreil.ly/KF9Kh">(Gemini)</a>, Anthropic <a href="https://oreil.ly/T5Wvo">(Claude)</a>, <a href="https://oreil.ly/PiKxN">Cohere</a>, <a href="https://oreil.ly/Y8T3q">AI21</a>, etc. that train 
<span class="keep-together">proprietary</span> LLMs and make them available as an API endpoint (LLM-as-a-service). Many of these companies have also partnered with cloud providers that facilitate access to these models as a fully managed service. The relevant offerings from the major cloud providers are <a href="https://oreil.ly/FVqRj">Amazon Bedrock</a> and <a href="https://oreil.ly/e0a59">SageMaker JumpStart by Amazon</a>, <a href="https://oreil.ly/mURoC">Vertex AI by Google</a>, and <a href="https://oreil.ly/Ag1r5">Azure OpenAI by Microsoft</a>.</p>
</dd>
<dt>Companies providing open source LLMs</dt>
<dd>
<p>These include companies that make the LLM weights public<a data-type="indexterm" data-primary="open source LLMs" id="id865"/> and monetize through providing deployment services (<a href="https://oreil.ly/urcAf">Together AI</a>), companies whose primary business would benefit from more LLM adoption (<a href="https://oreil.ly/2cVYY">Cerebras</a>), and research labs that have been releasing LLMs since the early days of Transformers (Microsoft, Google, Meta, Salesforce, etc.). Note that companies like Google have released both proprietary and open source LLMs.</p>
</dd>
<dt>Self-organizing open source collectives and community research organizations</dt>
<dd>
<p>This includes the pioneering community research organization <a href="https://oreil.ly/ZSlbG">Eleuther AI</a>, and <a href="https://oreil.ly/_NlUD">Big Science</a>. These organizations rely on grants for compute infrastructure.</p>
</dd>
<dt>Academia and government</dt>
<dd>
<p>Due to the high capital costs<a data-type="indexterm" data-primary="academia and government LLM providers" id="id866"/>, not many LLMs have come out of academia so far. Examples of LLMs from government/academia include the Abu Dhabi government-funded <a href="https://oreil.ly/aMwO2">Technology Innovation Institute</a>, which released the <a href="https://oreil.ly/vdhsL">Falcon model</a>, and Tsinghua University, which released the <a href="https://oreil.ly/K0_zX">GLM model</a>.</p>
</dd>
</dl>

<p><a data-type="xref" href="#llm-provider-categories">Table 5-1</a> shows the players in the LLM space, the category of entity they belong to, and the pre-trained models they have published.</p>
<table id="llm-provider-categories">
<caption><span class="label">Table 5-1. </span>LLM Providers</caption>
<thead>
<tr>
<th>Name</th>
<th>Category</th>
<th>Pre-trained models released</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Google</p></td>
<td><p>Company</p></td>
<td><p>BERT, MobileBERT,  T5, FLAN-T5, ByT5, Canine, UL2, Flan-UL2, Pegasus PaLM, PaLMV2, ELECTRA, Tapas, Switch</p></td>
</tr>
<tr>
<td><p>Microsoft</p></td>
<td><p>Company</p></td>
<td><p>DeBERTa, DialoGPT, BioGPT, MPNet</p></td>
</tr>
<tr>
<td><p>OpenAI</p></td>
<td><p>Company</p></td>
<td><p>GPT-2, GPT-3, GPT-3.5, GPT-4</p></td>
</tr>
<tr>
<td><p>Amazon</p></td>
<td><p>Company</p></td>
<td><p>Titan</p></td>
</tr>
<tr>
<td><p>Anthropic</p></td>
<td><p>Company</p></td>
<td><p>Claude, Claude-2</p></td>
</tr>
<tr>
<td><p>Cohere</p></td>
<td><p>Company</p></td>
<td><p>Cohere Command, Cohere Base</p></td>
</tr>
<tr>
<td><p>Meta</p></td>
<td><p>Company</p></td>
<td><p>RoBERTa, Llama, Llama 2, BART, OPT, Galactica</p></td>
</tr>
<tr>
<td><p>Salesforce</p></td>
<td><p>Company</p></td>
<td><p>CTRL, XGen, EinsteinGPT</p></td>
</tr>
<tr>
<td><p>MosaicML</p></td>
<td><p>Company (Acquired by Databricks)</p></td>
<td><p>MPT</p></td>
</tr>
<tr>
<td><p>Cerebras</p></td>
<td><p>Company</p></td>
<td><p>Cerebras-GPT, BTLM</p></td>
</tr>
<tr>
<td><p>Databricks</p></td>
<td><p>Company</p></td>
<td><p>Dolly-V1, Dolly-V2</p></td>
</tr>
<tr>
<td><p>Stability AI</p></td>
<td><p>Company</p></td>
<td><p>StableLM</p></td>
</tr>
<tr>
<td><p>Together AI</p></td>
<td><p>Company</p></td>
<td><p>RedPajama</p></td>
</tr>
<tr>
<td><p>Ontocord AI</p></td>
<td><p>Nonprofit</p></td>
<td><p>MDEL</p></td>
</tr>
<tr>
<td><p>Eleuther AI</p></td>
<td><p>Nonprofit</p></td>
<td><p>Pythia, GPT Neo, GPT-NeoX, GPT-J</p></td>
</tr>
<tr>
<td><p>Big Science</p></td>
<td><p>Nonprofit</p></td>
<td><p>BLOOM</p></td>
</tr>
<tr>
<td><p>Tsinghua University</p></td>
<td><p>Academic</p></td>
<td><p>GLM</p></td>
</tr>
<tr>
<td><p>Technology Innovation Institute</p></td>
<td><p>Academic</p></td>
<td><p>Falcon</p></td>
</tr>
<tr>
<td><p>UC Berkeley</p></td>
<td><p>Academic</p></td>
<td><p>OpenLLaMA</p></td>
</tr>
<tr>
<td><p>Adept AI</p></td>
<td><p>Company</p></td>
<td><p>Persimmon</p></td>
</tr>
<tr>
<td><p>Mistral AI</p></td>
<td><p>Company</p></td>
<td><p>Mistral</p></td>
</tr>
<tr>
<td><p>AI21 Labs</p></td>
<td><p>Company</p></td>
<td><p>Jurassic</p></td>
</tr>
<tr>
<td><p>X.AI</p></td>
<td><p>Company</p></td>
<td><p>Grok</p></td>
</tr>
</tbody>
</table>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Model Flavors"><div class="sect2" id="id67">
<h2>Model Flavors</h2>

<p>Each model is usually released with multiple variants<a data-type="indexterm" data-primary="use cases" data-secondary="model variants" id="xi_usecasesmodelvariants56054"/>. It is customary to release different-sized variants of the same model. As an example, Llama 2 comes in 7B, 13B, and 70B sizes, where these numbers refer to the number of parameters in the model.</p>

<p>These days, LLM providers augment<a data-type="indexterm" data-primary="augmented pre-trained models" id="id867"/><a data-type="indexterm" data-primary="pre-training models" id="id868"/> their pre-trained models in various ways to make them more amenable to user tasks. The augmentation process typically involves fine-tuning the model in some way, often incorporating human supervision. Some of these fine-tuning exercises can cost millions of dollars in terms of human annotations. We will refer to pre-trained models that have not undergone any augmentation as base models.</p>

<p>The following sections describe some of the popular augmentation types.</p>










<section data-type="sect3" data-pdf-bookmark="Instruct-models"><div class="sect3" id="id68">
<h3>Instruct-models</h3>

<p>Instruct-models<a data-type="indexterm" data-primary="datasets" data-secondary="instruction-tuned" id="xi_datasetsinstructiontuned56816"/><a data-type="indexterm" data-primary="instruction-tuned models" id="xi_instructiontunedmodels56816"/>, or instruction-tuned models, are specialized in following instructions written in natural language. While base models<a data-type="indexterm" data-primary="base models" data-secondary="versus instruction-tuned models" data-secondary-sortas="instruction-tuned models" id="id869"/><a data-type="indexterm" data-primary="instruction-tuned models" data-secondary="versus base models" data-secondary-sortas="base models" id="id870"/><a data-type="indexterm" data-primary="models" data-secondary="base" id="id871"/> possess powerful capabilities, they are akin to a rebellious teenager; effectively interacting with them is possible only after tediously engineering the right prompts through trial and error, which tend to be brittle. This is because the base models are trained on either denoising objectives or next-word prediction objectives, which are different from the tasks users typically want to solve. By instruction-tuning the base model, the resulting model is able to more effectively respond to human instructions and be helpful.</p>

<p>A typical instruction-tuning dataset consists of a diverse set of tasks expressed in natural language, along with input-output pairs. In <a data-type="xref" href="ch06.html#llm-fine-tuning">Chapter 6</a>, we will explore various techniques to construct instruction-tuning datasets and demonstrate how to perform instruction-tuning on a model.</p>

<p>Here is an example from a popular instruction-tuning dataset<a data-type="indexterm" data-primary="FLAN (Fine-tuned Language Net)" id="id872"/> called <a href="https://oreil.ly/YJ_Xr">FLAN</a>.</p>
<blockquote>
<p><em>Prompt:</em> “What is the sentiment of the following review? The pizza was ok but the service was terrible. I stopped in for a quick lunch and got the slice special but it ended up taking an hour after waiting several minutes for someone at the front counter and then again for the slices. The place was empty other than myself, yet I couldn’t get any help/service. OPTIONS: - negative - positive”</p>

<p><em>FLAN:</em> “Negative”</p></blockquote>

<p>In this example, the input consists of an instruction, “What is the sentiment of the following review?” expressed in a way that humans would naturally express, along with the input and output. The input is the actual review and the output is the solution to the task, either generated by a model or annotated by a human.</p>

<p><a data-type="xref" href="#instruction-tuning1">Figure 5-1</a> demonstrates the instruction-tuning process.</p>

<figure><div id="instruction-tuning1" class="figure">
<img src="assets/dllm_0501.png" alt="Instruction tuning process" width="600" height="186"/>
<h6><span class="label">Figure 5-1. </span>Instruction-tuning process</h6>
</div></figure>

<p>Instruction-tuning is one of several techniques that come under the umbrella of supervised fine-tuning (SFT)<a data-type="indexterm" data-primary="supervised fine-tuning (SFT)" data-secondary="instruction tuning" id="id873"/><a data-type="indexterm" data-primary="fine-tuning models" data-secondary="supervised fine-tuning" id="id874"/>. In addition to improving the ability of a model to respond effectively to user tasks, SFT-based approaches can also be used to make it less harmful by training on safety datasets that help align model outputs with the values and preferences of the model creators.</p>

<p>More advanced techniques to achieve this alignment include reinforcement learning-based methods like reinforcement learning from human feedback (RLHF)<a data-type="indexterm" data-primary="reinforcement learning from human feedback (RLHF)" id="id875"/><a data-type="indexterm" data-primary="reinforcement learning from AI feedback (RLAIF)" id="id876"/><a data-type="indexterm" data-primary="RLAIF (reinforcement learning from AI feedback)" id="id877"/><a data-type="indexterm" data-primary="RLHF (reinforcement learning from human feedback)" id="id878"/> and reinforcement learning from AI feedback (RLAIF).</p>

<p>In RLHF training, human annotators select or rank candidate outputs based on certain criteria, like helpfulness and harmlessness. These annotations are used to 
<span class="keep-together">iteratively</span> train a reward model, which ultimately leads to the LLM being more controllable, for example, by refusing to answer inappropriate requests from users.</p>

<p><a data-type="xref" href="#rlhf-1">Figure 5-2</a> shows the RLHF training process.</p>

<figure><div id="rlhf-1" class="figure">
<img src="assets/dllm_0502.png" alt="RLHF" width="600" height="161"/>
<h6><span class="label">Figure 5-2. </span>Reinforcement learning from human feedback</h6>
</div></figure>

<p>We will cover RLHF and other alignment techniques in detail in <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>.</p>

<p>Instead of relying on human feedback for alignment training, one can also leverage LLMs to choose between outputs based on their adherence to a set of principles (don’t be racist, don’t be rude, etc.). This technique was introduced by Anthropic<a data-type="indexterm" data-primary="Anthropic" data-secondary="RLAIF" id="id879"/> and is called RLAIF. In this technique, humans only provide a desired set of principles and values<a data-type="indexterm" data-primary="Constitutional AI" id="id880"/> (referred to as <a href="https://oreil.ly/d8FeW">Constitutional AI</a>), and the LLM is tasked with determining whether its outputs adhere to these principles.</p>

<p>Instruction-tuned models often take the suffix <em>instruct</em>, like RedPajama-Instruct.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id881">
<h1>Instruction Tuning Can Have Side Effects</h1>
<p>Is it beneficial to always prefer using an instruction-tuned variant over the base model<a data-type="indexterm" data-primary="base models" data-secondary="versus instruction-tuned models" data-secondary-sortas="instruction-tuned models" id="id882"/><a data-type="indexterm" data-primary="instruction-tuned models" data-secondary="versus base models" data-secondary-sortas="base models" id="id883"/><a data-type="indexterm" data-primary="models" data-secondary="base" id="id884"/> for your tasks? In most cases, yes. However, keep in mind that any tuning on top of a base model inevitably causes some regressions, thus losing access to some of the capabilities possessed by the base model.</p>

<p><a href="https://oreil.ly/-p0R4">Chung et al.</a> demonstrated an example of this. They noticed that instruction-tuning using the FLAN dataset worsened chain-of-thought (CoT)<a data-type="indexterm" data-primary="chain-of-thought (CoT) prompting" id="id885"/><a data-type="indexterm" data-primary="CoT (chain-of-thought) prompting" id="id886"/><a data-type="indexterm" data-primary="prompting" data-secondary="chain-of-thought" id="id887"/> capabilities, which are crucial for reasoning tasks. However, they also observed that adding CoT data to their instruction-tuning datasets increased the reasoning capabilities of the model compared to the base variant.</p>

<p>The side effects of instruction tuning are not well explored, so it is a good idea to experiment with the base model and see if you are losing out on any capabilities.</p>

<p>Similarly, alignment-tuned models<a data-type="indexterm" data-primary="alignment-tuned models" id="id888"/> are calibrated to respond to user queries in accordance with the principles, values, and ethics of the LLM provider. These may not be the same values that you or your organization hold.</p>

<p>In all these cases you can perform your own instruction and alignment tuning on the base model, the details of which are explored in the next three chapters. We will also analyze in what situations is it worthwhile to perform your own instruction/alignment tuning<a data-type="indexterm" data-startref="xi_datasetsinstructiontuned56816" id="id889"/><a data-type="indexterm" data-startref="xi_instructiontunedmodels56816" id="id890"/>.</p>
</div></aside>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Chat-models"><div class="sect3" id="id69">
<h3>Chat-models</h3>

<p>Chat-models<a data-type="indexterm" data-primary="chat-models" id="id891"/><a data-type="indexterm" data-primary="instruction-tuned models" data-secondary="chat-models" id="id892"/> are instruction-tuned models that are optimized for multi-turn dialog. Examples include ChatGPT, Llama 2-Chat, MPT-Chat, OpenAssistant, etc.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Long-context models"><div class="sect3" id="id70">
<h3>Long-context models</h3>

<p>As discussed in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, Transformer-based LLMs have a limited context<a data-type="indexterm" data-primary="long-context models" id="id893"/><a data-type="indexterm" data-primary="models" data-secondary="long-context" id="id894"/> length. To recap, context length typically refers to the sum of the number of input and output tokens processed by the model per invocation. Typical context lengths of modern LLMs range from 8,000 to 128,000 tokens, with some variants of Gemini<a data-type="indexterm" data-primary="Google Gemini" id="id895"/><a data-type="indexterm" data-primary="Gemini" id="id896"/> supporting over a million tokens. Some models are released with a long-context variant; for example GPT 3.5 comes with a default 4K context size but also has a 16K context size variant<a data-type="indexterm" data-primary="MPT" id="id897"/><a data-type="indexterm" data-primary="Hugging Face" data-secondary="MPT" id="id898"/>. <a href="https://oreil.ly/wKqdL">MPT</a> also has a long-context variant that has been trained on 65k context length but can potentially be used for even longer contexts during inference.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id899">
<h1>No Free Lunch for Long-Context Models</h1>
<p>As of yet, it has been <a href="https://oreil.ly/PSD_k">shown</a> that performance is not sustained as context length increases. LLMs tend to forget things in the middle of the context window<a data-type="indexterm" data-primary="context window" data-secondary="long context challenge" id="id900"/>. This is because of the characteristics of the documents that LLMs are trained on, wherein the most relevant context of a document necessary to predict the next token is more often found near the beginning or end of the context. In my experiments, I have observed that 8K context size is the tipping point for most models beyond which performance starts to degrade. You also can’t just stuff your entire context with instructions; LLMs can handle only a limited set of instructions in a prompt, beyond which performance drops.</p>

<p>However, long-context models are one area of LLMs where we are seeing the most rapid improvements. Claude and Gemini long-context models have shown excellent progress in sustaining performance over long contexts.</p>

<p>Various tests have been devised for measuring long-context performance, including <a href="https://oreil.ly/aop_Q">needle in a haystack tests</a>. We will discuss the shortcomings of these evaluation approaches and propose more holistic evaluation schemes in 
<span class="keep-together"><a data-type="xref" href="ch12.html#ch12">Chapter 12</a></span>.</p>
</div></aside>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Domain-adapted or task-adapted models"><div class="sect3" id="id71">
<h3>Domain-adapted or task-adapted models</h3>

<p>LLM providers also might perform fine-tuning on specific tasks<a data-type="indexterm" data-primary="domain-adaptive models" id="id901"/><a data-type="indexterm" data-primary="task-adaptive models" id="id902"/><a data-type="indexterm" data-primary="models" data-secondary="domain-adaptive" id="id903"/><a data-type="indexterm" data-primary="models" data-secondary="task-adaptive" id="id904"/> like summarization or financial sentiment analysis. They may also produce distilled versions of the model, where a smaller model is fine-tuned on outputs from the larger model for a particular task. Examples of task-specific fine-tunes<a data-type="indexterm" data-primary="FinBERT" id="id905"/><a data-type="indexterm" data-primary="UniversalNER" id="id906"/> include <a href="https://oreil.ly/uKUAp">FinBERT</a>, which is fine-tuned on financial sentiment analysis datasets, and <a href="https://oreil.ly/8A0pn">UniversalNER</a>, which is distilled using named-entity-recognition data<a data-type="indexterm" data-startref="xi_usecasesmodelvariants56054" id="id907"/>.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Open Source LLMs"><div class="sect2" id="id72">
<h2>Open Source LLMs</h2>

<p>Open source<a data-type="indexterm" data-primary="use cases" data-secondary="open source LLMs" id="xi_usecasesopensourceLLMs514512"/><a data-type="indexterm" data-primary="open source LLMs" id="xi_opensourceLLMs514512"/> is often used as a catch-all phrase to refer to models with some aspect that is publicly available. We will define open source as:</p>
<blockquote>
<p>Software artifacts that are released under a license that allows users to <em>study</em>, <em>use</em>, <em>modify</em>, and <em>redistribute</em> them to <em>anyone</em> and for any <em>purpose</em>.</p></blockquote>

<p>For a more formal and comprehensive definition of open source software, refer to the Open Source Initiative’s <a href="https://oreil.ly/7cezH">official definition</a>.</p>

<p>For an LLM to be considered fully open, all of the following needs to be published:</p>
<dl>
<dt>Model weights</dt>
<dd>
<p>This includes all the parameters of the model and the model configuration. Having access to this enables us to add to or modify the model parameters in any way we deem fit. Model checkpoints at various stages of training are also encouraged to be released.</p>
</dd>
<dt>Model code</dt>
<dd>
<p>Releasing only the weights of the model is akin to providing a software binary without providing the source code. Model code not only includes model training code and hyperparameter settings but also code used for pre-processing training data. Releasing information about infrastructure setup and configuration also goes a long way toward enhancing model reproducibility. In most cases, even with model code fully available, models may not be easily reproducible due to resource limitations and the nondeterministic nature of training.</p>
</dd>
<dt>Training data</dt>
<dd>
<p>This includes the training data used for the model, and ideally information or code on how it was sourced. It is also encouraged to release data at different stages of transformation of the data preprocessing pipeline, as well as the order in which the data was fed to the model. Training data is the component that is least published by model providers. Thus, most open source models are not <em>fully open</em> because the dataset is not public.</p>
</dd>
</dl>

<p>Training data is often not released due to competitive reasons. As discussed in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.html#chapter-LLM-tokenization">3</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#chapter_transformer-architecture">4</a>, most LLMs today use variants of the same architecture and training code. The distinguishing factor can often be the data content and preprocessing. Parts of the training data might be acquired using a licensing agreement, which prohibits the model provider from releasing the data publicly.</p>

<p>Another reason for not releasing training data is that there are unresolved legal issues pertaining to training data, especially surrounding copyright. As an example, The Pile dataset<a data-type="indexterm" data-primary="The Pile dataset" data-primary-sortas="Pile" id="id908"/><a data-type="indexterm" data-primary="Eleuther AI" data-secondary="The Pile dataset" data-secondary-sortas="Pile dataset" id="id909"/> created by Eleuther AI is no longer available at the official link because it contains text from copyrighted books (the Books3 dataset). Note that The Pile is 
<span class="keep-together">pre-processed</span> so the books are not in human-readable form and are not easily reproducible, as they are split, shuffled, and mixed.</p>

<p>Most training data is sourced from the open web and thus may potentially contain violent or sexual content that is illegal in certain jurisdictions. Despite the best intentions and rigorous filtering, some of these data might still be present in the final dataset. Thus many datasets that have been previously open are no longer open, LAION’s image datasets being one example.</p>

<p>Ultimately, the license under which the model has been released determines the terms under which you can use, modify, or redistribute the original or modified LLM. Broadly speaking, open LLMs are distributed under three types of licenses:</p>
<dl>
<dt>Noncommercial</dt>
<dd>
<p>These licenses<a data-type="indexterm" data-primary="noncommercial LLM distribution license" id="id910"/> only allow research and personal use and  prohibit the use of the model for commercial purposes. In many cases, the model artifacts are gated through an application form where a user would have to justify their need for access by providing a compelling research use case.</p>
</dd>
<dt>Copy-left</dt>
<dd>
<p>This type of license permits commercial usage<a data-type="indexterm" data-primary="copy-left LLM distribution license" id="id911"/>, but all source or derivative work needs to be released under the same license, thus making it harder to develop proprietary modifications. The degree to which this condition applies depends on the specific license being used.</p>
</dd>
<dt>Permissive</dt>
<dd>
<p>This type of license<a data-type="indexterm" data-primary="permissive LLM distribution license" id="id912"/> permits commercial usage, including modifying and redistributing it in proprietary applications, i.e., there is no obligation for the redistribution to be open source. Some licenses in this category also permit patents.</p>
</dd>
</dl>

<p>New types of licenses are being devised that restrict usage of the model for particular use cases, often for safety reasons. An example of this<a data-type="indexterm" data-primary="Open RAIL-M distribution license" id="id913"/> is the <a href="https://oreil.ly/2UVMe">Open RAIL-M license</a>, which prohibits usage of the model in use cases like providing medical advice, law enforcement, immigration and asylum processes, etc. For a full list of restricted use cases, see Attachment A of the license.</p>

<p>As a practitioner intending to use open LLMs in your organization for commercial reasons, it is best to use ones with permissive licenses. Popular examples of permissive licenses include the Apache 2.0 and the MIT license.</p>

<p><a href="https://oreil.ly/PQy6D">Creative Commons (CC) licenses</a> are a popular class<a data-type="indexterm" data-primary="Creative Commons (CC) distribution licenses" id="id914"/><a data-type="indexterm" data-primary="CC (Creative Commons) distribution licenses" id="id915"/> of licenses used to distribute open LLMs.The licenses have names like CC-BY-NC-SA, etc. Here is an easy way to remember what these names mean:</p>
<dl>
<dt>BY</dt>
<dd>
<p>If the license contains this term, it means attribution is needed. If it contains only CC-BY, it means the license is permissive.</p>
</dd>
<dt>SA</dt>
<dd>
<p>If the license contains this term, it means redistribution should occur under the same terms as this license. In other words, it is a copy-left license.</p>
</dd>
<dt>NC</dt>
<dd>
<p>NC stands for noncommercial. Thus, if the license contains this term, the model can only be used for research or personal use cases.</p>
</dd>
<dt>ND</dt>
<dd>
<p>ND stands for no derivatives. If the license contains this term, then distribution of modifications to the model is not allowed.</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Today, models that have open weights and open code and are released under a license that allows redistribution to anyone and for any use case are considered open source models. Arguably, however, access to the training data is also crucial to inspect and study the model, which is part of the open source definition we introduced earlier.</p>
</div>

<p><a data-type="xref" href="#llm-taxonomy">Table 5-2</a> shows the various LLMs available, the licenses under which they are published, and their available sizes and flavors. Note that the LLM may be instruction-tuned or chat-tuned by a different entity than the one that pre-trained the LLM<a data-type="indexterm" data-startref="xi_usecasesopensourceLLMs514512" id="id916"/><a data-type="indexterm" data-startref="xi_opensourceLLMs514512" id="id917"/>.</p>
<table id="llm-taxonomy">
<caption><span class="label">Table 5-2. </span>List of available LLMs</caption>
<thead>
<tr>
<th>Name</th>
<th>Availability</th>
<th>Sizes</th>
<th>Variants</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>GPT-4</p></td>
<td><p>Proprietary</p></td>
<td><p>Unknown</p></td>
<td><p>GPT-4 32K context, GPT-4 8K context</p></td>
</tr>
<tr>
<td><p>GPT-3.5 Turbo</p></td>
<td><p>Proprietary</p></td>
<td><p>Unknown</p></td>
<td><p>GPT-3.5 4K context, GPT-3.5 16K context</p></td>
</tr>
<tr>
<td><p>Claude Instant</p></td>
<td><p>Proprietary</p></td>
<td><p>Unknown</p></td>
<td><p>-</p></td>
</tr>
<tr>
<td><p>Claude 2</p></td>
<td><p>Proprietary</p></td>
<td><p>Unknown</p></td>
<td><p>-</p></td>
</tr>
<tr>
<td><p>MPT</p></td>
<td><p>Apache 2.0</p></td>
<td><p>1B, 7B, 30B</p></td>
<td><p>MPT 65K storywriter</p></td>
</tr>
<tr>
<td><p>CerebrasGPT</p></td>
<td><p>Apache 2.0</p></td>
<td><p>111M, 256M, 590M, 1.3B, 2.7B, 6.7B, 13B</p></td>
<td><p>CerebrasGPT</p></td>
</tr>
<tr>
<td><p>Stability LM</p></td>
<td><p>CC-BY-SA</p></td>
<td><p>7B</p></td>
<td><p>-</p></td>
</tr>
<tr>
<td><p>RedPajama</p></td>
<td><p>Apache 2.0</p></td>
<td><p>3B, 7B</p></td>
<td><p>RedPajama-INCITE-Instruct, RedPajama-INCITE-Chat</p></td>
</tr>
<tr>
<td><p>GPT-Neo X</p></td>
<td><p>Apache 2.0</p></td>
<td><p>20B</p></td>
<td><p>-</p></td>
</tr>
<tr>
<td><p>BLOOM</p></td>
<td><p>Open, restricted use</p></td>
<td><p>176B</p></td>
<td><p>BLOOMZ</p></td>
</tr>
<tr>
<td><p>Llama</p></td>
<td><p>Open, no commercial use</p></td>
<td><p>7B, 13B, 33B, 65B</p></td>
<td><p>-</p></td>
</tr>
<tr>
<td><p>Llama 2</p></td>
<td><p>Open, commercial use</p></td>
<td><p>7B, 13B, 70B</p></td>
<td><p>Llama 2-Chat</p></td>
</tr>
<tr>
<td><p>Zephyr</p></td>
<td><p>Apache 2.0</p></td>
<td><p>7B</p></td>
<td><p>-</p></td>
</tr>
<tr>
<td><p>Gemma</p></td>
<td><p>Open, restricted use</p></td>
<td><p>2B, 7B</p></td>
<td><p>Gemma-Instruction Tuned</p></td>
</tr>
</tbody>
</table>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="How to Choose an LLM for Your Task"><div class="sect1" id="id242">
<h1>How to Choose an LLM for Your Task</h1>

<p>Given the plethora of options available, how do you ensure you choose<a data-type="indexterm" data-primary="use cases" data-secondary="choosing an LLM" id="xi_usecaseschoosinganLLM522870"/> the right LLM for your task? Depending on your situation, there are a multitude of criteria to consider, including:</p>
<dl>
<dt>Cost</dt>
<dd>
<p>This includes inference or fine-tuning costs, and costs associated with building software scaffolding, monitoring and observability, deployment and maintenance (collectively referred to as LLMOps).</p>
</dd>
<dt><a href="https://oreil.ly/mEDRt">Time per output token (TPOT)</a></dt>
<dd>
<p>This is a metric used to measure the speed of text generation as experienced by the end user.</p>
</dd>
<dt>Task performance</dt>
<dd>
<p>This refers to the performance requirements of the task and the relevant metrics like precision or accuracy. What level of performance is <em>good enough</em>?</p>
</dd>
<dt>Type of tasks</dt>
<dd>
<p>The nature of the tasks the LLM will be used for, like summarization, question answering, classification, etc.</p>
</dd>
<dt>Capabilities required</dt>
<dd>
<p>Examples of capabilities include arithmetic reasoning, logical reasoning, planning, task decomposition, etc. A lot of these capabilities, to the extent that they actually exist or approximate, are <em>emergent properties</em> of an LLM as discussed in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, and are not exhibited by smaller models.</p>
</dd>
<dt>Licensing</dt>
<dd>
<p>You can use only those models that allow your mode of usage. Even models that explicitly allow commercial use can have restrictions on certain types of use cases. For example, as noted earlier, the Big Science OpenRAIL-M license restricts the usage of the LLM in use cases pertaining to law enforcement, immigration, or asylum processes.</p>
</dd>
<dt>In-house ML/MLOps talent</dt>
<dd>
<p>The strength of in-house talent determines the customizations you can afford. For example, do you have enough in-house talent for building inference optimization systems?</p>
</dd>
<dt>Other nonfunctional criteria</dt>
<dd>
<p>This includes safety, security, privacy, etc. Cloud providers and startups are already implementing solutions that can address these issues.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id918">
<h1>Exercise</h1>
<p>For your application, prepare an ordered list of priorities and determine which ones are fixed and which ones are flexible. For example, precision needs to be at least X or TPOT needs to be at least Y.</p>

<p>Based on the determined priorities, what LLM would you choose?</p>
</div></aside>

<p>You may have to choose between proprietary and open source LLMs.</p>








<section data-type="sect2" data-pdf-bookmark="Open Source Versus Proprietary LLMs"><div class="sect2" id="id73">
<h2>Open Source Versus Proprietary LLMs</h2>

<p>Debates about the merits of open source versus proprietary<a data-type="indexterm" data-primary="use cases" data-secondary="open source LLMs" id="id919"/><a data-type="indexterm" data-primary="open source LLMs" id="id920"/><a data-type="indexterm" data-primary="use cases" data-secondary="proprietary LLMs" id="id921"/> software have been commonplace in the tech industry for several decades now, and we are seeing it become increasingly relevant in the realm of LLMs as well. The biggest advantage of open source models are the transparency and flexibility they provide, not necessarily the cost.
Self-hosting open source LLMs can incur a lot of engineering overhead and compute/memory costs, and using managed services might not always be able to match proprietary models in terms of latency, throughput, and inference cost. Moreover, many open source LLMs are not easily accessible through managed services and other third-party deployment options. This situation is bound to change dramatically as the field matures, but in the meanwhile, run through your calculations for your specific situation to determine the costs incurred for using each (type of) model.</p>

<p>The flexibility provided by open source models helps with your ability to debug, interpret, and augment the LLM with any kind of training/fine-tuning you choose, instead of the restricted avenues made available by the LLM provider. This allows you to more substantially align the LLM to your preferences and values instead of the ones decided by the LLM provider. Having full availability of all the token probabilities (logits) is a superpower, as we will see throughout the book.</p>

<p>The availability of open source LLMs has enabled teams to develop models and applications that might not be lucrative for larger companies with a profit motive, like fine-tuning models to support low-resource languages (languages that do not have a significant data footprint on the internet, like regional languages of India or Indigenous languages of Canada). An example is the <a href="https://oreil.ly/hoBQ1">Kannada Llama model</a>,  built over Llama 2 by continually pre-training and fine-tuning on tokens from the Kannada language, a regional language of India.</p>

<p>Not all open source models are fully transparent. As mentioned earlier, most for-profit companies that release open source LLMs do not make the training datasets public. For instance, Meta hasn’t disclosed all the details of the training datasets used to train the Llama 2 model. Knowing which datasets are used to train the model can help you assess whether there is test set contamination and understand what kind of knowledge you can expect the LLM to possess.</p>

<p>As of this book’s writing, open source models like Llama 3.2 and DeepSeek v3 have more or less caught up to state-of-the-art proprietary models from OpenAI or Anthropic. However, there is a new gap developing between proprietary and open source models in the realm of reasoning models like OpenAI’s o3, that use inference-time compute techniques (discussed in <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>). Throughout this book, we will showcase scenarios where open source models have an advantage.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Always check if the model provider has an active developer community on GitHub/Discord/Slack, and that the development team is actively engaged in those channels, responding to user comments and questions. I recommend preferring models with active developer communities, provided they satisfy your primary criteria.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="LLM Evaluation"><div class="sect2" id="id243">
<h2>LLM Evaluation</h2>

<p>We will start this section with a caveat: evaluating LLMs<a data-type="indexterm" data-primary="use cases" data-secondary="evaluating individual LLMs" id="xi_usecasesevaluatingindividualLLMs527558"/><a data-type="indexterm" data-primary="benchmarking" data-secondary="evaluating LLMs" id="xi_benchmarkingevaluatingLLMs527558"/> is probably the most challenging task in the LLM space at present. Current methods of benchmarking are broken, easily gamed, and hard to interpret. Nevertheless, benchmarks are still a useful starting point on your road to evaluation. We will start by looking at current public benchmarks and then discuss how you can build more holistic internal benchmarks.</p>

<p>To evaluate LLMs on their task performance, there are a lot of benchmark datasets that test a wide variety of skills. Not all skills are relevant to your use case, so you can choose to focus on specific benchmarks that test the skills you need the LLM to perform well on.</p>

<p>The leaderboard on these benchmark tests changes very often, especially if only open source models are being evaluated, but that does not mean you need to change the LLMs you use every time there is a new leader on the board. Usually, the differences between the top models are quite marginal. The fine-grained choice of LLM usually isn’t the most important criteria determining the success of your task, and you are better off spending that bandwidth working on cleaning and understanding your data, which is still the most important component of the project.</p>

<p>Let’s look at a few popular ways in which the field is evaluating LLMs.</p>










<section data-type="sect3" data-pdf-bookmark="Eleuther AI LM Evaluation Harness"><div class="sect3" id="id74">
<h3>Eleuther AI LM Evaluation Harness</h3>

<p>Through the <a href="https://oreil.ly/SiOXq">LM Evaluation Harness</a>, Eleuther AI<a data-type="indexterm" data-primary="Eleuther AI" data-secondary="evaluation harness" id="xi_EleutherAIevaluationharness5285100"/> supports benchmarking on over 400 different benchmark tasks, evaluating skills as varied as open-domain question answering, arithmetic and logical reasoning, linguistic tasks, machine translation, toxic language detection, etc. You can use this tool to evaluate any model<a data-type="indexterm" data-primary="Hugging Face" data-secondary="evaluating LLMs on" id="id922"/> on the <a href="https://oreil.ly/IHd22">Hugging Face Hub</a>, a platform containing thousands of pre-trained and fine-tuned models, on the benchmarks of your choice.</p>

<p>Here is an example from <code>bigbench_formal_fallacies_syllogisms_negation</code>, one of the benchmark tasks:</p>

<pre data-type="programlisting" data-code-language="python"> <code class="p">{</code>
    <code class="s2">"input"</code><code class="p">:</code> <code class="s2">"</code><code class="se">\"</code><code class="s2">Some football fans admire various clubs, others love</code><code class="w"/>
    <code class="n">only</code> <code class="n">a</code> <code class="n">single</code> <code class="n">team</code><code class="o">.</code> <code class="n">But</code> <code class="n">who</code> <code class="ow">is</code> <code class="n">a</code> <code class="n">fan</code> <code class="n">of</code> <code class="n">whom</code> <code class="n">precisely</code><code class="err">?</code> <code class="n">The</code>
    <code class="n">following</code> <code class="n">argument</code> <code class="n">pertains</code> <code class="n">to</code> <code class="n">this</code> <code class="n">question</code><code class="p">:</code> <code class="n">First</code> <code class="n">premise</code><code class="p">:</code> <code class="n">Mario</code>
    <code class="ow">is</code> <code class="n">a</code> <code class="n">friend</code> <code class="n">of</code> <code class="n">FK</code> \<code class="n">u017dalgiris</code> <code class="n">Vilnius</code><code class="o">.</code> <code class="n">Second</code> <code class="n">premise</code><code class="p">:</code> <code class="n">Being</code> <code class="n">a</code>
    <code class="n">follower</code> <code class="n">of</code> <code class="n">F</code><code class="o">.</code><code class="n">C</code><code class="o">.</code> <code class="n">Copenhagen</code> <code class="ow">is</code> <code class="n">necessary</code> <code class="k">for</code> <code class="n">being</code> <code class="n">a</code> <code class="n">friend</code> <code class="n">of</code> <code class="n">FK</code>
    \<code class="n">u017dalgiris</code> <code class="n">Vilnius</code><code class="o">.</code> <code class="n">It</code> <code class="n">follows</code> <code class="n">that</code> <code class="n">Mario</code> <code class="ow">is</code> <code class="n">a</code> <code class="n">follower</code> <code class="n">of</code> <code class="n">F</code><code class="o">.</code><code class="n">C</code><code class="o">.</code>
    <code class="n">Copenhagen</code><code class="o">.</code>\<code class="s2">"</code><code class="se">\n</code><code class="s2"> Is the argument, given the explicitly stated</code><code class="w"/>
    <code class="n">premises</code><code class="p">,</code> <code class="n">deductively</code> <code class="n">valid</code> <code class="ow">or</code> <code class="n">invalid</code><code class="err">?</code><code class="s2">",</code><code class="w"/>
    <code class="s2">"target_scores"</code><code class="p">:</code> <code class="p">{</code>
        <code class="s2">"valid"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code>
        <code class="s2">"invalid"</code><code class="p">:</code> <code class="mi">0</code>
    <code class="p">}</code></pre>

<p>In this task, the model is asked to spot logical fallacies by deducing whether the presented argument is valid given the premises.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id923">
<h1>Exercise</h1>
<p>Let’s evaluate a few models on this task. Follow the <a href="https://oreil.ly/mZdGA">instructions</a> to install the harness. Now, you can run this code for evaluating Falcon 7B:</p>

<pre data-type="programlisting" data-code-language="python">   <code class="n">lm_eval</code> <code class="o">--</code><code class="n">model</code> <code class="n">hf</code><code class="o">-</code><code class="n">causal</code> \
           <code class="o">--</code><code class="n">model_args</code> <code class="n">pretrained</code><code class="o">=</code><code class="n">tiiuae</code><code class="o">/</code><code class="n">falcon</code><code class="o">-</code><code class="mi">7</code><code class="n">b</code> \
           <code class="o">--</code><code class="n">tasks</code> <code class="n">bigbench_formal_fallacies_syllogisms_negation</code> \
           <code class="o">--</code><code class="n">device</code> <code class="n">cuda</code><code class="p">:</code><code class="mi">0</code></pre>

<p>Try this for a few other 7B models, including Llama, Gemma, Mistral, MPT, RedPajama with both the base versions and the instruction-tuned versions where available. Do you find a large difference between their models in terms of performance?</p>

<p>Additionally, prepare ten more questions for the same task on your own (you can use an LLM to generate candidate questions you can then modify) pertaining to various domains. Do the models exhibit the same level of performance on your questions as they do on the benchmark tests?</p>
</div></aside>

<p>There is also support for evaluation of proprietary models using this harness. For example, here is how you would evaluate OpenAI models:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">export</code> <code class="n">OPENAI_API_SECRET_KEY</code><code class="o">=&lt;</code><code class="n">Key</code><code class="o">&gt;</code>
<code class="n">python</code> <code class="n">main</code><code class="o">.</code><code class="n">py</code> \
<code class="n">lm_eval</code> <code class="o">--</code><code class="n">model</code> <code class="n">openai</code><code class="o">-</code><code class="n">completions</code> \
        <code class="o">--</code><code class="n">model_args</code> <code class="n">model</code><code class="o">=</code><code class="n">gpt</code><code class="o">-</code><code class="mf">3.5</code><code class="o">-</code><code class="n">turbo</code> \
         <code class="o">--</code><code class="n">tasks</code> <code class="n">bigbench_formal_fallacies_syllogisms_negation</code></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id924">
<h1>Exercise</h1>
<p>Compare GPT 4o, 4o-mini, o1, and o3 on the logical fallacies task, including both the benchmark sets and the ones you prepared. How do they compare relative to each other and how do they fare compared to open source models?</p>
</div></aside>
<div data-type="tip"><h6>Tip</h6>
<p>While choosing or developing a benchmarking task to evaluate, I recommend focusing on picking ones that test the capabilities needed to solve the task of your interest, rather than the actual task itself. For example, if you are building a summarizer application that needs to perform a lot of logical reasoning to generate the summaries, it is better to focus on benchmark tests that directly test logical reasoning capabilities than ones that test summarization performance<a data-type="indexterm" data-startref="xi_EleutherAIevaluationharness5285100" id="id925"/>.</p>
</div>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Hugging Face Open LLM Leaderboard"><div class="sect3" id="id75">
<h3>Hugging Face Open LLM Leaderboard</h3>

<p>As of the book’s writing, the <a href="https://oreil.ly/tspBY">Open LLM Leaderboard</a> uses Eleuther AI’s LM Evaluation Harness<a data-type="indexterm" data-primary="Hugging Face" data-secondary="Open LLM Leaderboard" id="xi_HuggingFaceOpenLLMLeaderboard5359132"/><a data-type="indexterm" data-primary="Open LLM Leaderboard" id="xi_OpenLLMLeaderboard5359132"/><a data-type="indexterm" data-primary="Eleuther AI" data-secondary="Open LLM Leaderboard" id="xi_EleutherAIOpenLLMLeaderboard5359132"/> to evaluate the performance of models on six benchmark tasks:</p>
<dl>
<dt>Massive Multitask Language Understanding (MMLU)</dt>
<dd>
<p>This test evaluates the LLM<a data-type="indexterm" data-primary="Massive Multitask Language Understanding (MMLU)" id="id926"/><a data-type="indexterm" data-primary="MMLU (Massive Multitask Language Understanding)" id="id927"/> on knowledge-intensive tasks, drawing from fields like US history, biology, mathematics, and more than 50 other subjects in a multiple choice framework.</p>
</dd>
<dt>AI2 Reasoning Challenge (ARC)</dt>
<dd>
<p>This test evaluates<a data-type="indexterm" data-primary="AI2 Reasoning Challenge (ARC)" id="id928"/><a data-type="indexterm" data-primary="ARC (AI2 Reasoning Challenge)" id="id929"/> the LLM on multiple-choice grade school science questions that need complex reasoning as well as world knowledge to answer.</p>
</dd>
<dt>Hellaswag</dt>
<dd>
<p>This test evaluates<a data-type="indexterm" data-primary="Hellaswag" id="id930"/> commonsense reasoning by providing the LLM with a situation and asking it to predict what might happen next out of the given choices, based on common sense.</p>
</dd>
<dt>TruthfulQA</dt>
<dd>
<p>This test evaluates<a data-type="indexterm" data-primary="TruthfulQA" id="id931"/> the LLM’s ability to provide answers that don’t contain 
<span class="keep-together">falsehoods</span>.</p>
</dd>
<dt>Winogrande</dt>
<dd>
<p>This test<a data-type="indexterm" data-primary="Winogrande" id="id932"/> is composed of fill-in-the-blank questions that test commonsense 
<span class="keep-together">reasoning</span>.</p>
</dd>
<dt>GSM8K</dt>
<dd>
<p>This test<a data-type="indexterm" data-primary="GSM8K" id="id933"/> evaluates the LLM’s ability to complete grade school math problems involving a sequence of basic arithmetic operations.</p>
</dd>
</dl>

<p><a data-type="xref" href="#llm-leaderboard">Figure 5-3</a> shows a snapshot of the LLM leaderboard as of the time of the book’s writing. We can see that:</p>

<ul>
<li>
<p>Larger models perform better.</p>
</li>
<li>
<p>Instruction-tuned or fine-tuned variants of models perform better.</p>
</li>
</ul>

<figure><div id="llm-leaderboard" class="figure">
<img src="assets/dllm_0503.png" alt="Snapshot of the Open LLM Leaderboard" width="600" height="327"/>
<h6><span class="label">Figure 5-3. </span>Snapshot of the Open LLM Leaderboard</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id934">
<h1>Exercise</h1>
<p>It is <a href="https://oreil.ly/fwVEC">suspected</a> that a large number of models may have been trained on data contaminated with the GSM8K dataset. Explore the <a href="https://oreil.ly/dI87I">GSM8K dataset</a>, feed only part of the question, and check if the models you evaluated during the previous exercises correctly complete the question. Also, change the numbers in the problems and verify if the performance remains the same.</p>
</div></aside>

<p>The validity of these benchmarks are in question as complete test set decontamination is not guaranteed. Model providers are also optimizing to solve these benchmarks, thus reducing the value of these benchmarks to serve as reliable estimators of general-purpose performance<a data-type="indexterm" data-startref="xi_HuggingFaceOpenLLMLeaderboard5359132" id="id935"/><a data-type="indexterm" data-startref="xi_OpenLLMLeaderboard5359132" id="id936"/><a data-type="indexterm" data-startref="xi_EleutherAIOpenLLMLeaderboard5359132" id="id937"/>.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="HELM"><div class="sect3" id="id76">
<h3>HELM</h3>

<p><a href="https://oreil.ly/MNHDs">Holistic Evaluation of Language Models (HELM)</a> is an evaluation framework<a data-type="indexterm" data-primary="Holistic Evaluation of Language Models (HELM)" id="id938"/> by Stanford that aims to calculate a wide variety of metrics over a range of benchmark tasks. Fifty-nine metrics are calculated overall, testing accuracy, calibration, robustness, fairness, bias, toxicity, efficiency, summarization performance, copyright infringement, and more. The tasks tested include question answering, summarization, text classification, information retrieval, sentiment analysis, and toxicity detection.</p>

<p><a data-type="xref" href="#helm-leaderboard">Figure 5-4</a> shows a snapshot of the HELM leaderboard as of the time of the book’s writing.</p>

<figure><div id="helm-leaderboard" class="figure">
<img src="assets/dllm_0504.png" alt="Snapshot of the HELM leaderboard" width="600" height="327"/>
<h6><span class="label">Figure 5-4. </span>Snapshot of the HELM leaderboard</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id939">
<h1>Benchmark Evaluation Is Unreliable</h1>
<p>You can evaluate the same task in multiple ways. For example, consider the MMLU task. Questions in the MMLU task have four choices as answers: A, B, C, D. How do we evaluate performance on a multiple choice question-answering task?</p>

<ul>
<li>
<p>You can pick the token that has the highest output probability<a data-type="indexterm" data-primary="output probability" id="id940"/> out of the four options (A, B, C, D).</p>
</li>
<li>
<p>You can pick the token that has the highest output probability from the entire vocabulary and use that to match it with the correct answer to the question (not the label like A, B, C, D, but the actual answer).</p>
</li>
<li>
<p>You can produce a normalized sum of the probabilities of the token sequence generated by the model, where the expected token sequence is the label followed by the answer text, and use that to match it with the correct answer (represented by the label followed by answer text).</p>
</li>
</ul>

<p>Each of these types of calculations can produce a vastly different result and can lead to different leaders in the leaderboard. <a href="https://oreil.ly/QrBX4">Hugging Face published a blog post</a> about this after people noticed discrepancies in their numbers versus third-party evaluations.</p>
</div></aside>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Elo Rating"><div class="sect3" id="id77">
<h3>Elo Rating</h3>

<p>Now that we have seen the limitations of quantitative evaluation<a data-type="indexterm" data-primary="Elo rating system" id="xi_ELoratingsystem541665"/>, let’s explore how we can most effectively incorporate human evaluations. One promising framework is the <a href="https://oreil.ly/bTD7I">Elo rating system</a>, used in chess to rank players.</p>

<p><a href="https://oreil.ly/HGVz2">Large model systems organization (LMSYS Org)</a> has implemented an evaluation platform<a data-type="indexterm" data-primary="large model systems organization (LMSYS Org)" id="id941"/><a data-type="indexterm" data-primary="LMSYS Org (large model systems organization)" id="id942"/> based on the Elo rating system called<a data-type="indexterm" data-primary="Chatbot Arena" id="id943"/> the <a href="https://oreil.ly/evgQX">Chatbot Arena</a>. Chatbot Arena solicits crowdsourced evaluations by inviting people to choose between two randomized and anonymized LLMs by chatting with them side-by-side. The leaderboard is found <a href="https://oreil.ly/Y6zmN">online</a>, with models from OpenAi, DeepSeek, Google DeepMind, and Anthropic 
<span class="keep-together">dominating</span>.</p>

<p><a data-type="xref" href="#chatbotarena-leaderboard">Figure 5-5</a> shows a snapshot of the Chatbot Arena leaderboard as of the time of the book’s writing.</p>

<figure><div id="chatbotarena-leaderboard" class="figure">
<img src="assets/dllm_0505.png" alt="Snapshot of the Chatbot Arena leaderboard" width="600" height="340"/>
<h6><span class="label">Figure 5-5. </span>Snapshot of the Chatbot Arena leaderboard</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id944">
<h1>Elo Ratings Can Be Biased Too</h1>
<p>Elo ratings are not a panacea to the LLM evaluation problem. Human biases<a data-type="indexterm" data-primary="bias and fairness issues" data-secondary="and Elo rating" data-secondary-sortas="Elo rating" id="id945"/> can meaningfully impact the overall ratings even if the LLMs are being evaluated 
<span class="keep-together">anonymously</span>.</p>

<p>According to <a href="https://oreil.ly/3g_qd">Wu et al.</a>, these biases include:</p>

<ul>
<li>
<p>Humans tend to prefer longer texts.</p>
</li>
<li>
<p>Humans tend to overlook subtle factuality and consistency issues if the style is authoritative or convincing.</p>
</li>
<li>
<p>Humans can be indecisive and tend to grant ties instead of choosing a winner.</p>
</li>
<li>
<p>The order in which the LLM answers are presented can influence human ratings. This can be rectified by providing randomized answers to the user.</p>
</li>
</ul>

<p>Wu et al. propose a multi-Elo rating system that asks humans to evaluate the LLM across three different dimensions: helpfulness, accuracy, and language<a data-type="indexterm" data-startref="xi_ELoratingsystem541665" id="id946"/>.</p>
</div></aside>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Interpreting benchmark results"><div class="sect3" id="id78">
<h3>Interpreting benchmark results</h3>

<p>How do you interpret evaluation results presented in research papers? Try to methodically ask as many questions as possible, and check if the answers are covered in the paper or other material. As an example, let us take the Llama 2-chat evaluation graphs presented in the <a href="https://oreil.ly/BcgXs">Llama 2 paper</a>. In particular, study Figures 1 and 3, which demonstrate how Llama 2-Chat compares in helpfulness and safety with other chat models. Some of the questions that come to mind are:</p>

<ul>
<li>
<p>What does the evaluation dataset look like? Do we have access to it?</p>
</li>
<li>
<p>What is the difficulty level of the test set? Maybe the model is competitive with respect to ChatGPT for easier examples but how does it perform with more difficult examples?</p>
</li>
<li>
<p>What proportion of examples in the test set can be considered difficult?</p>
</li>
<li>
<p>What kinds of scenarios are covered in the test set? What degree of overlap do these scenarios have with the chat-tuning sets?</p>
</li>
<li>
<p>What definition do they use for safety?</p>
</li>
<li>
<p>Can there be a bias in the evaluation due to models being evaluated on the basis of a particular definition of safety, which Llama 2 was trained to adhere to, while other models may have different definitions of safety?</p>
</li>
</ul>

<p>Rigorously interrogating the results this way helps you develop a deeper understanding of what is being evaluated, and whether it aligns with the capabilities you need from the language model for your own tasks. For more rigorous LLM evaluation, I strongly recommend developing your own internal benchmarks.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Do not trust evaluations performed by GPT-4 or any other LLM. We have no idea what evaluation criteria it uses nor do we have a deeper understanding of its biases.</p>
</div>

<p>Robust evaluation of LLMs is further complicated by the sensitivity of the prompts and the probabilistic nature of generative models. For example, I often see papers claiming<a data-type="indexterm" data-primary="reasoning" data-secondary="issues for generative AI" id="id947"/><a data-type="indexterm" data-primary="OpenAI" data-secondary="GPT-4" id="id948"/><a data-type="indexterm" data-primary="GPT-4" id="id949"/><a data-type="indexterm" data-primary="generative AI" data-secondary="reasoning issues for" id="id950"/><a data-type="indexterm" data-primary="prompting" data-secondary="reasoning and prompt sensitivity" id="id951"/> that “GPT-4 does not have reasoning capabilities,” while not using any prompting techniques during evaluation. In many of these cases, it turns out that the model can in fact perform the task if prompted with CoT prompting<a data-type="indexterm" data-startref="xi_usecaseschoosinganLLM522870" id="id952"/>. While evaluation prompts need not be heavily engineered, using rudimentary techniques like CoT should be standard practice, and not using them means that the model capabilities are being underestimated<a data-type="indexterm" data-startref="xi_usecasesevaluatingindividualLLMs527558" id="id953"/><a data-type="indexterm" data-startref="xi_benchmarkingevaluatingLLMs527558" id="id954"/>.</p>
</div></section>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Loading LLMs"><div class="sect1" id="id79">
<h1>Loading LLMs</h1>

<p>While it is possible to load and run inference on LLMs<a data-type="indexterm" data-primary="use cases" data-secondary="loading LLMs" id="xi_usecasesloadingLLMs547555"/> with just CPUs, you need GPUs<a data-type="indexterm" data-primary="GPUs, for loading LLMs" id="id955"/> if you want acceptable text generation speeds. Choosing a GPU depends on cost, the size of the model, whether you are training the model or just running inference, and support for optimizations. Tim Dettmers<a data-type="indexterm" data-primary="Dettmers, Tim" id="id956"/> has developed a great <a href="https://oreil.ly/t6iPQ">flowchart</a> that you can use to figure out which GPU best serves your needs.</p>

<p>Let’s figure out the amount of GPU RAM needed to load an LLM of a given size. LLMs can be loaded in various <em>precisions</em>:</p>
<dl>
<dt>Float32</dt>
<dd>
<p>32-bit floating point<a data-type="indexterm" data-primary="Float32 precision, LLM loading" id="id957"/> representation, each parameter occupying 4 bytes of storage.</p>
</dd>
<dt>Float16</dt>
<dd>
<p>16-bit floating point<a data-type="indexterm" data-primary="Float16 precision, LLM loading" id="id958"/> representation. Only 5 bits are reserved for the exponent as opposed to 8 bits in Float32. This means that using Float16 comes with overflow/underflow problems for very large and small numbers.</p>
</dd>
<dt>bfloat16 (BF16)</dt>
<dd>
<p>16-bit floating point<a data-type="indexterm" data-primary="bfloat16 (BF16)" id="id959"/> representation. Just like Float32, 8 bits are reserved for the exponent, thus alleviating the underflow/overflow problems observed in Float16.</p>
</dd>
<dt>Int8</dt>
<dd>
<p>8-bit integer<a data-type="indexterm" data-primary="Int8 precision, LLM loading" id="id960"/> representation. Running inference in 8-bit mode is around 20% slower than running in Float16.</p>
</dd>
<dt>FP8, FP4</dt>
<dd>
<p>8-bit and 4-bit <a data-type="indexterm" data-primary="FP8 precision, LLM loading" id="id961"/><a data-type="indexterm" data-primary="FP4 precision, LLM loading" id="id962"/>floating point representation.</p>
</dd>
</dl>

<p>We will explore these formats in detail in <a data-type="xref" href="ch09.html#ch09">Chapter 9</a>. Generally, running inference on a model with 7B parameters will need around 7 GB of GPU RAM if running in 8-bit mode and around 14 GB if running in BF16. If you intend to fine-tune the whole model, you will need a lot more memory.</p>








<section data-type="sect2" data-pdf-bookmark="Hugging Face Accelerate"><div class="sect2" id="id80">
<h2>Hugging Face Accelerate</h2>

<p>You can run inference on models<a data-type="indexterm" data-primary="Hugging Face" data-secondary="Accelerate" id="id963"/><a data-type="indexterm" data-primary="Accelerate" id="id964"/> even if they don’t fit in the GPU RAM. The <a href="https://oreil.ly/OYdyf"><em>accelerate</em> library</a> by Hugging Face facilitates this by loading parts of the model into CPU RAM if the GPU RAM is filled, and then loading parts of the model into disk if the CPU RAM is also filled. <a href="https://oreil.ly/J8duc">“Accelerate Big Model Inference: How Does it Work?”</a> shows how the accelerate library operates under the hood. This whole process is abstracted from the user, so all you need to load a large model is to run this code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">transformers</code> <code class="n">accelerate</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoTokenizer</code><code class="p">,</code> <code class="n">AutoModelForCausalLM</code>
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"EleutherAI/gpt-neox-20B"</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">GPTNeoForCausalLM</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"EleutherAI/gpt-neox-20B"</code><code class="p">)</code>
<code class="n">input_ids</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="s2">"Language models are"</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code>
<code class="n">gen_tokens</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="o">**</code><code class="n">input_ids</code><code class="p">,</code> <code class="n">max_new_tokens</code> <code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Ollama"><div class="sect2" id="id81">
<h2>Ollama</h2>

<p>There are many tools available that facilitate loading LLMs locally, including on your own laptop. One such library is Ollama<a data-type="indexterm" data-primary="Ollama" id="id965"/>, which supports Windows, Mac, and Linux operating systems. Using Ollama, you can load 13B models if your machine has at least 16GB of available RAM. Ollama supports many open models like Mistral, Llama, Gemma, etc. Ollama provides a REST API that you can use to run inference and build LLM-driven applications. It also has several Terminal and UI integrations that enable you to build user-facing applications with ease.</p>

<p>Let’s see how we can use Google’s Gemma 2B model<a data-type="indexterm" data-primary="Gemma 2B model (Google)" id="id966"/> using Ollama. First, download <a href="https://oreil.ly/yly44">the version of Ollama</a> to your machine based on your operating system. Next, pull the Gemma model to your machine with:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ollama</code> <code class="n">pull</code> <code class="n">gemma</code><code class="p">:</code><code class="mi">2</code><code class="n">b</code></pre>

<p>You can also create a Modelfile that contains configuration information for the model. This includes system prompts and prompt templates, decoding parameters like temperature, and conversation history. Refer to the <a href="https://oreil.ly/ba-1u">documentation</a> for a full list of available options.</p>

<p>An example Modelfile is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">FROM</code> <code class="n">gemma</code><code class="p">:</code><code class="mi">2</code><code class="n">b</code>

<code class="n">PARAMETER</code> <code class="n">temperature</code> <code class="mf">0.2</code>

<code class="n">SYSTEM</code> <code class="s2">"""</code>
<code class="s2">You are a provocateur who speaks only in limericks.</code>
<code class="s2">"""</code></pre>

<p>After creating your Modelfile, you can run the model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ollama</code> <code class="n">create</code> <code class="n">local</code><code class="o">-</code><code class="n">gemma</code> <code class="o">-</code><code class="n">f</code> <code class="o">./</code><code class="n">Modelfile</code>
<code class="n">ollama</code> <code class="n">run</code> <code class="n">local</code><code class="o">-</code><code class="n">gemma</code></pre>

<p>The book’s GitHub repo contains a sample end-to-end application built using Ollama and one of its UI integrations. You can also experiment with similar tools like <a href="https://oreil.ly/uFsiR">LM Studio</a> and <a href="https://oreil.ly/XUXhq">GPT4All</a>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can load custom models using Ollama if they are in the GPT-Generated Unified Format (GGUF).</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="LLM Inference APIs"><div class="sect2" id="id82">
<h2>LLM Inference APIs</h2>

<p>While you can deploy an LLM yourself, modern-day inference<a data-type="indexterm" data-primary="LLM inference APIs" id="id967"/> consists of so many optimizations, many of them proprietary, that it takes a lot of effort to bring your inference speeds up to par with commercially available solutions. Several inference services<a data-type="indexterm" data-primary="Together AI" id="id968"/> like <a href="https://oreil.ly/L3zo0">Together AI</a> exist that facilitate inference of open source or custom models either through serverless endpoints or dedicated instances. Another option<a data-type="indexterm" data-primary="Hugging Face" data-secondary="TGI" id="id969"/><a data-type="indexterm" data-primary="Text Generation Inference (TGI)" id="id970"/> is Hugging Face’s <a href="https://oreil.ly/XXFpa">TGI (Text Generation Inference)</a>, which has been recently <a href="https://oreil.ly/BJJlY">reinstated</a> to a permissive open source license<a data-type="indexterm" data-startref="xi_usecasesloadingLLMs547555" id="id971"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Decoding Strategies"><div class="sect1" id="id83">
<h1>Decoding Strategies</h1>

<p>Now that we have learned how to load a model, let’s understand how to effectively generate text. To this end, several <em>decoding</em> strategies<a data-type="indexterm" data-primary="use cases" data-secondary="decoding strategies" id="xi_usecasesdecodingstrategies5557140"/><a data-type="indexterm" data-primary="decoding strategies" id="xi_decodingstrategies5557140"/> have been devised in the past few years. Let’s go through them in detail.</p>








<section data-type="sect2" data-pdf-bookmark="Greedy Decoding"><div class="sect2" id="id84">
<h2>Greedy Decoding</h2>

<p>The simplest form of decoding is to just generate<a data-type="indexterm" data-primary="greedy decoding" id="id972"/> the token that has the highest probability. The drawback of this approach is that it causes repetitiveness in the output. Here is an example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="nb">input</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="s1">'The keyboard suddenly came to life. It ventured up the'</code><code class="p">,</code>

<code class="n">return_tensors</code><code class="o">=</code><code class="s1">'pt'</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">torch_device</code><code class="p">)</code>
<code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="o">**</code><code class="n">inputs</code><code class="p">,</code> <code class="n">max_new_tokens</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="n">output</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">skip_special_tokens</code><code class="o">=</code><code class="kc">True</code><code class="p">))</code></pre>

<p>You can see that the output starts getting repetitive. Therefore, greedy decoding is not suitable unless you are generating really short sequences, like a token just producing a classification task output.</p>

<p><a data-type="xref" href="#greedy-decoding">Figure 5-6</a> shows an example of greedy decoding using the FLAN-T5 model. Note that we missed out on some great sequences because one of the desired tokens has slightly lower probability, ensuring it never gets picked.</p>

<figure><div id="greedy-decoding" class="figure">
<img src="assets/dllm_0506.png" alt="Greedy decoding" width="600" height="212"/>
<h6><span class="label">Figure 5-6. </span>Greedy decoding</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Beam Search"><div class="sect2" id="id85">
<h2>Beam Search</h2>

<p>An alternative to greedy decoding is beam search<a data-type="indexterm" data-primary="beam search" id="id973"/><a data-type="indexterm" data-primary="search systems" data-secondary="beam search" id="id974"/>. An important parameter of beam search is the beam size, <em>n</em>. At the first step, the top <em>n</em> tokens with the highest probabilities are selected as hypotheses. For the next few steps, the model generates token continuations for each of the hypotheses. The token chosen to be generated is the one whose continuations have the highest cumulative probability.</p>

<p>In the Hugging Face <code>transformers</code> library, the <code>num_beams</code> parameter of the <code>model.generate()</code> function determines the size of the beam. Here is how the decoding code would look if we used beam search:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="o">**</code><code class="n">inputs</code><code class="p">,</code> <code class="n">max_new_tokens</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">num_beams</code> <code class="o">=</code> <code class="mi">3</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="n">output</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">skip_special_tokens</code><code class="o">=</code><code class="kc">True</code><code class="p">))</code></pre>

<p><a data-type="xref" href="#beam-search">Figure 5-7</a> shows an example of beam search using the FLAN-T5 model. Note that the repetitiveness problem hasn’t really been solved using beam search. Similar to greedy decoding, the generated text also sounds very constricted and not humanlike, due to the complete absence of lower probability words.</p>

<figure><div id="beam-search" class="figure">
<img src="assets/dllm_0507.png" alt="Beam search" width="600" height="148"/>
<h6><span class="label">Figure 5-7. </span>Beam search</h6>
</div></figure>

<p>To resolve these issues, we will need to start introducing some randomness and begin sampling from the probability distribution to ensure not just the top two or three tokens get generated all the time.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Top-k Sampling"><div class="sect2" id="id86">
<h2>Top-k Sampling</h2>

<p>In top-k sampling<a data-type="indexterm" data-primary="top-k sampling" id="id975"/>, the model samples from a distribution of just the k tokens of the output distribution that have the highest probability. The probability mass is redistributed over the k tokens, and the model samples from this distribution to generate the next token. Hugging Face provides the <code>top_k</code> parameter in its generate function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="o">**</code><code class="n">inputs</code><code class="p">,</code> <code class="n">max_new_tokens</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">do_sample</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">top_k</code><code class="o">=</code><code class="mi">40</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="n">output</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">skip_special_tokens</code><code class="o">=</code><code class="kc">True</code><code class="p">))</code></pre>

<p class="pagebreak-before"><a data-type="xref" href="#topk-sampling">Figure 5-8</a> shows an example of top-k sampling using the FLAN-T5 model. Note that this is a vast improvement from greedy or beam search. However, top-k leads to problematic generations when used in cases where the probability is dominated by a few tokens, meaning that tokens with very low probability end up being included in the top-k.</p>

<figure><div id="topk-sampling" class="figure">
<img src="assets/dllm_0508.png" alt="Top-k sampling" width="600" height="182"/>
<h6><span class="label">Figure 5-8. </span>Top-k sampling</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Top-p Sampling"><div class="sect2" id="id87">
<h2>Top-p Sampling</h2>

<p>Top-p sampling<a data-type="indexterm" data-primary="top-p sampling" id="id976"/> solves the problem with top-k sampling by making the number of candidate tokens dynamic. Top-p involves choosing the smallest number of tokens whose cumulative distribution exceeds a given probability p. Here is how you can implement this using Hugging Face <code>transformers</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="o">**</code><code class="n">inputs</code><code class="p">,</code> <code class="n">max_new_tokens</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">top_p</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="n">output</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">skip_special_tokens</code><code class="o">=</code><code class="kc">True</code><code class="p">))</code></pre>

<p><a data-type="xref" href="#topp-sampling">Figure 5-9</a> shows an example of top-p sampling using the FLAN-T5 model.
Top-p sampling, also called nucleus sampling, is the most popular sampling strategy used today.</p>

<figure><div id="topp-sampling" class="figure">
<img src="assets/dllm_0509.png" alt="Top-p sampling" width="600" height="219"/>
<h6><span class="label">Figure 5-9. </span>Top-p sampling</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>So far, the decoding approaches we have seen operate serially; i.e., each token is generated one at a time, with a full pass through the model each time. This is too inefficient for latency-sensitive applications. In <a data-type="xref" href="ch09.html#ch09">Chapter 9</a>, we will discuss methods like speculative decoding, which can speed up the decoding process<a data-type="indexterm" data-startref="xi_usecasesdecodingstrategies5557140" id="id977"/><a data-type="indexterm" data-startref="xi_decodingstrategies5557140" id="id978"/>.</p>
</div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Running Inference on LLMs"><div class="sect1" id="id88">
<h1>Running Inference on LLMs</h1>

<p>Now that we have learned how to access and load LLMs<a data-type="indexterm" data-primary="use cases" data-secondary="LLM inference" id="xi_usecasesLLMinference564053"/><a data-type="indexterm" data-primary="LLM inference" id="xi_LLMinference564053"/><a data-type="indexterm" data-primary="inference, running on LLMs" id="xi_inferencerunningonLLMs564053"/><a data-type="indexterm" data-primary="models" data-secondary="inference, running on LLMs" id="xi_modelsinferencerunningonLLMs564053"/> and understood the decoding process, let’s begin using them to solve our tasks. We call this <em>LLM inference</em>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id979">
<h1>Exercise</h1>
<p>You are an intrepid musician embarking on a concert tour comprising seven cities: Amsterdam, Warsaw, Hamburg, Barcelona, Delhi, Shanghai, and Toronto. Ask the LLM if it can come up with a suggested visiting order of cities constituting the shortest travel time. Use prompting techniques and strategies you have learned in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a> to solve this.</p>

<p>Repeat this for multiple LLMs: a 3B LLM, a 7B LLM, an LLM that is at least 30B, and a proprietary LLM API. How easy do you find steering each model to do your 
<span class="keep-together">bidding</span>?</p>

<p>Additionally, the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a> contains multiple example tasks that you can test your prompting skills on. Try them out and see if you can get the LLMs to answer them correctly!</p>
</div></aside>

<p>You will have seen that LLM outputs are not consistent and sometimes differ wildly across multiple generations for the same prompt. As we learned in the section on decoding, unless you are using greedy search or any other deterministic algorithm, the LLM is sampling from a token distribution.</p>

<p>Some ways to make the generation more deterministic is to set the temperature to zero and keeping the random seed for the sampling constant. Even then, you may not be able to guarantee the same (deterministic) outputs every time you send the LLM the same input.</p>

<p>Sources of nondeterminism range from using multi-threading to floating-point rounding errors to use of certain model architectures (for example, it is known that the <a href="https://oreil.ly/pzchE">Sparse MoE architecture</a> produces nondeterministic 
<span class="keep-together">outputs</span>).</p>

<p class="pagebreak-before">Reducing the temperature to zero or close to zero impacts the LLM’s creativity and makes its outputs more predictable, which might not be suitable for many 
<span class="keep-together">applications</span>.</p>

<p>In production settings where reliability is important, you should run multiple generations for the same input and use a technique like majority voting or heuristics to select the right output. This is very important due to the nature of the decoding process; sometimes the wrong tokens can be generated, and since every token generated is a function of the tokens generated before it, the error can be propagated far ahead.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id980">
<h1>Exercise</h1>
<p>For each of the prompting exercises provided, run multiple generations on them and check how the output varies across generations. Does majority voting work well in selecting the correct output?</p>
</div></aside>

<p><a href="https://oreil.ly/wEE8q">Self-consistency</a> is a popular prompting<a data-type="indexterm" data-primary="prompting" data-secondary="self-consistency" id="id981"/><a data-type="indexterm" data-primary="self-consistency" data-secondary="and prompting" data-secondary-sortas="prompting" id="id982"/> technique that uses majority voting in conjunction with CoT prompting. In this technique, we add the CoT prompt “Let’s think step by step” to the input and run multiple generations (reasoning paths). We then use majority voting to select the correct output.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Structured Outputs"><div class="sect1" id="id89">
<h1>Structured Outputs</h1>

<p>We might want the output of the LLM to be in some structured format, so that it can be consumed by other software systems. But this is easier said than done; current LLMs aren’t as controllable as we would like them to be. Some LLMs can be excessively chatty. Ask them to give a Yes/No answer and they respond with “The answer to this question is ‘Yes’.”</p>

<p>One way to get structured outputs from the LLM is to define a JSON schema, provide the schema to the LLM, and prompt it to generate outputs adhering to the schema. For larger models, this works almost all the time, with some schema corruption errors that you can catch and handle.</p>

<p>For smaller models, you can use libraries<a data-type="indexterm" data-primary="Jsonformer" id="id983"/> like <a href="https://oreil.ly/aSc0f">Jsonformer</a>. Jsonformer delegates the generation of the content tokens to the LLM but fills the content in JSON form by itself. Jsonformer is built on top of Hugging Face and thus supports any model that is supported by Hugging Face.</p>
<aside data-type="sidebar" epub:type="sidebar" class="less_space pagebreak-before"><div class="sidebar" id="id984">
<h1>Exercise</h1>
<p>Extract text from the career section of the Wikipedia page of the actor <a href="https://oreil.ly/WXtgc">Andrew Garfield</a>. Design a JSON schema with content types coactors, director, year, and movie name. Use an open source LLM to extract details about his movies from the unstructured text, and use Jsonformer or a similar library to output them in structured form.
Are you able to get fully formed and accurate JSON 
<span class="keep-together">outputs</span>?</p>
</div></aside>

<p>More advanced structured outputs can be facilitated by using libraries<a data-type="indexterm" data-primary="Language Model Query Language (LMQL)" id="id985"/><a data-type="indexterm" data-primary="Guidance library" id="id986"/><a data-type="indexterm" data-primary="LMQL (Language Model Query Language)" id="id987"/><a data-type="indexterm" data-primary="queries" data-secondary="LMQL" id="id988"/> like <a href="https://oreil.ly/LlkEj">LMQL</a> or <a href="https://oreil.ly/cFe5s">Guidance</a>. These libraries provide a programming paradigm for prompting and facilitate controlled generation.</p>

<p>Features available through these libraries include:</p>
<dl>
<dt>Restricting output to a finite set of tokens</dt>
<dd>
<p>This is useful for classification problems, where you have a finite set of output labels. For example, you can restrict the output to be positive, negative, or neutral for a sentiment analysis task.</p>
</dd>
<dt>Controlling output format using regular expressions</dt>
<dd>
<p>For example, you can use regular expressions<a data-type="indexterm" data-primary="regular expressions" id="id989"/> to specify a custom date format.</p>
</dd>
<dt>Control output format using context-free grammars (CFG)</dt>
<dd>
<p>A CFG<a data-type="indexterm" data-primary="context-free grammars (CFG)" id="id990"/><a data-type="indexterm" data-primary="CFG (context-free grammars)" id="id991"/> defines the rules that generated strings need to follow. For more background on CFGs, refer to <a href="https://oreil.ly/M00us">Aditya’s blog</a>. Using CFGs, we can use LLMs to more effectively solve sequence tagging tasks like NER or part-of-speech tagging.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id992">
<h1>Exercise</h1>
<p>Named entity recognition (NER) is a sequence tagging task that tags named entities in text like numbers, dates, places, names, organizations, etc. For example, for the sentence “Padma sold 23 umbrellas in Guatemala,” the tagged output can be in this form:</p>
<blockquote>
<p>Padma: PER</p>

<p>sold:</p>

<p>23: NUM</p>

<p>umbrellas:</p>

<p>in:</p>

<p>Guatemala: LOC</p></blockquote>

<p class="pagebreak-before">where PER is the tag for a person, NUM is the tag for numbers, and LOC is the tag for location.</p>

<p>To generate the tagged output in the above format, use a CFG expression using the <a href="https://oreil.ly/R3XLQ">Guidance library</a>. Run the NER task on the Wikipedia page for the <a href="https://oreil.ly/tUrIt">Summer Olympics</a>. Use a 3B/7B open source LLM to solve this task<a data-type="indexterm" data-startref="xi_usecasesLLMinference564053" id="id993"/><a data-type="indexterm" data-startref="xi_LLMinference564053" id="id994"/><a data-type="indexterm" data-startref="xi_inferencerunningonLLMs564053" id="id995"/><a data-type="indexterm" data-startref="xi_modelsinferencerunningonLLMs564053" id="id996"/>.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Model Debugging and Interpretability"><div class="sect1" id="id90">
<h1>Model Debugging and Interpretability</h1>

<p>Now that we are comfortable with loading LLMs<a data-type="indexterm" data-primary="debugging" id="xi_debugging571546"/><a data-type="indexterm" data-primary="use cases" data-secondary="debugging and interpretability, model" id="xi_usecasesdebuggingandinterpretabilitymodel571546"/><a data-type="indexterm" data-primary="interpretability, model" id="xi_interpretabilitymodel571546"/> and generating text using them, we would like to be able to understand model behavior and explore the examples for which the model fails. Interpretability in LLMs is much less developed than in other areas of machine learning. However, we can get partial interpretability by exploring how the output changes upon minor variances in the input, and by analyzing the intermediate outputs as the inputs propagate through the Transformer architecture.</p>

<p>Google’s open source<a data-type="indexterm" data-primary="LIT-NLP" id="xi_LITNLP571821"/><a data-type="indexterm" data-primary="Google LIT-NLP" id="xi_GoogleLITNLP571821"/> tool <a href="https://oreil.ly/YFY4q">LIT-NLP</a> is a handy tool that supports visualizations of model behavior as well as various debugging workflows.</p>

<p><a data-type="xref" href="#lit-NLP">Figure 5-10</a> shows an example of LIT-NLP in action, providing interpretability for a T5 model running a summarization task.</p>

<figure><div id="lit-NLP" class="figure">
<img src="assets/dllm_0510.png" alt="lit-NLP" width="600" height="288"/>
<h6><span class="label">Figure 5-10. </span>LIT-NLP</h6>
</div></figure>

<p>LIT-NLP features that help you debug your models include:</p>

<ul>
<li>
<p>Visualization of the attention mechanism</p>
</li>
<li>
<p>Salience maps, which show parts of the input that are paid most attention to by the model</p>
</li>
<li>
<p>Visualization of embeddings</p>
</li>
<li>
<p>Counterfactual analysis that shows how your model behavior changes after a change to the input like adding or removing a token.</p>
</li>
</ul>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id997">
<h1>Exercise</h1>
<p>Using the sentences in the Canadian parliamentary proceedings dataset in the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repository</a>, classify the sentences based on the tone of their content. The output labels are supportive, antagonistic, mournful, celebratory, and other.
Use few-shot prompts to provide examples of each label. Use Google’s Gemma model (any flavor will do). You are likely not going to get 100% on your first try. Use LIT-NLP to observe the errors, and see if you can use the interpretability tools to gather insights to improve the model.</p>
</div></aside>

<p>For more details on using LIT-NLP for error analysis, refer to <a href="https://oreil.ly/zcsLu">Google’s tutorial</a> on using LIT-NLP with the Gemma LLM where they find errors in few-shot prompts by analyzing incorrect examples and observing which parts of the prompt contributed most to the output (salience)<a data-type="indexterm" data-startref="xi_LITNLP571821" id="id998"/><a data-type="indexterm" data-startref="xi_GoogleLITNLP571821" id="id999"/>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1000">
<h1>Mechanistic Interpretability</h1>
<p>As seen in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>, the smallest unit<a data-type="indexterm" data-primary="mechanistic interpretability" id="id1001"/> of a Transformer-based LLM is a neuron<a data-type="indexterm" data-primary="neurons (processing units)" id="id1002"/>. Thus, analyzing the behavior of individual neurons in an LLM is a fundamental step toward making LLMs interpretable.</p>

<p>However, in their <a href="https://oreil.ly/hLdVN">experiments</a>, researchers from Anthropic<a data-type="indexterm" data-primary="Anthropic" data-secondary="mechanistic interpretability" id="id1003"/> observed that a single neuron can be activated for many different types of input. Thus, any given neuron’s exact contribution is not entirely clear. The researchers<a data-type="indexterm" data-primary="features, neuron" id="id1004"/> introduced the notion of <em>features</em>, linear combinations of multiple neuron activations. They show that these features are more interpretable than a single neuron, as each feature is activated only on a single type of input. Some features are activated only on a single token, while others are activated on a broader type of input, like code.</p>

<p>For more details, refer to Anthropic’s <a href="https://oreil.ly/hLdVN">mechanistic interpretability paper</a>, where the authors perform experiments on a 1-layer Transformer block and identify features of interest.</p>

<p class="pagebreak-before">You can explore this further by using Anthropic’s <a href="https://oreil.ly/2YvE6">visualization tool</a>, which includes textual descriptions of the tokens for which a neuron gets activated. As an example, <a href="https://oreil.ly/cE27M">they show</a> how each neuron responds when the book <em>Alice in Wonderland</em> is fed as input<a data-type="indexterm" data-startref="xi_usecases5451" id="id1005"/><a data-type="indexterm" data-startref="xi_debugging571546" id="id1006"/><a data-type="indexterm" data-startref="xi_usecasesdebuggingandinterpretabilitymodel571546" id="id1007"/><a data-type="indexterm" data-startref="xi_interpretabilitymodel571546" id="id1008"/>.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id364">
<h1>Summary</h1>

<p>In this chapter, we journeyed through the LLM landscape and noted the various options we have at our disposal. We learned how to determine the criteria most relevant to our tasks and choose the right LLM accordingly. We explored various LLM benchmarks and showed how to interpret their results. We learned how to load LLMs and run inference on them, along with efficient decoding strategies. Finally, we showcased interpretability tools like LIT-NLP that can help us understand what is going on behind the scenes in the Transformer architecture.</p>

<p>In the next chapter, we will learn how to update a model to improve its performance on our tasks of interest. We will walk through a full-fledged fine-tuning example and explore the hyperparameter tuning decisions involved. We will also learn how to construct training datasets for fine-tuning.</p>
</div></section>
</div></section></div>
</div>
</body></html>