<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="ch-ae-vae">14 Latent space and generative modeling, autoencoders, and variational autoencoders</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Representing inputs with latent vectors</li>
<li class="co-summary-bullet">Geometrical view, smoothness, continuity, and regularization for latent spaces</li>
<li class="co-summary-bullet">PCA and linear latent spaces</li>
<li class="co-summary-bullet">Autoencoders and reconstruction loss</li>
<li class="co-summary-bullet">Variational autoencoders (VAEs) and regularizing latent spaces</li>
</ul>
<p class="body"><a id="marker-468"/>Mapping input vectors to a transformed space is often beneficial in machine learning. The transformed vector is called a <i class="fm-italics">latent vector</i>—latent because it is not directly observable—while the input is the underlying <i class="fm-italics">observed vector</i>. The latent vector (aka embedding) is a simpler representation of the input vector where only features that help accomplish the ultimate goal (such as estimating the probability of an input</p>
<p class="body">belonging to a specific class) are retained, and other features are forgotten. Typically, the latent representation has fewer dimensions than the input: that is, encoding an input into a latent vector results in <i class="fm-italics">dimensionality reduction</i>.</p>
<p class="body">The mapping from input to latent space (and vice versa) is usually learned—we train a machine, such as a neural network, to do it. The latent vector needs to be as faithful a representation as possible of the input within the dimensionality allocated to it. So, the neural network is incentivized to minimize the loss of information caused by the transformation. Later, we see that in autoencoders, this is achieved by reconstructing the input from the latent vector and trying to minimize the difference between the actual and reconstructed input. However, given the reduced number of dimensions, the network does not have the luxury of retaining everything in the input. It has to learn what is essential to the end goal and retain only that. Thus the embedding is a compact representation of the input that is streamlined to achieve the ultimate goal.</p>
<h2 class="fm-head" id="geometric-view-of-latent-spaces">14.1 Geometric view of latent spaces</h2>
<p class="body"><a id="marker-469"/>Consider the space of all digital images of height <i class="timesitalic">H</i>, width <i class="timesitalic">W</i>, with each pixel representing a <span class="math">24</span>-bit RGB color value. This is a gigantic space with <span class="math">(2<sup class="fm-superscript">24</sup>)<i class="fm-italics"><sup class="fm-superscript">HW</sup></i></span> points. Every possible <span class="math">RGB × <i class="fm-italics">H</i> × <i class="fm-italics">W</i></span> image is a point in this space. But if an image is a natural image, neighboring points tend to have similar colors. This means points corresponding to natural images are correlated: they are not distributed uniformly over the space of possible images. Furthermore, if the images have a common property (say, they all giraffes), the corresponding points form clusters in the <span class="math">(2<sup class="fm-superscript">24</sup>)<i class="fm-italics"><sup class="fm-superscript">HW</sup></i></span>-sized input space. In stochastic parlance, the probability distribution of natural images with a common property over the space of possible images is highly non-uniform (low entropy).</p>
<p class="body">Figure <a class="url" href="#fig-linear-latent-subspace">14.1a</a> illustrates points with some common property clustered around a planar manifold. Similarly, figure <a class="url" href="#fig-non-inear-latent-subspace">14.1b</a> illustrates points with some common property clustered around a curved manifold. These points have a common property. At the moment, we are not interested in what that property is or whether the manifold is planar or curved. All we care about is that these points of interest are distributed around a manifold. The manifold captures the essence of that common property, whatever it is. If the common property is, say, the presence of a giraffe in the image, then the manifold captures <i class="fm-italics">giraffeness</i>: the points on or near the manifold all correspond to images with giraffes. If we travel along the manifold, we encounter various flavors of giraffe photos. If we go far from the manifold—that is, travel a long distance in a direction orthogonal to the manifold—the probability of the point representing a photo with a giraffe is low.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="446" id="fig-linear-latent-subspace" src="../../OEBPS/Images/CH14_F01a_Chaudhury.png" width="510"/></p>
<p class="figurecaption">(a) Planar latent subspaces</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="455" id="fig-non-inear-latent-subspace" src="../../OEBPS/Images/CH14_F01b_Chaudhury.png" width="521"/></p>
<p class="figurecaption">(b) Curved latent subspace</p>
</div>
<p class="fm-table-caption" id="fig-latent-subspaces">Figure 14.1 Two examples of latent subspaces, with planar and curved manifolds, respectively. The solid line shows the latent vector, and the dashed line represents the information lost by projecting onto the latent subspace.</p>
<p class="body"><a id="marker-470"/>Given training data consisting of sampled points of interest (such as many giraffe photos), we can train a neural network to learn this manifold—it is the optimal manifold that minimizes the average distance of all the training data points from the manifold. Then, at inference time, given an arbitrary input point, we can estimate its distance from the manifold, giving us the probability of that input satisfying the property represented by the manifold.</p>
<p class="body">Thus, the input vector can be decomposed into an in-manifold component (solid line in figure <a class="url" href="#fig-latent-subspaces">14.1</a>) and an orthogonal-to-manifold component (dashed line in figure <a class="url" href="#fig-latent-subspaces">14.1</a>). Latent space modeling effectively eliminates the orthogonal component and retains the in-manifold component as the latent vector (aka embedding). Equivalently, we are projecting the input vector onto the manifold. This is the core idea of latent space modeling—we learn a manifold that represents a property of interest and represents all inputs by a latent vector, which is the input point’s projection onto this manifold. The latent vector is a more compact representation of the input where only information related to the property of interest is retained.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Latent space modeling in a nutshell</p>
<p class="fm-sidebar-text">In latent space modeling, we train a neural network to represent a manifold around which the input points satisfying a property of interest are distributed. The property of interest could be membership in a specific class, such as images containing a giraffe. Thus, the learned manifold is a collection of points that satisfy the property. The input point is projected onto this manifold to obtain a latent vector representation of the input (aka embedding). This is equivalent to throwing away the input vector component that is orthogonal to the manifold. The eliminated component is orthogonal to the manifold and hence unrelated to the property of interest (may represent background pixels of the image), so the information loss caused by the projection does not hurt. We have created a less noisy, more compact representation of the input that focuses on the things we care about.</p>
<p class="fm-sidebar-text">Training data consists of a set of sampled data inputs, all satisfying the property of interest. The system essentially learns the manifold, which is optimally located to minimize its average distance from all the training data points. During inferencing, given an arbitrary input point, its distance from the manifold is an indicator of the probability of that input satisfying the property of interest.</p>
</div>
<p class="body">A subtle point is that the latent vector is the in-manifold component of the original point’s position vector. By switching to the latent vector representation, we lose the location of the point in the original higher-dimensional input space. We can go back to the higher-dimensional space by providing the location of the manifold for the lost orthogonal component, but doing so does not recover the original point: it recovers only the projection of the original point onto the subspace. We are replacing the individual orthogonal components with an aggregate entity the location of a manifold) but do not recover the exact original point. Some information is irretrievably lost during projection.</p>
<p class="body">A special case of latent space representation is principal component analysis (PCA), introduced in section <a class="url" href="../Text/04.xhtml#sec-pca">4.4</a> (section <a class="url" href="#sec-pca-recap">14.4</a> provides a contextual recap of PCAs). It projects input points to an optimal planar latent subspace (as in figure <a class="url" href="#fig-linear-latent-subspace">14.1a</a>). But except for some lucky special cases, the best latent subspace is not a hyperplane. It is a complex curved surface (see figure <a class="url" href="#fig-non-inear-latent-subspace">14.1b</a>). Neural networks, such as autoencoders, can learn such nonlinear projections.<a id="marker-471"/></p>
<h2 class="fm-head" id="sec-generative-classifiers">14.2 Generative classifiers</h2>
<p class="body">During inferencing, the supervised classifiers we have encountered in previous chapters typically emit the class to which an input belongs, perhaps along with a bounding box. This is somewhat black-box-like behavior. We do not know how well the classifier has mastered the space except through the quantized end results. Such classifiers are called <i class="fm-italics">discriminative</i> classifiers. On the other hand, latent space models map arbitrary input points to probabilities of belonging to the class of interest. Such models are called <i class="fm-italics">generative</i> models, and they have some desirable properties:</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre22" height="236" id="fig-discriminative-good-model" src="../../OEBPS/Images/CH14_F02a_Chaudhury.png" width="361"/></p>
<p class="figurecaption">(a) A good discriminative classifier—smooth decision boundary</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre22" height="235" id="fig-discriminative-bad-model" src="../../OEBPS/Images/CH14_F02b_Chaudhury.png" width="358"/></p>
<p class="figurecaption">(b) A bad discriminative classifier—irregular decision boundary</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre22" height="259" id="fig-generative-model" src="../../OEBPS/Images/CH14_F02c_Chaudhury.png" width="398"/></p>
<p class="figurecaption">(c) Generative model—no decision boundary (heat map indicates the probability density)</p>
</div>
<p class="fm-table-caption" id="fig-discriminative-generative-model">Figure 14.2 Solid circles indicate training data points (all belonging to the class of interest). The dashed curve indicates the decision boundary separating the class of interest from the class of non-interest. In a generative model, there is no decision boundary. Every point in the space is associated with a probability of belonging to the class of interest (indicated as a heat map in figure <a class="url" href="#fig-generative-model">14.2c</a>)</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> We can always create a discriminative classifier from a generative classifier by putting a threshold on the probability.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Smoother, denser manifolds</i>—Discriminative models learn decision boundaries separating data points of interest from those not of interest in the input space. On the other hand, generative models try to model the distribution of the data points of interest in the input space using smooth probability density functions. As such, the generative model can't learn a very irregularly shaped function that overfits the training data. This is illustrated in figure <a class="url" href="#fig-discriminative-generative-model">14.2</a>, whereas the discriminative model may converge to a manifold that follows the nooks and bends of the training data too closely (overfits) as in figure~\ref{fig-discriminative-bad-model}. This difference between discriminative and generative classifiers becomes especially significant when we have less training data. We can always create a discriminative classifier from a generative classifier by putting a threshold on the probability.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Extra insight</i>—Generative models offer more insight into the inner workings of the model. Consider a model that recognizes horses. Suppose we feed some horse images to the model, and it calls them horses (good). Then we feed the model some zebra images, and it calls them horses, too (bad). Do we have a useless~model that calls everything a horse? If it is a discriminative model, we must test it with totally different images (say, bird images) to get the answer. But if we have a generative model, it says the probabilities of the true horse images are, say, 0.9 and above, while the probabilities for the zebra images are around 0.7. We begin to see that the model is behaving reasonably and does realize that zebras are less "horsy" than real horses.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">New class instances</i>—A generative model learns the <i class="fm-italics">distribution</i> of input points belonging to the class. An advantage related to learning the distribution is that we can sample the distribution to generate new members of the class (for example, to generate artificial horse images). This leads to the name <i class="fm-italics">generative</i> modes. If we train a generative model with writings of Shakespeare, it will emit Shakespeare-like text pieces. Believe it or not, this has been tried with some success.</p>
</li>
</ul>
<h2 class="fm-head" id="benefits-and-applications-of-latent-space-modeling">14.3 Benefits and applications of latent-space modeling<a id="marker-472"/></h2>
<p class="body">Let’s recap at a high level why we want to do latent-space modeling:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Generative models are often based on latent space models</i>—all the benefits of generative modeling as outlined in section 14.2 apply to latent space modeling too.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Attention to what matters</i>—Redundant information that does not contribute to the end goal is eliminated, and the system focuses on truly discriminative information. To visualize this, imagine an input data set of police mugshots consisting of people standing in front of the same background. Latent-space modeling trained to recognize people typically eliminates the common background from the representation and focuses on the photograph’s subject matter (people).</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Streamlined representation of data</i>—The latent vector is a more compact representation of the input vector (reduced dimensions and hence smaller) with no meaningful information lost.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Noise elimination</i>—Latent-space modeling eliminates the low-variance orthogonal-to-latent-subspace component of the data. This is mostly data that does not help in the problem of interest and hence is noise.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Transformation to a manifold that is friendlier toward the end goal</i>—We have seen this notion previously, but here let’s look at an interesting simple example. Consider a set of <span class="math">2</span>D points in Cartesian coordinates <span class="math">(<i class="fm-italics">x</i>, <i class="fm-italics">y</i>)</span>. Suppose we want to classify the points into two sets: those that lie <i class="fm-italics">inside</i> the circle <span class="math"><i class="fm-italics">x</i><sup class="fm-superscript">2</sup> + <i class="fm-italics">y</i><sup class="fm-superscript">2</sup> = <i class="fm-italics">a</i><sup class="fm-superscript">2</sup></span> and those that lie <i class="fm-italics">outside</i> the circle. In the original Cartesian space, the decision boundary is not linear (it is circular). But if we transform the Cartesian input points to a latent space in polar coordinates—that is, each <span class="math">(<i class="fm-italics">x</i>, <i class="fm-italics">y</i>)</span> is mapped to <span class="math">(<i class="fm-italics">r</i>, <i class="fm-italics">θ</i>)</span> such that <span class="math"><i class="fm-italics">x</i> = <i class="fm-italics">rcos</i>(<i class="fm-italics">θ</i>), <i class="fm-italics">y</i> = <i class="fm-italics">rsin</i>(<i class="fm-italics">θ</i>)</span>—the circle transforms into a line <span class="math"><i class="fm-italics">r</i> = <i class="fm-italics">a</i></span> in the latent space . A simple linear classifier <span class="math"><i class="fm-italics">r</i> = <i class="fm-italics">a</i></span> in the latent space can achieve the desired classification.</p>
</li>
</ul>
<p class="body">Some applications of latent-space modeling are as follows:<a id="marker-473"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Generating artificial images or text (as explained in the context of generative modeling).</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Similarity estimation between inputs</i>—If we map inputs to latent vectors, we can assess the similarity between inputs by computing the Euclidean distance between the latent vectors. Why is this better than taking the Euclidean distance between the input vectors? Suppose we are building a recommendation engine that suggests other clothing items “similar” to the one a potential buyer is currently browsing. We want to retrieve other clothing items that look similar but not identical to the one viewed. But similarity is a subjective concept, not quite measurable via the similarity of the inputs’ pixel colors. Consider a shirt with black vertical stripes on a white base. If we switch the stripe color with the base color, we get a shirt with white vertical stripes on a black base. If we do pixel-to-pixel color matching, these are very different, yet they are considered similar by humans. For this problem, we have to train the latent space model, creating neural networks so that images perceived to be similar by humans map to points in latent space that are close to each other. For example, both white-on-black and black-on-white shirts should map to latent vectors that are close to each other in the latent space even though they are far apart in the input space.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Image or other data compression</i>—The latent vector approximates the data with a smaller-dimensional vector that mimics the original vector as faithfully as possible. Thus the latent vector is a lossy compressed representation of the input.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Denoising</i>—The latent vector eliminates the non-meaningful part of the input information, which is noise.</p>
</li>
</ul>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for this chapter, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/6XG6">http://mng.bz/6XG6</a>.</p>
<h2 class="fm-head" id="sec-pca-recap">14.4 Linear latent space manifolds and PCA</h2>
<p class="body">PCAs (which we discussed in section <a class="url" href="../Text/04.xhtml#sec-pca">4.4</a>) project input data onto linear hyperplanar manifolds. Revisiting this topic will set up the correct context for the rest of this chapter. Consider a set of <span class="math">3</span>D input data points clustered closely around the <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub> = <i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> plane, as shown in figure <a class="url" href="#fig-pca-3d-recap">14.3</a>.<a id="marker-474"/></p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre22" height="409" id="fig-pca-3d-recap-original-data" src="../../OEBPS/Images/CH14_F03a_Chaudhury.png" width="535"/></p>
<p class="figurecaption">(a) Original <span class="math">3</span>D data</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre22" height="411" id="fig-pca-3d-recap-recon-data" src="../../OEBPS/Images/CH14_F03b_Chaudhury.png" width="535"/></p>
<p class="figurecaption">(b) Lower-dimensional <span class="math">2</span>D representation obtained by setting the third principal value to zero</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre22" height="544" id="fig-pca-3d-recap-principal-comp" src="../../OEBPS/Images/CH14_F03c_Chaudhury.png" width="566"/></p>
<p class="figurecaption">(c) The principal vectors of the original data. The third principal vector is normal to <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub> = <i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> plane; the other two are in-plane.</p>
</div>
<p class="fm-table-caption" id="fig-pca-3d-recap">Figure 14.3 The original <span class="math">3</span>D data in figure <a class="url" href="#fig-pca-3d-recap-original-data">14.3a</a> shows high correlation: points are clustered around the <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub> = <i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> plane. The first principal component corresponds to the direction of maximum variance. The last (third) principal reduced to a <span class="math">2</span>D latent vector.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> We denote the successive axes (dimensions) as <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> instead of the more traditional <span class="math"><i class="fm-italics">X</i>, <i class="fm-italics">Y</i>, <i class="fm-italics">Z</i></span> for easy extension to higher dimensions.</p>
<p class="body">Using PCA, we can recognize that the data has low variation along some dimensions. When we do PCA, we get the principal value and principal vector pairs. The largest principal value corresponds to the direction of maximum variance in the data. The corresponding principal vector yields that direction, and that principal value indicates the magnitude of the variance along that direction. The next principal value, the principal vector pair, is the orthogonal direction with the next-highest variance, and so on. For instance, in figure <a class="url" href="#fig-pca-3d-recap">14.3</a>, the principal vectors corresponding to the larger two principal values lie in the <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub> = <i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> plane, while the smallest principal value corresponds to the normal-to-plane vector. The third principal value is significantly smaller than the others. This tells us that variance along that axis is low, and components along that axis can be dropped with relatively little loss of information: that is, low reconstruction loss. The variations along the small principal value axes are likely noise, so eliminating them cleans up the data. In figure <a class="url" href="#fig-pca-3d-recap">14.3</a>, this effectively projects the data onto the <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub> = <i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> plane.<a id="marker-475"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Dimensionality reduction</p>
<p class="fm-sidebar-text">PCA essentially projects inputs to the <i class="fm-italics">best-fit plane</i> for the training data. Assuming all the training data points are sampled with a common property, this plane represents that common property. By projecting, we eliminate that common property and retain only the discriminating aspects of the data. The eliminated information is remembered <i class="fm-italics">approximately</i> in the parameters of the plane and supplied during reconstruction (aka decoding) to map us back to the same dimensionality as the input (but not exactly the same point). This is the essence of dimensionality reduction via PCA.</p>
</div>
<p class="body">Following are the steps involved in PCA-based dimensionality reduction. This was described in detail with proofs in section <a class="url" href="../Text/04.xhtml#sec-svd">4.5</a>; here we recap the main steps without proof.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> This treatment is similar but not identical to that in section <a class="url" href="../Text/04.xhtml#sec-svd">4.5</a>. Here we have switched the variables <i class="timesitalic">m</i> and <i class="timesitalic">n</i> to be consistent with our use of <i class="timesitalic">n</i> to denote the data instance count. We have also switched to a slightly different flavor of the SVD.</p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list">Represent the data as a matrix <i class="timesitalic">X</i>, where each row is an individual data instance. The number of rows <i class="timesitalic">n</i> is the size of the data set. The number of columns <i class="timesitalic">d</i> is the original (input) dimensionality of the data. Thus <i class="timesitalic">X</i> is a <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">d</i></span> matrix.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Compute the mean data vector<br class="calibre20"/>
<br class="calibre20"/>
<!--<span class="times">$$\vec{\mu} = \frac{1}{n}\sum\displaylimits_{i=1}^{n} \vec{x}^{ \left(i\right)}$$</span>--><span class="infigure"><img alt="" class="calibre5" height="58" src="../../OEBPS/Images/eq_14-00-a.png" width="110"/></span><br class="calibre20"/>
<br class="calibre20"/>
      where <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> for <span class="math"><i class="fm-italics">i</i> = 1</span> to <span class="math"><i class="fm-italics">i</i> = <i class="fm-italics">n</i></span> denote the training data vector instances (which form rows of the matrix <i class="timesitalic">X</i>).</p>
</li>
<li class="fm-list-bullet">
<p class="list">Shift the origin of the coordinate system to the mean by subtracting the mean vector from each data vector:<br class="calibre20"/>
<br class="calibre20"/>
<span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> – <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> for all <i class="fm-italics">i</i></span><br class="calibre20"/>
<br class="calibre20"/>
      The data matrix <i class="timesitalic">X</i> now has the mean-subtracted data instances as rows.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The matrix <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i> where <i class="timesitalic">X</i> is the mean-subtracted data matrix) is the covariance matrix (as discussed in detail in section <a class="url" href="../Text/05.xhtml#sec-var-covar-std">5.7.2</a>). The eigenvalue, eigenvector pairs of the matrix <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i> are known as principal values and principal vectors (together referred to as principal components). Since <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i> is a <span class="math"><i class="fm-italics">d</i> × <i class="fm-italics">d</i></span> matrix, there are <i class="timesitalic">d</i> scalar eigenvalues and <i class="timesitalic">d</i> eigenvectors, each of dimension <span class="math"><i class="fm-italics">d</i> × 1</span>. Let’s denote the principal components as <span class="math">(<i class="fm-italics">λ</i><sub class="fm-subscript">1</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">1</sub>)</span>, <span class="math">(<i class="fm-italics">λ</i><sub class="fm-subscript">2</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">2</sub>)</span>, <span class="math">⋯</span>, <span class="math">(<i class="fm-italics">λ<sub class="fm-subscript">dm</sub></i>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">d</sub></i>)</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We can assume <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub> ≥ <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub> ≥ ⋯ ≥ <i class="fm-italics">λ<sub class="fm-subscript">d</sub></i></span> if necessary, we can make this true by renumbering the principal components). Then the first principal component corresponds to the direction of maximum variance in the data (proof with geometrical intuition can be found in section <a class="url" href="../Text/05.xhtml#sec-var-covar-std">5.7.2</a>). The corresponding principal value yields the actual variance. The next principal value corresponds to the second-highest variance (among directions orthogonal to the first principal direction), and so forth. For every component, the principal value yields the actual variance, and the principal vector yields the direction.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Consider the matrix of principal vectors:<a id="marker-476"/><br class="calibre20"/>
<br class="calibre20"/>
<span class="math"><i class="fm-italics">V</i> = [<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">1</sub> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">2</sub> … <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">d</sub></i>]</span><br class="calibre20"/>
<br class="calibre20"/>
      If we want the data to be a space with <i class="timesitalic">m</i> dimensions with minimal loss of information, we should drop the last <i class="timesitalic">m</i> vectors of <i class="timesitalic">V</i>. This eliminates the <i class="timesitalic">m</i> least-variance dimensions. Dropping the last <i class="timesitalic">m</i> vectors from <i class="timesitalic">V</i> yields a matrix<br class="calibre20"/>
<br class="calibre20"/>
<span class="math"><i class="fm-italics">V<sub class="fm-subscript">d–m</sub></i> = [<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">1</sub> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">2</sub> … <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">d–m</sub></i>]</span><br class="calibre20"/>
<br class="calibre20"/>
      Note that the best way to obtain the <i class="timesitalic">V</i> matrix is to perform SVD on the mean-subtracted <i class="timesitalic">X</i> (see section <a class="url" href="../Text/04.xhtml#sec-svd">4.5</a>).</p>
</li>
<li class="fm-list-bullet">
<p class="list">Premultiplying <i class="timesitalic">V<sub class="fm-subscript">d−m</sub></i>, the truncated principal vectors matrix, with the original data matrix <i class="timesitalic">X</i> projects the data onto a space corresponding to the first <span class="math"><i class="fm-italics">d</i> − <i class="fm-italics">m</i></span> principal components. Thus, to create <span class="math"><i class="fm-italics">d</i> − <i class="fm-italics">m</i></span>-dimensional linearly encoded latent vectors from <i class="timesitalic">d</i>-dimensional data,<br class="calibre20"/>
<br class="calibre20"/>
<span class="math"><i class="fm-italics">X<sub class="fm-subscript">d−m</sub></i> = <i class="fm-italics">XV<sub class="fm-subscript">d−m</sub></i></span><br class="calibre20"/>
<br class="calibre20"/>
<i class="timesitalic">X<sub class="fm-subscript">d−m</sub></i> is the reduced dimension data set. Its dimensionality is <span class="math"><i class="fm-italics">n</i> × (<i class="fm-italics">d</i>−<i class="fm-italics">m</i>)</span>.<br class="calibre20"/>
      It can be shown that<br class="calibre20"/>
<br class="calibre20"/>
<span class="math"><i class="fm-italics">XV<sub class="fm-subscript">d−m</sub></i> = <i class="fm-italics">UΣ<sub class="fm-subscript">d−m</sub></i></span><br class="calibre20"/>
<br class="calibre20"/>
      where <i class="timesitalic">U</i> is from SVD (see section <a class="url" href="../Text/04.xhtml#sec-svd">4.5</a>) and <i class="timesitalic">Σ<sub class="fm-subscript">d−m</sub></i> is a truncated version of the diagonal matrix <span class="math">Σ</span> from SVD with its smallest <i class="timesitalic">m</i> elements chopped off. This offers an alternative way to do PCA-based dimensionality reduction.</p>
</li>
<li class="fm-list-bullet">
<p class="list">How do we reconstruct? In other words, what is the decoder? Well, to reconstruct, we need to save the original principal vectors: that is, the <i class="timesitalic">V</i> matrix. If we have that, we can introduce <i class="timesitalic">m</i> zeros at the right of every row in <i class="timesitalic">X<sub class="fm-subscript">d−m</sub></i> to make it a <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">d</i></span> matrix again. Then we post-multiply by <i class="timesitalic">V<sup class="fm-superscript">T</sup></i>, which rotates the coordinate system back from one with principal vectors as axes to one with the original input axes. Finally, we add the mean <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> to each row to shift the origin back to its original position, which yields the reconstructed data matrix <i class="timesitalic">X̃</i>. The reconstruction loss is <span class="math">||<i class="fm-italics">X</i> − <i class="fm-italics">X̃</i>||<sup class="fm-superscript">2</sup></span>. Note that, in effect, <i class="timesitalic">X̃</i> is <i class="timesitalic">UΣV<sup class="fm-superscript">T</sup></i> with the last <i class="timesitalic">m</i> diagonal elements of <span class="math">Σ</span> set to zero.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The reconstructed data <i class="timesitalic">X̃</i> is <i class="fm-italics">not</i> identical to the original data. The information we lost during dimensionality reduction the normal-to-plane components) is lost permanently. Nonetheless, this principled way of dropping information ensures that the reconstruction loss is minimal in some sense, at least among all <i class="timesitalic">X̃</i> linearly related to <i class="timesitalic">X</i>.</p>
</li>
</ol>
<h3 class="fm-head1" id="pytorch-code-for-dimensionality-reduction-using-pca">14.4.1 PyTorch code for dimensionality reduction using PCA</h3>
<p class="body"><a id="marker-477"/>Now, let’s implement dimensionality reduction in PyTorch. Let <i class="timesitalic">X</i> be a data matrix representing points clustered around the <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub> = <i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> plane. <i class="timesitalic">X</i> is of shape <span class="math">[1000,3]</span>, with each row of <i class="timesitalic">X</i> representing a three-dimensional data point. The following listing shows how to project <i class="timesitalic">X</i> into a lower-dimensional space with minimal loss of information. It also shows how to reconstruct the original data points from the lower-dimensional representations. Note that the reconstructions are approximate because we have lost information (albeit minimal) in the dimensionality-reduction process.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for dimensionality reduction using PCA, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/7yJg">http://mng.bz/7yJg</a>.</p>
<p class="fm-code-listing-caption" id="code-pca-recap">Listing 14.1 PyTorch- PCA revisited</p>
<pre class="programlisting">import torch

X = get_data()                                      <span class="fm-combinumeral">①</span>

X_mean = X.mean(axis=0)                             <span class="fm-combinumeral">②</span>

X = X - X_mean                                      <span class="fm-combinumeral">③</span>

U, S, Vh = torch.linalg.svd(X, full_matrices=False) <span class="fm-combinumeral">④</span>

V = Vh.T                                            <span class="fm-combinumeral">⑤</span>

V_trimmed =  V[:, 0: 2]                             <span class="fm-combinumeral">⑥</span>

X_proj = torch.matmul(X, V_trimmed)                 <span class="fm-combinumeral">⑦</span>

X_proj = torch.cat([X_proj,                         <span class="fm-combinumeral">⑧</span>
              torch.zeros((X_proj.shape[0], 1))], axis=1)

X_recon = torch.matmul(X_proj, Vh)                  <span class="fm-combinumeral">⑨</span>

X_recon = X_recon + X_mean                          <span class="fm-combinumeral">⑩</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Data matrix of shape (1000, 3)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Stores the mean so we can reconstruct the original data points later</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Subtracts the mean before performing SVD</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Runs SVD</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Columns of V are the principal vectors.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Removes the last principal vector. This is along the direction of least variance perpendicular to <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub> = <i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> plane).</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Projects the input data points into the lower-dimensional space</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Pads with zeros to make an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">d</i></span> matrix</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Post-multiplies with <i class="timesitalic">V<sup class="fm-superscript">T</sup></i> to project back to the original space</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Adds the mean</p>
<h2 class="fm-head" id="sec-ae">14.5 Autoencoders</h2>
<p class="body"><a id="marker-478"/>Autoencoders are neural network systems trained to generate latent-space representations corresponding to specified inputs. They can do nonlinear projections and hence are more powerful than PCA systems see figure <a class="url" href="#fig-curved-underlying-pattern-recap">14.4</a>). The neural network mapping the input vector to a latent vector is called an <i class="fm-italics">encoder</i>. We also train a neural network called a <i class="fm-italics">decoder</i> that maps the latent vector back to the input space. The decoder output is the reconstructed input from the latent vector. The reconstructed input (that is, the output of the decoder) will never match the original input exactly—information was lost during encoding and cannot be brought back—but we can try to ensure that they match as closely as possible within the constraints of the system. The reconstruction loss is a measure of the difference between the original input and the reconstructed input. The encoder-decoder pair is trained end to end to minimize reconstruction loss (along with, potentially, some other losses). This is an example of <i class="fm-italics">representation learning</i>, whereby we learn to represent input vectors with smaller latent vectors representing the input as closely as possible in the stipulated size budget. The budgeted size of the latent space is a hyperparameter.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="187" id="fig-curved-underlying-pattern-recap" src="../../OEBPS/Images/CH14_F04_Chaudhury.png" width="494"/></p>
<p class="figurecaption">Figure 14.4 A 2 data distribution with a curved underlying pattern. It is impossible to find a straight line or vector such that all points are near it. PCA will not do well.</p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> A hyperparameter is a neural network parameter that is <i class="fm-italics">not</i> learned. Its value is set based on our knowledge of the system and held constant during training.</p>
<p class="body">The desired output is implicitly known in autoencoders: it is the input. Consequently, no human labeling is needed to train autoencoders; they are <i class="fm-italics">unsupervised</i>. An autoencoder is shown schematically in figure <a class="url" href="#fig-ae-schematic">14.5</a>.<a id="marker-479"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre23" height="444" id="fig-ae-schematic" src="../../OEBPS/Images/CH14_F05_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 14.5 Schematic representation of an autoencoder. The encoder transforms input into a latent vector. The decoder transforms the latent vector into reconstructed input. We minimize the reconstruction loss—the distance between the reconstructed input and the original input.</p>
</div>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The encoder takes an input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> and maps it to a lower-dimensional latent vector <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>. An example of an encoding neural network for image inputs is shown in listing <a class="url" href="#code-ae-encoder">14.2</a>. Note how the image height and width keep decreasing with each successive sequence of convolution, ReLU, and max pool layers.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The decoder is a neural network that generates reconstructed image <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x3.png" width="14"/></span> from the latent vector <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>. Listing <a class="url" href="#code-ae-decoder">14.3</a> shows an example of a decoder neural network. Note the transposed convolutions and how the height and width of the image keep increasing with each successive sequence of transposed convolution, batch normalization, and ReLU. Transposed convolutions are discussed in section <a class="url" href="../Text/10.xhtml#sec-transposed-conv">10.5</a>.) The decoder essentially remembers—not exactly, but in an average sense—the information discarded during encoding. Equivalently, it remembers the position of the latent space manifold in the overall input space. Adding that back to the latent space representation takes us back to the same dimensionality as the input vector but not to the same input point.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The system minimizes the information loss from the encoding (the reconstruction loss). We are ensuring that for each input, the corresponding latent vector produced by the encoder can be mapped back to a reconstructed value by the decoder that is as close as possible to the input. Equivalently, each latent vector is a faithful representation of the input, and there is a <span class="math">1 : 1</span> mapping between inputs and latent vectors.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The encoder and decoder need not be symmetric.</p>
</li>
</ul>
<p class="body">Mathematically,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\text{Encoder:}
\overbrace{ \vec{z}
}^{ \text{latent vector} }
  =  E\left( \vec{x} \right) \\
&amp;\text{Decoder:}
\overbrace{
\tilde{ \vec{x} }
}^{ \text{reconstructed image} }
  =  D\left( \vec{z} \right) \\
&amp;\text{Loss:}
\mathcal{L}  = \overbrace{ \| \vec{x} - \tilde{ \vec{x} } \|^{2}
}^{  \text{distance between input and reconstructed image} }
\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="192" src="../../OEBPS/Images/eq_14-00-b.png" width="364"/></p>
</div>
<p class="body">The end-to-end system is trained to minimize the loss <span class="times2">ℒ</span>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for autoencoders, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/mOzM">http://mng.bz/mOzM</a>.<a id="marker-480"/></p>
<p class="fm-code-listing-caption" id="code-ae-encoder">Listing 14.2 PyTorch- Autoencoder encoder</p>
<pre class="programlisting">from torch import nn
nz = 10
input_image_size = (1, 32, 32)            <span class="fm-combinumeral">①</span>
conv_encoder = nn.Sequential(
            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),  <span class="fm-combinumeral">②</span>
            nn.Conv2d(32, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),  <span class="fm-combinumeral">③</span>
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),  <span class="fm-combinumeral">④</span>

            nn.Flatten()                  <span class="fm-combinumeral">⑤</span>
        )
fc = nn.Linear(4096, nz)                  <span class="fm-combinumeral">⑥</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Input image size in (c, h, w) format</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Reduces to a (32, 16, 16)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Reduces to a (128, 8, 8)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Reduces to a (256, 4, 4)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Flattens to a 4096-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Reduces the 4096-sized tensor to an nz-sized tensor</p>
<p class="fm-code-listing-caption" id="code-ae-decoder">Listing 14.3 PyTorch- Autoencoder decoder</p>
<pre class="programlisting">from torch import nn
decoder = nn.Sequential(
            nn.ConvTranspose2d(self.nz, out_channels=256,
             kernel_size=4, stride=1,
             padding=0, bias=False),            <span class="fm-combinumeral">①</span>

            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, kernel_size=2,
              stride=2, padding=0, bias=False), <span class="fm-combinumeral">②</span>
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 32, kernel_size=2,
              stride=2, padding=0, bias=False), <span class="fm-combinumeral">③</span>
            nn.BatchNorm2d(32),
            nn.ReLU(True),
            nn.ConvTranspose2d(32, in_channels, kernel_size=2,
              stride=2, padding=0, bias=False), <span class="fm-combinumeral">④</span>
            nn.Sigmoid()
        )</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Converts (nz, 1, 1) to a 256, 4, 4)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Increases to a 128, 8, 8)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Increases to a 32, 16, 16)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Increases to a 1, 32, 32)-sized tensor</p>
<p class="fm-code-listing-caption" id="code-ae-loss">Listing 14.4 PyTorch- Autoencoder training</p>
<pre class="programlisting">from torch import nn
from torch.nn import functional as F

conv_out = conv_encoder(X)     <span class="fm-combinumeral">①</span>

z =  fc(conv_out)              <span class="fm-combinumeral">②</span>
Xr = decoder(z)                <span class="fm-combinumeral">③</span>

recon_loss = F.mse_loss(Xr, X) <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Passes the input image through the convolutional encoder</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Reduces to nz dimensions</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Reconstructs the image using z via the decoder</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Computes the reconstruction loss</p>
<h3 class="fm-head1" id="autoencoders-and-pca">14.5.1 Autoencoders and PCA</h3>
<p class="body"><a id="marker-481"/>It is important to realize that autoencoders perform a much more powerful dimensionality reduction than PCA. PCA is a linear process; it can only project data points to best-fit hyperplanes. Autoencoders can fit arbitrary complex nonlinear hypersurfaces to the data, limited only by the expressive powers of the encoder-decoder pair. If the encoder and decoder have only a single linear layer (no ReLU or other nonlinearity), then the autoencoder projects the data points to a hyperplane like PCA not necessarily the same hyperplane).</p>
<h2 class="fm-head" id="sec-latent-space-regularizn">14.6 Smoothness, continuity, and regularization of latent spaces</h2>
<p class="body">Minimizing the reconstruction loss does not yield a unique solution. For instance, figure <a class="url" href="#fig-latent-space-regularization">14.6</a> shows two examples of transforming <span class="math">2</span>D inputs into <span class="math">1</span>D latent-space representations, linear and curved, respectively. Both the regularized solid line) and the non-regularized zigzag manifold (dashed line) fit the training data well with low reconstruction error. But the former is smoother and more desirable.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="693" id="fig-latent-space-regularization-linear" src="../../OEBPS/Images/CH14_F06a_Chaudhury.png" width="710"/></p>
<p class="figurecaption">(a) Linear latent space</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="693" id="fig-latent-space-regularization-nonlinear" src="../../OEBPS/Images/CH14_F06b_Chaudhury.png" width="710"/></p>
<p class="figurecaption">(b) Curved latent space</p>
</div>
<p class="fm-table-caption" id="fig-latent-space-regularization">Figure 14.6 Two examples of mapping from a <span class="math">2</span>D input space to a <span class="math">1</span>D latent space. Both show regularized solid) vs. unregularized (dashed) latent space manifolds. Solid little circles depict training data points.<a id="marker-482"/></p>
<p class="body">Note the pair of points marked <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">p</i><sub class="fm-subscript">2</sub></span> and <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">p</i><sub class="fm-subscript">4</sub></span>(square markers). The distance between them is more or less the same in the input space. But when projected on the dashed curve (unregularized latent space), their distances (measured along the curve) become quite different. This is undesirable and does not happen in the regularized latent space (here, the distance is measured along a solid line). It becomes much more pronounced in high dimensions.</p>
<p class="body">The zigzag curve segment containing the training data set is longer than the smooth one. A good latent manifold typically has fewer twists and turns (is smooth) and hence has a “length” that is minimal in a sense. This is reminiscent of the minimum descriptor length (MDL) principle, which we discussed in section <a class="url" href="../Text/09.xhtml#sec-MDL">9.3.1</a>.</p>
<p class="body">How do we ensure that this smoothest latent space is chosen over others that also minimize the reconstruction loss? By putting additional constraints (losses) over and above the ubiquitous reconstruction loss. Recall the notion of regularization, which we looked at in sections <a class="url" href="../Text/06.xhtml#sec-MAP_estimation">6.6.3</a> and <a class="url" href="../Text/09.xhtml#sec-regularization">9.3</a>. There we introduced an explicit loss that penalizes longer solutions (which was equivalent to maximizing the a posteriori probability of parameter values as opposed to the likelihood). A related approach that we explore in this chapter is to model the latent space as a probability distribution belonging to a known family (for example, Gaussian) and minimize the difference (KL divergence) of this estimated distribution from a zero-mean univariance Gaussian. The encoder-decoder neural network pair is trained end to end to minimize a loss that is a weighted sum of the reconstruction loss and this KL divergence. Trying to remain close to the zero-mean unit-variance Gaussian penalizes departure from compactness and smoothness. This is the basic idea of variational autoencoders VAEs).</p>
<p class="body">The overall effect of regularization is to create a latent space that is more compact. If we only minimize the reconstruction loss, the system can achieve that by mapping points very far from each other (space being infinite). Regularization combats that and incentivizes the system to not map the training points too far from one another. It tries to limit the total latent-space volume occupied by the points corresponding to the training inputs.<a id="marker-483"/></p>
<h2 class="fm-head" id="sec-vae">14.7 Variational autoencoders</h2>
<p class="body">VAEs are a special case of autoencoders. They have the same architecture: a pair of neural networks that encode and decode the input vector, respectively. They also have the reconstruction loss term. But they have an additional loss term called KL divergence loss that we explain shortly.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Throughout this chapter, we denote latent variables with <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> and input variables with <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>.</p>
<h3 class="fm-head1" id="geometric-overview-of-vaes">14.7.1 Geometric overview of VAEs</h3>
<p class="body">Figure <a class="url" href="#fig-latent_space_distributions">14.7</a> attempts to provide a geometrical view of VAE latent-space modeling. During training, given an input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, the encoder does not directly emit the corresponding latent-space representation <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>. Instead, the encoder emits the parameters of a distribution from a prechosen family. For instance, if the prechosen family is Gaussian, the encoder emits a pair of parameter values <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>, <span class="math"><b class="fm-bold">Σ</b>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. These are the mean and covariance matrix of a specific Gaussian distribution <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>), <b class="fm-bold">Σ</b>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>))</span> in the latent space. The latent-space representation <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> corresponding to the input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is obtained by sampling this distribution emitted by the encoder. Thus in the Gaussian case, we have <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> ∼ <span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>), <b class="fm-bold">Σ</b>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>))</span>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The symbol <span class="math">∼</span> indicates sampling from a distribution.<a id="marker-484"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="723" id="fig-latent_space_distributions" src="../../OEBPS/Images/CH14_F07_Chaudhury.png" width="743"/></p>
<p class="figurecaption">Figure 14.7 Geometric depiction of VAE latent-space modeling distributions</p>
</div>
<p class="body">This distribution, which we call the <i class="fm-italics">latent-space map</i> of the input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, is shown by hollow circles with dark borders in figure <a class="url" href="#fig-latent_space_distributions">14.7</a>. Such mapping is called <i class="fm-italics">stochastic mapping</i>.</p>
<p class="body">The latent-space map distribution should have a narrow, single-peaked probability density function (for example, a Gaussian with small variance: that is, small <span class="math">||<b class="fm-bold">Σ</b>||</span>). The narrow-peakedness of the probability density function implies that the cloud of sample points forms a tight, small cluster—any random sample from the distribution will likely be close to the mean. So, sampling <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> from such a distribution is not very different from a deterministic mapping from <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> to <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. This sampling to obtain the latent vector is done only during training. During inferencing, we use the mean emitted by the encoder directly as the latent-space representation of the input: that is, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>.</p>
<p class="body">The decoder maps the latent vector representation <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> back to a point, say <i class="timesitalic">x̃</i>, in the input space. This is the reconstructed version of the input vector (shown by a little white square with a black border in figure <a class="url" href="#fig-latent_space_distributions">14.7</a>). The decoder is thus estimating (reconstructing) the input given the latent vector.</p>
<h3 class="fm-head1" id="vae-training-losses-and-inferencing">14.7.2 VAE training, losses, and inferencing</h3>
<p class="body">Training comprises the following steps:</p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list">Choose a simple distribution family for <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> | <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. Gaussian is a popular choice.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Each input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> maps to a separate distribution. The encoder neural network emits the parameters of this distribution. For the Gaussian case, the encoder emits <span class="math"><i class="fm-italics">μ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>), Σ(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. The latent vector <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> is sampled from this emitted distribution.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The decoder neural network takes <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> as input and emits the reconstructed input <i class="timesitalic">x̃</i>.</p>
</li>
</ol>
<p class="body"><a id="marker-485"/>Given the input, reconstructed input, and latent vector we can compute the reconstruction loss and KL Divergence loss described below. The goal of the training process is to iteratively minimize these losses. Thus, the VAE is trained to minimize a weighted sum of the following two loss terms on each input batch-</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Reconstruction Loss</i>—Just as in an autoencoder, in a properly trained VAE, the reconstruction <i class="timesitalic">x̃</i> should be close to the original input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. So, reconstruction loss is<br class="calibre20"/>
<br class="calibre20"/>
<!--<span class="times"><span class="cambria">ℒ</span><i class="fm-italics"><sub class="FM-Subscript">recon</sub></em> = ||<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span> − <i class="fm-italics">x̃</em>||<sup class="FM-Superscript">2</sup></span>--><span class="infigure"><img alt="" class="calibre5" height="32" src="../../OEBPS/Images/eq_14-00-c.png" width="132"/></span></p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">KL divergence loss</i>— In VAE, we also have a loss term proportional to the KL divergence between the distribution emitted by the encoder and the zero mean unit variance Gaussian. KL divergence measured the dissimilarity between two probability distributions and was discussed in detail in section <a class="url" href="../Text/06.xhtml#sec-kld">6.4</a>. Here we state (following equation <a class="url" href="../Text/06.xhtml#eq-kld-cont-multivar">6.13</a>) that the KL divergence loss for VAE is<br class="calibre20"/>
<br class="calibre20"/>
<!--<span class="times">$$\mathcal{L}_{kld} = KLD\left( q\left( \vec{z}
\middle\vert \vec{x} \right),  p\left( \vec{z} \right) \right) =
\int\displaylimits_{ \vec{z} \in D} q\left( \vec{z} \middle\vert \vec{x}
\right); \log \left( \frac{ q\left( \vec{z} \middle\vert \vec{x}
\right)} { p\left( \vec{z} \right) }  \right) d\vec{z}$$</span>--><span class="infigure"><img alt="" class="calibre5" height="68" src="../../OEBPS/Images/eq_14-00-d.png" width="437"/></span><br class="calibre20"/>
<br class="calibre20"/>
      where <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> denotes the latent-space map probability distribution and <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> is a fixed target distribution. We want our global distribution of latent vectors to mimic the target distribution. The target is typically chosen to be a compact distribution so that the global latent vector distribution is also compact.<br class="calibre20"/>
      The popular choice for the prechosen distribution family is Gaussian and for the fixed distribution is the zero-mean unit covariance matrix Gaussian:<br class="calibre20"/>
<br class="calibre20"/>
<!--<span class="times">$$\begin{aligned}
&amp;q\left( \vec{z} \middle\vert \vec{x} \right) = \mathcal{N}\left(
\vec{z} ;  \; \vec{\mu}\left( \vec{x} \right), \boldsymbol{\Sigma}\left(
\vec{x} \right) \right)
\text{ and } p\left( \vec{z} \right) = \mathcal{N}\left( \vec{z} ;  \; \vec{0},
\mathbf{I}  \right)\end{aligned}$$</span>--><span class="infigure"><img alt="" class="calibre5" height="38" src="../../OEBPS/Images/eq_14-00-e.png" width="393"/></span><br class="calibre20"/>
<br class="calibre20"/>
      It should be noted that for the above choice of prior, we can evaluate the KLD loss via a closed form formula as described in section 14.7.7.<br class="calibre20"/>
      Minimizing <span class="math"><span class="cambria">ℒ</span><i class="fm-italics"><sub class="fm-subscript">kld</sub></i></span> essentially demands that <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is high—that is, close to one—at the <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> values where <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> is high (see figure <a class="url" href="#fig-vae-kld-loss">14.8</a>), because then their ratio is close to one and the logarithm is close to zero. The values of <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> at the places where <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is low close to zero) do not matter because <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> appears as a factor in <span class="math"><span class="cambria">ℒ</span><i class="fm-italics"><sub class="fm-subscript">kld</sub></i></span>—the contributions to the loss by these terms are close to zero anyway.<br class="calibre20"/>
      Thus, KLD loss essentially tries to ensure that most of the sample point cloud of <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> falls on a densely populated region of the sample point cloud of <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span>. Geometrically, this means the cloud of little hollow circles with black borders has a lot of overlapping mass with the target distribution. If every training data point is like that, the overall global cloud of latent vectors will also have significant overlap with the target distribution. Since the target distribution is typically chosen to be compact, this in turn ensures that the overall latent vector distribution (dark filled circles in figure <a class="url" href="#fig-latent_space_distributions">14.7</a>) is compact. For instance, in the case when the target distribution is the zero-mean unit covariance matrix Gaussian <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span>, <b class="fm-bold">I</b>)</span>, most of the mass of the latent vectors is contained within the unit radius ball. Without the KL divergence term, the latent vectors will spread throughout the latent space. In short, the KLD loss <i class="fm-italics">regularizes the latent space</i>.</p>
</li>
</ul>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="519" id="fig-vae-kld-loss-low" src="../../OEBPS/Images/CH14_F08a_Chaudhury.png" width="534"/></p>
<p class="figurecaption">(a) Low KL divergence loss (high <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> coincides with high <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="519" id="fig-vae-kld-loss-high" src="../../OEBPS/Images/CH14_F08b_Chaudhury.png" width="534"/></p>
<p class="figurecaption">(b) High KL divergence loss (high <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> coincides with low <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span></p>
</div>
<p class="figurecaption" id="fig-vae-kld-loss">Figure 14.8 between the encoder-generated distribution (<span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>), <b class="fm-bold">Σ</b>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>))</span> ) and the target distribution <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> (here <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>) ≡ <span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span>, <b class="fm-bold">I</b>)</span>) or, equivalently, low KL divergence between them.<a id="marker-486"/></p>
<p class="body">The encoder-decoder pair of neural networks is trained end to end to minimize the weighted sum of reconstruction loss and KLD loss. In particular, the encoder learns to emit the parameters of the <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> distribution.</p>
<p class="body">During inferencing, only the encoder is used. The encoder takes an input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> and outputs <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> and <span class="math"><b class="fm-bold">Σ</b>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. We do not sample here. Instead, we use the mean directly as the latent-space representation of the input.</p>
<p class="body">Notice that each input point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> maps to a separate Gaussian distribution <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">N</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>), ∑(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>))</span>. The overall distribution <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> modeled by all of these together can be very complex. Yet that complexity does not affect our computation which involves only <span class="math"><i class="fm-italics">q</i>(<i class="fm-italics">z</i>|<i class="fm-italics">x</i>)</span> and <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">z</i>)</span>. This is what makes the approach powerful.</p>
<h3 class="fm-head1" id="sec-vae-bayes">14.7.3 VAEs and Bayes’ theorem</h3>
<p class="body">During training, the encoder neural network stochastically maps a specific input data instance, a point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> in the input space, to a latent-space point <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> ~ <span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>), <b class="fm-bold">∑</b>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>))</span>. Thus the latent-space map effectively models the posterior probability <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. Note that we are using the symbol <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> to denote the actual distribution emitted by the encoder, while we are using the symbol <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> to denote the true (unknown) posterior probability distribution. Of course, we want these two to be as close as possible to each other: that is, we want the KL divergence between them to be minimal. Later in this section, we see how minimizing the KL divergence between <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> and <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> leads to the entire VAE algorithm.<a id="marker-487"/></p>
<p class="body">The decoder maps this point (<span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>) in latent space back to the input space point <i class="timesitalic">x̃</i>. As such, it models the probability distribution <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span>.</p>
<p class="body">The global distributions of the latent vectors <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> effectively model <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> (shown by dark-shaded filled little circles in figure <a class="url" href="#fig-latent_space_distributions">14.7</a>). These probabilities are connected by our old friend, Bayes’ theorem:</p><!--<p class="Body"><span class="times">$$\overbrace{
  p\left( \vec{z} \middle\vert \vec{x} \right)
}^{ \text{encoder / posterior} }
=
\frac{
\overbrace{
  p\left( \vec{x} \middle\vert  \vec{z}  \right)
}^{ \text{decoder / likelihood} }
\;\;\;
\overbrace{
  p\left(\vec{z}\right)
}^{ \text{prior} }
}
{
\underbrace{
  p\left(\vec{x}\right)
}_{ \text{evidence, constant wrt } \vec{z} }
}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="134" src="../../OEBPS/Images/eq_14-00-f.png" width="332"/></p>
</div>
<h3 class="fm-head1" id="stochastic-mapping-leads-to-latent-space-smoothness">14.7.4 Stochastic mapping leads to latent-space smoothness</h3>
<p class="body">Sampling the encoder’s output from a narrow distribution is similar, but not identical, to deterministic mapping. It has a rather unexpected advantage over direct encoding. A specific input point is mapped to a slightly different point in the latent space every time it is encountered during training—all these points have to decode back to the same region in the input space. This enforces an overall smoothness over the latent space: nearby <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> values all correspond to nearby <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> values.</p>
<h3 class="fm-head1" id="direct-minimization-of-the-posterior-requires-prohibitively-expensive-normalization">14.7.5 Direct minimization of the posterior requires prohibitively expensive normalization</h3>
<p class="body">The Bayes’ theorem expression of a VAE in section <a class="url" href="#sec-vae-bayes">14.7.3</a> gives us an idea. Why not train the neural network to directly maximize the posterior probability <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre15" height="24" src="../../OEBPS/Images/AR_Xc.png" width="19"/></span>)</span>, where <i class="timesitalic">X</i> denotes the training data set? It certainly makes theoretical sense; we are choosing the latent space whose posterior probability is maximum given the training data. Of course, we must optimize one batch at a time, as we always do with neural networks.</p>
<p class="body">How do we evaluate the posterior probability? The formula is as follows:</p><!--<p class="Body"><span class="times">$$p\left( \vec{z} \middle\vert
\vec{x} \right)
=
\frac{
  p\left( \vec{x} \middle\vert  \vec{z}  \right)
\;\;\; p\left(\vec{z}\right)
}
{
  p\left(\vec{x}\right)
}
=
\frac{
  p\left( \vec{x} \middle\vert  \vec{z}  \right)
\;\;\; p\left(\vec{z}\right)
}
{
\int\displaylimits_{ \vec{z}  } p\left( \vec{x}
\middle\vert  \vec{z}  \right) p\left(\vec{z}\right)  d\vec{z}
}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_14-00-g.png" width="322"/></p>
</div>
<p class="body">The denominator contains a sum over all values of <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>. Remember, with every iteration, the neural network weights change, and all previously computed latent vectors become invalid. This means we have to recompute all latent vectors every iteration, which is intractable. Each iteration is <span class="math"><span class="segoe">𝒪</span>(<i class="fm-italics">n</i>)</span>, and each epoch then is <span class="math"><span class="segoe">𝒪</span>(<i class="fm-italics">n</i><sup class="fm-superscript">2</sup>)</span>, where <i class="timesitalic">n</i> is the number of training data instances (could be on the order of millions). We have to look for other methods. That takes us to <i class="fm-italics">evidence lower bound</i> (ELBO) types of approaches.</p>
<h3 class="fm-head1" id="elbo-and-vaes">14.7.6 ELBO and VAEs</h3>
<p class="body">We do not know the true probability distribution <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. Let’s try to learn an approximate probability distribution <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> that is as close as possible to <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. In other words, we want to minimize the KL divergence between the two (KL divergence was introduced in section <a class="url" href="../Text/06.xhtml#sec-kld">6.4</a>). This KL divergence is<a id="marker-488"/></p><!--<p class="Body"><span class="times">$$KLD(q, p) =
\int\displaylimits_{ \vec{z} \in D} q\left( \vec{z}
\middle\vert  \vec{x}  \right) \; ln \left( \frac{ q\left( \vec{z}
\middle\vert  \vec{x}  \right) }{ p\left( \vec{z}
\middle\vert  \vec{x}  \right) } \right) \; d\vec{z}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_14-00-h.png" width="306"/></p>
</div>
<p class="body">We can expand this as</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;KLD(q, p) = \int\displaylimits_{ \vec{z} \in D} q\left( \vec{z}
\middle\vert  \vec{x}  \right) \; ln \left( \frac{ q\left( \vec{z}
\middle\vert  \vec{x}  \right) }{ p\left( \vec{z}
\middle\vert  \vec{x}  \right) } \right) \; d\vec{z} \\
             &amp;= \int\displaylimits_{ \vec{z} \in D } q \left(
\vec{z} \middle\vert  \vec{x}  \right) \;
                                                                     ln
\left(  q\left( \vec{z}
\middle\vert  \vec{x}  \right)  \right)  d\vec{z}
                  -
                  \int\displaylimits_{ \vec{z} \in D} q\left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                                      ln
\left(  p\left( \vec{z} \middle\vert  \vec{x}   \right)  \right) d\vec{z} \\
             &amp;= \int\displaylimits_{ \vec{z} \in D } q \left(
\vec{z} \middle\vert  \vec{x}  \right) \;
                                                                     ln
\left(  q\left( \vec{z}
\middle\vert  \vec{x}  \right)  \right)  d\vec{z}
                  \;-\;
                  \int\displaylimits_{ \vec{z} \in D} q\left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                                      ln
\left(  p\left( \vec{z},  \vec{x}   \right)  \right) d\vec{z}
                  \;+\;
                  \int\displaylimits_{ \vec{z} \in D} q\left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                                      ln
\left(  p\left( \vec{x}   \right)  \right) d\vec{z}  \\
             &amp;= \overbrace{
                        \int\displaylimits_{ \vec{z} \in D } q \left(
\vec{z} \middle\vert  \vec{x}  \right) \;
                                                                            ln
\left(  q\left( \vec{z}
\middle\vert  \vec{x}  \right)  \right)  d\vec{z}
                   }^{ -H_{q}\text{  (negative of entropy) } }
                  \;-\;
                  \overbrace{
                        \int\displaylimits_{ \vec{z} \in D} q\left(
\vec{z} \middle\vert  \vec{x}  \right) \;
                                                                      ln
\left(  p\left( \vec{z},  \vec{x}   \right)  \right) d\vec{z}
                  }^{ E_{q}\left( ln\left( p\left( \vec{x}, \vec{z}
\right) \right) \right) }
                  \;+\;
                 ln \left(  p\left( \vec{x}   \right)  \right)
                 \overbrace{ \int\displaylimits_{ \vec{z} \in D} q\left(
\vec{z} \middle\vert  \vec{x}  \right) \; d\vec{z} }^{ =1 }\\
                 &amp;= -H_{q} - E_{q}\left( ln\left( p\left( \vec{x},
\vec{z} \right) \right) \right) + ln \left(  p\left(
\vec{x}   \right)  \right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="340" src="../../OEBPS/Images/eq_14-00-i.png" width="638"/></p>
</div>
<p class="body">where <i class="timesitalic">D</i> is the domain of <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>: that is, the latent space, <i class="timesitalic">H<sub class="fm-subscript">q</sub></i> is the entropy of the probability distribution (entropy was introduced in section <a class="url" href="../Text/06.xhtml#sec-entropy">6.2</a>), and <span class="math"><i class="fm-italics">E<sub class="fm-subscript">q</sub></i>(<i class="fm-italics">ln</i>(<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)))</span> is the expected value of <span class="math"><i class="fm-italics">ln</i>(<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>))</span> under the probability density <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. Rearranging terms, we get</p><!--<p class="Body"><span class="times">$$KLD(q, p) + H_{q}
+ E_{q}\left( ln\left( p\left( \vec{x}, \vec{z} \right) \right)  \right)
=  \overbrace{ ln \left(  p\left( \vec{x}   \right)  \right) }^{
\text{constant} }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_14-00-j.png" width="360"/></p>
</div>
<p class="body">where the right-hand side is constant because it is a property of the data and cannot be adjusted during optimization. Defining the evidence lower bound (ELBO) as</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">ELBO</i> = <i class="fm-italics">H<sub class="fm-subscript">q</sub></i> + <i class="fm-italics">E<sub class="fm-subscript">q</sub></i>(<i class="fm-italics">ln</i>(<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)))</span></p>
<p class="body">we get</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">KLD</i>(<i class="fm-italics">q</i>, <i class="fm-italics">p</i>) + <i class="fm-italics">ELBO</i> = <i class="fm-italics">constant</i></span></p>
<p class="body">So, <i class="fm-italics">minimizing</i> the KL divergence between <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> and its approximation <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is equivalent to <i class="fm-italics">maximizing</i> the ELBO. We soon see that this leads to a technique for optimizing variational autoencoders.</p>
<p class="fm-head2" id="significance-of-the-name-elbo">Significance of the name ELBO</p>
<p class="body">Why do we call it <i class="fm-italics">evidence lower bound</i>? Well, the answer is hidden in the relation <span class="math"><i class="fm-italics">KLD</i>(<i class="fm-italics">q</i>, <i class="fm-italics">p</i>) + <i class="fm-italics">ELBO</i> = <i class="fm-italics">ln</i>(<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>))</span>. The right-hand side is the evidence log-likelihood. Remember, KL divergence is always non-negative. So, the lowest value of <span class="math"><i class="fm-italics">ln</i>(<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>))</span> happens when KL divergence is zero when <span class="math"><i class="fm-italics">ln</i>(<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)) = <i class="fm-italics">ELBO</i></span>. This means the evidence log-likelihood cannot be lower than the ELBO value. Thus the ELBO is the lower bound of the evidence log-likelihood; in short, it is the evidence lower bound.<a id="marker-489"/></p>
<p class="fm-head2" id="physical-significance-of-the-elbo">Physical significance of the ELBO</p>
<p class="body">Let’s look at the physical significance of ELBO maximization:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">ELBO</i> = <i class="fm-italics">H<sub class="fm-subscript">q</sub></i> + <i class="fm-italics">E<sub class="fm-subscript">q</sub></i>(<i class="fm-italics">ln</i>(<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)))</span></p>
<p class="body">The first term is entropy. As we saw in section <a class="url" href="../Text/06.xhtml#sec-entropy">6.2</a>, this is a measure of the diffuseness of the distribution. If the points are evenly spread out in the distribution—the probability density is flat with no high peak—the entropy is high. When the distribution has few tall peaks and low values elsewhere, entropy is low (remember, for a probability density, having tall peaks implies low values elsewhere since the total volume under the function is constant: one). Thus, maximizing the ELBO means we are looking for a diffuse distribution <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. This, in turn, encourages smoothness in the latent space since we are effectively saying an input point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> can map to any point around the mean <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> (as emitted by the encoder) with almost equal high probability. Note that this fights a bit with the notion that each input should map to a unique point in the latent space. The solution tries to optimize between these conflicting requirements.</p>
<p class="body">The other term—expectation of the log of joint density <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> under the probability density <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>—effectively measures the overlap between the two. Maximizing it is equivalent to saying <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> must be high where <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> is high. This seems intuitively true. The joint density <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>) = <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span>. It is high where <i class="fm-italics">both</i> the posterior <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> and prior <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> are high. If <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> approximates the posterior, it should be high where the joint is high.</p>
<p class="body">Let’s continue to explore the ELBO. More physical significances will emerge along with an algorithm for VAE optimization:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;ELBO = H_{q} + E_{q}\left( ln\left( p\left( \vec{x}, \vec{z}
\right) \right)  \right) \\
&amp;= -\int\displaylimits_{ \vec{z} \in D } q \left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                             ln
\left(  q\left( \vec{z}
\middle\vert  \vec{x}  \right)  \right)  d\vec{z}
     + \int\displaylimits_{ \vec{z} \in D} q\left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                             ln
\left(  p\left( \vec{z},  \vec{x}   \right)  \right) d\vec{z}\\
&amp;= -\int\displaylimits_{ \vec{z} \in D } q \left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                             ln
\left(  q\left( \vec{z}
\middle\vert  \vec{x}  \right)  \right)  d\vec{z}
     + \int\displaylimits_{ \vec{z} \in D} q\left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                             ln
\left(  p\left( \vec{x} \middle\vert  \vec{z}   \right) p\left( \vec{z}
\right) \right) d\vec{z}  \\
&amp;= -\int\displaylimits_{ \vec{z} \in D } q \left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                             ln
\left(  q\left( \vec{z}
\middle\vert  \vec{x}  \right)  \right)  d\vec{z}
     + \int\displaylimits_{ \vec{z} \in D} q\left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                            ln
\left(  p\left( \vec{x}
\middle\vert  \vec{z}   \right)   \right)  d\vec{z}
     + \int\displaylimits_{ \vec{z} \in D } q\left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                            ln \left( p\left( \vec{z} \right) \right) d\vec{z}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="218" src="../../OEBPS/Images/eq_14-00-k.png" width="631"/></p>
</div>
<p class="body">Rearranging terms and simplifying</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;ELBO =
\overbrace{
    \int\displaylimits_{ \vec{z} \in D } q\left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                        ln
\left(  p\left( \vec{x}
\middle\vert  \vec{z}   \right)   \right)  d\vec{z}
}^{ E_{q} \left( ln\left( p\left( \vec{x} \middle\vert \vec{z} \right)
\right) \right) }
-
\overbrace{
    \int\displaylimits_{ \vec{z} \in D } q\left( \vec{z}
\middle\vert  \vec{x}  \right) \;
                                                     ln \left(
                                                         \frac{ q\left(
\vec{z} \middle\vert  \vec{x}   \right) }{ p\left( \vec{z} \right) }
                                                     \right)  d\vec{z}
}^{ KLD\left( q\left( \vec{z} \middle\vert \vec{x} \right), p\left(
\vec{z} \right)\right) }\\
&amp;ELBO = E_{q} \left( ln\left( p\left( \vec{x} \middle\vert \vec{z}
\right) \right) \right)  - KLD\left( q\left( \vec{z}
\middle\vert  \vec{x} \right), p\left( \vec{z}
\right)\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="140" src="../../OEBPS/Images/eq_14-00-l.png" width="483"/></p>
</div>
<p class="body"><a id="marker-490"/>This last expression yields more physical interpretation and leads to the VAE algorithm. Let’s examine the two terms in the final ELBO expression in detail. The first term is <span class="math"><i class="fm-italics">E<sub class="fm-subscript">q</sub></i>(<i class="fm-italics">ln</i>(<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)))</span>. This is high when <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> and <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> are both high at the same <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> values. For a given <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is high at those <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> values that are likely encoder outputs (that is, latent representations) of input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. High <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> at these same <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> locations implies a high probability of decoding back to the same <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> value from those <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> locations. Thus, this term basically says if <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <i class="fm-italics">encodes</i> to <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> with a high probability, then <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> should <i class="fm-italics">decode</i> back to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> with a high probability, too. Stated differently, a round trip from input to latent space back to input space should not take us far from the original input. In figure <a class="url" href="#fig-latent_space_distributions">14.7</a>, this means the input point marked <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> lies close to the output point marked <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x3.png" width="14"/></span>. In other words, <i class="fm-italics">minimizing reconstruction loss leads to ELBO maximization</i>.</p>
<p class="body">Now consider the second term. It comes with a minus sign. Maximizing this is equivalent to minimizing the KL divergence between <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> and <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span>. This is the regularizing term. Viewed in another way, this is the term through which we inject our belief about the basic organization of the latent space into the system. Remember that the KL divergence <span class="math"><i class="fm-italics">KLD</i>(<i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>), <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>))</span> sees very little contribution from the small values of <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. It is dominated by the large values of <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. In terms of figure <a class="url" href="#fig-latent_space_distributions">14.7</a>, minimizing this KL divergence essentially ensures that most of the hollow circles fall on an area highly populated with filled circles.</p>
<p class="body">Thus, overall, maximization of ELBO is equivalent to minimizing reconstruction loss with regularization in the form of minimizing KL divergence from a specific prior distribution. This is what we do in VAEs. In every iteration, we minimize the reconstruction loss (as in ordinary AEs) and also minimize divergence from a known (or guessed) prior. Note that this does not require us to encode all training inputs per iteration. The approach is <i class="fm-italics">incremental</i>—one input or input batch at a time—like any other neural network optimization. Also, although we started from finding an approximation to <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>, the final expression does not have that anywhere. There is only the prior <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> for which we can use some suitable fixed distribution.</p>
<h3 class="fm-head1" id="choice-of-prior-zero-mean-unit-covariance-gaussian">14.7.7 Choice of prior: Zero-mean, unit-covariance Gaussian</h3>
<p class="body">The popular choice for the known prior is a zero-mean, unit-covariance matrix Gaussian, <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span>, <b class="fm-bold">I</b>)</span> , where <span class="timesbold">I</span> is the <span class="math"><i class="fm-italics">d</i> × <i class="fm-italics">d</i></span> identity matrix <i class="timesitalic">d</i> is the dimensionality of the latent space), <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span> is <span class="math"><i class="fm-italics">d</i> × 1</span> vector of all zeros. Note that minimizing the KL divergence from <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span>, <b class="fm-bold">I</b>)</span> is equivalent to restricting most of the mass within the unit ball (a hypersphere with its center at the origin and radius <span class="math">1</span>). In other words, this KL divergence term restrains the latent vectors from spreading over the <span class="math"><span class="segoe">ℜ</span><i class="fm-italics"><sub class="fm-subscript">d</sub></i></span> and remains mostly within the unit ball. Remember that a compact set of latent vectors translates in a sense to the simplest (minimum descriptor length) representations for the input vectors: that is, a regularized latent space (section <a class="url" href="#sec-latent-space-regularizn">14.6</a>).</p>
<p class="body">KL divergence from a Gaussian has a closed-form expression that we derive in section <a class="url" href="../Text/06.xhtml#sec-kld-gaussians">6.4.1</a>. We first repeat equation <a class="url" href="../Text/06.xhtml#eq-kld-univar-gauss">6.14</a> for KL divergence between two Gaussians and then obtain the expression for the special case where one of the Gaussians is a zero-mean, unit-covariance Gaussian:<a id="marker-491"/></p><!--<div class="flalign">
<p class="Body">D( q, p ) = ( tr( _p</span>^-1</span> _q</span> ) +  _p</span> - _q</span> )^T</span>
_p</span>^-1</span> ( _p</span> - _q</span> ) -d
+ ( ) )</p>
</div>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_14-01.png" width="550"/></p>
</div>
<p class="fm-equation-caption">Equation 14.1 <span class="calibre" id="eq-kld-multivar-gauss-ch14"/></p>
<p class="body">where the operator <i class="timesitalic">tr</i> denotes the <i class="fm-italics">trace</i> of a matrix (sum of diagonal elements) and operator <i class="timesitalic">det</i> denotes the determinant. By assumption, <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>) = <span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span>, <b class="fm-bold">I</b>)</span>: that is, <span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">p</sub></i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span></span> and <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">p</sub></i> = <b class="fm-bold">I</b></span>. Thus,</p><!--<p class="Body"><span class="times">$$D\left( q, p \right) = \frac{1}{2}
\left(
     tr\left( \boldsymbol{\Sigma}_{q}  \right) + \vec{\mu}_{q}^{T}
\mathbf{I} {\vec{\mu}_q} -d - \log\left( \det \boldsymbol{\Sigma}_{q}
\right)
\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_14-01-a.png" width="367"/></p>
</div>
<p class="body">At this point, we introduce another simplifying assumption: that <i class="fm-italics">the covariance matrix <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">q</sub></i></span> is a diagonal matrix</i>. This means the matrix can be expressed compactly as</p>
<p class="fm-equation"><span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">q</sub></i> = <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_sigma.png" width="14"/></span><sub class="fm-subscript">q</sub></i></span></p>
<p class="body">where <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_sigma.png" width="14"/></span><sub class="fm-subscript">q</sub></i> contains the elements of the main diagonal and we are not redundantly expressing the zeros in the off-diagonal elements. Note that this is not an outlandish assumption to make. We are approximating <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> with a Gaussian <span class="math"><i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> whose axes are uncorrelated.</p>
<p class="body">Because of this assumption,</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned}
&amp;Tr\left( \boldsymbol{\Sigma}_{q} \right) =
\sum\displaylimits_{i=1}^{d} \vec{ \sigma }_{q}^{2} \left[ i \right]\\
&amp;\log\left( \det \boldsymbol{\Sigma}_{q} \right) =
\sum\displaylimits_{i=1}^{d} log \left( \vec{ \sigma }_{q}^{2} \left[ i
\right] \right)\\
&amp;D\left( q, p \right) = \frac{1}{2}
\left(
    \sum\displaylimits_{i=1}^{d} \vec{ \sigma }_{q}^{2} \left[ i \right]
    + \| \vec{ \mu }_{q} \|^{2}
    - \sum\displaylimits_{i=1}^{d} log \left( \vec{ \sigma }_{q}^{2}
\left[ i \right] \right)a-d\right) \\
&amp;= \frac{1}{2}
\left(
     \| \vec{ \mu }_{q} \|^{2} +
     \sum\displaylimits_{i=1}^{d} \left( \vec{ \sigma }_{q}^{2} \left[ i
\right] - 2log \left( \vec{ \sigma }_{q} \left[ i \right] \right)
\right)
     -d
\right)\\
&amp;= \frac{1}{2} \left(\sum\displaylimits_{i=1}^{d}  \left(
        \vec{ \mu }^{2}_{q}\left[ i \right]
        +
        \vec{ \sigma }_{q}^{2} \left[ i \right]
        -
        2log \left( \vec{\sigma}_{q} \left[i\right] \right)
        -
        1
     \right)
  \right)
\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="326" src="../../OEBPS/Images/eq_14-01-b.png" width="432"/></p>
</div>
<p class="body">It is easy to see the expression <span class="math">(<i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_sigma.png" width="14"/></span><sub class="fm-subscript">q</sub></i><sup class="fm-superscript">2</sup>[<i class="fm-italics">i</i>] − 2<i class="fm-italics">log</i>(<i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_sigma.png" width="14"/></span><sub class="fm-subscript">q</sub></i>[<i class="fm-italics">i</i>]))</span> reaches a minimum when <span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_sigma.png" width="14"/></span><sub class="fm-subscript">q</sub></i>[<i class="fm-italics">i</i>] = 1</span>. Thus, overall, KL divergence with the zero-mean, unit-covariance Gaussian is minimized when the mean is at the origin and the variances are all ones. This is equivalent to minimizing the spread of the latent vectors outside the ball of unit radius centered on the origin.</p>
<p class="body">An alternative choice for the prior is a Gaussian mixture with as many components as the known number of classes. We do not discuss that here.</p>
<h3 class="fm-head1" id="reparameterization-trick">14.7.8 Reparameterization trick</h3>
<p class="body"><a id="marker-492"/>We have avoided talking about one nasty problem so far. We said that in VAEs, the encoder emits the mean and variance of the probability density function <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> from which we <i class="fm-italics">sample</i> the encoder output. There is a problem, however. The encoder-decoder pair are neural networks that learn via backpropagation. That is based on differentiation. Sampling is <i class="fm-italics">not</i> differentiable. How do we deal with this?</p>
<p class="body">We can use a neat trick: the so-called <i class="fm-italics">reparameterization trick</i>. Let’s first explain it in the univariate case. Sampling from a Gaussian <span class="math"><span class="cambria">𝒩</span>(<i class="fm-italics">μ</i>, <i class="fm-italics">σ</i>)</span> can be viewed as a combination of the following two steps:</p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list">Take a random sample from <i class="timesitalic">x</i> from <span class="math"><span class="cambria">𝒩</span>(0,1)</span>. Note that there is no learnable parameter here; it’s a sample from a constant density function.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Translate the sample (add <i class="timesitalic">μ</i>), and scale it (multiply by <i class="timesitalic">σ</i>).</p>
</li>
</ol>
<p class="body">This essentially takes the sampling part out of the path for backpropagation. The encoder emits <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i>, which are differentiable entities that we learn. Sampling is done separately from a constant density function.</p>
<p class="body">The idea can be extended to a multivariate Gaussian. Sampling from <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>, <b class="fm-bold">Σ</b>)</span> can be broken down into sampling from <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span>, <b class="fm-bold">I</b>)</span> and scaling the vector by multiplying by the matrix <span class="timesbold">Σ</span> and translating by <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>. Thus, we have a multivariate encoder that can learn via backpropagation.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for VAEs, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/5QYD">http://mng.bz/5QYD</a>.</p>
<p class="fm-code-listing-caption" id="code-vae-reparameterization">Listing 14.5 PyTorch- Reparameterization trick</p>
<pre class="programlisting">def reparameterize(mu, log_var):
        std = torch.exp(0.5 * log_var) <span class="fm-combinumeral">①</span>



        eps = torch.randn_like(std)    <span class="fm-combinumeral">②</span>

        return mu + eps * std          <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Converts the log variance to the standard deviation</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Samples from <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span>, <b class="fm-bold">I</b>)</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Scales by multiplying by <span class="timesbold">Σ</span> and translates by <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span></p>
<p class="fm-code-listing-caption" id="code-vae-encoder">Listing 14.6 PyTorch- VAE</p>
<pre class="programlisting">from torch import nn
nz = 10
input_image_size = (1, 32, 32)            <span class="fm-combinumeral">①</span>
conv_encoder = nn.Sequential(
            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),  <span class="fm-combinumeral">②</span>
            nn.Conv2d(32, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),  <span class="fm-combinumeral">③</span>
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),  <span class="fm-combinumeral">④</span>

            nn.Flatten()                  <span class="fm-combinumeral">⑤</span>
        )
mu_fc = nn.Linear(4096, nz)               <span class="fm-combinumeral">⑥</span>
logvar_fc = nn.Linear(4096, nz)           <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Input image size in (c, h, w) format</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Reduces to a (32, 16, 16)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Reduces to a (128, 8, 8)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Reduces to a (256, 4, 4)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Flattens to a 4096-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Reduces a 4096-sized tensor to an nz-sized <i class="timesitalic">μ</i> tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Reduces a 4096-sized tensor to an nz-sized <span class="math"><i class="fm-italics">log</i>(<i class="fm-italics">σ</i><sup class="fm-superscript">2</sup>)</span> tensor</p>
<p class="fm-code-listing-caption" id="code-vae-decoder">Listing 14.7 PyTorch- VAE decoder<a id="marker-493"/></p>
<pre class="programlisting">from torch import nn
decoder = nn.Sequential(
            nn.ConvTranspose2d(self.nz, out_channels=256,
             kernel_size=4, stride=1,
             padding=0, bias=False),            ①
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, kernel_size=2,
              stride=2, padding=0, bias=False), ②
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 32, kernel_size=2,
              stride=2, padding=0, bias=False), ③
            nn.BatchNorm2d(32),
            nn.ReLU(True),
            nn.ConvTranspose2d(32, in_channels, kernel_size=2,
              stride=2, padding=0, bias=False), ④
            nn.Sigmoid()
        )</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Converts (nz, 1, 1) to a (256, 4, 4)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Increases to a 128, 8, 8)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Increases to a 32, 16, 16)-sized tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Increases to a 1, 32, 32)-sized tensor</p>
<p class="fm-code-listing-caption" id="code-vae-loss">Listing 14.8 PyTorch- VAE loss</p>
<pre class="programlisting">recon_loss = F.binary_cross_entropy(Xr, X,
                     reduction="sum")                     <span class="fm-combinumeral">①</span>

kld_loss = -0.5 * torch.sum(1 + log_var
                             - mu.pow(2) - log_var.exp()) <span class="fm-combinumeral">②</span>

total_loss = recon_loss + beta * kld_loss                 <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Binary cross-entropy loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math"><i class="fm-italics">KLD</i>(<i class="fm-italics">q</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>), <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>))</span> where <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> ~ <span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span>, <b class="fm-bold">I</b>)</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Computes the total loss</p>
<p class="fm-code-listing-caption" id="code-vae-training">Listing 14.9 PyTorch- VAE training<a id="marker-494"/></p>
<pre class="programlisting">conv_out = conv_encoder(X)                <span class="fm-combinumeral">①</span>

mu = mu_fc(conv_out)                      <span class="fm-combinumeral">②</span>
log_var = logvar_fc(conv_out)             <span class="fm-combinumeral">③</span>

z = reparameterize(mu, log_var)           <span class="fm-combinumeral">④</span>

Xr = self.decoder(z)                      <span class="fm-combinumeral">⑤</span>

total_loss = recon_loss + beta * kld_loss <span class="fm-combinumeral">⑥</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Passes the input image through the convolutional encoder</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Computes <i class="timesitalic">μ</i>, an nz-dimensional tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Computes <span class="math"><i class="fm-italics">log</i>(<i class="fm-italics">σ</i><sup class="fm-superscript">2</sup>)</span>, an nz-dimensional tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Samples <i class="timesitalic">z</i> via the reparameterization trick</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Reconstructs the image using z via the decoder</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Computes the total loss</p>
<p class="fm-head2" id="autoencoders-vs.-vaes">Autoencoders vs. VAEs</p>
<p class="body">Let’s revisit the familiar MNIST digits data set. It contains a training set of 60,000 images and a test set of 10,000 images. Each image is 28 <span class="math">×</span> 28 in size and contains a center crop of a single digit.</p>
<p class="body">Earlier, we used this data set for classification. Here, we use it an unsupervised manner: we ignore the labels during training/testing. We train both the autoencoder and the VAE end to end on this data set and look at the results (see figures <a class="url" href="#fig-ae-vae-recon">14.9</a> and <a class="url" href="#fig-ae-vae-latent-space">14.10</a>).</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre28" height="444" id="fig-ae-recon" src="../../OEBPS/Images/CH14_F09a_Chaudhury.png" width="257"/></p>
<p class="figurecaption">(a) Autoencoder- reconstructed images</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre28" height="444" id="fig-vae-recon" src="../../OEBPS/Images/CH14_F09b_Chaudhury.png" width="257"/></p>
<p class="figurecaption">(b) VAE-reconstructed images</p>
</div>
<p class="fm-table-caption" id="fig-ae-vae-recon">Figure 14.9 Comparing the reconstructed images on the test set for the autoencoder and VAE trained end to end. On a autoencoder and VAE do a pretty good job of reconstructing images from the test set.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="452" id="fig-ae-latent-space" src="../../OEBPS/Images/CH14_F10a_Chaudhury.png" width="541"/></p>
<p class="figurecaption">(a) Autoencoder latent space nz=2)</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="451" id="fig-vae-latent-space" src="../../OEBPS/Images/CH14_F10b_Chaudhury.png" width="539"/></p>
<p class="figurecaption">(b) VAE latent space (nz=2)</p>
</div>
<p class="fm-table-caption" id="fig-ae-vae-latent-space">Figure 14.10 The difference between the learned latent spaces of the autoencoder and VAE. We train an autoencoder and a VAE with <span class="math"><i class="fm-italics">nz</i> = 2</span> on MNIST and plot the latent space for the test set. Autoencoders only minimize the reconstruction loss, so any latent space is equally acceptable as long as the reconstruction loss is low. As expected, the learned latent space is sparse and has a very high spread. VAEs, in contrast, minimize reconstruction loss with regularization. This is done by minimizing the KL divergence between the learned latent space and a known prior distribution <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span>, <b class="fm-bold">I</b>)</span>. Adding this regularization term ensures that the latent space is constrained within a unit ball. This can be seen in figure <a class="url" href="#fig-vae-latent-space">14.10b</a>, where the learned latent space is much more compact.</p>
<p class="body"><a id="marker-495"/>The autoencoder is trained to minimize the MSE between the input image and the reconstructed image. There is no other restriction on the latent space.</p>
<p class="body">The VAE is trained to maximize the ELBO. As we saw in the previous section, we can maximize the ELBO by minimizing the reconstruction loss with regularization in the form of minimizing KL divergence from a specific prior distribution: <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span>, <b class="fm-bold">I</b>)</span> in the case of VAE. So the network is incentivized to ensure that the latent space learned is constrained within the unit ball.</p>
<p class="body">One minor implementation detail to note is that we use binary cross-entropy instead of MSE when training VAEs. In practice, this leads to better convergence.</p>
<h2 class="fm-head" id="summary-13">Summary</h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">In latent-space modeling, we map input data points onto a lower-dimensional latent space. The latent space is typically a manifold consisting of points that have a property of interest in common. The property of interest can be membership to a specific class, such as all paragraphs written by Shakespeare. The latent vectors are simpler, more compact representations of the input data in which only information related to the property of interest is retained and other information is eliminated.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In latent space modeling, all training data input satisfies the property of interest. For instance, we can train a latent space model on paragraphs written by Shakespeare. Then the learned manifold contains points corresponding to various Shakespeare like paragraphs. Points far away from the manifold are less Shakespeare-like. By inspecting this distance, we can estimate the probability of a paragraph being written by Shakespeare. By sampling the probability distribution, we may even be able to emit pseudo-Shakespeare paragraphs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Geometrically speaking, we project the input point onto the manifold. PCA performs a special form of latent space modeling where the manifold is a best-fit hyperplane for the training data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Autoencoders can perform a much more powerful dimensionality reduction than PCA. An autoencoder consists of an encoder <i class="timesitalic">E</i>), which maps the input data point into the lower-dimensional space, and a decoder (<i class="timesitalic">D</i>), which maps the lower-dimensional representation back into the input space. It is trained to minimize the reconstruction loss: that is, the distance between the input and reconstructed (encoded, then decoded) vectors.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Variational autoencoders (VAEs) model latent spaces as probability distributions to impose additional constraints (over and above reconstruction loss) so that we can generate more regularized latent spaces.</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">In VAEs, the encoder maps the input to a latent representation via a stochastic process (rather than a deterministic one). It emits <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> as opposed to directly emitting <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>. <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> is obtained by sampling <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. The decoder maps a point in latent space <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span> back to the input space. It is also modeled as a probability distribution <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The latent space learned by a VAE is much more compact and smoother (and hence more desirable) than that learned by an autoencoder.<a id="marker-496"/></p>
</li>
</ul>
</li>
</ul>
</div></body></html>