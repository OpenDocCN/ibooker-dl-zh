- en: Chapter 3\. Linear and Logistic Regression with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will show you how to build simple, but nontrivial, examples of
    learning systems in TensorFlow. The first part of this chapter reviews the mathematical
    foundations for building learning systems and in particular will cover functions,
    continuity, and differentiability. We introduce the idea of loss functions, then
    discuss how machine learning boils down to the ability to find the minimal points
    of complicated loss functions. We then cover the notion of gradient descent, and
    explain how it can be used to minimize loss functions. We end the first section
    by briefly discussing the algorithmic idea of automatic differentiation. The second
    section focuses on introducing the TensorFlow concepts underpinned by these mathematical
    ideas. These concepts include placeholders, scopes, optimizers, and TensorBoard,
    and enable the practical construction and analysis of learning systems. The final
    section provides case studies of how to train linear and logistic regression models
    in TensorFlow.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is long and introduces many new ideas. It’s OK if you don’t grasp
    all the subtleties of these ideas in a first reading. We recommend moving forward
    and coming back to refer to the concepts here as needed later. We will repeatedly
    use these fundamentals in the remainder of the book in order to let these ideas
    sink in gradually.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical Review
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This first section reviews the mathematical tools needed to conceptually understand
    machine learning. We attempt to minimize the number of Greek symbols required,
    and focus instead on building conceptual understanding rather than technical manipulations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Functions and Differentiability
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will provide you with a brief overview of the concepts of functions
    and differentiability. A function *f* is a rule that takes an input to an output.
    There are functions in all computer programming languages, and the mathematical
    definition of a function isn’t really much different. However, mathematical functions
    commonly used in physics and engineering have other important properties such
    as continuity and differentiability. A continuous function, loosely speaking,
    is one that can be drawn without lifting your pencil from the paper, as shown
    in [Figure 3-1](#ch3-cont). (This is of course not the technical definition, but
    it captures the spirit of the continuity condition.)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![continuous_1.gif](assets/tfdl_0301.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Some continuous functions.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Differentiability is a type of smoothness condition on functions. It says no
    sharp corners or turns are allowed in the function ([Figure 3-2](#ch3-diff)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![Math_images_4.jpg](assets/tfdl_0302.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. A differentiable function.
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The key advantage of differentiable functions is that we can use the slope of
    the function at a particular point as a guide to find places where the function
    is higher or lower than our current position. This allows us to find the *minima*
    of the function. The *derivative* of differentiable function *f*, denoted <math><msup><mi>f</mi>
    <mo>'</mo></msup></math> , is another function that provides the slope of the
    original function at all points. The conceptual idea is that the derivative of
    a function at a given point gives a signpost pointing to directions where the
    function is higher or lower than its current value. An optimization algorithm
    can follow this signpost to move closer to a minima of *f*. At the minima itself,
    the function will have derivative zero.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: The power of derivative-driven optimization isn’t apparent at first. Generations
    of calculus students have suffered through stultifying exercises minimizing tiny
    functions on paper. These exercises aren’t useful since finding the minima of
    a function with only a small number of input parameters is a trivial exercise
    best done graphically. The power of derivative-driven optimization only becomes
    evident when there are hundreds, thousands, millions, or billions of variables.
    At these scales, understanding the function analytically is nigh impossible, and
    all visualizations are fraught exercises that may well miss the key attributes
    of the function. At these scales, the *gradient* of the function <math><mrow><mi>∇</mi>
    <mi>f</mi></mrow></math> , a generalization of <math><msup><mi>f</mi> <mo>'</mo></msup></math>
    to multivariate functions, is likely the most powerful mathematical tool to understand
    the function and its behavior. We will dig into gradients in more depth later
    in this chapter. (Conceptually that is; we won’t cover the technical details of
    gradients in this work.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，导数驱动的优化的力量并不明显。几代微积分学生都在纸上进行枯燥的最小化函数练习中受苦。这些练习并不有用，因为找到具有少量输入参数的函数的最小值是一个最好通过图形方式完成的微不足道的练习。导数驱动的优化的力量只有在有数百、数千、数百万或数十亿个变量时才会显现出来。在这些规模上，通过解析理解函数几乎是不可能的，所有的可视化都是充满风险的练习，很可能会忽略函数的关键属性。在这些规模上，函数的*梯度*，一个多变量函数的<math><msup><mi>f</mi>
    <mo>'</mo></msup></math>的推广，很可能是理解函数及其行为的最强大的数学工具。我们将在本章后面更深入地探讨梯度。（概念上是这样；我们不会在这项工作中涵盖梯度的技术细节。）
- en: 'At a very high level, machine learning is simply the act of function minimization:
    learning algorithms are nothing more than minima finders for suitably defined
    functions. This definition has the advantage of mathematical simplicity. But,
    what are these special differentiable functions that encode useful solutions in
    their minima and how can we find them?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常高的层面上，机器学习只是函数最小化的行为：学习算法只不过是适当定义的函数的最小值查找器。这个定义具有数学上的简单性优势。但是，这些特殊的可微函数是什么，它们如何在它们的最小值中编码有用的解决方案，我们如何找到它们呢？
- en: Loss Functions
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: In order to solve a given machine learning problem, a data scientist must find
    a way of constructing a function whose minima encode solutions to the real-world
    problem at hand. Luckily for our hapless data scientist, the machine learning
    literature has built up a rich history of *loss functions* that perform such encodings.
    Practical machine learning boils down to understanding the different types of
    loss functions available and knowing which loss function should be applied to
    which problems. Put another way, the loss function is the mechanism by which a
    data science project is transmuted into mathematics. All of machine learning,
    and much of artificial intelligence, boils down to the creation of the right loss
    function to solve the problem at hand. We will give you a whirlwind tour of some
    common families of loss functions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决给定的机器学习问题，数据科学家必须找到一种构建函数的方法，其最小值编码了手头的现实世界问题的解决方案。幸运的是，对于我们这位不幸的数据科学家来说，机器学习文献已经建立了一个丰富的*损失函数*历史，执行这种编码。实际机器学习归结为理解不同类型的可用损失函数，并知道应该将哪种损失函数应用于哪些问题。换句话说，损失函数是将数据科学项目转化为数学的机制。所有的机器学习，以及大部分人工智能，都归结为创建正确的损失函数来解决手头的问题。我们将为您介绍一些常见的损失函数家族。
- en: We start by noting that a loss function <math alttext="script upper L"><mi>ℒ</mi></math>
    must satisfy some mathematical properties to be meaningful. First <math alttext="script
    upper L"><mi>ℒ</mi></math> must use both datapoints *x* and labels *y*. We denote
    this by writing the loss function as <math><mrow><mi>ℒ</mi> <mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math> . Using our language from the previous
    chapter, both *x* and *y* are tensors, and <math alttext="script upper L"><mi>ℒ</mi></math>
    is a function from pairs of tensors to scalars. What should the functional form
    of the loss function be? A common assumption that people use is to make loss functions
    *additive*. Suppose that <math alttext="left-parenthesis x Subscript i Baseline
    comma y Subscript i Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math>
    are the data available for example *i* and that there are *N* total examples.
    Then the loss function can be decomposed as
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到，损失函数<math alttext="script upper L"><mi>ℒ</mi></math>必须满足一些数学属性才能有意义。首先，<math
    alttext="script upper L"><mi>ℒ</mi></math>必须使用数据点*x*和标签*y*。我们通过将损失函数写成<math><mrow><mi>ℒ</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math>来表示这一点。使用我们在上一章中的术语，*x*和*y*都是张量，<math
    alttext="script upper L"><mi>ℒ</mi></math>是从张量对到标量的函数。损失函数的函数形式应该是什么？人们常用的一个假设是使损失函数*可加性*。假设<math
    alttext="left-parenthesis x Subscript i Baseline comma y Subscript i Baseline
    right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math>是示例*i*的可用数据，并且总共有*N*个示例。那么损失函数可以分解为
- en: <math display="block"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msub><mi>ℒ</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msub><mi>ℒ</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: '(In practice <math><msub><mi>ℒ</mi> <mi>i</mi></msub></math> is the same for
    every datapoint.) This additive decomposition allows for many useful advantages.
    The first is that derivatives factor through addition, so computing the gradient
    of the total loss simplifies as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>∇</mi> <mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mi>∇</mi> <msub><mi>ℒ</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: This mathematical trick means that so long as the smaller functions <math><msub><mi>ℒ</mi>
    <mi>i</mi></msub></math> are differentiable, so too will the total loss function
    be. It follows that the problem of designing loss functions resolves into the
    problem of designing smaller functions <math alttext="script upper L Subscript
    i Baseline left-parenthesis x Subscript i Baseline comma y Subscript i Baseline
    right-parenthesis"><mrow><msub><mi>ℒ</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
    . Before we dive into designing the <math alttext="script upper L Subscript i"><msub><mi>ℒ</mi>
    <mi>i</mi></msub></math> , it will be convenient to take a small detour that explains
    the difference between classification and regression problems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Classification and regression
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning algorithms can be broadly categorized as supervised or unsupervised
    problems. Supervised problems are those for which both datapoints *x* and labels
    *y* are available, while unsupervised problems have only datapoints *x* without
    labels *y*. In general, unsupervised machine learning is much harder and less
    well-defined (what does it mean to “understand” datapoints *x*?). We won’t delve
    into unsupervised loss functions at this point since, in practice, most unsupervised
    losses are cleverly repurposed supervised losses.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning can be broken up into the two subproblems of classification
    and regression. A classification problem is one in which you seek to design a
    machine learning system that assigns a discrete label, say 0/1 (or more generally
    <math><mrow><mn>0</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>n</mi></mrow></math>
    ) to a given datapoint. Regression is the problem of designing a machine learning
    system that attaches a real valued label (in <math alttext="double-struck upper
    R"><mi>ℝ</mi></math> ) to a given datapoint.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, these problems may appear rather different. Discrete objects
    and continuous objects are typically treated differently by mathematics and common
    sense. However, part of the trickery used in machine learning is to use continuous,
    differentiable loss functions to encode both classification and regression problems.
    As we’ve mentioned previously, much of machine learning is simply the art of turning
    complicated real-world systems into suitably simple differentiable functions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will introduce you to a pair of mathematical functions
    that will prove very useful for transforming classification and regression tasks
    into suitable loss functions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: L² Loss
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *L*² loss (pronounced *ell-two* loss) is commonly used for regression problems.
    The *L*² loss (or *L*²-norm as it’s commonly called elsewhere) provides for a
    measure of the magnitude of a vector:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mrow><mo>∥</mo><mi>a</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msubsup><mi>a</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></msqrt></mrow></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *a* is assumed to be a vector of length *N*. The *L*² norm is commonly
    used to define the distance between two vectors:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mrow><mo>∥</mo><mi>a</mi><mo>-</mo><mi>b</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>a</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>b</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: This idea of *L*² as a distance measurement is very useful for solving regression
    problems in supervised machine learning. Suppose that *x* is a collection of data
    and *y* the associated labels. Let *f* be some differentiable function that encodes
    our machine learning model. Then to encourage *f* to predict *y*, we create the
    *L*² loss function
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mrow><mo>∥</mo><mi>f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>-</mo><mi>y</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub></mrow></math>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: As a quick note, it’s common in practice to not use the *L*² loss directly,
    but rather its square
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msubsup><mrow><mo>∥</mo><mi>a</mi><mo>-</mo><mi>b</mi><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>a</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>b</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: in order to avoid dealing with terms of the form <math alttext="1 slash StartRoot
    left-parenthesis EndRoot x right-parenthesis"><mrow><mn>1</mn> <mo>/</mo> <msqrt><mo>(</mo></msqrt>
    <mrow><mi>x</mi> <mo>)</mo></mrow></mrow></math> in the gradient. We will use
    the squared *L*² loss repeatedly in the remainder of this chapter and book.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Probability distributions
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before introducing loss functions for classification problems, it will be useful
    to take a quick aside to introduce probability distributions. To start, what is
    a probability distribution and why should we care about it for the purposes of
    machine learning? Probability is a deep subject, so we will only delve far enough
    into it for you to gain the required minimal understanding. At a high level, probability
    distributions provide a mathematical trick that allows you to relax a discrete
    set of choices into a continuum. Suppose, for example, you need to design a machine
    learning system that predicts whether a coin will fall heads up or heads down.
    It doesn’t seem like heads up/down can be encoded as a continuous function, much
    less a differentiable one. How can you then use the machinery of calculus or TensorFlow
    to solve problems involving discrete choices?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Enter the probability distribution. Instead of hard choices, make the classifier
    predict the chance of getting heads up or heads down. For example, the classifier
    may learn to predict that heads has probability 0.75 and tails has probability
    0.25\. Note that probabilities vary continuously! Consequently by working with
    the probabilities of discrete events rather than with the events themselves, you
    can neatly sidestep the issue that calculus doesn’t really work with discrete
    events.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: A probability distribution *p* is simply a listing of the probabilities for
    the possible discrete events at hand. In this case, *p* = (0.75, 0.25). Note,
    alternatively, you can view <math alttext="p colon StartSet 0 comma 1 EndSet right-arrow
    double-struck upper R"><mrow><mi>p</mi> <mo>:</mo> <mo>{</mo> <mn>0</mn> <mo>,</mo>
    <mn>1</mn> <mo>}</mo> <mo>→</mo> <mi>ℝ</mi></mrow></math> as a function from the
    set of two elements to the real numbers. This viewpoint will be useful notationally
    at times.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: We briefly note that the technical definition of a probability distribution
    is more involved. It is feasible to assign probability distributions to real-valued
    events. We will discuss such distributions later in the chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cross-entropy is a mathematical method for gauging the distance between two
    probability distributions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo> <mi>x</mi></munder>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <mi>q</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Here *p* and *q* are two probability distributions. The notation *p*(*x*) denotes
    the probability *p* accords to event *x*. This definition is worth discussing
    carefully. Like the *L*² norm, *H* provides a notion of distance. Note that in
    the case where *p* = *q*,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo> <mi>x</mi></munder>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: This quantity is the entropy of *p* and is usually written simply *H*(*p*).
    It’s a measure of how disordered the distribution is; the entropy is maximized
    when all events are equally likely. *H*(*p*) is always less than or equal to *H*(*p*,
    *q*). In fact, the “further away” distribution *q* is from *p*, the larger the
    cross-entropy gets. We won’t dig deeply into the precise meanings of these statements,
    but the intuition of cross-entropy as a distance mechanism is worth remembering.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: As an aside, note that unlike *L*² norm, *H* is asymmetric! That is, <math alttext="upper
    H left-parenthesis p comma q right-parenthesis not-equals upper H left-parenthesis
    q comma p right-parenthesis"><mrow><mi>H</mi> <mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>q</mi> <mo>)</mo> <mo>≠</mo> <mi>H</mi> <mo>(</mo> <mi>q</mi> <mo>,</mo> <mi>p</mi>
    <mo>)</mo></mrow></math> . For this reason, reasoning with cross-entropy can be
    a little tricky and is best done with some caution.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Returning to concrete matters, now suppose that <math alttext="p equals left-parenthesis
    y comma 1 minus y right-parenthesis"><mrow><mi>p</mi> <mo>=</mo> <mo>(</mo> <mi>y</mi>
    <mo>,</mo> <mn>1</mn> <mo>-</mo> <mi>y</mi> <mo>)</mo></mrow></math> is the true
    data distribution for a discrete system with two outcomes, and <math alttext="q
    equals left-parenthesis y Subscript pred Baseline comma 1 minus y Subscript pred
    Baseline right-parenthesis"><mrow><mi>q</mi> <mo>=</mo> <mo>(</mo> <msub><mi>y</mi>
    <mtext>pred</mtext></msub> <mo>,</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mtext>pred</mtext></msub>
    <mo>)</mo></mrow></math> is that predicted by a machine learning system. Then
    the cross-entropy loss is
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>y</mi> <mo form="prefix">log</mo>
    <msub><mi>y</mi> <mtext>pred</mtext></msub> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>y</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mtext>pred</mtext></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: This form of the loss is used widely in machine learning systems to train classifiers.
    Empirically, minimizing *H*(*p*, *q*) seems to construct classifiers that reproduce
    provided training labels well.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far in this chapter, you have learned about the notion of function minimization
    as a proxy for machine learning. As a short recap, minimizing a suitable function
    is often sufficient to learn to solve a desired task. In order to use this framework,
    you need to use suitable loss functions, such as the *L*² or *H*(*p*, *q*) cross-entropy
    in order to transform classification and regression problems into suitable loss
    functions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Learnable Weights
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we’ve explained that machine learning is the act of
    minimizing suitably defined loss function <math alttext="script upper L left-parenthesis
    x comma y right-parenthesis"><mrow><mi>ℒ</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow></math> . That is, we attempt to find arguments to
    the loss function <math alttext="script upper L"><mi>ℒ</mi></math> that minimize
    it. However, careful readers will recall that (*x*,*y*) are fixed quantities that
    cannot be changed. What arguments to <math alttext="script upper L"><mi>ℒ</mi></math>
    are we changing during learning then?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经解释了机器学习是通过最小化适当定义的损失函数<math alttext="script upper L left-parenthesis
    x comma y right-parenthesis"><mrow><mi>ℒ</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow></math>来实现的。也就是说，我们试图找到最小化它的损失函数<math alttext="script
    upper L"><mi>ℒ</mi></math>的参数。然而，细心的读者会记得(*x*,*y*)是固定的量，不能改变。那么在学习过程中我们改变的是什么参数呢？
- en: Enter learnable weights *W*. Suppose *f*(*x*) is a differentiable function we
    wish to fit with our machine learning model. We will dictate that *f* be *parameterized*
    by choice of *W*. That is, our function actually has two arguments *f*(*W*, *x*).
    Fixing the value of *W* results in a function that depends solely on datapoints
    *x*. These learnable weights are the quantities actually selected by minimization
    of the loss function. We will see later in the chapter how TensorFlow can be used
    to encode learnable weights using `tf.Variable`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输入可学习权重*W*。假设*f*(*x*)是我们希望用机器学习模型拟合的可微函数。我们将规定*f*由选择*W*的方式进行*参数化*。也就是说，我们的函数实际上有两个参数*f*(*W*,
    *x*)。固定*W*的值会导致一个仅依赖于数据点*x*的函数。这些可学习权重实际上是通过最小化损失函数选择的量。我们将在本章后面看到如何使用`tf.Variable`来编码可学习权重。
- en: But now, suppose that we have encoded our learning problem with a suitable loss
    function? How can we actually find minima of this loss function in practice? The
    key trick we will use is minimization by gradient descent. Suppose that *f* is
    a function that depends on some weights *W*. Then <math alttext="normal nabla
    upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math> denotes the direction change
    in *W* that would maximally increase *f*. It follows that taking a step in the
    opposite direction would get us closer to the minima of *f*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，现在假设我们已经用适当的损失函数编码了我们的学习问题？在实践中，我们如何找到这个损失函数的最小值？我们将使用的关键技巧是梯度下降最小化。假设*f*是一个依赖于一些权重*W*的函数。那么<math
    alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>表示的是在*W*中会最大程度增加*f*的方向变化。由此可知，朝着相反方向迈出一步会让我们更接近*f*的最小值。
- en: Notation for Gradients
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度的符号
- en: 'We have written the gradient for learnable weight *W* as <math alttext="normal
    nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math> . At times, it will be
    convenient to use the following alternative notation for the gradient:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将可学习权重*W*的梯度写成了<math alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>。有时，使用以下替代符号表示梯度会更方便：
- en: <math display="block"><mrow><mi>∇</mi> <mi>W</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow>
    <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac></mrow></math>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>∇</mi> <mi>W</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow>
    <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac></mrow></math>
- en: Read this equation as saying that gradient <math alttext="normal nabla upper
    W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math> encodes the direction that maximally
    changes the loss <math alttext="script upper L"><mi>ℒ</mi></math> .
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个方程理解为梯度<math alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>编码了最大程度改变损失<math
    alttext="script upper L"><mi>ℒ</mi></math>的方向。
- en: TheI idea of gradient descent is to find the minima of functions by repeatedly
    following the negative gradient. Algorithmically, this update rule can be expressed
    as
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的思想是通过反复遵循负梯度来找到函数的最小值。从算法上讲，这个更新规则可以表示为
- en: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mi>W</mi> <mo>-</mo> <mi>α</mi>
    <mi>∇</mi> <mi>W</mi></mrow></math>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mi>W</mi> <mo>-</mo> <mi>α</mi>
    <mi>∇</mi> <mi>W</mi></mrow></math>
- en: where <math><mi>α</mi></math> is the *step-size* and dictates how much weight
    is given to new gradient <math alttext="normal nabla upper W"><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math> . The idea is to take many little steps each in the direction
    of <math alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>
    . Note that <math alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>
    is itself a function of *W*, so the actual step changes at each iteration. Each
    step performs a little update to the weight matrix *W*. The iterative process
    of performing updates is typically called *learning* the weight matrix *W*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math><mi>α</mi></math>是*步长*，决定了新梯度<math alttext="normal nabla upper W"><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math>被赋予多少权重。这个想法是每次都朝着<math alttext="normal nabla upper W"><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math>的方向迈出许多小步。注意<math alttext="normal nabla upper W"><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math>本身是*W*的一个函数，所以实际步骤在每次迭代中都会改变。每一步都对权重矩阵*W*进行一点更新。执行更新的迭代过程通常称为*学习*权重矩阵*W*。
- en: Computing Gradients Efficiently with Minibatches
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用小批量高效计算梯度
- en: One issue is that computing <math alttext="normal nabla upper W"><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math> can be very slow. Implicitly, <math alttext="normal nabla
    upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math> depends on the loss function
    <math alttext="script upper L"><mi>ℒ</mi></math> . Since <math alttext="script
    upper L"><mi>ℒ</mi></math> depends on the entire dataset, computing <math alttext="normal
    nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math> can become very slow
    for large datasets. In practice, people usually estimate <math><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math> on a fraction of the dataset called a *minibatch*. Each
    minibatch is of size typically 50–100\. The size of the minibatch is a *hyperparameter*
    in a deep learning algorithm. The step-size for each step <math><mi>α</mi></math>
    is another hyperparameter. Deep learning algorithms typically have clusters of
    hyperparameters, which are not themselves learned via the stochastic gradient
    descent.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是计算<math alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>可能非常慢。隐含地，<math
    alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>取决于损失函数<math
    alttext="script upper L"><mi>ℒ</mi></math>。由于<math alttext="script upper L"><mi>ℒ</mi></math>取决于整个数据集，对于大型数据集来说，计算<math
    alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>可能会变得非常缓慢。在实践中，人们通常在称为*minibatch*的数据集的一部分上估计<math><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math>。每个minibatch通常包含50-100个样本。Minibatch的大小是深度学习算法中的一个*超参数*。每个步骤<math><mi>α</mi></math>的步长是另一个超参数。深度学习算法通常具有超参数的集群，这些超参数本身不是通过随机梯度下降学习的。
- en: This tension between learnable parameters and hyperparameters is one of the
    weaknesses and strengths of deep architectures. The presence of hyperparameters
    provides much room for utilizing the expert’s strong intuition, while the learnable
    parameters allow the data to speak for itself. However, this flexibility itself
    quickly becomes a weakness, with understanding of the behavior of hyperparameters
    something of a black art that blocks beginners from widely deploying deep learning.
    We will spend significant effort discussing hyperparameter optimization later
    in this book.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可学习参数和超参数之间的这种张力是深度结构的弱点和优势之一。超参数的存在为利用专家的强烈直觉提供了很大的空间，而可学习参数则允许数据自己说话。然而，这种灵活性本身很快变成了一个弱点，对于超参数行为的理解有点像黑魔法，阻碍了初学者广泛部署深度学习。我们将在本书的后面花费大量精力讨论超参数优化。
- en: We end this section by introducing the notion of an *epoch*. An epoch is a full
    pass of the gradient descent algorithm over the data *x*. More particularly, an
    epoch consists of however many gradient descent steps are required to view all
    the data at a given minibatch size. For example, suppose that a dataset has 1,000
    datapoints and training uses a minibatch of size 50\. Then an epoch will consist
    of 20 gradient descent updates. Each epoch of training increases the amount of
    useful knowledge the model has gained. Mathematically, this will correspond to
    reductions in the value of the loss function on the training set.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过介绍*时代*的概念来结束本节。一个时代是梯度下降算法在数据*x*上的完整遍历。更具体地说，一个时代包括需要查看给定minibatch大小的所有数据所需的梯度下降步骤。例如，假设一个数据集有1,000个数据点，训练使用大小为50的minibatch。那么一个时代将包括20个梯度下降更新。每个训练时代增加了模型获得的有用知识量。从数学上讲，这将对应于训练集上损失函数值的减少。
- en: Early epochs will cause dramatic drops in the loss function. This process is
    often referred to as *learning the prior* on that dataset. While it appears that
    the model is learning rapidly, it is in fact only adjusting itself to reside in
    the portion of parameter space that is pertinent to the problem at hand. Later
    epochs will correspond to much smaller drops in the loss function, but it is often
    in these later epochs that meaningful learning will happen. A few epochs is usually
    too little time for a nontrivial model to learn anything useful; models are usually
    trained from 10–1,000 epochs or until convergence. While this appears large, it’s
    important to note that the number of epochs required usually doesn’t scale with
    the size of the dataset at hand. Consequently, gradient descent scales linearly
    with the size of data and not quadratically! This is one of the greatest strengths
    of the stochastic gradient descent method versus other learning algorithms. More
    complicated learning algorithms may only require a single pass over a dataset,
    but may use total compute that scales quadratically with the number of datapoints.
    In this era of big datasets, quadratic runtimes are a fatal weakness.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的时代将导致损失函数的急剧下降。这个过程通常被称为在该数据集上*学习先验*。虽然看起来模型正在快速学习，但实际上它只是在调整自己以适应与手头问题相关的参数空间的部分。后续时代将对应于损失函数的较小下降，但通常在这些后续时代中才会发生有意义的学习。几个时代通常对于一个非平凡的模型来说时间太短，模型通常从10-1,000个时代或直到收敛进行训练。虽然这看起来很大，但重要的是要注意，所需的时代数量通常不随手头数据集的大小而增加。因此，梯度下降与数据大小成线性关系，而不是二次关系！这是随机梯度下降方法相对于其他学习算法的最大优势之一。更复杂的学习算法可能只需要对数据集进行一次遍历，但可能使用的总计算量与数据点数量成二次关系。在大数据集的时代，二次运行时间是一个致命的弱点。
- en: Tracking the drop in the loss function as a function of the number of epochs
    can be an extremely useful visual shorthand for understanding the learning process.
    These plots are often referred to as loss curves (see [Figure 3-4](#ch3-smoothloss)).
    With time, an experienced practitioner can diagnose common failures in learning
    with just a quick glance at the loss curve. We will pay significant attention
    to the loss curves for various deep learning models over the course of this book.
    In particular, later in this chapter, we will introduce TensorBoard, a powerful
    visualization suite that TensorFlow provides for tracking quantities such as loss
    functions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![smooth_loss.png](assets/tfdl_0304.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. An example of a loss curve for a model. Note that this loss curve
    is from a model trained with the true gradient (that is, not a minibatch estimate)
    and is consequently smoother than other loss curves you will encounter later in
    this book.
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Automatic Differentiation Systems
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning is the art of defining loss functions suited to datasets and
    then minimizing them. In order to minimize loss functions, we need to compute
    their gradients and use the gradient descent algorithm to iteratively reduce the
    loss. However, we still need to discuss how gradients are actually computed. Until
    recently, the answer was “by hand.” Machine learning experts would break out pen
    and paper and compute matrix derivatives by hand to compute the analytical formulas
    for all gradients in a learning system. These formulas would then be manually
    coded to implement the learning algorithm. This process was notoriously buggy,
    and more than one machine learning expert has stories of accidental gradient errors
    in published papers and production systems going undiscovered for years.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'This state of affairs has changed significantly with the widespread availability
    of automatic differentiation engines. Systems like TensorFlow are capable of automatically
    computing gradients for almost all loss functions. This automatic differentiation
    is one of the greatest advantages of TensorFlow and similar systems, since machine
    learning practitioners no longer need to be experts at matrix calculus. However,
    it’s still worth understanding at a high level how TensorFlow can automatically
    take derivatives of complex functions. For those readers who suffered through
    an introductory class in calculus, you might remember that taking derivatives
    of functions is surprisingly mechanical. There are a series of simple rules that
    can be applied to take derivatives of most functions. For example:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>x</mi></mrow></mfrac>
    <msup><mi>x</mi> <mi>n</mi></msup> <mo>=</mo> <mi>n</mi> <msup><mi>x</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></math><math
    display="block"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>x</mi></mrow></mfrac>
    <msup><mi>e</mi> <mi>x</mi></msup> <mo>=</mo> <msup><mi>e</mi> <mi>x</mi></msup></mrow></math>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'These rules can be combined through the power of the chain rule:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>x</mi></mrow></mfrac>
    <mi>f</mi> <mrow><mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo>
    <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <msup><mi>g</mi>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: where <math><msup><mi>f</mi> <mo>'</mo></msup></math> is used to denote the
    derivative of *f* and <math alttext="g prime"><msup><mi>g</mi> <mo>'</mo></msup></math>
    that of *g*. With these rules, it’s straightforward to envision how one might
    program an automatic differentiation engine for one-dimensional calculus. Indeed,
    the creation of such a differentiation engine is often a first-year programming
    exercise in Lisp-based classes. (It turns out that correctly parsing functions
    is a much trickier problem than taking derivatives. Lisp makes it trivial to parse
    formulas using its syntax, while in other languages, waiting to do this exercise
    until you take a course on compilers is often easier).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math><msup><mi>f</mi> <mo>'</mo></msup></math> 用于表示 *f* 的导数，<math alttext="g
    prime"><msup><mi>g</mi> <mo>'</mo></msup></math> 用于表示 *g* 的导数。有了这些规则，很容易想象如何为一维微积分编写自动微分引擎。事实上，在基于
    Lisp 的课程中，创建这样一个微分引擎通常是一年级的编程练习。（事实证明，正确解析函数比求导数更加困难。Lisp 使用其语法轻松解析公式，而在其他语言中，等到上编译器课程再做这个练习通常更容易）。
- en: How might these rules be extended to calculus of higher dimensions? Getting
    the math right is trickier, since there are many more numbers to consider. For
    example, given *X* = *AB* where *X*, *A*, *B* are all matrices, the formula comes
    out to be
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将这些规则扩展到更高维度的微积分？搞定数学更加棘手，因为需要考虑更多的数字。例如，给定 *X* = *AB*，其中 *X*、*A*、*B* 都是矩阵，公式变成了
- en: <math display="block"><mrow><mi>∇</mi> <mi>A</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><mi>A</mi></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><mi>X</mi></mrow></mfrac> <msup><mi>B</mi> <mi>T</mi></msup> <mo>=</mo>
    <mrow><mo>(</mo> <mi>∇</mi> <mi>X</mi> <mo>)</mo></mrow> <msup><mi>B</mi> <mi>T</mi></msup></mrow></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>∇</mi> <mi>A</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><mi>A</mi></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><mi>X</mi></mrow></mfrac> <msup><mi>B</mi> <mi>T</mi></msup> <mo>=</mo>
    <mrow><mo>(</mo> <mi>∇</mi> <mi>X</mi> <mo>)</mo></mrow> <msup><mi>B</mi> <mi>T</mi></msup></mrow></math>
- en: Formulas like this can be combined to provide a symbolic differentiation system
    for vectorial and tensorial calculus.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的公式可以组合起来提供一个矢量和张量微积分的符号微分系统。
- en: Learning with TensorFlow
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行学习
- en: In the rest of this chapter, we will cover the concepts that you need to learn
    basic machine learning models with TensorFlow. We will start by introducing the
    concept of toy datasets, and will explain how to create meaningful toy datasets
    using common Python libraries. Next, we will discuss new TensorFlow ideas such
    as placeholders, feed dictionaries, name scopes, optimizers, and gradients. The
    next section will show you how to use these concepts to train simple regression
    and classification models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将介绍您学习使用 TensorFlow 创建基本机器学习模型所需的概念。我们将从介绍玩具数据集的概念开始，然后解释如何使用常见的
    Python 库创建有意义的玩具数据集。接下来，我们将讨论新的 TensorFlow 想法，如占位符、喂养字典、名称范围、优化器和梯度。下一节将向您展示如何使用这些概念训练简单的回归和分类模型。
- en: Creating Toy Datasets
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建玩具数据集
- en: In this section, we will discuss how to create simple but meaningful synthetic
    datasets, or toy datasets, that we will use to train simple supervised classification
    and regression models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何创建简单但有意义的合成数据集，或称为玩具数据集，用于训练简单的监督分类和回归模型。
- en: An (extremely) brief introduction to NumPy
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对 NumPy 的（极其）简要介绍
- en: We will make heavy use of NumPy in order to define useful toy datasets. NumPy
    is a Python package that allows for manipulation of tensors (called `ndarray`s
    in NumPy). [Example 3-1](#ch3-numpy) shows some basics.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将大量使用 NumPy 来定义有用的玩具数据集。NumPy 是一个允许操作张量（在 NumPy 中称为 `ndarray`）的 Python 包。[示例
    3-1](#ch3-numpy) 展示了一些基础知识。
- en: Example 3-1\. Some examples of basic NumPy usage
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-1。一些基本 NumPy 用法示例
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You may notice that NumPy `ndarray` manipulation looks remarkably similar to
    TensorFlow tensor manipulation. This similarity was purposefully designed by TensorFlow’s
    architects. Many key TensorFlow utility functions have similar arguments and forms
    to analogous functions in NumPy. For this purpose, we will not attempt to introduce
    NumPy in great depth, and will trust readers to use experimentation to work out
    NumPy usage. There are numerous online resources that provide tutorial introductions
    to NumPy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到 NumPy `ndarray` 操作看起来与 TensorFlow 张量操作非常相似。这种相似性是 TensorFlow 架构师特意设计的。许多关键的
    TensorFlow 实用函数具有与 NumPy 中类似函数的参数和形式。出于这个目的，我们不会试图深入介绍 NumPy，并相信读者通过实验来掌握 NumPy
    的用法。有许多在线资源提供了 NumPy 的教程介绍。
- en: Why are toy datasets important?
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么玩具数据集很重要？
- en: In machine learning, it is often critical to learn to properly use toy datasets.
    Learning is challenging, and one of the most common mistakes beginners make is
    trying to learn nontrivial models on complex data too soon. These attempts often
    end in abject failure, and the would-be machine learner walks away dejected and
    convinced machine learning isn’t for them.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，学会正确使用玩具数据集通常至关重要。学习是具有挑战性的，初学者经常犯的一个最常见的错误是尝试在太早的时候在复杂数据上学习非平凡的模型。这些尝试往往以惨败告终，想要成为机器学习者的人会灰心丧气，认为机器学习不适合他们。
- en: The real culprit here of course isn’t the student, but rather the fact that
    real-world datasets have many idiosyncrasies. Seasoned data scientists have learned
    that real-world datasets often require many clean-up and preprocessing transformations
    before becoming amenable to learning. Deep learning exacerbates this problem,
    since most deep learning models are notoriously sensitive to infelicities in data.
    Issues like a wide range of regression labels, or underlying strong noise patterns
    can throw off gradient-descent–based methods, even when other machine learning
    algorithms (such as random forests) would have no issues.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, it’s almost always possible to deal with these issues, but doing so
    can require considerable sophistication on the part of the data scientist. These
    sensitivity issues are perhaps the biggest roadblock to the commoditization of
    machine learning as a technology. We will go into depth on data clean-up strategies,
    but for the time being, we recommend a much simpler alternative: use toy datasets!'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Toy datasets are critical for understanding learning algorithms. Given very
    simple synthetic datasets, it is trivial to gauge whether the algorithm has learned
    the correct rule. On more complex datasets, this judgment can be highly challenging.
    Consequently, for the remainder of this chapter, we will only use toy datasets
    as we cover the fundamentals of gradient-descent–based learning with TensorFlow.
    We will dive deep into case studies with real-world data in the following chapters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise with Gaussians
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier, we discussed discrete probability distributions as a tool for turning
    discrete choices into continuous values. We also alluded to the idea of a continuous
    probability distribution but didn’t dive into it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Continuous probability distributions (more accurately known as probability density
    functions) are a useful mathematical tool for modeling random events that may
    have a range of outcomes. For our purposes, it is enough to think of probability
    density functions as a useful tool for modeling some measurement error in gathering
    data. The Gaussian distribution is widely used for noise modeling.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 3-5](#ch3-gaussian) shows, note that Gaussians can have different
    *means* <math><mi>μ</mi></math> and *standard deviations* <math alttext="sigma"><mi>σ</mi></math>
    . The mean of a Gaussian is the average value it takes, while the standard deviation
    is a measure of the spread around this average value. In general, adding a Gaussian
    random variable onto some quantity provides a structured way to fuzz the quantity
    by making it vary slighty. This is a very useful trick for coming up with nontrivial
    synthetic datasets.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![gaussian.png](assets/tfdl_0305.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Illustrations of various Gaussian probability distributions with
    different means and standard deviations.
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We quickly note that the Gaussian distribution is also called the Normal distribution.
    A Gaussian with mean <math><mi>μ</mi></math> and standard deviation <math alttext="sigma"><mi>σ</mi></math>
    is written <math alttext="upper N left-parenthesis mu comma sigma right-parenthesis"><mrow><mi>N</mi>
    <mo>(</mo> <mi>μ</mi> <mo>,</mo> <mi>σ</mi> <mo>)</mo></mrow></math> . This shorthand
    notation is convenient, and we will use it many times in the coming chapters.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Toy regression datasets
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest form of linear regression is learning the parameters for a one-dimensional
    line. Suppose that our datapoints *x* are one-dimensional. Then suppose that real-valued
    labels *y* are generated by a linear rule
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo>
    <mi>b</mi></mrow></math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Here, *w*, *b* are the learnable parameters that must be estimated from data
    by gradient descent. In order to test that we can learn these parameters with
    TensorFlow, we will generate an artificial dataset consisting of points upon a
    straight line. To make the learning challenge a little more difficult, we will
    add a small amount of Gaussian noise to the dataset.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write down the equation for our line perturbed by a small amount of Gaussian
    noise:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo>
    <mi>b</mi> <mo>+</mo> <mi>N</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>ϵ</mi> <mo>)</mo></mrow></math>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Here <math alttext="epsilon"><mi>ϵ</mi></math> is the standard deviation of
    the noise term. We can then use NumPy to generate an artificial dataset drawn
    from this distribution, as shown in [Example 3-2](#ch3-numpy-sample).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-2\. Using NumPy to sample an artificial dataset
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We plot this dataset using Matplotlib in [Figure 3-6](#ch3-toyregplot). (you
    can find the code in [the GitHub repo](https://github.com/matroid/dlwithtf) associated
    with this book to see the exact plotting code) to verify that synthetic data looks
    reasonable. As expected, the data distribution is a straight line, with a small
    amount of measurement error.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![lr_data.png](assets/tfdl_0306.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Plot of the toy regression data distribution.
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Toy classification datasets
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s a little trickier to create a synthetic classification dataset. Logically,
    we want two distinct classes of points, which are easily separated. Suppose that
    the dataset consists of only two types of points, (–1, –1) and (1, 1). Then a
    learning algorithm would have to learn a rule that separates these two data values.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '*y*[0] = (–1, –1)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*[1] = (1, 1)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As before, let’s make the challenge a little more difficult by adding some
    Gaussian noise to both types of points:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '*y*[0] = (–1, –1) + *N*(0, ϵ)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*[1] = (1, 1) + *N*(0, ϵ)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, there’s a slight bit of trickiness here. Our points are two-dimensional,
    while the Gaussian noise we introduced previously is one-dimensional. Luckily,
    there exists a multivariate extension of the Gaussian. We won’t discuss the intricacies
    of the multivariate Gaussian here, but you do not need to understand the intricacies
    to follow our discussion.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The NumPy code to generate the synthetic dataset in [Example 3-3](#ch3-synth-2d)
    is slightly trickier than that for the linear regression problem since we have
    to use the stacking function `np.vstack` to combine the two different types of
    datapoints and associate them with different labels. (We use the related function
    `np.concatenate` to combine the one-dimensional labels.)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-3\. Sample a toy classification dataset with NumPy
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Figure 3-7](#ch3-toyclassplot) plots the data generated by this code with
    Matplotlib to verify that the distribution is as expected. We see that the data
    resides in two classes that are neatly separated.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic_data.png](assets/tfdl_0307.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. Plot of the toy classification data distribution.
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: New TensorFlow Concepts
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating simple machine learning systems in TensorFlow will require that you
    learn some new TensorFlow concepts.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Placeholders
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A placeholder is a way to input information into a TensorFlow computation graph.
    Think of placeholders as the input nodes through which information enters TensorFlow.
    The key function used to create placeholders is `tf.placeholder` ([Example 3-4](#ch3-place)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-4\. Create a TensorFlow placeholder
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will use placeholders to feed datapoints *x* and labels *y* to our regression
    and classification algorithms.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Feed dictionaries and Fetches
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that we can evaluate tensors in TensorFlow by using `sess.run(var)`.
    How do we feed in values for placeholders in our TensorFlow computations then?
    The answer is to construct *feed dictionaries*. Feed dictionaries are Python dictionaries
    that map TensorFlow tensors to `np.ndarray` objects that contain the concrete
    values for these placeholders. A feed dictionary is best viewed as an input to
    a TensorFlow computation graph. What then is an output? TensorFlow calls these
    outputs *fetches*. You have seen fetches already. We used them extensively in
    the previous chapter without calling them as such; the fetch is a tensor (or tensors)
    whose value is retrieved from the computation graph after the computation (using
    placeholder values from the feed dictionary) is run to completion ([Example 3-5](#ch3-fetch)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-5\. Using fetches
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Name scopes
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In complicated TensorFlow programs, there will be many tensors, variables, and
    placeholders defined throughout the program. `tf.name_scope(name)` provides a
    simple scoping mechanism for managing these collections of variables ([Example 3-6](#ch3-namescope)).
    All computational graph elements created within the scope of a `tf.name_scope(name)`
    call will have `name` prepended to their names.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: This organizational tool is most useful when combined with TensorBoard, since
    it aids the visualization system in automatically grouping graph elements within
    the same name scope. You will learn more about TensorBoard further in the next
    section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-6\. Using namescopes to organize placeholders
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Optimizers
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The primitives introduced in the last two sections already hint at how machine
    learning is done in TensorFlow. You have learned how to add placeholders for datapoints
    and labels and how to use tensorial operations to define the loss function. The
    missing piece is that you still don’t know how to perform gradient descent using
    TensorFlow.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: While it is in fact possible to define optimization algorithms such as gradient
    descent directly in Python using TensorFlow primitives, TensorFlow provides a
    collection of optimization algorithms in the `tf.train` module. These algorithms
    can be added as nodes to the TensorFlow computation graph.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Which optimizer should I use?
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many possible optimizers available in `tf.train`. For a short preview,
    this list includes `tf.train.GradientDescentOptimizer`, `tf.train.MomentumOptimizer`,
    `tf.train.AdagradOptimizer`, `tf.train.AdamOptimizer`, and many more. What’s the
    difference between these various optimizers?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Almost all of these optimizers are based on the idea of gradient descent. Recall
    the simple gradient descent rule we previously introduced:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mi>W</mi> <mo>-</mo> <mi>α</mi>
    <mi>∇</mi> <mi>W</mi></mrow></math>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, this update rule is primitive. There are a variety of mathematical
    tricks that researchers have discovered that enable faster optimization without
    using too much extra computation. In general, `tf.train.AdamOptimizer` is a good
    default that is relatively robust. (Many optimizer methods are very sensitive
    to hyperparameter choice. It’s better for beginners to avoid trickier methods
    until they have a good grasp of the behavior of different optimization algorithms.)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 3-7](#ch3-optim) is a short bit of code that adds an optimizer to
    the computation graph that minimizes a predefined loss `l`.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-7\. Adding an Adam optimizer to TensorFlow computation graph
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Taking gradients with TensorFlow
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We mentioned previously that it is possible to directly implement gradient descent
    algorithms in TensorFlow. While most use cases don’t need to reimplement the contents
    of `tf.train`, it can be useful to look at gradient values directly for debugging
    purposes. `tf.gradients` provides a useful tool for doing so ([Example 3-8](#ch3-grad)).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-8\. Taking gradients directly
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code snippet symbolically pulls down the gradients of loss `l` with respect
    to learnable parameter (`tf.Variable`) `W`. `tf.gradients` returns a list of the
    desired gradients. Note that the gradients are themselves tensors! TensorFlow
    performs symbolic differentiation, which means that gradients themselves are parts
    of the computational graph. One neat side effect of TensorFlow’s symbolic gradients
    is that it’s possible to stack derivatives in TensorFlow. This can sometimes be
    useful for more advanced algorithms.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Summaries and file writers for TensorBoard
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gaining a visual understanding of the structure of a tensorial program can be
    very useful. The TensorFlow team provides the TensorBoard package for this purpose.
    TensorBoard starts a web server (on localhost by default) that displays various
    useful visualizations of a TensorFlow program. However, in order for TensorFlow
    programs to be inspected with TensorBoard, programmers must manually write logging
    statements. `tf.train.FileWriter()` specifies the logging directory for a TensorBoard
    program and `tf.summary` writes summaries of various TensorFlow variables to the
    specified logging directory. In this chapter, we will only use `tf.summary.scalar`,
    which summarizes a scalar quantity, to track the value of the loss function. `tf.summary.merge_all()`
    is a useful logging aid that merges multiple summaries into a single summary for
    convenience.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The code snippet in [Example 3-9](#ch3-logging) adds a summary for the loss
    and specifies a logging directory.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-9\. Adding a summary for the loss
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training models with TensorFlow
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose now that we have specified placeholders for datapoints and labels, and
    have defined a loss with tensorial operations. We have added an optimizer node
    `train_op` to the computational graph, which we can use to perform gradient descent
    steps (while we may actually use a different optimizer, we will refer to updates
    as gradient descent for convenience). How can we iteratively perform gradient
    descent to learn on this dataset?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The simple answer is that we use a Python `for`-loop. In each iteration, we
    use `sess.run()` to fetch the `train_op` along with the merged summary op `merged`
    and the loss `l` from the graph. We feed all datapoints and labels into `sess.run()`
    using a feed dictionary.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The code snippet in [Example 3-10](#ch3-simplelearn) demonstrates this simple
    learning method. Note that we don’t make use of minibatches for pedagogical simplicity.
    Code in following chapters will use minibatches when training on larger datasets.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-10\. A simple example of training a model
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Training Linear and Logistic Models in TensorFlow
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section ties together all the TensorFlow concepts introduced in the previous
    section to train linear and logistic regression models upon the toy datasets we
    introduced previously in the chapter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression in TensorFlow
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will provide code to define a linear regression model in
    TensorFlow and learn its weights. This task is straightforward and you can do
    it without TensorFlow easily. Nevertheless, it’s a good exercise to do in TensorFlow
    since it will bring together the new concepts that we have introduced throughout
    the chapter.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Defining and training linear regression in TensorFlow
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The model for a linear regression is straightforward:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo>
    <mi>b</mi></mrow></math>
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Here *w* and *b* are the weights we wish to learn. We transform these weights
    into `tf.Variable` objects. We then use tensorial operations to construct the
    *L*² loss:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mi>w</mi><mi>x</mi><mo>-</mo><mi>b</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Example 3-11](#ch3-linmod) implements these mathematical operations
    in TensorFlow. It also uses `tf.name_scope` to group various operations, and adds
    a `tf.train.AdamOptimizer` for learning and `tf.summary` operations for TensorBoard
    usage.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-11\. Defining a linear regression model
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Example 3-12](#ch3-lintrain) then trains this model as discussed previously
    (without using minibatches).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-12\. Training the linear regression model
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: All code for this example is provided in the [GitHub repository](https://github.com/matroid/dlwithtf)
    associated with this book. We encourage all readers to run the full script for
    the linear regression example to gain a firsthand sense for how the learning algorithm
    functions. The example is small enough that readers will not need access to any
    special-purpose computing hardware to run.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Taking Gradients for Linear Regression
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The equation for the linear system we’re modeling is *y* = *wx* + *b* where
    *w*, *b* are the learnable weights. As we mentioned previously, the loss for this
    system is <math alttext="script upper L equals left-parenthesis y minus w x minus
    b right-parenthesis squared"><mrow><mi>ℒ</mi> <mo>=</mo> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mi>w</mi><mi>x</mi><mo>-</mo><mi>b</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> . Some matrix calculus can be used to compute
    the gradients of the learnable parameters directly for *w*:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>∇</mi> <mi>w</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow>
    <mrow><mi>∂</mi><mi>w</mi></mrow></mfrac> <mo>=</mo> <mo>-</mo> <mn>2</mn> <mrow><mo>(</mo>
    <mi>y</mi> <mo>-</mo> <mi>w</mi> <mi>x</mi> <mo>-</mo> <mi>b</mi> <mo>)</mo></mrow>
    <msup><mi>x</mi> <mi>T</mi></msup></mrow></math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: and for *b*
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>∇</mi> <mi>b</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow>
    <mrow><mi>∂</mi><mi>b</mi></mrow></mfrac> <mo>=</mo> <mo>-</mo> <mn>2</mn> <mrow><mo>(</mo>
    <mi>y</mi> <mo>-</mo> <mi>w</mi> <mi>x</mi> <mo>-</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: We place these equations here only for reference for curious readers. We will
    not attempt to systematically teach how to take the derivatives of the loss functions
    we encounter in this book. However, we will note that for complicated systems,
    taking the derivative of the loss function by hand helps build up an intuition
    for how the deep network learns. This intuition can serve as a powerful guide
    for the designer, so we encourage advanced readers to pursue this topic on their
    own.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing linear regression models with TensorBoard
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model defined in the previous section uses `tf.summary.FileWriter` to write
    logs to a logging directory */tmp/lr-train*. We can invoke TensorBoard on this
    logging directory with the command in [Example 3-13](#ch3-tensorboard) (TensorBoard
    is installed by default with TensorFlow).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-13\. Invoking TensorBoard
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This command will start TensorBoard on a port attached to localhost. Use your
    browser to open this port. The TensorBoard screen will look something like [Figure 3-8](#ch3-tensorboardscreen).
    (The precise appearance may vary depending on your version of TensorBoard.)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![tensorboard_lr_raw.png](assets/tfdl_0308.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. Screenshot of TensorBoard panel.
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Navigate to the Graphs tab, and you will see a visualization of the TensorFlow
    architecture we have defined as illustrated in [Figure 3-9](#ch3-tensorboardarch).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![lr_graph.png](assets/tfdl_0309.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. Visualization of linear regression architecture in TensorBoard.
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that this visualization has grouped all computational graph elements belonging
    to various `tf.name_scopes`. Different groups are connected according to their
    dependencies in the computational graph. You can expand all of the grouped elements
    to view their contents. [Figure 3-10](#ch3-tensorboardarchexp) illustrates the
    expanded architecture.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are many hidden nodes that suddenly become visible! TensorFlow
    functions like `tf.train.AdamOptimizer` often hide many internal variables under
    a `tf.name_scope` of their own. Expanding in TensorBoard provides an easy way
    to peer underneath the hood to see what the system is actually creating. Although
    the visualization looks quite complex, most of these details are under the hood
    and not anything you need to worry about just yet.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![lr_expanded.png](assets/tfdl_0310.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. Expanded visualization of architecture.
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Navigate back to the Home tab and open the Summaries section. You should now
    see a loss curve that looks something like [Figure 3-11](#ch3-tensorboardloss).
    Note the smooth falling shape. The loss falls rapidly at the beginning as the
    prior is learned, then tapers off and settles.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![lr_loss_tensorboard.png](assets/tfdl_0311.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. Viewing the loss curve in TensorBoard.
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Visual and Nonvisual Debugging Styles
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is using a tool like TensorBoard necessary to get good use out of a system like
    TensorFlow? It depends. Is using a GUI or an interactive debugger necessary to
    be a professional programmer?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Different programmers have different styles. Some will find that the visualization
    capabilities of TensorBoard come to form a critical part of their tensorial programming
    workflows. Others will find that TensorBoard isn’t terribly useful and will make
    greater use of print-statement debugging. Both styles of tensorial programming
    and debugging are valid, just as there are great programmers who swear by debuggers
    and others who loathe them.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: In general, TensorBoard is quite useful for debugging and for building basic
    intuition about the dataset at hand. We recommend that you follow the style that
    works best for you.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for evaluating regression models
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we haven’t discussed how to evaluate whether a trained model has actually
    learned anything. The first tool for evaluating whether a model has trained is
    by looking at the loss curve to ensure it has a reasonable shape. You learned
    how to do this in the previous section. What’s the next thing to try?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'We now want you to look at *metrics* associated with the model. A metric is
    a tool for comparing predicted labels to true labels. For regression problems,
    there are two common metrics: *R*² and RMSE (root-mean-squared error). The *R*²
    is a measure of the correlation between two variables that takes values between
    +1 and 0\. +1 indicates perfect correlation, while 0 indicates no correlation.
    Mathematically, the *R*² for two datasets *X* and *Y* is defined as'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mtext>cov</mtext><msup><mrow><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow> <mrow><msubsup><mi>σ</mi> <mi>X</mi> <mn>2</mn></msubsup>
    <msubsup><mi>σ</mi> <mi>Y</mi> <mn>2</mn></msubsup></mrow></mfrac></mrow></math>
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Where cov(*X*, *Y*) is the covariance of *X* and *Y*, a measure of how the two
    datasets jointly vary, while <math alttext="sigma Subscript upper X"><msub><mi>σ</mi>
    <mi>X</mi></msub></math> and <math alttext="sigma Subscript upper Y"><msub><mi>σ</mi>
    <mi>Y</mi></msub></math> are standard deviations, measures of how much each set
    individually varies. Intuitively, the *R*² measures how much of the independent
    variation in each set can be explained by their joint variation.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Types of R²!
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that there are two common definitions of *R*² used in practice. A common
    beginner (and expert) mistake is to confuse the two definitions. In this book,
    we will always use the squared Pearson correlation coefficient ([Figure 3-12](#ch3-corr)).
    The other definition is called the coefficient of determination. This other *R*²
    is often much more confusing to deal with since it doesn’t have a lower bound
    of 0 like the squared Pearson correlation does.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 3-12](#ch3-corr), predicted and true values are highly correlated
    with an *R*² of nearly 1\. It looks like learning has done a wonderful job on
    this system and succeeded in learning the true rule. *Not so fast*. You will note
    that the scale on the two axes in the figure isn’t the same! It turns out that
    *R*² doesn’t penalize for differences in scale. In order to understand what’s
    happened on this system, we need to consider an alternate metric in [Figure 3-13](#ch3-rms).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![lr_pred.png](assets/tfdl_0312.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Plotting the Pearson correlation coefficient.
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![lr_learned.png](assets/tfdl_0313.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. Plotting the root-mean-squared error (RMSE).
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The RMSE is a measure of the average difference between predicted values and
    true values. In [Figure 3-13](#ch3-rms) we plot predicted values and true labels
    as two separate functions using datapoints *x* as our x-axis. Note that the line
    learned isn’t the true function! The RMSE is relatively high and diagnoses the
    error, unlike the *R*², which didn’t pick up on this error.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: What happened on this system? Why didn’t TensorFlow learn the correct function
    despite being trained to convergence? This example provides a good illustration
    of one of the weaknesses of gradient descent algorithms. There is no guarantee
    of finding the true solution! The gradient descent algorithm can get trapped in
    *local minima*. That is, it can find solutions that look good, but are not in
    fact the lowest minima of the loss function <math alttext="script upper L"><mi>ℒ</mi></math>
    .
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Why use gradient descent at all then? For simple systems, it is indeed often
    better to avoid gradient descent and use other algorithms that have stronger performance
    guarantees. However, on complicated systems, such as those we will show you in
    later chapters, there do not yet exist alternative algorithms that perform better
    than gradient descent. We encourage you to remember this fact as we proceed further
    into deep learning.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression in TensorFlow
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will define a simple classifier using TensorFlow. It’s worth
    first considering what the equation is for a classifier. The mathematical trick
    that is commonly used is exploiting the sigmoid function. The sigmoid, plotted
    in [Figure 3-14](#ch3-sigmoid), commonly denoted by <math alttext="sigma"><mi>σ</mi></math>
    , is a function from the real numbers <math alttext="double-struck upper R"><mi>ℝ</mi></math>
    to (0, 1). This property is convenient since we can interpret the output of a
    sigmoid as probability of an event happening. (The trick of converting discrete
    events into continuous values is a recurring theme in machine learning.)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic.gif](assets/tfdl_0314.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 3-14\. Plotting the sigmoid function.
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The equations for predicting the probabilities of a discrete 0/1 variable follow.
    These equations define a simple logistic regression model:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>y</mi> <mn>0</mn></msub> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math><math
    display="block"><mrow><msub><mi>y</mi> <mn>1</mn></msub> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow provides utility functions to compute the cross-entropy loss for
    sigmoidal values. The simplest of these functions is `tf.nn.sigmoid_cross_​entropy_with_logits`.
    (A logit is the inverse of the sigmoid. In practice, this simply means passing
    the argument to the sigmoid, *wx* + *b*, directly to TensorFlow instead of the
    sigmoidal value <math><mrow><mi>σ</mi> <mo>(</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo>
    <mi>b</mi> <mo>)</mo></mrow></math> itself). We recommend using TensorFlow’s implementation
    instead of manually defining the cross-entropy, since there are tricky numerical
    issues that arise when computing the cross-entropy loss.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 3-14](#ch3-logistic) defines a simple logistic regression model in
    TensorFlow.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-14\. Defining a simple logistic regression model
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The training code for this model in [Example 3-15](#ch3-logistic-train) is identical
    to that for the linear regression model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-15\. Training a logistic regression model
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Visualizing logistic regression models with TensorBoard
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As before, you can use TensorBoard to visualize the model. Start by visualizing
    the loss function as shown in [Figure 3-15](#ch3-tensorboardlogistic). Note that
    as before, the loss function follows a neat pattern. There is a steep drop in
    the loss followed by a gradual smoothening.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic_loss_tensorboard.png](assets/tfdl_0315.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Figure 3-15\. Visualizing the logistic regression loss function.
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also view the TensorFlow graph in TensorBoard. Since the scoping structure
    was similar to that used for linear regression, the simplified graph doesn’t display
    much differently, as shown in [Figure 3-16](#ch3-tensorboardlogisticgraph).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic_graph.png](assets/tfdl_0316.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Figure 3-16\. Visualizing the computation graph for logistic regression.
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: However, if you expand the nodes in this grouped graph, as in [Figure 3-17](#ch3-tensorboardlogisticgraphexp),
    you will find that the underlying computational graph is different. In particular,
    the loss function is quite different from that used for linear regression (as
    it should be).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic_expanded.png](assets/tfdl_0317.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: Figure 3-17\. The expanded computation graph for logistic regression.
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Metrics for evaluating classification models
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you have trained a classification model for logistic regression, you
    need to learn about metrics suitable for evaluating classification models. Although
    the equations for logistic regression are more complicated than they are for linear
    regression, the basic evaluation metrics are simpler. The classification accuracy
    simply checks for the fraction of datapoints that are classified correctly by
    the learned model. In fact, with a little more effort, it is possible to back
    out the *separating line* learned by the logistic regression model. This line
    displays the cutoff boundary the model has learned to separate positive and negative
    examples. (We leave the derivation of this line from the logistic regression equations
    as an exercise for the interested reader. The solution is in the code for this
    section.)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: We display the learned classes and the separating line in [Figure 3-18](#ch3-loglearnedclasses).
    Note that the line neatly separates the positive and negative examples and has
    perfect accuracy (1.0). This result raises an interesting point. Regression is
    often a harder problem to solve than classification. There are many possible lines
    that would neatly separate the datapoints in [Figure 3-18](#ch3-loglearnedclasses),
    but only one that would have perfectly matched the data for the linear regression.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic_pred.png](assets/tfdl_0318.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: Figure 3-18\. Viewing the learned classes and separating line for logistic regression.
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Review
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve shown you how to build and train some simple learning
    systems in TensorFlow. We started by reviewing some foundational mathematical
    concepts including loss functions and gradient descent. We then introduced you
    to some new TensorFlow concepts such as placeholders, scopes, and TensorBoard.
    We ended the chapter with case studies that trained linear and logistic regression
    systems on toy datasets. We covered a lot of material in this chapter, and it’s
    OK if you haven’t yet internalized everything. The foundational material introduced
    here will be used throughout the remainder of this book.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#fully_connected_networks), we will introduce you to
    your first deep learning model and to fully connected networks, and will show
    you how to define and train fully connected networks in TensorFlow. In following
    chapters, we will explore more complicated deep networks, but all of these architectures
    will use the same fundamental learning principles introduced in this chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
