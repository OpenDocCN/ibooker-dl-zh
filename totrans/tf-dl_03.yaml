- en: Chapter 3\. Linear and Logistic Regression with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。使用TensorFlow进行线性和逻辑回归
- en: This chapter will show you how to build simple, but nontrivial, examples of
    learning systems in TensorFlow. The first part of this chapter reviews the mathematical
    foundations for building learning systems and in particular will cover functions,
    continuity, and differentiability. We introduce the idea of loss functions, then
    discuss how machine learning boils down to the ability to find the minimal points
    of complicated loss functions. We then cover the notion of gradient descent, and
    explain how it can be used to minimize loss functions. We end the first section
    by briefly discussing the algorithmic idea of automatic differentiation. The second
    section focuses on introducing the TensorFlow concepts underpinned by these mathematical
    ideas. These concepts include placeholders, scopes, optimizers, and TensorBoard,
    and enable the practical construction and analysis of learning systems. The final
    section provides case studies of how to train linear and logistic regression models
    in TensorFlow.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向您展示如何在TensorFlow中构建简单但非平凡的学习系统示例。本章的第一部分回顾了构建学习系统的数学基础，特别涵盖了函数、连续性和可微性。我们介绍了损失函数的概念，然后讨论了机器学习如何归结为找到复杂损失函数的最小点的能力。然后我们介绍了梯度下降的概念，并解释了如何使用它来最小化损失函数。我们最后简要讨论了自动微分的算法思想。第二部分重点介绍了这些数学思想支撑的TensorFlow概念。这些概念包括占位符、作用域、优化器和TensorBoard，可以实现学习系统的实际构建和分析。最后一部分提供了如何在TensorFlow中训练线性和逻辑回归模型的案例研究。
- en: This chapter is long and introduces many new ideas. It’s OK if you don’t grasp
    all the subtleties of these ideas in a first reading. We recommend moving forward
    and coming back to refer to the concepts here as needed later. We will repeatedly
    use these fundamentals in the remainder of the book in order to let these ideas
    sink in gradually.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章很长，介绍了许多新的概念。如果您在第一次阅读时没有完全理解这些概念的微妙之处，那没关系。我们建议继续前进，以后有需要时再回来参考这里的概念。我们将在本书的其余部分中反复使用这些基础知识，以便让这些思想逐渐沉淀。
- en: Mathematical Review
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学复习
- en: This first section reviews the mathematical tools needed to conceptually understand
    machine learning. We attempt to minimize the number of Greek symbols required,
    and focus instead on building conceptual understanding rather than technical manipulations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了概念上理解机器学习所需的数学工具。我们试图尽量减少所需的希腊符号数量，而是专注于建立概念理解而不是技术操作。
- en: Functions and Differentiability
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 函数和可微性
- en: This section will provide you with a brief overview of the concepts of functions
    and differentiability. A function *f* is a rule that takes an input to an output.
    There are functions in all computer programming languages, and the mathematical
    definition of a function isn’t really much different. However, mathematical functions
    commonly used in physics and engineering have other important properties such
    as continuity and differentiability. A continuous function, loosely speaking,
    is one that can be drawn without lifting your pencil from the paper, as shown
    in [Figure 3-1](#ch3-cont). (This is of course not the technical definition, but
    it captures the spirit of the continuity condition.)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将为您提供函数和可微性概念的简要概述。函数*f*是将输入映射到输出的规则。所有计算机编程语言中都有函数，数学上对函数的定义实际上并没有太大不同。然而，在物理学和工程学中常用的数学函数具有其他重要属性，如连续性和可微性。连续函数，粗略地说，是可以在不从纸上抬起铅笔的情况下绘制的函数，如[图3-1](#ch3-cont)所示。（这当然不是技术定义，但它捕捉了连续性条件的精神。）
- en: '![continuous_1.gif](assets/tfdl_0301.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![continuous_1.gif](assets/tfdl_0301.png)'
- en: Figure 3-1\. Some continuous functions.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。一些连续函数。
- en: Differentiability is a type of smoothness condition on functions. It says no
    sharp corners or turns are allowed in the function ([Figure 3-2](#ch3-diff)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 可微性是函数上的一种平滑条件。它表示函数中不允许有尖锐的角或转折（[图3-2](#ch3-diff)）。
- en: '![Math_images_4.jpg](assets/tfdl_0302.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![Math_images_4.jpg](assets/tfdl_0302.png)'
- en: Figure 3-2\. A differentiable function.
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2。一个可微函数。
- en: The key advantage of differentiable functions is that we can use the slope of
    the function at a particular point as a guide to find places where the function
    is higher or lower than our current position. This allows us to find the *minima*
    of the function. The *derivative* of differentiable function *f*, denoted <math><msup><mi>f</mi>
    <mo>'</mo></msup></math> , is another function that provides the slope of the
    original function at all points. The conceptual idea is that the derivative of
    a function at a given point gives a signpost pointing to directions where the
    function is higher or lower than its current value. An optimization algorithm
    can follow this signpost to move closer to a minima of *f*. At the minima itself,
    the function will have derivative zero.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 可微函数的关键优势在于我们可以利用函数在特定点的斜率作为指导，找到函数高于或低于当前位置的地方。这使我们能够找到函数的*最小值*。可微函数*f*的*导数*，表示为<math><msup><mi>f</mi>
    <mo>'</mo></msup></math>，是另一个函数，提供原始函数在所有点的斜率。概念上，函数在给定点的导数指示了函数高于或低于当前值的方向。优化算法可以遵循这个指示牌，向*
    f *的最小值靠近。在最小值处，函数的导数为零。
- en: The power of derivative-driven optimization isn’t apparent at first. Generations
    of calculus students have suffered through stultifying exercises minimizing tiny
    functions on paper. These exercises aren’t useful since finding the minima of
    a function with only a small number of input parameters is a trivial exercise
    best done graphically. The power of derivative-driven optimization only becomes
    evident when there are hundreds, thousands, millions, or billions of variables.
    At these scales, understanding the function analytically is nigh impossible, and
    all visualizations are fraught exercises that may well miss the key attributes
    of the function. At these scales, the *gradient* of the function <math><mrow><mi>∇</mi>
    <mi>f</mi></mrow></math> , a generalization of <math><msup><mi>f</mi> <mo>'</mo></msup></math>
    to multivariate functions, is likely the most powerful mathematical tool to understand
    the function and its behavior. We will dig into gradients in more depth later
    in this chapter. (Conceptually that is; we won’t cover the technical details of
    gradients in this work.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，导数驱动的优化的力量并不明显。几代微积分学生都在纸上进行枯燥的最小化函数练习中受苦。这些练习并不有用，因为找到具有少量输入参数的函数的最小值是一个最好通过图形方式完成的微不足道的练习。导数驱动的优化的力量只有在有数百、数千、数百万或数十亿个变量时才会显现出来。在这些规模上，通过解析理解函数几乎是不可能的，所有的可视化都是充满风险的练习，很可能会忽略函数的关键属性。在这些规模上，函数的*梯度*，一个多变量函数的<math><msup><mi>f</mi>
    <mo>'</mo></msup></math>的推广，很可能是理解函数及其行为的最强大的数学工具。我们将在本章后面更深入地探讨梯度。（概念上是这样；我们不会在这项工作中涵盖梯度的技术细节。）
- en: 'At a very high level, machine learning is simply the act of function minimization:
    learning algorithms are nothing more than minima finders for suitably defined
    functions. This definition has the advantage of mathematical simplicity. But,
    what are these special differentiable functions that encode useful solutions in
    their minima and how can we find them?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常高的层面上，机器学习只是函数最小化的行为：学习算法只不过是适当定义的函数的最小值查找器。这个定义具有数学上的简单性优势。但是，这些特殊的可微函数是什么，它们如何在它们的最小值中编码有用的解决方案，我们如何找到它们呢？
- en: Loss Functions
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: In order to solve a given machine learning problem, a data scientist must find
    a way of constructing a function whose minima encode solutions to the real-world
    problem at hand. Luckily for our hapless data scientist, the machine learning
    literature has built up a rich history of *loss functions* that perform such encodings.
    Practical machine learning boils down to understanding the different types of
    loss functions available and knowing which loss function should be applied to
    which problems. Put another way, the loss function is the mechanism by which a
    data science project is transmuted into mathematics. All of machine learning,
    and much of artificial intelligence, boils down to the creation of the right loss
    function to solve the problem at hand. We will give you a whirlwind tour of some
    common families of loss functions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决给定的机器学习问题，数据科学家必须找到一种构建函数的方法，其最小值编码了手头的现实世界问题的解决方案。幸运的是，对于我们这位不幸的数据科学家来说，机器学习文献已经建立了一个丰富的*损失函数*历史，执行这种编码。实际机器学习归结为理解不同类型的可用损失函数，并知道应该将哪种损失函数应用于哪些问题。换句话说，损失函数是将数据科学项目转化为数学的机制。所有的机器学习，以及大部分人工智能，都归结为创建正确的损失函数来解决手头的问题。我们将为您介绍一些常见的损失函数家族。
- en: We start by noting that a loss function <math alttext="script upper L"><mi>ℒ</mi></math>
    must satisfy some mathematical properties to be meaningful. First <math alttext="script
    upper L"><mi>ℒ</mi></math> must use both datapoints *x* and labels *y*. We denote
    this by writing the loss function as <math><mrow><mi>ℒ</mi> <mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math> . Using our language from the previous
    chapter, both *x* and *y* are tensors, and <math alttext="script upper L"><mi>ℒ</mi></math>
    is a function from pairs of tensors to scalars. What should the functional form
    of the loss function be? A common assumption that people use is to make loss functions
    *additive*. Suppose that <math alttext="left-parenthesis x Subscript i Baseline
    comma y Subscript i Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math>
    are the data available for example *i* and that there are *N* total examples.
    Then the loss function can be decomposed as
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到，损失函数<math alttext="script upper L"><mi>ℒ</mi></math>必须满足一些数学属性才能有意义。首先，<math
    alttext="script upper L"><mi>ℒ</mi></math>必须使用数据点*x*和标签*y*。我们通过将损失函数写成<math><mrow><mi>ℒ</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math>来表示这一点。使用我们在上一章中的术语，*x*和*y*都是张量，<math
    alttext="script upper L"><mi>ℒ</mi></math>是从张量对到标量的函数。损失函数的函数形式应该是什么？人们常用的一个假设是使损失函数*可加性*。假设<math
    alttext="left-parenthesis x Subscript i Baseline comma y Subscript i Baseline
    right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math>是示例*i*的可用数据，并且总共有*N*个示例。那么损失函数可以分解为
- en: <math display="block"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msub><mi>ℒ</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msub><mi>ℒ</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: '(In practice <math><msub><mi>ℒ</mi> <mi>i</mi></msub></math> is the same for
    every datapoint.) This additive decomposition allows for many useful advantages.
    The first is that derivatives factor through addition, so computing the gradient
    of the total loss simplifies as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: （在实践中，<math><msub><mi>ℒ</mi> <mi>i</mi></msub></math> 对于每个数据点都是相同的。）这种加法分解带来了许多有用的优势。首先是导数通过加法因子化，因此计算总损失的梯度简化如下：
- en: <math display="block"><mrow><mi>∇</mi> <mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mi>∇</mi> <msub><mi>ℒ</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>∇</mi> <mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mi>∇</mi> <msub><mi>ℒ</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math>
- en: This mathematical trick means that so long as the smaller functions <math><msub><mi>ℒ</mi>
    <mi>i</mi></msub></math> are differentiable, so too will the total loss function
    be. It follows that the problem of designing loss functions resolves into the
    problem of designing smaller functions <math alttext="script upper L Subscript
    i Baseline left-parenthesis x Subscript i Baseline comma y Subscript i Baseline
    right-parenthesis"><mrow><msub><mi>ℒ</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
    . Before we dive into designing the <math alttext="script upper L Subscript i"><msub><mi>ℒ</mi>
    <mi>i</mi></msub></math> , it will be convenient to take a small detour that explains
    the difference between classification and regression problems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数学技巧意味着只要较小的函数<math><msub><mi>ℒ</mi> <mi>i</mi></msub></math>是可微的，总损失函数也将是可微的。由此可见，设计损失函数的问题归结为设计较小函数<math
    alttext="script upper L Subscript i Baseline left-parenthesis x Subscript i Baseline
    comma y Subscript i Baseline right-parenthesis"><mrow><msub><mi>ℒ</mi> <mi>i</mi></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>。在我们深入设计<math alttext="script
    upper L Subscript i"><msub><mi>ℒ</mi> <mi>i</mi></msub></math>之前，我们将方便地进行一个小的旁观，解释分类和回归问题之间的区别。
- en: Classification and regression
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类和回归
- en: Machine learning algorithms can be broadly categorized as supervised or unsupervised
    problems. Supervised problems are those for which both datapoints *x* and labels
    *y* are available, while unsupervised problems have only datapoints *x* without
    labels *y*. In general, unsupervised machine learning is much harder and less
    well-defined (what does it mean to “understand” datapoints *x*?). We won’t delve
    into unsupervised loss functions at this point since, in practice, most unsupervised
    losses are cleverly repurposed supervised losses.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以广泛地分为监督或无监督问题。监督问题是指数据点*x*和标签*y*都是可用的问题，而无监督问题只有数据点*x*没有标签*y*。一般来说，无监督机器学习更加困难且定义不明确（“理解”数据点*x*是什么意思？）。我们暂时不会深入讨论无监督损失函数，因为在实践中，大多数无监督损失都是巧妙地重新利用监督损失。
- en: Supervised machine learning can be broken up into the two subproblems of classification
    and regression. A classification problem is one in which you seek to design a
    machine learning system that assigns a discrete label, say 0/1 (or more generally
    <math><mrow><mn>0</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>n</mi></mrow></math>
    ) to a given datapoint. Regression is the problem of designing a machine learning
    system that attaches a real valued label (in <math alttext="double-struck upper
    R"><mi>ℝ</mi></math> ) to a given datapoint.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习可以分为分类和回归两个子问题。分类问题是指您试图设计一个机器学习系统，为给定的数据点分配一个离散标签，比如0/1（或更一般地<math><mrow><mn>0</mn>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>n</mi></mrow></math>）。回归是指设计一个机器学习系统，为给定的数据点附加一个实值标签（在<math
    alttext="double-struck upper R"><mi>ℝ</mi></math>）。
- en: At a high level, these problems may appear rather different. Discrete objects
    and continuous objects are typically treated differently by mathematics and common
    sense. However, part of the trickery used in machine learning is to use continuous,
    differentiable loss functions to encode both classification and regression problems.
    As we’ve mentioned previously, much of machine learning is simply the art of turning
    complicated real-world systems into suitably simple differentiable functions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层来看，这些问题可能看起来相当不同。离散对象和连续对象通常在数学和常识上被不同对待。然而，机器学习中使用的一种技巧是使用连续、可微的损失函数来编码分类和回归问题。正如我们之前提到的，机器学习的很大一部分就是将复杂的现实系统转化为适当简单的可微函数的艺术。
- en: In the following sections, we will introduce you to a pair of mathematical functions
    that will prove very useful for transforming classification and regression tasks
    into suitable loss functions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将向您介绍一对数学函数，这对函数将非常有用，可以将分类和回归任务转换为适当的损失函数。
- en: L² Loss
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L²损失
- en: 'The *L*² loss (pronounced *ell-two* loss) is commonly used for regression problems.
    The *L*² loss (or *L*²-norm as it’s commonly called elsewhere) provides for a
    measure of the magnitude of a vector:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*L*²损失（读作*ell-two*损失）通常用于回归问题。*L*²损失（或者在其他地方通常称为*L*²范数）提供了一个向量大小的度量：'
- en: <math display="block"><mrow><msub><mrow><mo>∥</mo><mi>a</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msubsup><mi>a</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></msqrt></mrow></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mrow><mo>∥</mo><mi>a</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msubsup><mi>a</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></msqrt></mrow></math>
- en: 'Here, *a* is assumed to be a vector of length *N*. The *L*² norm is commonly
    used to define the distance between two vectors:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*a*被假定为长度为*N*的向量。*L*²范数通常用来定义两个向量之间的距离：
- en: <math display="block"><mrow><msub><mrow><mo>∥</mo><mi>a</mi><mo>-</mo><mi>b</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>a</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>b</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: This idea of *L*² as a distance measurement is very useful for solving regression
    problems in supervised machine learning. Suppose that *x* is a collection of data
    and *y* the associated labels. Let *f* be some differentiable function that encodes
    our machine learning model. Then to encourage *f* to predict *y*, we create the
    *L*² loss function
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mrow><mo>∥</mo><mi>f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>-</mo><mi>y</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub></mrow></math>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: As a quick note, it’s common in practice to not use the *L*² loss directly,
    but rather its square
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msubsup><mrow><mo>∥</mo><mi>a</mi><mo>-</mo><mi>b</mi><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>a</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>b</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: in order to avoid dealing with terms of the form <math alttext="1 slash StartRoot
    left-parenthesis EndRoot x right-parenthesis"><mrow><mn>1</mn> <mo>/</mo> <msqrt><mo>(</mo></msqrt>
    <mrow><mi>x</mi> <mo>)</mo></mrow></mrow></math> in the gradient. We will use
    the squared *L*² loss repeatedly in the remainder of this chapter and book.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Probability distributions
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before introducing loss functions for classification problems, it will be useful
    to take a quick aside to introduce probability distributions. To start, what is
    a probability distribution and why should we care about it for the purposes of
    machine learning? Probability is a deep subject, so we will only delve far enough
    into it for you to gain the required minimal understanding. At a high level, probability
    distributions provide a mathematical trick that allows you to relax a discrete
    set of choices into a continuum. Suppose, for example, you need to design a machine
    learning system that predicts whether a coin will fall heads up or heads down.
    It doesn’t seem like heads up/down can be encoded as a continuous function, much
    less a differentiable one. How can you then use the machinery of calculus or TensorFlow
    to solve problems involving discrete choices?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Enter the probability distribution. Instead of hard choices, make the classifier
    predict the chance of getting heads up or heads down. For example, the classifier
    may learn to predict that heads has probability 0.75 and tails has probability
    0.25\. Note that probabilities vary continuously! Consequently by working with
    the probabilities of discrete events rather than with the events themselves, you
    can neatly sidestep the issue that calculus doesn’t really work with discrete
    events.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: A probability distribution *p* is simply a listing of the probabilities for
    the possible discrete events at hand. In this case, *p* = (0.75, 0.25). Note,
    alternatively, you can view <math alttext="p colon StartSet 0 comma 1 EndSet right-arrow
    double-struck upper R"><mrow><mi>p</mi> <mo>:</mo> <mo>{</mo> <mn>0</mn> <mo>,</mo>
    <mn>1</mn> <mo>}</mo> <mo>→</mo> <mi>ℝ</mi></mrow></math> as a function from the
    set of two elements to the real numbers. This viewpoint will be useful notationally
    at times.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: We briefly note that the technical definition of a probability distribution
    is more involved. It is feasible to assign probability distributions to real-valued
    events. We will discuss such distributions later in the chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cross-entropy is a mathematical method for gauging the distance between two
    probability distributions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵是衡量两个概率分布之间距离的数学方法：
- en: <math display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo> <mi>x</mi></munder>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <mi>q</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo> <mi>x</mi></munder>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <mi>q</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
- en: Here *p* and *q* are two probability distributions. The notation *p*(*x*) denotes
    the probability *p* accords to event *x*. This definition is worth discussing
    carefully. Like the *L*² norm, *H* provides a notion of distance. Note that in
    the case where *p* = *q*,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*p*和*q*是两个概率分布。符号*p*(*x*)表示*p*赋予事件*x*的概率。这个定义值得仔细讨论。与*L*²范数一样，*H*提供了距离的概念。请注意，在*p*
    = *q*的情况下，
- en: <math display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo> <mi>x</mi></munder>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo> <mi>x</mi></munder>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
- en: This quantity is the entropy of *p* and is usually written simply *H*(*p*).
    It’s a measure of how disordered the distribution is; the entropy is maximized
    when all events are equally likely. *H*(*p*) is always less than or equal to *H*(*p*,
    *q*). In fact, the “further away” distribution *q* is from *p*, the larger the
    cross-entropy gets. We won’t dig deeply into the precise meanings of these statements,
    but the intuition of cross-entropy as a distance mechanism is worth remembering.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数量是*p*的熵，通常简单地写作*H*(*p*)。这是分布无序程度的度量；当所有事件等可能时，熵最大。*H*(*p*)总是小于或等于*H*(*p*,
    *q*)。事实上，分布*q*距离*p*越远，交叉熵就越大。我们不会深入探讨这些陈述的确切含义，但将交叉熵视为距离机制的直觉值得记住。
- en: As an aside, note that unlike *L*² norm, *H* is asymmetric! That is, <math alttext="upper
    H left-parenthesis p comma q right-parenthesis not-equals upper H left-parenthesis
    q comma p right-parenthesis"><mrow><mi>H</mi> <mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>q</mi> <mo>)</mo> <mo>≠</mo> <mi>H</mi> <mo>(</mo> <mi>q</mi> <mo>,</mo> <mi>p</mi>
    <mo>)</mo></mrow></math> . For this reason, reasoning with cross-entropy can be
    a little tricky and is best done with some caution.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意，与*L*²范数不同，*H*是不对称的！也就是说，<math alttext="upper H left-parenthesis p comma
    q right-parenthesis not-equals upper H left-parenthesis q comma p right-parenthesis"><mrow><mi>H</mi>
    <mo>(</mo> <mi>p</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo> <mo>≠</mo> <mi>H</mi> <mo>(</mo>
    <mi>q</mi> <mo>,</mo> <mi>p</mi> <mo>)</mo></mrow></math>。因此，使用交叉熵进行推理可能有点棘手，最好谨慎处理。
- en: Returning to concrete matters, now suppose that <math alttext="p equals left-parenthesis
    y comma 1 minus y right-parenthesis"><mrow><mi>p</mi> <mo>=</mo> <mo>(</mo> <mi>y</mi>
    <mo>,</mo> <mn>1</mn> <mo>-</mo> <mi>y</mi> <mo>)</mo></mrow></math> is the true
    data distribution for a discrete system with two outcomes, and <math alttext="q
    equals left-parenthesis y Subscript pred Baseline comma 1 minus y Subscript pred
    Baseline right-parenthesis"><mrow><mi>q</mi> <mo>=</mo> <mo>(</mo> <msub><mi>y</mi>
    <mtext>pred</mtext></msub> <mo>,</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mtext>pred</mtext></msub>
    <mo>)</mo></mrow></math> is that predicted by a machine learning system. Then
    the cross-entropy loss is
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 回到具体问题，现在假设<math alttext="p equals left-parenthesis y comma 1 minus y right-parenthesis"><mrow><mi>p</mi>
    <mo>=</mo> <mo>(</mo> <mi>y</mi> <mo>,</mo> <mn>1</mn> <mo>-</mo> <mi>y</mi> <mo>)</mo></mrow></math>是具有两个结果的离散系统的真实数据分布，<math
    alttext="q equals left-parenthesis y Subscript pred Baseline comma 1 minus y Subscript
    pred Baseline right-parenthesis"><mrow><mi>q</mi> <mo>=</mo> <mo>(</mo> <msub><mi>y</mi>
    <mtext>pred</mtext></msub> <mo>,</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mtext>pred</mtext></msub>
    <mo>)</mo></mrow></math>是机器学习系统预测的。那么交叉熵损失是
- en: <math display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>y</mi> <mo form="prefix">log</mo>
    <msub><mi>y</mi> <mtext>pred</mtext></msub> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>y</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mtext>pred</mtext></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo>
    <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>y</mi> <mo form="prefix">log</mo>
    <msub><mi>y</mi> <mtext>pred</mtext></msub> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>y</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mtext>pred</mtext></msub> <mo>)</mo></mrow></mrow></math>
- en: This form of the loss is used widely in machine learning systems to train classifiers.
    Empirically, minimizing *H*(*p*, *q*) seems to construct classifiers that reproduce
    provided training labels well.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这种损失形式在机器学习系统中被广泛使用来训练分类器。经验上，最小化*H*(*p*, *q*)似乎能够构建出很好地复制提供的训练标签的分类器。
- en: Gradient Descent
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降
- en: So far in this chapter, you have learned about the notion of function minimization
    as a proxy for machine learning. As a short recap, minimizing a suitable function
    is often sufficient to learn to solve a desired task. In order to use this framework,
    you need to use suitable loss functions, such as the *L*² or *H*(*p*, *q*) cross-entropy
    in order to transform classification and regression problems into suitable loss
    functions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这一章中，您已经了解了将函数最小化作为机器学习的代理的概念。简而言之，最小化适当的函数通常足以学会解决所需的任务。为了使用这个框架，您需要使用适当的损失函数，比如*L*²或*H*(*p*,
    *q*) 交叉熵，以将分类和回归问题转化为适当的损失函数。
- en: Learnable Weights
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可学习权重
- en: So far in this chapter, we’ve explained that machine learning is the act of
    minimizing suitably defined loss function <math alttext="script upper L left-parenthesis
    x comma y right-parenthesis"><mrow><mi>ℒ</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow></math> . That is, we attempt to find arguments to
    the loss function <math alttext="script upper L"><mi>ℒ</mi></math> that minimize
    it. However, careful readers will recall that (*x*,*y*) are fixed quantities that
    cannot be changed. What arguments to <math alttext="script upper L"><mi>ℒ</mi></math>
    are we changing during learning then?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经解释了机器学习是通过最小化适当定义的损失函数<math alttext="script upper L left-parenthesis
    x comma y right-parenthesis"><mrow><mi>ℒ</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow></math>来实现的。也就是说，我们试图找到最小化它的损失函数<math alttext="script
    upper L"><mi>ℒ</mi></math>的参数。然而，细心的读者会记得(*x*,*y*)是固定的量，不能改变。那么在学习过程中我们改变的是什么参数呢？
- en: Enter learnable weights *W*. Suppose *f*(*x*) is a differentiable function we
    wish to fit with our machine learning model. We will dictate that *f* be *parameterized*
    by choice of *W*. That is, our function actually has two arguments *f*(*W*, *x*).
    Fixing the value of *W* results in a function that depends solely on datapoints
    *x*. These learnable weights are the quantities actually selected by minimization
    of the loss function. We will see later in the chapter how TensorFlow can be used
    to encode learnable weights using `tf.Variable`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输入可学习权重*W*。假设*f*(*x*)是我们希望用机器学习模型拟合的可微函数。我们将规定*f*由选择*W*的方式进行*参数化*。也就是说，我们的函数实际上有两个参数*f*(*W*,
    *x*)。固定*W*的值会导致一个仅依赖于数据点*x*的函数。这些可学习权重实际上是通过最小化损失函数选择的量。我们将在本章后面看到如何使用`tf.Variable`来编码可学习权重。
- en: But now, suppose that we have encoded our learning problem with a suitable loss
    function? How can we actually find minima of this loss function in practice? The
    key trick we will use is minimization by gradient descent. Suppose that *f* is
    a function that depends on some weights *W*. Then <math alttext="normal nabla
    upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math> denotes the direction change
    in *W* that would maximally increase *f*. It follows that taking a step in the
    opposite direction would get us closer to the minima of *f*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，现在假设我们已经用适当的损失函数编码了我们的学习问题？在实践中，我们如何找到这个损失函数的最小值？我们将使用的关键技巧是梯度下降最小化。假设*f*是一个依赖于一些权重*W*的函数。那么<math
    alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>表示的是在*W*中会最大程度增加*f*的方向变化。由此可知，朝着相反方向迈出一步会让我们更接近*f*的最小值。
- en: Notation for Gradients
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度的符号
- en: 'We have written the gradient for learnable weight *W* as <math alttext="normal
    nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math> . At times, it will be
    convenient to use the following alternative notation for the gradient:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将可学习权重*W*的梯度写成了<math alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>。有时，使用以下替代符号表示梯度会更方便：
- en: <math display="block"><mrow><mi>∇</mi> <mi>W</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow>
    <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac></mrow></math>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>∇</mi> <mi>W</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow>
    <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac></mrow></math>
- en: Read this equation as saying that gradient <math alttext="normal nabla upper
    W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math> encodes the direction that maximally
    changes the loss <math alttext="script upper L"><mi>ℒ</mi></math> .
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个方程理解为梯度<math alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>编码了最大程度改变损失<math
    alttext="script upper L"><mi>ℒ</mi></math>的方向。
- en: TheI idea of gradient descent is to find the minima of functions by repeatedly
    following the negative gradient. Algorithmically, this update rule can be expressed
    as
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的思想是通过反复遵循负梯度来找到函数的最小值。从算法上讲，这个更新规则可以表示为
- en: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mi>W</mi> <mo>-</mo> <mi>α</mi>
    <mi>∇</mi> <mi>W</mi></mrow></math>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mi>W</mi> <mo>-</mo> <mi>α</mi>
    <mi>∇</mi> <mi>W</mi></mrow></math>
- en: where <math><mi>α</mi></math> is the *step-size* and dictates how much weight
    is given to new gradient <math alttext="normal nabla upper W"><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math> . The idea is to take many little steps each in the direction
    of <math alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>
    . Note that <math alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>
    is itself a function of *W*, so the actual step changes at each iteration. Each
    step performs a little update to the weight matrix *W*. The iterative process
    of performing updates is typically called *learning* the weight matrix *W*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math><mi>α</mi></math>是*步长*，决定了新梯度<math alttext="normal nabla upper W"><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math>被赋予多少权重。这个想法是每次都朝着<math alttext="normal nabla upper W"><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math>的方向迈出许多小步。注意<math alttext="normal nabla upper W"><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math>本身是*W*的一个函数，所以实际步骤在每次迭代中都会改变。每一步都对权重矩阵*W*进行一点更新。执行更新的迭代过程通常称为*学习*权重矩阵*W*。
- en: Computing Gradients Efficiently with Minibatches
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用小批量高效计算梯度
- en: One issue is that computing <math alttext="normal nabla upper W"><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math> can be very slow. Implicitly, <math alttext="normal nabla
    upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math> depends on the loss function
    <math alttext="script upper L"><mi>ℒ</mi></math> . Since <math alttext="script
    upper L"><mi>ℒ</mi></math> depends on the entire dataset, computing <math alttext="normal
    nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math> can become very slow
    for large datasets. In practice, people usually estimate <math><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math> on a fraction of the dataset called a *minibatch*. Each
    minibatch is of size typically 50–100\. The size of the minibatch is a *hyperparameter*
    in a deep learning algorithm. The step-size for each step <math><mi>α</mi></math>
    is another hyperparameter. Deep learning algorithms typically have clusters of
    hyperparameters, which are not themselves learned via the stochastic gradient
    descent.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是计算<math alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>可能非常慢。隐含地，<math
    alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>取决于损失函数<math
    alttext="script upper L"><mi>ℒ</mi></math>。由于<math alttext="script upper L"><mi>ℒ</mi></math>取决于整个数据集，对于大型数据集来说，计算<math
    alttext="normal nabla upper W"><mrow><mi>∇</mi> <mi>W</mi></mrow></math>可能会变得非常缓慢。在实践中，人们通常在称为*minibatch*的数据集的一部分上估计<math><mrow><mi>∇</mi>
    <mi>W</mi></mrow></math>。每个minibatch通常包含50-100个样本。Minibatch的大小是深度学习算法中的一个*超参数*。每个步骤<math><mi>α</mi></math>的步长是另一个超参数。深度学习算法通常具有超参数的集群，这些超参数本身不是通过随机梯度下降学习的。
- en: This tension between learnable parameters and hyperparameters is one of the
    weaknesses and strengths of deep architectures. The presence of hyperparameters
    provides much room for utilizing the expert’s strong intuition, while the learnable
    parameters allow the data to speak for itself. However, this flexibility itself
    quickly becomes a weakness, with understanding of the behavior of hyperparameters
    something of a black art that blocks beginners from widely deploying deep learning.
    We will spend significant effort discussing hyperparameter optimization later
    in this book.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可学习参数和超参数之间的这种张力是深度结构的弱点和优势之一。超参数的存在为利用专家的强烈直觉提供了很大的空间，而可学习参数则允许数据自己说话。然而，这种灵活性本身很快变成了一个弱点，对于超参数行为的理解有点像黑魔法，阻碍了初学者广泛部署深度学习。我们将在本书的后面花费大量精力讨论超参数优化。
- en: We end this section by introducing the notion of an *epoch*. An epoch is a full
    pass of the gradient descent algorithm over the data *x*. More particularly, an
    epoch consists of however many gradient descent steps are required to view all
    the data at a given minibatch size. For example, suppose that a dataset has 1,000
    datapoints and training uses a minibatch of size 50\. Then an epoch will consist
    of 20 gradient descent updates. Each epoch of training increases the amount of
    useful knowledge the model has gained. Mathematically, this will correspond to
    reductions in the value of the loss function on the training set.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过介绍*时代*的概念来结束本节。一个时代是梯度下降算法在数据*x*上的完整遍历。更具体地说，一个时代包括需要查看给定minibatch大小的所有数据所需的梯度下降步骤。例如，假设一个数据集有1,000个数据点，训练使用大小为50的minibatch。那么一个时代将包括20个梯度下降更新。每个训练时代增加了模型获得的有用知识量。从数学上讲，这将对应于训练集上损失函数值的减少。
- en: Early epochs will cause dramatic drops in the loss function. This process is
    often referred to as *learning the prior* on that dataset. While it appears that
    the model is learning rapidly, it is in fact only adjusting itself to reside in
    the portion of parameter space that is pertinent to the problem at hand. Later
    epochs will correspond to much smaller drops in the loss function, but it is often
    in these later epochs that meaningful learning will happen. A few epochs is usually
    too little time for a nontrivial model to learn anything useful; models are usually
    trained from 10–1,000 epochs or until convergence. While this appears large, it’s
    important to note that the number of epochs required usually doesn’t scale with
    the size of the dataset at hand. Consequently, gradient descent scales linearly
    with the size of data and not quadratically! This is one of the greatest strengths
    of the stochastic gradient descent method versus other learning algorithms. More
    complicated learning algorithms may only require a single pass over a dataset,
    but may use total compute that scales quadratically with the number of datapoints.
    In this era of big datasets, quadratic runtimes are a fatal weakness.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的时代将导致损失函数的急剧下降。这个过程通常被称为在该数据集上*学习先验*。虽然看起来模型正在快速学习，但实际上它只是在调整自己以适应与手头问题相关的参数空间的部分。后续时代将对应于损失函数的较小下降，但通常在这些后续时代中才会发生有意义的学习。几个时代通常对于一个非平凡的模型来说时间太短，模型通常从10-1,000个时代或直到收敛进行训练。虽然这看起来很大，但重要的是要注意，所需的时代数量通常不随手头数据集的大小而增加。因此，梯度下降与数据大小成线性关系，而不是二次关系！这是随机梯度下降方法相对于其他学习算法的最大优势之一。更复杂的学习算法可能只需要对数据集进行一次遍历，但可能使用的总计算量与数据点数量成二次关系。在大数据集的时代，二次运行时间是一个致命的弱点。
- en: Tracking the drop in the loss function as a function of the number of epochs
    can be an extremely useful visual shorthand for understanding the learning process.
    These plots are often referred to as loss curves (see [Figure 3-4](#ch3-smoothloss)).
    With time, an experienced practitioner can diagnose common failures in learning
    with just a quick glance at the loss curve. We will pay significant attention
    to the loss curves for various deep learning models over the course of this book.
    In particular, later in this chapter, we will introduce TensorBoard, a powerful
    visualization suite that TensorFlow provides for tracking quantities such as loss
    functions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪损失函数随着周期数的减少可以是理解学习过程的极其有用的视觉简写。这些图通常被称为损失曲线（见[图3-4](#ch3-smoothloss)）。随着时间的推移，一个经验丰富的从业者可以通过快速查看损失曲线来诊断学习中的常见失败。我们将在本书的过程中对各种深度学习模型的损失曲线给予重要关注。特别是在本章后面，我们将介绍TensorBoard，这是TensorFlow提供的用于跟踪诸如损失函数之类的量的强大可视化套件。
- en: '![smooth_loss.png](assets/tfdl_0304.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: 这些规则可以通过链式法则结合起来：
- en: Figure 3-4\. An example of a loss curve for a model. Note that this loss curve
    is from a model trained with the true gradient (that is, not a minibatch estimate)
    and is consequently smoother than other loss curves you will encounter later in
    this book.
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 机器学习是定义适合数据集的损失函数，然后将其最小化的艺术。为了最小化损失函数，我们需要计算它们的梯度，并使用梯度下降算法迭代地减少损失。然而，我们仍然需要讨论梯度是如何实际计算的。直到最近，答案是“手动”。机器学习专家会拿出笔和纸，手动计算矩阵导数，以计算学习系统中所有梯度的解析公式。然后这些公式将被手动编码以实现学习算法。这个过程以前是臭名昭著的，不止一位机器学习专家在发表的论文和生产系统中意外梯度错误的故事被发现了多年。
- en: Automatic Differentiation Systems
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图3-4。一个模型的损失曲线示例。请注意，这个损失曲线来自使用真实梯度（即非小批量估计）训练的模型，因此比您在本书后面遇到的其他损失曲线更平滑。
- en: Machine learning is the art of defining loss functions suited to datasets and
    then minimizing them. In order to minimize loss functions, we need to compute
    their gradients and use the gradient descent algorithm to iteratively reduce the
    loss. However, we still need to discuss how gradients are actually computed. Until
    recently, the answer was “by hand.” Machine learning experts would break out pen
    and paper and compute matrix derivatives by hand to compute the analytical formulas
    for all gradients in a learning system. These formulas would then be manually
    coded to implement the learning algorithm. This process was notoriously buggy,
    and more than one machine learning expert has stories of accidental gradient errors
    in published papers and production systems going undiscovered for years.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况已经发生了显著变化，随着自动微分引擎的广泛可用。像TensorFlow这样的系统能够自动计算几乎所有损失函数的梯度。这种自动微分是TensorFlow和类似系统的最大优势之一，因为机器学习从业者不再需要成为矩阵微积分的专家。然而，了解TensorFlow如何自动计算复杂函数的导数仍然很重要。对于那些在微积分入门课程中受苦的读者，你可能记得计算函数的导数是令人惊讶地机械化的。有一系列简单的规则可以应用于计算大多数函数的导数。例如：
- en: 'This state of affairs has changed significantly with the widespread availability
    of automatic differentiation engines. Systems like TensorFlow are capable of automatically
    computing gradients for almost all loss functions. This automatic differentiation
    is one of the greatest advantages of TensorFlow and similar systems, since machine
    learning practitioners no longer need to be experts at matrix calculus. However,
    it’s still worth understanding at a high level how TensorFlow can automatically
    take derivatives of complex functions. For those readers who suffered through
    an introductory class in calculus, you might remember that taking derivatives
    of functions is surprisingly mechanical. There are a series of simple rules that
    can be applied to take derivatives of most functions. For example:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数学显示="block"的<math><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>x</mi></mrow></mfrac>
    <msup><mi>e</mi> <mi>x</mi></msup> <mo>=</mo> <msup><mi>e</mi> <mi>x</mi></msup></mrow></math>
- en: <math display="block"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>x</mi></mrow></mfrac>
    <msup><mi>x</mi> <mi>n</mi></msup> <mo>=</mo> <mi>n</mi> <msup><mi>x</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></math><math
    display="block"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>x</mi></mrow></mfrac>
    <msup><mi>e</mi> <mi>x</mi></msup> <mo>=</mo> <msup><mi>e</mi> <mi>x</mi></msup></mrow></math>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数学显示="block"的<math><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>x</mi></mrow></mfrac>
    <msup><mi>x</mi> <mi>n</mi></msup> <mo>=</mo> <mi>n</mi> <msup><mi>x</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>
- en: 'These rules can be combined through the power of the chain rule:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数学显示="block"的<math><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>x</mi></mrow></mfrac>
    <mi>f</mi> <mrow><mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo>
    <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <msup><mi>g</mi>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
- en: <math display="block"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>x</mi></mrow></mfrac>
    <mi>f</mi> <mrow><mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo>
    <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <msup><mi>g</mi>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分系统
- en: where <math><msup><mi>f</mi> <mo>'</mo></msup></math> is used to denote the
    derivative of *f* and <math alttext="g prime"><msup><mi>g</mi> <mo>'</mo></msup></math>
    that of *g*. With these rules, it’s straightforward to envision how one might
    program an automatic differentiation engine for one-dimensional calculus. Indeed,
    the creation of such a differentiation engine is often a first-year programming
    exercise in Lisp-based classes. (It turns out that correctly parsing functions
    is a much trickier problem than taking derivatives. Lisp makes it trivial to parse
    formulas using its syntax, while in other languages, waiting to do this exercise
    until you take a course on compilers is often easier).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math><msup><mi>f</mi> <mo>'</mo></msup></math> 用于表示 *f* 的导数，<math alttext="g
    prime"><msup><mi>g</mi> <mo>'</mo></msup></math> 用于表示 *g* 的导数。有了这些规则，很容易想象如何为一维微积分编写自动微分引擎。事实上，在基于
    Lisp 的课程中，创建这样一个微分引擎通常是一年级的编程练习。（事实证明，正确解析函数比求导数更加困难。Lisp 使用其语法轻松解析公式，而在其他语言中，等到上编译器课程再做这个练习通常更容易）。
- en: How might these rules be extended to calculus of higher dimensions? Getting
    the math right is trickier, since there are many more numbers to consider. For
    example, given *X* = *AB* where *X*, *A*, *B* are all matrices, the formula comes
    out to be
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将这些规则扩展到更高维度的微积分？搞定数学更加棘手，因为需要考虑更多的数字。例如，给定 *X* = *AB*，其中 *X*、*A*、*B* 都是矩阵，公式变成了
- en: <math display="block"><mrow><mi>∇</mi> <mi>A</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><mi>A</mi></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><mi>X</mi></mrow></mfrac> <msup><mi>B</mi> <mi>T</mi></msup> <mo>=</mo>
    <mrow><mo>(</mo> <mi>∇</mi> <mi>X</mi> <mo>)</mo></mrow> <msup><mi>B</mi> <mi>T</mi></msup></mrow></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>∇</mi> <mi>A</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><mi>A</mi></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><mi>X</mi></mrow></mfrac> <msup><mi>B</mi> <mi>T</mi></msup> <mo>=</mo>
    <mrow><mo>(</mo> <mi>∇</mi> <mi>X</mi> <mo>)</mo></mrow> <msup><mi>B</mi> <mi>T</mi></msup></mrow></math>
- en: Formulas like this can be combined to provide a symbolic differentiation system
    for vectorial and tensorial calculus.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的公式可以组合起来提供一个矢量和张量微积分的符号微分系统。
- en: Learning with TensorFlow
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行学习
- en: In the rest of this chapter, we will cover the concepts that you need to learn
    basic machine learning models with TensorFlow. We will start by introducing the
    concept of toy datasets, and will explain how to create meaningful toy datasets
    using common Python libraries. Next, we will discuss new TensorFlow ideas such
    as placeholders, feed dictionaries, name scopes, optimizers, and gradients. The
    next section will show you how to use these concepts to train simple regression
    and classification models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将介绍您学习使用 TensorFlow 创建基本机器学习模型所需的概念。我们将从介绍玩具数据集的概念开始，然后解释如何使用常见的
    Python 库创建有意义的玩具数据集。接下来，我们将讨论新的 TensorFlow 想法，如占位符、喂养字典、名称范围、优化器和梯度。下一节将向您展示如何使用这些概念训练简单的回归和分类模型。
- en: Creating Toy Datasets
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建玩具数据集
- en: In this section, we will discuss how to create simple but meaningful synthetic
    datasets, or toy datasets, that we will use to train simple supervised classification
    and regression models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何创建简单但有意义的合成数据集，或称为玩具数据集，用于训练简单的监督分类和回归模型。
- en: An (extremely) brief introduction to NumPy
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对 NumPy 的（极其）简要介绍
- en: We will make heavy use of NumPy in order to define useful toy datasets. NumPy
    is a Python package that allows for manipulation of tensors (called `ndarray`s
    in NumPy). [Example 3-1](#ch3-numpy) shows some basics.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将大量使用 NumPy 来定义有用的玩具数据集。NumPy 是一个允许操作张量（在 NumPy 中称为 `ndarray`）的 Python 包。[示例
    3-1](#ch3-numpy) 展示了一些基础知识。
- en: Example 3-1\. Some examples of basic NumPy usage
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-1。一些基本 NumPy 用法示例
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You may notice that NumPy `ndarray` manipulation looks remarkably similar to
    TensorFlow tensor manipulation. This similarity was purposefully designed by TensorFlow’s
    architects. Many key TensorFlow utility functions have similar arguments and forms
    to analogous functions in NumPy. For this purpose, we will not attempt to introduce
    NumPy in great depth, and will trust readers to use experimentation to work out
    NumPy usage. There are numerous online resources that provide tutorial introductions
    to NumPy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到 NumPy `ndarray` 操作看起来与 TensorFlow 张量操作非常相似。这种相似性是 TensorFlow 架构师特意设计的。许多关键的
    TensorFlow 实用函数具有与 NumPy 中类似函数的参数和形式。出于这个目的，我们不会试图深入介绍 NumPy，并相信读者通过实验来掌握 NumPy
    的用法。有许多在线资源提供了 NumPy 的教程介绍。
- en: Why are toy datasets important?
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么玩具数据集很重要？
- en: In machine learning, it is often critical to learn to properly use toy datasets.
    Learning is challenging, and one of the most common mistakes beginners make is
    trying to learn nontrivial models on complex data too soon. These attempts often
    end in abject failure, and the would-be machine learner walks away dejected and
    convinced machine learning isn’t for them.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，学会正确使用玩具数据集通常至关重要。学习是具有挑战性的，初学者经常犯的一个最常见的错误是尝试在太早的时候在复杂数据上学习非平凡的模型。这些尝试往往以惨败告终，想要成为机器学习者的人会灰心丧气，认为机器学习不适合他们。
- en: The real culprit here of course isn’t the student, but rather the fact that
    real-world datasets have many idiosyncrasies. Seasoned data scientists have learned
    that real-world datasets often require many clean-up and preprocessing transformations
    before becoming amenable to learning. Deep learning exacerbates this problem,
    since most deep learning models are notoriously sensitive to infelicities in data.
    Issues like a wide range of regression labels, or underlying strong noise patterns
    can throw off gradient-descent–based methods, even when other machine learning
    algorithms (such as random forests) would have no issues.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, it’s almost always possible to deal with these issues, but doing so
    can require considerable sophistication on the part of the data scientist. These
    sensitivity issues are perhaps the biggest roadblock to the commoditization of
    machine learning as a technology. We will go into depth on data clean-up strategies,
    but for the time being, we recommend a much simpler alternative: use toy datasets!'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Toy datasets are critical for understanding learning algorithms. Given very
    simple synthetic datasets, it is trivial to gauge whether the algorithm has learned
    the correct rule. On more complex datasets, this judgment can be highly challenging.
    Consequently, for the remainder of this chapter, we will only use toy datasets
    as we cover the fundamentals of gradient-descent–based learning with TensorFlow.
    We will dive deep into case studies with real-world data in the following chapters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise with Gaussians
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier, we discussed discrete probability distributions as a tool for turning
    discrete choices into continuous values. We also alluded to the idea of a continuous
    probability distribution but didn’t dive into it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Continuous probability distributions (more accurately known as probability density
    functions) are a useful mathematical tool for modeling random events that may
    have a range of outcomes. For our purposes, it is enough to think of probability
    density functions as a useful tool for modeling some measurement error in gathering
    data. The Gaussian distribution is widely used for noise modeling.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 3-5](#ch3-gaussian) shows, note that Gaussians can have different
    *means* <math><mi>μ</mi></math> and *standard deviations* <math alttext="sigma"><mi>σ</mi></math>
    . The mean of a Gaussian is the average value it takes, while the standard deviation
    is a measure of the spread around this average value. In general, adding a Gaussian
    random variable onto some quantity provides a structured way to fuzz the quantity
    by making it vary slighty. This is a very useful trick for coming up with nontrivial
    synthetic datasets.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![gaussian.png](assets/tfdl_0305.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Illustrations of various Gaussian probability distributions with
    different means and standard deviations.
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We quickly note that the Gaussian distribution is also called the Normal distribution.
    A Gaussian with mean <math><mi>μ</mi></math> and standard deviation <math alttext="sigma"><mi>σ</mi></math>
    is written <math alttext="upper N left-parenthesis mu comma sigma right-parenthesis"><mrow><mi>N</mi>
    <mo>(</mo> <mi>μ</mi> <mo>,</mo> <mi>σ</mi> <mo>)</mo></mrow></math> . This shorthand
    notation is convenient, and we will use it many times in the coming chapters.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Toy regression datasets
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest form of linear regression is learning the parameters for a one-dimensional
    line. Suppose that our datapoints *x* are one-dimensional. Then suppose that real-valued
    labels *y* are generated by a linear rule
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo>
    <mi>b</mi></mrow></math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Here, *w*, *b* are the learnable parameters that must be estimated from data
    by gradient descent. In order to test that we can learn these parameters with
    TensorFlow, we will generate an artificial dataset consisting of points upon a
    straight line. To make the learning challenge a little more difficult, we will
    add a small amount of Gaussian noise to the dataset.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w*，*b*是必须通过梯度下降从数据中估计出来的可学习参数。为了测试我们是否可以使用TensorFlow学习这些参数，我们将生成一个由直线上的点组成的人工数据集。为了使学习挑战稍微困难一些，我们将在数据集中添加少量高斯噪声。
- en: 'Let’s write down the equation for our line perturbed by a small amount of Gaussian
    noise:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写下我们的直线方程，受到少量高斯噪声的干扰：
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo>
    <mi>b</mi> <mo>+</mo> <mi>N</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>ϵ</mi> <mo>)</mo></mrow></math>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo>
    <mi>b</mi> <mo>+</mo> <mi>N</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>ϵ</mi> <mo>)</mo></mrow></math>
- en: Here <math alttext="epsilon"><mi>ϵ</mi></math> is the standard deviation of
    the noise term. We can then use NumPy to generate an artificial dataset drawn
    from this distribution, as shown in [Example 3-2](#ch3-numpy-sample).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里<math alttext="epsilon"><mi>ϵ</mi></math>是噪声项的标准差。然后我们可以使用NumPy从这个分布中生成一个人工数据集，如[示例3-2](#ch3-numpy-sample)所示。
- en: Example 3-2\. Using NumPy to sample an artificial dataset
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-2. 使用NumPy对人工数据集进行抽样
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We plot this dataset using Matplotlib in [Figure 3-6](#ch3-toyregplot). (you
    can find the code in [the GitHub repo](https://github.com/matroid/dlwithtf) associated
    with this book to see the exact plotting code) to verify that synthetic data looks
    reasonable. As expected, the data distribution is a straight line, with a small
    amount of measurement error.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Matplotlib在[图3-6](#ch3-toyregplot)中绘制这个数据集（您可以在与本书相关的[GitHub存储库](https://github.com/matroid/dlwithtf)中找到确切的绘图代码）以验证合成数据看起来是否合理。如预期的那样，数据分布是一条直线，带有少量测量误差。
- en: '![lr_data.png](assets/tfdl_0306.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![lr_data.png](assets/tfdl_0306.png)'
- en: Figure 3-6\. Plot of the toy regression data distribution.
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6. 玩具回归数据分布的绘图。
- en: Toy classification datasets
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 玩具分类数据集
- en: It’s a little trickier to create a synthetic classification dataset. Logically,
    we want two distinct classes of points, which are easily separated. Suppose that
    the dataset consists of only two types of points, (–1, –1) and (1, 1). Then a
    learning algorithm would have to learn a rule that separates these two data values.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 创建合成分类数据集有点棘手。从逻辑上讲，我们希望有两个不同的、容易分离的点类。假设数据集只包含两种类型的点，（-1，-1）和（1，1）。然后学习算法将不得不学习一个将这两个数据值分开的规则。
- en: '*y*[0] = (–1, –1)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*[0] = (-1, -1)'
- en: '*y*[1] = (1, 1)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*[1] = (1, 1)'
- en: 'As before, let’s make the challenge a little more difficult by adding some
    Gaussian noise to both types of points:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，让我们通过向两种类型的点添加一些高斯噪声来增加一些挑战：
- en: '*y*[0] = (–1, –1) + *N*(0, ϵ)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*[0] = (-1, -1) + *N*(0, ϵ)'
- en: '*y*[1] = (1, 1) + *N*(0, ϵ)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*[1] = (1, 1) + *N*(0, ϵ)'
- en: However, there’s a slight bit of trickiness here. Our points are two-dimensional,
    while the Gaussian noise we introduced previously is one-dimensional. Luckily,
    there exists a multivariate extension of the Gaussian. We won’t discuss the intricacies
    of the multivariate Gaussian here, but you do not need to understand the intricacies
    to follow our discussion.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一点小技巧。我们的点是二维的，而我们之前引入的高斯噪声是一维的。幸运的是，存在高斯的多变量扩展。我们不会在这里讨论多变量高斯的复杂性，但您不需要理解这些复杂性来跟随我们的讨论。
- en: The NumPy code to generate the synthetic dataset in [Example 3-3](#ch3-synth-2d)
    is slightly trickier than that for the linear regression problem since we have
    to use the stacking function `np.vstack` to combine the two different types of
    datapoints and associate them with different labels. (We use the related function
    `np.concatenate` to combine the one-dimensional labels.)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例3-3](#ch3-synth-2d)中生成合成数据集的NumPy代码比线性回归问题稍微棘手，因为我们必须使用堆叠函数`np.vstack`将两种不同类型的数据点组合在一起，并将它们与不同的标签关联起来。（我们使用相关函数`np.concatenate`将一维标签组合在一起。）
- en: Example 3-3\. Sample a toy classification dataset with NumPy
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-3. 使用NumPy对玩具分类数据集进行抽样
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Figure 3-7](#ch3-toyclassplot) plots the data generated by this code with
    Matplotlib to verify that the distribution is as expected. We see that the data
    resides in two classes that are neatly separated.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-7](#ch3-toyclassplot)使用Matplotlib绘制了这段代码生成的数据，以验证分布是否符合预期。我们看到数据分布在两个清晰分开的类中。'
- en: '![logistic_data.png](assets/tfdl_0307.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![logistic_data.png](assets/tfdl_0307.png)'
- en: Figure 3-7\. Plot of the toy classification data distribution.
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7. 玩具分类数据分布的绘图。
- en: New TensorFlow Concepts
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新的TensorFlow概念
- en: Creating simple machine learning systems in TensorFlow will require that you
    learn some new TensorFlow concepts.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中创建简单的机器学习系统将需要您学习一些新的TensorFlow概念。
- en: Placeholders
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 占位符
- en: A placeholder is a way to input information into a TensorFlow computation graph.
    Think of placeholders as the input nodes through which information enters TensorFlow.
    The key function used to create placeholders is `tf.placeholder` ([Example 3-4](#ch3-place)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 占位符是将信息输入到TensorFlow计算图中的一种方式。将占位符视为信息进入TensorFlow的输入节点。用于创建占位符的关键函数是`tf.placeholder`（[示例3-4](#ch3-place)）。
- en: Example 3-4\. Create a TensorFlow placeholder
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-4. 创建一个TensorFlow占位符
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will use placeholders to feed datapoints *x* and labels *y* to our regression
    and classification algorithms.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用占位符将数据点*x*和标签*y*馈送到我们的回归和分类算法中。
- en: Feed dictionaries and Fetches
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 馈送字典和获取
- en: Recall that we can evaluate tensors in TensorFlow by using `sess.run(var)`.
    How do we feed in values for placeholders in our TensorFlow computations then?
    The answer is to construct *feed dictionaries*. Feed dictionaries are Python dictionaries
    that map TensorFlow tensors to `np.ndarray` objects that contain the concrete
    values for these placeholders. A feed dictionary is best viewed as an input to
    a TensorFlow computation graph. What then is an output? TensorFlow calls these
    outputs *fetches*. You have seen fetches already. We used them extensively in
    the previous chapter without calling them as such; the fetch is a tensor (or tensors)
    whose value is retrieved from the computation graph after the computation (using
    placeholder values from the feed dictionary) is run to completion ([Example 3-5](#ch3-fetch)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们可以通过`sess.run(var)`在TensorFlow中评估张量。那么我们如何为占位符提供值呢？答案是构建*feed字典*。Feed字典是Python字典，将TensorFlow张量映射到包含这些占位符具体值的`np.ndarray`对象。Feed字典最好被视为TensorFlow计算图的输入。那么输出是什么？TensorFlow称这些输出为*fetches*。您已经见过fetches了。我们在上一章中广泛使用了它们，但没有这样称呼；fetch是一个张量（或张量），其值是在计算图中的计算（使用feed字典中的占位符值）完成后检索的（[示例3-5](#ch3-fetch)）。
- en: Example 3-5\. Using fetches
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-5。使用fetches
- en: '[PRE4]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Name scopes
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名空间
- en: In complicated TensorFlow programs, there will be many tensors, variables, and
    placeholders defined throughout the program. `tf.name_scope(name)` provides a
    simple scoping mechanism for managing these collections of variables ([Example 3-6](#ch3-namescope)).
    All computational graph elements created within the scope of a `tf.name_scope(name)`
    call will have `name` prepended to their names.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂的TensorFlow程序中，将在整个程序中定义许多张量、变量和占位符。`tf.name_scope(name)`为管理这些变量集合提供了一个简单的作用域机制（[示例3-6](#ch3-namescope)）。在`tf.name_scope(name)`调用的作用域内创建的所有计算图元素将在其名称前加上`name`。
- en: This organizational tool is most useful when combined with TensorBoard, since
    it aids the visualization system in automatically grouping graph elements within
    the same name scope. You will learn more about TensorBoard further in the next
    section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这种组织工具在与TensorBoard结合使用时最有用，因为它有助于可视化系统自动将图元素分组到相同的命名空间中。您将在下一节中进一步了解TensorBoard。
- en: Example 3-6\. Using namescopes to organize placeholders
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-6。使用命名空间来组织占位符
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Optimizers
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器
- en: The primitives introduced in the last two sections already hint at how machine
    learning is done in TensorFlow. You have learned how to add placeholders for datapoints
    and labels and how to use tensorial operations to define the loss function. The
    missing piece is that you still don’t know how to perform gradient descent using
    TensorFlow.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两节介绍的基本概念已经暗示了在TensorFlow中如何进行机器学习。您已经学会了如何为数据点和标签添加占位符，以及如何使用张量操作定义损失函数。缺失的部分是您仍然不知道如何使用TensorFlow执行梯度下降。
- en: While it is in fact possible to define optimization algorithms such as gradient
    descent directly in Python using TensorFlow primitives, TensorFlow provides a
    collection of optimization algorithms in the `tf.train` module. These algorithms
    can be added as nodes to the TensorFlow computation graph.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，可以直接在Python中使用TensorFlow原语定义优化算法，TensorFlow在`tf.train`模块中提供了一系列优化算法。这些算法可以作为节点添加到TensorFlow计算图中。
- en: Which optimizer should I use?
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我应该使用哪个优化器？
- en: There are many possible optimizers available in `tf.train`. For a short preview,
    this list includes `tf.train.GradientDescentOptimizer`, `tf.train.MomentumOptimizer`,
    `tf.train.AdagradOptimizer`, `tf.train.AdamOptimizer`, and many more. What’s the
    difference between these various optimizers?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在`tf.train`中有许多可能的优化器可用。简短预览中包括`tf.train.GradientDescentOptimizer`、`tf.train.MomentumOptimizer`、`tf.train.AdagradOptimizer`、`tf.train.AdamOptimizer`等。这些不同优化器之间有什么区别呢？
- en: 'Almost all of these optimizers are based on the idea of gradient descent. Recall
    the simple gradient descent rule we previously introduced:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有这些优化器都是基于梯度下降的思想。回想一下我们之前介绍的简单梯度下降规则：
- en: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mi>W</mi> <mo>-</mo> <mi>α</mi>
    <mi>∇</mi> <mi>W</mi></mrow></math>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mi>W</mi> <mo>-</mo> <mi>α</mi>
    <mi>∇</mi> <mi>W</mi></mrow></math>
- en: Mathematically, this update rule is primitive. There are a variety of mathematical
    tricks that researchers have discovered that enable faster optimization without
    using too much extra computation. In general, `tf.train.AdamOptimizer` is a good
    default that is relatively robust. (Many optimizer methods are very sensitive
    to hyperparameter choice. It’s better for beginners to avoid trickier methods
    until they have a good grasp of the behavior of different optimization algorithms.)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这个更新规则是原始的。研究人员发现了许多数学技巧，可以在不使用太多额外计算的情况下实现更快的优化。一般来说，`tf.train.AdamOptimizer`是一个相对稳健的好默认值。（许多优化方法对超参数的选择非常敏感。对于初学者来说，最好避开更复杂的方法，直到他们对不同优化算法的行为有很好的理解。）
- en: '[Example 3-7](#ch3-optim) is a short bit of code that adds an optimizer to
    the computation graph that minimizes a predefined loss `l`.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例3-7](#ch3-optim)是一小段代码，它向计算图中添加了一个优化器，用于最小化预定义的损失`l`。'
- en: Example 3-7\. Adding an Adam optimizer to TensorFlow computation graph
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-7。向TensorFlow计算图添加Adam优化器
- en: '[PRE6]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Taking gradients with TensorFlow
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用TensorFlow计算梯度
- en: We mentioned previously that it is possible to directly implement gradient descent
    algorithms in TensorFlow. While most use cases don’t need to reimplement the contents
    of `tf.train`, it can be useful to look at gradient values directly for debugging
    purposes. `tf.gradients` provides a useful tool for doing so ([Example 3-8](#ch3-grad)).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，在TensorFlow中直接实现梯度下降算法是可能的。虽然大多数用例不需要重新实现`tf.train`的内容，但直接查看梯度值以进行调试可能很有用。`tf.gradients`提供了一个有用的工具来实现这一点（[示例3-8](#ch3-grad)）。
- en: Example 3-8\. Taking gradients directly
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-8。直接计算梯度
- en: '[PRE7]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code snippet symbolically pulls down the gradients of loss `l` with respect
    to learnable parameter (`tf.Variable`) `W`. `tf.gradients` returns a list of the
    desired gradients. Note that the gradients are themselves tensors! TensorFlow
    performs symbolic differentiation, which means that gradients themselves are parts
    of the computational graph. One neat side effect of TensorFlow’s symbolic gradients
    is that it’s possible to stack derivatives in TensorFlow. This can sometimes be
    useful for more advanced algorithms.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Summaries and file writers for TensorBoard
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gaining a visual understanding of the structure of a tensorial program can be
    very useful. The TensorFlow team provides the TensorBoard package for this purpose.
    TensorBoard starts a web server (on localhost by default) that displays various
    useful visualizations of a TensorFlow program. However, in order for TensorFlow
    programs to be inspected with TensorBoard, programmers must manually write logging
    statements. `tf.train.FileWriter()` specifies the logging directory for a TensorBoard
    program and `tf.summary` writes summaries of various TensorFlow variables to the
    specified logging directory. In this chapter, we will only use `tf.summary.scalar`,
    which summarizes a scalar quantity, to track the value of the loss function. `tf.summary.merge_all()`
    is a useful logging aid that merges multiple summaries into a single summary for
    convenience.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The code snippet in [Example 3-9](#ch3-logging) adds a summary for the loss
    and specifies a logging directory.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-9\. Adding a summary for the loss
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training models with TensorFlow
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose now that we have specified placeholders for datapoints and labels, and
    have defined a loss with tensorial operations. We have added an optimizer node
    `train_op` to the computational graph, which we can use to perform gradient descent
    steps (while we may actually use a different optimizer, we will refer to updates
    as gradient descent for convenience). How can we iteratively perform gradient
    descent to learn on this dataset?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The simple answer is that we use a Python `for`-loop. In each iteration, we
    use `sess.run()` to fetch the `train_op` along with the merged summary op `merged`
    and the loss `l` from the graph. We feed all datapoints and labels into `sess.run()`
    using a feed dictionary.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The code snippet in [Example 3-10](#ch3-simplelearn) demonstrates this simple
    learning method. Note that we don’t make use of minibatches for pedagogical simplicity.
    Code in following chapters will use minibatches when training on larger datasets.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-10\. A simple example of training a model
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Training Linear and Logistic Models in TensorFlow
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section ties together all the TensorFlow concepts introduced in the previous
    section to train linear and logistic regression models upon the toy datasets we
    introduced previously in the chapter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression in TensorFlow
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will provide code to define a linear regression model in
    TensorFlow and learn its weights. This task is straightforward and you can do
    it without TensorFlow easily. Nevertheless, it’s a good exercise to do in TensorFlow
    since it will bring together the new concepts that we have introduced throughout
    the chapter.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Defining and training linear regression in TensorFlow
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The model for a linear regression is straightforward:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo>
    <mi>b</mi></mrow></math>
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Here *w* and *b* are the weights we wish to learn. We transform these weights
    into `tf.Variable` objects. We then use tensorial operations to construct the
    *L*² loss:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mi>w</mi><mi>x</mi><mo>-</mo><mi>b</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Example 3-11](#ch3-linmod) implements these mathematical operations
    in TensorFlow. It also uses `tf.name_scope` to group various operations, and adds
    a `tf.train.AdamOptimizer` for learning and `tf.summary` operations for TensorBoard
    usage.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例3-11](#ch3-linmod)中的代码在TensorFlow中实现了这些数学操作。它还使用`tf.name_scope`来分组各种操作，并添加了`tf.train.AdamOptimizer`用于学习和`tf.summary`操作用于TensorBoard的使用。'
- en: Example 3-11\. Defining a linear regression model
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-11. 定义线性回归模型
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Example 3-12](#ch3-lintrain) then trains this model as discussed previously
    (without using minibatches).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例3-12](#ch3-lintrain)然后训练这个模型，如之前讨论的（不使用小批量）。'
- en: Example 3-12\. Training the linear regression model
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-12. 训练线性回归模型
- en: '[PRE11]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: All code for this example is provided in the [GitHub repository](https://github.com/matroid/dlwithtf)
    associated with this book. We encourage all readers to run the full script for
    the linear regression example to gain a firsthand sense for how the learning algorithm
    functions. The example is small enough that readers will not need access to any
    special-purpose computing hardware to run.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的所有代码都在与本书相关的[GitHub存储库](https://github.com/matroid/dlwithtf)中提供。我们鼓励所有读者运行线性回归示例的完整脚本，以获得对学习算法如何运行的第一手感觉。这个示例足够小，读者不需要访问任何专用计算硬件来运行。
- en: Taking Gradients for Linear Regression
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归的梯度
- en: 'The equation for the linear system we’re modeling is *y* = *wx* + *b* where
    *w*, *b* are the learnable weights. As we mentioned previously, the loss for this
    system is <math alttext="script upper L equals left-parenthesis y minus w x minus
    b right-parenthesis squared"><mrow><mi>ℒ</mi> <mo>=</mo> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mi>w</mi><mi>x</mi><mo>-</mo><mi>b</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> . Some matrix calculus can be used to compute
    the gradients of the learnable parameters directly for *w*:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建模的线性系统的方程是*y* = *wx* + *b*，其中*w*，*b*是可学习的权重。正如我们之前提到的，这个系统的损失是<math alttext="script
    upper L equals left-parenthesis y minus w x minus b right-parenthesis squared"><mrow><mi>ℒ</mi>
    <mo>=</mo> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mi>w</mi><mi>x</mi><mo>-</mo><mi>b</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>。一些矩阵微积分可以用来直接计算*w*的可学习参数的梯度：
- en: <math display="block"><mrow><mi>∇</mi> <mi>w</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow>
    <mrow><mi>∂</mi><mi>w</mi></mrow></mfrac> <mo>=</mo> <mo>-</mo> <mn>2</mn> <mrow><mo>(</mo>
    <mi>y</mi> <mo>-</mo> <mi>w</mi> <mi>x</mi> <mo>-</mo> <mi>b</mi> <mo>)</mo></mrow>
    <msup><mi>x</mi> <mi>T</mi></msup></mrow></math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>∇</mi> <mi>w</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow>
    <mrow><mi>∂</mi><mi>w</mi></mrow></mfrac> <mo>=</mo> <mo>-</mo> <mn>2</mn> <mrow><mo>(</mo>
    <mi>y</mi> <mo>-</mo> <mi>w</mi> <mi>x</mi> <mo>-</mo> <mi>b</mi> <mo>)</mo></mrow>
    <msup><mi>x</mi> <mi>T</mi></msup></mrow></math>
- en: and for *b*
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*b*
- en: <math display="block"><mrow><mi>∇</mi> <mi>b</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow>
    <mrow><mi>∂</mi><mi>b</mi></mrow></mfrac> <mo>=</mo> <mo>-</mo> <mn>2</mn> <mrow><mo>(</mo>
    <mi>y</mi> <mo>-</mo> <mi>w</mi> <mi>x</mi> <mo>-</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>∇</mi> <mi>b</mi> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow>
    <mrow><mi>∂</mi><mi>b</mi></mrow></mfrac> <mo>=</mo> <mo>-</mo> <mn>2</mn> <mrow><mo>(</mo>
    <mi>y</mi> <mo>-</mo> <mi>w</mi> <mi>x</mi> <mo>-</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
- en: We place these equations here only for reference for curious readers. We will
    not attempt to systematically teach how to take the derivatives of the loss functions
    we encounter in this book. However, we will note that for complicated systems,
    taking the derivative of the loss function by hand helps build up an intuition
    for how the deep network learns. This intuition can serve as a powerful guide
    for the designer, so we encourage advanced readers to pursue this topic on their
    own.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些方程放在这里，仅供好奇的读者参考。我们不会试图系统地教授如何计算我们在本书中遇到的损失函数的导数。然而，我们将指出，对于复杂系统，通过手工计算损失函数的导数有助于建立对深度网络学习方式的直觉。这种直觉可以作为设计者的强大指导，因此我们鼓励高级读者自行探索这个主题。
- en: Visualizing linear regression models with TensorBoard
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用TensorBoard可视化线性回归模型
- en: The model defined in the previous section uses `tf.summary.FileWriter` to write
    logs to a logging directory */tmp/lr-train*. We can invoke TensorBoard on this
    logging directory with the command in [Example 3-13](#ch3-tensorboard) (TensorBoard
    is installed by default with TensorFlow).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节中定义的模型使用`tf.summary.FileWriter`将日志写入日志目录*/tmp/lr-train*。我们可以使用[示例3-13](#ch3-tensorboard)中的命令在此日志目录上调用TensorBoard（TensorBoard默认与TensorFlow一起安装）。
- en: Example 3-13\. Invoking TensorBoard
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-13. 调用TensorBoard
- en: '[PRE12]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This command will start TensorBoard on a port attached to localhost. Use your
    browser to open this port. The TensorBoard screen will look something like [Figure 3-8](#ch3-tensorboardscreen).
    (The precise appearance may vary depending on your version of TensorBoard.)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将在连接到localhost的端口上启动TensorBoard。使用浏览器打开此端口。TensorBoard屏幕将类似于[图3-8](#ch3-tensorboardscreen)。
    （具体外观可能会因您使用的TensorBoard版本而有所不同。）
- en: '![tensorboard_lr_raw.png](assets/tfdl_0308.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![tensorboard_lr_raw.png](assets/tfdl_0308.png)'
- en: Figure 3-8\. Screenshot of TensorBoard panel.
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8. TensorBoard面板截图。
- en: Navigate to the Graphs tab, and you will see a visualization of the TensorFlow
    architecture we have defined as illustrated in [Figure 3-9](#ch3-tensorboardarch).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 转到Graphs选项卡，您将看到我们定义的TensorFlow架构的可视化，如[图3-9](#ch3-tensorboardarch)所示。
- en: '![lr_graph.png](assets/tfdl_0309.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![lr_graph.png](assets/tfdl_0309.png)'
- en: Figure 3-9\. Visualization of linear regression architecture in TensorBoard.
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9. 在TensorBoard中可视化线性回归架构。
- en: Note that this visualization has grouped all computational graph elements belonging
    to various `tf.name_scopes`. Different groups are connected according to their
    dependencies in the computational graph. You can expand all of the grouped elements
    to view their contents. [Figure 3-10](#ch3-tensorboardarchexp) illustrates the
    expanded architecture.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此可视化已将属于各种`tf.name_scopes`的所有计算图元素分组。不同的组根据计算图中的依赖关系连接。您可以展开所有分组的元素以查看其内容。[图3-10](#ch3-tensorboardarchexp)展示了扩展的架构。
- en: As you can see, there are many hidden nodes that suddenly become visible! TensorFlow
    functions like `tf.train.AdamOptimizer` often hide many internal variables under
    a `tf.name_scope` of their own. Expanding in TensorBoard provides an easy way
    to peer underneath the hood to see what the system is actually creating. Although
    the visualization looks quite complex, most of these details are under the hood
    and not anything you need to worry about just yet.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![lr_expanded.png](assets/tfdl_0310.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. Expanded visualization of architecture.
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Navigate back to the Home tab and open the Summaries section. You should now
    see a loss curve that looks something like [Figure 3-11](#ch3-tensorboardloss).
    Note the smooth falling shape. The loss falls rapidly at the beginning as the
    prior is learned, then tapers off and settles.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![lr_loss_tensorboard.png](assets/tfdl_0311.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. Viewing the loss curve in TensorBoard.
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Visual and Nonvisual Debugging Styles
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is using a tool like TensorBoard necessary to get good use out of a system like
    TensorFlow? It depends. Is using a GUI or an interactive debugger necessary to
    be a professional programmer?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Different programmers have different styles. Some will find that the visualization
    capabilities of TensorBoard come to form a critical part of their tensorial programming
    workflows. Others will find that TensorBoard isn’t terribly useful and will make
    greater use of print-statement debugging. Both styles of tensorial programming
    and debugging are valid, just as there are great programmers who swear by debuggers
    and others who loathe them.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: In general, TensorBoard is quite useful for debugging and for building basic
    intuition about the dataset at hand. We recommend that you follow the style that
    works best for you.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for evaluating regression models
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we haven’t discussed how to evaluate whether a trained model has actually
    learned anything. The first tool for evaluating whether a model has trained is
    by looking at the loss curve to ensure it has a reasonable shape. You learned
    how to do this in the previous section. What’s the next thing to try?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'We now want you to look at *metrics* associated with the model. A metric is
    a tool for comparing predicted labels to true labels. For regression problems,
    there are two common metrics: *R*² and RMSE (root-mean-squared error). The *R*²
    is a measure of the correlation between two variables that takes values between
    +1 and 0\. +1 indicates perfect correlation, while 0 indicates no correlation.
    Mathematically, the *R*² for two datasets *X* and *Y* is defined as'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mtext>cov</mtext><msup><mrow><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow> <mrow><msubsup><mi>σ</mi> <mi>X</mi> <mn>2</mn></msubsup>
    <msubsup><mi>σ</mi> <mi>Y</mi> <mn>2</mn></msubsup></mrow></mfrac></mrow></math>
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Where cov(*X*, *Y*) is the covariance of *X* and *Y*, a measure of how the two
    datasets jointly vary, while <math alttext="sigma Subscript upper X"><msub><mi>σ</mi>
    <mi>X</mi></msub></math> and <math alttext="sigma Subscript upper Y"><msub><mi>σ</mi>
    <mi>Y</mi></msub></math> are standard deviations, measures of how much each set
    individually varies. Intuitively, the *R*² measures how much of the independent
    variation in each set can be explained by their joint variation.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Types of R²!
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that there are two common definitions of *R*² used in practice. A common
    beginner (and expert) mistake is to confuse the two definitions. In this book,
    we will always use the squared Pearson correlation coefficient ([Figure 3-12](#ch3-corr)).
    The other definition is called the coefficient of determination. This other *R*²
    is often much more confusing to deal with since it doesn’t have a lower bound
    of 0 like the squared Pearson correlation does.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 3-12](#ch3-corr), predicted and true values are highly correlated
    with an *R*² of nearly 1\. It looks like learning has done a wonderful job on
    this system and succeeded in learning the true rule. *Not so fast*. You will note
    that the scale on the two axes in the figure isn’t the same! It turns out that
    *R*² doesn’t penalize for differences in scale. In order to understand what’s
    happened on this system, we need to consider an alternate metric in [Figure 3-13](#ch3-rms).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![lr_pred.png](assets/tfdl_0312.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Plotting the Pearson correlation coefficient.
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![lr_learned.png](assets/tfdl_0313.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. Plotting the root-mean-squared error (RMSE).
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The RMSE is a measure of the average difference between predicted values and
    true values. In [Figure 3-13](#ch3-rms) we plot predicted values and true labels
    as two separate functions using datapoints *x* as our x-axis. Note that the line
    learned isn’t the true function! The RMSE is relatively high and diagnoses the
    error, unlike the *R*², which didn’t pick up on this error.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: What happened on this system? Why didn’t TensorFlow learn the correct function
    despite being trained to convergence? This example provides a good illustration
    of one of the weaknesses of gradient descent algorithms. There is no guarantee
    of finding the true solution! The gradient descent algorithm can get trapped in
    *local minima*. That is, it can find solutions that look good, but are not in
    fact the lowest minima of the loss function <math alttext="script upper L"><mi>ℒ</mi></math>
    .
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Why use gradient descent at all then? For simple systems, it is indeed often
    better to avoid gradient descent and use other algorithms that have stronger performance
    guarantees. However, on complicated systems, such as those we will show you in
    later chapters, there do not yet exist alternative algorithms that perform better
    than gradient descent. We encourage you to remember this fact as we proceed further
    into deep learning.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression in TensorFlow
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will define a simple classifier using TensorFlow. It’s worth
    first considering what the equation is for a classifier. The mathematical trick
    that is commonly used is exploiting the sigmoid function. The sigmoid, plotted
    in [Figure 3-14](#ch3-sigmoid), commonly denoted by <math alttext="sigma"><mi>σ</mi></math>
    , is a function from the real numbers <math alttext="double-struck upper R"><mi>ℝ</mi></math>
    to (0, 1). This property is convenient since we can interpret the output of a
    sigmoid as probability of an event happening. (The trick of converting discrete
    events into continuous values is a recurring theme in machine learning.)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic.gif](assets/tfdl_0314.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 3-14\. Plotting the sigmoid function.
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The equations for predicting the probabilities of a discrete 0/1 variable follow.
    These equations define a simple logistic regression model:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>y</mi> <mn>0</mn></msub> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math><math
    display="block"><mrow><msub><mi>y</mi> <mn>1</mn></msub> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow provides utility functions to compute the cross-entropy loss for
    sigmoidal values. The simplest of these functions is `tf.nn.sigmoid_cross_​entropy_with_logits`.
    (A logit is the inverse of the sigmoid. In practice, this simply means passing
    the argument to the sigmoid, *wx* + *b*, directly to TensorFlow instead of the
    sigmoidal value <math><mrow><mi>σ</mi> <mo>(</mo> <mi>w</mi> <mi>x</mi> <mo>+</mo>
    <mi>b</mi> <mo>)</mo></mrow></math> itself). We recommend using TensorFlow’s implementation
    instead of manually defining the cross-entropy, since there are tricky numerical
    issues that arise when computing the cross-entropy loss.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 3-14](#ch3-logistic) defines a simple logistic regression model in
    TensorFlow.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-14\. Defining a simple logistic regression model
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The training code for this model in [Example 3-15](#ch3-logistic-train) is identical
    to that for the linear regression model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-15\. Training a logistic regression model
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Visualizing logistic regression models with TensorBoard
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As before, you can use TensorBoard to visualize the model. Start by visualizing
    the loss function as shown in [Figure 3-15](#ch3-tensorboardlogistic). Note that
    as before, the loss function follows a neat pattern. There is a steep drop in
    the loss followed by a gradual smoothening.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic_loss_tensorboard.png](assets/tfdl_0315.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Figure 3-15\. Visualizing the logistic regression loss function.
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also view the TensorFlow graph in TensorBoard. Since the scoping structure
    was similar to that used for linear regression, the simplified graph doesn’t display
    much differently, as shown in [Figure 3-16](#ch3-tensorboardlogisticgraph).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic_graph.png](assets/tfdl_0316.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Figure 3-16\. Visualizing the computation graph for logistic regression.
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: However, if you expand the nodes in this grouped graph, as in [Figure 3-17](#ch3-tensorboardlogisticgraphexp),
    you will find that the underlying computational graph is different. In particular,
    the loss function is quite different from that used for linear regression (as
    it should be).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic_expanded.png](assets/tfdl_0317.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: Figure 3-17\. The expanded computation graph for logistic regression.
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Metrics for evaluating classification models
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you have trained a classification model for logistic regression, you
    need to learn about metrics suitable for evaluating classification models. Although
    the equations for logistic regression are more complicated than they are for linear
    regression, the basic evaluation metrics are simpler. The classification accuracy
    simply checks for the fraction of datapoints that are classified correctly by
    the learned model. In fact, with a little more effort, it is possible to back
    out the *separating line* learned by the logistic regression model. This line
    displays the cutoff boundary the model has learned to separate positive and negative
    examples. (We leave the derivation of this line from the logistic regression equations
    as an exercise for the interested reader. The solution is in the code for this
    section.)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: We display the learned classes and the separating line in [Figure 3-18](#ch3-loglearnedclasses).
    Note that the line neatly separates the positive and negative examples and has
    perfect accuracy (1.0). This result raises an interesting point. Regression is
    often a harder problem to solve than classification. There are many possible lines
    that would neatly separate the datapoints in [Figure 3-18](#ch3-loglearnedclasses),
    but only one that would have perfectly matched the data for the linear regression.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![logistic_pred.png](assets/tfdl_0318.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: Figure 3-18\. Viewing the learned classes and separating line for logistic regression.
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Review
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve shown you how to build and train some simple learning
    systems in TensorFlow. We started by reviewing some foundational mathematical
    concepts including loss functions and gradient descent. We then introduced you
    to some new TensorFlow concepts such as placeholders, scopes, and TensorBoard.
    We ended the chapter with case studies that trained linear and logistic regression
    systems on toy datasets. We covered a lot of material in this chapter, and it’s
    OK if you haven’t yet internalized everything. The foundational material introduced
    here will be used throughout the remainder of this book.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#fully_connected_networks), we will introduce you to
    your first deep learning model and to fully connected networks, and will show
    you how to define and train fully connected networks in TensorFlow. In following
    chapters, we will explore more complicated deep networks, but all of these architectures
    will use the same fundamental learning principles introduced in this chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#fully_connected_networks)中，我们将向您介绍您的第一个深度学习模型和全连接网络，并向您展示如何在TensorFlow中定义和训练全连接网络。在接下来的章节中，我们将探索更复杂的深度网络，但所有这些架构都将使用本章介绍的相同基本学习原则。
