<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">Appendix B. Minimally qualified readers and deep learning basics<a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/></h1>
<p class="body">This book is intended for machine learning enthusiasts and data scientists across various business fields who possess intermediate Python programming skills and are interested in learning about generative AI. Through this book, readers will learn to create novel and innovative content—such as images, text, numbers, shapes, and audio—that can benefit their employers’ businesses and advance their own careers.<a id="idIndexMarker000"/><a id="marker-395"/></p>
<p class="body">This book is designed for those who have a solid grasp of Python. You should be familiar with variable types like integers, floats, strings, and Booleans. You should also be comfortable creating <i class="fm-italics">for</i> and <i class="fm-italics">while</i> loops and understand conditional execution and branching (e.g., using <i class="fm-italics">if</i>, <i class="fm-italics">elif</i>, and <i class="fm-italics">else</i> statements). The book involves frequent use of Python functions and classes, and you should know how to install and import third-party Python libraries and packages. If you need to brush up on these skills, the free online Python tutorial provided by W3Schools is a great resource (<a class="url" href="https://www.w3schools.com/python/">https://www.w3schools.com/python/</a>).</p>
<p class="body">Additionally, you should have a basic understanding of machine learning, particularly neural networks and deep learning. In this appendix, we will review key concepts such as loss functions, activation functions, and optimizers, which are essential for developing and training deep neural networks. However, this appendix is not meant to be a comprehensive tutorial on these topics. If you find gaps in your understanding, it is strongly recommended that you address them before proceeding with the projects in this book. A good book for this purpose is <i class="fm-italics">Deep Learning with PyTorch</i> by Stevens, Antiga, and Viehmann (2020).<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">1</a></sup> <a id="idIndexMarker001"/><a id="idIndexMarker002"/><a id="idIndexMarker003"/><a id="idIndexMarker004"/></p>
<p class="body">No prior experience with PyTorch or generative AI is required. In chapter 2, you will learn the basics of PyTorch, starting with its basic data types. You will also implement an end-to-end deep learning project in PyTorch to get hands-on experience. The goal of chapter 2 is to prepare you to use PyTorch for building and training various generative models in the book.</p>
<h2 class="fm-head" id="heading_id_3">B.1 Deep learning and deep neural netw<a id="idTextAnchor006"/>orks</h2>
<p class="body"><a id="marker-396"/>Machine learning (ML) represents a new paradigm in AI. Unlike traditional rule-based AI, which involves programming explicit rules into a computer, ML involves feeding the computer various examples and allowing it to learn the rules on its own. Deep learning is a subset of ML that employs deep neural networks for this learning process.<a id="idIndexMarker005"/><a id="idIndexMarker006"/></p>
<p class="body">In this section, you’ll learn about neural networks and why some are considered deep neural net<a id="idTextAnchor007"/>works.</p>
<h3 class="fm-head1" id="heading_id_4">B.1.1 Anatomy of a neural network</h3>
<p class="body">A neural network aims to mimic the functioning of the human brain. It consists of an input layer, an output layer, and zero, one, or more hidden layers in between. The term “deep neural networks” refers to networks with many hidden layers, which tend to be more powerful.<a id="idIndexMarker007"/><a id="idIndexMarker008"/><a id="idIndexMarker009"/></p>
<p class="body">We’ll start with a simpler example featuring two hidden layers, as shown in figure B.1.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="295" src="../../OEBPS/Images/APPB_F01_Liu.png" width="425"/></p>
<p class="figurecaption">Figure B.1 The structure of a neural network. A neural network is composed of an input layer; zero, one, or more hidden layers; and an output layer. Each layer contains one or more neurons. Neurons in each layer are connected to those in the preceding and subsequent layers, with the strength of these connections represented by weights. In this figure, the neural network features an input layer with three neurons, two hidden layers with six and four neurons, respectively, and an output layer with two neurons.</p>
</div>
<p class="body">A neural network consists of an input layer, a variable number of hidden layers, and an output layer. Each layer is made up of one or more neurons. Neurons in one layer connect to neurons in the previous and next layers, with the connection strengths measured by weights. In the example illustrated in figure B.1, the neural network features an input layer with three neurons, two hidden layers containing six and four neurons, respectively, and an output layer with two neurons.</p>
<h3 class="fm-head1" id="heading_id_5">B.1.2 Different types of layers in neural networks</h3>
<p class="body"><a id="marker-397"/>Within a neural network, various types of layers serve distinct purposes. The most common is the dense layer, where each neuron is connected to every neuron in the next layer. Because of this full connectivity, a dense layer is also referred to as a fully connected layer. <a id="idIndexMarker010"/><a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="idIndexMarker013"/></p>
<p class="body">Another frequently used type of neural layer, especially in this book, is the convolutional layer. Convolutional layers treat input as multidimensional data and are adept at extracting patterns from it. In our book, convolutional layers are often employed to extract spatial features from images.</p>
<p class="body">Convolutional layers differ from fully connected (dense) layers in several key ways. First, each neuron in a convolutional layer connects only to a small region of the input. This design is based on the understanding that in image data, local groups of pixels are more likely to be related. This local connectivity significantly reduces the number of parameters, making convolutional neural networks (CNNs) more efficient. Second, CNNs utilize shared weights—the same weights are applied across different regions of the input. This mechanism is similar to sliding a filter across the entire input space. This filter detects specific features (e.g., edges or textures) regardless of their position in the input, which leads to the property of translation invariance. Due to their structure, CNNs are more efficient for image processing, requiring fewer parameters than fully connected networks of similar size. This results in faster training times and lower computational costs. Additionally, CNNs are generally more effective at capturing spatial hierarchies in image data. We discuss CNNs in detail in chapter 4.<a id="idIndexMarker014"/><a id="idIndexMarker015"/></p>
<p class="body">The third type of neural network is the recurrent neural network (RNN). Fully connected networks treat each input independently, processing each input separately without considering any relationship or order between different inputs. In contrast, RNNs are specifically designed to handle sequential data. In an RNN, the output at a given time step depends not only on the current input but also on previous inputs. This allows RNNs to maintain a form of memory, capturing information from previous time steps to influence the processing of the current input. See chapter 8 for details on RNNs.<a id="idIndexMarker016"/><a id="idIndexMarker017"/></p>
<h3 class="fm-head1" id="heading_id_6">B.1.3 Activation Functions</h3>
<p class="body">Activation functions are a crucial component of neural networks, functioning as the mechanisms that transform inputs into outputs and determine when a neuron should activate. Some functions are akin to on-off switches, playing a pivotal role in enhancing the power of neural networks. Without activation functions, neural networks would be limited to learning only linear relationships in data. By introducing nonlinearity, activation functions enable the creation of complex, nonlinear relationships between inputs and outputs.<a id="idIndexMarker018"/><a id="idIndexMarker019"/><a id="idIndexMarker020"/><a id="idIndexMarker021"/></p>
<p class="body">The most commonly-used activation function is the rectified linear unit (ReLU). A ReLU activates the neuron when the input is positive, effectively allowing information to pass through. When the input is negative, the neuron is deactivated. This straightforward on-off behavior facilitates the modeling of nonlinear relationships.<a id="idIndexMarker022"/><a id="idIndexMarker023"/></p>
<p class="body">Another commonly used activation function is the sigmoid function, which is particularly suited for binary classification problems. The sigmoid function compresses inputs into a range between 0 and 1, effectively representing the probabilities of a binary outcome.<a id="idIndexMarker024"/></p>
<p class="body">For multicategory classification tasks, the softmax function is employed. The softmax function transforms a vector of values into a probability distribution, where the values sum to 1. This is ideal for modeling the probabilities of multiple outcomes.<a id="marker-398"/><a id="idIndexMarker025"/></p>
<p class="body">Lastly, the tanh activation function is noteworthy. Similar to the sigmoid function, tanh<a id="idTextAnchor008"/> produces values between –1 and 1. This characteristic is especially useful when working with images, as image data often contains values within this range.<a id="idIndexMarker026"/><a id="idIndexMarker027"/><a id="idIndexMarker028"/></p>
<h2 class="fm-head" id="heading_id_7">B.2 Training a deep neural network</h2>
<p class="body">This section provides an overview of the steps involved in training a neural network. A key aspect of this process is dividing your training dataset into a train set, a validation set, and a test set, which is crucial for developing a robust deep neural network. We will also discuss various loss functions and optimizers used in training neural networks. <a id="idIndexMarker029"/><a id="idIndexMarker030"/></p>
<h3 class="fm-head1" id="heading_id_8">B.2.1 The training process</h3>
<p class="body">Once a neural network is built, the next step is to gather a training dataset to train the model. Figure B.2 illustrates the steps in the training process.<a id="idIndexMarker031"/><a id="idIndexMarker032"/></p>
<p class="body">On the left side of figure B.2, we see the initial division of the training dataset into three subsets: the train set, the validation set, and the test set. This division is critical for building a robust deep neural network. The training set is the subset of data used to train the model, where the model learns patterns, weights, and biases. The validation set is used to evaluate the model’s performance during training and to decide when to stop training. The test set is used to assess the final performance of the model after training is complete, providing an unbiased evaluation of the model’s ability to generalize to new, unseen data.</p>
<p class="body">During the training phase, the model is trained on data in the train set. It iteratively adjusts its parameters to minimize the loss function (see the next subsection on different loss functions). After each epoch, the model’s performance is evaluated using the validation set. If the performance on the validation set continues to improve, training proceeds. If the performance ceases to improve, training is stopped to prevent overfitting.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="356" src="../../OEBPS/Images/APPB_F02_Liu.png" width="601"/></p>
<p class="figurecaption">Figure B.2 Training a neural network. The training dataset is divided into three subsets: the train set, the validation set, and the test set. The process for training a neural network involves the following steps. In the training phase, the train set is used to train the neural network and adjust its parameters to minimize the loss function. During each iteration of training, the model updates its parameters based on data in the train set. In the validation phase of each iteration, the model is evaluated using the validation set. The performance on the validation set helps determine if the model is still improving. If the model’s performance on the validation set continues to improve, the next iteration of training proceeds using the train set. If the model’s performance on the validation set stops improving, the training process is stopped to prevent overfitting. Once training is complete, the trained model is evaluated on the test set. This evaluation provides the final testing results, giving an estimate of the model’s performance on unseen data.</p>
</div>
<p class="body">Once training is complete, the testing phase begins. The model is applied to the test set (unseen data) to assess its final performance and report results.</p>
<p class="body">Dividing the dataset into three different sets is essential for several reasons. The train subset allows the model to learn patterns and features from the data and to adjust its parameters. The validation subset serves as a check against overfitting by enabling performance monitoring during training. The test subset provides an unbiased evaluation of the model’s generalization ability, estimating its real-world performance.</p>
<p class="body">By appropriately splitting the data and utilizing each set for its intended purpose, we ensure that the model is well trained and unbiasedly evaluated.</p>
<h3 class="fm-head1" id="heading_id_9">B.2.2 Loss functions</h3>
<p class="body"><a id="marker-399"/>Loss functions are essential for measuring the accuracy of our predictions and guiding the optimization process when training deep neural networks.<a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/></p>
<p class="body">A commonly used loss function is the mean squared error (MSE or L2 loss). MSE calculates the average squared difference between the model’s predictions and the actual values. A closely related loss function is the mean absolute error (MAE or L1 loss). MAE calculates the average absolute difference between predictions and actual values. MAE is often used if the data are noisy and have many outliers since it punishes extreme values less than the L2 loss.</p>
<p class="body">For binary classification tasks, where predictions are binary (0 or 1), the preferred loss function is binary cross-entropy. This function measures the average difference between predicted probabilities and actual binary labels.</p>
<p class="body">In multicategory classification tasks, where predictions can take multiple discrete values, the categorical cross-entropy loss function is employed. This function measures the average difference between predicted probability distributions and actual distributions.</p>
<p class="body">During the training of ML models such as deep neural networks, we adjust the model parameters to minimize the loss function. The adjustment magnitude is proportional to the first derivative of the loss function with respect to the model parameters. The learning rate controls the speed of these adjustments. If the learning rate is too high, the model parameters may oscillate around the optimal values and never converge. Conversely, if the learning rate is too low, the learning process becomes slow, and it takes a long time for the parameters to converge.</p>
<h3 class="fm-head1" id="heading_id_10">B.2.3 Optimizers</h3>
<p class="body">Optimizers are algorithms used in training deep neural networks to adjust the model’s weights to minimize the loss function. They guide the learning process by determining how the model’s parameters should be updated at each step, thus enhancing performance over time.<a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="marker-400"/><a id="idIndexMarker038"/></p>
<p class="body">One example of an optimizer is a stochastic gradient descent (SGD). A SGD adjusts weights by moving them in the direction of the negative gradient of the loss function. It updates weights using a subset of the data (mini-batch) at each iteration, which helps speed up the training process and improve generalization.</p>
<p class="body">In this book, the most commonly used optimizer is Adam (Adaptive Moment Estimation). Adam combines the benefits of two other extensions of SGD: AdaGrad and RMSProp. It computes adaptive learning rates for each parameter based on estimates of the first and second moments of the gradients. This adaptability makes Adam particularly suitable for problems involving large datasets and/or numerous parameters.<a id="idIndexMarker039"/><a id="idIndexMarker040"/></p>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">1</a></sup>  Eli Stevens, Luca Antiga, and Thomas Viehmann, 2020, <i class="fm-italics">Deep Learning with PyTorch</i>, Manning Publications.</p>
</div></body></html>