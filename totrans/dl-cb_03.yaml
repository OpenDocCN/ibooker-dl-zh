- en: Chapter 3\. Calculating Text Similarity Using Word Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。使用单词嵌入计算文本相似性
- en: Tip
  id: totrans-1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Before we get started, this is the first chapter with actual code in it. Chances
    are you skipped straight to here, and who would blame you? To follow the recipes
    it really helps though if you have the accompanying code up and running. You can
    easily do this by executing the following commands in a shell:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，这是第一个包含实际代码的章节。有可能你直接跳到这里，谁会责怪你呢？不过，要按照这些步骤，确保你已经准备好了相关的代码是非常有帮助的。你可以通过在shell中执行以下命令来轻松实现这一点：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can find a more detailed explanation in [“What Do You Need to Know?”](preface01.html#preface_what_do_you_need_to_know).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[“你需要知道什么？”](preface01.html#preface_what_do_you_need_to_know)中找到更详细的解释。
- en: In this chapter we’ll look at word embeddings and how they can help us to calculate
    the similarities between pieces of text. Word embeddings are a powerful technique
    used in natural language processing to represent words as vectors in an *n*-dimensional
    space. The interesting thing about this space is that words that have similar
    meanings will appear close to each other.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看看单词嵌入以及它们如何帮助我们计算文本片段之间的相似性。单词嵌入是自然语言处理中使用的一种强大技术，将单词表示为*n*维空间中的向量。这个空间的有趣之处在于具有相似含义的单词会彼此靠近。
- en: The main model we’ll use here is a version of Google’s Word2vec. This is not
    a deep neural model. In fact, it is no more than a big lookup table from word
    to vector and therefore hardly a model at all. The Word2vec embeddings are produced
    as a side effect of training a network to predict a word from context for sentences
    taken from Google News. Moreover, it is possibly the best-known example of an
    embedding, and embeddings are an important concept in deep learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用Google的Word2vec的一个版本作为主要模型。这不是一个深度神经模型。事实上，它只是一个从单词到向量的大型查找表，因此几乎根本不算是一个模型。Word2vec的嵌入是在训练网络以从Google新闻中获取的句子的上下文中预测单词的副作用。此外，它可能是嵌入的最著名的例子，嵌入是深度学习中一个重要的概念。
- en: Once you start looking for them, high-dimensional spaces with semantic properties
    start popping up everywhere in deep learning. We can build a movie recommender
    by projecting movies into a high-dimensional space ([Chapter 4](ch04.html#movie_recommender))
    or create a map of handwritten digits using only two dimensions ([Chapter 13](ch13.html#autoencoders)).
    Image recognition networks project images into a space such that similar images
    are near to each other ([Chapter 10](ch10.html#image_search)).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你开始寻找它们，具有语义属性的高维空间就会在深度学习中随处可见。我们可以通过将电影投影到高维空间中来构建电影推荐系统([第4章](ch04.html#movie_recommender))，或者仅使用两个维度创建手写数字的地图([第13章](ch13.html#autoencoders))。图像识别网络将图像投影到一个空间中，使得相似的图像彼此靠近([第10章](ch10.html#image_search))。
- en: In the current chapter we’ll focus on just word embeddings. We’ll start with
    using a pretrained word embedding model to calculate word similarities, then show
    some interesting Word2vec math. We’ll then explore how to visualize these high-dimensional
    spaces.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前章节中，我们将专注于单词嵌入。我们将从使用预训练的单词嵌入模型计算单词相似性开始，然后展示一些有趣的Word2vec数学。然后，我们将探索如何可视化这些高维空间。
- en: Next, we’ll take a look at how we can exploit the semantic properties of word
    embeddings like Word2vec for domain-specific ranking. We’ll treat the words and
    their embeddings as the entities they represent, with some interesting results.
    We’ll start with finding entity classes in Word2vec embeddings—in this case, countries.
    We’ll then show how to rank terms against these countries and how to visualize
    these results on a map.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何利用Word2vec等单词嵌入的语义属性进行特定领域的排名。我们将把单词及其嵌入视为它们所代表的实体，得到一些有趣的结果。我们将从在Word2vec嵌入中找到实体类开始——在这种情况下是国家。然后，我们将展示如何对这些国家进行排名术语，以及如何在地图上可视化这些结果。
- en: Word embeddings are a powerful way to map words to vectors and have many uses.
    They are often used as a preprocessing step for text.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入是将单词映射到向量的强大方式，有许多用途。它们经常被用作文本的预处理步骤。
- en: 'There are two Python notebooks associated with this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章相关的有两个Python笔记本：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 3.1 Using Pretrained Word Embeddings to Find Word Similarity
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.1 使用预训练的单词嵌入查找单词相似性
- en: Problem
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to find out whether two words are similar but not equal, for example
    when you’re verifying user input and you don’t want to require the user to exactly
    enter the expected word.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要找出两个单词是否相似但不相等，例如当你验证用户输入时，不希望要求用户完全输入预期的单词。
- en: Solution
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: You can use a pretrained word embedding model. We’ll use `gensim` in this example,
    a useful library in general for topic modeling in Python.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用一个预训练的单词嵌入模型。在这个例子中，我们将使用`gensim`，这是一个在Python中用于主题建模的有用库。
- en: 'The first step is to acquire a pretrained model. There are a number of pretrained
    models available for download on the internet, but we’ll go with the Google News
    one. It has embeddings for 3 million words and was trained on roughly 100 billion
    words taken from the Google News archives. Downloading it will take a while, so
    we’ll cache the file locally:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是获取一个预训练模型。互联网上有许多可供下载的预训练模型，但我们将选择Google News的一个。它包含300万个单词的嵌入，并且是在大约1000亿个来自Google新闻档案的单词上进行训练的。下载需要一些时间，所以我们将在本地缓存文件：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we have the model downloaded, we can load it into memory. The model
    is quite big and this will take around 5 GB of RAM:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经下载了模型，我们可以将其加载到内存中。这个模型非常庞大，大约需要5GB的RAM。
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the model has finished loading, we can use it to find similar words:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 模型加载完成后，我们可以使用它来查找相似的单词：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Discussion
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Word embeddings associate an *n*-dimensional vector with each word in the vocabulary
    in such a way that similar words are near each other. Finding similar words is
    a mere nearest-neighbor search, for which there are efficient algorithms even
    in high-dimensional spaces.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入将一个*n*维向量与词汇表中的每个单词相关联，使得相似的单词彼此靠近。查找相似单词只是一个最近邻搜索，即使在高维空间中也有高效的算法。
- en: Simplifying things somewhat, the Word2vec embeddings are obtained by training
    a neural network to predict a word from its context. So, we ask the network to
    predict which word it should pick for X in a series of fragments; for example,
    “the cafe served a X that really woke me up.”
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 简化一下，Word2vec嵌入是通过训练神经网络来预测单词的上下文而获得的。因此，我们要求网络预测在一系列片段中应该选择哪个单词作为X；例如，“咖啡馆提供了一种让我真正清醒的X。”
- en: This way words that can be inserted into similar patterns will get vectors that
    are close to each other. We don’t care about the actual task, just about the assigned
    weights, which we will get as a side effect of training this network.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，可以插入类似模式的单词将获得彼此接近的向量。我们不关心实际任务，只关心分配的权重，这将作为训练这个网络的副作用得到。
- en: Later in this book we’ll see how word embeddings can also be used to feed words
    into a neural network. It is much more feasible to feed a 300-dimensional embedding
    vector into a network than a 3-million-dimensional one that is one-hot encoded.
    Moreover, a network fed with pretrained word embeddings doesn’t have to learn
    the relationships between the words, but can start with the real task at hand
    immediately.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后面部分，我们将看到词嵌入也可以用来将单词输入神经网络。将一个300维的嵌入向量输入网络比输入一个300万维的one-hot编码更可行。此外，使用预训练的词嵌入来喂养网络不需要学习单词之间的关系，而是可以立即开始处理真正的任务。
- en: 3.2 Word2vec Math
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.2 Word2vec数学
- en: Problem
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you automatically answer questions of the form “A is to B as C is to
    what?”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如何自动回答“A是B，C是什么”的问题？
- en: Solution
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the semantic properties of the Word2vec model. The `gensim` library makes
    this rather straightforward:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 利用Word2vec模型的语义属性。`gensim`库使这变得相当简单：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now apply this to arbitrary words—for example, to find what relates
    to “king” the way “son” relates to “daughter”:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将这种方法应用于任意单词，例如，找到与“国王”相关的单词，就像“儿子”与“女儿”相关一样：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can also use this approach to look up the capitals of selected countries:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用这种方法查找选定国家的首都：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'or to find the main products of companies (note the # placeholder for any number
    used in these embeddings):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 或者查找公司的主要产品（注意这些嵌入中使用的任何数字的占位符#）：
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Discussion
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'As we saw in the previous step, the vectors associated with the words encode
    the meaning of the words—words that are similar to each other have vectors that
    are close to each other. It turns out that the difference between word vectors
    also encodes the difference between words, so if we take the vector for the word
    “son” and deduct the vector for the word “daughter” we end up with a difference
    that can be interpreted as “going from male to female.” If we add this difference
    to the vector for the word “king” we end up near the vector for the word “queen”:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的步骤中看到的，与单词相关联的向量编码了单词的含义——相互相似的单词具有彼此接近的向量。事实证明，单词向量之间的差异也编码了单词之间的差异，因此如果我们取单词“儿子”的向量并减去单词“女儿”的向量，我们最终得到一个可以解释为“从男性到女性”的差异。如果我们将这个差异加到单词“国王”的向量上，我们最终会接近单词“女王”的向量。
- en: '![clarifying diagram](assets/dlcb_03in01.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![澄清图](assets/dlcb_03in01.png)'
- en: The `most_similar` method takes one or more positive words and one or more negative
    words. It looks up the corresponding vectors, then deducts the negative from the
    positive and returns the words that have vectors nearest to the resulting vector.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`most_similar`方法接受一个或多个正词和一个或多个负词。它查找相应的向量，然后从正向量中减去负向量，并返回与结果向量最接近的单词。'
- en: So in order to answer the question “A is to B as C is to?” we want to deduct
    A from B and then add C, or call `most_similar` with `positive = [B, C]` and `negative
    = [A]`. The example `A_is_to_B_as_C_is_to` adds two small features to this behavior.
    If we request only one example, it will return a single item, rather than a list
    with one item. Similarly, we can return either lists or single items for A, B,
    and C.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了回答“A是B，C是什么”的问题，我们希望从B中减去A，然后加上C，或者使用`positive = [B, C]`和`negative = [A]`调用`most_similar`。示例`A_is_to_B_as_C_is_to`为这种行为添加了两个小特性。如果我们只请求一个示例，它将返回一个单个项目，而不是一个包含一个项目的列表。同样，我们可以为A、B和C返回列表或单个项目。
- en: Being able to provide lists turned out to be useful in the product example.
    We asked for three products per company, which makes it more important to get
    the vector exactly right than if we only asked for one. By providing “Starbucks”
    and “Apple,” we get a more exact vector for the concept of “is a product of.”
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 提供列表的能力在产品示例中证明是有用的。我们要求每家公司三种产品，这使得准确获取向量比仅要求一种产品更为重要。通过提供“星巴克”和“苹果”，我们可以获得更准确的“是产品”的概念向量。
- en: 3.3 Visualizing Word Embeddings
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.3 可视化词嵌入
- en: Problem
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to get some insight into how word embeddings partition a set of objects.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要了解单词嵌入如何将一组对象分区。
- en: Solution
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: A 300-dimensional space is hard to browse, but luckily we can use an algorithm
    called *t-distributed stochastic neighbor embedding* (t-SNE) to fold a higher-dimensional
    space into something more comprehensible, like two dimensions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 300维空间很难浏览，但幸运的是，我们可以使用一种称为t-分布随机邻居嵌入（t-SNE）的算法将高维空间折叠成更易理解的二维空间。
- en: 'Let’s say we want to look at how three sets of terms are partitioned. We’ll
    pick countries, sports, and drinks:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想看看三组术语是如何分区的。我们选择国家、体育和饮料：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now let’s look up their vectors:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们查找它们的向量：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now use t-SNE to find the clusters in the 300-dimensional space:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用t-SNE来找到300维空间中的聚类：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s use matplotlib to show the results in a nice scatter plot:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用matplotlib在一个漂亮的散点图中展示结果：
- en: '[PRE17]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result is:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '![Scatter plot of items](assets/dlcb_03in02.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![项目的散点图](assets/dlcb_03in02.png)'
- en: Discussion
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: t-SNE is a clever algorithm; you give it a set of points in a high-dimensional
    space, and it iteratively tries to find the best projection onto a lower-dimensional
    space (usually a plane) that maintains the distances between the points as well
    as possible. It is therefore very suitable for visualizing higher dimensions like
    (word) embeddings.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: For more complex situations, the `perplexity` parameter is something to play
    around with. This variable loosely determines the balance between local accuracy
    and overall accuracy. Setting it to a low value creates small clusters that are
    locally accurate; setting it higher leads to more local distortions, but with
    better overall clusters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Finding Entity Classes in Embeddings
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In high-dimensional spaces there are often subspaces that contain only entities
    of one class. How do you find those spaces?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apply a support vector machine (SVM) on a set of examples and counterexamples.
    For example, let’s find the countries in the Word2vec space. We’ll start by loading
    up the model again and exploring things similar to a country, Germany:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As you can see there are a number of countries nearby, but words like “German”
    and the names of German cities also show up in the list. We could try to construct
    a vector that best represents the concept of “country” by adding up the vectors
    of many countries rather than just using Germany, but that only goes so far. The
    concept of country in the embedding space isn’t a point, it is a shape. What we
    need is a real classifier.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Support vector machines have proven effective for classification tasks like
    this. Scikit-learn has an easy-to-deploy solution. The first step is to build
    a training set. For this recipe getting positive examples is not hard since there
    are only so many countries:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Having more positive examples is of course better, but for this example using
    40–50 will give us a good idea of how the solution works.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need some negative examples. We sample these directly from the general
    vocabulary of the Word2vec model. We could get unlucky and draw a country and
    put it in the negative examples, but given the fact that we have 3 million words
    in the model and there are less than 200 countries in the world, we’d have to
    be very unlucky indeed:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now we’ll create a labeled training set based on the positive and negative
    examples. We’ll use `1` as the label for something being a country, and `0` for
    it not being a country. We’ll follow the convention of storing the training data
    in a variable `X` and the labels in a variable `y`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s train the model. We’ll set aside a fraction of the data to evaluate how
    we are doing:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The training should happen almost instantaneously even on a not very powerful
    computer since our dataset is relatively small. We can have a peek at how we are
    doing by looking at how many times the model has the right prediction for the
    bits of the eval set:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The results you get will depend a bit on the positive countries selected and
    which negative samples you happened to draw. I mostly get a list of countries
    that it missed—typically because the country name also means something else, like
    Jordan, but there are also some genuine misses in there. The precision comes out
    at 99.9% or so.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now run the classifier over all of the words to extract the countries:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The results are pretty good, though not perfect. The word “countries” itself,
    for example, is classified as a country, as are entities like continents or US
    states.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Support vector machines are effective tools when it comes to finding classes
    within a higher-dimensional space like word embeddings. They work by trying to
    find hyperplanes that separate the positive examples from the negative examples.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Countries in Word2vec are all somewhat near to each other since they share
    a semantic aspect. SVMs help us find the cloud of countries and come up with boundaries.
    The following diagram visualizes this in two dimensions:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![clever diagram](assets/dlcb_03in03.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: SVMs can be used for all kinds of ad hoc classifiers in machine learning since
    they are effective even if the number of dimensions is greater than the number
    of samples, like in this case. The 300 dimensions could allow the model to overfit
    the data, but because the SVM tries to find a simple model to fit the data, we
    can still generalize from a dataset as small as a few dozen examples.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs可以用于机器学习中各种特定的分类器，因为即使维度数大于样本数，它们也是有效的，就像在这种情况下一样。300个维度可能使模型过度拟合数据，但由于SVM试图找到一个简单的模型来拟合数据，我们仍然可以从一个只有几十个示例的数据集中推广。
- en: The results achieved are pretty good, though it is worth noting that in a situation
    where we have 3 million negative examples, 99.7% precision would still give us
    9,000 false positives, drowning out the actual countries.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 取得的结果相当不错，尽管值得注意的是，在有300万个负例的情况下，99.7%的精度仍会给我们带来9000个假阳性，淹没了实际的国家。
- en: 3.5 Calculating Semantic Distances Inside a Class
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.5 计算类内的语义距离
- en: Problem
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you find the most relevant items from a class for a given criterion?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如何找到一个类中对于给定标准最相关的项目？
- en: Solution
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Given a class, for example *countries*, we can rank the members of that class
    against a criterion, by looking at the relative distances:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个类，例如*国家*，我们可以根据一个标准对该类的成员进行排名，通过查看相对距离：
- en: '[PRE28]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can now, as before, extract the vectors for the countries into a `numpy`
    array that lines up with the countries:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以像以前一样，将国家的向量提取到一个与国家对齐的`numpy`数组中：
- en: '[PRE30]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'A quick sanity check to see which countries are most like Canada:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 快速检查看哪些国家最像加拿大：
- en: '[PRE31]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The Caribbean countries are somewhat surprising and a lot of the news about
    Canada must be related to hockey, given the appearance of Slovakia and Finland
    in the list, but otherwise it doesn’t look unreasonable.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 加勒比国家有些令人惊讶，关于加拿大的许多新闻必须与曲棍球有关，鉴于斯洛伐克和芬兰出现在列表中，但除此之外看起来并不不合理。
- en: 'Let’s switch gears and do some ranking for an arbitrary term over the set of
    countries. For each country we’ll calculate the distance between the name of the
    country and the term we want to rank against. Countries that are “closer” to the
    term are more relevant for the term:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转换思路，对一组国家的任意术语进行排名。对于每个国家，我们将计算国家名称与我们想要排名的术语之间的距离。与术语“更接近”的国家对于该术语更相关：
- en: '[PRE33]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'For example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE34]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Since the Word2vec model we are using was trained on Google News, the ranker
    will return countries that are mostly known for the given term in the recent news.
    India might be more often mentioned for cricket, but as long as it is also covered
    for other things, Sri Lanka can still win.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的Word2vec模型是在Google新闻上训练的，排名器将返回最近新闻中大多数以给定术语而闻名的国家。印度可能更常被提及与板球有关，但只要它也涵盖其他事物，斯里兰卡仍然可以获胜。
- en: Discussion
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In spaces where we have members of different classes projected into the same
    dimensions, we can use the cross-class distances as a measure of affinity. Word2vec
    doesn’t quite represent a conceptual space (the word “Jordan” can refer to the
    river, the country, or a person), but it is good enough to nicely rank countries
    on relevance for various concepts.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将不同类的成员投影到相同维度的空间中的情况下，我们可以使用跨类距离作为亲和度的度量。Word2vec并不完全代表一个概念空间（单词“Jordan”可以指河流、国家或一个人），但它足够好，可以很好地对各种概念的国家进行排名。
- en: A similar approach is often taken when building recommender systems. For the
    Netflix challenge, for example, a popular strategy was to use user ratings for
    movies as a way to project users and movies into a shared space. Movies that are
    close to a user are then expected to be rated highly by that user.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 构建推荐系统时通常采用类似方法。例如，在Netflix挑战中，一种流行的策略是使用用户对电影的评分来将用户和电影投影到一个共享空间中。接近用户的电影预计会受到用户高度评价。
- en: In situations where we have two spaces that are not the same, we can still use
    this trick if we can calculate the projection matrix to go from one space to the
    other. This is possible if we have enough candidates whose positions we know in
    both spaces.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们有两个不同的空间的情况下，如果我们可以计算从一个空间到另一个空间的投影矩阵，我们仍然可以使用这个技巧。如果我们有足够多的候选者在两个空间中的位置我们都知道，这是可能的。
- en: 3.6 Visualizing Country Data on a Map
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.6 在地图上可视化国家数据
- en: Problem
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you visalize country rankings from an experiment on a map?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在地图上可视化实验中的国家排名？
- en: Solution
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: GeoPandas is a perfect tool to visualize numerical data on top of a map.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: GeoPandas是在地图上可视化数值数据的完美工具。
- en: 'This nifty library combines the power of Pandas with geographical primitives
    and comes preloaded with a few maps. Let’s load up the world:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个巧妙的库将Pandas的强大功能与地理原语结合在一起，并预装了一些地图。让我们加载世界：
- en: '[PRE36]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This shows us some basic information about a set of countries. We can add a
    column to the `world` object based on our `rank_countries` function:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这向我们展示了一些关于一组国家的基本信息。我们可以根据我们的`rank_countries`函数向`world`对象添加一列：
- en: '[PRE37]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This draws, for example, the map for coffee quite nicely, highlighting the coffee
    consuming countries and the coffee producing countries.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这很好地绘制了咖啡地图，突出了咖啡消费国家和咖啡生产国家。
- en: '![World map for coffee](assets/dlcb_03in04.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![咖啡世界地图](assets/dlcb_03in04.png)'
- en: Discussion
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Visualizing data is an important technique for machine learning. Being able
    to look at the data, whether it is the input or the result of some algorithm,
    allows us to quickly spot anomalies. Do people in Greenland really drink that
    much coffee? Or are we seeing an artifact because of “Greenlandic coffee” (a variation
    on Irish coffee)? And those countries in the middle of Africa—do they really neither
    drink nor produce coffee? Or do we just have no data on them because they don’t
    occur in our embeddings?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化数据是机器学习中的一项重要技术。能够查看数据，无论是输入还是某些算法的结果，都可以让我们快速发现异常。格陵兰岛的人们真的喝那么多咖啡吗？还是因为“格陵兰咖啡”（爱尔兰咖啡的变种）而看到了一种人为现象？那些位于非洲中部的国家——他们真的既不喝咖啡也不生产咖啡吗？还是因为我们的嵌入中没有关于它们的数据？
- en: GeoPandas is the perfect tool to analyze geographically coded information and
    builds on the general data capabilities of Pandas, which we’ll see more of in
    [Chapter 6](ch06.html#question_matching).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: GeoPandas 是分析地理编码信息的理想工具，它基于 Pandas 的通用数据功能，我们将在第6章中更多地了解到。
