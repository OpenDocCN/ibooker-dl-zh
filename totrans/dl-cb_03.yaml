- en: Chapter 3\. Calculating Text Similarity Using Word Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Before we get started, this is the first chapter with actual code in it. Chances
    are you skipped straight to here, and who would blame you? To follow the recipes
    it really helps though if you have the accompanying code up and running. You can
    easily do this by executing the following commands in a shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can find a more detailed explanation in [“What Do You Need to Know?”](preface01.html#preface_what_do_you_need_to_know).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ll look at word embeddings and how they can help us to calculate
    the similarities between pieces of text. Word embeddings are a powerful technique
    used in natural language processing to represent words as vectors in an *n*-dimensional
    space. The interesting thing about this space is that words that have similar
    meanings will appear close to each other.
  prefs: []
  type: TYPE_NORMAL
- en: The main model we’ll use here is a version of Google’s Word2vec. This is not
    a deep neural model. In fact, it is no more than a big lookup table from word
    to vector and therefore hardly a model at all. The Word2vec embeddings are produced
    as a side effect of training a network to predict a word from context for sentences
    taken from Google News. Moreover, it is possibly the best-known example of an
    embedding, and embeddings are an important concept in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Once you start looking for them, high-dimensional spaces with semantic properties
    start popping up everywhere in deep learning. We can build a movie recommender
    by projecting movies into a high-dimensional space ([Chapter 4](ch04.html#movie_recommender))
    or create a map of handwritten digits using only two dimensions ([Chapter 13](ch13.html#autoencoders)).
    Image recognition networks project images into a space such that similar images
    are near to each other ([Chapter 10](ch10.html#image_search)).
  prefs: []
  type: TYPE_NORMAL
- en: In the current chapter we’ll focus on just word embeddings. We’ll start with
    using a pretrained word embedding model to calculate word similarities, then show
    some interesting Word2vec math. We’ll then explore how to visualize these high-dimensional
    spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll take a look at how we can exploit the semantic properties of word
    embeddings like Word2vec for domain-specific ranking. We’ll treat the words and
    their embeddings as the entities they represent, with some interesting results.
    We’ll start with finding entity classes in Word2vec embeddings—in this case, countries.
    We’ll then show how to rank terms against these countries and how to visualize
    these results on a map.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are a powerful way to map words to vectors and have many uses.
    They are often used as a preprocessing step for text.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two Python notebooks associated with this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 3.1 Using Pretrained Word Embeddings to Find Word Similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to find out whether two words are similar but not equal, for example
    when you’re verifying user input and you don’t want to require the user to exactly
    enter the expected word.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use a pretrained word embedding model. We’ll use `gensim` in this example,
    a useful library in general for topic modeling in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to acquire a pretrained model. There are a number of pretrained
    models available for download on the internet, but we’ll go with the Google News
    one. It has embeddings for 3 million words and was trained on roughly 100 billion
    words taken from the Google News archives. Downloading it will take a while, so
    we’ll cache the file locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the model downloaded, we can load it into memory. The model
    is quite big and this will take around 5 GB of RAM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model has finished loading, we can use it to find similar words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word embeddings associate an *n*-dimensional vector with each word in the vocabulary
    in such a way that similar words are near each other. Finding similar words is
    a mere nearest-neighbor search, for which there are efficient algorithms even
    in high-dimensional spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying things somewhat, the Word2vec embeddings are obtained by training
    a neural network to predict a word from its context. So, we ask the network to
    predict which word it should pick for X in a series of fragments; for example,
    “the cafe served a X that really woke me up.”
  prefs: []
  type: TYPE_NORMAL
- en: This way words that can be inserted into similar patterns will get vectors that
    are close to each other. We don’t care about the actual task, just about the assigned
    weights, which we will get as a side effect of training this network.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this book we’ll see how word embeddings can also be used to feed words
    into a neural network. It is much more feasible to feed a 300-dimensional embedding
    vector into a network than a 3-million-dimensional one that is one-hot encoded.
    Moreover, a network fed with pretrained word embeddings doesn’t have to learn
    the relationships between the words, but can start with the real task at hand
    immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Word2vec Math
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you automatically answer questions of the form “A is to B as C is to
    what?”
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the semantic properties of the Word2vec model. The `gensim` library makes
    this rather straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now apply this to arbitrary words—for example, to find what relates
    to “king” the way “son” relates to “daughter”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use this approach to look up the capitals of selected countries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'or to find the main products of companies (note the # placeholder for any number
    used in these embeddings):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw in the previous step, the vectors associated with the words encode
    the meaning of the words—words that are similar to each other have vectors that
    are close to each other. It turns out that the difference between word vectors
    also encodes the difference between words, so if we take the vector for the word
    “son” and deduct the vector for the word “daughter” we end up with a difference
    that can be interpreted as “going from male to female.” If we add this difference
    to the vector for the word “king” we end up near the vector for the word “queen”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![clarifying diagram](assets/dlcb_03in01.png)'
  prefs: []
  type: TYPE_IMG
- en: The `most_similar` method takes one or more positive words and one or more negative
    words. It looks up the corresponding vectors, then deducts the negative from the
    positive and returns the words that have vectors nearest to the resulting vector.
  prefs: []
  type: TYPE_NORMAL
- en: So in order to answer the question “A is to B as C is to?” we want to deduct
    A from B and then add C, or call `most_similar` with `positive = [B, C]` and `negative
    = [A]`. The example `A_is_to_B_as_C_is_to` adds two small features to this behavior.
    If we request only one example, it will return a single item, rather than a list
    with one item. Similarly, we can return either lists or single items for A, B,
    and C.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to provide lists turned out to be useful in the product example.
    We asked for three products per company, which makes it more important to get
    the vector exactly right than if we only asked for one. By providing “Starbucks”
    and “Apple,” we get a more exact vector for the concept of “is a product of.”
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Visualizing Word Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to get some insight into how word embeddings partition a set of objects.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A 300-dimensional space is hard to browse, but luckily we can use an algorithm
    called *t-distributed stochastic neighbor embedding* (t-SNE) to fold a higher-dimensional
    space into something more comprehensible, like two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we want to look at how three sets of terms are partitioned. We’ll
    pick countries, sports, and drinks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s look up their vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use t-SNE to find the clusters in the 300-dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use matplotlib to show the results in a nice scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatter plot of items](assets/dlcb_03in02.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: t-SNE is a clever algorithm; you give it a set of points in a high-dimensional
    space, and it iteratively tries to find the best projection onto a lower-dimensional
    space (usually a plane) that maintains the distances between the points as well
    as possible. It is therefore very suitable for visualizing higher dimensions like
    (word) embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: For more complex situations, the `perplexity` parameter is something to play
    around with. This variable loosely determines the balance between local accuracy
    and overall accuracy. Setting it to a low value creates small clusters that are
    locally accurate; setting it higher leads to more local distortions, but with
    better overall clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Finding Entity Classes in Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In high-dimensional spaces there are often subspaces that contain only entities
    of one class. How do you find those spaces?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apply a support vector machine (SVM) on a set of examples and counterexamples.
    For example, let’s find the countries in the Word2vec space. We’ll start by loading
    up the model again and exploring things similar to a country, Germany:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As you can see there are a number of countries nearby, but words like “German”
    and the names of German cities also show up in the list. We could try to construct
    a vector that best represents the concept of “country” by adding up the vectors
    of many countries rather than just using Germany, but that only goes so far. The
    concept of country in the embedding space isn’t a point, it is a shape. What we
    need is a real classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Support vector machines have proven effective for classification tasks like
    this. Scikit-learn has an easy-to-deploy solution. The first step is to build
    a training set. For this recipe getting positive examples is not hard since there
    are only so many countries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Having more positive examples is of course better, but for this example using
    40–50 will give us a good idea of how the solution works.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need some negative examples. We sample these directly from the general
    vocabulary of the Word2vec model. We could get unlucky and draw a country and
    put it in the negative examples, but given the fact that we have 3 million words
    in the model and there are less than 200 countries in the world, we’d have to
    be very unlucky indeed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll create a labeled training set based on the positive and negative
    examples. We’ll use `1` as the label for something being a country, and `0` for
    it not being a country. We’ll follow the convention of storing the training data
    in a variable `X` and the labels in a variable `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s train the model. We’ll set aside a fraction of the data to evaluate how
    we are doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The training should happen almost instantaneously even on a not very powerful
    computer since our dataset is relatively small. We can have a peek at how we are
    doing by looking at how many times the model has the right prediction for the
    bits of the eval set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The results you get will depend a bit on the positive countries selected and
    which negative samples you happened to draw. I mostly get a list of countries
    that it missed—typically because the country name also means something else, like
    Jordan, but there are also some genuine misses in there. The precision comes out
    at 99.9% or so.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now run the classifier over all of the words to extract the countries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The results are pretty good, though not perfect. The word “countries” itself,
    for example, is classified as a country, as are entities like continents or US
    states.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Support vector machines are effective tools when it comes to finding classes
    within a higher-dimensional space like word embeddings. They work by trying to
    find hyperplanes that separate the positive examples from the negative examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Countries in Word2vec are all somewhat near to each other since they share
    a semantic aspect. SVMs help us find the cloud of countries and come up with boundaries.
    The following diagram visualizes this in two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![clever diagram](assets/dlcb_03in03.png)'
  prefs: []
  type: TYPE_IMG
- en: SVMs can be used for all kinds of ad hoc classifiers in machine learning since
    they are effective even if the number of dimensions is greater than the number
    of samples, like in this case. The 300 dimensions could allow the model to overfit
    the data, but because the SVM tries to find a simple model to fit the data, we
    can still generalize from a dataset as small as a few dozen examples.
  prefs: []
  type: TYPE_NORMAL
- en: The results achieved are pretty good, though it is worth noting that in a situation
    where we have 3 million negative examples, 99.7% precision would still give us
    9,000 false positives, drowning out the actual countries.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Calculating Semantic Distances Inside a Class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you find the most relevant items from a class for a given criterion?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given a class, for example *countries*, we can rank the members of that class
    against a criterion, by looking at the relative distances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now, as before, extract the vectors for the countries into a `numpy`
    array that lines up with the countries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'A quick sanity check to see which countries are most like Canada:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The Caribbean countries are somewhat surprising and a lot of the news about
    Canada must be related to hockey, given the appearance of Slovakia and Finland
    in the list, but otherwise it doesn’t look unreasonable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s switch gears and do some ranking for an arbitrary term over the set of
    countries. For each country we’ll calculate the distance between the name of the
    country and the term we want to rank against. Countries that are “closer” to the
    term are more relevant for the term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Since the Word2vec model we are using was trained on Google News, the ranker
    will return countries that are mostly known for the given term in the recent news.
    India might be more often mentioned for cricket, but as long as it is also covered
    for other things, Sri Lanka can still win.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In spaces where we have members of different classes projected into the same
    dimensions, we can use the cross-class distances as a measure of affinity. Word2vec
    doesn’t quite represent a conceptual space (the word “Jordan” can refer to the
    river, the country, or a person), but it is good enough to nicely rank countries
    on relevance for various concepts.
  prefs: []
  type: TYPE_NORMAL
- en: A similar approach is often taken when building recommender systems. For the
    Netflix challenge, for example, a popular strategy was to use user ratings for
    movies as a way to project users and movies into a shared space. Movies that are
    close to a user are then expected to be rated highly by that user.
  prefs: []
  type: TYPE_NORMAL
- en: In situations where we have two spaces that are not the same, we can still use
    this trick if we can calculate the projection matrix to go from one space to the
    other. This is possible if we have enough candidates whose positions we know in
    both spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Visualizing Country Data on a Map
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you visalize country rankings from an experiment on a map?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GeoPandas is a perfect tool to visualize numerical data on top of a map.
  prefs: []
  type: TYPE_NORMAL
- en: 'This nifty library combines the power of Pandas with geographical primitives
    and comes preloaded with a few maps. Let’s load up the world:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us some basic information about a set of countries. We can add a
    column to the `world` object based on our `rank_countries` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This draws, for example, the map for coffee quite nicely, highlighting the coffee
    consuming countries and the coffee producing countries.
  prefs: []
  type: TYPE_NORMAL
- en: '![World map for coffee](assets/dlcb_03in04.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizing data is an important technique for machine learning. Being able
    to look at the data, whether it is the input or the result of some algorithm,
    allows us to quickly spot anomalies. Do people in Greenland really drink that
    much coffee? Or are we seeing an artifact because of “Greenlandic coffee” (a variation
    on Irish coffee)? And those countries in the middle of Africa—do they really neither
    drink nor produce coffee? Or do we just have no data on them because they don’t
    occur in our embeddings?
  prefs: []
  type: TYPE_NORMAL
- en: GeoPandas is the perfect tool to analyze geographically coded information and
    builds on the general data capabilities of Pandas, which we’ll see more of in
    [Chapter 6](ch06.html#question_matching).
  prefs: []
  type: TYPE_NORMAL
