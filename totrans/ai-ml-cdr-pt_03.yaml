- en: Chapter 2\. Introduction to Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章\. 计算机视觉简介
- en: '[Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566) introduced
    the basics of how machine learning works. You saw how to get started with programming
    using neural networks to match data to labels, and from there, you saw how to
    infer the rules that can be used to distinguish items.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一章](ch01.html#ch01_introduction_to_pytorch_1748548870019566)介绍了机器学习工作的基础知识。您了解了如何使用神经网络编程入门，将数据与标签匹配，并从中看到如何推断出区分物品的规则。'
- en: In this chapter, we’ll consider the next logical step, which is to apply these
    concepts to computer vision. In this process, a model learns how to recognize
    content in pictures so it can “see” what’s in them. You’ll work with a popular
    dataset of clothing items and build a model that can differentiate between them
    and thus “see” the difference between different types of clothing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将考虑下一个逻辑步骤，即将这些概念应用于计算机视觉。在这个过程中，模型学习如何在图片中识别内容，以便它能够“看到”其中的内容。您将使用一个流行的衣物数据集，并构建一个能够区分它们并“看到”不同类型衣物之间差异的模型。
- en: How Computer Vision Works
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉是如何工作的
- en: '*Computer vision* is the ability of a computer to recognize items beyond just
    storing their pixels. For example, consider items of clothing that might look
    like those in [Figure 2-1](#ch02_figure_1_1748548889066336). They’re very complex,
    with lots of different varieties of the same item. Take a look at the two shoes—they’re
    very different, but they’re still shoes!'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*计算机视觉*是计算机识别物品的能力，而不仅仅是存储它们的像素。例如，考虑一下可能像[图2-1](#ch02_figure_1_1748548889066336)中的物品。它们非常复杂，有很多不同种类的相同物品。看看这两双鞋——它们非常不同，但它们仍然是鞋！'
- en: '![](assets/aiml_0201.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0201.png)'
- en: Figure 2-1\. Clothing examples
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 衣物示例
- en: There are a number of different recognizable clothing items here. You understand
    the difference between a shirt, a coat, and a dress, and you fundamentally know
    what each of these items are—but how would you explain all that to somebody who
    has never seen clothing? How about a shoe? There are two shoes in this image,
    but given the major differences between them, how would you explain to someone
    what makes them both shoes? This is another area where the rules-based programming
    we spoke about in [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566)
    can fall apart. Sometimes, it’s just unfeasible to describe something with rules.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有许多可识别的衣物。您理解衬衫、外套和连衣裙之间的区别，并且基本知道每种物品是什么——但您如何向从未见过衣物的人解释这一切？鞋子呢？这张图片中有两双鞋，但鉴于它们之间的主要差异，您如何向某人解释使它们成为鞋子的共同点？这是另一个我们曾在[第一章](ch01.html#ch01_introduction_to_pytorch_1748548870019566)中提到的基于规则的编程可能失效的领域。有时，用规则描述某物是不切实际的。
- en: Of course, computer vision is no exception to this issue. But consider how you
    learned to recognize all these items—by seeing lots of different examples and
    gaining experience with how they’re used. Can a computer learn the same way? The
    answer is yes, but with limitations. Throughout the rest of this chapter, we’ll
    take a look at an example of how to teach a computer to recognize items of clothing
    using a well-known dataset called Fashion MNIST.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，计算机视觉也不例外。但考虑一下你是如何学会识别所有这些物品的——通过看到很多不同的例子，并积累它们使用的经验。计算机能否以同样的方式学习？答案是肯定的，但有一定的限制。在本章的剩余部分，我们将通过一个例子来了解如何使用一个名为Fashion
    MNIST的知名数据集来教计算机识别衣物。
- en: The Fashion MNIST Database
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: The Fashion MNIST Database
- en: One of the foundational datasets for learning and benchmarking algorithms is
    the Modified National Institute of Standards and Technology (MNIST) database,
    which was created by Yann LeCun, Corinna Cortes, and Christopher Burges. This
    dataset consists of images of 70,000 handwritten digits from 0 to 9, and the images
    are 28 × 28 grayscale.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 学习和基准测试算法的基础数据集之一是修改后的国家标准与技术研究院（MNIST）数据库，该数据库由Yann LeCun、Corinna Cortes和Christopher
    Burges创建。这个数据集包含从0到9的70,000个手写数字图像，图像为28 × 28灰度图。
- en: '[Fashion MNIST](https://oreil.ly/f-mnist) is designed to be a drop-in replacement
    for MNIST that has the same number of records, the same image dimensions, and
    the same number of classes. Rather than images of the digits 0 through 9, Fashion
    MNIST contains images of 10 different types of clothing.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[Fashion MNIST](https://oreil.ly/f-mnist)被设计成MNIST的一个直接替代品，它具有相同的记录数、相同的图像尺寸和相同的类别数。与0到9的数字图像不同，Fashion
    MNIST包含10种不同类型衣物的图像。'
- en: You can see an example of the dataset contents in [Figure 2-2](#ch02_figure_2_1748548889066375),
    in which three lines are dedicated to each clothing item type.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0202.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Exploring the Fashion MNIST dataset
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fashion MNIST has a nice variety of clothing, including shirts, trousers, dresses,
    and lots of types of shoes! Also, as you may notice, it’s monochrome, so each
    picture consists of a certain number of pixels with values between 0 and 255\.
    This makes the dataset simpler to manage.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: You can see a close-up of a particular image from the dataset in [Figure 2-3](#ch02_figure_3_1748548889066397).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0203.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Close-up of an image in the Fashion MNIST dataset
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Like any image, this one is a rectangular grid of pixels. In this case, the
    grid size is 28 × 28, and each pixel is a value between 0 and 255, so it is represented
    by a square in grayscale. To make it easier to see, I have expanded it so that
    it looks pixelated.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now take a look at how you can use these pixel values with the functions
    we saw previously.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Neurons for Vision
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566), you
    saw a very simple scenario in which a machine was given a set of *x* and *y* values
    and it learned that the relationship between them was *y* = 2*x* – 1\. This was
    done using a very simple neural network with one layer and one neuron. If you
    were to draw that visually, it might look like [Figure 2-4](#ch02_figure_4_1748548889066415).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0204.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. A single neuron learning a linear relationship
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each of our images is a set of 784 values (28 × 28) between 0 and 255\. They
    can be our *x*. We also know that we have 10 different types of images in our
    dataset, so let’s consider them to be our *y*. Now, we want to learn what the
    function looks like in which *y* is a function of *x*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Given that we have 784 *x* values per image and our *y* is going to be between
    0 and 9, a simple equation like *y* = *mx* + *c* isn’t going to be enough to solve
    the problem. That’s because there’s a large variety of possible values and the
    equation can only plot values on a line.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: But what we *can* do is have several neurons working together. Each neuron will
    learn *parameters*, and when we have a combined function of all of these parameters
    working together, we can see whether we can match that pattern to our desired
    answer (see [Figure 2-5](#ch02_figure_5_1748548889066432)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0205.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. Extending our pattern for a more complex example
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The gray boxes at the top of this diagram can be considered the pixels in the
    image, which are our *X* values. When we train the neural network, we load the
    pixels into a layer of neurons—[Figure 2-5](#ch02_figure_5_1748548889066432) shows
    them being loaded into the first neuron, but the values are loaded into just each
    of them. Also, consider each neuron’s weight and bias (*w* and *b*) to be randomly
    initialized. Then, when we sum up the values of the output of each neuron, we’re
    going to get a value. We’ll do this for *every* neuron in the output layer, so
    neuron 0 will contain the value of the probability that the pixels will add up
    to label 0, neuron 1 will contain the value of the probability that the pixels
    will add up to label 1, etc.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此图顶部的灰色方框可以被认为是图像中的像素，它们是我们的*X*值。当我们训练神经网络时，我们将像素加载到一个神经元的层中——[图2-5](#ch02_figure_5_1748548889066432)显示了它们被加载到第一个神经元中，但值只加载到每个神经元中。此外，考虑每个神经元的权重和偏差（*w*和*b*）是随机初始化的。然后，当我们对每个神经元的输出值求和时，我们将得到一个值。我们将对输出层中的每个神经元都这样做，因此神经元0将包含像素加起来等于标签0的概率值，神经元1将包含像素加起来等于标签1的概率值，等等。
- en: Over time, we want to match that value to the desired output―which, for this
    image, is the number 9, which is also the label for the ankle boot that was shown
    in [Figure 2-3](#ch02_figure_3_1748548889066397). So, in other words, this neuron
    should have the largest value of all of the output neurons.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们希望将这个值匹配到期望的输出——对于这张图片来说，是数字9，这也是[图2-3](#ch02_figure_3_1748548889066397)中展示的踝靴的标签。换句话说，这个神经元应该拥有所有输出神经元中最大的值。
- en: Given that there are 10 labels, a random initialization should get the right
    answer about 10% of the time. From that, the loss function and optimizer can do
    their job epoch by epoch to tweak the internal parameters of each neuron to improve
    on that 10%. And thus, over time, the computer will learn to “see” what makes
    a shoe a shoe or a dress a dress. You’ll see this process of improvement when
    you run the code and your neural network effectively learns to distinguish the
    different items.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有10个标签，随机初始化应该有大约10%的时间能够得到正确答案。基于这一点，损失函数和优化器可以逐个epoch调整每个神经元的内部参数，以改善这10%的准确性。因此，随着时间的推移，计算机将学会“看到”什么使鞋子成为鞋子，连衣裙成为连衣裙。当你运行代码并看到你的神经网络有效地学会区分不同的物品时，你会看到这个过程不断改进。
- en: Designing the Neural Network
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计神经网络
- en: 'Let’s take the example we just walked through and explore what it looks like
    in code. First, we’ll look at the design of the neural network that was shown
    in [Figure 2-5](#ch02_figure_5_1748548889066432):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以我们刚刚讨论的例子为例，探索它在代码中的样子。首先，我们将查看[图2-5](#ch02_figure_5_1748548889066432)中展示的神经网络设计：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you remember, in [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566)
    we had a `Sequential` model to specify that we had many layers. In that case,
    we had only one layer, but now we’re using it to define multiple layers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得，在[第1章](ch01.html#ch01_introduction_to_pytorch_1748548870019566)中，我们有一个`Sequential`模型来指定我们有很多层。在那个例子中，我们只有一个层，但现在我们使用它来定义多个层。
- en: 'The first layer, a `Linear`, is a layer of neurons that learn a linear relationship
    between their inputs and their outputs. As before, when using a `Linear`, you
    give two parameters: the input shape and the output shape. Conveniently, the output
    shape is effectively the number of neurons you want in this layer, and we’re specifying
    that we want 128 of them. The *input* shape is defined as (28 × 28), which is
    the size of the data coming into the network, and as you saw earlier, this is
    the dimension of a Fashion MNIST image.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层，一个`Linear`层，是一层学习其输入和输出之间线性关系的神经元。正如之前一样，当使用`Linear`时，你给出两个参数：输入形状和输出形状。方便的是，输出形状实际上是这个层中你想要的神经元数量，我们指定我们想要128个。*输入*形状定义为(28
    × 28)，这是进入网络的数据大小，正如你之前看到的，这是Fashion MNIST图像的维度。
- en: The input is shown as the middle layer in [Figure 2-5](#ch02_figure_5_1748548889066432),
    and you’ll often hear such layers described as *hidden layers*. The term *hidden*
    just means that there’s no direct interface to that layer. This takes a little
    bit of getting used to—the middle layer is the first layer that you *define*,
    and in a diagram like [Figure 2-5](#ch02_figure_5_1748548889066432), you can see
    that it’s in the middle of the diagram. This is because we also drew the data
    “coming in” to this layer. One other thing to note is that image data from datasets
    like Fashion MNIST is usually rectangular in shape, but a layer doesn’t recognize
    that, so it will need to be “flattened” into a 1-D array, as shown across the
    top of [Figure 2-5](#ch02_figure_5_1748548889066432). You’ll see the code for
    that in a moment.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 输入在 [图 2-5](#ch02_figure_5_1748548889066432) 中显示为中间层，你经常会听到这样的层被描述为 *隐藏层*。术语
    *隐藏* 只是意味着没有直接接口到该层。这需要一点时间来适应——中间层是你 *定义* 的第一个层，在一个像 [图 2-5](#ch02_figure_5_1748548889066432)
    这样的图中，你可以看到它在图的中间。这是因为我们还绘制了数据“进入”这个层。另一件需要注意的事情是，来自 Fashion MNIST 等数据集的图像数据通常是矩形的，但层不会识别这一点，因此它需要被“展平”成一个
    1-D 数组，如 [图 2-5](#ch02_figure_5_1748548889066432) 的顶部所示。你很快就会看到相关的代码。
- en: With this first `Linear`, we’re asking for 128 neurons to have their internal
    parameters randomly initialized. Often, the question I’ll get asked at this point
    is “Why 128?” This is entirely arbitrary—there’s no fixed rule for the number
    of neurons to use. As you design the layers, you want to pick the appropriate
    number of values to enable your model to actually learn. More neurons means it
    will run more slowly, as it has to learn more parameters. More neurons could also
    lead to a network that is great at recognizing the training data but not so good
    at recognizing data that it hasn’t previously seen. (This is known as *overfitting*,
    and we’ll discuss it later in this chapter). On the other hand, fewer neurons
    means that the model might not have sufficient parameters to learn.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个第一个 `Linear`，我们要求有 128 个神经元，它们的内部参数随机初始化。通常，在这个时候我会被问到“为什么是 128？” 这完全是随机的——没有使用神经元数量的固定规则。当你设计层时，你想要选择适当数量的值，以便你的模型能够真正学习。更多的神经元意味着它将运行得更慢，因为它必须学习更多的参数。更多的神经元也可能导致一个网络在识别训练数据方面做得很好，但在识别它之前没有见过的数据方面做得不好。（这被称为
    *过拟合*，我们将在本章后面讨论）。另一方面，更少的神经元意味着模型可能没有足够的参数来学习。
- en: You will need to explore this trade-off between speed of learning and accuracy
    of learning and do some experimentation over time to pick the right values. This
    process is typically called *hyperparameter tuning*. In ML, a *hyperparameter*
    is a value that is used to control the training, as opposed to the internal values
    of the neurons that get trained/learned, which are referred to as *parameters*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你将需要探索学习速度和学习精度之间的权衡，并在一段时间内进行实验以选择正确的值。这个过程通常被称为 *超参数调整*。在机器学习中，*超参数* 是用于控制训练的值，而不是经过训练/学习的神经元的内部值，这些内部值被称为
    *参数*。
- en: When you’re defining a neural network with PyTorch and using the `Sequential`,
    you don’t just define the layers of the network and what types of neurons they
    may use. You can also define functions that execute on the data while it flows
    between the neural network layers. These are typically called *activation functions*,
    and an activation function is the next thing you see specified in the code as
    `nn.ReLU()`. An activation function is code that will execute on each neuron in
    the layer. PyTorch supports a number of activation functions out of the box, and
    a very common one in middle layers is `ReLU`, which stands for *rectified linear
    unit*. It’s a simple function that returns a value only if it’s greater than 0\.
    In this case, we don’t want negative values being passed to the next layer to
    potentially impact the summing function, so instead of writing a lot of `if-then`
    code, we can simply activate the layer with `ReLU`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 PyTorch 定义神经网络并使用 `Sequential` 时，你不仅定义了网络的层以及它们可能使用的神经元类型。你还可以定义在数据在神经网络层之间流动时执行的功能。这些通常被称为
    *激活函数*，激活函数是你在代码中看到的下一个指定为 `nn.ReLU()` 的东西。激活函数是将在层中的每个神经元上执行代码。PyTorch 支持许多开箱即用的激活函数，其中在中层中非常常见的一个是
    `ReLU`，它代表 *修正线性单元*。这是一个简单的函数，只有当它的值大于 0 时才返回值。在这种情况下，我们不希望负值传递到下一层，从而可能影响求和函数，因此，我们不必编写大量的
    `if-then` 代码，而可以简单地使用 `ReLU` 激活层。
- en: 'Finally, there’s another `Linear` layer, which will be the *output layer*.
    If you look at the defined shape of (128, 10) and think of it through that “input
    size, output size” framework, you’ll see that it has 128 “inputs” (i.e., the number
    of neurons in the layer above) and 10 “outputs.” What are these 10? Recall that
    Fashion MNIST has 10 classes of clothing. Each of these neurons is effectively
    assigned one class, and it will end up with a probability that the input pixels
    match that class, so our job is to determine which one has the highest value.
    You might wonder how these assignments happen: where is the code that says one
    neuron is for a shoe and another is for a shirt? To answer that question, recall
    the *y* = 2*x* ‒ 1 example in [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566),
    where we had a set of input data and a set of known, correct answers that is sometimes
    called the *ground truth*. Fashion MNIST will work in the same way. When training
    the network, we provide the input images *and* their known answers as a set of
    what we want the output neurons to look like. Thus, the network will “learn” that
    when it sees a shoe, the output neurons that don’t represent that shoe should
    have a zero value and the ones that do should have a “1” value.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有一个`Linear`层，它将是*输出层*。如果你看看定义的形状（128, 10），并通过“输入大小，输出大小”框架来思考，你会发现它有128个“输入”（即上一层中的神经元数量）和10个“输出”。这10个是什么？回想一下，Fashion
    MNIST有10种服装类别。这些神经元中的每一个实际上被分配了一个类别，并且它最终会得到一个概率，即输入像素与该类别的匹配概率，因此我们的任务是确定哪一个具有最高的值。你可能想知道这些分配是如何发生的：代码在哪里说一个神经元代表鞋子，另一个代表衬衫？为了回答这个问题，回想一下[第1章](ch01.html#ch01_introduction_to_pytorch_1748548870019566)中的*y*
    = 2*x* ‒ 1示例，在那里我们有一组输入数据和一组已知的正确答案，有时被称为*地面真相*。Fashion MNIST将以相同的方式工作。在训练网络时，我们提供输入图像及其已知答案作为一组我们希望输出神经元看起来像的东西。因此，网络将“学习”到当它看到鞋子时，不表示该鞋子的输出神经元应该为零值，而表示该鞋子的神经元应该有“1”值。
- en: We *could* also loop through the output neurons to find the highest value, but
    the `LogSoftmax` activation function does that for us.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们*也可以*遍历输出神经元以找到最大值，但`LogSoftmax`激活函数已经为我们做了这件事。
- en: So now, when we train our neural network, we have two goals. We want to be able
    to feed in a 28 × 28–pixel array, and we want the neurons in the middle layer
    to have weights and biases (*w* and *B* values) that, when combined, will match
    those pixels to one of the 10 output values.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在当我们训练我们的神经网络时，我们有两个目标。我们希望能够输入一个28 × 28像素的数组，并且我们希望中间层的神经元具有权重和偏差（*w* 和
    *B* 值），当它们结合在一起时，能够匹配这些像素到10个输出值中的某一个。
- en: The Complete Code
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整代码
- en: Now that we’ve explored the architecture of the neural network, let’s look at
    the complete code for training a model with the Fashion MNIST data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了神经网络的架构，让我们来看看使用Fashion MNIST数据训练模型的完整代码。
- en: 'Here’s the complete code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是完整的代码：
- en: '[PRE1] `# Training process` `epochs` `=` `5` `for` `t` `in` `range``(``epochs``):`     `print``(``f``"Epoch`
    `{``t``+``1``}``\n``-------------------------------"``)`     `train``(``train_loader``,`
    `model``,` `loss_function``,` `optimizer``)` `print``(``"Done!"``)` [PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1] `# 训练过程` `epochs` `=` `5` `for` `t` `in` `range``(``epochs``):`     `print``(``f``"Epoch`
    `{``t``+``1``}``\n``-------------------------------"``)`     `train``(``train_loader``,`
    `model``,` `loss_function``,` `optimizer``)` `print``(``"Done!"``)` [PRE2]'
- en: '[PRE3]`Let’s walk through this piece by piece. First, let’s consider where
    the data comes from. In the torchvision library, there’s a datasets collection,
    and we can load Fashion MNIST from that, addressed as follows:    [PRE4]    So,
    in our first block of code, you’ll see these:    [PRE5]    Now, you might wonder
    why we’re using *two* datasets. It’s simple: one is for training, and one is for
    testing. The idea here is also simple: if you train a neural network on a set
    of data, it can become an expert on *that* set of data, but it may not be effective
    at understanding or classifying *other* data that it previously has not seen.
    In the case of Fashion MNIST, it might become excellent at understanding the difference
    between a subset of shoes and shirts, but it will do poorly when new data is presented
    to it. So, it’s good practice to always hold back a little of your data and *not*
    train the neural network with it. In this case, Fashion MNIST has 70,000 items
    of data, but only 60,000 of them are used to train the network and the other 10,000
    are used to test it. If you look at the preceding code carefully, you’ll see that
    the difference between the two lines is the `train=` parameter. For the first
    one, the training set the parameter is set to True. For the other, it’s set to
    False.    You’ll also see the `transform` parameter in the datasets. It specifies
    a transformation to apply to the data, which was defined like this:    [PRE6]    Neural
    networks typically work with *normalized* values (i.e., those between 0 and 1).
    However, the pixels in our image are in the range of 0–255, and the values indicate
    their color depth, with 0 being black, 255 being white, and everything in between
    being shades of gray. To prepare the data for the neural network, we should map
    these shades to values between 0 and 1\. The preceding code will automatically
    do that for you in PyTorch, so applying this `transform` parameter as you’re loading
    the code will then map the pixel values from the [0, 255] integer range to a [0,
    1] floating-point range and load them into an array that’s suitable for the neural
    network (aka a Tensor).    Our job will be to fit the training images to the training
    labels in a manner that’s similar to how we fit *y* to *x* in [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566).    The
    math for [why normalized data is better for training neural networks](https://oreil.ly/6d_Po)
    is beyond the scope of this book, but bear in mind that when you’re training a
    neural network in PyTorch, normalization will improve performance. Often, your
    network will not learn and will have massive errors when dealing with nonnormalized
    data. You’ll recall that the *y* = 2*x* – 1 example from [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566)
    didn’t require the data to be normalized because it was very simple, but for fun,
    try training it with different values of *x* and *y* where *x* is much larger—and
    you’ll see it quickly fail!    Next, we define the neural network that makes up
    our model, as discussed earlier, but we’ll flesh it out with a bit more detail—including
    the flattening layers and how we want the “forward” pass to work in the model.    Here’s
    the code:    [PRE7]    Some key things to note here are that the `FashionMNISTModel`
    class subclasses `nn.Module`, which gives you the ability to override its `forward`
    method. We use this method when data is passing forward through the network. Remember
    back in [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566) when
    we saw the `loss.backward()` call that did backpropagation and changed the parameters
    of the network? You’ll frequently encounter that same pattern when training models
    with PyTorch. You’ll define functions to execute as the data moves *forward* through
    the network, and then you’ll define others to execute as the gradients that we
    calculate from the loss move *backward* through the network.    So, if we look
    at the `init` for the class, we define two methods: `flatten`, which is set to
    `nn.FLatten()` (a built-in function to flatten the 2D image to 1D), and `lin⁠ear​_relu_stack`,
    which is set to the sequence of layers and operations (often abbreviated to *ops*)
    that define the behavior of the network.    In `forward`, we then simply define
    how these work. First, we flatten our data, `x`, by calling `self.flatten`, and
    then the results will be passed into `linear_relu_stack` to get the results. The
    results are called *logits*, which are log probabilities (as defined by `LogSoftmax`)
    that indicate the confidence the model has that each class is the correct classification.    To
    learn from our data, we need a loss function to calculate how good or bad our
    current “guess” is, and we also need an optimizer to figure out the next set of
    parameters for an improved guess.    Here’s an example of how to define both:    [PRE8]    First,
    let’s look at the loss function. It’s defined as `nn.NLLLoss(),` which stands
    for “Negative Log Likelihood Loss.” Don’t worry—nobody expects you to understand
    what that means at this point! Ultimately, as you work through learning how to
    do ML, you’ll learn about different loss functions, and you’ll experiment with
    which ones work well in particular scenarios. In this case, given that the output
    logits are log probabilities, I chose this loss function because it works particularly
    well for this scenario. As mentioned, over time, you’ll learn a lot more about
    the library of loss functions, and you can choose the best ones for your scenario.
    But for now, just go with the flow and use this one!    For the optimizer, I’ve
    opted to use the `Adam` optimization algorithm. It’s similar to the stochastic
    gradient descent that we used for the *y* = 2*x* – 1 model in [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566),
    but it’s generally faster and more accurate. As with the loss function, you’ll
    learn more about optimization algorithms over time, and you’ll be able to choose
    from the menu of optimizers that fit your scenario best. One important thing here
    is to note that I’ve passed in `model.parameters()` as a parameter to this. This
    parameter passes all the trainable parameters in the model to the optimizer so
    that it can adjust them to help minimize the loss calculated by the loss function.    Now,
    let’s get down to the specifics and explore what the code we use for training
    the network looks like:    [PRE9]    While some of this will look familiar because
    it builds on the simple neural network from [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566),
    there are a few new concepts here, given that we’re using much more data. First,
    you’ll see that we get the `size` of the dataset. We simply use this to report
    on progress, as shown in the very last line.    Then, we call `model.train` to
    explicitly set the model into training mode. PyTorch has optimizations that occur
    during training that are beyond the scope of this chapter. (To take advantage
    of them, you’ll switch the model between training and inference modes.) Note that
    this is more a property of the model than a method, but the method syntax is there.
    Sorry if it’s a little confusing!    Next up is this interesting line:    [PRE10]    Let’s
    explore this in a little more detail. We made the Fashion MNIST dataset available
    to our code by using a data loader. There are 60,000 records available for training,
    each of which is 784 pixels. That’s a lot of data, and you don’t necessarily need
    all of it in memory at once. The idea of a `batch` is to take a chunk of that
    data—which, by default, is 64 items—and work with it. Enumerating the data loader
    gives us that, so we’ll train with 938 batches, 937 of 64, and the last one of
    32 because you can’t evenly divide 60,000 by 64!    Now, for each batch, we’ll
    go through the same loop that we saw for the previous example. We’ll get the predictions
    from the model, calculate the loss, backpropagate the gradients from the loss
    function, and optimize with new parameters.    We’ll also use the term *epoch*
    for a training cycle with *all* of the data (i.e., every batch). We can then output
    the status of the training every one hundred batches so as not to overload the
    output console!    So, to train the network for five epochs, we can use code like
    this:    [PRE11]    This will simply call the train function we specified five
    times—putting the network through the training loop by calculating the predictions,
    figuring out the loss, optimizing the parameters, and repeating five times.[PRE12]``  [PRE13][PRE14]``py[PRE15]
    Epoch 5 ------------------------------- loss: 0.429329  [    0/60000] loss: 0.348756  [
    6400/60000] loss: 0.237481  [12800/60000] loss: 0.336960  [19200/60000] loss:
    0.435592  [25600/60000] loss: 0.272769  [32000/60000] loss: 0.362881  [38400/60000]
    loss: 0.202799  [44800/60000] loss: 0.354268  [51200/60000] loss: 0.205381  [57600/60000]
    Done! [PRE16] # Function to test the model def test(dataloader, model):     size
    = len(dataloader.dataset)     num_batches = len(dataloader)     model.eval()  #
    Set the model to evaluation mode     test_loss, correct = 0, 0     with torch.no_grad():         for
    X, y in dataloader:             pred = model(X)             test_loss += loss_function(pred,
    y).item()             correct += (pred.argmax(1) ==                          y).type(torch.float).sum().item()     test_loss
    /= num_batches     correct /= size     print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%,              `Avg`
    `loss``:` `{``test_loss``:``>``8``f``}` \`n``")` [PRE17] [PRE18][PRE19][PRE20]
    correct += (pred.argmax(1) ==  y).type(torch.float).sum().item() [PRE21] Test
    Error:  Accuracy: 86.9%, Avg loss: 0.366243 [PRE22] # Function to calculate accuracy
    def get_accuracy(pred, labels):     _, predictions = torch.max(pred, 1)     correct
    = (predictions == labels).float().sum()     accuracy = correct / labels.shape[0]     return
    accuracy [PRE23] for batch, (X, y) in enumerate(dataloader):     # Compute prediction
    and loss     pred = model(X)     loss = loss_fn(pred, y)     accuracy = get_accuracy(pred,
    y)       # Backpropagation [PRE24] if batch % 100 == 0:     current = batch *
    len(X)     avg_loss = total_loss / (batch + 1)     avg_accuracy = total_accuracy
    / (batch + 1) * 100     print(f"Batch {batch}, Loss: {avg_loss:>7f},              `Accuracy``:`
    `{``avg_accuracy``:``>``0.2``f``}``%`                        `[{``current``:``>``5``d``}``/``{``size``:``>``5``d``}]``")`
    [PRE25]`` [PRE26] Epoch 5 ------------------------------- Batch 0, Loss: 0.177518,
    Accuracy: 95.31% [    0/60000] Batch 100, Loss: 0.304973, Accuracy: 88.89% [ 6400/60000]
    Batch 200, Loss: 0.311628, Accuracy: 88.51% [12800/60000] Batch 300, Loss: 0.307373,
    Accuracy: 88.63% [19200/60000] Batch 400, Loss: 0.309722, Accuracy: 88.67% [25600/60000]
    Batch 500, Loss: 0.310240, Accuracy: 88.60% [32000/60000] Batch 600, Loss: 0.306988,
    Accuracy: 88.70% [38400/60000] Batch 700, Loss: 0.308556, Accuracy: 88.64% [44800/60000]
    Batch 800, Loss: 0.309518, Accuracy: 88.67% [51200/60000] Batch 900, Loss: 0.311487,
    Accuracy: 88.59% [57600/60000] Done! [PRE27]` [PRE28][PRE29][PRE30][PRE31][PRE32][PRE33]  import
    matplotlib.pyplot as plt   def predict_single_image(image, label, model):     #
    Set the model to evaluation mode     model.eval()   # Unsqueeze image as the model
    expects a batch dimension     image = image.unsqueeze(0)       with torch.no_grad():         prediction
    = model(image)         print(prediction)         predicted_label = prediction.argmax(1).item()       #
    Display the image and predictions     plt.imshow(image.squeeze(), cmap=''gray'')     plt.title(f''Predicted:
    {predicted_label}, Actual: {label}'')     plt.show()       return predicted_label   #
    Choose an image from the test set image, label = test_dataset[0]  # Change index
    to test different images   # Predict the class for the chosen image predicted_label
    = predict_single_image(image, label, model) print(f"The model predicted {predicted_label},
    and the actual label is {label}.") [PRE34]     with torch.no_grad():         prediction
    = model(image)         print(prediction)         predicted_label = prediction.argmax(1).item()
    [PRE35] tensor([[–12.4290, –16.0639, –14.3148, –16.2861, –13.1672,  –4.5377, –13.6284,           –1.3124,  –8.9946,  –0.3285]])
    [PRE36] Epoch 50 ------------------------------- Batch 0, Loss: 0.077159, Accuracy:
    96.88% [    0/60000] Batch 100, Loss: 0.094825, Accuracy: 96.57% [ 6400/60000]
    Batch 200, Loss: 0.093598, Accuracy: 96.67% [12800/60000] Batch 300, Loss: 0.095906,
    Accuracy: 96.54% [19200/60000] Batch 400, Loss: 0.096683, Accuracy: 96.48% [25600/60000]
    Batch 500, Loss: 0.101872, Accuracy: 96.31% [32000/60000] Batch 600, Loss: 0.103130,
    Accuracy: 96.22% [38400/60000] Batch 700, Loss: 0.103901, Accuracy: 96.17% [44800/60000]
    Batch 800, Loss: 0.104216, Accuracy: 96.15% [51200/60000] Batch 900, Loss: 0.104010,
    Accuracy: 96.15% [57600/60000] Done! [PRE37] Test Error:  Accuracy: 89.2%, Avg
    loss: 0.433885 [PRE38] if batch % 100 == 0:     current = batch * len(X)     avg_loss
    = total_loss / (batch + 1)     avg_accuracy = total_accuracy / (batch + 1) * 100     print(f"Batch
    {batch}, Loss: {avg_loss:>7f},             `Accuracy``:` `{``avg_accuracy``:``>``0.2``f``}``%`
    `[{``current``:``>``5``d``}``/``{``size``:``>``5``d``}]``")` [PRE39] [PRE40]``
    [PRE41] Epoch 36 ------------------------------- Batch 0, Loss: 0.098307, Accuracy:
    96.88% [    0/60000] Batch 100, Loss: 0.119195, Accuracy: 95.45% [ 6400/60000]
    Batch 200, Loss: 0.127049, Accuracy: 95.20% [12800/60000] Batch 300, Loss: 0.126001,
    Accuracy: 95.34% [19200/60000] Batch 400, Loss: 0.127823, Accuracy: 95.25% [25600/60000]
    Batch 500, Loss: 0.131262, Accuracy: 95.11% [32000/60000] Batch 600, Loss: 0.135573,
    Accuracy: 94.95% [38400/60000] Batch 700, Loss: 0.135920, Accuracy: 94.95% [44800/60000]
    Batch 800, Loss: 0.135125, Accuracy: 94.99% [51200/60000] Batch 900, Loss: 0.134854,
    Accuracy: 94.99% [57600/60000] Epoch 37 ------------------------------- Batch
    0, Loss: 0.104421, Accuracy: 96.88% [    0/60000] Batch 100, Loss: 0.122693, Accuracy:
    95.34% [ 6400/60000] Batch 200, Loss: 0.124787, Accuracy: 95.26% [12800/60000]
    Batch 300, Loss: 0.127841, Accuracy: 95.16% [19200/60000] Batch 400, Loss: 0.130558,
    Accuracy: 95.05% [25600/60000] Batch 500, Loss: 0.131684, Accuracy: 95.00% [32000/60000]
    Batch 600, Loss: 0.132620, Accuracy: 94.95% [38400/60000] Batch 700, Loss: 0.132498,
    Accuracy: 95.01% [44800/60000] Batch 800, Loss: 0.132462, Accuracy: 95.05% [51200/60000]
    Batch 900, Loss: 0.133915, Accuracy: 95.03% [57600/60000] Reached 95% accuracy,
    stopping training. [PRE42]` [PRE43][PRE44][PRE45][PRE46][PRE47] [PRE48]'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
