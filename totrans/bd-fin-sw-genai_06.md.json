["```py\n@startuml #1\n\nobject ach_file_uploads { #2\n    ach_files_id: UUID   #3\n    filename: VARCHAR(255)  #3\n    file_hash: VARCHAR(32)  #3\n    credit_total: NUMERIC(12,2)  #3\n    debit_total: NUMERIC(12,2) \n}\n\nobject ach_files {\n    ach_files_id: UUID\n    record_type: VARCHAR(1)\n    record_id: UUID\n    parsed: BOOLEAN    \n    sequence: NUMERIC\n    unparsed_record: VARCHAR(94)\n}\n\nobject ach_file_header_records {\n    record_id: UUID\n    fields for record type\n}\n\nobject ach_batch_header_records {\n    record_id: UUID\n    file_header_id: UUID\n    fields for record type  \n}\n\nobject ach_entry_detail_ppd_records {\n    record_id: UUID\n    batch_header_id: UUID\n    fields for record type\n}\n\nobject ach_addenda_ppd_records {\n    record_id: UUID\n    entry_detail_id: UUID\n    fields for record type\n}\n\nobject ach_batch_control_records {\n    record_id: UUID\n    batch_header_id: UUID\n    fields for record type\n}\n\nobject ach_file_control_records {\n    record_id: UUID\n    file_header_id: UUID\n    fields for record type\n}\n\nach_file_uploads::ach_files_id <-- ach_files::ach_files_id  #4\n #4\nach_files::record_id <-- ach_file_header_records::record_id #4\nach_files::record_id <-- ach_batch_header_records::record_id #4\nach_files::record_id <-- ach_entry_detail_ppd_records::record_id #4\nach_files::record_id <-- ach_addenda_ppd_records::record_id #4\nach_files::record_id <-- ach_batch_control_records::record_id #4\nach_files::record_id <-- ach_file_control_records::record_id #4\n #4\nach_batch_header_records::file_header_id -> \nach_file_header_records::record_id #4\nach_entry_detail_ppd_records::batch_header_id -> \nach_batch_header_records::record_id #4\nach_addenda_ppd_records::entry_detail_id -> \nach_entry_detail_ppd_records::record_id #4\nach_batch_control_records::batch_header_id --> \nach_batch_header_records::record_id #4\nach_file_control_records::file_header_id --> \nach_file_header_records::record_id #4\n #4\n@enduml  #4#5\n```", "```py\nclass AchFileProcessor:\n    records = []\n    exceptions = []\n    last_trace_number = None\n    expected_record_types = [\"1\"]\n POSTGRES_USER = \"someuser\" #1\n POSTGRES_PASSWORD = \"supersecret\" \n DATABASE_URL = f\"dbname={POSTGRES_USER} user={POSTGRES_USER}\n➥ password={POSTGRES_PASSWORD} host=postgres port=5432\" #2\n…    \n def get_db(): #3\n conn = psycopg.connect(self.DATABASE_URL)\n return conn\n```", "```py\n-- Create the uuid extension\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\"; #1\n\n-- Create the ach_files table\nCREATE TABLE ach_files (\n    ach_files_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(), #2\n    file_name VARCHAR(255) NOT NULL, \n    unparsed_record VARCHAR(94) NOT NULL, #3\n    sequence_number INTEGER NOT NULL #4\n);\n```", "```py\n    def parse(self, filename) -> [List, List]:\n\n conn = self.get_db() #1\n cursor = conn.cursor() #2\n cursor.close() #3\n conn.close() #4\n\n        with open(filename, \"r\") as file:\n            lines = file.readlines()\n```", "```py\n  postgres:\n    build: \n      context: ./db\n      dockerfile: Dockerfile\n ports:\n - 5432:5432 #1\n    env_file:\n      - ./.sql_server.conf\n```", "```py\n    def parse(self, filename) -> [List, List]:\n\n        with open(filename, \"r\") as file, self.get_db() as conn: #1\n            lines = file.readlines()\n            sequence_number = 0  #2\n\n            for line in lines:\n                sequence_number += 1 #3\n                line = line.replace(\"\\n\", \"\")\n\n                with conn.cursor() as cursor: #4\n cursor.execute(f\"INSERT INTO ach_files #4\n➥ (file_name, unparsed_record, sequence_number) #4\n➥ VALUES (%s, %s, %s)\", (filename, line, sequence_number)) #4\n conn.commit() \n```", "```py\nimport os\nimport psycopg\nimport pytest\n\nfrom ach_processor.AchFileProcessor import AchFileProcessor\n\nPOSTGRES_USER = \"someuser\" #1\nPOSTGRES_PASSWORD = \"supersecret\"             #1\n #1\nDATABASE_URL = f\"dbname={POSTGRES_USER} user={POSTGRES_USER}  #1\n➥password={POSTGRES_PASSWORD} host=localhost port=5432\"  #1\n #1\ndef get_db():                              #1\n conn = psycopg.connect(DATABASE_URL) #1\n return conn #1\n\ndef test_record_count():\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    file_path = os.path.join(dir_path, \"data\", \"sample.ach\")\n    expected_result = 41\n    parser = AchFileProcessor()\n    records, exceptions = parser.parse(file_path)\n with get_db() as conn, conn.cursor() as cursor: #2\n cursor.execute(\"SELECT COUNT(*) FROM ach_files\") \n record_count = cursor.fetchone()[0]\n    assert (\n        record_count == expected_result\n    ), f\"Expected {expected_result}, but got {record_count}\"\n```", "```py\nExpected :41\nActual   :82\n```", "```py\nExpected :41\nActual   :123\n```", "```py\n@pytest.fixture\ndef setup_teardown_method():\n    print(\"\\nsetup test\\n\")       #1\n    yield                         #2\n    print(\"\\nteardown test\\n\")    #3\n    with get_db() as conn, conn.cursor() as cursor: #4\n        cursor.execute(\"TRUNCATE ach_files\")        \n…\ndef test_record_count(setup_teardown_method):       #5\n```", "```py\nCREATE TABLE nacha_file_header (\n    id SERIAL PRIMARY KEY,\n    record_type_code CHAR(1) NOT NULL,\n    priority_code CHAR(2) NOT NULL,\n    immediate_destination CHAR(10) NOT NULL,\n    immediate_origin CHAR(10) NOT NULL,\n    file_creation_date CHAR(6) NOT NULL,\n    file_creation_time CHAR(4),  #1\n    file_id_modifier CHAR(1) NOT NULL,\n    record_size CHAR(3) NOT NULL,\n    blocking_factor CHAR(2) NOT NULL,\n    format_code CHAR(1) NOT NULL,\n    immediate_destination_name VARCHAR(23), #2\n    immediate_origin_name VARCHAR(23),  #3\n    reference_code VARCHAR(8) \n);\n```", "```py\n        return {\n            \"record_type_code\": line[0],   #1\n            \"priority_code\": line[1:3],    \n            \"immediate_destination\": line[3:13].strip(), #2\n            \"immediate_origin\": line[13:23].strip(), \n            \"file_creation_date\": line[23:29],\n            \"file_creation_time\": line[29:33],\n            \"file_id_modifier\": line[33],\n            \"record_size\": line[34:37],\n            \"blocking_factor\": line[37:39],\n            \"format_code\": line[39],\n            \"immediate_destination_name\": line[40:63].strip(),\n            \"immediate_origin_name\": line[63:86].strip(),\n            \"reference_code\": line[86:94].strip(),\n        }\n```", "```py\nCREATE TABLE ach_file_headers (\n    ach_file_headers_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    record_type_code VARCHAR(1) NOT NULL,\n    priority_code VARCHAR(2) NOT NULL,\n    immediate_destination VARCHAR(10) NOT NULL,\n    immediate_origin VARCHAR(10) NOT NULL,\n    file_creation_date VARCHAR(6) NOT NULL,\n    file_creation_time VARCHAR(4),\n    file_id_modifier VARCHAR(1) NOT NULL,\n    record_size VARCHAR(3) NOT NULL,\n    blocking_factor VARCHAR(2) NOT NULL,\n    format_code VARCHAR(1) NOT NULL,\n    immediate_destination_name VARCHAR(23),\n    immediate_origin_name VARCHAR(23),\n    reference_code VARCHAR(8)\n);\n```", "```py\nmatch record_type:\n    case \"1\":\n        result = self._parse_file_header(conn, line)\n    case \"5\":\n```", "```py\ndef _parse_file_header(self, conn: Connection[tuple[Any, ...]],\n➥ line: str) -> Dict[str, str]: #1\n    self.expected_record_types = [\"5\"]\n\n    file_header = {\n    … #2\n    }\n\n    conn.execute(f\"INSERT INTO ach_file_headers (ach_file_headers_id, \" #3\n➥                     f\"record_type_code, priority_code, \n➥ immediate_destination, immediate_origin,\" \n➥                     f\"file_creation_date, file_creation_time, \n➥file_id_modifier, record_size,\" \n                       f\"blocking_factor, format_code,\n➥immediate_destination_name,\" \n                       f\"immediate_origin_name, reference_code) \" \n                       f\"VALUES (DEFAULT, %(record_type_code)s, #4\n%(priority_code)s, %(immediate_destination)s, \"\n                       f\"%(immediate_origin)s, %(file_creation_date)s,\n➥%(file_creation_time)s, \"\n                       f\"%(file_id_modifier)s, %(record_size)s,\n➥%(blocking_factor)s, %(format_code)s, \"\n                       f\"%(immediate_destination_name)s,\n➥%(immediate_origin_name)s, %(reference_code)s)\" \n➥                           , file_header) #5\n\n        return file_header\n```", "```py\nimport pytest\nimport psycopg\nfrom psycopg.rows import dict_row\nfrom typing import Dict\nfrom ach_processor.AchFileProcessor import AchFileProcessor\n\nPOSTGRES_USER = \"someuser\"\nPOSTGRES_PASSWORD = \"supersecret\"\n\nDATABASE_URL = f\"dbname={POSTGRES_USER} user={POSTGRES_USER} \n➥ password={POSTGRES_PASSWORD} host=localhost port=5432\"\n\n@pytest.fixture\ndef setup_teardown_method():\n    print(\"\\nsetup test\\n\")\n    yield\n    print(\"\\nteardown test\\n\")\n    with get_db() as conn:\n        conn.execute(\"TRUNCATE ach_file_headers\")\n\ndef get_db(row_factory = None): #1\n    conn = psycopg.connect(DATABASE_URL, row_factory=row_factory) #2\n    return conn\n\ndef test_parse_file_header(setup_teardown_method):\n    sample_header = \"101 267084131 6910001340402200830A094101DEST NAME\n➥              ORIGIN NAME            XXXXXXXX\"\n\n    expected_result: Dict[str:str] = {\n… #3\n    }\n\n    parser = AchFileProcessor()\n    with get_db() as conn:\n        result = parser._parse_file_header(conn, sample_header) #4\n\n    with get_db(dict_row) as conn: #5\n        actual_result = conn.execute(\"SELECT * FROM \n➥ ach_file_headers\").fetchone() #6\n        del actual_result[\"ach_file_headers_id\"] #7\n\n    assert result == expected_result, f\"Expected {expected_result},\n➥ but got {result}\"\n    assert actual_result == expected_result,\n➥ f\"Expected {expected_result}, but got {actual_result}\" #8\n```", "```py\nCREATE TABLE ach_exceptions (\n    ach_exceptions_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    exception_description VARCHAR(255) NOT NULL\n);\n```", "```py\n    def _add_exception(self, conn: Connection[tuple[Any, ...]],\n➥ exception: str) -> None: #1\n        conn.execute(f\"INSERT INTO ach_exceptions#2\n➥ (ach_exceptions_id, exception_description) \"  #2\n                     f\"VALUES (DEFAULT, %(exception)s)\", #2\n➥ {\"exception\": exception} )   #2\n        return\n```", "```py\n    def get_exceptions() -> list:\n        with SqlUtils.get_db() as conn:  #1\n            exceptions = conn.execute(f\"SELECT exception_description\n➥ FROM ach_exceptions\").fetchall() #2\n        flattened_list = [item[0] for item in exceptions] #3\n        return flattened_list  \n```", "```py\nDO $$         #1\n   DECLARE        #2\n      r RECORD;   \nBEGIN             #3\n   FOR r IN SELECT table_name FROM information_schema.tables\n➥ WHERE #D  table_schema = 'public' #4\n   LOOP                  #4\n      EXECUTE 'ALTER TABLE ' || quote_ident(r.table_name) #4\n➥ || ' DISABLE TRIGGER ALL';  #4\n   END LOOP;  #4\n\n   EXECUTE (  #5\n      SELECT 'TRUNCATE TABLE ' || string_agg( #5\n➥quote_ident(table_name), ', ') || ' CASCADE'   #5\n        FROM information_schema.tables #5\n       WHERE table_schema = 'public'   #5\n   );  #5\n\n   FOR r IN SELECT table_name FROM information_schema.tables\n➥ WHERE table_schema = 'public' #6\n   LOOP #6\n      EXECUTE 'ALTER TABLE ' || quote_ident(r.table_name) || #6\n➥ ' ENABLE TRIGGER ALL';  #6\n   END LOOP;  #6\nEND $$; #7\n```", "```py\nfrom fastapi.testclient import TestClient #1\n\nfrom app.main import app  #2\n\nclient = TestClient(app) #3\n\ndef test_read_files(): #4\n    response = client.get(\"/api/v1/files\") #5\n assert response.status_code == 200 #6\n assert response.json() == [{\"file\": \"File_1\"}, #6\n➥ {\"file\": \"File_2\"}] \n```", "```py\nE   RuntimeError: Form data requires \"python-multipart\" to be installed. \nE   You can install \"python-multipart\" with: \nE   \nE   pip install python-multipart\n```", "```py\ndef test_upload_file():\n    with open(\"data/sample.ach\", \"rb\") as test_file:  #1\n        response = client.post(\"/api/v1/files\", #2\n  files={\"file\": test_file})    \n    assert response.status_code == 201  #3\n```", "```py\nfrom fastapi import APIRouter, Request, status,\n➥ File, UploadFile #1\n@router.post(\"\", status_code=status.HTTP_201_CREATED)\nasync def create_file(file: UploadFile = File(...)): #2\n    return None  #3\n```", "```py\nCREATE TABLE ach_files (         #1\n    ach_files_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(), \n    file_name VARCHAR(255) NOT NULL, \n    file_hash VARCHAR(32) NOT NULL, \n    created_at TIMESTAMP NOT NULL DEFAULT NOW(), \n); #2\n\nCREATE TABLE ach_records (  #3\n    ach_records_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    ach_files_id UUID NOT NULL REFERENCES ach_files(ach_files_id)\n➥ ON DELETE CASCADE ON UPDATE CASCADE, #4\n    file_name VARCHAR(255) NOT NULL,  #5\n    unparsed_record VARCHAR(94) NOT NULL,\n    sequence_number INTEGER NOT NULL\n);\n```", "```py\n@router.post(\"\", status_code=status.HTTP_201_CREATED)\nasync def create_file(file: UploadFile = File(...)):\n    contents = await file.read() #1\n md5_hash  = hashlib.md5(contents).hexdigest() #2\n with DbUtils.get_db_connection() as conn: #3\n conn.execute( #4\n f\"\"\" #4\n INSERT INTO ach_files (file_name, file_hash) #4\n VALUES (%s, %s) #4\n \"\"\", (file.filename, md5_hash) #4\n ) #4\n    return None #4\n```", "```py\nimport psycopg\n\nPOSTGRES_USER = \"someuser\" #1\nPOSTGRES_PASSWORD = \"supersecret\"  #1\n #1\nDATABASE_URL = f\"dbname={POSTGRES_USER} user={POSTGRES_USER} #1\n➥ password={POSTGRES_PASSWORD} host=localhost port=5432\" #1\n\ndef get_db_connection(row_factory = None):  #2\n    conn = psycopg.connect(DATABASE_URL,  #2\n➥row_factory=row_factory)  #2\n    return conn  \n```", "```py\n…\nfrom tests.SqlUtils import SqlUtils #1\nimport pytest\n…\n@pytest.fixture #2\ndef setup_teardown_method():  #2\n yield #2\n SqlUtils.truncate_all() #2\n…\ndef test_upload_file(setup_teardown_method): #3\n    with open(\"data/sample.ach\", \"rb\") as test_file:\n        response = client.post(\"/api/v1/files\", files={\"file\": test_file})\n    assert response.status_code == 201\n assert SqlUtils.get_row_count_of_1('ach_files') #4\n```", "```py\nconn.execute(\"\"\"\n                INSERT INTO ach_files\n[CA] (file_name, unparsed_record, sequence_number) \n                VALUES (%s, %s, %s)\n                \"\"\", (os.path.basename(filename), line, sequence_number))\n```", "```py\nfrom typing import Optional           #1\nfrom datetime import datetime          #1\nfrom pydantic import BaseModel, UUID4 #1\n\nclass AchFileSchema(BaseModel):  #2\n    id: Optional[UUID4] = None   #3\n    file_name: str                #3\n    file_hash: str                #3\n    created_at: Optional[datetime] = None #3\n```", "```py\nfrom typing import Optional  #1\nfrom uuid import UUID    #1\nfrom psycopg.rows import dict_row #1\nfrom ach_processor.database import DbUtils #1\nfrom ach_processor.schemas.AchFileSchema  #1\n➥import AchFileSchema #1\n\nclass AchFileSql:\n    def insert_record(self, ach_file: AchFileSchema)\n➥ -> UUID: #2\n        with DbUtils.get_db_connection() as conn:\n            result = conn.execute(\n                \"\"\"\n                INSERT INTO ach_files(ach_files_id,\n➥ file_name, file_hash, created_at)\n                               VALUES \n➥(DEFAULT, %(file_name)s, %(file_hash)s, DEFAULT)\n                               RETURNING ach_files_id #3\n                                \"\"\", ach_file.model_dump()) #4\n\n        return result.fetchone()[0]\n\n    def get_record(self, ach_file_id: UUID)\n➥ -> AchFileSchema: #5\n        with DbUtils.get_db_connection(row_factory=class_row(AchFileSchema))\n➥ as conn: #6\n            result = conn.execute(\n                \"\"\"\n                SELECT * FROM ach_files WHERE ach_files_id = %s\n                \"\"\", [ach_file_id.hex])\n\n            record = result.fetchone()\n\n            if not record: #7\n                raise KeyError(f\"Record with id #7\n➥ {ach_file_id} not found\")  #7\n\n            return record\n```", "```py\nimport pytest  #1\nfrom tests.SqlUtils import SqlUtils #1\nfrom ach_processor.database.AchFileSql import AchFileSql #1\nfrom ach_processor.schemas.AchFileSchema #1\n➥ import AchFileSchema #1\n\nclass TestAchFileSql:\n    @pytest.fixture(autouse=True)   #2\n    def setup_teardown_method(self):\n        print(\"\\nsetup test\\n\")\n        yield\n        SqlUtils.truncate_all()  #3\n\n    def test_insert_record(self):\n        ach_file_record = AchFileSchema(\n            file_name=\"sample.ach\",\n            file_hash=\"1234567890\"\n        )\n        sql = AchFileSql()\n        ach_file_id = sql.insert_record(ach_file_record)  #4\n        retrieved_record = sql.get_record(ach_file_id)    #5\n        assert SqlUtils.get_row_count_of_1(\"ach_files\")\n➥ is True, \"Expected 1 record\" #6\n        assert retrieved_record.file_name == ach_file_record.file_name, #7\n➥ f\"Expected {ach_file_record.file_name}, but got \n{retrieved_record.file_name}\"  #7\n```", "```py\nCREATE VIEW combined_ach_records AS\nSELECT \n    r1.ach_records_type_1_id AS primary_key, \n    r1.unparsed_record, \n    r1.sequence_number,\n    r1.ach_files_id\nFROM \n    ach_records_type_1 AS r1\n\nUNION ALL                 #1\n\nSELECT \n    r5.ach_records_type_5_id, \n    r5.unparsed_record, \n    r5.sequence_number,\n    r1_r5.ach_files_id\nFROM \n    ach_records_type_5 AS r5\nJOIN ach_records_type_1 AS r1_r5 \n    USING (ach_records_type_1_id) #2\n #3\n\n…\nUNION ALL\n\nSELECT \n       r9.ach_records_type_9_id, \n       r9.unparsed_record, \n       r9.sequence_number,\n       r1_r9.ach_files_id\n  FROM \n       ach_records_type_9 AS r9\n  JOIN ach_records_type_1 AS r1_r9 \n USING (ach_records_type_1_id)\n```", "```py\nfrom abc import ABC  #1\nfrom pydantic import BaseModel #2\n\nclass AchRecordBaseSchema(ABC, BaseModel):  #3\n    unparsed_record: str    #4\n    sequence_number: int  \n```", "```py\nfrom typing import Optional #1\nfrom pydantic import UUID4 #1\n #1\nfrom ach_processor.schemas.ach_record.ach_record_base_schema import AchRecordBaseSchema #1\n #1\nclass AchRecordType1Schema(AchRecordBaseSchema): #2\n    ach_records_type_1_id: Optional[UUID4] = None #3\n    ach_files_id: UUID4 #4\n```", "```py\nfrom pydantic import BaseModel, UUID4\n\nclass AchBatchHeaderSchema(BaseModel):\n    ach_records_type_5_id: UUID4\n    record_type_code: str\n    service_class_code: str\n    company_name: str\n    company_discretionary_data: str\n    company_identification: str\n    standard_entry_class_code: str\n    company_entry_description: str\n    company_descriptive_date: str\n    effective_entry_date: str\n    settlement_date: str\n    originator_status_code: str\n    originating_dfi_identification: str\n    batch_number: str\n```", "```py\n@pytest.fixture(autouse=True)\ndef setup_teardown_method():\n    SqlUtils.truncate_all()\n    yield\n```"]