<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">2</span></span> <span class="chapter-title-text">Graph embeddings</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Exploring graph embeddings and their importance</li>
<li class="readable-text" id="p3">Creating node embeddings using non-GNN and GNN methods</li>
<li class="readable-text" id="p4">Comparing node embeddings on a semi-supervised problem</li>
<li class="readable-text" id="p5">Taking a deeper dive into embedding methods</li>
</ul>
</div>
<div class="readable-text" id="p6">
<p>Graph embeddings are essential tools in graph-based machine learning. They transform the intricate structure of graphs—be it the entire graph, individual nodes, or edges—into a more manageable, lower-dimensional space. We do this to compress a complex dataset into a form that’s easier to work with, without losing its inherent patterns and relationships, the information to which we’ll apply a graph neural network (GNN) or other machine learning method.<span class="aframe-location"/></p>
</div>
<div class="readable-text intended-text" id="p7">
<p>Graphs, as we’ve learned, encapsulate relationships and interactions within networks, whether they’re social networks, biological networks, or any system where entities are interconnected. Embeddings capture these real-life relationships in a compact form, facilitating tasks such as visualization, clustering, or predictive modeling.</p>
</div>
<div class="readable-text intended-text" id="p8">
<p>There are numerous strategies to derive these embeddings, each with its unique approach and application: from classical graph algorithms that use the network’s topology, to linear algebra techniques that decompose matrices representing the graph, and more advanced methods such as GNNs [1]. GNNs stand out because they can integrate the embedding process directly into the learning algorithm itself.</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>In traditional machine learning workflows, embeddings are generated as a separate step, serving as a dimensionality-reduction technique in tasks such as regression or classification. However, GNNs merge embedding generation with the model’s learning process. As the network processes inputs through its layers, the embeddings are refined and updated, making the learning phase and the embedding phase inseparable. This means that GNNs learn the most informative representation of the graph data during training time. </p>
</div>
<div class="readable-text intended-text" id="p10">
<p>Using graph embeddings can significantly enhance your data science and machine learning projects, especially when dealing with complex networked data. By capturing the essence of the graph in a lower-dimensional space, embeddings make it feasible to apply a variety of other machine learning techniques to graph data, opening up a world of possibilities for analysis and model building.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>In this chapter, we begin with an introduction to graph embeddings and a case study on a graph of political book purchases. We start with Node2Vec (N2V) to establish a baseline with a non-GNN approach, guiding you through its practical application. In section 2.2, we shift to GNNs, offering a hands-on introduction to GNN-based embeddings, including setup, preprocessing, and visualization. Section 2.3 provides a comparative analysis of N2V and GNN embeddings, highlighting their applications. The chapter then rounds off with a discussion of the theoretical aspects of these embedding methods, with a special focus on the principles behind N2V and the message-passing mechanism in GNNs. The process we take in this chapter is illustrated in figure 2.1.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p12">
<img alt="figure" height="954" src="../Images/2-1.png" width="1046"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.1</span> Summary of process and objectives in chapter 2</h5>
</div>
<div class="readable-text print-book-callout" id="p13">
<p><span class="print-book-callout-head">Note</span>  Code from this chapter can be found in notebook form at the GitHub repository (<a href="https://mng.bz/qxnE">https://mng.bz/qxnE</a>). Colab links and data from this chapter can be accessed in the same location.</p>
</div>
<div class="readable-text" id="p14">
<h2 class="readable-text-h2"><span class="num-string">2.1</span> Creating embeddings with Node2Vec</h2>
</div>
<div class="readable-text" id="p15">
<p>Understanding the relationships within a network is a core task in many fields, from social network analysis to biology and recommendation systems. In this section, we’ll explore how to create node embeddings using <em>Node2Vec (N2V)</em>, a technique inspired by Word2Vec from natural language processing (NLP) [2]. N2V captures the context of nodes within a graph by simulating random walks, allowing us to understand the neighborhood relationships between nodes in a low-dimensional space. This approach is effective for identifying patterns, clustering similar nodes, and preparing data for machine learning tasks.</p>
</div>
<div class="readable-text intended-text" id="p16">
<p>To make this process accessible, we’ll use the <code>Node2Vec</code> Python library, which is beginner-friendly, although it may be slower on larger graphs. N2V helps create embeddings that capture the structural relationships between nodes, which we can then visualize to uncover insights about the graph’s structure. Our workflow involves several steps:</p>
</div>
<ol>
<li class="readable-text" id="p17"> <em>Load data and set N2V parameters.</em> We start by loading our graph data and initializing N2V with specific parameters to control the random walks, such as walk length and the number of walks per node. </li>
<li class="readable-text" id="p18"> <em>Create embeddings.</em> N2V generates node embeddings by performing random walks on the graph, effectively summarizing each node’s local neighborhood into a vector format. </li>
<li class="readable-text" id="p19"> <em>Transform embeddings.</em> The resulting embeddings are saved and then transformed into a format suitable for visualization. </li>
<li class="readable-text" id="p20"> <em>Visualize embeddings in two dimensions.</em> We use UMAP, a dimensionality reduction technique, to project these embeddings into two dimensions, making it easier to visualize and interpret the results. </li>
</ol>
<div class="readable-text" id="p21">
<p>Our data is the Political Books dataset, which comprises books (nodes) connected by frequent co-purchases on Amazon.com during the 2004 US election period (edges) [3]. Using this dataset provides a compelling example of how N2V can reveal underlying patterns in co-purchasing behavior, potentially reflecting broader ideological groupings among book buyers [4]. Table 2.1 provides key information about the Political Books graph.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p22">
<h5 class="browsable-container-h5"><span class="num-string">Table 2.1</span> Overview of the Political Books dataset </h5>
<table>
<thead>
<tr>
<th>
<div>
         Books in the political genre co-purchased on Amazon.com 
       </div></th>
<th/>
</tr>
</thead>
<tbody>
<tr>
<td>  Number of nodes (books) <br/></td>
<td>  105 <br/></td>
</tr>
<tr>
<td>  Left-leaning nodes <br/></td>
<td>  41.0% <br/></td>
</tr>
<tr>
<td>  Right-leaning nodes <br/></td>
<td>  46.7% <br/></td>
</tr>
<tr>
<td>  Neutral nodes <br/></td>
<td>  12.4% <br/></td>
</tr>
<tr>
<td>  Number of edges Edges represent the prevalence of a co-purchase between two books. <br/></td>
<td>  441 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p23">
<p>The Political Books dataset contains the following:</p>
</div>
<ul>
<li class="readable-text" id="p24"> <em>Nodes</em> —Represent books about US politics sold by <a href="http://Amazon.com">Amazon.com</a>. </li>
<li class="readable-text" id="p25"> <em>Edges</em> —Indicate frequent co-purchasing by the same buyers, as suggested by Amazon’s “customers who bought this book also bought these other books” feature. </li>
</ul>
<div class="readable-text" id="p26">
<p>In figure 2.2, books are shaded based on their political alignment—darker shade for liberal, lighter shade for conservative, and striped for neutral. The categories were assigned by Mark Newman through a qualitative analysis of book descriptions and reviews posted on Amazon.</p>
</div>
<div class="browsable-container figure-container" id="p29">
<img alt="figure" height="955" src="../Images/2-2.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.2</span> Graph visualization of the Political Books dataset. Right-leaning books (nodes) are in a lighter shade and are clustered in the top half of the figure, left-leaning are darker shaded circles and clustered in the lower half of the figure, and neutral political stance are dark squares and appear in the middle. When two nodes are connected, it indicates that they have been purchased together frequently on Amazon.com.</h5>
</div>
<div class="readable-text intended-text" id="p27">
<p>This dataset, compiled by Valdis Krebs and available through the GNN in Action repository (<a href="https://mng.bz/qxnE">https://mng.bz/qxnE</a>) or the Carnegie Mellon University website (<a href="https://mng.bz/mG8M">https://mng.bz/mG8M</a>), contains 105 books (nodes) and 441 edges (co-purchases). If you want to learn more about the background of this dataset, Krebs has written an article with this information [4].</p>
</div>
<div class="readable-text intended-text" id="p28">
<p>Using N2V, we aim to explore the structure of this collection of books, uncovering insights based on political leanings and the potential associations between different book categories. By visualizing the embeddings created by N2V, we can gain a better understanding of how books are grouped and which ones might share a common audience, providing valuable insights into consumer behavior during a politically charged period.<span class="aframe-location"/></p>
</div>
<div class="readable-text" id="p30">
<p>From the visualization, note that the data is already clustered in a logical way. This is thanks to the <em>Kamada-Kawai algorithm</em> graph algorithm, which exploits the topological data only without the metadata and is useful for visualizing the graph. This graph visualization technique positions nodes in a way that reflects their connections, aiming for an arrangement where closely connected nodes are near each other but less connected nodes are farther apart. It achieves this by treating the nodes like points connected by springs, iteratively adjusting their positions until the “tension” in the springs is minimized. This results in a layout that naturally reveals clusters and relationships within the graph based purely on its structure.</p>
</div>
<div class="readable-text intended-text" id="p31">
<p>For the Political Books dataset, the Kamada-Kawai algorithm helps us visualize books (nodes) based on how often they are co-purchased on Amazon, without using any external information such as political alignment or book titles. This gives us an initial view of how books are grouped together by buying behavior. In the next steps, we’ll use methods such as N2V to create embeddings that capture more detailed patterns and further distinguish different book groups.</p>
</div>
<div class="readable-text" id="p32">
<h3 class="readable-text-h3"><span class="num-string">2.1.1</span> Loading data, setting parameters, and creating embeddings</h3>
</div>
<div class="readable-text" id="p33">
<p>We use the <code>Node2Vec</code> and <code>NetworkX</code> libraries for our first hands-on encounter with graph embeddings. After installing these packages using pip, we load our dataset’s graph data, which is stored in .gml format (Graph Modeling Language, GML), using the <code>NetworkX</code> library and generate the embeddings with the <code>Node2Vec</code> library.</p>
</div>
<div class="readable-text intended-text" id="p34">
<p>GML is a simple, human-readable plain text file format used to represent graph structures. It stores information about nodes, edges, and their attributes in a structured way, making it easy to read and write graph data. For instance, a .gml file might contain a list of nodes (e.g., books in our dataset) and edges (connections representing co-purchases) along with additional properties such as labels or weights. This format is widely used for exchanging graph data between different software and tools. By loading the .gml file with <code>NetworkX</code>, we can easily manipulate and analyze the graph in Python.</p>
</div>
<div class="readable-text intended-text" id="p35">
<p>In the <code>Node2Vec</code> library’s <code>Node2Vec</code> function, we can use the following parameters to specify the calculations done and the properties of the output embedding: </p>
</div>
<ul>
<li class="readable-text" id="p36"> <em>Size of the embedding (</em><code>dimensions</code><em>)</em> —Think of this as how detailed each node’s profile is, as in how many different traits you’re noting down. The standard detail level is 128 traits, but you can tweak this based on how complex you want each node’s profile to be. </li>
<li class="readable-text" id="p37"> <em>Length of each walk (</em><code>Walk</code> <code>Length</code><em>)</em> —This is about how far each random walk through your graph goes, with 80 steps being the usual journey. If you want to see more of the neighborhood around a node, increase this number. </li>
<li class="readable-text" id="p38"> <em>Number of walks per node (</em><code>Num</code> <code>Walks</code><em>)</em> —This tells us how many times we’ll take a walk starting from each node. Starting with 10 walks gives a good overview, but if you want a fuller picture of a node’s surroundings, consider going on more walks. </li>
<li class="readable-text" id="p39"> <em>Backtracking control (Return Parameter, </em><code>p</code><em>)</em> —This setting helps decide if our walk should circle back to where it’s been. Setting it at 1 keeps things balanced, but adjusting it can make your walks more or less exploratory. </li>
<li class="readable-text" id="p40"> <em>Exploration Depth (In-Out Parameter, </em><code>q</code><em>)</em> —This one’s about choosing between taking in the broader neighborhood scene (e.g., a breadth-first search with <code>q</code> greater than 1) or diving deep into specific paths (e.g., a depth-first search with <code>q</code> less than 1), with 1 being a mix of both. </li>
</ul>
<div class="readable-text" id="p41">
<p>Adjust these settings based on what you’re looking to understand about your nodes and their connections. Want more depth? Tweak the exploration depth. Looking for broader context? Adjust the walk length and the number of walks. In addition, keep in mind that the size of your embeddings should match the level of detail you need. In general, it’s a good idea to try different combinations of these parameters to see the effect on the embeddings. </p>
</div>
<div class="readable-text intended-text" id="p42">
<p>For this exercise, we’ll use the first four parameters. Deeper details on these parameters are found in section 2.4.</p>
</div>
<div class="readable-text intended-text" id="p43">
<p>The code in listing 2.1 begins by loading the graph into a variable called <code>books_ graph</code>, using the <code>read_gml</code> method from the <code>NetworkX</code> library. Next, a N2V <code>model</code> is initialized with the loaded graph. This model is set up with specific parameters: it will create 64-dimensional embeddings for each node, use walks of 30 steps long, perform 200 walks starting from each node to gather context, and run these operations in parallel across four workers to speed up the process.</p>
</div>
<div class="readable-text intended-text" id="p44">
<p>The N2V model is then trained with additional parameters defined in the <code>fit</code> method. This involves setting a context window size of 10 nodes around each target node to learn the embeddings, considering all nodes at least once (<code>min_count=1</code>), and processing four words (nodes, in this context) each time during training.</p>
</div>
<div class="readable-text intended-text" id="p45">
<p>Once trained, we access the node embeddings using the <code>model</code>’s <code>wv</code> method (reflecting its NLP heritage, wv stands for word vectors). For our downstream tasks, we map each node to its embedding using a dictionary comprehension.</p>
</div>
<div class="browsable-container listing-container" id="p46">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.1</span> Generating N2V embeddings</h5>
<div class="code-area-container">
<pre class="code-area">import NetworkX as nx
from Node2Vec import Node2Vec
books_graph = nx.read_gml('PATH_TO_GML_FILE')  <span class="aframe-location"/> #1
node2vec = Node2Vec(books_graph, dimensions=64,
 walk_length=30, num_walks=200, workers=4)  <span class="aframe-location"/> #2
model = node2vec.fit(window=10, min_count=1,\
batch_words=4)  <span class="aframe-location"/> #3
embeddings = {str(node): model.wv[str(node)]\
 for node in gml_graph.nodes()}  <span class="aframe-location"/> #4</pre>
<div class="code-annotations-overlay-container">
     #1 Loads the graph data from a GML file into a NetworkX graph object
     <br/>#2 Initializes the N2V model with specified parameters for the input graph
     <br/>#3 Trains the N2V model
     <br/>#4 Extracts and stores the node embeddings generated by the N2V model in a dictionary
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p47">
<h3 class="readable-text-h3"><span class="num-string">2.1.2</span> Demystifying embeddings</h3>
</div>
<div class="readable-text" id="p48">
<p>Let’s explore what these embeddings are and why they are valuable. An <em>embedding</em> is a dense numerical vector that represents the identity of a node, edge, or graph in a way that captures essential information about its structure and relationships. In our context, an embedding created by N2V captures a node’s position and neighborhood within the graph using topological information. This means it summarizes how the node is connected to others, effectively capturing its role and importance in the network. Later, when we use GNNs to create embeddings, they will also encapsulate the node’s features, providing an even richer representation that includes both structure and attributes. We get deeper into theoretical aspects of embeddings in section 2.4. </p>
</div>
<div class="readable-text intended-text" id="p49">
<p>These embeddings are powerful because they transform complex, high-dimensional graph data into a fixed-size vector format that can be easily used in various analyses and machine learning tasks. For example, they allow us to perform exploratory data analysis by revealing patterns, clusters, and relationships within the graph. Beyond this, embeddings can be directly used as features in machine learning models, where each dimension of the vector represents a distinct feature. This is particularly useful in applications where understanding the structure and connections between data points, such as in social networks or recommendation systems, can significantly improve model performance.</p>
</div>
<div class="readable-text intended-text" id="p50">
<p>To illustrate, consider the node representing the book <em>Losing Bin Laden</em> in our Political Books dataset. Using the command <code>model.wv['Losing</code> <code>Bin</code> <code>Laden']</code>, we retrieve its dense vector embedding. This vector, shown in figure 2.3, captures various aspects of the book’s role within the network of co-purchased books, providing a compact, informative representation that can be used for further analysis or as input to other models.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p51">
<img alt="figure" height="444" src="../Images/2-3.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.3</span> Extracting the embedding for the node associated with the political book Losing Bin Laden. The output is a dense vector represented as a Python list.</h5>
</div>
<div class="readable-text" id="p52">
<p>These embeddings can be used for exploratory data analysis to see the patterns and relationships in a graph. However, their usage extends further. One common application is to use these vectors as features in a machine learning problem that uses tabular data. In that case, each element in our embedding array will become a distinct feature column in the tabular data. This can add a rich representation of each node to complement other attributes in model training. In the next section, we’ll look at how to visualize these embeddings to gain deeper insights into the patterns and relationships they represent.</p>
</div>
<div class="readable-text" id="p53">
<h3 class="readable-text-h3"><span class="num-string">2.1.3</span> Transforming and visualizing the embeddings</h3>
</div>
<div class="readable-text" id="p54">
<p>Visualization methods such as Uniform Manifold Approximation and Projection (UMAP) are powerful tools for reducing high-dimensional datasets into lower-dimensional space [5]. UMAP is particularly effective for identifying inherent clusters and visualizing complex structures that are difficult to perceive in high-dimensional data. Compared to other methods, such as t-SNE, UMAP excels in preserving both local and global structures, making it ideal for revealing patterns and relationships across different scales in the data.</p>
</div>
<div class="readable-text intended-text" id="p55">
<p>While N2V generates embeddings by capturing the network structure of our data, UMAP takes these high-dimensional embeddings and maps them onto a lower-dimensional space (typically two or three dimensions). This mapping aims to keep similar nodes close together while also preserving broader structural relationships, providing a more comprehensive visualization of the graph’s topology. After obtaining our N2V embeddings and converting them into a numerical array, we initialize the UMAP model with two components to project our data onto a 2D plane. By carefully selecting parameters such as the number of neighbors and minimum distance, UMAP can balance between revealing fine-grained local relationships and maintaining global distances between clusters.</p>
</div>
<div class="readable-text intended-text" id="p56">
<p>By using UMAP, we gain a more accurate and interpretable visualization of our graph embeddings as shown in the following listing, allowing us to explore and analyze patterns, clusters, and structures more effectively than with traditional methods such as t-SNE.</p>
</div>
<div class="browsable-container listing-container" id="p57">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.2</span> Visualizing the embeddings using UMAP </h5>
<div class="code-area-container">
<pre class="code-area">node_embeddings = [embeddings[str(node)] \
for node in gml_graph.nodes()]  <span class="aframe-location"/> #1
node_embeddings_array = np.array(node_embeddings)  

umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, \
random_state=42)
umap_features = umap_model.fit_transform\
(node_embeddings_array) <span class="aframe-location"/> #2

plt.scatter(umap_features[:, 0], \
umap_features[:, 1], color=node_colors, alpha=0.7) <span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 Transforms the embeddings into a list of vectors for UMAP
     <br/>#2 Initializes and fits UMAP
     <br/>#3 Plots the nodes with UMAP embeddings and color by their value
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p58">
<p>The resultant figure 2.4 encapsulates the political book graph’s embeddings as distilled by N2V and subsequently visualized through UMAP. The nodes appear in different shades according to their political alignment. The visualization unfolds a discernible structure, with potential clusters that correspond to the various political leanings.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p59">
<img alt="figure" height="766" src="../Images/2-4.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.4</span> Embeddings of the Political Books dataset graph generated by N2V and visualized using UMAP. Shape and shading variations distinguish the three political classes.</h5>
</div>
<div class="readable-text intended-text" id="p60">
<p>You might wonder why we don’t just reduce the dimensions of the N2V embeddings from <code>64</code> to <code>2</code> and visualize them directly, bypassing UMAP altogether? In listing 2.3, we show this approach, applying a 2D N2V transformation directly to our <code>books_graph</code> object. (For more technical detail and theory of these methods, see section 2.4.) </p>
</div>
<div class="readable-text intended-text" id="p61">
<p>The <code>dimensions</code> parameter is set to <code>2</code>, aiming for a direct 2D representation suitable for immediate visualization without further dimensionality reduction. The other parameters are kept the same.</p>
</div>
<div class="readable-text intended-text" id="p62">
<p>Once the model is fitted with the specified window and word batch settings, we extract the 2D embeddings and store them in a dictionary keyed by the string representation of each node. This enables a direct mapping from the node to its embedding vector.</p>
</div>
<div class="readable-text intended-text" id="p63">
<p>The extracted 2D points are compiled into a NumPy array and plotted. We use the standard <code>Matplotlib</code> library to create a scatterplot of these points using the prepared color scheme to represent the political leaning of each node visually.</p>
</div>
<div class="browsable-container listing-container" id="p64">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.3</span> Visualizing 2D N2V embeddings without t-SNE</h5>
<div class="code-area-container">
<pre class="code-area">node2vec = Node2Vec(gml_graph, dimensions=2, \
walk_length=30, num_walks=200, workers=4)  <span class="aframe-location"/> #1
model = node2vec.fit(window=10, min_count=1,\
 batch_words=4)  <span class="aframe-location"/> #2

embeddings_2d = {str(node): model.wv[str(node)] \
for node in gml_graph.nodes()}  <span class="aframe-location"/> #3

points = np.array([embeddings_2d[node] \
for node in gml_graph.nodes()])  <span class="aframe-location"/> #4

plt.scatter(points[:, 0], points[:, 1], \
color=node_colors, alpha=0.7)  <span class="aframe-location"/> #5</pre>
<div class="code-annotations-overlay-container">
     #1 Initializes N2V with 2D embeddings for visualization
     <br/>#2 Trains N2V model with specified window and walks settings
     <br/>#3 Maps nodes to their 2D embeddings
     <br/>#4 Forms an array of 2D points for each node’s embedding
     <br/>#5 Plots the 2D embeddings with specified node colors
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p65">
<p>The outcome shows how the books are separated by political leanings, similar to the UMAP result, but where the books are more bunched together (see figure 2.5). The two embeddings are then shown in figure 2.6.<span class="aframe-location"/><span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p66">
<img alt="figure" height="766" src="../Images/2-5.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.5</span> Embeddings of the Political Books dataset graph generated and visualized by N2V for two dimensions. Shape and shading variations distinguish the three political classes. Here, we see a similar clustering by political leaning as earlier in figure 2.4 but more bunched together. </h5>
</div>
<div class="browsable-container figure-container" id="p67">
<img alt="figure" height="907" src="../Images/2-6.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.6</span> Comparison of embeddings generated by N2V and t-SNE and a direct visualization of 2D Node2Vec</h5>
</div>
<div class="readable-text" id="p68">
<p>It’s clear that both methods know to separate the books into groups based on political leanings. N2V is less expressive in how it separates the books, bunching them together across the two dimensions. Meanwhile, UMAP is better for spreading out the books in two dimensions. The relevant benefit or information contained within these dimensions depends on the task at hand. </p>
</div>
<div class="readable-text" id="p69">
<h3 class="readable-text-h3"><span class="num-string">2.1.4</span> Beyond visualization: Applications and considerations of N2V embeddings</h3>
</div>
<div class="readable-text" id="p70">
<p>While visualizing N2V embeddings offers intuitive insights into the dataset’s structure, their usage extends far beyond graphical representation. N2V is an embedding method designed specifically for graphs; it captures both the local and global structural properties of nodes by simulating random walks through the graph. This process allows N2V to create dense, numerical vectors that summarize the position and context of each node within the overall network. </p>
</div>
<div class="readable-text intended-text" id="p71">
<p>These embeddings can then serve as feature-rich inputs for a variety of machine learning tasks, such as classification, recommendation, or clustering. For example, in our Political Books dataset, embeddings could help predict a book’s political leaning based on its co-purchase patterns or could recommend books to users with similar political interests. They might even be used to forecast future sales based on the content of a book.</p>
</div>
<div class="readable-text intended-text" id="p72">
<p>However, it’s important to understand the nature of N2V’s learning approach, which is <em>transductive</em>. Transductive learning is designed to work only with the specific dataset it was trained on and can’t generalize to new, unseen nodes without retraining the model. This characteristic makes N2V highly effective for static datasets where all nodes and edges are known up front but less suitable for dynamic settings where new data points or connections frequently appear. Essentially, N2V focuses on extracting detailed patterns and relationships from the existing graph rather than developing a model that can easily adapt to new data.</p>
</div>
<div class="readable-text intended-text" id="p73">
<p>While this transductive nature has its limitations, it also offers significant advantages. Because N2V uses the full structure of the graph during training, it can capture intricate relationships and dependencies that might be missed by more generalized methods. This makes N2V particularly powerful for tasks where the complete, fixed structure of the data is known and stable. However, to apply N2V effectively, it’s crucial to ensure that the graph data is represented in a way that captures all relevant features. In some cases, additional edges or nodes may need to be added to the graph to fully represent the underlying relationships.</p>
</div>
<div class="readable-text intended-text" id="p74">
<p>For those interested in a deeper understanding of transductive models and how N2V’s approach compares to other methods, further details are provided in section 2.4.2. That section will explore the tradeoffs between transductive and inductive learning in greater depth [6, 7], helping you understand when each approach is most appropriate.</p>
</div>
<div class="readable-text intended-text" id="p75">
<p>While N2V is effective for generating embeddings that capture the structure of a fixed graph, real-world data often demands a more flexible and generalizable approach. This need brings us to our first GNN architecture for creating node embeddings. Unlike N2V, which is a transductive method limited to the specific nodes and edges in the training data, GNNs can learn in an <em>inductive</em> manner. This means GNNs are capable of generalizing to new, unseen nodes or edges without requiring retraining on the entire graph.</p>
</div>
<div class="readable-text intended-text" id="p76">
<p>GNNs achieve this by not only understanding the network’s complex structure but also by incorporating node features and relationships into the learning process. This approach allows GNNs to adapt dynamically to changes in the graph, making them well-suited for applications where the data is continually evolving. The shift from N2V to GNNs represents a key transition from focusing on deep analysis within a static dataset to a broader applicability across diverse, evolving networks. This adaptability sets the stage for a wider range of graph-based machine learning applications that require flexibility and scalability. In the next section, we’ll explore how GNNs go beyond the capabilities of N2V and other transductive methods, allowing for more versatile and powerful models that can handle the dynamic nature of real-world data.</p>
</div>
<div class="readable-text" id="p77">
<h2 class="readable-text-h2"><span class="num-string">2.2</span> Creating embeddings with a GNN</h2>
</div>
<div class="readable-text" id="p78">
<p>While N2V provides a powerful method for generating embeddings by capturing the local and global structure of a graph, it’s fundamentally a transductive approach, meaning it can’t easily generalize to unseen nodes or edges without retraining. Although there have been extensions to N2V that enable it to work in inductive settings, GNNs are inherently designed for inductive learning. This means they can learn general patterns from the graph data that allow them to make predictions or to generate embeddings for new nodes or edges without needing to retrain the entire model. This gives GNNs a significant edge in scenarios where flexibility and adaptability are crucial.</p>
</div>
<div class="readable-text intended-text" id="p79">
<p>GNNs not only incorporate the structural information of the graph, like N2V, but they also use node features to create richer representations. This dual capacity allows GNNs to learn both the complex relationships within the graph and the specific characteristics of individual nodes, enabling them to excel in tasks where both types of information are important. </p>
</div>
<div class="readable-text intended-text" id="p80">
<p>That said, while GNNs have demonstrated impressive performance across many applications, they don’t universally outperform methods such as N2V in all cases. For instance, N2V and other random walk-based methods can sometimes perform better in scenarios where labeled data is scarce or noisy, thanks to their ability to work with just the graph structure without needing additional node features.</p>
</div>
<div class="readable-text" id="p81">
<h3 class="readable-text-h3"><span class="num-string">2.2.1</span> Constructing the embeddings</h3>
</div>
<div class="readable-text" id="p82">
<p>Unlike N2V, GNNs learn graph representations and perform tasks such as node classification or link prediction simultaneously during training. Information from the entire graph is processed through successive GNN layers, each refining the node embeddings without requiring a separate step for their creation.</p>
</div>
<div class="readable-text intended-text" id="p83">
<p>To demonstrate how a GNN extracts features from graph data, we’ll perform a straightforward pass-through using an untrained model to generate preliminary embeddings. Even without the optimization typically involved in training, this approach will show how GNNs use message passing (explored further in section 2.4.4) to update embeddings, capturing both the graph’s structure and its node features. When optimization is added, these embeddings become tailored to specific tasks such as node classification or link prediction.</p>
</div>
<div class="readable-text" id="p84">
<h4 class="readable-text-h4">Defining our GNN architecture</h4>
</div>
<div class="readable-text" id="p85">
<p>We initiate our process by defining a simple GCN architecture, as shown in listing 2.4. Our <code>SimpleGNN</code> class inherits from <code>torch.nn.Module</code> and is composed of two <code>GCNConv</code> layers, which are the building blocks of our GNN. This architecture is shown in figure 2.7, consisting of the first layer, a message passing layer (<code>self.conv1</code>), an activation (<code>torch.relu</code>), a dropout layer (<code>torch.dropout</code>), and a second message passing layer.</p>
</div>
<div class="browsable-container listing-container" id="p86">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.4</span> Our <code>SimpleGNN</code> class </h5>
<div class="code-area-container">
<pre class="code-area">class SimpleGNN_embeddings(torch.nn.Module):
    def __init__(self, num_features, hidden_channels):  <span class="aframe-location"/> #1
        super(SimpleGNN, self).__init__()
        self.conv1 = GCNConv(num_features, \
hidden_channels)  <span class="aframe-location"/> #2
        self.conv2 = GCNConv(hidden_channels,\
 hidden_channels) <span class="aframe-location"/> #3

    def forward(self, x, edge_index):  <span class="aframe-location"/> #4
        x = self.conv1(x, edge_index) <span class="aframe-location"/> #5
        x = torch.relu(x)  <span class="aframe-location"/> #6
        x = torch.dropout(x, p=0.5, train=self.training)  <span class="aframe-location"/> #7
        x = self.conv2(x, edge_index)   <span class="aframe-location"/> #8
        return x  <span class="aframe-location"/> #9</pre>
<div class="code-annotations-overlay-container">
     #1 Initializes the GNN class with input and hidden layer sizes
     <br/>#2 First GCN layer from input features to hidden channels
     <br/>#3 Second GCN layer within the hidden space
     <br/>#4 Forward pass function defines data flow
     <br/>#5 First GCN layer processing
     <br/>#6 Activation function for nonlinearity
     <br/>#7 Dropout for regularization during training
     <br/>#8 Second GCN layer processing
     <br/>#9 Returns the final node embeddings
     <br/>
</div>
</div>
</div>
<div class="browsable-container figure-container" id="p87">
<img alt="figure" height="705" src="../Images/2-7.png" width="270"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.7</span> Architecture diagram of the <code>SimpleGNN</code> model</h5>
</div>
<div class="readable-text" id="p88">
<p>Let’s talk about the architecture aspects specific to GNNs. The activation and dropout are common in many deep learning scenarios. The GNN layers, however, are different from conventional deep learning layers in a fundamental way. The core principle that allows GNNs to learn from graph data is message passing. For each GNN layer, in addition to updating the layer’s weights, a “message” is gathered from every node or edge neighborhood and used to update an embedding. Essentially, each node sends messages to its neighbors and simultaneously receives messages from them. For every node, its new embedding is computed by combining its own features with the aggregated messages from its neighbors, through a combination of nonlinear transformations. </p>
</div>
<div class="readable-text intended-text" id="p89">
<p>In this example, we’re going to be using a graph convolutional network (GCN) to act as our message-passing GNN layers. We describe GCNs in much more detail in chapter 3. For now, you just need to know that GCNs act as message-passing layers that are critical in constructing embeddings. </p>
</div>
<div class="readable-text" id="p90">
<h4 class="readable-text-h4">Data preparation</h4>
</div>
<div class="readable-text" id="p91">
<p>Next, we prepare our data. We’ll start with the same graph from the previous section, <code>books_gml</code>, in its <code>NetworkX</code> form. We have to convert this <code>NetworkX</code> object into a tensor form that is suitable to use with PyTorch operations. Because PyTorch Geometric (PyG) has many functions that convert graph objects, we can do this quite simply with <code>data</code> <code>=</code> <code>from_NetworkX(gml_graph)</code>. Method <code>from_NetworkX</code> specifically translates the edge lists and node/edge attributes into PyTorch tensors. </p>
</div>
<div class="readable-text intended-text" id="p92">
<p>For GNNs, generating node embeddings requires initializing node features. In our case, we don’t have any predefined node features. When no node features are available or they aren’t informative, it’s common practice to initialize the node features randomly. A more effective approach is to use <em>Xavier initialization</em>, which sets the initial node features with values drawn from a distribution that keeps the variety of activations consistent across layers. This technique ensures that the model starts with a balanced representation, preventing problems such as vanishing or exploding gradients.</p>
</div>
<div class="readable-text intended-text" id="p93">
<p>By initializing <code>data.x</code> with Xavier initialization, we provide the GNN with a starting point that allows it to learn meaningful node embeddings from noninformative features. During training, the network adjusts these initial values to minimize the loss function. When the loss function is aligned with a specific target, such as node prediction, the embeddings learned from the initial random features will become tailored to the task at hand, resulting in more effective representations. We randomize the node features using the following:</p>
</div>
<div class="browsable-container listing-container" id="p94">
<div class="code-area-container">
<pre class="code-area">data.x = torch.randn((data.num_nodes, 64), dtype=torch.float)
'nn.init.xavier_uniform_(data.x) '</pre>
</div>
</div>
<div class="readable-text" id="p95">
<p>We could have also used the embeddings from the N2V exercise to use as node features. Recall the <code>node_embeddings</code> object from section 2.1.3:</p>
</div>
<div class="browsable-container listing-container" id="p96">
<div class="code-area-container">
<pre class="code-area">node_embeddings = [embeddings[str(node)] for node in gml_graph.nodes()]</pre>
</div>
</div>
<div class="readable-text" id="p97">
<p>From this, we can convert the node embedding to a PyTorch tensor object and assign it to the node feature object, <code>data.x</code>:</p>
</div>
<div class="browsable-container listing-container" id="p98">
<div class="code-area-container">
<pre class="code-area">node_features = torch.tensor(node_embeddings, dtype=torch.float) 
data.x = node_features</pre>
</div>
</div>
<div class="readable-text" id="p99">
<h4 class="readable-text-h4">Passing the graph through the GNN</h4>
</div>
<div class="readable-text" id="p100">
<p>With the structure of our GNN model defined and our graph data formatted for PyG, we proceed to the embedding generation step. We initialize our model, <code>SimpleGNN</code>, specifying the number of features for each node and the size of the hidden channels within the network. </p>
</div>
<div class="browsable-container listing-container" id="p101">
<div class="code-area-container">
<pre class="code-area">model = SimpleGNN(num_features=data.x.shape[1], hidden_channels=64)</pre>
</div>
</div>
<div class="readable-text" id="p102">
<p>Here, we specify 64 hidden channels because we want to compare the resulting embeddings to the ones we produced using the <code>node2vec</code> method, which had 64 dimensions. Because the second GNN layer is the last layer, the output will be a 64-element vector.</p>
</div>
<div class="readable-text intended-text" id="p103">
<p>Once initialized, we switch the model to evaluation mode using <code>model.eval()</code>. This mode is used during inference or validation phases when we want to make predictions or assess model performance without modifying the model’s parameters. Specifically, <code>model.eval()</code> turns off certain behaviors specific to training, such as <em>dropout</em>, which randomly deactivates some neurons to prevent overfitting, and <em>batch normaliza</em><em>tion</em>, which normalizes inputs across a mini-batch. By disabling these features, the model provides consistent and deterministic outputs, ensuring that the evaluation accurately reflects its true performance on unseen data.</p>
</div>
<div class="readable-text intended-text" id="p104">
<p>It’s important to disable gradient computations because they’re not necessary for the forward pass and embedding generation. So, we employ <code>torch.no_grad()</code>, which ensures that the computational graph that records operations for backpropagation isn’t constructed, preventing us from accidentally changing performance. </p>
</div>
<div class="readable-text intended-text" id="p105">
<p>Next, we pass our node-feature matrix (<code>data.x)</code> and the edge index (<code>data.edge_ index</code>) through the model. The result is <code>gnn_embeddings</code>, a tensor where each row corresponds to the embedding of a node in our graph—a numerical representation learned by our GNN, ready for downstream tasks such as visualization or classification:</p>
</div>
<div class="browsable-container listing-container" id="p106">
<div class="code-area-container">
<pre class="code-area">model.eval()
with torch.no_grad():
    gnn_embeddings = model(data.x, data.edge_index)</pre>
</div>
</div>
<div class="readable-text" id="p107">
<p>After producing these embeddings, we use UMAP to visualize them, as we did in section 2.1.3. Since we’ve been working with PyTorch tensor data types running on a GPU, we need to convert our embeddings to a NumPy array data type to use analysis methods outside of PyTorch, which are done on a CPU:</p>
</div>
<div class="browsable-container listing-container" id="p108">
<div class="code-area-container">
<pre class="code-area">gnn_embeddings_np = gnn_embeddings.detach().cpu().numpy()</pre>
</div>
</div>
<div class="readable-text" id="p109">
<p>With this conversion, we can produce the UMAP calculations and visualization following the process we used in the N2V case. The resulting scatterplot (figure 2.8) is a first glimpse at the clusters within our graph. We add different shadings based on each node’s label (left-, right-, or neutral-leaning) to see that similar leaning books are fairly well grouped, given that these embeddings were constructed from topology alone. </p>
</div>
<div class="readable-text intended-text" id="p110">
<p>Next, let’s discuss both how GNN embeddings are used and how they differ from those produced with N2V.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p111">
<img alt="figure" height="1127" src="../Images/2-8.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.8</span> Visualization of embeddings generated from passing a graph through a GNN</h5>
</div>
<div class="readable-text" id="p112">
<h3 class="readable-text-h3"><span class="num-string">2.2.2</span> GNN vs. N2V embeddings</h3>
</div>
<div class="readable-text" id="p113">
<p>Throughout this book, we predominantly use GNNs to generate embeddings because this embedding process is intrinsic to a GNN’s architecture. While embeddings play a pivotal role in the methodologies and applications we explore in the rest of the book, their presence is often subtle and not always highlighted. This approach allows us to focus on the broader concepts and applications of GNN-based machine learning without getting slowed down by the technicalities. Nonetheless, it’s important to acknowledge that the underlying power and adaptability of embeddings are central to the advanced techniques and insights we get into throughout the text.</p>
</div>
<div class="readable-text intended-text" id="p114">
<p>GNN-produced node embeddings are particularly powerful because they enable us to tackle a broad range of graph-related tasks by using their inductive nature. Inductive learning allows these embeddings to generalize to new, unseen nodes or even entirely new graphs without needing to retrain the model. In contrast, N2V embeddings are limited to the specific graphs they were trained on and can’t easily adapt to new data. Let’s reiterate the key ways in which GNN embeddings differ from other embedding methods, such as N2V [1, 3].</p>
</div>
<div class="readable-text" id="p115">
<h4 class="readable-text-h4">Adaptability to new graphs</h4>
</div>
<div class="readable-text" id="p116">
<p>One of the critical features of GNN embeddings is their adaptability. Because GNNs learn a function that maps node features to embeddings, this function can be applied to nodes in new graphs without needing to be retrained, provided the nodes have similar feature spaces. This inductive capability is particularly valuable in dynamic environments where the graph may evolve over time or in applications where the model needs to be applied to different but structurally similar graphs. N2V, on the other hand, needs to be reapplied for each new graph or set of nodes. </p>
</div>
<div class="readable-text" id="p117">
<h4 class="readable-text-h4">Enhanced feature integration</h4>
</div>
<div class="readable-text" id="p118">
<p>GNNs inherently consider node features during the embedding process, allowing for a complex and nuanced representation of each node. This integration of node features, alongside the structural information, offers a more comprehensive view compared to N2V and other methods that focus on a graph’s topology. This capability makes GNN embeddings particularly suited for tasks where node features contain significant additional information.</p>
</div>
<div class="readable-text" id="p119">
<h4 class="readable-text-h4">Task-specific optimization</h4>
</div>
<div class="readable-text" id="p120">
<p>GNN embeddings are trained alongside specific tasks, such as node classification, link prediction, or even graph classification. Through end-to-end training, the GNN model learns to optimize the embeddings for the task at hand, leading to potentially higher performance and efficiency compared to using pre-generated embeddings such as those from N2V.</p>
</div>
<div class="readable-text intended-text" id="p121">
<p>That said, while GNN embeddings offer clear advantages in terms of adaptability and applicability to new data, N2V embeddings have their strengths, particularly in capturing nuanced patterns within a specific graph’s structure. In practice, the choice between GNN and N2V embeddings may depend on the specific requirements of the task, the nature of the graph data, and the constraints of the computational environment.</p>
</div>
<div class="readable-text intended-text" id="p122">
<p>For tasks where the graph structure is static and well-defined, N2V might provide a simpler and computationally efficient solution. Conversely, for dynamic graphs, large-scale applications, or scenarios requiring the incorporation of node features, GNNs will often be the more robust and versatile choice. Additionally, when the task itself is not well-defined and the work is exploratory, N2V is likely faster and easier to use.</p>
</div>
<div class="readable-text intended-text" id="p123">
<p>We’ve now successfully built our first GNN embedding. This is the key first step for all GNN models, and everything from this point will build on it. In the next section, we give an example of some of these next steps and show how to use embeddings to solve a machine learning problem.</p>
</div>
<div class="readable-text" id="p124">
<h2 class="readable-text-h2"><span class="num-string">2.3</span> Using node embeddings</h2>
</div>
<div class="readable-text" id="p125">
<p>Semi-supervised learning, which involves a combination of labeled and unlabeled data, provides a valuable opportunity to compare different embedding techniques. In this chapter, we’ll explore how GNN and N2V embeddings can be used to predict labels when the majority of the data lacks labels.</p>
</div>
<div class="readable-text intended-text" id="p126">
<p>Our task involves the Political Books dataset (<code>books_graph</code>), where nodes represent political books and edges indicate co-purchase relationships. To make the process clearer, let’s review the steps taken so far and outline our next steps, as illustrated in figure 2.9.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p127">
<img alt="figure" height="571" src="../Images/2-9.png" width="627"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.9</span> Overview of steps taken in chapter 2</h5>
</div>
<div class="readable-text" id="p128">
<p>We began with the <code>books_graph</code> dataset in graph format and performed light preprocessing to prepare the data for embedding. For N2V, this involved converting the dataset from a .gml file to a <code>NetworkX</code> format. For the GNN-based embeddings, we converted the <code>NetworkX</code> graph into a PyTorch tensor and initialized the node features using Xavier initialization to ensure balanced variability across layers.</p>
</div>
<div class="readable-text intended-text" id="p129">
<p>After preparing the data, we generated embeddings using both N2V and GCNs. Now, in this section, we’ll apply these embeddings to a semi-supervised classification problem. This involves further processing to define the classification task, where only 20% of the book labels are retained, simulating a realistic scenario with sparse labeled data.</p>
</div>
<div class="readable-text intended-text" id="p130">
<p>We’ll use the two sets of embeddings (N2V and GCN) with two different classifiers: a random forest classifier (to use the embeddings as tabular features) and a GCN classifier (to use the graph structure and node features). The goal is to predict the political orientation of the books, with the remaining 80% of the labels inferred based on the given embeddings.</p>
</div>
<div class="readable-text" id="p131">
<h3 class="readable-text-h3"><span class="num-string">2.3.1</span> Data preprocessing</h3>
</div>
<div class="readable-text" id="p132">
<p>To start, we do a little more preprocessing to our <code>books_gml</code> dataset (see listing 2.5). We must format the labels in a suitable way for the learning process. Because all the nodes are labeled, we also have to set up the semi-supervised problem by randomly selecting the nodes from which we hide the labels.</p>
</div>
<div class="readable-text intended-text" id="p133">
<p>Nodes associated with attribute <code>'c'</code> are classified as <code>'right'</code>, while those with <code>'l'</code> are classified as <code>'left'</code>. Nodes that don’t fit these criteria, including those with neutral or unspecified attributes, are categorized as <code>'neutral'</code>. These classifications are then placed into a NumPy array, <code>labels</code>, for optimized computational handling.</p>
</div>
<div class="readable-text intended-text" id="p134">
<p>Then, an array, <code>indices</code>, is created, representing the positional indexes of all nodes within the dataset. A subset of these indices, corresponding to 20% of the total node count, is designated as our labeled data.</p>
</div>
<div class="readable-text intended-text" id="p135">
<p>To manage the labeled and unlabeled data, Boolean masks, <code>labelled_mask</code> and <code>unlabelled_mask</code>, are initialized and populated. The <code>labelled_mask</code> is set to <code>True</code> for indices selected as labeled; these are the ground truth labels for corresponding nodes. Similarly, <code>unlabelled_mask</code> is set to <code>False</code>. These masks segment the dataset for training and evaluation, ensuring that algorithms are correctly trained and validated on the correct subsets of data.</p>
</div>
<div class="browsable-container listing-container" id="p136">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.5</span> Preprocessing for semi-supervised problem</h5>
<div class="code-area-container">
<pre class="code-area">labels = []
for node, data in gml_graph.nodes(data=True):  <span class="aframe-location"/> #1
    if data['value'] == 'c':
        labels.append('right')
    elif data['value'] == 'l':
        labels.append('left')
    else:  
        labels.append('neutral')
labels = np.array(labels)

random.seed(52)  <span class="aframe-location"/> #2

indices = list(range(len(labels)))  <span class="aframe-location"/> #3

labelled_percentage = 0.2   <span class="aframe-location"/> #4

labelled_indices = random.sample(indices, \
int(labelled_percentage * len(labels)))  <span class="aframe-location"/> #5

labelled_mask = np.zeros(len(labels), dtype=bool)  <span class="aframe-location"/> #6
unlabelled_mask = np.ones(len(labels), dtype=bool)

labelled_mask[labelled_indices] = True  <span class="aframe-location"/> #7
unlabelled_mask[labelled_indices] = False

labelled_labels = labels[labelled_mask]  <span class="aframe-location"/> #8
unlabelled_labels = labels[unlabelled_mask]

label_mapping = {'left': 0, 'right': 1, 'neutral': 2}  <span class="aframe-location"/> #9
numeric_labels = np.array([label_mapping[label] for label in labels])</pre>
<div class="code-annotations-overlay-container">
     #1 Extracts labels and handles neutral values
     <br/>#2 Random seed for reproducibility
     <br/>#3 Indices of all nodes
     <br/>#4 20% of data to keep as labeled
     <br/>#5 Selects a subset of indices to remain labeled
     <br/>#6 Initializes masks for labeled and unlabeled data
     <br/>#7 Updates masks
     <br/>#8 Uses masks to split the dataset
     <br/>#9 Transformed labels to numerical form
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p137">
<p>Now we transform the data for model training, as shown in listing 2.6. For the GNN-derived embeddings, <code>X_train_gnn</code> and <code>y_train_gnn</code> are assigned arrays of embeddings and corresponding numeric labels filtered by a <code>labelled_mask</code>. This mask is a Boolean array indicating which nodes in the graph are part of the labeled subset, ensuring that only data points with known labels are included in the training set.</p>
</div>
<div class="readable-text intended-text" id="p138">
<p>For N2V embeddings, a similar approach is adopted with an added preprocessing step to align the embeddings with their corresponding labels. The embeddings for each node are aggregated into NumPy array <code>X_n2v</code> in the same order as the nodes appear in the <code>books_graph</code>. This ensures consistency between the embeddings and their labels, a crucial step for supervised learning tasks. Subsequently, <code>X_train_n2v</code> and <code>y_train_n2v</code> are populated with N2V embeddings and labels, again applying the <code>labelled_mask</code> to filter for the labeled data points.</p>
</div>
<div class="browsable-container listing-container" id="p139">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.6</span> Preprocessing: constructing training data</h5>
<div class="code-area-container">
<pre class="code-area">X_train_gnn = gnn_embeddings[labelled_mask]  <span class="aframe-location"/> #1
Y_train_gnn = numeric_labels[labelled_mask]  

X_n2v = np.array([embeddings[str(node)] \
for node in gml_graph.nodes()]) <span class="aframe-location"/> #2
X_train_n2v = X_n2v[labelled_mask]             <span class="aframe-location"/> #3
y_train_n2v = numeric_labels[labelled_mask]     #3</pre>
<div class="code-annotations-overlay-container">
     #1 For GNN embeddings
     <br/>#2 For N2V embeddings
     <br/>#3 Ensures N2V embeddings are in the same order as labels
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p140">
<p>The extra alignment step for the N2V embeddings isn’t necessary for the GNN embeddings because GNN models inherently maintain the order of nodes as they process the entire graph in a structured manner. As a result, the output embeddings from a GNN are naturally ordered in correspondence with the input graph’s node order.</p>
</div>
<div class="readable-text intended-text" id="p141">
<p>In contrast, N2V generates embeddings through independent random walks starting from each node, and the order of the resulting embeddings doesn’t necessarily match the order of nodes in the original graph data structure. Therefore, an explicit alignment step is required to ensure that each N2V embedding is correctly associated with its corresponding label, as extracted from the graph. This step is critical for supervised learning tasks where the correct matching of features (embeddings) to labels is essential for model training and evaluation. For this task, we use the attribute <code>index_to_key</code>, which contains the identifiers of the nodes in the order they are processed and stored within the model.</p>
</div>
<div class="readable-text" id="p142">
<h3 class="readable-text-h3"><span class="num-string">2.3.2</span> Random forest classification</h3>
</div>
<div class="readable-text" id="p143">
<p>With our data prepped, we use GNN and N2V embeddings from sections 2.1 and 2.2 as input features for a <code>RandomForestClassifier</code>, as shown in listing 2.7. </p>
</div>
<div class="browsable-container listing-container" id="p144">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.7</span> Preprocessing: constructing training data</h5>
<div class="code-area-container">
<pre class="code-area">clf_gnn = RandomForestClassifier()  <span class="aframe-location"/> #1
clf_gnn.fit(X_train_gnn, y_train_gnn)

clf_n2v = RandomForestClassifier() <span class="aframe-location"/> #2
clf_n2v.fit(X_train_n2v, y_train_n2v)</pre>
<div class="code-annotations-overlay-container">
     #1 Classifier for GNN embeddings
     <br/>#2 Classifier for N2V embeddings
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p145">
<p>This approach allows us to directly compare the embeddings’ predictive power, where we compare results in table 2.2.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p146">
<h5 class="browsable-container-h5"><span class="num-string">Table 2.2</span> Classification performance </h5>
<table>
<thead>
<tr>
<th>
<div>
         Embedding Type 
       </div></th>
<th>
<div>
         Accuracy 
       </div></th>
<th>
<div>
         F1 Score 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  GNN <br/></td>
<td>  83.33% <br/></td>
<td>  82.01% <br/></td>
</tr>
<tr>
<td>  N2V <br/></td>
<td>  84.52% <br/></td>
<td>  80.72% <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p147">
<p>For this basic classification exercise, we’ll evaluate the performance of our models using two fundamental metrics:</p>
</div>
<ul>
<li class="readable-text" id="p148"> <em>Accuracy —</em>This metric measures the proportion of correct predictions made by the model out of all predictions. It provides a straightforward assessment of how often the classifier correctly identifies the political orientation of the books. For instance, an accuracy of 84.52% means that the model correctly predicted the orientation of the books approximately 85 times out of 100. </li>
<li class="readable-text" id="p149"> <em>F1 score</em>—This is a more nuanced metric that balances precision and recall, which is particularly useful in cases where the data is imbalanced—meaning the classes aren’t equally represented. It provides a harmonic mean of precision (the number of true positive predictions divided by the total number of positive predictions) and recall (the number of true positive predictions divided by the total number of actual positives). A higher F1 score indicates a model’s robust performance in correctly identifying both the presence and absence of the different classes, minimizing both false positives and false negatives. </li>
</ul>
<div class="readable-text" id="p150">
<p>The performance metrics reveal that N2V embeddings yield a slightly higher accuracy of 84.52% when used within a <code>RandomForestClassifier</code>, compared to 83.33% for GNN embeddings. However, GNN embeddings achieve a marginally better F1 score of 82.01%, compared to 80.72% for N2V embeddings. This nuanced difference underscores potential tradeoffs between the two embedding types: while N2V provides slightly better overall prediction accuracy, GNN embeddings may offer a more balanced performance across both the majority and minority classes.</p>
</div>
<div class="readable-text intended-text" id="p151">
<p>In general, the inductive nature of GNNs presents a robust framework for learning node representations for graphs of many different sizes. Even on smaller graphs, GNNs can effectively learn the underlying patterns and interactions between nodes, as evidenced by the higher F1 score, indicating a better balance between precision and recall in classification tasks.</p>
</div>
<div class="readable-text intended-text" id="p152">
<p>In this context, the choice between GNN and N2V embeddings might also hinge on the specific goals of the analysis and the performance metrics of greatest interest. If the priority is achieving the highest possible accuracy and the dataset is unlikely to expand significantly, N2V could be the more suitable option. Conversely, if the task values a balance between precision and recall and there’s potential for applying the learned model to similar but new graphs, GNNs offer valuable flexibility and robustness, even for smaller datasets. Having used the N2V and GNN embeddings as inputs to a random forest model, let’s next study what happens when we use them as inputs to a full end-to-end GNN model.</p>
</div>
<div class="readable-text" id="p153">
<h3 class="readable-text-h3"><span class="num-string">2.3.3</span> Embeddings in an end-to-end model</h3>
</div>
<div class="readable-text" id="p154">
<p>In the previous section, we used GNN and N2V embeddings as static inputs to a traditional machine learning model, namely a random forest classifier. Here, we use an end-to-end GNN model applied to the same problem of label prediction. By <em>end-to-end</em>, we mean that the embeddings will be generated while we also predict labels. This means that the embeddings here won’t be static because, as the GNN learns, it will update the node embeddings.</p>
</div>
<div class="readable-text intended-text" id="p155">
<p>To build this model, we’ll use the same tools as before—the <code>books_gml</code> dataset, and the <code>SimpleGNN</code> architecture. We’ll change the GNN slightly, by adding a log <code>softmax</code> activation at the end, to facilitate the output for a three-label classification problem. We’ll also slightly modify the output of our <code>SimpleGNN</code> class, allowing us to observe the embeddings as well as the predictive output. Our process includes the following: </p>
</div>
<ul>
<li class="readable-text" id="p156"> Data prep </li>
<li class="readable-text" id="p157"> Model/architecture modification </li>
<li class="readable-text" id="p158"> Establish the training loop </li>
<li class="readable-text" id="p159"> Study performance </li>
<li class="readable-text" id="p160"> Study embeddings pre-training and post-training </li>
</ul>
<div class="readable-text" id="p161">
<h4 class="readable-text-h4">Data prep</h4>
</div>
<div class="readable-text" id="p162">
<p>Assuming we use the <code>books_gml</code> data set, the process to transform it for use within the PyG framework remains the same. We’ll train two versions of the data: one with node features initialized randomly, and one with node features using the N2V embeddings.</p>
</div>
<div class="readable-text" id="p163">
<h4 class="readable-text-h4">Model modification</h4>
</div>
<div class="readable-text" id="p164">
<p>We use the same <code>SimpleGNN</code> class with modifications. First, in this enhanced version of the <code>SimpleGNN</code> class, we extend its functionality to provide a predictive output for each node. This is achieved by applying a log <code>softmax</code> activation to the embeddings produced by the second GCN layer. The log <code>softmax</code> output provides a normalized log probability distribution over the potential classes for each node for the classification task.</p>
</div>
<div class="readable-text intended-text" id="p165">
<p>Second, we introduce dual outputs. The method returns two values: the raw embeddings from the <code>conv2</code> layer, which capture the node representations, and the log <code>softmax</code> of these embeddings. For us to observe both the embedding and the predictions, we have the <code>forward</code> method return both. In addition to this two-layer model, we added two layers to this architecture to have a four-layer model for comparison, as shown in listing 2.8.</p>
</div>
<div class="browsable-container listing-container" id="p166">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.8</span> Preprocessing: Constructing training data</h5>
<div class="code-area-container">
<pre class="code-area">class SimpleGNN_inference(torch.nn.Module):
    def __init__(self, num_features, hidden_channels):
        super(SimpleGNN, self).__init__()
        self.conv1 = GCNConv(num_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)

    def forward(self, x, edge_index):
        # First Graph Convolutional layer
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)

        # Second Graph Convolutional layer
        x = self.conv2(x, edge_index)
        predictions = F.log_softmax(x, dim=1)   <span class="aframe-location"/> #1

        return x, predictions  <span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 Predicts classes by passing the final conv layer through a log softmax 
     <br/>#2 The class returns both the last embedding and the prediction.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p167">
<h4 class="readable-text-h4">Establish the training loop</h4>
</div>
<div class="readable-text" id="p168">
<p>We program the training loop for the GNN model in a semi-supervised learning context, as shown in listing 2.9. This loop iterates over a specified number of epochs, where an epoch represents a complete pass through the entire training dataset. Within each epoch, the model’s parameters are updated to minimize a loss function, which quantifies the difference between the predicted outputs and the actual labels for the nodes in the training set. For those familiar with programming deep learning training loops, this should be very familiar. For those who could do with a quick reminder, the following describes some of the key steps in initializing and running the training loop:</p>
</div>
<ul>
<li class="readable-text" id="p169"> <em>Optimizer initialization</em>—The optimizer is initialized with a specific learning rate when it’s created. For example, here we use the Adam optimizer, with an initial learning rate of 0.01. </li>
<li class="readable-text" id="p170"> <em>Zeroing the gradients</em>—<code>optimizer.zero_grad()</code> ensures that the gradients are reset before each update, preventing them from accumulating across epochs. </li>
<li class="readable-text" id="p171"> <em>Model forward pass</em>—The model processes the node features (<code>data.x</code>) and the graph structure (<code>data.edge_index</code>) to produce output predictions. In semi-supervised settings, not all nodes have labels, so the model’s output includes predictions for both labeled and unlabeled nodes. </li>
<li class="readable-text" id="p172"> <em>Applying the training mask</em>—<code>out_masked</code> <code>=</code> <code>out[data.train_mask]</code> applies a mask to the model’s output to select only the predictions corresponding to labeled nodes. This is crucial in semi-supervised learning, where only a subset of nodes has known labels. </li>
<li class="readable-text" id="p173"> <em>Loss computation and backpropagation</em>—Loss function <code>loss_fn</code> compares the selected predictions (<code>out_masked</code>) with the true labels of the labeled nodes (<code>train_labels</code>). The <code>loss.backward()</code> call computes the gradient of the loss function with respect to the model parameters, which is then used to update these parameters via <code>optimizer.step()</code>. </li>
<li class="readable-text" id="p174"> <em>Logging</em>—The training loop prints the loss at regular intervals (every 10 epochs in this case) to monitor the training progress. </li>
</ul>
<div class="browsable-container listing-container" id="p175">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.9</span> Training loop</h5>
<div class="code-area-container">
<pre class="code-area">for epoch in range(3000):   <span class="aframe-location"/> #1
    optimizer.zero_grad()

    _, out = model(data.x, data.edge_index)  <span class="aframe-location"/> #2

    out_masked = out[data.train_mask]  <span class="aframe-location"/> #3

    loss = loss_fn(out_masked, train_labels)  <span class="aframe-location"/> #4
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:   <span class="aframe-location"/> #5
        print(f'Epoch {epoch}, Log Loss: {loss.item()}')</pre>
<div class="code-annotations-overlay-container">
     #1 Number of epochs
     <br/>#2 Passes both node features and edge_index to the model
     <br/>#3 Applies the training mask to select only the outputs for the labeled nodes
     <br/>#4 Computes the loss using only the labeled nodes
     <br/>#5 Prints the loss every 10 epochs
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p176">
<p>This process iteratively refines the model’s parameters to improve its predictions on the labeled portion of the dataset, with the goal of learning a model that can generalize well to the unlabeled nodes and potentially to new, unseen data.</p>
</div>
<div class="readable-text" id="p177">
<h4 class="readable-text-h4">GNN results: Randomized vs. N2V node features</h4>
</div>
<div class="readable-text" id="p178">
<p>Let’s compare the classification task comparing the GNN performance from the randomized node features versus the N2V node features, as shown in table 2.3.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p179">
<h5 class="browsable-container-h5"><strong><span class="num-string">Table 2.3</span> Classification performance of the GNN model where the input graph uses different node features</strong></h5>
<table>
<thead>
<tr>
<th>
<div>
         Model 
       </div></th>
<th>
<div>
         GNN Accuracy 
       </div></th>
<th>
<div>
         GNN F1 Score 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Two-layer, randomized features <br/></td>
<td>  82.27% <br/></td>
<td>  82.14% <br/></td>
</tr>
<tr>
<td>  Two-layer, N2V features <br/></td>
<td>  87.79% <br/></td>
<td>  88.10% <br/></td>
</tr>
<tr>
<td>  Four-layer, randomized features <br/></td>
<td>  86.58% <br/></td>
<td>  86.90% <br/></td>
</tr>
<tr>
<td>  Four-layer, N2V features <br/></td>
<td>  88.99% <br/></td>
<td>  89.29% <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p180">
<p>The table summarizes the performance of different GNN models based on their accuracy and F1 score. It highlights that GNNs using N2V features consistently outperform those using randomized features across all model configurations. Specifically, the four-layer GNN with N2V features achieves the highest accuracy and F1 score, indicating the effectiveness of incorporating meaningful node representations derived from N2V embeddings. If we had more information about specific features for the node, as we do in chapter 3, the GNN embeddings may further improve accuracy for the GNN model. </p>
</div>
<div class="readable-text" id="p181">
<h4 class="readable-text-h4">Results: GNN vs. random forest</h4>
</div>
<div class="readable-text" id="p182">
<p>We now compare the performance of the GNN model from this section with the random forest model from the previous section (see table 2.4). </p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p183">
<h5 class="browsable-container-h5"><strong><span class="num-string">Table 2.4</span> Comparison of classification performance between the GNN model and the random forest model</strong></h5>
<table>
<thead>
<tr>
<th>
<div>
         Model 
       </div></th>
<th>
<div>
         Data Input 
       </div></th>
<th>
<div>
         Accuracy 
       </div></th>
<th>
<div>
         F1 Score 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Random forest <br/></td>
<td>  Embedding from GNN <br/></td>
<td>  83.33% <br/></td>
<td>  82.01% <br/></td>
</tr>
<tr>
<td>  Random forest <br/></td>
<td>  Embedding from N2V <br/></td>
<td>  84.52% <br/></td>
<td>  80.72% <br/></td>
</tr>
<tr>
<td>  Two-layer simple GNN <br/></td>
<td>  Graph with randomized node features <br/></td>
<td>  82.27% <br/></td>
<td>  82.14% <br/></td>
</tr>
<tr>
<td>  Two-layer simple GNN <br/></td>
<td>  Graph with n2v embeddings as node features <br/></td>
<td>  87.79% <br/></td>
<td>  88.10% <br/></td>
</tr>
<tr>
<td>  Four-layer simple GNN <br/></td>
<td>  Graph with randomized node features <br/></td>
<td>  86.58% <br/></td>
<td>  86.90% <br/></td>
</tr>
<tr>
<td>  Four-layer simple GNN <br/></td>
<td>  Graph with n2v embeddings as node features <br/></td>
<td>  88.99% <br/></td>
<td>  89.29% <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p184">
<p>Figure 2.10 visualizes the results from table 2.4. Overall, the GNN models outperformed the random forest models.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p185">
<img alt="figure" height="674" src="../Images/2-10.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.10</span> Chart comparing the classification performance of the random forest with the GNN. Only one GNN model is outperformed by the random forest: The two-layer model trained on graph data with randomized node features is outperformed in terms of accuracy. </h5>
</div>
<div class="readable-text" id="p186">
<p>When comparing the performance of the GNN models with the random forest models, we can make several observations. Random forest, when trained on embeddings derived from GNN pass-through or N2V embeddings, achieves comparable accuracy to the two-layer simple GNN model. However, when considering the F1 score, both GNN models outperform random forest. Notably, the four-layer simple GNN model, especially when using N2V embeddings as features, exhibits significantly better performance than the random forest model, showcasing higher accuracy and F1 score.</p>
</div>
<div class="readable-text intended-text" id="p187">
<p>This indicates that while random forest may outperform simpler GNN architectures such as the two-layer model in terms of accuracy, the more complex GNN architectures demonstrate superior performance in terms of F1 score, especially when using sophisticated node embeddings such as N2V. Therefore, the choice between random forest and GNN should consider both accuracy and F1 score, as well as the complexity of the model architecture and the nature of the input features, to achieve optimal performance for the given task and dataset.</p>
</div>
<div class="readable-text intended-text" id="p188">
<p>It’s important to note that this short example didn’t extensively fine-tune either the GNN or the random forest models. Further optimization of both types of models could potentially lead to significant improvements in their performance. Fine-tuning hyperparameters, adjusting model architectures, and optimizing training processes could all contribute to enhancing the accuracy and F1 score of both GNNs and random forest classifiers. Therefore, while the results presented here provide starting insights into the performance on a small graph dataset, we suggest you try out the models and experiment with performance.</p>
</div>
<div class="readable-text" id="p189">
<h2 class="readable-text-h2"><span class="num-string">2.4</span> Under the Hood</h2>
</div>
<div class="readable-text" id="p190">
<p>This section gets deeper into the theoretical foundations of graph representations and embeddings, particularly in the context of GNNs. It emphasizes the importance of embeddings in transforming complex graph data into lower-dimensional, manageable forms that retain essential information. </p>
</div>
<div class="readable-text intended-text" id="p191">
<p>We distinguish between two primary types of learning: transductive and inductive. Transductive methods, such as N2V, optimize embeddings specifically for the training data, making them effective within a known dataset but less adaptable to new data. In contrast, inductive methods, as exemplified by GNNs, enable generalization to new, unseen data by integrating both graph structure and node features during training. This section also examines the mechanisms behind N2V (random walk) and GNNs (message passing). </p>
</div>
<div class="readable-text" id="p192">
<h3 class="readable-text-h3"><span class="num-string">2.4.1</span> Representations and embeddings</h3>
</div>
<div class="readable-text" id="p193">
<p>Understanding graph representations and the role of embeddings is crucial for effectively applying GNNs in machine learning. Representations convert complex graph data into simpler, manageable forms without losing essential information, facilitating analysis and interpretation of the underlying structures within graphs. In the context of GNNs, representations enable the processing of graph data in a way that is compatible with machine learning algorithms, ensuring that the rich and complex structures of graphs are preserved.</p>
</div>
<div class="readable-text intended-text" id="p194">
<p>Traditional methods such as adjacency matrices and edge lists provide a foundational way to represent graph structures, but they often fall short in capturing richer information, such as node features or subtle topological details. This limitation is where graph embeddings come into play. A graph embedding is a low-dimensional vector representation of a graph, node, or edge that retains essential structural and relational information. Much like reducing a high-resolution image to a compact feature vector, embeddings condense the graph’s complexity while preserving its distinguishing characteristics.</p>
</div>
<div class="readable-text intended-text" id="p195">
<p>Embeddings simplify data handling and open new possibilities for machine learning applications. They enable visualization of complex graphs in two or three dimensions, allowing us to explore their inherent structures and relationships more intuitively. Furthermore, embeddings serve as versatile inputs for various downstream tasks, such as node classification and link prediction, as demonstrated in earlier sections of this chapter. By providing a bridge between raw graph data and machine learning models, embeddings are key to unlocking the full potential of GNNs.</p>
</div>
<div class="readable-text" id="p196">
<h4 class="readable-text-h4">The significance of node similarity and context</h4>
</div>
<div class="readable-text" id="p197">
<p>An important use of graph embeddings is to encapsulate the notion of similarity and context within the graph. In a spatial context, proximity (or similarity) often translates to a measurable distance or angle between points. </p>
</div>
<div class="readable-text intended-text" id="p198">
<p>For graphs, however, these concepts are redefined in terms of connections and paths. The similarity between nodes can be interpreted through their connectivity, that is, how many “hops” or steps it takes to move from one node to another, or the likelihood of traversing from one node to another during random walks on the graph (figure 2.11).<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p199">
<img alt="figure" height="309" src="../Images/2-11.png" width="837"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.11</span> Comparison of similarity concepts: using distance on a plane (left) and using steps along a graph (right)</h5>
</div>
<div class="readable-text" id="p200">
<p>Another way to think about proximity is in terms of probability: Given two nodes (node A and node B), what is the chance that I will encounter node B if I start to hop from node A? In figure 2.12, if the number of hops is 1, the probability is 0, as there is no way to reach node B from <span class="aframe-location"/>node A in one hop. However, if the number of hops is 2, then we need to first count to see how many different possible routes there are. Let’s also assume that no node can be encountered twice in a traversal and that each direction is equally likely. With these assumptions, there are three unique routes of 2 hops starting from node A. Of those, only one leads to node B. Thus, the probability is one out of three, or 33%. This probabilistic approach to measuring proximity between nodes offers a nuanced understanding of the graph’s topology, meaning that graph structures can be encoded within a probability space.</p>
</div>
<div class="browsable-container figure-container" id="p201">
<img alt="figure" height="299" src="../Images/2-12.png" width="699"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.12</span> Illustrating the notion of proximity computed in terms of probability: given a walk from node A, the probability of encountering node B is a measure of proximity. </h5>
</div>
<div class="readable-text intended-text" id="p202">
<p>The ideas explained here are relevant as we approach the topic of inductive and transductive methods as applied to graph embeddings. Both of these methodologies use the notion of node proximity, although in distinct ways, to generate embeddings that capture the essence of node relationships and graph structure. Inductive methods excel in generalizing to accommodate new, unseen data, enabling models to adapt and learn beyond their initial training set. Conversely, transductive methods specialize in optimizing embeddings specifically for the training data itself, making them highly effective within their learned context but less flexible when introduced to new data.</p>
</div>
<div class="readable-text" id="p203">
<h3 class="readable-text-h3"><span class="num-string">2.4.2</span> Transductive and inductive methods</h3>
</div>
<div class="readable-text" id="p204">
<p>The way an embedding is created determines the scope of its subsequent usage. Here we examine embedding methods that can be broadly classified as transductive and inductive. Transductive embedding methods learn representations for a fixed set of nodes in a single, static graph:</p>
</div>
<ul>
<li class="readable-text" id="p205"> These methods directly optimize individual embeddings for each node. </li>
<li class="readable-text" id="p206"> The entire graph structure must be available during training. </li>
<li class="readable-text" id="p207"> These methods can’t naturally generalize to unseen nodes or graphs. </li>
<li class="readable-text" id="p208"> Adding new nodes requires retraining the entire model. </li>
<li class="readable-text" id="p209"> Examples include DeepWalk [8], N2V, and matrix factorization approaches. </li>
<li class="readable-text" id="p210"> Transductive methods allow us to reduce the scope of the prediction problem. For transduction, we’re only concerned with the data we’re presented with. </li>
<li class="readable-text" id="p211"> These methods are computationally costly for large amounts of data. </li>
</ul>
<div class="readable-text" id="p212">
<p>Inductive embedding methods learn a function to generate embeddings, allowing generalization to unseen nodes and even entirely new graphs:</p>
</div>
<ul>
<li class="readable-text" id="p213"> These methods learn to aggregate and transform node features and local graph structure. </li>
<li class="readable-text" id="p214"> These methods can generate embeddings for previously unseen nodes without retraining. </li>
<li class="readable-text" id="p215"> Node attributes or structural features are often used. </li>
<li class="readable-text" id="p216"> These methods are more flexible and scalable for dynamic or expanding graphs. </li>
<li class="readable-text" id="p217"> Examples include GraphSAGE, GCNs, and graph attention networks (GATs). </li>
</ul>
<div class="readable-text" id="p218">
<p>Let’s illustrate this with two examples:</p>
</div>
<ul>
<li class="readable-text" id="p219"> <em>Example 1: Email spam detection</em>—An inductive model for email spam detection is trained on a dataset of labeled emails (spam or not spam) and learns to generalize from the training data. Once trained, the model can classify new incoming emails as spam or not spam without needing to retrain. </li>
</ul>
<div class="readable-text list-body-item" id="p220">
<p>Transductive wouldn’t be better in this example because models would require retraining with every new batch of emails, making them computationally expensive and impractical for real-time spam detection.</p>
</div>
<ul>
<li class="readable-text" id="p221"> <em>Example 2: Semi-supervised learning for community detection in social networks</em>—A transductive model uses the entire graph to identify communities within a social network. Using a combination of labeled and unlabeled nodes, the model exploits the network in a better way: inductive models wouldn’t take full advantage of the specific network structure and node interconnections because they only process part of the data—the training set. This isn’t enough information for accurate community detection. </li>
</ul>
<div class="readable-text" id="p222">
<p>Table 2.5 compares the types of graph representation we’ve learned so far, consisting of representations generated by both nonembedding methods, and embedding methods.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p223">
<h5 class="browsable-container-h5"><span class="num-string">Table 2.5</span> Different methods of graph representation</h5>
<table>
<thead>
<tr>
<th>
<div>
         Representation 
       </div></th>
<th>
<div>
         Description 
       </div></th>
<th>
<div>
         Examples 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Basic data representations <br/></td>
<td>  • Great for analytical methods that involve network traversal <br/>  • Useful for some node classification algorithms <br/>  • Information provided: Node and edge neighbors <br/></td>
<td>  • Adjacency list <br/>  • Edge list <br/>  • Adjacency matrix <br/></td>
</tr>
<tr>
<td>  Transductive (shallow) embeddings <br/></td>
<td>  • Useless for data not trained on <br/>  • Difficult to scale <br/></td>
<td>  • DeepWalk <br/>  • N2V <br/>  • TransE <br/>  • RESCAL <br/>  • Graph factorization <br/>  • Spectral techniques <br/></td>
</tr>
<tr>
<td>  Inductive embeddings <br/></td>
<td>  • Models can be generalized to new and structurally different graphs <br/>  • Represents data as vectors in continuous space <br/>  • Learns a mapping from data (new and old) to positions within the continuous space <br/></td>
<td>  • GNNs can be used to inductively generate embeddings <br/>  • Transformers <br/>  • N2V with feature concatenation <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p224">
<h5 class="callout-container-h5 readable-text-h5">Summary of terms related to transductive embedding methods</h5>
</div>
<div class="readable-text" id="p225">
<p>Two additional terms related to embedding methods and sometimes used interchangeably with it are <em>shallow methods</em> and <em>encoders</em>. Here, we’ll briefly distinguish these terms.</p>
</div>
<div class="readable-text" id="p226">
<p>Transductive methods, explained earlier, are a large class of methods of which graph embedding is one application. So, outside of our present context of representation learning, the attributes of transductive learning remain the same.</p>
</div>
<div class="readable-text" id="p227">
<p>In machine learning, <em>shallow</em> is often used to refer to the opposite of deep learning models or algorithms. Such models are distinguished from deep learning models in that they don’t use multiple processing layers to produce an output from input data. In our context of graph/node embeddings, this term also refers to methods that aren’t based on deep learning, but more specifically points to methods that mimic a simple lookup table, rather than a generalized model produced from a supervised learning algorithm. </p>
</div>
<div class="readable-text" id="p228">
<p>Any method that reproduces a low dimensional representation of data, an embedding, is often known as an <em>encoder</em>. This encoder simply matches a given data point such as a node (or even an entire graph) to its respective embedding in low dimensional space. GNNs can be broadly understood as a class of encoders, similar to Transformers. However, there are specific GNN encoders, such as the graph autoencoder (GAE), which you’ll meet in chapter 5. </p>
</div>
</div>
<div class="readable-text" id="p229">
<h3 class="readable-text-h3"><span class="num-string">2.4.3</span> N2V: Random walks across graphs</h3>
</div>
<div class="readable-text" id="p230">
<p>Random walk approaches construct embeddings by using random walks across the graph. With these, the similarity between two nodes, A and B, is defined as the probability that one will encounter node B on a random graph traversal from node A (as we described in section 2.4.1). These walks are unrestricted, with no restriction preventing a walk from backtracking or encountering the same node multiple times. </p>
</div>
<div class="readable-text intended-text" id="p231">
<p>For each node, we perform a random walk within its neighborhood. As we perform more and more random walks, we begin to notice similarities in the types of nodes we encounter. A potential mental model is exploring a city or forest. In a distinct neighborhood, for example, as we take the same streets or paths multiple times, we begin to notice that houses have a similar style and trees have a similar species. </p>
</div>
<div class="readable-text intended-text" id="p232">
<p>The result of a random walk method is a vector of nodes visited for each walk, with different starting nodes. In the upcoming figure 2.13, we show some examples of how we can walk (or search) across a graph. </p>
</div>
<div class="readable-text intended-text" id="p233">
<p>DeepWalk is one method that creates embeddings by performing several random walks of a fixed size for each node and calculating embeddings from each of these. Here, any path is equally likely to occur, making the walks unbiased and meaning that all nodes connected by an edge are equally likely to be encountered at each step. The output for a DeepWalk on the graph in figure 2.13 might be the vector [u, s1, s3] or the vector [u, s1, s2, s4, s5]. Each of these vectors contains the unique nodes visited in a random walk.</p>
</div>
<div class="readable-text intended-text" id="p234">
<p>N2V improved on DeepWalk by introducing tunable bias in these random walks. The idea is to be able to trade off learnings from a node’s close-by neighborhood and from further away. N2V captures this in two parameters:</p>
</div>
<ul>
<li class="readable-text" id="p235"> <code>p</code>—Controls the probability that the path walked will return to the previous node. </li>
<li class="readable-text" id="p236"> <code>q</code>—Controls the probability of whether a depth-first search (DFS, a hopping strategy that emphasizes faraway nodes) or breadth-first search (BFS, a strategy that emphasizes nearby nodes). DFS and BFS are illustrated in figure 2.13, where we demonstrate what happens for four hops.  </li>
</ul>
<div class="readable-text" id="p237">
<p>To mimic the DeepWalk algorithm, both <code>p</code> and <code>q</code> would be set to <code>0</code> such that the search is <em>unbiased</em>. So, for figure 2.13, the output for N2V could be [u, s1,s2] or [u,s4,s5,s6] depending on whether the walks are BFS or DFS. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p238">
<img alt="figure" height="310" src="../Images/2-13.png" width="675"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.13</span> Depth-first search (DFS) and breadth first search (BFS) on a graph where embeddings are generated based on random walks using these graph-traversal strategies. DFS (light arrows) prioritizes going deep down one path, while BFS (dark arrows) prioritizes checking all adjacent and nearby paths.</h5>
</div>
<div class="readable-text" id="p239">
<p>Once we have the vector of nodes, we create embeddings by using a neural network to predict the most likely neighboring node for a given node. Usually, this neural network is shallow, with one hidden layer. After training, the hidden layer becomes that node’s embedding.</p>
</div>
<div class="readable-text" id="p240">
<h3 class="readable-text-h3"><span class="num-string">2.4.4</span> Message passing as deep learning</h3>
</div>
<div class="readable-text" id="p241">
<p>Deep learning methods in general are composed of building blocks, or layers, that take some tensor-based input and then produce an output that is transformed as it flows through the various layers. At the end, more transformations and aggregations are applied to yield a prediction. However, often the output of the hidden layers is directly exploited for other tasks within the model architecture or are used as inputs to other models. This is what we saw in our classification problem in section 2.3. We constructed a vector of visited nodes, and these nodes were then passed to a deep learning model. The deep learning model learned to predict future nodes based on a starting node. But the actual embeddings were contained within <em>the hidden layer</em> of the network. </p>
</div>
<div class="readable-text print-book-callout" id="p242">
<p><span class="print-book-callout-head">Tip</span>  For a refresher on deep learning, read <em>Deep Learning with Python</em> by François Chollet (Manning, 2021). </p>
</div>
<div class="readable-text" id="p243">
<p>We show the classic architecture for a deep feed-forward neural network, specifically a multilayer perceptron (MLP), in figure 2.14. Briefly, the network takes a node vector as input, and the hidden layers are trained to produce an output vector that achieves some task, such as identifying a node class. The input vector may be flattened images, and the output may be a single number reflecting where there is a dog or cat in the image. For the N2V example, the input is a vector of starting nodes, and the output is the corresponding other node vectors that are visited after traversing the graph from the starting node. In the image example, the output is the explicit task function, namely to classify images based on whether they contain a dog or cat. In N2V, the output is implicit in the graph structure. We know the subsequent nodes that are visited, but we’re interested in the way the network has encoded the data, that is, how it has built an embedding of the node data. This is contained within the hidden layer, where typically we just take the last layer.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p244">
<img alt="figure" height="340" src="../Images/2-14.png" width="450"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.14</span> Structure of a multilayer perceptron </h5>
</div>
<div class="readable-text" id="p245">
<p>For GNNs, the input will be the entire graph structure, and the output will be the embeddings. Therefore, the model is explicit in how it constructs the embeddings. However, the output isn’t restricted to embeddings. Instead, we can have the output as a classification, such as whether the book has a specific political affiliation in that node in the graph. The embeddings are again implicit in the hidden layers. However, the entire process is wrapped into a single model, so we don’t need to extract this data. Instead, we’ve used the embeddings to achieve our goal, such as node classification. </p>
</div>
<div class="readable-text intended-text" id="p246">
<p>While the architecture of GNNs is very different from feed-forward neural networks, there are some parallels. In many of the GNNs we learn about, a graph in tensor form is passed into the GNN architecture, and one or more iterations of message passing is applied. The message-passing process is shown in figure 2.15.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p247">
<img alt="figure" height="235" src="../Images/2-15.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.15</span> Elements of our message-passing layer. Each message-passing layer consists of an aggregation, a transformation, and an update step.</h5>
</div>
<div class="readable-text" id="p248">
<p>In chapter 1, we first discussed the idea of message passing. In its simplest form, message passing reflects that we’re taking information or data from nodes or edges and sending it somewhere else [1]. The messages are the data, and we’re passing the messages across the structure of our graph. Each message can contain information from either sender or receiver, or often both. </p>
</div>
<div class="readable-text intended-text" id="p249">
<p>We can now explain further why message passing is so important to GNNs. The message passing step updates the information about each node by using the node information and nodes neighborhood information (both in terms of nearby node data and the edge data connecting them). Message passing is how we construct representations about our graph. These are the critical mechanisms that build graph embeddings, which inform other tasks such as node classification. There are two important aspects to consider when constructing these node (or edge) embeddings. </p>
</div>
<div class="readable-text intended-text" id="p250">
<p>First, we need to think about what is inside a message. In our earlier example, we had a list of books on political topics. This dataset only had information about the books’ co-purchasing connections and their political leaning labels. However, if we had additional information such as the book length, author name, or even the synopsis, then those node features could be contained within our messages. However, it’s important to remember that it could also be edge data, such as when another book was bought together, that could also be contained in messages. In fact, sometimes messages can contain both node and edge data. </p>
</div>
<div class="readable-text intended-text" id="p251">
<p>Second, we need to think about how much local information we want to consider when making each embedding. We want to know how much of the neighborhood to sample. We already discussed this when we introduced random walk methods. We need to define how many hops to take when sampling our graph.</p>
</div>
<div class="readable-text intended-text" id="p252">
<p>Both the data and the number of hops are critical to message passing in GNNs. The features, either node or edge data, are the messages, and the number of hops is the number of times we pass a message. Both of these are controlled by the layers of a GNN. The number of hidden layers is the number of hops that we’ll be sending messages. The input to each hidden layer is the data contained in a message. This is almost always the case for GNNs but it’s worth noting that it isn’t always true. Sometimes, other mechanisms such as attention can determine the depth of message-passing samples from the neighborhood. We’ll discuss graph attention networks (GATs) in chapter 4. Until then, understanding that the number of layers in a GNN reflects the number of hops undertaken during message passing is a good intuition to have. </p>
</div>
<div class="readable-text intended-text" id="p253">
<p>For a feed-forward network, like the one on the left in figure 2.15, information is passed between the nodes of our neural network. In a GNN, this information comprises the messages that we send over our graph. For each message-passing step, the vertex in our neural network collects information from nodes or edges one hop away. So, if we want our node representations to take account of nodes from three hops away from each node, we need three hidden message-passing layers. Three layers may not seem like very many, but the amount of a graph we cover scales exponentially with the number of hops. Intuitively, we can understand this as a type of six degrees of separation principle—that all people are only six degrees of social separation apart from each other. This would mean that you and I could be connected by six short hops across the global combined social network. </p>
</div>
<div class="readable-text intended-text" id="p254">
<p>Different message-passing schemes lead to different flavors of GNNs. So, for each GNN we study in this book, we’ll pay close attention to the math and code implementation for message passing. One important aspect is how we aggregate messages, which we’ll discuss in chapter 3 when we discuss GCNs in depth. </p>
</div>
<div class="readable-text intended-text" id="p255">
<p>After message passing, the resulting tensor is passed through feed-forward layers that result in a prediction. In the left of figure 2.16, which illustrates a GNN model for node prediction, the data flows through message-passing layers, the tensor then passes through an additional MLP and activation function to output a prediction. For example, we could use our GNN to classify whether employees are likely to join a new company or have good recommendations. </p>
</div>
<div class="browsable-container figure-container" id="p257">
<img alt="figure" height="1533" src="../Images/2-16.png" width="1096"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.16</span> A simple GNN architecture diagram (left). A graph is input on the left, encountering node-information-passing layers. This is followed by MLP layers. After an activation is applied, a prediction is yielded. A GNN architecture (right).</h5>
</div>
<div class="readable-text intended-text" id="p256">
<p>However, as with the feed-forward neural network illustrated previously, we can also output just the hidden layers and work directly with that output. For GNNs, this output is the graph, node, or edge embeddings.<span class="aframe-location"/></p>
</div>
<div class="readable-text intended-text" id="p258">
<p>One final note on message passing in GNNs is that in each step in the message-passing layer of our GNNs, we’ll be passing information from nodes to another node one hop away. Importantly, a neural network then takes the data from the one-hop neighbors and applies a nonlinear transformation. This is the beauty of GNNs; we’re applying many small neural networks at the level of individual nodes and/or edges to build embeddings of the graph features. Therefore,<span class="aframe-location"/> when we say that a message-passing layer is like the first layer of a neural network, we’re really saying that it’s the first layer of many individual neural networks that are all learning on local node data or edge-specific data. In practice, the overall code constructing and training is the same as if it were one single transformation, but the intuition that we’re applying individual nonlinear transformations will become useful as we travel deeper into the workings of complex GNN models. </p>
</div>
<div class="readable-text" id="p259">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p260"> Node and graph embeddings are powerful methods to extract insights from our data, and they can serve as inputs/features in our machine learning models. There are several independent methods for generating such embeddings. GNNs have embedding built into the architecture. </li>
<li class="readable-text" id="p261"> Graph embeddings, including node and edge embeddings, serve as foundational techniques to transform complex graph data into structured formats suitable for machine learning tasks. </li>
<li class="readable-text" id="p262"> We explored two main types of graph embeddings: N2V, a transductive method, and GNN-based embeddings, an inductive method, each with distinct characteristics and applications. </li>
<li class="readable-text" id="p263"> N2V operates on fixed datasets using random walks to establish node contexts and similarities, but it doesn’t generalize to unseen data or graphs. </li>
<li class="readable-text" id="p264"> GNNs, on the other hand, are versatile, inductive frameworks that can generate embeddings for new, unseen data, making them adaptable across different graph structures. </li>
<li class="readable-text" id="p265"> The comparison of embeddings in machine learning tasks, such as semi-supervised learning, reveals the importance of choosing the right embedding method based on the data size, complexity, and specific problem at hand. </li>
<li class="readable-text" id="p266"> Despite the effectiveness of random forest classifiers in handling both N2V and GNN embeddings for smaller graphs, GNNs demonstrate a unique ability to use graph topology and node features, particularly in larger and more complex graphs. </li>
<li class="readable-text" id="p267"> Embeddings can be used as features in traditional machine learning models and in graph data visualization and insight extraction. </li>
</ul>
</div></body></html>