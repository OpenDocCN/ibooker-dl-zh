<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">12</span> </span> <span class="chapter-title-text">Evaluations and benchmarks</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Understanding the significance of benchmarking and evaluating LLMs</li> 
    <li class="readable-text" id="p3">Learning different evaluation metrics</li> 
    <li class="readable-text" id="p4">Benchmarking model performance</li> 
    <li class="readable-text" id="p5">Implementing comprehensive evaluation strategies</li> 
    <li class="readable-text" id="p6">Best practices for evaluation benchmarks and key evaluation criteria to consider</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Taking into account the recent surge of interest in GenAI and specifically in large language models (LLMs), it’s crucial to approach these novel and uncertain features cautiously and responsibly. Many leaderboards and studies have shown that LLMs can match human performance in various tasks, such as taking standardized tests or creating art, sparking enthusiasm and attention. However, their novelty and uncertainties necessitate careful handling.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>The role of benchmarking LLMs in production deployment cannot be overstated. It involves evaluating performance, comparing models, guiding improvements, accelerating technological advancement, managing costs and latency, and ensuring efficient task flow for real-world applications. While evaluations are part of LLMOps, their criticality in ensuring LLMs meet the demands of various applications warrants a separate discussion in this chapter.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>Evaluating LLMs is not a simple task but a complex and multifaceted process that demands quantitative and qualitative approaches. When evaluating LLMs, comprehensive assessment methods covering various aspects of model performance and effect must be employed. Stanford University’s Human-Centered Artificial Intelligence (HAI) publishes an annual AI Index report [1] that aims to collate and track different data points related to AI. One of the most significant challenges we face is the lack of standardized evaluations, which makes a systematic comparison between different models incredibly difficult when it comes to capabilities and potential risks and harms. This means we don’t have an objective measure of how good or smart any of these specific models are, which underscores the complexity and importance of the evaluation process.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>When we discuss GenAI evaluations in this initial stage, most discussions concern accuracy and performance evaluations that assess how well a language model can comprehend and produce text that resembles human language. This aspect is very important for applications that rely on the quality and relevance of the content they generate, such as chatbots, content creation, and summarization tasks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>There are three general types of evaluations that can measure accuracy and performance: traditional evaluation metrics that judge language quality, LLM task-specific benchmarks for assessing specific tasks, and human evaluations. Let’s start by understanding what LLM evaluations are and learn about some of the best practices associated with evaluations.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_209"><span class="num-string">12.1</span> LLM evaluations</h2> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>It is essential to evaluate LLMs to ensure they are reliable and appropriate for real-world applications. A strong evaluation strategy covers performance metrics such as accuracy, fluency, coherence, and relevance. These metrics help us to understand the model’s advantages and disadvantages across different contexts. I summarize here a few areas as best practices to consider when evaluating LLMs:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p14"> To evaluate the LLM meaningfully, it must be tested on the use cases it is designed for, meaning using the model on various natural language processing (NLP) tasks, such as summarization, question-answering, and translation. The evaluation process should use standard metrics such as ROUGE (Recall-Oriented Understudy for Gisting Evaluation) for summarization to maintain reliability and comparability. </li> 
   <li class="readable-text" id="p15"> Another important aspect of LLM evaluation is the creation of prompts. Prompts must be unambiguous and fair, providing a valid assessment of the model’s abilities. This ensures that the evaluation outcomes reflect the model’s actual performance. </li> 
   <li class="readable-text" id="p16"> Benchmarking is a crucial practice that enables evaluating an LLM’s performance based on existing criteria and other models. This not only tracks progress but also identifies areas requiring improvement. A continuous evaluation process, combined with constant development practices, allows for periodic assessment and refinement of the LLM. </li> 
   <li class="readable-text" id="p17"> The evaluation of LLMs must involve ethical considerations at every step. The process must check the model for biases, fairness, and ethical problems, looking at the training data and the outputs. Moreover, the user experience should be a key part of the evaluation, ensuring that the model’s outputs match user needs and expectations. </li> 
   <li class="readable-text" id="p18"> The evaluation must be transparent at every stage. Recording the criteria, methods, and results allows for independent verification and increases confidence in the LLM’s abilities. Finally, the evaluation outcomes should inform a continuous improvement cycle, improving the model, training data, and the evaluation process based on performance measures and feedback. </li> 
  </ul> 
  <div class="readable-text" id="p19"> 
   <p>These practices underscore the importance of a rigorous and systematic approach to evaluating LLMs, ensuring that they are accurate but also fair, ethical, and suitable for various applications.</p> 
  </div> 
  <div class="readable-text intended-text" id="p20"> 
   <p>By following these practices, enterprises can conduct reliable and effective evaluations, developing trustworthy and helpful LLMs for different uses. Now that we know what evaluations are, let’s take a look at some metrics we should use. They can be categorized into traditional and newer LLM-specific evaluation metrics.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_210"><span class="num-string">12.2</span> Traditional evaluation metrics</h2> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>BLEU (Bilingual Evaluation Understudy), ROUGE (Recall-Oriented Understudy for Gisting Evaluation), and BERTScore (BERT Similarity Score) are some of the more standardized metrics. These metrics help quantify the linguistic quality of model outputs against reference texts and are used to evaluate text quality in tasks such as machine translation or text summarization. Still, they differ in their approaches and focus on different aspects of the text. Table 12.1 shows a detailed explanation of what each of the three scores indicates. We will show how to compute these in the next section.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p23"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 12.1</span> Traditional evaluation metrics</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Metric 
       </div></th> 
      <th> 
       <div>
         Focus 
       </div></th> 
      <th> 
       <div>
         Method 
       </div></th> 
      <th> 
       <div>
         Limitations 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  BLEU <br/></td> 
      <td>  It primarily measures precision, the percentage of words in the machine- generated text that appear in the reference text. <br/></td> 
      <td>  It compares n-grams (word sequences) of the candidate translation with the reference translation and counts the matches. <br/></td> 
      <td>  It can miss the mark on semantic meaning because it doesn’t account for synonyms or the context of words. It also doesn’t handle word reordering well. <br/></td> 
     </tr> 
     <tr> 
      <td>  ROUGE <br/></td> 
      <td>  It is more recall oriented, focusing on the percentage of words from the reference text that appear in the generated text. <br/></td> 
      <td>  It has several variants, such as ROUGE-N, which compares n-grams, and ROUGE-L, which looks at the longest common subsequence. <br/></td> 
      <td>  Like BLEU, ROUGE can overlook semantic similarities and paraphrasing because it’s based on exact word matches. <br/></td> 
     </tr> 
     <tr> 
      <td>  BERTScore <br/></td> 
      <td>  It evaluates semantic similarity rather than relying on exact word matches. <br/></td> 
      <td>  It uses contextual embeddings from models such as BERT to represent the text and calculates the cosine similarity between these embeddings. <br/></td> 
      <td>  It can capture paraphrasing and semantic meaning better than BLEU and ROUGE because it considers each word’s context. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p24"> 
   <p>Metrics such as ROUGE, BLEU, and BERTScore compare the similarities between text generated by an LLM and reference text written by humans. They are commonly used for evaluating tasks such as summarization and machine translation.</p> 
  </div> 
  <div class="readable-text" id="p25"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_211"><span class="num-string">12.2.1</span> BLEU</h3> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>BLEU (Bilingual Evaluation Understudy) [2] is an algorithm used to evaluate the quality of machine-translated text from one natural language to another. Its central idea is to measure the correspondence between a machine’s output and that of a human translator. In other words, according to BLEU, the closer a machine translation is to a professional human translation, the better it is. BLEU does not consider intelligibility or grammatical correctness; it focuses on content overlap.</p> 
  </div> 
  <div class="readable-text" id="p27"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_212"><span class="num-string">12.2.2</span> ROUGE</h3> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation) [3] is a set of measures used in NLP to assess how well automatic text summarization and machine translation perform. Its main goal is to contrast summaries or translations produced by machines with human reference summaries. It evaluates the following aspects:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p29"> <em>Recall</em><em> </em>—ROUGE measures how much of the reference summary the system summary captures. It evaluates how well the system recovers or captures content from the reference. </li> 
   <li class="readable-text" id="p30"> <em>Precision</em><em> </em>—It also assesses how much of the system summary is relevant, needed, or useful. </li> 
   <li class="readable-text" id="p31"> <em>F-measure</em><em> </em>—It combines precision and recall to provide a balanced view of system performance. </li> 
  </ul> 
  <div class="readable-text" id="p32"> 
   <p>ROUGE has different versions, such as ROUGE-N (which uses n-grams) and ROUGE-L (based on the Longest Common Subsequence algorithm). By looking at single words and sequences, ROUGE helps us measure the effectiveness of NLP algorithms in summarization and translation tasks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p33"> 
   <p>However, ROUGE has limitations. It relies solely on surface-level overlap and doesn’t account for semantic meaning or fluency. Sensitivity to stop words, stemming, and word order can affect scores. While ROUGE provides valuable insights, it’s essential to consider other evaluation metrics and human judgment to assess summary quality comprehensively. Researchers often use a combination of metrics to evaluate summarization models.</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_213"><span class="num-string">12.2.3</span> BERTScore</h3> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>BERTScore [4] is a measure of how good text generation is. It uses pretrained BERT model embeddings to compare candidate and reference sentences. The idea is to find similar words in the candidate and reference sentences based on cosine similarity. This metric agrees with human opinion in sentence- and system-level evaluations. It has the following elements:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p36"> <em>Contextual embeddings</em><em> </em>—BERTScore represents both the candidate and reference sentences with embeddings that consider each word’s context. </li> 
   <li class="readable-text" id="p37"> <em>Cosine similarity</em><em> </em>—It calculates the cosine similarity between the embeddings of the candidate and reference texts. </li> 
   <li class="readable-text" id="p38"> <em>Token matching</em><em> </em>—To compute precision and recall scores, each token in the candidate text matches the most similar token in the reference text. </li> 
   <li class="readable-text" id="p39"> <em>F1 score</em><em> </em>—The precision and recall are combined to calculate the F1 score, providing a single quality measure. </li> 
  </ul> 
  <div class="readable-text" id="p40"> 
   <p>The key advantage of BERTScore over traditional metrics such as BLEU is its ability to capture semantic similarity. This means it can recognize when different words have similar meanings and when the same words are used in different contexts.</p> 
  </div> 
  <div class="readable-text" id="p41"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_214"><span class="num-string">12.2.4</span> An example of traditional metric evaluation</h3> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>Let’s bring it all together and make it real through a simple example. Here we have two information summaries and can evaluate which one might be better. </p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>For this example, we take the AI development principles of the Bill and Melinda Gates Foundation as the article we want to analyze and understand. This article is available at <a href="https://mng.bz/vJe4">https://mng.bz/vJe4</a>. From the article, we create two summaries that we’ll compare. In this case, one is created by NLTK and the other by another LLM (GPT-3.5). This could also be two different human-written versions or any other combination. We use the <code>newspaper3K</code> and <code>bert_score</code> packages to download the article and the Hugging Face Evaluate package for the evaluations. These can be installed in conda using <code>conda</code> <code>install</code> <code>-c</code> <code>conda-forge</code> <code>newspaper3k</code> <code>evaluate</code> <code>bert_score</code>. In pip, use <code>pip install evaluate newspaper3k bert_score</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p44"> 
   <p>We use <code>newspaper3k</code> to download and parse the article first. Then we apply the <code>nlp()</code> function to process the article and get the summary from the summary property. We must ensure the article is downloaded and parsed before using NLP; note that this only works for Western languages. We use the summary created by NLP as our reference summary and the <code>Evaluate</code> library to calculate the specific metrics. The listing shows the code to implement this.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p45"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.1</span> Automated evaluation metrics</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from openai import AzureOpenAI
import evaluate
from bert_score import BERTScorer
...

AOAI_API_KEY = os.getenv("AOAI_KEY")          <span class="aframe-location"/> #1
AZURE_ENDPOINT = os.getenv("AOAI_ENDPOINT")   #1
...

URL = "https://www.gatesfoundation.org/ideas/articles/ 
             <span class="">↪</span>artificial-intelligence-ai-development-principles"

def get_article(URL, config):                <span class="aframe-location"/> #2
    article = Article(URL, config=config)
    article.download(recursion_counter=2)
    article.parse()
    article.nlp()
    return article.text, article.summary

def generate_summary(client, article_text):              <span class="aframe-location"/> #3
    prompt = f"Summarize the following article:\n\n{article_text}"
    conversation = [{"role": "system", "content": 
                             <span class="">↪</span>"You are a helpful assistant."}]
    conversation.append({"role": "user", "content": prompt})

    response = client.chat.completions.create(
        model=MODEL,
        messages = conversation,
        temperature = TEMPERATURE,
        max_tokens = MAX_TOKENS,
    )
    return response.choices[0].message.content.strip()

def calculate_scores(generated_summary, reference_summary):<span class="aframe-location"/> #4
    metric = evaluate.load("bleu", trust_remote_code=True)
    bleu_score = metric.compute(predictions=
                 <span class="">↪</span>[generated_summary], references=[reference_summary])

    metric = evaluate.load("rouge", trust_remote_code=True)
    rouge_score = metric.compute(predictions=
                  <span class="">↪</span>[generated_summary], references=[reference_summary])

    scorer = BERTScorer(lang="en")
    p1, r1, f1 = scorer.score([generated_summary], [reference_summary])
    bert_score = f"Precision: {p1} Recall: {r1} F1 Score: {f1.tolist()[0]}"

    return bleu_score, rouge_score, bert_score

# Main code
client = AzureOpenAI(
      azure_endpoint = AZURE_ENDPOINT,
      api_key=AOAI_API_KEY,
      api_version=API_VERSION
)

config = Config()                                            <span class="aframe-location"/> #5
config.browser_user_agent = USER_AGENT                       #5
config.request_timeout = 10                                  #5
article_text, reference_summary = get_article(URL, config)

generated_summary = generate_summary(client, article_text)
bleu_score, rouge_score, bert_score = calculate_scores(
                         <span class="">↪</span>generated_summary, reference_summary)

print(f"BLEU:{bleu_score}, ROUGE:{rouge_score}, BERT: {bert_score}")</pre> 
    <div class="code-annotations-overlay-container">
     #1 Sets up the OpenAI details
     <br/>#2 Function to download and parse the article; returns both the article text and a summary
     <br/>#3 Summarizes the article using OpenAI
     <br/>#4 Function to calculate metrics (BLEU, ROUGE, etc.)
     <br/>#5 Configures newspaper3k to allow downloading articles
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>This is the output we can observe when executing the code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p47"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">BLEU score: {'bleu': 0.04699157347901134, 
             <span class="">↪</span>'precisions': [0.32857142857142857, 
                    <span class="">↪</span>0.09352517985611511, 
                    <span class="">↪</span>0.021739130434782608, 
                    <span class="">↪</span>0.0072992700729927005], 
            <span class="">↪</span>'brevity_penalty': 1.0, 
            <span class="">↪</span>'length_ratio': 1.2727272727272727, 
            <span class="">↪</span>'translation_length': 140, [
            <span class="">↪</span>'reference_length': 110}

ROUGE score: {'rouge1': 0.3463203463203463, ]
              <span class="">↪</span>'rouge2': 0.09606986899563319, 
              <span class="">↪</span>'rougeL': 0.1645021645021645, 
              <span class="">↪</span>'rougeLsum': 0.2683982683982684}

BERT score: Precision: tensor([0.8524]) 
            <span class="">↪</span>Recall: tensor([0.8710]) 
            <span class="">↪</span>F1 Score: 0.8616269826889038</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>As we have seen, the BLEU score is composed of several components that collectively assess the quality of a machine-generated translation against a set of reference translations. Let’s examine each component and see what it means, starting with the BLEU score outlined in table 12.2. Tables 12.3 and 12.4 show the results for the ROUGE score and the BERT score, respectively.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p49"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 12.2</span> BLEU score</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Component value 
       </div></th> 
      <th> 
       <div>
         Meaning 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  BLEU: 0.047 (4.7%) <br/></td> 
      <td>  This is the overall BLEU score, which is quite low. BLEU scores range from 0 to 1 (or 0% to 100%), with higher scores indicating better translation quality. A score below 10% is generally considered poor. <br/></td> 
     </tr> 
     <tr> 
      <td>  Precisions <br/></td> 
      <td>  These are the n-gram precision scores for 1-gram, 2-gram, 3-gram, and 4-gram matches. Our scores indicate a decent number of 1-gram matches but few longer matches, suggesting that the translation has some correct words but lacks coherent phrases and sentences. <br/></td> 
     </tr> 
     <tr> 
      <td>  Brevity penalty: 1.0 <br/></td> 
      <td>  This means there was no penalty for brevity; the translation length was appropriate compared to the reference length. <br/></td> 
     </tr> 
     <tr> 
      <td>  Length ratio: 1.27 <br/></td> 
      <td>  The translation is 27% longer than the reference, which might suggest some verbosity. <br/></td> 
     </tr> 
     <tr> 
      <td>  Translation length: 140 <br/></td> 
      <td>  The length of the machine-translated text <br/></td> 
     </tr> 
     <tr> 
      <td>  Reference length: 110 <br/></td> 
      <td>  The length of the reference text <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p50"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 12.3</span> ROUGE score</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Component value 
       </div></th> 
      <th> 
       <div>
         Meaning 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  ROUGE-1: 0.3463 (34.63%) <br/></td> 
      <td>  It measures the overlap of 1-gram between the system output and the reference summary. A moderate score indicates a fair amount of overlap. <br/></td> 
     </tr> 
     <tr> 
      <td>  ROUGE-2 : 0.0961 (9.61%) <br/></td> 
      <td>  It measures the overlap of bigrams and is a stricter metric than ROUGE-1. A low score suggests that the system struggles to form accurate phrases. <br/></td> 
     </tr> 
     <tr> 
      <td>  ROUGE-L: 0.1645 (16.45%) <br/></td> 
      <td>  It measures the longest common subsequence, indicating the fluency and order of the words. The score suggests limited fluency. <br/></td> 
     </tr> 
     <tr> 
      <td>  ROUGE-Lsum: 0.2684 (26.84%) <br/></td> 
      <td>  It is similar to ROUGE-L but considers the sum of the longest common subsequences, indicating a slightly better grasp of the content structure. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p51"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 12.4</span> BERT Score</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Component value 
       </div></th> 
      <th> 
       <div>
         Meaning 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Precision: 0.8524 (85.24%) <br/></td> 
      <td>  It measures how many words in the candidate text are relevant or needed. <br/></td> 
     </tr> 
     <tr> 
      <td>  Recall: 0.8710 (87.10%) <br/></td> 
      <td>  It measures how much of the candidate text captures the reference content. <br/></td> 
     </tr> 
     <tr> 
      <td>  F1 Score: 0.8616 (86.16%) <br/></td> 
      <td>  This harmonic mean of precision and recall provides a single score that balances both. An F1 score closer to 1 indicates better performance. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>The BLEU and ROUGE scores suggest that the translation or summary has room for improvement, particularly in forming coherent phrases and sentences. However, the BERT score is quite high, indicating that the candidate text is semantically similar to the reference text and captures most of its content. Thus, while the translation may not match the reference text word for word, it does convey the same overall meaning quite well.</p> 
  </div> 
  <div class="readable-text intended-text" id="p53"> 
   <p>Even though metrics such as BERTScore, ROUGE, and BLEU help compare similar text, they primarily focus on surface-level similarity. They may not capture semantic equivalence or the overall quality of the generated text. These more traditional metrics often penalize LLMs, which can produce coherent and fluent generations. For these, we need LLM task-specific benchmarks.</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_215"><span class="num-string">12.3</span> LLM task-specific benchmarks</h2> 
  </div> 
  <div class="readable-text" id="p55"> 
   <p>Measuring the performance of LLMs across various NLP tasks requires task-specific benchmarks. They are created to test how well the models can understand, reason, and generate natural language for specific domains or tasks, providing a clear way to compare different models. These benchmarks can reveal a model’s abilities and limitations, enabling focused improvements.</p> 
  </div> 
  <div class="readable-text intended-text" id="p56"> 
   <p>Task-specific benchmarks assess LLMs on specific NLP tasks such as text classification, sentiment analysis, question answering, summarization, and more. These benchmarks usually consist of datasets with predefined inputs and expected outputs, allowing for quantitative assessment of model performance through metrics such as accuracy, F1 score, or BLEU score, depending on the task. Some key LLM benchmarks are groundedness, relevance, coherence, fluency, and GPT similarity; these evaluation metrics are outlined in table 12.5.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p57"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 12.5</span> LLM evaluation metrics</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Metric 
       </div></th> 
      <th> 
       <div>
         Focus 
       </div></th> 
      <th> 
       <div>
         Method 
       </div></th> 
      <th> 
       <div>
         When to use? 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Groundedness <br/></td> 
      <td>  It evaluates how well the answers the model produces match the information in the source data (context that the user provides). This metric ensures that the context backs up the answers generated by AI. <br/></td> 
      <td>  It evaluates how well the statements in an AI-generated answer match the source context, ensuring that the context supports these statements. It is rated from 1 (bad) to 5 (good). <br/></td> 
      <td>  It is used when we want to check that the AI responses match and are confirmed by the given context. It is also used when being factually correct and contextually precise is important, such as when finding information, answering questions, and summarizing content. <br/></td> 
     </tr> 
     <tr> 
      <td>  Coherence <br/></td> 
      <td>  It evaluates the model’s ability to generate coherent, natural output similar to human language. <br/></td> 
      <td>  It evaluates how well the generation is structured and connected. This is rated from 1 (bad) to 5 (good). <br/></td> 
      <td>  Use it when evaluating how easy and user-friendly your model’s generated responses are in real-world situations. <br/></td> 
     </tr> 
     <tr> 
      <td>  Fluency <br/></td> 
      <td>  It measures the grammar proficiency and readability of a model’s generated response. <br/></td> 
      <td>  The fluency measure evaluates how well the generated text follows grammatical rules, syntactic structures, and suitable word choices. It is scored from 1 (bad) to 5 (good). <br/></td> 
      <td>  This tool assesses the linguistic accuracy of the generated text, ensuring that it follows appropriate grammar rules, syntax structures, and word choices. <br/></td> 
     </tr> 
     <tr> 
      <td>  GPT similarity <br/></td> 
      <td>  It compares how similar a source data (ground truth) sentence is to the output from an AI model. <br/></td> 
      <td>  This assessment involves creating sentence-level embeddings for both the ground truth and the model’s prediction, which are high-dimensional vector representations that encode the semantic meaning and context of the sentences. <br/></td> 
      <td>  Use it to get an unbiased measure of a model’s performance, especially in text generation tasks where we have the correct responses available. This lets us check how closely the generated text matches the intended content, which helps us evaluate the model’s quality and accuracy. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>To illustrate how this works, we will apply a reference-free evaluation method based on the G-Eval method. Reference-free means that we do not depend on comparing a generated summary to a preexisting reference summary. Let’s start with understanding G-Eval.</p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_216"><span class="num-string">12.3.1</span> G-Eval: A measuring approach for NLG evaluation</h3> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>G-Eval [5] introduces a new framework for measuring the quality of text produced by NLG systems. Using LLMs, G-Eval combines a chain-of-thought–prompting method with a form-filling technique to examine different aspects of the NLG output, such as coherence, consistency, and relevance. G-Eval judges the quality of the generated content based on the input prompt and text alone, without any reference texts, and is thus considered reference free.</p> 
  </div> 
  <div class="readable-text intended-text" id="p61"> 
   <p>The method is particularly useful for novel datasets and tasks with few human references available. This flexibility makes G-Eval suitable for various innovative applications, especially in fields where data is continuously evolving or is highly specific. Here are a few scenarios where G-Eval would be beneficial:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p62"> <em>Medical report generation</em><em> </em>—In the medical domain, where automated systems produce customized reports from various patient data, G-Eval can evaluate the reports for correctness, consistency, and medical relevance. As patient scenarios differ a lot, conventional reference-based metrics might not always work, making G-Eval a more adaptable and appropriate option that guarantees the quality and dependability of medical reports. </li> 
   <li class="readable-text" id="p63"> <em>Legal document writing</em><em> </em>—When AI creates legal documents that suit particular cases, G-Eval assesses how well the documents meet legal requirements, how clear and coherent they are, and how well they follow the rules. This is important in legal situations where having precise reference texts for every situation is not feasible, but accuracy and conformity to legal standards are vital. </li> 
   <li class="readable-text" id="p64"> <em>Creative content evaluation</em><em> </em>—Novelty is essential in fields that require creativity, such as advertising or video game storytelling. G-Eval helps assess the novelty, appeal, and target audience suitability of such content, providing a way to gauge the quality of creativity that is more than just word or phrase similarity. </li> 
   <li class="readable-text" id="p65"> <em>AI-based content moderation</em><em> </em>—G-Eval can help verify that moderation actions are suitable and successful, even when there is no reliable reference data, by using AI systems to moderate changing online content. This is especially important in online settings where context and sensitivity matter. </li> 
  </ul> 
  <div class="readable-text" id="p66"> 
   <p>These examples show how G-Eval can assess the quality of AI-generated text with human-like standards and flexibility to meet different needs. This is important for GenAI applications where conventional metrics are insufficient. G-Eval has many advantages for businesses that want to develop and use effective NLG solutions:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p67"> G-Eval shows much better agreement with human evaluation than conventional metrics such as BLEU and ROUGE. This is especially clear in open-ended and creative NLG tasks, where conventional metrics often fail. By giving a more precise measurement of NLG system quality, G-EVAL helps enterprises make smart choices about their development and deployment. </li> 
   <li class="readable-text" id="p68"> G-Eval uses the probabilities of output tokens from LLMs to produce fine-grained continuous scores. This enables the capture of slight differences between generated texts, giving more detailed feedback than traditional metrics that often depend on discrete scoring. Such precise feedback can be very helpful for enterprises as they adjust their NLG systems for the best performance. </li> 
   <li class="readable-text" id="p69"> An interesting feature of G-Eval is that it can be customized to evaluate different NLG tasks by changing the prompt and evaluation criteria. This flexibility removes the need for task-specific evaluators, making the evaluation process easier for enterprises working with various NLG applications. </li> 
  </ul> 
  <div class="readable-text" id="p70"> 
   <p>However, a possible problem with LLM-based evaluators is that they may prefer text generated by LLMs. This problem needs more research and solutions to ensure reliable and correct evaluation, especially when using LLM-based metrics to enhance NLG systems.</p> 
  </div> 
  <div class="readable-text intended-text" id="p71"> 
   <p>G-Eval provides a potential method for NLG evaluation in enterprises, which can help create and use more efficient and dependable NLG systems for different purposes. Let’s see how we can use this.</p> 
  </div> 
  <div class="readable-text intended-text" id="p72"> 
   <p>We can demonstrate how G-Eval can be very helpful with a simple example. Imagine an enterprise that wants to evaluate customer service chatbots. These chatbots are usually trained to deal with many kinds of customer questions and problems, and their performance is essential for maintaining customer satisfaction and loyalty. For example, let’s think about a customer complaint about a service. Suppose a customer writes the following complaint in an email:</p> 
  </div> 
  <div class="readable-text" id="p73"> 
   <blockquote>
    <div>
     I am extremely disappointed with the delay in service. I was promised a two-day delivery, and it’s already been a week without any updates. This is unacceptable.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p> Now imagine two different automated responses generated by customer service bots:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p75"> <em>Response A (more literal and generic)</em><em> </em>—“We apologize for any inconvenience caused. Your complaint has been registered. We will update you shortly.” </li> 
   <li class="readable-text" id="p76"> <em>Response B (more empathetic and specific)</em><em> </em>—“We’re really sorry to hear about this delay and completely understand your frustration. It’s not the experience we want to provide. Our team is looking into this as a priority, and we’ll reach out with an update on your delivery by tomorrow morning.” </li> 
  </ul> 
  <div class="readable-text" id="p77"> 
   <p>Conventional metrics such as BLEU and ROUGE would likely assess these responses based on how closely certain words or phrases match a set of predefined correct responses. Response A might score reasonably well if the reference responses favor generic acknowledgments. However, these metrics might miss nuances in tone and specificity crucial for customer satisfaction. When evaluating with G-Eval, it would be more likely to assess the content, tone, empathy, and relevance of the response to the specific complaint. It would consider how effectively the response addresses the customer’s emotional state and the problem raised. In our example, response B would likely score higher on G-Eval because it acknowledges the customer’s feelings, provides a specific promise, and sets clear expectations—all of which are important to human judges (i.e., customers) in evaluating the quality of customer service.</p> 
  </div> 
  <div class="readable-text intended-text" id="p78"> 
   <p>For enterprises, particularly in areas such as customer service, the effectiveness of automated responses can significantly affect customer satisfaction and loyalty. G-Eval aligns better with human evaluation because it captures the qualitative aspects of communication that are important in real-life interactions—such as empathy, specificity, and reassurance—but that are often overlooked by traditional metrics such as BLEU and ROUGE. </p> 
  </div> 
  <div class="readable-text" id="p79"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_217"><span class="num-string">12.3.2</span> An example of LLM-based evaluation metrics</h3> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>In this example, we implement a G-Eval approach using Azure OpenAI’s GPT-4 model to measure how good text summaries are. It uses the following four criteria: relevance, coherence, consistency, and fluency. We have an article and two summaries that are based on it. In addition, we use the code to score each summary on the four criteria and show which is better. As an example, we use the AI principles of the Bill and Melinda Gates Foundation, which are listed as “The first principles guiding our work with AI” and can be accessed online at <a href="https://mng.bz/vJe4">https://mng.bz/vJe4</a>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p81"> 
   <p>We have two summaries made from this source article that we want to compare with the article. The NLP library makes one summary, and another is made by LLM (Google’s Gemini Pro 1.5). We have saved all these locally for easy access and are reading them from there. The full code where we download the article and create the summaries is shown in the book’s GitHub repository. </p> 
  </div> 
  <div class="readable-text intended-text" id="p82"> 
   <p>Listings 12.2 and 12.3 show the key areas of this example with the full code. (<a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>). We define the evaluation metrics and their criteria and steps using prompt engineering and RAG. Each describes the scoring criteria and the steps to follow when evaluating a summary. Note that we don’t show all the code for brevity reasons.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p83"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.2</span> LLM-based evaluation metrics</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">EVALUATION_PROMPT_TEMPLATE = """                      <span class="aframe-location"/> #1
You will be given one summary written for an article. Your task [
<span class="">↪</span>is to rate the summary using one metric. Make sure you read 
<span class="">↪</span>and understand these instructions very carefully. 

Evaluation Criteria:
{criteria}

Evaluation Steps:
{steps}


Example:
Source Text:

{document}

Summary:
{summary}

Evaluation Form (scores ONLY):
- {metric_name}
"""

# Metric 1: Relevance            <span class="aframe-location"/> #2
RELEVANCY_SCORE_CRITERIA = """
Relevance(1-5) - selection of important content from the source. \
The summary should include only important information from the 
<span class="">↪</span>source document. \
Annotators were instructed to penalize summaries which contained 
<span class="">↪</span>redundancies and excess information.
"""

RELEVANCY_SCORE_STEPS = """       <span class="aframe-location"/> #3
1. Read the summary and the source document carefully.
2. Compare the summary to the source document and identify the 
   <span class="">↪</span>main points of the article.
3. Assess how well the summary covers the main points of the article,
   <span class="">↪</span>and how much irrelevant or redundant information it contains.
4. Assign a relevance score from 1 to 5.
"""

# Metric 2: Coherence            <span class="aframe-location"/> #4
COHERENCE_SCORE_CRITERIA = """
Coherence(1-5) - the collective quality of all sentences. \
...

COHERENCE_SCORE_STEPS = """                 <span class="aframe-location"/> #5
1. Read the article carefully and identify the main topic and key points.
2. Read the summary and compare it to the article. Check if the summary 
...

# Metric 3: Consistency                 <span class="aframe-location"/> #6
CONSISTENCY_SCORE_CRITERIA = """
Consistency(1-5) - the factual alignment between the summary and the 
<span class="">↪</span>summarized source.
...

CONSISTENCY_SCORE_STEPS = """              <span class="aframe-location"/> #7
1. Read the article carefully and identify its main facts and details.
2. Read the summary and compare it to the article. Check if the 
   <span class="">↪</span>summary ...

# Metric 4: Fluency                        <span class="aframe-location"/> #8
FLUENCY_SCORE_CRITERIA = """
Fluency(1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.
...

FLUENCY_SCORE_STEPS = """                <span class="aframe-location"/> #9
Read the summary and evaluate its fluency based on the given criteria. 
<span class="">↪</span>Assign a fluency score from 1 to 3.
...</pre> 
    <div class="code-annotations-overlay-container">
     #1 Evaluation prompt template based on G-Eval
     <br/>#2 Defines the relevance metric as outlined by G-Eval
     <br/>#3 Outlines the rules of the relevance metrics and how to measure
     <br/>#4 Defines the coherence metric as outlined by G-Eval
     <br/>#5 Outlines the rules of the coherence metric and how to measure
     <br/>#6 Defines the consistency metric as outlined by G-Eval
     <br/>#7 Outlines the rules of the consistency metrics and how to measure
     <br/>#8 Defines the fluency metric as outlined by G-Eval
     <br/>#9 Outlines the rules of the fluency metrics and how to measure
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>We have already explained the prompts that define the metrics and the rules for computing them. Now look at the rest of the code in listing 12.3. This is simple, and we do the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p85"> Use the <code>get_article()</code> function to get the article and summaries. </li> 
   <li class="readable-text" id="p86"> Use the <code>get_geval_score()</code> function, loop over the evaluation metrics and summaries, generate a G-Eval score for each combination, and store the results in a dictionary. </li> 
   <li class="readable-text" id="p87"> Finally, convert the dictionary to a data frame so we can pivot it and print it to the console. </li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p88"> 
   <p><span class="print-book-callout-head">Note </span> The parameters for the Azure OpenAI are quite strict, with <code>max_ tokens</code> set to 5, <code>temperature</code> set to 0, and <code>top_p</code> set to 1.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p89"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.3</span> LLM-based evaluation metrics</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">def get_article():                 <span class="aframe-location"/> #1
    with open('./data/gates_foundation_article.txt', 'r') as f:
        article = f.read()
    with open('./data/gates_foundation_summary1.txt', 'r') as f:
        nlp_summary = f.read()
    with open('./data/gates_foundation_summary2.txt', 'r') as f:
        llm_summary = f.read()
    return article, nlp_summary, llm_summary

def get_geval_score(criteria: str, steps: str, document: str, 
<span class="">↪</span>summary: str, metric_name: str):                <span class="aframe-location"/> #2
    prompt = EVALUATION_PROMPT_TEMPLATE.format(       <span class="aframe-location"/> #3
        criteria=criteria,
        steps=steps,
        metric_name=metric_name,
        document=document,
        summary=summary,
    )

    response = client.chat.completions.create(     <span class="aframe-location"/> #4
        model=MODEL,
        messages = [{"role": "user", "content": prompt}],
        temperature = 0,
        max_tokens = 5,
        top_p = 1,
        frequency_penalty = 0,
        presence_penalty = 0,
        stop = None
    )
    return response.choices[0].message.content

evaluation_metrics = {
    "Relevance": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),      
    "Coherence": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),
    "Consistency": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),
    "Fluency": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),
}

# Main code
client = AzureOpenAI(
    azure_endpoint = AZURE_ENDPOINT,
    api_key=AOAI_API_KEY,
    api_version=API_VERSION
)

article_text, nlp_summary, llm_summary = get_article()

summaries = {"NLP Summary (1)": nlp_summary,
             "LLM Summary (2)": llm_summary}

data = {"Evaluation Score": [], "Summary": [], "Score": []}  <span class="aframe-location"/> #5

print("Starting evaluation...")
for eval_type, (criteria, steps) in evaluation_metrics.items():
    for summ_type, summary in summaries.items():               <span class="aframe-location"/> #6
        data["Evaluation Score"].append(eval_type)                 #6
        data["Summary"].append(summ_type)                          #6
        result = get_geval_score(criteria, steps, article_text,    #6
                                 summary, eval_type)               #6 
        numeric_part = ''.join(filter(str.isdigit, 
                          <span class="">↪</span>result.strip()))             <span class="aframe-location"/> #7
        if numeric_part:
            score_num = int(float(result.strip()))                  
          <span class="aframe-location"/>  data["Score"].append(score_num)                         #8

max_values = {key: max(values) for key, values in data.items()}

df = pd.DataFrame(data)                            <span class="aframe-location"/> #9

pivot_df = df.pivot(index='Evaluation Score',                 <span class="aframe-location"/> #10
                    columns='Summary',
                    values='Score')

print(pivot_df)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Function to load the files from disk
     <br/>#2 Function to calculate various evaluation metrics
     <br/>#3 Sets up the prompt for G-Eval
     <br/>#4 Completion API to run the evaluation
     <br/>#5 Dictionary to store the evaluation results
     <br/>#6 Loops over the evaluation metrics and summaries
     <br/>#7 Checks if result is not empty and if it is a number
     <br/>#8 Evaluation result stored in a dictionary
     <br/>#9 Converts the dictionary to a Pandas DataFrame
     <br/>#10 Pivots the DataFrame to allow easy visualization
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>The output of this is as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p91"> 
   <div class="code-area-container"> 
    <pre class="code-area">Starting evaluation...
Summary          LLM Summary (2) NLP Summary (1)
Evaluation Score
Coherence                     5*              1
Consistency                   5*              4
Fluency                       3               2
Relevance                     5*              2</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>Let’s see how we interpret these results and what these scores mean:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p93"> Coherence measures how logically and smoothly ideas transition from one sentence to another. A score of 5 for the LLM summary indicates it presents information logically, is well-organized, and is easy for readers to follow. The traditional NLP summary, with a score of 1, likely struggles with disjointed ideas or lacks logical flow, which makes it difficult for readers to understand the sequence or connection of thoughts. </li> 
   <li class="readable-text" id="p94"> Consistency relates to the absence of contradictions within the text and maintaining the same standards throughout the summary. The LLM’s high score of 5 suggests it maintains a uniform tone, style, and factual accuracy. While good, the traditional NLP’s score of 4 indicates minor problems with maintaining these elements uniformly. </li> 
   <li class="readable-text" id="p95"> Fluency assesses the text’s smoothness and the language’s naturalness. A score of 3 for the LLM indicates moderate fluency; the language is generally clear but might have some awkward phrasing or complexity that could impede readability. The traditional NLP, scoring lower, might exhibit more significant problems such as grammatical errors or unnatural sentence structures. </li> 
   <li class="readable-text" id="p96"> Relevance measures how well the summary addresses the main points and purpose of the original content. The LLM’s score of 5 suggests that it effectively captures and focuses on the key elements of the original text, providing a summary that meets the informational needs of the reader. The traditional NLP, with a score of 2, likely includes some relevant information but misses important details or includes irrelevant content. </li> 
  </ul> 
  <div class="readable-text" id="p97"> 
   <p>As we have seen, assessing LLMs requires more than traditional tasks and includes more difficult benchmarks that measure higher-level understanding, logic, and adaptation skills. Some of these benchmarks, such as HELM, HEIM, HellaSWAG, and MMLU (Massive Multitask Language Understanding), are notable for their difficulty and scope.</p> 
  </div> 
  <div class="readable-text" id="p98"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_218"><span class="num-string">12.3.3</span> HELM</h3> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>HELM (Holistic Evaluation of Language Models) [6] is a holistic framework for evaluating foundational models introduced by Stanford University. The framework aims to comprehensively assess language models, focusing on their abilities, limitations, and associated risks. Developed to enhance model transparency, HELM offers a more detailed understanding of model performance across diverse scenarios. It categorizes the extensive range of potential scenarios and metrics relevant to language models. A subset of these scenarios and metrics is then evaluated based on their coverage and practicality, ensuring that HELM is a practical and useful tool for enterprises.</p> 
  </div> 
  <div class="readable-text intended-text" id="p100"> 
   <p>The HELM approach uses a multimetric evaluation, assessing various factors such as accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency for each chosen scenario. It consists of large-scale evaluations of different language models performed under standardized conditions to guarantee comparability. Moreover, HELM promotes openness by sharing all model prompts and completions with the public for further analysis. This is supplemented by a modular toolkit that facilitates continuous benchmarking within the community.</p> 
  </div> 
  <div class="readable-text intended-text" id="p101"> 
   <p>For enterprises considering the use of GenAI language models, HELM provides valuable information to make informed choices. It allows for the comparison of different models on various metrics, aiding in the selection of the most suitable model. Moreover, HELM helps mitigate risks by assessing potential harms such as bias and toxicity, enabling enterprises to identify and address problems before real-world application. The transparency and trust promoted by HELM through access to raw model predictions and evaluation data further enhance understanding and confidence in using LMs within an organization.</p> 
  </div> 
  <div class="readable-text" id="p102"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_219"><span class="num-string">12.3.4</span> HEIM</h3> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>Stanford University introduced a benchmark called Holistic Evaluation of Text-To-Image Models (HEIM) [7] to provide a complete quantitative analysis of the strengths and weaknesses of text-to-image models. Unlike other evaluations measuring text–image alignment and image quality, HEIM examines 12 important aspects of using models in the real world. Some of these aspects are aesthetics, originality, reasoning, knowledge, bias, toxicity, and efficiency.</p> 
  </div> 
  <div class="readable-text intended-text" id="p104"> 
   <p>HEIM takes a holistic approach to evaluating the text-to-image models by curating 62 scenarios. This holistic approach reveals that no single model excels in all areas, highlighting different strengths and weaknesses among various models.</p> 
  </div> 
  <div class="readable-text intended-text" id="p105"> 
   <p>HEIM should be a key evaluation criterion for enterprises, as it provides a transparent and standardized way to assess text-to-image models. By understanding the strengths and limitations of these models, enterprises can make informed decisions about which models to use for specific tasks or services. Moreover, the evaluation helps identify potential risks such as bias or toxicity, which could have legal and reputational implications for businesses. Consequently, for enterprises building and deploying GenAI applications to production, the HEIM benchmark offers valuable insights across the following four dimensions:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p106"> <em>Model selection</em><em> </em>—HEIM highlights that no single model excels in all aspects. Enterprises must carefully evaluate and select models based on their application’s specific requirements. For example, applications focused on artistic creation might prioritize aesthetics and originality, while those requiring factual accuracy might focus on alignment and knowledge. </li> 
   <li class="readable-text" id="p107"> <em>Risk mitigation</em><em> </em>—HEIM emphasizes evaluating bias, toxicity, and fairness. Enterprises must ensure their applications are ethically sound and avoid perpetuating harmful stereotypes or generating inappropriate content. This necessitates careful model selection, fine-tuning, and implementation of safety measures. </li> 
   <li class="readable-text" id="p108"> <em>Performance optimization</em><em> </em>—Evaluating reasoning, robustness, and multilinguality is crucial for ensuring application reliability and user satisfaction. Enterprises must select models that perform well across diverse scenarios and user inputs. </li> 
   <li class="readable-text" id="p109"> <em>Efficiency considerations</em><em> </em>—Image generation efficiency affects user experience and operational costs. Enterprises should consider the tradeoffs between model size, speed, and resource requirements when selecting and deploying models. </li> 
  </ul> 
  <div class="readable-text" id="p110"> 
   <p>We will show an example and explain how we can apply HEIM to think about models. As mentioned before, HEIM assesses text-to-image models by creating scenarios that cover the 12 aspects it measures. If we wanted to test one of those 12 aspects—the text–image alignment aspect—HEIM might use a scenario where the model gets a complicated textual prompt and then checks how well the image it generates matches the prompt details and context. Here’s a possible evaluation scenario:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p111"> <em>Prompt</em><em> </em>—“A futuristic cityscape at dusk with flying cars and neon signs reflecting in the water below.” </li> 
   <li class="readable-text" id="p112"> <em>Model generation</em><em> </em>—The model generates an image based on the prompt. </li> 
   <li class="readable-text" id="p113"> <em>Evaluation</em><em> </em>—Human evaluators or automated metrics assess the image on various factors: 
    <ul> 
     <li> Does the image accurately depict a cityscape at dusk? </li> 
     <li> Are there flying cars and neon signs as described? </li> 
     <li> Is there a reflection in the water, and how realistic does it look? </li> 
     <li> Overall, how well does the image align with the prompt? </li> 
    </ul></li> 
  </ul> 
  <div class="readable-text" id="p114"> 
   <p>The model’s performance in this scenario would contribute to its overall score in text-image alignment. Similar scenarios would be created for other aspects, such as image quality, originality, reasoning, and so forth. The comprehensive evaluation across all 12 aspects provides insights into the model’s capabilities and limitations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>HEIM’s approach ensures that models are evaluated on their ability to generate visually appealing images and their understanding of the text, creativity, and potential biases or ethical concerns. This holistic evaluation is crucial for enterprises, as it helps them choose models that align with their values and needs, while being aware of the risks involved.</p> 
  </div> 
  <div class="readable-text intended-text" id="p116"> 
   <p>More details on HEIM, including the leaderboard, dataset, and other dependencies such as model access unified APIs, can be found at <a href="https://crfm.stanford.edu/helm/heim/latest">https://crfm.stanford.edu/helm/heim/latest</a>.</p> 
  </div> 
  <div class="readable-text" id="p117"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_220"><span class="num-string">12.3.5</span> HellaSWAG</h3> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>HellaSWAG [8] is a challenging benchmark that tests AI models’ common-sense reasoning skills. It improves on its previous version, SWAG, by adding a more diverse and complex set of multiple-choice questions that require the models to do more than just language processing.</p> 
  </div> 
  <div class="readable-text intended-text" id="p119"> 
   <p>HellaSWAG is a task in which each question presents a scenario with four possible endings, and the LLM must choose the most fitting ending among the options. For instance, the following question is from HellaSWAG’s dataset. The question consists of the context given to the LLM and the four options, which are the possible endings for that context. Only one of these options makes sense with common-sense reasoning. In this example, option C is highlighted:</p> 
  </div> 
  <div class="readable-text list-body-item" id="p120"> 
   <p>A woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She…</p> 
  </div> 
  <div class="readable-text list-body-item" id="p121"> 
   <p>A. rinses the bucket off with soap and blow-dries the dog’s head.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p122"> 
   <p>B. uses a hose to keep it from getting soapy.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p123"> 
   <p>C. gets the dog wet, then it runs away again.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p124"> 
   <p>D. gets into a bathtub with the dog.</p> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>The model’s selections are compared to the correct answers to measure its performance. This testing method evaluates the LLM’s knowledge of language nuances and its deeper comprehension of common-sense logic and the complexities of real-world situations. Doing well in HellaSWAG means that a model has a nuanced understanding and reasoning ability, which is essential for applications that need a sophisticated grasp of context and logic.</p> 
  </div> 
  <div class="readable-text intended-text" id="p126"> 
   <p>If interested, the HellaSWAG’s dataset is available online via HuggingFace at <a href="https://huggingface.co/datasets/Rowan/hellaswag">https://huggingface.co/datasets/Rowan/hellaswag</a>.</p> 
  </div> 
  <div class="readable-text" id="p127"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_221"><span class="num-string">12.3.6</span> Massive Multitask Language Understanding</h3> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>The Massive Multitask Language Understanding (MMLU) [9] benchmark assesses the breadth and depth of an LLM’s knowledge on various topics and domains. MMLU stands out by covering hundreds of tasks linked to different knowledge areas, from science and literature to history and social sciences.</p> 
  </div> 
  <div class="readable-text intended-text" id="p129"> 
   <p>MMLU was developed in response to the observation that while many language models excelled at NLP tasks, they struggled with natural language understanding (NLU). Previous benchmarks such as GLUE and SuperGLUE were quickly mastered by LLMs, indicating a need for more rigorous testing. MMLU aimed to fill this gap by testing language understanding and problem-solving abilities using knowledge encountered during training.</p> 
  </div> 
  <div class="readable-text intended-text" id="p130"> 
   <p>The benchmark includes questions from various subjects, including humanities, social sciences, hard sciences, and other specialized areas, varying from elementary to advanced professional levels. This approach was unique because most NLU benchmarks at the time focused on elementary knowledge. MMLU sought to push the boundaries by testing specialized knowledge, a new challenge for LLMs2.</p> 
  </div> 
  <div class="readable-text intended-text" id="p131"> 
   <p>Like HellaSWAG, MMLU often uses a multiple-choice format where the model must identify the right answer from a set of options. The overall accuracy across these diverse tasks shows the model’s general performance, comprehensively measuring its language understanding and knowledge application across domains. High performance on MMLU means that a model has a large amount of information and is skilled at using this knowledge to answer questions and problems correctly. This wide range of understanding is essential for developing LLMs that can handle the complexities of human knowledge and language subtly and informatively.</p> 
  </div> 
  <div class="readable-text intended-text" id="p132"> 
   <p>While comprehensive, the MMLU faces several limitations. First, the performance of language models on this test can be constrained by the diversity and quality of their training data. If certain topics are underrepresented, models may underperform in those areas. Additionally, MMLU primarily assesses whether models can generate correct answers but does not evaluate how they arrive at these conclusions. It is crucial in applications where the reasoning process is as important as the outcome.</p> 
  </div> 
  <div class="readable-text intended-text" id="p133"> 
   <p>Another significant concern is the potential for bias within the test. Because MMLU is constructed from various sources, it may inadvertently include biases from these materials, affecting the fairness of model assessments, especially on sensitive topics. Furthermore, there is a risk that models could be overfitting to the specific MMLU format and style, optimizing for test performance rather than genuine understanding and applicability in real-world scenarios.</p> 
  </div> 
  <div class="readable-text intended-text" id="p134"> 
   <p>Moreover, the logistical demands of running such a comprehensive test are substantial, requiring significant computational resources that might not be available to all researchers. This limitation can restrict the range of insights gleaned from the test. Finally, the scalability of knowledge poses a challenge; as fields evolve, the test must be updated regularly to stay relevant, necessitating ongoing resource investment. These factors highlight the complexities of using MMLU as a benchmark and underscore the need for continuous refinement to maintain its efficacy and relevance.</p> 
  </div> 
  <div class="readable-text" id="p135"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_222"><span class="num-string">12.3.7</span> Using Azure AI Studio for evaluations</h3> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>As an Azure customer, you can easily use these evaluations, as they are already integrated into Azure AI Studio. It includes tools for recording, seeing, and exploring detailed evaluation metrics, with the option to use custom evaluation flows and batch runs without evaluation. With AI Studio, we can make an evaluation run from a test dataset or flow with ready-made evaluation metrics. For more adaptability, we can make our evaluation flow.</p> 
  </div> 
  <div class="readable-text intended-text" id="p137"> 
   <p>Before you can use AI-assisted metrics to evaluate your model, make sure you have these things prepared:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p138"> A test dataset in either CSV or JSONL format. If you don’t have a dataset ready, you can also enter data by hand from the UI. </li> 
   <li class="readable-text" id="p139"> One of these models deployed: GPT-3.5 models, GPT-4 models, or Davinci models. </li> 
   <li class="readable-text" id="p140"> A compute instance runtime to run the evaluation. </li> 
  </ul> 
  <div class="readable-text" id="p141"> 
   <p>Figure 12.1 shows an example of how to set up the various tests. More details can be found at <a href="https://mng.bz/4pMj">https://mng.bz/4pMj</a>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p142"> 
   <p>If you are not using Azure, other similar options exist, such as DeepEval (see the next section). This open source LLM evaluation framework allows running multiple LLM metrics and makes this process quite easy.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p143">  
   <img alt="figure" src="../Images/CH12_F01_Bahree.png" width="1012" height="826"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.1</span> Azure AI Studio evaluations<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p144"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_223"><span class="num-string">12.3.8</span> DeepEval: An LLM evaluation framework</h3> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>DeepEval is a free LLM evaluation framework that works like <code>pytest</code> (a popular testing framework for Python) but focuses on unit-testing LLM outputs. DeepEval uses the newest research to assess LLM outputs based on metrics such as hallucination, answer relevancy, and RAGAS (Retrieval Augmented Generation Assessment). These metrics rely on LLMs and other NLP models that run on your computer locally for evaluation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>DeepEval supports many useful features for enterprise applications. It can evaluate whole datasets simultaneously, create custom metrics, compare any LLM to popular benchmarks, and evaluate in real-time in production. In addition, it works with tools such as LlamaIndex and Hugging Face and is automatically connected to Confident AI for ongoing evaluation of your LLM application throughout its lifetime.</p> 
  </div> 
  <div class="readable-text intended-text" id="p147"> 
   <p>The framework also offers a platform for recording test outcomes, measuring metrics’ passes/fails, selecting and comparing the best hyperparameters, organizing evaluation test cases/datasets, and monitoring live LLM responses in production. This book does not cover DeepEval in detail; more details are available at their GitHub repository: <a href="https://github.com/confident-ai/deepeval">https://github.com/confident-ai/deepeval</a>. Figure 12.2 shows a simple example of a test metric.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p148">  
   <img alt="figure" src="../Images/CH12_F02_Bahree.png" width="1020" height="613"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.2</span> DeepEval test session example</h5>
  </div> 
  <div class="readable-text" id="p149"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_224"><span class="num-string">12.4</span> New evaluation benchmarks</h2> 
  </div> 
  <div class="readable-text" id="p150"> 
   <p>Over the last 12 to 18 months, we have seen that AI models have reached performance saturation on established industry benchmarks such as ImageNet, SQuAD, and SuperGLUE, to name a few. This has spurred the industry to develop more challenging benchmarks. Some of the newer ones are SWE-bench for coding, MMMU for general reasoning, MoCa for moral reasoning, and HaluEval for hallucinations.</p> 
  </div> 
  <div class="readable-text" id="p151"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_225"><span class="num-string">12.4.1</span> SWE-bench</h3> 
  </div> 
  <div class="readable-text" id="p152"> 
   <p>To measure the progress of GenAI systems that can code, we need more difficult tasks to evaluate them. SWE-bench [10] is a dataset containing nearly hundreds of software engineering problems from real-world GitHub and Python repositories. It poses a harder challenge for AI coding skills, requiring that systems make changes across multiple functions, deal with different execution environments, and perform complex reasoning.</p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>The SWE-bench dataset evaluates systems’ abilities to solve GitHub problems automatically. It collects 2,294 problem-pull request pairs from 12 popular Python repositories. The evaluation is performed by verifying the proposed solutions using unit tests and comparing them to the post-PR behavior as the reference solution.</p> 
  </div> 
  <div class="readable-text intended-text" id="p154"> 
   <p>The primary evaluation metric for SWE-bench is the <em>percentage of resolved task instances.</em> In other words, it measures how effectively a model can address the given problems—the higher the percentage of resolved instances, the better the model’s performance. More details on SWE-bench can be found at <a href="https://www.swebench.com">https://www.swebench.com</a>.</p> 
  </div> 
  <div class="readable-text" id="p155"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_226"><span class="num-string">12.4.2</span> MMMU</h3> 
  </div> 
  <div class="readable-text" id="p156"> 
   <p>MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark) [9] is a new benchmark designed to evaluate multimodal models’ capabilities on tasks requiring college-level subject knowledge and expert-level reasoning across multiple disciplines. It includes 11.5K multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: art and design, business, science, health and medicine, humanities and social science, and tech and engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures.</p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>MMMU is unique because it focuses on advanced perception and reasoning with domain-specific knowledge and challenging models to perform tasks that experts face. The benchmark has been used to evaluate several open source LLMs and proprietary models such as GPT-4V, highlighting the substantial challenges MMMU poses. Even the advanced models only achieve accuracies between 56% and 59%, indicating significant room for improvement. It operates by assessing LLMs’ ability to perceive, understand, and reason across different disciplines and subfields using various image types. The benchmark focuses on three essential skills in LLMs: perception, knowledge, and reasoning.</p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>Note that it might seem that the MMLU discussed earlier is the same as MMMU; however, they are different. MMLU evaluates language models on a wide range of text-based tasks across various domains, focusing solely on language understanding. In contrast, MMMU assesses multimodal models, requiring both visual and textual comprehension across specialized disciplines, thus challenging models with complex, domain-specific multimodal content.</p> 
  </div> 
  <div class="readable-text intended-text" id="p159"> 
   <p>The MMMU benchmark presents several key challenges for multimodal models, which include</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p160"> <em>Comprehensiveness</em><em> </em>—Since the benchmark includes a wide array of 11.5K college-level problems across broad disciplines, the models must have a broad knowledge base and understanding across multiple fields. </li> 
   <li class="readable-text" id="p161"> <em>Highly heterogeneous image types</em><em> </em>—The questions involve 30 different types of images, such as charts, diagrams, maps, tables, music sheets, and chemical structures. This means the models must be able to interpret and understand various visual information. </li> 
   <li class="readable-text" id="p162"> <em>Interleaved text and images</em><em> </em>—Many questions feature a mix of text and images, requiring models to process and integrate information from both modalities to arrive at the correct answer. </li> 
   <li class="readable-text" id="p163"> <em>Expert-level perception and reasoning</em><em> </em>—The tasks demand deep subject knowledge and expert-level reasoning, akin to the challenges faced by human experts in their respective fields. </li> 
  </ul> 
  <div class="readable-text" id="p164"> 
   <p>These challenges aim to stretch the limits of existing multimodal models, testing their capacity to do sophisticated perception, analytical thinking, and domain-specific reasoning. The questions demand a profound understanding of the topic and the ability to use knowledge in intricate scenarios, even for human experts.</p> 
  </div> 
  <div class="readable-text" id="p165"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_227"><span class="num-string">12.4.3</span> MoCa</h3> 
  </div> 
  <div class="readable-text" id="p166"> 
   <p>The MoCa (Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks) [11] framework evaluates how well LLMs align with human participants in making causal and moral judgments about text-based scenarios. AI models can perform well in language and vision tasks, but their ability to make moral decisions, especially those that match human opinions, is unclear. To investigate this topic, a group of Stanford researchers created a new dataset (MoCa) of human stories with moral aspects. Here, we will look at the details of each:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p167"> <em>Causal judgments</em><em> </em>—Humans intuitively understand events, people, and the world around them by organizing their understanding into intuitive theories. These theories help us reason about how objects and agents interact with one another, including concepts related to causality. The MoCa framework collects a dataset of stories from cognitive science papers and annotates each story with the factors they investigate. It then tests whether LLMs make causal judgments about text scenarios that align with those of humans. On an aggregate level, alignment has improved with more recent LLMs. However, statistical analyses reveal that LLMs weigh various factors in a different way than human participants. </li> 
   <li class="readable-text" id="p168"> <em>Moral judgments</em><em> </em>—Tasks evaluate agents in narrative-like text for moral reasoning. These tasks and datasets vary in structure, ranging from free-form anecdotes to more structured inputs. The MoCa framework assesses how well LLMs align with human moral intuitions in these scenarios. </li> 
  </ul> 
  <div class="readable-text" id="p169"> 
   <p>The main metric used in MoCa is the Area under the Receiver Operating Characteristic (AuROC) curve, which measures the alignment between LLMs and human judgments. Furthermore, accuracy serves as a secondary metric for comparison between models. </p> 
  </div> 
  <div class="readable-text intended-text" id="p170"> 
   <p>A higher score indicates closer alignment with human moral judgment. The study yielded intriguing results. No model perfectly matches human moral systems. However, newer, larger models such as GPT-4 and Claude show greater alignment with human moral sentiments than smaller models such as GPT-3, suggesting that as AI models scale, they are gradually becoming more morally aligned with humans.</p> 
  </div> 
  <div class="readable-text intended-text" id="p171"> 
   <p>In summary, MoCa provides insights into how LLMs handle causal and moral reasoning, shedding light on their implicit tendencies and alignment (or lack thereof) with human intuitions. We can get more details on MoCa at <a href="https://moca-llm.github.io">https://moca-llm.github.io</a>.</p> 
  </div> 
  <div class="readable-text" id="p172"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_228"><span class="num-string">12.4.4</span> HaluEval</h3> 
  </div> 
  <div class="readable-text" id="p173"> 
   <p>HaluEval [12] benchmark is a large-scale evaluation framework designed to assess LLMs’ performance in recognizing hallucinations. In this context, hallucinations refer to LLM-generated content that conflicts with the source or cannot be verified by factual knowledge. The benchmark includes a collection of generated and human-annotated hallucinated samples.</p> 
  </div> 
  <div class="readable-text intended-text" id="p174"> 
   <p>A two-step framework involving sampling-then-filtering is used to create these samples, often based on responses from models such as ChatGPT. Human labelers also contribute by annotating hallucinations in the responses. The empirical results from HaluEval suggest that LLMs, including ChatGPT, can generate hallucinated content, particularly on specific topics, by fabricating unverifiable information.</p> 
  </div> 
  <div class="readable-text intended-text" id="p175"> 
   <p>The study also explores how good current LLMs are at finding hallucinations. It can lead LLMs to spot hallucinations in tasks such as question-answering knowledge-grounded dialogue and text summarization. The results show that many LLMs have difficulties with these tasks, emphasizing that hallucination is a serious, persistent problem.</p> 
  </div> 
  <div class="readable-text intended-text" id="p176"> 
   <p>The HaluEval benchmark includes 5,000 general user queries with ChatGPT responses and 30,000 task-specific examples from three tasks: question answering, knowledge-grounded dialogue, and text summarization. It's a significant step toward understanding and improving the reliability of LLMs in generating accurate and verifiable content. HaluEval’s GitHub repository at <a href="https://github.com/RUCAIBox/HaluEval">https://github.com/RUCAIBox/HaluEval</a> provides more details.</p> 
  </div> 
  <div class="readable-text" id="p177"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_229"><span class="num-string">12.5</span> Human evaluation</h2> 
  </div> 
  <div class="readable-text" id="p178"> 
   <p>Human evaluation plays a crucial role in understanding the quality of LLMs, as it captures nuances, context, and potential biases that automated metrics might overlook. For enterprises to conduct effective human evaluations, they need to start by defining clear criteria to guide the assessment of LLM outputs. These criteria should cover aspects such as accuracy, fluency, relevance, and the presence of any biases. To ensure consistency and objectivity, enterprises should develop comprehensive guidelines and rubrics for evaluators to follow.</p> 
  </div> 
  <div class="readable-text intended-text" id="p179"> 
   <p>When choosing the right evaluation methods, enterprises have several options. They can either engage domain experts or trained annotators for detailed assessments or opt for crowdsourcing platforms such as Amazon Mechanical Turk (<a href="https://www.mturk.com">https://www.mturk.com</a>) to access a wider pool of evaluators. The next step involves data collection and annotation, which requires user-friendly interfaces and clear instructions to ensure quality and consistency. It’s also important to collect enough data to yield statistically significant results.</p> 
  </div> 
  <div class="readable-text intended-text" id="p180"> 
   <p>After collecting data, a thorough analysis is necessary. Enterprises should employ statistical methods to measure interrater agreement and confirm the reliability of the evaluations. The insights derived from this process should then be used to make iterative improvements to LLMs, including adjustments to the training data, model architecture, and prompt engineering. Regular human evaluations are essential for monitoring progress and pinpointing areas that need further improvement.</p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>While human evaluation is invaluable, it does come with its challenges. It can be expensive and time-consuming, especially when dealing with large datasets. There’s also the risk of subjectivity and bias in human judgments. However, these problems can be mitigated by providing clear guidelines, adequate training for evaluators, and employing proper aggregation methods.</p> 
  </div> 
  <div class="readable-text intended-text" id="p182"> 
   <p>Several tools and platforms are available to help streamline the human evaluation process. Crowdsourcing platforms provide access to a diverse workforce, while annotation tools offer efficient data-labeling features. Evaluation frameworks are also available, including libraries of metrics and scripts designed specifically for LLM evaluation and to support human evaluation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p183"> 
   <p>Some examples of tools that assist with annotations are Label Studio (<a href="https://labelstud.io">https://labelstud.io</a>), which offers both open source and enterprise offerings. Prodigy (<a href="https://prodi.gy">https://prodi.gy</a>) is another annotation tool that supports text, images, videos, and audio. Text-only annotation tools also exist, such as Labellerr (<a href="https://www.labellerr.com">https://www.labellerr.com</a>).</p> 
  </div> 
  <div class="readable-text intended-text" id="p184"> 
   <p>Some companies specialize in LLM evaluation, offering robust testing frameworks and various resources to assist the evaluation process. For enterprises that are not comfortable implementing their evaluation frameworks using tools such as PromptFlow, which we saw earlier in the previous chapter, Weights and Biases (<a href="https://wandb.ai">https://wandb.ai</a>), and so forth, there are new companies that specialize in LLM evaluations, such as Giskard (<a href="https://www.giskard.ai">https://www.giskard.ai</a>).</p> 
  </div> 
  <div class="readable-text intended-text" id="p185"> 
   <p>By following these steps and utilizing the available resources, enterprises can implement a structured and effective human evaluation process for LLMs. It’s important to remember that this is an evolving field, and staying up to date on the latest developments and training is crucial for maintaining the quality of evaluations.</p> 
  </div> 
  <div class="readable-text" id="p186"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_230">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p187"> Benchmarking systems are essential for verifying the performance of GenAI and LLMs, directing enhancements, and confirming real-world suitability. They assist us in evaluating the efficiency and preparedness of generative AI and LLMs for deployment in production environments. </li> 
   <li class="readable-text" id="p188"> The correlation between evaluations and LLMs is a new and emerging area. We should use traditional metrics, LLM task-specific benchmarks, and human evaluation to assess LLM performance and ensure its suitability for real-world applications. G-Eval is a reference-free evaluation method using LLMs to assess the generated text’s coherence, consistency, and relevance. </li> 
   <li class="readable-text" id="p189"> Conventional metrics such as BLEU, ROUGE, and BERTScore help measure text generation quality and evaluate text numerically based on n-gram matching or semantic similarity. They do face some challenges in fully representing contextual meaning and paraphrasing. </li> 
   <li class="readable-text" id="p190"> LLM-specific benchmarks measure how well LLMs perform tasks such as text classification, sentiment analysis, and question answering. They introduce new metrics such as groundedness, coherence, fluency, and GPT similarity that help assess the quality of LLM outputs and how close they are to human-like standards. </li> 
   <li class="readable-text" id="p191"> Effective evaluation methods for meaningful LLM evaluations include testing in relevant settings, creating fair prompts, conducting ethical reviews, and assessing the user experience. These include advanced benchmarks such as HELM, HEIM, HellaSWAG, and MMLU, which test LLMs against various scenarios and capabilities. </li> 
   <li class="readable-text" id="p192"> Tools such as Azure AI Studio and the DeepEval framework enable effective LLM evaluations in an enterprise context. These tools allow the development of customized evaluation workflows, batch executions, and the incorporation of real-time evaluations into production settings. </li> 
  </ul>
 </div></div></body></html>