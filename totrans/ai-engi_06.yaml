- en: Chapter 6\. RAG and Agents
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章\. RAG和代理
- en: To solve a task, a model needs both the instructions on how to do it, and the
    necessary information to do so. Just like how a human is more likely to give a
    wrong answer when lacking information, AI models are more likely to make mistakes
    and hallucinate when they are missing context. For a given application, the model’s
    instructions are common to all queries, whereas context is specific to each query.
    The last chapter discussed how to write good instructions to the model. This chapter
    focuses on how to construct the relevant context for each query.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决一个任务，模型需要完成它的指令以及完成它所需的信息。就像人类在没有信息的情况下更有可能给出错误答案一样，当AI模型缺少上下文时，它们更有可能犯错和产生幻觉。对于特定的应用，模型的指令对所有查询都是通用的，而上下文对每个查询都是特定的。上一章讨论了如何为模型编写好的指令。本章重点介绍如何为每个查询构建相关的上下文。
- en: Two dominating patterns for context construction are RAG, or retrieval-augmented
    generation, and agents. The RAG pattern allows the model to retrieve relevant
    information from external data sources. The agentic pattern allows the model to
    use tools such as web search and news APIs to gather information.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 两种主导的上下文构建模式是RAG，即检索增强生成，和代理。RAG模式允许模型从外部数据源检索相关信息。代理模式允许模型使用诸如网络搜索和新闻API等工具来收集信息。
- en: While the RAG pattern is chiefly used for constructing context, the agentic
    pattern can do much more than that. External tools can help models address their
    shortcomings and expand their capabilities. Most importantly, they give models
    the ability to directly interact with the world, enabling them to automate many
    aspects of our lives.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RAG模式主要用于构建上下文，但代理模式可以做得更多。外部工具可以帮助模型克服其不足并扩展其能力。最重要的是，它们赋予模型直接与世界互动的能力，使它们能够自动化我们生活中的许多方面。
- en: Both RAG and agentic patterns are exciting because of the capabilities they
    bring to already powerful models. In a short amount of time, they’ve managed to
    capture the collective imagination, leading to incredible demos and products that
    convince many people that they are the future. This chapter will go into detail
    about each of these patterns, how they work, and what makes them so promising.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 和代理模式之所以令人兴奋，是因为它们为已经强大的模型带来了新的能力。在很短的时间内，它们成功地吸引了公众的集体想象力，产生了令人难以置信的演示和产品，让许多人相信它们是未来。本章将详细介绍这些模式，它们是如何工作的，以及为什么它们如此有前景。
- en: RAG
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG
- en: RAG is a technique that enhances a model’s generation by retrieving the relevant
    information from external memory sources. An external memory source can be an
    internal database, a user’s previous chat sessions, or the internet.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一种通过从外部记忆源检索相关信息来增强模型生成能力的技巧。外部记忆源可以是内部数据库、用户的先前聊天会话或互联网。
- en: The *retrieve-then-generate* pattern was first introduced in “Reading Wikipedia
    to Answer Open-Domain Questions” ([Chen et al., 2017](https://arxiv.org/abs/1704.00051)).
    In this work, the system first retrieves five Wikipedia pages most relevant to
    a question, then a model^([1](ch06.html#id1227)) uses, or reads, the information
    from these pages to generate an answer, as visualized in [Figure 6-1](#ch06_figure_1_1730157386529219).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*检索后生成*模式首次在“通过阅读维基百科来回答开放域问题”中提出（[Chen等人，2017](https://arxiv.org/abs/1704.00051)）。在这项工作中，系统首先检索与问题最相关的五个维基百科页面，然后模型^([1](ch06.html#id1227))使用或读取这些页面上的信息来生成答案，如图[图6-1](#ch06_figure_1_1730157386529219)所示。'
- en: '![A diagram of a document'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![文档的示意图'
- en: Description automatically generated](assets/aien_0601.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动生成的描述](assets/aien_0601.png)'
- en: Figure 6-1\. The retrieve-then-generate pattern. The model was referred to as
    the *document reader*.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 检索后生成模式。该模型被称为*文档阅读器*。
- en: The term retrieval-augmented generation was coined in “Retrieval-Augmented Generation
    for Knowledge-Intensive NLP Tasks” ([Lewis et al., 2020](https://arxiv.org/abs/2005.11401)).
    The paper proposed RAG as a solution for knowledge-intensive tasks where all the
    available knowledge can’t be input into the model directly. With RAG, only the
    information most relevant to the query, as determined by the retriever, is retrieved
    and input into the model. Lewis et al. found that having access to relevant information
    can help the model generate more detailed responses while reducing hallucinations.^([2](ch06.html#id1228))
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “检索增强生成”（Retrieval-Augmented Generation）这一术语是在“Retrieval-Augmented Generation
    for Knowledge-Intensive NLP Tasks”一文中提出的（[Lewis et al., 2020](https://arxiv.org/abs/2005.11401)）。该论文提出RAG作为解决知识密集型任务的解决方案，在这些任务中，所有可用的知识都无法直接输入到模型中。通过RAG，只有由检索器确定的与查询最相关的信息被检索并输入到模型中。Lewis等人发现，能够访问相关信息可以帮助模型生成更详细的回应，同时减少幻觉。（^([2](ch06.html#id1228)))
- en: For example, given the query “Can Acme’s fancy-printer-A300 print 100pps?”,
    the model will be able to respond better if it’s given the specifications of fancy-printer-A300.^([3](ch06.html#id1229))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定查询“Acme的fancy-printer-A300打印机能否以100pps的速度打印？”，如果模型能够获得fancy-printer-A300打印机的规格，它将能够给出更好的回应。（^([3](ch06.html#id1229)))
- en: You can think of RAG as a technique to construct context specific to each query,
    instead of using the same context for all queries. This helps with managing user
    data, as it allows you to include data specific to a user only in queries related
    to this user.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将RAG视为一种构建针对每个查询特定上下文的技巧，而不是为所有查询使用相同的上下文。这有助于管理用户数据，因为它允许你只将特定于用户的仅在涉及该用户的查询中包含数据。
- en: 'Context construction for foundation models is equivalent to feature engineering
    for classical ML models. They serve the same purpose: giving the model the necessary
    information to process an input.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的上下文构建相当于经典机器学习模型的特征工程。它们服务于相同的目的：为模型提供处理输入所需的信息。
- en: In the early days of foundation models, RAG emerged as one of the most common
    patterns. Its main purpose was to overcome the models’ context limitations. Many
    people think that a sufficiently long context will be the end of RAG. I don’t
    think so. First, no matter how long a model’s context length is, there will be
    applications that require context longer than that. After all, the amount of available
    data only grows over time. People generate and add new data but rarely delete
    data. Context length is expanding quickly, but not fast enough for the data needs
    of arbitrary applications.^([4](ch06.html#id1230))
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础模型发展的早期阶段，RAG（Retrieval-Augmented Generation，检索增强生成）成为最常见模式之一。其主要目的是克服模型在上下文上的限制。许多人认为，足够长的上下文将是RAG的终结。我不这么认为。首先，无论模型上下文的长度有多长，总会有需要比这更长上下文的应用。毕竟，可用的数据量只会随着时间的推移而增长。人们生成并添加新的数据，但很少删除数据。上下文长度正在迅速扩展，但不足以满足任意应用的数据需求。（^([4](ch06.html#id1230)))
- en: Second, a model that can process long context doesn’t necessarily use that context
    well, as discussed in [“Context Length and Context Efficiency”](ch05.html#ch05a_context_length_and_context_efficiency_1730156991195850).
    The longer the context, the more likely the model is to focus on the wrong part
    of the context. Every extra context token incurs extra cost and has the potential
    to add extra latency. RAG allows a model to use only the most relevant information
    for each query, reducing the number of input tokens while potentially increasing
    the model’s performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，正如在“上下文长度与上下文效率”（[“Context Length and Context Efficiency”](ch05.html#ch05a_context_length_and_context_efficiency_1730156991195850)）中讨论的那样，一个能够处理长上下文的模型并不一定能够很好地使用该上下文。上下文越长，模型越有可能关注上下文的错误部分。每个额外的上下文标记都会产生额外的成本，并有可能增加额外的延迟。RAG允许模型只为每个查询使用最相关的信息，从而在减少输入标记数量的同时，可能提高模型的表现。
- en: Efforts to expand context length are happening in parallel with efforts to make
    models use context more effectively. I wouldn’t be surprised if a model provider
    incorporates a retrieval-like or attention-like mechanism to help a model pick
    out the most salient parts of a context to use.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在努力扩展上下文长度的同时，也在努力使模型更有效地使用上下文。如果模型提供商整合了类似检索或注意力的机制来帮助模型挑选出上下文中最显著的部分进行使用，我并不会感到惊讶。
- en: Note
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Anthropic suggested that for Claude models, if “your knowledge base is smaller
    than 200,000 tokens (about 500 pages of material), you can just include the entire
    knowledge base in the prompt that you give the model, with no need for RAG or
    similar methods” ([Anthropic, 2024](https://oreil.ly/v-T_4)). It’d be amazing
    if other model developers provide similar guidance for RAG versus long context
    for their models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic 建议，对于 Claude 模型，如果“你的知识库小于 200,000 个标记（约 500 页材料），你只需将整个知识库包含在你提供给模型的提示中，无需
    RAG 或类似方法” ([Anthropic, 2024](https://oreil.ly/v-T_4))。如果其他模型开发者能为他们的模型提供类似的关于
    RAG 与长上下文之间的指导，那就太棒了。
- en: RAG Architecture
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG 架构
- en: 'A RAG system has two components: a retriever that retrieves information from
    external memory sources and a generator that generates a response based on the
    retrieved information. [Figure 6-2](#ch06_figure_2_1730157386529231) shows a high-level
    architecture of a RAG system.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 系统有两个组件：一个检索器，用于从外部内存源检索信息，和一个生成器，根据检索到的信息生成响应。[图 6-2](#ch06_figure_2_1730157386529231)
    展示了 RAG 系统的高级架构。
- en: '![A diagram of a computer program'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机程序图'
- en: Description automatically generated](assets/aien_0602.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0602.png)
- en: Figure 6-2\. A basic RAG architecture.
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 基本的 RAG 架构。
- en: In the original RAG paper, [Lewis et al.](https://arxiv.org/abs/2005.11401)
    trained the retriever and the generative model together. In today’s RAG systems,
    these two components are often trained separately, and many teams build their
    RAG systems using off-the-shelf retrievers and models. However, finetuning the
    whole RAG system end-to-end can improve its performance significantly.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始 RAG 论文中，[Lewis 等人](https://arxiv.org/abs/2005.11401) 一起训练了检索器和生成模型。在今天的
    RAG 系统中，这两个组件通常分别训练，许多团队使用现成的检索器和模型构建他们的 RAG 系统。然而，端到端微调整个 RAG 系统可以显著提高其性能。
- en: 'The success of a RAG system depends on the quality of its retriever. A retriever
    has two main functions: indexing and querying. Indexing involves processing data
    so that it can be quickly retrieved later. Sending a query to retrieve data relevant
    to it is called querying. How to index data depends on how you want to retrieve
    it later on.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 系统的成功取决于其检索器的质量。检索器有两个主要功能：索引和查询。索引涉及处理数据，以便可以快速检索。向检索器发送查询以检索与其相关的数据称为查询。如何索引数据取决于你希望如何稍后检索它。
- en: Now that we’ve covered the primary components, let’s consider an example of
    how a RAG system works. For simplicity, let’s assume that the external memory
    is a database of documents, such as a company’s memos, contracts, and meeting
    notes. A document can be 10 tokens or 1 million tokens. Naively retrieving whole
    documents can cause your context to be arbitrarily long. To avoid this, you can
    split each document into more manageable chunks. Chunking strategies will be discussed
    later in this chapter. For now, let’s assume that all documents have been split
    into workable chunks. For each query, our goal is to retrieve the data chunks
    most relevant to this query. Minor post-processing is often needed to join the
    retrieved data chunks with the user prompt to generate the final prompt. This
    final prompt is then fed into the generative model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了主要组件，让我们考虑一个 RAG 系统如何工作的例子。为了简单起见，让我们假设外部内存是一个文档数据库，例如公司的备忘录、合同和会议记录。一个文档可以是
    10 个标记或 1,000,000 个标记。天真地检索整个文档可能导致你的上下文任意长。为了避免这种情况，你可以将每个文档分成更易于管理的块。本章后面将讨论块化策略。现在，让我们假设所有文档都已分成可操作的块。对于每个查询，我们的目标是检索与该查询最相关的数据块。通常需要一些轻微的后处理来将检索到的数据块与用户提示连接起来，以生成最终的提示。然后，将这个最终的提示输入到生成模型中。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this chapter, I use the term “document” to refer to both “document” and “chunk”,
    because technically, a chunk of a document is also a document. I do this to keep
    this book’s terminologies consistent with classical NLP and information retrieval
    (IR) terminologies.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我使用“文档”一词来指代“文档”和“块”，因为从技术上讲，文档的块也是一个文档。我这样做是为了使本书的术语与经典自然语言处理（NLP）和信息检索（IR）术语保持一致。
- en: Retrieval Algorithms
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索算法
- en: Retrieval isn’t unique to RAG. Information retrieval is a century-old idea.^([5](ch06.html#id1238))
    It’s the backbone of search engines, recommender systems, log analytics, etc.
    Many retrieval algorithms developed for traditional retrieval systems can also
    be used for RAG. For instance, information retrieval is a fertile research area
    with a large supporting industry that can hardly be sufficiently covered within
    a few pages. Accordingly, this section will cover only the broad strokes. See
    this book’s [GitHub repository](https://oreil.ly/aie-book) for more in-depth resources
    on information retrieval.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 检索并不特指RAG。信息检索是一个有着一个世纪历史的理念.^([5](ch06.html#id1238)) 它是搜索引擎、推荐系统、日志分析等系统的骨架。许多为传统检索系统开发的检索算法也可以用于RAG。例如，信息检索是一个充满活力、拥有庞大支持产业的领域，其内容很难在几页纸内充分涵盖。因此，本节将仅概述主要观点。有关信息检索的更深入资源，请参阅本书的[GitHub仓库](https://oreil.ly/aie-book)。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Retrieval is typically limited to one database or system, whereas search involves
    retrieval across various systems. This chapter uses retrieval and search interchangeably.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 检索通常限于一个数据库或系统，而搜索则涉及跨多个系统的检索。本章将检索和搜索互换使用。
- en: 'At its core, retrieval works by ranking documents based on their relevance
    to a given query. Retrieval algorithms differ based on how relevance scores are
    computed. I’ll start with two common retrieval mechanisms: term-based retrieval
    and embedding-based retrieval.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，检索是通过根据文档与给定查询的相关性对文档进行排序来工作的。检索算法的不同之处在于如何计算相关性分数。我将从两种常见的检索机制开始：基于术语的检索和基于嵌入的检索。
- en: Term-based retrieval
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于术语的检索
- en: 'Given a query, the most straightforward way to find relevant documents is with
    keywords. Some people call this approach *lexical retrieval*. For example, given
    the query “AI engineering”, the model will retrieve all the documents that contain
    “AI engineering”. However, this approach has two problems:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个查询，找到相关文档的最直接方法是使用关键词。有些人称这种方法为*词汇检索*。例如，给定查询“AI engineering”，模型将检索包含“AI
    engineering”的所有文档。然而，这种方法有两个问题：
- en: Many documents might contain the given term, and your model might not have sufficient
    context space to include all of them as context. A heuristic is to include the
    documents that contain the term the greatest number of times. The assumption is
    that the more a term appears in a document, the more relevant this document is
    to this term. The number of times a term appears in a document is called *term
    frequency* (TF).
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多文档可能包含给定的术语，而你的模型可能没有足够的空间来包含所有这些文档作为上下文。一个启发式方法是包括包含该术语次数最多的文档。假设是，一个术语在文档中出现的次数越多，这个文档对这个术语就越相关。一个术语在文档中出现的次数称为*词频*（TF）。
- en: 'A prompt can be long and contain many terms. Some are more important than others.
    For example, the prompt “Easy-to-follow recipes for Vietnamese food to cook at
    home” contains nine terms: *easy-to-follow, recipes, for, vietnamese, food, to,
    cook, at, home*. You want to focus on more informative terms like *vietnamese*
    and *recipes*, not *for* and *at*. You need a way to identify important terms.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个提示可能很长，包含许多术语。其中一些比其他更重要。例如，提示“Easy-to-follow recipes for Vietnamese food
    to cook at home”包含九个术语：*easy-to-follow, recipes, for, vietnamese, food, to, cook,
    at, home*。你想要关注像*vietnamese*和*recipes*这样的更具信息量的术语，而不是*for*和*at*。你需要一种方法来识别重要术语。
- en: An intuition is that the more documents contain a term, the less informative
    this term is. “For” and “at” are likely to appear in most documents, hence, they
    are less informative. So a term’s importance is inversely proportional to the
    number of documents it appears in. This metric is called *inverse document frequency*
    (IDF). To compute IDF for a term, count all the documents that contain this term,
    then divide the total number of documents by this count. If there are 10 documents
    and 5 of them contain a given term, then the IDF of this term is 10 / 5 = 2\.
    The higher a term’s IDF, the more important it is.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个直观的想法是，包含一个术语的文档越多，这个术语就越不具信息量。“对于”和“在”可能出现在大多数文档中，因此它们的信息量较小。所以一个术语的重要性与它在文档中出现的次数成反比。这个度量称为*逆文档频率*（IDF）。要计算一个术语的IDF，首先计算包含该术语的所有文档的数量，然后将总文档数除以这个数量。如果有10个文档，其中5个包含一个特定的术语，那么这个术语的IDF是10
    / 5 = 2。一个术语的IDF越高，它就越重要。
- en: 'TF-IDF is an algorithm that combines these two metrics: term frequency (TF)
    and inverse document frequency (IDF). Mathematically, the TF-IDF score of document
    *D* for the query *Q* is computed as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF是一种结合这两个度量标准的算法：词频（TF）和逆文档频率（IDF）。从数学上讲，对于查询 *Q* 的文档 *D* 的TF-IDF得分计算如下：
- en: Let $t 1 comma t 2 comma period period period comma t Subscript q Baseline$
    be the terms in the query *Q*.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设 $t 1 comma t 2 comma period period period comma t Subscript q Baseline$ 为查询
    *Q* 中的词。
- en: Given a term *t*, the term frequency of this term in the document *D* is *f(t,
    D)*.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个词 *t*，该词在文档 *D* 中的词频是 *f(t, D)*。
- en: Let *N* be the total number of documents, and *C(t)* be the number of documents
    that contain *t*. The IDF value of the term *t* can be written as $IDF left-parenthesis
    t right-parenthesis equals log StartFraction upper N Over upper C left-parenthesis
    t right-parenthesis EndFraction$ .
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设 *N* 为文档的总数，*C(t)* 为包含 *t* 的文档数量。词 *t* 的IDF值可以表示为 $IDF left-parenthesis t right-parenthesis
    equals log StartFraction upper N Over upper C left-parenthesis t right-parenthesis
    EndFraction$ .
- en: Naively, the TF-IDF score of a document *D* with respect to *Q* is defined as
    $Score left-parenthesis upper D comma upper Q right-parenthesis equals sigma-summation
    Underscript i equals 1 Overscript q Endscripts IDF left-parenthesis t Subscript
    i Baseline right-parenthesis times f left-parenthesis t Subscript i Baseline comma
    upper D right-parenthesis$ .
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于文档 *D* 相对于查询 *Q* 的TF-IDF得分，直观地定义为 $Score left-parenthesis upper D comma upper
    Q right-parenthesis equals sigma-summation Underscript i equals 1 Overscript q
    Endscripts IDF left-parenthesis t Subscript i Baseline right-parenthesis times
    f left-parenthesis t Subscript i Baseline comma upper D right-parenthesis$ .
- en: Two common term-based retrieval solutions are Elasticsearch and BM25\. [Elasticsearch](https://github.com/elastic/elasticsearch)
    (Shay Banon, 2010), built on top of [Lucene](https://github.com/apache/lucene),
    uses a data structure called an inverted index. It’s a dictionary that maps from
    terms to documents that contain them. This dictionary allows for fast retrieval
    of documents given a term. The index might also store additional information such
    as the term frequency and the document count (how many documents contain this
    term), which are helpful for computing TF-IDF scores. [Table 6-1](#ch06_table_1_1730157386543390)
    illustrates an inverted index.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 两种常见的基于词的检索解决方案是Elasticsearch和BM25。 [Elasticsearch](https://github.com/elastic/elasticsearch)（Shay
    Banon，2010），建立在 [Lucene](https://github.com/apache/lucene) 之上，使用一种称为倒排索引的数据结构。这是一个将词映射到包含它们的文档的字典。这个字典允许根据词快速检索文档。索引还可能存储其他信息，如词频和文档计数（包含此词的文档数量），这些信息对于计算TF-IDF得分很有帮助。[表6-1](#ch06_table_1_1730157386543390)说明了倒排索引。
- en: Table 6-1\. A simplified example of an inverted index.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-1\. 一个简化的倒排索引示例。
- en: '| Term | Document count | (Document index, term frequency) for all documents
    containing the term |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 词 | 文档计数 | 包含该词的所有文档的（文档索引，词频） |'
- en: '| --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| banana | 2 | (10, 3), (5, 2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 香蕉 | 2 | (10, 3), (5, 2) |'
- en: '| machine | 4 | (1, 5), (10, 1), (38, 9), (42, 5) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 机器 | 4 | (1, 5), (10, 1), (38, 9), (42, 5) |'
- en: '| learning | 3 | (1, 5), (38, 7), (42, 5) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 学习 | 3 | (1, 5), (38, 7), (42, 5) |'
- en: '| … | … | … |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … |'
- en: '[Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25), the 25th generation
    of the Best Matching algorithm, was developed by Robertson et al. in the 1980s.
    Its scorer is a modification of TF-IDF. Compared to naive TF-IDF, BM25 normalizes
    term frequency scores by document length. Longer documents are more likely to
    contain a given term and have higher term frequency values.^([6](ch06.html#id1245))'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25)，最佳匹配算法的第25代，由Robertson等人于20世纪80年代开发。其评分器是TF-IDF的一种改进。与原始的TF-IDF相比，BM25通过文档长度对词频得分进行归一化。较长的文档更有可能包含某个特定词，并且具有更高的词频值.^([6](ch06.html#id1245))'
- en: BM25 and its variances (BM25+, BM25F) are still widely used in the industry
    and serve as formidable baselines to compare against modern, more sophisticated
    retrieval algorithms, such as embedding-based retrieval, discussed next.^([7](ch06.html#id1246))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: BM25及其变体（BM25+，BM25F）在业界仍被广泛使用，并作为强大的基线，用于比较现代、更复杂的检索算法，如接下来讨论的基于嵌入的检索.^([7](ch06.html#id1246))
- en: One process I glossed over is tokenization, the process of breaking a query
    into individual terms. The simplest method is to split the query into words, treating
    each word as a separate term. However, this can lead to multi-word terms being
    broken into individual words, losing their original meaning. For example, “hot
    dog” would be split into “hot” and “dog”. When this happens, neither retains the
    meaning of the original term. One way to mitigate this issue is to treat the most
    common n-grams as terms. If the bigram “hot dog” is common, it’ll be treated as
    a term.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我略过的一个过程是分词，即将查询分解成单个术语的过程。最简单的方法是将查询分解成单词，将每个单词视为一个单独的术语。然而，这可能导致多词术语被分解成单个单词，失去其原始含义。例如，“hot
    dog”会被分解成“hot”和“dog”。当这种情况发生时，它们都不保留原始术语的含义。缓解这一问题的方法之一是将最常见的n-gram视为术语。如果二元组“hot
    dog”很常见，它将被视为一个术语。
- en: Additionally, you might want to convert all characters to lowercase, remove
    punctuation, and eliminate stop words (like “the”, “and”, “is”, etc.). Term-based
    retrieval solutions often handle these automatically. Classical NLP packages,
    such as [NLTK](https://www.nltk.org) (Natural Language Toolkit), [spaCy](https://github.com/explosion/spaCy),
    and [Stanford’s CoreNLP](https://github.com/stanfordnlp/CoreNLP), also offer tokenization
    functionalities.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可能还想将所有字符转换为小写，删除标点符号，并消除停用词（如“the”，“and”，“is”等）。基于术语的检索解决方案通常可以自动处理这些问题。经典的自然语言处理包，如[NLTK](https://www.nltk.org)（自然语言工具包）、[spaCy](https://github.com/explosion/spaCy)和[斯坦福的CoreNLP](https://github.com/stanfordnlp/CoreNLP)，也提供标记化功能。
- en: '[Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863) discusses
    measuring the lexical similarity between two texts based on their n-gram overlap.
    Can we retrieve documents based on the extent of their n-gram overlap with the
    query? Yes, we can. This approach works best when the query and the documents
    are of similar lengths. If the documents are much longer than the query, the likelihood
    of them containing the query’s n-grams increases, leading to many documents having
    similarly high overlap scores. This makes it difficult to distinguish truly relevant
    documents from less relevant ones.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[第4章](ch04.html#ch04_evaluate_ai_systems_1730130866187863)讨论了基于两个文本的n-gram重叠来衡量它们之间的词汇相似度。我们能否根据文档与查询的n-gram重叠程度来检索文档？是的，我们可以。当查询和文档的长度相似时，这种方法效果最好。如果文档比查询长得多，它们包含查询n-gram的可能性会增加，导致许多文档具有相似的高重叠分数。这使得很难区分真正相关的文档和不太相关的文档。'
- en: Embedding-based retrieval
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于嵌入的检索
- en: Term-based retrieval computes relevance at a lexical level rather than a semantic
    level. As mentioned in [Chapter 3](ch03.html#ch03a_evaluation_methodology_1730150757064067),
    the appearance of a text doesn’t necessarily capture its meaning. This can result
    in returning documents irrelevant to your intent. For example, querying “transformer
    architecture” might return documents about the electric device or the movie *Transformers*.
    On the other hand, *embedding-based retrievers* aim to rank documents based on
    how closely their meanings align with the query. This approach is also known as
    *semantic retrieval*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基于术语的检索在词汇层面上而不是语义层面上计算相关性。如[第3章](ch03.html#ch03a_evaluation_methodology_1730150757064067)中所述，文本的出现并不一定能够捕捉其含义。这可能导致返回与你的意图无关的文档。例如，查询“transformer架构”可能会返回关于电器设备或电影*变形金刚*的文档。另一方面，*基于嵌入的检索器*旨在根据文档的含义与查询的接近程度对文档进行排序。这种方法也被称为*语义检索*。
- en: 'With embedding-based retrieval, indexing has an extra function: converting
    the original data chunks into embeddings. The database where the generated embeddings
    are stored is called a *vector database*. Querying then consists of two steps,
    as shown in [Figure 6-3](#ch06_figure_3_1730157386529243):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于嵌入的检索，索引具有额外功能：将原始数据块转换为嵌入。存储生成的嵌入的数据库称为*向量数据库*。查询包括两个步骤，如图[图6-3](#ch06_figure_3_1730157386529243)所示：
- en: 'Embedding model: convert the query into an embedding using the same embedding
    model used during indexing.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入模型：使用与索引期间相同的嵌入模型将查询转换为嵌入。
- en: 'Retriever: fetch *k* data chunks whose embeddings are closest to the query
    embedding, as determined by the retriever. The number of data chunks to fetch,
    *k*, depends on the use case, the generative model, and the query.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索器：检索与查询嵌入最接近的*k*个数据块，由检索器确定。要检索的数据块数量*k*取决于用例、生成模型和查询。
- en: '![A diagram of a model'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![模型图](assets/aien_0603.png)'
- en: Description automatically generated](assets/aien_0603.png)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0603.png)
- en: Figure 6-3\. A high-level view of how an embedding-based, or semantic, retriever
    works.
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 基于嵌入或语义的检索器的高级视图。
- en: The embedding-based retrieval workflow shown here is simplified. Real-world
    semantic retrieval systems might contain other components, such as a reranker
    to rerank all retrieved candidates, and caches to reduce latency.^([8](ch06.html#id1251))
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里所示基于嵌入的检索工作流程是简化的。现实世界的语义检索系统可能包含其他组件，例如重新排序器以重新排序所有检索到的候选者，以及缓存以减少延迟。[8](ch06.html#id1251)
- en: With embedding-based retrieval, we again encounter embeddings, which are discussed
    in [Chapter 3](ch03.html#ch03a_evaluation_methodology_1730150757064067). As a
    reminder, an embedding is typically a vector that aims to preserve the important
    properties of the original data. An embedding-based retriever doesn’t work if
    the embedding model is bad.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于嵌入的检索中，我们再次遇到了嵌入，这在[第 3 章](ch03.html#ch03a_evaluation_methodology_1730150757064067)中已有讨论。作为提醒，嵌入通常是一个旨在保留原始数据重要属性的向量。如果嵌入模型不好，基于嵌入的检索器将无法工作。
- en: 'Embedding-based retrieval also introduces a new component: vector databases.
    A vector database stores vectors. However, storing is the easy part of a vector
    database. The hard part is vector search. Given a query embedding, a vector database
    is responsible for finding vectors in the database close to the query and returning
    them. Vectors have to be indexed and stored in a way that makes vector search
    fast and efficient.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于嵌入的检索还引入了一个新的组件：向量数据库。向量数据库存储向量。然而，存储只是向量数据库的简单部分。困难的部分是向量搜索。给定一个查询嵌入，向量数据库负责在数据库中找到接近查询的向量并将它们返回。向量必须以使向量搜索快速高效的方式索引和存储。
- en: 'Like many other mechanisms that generative AI applications depend on, vector
    search isn’t unique to generative AI. Vector search is common in any application
    that uses embeddings: search, recommendation, data organization, information retrieval,
    clustering, fraud detection, and more.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他生成式人工智能应用所依赖的机制一样，向量搜索并非仅限于生成式人工智能。向量搜索在所有使用嵌入的应用中都很常见：搜索、推荐、数据组织、信息检索、聚类、欺诈检测等等。
- en: 'Vector search is typically framed as a nearest-neighbor search problem. For
    example, given a query, find the *k* nearest vectors. The naive solution is k-nearest
    neighbors (k-NN), which works as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 向量搜索通常被框架化为最近邻搜索问题。例如，给定一个查询，找到 *k* 个最近的向量。直观的解决方案是 k-最近邻（k-NN），其工作原理如下：
- en: Compute the similarity scores between the query embedding and all vectors in
    the database, using metrics such as cosine similarity.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用诸如余弦相似度等度量标准，计算查询嵌入与数据库中所有向量的相似度得分。
- en: Rank all vectors by their similarity scores.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按相似度得分对所有向量进行排序。
- en: Return *k* vectors with the highest similarity scores.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回相似度得分最高的 *k* 个向量。
- en: This naive solution ensures that the results are precise, but it’s computationally
    heavy and slow. It should be used only for small datasets.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这种直观的解决方案确保了结果的准确性，但计算量大且速度慢。它仅适用于小型数据集。
- en: For large datasets, vector search is typically done using an approximate nearest
    neighbor (ANN) algorithm. Due to the importance of vector search, many algorithms
    and libraries have been developed for it. Some popular vector search libraries
    are *FAISS* (Facebook AI Similarity Search) ([Johnson et al., 2017](https://arxiv.org/abs/1702.08734)),
    Google’s *ScaNN* (Scalable Nearest Neighbors) ([Sun et al., 2020](https://oreil.ly/faJqj)),
    [Spotify’s *Annoy*](https://github.com/spotify/annoy) (Bernhardsson, 2013), and
    [*Hnswlib*](https://oreil.ly/4ATBC) ([Hierarchical Navigable Small World](https://github.com/nmslib/hnswlib))
    (Malkov and Yashunin, 2016).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型数据集，通常使用近似最近邻（ANN）算法进行向量搜索。由于向量搜索的重要性，已经为它开发了众多算法和库。一些流行的向量搜索库包括 *FAISS*（Facebook
    AI Similarity Search）([Johnson 等人，2017](https://arxiv.org/abs/1702.08734))，谷歌的
    *ScaNN*（Scalable Nearest Neighbors）([Sun 等人，2020](https://oreil.ly/faJqj))，[Spotify
    的 *Annoy*](https://github.com/spotify/annoy)（Bernhardsson，2013），以及 [*Hnswlib*](https://oreil.ly/4ATBC)（层次可导航小世界，[Hierarchical
    Navigable Small World](https://github.com/nmslib/hnswlib)）(Malkov 和 Yashunin，2016)。
- en: Most application developers won’t implement vector search themselves, so I’ll
    give only a quick overview of different approaches. This overview might be helpful
    as you evaluate solutions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数应用程序开发者不会自己实现向量搜索，因此我将只简要概述不同的方法。在评估解决方案时，这个概述可能很有帮助。
- en: 'In general, vector databases organize vectors into buckets, trees, or graphs.
    Vector search algorithms differ based on the heuristics they use to increase the
    likelihood that similar vectors are close to each other. Vectors can also be quantized
    (reduced precision) or made sparse. The idea is that quantized and sparse vectors
    are less computationally intensive to work with. For those wanting to learn more
    about vector search, Zilliz has an excellent [series](https://oreil.ly/MVsgB)
    on it. Here are some significant vector search algorithms:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，向量数据库将向量组织成桶、树或图。向量搜索算法根据它们使用的启发式方法来增加相似向量彼此靠近的可能性而有所不同。向量也可以进行量化（降低精度）或稀疏化。其想法是，量化和稀疏向量处理起来计算量更小。对于那些想要了解更多关于向量搜索的人来说，Zilliz有一个关于它的优秀[系列](https://oreil.ly/MVsgB)。以下是一些重要的向量搜索算法：
- en: LSH (locality-sensitive hashing) ([Indyk and Motwani, 1999](https://oreil.ly/slO9x))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: LSH（局部敏感哈希）([Indyk and Motwani, 1999](https://oreil.ly/slO9x))
- en: This is a powerful and versatile algorithm that works with more than just vectors.
    This involves hashing similar vectors into the same buckets to speed up similarity
    search, trading some accuracy for efficiency. It’s implemented in FAISS and Annoy.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个功能强大且多用途的算法，它不仅与向量一起工作。这涉及到将相似向量散列到相同的桶中以提高相似性搜索的速度，以牺牲一些精度来换取效率。它在FAISS和Annoy中实现。
- en: HNSW (Hierarchical Navigable Small World) ([Malkov and Yashunin, 2016](https://github.com/nmslib/hnswlib))
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: HNSW（Hierarchical Navigable Small World）([Malkov and Yashunin, 2016](https://github.com/nmslib/hnswlib))
- en: HNSW constructs a multi-layer graph where nodes represent vectors, and edges
    connect similar vectors, allowing nearest-neighbor searches by traversing graph
    edges. Its implementation by the authors is open source, and it’s also implemented
    in FAISS and Milvus.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: HNSW（Hierarchical Navigable Small World）构建了一个多层图，其中节点代表向量，边连接相似向量，通过遍历图边进行最近邻搜索。作者们的实现是开源的，它也被FAISS和Milvus实现。
- en: Product Quantization ([Jégou et al., 2011](https://oreil.ly/VaLf4))
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 产品量化([Jégou et al., 2011](https://oreil.ly/VaLf4))
- en: This works by reducing each vector into a much simpler, lower-dimensional representation
    by decomposing each vector into multiple subvectors. The distances are then computed
    using the lower-dimensional representations, which are much faster to work with.
    Product quantization is a key component of FAISS and is supported by almost all
    popular vector search libraries.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过将每个向量分解成多个子向量，将其简化为更简单、更低维度的表示来实现。然后使用这些低维表示来计算距离，这些表示处理起来要快得多。产品量化是FAISS的关键组件，并且几乎所有的流行向量搜索库都支持它。
- en: IVF (inverted file index) ([Sivic and Zisserman, 2003](https://oreil.ly/9BcYN))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: IVF（倒排文件索引）([Sivic and Zisserman, 2003](https://oreil.ly/9BcYN))
- en: IVF uses K-means clustering to organize similar vectors into the same cluster.
    Depending on the number of vectors in the database, it’s typical to set the number
    of clusters so that, on average, there are 100 to 10,000 vectors in each cluster.
    During querying, IVF finds the cluster centroids closest to the query embedding,
    and the vectors in these clusters become candidate neighbors. Together with product
    quantization, IVF forms the backbone of FAISS.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: IVF使用K-means聚类将相似向量组织到同一个簇中。根据数据库中向量的数量，通常将簇的数量设置为平均每个簇有100到10,000个向量。在查询过程中，IVF找到最接近查询嵌入的簇中心，这些簇中的向量成为候选邻居。与产品量化一起，IVF构成了FAISS的骨干。
- en: Annoy (Approximate Nearest Neighbors Oh Yeah) ([Bernhardsson, 2013](https://github.com/spotify/annoy))
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Annoy（近似最近邻哦耶）([Bernhardsson, 2013](https://github.com/spotify/annoy))
- en: Annoy is a tree-based approach. It builds multiple binary trees, where each
    tree splits the vectors into clusters using random criteria, such as randomly
    drawing a line and splitting the vectors into two branches using this line. During
    a search, it traverses these trees to gather candidate neighbors. Spotify has
    open sourced its implementation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Annoy是一种基于树的算法。它构建多个二叉树，其中每个树使用随机标准（如随机画一条线，并使用这条线将向量分成两个分支）将向量分成簇。在搜索过程中，它遍历这些树以收集候选邻居。Spotify已经开源了其实现。
- en: There are other algorithms, such as [Microsoft’s SPTAG](https://github.com/microsoft/SPTAG)
    (Space Partition Tree And Graph), and [FLANN](https://github.com/flann-lib/flann)
    (Fast Library for Approximate Nearest Neighbors).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他算法，例如[微软的SPTAG](https://github.com/microsoft/SPTAG)（空间划分树和图）和[FLANN](https://github.com/flann-lib/flann)（快速近似最近邻库）。
- en: Even though vector databases emerged as their own category with the rise of
    RAG, any database that can store vectors can be called a vector database. Many
    traditional databases have extended or will extend to support vector storage and
    vector search.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随着 RAG 的兴起，向量数据库作为一个独立的类别出现，但任何可以存储向量的数据库都可以被称为向量数据库。许多传统数据库已经扩展或将要扩展以支持向量存储和向量搜索。
- en: Comparing retrieval algorithms
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较检索算法
- en: Due to the long history of retrieval, its many mature solutions make both term-based
    and embedding-based retrieval relatively easy to start. Each approach has its
    pros and cons.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于检索有着悠久的历史，其众多成熟的解决方案使得基于词项和基于嵌入的检索相对容易开始。每种方法都有其优缺点。
- en: Term-based retrieval is generally much faster than embedding-based retrieval
    during both indexing and query. Term extraction is faster than embedding generation,
    and mapping from a term to the documents that contain it can be less computationally
    expensive than a nearest-neighbor search.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引和查询过程中，基于词项的检索通常比基于嵌入的检索要快得多。词项提取比嵌入生成更快，并且从词项到包含该词项的文档的映射可能比最近邻搜索的计算成本更低。
- en: Term-based retrieval also works well out of the box. Solutions like Elasticsearch
    and BM25 have successfully powered many search and retrieval applications. However,
    its simplicity also means that it has fewer components you can tweak to improve
    its performance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词项的检索也能直接使用就表现出良好的效果。像 Elasticsearch 和 BM25 这样的解决方案已经成功地为许多搜索和检索应用提供了动力。然而，它的简单性也意味着它有更少的组件可以调整以提升其性能。
- en: Embedding-based retrieval, on the other hand, can be significantly improved
    over time to outperform term-based retrieval. You can finetune the embedding model
    and the retriever, either separately, together, or in conjunction with the generative
    model. However, converting data into embeddings can obscure keywords, such as
    specific error codes, e.g., EADDRNOTAVAIL (99), or product names, making them
    harder to search later on. This limitation can be addressed by combining embedding-based
    retrieval with term-based retrieval, as discussed later in this chapter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于嵌入的检索，另一方面，随着时间的推移可以显著提升，从而超越基于词项的检索。你可以分别、一起或与生成模型结合，微调嵌入模型和检索器。然而，将数据转换为嵌入可能会模糊关键词，例如特定的错误代码，例如
    EADDRNOTAVAIL (99)，或产品名称，使得后续搜索更加困难。这一限制可以通过将基于嵌入的检索与基于词项的检索相结合来解决，正如本章后面所讨论的。
- en: The quality of a retriever can be evaluated based on the quality of the data
    it retrieves. Two metrics often used by RAG evaluation frameworks are *context
    precision* and *context recall*, or precision and recall for short (context precision
    is also called *context relevance):*
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '检索器的质量可以根据其检索数据的质量来评估。RAG 评估框架中经常使用的两个指标是上下文精确度和上下文召回率，或简称精确率和召回率（上下文精确度也称为上下文相关性）： '
- en: Context precision
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文精确度
- en: Out of all the documents retrieved, what percentage is relevant to the query?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有检索到的文档中，有多少百分比与查询相关？
- en: Context recall
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文召回率
- en: Out of all the documents that are relevant to the query, what percentage is
    retrieved?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有与查询相关的文档中，有多少百分比被检索到了？
- en: To compute these metrics, you curate an evaluation set with a list of test queries
    and a set of documents. For each test query, you annotate each test document to
    be relevant or not relevant. The annotation can be done either by humans or AI
    judges. You then compute the precision and recall score of the retriever on this
    evaluation set.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这些指标，你需要创建一个评估集，其中包含一系列测试查询和一组文档。对于每个测试查询，你需要标注每个测试文档是否相关。标注可以由人类或人工智能裁判完成。然后，你计算检索器在这个评估集上的精确率和召回率得分。
- en: In production, some RAG frameworks only support context precision, not context
    recall To compute context recall for a given query, you need to annotate the relevance
    of all documents in your database to that query. Context precision is simpler
    to compute. You only need to compare the retrieved documents to the query, which
    can be done by an AI judge.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，一些 RAG 框架只支持上下文精确度，而不支持上下文召回率。为了计算给定查询的上下文召回率，你需要标注数据库中所有文档与该查询的相关性。上下文精确度计算起来更简单。你只需要比较检索到的文档与查询，这可以通过人工智能裁判来完成。
- en: If you care about the ranking of the retrieved documents, for example, more
    relevant documents should be ranked first, you can use metrics such as [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)
    (normalized discounted cumulative gain), [MAP](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)
    (Mean Average Precision), and [MRR](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)
    (Mean Reciprocal Rank).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你关心检索到的文档的排名，例如，更相关的文档应该排在前面，你可以使用诸如[NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)（归一化累积增益）、[MAP](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)（平均平均精度）和[MRR](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)（平均倒数排名）等指标。
- en: For semantic retrieval, you need to also evaluate the quality of your embeddings.
    As discussed in [Chapter 3](ch03.html#ch03a_evaluation_methodology_1730150757064067),
    embeddings can be evaluated independently—they are considered good if more-similar
    documents have closer embeddings. Embeddings can also be evaluated by how well
    they work for specific tasks. The [MTEB](https://arxiv.org/abs/2210.07316) benchmark
    (Muennighoff et al., 2023) evaluates embeddings for a broad range of tasks including
    retrievals, classification, and clustering.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语义检索，你还需要评估你的嵌入质量。如[第3章](ch03.html#ch03a_evaluation_methodology_1730150757064067)中所述，嵌入可以独立评估——如果更相似的文档有更接近的嵌入，则被认为是好的。嵌入也可以通过它们在特定任务中的表现来评估。[MTEB](https://arxiv.org/abs/2210.07316)基准（Muennighoff等人，2023）评估了包括检索、分类和聚类在内的广泛任务的嵌入。
- en: The quality of a retriever should also be evaluated in the context of the whole
    RAG system. Ultimately, a retriever is good if it helps the system generate high-quality
    answers. Evaluating outputs of generative models is discussed in Chapters [3](ch03.html#ch03a_evaluation_methodology_1730150757064067)
    and [4](ch04.html#ch04_evaluate_ai_systems_1730130866187863).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个RAG系统的背景下，也应该评估检索器的质量。最终，如果检索器有助于系统生成高质量的答案，则检索器是好的。生成模型输出的评估在第[3章](ch03.html#ch03a_evaluation_methodology_1730150757064067)和[4章](ch04.html#ch04_evaluate_ai_systems_1730130866187863)中讨论。
- en: Whether the performance promise of a semantic retrieval system is worth pursuing
    depends on how much you prioritize cost and latency, particularly during the querying
    phase. Since much of RAG latency comes from output generation, especially for
    long outputs, *the added latency by query embedding generation and vector search
    might be minimal compared to the total RAG latency.* Even so, the added latency
    still can impact user experience.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 语义检索系统的性能承诺是否值得追求取决于你多么重视成本和延迟，尤其是在查询阶段。由于RAG的延迟很大程度上来自输出生成，尤其是对于长输出，*查询嵌入生成和向量搜索带来的额外延迟可能相对于总RAG延迟来说微不足道。*
    即使如此，额外的延迟仍然可能影响用户体验。
- en: Another concern is cost. Generating embeddings costs money. This is especially
    an issue if your data changes frequently and requires frequent embedding regeneration.
    Imagine having to generate embeddings for 100 million documents every day! Depending
    on what vector databases you use, vector storage and vector search queries can
    be expensive, too. It’s not uncommon to see a company’s vector database spending
    be one-fifth or even half of their spending on model APIs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个担忧是成本。生成嵌入需要花钱。如果你的数据频繁变化并需要频繁地重新生成嵌入，这尤其是一个问题。想象一下每天需要为1亿份文档生成嵌入！根据你使用的向量数据库，向量存储和向量搜索查询也可能很昂贵。看到公司的向量数据库支出占其模型API支出的五分之一甚至一半并不罕见。
- en: '[Table 6-2](#ch06_table_2_1730157386543401) shows a side-by-side comparison
    of term-based retrieval and embedding-based retrieval.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6-2](#ch06_table_2_1730157386543401)显示了基于术语的检索和基于嵌入的检索的并列比较。'
- en: Table 6-2\. Term-based retrieval and semantic retrieval by speed, performance,
    and cost.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-2\. 基于术语的检索和语义检索的速度、性能和成本。
- en: '|  | Term-based retrieval | Embedding-based retrieval |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于术语的检索 | 基于嵌入的检索 |'
- en: '| --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Querying speed | Much faster than embedding-based retrieval | Query embedding
    generation and vector search can be slow |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 查询速度 | 比基于嵌入的检索快得多 | 查询嵌入生成和向量搜索可能会很慢 |'
- en: '| Performance | Typically strong performance out of the box, but hard to improve'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '| 性能 | 通常性能出色，但难以改进'
- en: Can retrieve wrong documents due to term ambiguity | Can outperform term-based
    retrieval with finetuning
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 可能由于术语歧义而检索到错误的文档 | 通过微调可以优于基于术语的检索
- en: Allows for the use of more natural queries, as it focuses on semantics instead
    of terms |
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 允许使用更自然的查询，因为它关注语义而不是术语 |
- en: '| Cost | Much cheaper than embedding-based retrieval | Embedding, vector storage,
    and vector search solutions can be expensive |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 成本 | 比基于嵌入的检索便宜得多 | 嵌入、向量存储和向量搜索解决方案可能很昂贵 |'
- en: With retrieval systems, you can make certain trade-offs between indexing and
    querying. The more detailed the index is, the more accurate the retrieval process
    will be, but the indexing process will be slower and more memory-consuming. Imagine
    building an index of potential customers. Adding more details (e.g., name, company,
    email, phone, interests) makes it easier to find relevant people but takes longer
    to build and requires more storage.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用检索系统，你可以在索引和查询之间做出某些权衡。索引越详细，检索过程越准确，但索引过程会变慢且消耗更多内存。想象一下构建潜在客户的索引。添加更多细节（例如，姓名、公司、电子邮件、电话、兴趣）可以更容易地找到相关的人，但构建时间会更长，需要更多存储。
- en: In general, a detailed index like HNSW provides high accuracy and fast query
    times but requires significant time and memory to build. In contrast, a simpler
    index like LSH is quicker and less memory-intensive to create, but it results
    in slower and less accurate queries.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，像HNSW这样的详细索引提供了高精度和快速的查询时间，但构建它需要大量时间和内存。相比之下，像LSH这样的简单索引创建更快，内存消耗更少，但查询速度较慢且精度较低。
- en: 'The [ANN-Benchmarks website](https://oreil.ly/pbh3y) compares different ANN
    algorithms on multiple datasets using four main metrics, taking into account the
    trade-offs between indexing and querying. These include the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[ANN-Benchmarks网站](https://oreil.ly/pbh3y)使用四个主要指标比较多个数据集上的不同ANN算法，考虑到索引和查询之间的权衡。这些包括以下内容：'
- en: Recall
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率
- en: The fraction of the nearest neighbors found by the algorithm.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 算法找到的最近邻的比例。
- en: Query per second (QPS)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒查询数（QPS）。
- en: The number of queries the algorithm can handle per second. This is crucial for
    high-traffic applications.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 算法每秒可以处理的查询数量。这对于高流量应用至关重要。
- en: Build time
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 构建时间
- en: The time required to build the index. This metric is especially important if
    you need to frequently update your index (e.g., because your data changes).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 构建索引所需的时间。如果需要频繁更新索引（例如，因为数据发生变化），这个指标尤为重要。
- en: Index size
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 索引大小
- en: The size of the index created by the algorithm, which is crucial for assessing
    its scalability and storage requirements.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 算法创建的索引大小，这对于评估其可扩展性和存储需求至关重要。
- en: Additionally, BEIR (Benchmarking IR) ([Thakur et al., 2021](https://arxiv.org/abs/2104.08663))
    is an evaluation harness for retrieval. It supports retrieval systems across 14
    common retrieval benchmarks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，BEIR（Benchmarking IR）([Thakur等人，2021](https://arxiv.org/abs/2104.08663))是一个用于检索的评估工具。它支持14个常见检索基准上的检索系统。
- en: 'To summarize, the quality of a RAG system should be evaluated both component
    by component and end to end. To do this, you should do the following things:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，RAG系统的质量应该从组件到组件、从端到端进行评估。为此，你应该做以下事情：
- en: Evaluate the retrieval quality.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估检索质量。
- en: Evaluate the final RAG outputs.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估最终的RAG输出。
- en: Evaluate the embeddings (for embedding-based retrieval).
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估嵌入（对于基于嵌入的检索）。
- en: Combining retrieval algorithms
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合检索算法
- en: Given the distinct advantages of different retrieval algorithms, a production
    retrieval system typically combines several approaches. Combining term-based retrieval
    and embedding-based retrieval is called *hybrid search*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不同的检索算法具有不同的优势，生产级检索系统通常结合几种方法。结合基于术语的检索和基于嵌入的检索被称为*混合搜索*。
- en: Different algorithms can be used in sequence. First, a cheap, less precise retriever,
    such as a term-based system, fetches candidates. Then, a more precise but more
    expensive mechanism, such as k-nearest neighbors, finds the best of these candidates.
    This second step is also called *reranking*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按顺序使用不同的算法。首先，一个便宜但不太精确的检索器，例如基于术语的系统，检索候选者。然后，一个更精确但更昂贵的机制，例如k最近邻，找到这些候选者中的最佳者。这一步也被称为*重排序*。
- en: For example, given the term “transformer”, you can fetch all documents that
    contain the word transformer, regardless of whether they are about the electric
    device, the neural architecture, or the movie. Then you use vector search to find
    among these documents those that are actually related to your transformer query.
    As another example, consider the query “Who’s responsible for the most sales to
    X?” First, you might fetch all documents associated with X using the keyword X.
    Then, you use vector search to retrieve the context associated with “Who’s responsible
    for the most sales?”
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定术语“transformer”，你可以获取所有包含单词 transformer 的文档，无论它们是关于电器设备、神经网络架构还是电影。然后你使用向量搜索在这些文档中找到与你的
    transformer 查询实际相关的文档。作为另一个例子，考虑查询“谁对 X 的最大销售额负责？”首先，你可能会使用关键词 X 获取所有与 X 相关的文档。然后，你使用向量搜索检索与“谁对最大销售额负责？”相关的上下文。
- en: Different algorithms can also be used in parallel as an ensemble. Remember that
    a retriever works by ranking documents by their relevance scores to the query.
    You can use multiple retrievers to fetch candidates at the same time, then combine
    these different rankings together to generate a final ranking.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的算法也可以并行使用作为一个集成。记住，检索器通过按其与查询的相关性分数对文档进行排序来工作。你可以使用多个检索器同时获取候选文档，然后将这些不同的排名组合起来生成最终的排名。
- en: An algorithm for combining different rankings is called [reciprocal rank fusion
    (RRF)](https://oreil.ly/3xtwh) (Cormack et al., 2009). It assigns each document
    a score based on its ranking by a retriever. Intuitively, if it ranks first, its
    score is 1/1 = 1\. If it ranks second, its score is ½ = 0.5\. The higher it ranks,
    the higher its score.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于组合不同排名的算法称为[互逆排名融合（RRF）](https://oreil.ly/3xtwh)（Cormack 等，2009）。它根据检索器对每个文档的排名分配一个分数。直观地说，如果它排名第一，其分数是
    1/1 = 1。如果它排名第二，其分数是 ½ = 0.5。排名越高，分数越高。
- en: 'A document’s final score is the sum of its scores with respect to all retrievers.
    If a document is ranked first by one retriever and second by another retriever,
    its score is 1 + 0.5 = 1.5\. This example is an oversimplification of RRF, but
    it shows the basics. The actual formula for a document *D* is more complicated,
    as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的最终得分是其与所有检索器得分的总和。如果一个文档被一个检索器排在第一位，而另一个检索器排在第二位，其得分是 1 + 0.5 = 1.5。这个例子是
    RRF 的过度简化，但它展示了基本原理。实际公式对于文档 *D* 更为复杂，如下所示：
- en: $Score left-parenthesis upper D right-parenthesis equals sigma-summation Underscript
    i equals 1 Overscript n Endscripts StartFraction 1 Over k plus r Subscript i Baseline
    left-parenthesis upper D right-parenthesis EndFraction$
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: $Score\left(D\right) = \sum_{i=1}^{n} \frac{1}{k + r_i(D)}$
- en: '*n* is the number of ranked lists; each rank list is produced by a retriever.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* 是排名列表的数量；每个排名列表由一个检索器生成。'
- en: $r Subscript i Baseline left-parenthesis upper D right-parenthesis$ is the rank
    of the document by the retriever *i*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $r_i(D)$ 是检索器 *i* 对文档的排名。
- en: '*k* is a constant to avoid division by zero and to control the influence of
    lower-ranked documents. A typical value for *k* is 60.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k* 是一个常数，用于避免除以零并控制低排名文档的影响。*k* 的典型值是 60。'
- en: Retrieval Optimization
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索优化
- en: Depending on the task, certain tactics can increase the chance of relevant documents
    being fetched. Four tactics discussed here are chunking strategy, reranking, query
    rewriting, and contextual retrieval.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 根据任务的不同，某些策略可以增加检索相关文档的机会。这里讨论的四种策略是分块策略、重新排序、查询重写和上下文检索。
- en: Chunking strategy
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分块策略
- en: How your data should be indexed depends on how you intend to retrieve it later.
    The last section covered different retrieval algorithms and their respective indexing
    strategies. There, the discussion was based on the assumption that documents have
    already been split into manageable chunks. In this section, I’ll cover different
    chunking strategies. This is an important consideration because the chunking strategy
    you use can significantly impact the performance of your retrieval system.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据应该如何索引取决于你打算如何稍后检索它。最后一节涵盖了不同的检索算法及其相应的索引策略。在那里，讨论基于假设文档已经被分割成可管理的块。在本节中，我将介绍不同的分块策略。这是一个重要的考虑因素，因为你所使用的分块策略可以显著影响你的检索系统的性能。
- en: The simplest strategy is to chunk documents into chunks of equal length based
    on a certain unit. Common units are characters, words, sentences, and paragraphs.
    For example, you can split each document into chunks of 2,048 characters or 512
    words. You can also split each document so that each chunk can contain a fixed
    number of sentences (such as 20 sentences) or paragraphs (such as each paragraph
    is its own chunk).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的策略是根据某个单位将文档块化为等长的块。常见的单位是字符、单词、句子和段落。例如，您可以将每个文档拆分为2,048个字符或512个单词的块。您也可以将每个文档拆分，以便每个块可以包含固定数量的句子（例如20个句子）或段落（例如每个段落是其自己的块）。
- en: You can also split documents recursively using increasingly smaller units until
    each chunk fits within your maximum chunk size. For example, you can start by
    splitting a document into sections. If a section is too long, split it into paragraphs.
    If a paragraph is still too long, split it into sentences. This reduces the chance
    of related texts being arbitrarily broken off.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用越来越小的单元递归地拆分文档，直到每个块的大小适合您的最大块大小。例如，您可以从将文档拆分为章节开始。如果一个章节太长，可以将其拆分为段落。如果一个段落仍然太长，可以将其拆分为句子。这样可以减少相关文本被任意拆分的机会。
- en: Specific documents might also support creative chunking strategies. For example,
    there are [splitters](https://github.com/grantjenks/py-tree-sitter-languages#license)
    developed especially for different programming languages. Q&A documents can be
    split by question or answer pair, where each pair makes up a chunk. Chinese texts
    might need to be split differently from English texts.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 特定文档可能也支持创新的块拆分策略。例如，有专门为不同编程语言开发的[splitters](https://github.com/grantjenks/py-tree-sitter-languages#license)。问答文档可以按问题或答案对进行拆分，其中每一对构成一个块。中文文本可能需要与英文文本不同的拆分方式。
- en: When a document is split into chunks without overlap, the chunks might be cut
    off in the middle of important context, leading to the loss of critical information.
    Consider the text “I left my wife a note”. If it’s split into “I left my wife”
    and “a note”, neither of these two chunks conveys the key information of the original
    text. Overlapping ensures that important boundary information is included in at
    least one chunk. If you set the chunk size to be 2,048 characters, you can perhaps
    set the overlapping size to be 20 characters.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当文档拆分为无重叠的块时，块可能在重要上下文的中间被切断，导致关键信息的丢失。考虑文本“我给我的妻子留了一张便条”。如果它被拆分为“我给我的妻子”和“一张便条”，这两个块都没有传达原始文本的关键信息。重叠确保至少在一个块中包含重要的边界信息。如果您将块大小设置为2,048个字符，您可能可以将重叠大小设置为20个字符。
- en: The chunk size shouldn’t exceed the maximum context length of the generative
    model. For the embedding-based approach, the chunk size also shouldn’t exceed
    the embedding model’s context limit.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小不应超过生成模型的最大上下文长度。对于基于嵌入的方法，块大小也不应超过嵌入模型上下文限制。
- en: You can also chunk documents using tokens, determined by the generative model’s
    tokenizer, as a unit. Let’s say that you want to use Llama 3 as your generative
    model. You then first tokenize documents using Llama 3’s tokenizer. You can then
    split documents into chunks using tokens as the boundaries. Chunking by tokens
    makes it easier to work with downstream models. However, the downside of this
    approach is that if you switch to another generative model with a different tokenizer,
    you’d need to reindex your data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用生成模型分词器确定的标记作为单位来拆分文档。假设您想使用Llama 3作为您的生成模型。然后您首先使用Llama 3的分词器对文档进行分词。然后您可以使用标记作为边界将文档拆分为块。按标记拆分使得与下游模型一起工作更容易。然而，这种方法的一个缺点是，如果您切换到另一个具有不同分词器的生成模型，您需要重新索引您的数据。
- en: Regardless of which strategy you choose, chunk sizes matter. A smaller chunk
    size allows for more diverse information. Smaller chunks mean that you can fit
    more chunks into the model’s context. If you halve the chunk size, you can fit
    twice as many chunks. More chunks can provide a model with a wider range of information,
    which can enable the model to produce a better answer.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择哪种策略，块大小都很重要。较小的块大小允许包含更多样化的信息。较小的块意味着您可以放入模型上下文中的块更多。如果您将块大小减半，您可以放入两倍的块。更多的块可以为模型提供更广泛的信息范围，这可以使模型产生更好的答案。
- en: Small chunk sizes, however, can cause the loss of important information. Imagine
    a document that contains important information about the topic X throughout the
    document, but X is only mentioned in the first half. If you split this document
    into two chunks, the second half of the document might not be retrieved, and the
    model won’t be able to use its information.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 小块大小然而，可能会导致重要信息的丢失。想象一个包含关于主题X的重要信息的文档，但X仅在文档的前半部分提到。如果你将这个文档分成两个块，文档的后半部分可能不会被检索到，模型也无法使用其信息。
- en: Smaller chunk sizes can also increase computational overhead. This is especially
    an issue for embedding-based retrieval. Halving the chunk size means that you
    have twice as many chunks to index and twice as many embedding vectors to generate
    and store. Your vector search space will be twice as big, which can reduce the
    query speed.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的块大小也可能增加计算开销。这对于基于嵌入的检索尤其是一个问题。将块大小减半意味着你需要索引两倍的块，生成和存储两倍的嵌入向量。你的向量搜索空间将扩大一倍，这可能会降低查询速度。
- en: There is no universal best chunk size or overlap size. You have to experiment
    to find what works best for you.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 没有通用的最佳块大小或重叠大小。你必须进行实验以找到最适合你的方案。
- en: Reranking
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新排序
- en: The initial document rankings generated by the retriever can be further reranked
    to be more accurate. Reranking is especially useful when you need to reduce the
    number of retrieved documents, either to fit them into your model’s context or
    to reduce the number of input tokens.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器生成的初始文档排名可以进一步重新排序以更准确。重新排序在需要减少检索到的文档数量时特别有用，无论是为了适应你的模型上下文还是为了减少输入标记的数量。
- en: One common pattern for reranking is discussed in [“Combining retrieval algorithms”](#ch06_combining_retrieval_algorithms_1730157386571869).
    A cheap but less precise retriever fetches candidates, then a more precise but
    more expensive mechanism reranks these candidates.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“结合检索算法”](#ch06_combining_retrieval_algorithms_1730157386571869)中讨论了一种常见的重新排序模式。一个便宜但不太精确的检索器获取候选项，然后一个更精确但更昂贵的机制对这些候选项进行重新排序。
- en: Documents can also be reranked based on time, giving higher weight to more recent
    data. This is useful for time-sensitive applications such as news aggregation,
    chat with your emails (e.g., a chatbot that can answer questions about your emails),
    or stock market analysis.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 文档也可以根据时间重新排序，给予较近的数据更高的权重。这对于需要时效性应用，如新闻聚合、与电子邮件的聊天（例如，可以回答有关你的电子邮件问题的聊天机器人）或股市分析非常有用。
- en: Context reranking differs from traditional search reranking in that the exact
    position of items is less critical. In search, the rank (e.g., first or fifth)
    is crucial. In context reranking, the order of documents still matters because
    it affects how well a model can process them. Models might better understand documents
    at the beginning and end of the context, as discussed in [“Context Length and
    Context Efficiency”](ch05.html#ch05a_context_length_and_context_efficiency_1730156991195850).
    However, as long as a document is included, the impact of its order is less significant
    compared to search ranking.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文重新排序与传统搜索重新排序的不同之处在于，项目的确切位置不太关键。在搜索中，排名（例如，第一或第五）至关重要。在上下文重新排序中，文档的顺序仍然很重要，因为它会影响模型处理文档的效果。模型可能更好地理解上下文的开头和结尾的文档，如[“上下文长度和上下文效率”](ch05.html#ch05a_context_length_and_context_efficiency_1730156991195850)中讨论的那样。然而，只要文档被包含在内，其顺序的影响与搜索排名相比就不那么重要了。
- en: Query rewriting
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询重写
- en: '*Query rewriting* is also known as query reformulation, query normalization,
    and sometimes query expansion. Consider the following conversation:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*查询重写*也被称为查询重构、查询归一化和有时是查询扩展。考虑以下对话：'
- en: '[PRE0]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The last question, “How about Emily Doe?”, is ambiguous without context. If
    you use this query verbatim to retrieve documents, you’ll likely get irrelevant
    results. You need to rewrite this query to reflect what the user is actually asking.
    The new query should make sense on its own. In this case, the query should be
    rewritten to “When was the last time Emily Doe bought something from us?”
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个问题，“Emily Doe怎么样？”在没有上下文的情况下是模糊的。如果你直接使用这个查询来检索文档，你很可能会得到不相关的结果。你需要重写这个查询以反映用户实际提出的问题。新的查询应该单独有意义。在这种情况下，查询应该重写为“Emily
    Doe上次是从我们这里买东西是什么时候？”
- en: While I put query rewriting in [“RAG”](#ch06_rag_1730157386571628), query rewriting
    isn’t unique to RAG. In traditional search engines, query rewriting is often done
    using heuristics. In AI applications, query rewriting can also be done using other
    AI models, using a prompt similar to “Given the following conversation, rewrite
    the last user input to reflect what the user is actually asking”. [Figure 6-4](#ch06_figure_4_1730157386529250)
    shows how ChatGPT rewrote the query using this prompt.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我在“RAG”中提到了查询重写[“RAG”](#ch06_rag_1730157386571628)，但查询重写并不局限于RAG。在传统搜索引擎中，查询重写通常使用启发式方法完成。在AI应用中，查询重写也可以使用其他AI模型完成，使用类似于“给定以下对话，重写最后用户的输入以反映用户实际询问的内容”的提示。[图6-4](#ch06_figure_4_1730157386529250)展示了ChatGPT如何使用此提示重写查询。
- en: '![A screenshot of a chat'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![聊天截图'
- en: Description automatically generated](assets/aien_0604.png)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](assets/aien_0604.png)
- en: Figure 6-4\. You can use other generative models to rewrite queries.
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4。您可以使用其他生成模型来重写查询。
- en: Query rewriting can get complicated, especially if you need to do identity resolution
    or incorporate other knowledge. For example, if the user asks “How about his wife?”
    you will first need to query your database to find out who his wife is. If you
    don’t have this information, the rewriting model should acknowledge that this
    query isn’t solvable instead of hallucinating a name, leading to a wrong answer.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 查询重写可能会变得复杂，尤其是如果您需要执行身份解析或结合其他知识。例如，如果用户问“他的妻子怎么样？”您首先需要查询数据库以找出他的妻子是谁。如果您没有这些信息，重写模型应承认此查询无法解决，而不是凭空猜测一个名字，导致错误答案。
- en: Contextual retrieval
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文检索
- en: The idea behind contextual retrieval is to augment each chunk with relevant
    context to make it easier to retrieve the relevant chunks. A simple technique
    is to augment a chunk with metadata like tags and keywords. For ecommerce, a product
    can be augmented by its description and reviews. Images and videos can be queried
    by their titles or captions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文检索背后的理念是向每个片段添加相关上下文，以便更容易检索相关片段。一种简单的方法是向片段添加元数据，如标签和关键词。对于电子商务，可以通过描述和评论来增强产品。可以通过标题或字幕查询图片和视频。
- en: The metadata may also include entities automatically extracted from the chunk.
    If your document contains specific terms like the error code EADDRNOTAVAIL (99),
    adding them to the document’s metadata allows the system to retrieve it by that
    keyword, even after the document has been converted into embeddings.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据还可以包括从片段中自动提取的实体。如果您的文档包含特定的术语，如错误代码EADDRNOTAVAIL（99），将其添加到文档的元数据中，允许系统通过该关键词检索它，即使文档已被转换为嵌入。
- en: You can also augment each chunk with the questions it can answer. For customer
    support, you can augment each article with related questions. For example, the
    article on how to reset your password can be augmented with queries like “How
    to reset password?”, “I forgot my password”, “I can’t log in”, or even “Help,
    I can’t find my account”.^([9](ch06.html#id1296))
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以向每个片段添加它可以回答的问题。对于客户支持，您可以向每篇文章添加相关问题。例如，关于如何重置密码的文章可以增强为查询“如何重置密码？”，“我忘记了密码”，“我无法登录”，甚至“帮助，我找不到我的账户”。^([9](ch06.html#id1296))
- en: 'If a document is split into multiple chunks, some chunks might lack the necessary
    context to help the retriever understand what the chunk is about. To avoid this,
    you can augment each chunk with the context from the original document, such as
    the original document’s title and summary. Anthropic used AI models to generate
    a short context, usually 50-100 tokens, that explains the chunk and its relationship
    to the original document. Here’s the prompt Anthropic used for this purpose ([Anthropic,
    2024](https://oreil.ly/-Sny7)):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文档被分成多个片段，某些片段可能缺乏帮助检索器理解片段内容的必要上下文。为了避免这种情况，您可以为每个片段添加来自原始文档的上下文，例如原始文档的标题和摘要。Anthropic使用AI模型生成简短上下文，通常50-100个标记，解释片段及其与原始文档的关系。以下是Anthropic用于此目的的提示([Anthropic,
    2024](https://oreil.ly/-Sny7))：
- en: '[PRE1]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The generated context for each chunk is prepended to each chunk, and the augmented
    chunk is then indexed by the retrieval algorithm. [Figure 6-5](#ch06_figure_5_1730157386529262)
    visualizes the process that Anthropic follows.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 每个片段生成的上下文都添加到每个片段之前，然后增强后的片段由检索算法索引。[图6-5](#ch06_figure_5_1730157386529262)展示了Anthropic遵循的过程。
- en: '![A diagram of a process'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![流程图'
- en: Description automatically generated](assets/aien_0605.png)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0605.png)
- en: Figure 6-5\. Anthropic augments each chunk with a short context that situates
    this chunk within the original document, making it easier for the retriever to
    find the relevant chunks given a query. Image from “Introducing Contextual Retrieval”
    (Anthropic, 2024).
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5。Anthropic通过在每个块中添加一个简短上下文来增强每个块，将此块定位在原始文档中，这使得检索器在给定查询的情况下更容易找到相关块。图片来自“介绍上下文检索”（Anthropic，2024）。
- en: RAG Beyond Texts
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG超越文本
- en: The last section discussed text-based RAG systems where the external data sources
    are text documents. However, external data sources can also be multimodal and
    tabular data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节讨论了基于文本的RAG系统，其中外部数据源是文本文档。然而，外部数据源也可以是多模态和表格数据。
- en: Multimodal RAG
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模态RAG
- en: If your generator is multimodal, its contexts might be augmented not only with
    text documents but also with images, videos, audio, etc., from external sources.
    I’ll use images in the examples to keep the writing concise, but you can replace
    images with any other modality. Given a query, the retriever fetches both texts
    and images relevant to it. For example, given “What’s the color of the house in
    the Pixar movie Up?” the retriever can fetch a picture of the house in *Up* to
    help the model answer, as shown in [Figure 6-6](#ch06_figure_6_1730157386529270).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的生成器是多模态的，其上下文不仅可以增强文本文档，还可以增强来自外部来源的图像、视频、音频等。我将使用图像作为示例以保持写作简洁，但你可以用任何其他模态来替换图像。给定一个查询，检索器会检索与查询相关的文本和图像。例如，给定“皮克斯电影《Up》中房子的颜色是什么？”的查询，检索器可以检索《Up》中房子的图片以帮助模型回答，如图[图6-6](#ch06_figure_6_1730157386529270)所示。
- en: '![A diagram of a diagram'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图表的图表'
- en: Description automatically generated](assets/aien_0606.png)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0606.png)
- en: Figure 6-6\. Multimodal RAG can augment a query with both text and images. (*The
    real image from *Up* is not used, for copyright reasons.)
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6。多模态RAG可以同时用文本和图像增强查询。（由于版权原因，未使用《Up》中的真实图像。）
- en: If the images have metadata—such as titles, tags, and captions—they can be retrieved
    using the metadata. For example, an image is retrieved if its caption is considered
    relevant to the query.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图像有元数据——例如标题、标签和字幕——可以使用元数据进行检索。例如，如果图像的字幕被认为与查询相关，则可以检索该图像。
- en: 'If you want to retrieve images based on their content, you’ll need to have
    a way to compare images to queries. If queries are texts, you’ll need a multimodal
    embedding model that can generate embeddings for both images and texts. Let’s
    say you use CLIP ([Radford et al., 2021](https://arxiv.org/abs/2103.00020)) as
    the multimodal embedding model. The retriever works as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想根据内容检索图像，你需要一种比较图像与查询的方法。如果查询是文本，你需要一个多模态嵌入模型，该模型可以为图像和文本生成嵌入。假设你使用CLIP
    ([Radford等人，2021](https://arxiv.org/abs/2103.00020)) 作为多模态嵌入模型。检索器的工作方式如下：
- en: Generate CLIP embeddings for all your data, both texts and images, and store
    them in a vector database.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为你的所有数据生成CLIP嵌入，包括文本和图像，并将它们存储在向量数据库中。
- en: Given a query, generate its CLIP embedding.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个查询，生成其CLIP嵌入。
- en: Query in the vector database for all images and texts whose embeddings are close
    to the query embedding.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在向量数据库中查询所有与查询嵌入相似的图像和文本。
- en: RAG with tabular data
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RAG与表格数据
- en: Most applications work not only with unstructured data like texts and images
    but also with tabular data. Many queries might need information from data tables
    to answer. The workflow for augmenting a context using tabular data is significantly
    different from the classic RAG workflow.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数应用程序不仅与无结构数据（如文本和图像）一起工作，还与表格数据一起工作。许多查询可能需要从数据表中获取信息来回答。使用表格数据增强上下文的流程与经典的RAG流程有显著不同。
- en: Imagine you work for an ecommerce site called Kitty Vogue that specializes in
    cat fashion. This store has an order table named Sales, as shown in [Table 6-3](#ch06_table_3_1730157386543408).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你为一家名为Kitty Vogue的电子商务公司工作，该公司专门经营猫时尚。这家商店有一个名为Sales的订单表，如图[表6-3](#ch06_table_3_1730157386543408)所示。
- en: Table 6-3\. An example of an order table, Sales, for the imaginary ecommerce
    site Kitty Vogue.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-3。一个订单表示例，Kitty Vogue假想电子商务网站的Sales。
- en: '| Order ID | Timestamp | Product ID | Product | Unit price ($) | Units | Total
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 订单ID | 时间戳 | 产品ID | 产品 | 单价（$） | 单位 | 总计 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | … | 2044 | Meow Mix Seasoning | 10.99 | 1 | 10.99 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 1 | … | 2044 | Meow Mix调味料 | 10.99 | 1 | 10.99 |'
- en: '| 2 | … | 3492 | Purr & Shake | 25 | 2 | 50 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 2 | … | 3492 | Purr & Shake | 25 | 2 | 50 |'
- en: '| 3 | … | 2045 | Fruity Fedora | 18 | 1 | 18 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 3 | … | 2045 | Fruity Fedora | 18 | 1 | 18 |'
- en: '| … | … | … | … | … | … | … |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … | … | … | … | … |'
- en: 'To generate a response to the question “How many units of Fruity Fedora were
    sold in the last 7 days?”, your system needs to query this table for all orders
    involving Fruity Fedora and sum the number of units across all orders. Assume
    that this table can be queried using SQL. The SQL query might look like this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成对问题“在过去的 7 天内售出了多少单位的 Fruity Fedora？”的响应，您的系统需要查询涉及 Fruity Fedora 的所有订单，并汇总所有订单的单位数量。假设可以使用
    SQL 查询此表。SQL 查询可能如下所示：
- en: '[PRE2]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The workflow is as follows, visualized in [Figure 6-7](#ch06_figure_7_1730157386529276).
    To run this workflow, your system must have the ability to generate and execute
    the SQL query:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程如下，如图 [6-7](#ch06_figure_7_1730157386529276) 所示。要运行此工作流程，您的系统必须具备生成和执行 SQL
    查询的能力：
- en: 'Text-to-SQL: based on the user query and the provided table schemas, determine
    what SQL query is needed. Text-to-SQL is an example of semantic parsing, as discussed
    in [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359).'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本到 SQL：根据用户查询和提供的表模式，确定所需的 SQL 查询。文本到 SQL 是语义解析的一个例子，如第 2 章所述（ch02.html#ch02_understanding_foundation_models_1730147895571359）。
- en: 'SQL execution: execute the SQL query.'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SQL 执行：执行 SQL 查询。
- en: 'Generation: generate a response based on the SQL result and the original user
    query.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成：根据 SQL 结果和原始用户查询生成响应。
- en: '![A diagram of a product'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![产品图'
- en: Description automatically generated](assets/aien_0607.png)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0607.png)
- en: Figure 6-7\. A RAG system that augments context with tabular data.
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 增强表格数据的上下文的 RAG 系统。
- en: For the text-to-SQL step, if there are many available tables whose schemas can’t
    all fit into the model context, you might need an intermediate step to predict
    what tables to use for each query. Text-to-SQL can be done by the same generator
    that generates the final response or a specialized text-to-SQL model.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本到 SQL 的步骤，如果有许多可用的表，其模式无法全部适应模型上下文，您可能需要一个中间步骤来预测每个查询应使用哪些表。文本到 SQL 可以由生成最终响应的同一生成器或专门的文本到
    SQL 模型来完成。
- en: In this section, we’ve discussed how tools such as retrievers and SQL executors
    can enable models to handle more queries and generate higher-quality responses.
    Would giving a model access to more tools improve its capabilities even more?
    Tool use is a core characteristic of the agentic pattern, which we’ll discuss
    in the next section.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了检索器和 SQL 执行器等工具如何使模型能够处理更多查询并生成高质量的响应。给模型更多工具的访问是否会进一步提高其能力？工具的使用是代理模式的核心特征，我们将在下一节中讨论。
- en: Agents
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理
- en: 'Intelligent agents are considered by many to be the ultimate goal of AI. The
    classic book by Stuart Russell and Peter Norvig, *Artificial Intelligence: A Modern
    Approach* (Prentice Hall, 1995) defines the field of *artificial intelligence
    research* as “the study and design of rational agents.”'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 智能代理被许多人视为人工智能的最终目标。斯图尔特·罗素和彼得·诺维格的经典著作《人工智能：一种现代方法》（Prentice Hall，1995）将人工智能研究领域定义为“理性和代理的研究与设计。”
- en: The unprecedented capabilities of foundation models have opened the door to
    agentic applications that were previously unimaginable. These new capabilities
    make it finally possible to develop autonomous, intelligent agents to act as our
    assistants, coworkers, and coaches. They can help us create a website, gather
    data, plan a trip, do market research, manage a customer account, automate data
    entry, prepare us for interviews, interview our candidates, negotiate a deal,
    etc. The possibilities seem endless, and the potential economic value of these
    agents is enormous.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型前所未有的能力为以前无法想象的代理应用打开了大门。这些新能力最终使我们能够开发自主、智能的代理，作为我们的助手、同事和教练。他们可以帮助我们创建网站、收集数据、规划旅行、进行市场研究、管理客户账户、自动化数据录入、为我们准备面试、面试候选人、谈判交易等。可能性似乎无穷无尽，这些代理的经济价值潜力巨大。
- en: Warning
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: AI-powered agents are an emerging field, with no established theoretical frameworks
    for defining, developing, and evaluating them. This section is a best-effort attempt
    to build a framework from the existing literature, but it will evolve as the field
    does. Compared to the rest of the book, this section is more experimental.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能驱动的代理是一个新兴领域，没有为定义、开发和评估它们而建立的既定理论框架。本节是尝试从现有文献中构建框架的最佳努力，但随着该领域的发展，它将不断演变。与本书的其他部分相比，本节更具实验性。
- en: 'This section will start with an overview of agents, and then continue with
    two aspects that determine the capabilities of an agent: tools and planning. Agents,
    with their new modes of operations, have new modes of failures. This section will
    end with a discussion on how to evaluate agents to catch these failures.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将从代理的概述开始，然后继续讨论决定代理能力的两个方面：工具和规划。代理，凭借其新的操作模式，也有新的故障模式。本节将以讨论如何评估代理以捕捉这些故障结束。
- en: Even though agents are novel, they are built upon concepts that have already
    appeared in this book, including self-critique, chain-of-thought, and structured
    outputs.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代理是新颖的，但它们建立在本书中已经出现过的概念之上，包括自我批评、思维链和结构化输出。
- en: Agent Overview
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理概述
- en: The term *agent* has been used in many different engineering contexts, including
    but not limited to a software agent, intelligent agent, user agent, conversational
    agent, and reinforcement learning agent. So, what exactly is an agent?
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*代理*在许多不同的工程环境中被使用，包括但不限于软件代理、智能代理、用户代理、对话代理和强化学习代理。那么，究竟什么是代理呢？
- en: An agent is anything that can perceive its environment and act upon that environment.^([10](ch06.html#id1313))
    This means that an agent is characterized by the *environment* it operates in
    and *the set of actions* it can perform.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 代理是任何能够感知其环境并对该环境采取行动的东西。[10](ch06.html#id1313) 这意味着代理的特征在于其操作的环境和它能够执行的动作集。
- en: The *environment* an agent can operate in is defined by its use case. If an
    agent is developed to play a game (e.g., *Minecraft,* Go, *Dota*), that game is
    its environment. If you want an agent to scrape documents from the internet, the
    environment is the internet. If your agent is a cooking robot, the kitchen is
    its environment. A self-driving car agent’s environment is the road system and
    its adjacent areas.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以操作的环境由其用例定义。如果一个代理被开发来玩游戏（例如，*Minecraft*、围棋、*Dota*），那么这个游戏就是它的环境。如果你想让代理从互联网上抓取文档，那么环境就是互联网。如果你的代理是一个烹饪机器人，那么厨房就是它的环境。自动驾驶汽车代理的环境是道路系统及其相邻区域。
- en: The *set of actions* an AI agent can perform is augmented by the *tools* it
    has access to. Many generative AI-powered applications you interact with daily
    are agents with access to tools, albeit simple ones. ChatGPT is an agent. It can
    search the web, execute Python code, and generate images. RAG systems are agents,
    and text retrievers, image retrievers, and SQL executors are their tools.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能代理可以执行的动作集通过它能够访问的工具得到了增强。你每天与之互动的许多基于生成式AI的应用程序都是能够访问工具的代理，尽管这些工具很简单。ChatGPT
    是一个代理。它可以搜索网络、执行Python代码和生成图像。RAG系统是代理，文本检索器、图像检索器和SQL执行器是它们的工具。
- en: There’s a strong dependency between an agent’s environment and its set of tools.
    The environment determines what tools an agent can potentially use. For example,
    if the environment is a chess game, the only possible actions for an agent are
    the valid chess moves. However, an agent’s tool inventory restricts the environment
    it can operate in. For example, if a robot’s only action is swimming, it’ll be
    confined to a water environment.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的环境和其工具集之间存在强烈的依赖关系。环境决定了代理可能使用的工具。例如，如果环境是棋局，那么代理唯一可能采取的行动就是有效的棋步。然而，代理的工具库存限制了它能够操作的环境。例如，如果一个机器人的唯一行动是游泳，它将被限制在水中环境。
- en: '[Figure 6-8](#ch06_figure_8_1730157386529282) shows a visualization of SWE-agent
    ([Yang et al., 2024](https://arxiv.org/abs/2405.15793)), an agent built on top
    of GPT-4\. Its environment is the computer with the terminal and the file system.
    Its set of actions include navigate repo, search files, view files, and edit lines.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-8](#ch06_figure_8_1730157386529282) 展示了基于 GPT-4 的代理 SWE-agent ([Yang 等人，2024](https://arxiv.org/abs/2405.15793))
    的可视化。其环境是带有终端和文件系统的计算机。其动作集包括导航仓库、搜索文件、查看文件和编辑行。'
- en: '![A screenshot of a computer interface'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机界面的截图'
- en: Description automatically generated](assets/aien_0608.png)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0608.png)
- en: Figure 6-8\. SWE-agent (Yang et al., 2024) is a coding agent whose environment
    is the computer and whose actions include navigation, search, and editing. Adapted
    from an original image licensed under CC BY 4.0.
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8\. SWE-agent (Yang 等人，2024) 是一个编码代理，其环境是计算机，其动作包括导航、搜索和编辑。改编自一个许可协议为 CC
    BY 4.0 的原始图像。
- en: An AI agent is meant to accomplish tasks typically provided by the users in
    the inputs. In an AI agent, AI is the brain that processes the information it
    receives, including the task and feedback from the environment, plans a sequence
    of actions to achieve this task, and determines whether the task has been accomplished.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能代理旨在完成用户在输入中提供的典型任务。在人工智能代理中，AI是处理它接收到的信息的“大脑”，包括任务和环境反馈，规划一系列动作以实现此任务，并确定任务是否已完成。
- en: 'Let’s get back to the RAG system with tabular data in the Kitty Vogue example.
    This is a simple agent with three actions: response generation, SQL query generation,
    and SQL query execution. Given the query “Project the sales revenue for Fruity
    Fedora over the next three months”, the agent might perform the following sequence
    of actions:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到Kitty Vogue示例中的RAG系统，该系统包含表格数据。这是一个具有三个动作的简单代理：响应生成、SQL查询生成和SQL查询执行。给定查询“预测未来三个月Fruity
    Fedora的销售收入”，代理可能会执行以下动作序列：
- en: Reason about how to accomplish this task. It might decide that to predict future
    sales, it first needs the sales numbers from the last five years. Note that the
    agent’s reasoning is shown as its intermediate response.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推理如何完成此任务。它可能会决定，为了预测未来的销售，它首先需要过去五年的销售数据。请注意，代理的推理以它的中间响应形式展示。
- en: Invoke SQL query generation to generate the query to get sales numbers from
    the last five years.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用SQL查询生成来生成查询，以获取过去五年的销售数据。
- en: Invoke SQL query execution to execute this query.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用SQL查询执行来执行此查询。
- en: Reason about the tool outputs and how they help with sales prediction. It might
    decide that these numbers are insufficient to make a reliable projection, perhaps
    because of missing values. It then decides that it also needs information about
    past marketing campaigns.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推理工具输出以及它们如何帮助销售预测。它可能会决定，这些数字不足以做出可靠的预测，可能是因为缺失值。然后决定，它还需要有关过去营销活动的信息。
- en: Invoke SQL query generation to generate the queries for past marketing campaigns.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用SQL查询生成功能以生成过去营销活动的查询。
- en: Invoke SQL query execution.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用SQL查询执行。
- en: Reason that this new information is sufficient to help predict future sales.
    It then generates a projection.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推理新信息足以帮助预测未来的销售。然后生成预测。
- en: Reason that the task has been successfully completed.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推理任务已成功完成。
- en: 'Compared to non-agent use cases, agents typically require more powerful models
    for two reasons:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 与非代理用例相比，代理通常需要更强大的模型，原因有两个：
- en: 'Compound mistakes: an agent often needs to perform multiple steps to accomplish
    a task, and the overall accuracy decreases as the number of steps increases. If
    the model’s accuracy is 95% per step, over 10 steps, the accuracy will drop to
    60%, and over 100 steps, the accuracy will be only 0.6%.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复合错误：代理通常需要执行多个步骤才能完成任务，随着步骤数量的增加，整体准确性会降低。如果模型的每步准确率为95%，经过10步后，准确率将降至60%，经过100步后，准确率将仅为0.6%。
- en: 'Higher stakes: with access to tools, agents are capable of performing more
    impactful tasks, but any failure could have more severe consequences.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风险更高：有了工具的访问权限，代理能够执行更具影响力的任务，但任何失败都可能带来更严重的后果。
- en: A task that requires many steps can take time and money to run.^([11](ch06.html#id1315))
    However, if agents can be autonomous, they can save a lot of human time, making
    their costs worthwhile.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 需要许多步骤的任务可能需要时间和金钱来运行.^([11](ch06.html#id1315)) 然而，如果代理能够自主行动，它们可以节省大量的人力和时间，使它们的成本变得值得。
- en: Given an environment, the success of an agent in an environment depends on the
    tool inventory it has access to and the strength of its AI planner. Let’s start
    by looking into different kinds of tools a model can use.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定环境中，代理在该环境中的成功取决于其可访问的工具库存和其AI规划器的强度。让我们首先探讨模型可以使用的不同类型的工具。
- en: Tools
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具
- en: A system doesn’t need access to external tools to be an agent. However, without
    external tools, the agent’s capabilities would be limited. By itself, a model
    can typically perform one action—for example, an LLM can generate text, and an
    image generator can generate images. External tools make an agent vastly more
    capable.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一个系统不需要访问外部工具就可以成为代理。然而，没有外部工具，代理的能力将受到限制。单独的模型通常只能执行一个动作——例如，一个大型语言模型可以生成文本，一个图像生成器可以生成图像。外部工具使代理的能力大大增强。
- en: Tools help an agent to both perceive the environment and act upon it. Actions
    that allow an agent to perceive the environment are *read-only actions*, whereas
    actions that allow an agent to act upon the environment are *write actions*.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 工具帮助智能体感知环境并对其产生影响。允许智能体感知环境的动作是*只读动作*，而允许智能体对环境产生影响的是*写入动作*。
- en: This section gives an overview of external tools. How tools can be used will
    be discussed in [“Planning”](#ch06_planning_1730157386572280).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了外部工具。工具的使用方法将在[“规划”](#ch06_planning_1730157386572280)中进行讨论。
- en: The set of tools an agent has access to is its tool inventory. Since an agent’s
    tool inventory determines what an agent can do, it’s important to think through
    what and how many tools to give an agent. More tools give an agent more capabilities.
    However, the more tools there are, the more challenging it is to understand and
    utilize them well. Experimentation is necessary to find the right set of tools,
    as discussed in [“Tool selection”](#ch06_tool_selection_1730157386572520).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体可访问的工具集合是智能体的工具库存。由于智能体的工具库存决定了智能体能做什么，因此思考要给智能体提供哪些工具以及多少工具是很重要的。更多的工具给智能体带来更多的能力。然而，工具越多，理解和有效利用它们的挑战就越大。正如在[“工具选择”](#ch06_tool_selection_1730157386572520)中讨论的那样，实验是找到正确工具集的必要步骤。
- en: 'Depending on the agent’s environment, there are many possible tools. Here are
    three categories of tools that you might want to consider: knowledge augmentation
    (i.e., context construction), capability extension, and tools that let your agent
    act upon its environment.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 根据智能体的环境，有许多可能的工具。以下是你可能想要考虑的三种工具类别：知识增强（即，构建上下文）、能力扩展以及允许智能体对其环境产生影响的工具。
- en: Knowledge augmentation
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 知识增强
- en: 'I hope that this book, so far, has convinced you of the importance of having
    the relevant context for a model’s response quality. An important category of
    tools includes those that help augment your agent’s knowledge of your agent. Some
    of them have already been discussed: text retriever, image retriever, and SQL
    executor. Other potential tools include internal people search, an inventory API
    that returns the status of different products, Slack retrieval, an email reader,
    etc.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望到目前为止，这本书已经让你相信，对于模型响应质量的相关上下文的重要性。一类重要的工具包括那些帮助增强智能体对你智能体的知识。其中一些已经讨论过：文本检索器、图像检索器和SQL执行器。其他潜在的工具包括内部人员搜索、返回不同产品状态的库存API、Slack检索、电子邮件阅读器等。
- en: Many such tools augment a model with your organization’s private processes and
    information. However, tools can also give models access to public information,
    especially from the internet.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 许多此类工具通过你组织的私有流程和信息来增强模型。然而，工具也可以让模型访问公共信息，尤其是来自互联网的信息。
- en: Web browsing was among the earliest and most anticipated capabilities to be
    incorporated into chatbots like ChatGPT. Web browsing prevents a model from going
    stale. A model goes stale when the data it was trained on becomes outdated. If
    the model’s training data was cut off last week, it won’t be able to answer questions
    that require information from this week unless this information is provided in
    the context. Without web browsing, a model won’t be able to tell you about the
    weather, news, upcoming events, stock prices, flight status, etc.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 网络浏览是ChatGPT等聊天机器人最早且最期待的功能之一。网络浏览可以防止模型过时。当模型训练的数据变得过时，模型就会过时。如果模型的训练数据上周被切断，除非在上下文中提供这些信息，否则它将无法回答需要本周信息的问题。
- en: I use web browsing as an umbrella term to cover all tools that access the internet,
    including web browsers and specific APIs such as search APIs, news APIs, GitHub
    APIs, or social media APIs such as those of X, LinkedIn, and Reddit.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用网络浏览作为一个总称，涵盖所有访问互联网的工具，包括网络浏览器和特定的API，如搜索API、新闻API、GitHub API或社交媒体API，如X、LinkedIn和Reddit。
- en: While web browsing allows your agent to reference up-to-date information to
    generate better responses and reduce hallucinations, it can also open up your
    agent to the cesspools of the internet. Select your Internet APIs with care.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然网络浏览允许你的智能体引用最新信息以生成更好的响应并减少幻觉，但它也可能使你的智能体暴露于互联网的泥潭之中。仔细选择你的互联网API。
- en: Capability extension
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 能力扩展
- en: The second category of tools to consider are those that address the inherent
    limitations of AI models. They are easy ways to give your model a performance
    boost. For example, AI models are notorious for being bad at math. If you ask
    a model what is 199,999 divided by 292, the model will likely fail. However, this
    calculation is trivial if the model has access to a calculator. Instead of trying
    to train the model to be good at arithmetic, it’s a lot more resource-efficient
    to just give the model access to a tool.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的第二类工具是解决 AI 模型固有局限性的工具。它们是给模型提升性能的简单方法。例如，AI 模型因数学能力差而闻名。如果你问一个模型 199,999
    除以 292 等于多少，模型很可能会失败。然而，如果模型可以访问计算器，这个计算就变得微不足道。与其试图训练模型擅长算术，不如直接给模型提供工具访问权限，这样更有效率。
- en: Other simple tools that can significantly boost a model’s capability include
    a calendar, timezone converter, unit converter (e.g., from lbs to kg), and translator
    that can translate to and from the languages that the model isn’t good at.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可以显著提升模型能力的简单工具包括日历、时区转换器、单位转换器（例如，从磅到千克），以及能够翻译到模型不擅长的语言的翻译器。
- en: More complex but powerful tools are code interpreters. Instead of training a
    model to understand code, you can give it access to a code interpreter so that
    it can execute a piece of code, return the results, or analyze the code’s failures.
    This capability lets your agents act as coding assistants, data analysts, and
    even research assistants that can write code to run experiments and report results.
    However, automated code execution comes with the risk of code injection attacks,
    as discussed in [“Defensive Prompt Engineering”](ch05.html#ch05a_defensive_prompt_engineering_1730156991196256).
    Proper security measurements are crucial to keep you and your users safe.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂但功能强大的工具是代码解释器。你不必训练模型理解代码，而是可以给它提供代码解释器的访问权限，使其能够执行一段代码，返回结果或分析代码的失败。这种能力使你的代理能够作为编码助手、数据分析师，甚至能够编写代码运行实验并报告结果的科研助手。然而，自动代码执行伴随着代码注入攻击的风险，如“防御性提示工程”中讨论的。[“防御性提示工程”](ch05.html#ch05a_defensive_prompt_engineering_1730156991196256)。采取适当的安全措施对于保护你和你的用户至关重要。
- en: External tools can make a text-only or image-only model multimodal. For example,
    a model that can generate only texts can leverage a text-to-image model as a tool,
    allowing it to generate both texts and images. Given a text request, the agent’s
    AI planner decides whether to invoke text generation, image generation, or both.
    This is how ChatGPT can generate both text and images—it uses DALL-E as its image
    generator. Agents can also use a code interpreter to generate charts and graphs,
    a LaTeX compiler to render math equations, or a browser to render web pages from
    HTML code.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 外部工具可以使纯文本或纯图像模型实现多模态。例如，只能生成文本的模型可以利用文本到图像模型作为工具，使其能够同时生成文本和图像。给定一个文本请求，代理的
    AI 计划者决定是否调用文本生成、图像生成或两者都调用。这就是 ChatGPT 能够生成文本和图像的原因——它使用 DALL-E 作为其图像生成器。代理还可以使用代码解释器来生成图表和图形，LaTeX
    编译器来渲染数学方程式，或浏览器从 HTML 代码渲染网页。
- en: Similarly, a model that can process only text inputs can use an image captioning
    tool to process images and a transcription tool to process audio. It can use an
    OCR (optical character recognition) tool to read PDFs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，只能处理文本输入的模型可以使用图像描述工具处理图像，转录工具处理音频。它可以使用 OCR（光学字符识别）工具读取 PDF 文件。
- en: '*Tool use can significantly boost a model’s performance compared to just prompting
    or even finetuning*. Chameleon ([Lu et al., 2023](https://arxiv.org/abs/2304.09842))
    shows that a GPT-4-powered agent, augmented with a set of 13 tools, can outperform
    GPT-4 alone on several benchmarks. Examples of tools this agent used are knowledge
    retrieval, a query generator, an image captioner, a text detector, and Bing search.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '*工具的使用可以显著提升模型的表现，相较于仅仅提示或微调来说*。Chameleon ([Lu et al., 2023](https://arxiv.org/abs/2304.09842))
    展示了一个由 GPT-4 驱动的代理，通过增加一套 13 个工具，可以在多个基准测试中超越单独的 GPT-4。这个代理使用的工具示例包括知识检索、查询生成器、图像描述器、文本检测器和
    Bing 搜索。'
- en: On ScienceQA, a science question answering benchmark, Chameleon improves the
    best published few-shot result by 11.37%. On TabMWP (Tabular Math Word Problems)
    (Lu et al., 2022), a benchmark involving tabular math questions, Chameleon improves
    the accuracy by 17%.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在科学问答基准测试 ScienceQA 上，Chameleon 将最佳已发布少样本结果提升了 11.37%。在涉及表格数学问题的基准测试 TabMWP（Tabular
    Math Word Problems）(Lu et al., 2022) 上，Chameleon 将准确率提升了 17%。
- en: Write actions
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写动作
- en: So far, we’ve discussed read-only actions that allow a model to read from its
    data sources. But tools can also perform write actions, making changes to the
    data sources. A SQL executor can retrieve a data table (read) but can also change
    or delete the table (write). An email API can read an email but can also respond
    to it. A banking API can retrieve your current balance but can also initiate a
    bank transfer.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了只读操作，允许模型从其数据源中读取。但工具也可以执行写入操作，对数据源进行更改。一个 SQL 执行器可以检索数据表（读取），但也可以更改或删除表（写入）。一个电子邮件
    API 可以读取电子邮件，但也可以回复。一个银行 API 可以检索你的当前余额，但也可以发起银行转账。
- en: 'Write actions enable a system to do more. They can enable you to automate the
    whole customer outreach workflow: researching potential customers, finding their
    contacts, drafting emails, sending first emails, reading responses, following
    up, extracting orders, updating your databases with new orders, etc.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 写入操作使系统能够做更多。它们可以使你自动化整个客户接触工作流程：研究潜在客户、寻找他们的联系方式、起草电子邮件、发送第一封电子邮件、阅读回复、跟进、提取订单、用新订单更新你的数据库等。
- en: However, the prospect of giving AI the ability to automatically alter our lives
    is frightening. Just as you shouldn’t give an intern the authority to delete your
    production database, you shouldn’t allow an unreliable AI to initiate bank transfers.
    Trust in the system’s capabilities and its security measures is crucial. You need
    to ensure that the system is protected from bad actors who might try to manipulate
    it into performing harmful actions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，赋予人工智能自动改变我们生活的能力的前景令人恐惧。正如你不应该让实习生有权删除你的生产数据库一样，你不应该允许不可靠的人工智能发起银行转账。对系统能力和其安全措施的信任至关重要。你需要确保系统受到保护，防止不良分子试图操纵它执行有害行为。
- en: When I talk about autonomous AI agents to a group of people, there is often
    someone who brings up self-driving cars. “What if someone hacks into the car to
    kidnap you?” While the self-driving car example seems visceral because of its
    physicality, an AI system can cause harm without a presence in the physical world.
    It can manipulate the stock market, steal copyrights, violate privacy, reinforce
    biases, spread misinformation and propaganda, and more, as discussed in [“Defensive
    Prompt Engineering”](ch05.html#ch05a_defensive_prompt_engineering_1730156991196256).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当我向一群人谈论自主人工智能代理时，通常会有一些人提出自动驾驶汽车的问题。“如果有人黑入汽车绑架你怎么办？”虽然自动驾驶汽车示例因其物理性而显得直观，但人工智能系统可以在没有物理世界存在的情况下造成伤害。它可以操纵股市、窃取版权、侵犯隐私、强化偏见、传播错误信息和宣传，等等，正如在“防御性提示工程”中讨论的那样[“Defensive
    Prompt Engineering”](ch05.html#ch05a_defensive_prompt_engineering_1730156991196256)。
- en: These are all valid concerns, and any organization that wants to leverage AI
    needs to take safety and security seriously. However, this doesn’t mean that AI
    systems should never be given the ability to act in the real world. If we can
    get people to trust a machine to take us into space, I hope that one day, security
    measures will be sufficient for us to trust autonomous AI systems. Besides, humans
    can fail, too. Personally, I would trust a self-driving car more than the average
    stranger to drive me around.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是合理的担忧，任何希望利用人工智能的组织都需要认真对待安全和安保。然而，这并不意味着人工智能系统永远不会被赋予在现实世界中行动的能力。如果我们能让人们相信机器能带我们进入太空，我希望有一天，安全措施将足够让我们信任自主人工智能系统。此外，人类也会失败。就我个人而言，我会比普通陌生人更信任自动驾驶汽车带我在周围驾驶。
- en: Just as the right tools can help humans be vastly more productive—can you imagine
    doing business without Excel or building a skyscraper without cranes?—tools enable
    models to accomplish many more tasks. Many model providers already support tool
    use with their models, a feature often called function calling. Going forward,
    I would expect function calling with a wide set of tools to be common with most
    models.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 正如合适的工具可以帮助人类大幅提高生产力——你能想象没有 Excel 做生意或者没有起重机建造摩天大楼吗？——工具使模型能够完成更多任务。许多模型提供商已经通过他们的模型支持工具的使用，这一功能通常被称为函数调用。展望未来，我预计大多数模型都将普遍使用广泛工具集的函数调用。
- en: Planning
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规划
- en: At the heart of a foundation model agent is the model responsible for solving
    a task. A task is defined by its goal and constraints. For example, one task is
    to schedule a two-week trip from San Francisco to India with a budget of $5,000\.
    The goal is the two-week trip. The constraint is the budget.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型代理的核心是负责解决任务的模型。任务由其目标和约束定义。例如，一个任务是从旧金山到印度的两周旅行，预算为 5000 美元。目标是两周的旅行。约束是预算。
- en: Complex tasks require planning. The output of the planning process is a plan,
    which is a roadmap outlining the steps needed to accomplish a task. Effective
    planning typically requires the model to understand the task, consider different
    options to achieve this task, and choose the most promising one.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的任务需要规划。规划过程的结果是一个计划，这是一个概述任务所需步骤的路线图。有效的规划通常需要模型理解任务，考虑实现此任务的不同选项，并选择最有希望的方案。
- en: If you’ve ever been in any planning meeting, you know that planning is hard.
    As an important computational problem, planning is well studied and would require
    several volumes to cover. I’ll only be able to cover the surface here.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经参加过任何规划会议，你就会知道规划是多么困难。作为一个重要的计算问题，规划已经被广泛研究，要全面覆盖它可能需要几卷书。在这里我只能触及表面。
- en: Planning overview
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规划概述
- en: 'Given a task, there are many possible ways to decompose it, but not all of
    them will lead to a successful outcome. Among the correct solutions, some are
    more efficient than others. Consider the query, “How many companies without revenue
    have raised at least $1 billion?” There are many possible ways to solve this,
    but as an illustration, consider the two options:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个任务，有无数种分解它的方法，但并非所有方法都能导致成功的成果。在正确的解决方案中，有些比其他方案更有效率。考虑查询，“有多少没有收入的公司筹集了至少10亿美元？”有无数种解决这个问题的方法，但为了说明，考虑以下两种选项：
- en: Find all companies without revenue, then filter them by the amount raised.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有没有收入的公司，然后根据筹集的资金量进行筛选。
- en: Find all companies that have raised at least $1 billion, then filter them by
    revenue.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有至少筹集了10亿美元的公司，然后根据收入进行筛选。
- en: The second option is more efficient. There are vastly more companies without
    revenue than companies that have raised $1 billion. Given only these two options,
    an intelligent agent should choose option 2.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项更有效率。没有收入的公司的数量远远多于筹集了10亿美元的公司。给定这两个选项，一个智能代理应该选择第二个选项。
- en: You can couple planning with execution in the same prompt. For example, you
    give the model a prompt, ask it to think step by step (such as with a chain-of-thought
    prompt), and then execute those steps all in one prompt. But what if the model
    comes up with a 1,000-step plan that doesn’t even accomplish the goal? Without
    oversight, an agent can run those steps for hours, wasting time and money on API
    calls, before you realize that it’s not going anywhere.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在同一个提示中结合规划和执行。例如，你给模型一个提示，要求它逐步思考（例如使用思维链提示），然后在同一个提示中执行这些步骤。但假设模型提出一个1000步的计划，而这个计划甚至没有完成目标？如果没有监督，代理可能会运行这些步骤数小时，浪费时间和金钱在API调用上，在你意识到它没有进展之前。
- en: To avoid fruitless execution, *planning* should be decoupled from *execution*.
    You ask the agent to first generate a plan, and only after this plan is *validated*
    is it executed. The plan can be validated using heuristics. For example, one simple
    heuristic is to eliminate plans with invalid actions. If the generated plan requires
    a Google search and the agent doesn’t have access to Google Search, this plan
    is invalid. Another simple heuristic might be eliminating all plans with more
    than X steps. A plan can also be validated using AI judges. You can ask a model
    to evaluate whether the plan seems reasonable or how to improve it.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免无果而终的执行，*规划*应该与*执行*解耦。你要求代理首先生成一个计划，只有在这个计划被*验证*之后，它才会被执行。计划可以通过启发式方法进行验证。例如，一个简单的启发式方法是消除包含无效动作的计划。如果生成的计划需要谷歌搜索，而代理没有访问谷歌搜索的权限，那么这个计划就是无效的。另一个简单的启发式方法可能是消除所有包含超过X个步骤的计划。计划也可以通过人工智能法官进行验证。你可以要求模型评估计划是否合理，或者如何改进它。
- en: If the generated plan is evaluated to be bad, you can ask the planner to generate
    another plan. If the generated plan is good, execute it. If the plan consists
    of external tools, function calling will be invoked. Outputs from executing this
    plan will then again need to be evaluated. Note that the generated plan doesn’t
    have to be an end-to-end plan for the whole task. It can be a small plan for a
    subtask. The whole process looks like [Figure 6-9](#ch06_figure_9_1730157386529290).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生成的计划被评估为不好，你可以要求规划者生成另一个计划。如果生成的计划是好的，就执行它。如果计划包含外部工具，将调用函数调用。执行此计划产生的输出将再次需要评估。请注意，生成的计划不必是整个任务的端到端计划。它可以是一个子任务的较小计划。整个过程看起来像[图6-9](#ch06_figure_9_1730157386529290)。
- en: '![A diagram of a tool'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '![工具的示意图'
- en: Description automatically generated](assets/aien_0609.png)
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0609.png)
- en: Figure 6-9\. Decoupling planning and execution so that only validated plans
    are executed.
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9\. 解耦计划和执行，以确保只执行经过验证的计划。
- en: 'Your system now has three components: one to generate plans, one to validate
    plans, and another to execute plans. If you consider each component an agent,
    this is a multi-agent system.^([12](ch06.html#id1324))'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 您的系统现在有三个组件：一个用于生成计划，一个用于验证计划，另一个用于执行计划。如果您将每个组件视为一个代理，这是一个多代理系统。[^([12](ch06.html#id1324))]
- en: To speed up the process, instead of generating plans sequentially, you can generate
    several plans in parallel and ask the evaluator to pick the most promising one.
    This is another latency/cost trade-off, as generating multiple plans simultaneously
    will incur extra costs.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快进程，您可以在并行生成多个计划的同时，让评估者挑选最有希望的一个。这又是一个延迟/成本权衡，因为同时生成多个计划将产生额外的成本。
- en: 'Planning requires understanding the intention behind a task: what’s the user
    trying to do with this query? An intent classifier is often used to help agents
    plan. As shown in [“Break Complex Tasks into Simpler Subtasks”](ch05.html#ch05a_break_complex_tasks_into_simpler_subtasks_1730156991196113),
    intent classification can be done using another prompt or a classification model
    trained for this task. The intent classification mechanism can be considered another
    agent in your multi-agent system.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 计划需要理解任务背后的意图：用户试图用这个查询做什么？意图分类器通常用于帮助代理计划。如[“将复杂任务分解为更简单的子任务”](ch05.html#ch05a_break_complex_tasks_into_simpler_subtasks_1730156991196113)所示，意图分类可以使用另一个提示或为此任务训练的分类模型来完成。意图分类机制可以被视为您多代理系统中的另一个代理。
- en: Knowing the intent can help the agent pick the right tools. For example, for
    customer support, if the query is about billing, the agent might need access to
    a tool to retrieve a user’s recent payments. But if the query is about how to
    reset a password, the agent might need to access documentation retrieval.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 了解意图可以帮助代理选择正确的工具。例如，对于客户支持，如果查询是关于账单，代理可能需要访问一个工具来检索用户的最近付款。但如果查询是关于如何重置密码，代理可能需要访问文档检索。
- en: Tip
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Some queries might be out of the scope of the agent. The intent classifier should
    be able to classify requests as IRRELEVANT so that the agent can politely reject
    those instead of wasting FLOPs coming up with impossible solutions.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一些查询可能超出了代理的范围。意图分类器应该能够将请求分类为**不相关**，这样代理就可以礼貌地拒绝这些请求，而不是浪费浮点运算资源来提出不可能的解决方案。
- en: 'So far, we’ve assumed that the agent automates all three stages: generating
    plans, validating plans, and executing plans. In reality, humans can be involved
    at any of those stages to aid with the process and mitigate risks. A human expert
    can provide a plan, validate a plan, or execute parts of a plan. For example,
    for complex tasks for which an agent has trouble generating the whole plan, a
    human expert can provide a high-level plan that the agent can expand upon. If
    a plan involves risky operations, such as updating a database or merging a code
    change, the system can ask for explicit human approval before executing or let
    humans execute these operations. To make this possible, you need to clearly define
    the level of automation an agent can have for each action.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设代理自动化了所有三个阶段：生成计划、验证计划和执行计划。在现实中，人类可以在这些阶段中的任何一个阶段参与以帮助过程并降低风险。一个人类专家可以提供计划、验证计划或执行计划的一部分。例如，对于代理难以生成整个计划的复杂任务，人类专家可以提供一个高级计划，代理可以在此基础上扩展。如果计划涉及风险操作，例如更新数据库或合并代码更改，系统可以在执行之前要求明确的人类批准，或者让人类执行这些操作。为了使这成为可能，您需要明确定义代理对每个动作可以拥有的自动化程度。
- en: 'To summarize, solving a task typically involves the following processes. Note
    that reflection isn’t mandatory for an agent, but it’ll significantly boost the
    agent’s performance:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，解决一个任务通常涉及以下过程。请注意，反思对于代理不是强制性的，但它将显著提高代理的性能：
- en: '*Plan generation*: come up with a plan for accomplishing this task. A plan
    is a sequence of manageable actions, so this process is also called task decomposition.'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计划生成*：为完成这个任务想出一个计划。计划是一系列可管理的行动，因此这个过程也被称为任务分解。'
- en: '*Reflection and error correction*: evaluate the generated plan. If it’s a bad
    plan, generate a new one.'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*反思和错误纠正*：评估生成的计划。如果它是一个糟糕的计划，就生成一个新的计划。'
- en: '*Execution*: take the actions outlined in the generated plan. This often involves
    calling specific functions.'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*执行*：采取在生成的计划中概述的行动。这通常涉及调用特定的函数。'
- en: '*Reflection and error correction*: upon receiving the action outcomes, evaluate
    these outcomes and determine whether the goal has been accomplished. Identify
    and correct mistakes. If the goal is not completed, generate a new plan.'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*反思和错误纠正*：在收到行动结果后，评估这些结果并确定目标是否已经实现。识别并纠正错误。如果目标没有完成，生成一个新的计划。'
- en: You’ve already seen some techniques for plan generation and reflection in this
    book. When you ask a model to “think step by step”, you’re asking it to decompose
    a task. When you ask a model to “verify if your answer is correct”, you’re asking
    it to reflect.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在这本书中看到了一些关于计划生成和反思的技术。当你要求模型“逐步思考”时，你是在要求它分解一个任务。当你要求模型“验证你的答案是否正确”时，你是在要求它进行反思。
- en: Foundation models as planners
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础模型作为规划者
- en: An open question is how well foundation models can plan. Many researchers believe
    that foundation models, at least those built on top of autoregressive language
    models, cannot. Meta’s Chief AI Scientist Yann LeCun states unequivocally that
    [autoregressive LLMs can’t plan](https://x.com/ylecun/status/1702027572077326505)
    (2023). In the article “Can LLMs Really Reason and Plan?” [Kambhampati (2023)](https://oreil.ly/8_j7E)
    argues that LLMs are great at extracting knowledge but not planning. Kambhampati
    suggests that the papers claiming planning abilities of LLMs confuse general planning
    knowledge extracted from the LLMs with executable plans. “The plans that come
    out of LLMs may look reasonable to the lay user, and yet lead to execution time
    interactions and errors.”
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 一个开放的问题是基础模型能够规划得有多好。许多研究人员认为，基础模型，至少是建立在自回归语言模型之上的那些，不能。Meta 的首席人工智能科学家 Yann
    LeCun 明确表示[自回归语言模型无法规划](https://x.com/ylecun/status/1702027572077326505)（2023）。在文章“LLMs
    真的能推理和规划吗？”中，[Kambhampati (2023)](https://oreil.ly/8_j7E) 认为 LLMs 在提取知识方面很出色，但不擅长规划。Kambhampati
    指出，声称 LLMs 具有规划能力的论文混淆了从 LLMs 中提取的通用规划知识与可执行的计划。“从 LLMs 中产生的计划可能对普通用户来说看起来合理，但实际上会导致执行时间交互和错误。”
- en: However, while there is a lot of anecdotal evidence that LLMs are poor planners,
    it’s unclear whether it’s because we don’t know how to use LLMs the right way
    or because LLMs, fundamentally, can’t plan.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管有大量关于 LLMs 是糟糕规划者的轶事证据，但还不清楚这是否是因为我们不知道如何正确使用 LLMs，还是因为 LLMs 本质上无法规划。
- en: '*Planning, at its core, is a search problem*. You search among different paths
    to the goal, predict the outcome (reward) of each path, and pick the path with
    the most promising outcome. Often, you might determine that no path exists that
    can take you to the goal.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*规划本质上是一个搜索问题*。你在不同的通往目标的路径中进行搜索，预测每条路径的（奖励）结果，并选择最有希望的路径。通常，你可能会确定没有一条路径可以带你到达目标。'
- en: 'Search often requires *backtracking*. For example, imagine you’re at a step
    where there are two possible actions: A and B. After taking action A, you enter
    a state that’s not promising, so you need to backtrack to the previous state to
    take action B.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索通常需要*回溯*。例如，想象你处于一个有两个可能行动：A 和 B 的步骤。执行行动 A 后，你进入了一个没有希望的状态，因此你需要回溯到上一个状态来执行行动
    B。
- en: Some people argue that an autoregressive model can only generate forward actions.
    It can’t backtrack to generate alternate actions. Because of this, they conclude
    that autoregressive models can’t plan. However, this isn’t necessarily true. After
    executing a path with action A, if the model determines that this path doesn’t
    make sense, it can revise the path using action B instead, effectively backtracking.
    The model can also always start over and choose another path.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为自回归模型只能生成向前行动。它不能回溯来生成替代行动。正因为如此，他们得出结论，自回归模型无法规划。然而，这并不一定正确。在执行了带有行动 A
    的路径后，如果模型确定这条路径没有意义，它可以使用行动 B 修改路径，从而有效地回溯。模型也可以始终从头开始并选择另一条路径。
- en: It’s also possible that LLMs are poor planners because they aren’t given the
    toolings needed to plan. To plan, it’s necessary to know not only the available
    actions but also *the potential outcome of each action*. As a simple example,
    let’s say you want to walk up a mountain. Your potential actions are turn right,
    turn left, turn around, or go straight ahead. However, if turning right will cause
    you to fall off the cliff, you might not want to consider this action. In technical
    terms, an action takes you from one state to another, and it’s necessary to know
    the outcome state to determine whether to take an action.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能是因为LLM（大型语言模型）规划能力不足，因为它们没有获得进行规划所需的工具。要进行规划，不仅需要知道可用的动作，还需要了解每个动作的*潜在结果*。以一个简单的例子来说，假设你想要爬上一座山。你的潜在动作包括向右转、向左转、转身或直行。然而，如果你向右转会导致你从悬崖上掉下去，你可能不想考虑这个动作。从技术角度讲，一个动作将你从一个状态转移到另一个状态，了解结果状态是确定是否采取该动作的必要条件。
- en: This means it’s not sufficient to prompt a model to generate only a sequence
    of actions like what the popular chain-of-thought prompting technique does. The
    paper “Reasoning with Language Model is Planning with World Model” ([Hao et al.,
    2023](https://arxiv.org/abs/2305.14992)) argues that an LLM, by containing so
    much information about the world, is capable of predicting the outcome of each
    action. This LLM can incorporate this outcome prediction to generate coherent
    plans.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着仅仅提示模型生成一系列动作，就像流行的思维链提示技术所做的那样，是不够的。论文“使用语言模型进行推理即使用世界模型进行规划”（[Hao等人，2023](https://arxiv.org/abs/2305.14992)）认为，由于包含大量关于世界的信息，LLM能够预测每个动作的结果。这个LLM可以将这种结果预测纳入，以生成连贯的计划。
- en: Even if AI can’t plan, it can still be a part of a planner. It might be possible
    to augment an LLM with a search tool and state tracking system to help it plan.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 即使AI不能进行规划，它仍然可以是规划器的一部分。可能通过将搜索工具和状态跟踪系统添加到LLM中，帮助它进行规划。
- en: Plan generation
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计划生成
- en: 'The simplest way to turn a model into a plan generator is with prompt engineering.
    Imagine that you want to create an agent to help customers learn about products
    at Kitty Vogue. You give this agent access to three external tools: retrieve products
    by price, retrieve top products, and retrieve product information. Here’s an example
    of a prompt for plan generation. This prompt is for illustration purposes only.
    Production prompts are likely more complex:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型转变为计划生成器的最简单方法是通过提示工程。想象一下，你想要创建一个代理来帮助Kitty Vogue的客户了解产品。你给这个代理提供访问三个外部工具的权限：按价格检索产品、检索热门产品和检索产品信息。以下是一个用于计划生成的提示示例。这个提示仅用于说明目的。生产提示可能更复杂：
- en: '[PRE3]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'There are two things to note about this example:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个例子有两点需要注意：
- en: The plan format used here—a list of functions whose parameters are inferred
    by the agent—is just one of many ways to structure the agent control flow.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里使用的计划格式——一个由代理推断参数的函数列表——只是结构化代理控制流的方式之一。
- en: The `generate_query` function takes in the task’s current history and the most
    recent tool outputs to generate a query to be fed into the response generator.
    The tool output at each step is added to the task’s history.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_query`函数接收任务当前的历史记录和最新的工具输出，以生成要输入到响应生成器的查询。每个步骤的工具输出都添加到任务的历史记录中。'
- en: 'Given the user input “What’s the price of the best-selling product last week”,
    a generated plan might look like this:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 给定用户输入“上周最畅销产品的价格是多少”，生成的计划可能看起来像这样：
- en: '[PRE4]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You might wonder, “What about the parameters needed for each function?” The
    exact parameters are hard to predict in advance since they are often extracted
    from the previous tool outputs. If the first step, `get_time()`, outputs “2030-09-13”,
    then the agent can reason that the parameters for the next step should be called
    with the following parameters:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，“每个函数所需的参数是什么？”由于它们通常是从之前的工具输出中提取的，因此很难提前预测确切的参数。如果第一步`get_time()`输出“2030-09-13”，那么代理可以推理出下一步的参数应该使用以下参数调用：
- en: '[PRE5]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Often, there’s insufficient information to determine the exact parameter values
    for a function. For example, if a user asks, “What’s the average price of best-selling
    products?”, the answers to the following questions are unclear:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，信息不足，无法确定函数的确切参数值。例如，如果用户问，“最畅销产品的平均价格是多少？”，以下问题的答案并不明确：
- en: How many best-selling products does the user want to look at?
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户想查看多少个最畅销产品？
- en: Does the user want the best-selling products last week, last month, or of all
    time?
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户想要上周、上个月还是所有时间段的畅销产品？
- en: This means that models frequently have to guess, and guesses can be wrong.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着模型经常需要猜测，而猜测可能是错误的。
- en: Because both the action sequence and the associated parameters are generated
    by AI models, they can be hallucinated. Hallucinations can cause the model to
    call an invalid function or call a valid function but with wrong parameters. Techniques
    for improving a model’s performance in general can be used to improve a model’s
    planning capabilities.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 因为动作序列和相关参数都是由AI模型生成的，所以它们可能会出现幻觉。幻觉可能导致模型调用无效函数，或者调用有效函数但参数错误。可以用于提高模型性能的一般技术可以用来提高模型的规划能力。
- en: 'Here are a few approaches to make an agent better at planning:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些方法可以使代理在规划方面表现得更好：
- en: Write a better system prompt with more examples.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写更好的系统提示，包含更多示例。
- en: Give better descriptions of the tools and their parameters so that the model
    understands them better.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供更好的工具及其参数描述，以便模型更好地理解它们。
- en: Rewrite the functions themselves to make them simpler, such as refactoring a
    complex function into two simpler functions.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新编写函数本身，使其更简单，例如将一个复杂的函数重构为两个更简单的函数。
- en: Use a stronger model. In general, stronger models are better at planning.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更强的模型。一般来说，更强的模型在规划方面表现得更好。
- en: Finetune a model for plan generation.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调模型以生成计划。
- en: Function calling
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 函数调用
- en: 'Many model providers offer tool use for their models, effectively turning their
    models into agents. A tool is a function. Invoking a tool is, therefore, often
    called *function calling*. Different model APIs work differently, but in general,
    function calling works as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型提供商为他们的模型提供工具使用，有效地将他们的模型转变为代理。工具是一个函数。因此，调用工具通常被称为*函数调用*。不同的模型API工作方式不同，但一般来说，函数调用工作如下：
- en: '*Create a tool inventory.*'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建工具清单。*'
- en: Declare all the tools that you might want a model to use. Each tool is described
    by its execution entry point (e.g., its function name), its parameters, and its
    documentation (e.g., what the function does and what parameters it needs).
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 声明所有你可能希望模型使用的工具。每个工具都由其执行入口点（例如，其函数名）、其参数和其文档（例如，函数做什么以及需要哪些参数）来描述。
- en: '*Specify what tools the agent can use.*'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*指定代理可以使用哪些工具。*'
- en: 'Because different queries might need different tools, many APIs let you specify
    a list of declared tools to be used per query. Some let you control tool use further
    by the following settings:'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因为不同的查询可能需要不同的工具，许多API允许你为每个查询指定要使用的声明工具列表。一些API允许你通过以下设置进一步控制工具的使用：
- en: '`required`'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`required`'
- en: The model must use at least one tool.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型必须使用至少一个工具。
- en: '`none`'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`none`'
- en: The model shouldn’t use any tool.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型不应使用任何工具。
- en: '`auto`'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`auto`'
- en: The model decides which tools to use.
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型决定使用哪些工具。
- en: Function calling is illustrated in [Figure 6-10](#ch06_figure_10_1730157386529297).
    This is written in pseudocode to make it representative of multiple APIs. To use
    a specific API, please refer to its documentation.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 函数调用在[图6-10](#ch06_figure_10_1730157386529297)中展示。这是用伪代码编写的，以使其代表多个API。要使用特定的API，请参阅其文档。
- en: '![A screenshot of a computer program'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机程序的截图'
- en: Description automatically generated](assets/aien_0610.png)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0610.png)
- en: Figure 6-10\. An example of a model using two simple tools.
  id: totrans-346
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-10\. 使用两个简单工具的模型示例。
- en: Given a query, an agent defined as in [Figure 6-10](#ch06_figure_10_1730157386529297)
    will automatically generate what tools to use and their parameters. Some function
    calling APIs will make sure that only valid functions are generated, though they
    won’t be able to guarantee the correct parameter values.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个查询，一个如[图6-10](#ch06_figure_10_1730157386529297)中定义的代理将自动生成要使用的工具及其参数。一些函数调用API将确保只生成有效的函数，尽管它们无法保证正确的参数值。
- en: 'For example, given the user query “How many kilograms are 40 pounds?”, the
    agent might decide that it needs the tool `lbs_to_kg_tool` with one parameter
    value of 40\. The agent’s response might look like this:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定用户查询“40磅等于多少千克？”，代理可能会决定需要具有一个参数值为40的工具`lbs_to_kg_tool`。代理的响应可能如下所示：
- en: '[PRE6]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: From this response, you can evoke the function `lbs_to_kg(lbs=40)` and use its
    output to generate a response to the users.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个响应中，你可以调用函数`lbs_to_kg(lbs=40)`并使用其输出生成对用户的响应。
- en: Tip
  id: totrans-351
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When working with agents, always ask the system to report what parameter values
    it uses for each function call. Inspect these values to make sure they are correct.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 当与代理一起工作时，始终要求系统报告每个函数调用使用的参数值。检查这些值以确保它们是正确的。
- en: Planning granularity
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计划粒度
- en: A plan is a roadmap outlining the steps needed to accomplish a task. A roadmap
    can be of different levels of granularity. To plan for a year, a quarter-by-quarter
    plan is higher-level than a month-by-month plan, which is, in turn, higher-level
    than a week-to-week plan.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 计划是一个概述完成任务所需步骤的路线图。路线图可以是不同粒度级别的。为了规划一年，按季度规划的路线图比按月规划的路线图更高级，而按月规划的路线图又比按周规划的路线图更高级。
- en: There’s a planning/execution trade-off. A detailed plan is harder to generate
    but easier to execute. A higher-level plan is easier to generate but harder to
    execute. An approach to circumvent this trade-off is to plan hierarchically. First,
    use a planner to generate a high-level plan, such as a quarter-to-quarter plan.
    Then, for each quarter, use the same or a different planner to generate a month-to-month
    plan.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着规划和执行的权衡。详细的计划更难生成但更容易执行。高级计划更容易生成但更难执行。绕过这种权衡的一种方法是分层规划。首先，使用规划器生成一个高级计划，例如季度到季度的计划。然后，对于每个季度，使用相同的或不同的规划器生成月到月的计划。
- en: So far, all examples of generated plans use the exact function names, which
    is very granular. A problem with this approach is that an agent’s tool inventory
    can change over time. For example, the function to get the current date `get_time()`
    can be renamed to `get_current_time()`. When a tool changes, you’ll need to update
    your prompt and all your examples. Using the exact function names also makes it
    harder to reuse a planner across different use cases with different tool APIs.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有生成的计划示例都使用了确切的函数名，这非常细粒度。这种方法的缺点是代理的工具库存可能会随时间变化。例如，获取当前日期的函数`get_time()`可能会重命名为`get_current_time()`。当工具发生变化时，你需要更新你的提示和所有示例。使用确切的函数名也使得在不同工具API的不同用例中重用规划器变得更加困难。
- en: If you’ve previously finetuned a model to generate plans based on the old tool
    inventory, you’ll need to finetune the model again on the new tool inventory.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前已经微调了一个模型来根据旧的工具库存生成计划，那么你需要在新工具库存上再次微调该模型。
- en: 'To avoid this problem, plans can also be generated using a more natural language,
    which is higher-level than domain-specific function names. For example, given
    the query “What’s the price of the best-selling product last week”, an agent can
    be instructed to output a plan that looks like this:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，也可以使用更自然语言来生成计划，这种语言比特定领域的函数名更高级。例如，给定查询“上周最畅销产品的价格是多少”，可以指示代理输出如下所示的计划：
- en: '[PRE7]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using more natural language helps your plan generator become robust to changes
    in tool APIs. If your model was trained mostly on natural language, it’ll likely
    be better at understanding and generating plans in natural language and less likely
    to hallucinate.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更自然语言有助于使你的计划生成器对工具API的变化具有鲁棒性。如果你的模型主要是在自然语言上训练的，那么它可能更擅长理解和生成自然语言计划，并且不太可能产生幻觉。
- en: The downside of this approach is that you need a translator to translate each
    natural language action into executable commands.^([13](ch06.html#id1330)) However,
    translating is a much simpler task than planning and can be done by weaker models
    with a lower risk of hallucination.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，你需要一个翻译者将每个自然语言动作翻译成可执行命令。[13](ch06.html#id1330) 然而，翻译是一个比规划简单得多的任务，并且可以由能力较弱的模型完成，其幻觉风险较低。
- en: Complex plans
  id: totrans-362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 复杂计划
- en: 'The plan examples so far have been sequential: the next action in the plan
    is *always* executed after the previous action is done. The order in which actions
    can be executed is called a *control flow*. The sequential form is just one type
    of control flow. Other types of control flows include the parallel, if statement,
    and for loop. The following list provides an overview of each control flow, including
    sequential for comparison:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止的计划示例都是顺序的：计划中的下一个动作总是在前一个动作完成后执行。动作可以执行的顺序称为*控制流*。顺序形式只是控制流的一种类型。其他类型的控制流包括并行、if语句和for循环。以下列表提供了每种控制流的概述，包括顺序以供比较：
- en: Sequential
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序
- en: Executing task B after task A is complete, likely because task B depends on
    task A. For example, the SQL query can be executed only after it’s been translated
    from the natural language input.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务A完成后执行任务B，这很可能是由于任务B依赖于任务A。例如，SQL查询只能在从自然语言输入翻译过来之后才能执行。
- en: Parallel
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 并行
- en: Executing tasks A and B at the same time. For example, given the query “Find
    me best-selling products under $100”, an agent might first retrieve the top 100
    best-selling products and, for each of these products, retrieve its price.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 同时执行任务A和B。例如，给定查询“找到售价低于100美元的热销产品”，代理可能会首先检索前100个热销产品，并为这些产品中的每一个检索其价格。
- en: If statement
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如果语句
- en: Executing task B or task C depending on the output from the previous step. For
    example, the agent first checks NVIDIA’s earnings report. Based on this report,
    it can then decide to sell or buy NVIDIA stocks.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前一步的输出执行任务B或任务C。例如，代理首先检查NVIDIA的收益报告。基于这份报告，它可以决定买卖NVIDIA股票。
- en: For loop
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 循环
- en: Repeat executing task A until a specific condition is met. For example, keep
    on generating random numbers until a prime number.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 重复执行任务A，直到满足特定条件。例如，继续生成随机数，直到得到一个素数。
- en: These different control flows are visualized in [Figure 6-11](#ch06_figure_11_1730157386529304).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的控制流在[图6-11](#ch06_figure_11_1730157386529304)中得到了可视化。
- en: '![A diagram of a task'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '![任务图'
- en: Description automatically generated](assets/aien_0611.png)
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动生成的描述](assets/aien_0611.png)'
- en: Figure 6-11\. Examples of different orders in which a plan can be executed.
  id: totrans-375
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11\. 计划可以执行的不同顺序的示例。
- en: In traditional software engineering, conditions for control flows are exact.
    With AI-powered agents, AI models determine control flows. Plans with non-sequential
    control flows are more difficult to both generate and translate into executable
    commands.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的软件工程中，控制流的条件是精确的。在AI驱动的代理中，AI模型决定了控制流。具有非顺序控制流的计划在生成和转换为可执行命令时都更加困难。
- en: When evaluating an agent framework, check what control flows it supports. For
    example, if the system needs to browse ten websites, can it do so simultaneously?
    Parallel execution can significantly reduce the latency perceived by users.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估代理框架时，检查它支持哪些控制流。例如，如果系统需要浏览十个网站，它能否同时进行？并行执行可以显著减少用户感知的延迟。
- en: Reflection and error correction
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反思和错误纠正
- en: Even the best plans need to be constantly evaluated and adjusted to maximize
    their chance of success. While reflection isn’t strictly necessary for an agent
    to operate, it’s necessary for an agent to succeed.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最佳的计划也需要不断地评估和调整，以最大化其成功的可能性。虽然反思对于代理的运作并非绝对必要，但对于代理的成功却是必要的。
- en: 'Reflection can be useful in many places during a task process:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 反思可以在任务过程中许多地方发挥作用：
- en: After receiving a user query to evaluate if the request is feasible.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在收到用户查询以评估请求是否可行之后。
- en: After the initial plan generation to evaluate whether the plan makes sense.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在初始计划生成后评估计划是否合理。
- en: After each execution step to evaluate if it’s on the right track.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个执行步骤之后评估是否在正确的轨道上。
- en: After the whole plan has been executed to determine if the task has been accomplished.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个计划执行完毕后确定任务是否完成。
- en: Reflection and error correction are two different mechanisms that go hand in
    hand. Reflection generates insights that help uncover errors to be corrected.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 反思和错误纠正是两种相辅相成的机制。反思产生洞察力，有助于揭示需要纠正的错误。
- en: 'Reflection can be done with the same agent using self-critique prompts. It
    can also be done with a separate component, such as a specialized scorer: a model
    that outputs a concrete score for each outcome.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 反思可以通过使用自我批评提示与同一代理进行，也可以通过使用单独的组件进行，例如专门的评分器：一个为每个结果输出具体分数的模型。
- en: 'First proposed by ReAct ([Yao et al., 2022](https://arxiv.org/abs/2210.03629)),
    interleaving reasoning and action has become a common pattern for agents. Yao
    et al. used the term “reasoning” to encompass both planning and reflection. At
    each step, the agent is asked to explain its thinking (planning), take actions,
    then analyze observations (reflection), until the task is considered finished
    by the agent. The agent is typically prompted, using examples, to generate outputs
    in the following format:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 首先由ReAct([姚等，2022](https://arxiv.org/abs/2210.03629))提出，交错推理和动作已成为代理的常见模式。姚等使用“推理”一词来涵盖计划和反思。在每一步，代理被要求解释其思考（计划），采取行动，然后分析观察结果（反思），直到代理认为任务完成。代理通常通过示例被提示，以以下格式生成输出：
- en: '[PRE8]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Figure 6-12](#ch06_figure_12_1730157386529311) shows an example of an agent
    following the ReAct framework responding to a question from HotpotQA ([Yang et
    al., 2018](https://arxiv.org/abs/1809.09600)), a benchmark for multi-hop question
    answering.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-12](#ch06_figure_12_1730157386529311)展示了遵循ReAct框架的代理对HotpotQA ([Yang et
    al., 2018](https://arxiv.org/abs/1809.09600))问题的响应，HotpotQA是一个多跳问答的基准。'
- en: 'You can implement reflection in a multi-agent setting: one agent plans and
    takes actions, and another agent evaluates the outcome after each step or after
    a number of steps.^([14](ch06.html#id1335))'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在多代理环境中实现反思：一个代理进行计划和采取行动，另一个代理在每一步或几步之后评估结果.^([14](ch06.html#id1335))
- en: If the agent’s response failed to accomplish the task, you can prompt the agent
    to reflect on why it failed and how to improve. Based on this suggestion, the
    agent generates a new plan. This allows agents to learn from their mistakes. For
    example, given a coding generation task, an evaluator might evaluate that the
    generated code fails ⅓ of test cases. The agent then reflects the reason it failed
    is because it didn’t take into account arrays where all numbers are negative.
    The actor then generates new code, taking into account all-negative arrays.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代理的响应未能完成任务，你可以提示代理反思为什么失败以及如何改进。基于这个建议，代理生成一个新的计划。这允许代理从他们的错误中学习。例如，对于一个代码生成任务，评估者可能会评估生成的代码有⅓的测试用例失败。然后代理反思失败的原因是因为它没有考虑到所有数字都是负数的数组。然后行动者生成新的代码，考虑到所有负数数组。
- en: '![A screenshot of a computer program'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机程序截图'
- en: Description automatically generated](assets/aien_0612.png)
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成描述](assets/aien_0612.png)
- en: Figure 6-12\. A ReAct agent in action. Image from the ReAct paper (Yao et al.,
    2022). The image is licensed under CC BY 4.0.
  id: totrans-394
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12\. ReAct代理在行动中的示例。图片来自ReAct论文（Yao et al., 2022）。此图片受CC BY 4.0许可。
- en: 'This is the approach that Reflexion ([Shinn et al., 2023](https://arxiv.org/abs/2303.11366))
    took. In this framework, reflection is separated into two modules: an evaluator
    that evaluates the outcome and a self-reflection module that analyzes what went
    wrong. [Figure 6-13](#ch06_figure_13_1730157386529317) shows examples of Reflexion
    agents in action. The authors used the term “trajectory” to refer to a plan. At
    each step, after evaluation and self-reflection, the agent proposes a new trajectory.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Reflexion ([Shinn et al., 2023](https://arxiv.org/abs/2303.11366))所采用的方法。在这个框架中，反思被分为两个模块：一个评估器用于评估结果，一个自我反思模块用于分析出错的原因。[图6-13](#ch06_figure_13_1730157386529317)展示了Reflexion代理在行动中的示例。作者使用“轨迹”一词来指代一个计划。在每一步评估和自我反思之后，代理会提出一个新的轨迹。
- en: Compared to plan generation, reflection is relatively easy to implement and
    can bring surprisingly good performance improvement. The downside of this approach
    is latency and cost. Thoughts, observations, and sometimes actions can take a
    lot of tokens to generate, which increases cost and user-perceived latency, especially
    for tasks with many intermediate steps. To nudge their agents to follow the format,
    both ReAct and Reflexion authors used plenty of examples in their prompts. This
    increases the cost of computing input tokens and reduces the context space available
    for other information.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 与计划生成相比，反思相对容易实现，并且可以带来令人惊讶的性能提升。这种方法的缺点是延迟和成本。思考、观察有时甚至行动可能需要大量的标记来生成，这增加了成本和用户感知的延迟，尤其是在有许多中间步骤的任务中。为了引导他们的代理遵循格式，ReAct和Reflexion的作者在他们的提示中使用了大量的示例。这增加了计算输入标记的成本，并减少了可用于其他信息的上下文空间。
- en: '![A screenshot of a computer program'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机程序截图'
- en: Description automatically generated](assets/aien_0613.png)
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成描述](assets/aien_0613.png)
- en: Figure 6-13\. Examples of how Reflexion agents work. Images from the [Reflexion
    GitHub repo](https://github.com/noahshinn/reflexion).
  id: totrans-399
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-13\. Reflexion代理工作示例。图片来自[Reflexion GitHub仓库](https://github.com/noahshinn/reflexion)。
- en: Tool selection
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工具选择
- en: Because tools often play a crucial role in a task’s success, tool selection
    requires careful consideration. The tools to give your agent depend on the environment
    and the task, but they also depend on the AI model that powers the agent.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 由于工具在任务成功中往往起着关键作用，因此工具选择需要仔细考虑。提供给代理的工具取决于环境和任务，但也取决于驱动代理的AI模型。
- en: There’s no foolproof guide on how to select the best set of tools. Agent literature
    consists of a wide range of tool inventories. For example, Toolformer ([Schick
    et al., 2023](https://arxiv.org/abs/2302.04761)) finetuned GPT-J to learn five
    tools. Chameleon ([Lu et al., 2023](https://arxiv.org/abs/2304.09842)) uses 13
    tools. On the other hand, Gorilla ([Patil et al., 2023](https://arxiv.org/abs/2305.15334))
    attempted to prompt agents to select the right API call among 1,645 APIs.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一套万无一失的指南来选择最佳的工具集。代理文献包括广泛的工具清单。例如，Toolformer（Schick等人，2023）微调了GPT-J以学习五个工具。Chameleon（Lu等人，2023）使用了13个工具。另一方面，Gorilla（Patil等人，2023）试图提示代理在1,645个API中选择正确的API调用。
- en: More tools give the agent more capabilities. However, the more tools there are,
    the harder it is to efficiently use them. It’s similar to how it’s harder for
    humans to master a large set of tools. Adding tools also means increasing tool
    descriptions, which might not fit into a model’s context.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的工具给代理带来更多的能力。然而，工具越多，就越难高效地使用它们。这类似于人类掌握大量工具会更困难。添加工具也意味着增加工具描述，这些描述可能不适合模型的环境。
- en: 'Like many other decisions while building AI applications, tool selection requires
    experimentation and analysis. Here are a few things you can do to help you decide:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 像在构建人工智能应用时所做的许多其他决策一样，工具选择需要实验和分析。以下是一些可以帮助你做出决定的建议：
- en: Compare how an agent performs with different sets of tools.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较代理在不同工具集上的表现。
- en: Do an ablation study to see how much the agent’s performance drops if a tool
    is removed from its inventory. If a tool can be removed without a performance
    drop, remove it.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行消融研究，看看如果从其库存中移除一个工具，代理的性能会下降多少。如果一个工具可以被移除而不会导致性能下降，那么就移除它。
- en: Look for tools that the agent frequently makes mistakes on. If a tool proves
    too hard for the agent to use—for example, extensive prompting and even finetuning
    can’t get the model to learn to use it—change the tool.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找代理经常犯错的工具。如果一个工具对代理来说太难使用——例如，即使是大量的提示和微调也无法让模型学会使用它——那么就更换工具。
- en: Plot the distribution of tool calls to see what tools are most used and what
    tools are least used. [Figure 6-14](#ch06_figure_14_1730157386529326) shows the
    differences in tool use patterns of GPT-4 and ChatGPT in Chameleon (Lu et al.,
    2023).
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制工具调用的分布图，以查看哪些工具使用得最多，哪些工具使用得最少。[图6-14](#ch06_figure_14_1730157386529326)显示了Chameleon（Lu等人，2023）中GPT-4和ChatGPT在工具使用模式上的差异。
- en: '![A screenshot of a graph'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图表的截图'
- en: Description automatically generated](assets/aien_0614.png)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0614.png)
- en: Figure 6-14\. Different models and tasks express different tool use patterns.
    Image from Lu et al. (2023). Adapted from an original image licensed under CC
    BY 4.0.
  id: totrans-411
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-14。不同的模型和任务表现出不同的工具使用模式。图片来自Lu等人（2023）。改编自一个许可在CC BY 4.0下的原始图片。
- en: 'Experiments by Lu et al. (2023) also demonstrate two points:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: Lu等人（2023）的实验也证明了两个观点：
- en: Different tasks require different tools. ScienceQA, the science question answering
    task, relies much more on knowledge retrieval tools than TabMWP, a tabular math
    problem-solving task.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同的任务需要不同的工具。科学问答任务ScienceQA比表格数学问题解决任务TabMWP更依赖于知识检索工具。
- en: Different models have different tool preferences. For example, GPT-4 seems to
    select a wider set of tools than ChatGPT. ChatGPT seems to favor image captioning,
    while GPT-4 seems to favor knowledge retrieval.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同的模型有不同的工具偏好。例如，GPT-4似乎选择比ChatGPT更广泛的工具集。ChatGPT似乎更偏好图像字幕，而GPT-4似乎更偏好知识检索。
- en: Tip
  id: totrans-415
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When evaluating an agent framework, evaluate what planners and tools it supports.
    Different frameworks might focus on different categories of tools. For example,
    AutoGPT focuses on social media APIs (Reddit, X, and Wikipedia), whereas Composio
    focuses on enterprise APIs (Google Apps, GitHub, and Slack).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估代理框架时，评估它支持哪些规划器和工具。不同的框架可能专注于不同的工具类别。例如，AutoGPT专注于社交媒体API（Reddit、X和Wikipedia），而Composio专注于企业API（Google
    Apps、GitHub和Slack）。
- en: As your needs will likely change over time, evaluate how easy it is to extend
    your agent to incorporate new tools.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你的需求可能会随着时间的推移而变化，评估扩展你的代理以包含新工具的难易程度。
- en: As humans, we become more productive not just by using the tools we’re given,
    but also by creating progressively more powerful tools from simpler ones. Can
    AI create new tools from its initial tools?
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们不仅通过使用我们拥有的工具变得更有生产力，而且通过从更简单的工具中创造越来越强大的工具。人工智能能否从其初始工具中创建新的工具？
- en: 'Chameleon (Lu et al., 2023) proposes the study of tool transition: after tool
    *X*, how likely is the agent to call tool *Y*? [Figure 6-15](#ch06_figure_15_1730157386529332)
    shows an example of tool transition. If two tools are frequently used together,
    they can be combined into a bigger tool. If an agent is aware of this information,
    the agent itself can combine initial tools to continually build more complex tools.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: Chameleon (Lu等人，2023) 提出了工具转换的研究：在工具 *X* 之后，代理调用工具 *Y* 的可能性有多大？[图6-15](#ch06_figure_15_1730157386529332)
    展示了一个工具转换的例子。如果两个工具经常一起使用，它们可以组合成一个更大的工具。如果一个代理知道这个信息，代理本身就可以将初始工具组合起来，不断构建更复杂的工具。
- en: '![A diagram of a diagram'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图表的图表'
- en: Description automatically generated](assets/aien_0615.png)
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0615.png)
- en: Figure 6-15\. A tool transition tree by Lu et al. (2023). Adapted from an original
    image licensed under CC BY 4.0.
  id: totrans-422
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-15\. 由Lu等人（2023）创建的工具转换树。改编自一个根据CC BY 4.0许可的原始图像。
- en: Vogager ([Wang et al., 2023](https://arxiv.org/abs/2305.16291)) proposes a skill
    manager to keep track of new skills (tools) that an agent acquires for later reuse.
    Each skill is a coding program. When the skill manager determines a newly created
    skill is to be useful (e.g., because it’s successfully helped an agent accomplish
    a task), it adds this skill to the skill library (conceptually similar to the
    tool inventory). This skill can be retrieved later to use for other tasks.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: Vogager ([王等，2023](https://arxiv.org/abs/2305.16291)) 提出了一种技能管理器，用于跟踪代理（智能体）获取的新技能（工具），以便以后重用。每个技能都是一个编码程序。当技能管理器确定一个新创建的技能是有用的（例如，因为它成功帮助代理完成了一个任务）时，它会将这个技能添加到技能库中（概念上类似于工具库存）。这个技能可以在以后检索出来用于其他任务。
- en: Earlier in this section, we mentioned that the success of an agent in an environment
    depends on its tool inventory and its planning capabilities. Failures in either
    aspect can cause the agent to fail. The next section will discuss different failure
    modes of an agent and how to evaluate them.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节前面，我们提到代理在环境中的成功取决于其工具库存和其规划能力。任何一方面的失败都可能导致代理失败。下一节将讨论代理的不同失败模式及其评估方法。
- en: Agent Failure Modes and Evaluation
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理失败模式和评估
- en: Evaluation is about detecting failures. The more complex a task an agent performs,
    the more possible failure points there are. Other than the failure modes common
    to all AI applications discussed in Chapters [3](ch03.html#ch03a_evaluation_methodology_1730150757064067)
    and [4](ch04.html#ch04_evaluate_ai_systems_1730130866187863), agents also have
    unique failures caused by planning, tool execution, and efficiency. Some of the
    failures are easier to catch than others.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 评估是关于检测失败。代理执行的任务越复杂，可能的失败点就越多。除了第[3](ch03.html#ch03a_evaluation_methodology_1730150757064067)章和第[4](ch04.html#ch04_evaluate_ai_systems_1730130866187863)章中讨论的所有AI应用共有的失败模式之外，代理还有由规划、工具执行和效率引起的独特失败。其中一些失败比其他失败更容易被发现。
- en: To evaluate an agent, identify its failure modes and measure how often each
    of these failure modes happens.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估代理，确定其失败模式并测量每种失败模式发生的频率。
- en: I created a simple benchmark to illustrate these different failure modes that
    you can see on the book’s [GitHub repository](https://github.com/aie-book). There
    are also agent benchmarks and leaderboards such as the [Berkeley Function Calling
    Leaderboard](https://oreil.ly/lKB61), the [AgentOps evaluation harness](https://github.com/AgentOps-AI/agentops),
    and the [TravelPlanner benchmark](https://github.com/OSU-NLP-Group/TravelPlanner).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了一个简单的基准来展示这些不同的失败模式，您可以在本书的[GitHub仓库](https://github.com/aie-book)中看到。还有其他代理基准和排行榜，例如[伯克利函数调用排行榜](https://oreil.ly/lKB61)、[AgentOps评估工具](https://github.com/AgentOps-AI/agentops)和[TravelPlanner基准](https://github.com/OSU-NLP-Group/TravelPlanner)。
- en: Planning failures
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计划失败
- en: 'Planning is hard and can fail in many ways. The most common mode of planning
    failure is tool use failure. The agent might generate a plan with one or more
    of these errors:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 规划很困难，可能会以多种方式失败。规划失败最常见的方式是工具使用失败。代理可能会生成包含一个或多个这些错误之一的计划：
- en: Invalid tool
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 无效的工具
- en: For example, it generates a plan that contains `bing_search`, but `bing_search`
    isn’t in the agent’s tool inventory.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，它生成了一个包含`bing_search`的计划，但`bing_search`不在代理的工具库存中。
- en: Valid tool, invalid parameters.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的工具，无效的参数。
- en: For example, it calls `lbs_to_kg` with two parameters. `lbs_to_kg` is in the
    tool inventory but requires only one parameter, `lbs`.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，它用两个参数调用`lbs_to_kg`。`lbs_to_kg`在工具库存中，但只需要一个参数，即`lbs`。
- en: Valid tool, incorrect parameter values
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的工具，参数值不正确
- en: For example, it calls `lbs_to_kg` with one parameter, `lbs`, but uses the value
    100 for lbs when it should be 120.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，它用参数 `lbs` 调用 `lbs_to_kg`，但将 `lbs` 的值设为100，而应该是120。
- en: 'Another mode of planning failure is goal failure: the agent fails to achieve
    the goal. This can be because the plan doesn’t solve a task, or it solves the
    task without following the constraints. To illustrate this, imagine you ask the
    model to plan a two-week trip from San Francisco to Hanoi with a budget of $5,000\.
    The agent might plan a trip from San Francisco to Ho Chi Minh City, or plan a
    two-week trip from San Francisco to Hanoi that will be way over the budget.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种计划失败模式是目标失败：智能体未能实现目标。这可能是因为计划没有解决任务，或者它没有遵循约束来解决任务。为了说明这一点，想象你要求模型计划从旧金山到河内的两周旅行，预算为5000美元。智能体可能会计划从旧金山到胡志明市，或者计划从旧金山到河内的两周旅行，这将远远超出预算。
- en: A common constraint that is often overlooked by agent evaluation is time. In
    many cases, the time an agent takes matters less, because you can assign a task
    to an agent and only need to check in when it’s done. However, in many cases,
    the agent becomes less useful with time. For example, if you ask an agent to prepare
    a grant proposal and the agent finishes it after the grant deadline, the agent
    isn’t very helpful.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 代理评估中经常被忽视的一个常见约束是时间。在许多情况下，代理花费的时间并不重要，因为你可以分配一个任务给代理，并且只有在它完成时才需要检查。然而，在许多情况下，随着时间推移，代理变得不那么有用。例如，如果你要求代理准备一份拨款提案，而代理在拨款截止日期后完成，那么代理并不太有帮助。
- en: An interesting mode of planning failure is caused by errors in reflection. The
    agent is convinced that it’s accomplished a task when it hasn’t. For example,
    you ask the agent to assign 50 people to 30 hotel rooms. The agent might assign
    only 40 people and insist that the task has been accomplished.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有趣的计划失败模式是由反思错误引起的。智能体确信它已经完成了任务，而实际上并没有。例如，你要求智能体将50人分配到30个酒店房间。智能体可能只分配了40人，并坚持认为任务已经完成。
- en: 'To evaluate an agent for planning failures, one option is to create a planning
    dataset where each example is a tuple `(task, tool inventory)`. For each task,
    use the agent to generate a K number of plans. Compute the following metrics:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估智能体的计划失败，一个选项是创建一个规划数据集，其中每个示例都是一个元组 `(task, tool inventory)`。对于每个任务，使用智能体生成K个计划。计算以下指标：
- en: Out of all generated plans, how many are valid?
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有生成的计划中，有多少是有效的？
- en: For a given task, how many plans does the agent have to generate, on average,
    to get a valid plan?
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一个给定的任务，智能体平均需要生成多少个计划才能得到一个有效的计划？
- en: Out of all tool calls, how many are valid?
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有工具调用中，有多少是有效的？
- en: How often are invalid tools called?
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有多少次无效的工具被调用？
- en: How often are valid tools called with invalid parameters?
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有多少次有效的工具被调用时使用了无效的参数？
- en: How often are valid tools called with incorrect parameter values?
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有多少次有效的工具被调用时使用了错误的参数值？
- en: Analyze the agent’s outputs for patterns. What types of tasks does the agent
    fail more on? Do you have a hypothesis why? What tools does the model frequently
    make mistakes with? Some tools might be harder for an agent to use. You can improve
    an agent’s ability to use a challenging tool by better prompting, more examples,
    or finetuning. If all fail, you might consider swapping this tool for something
    easier to use.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 分析智能体的输出以寻找模式。智能体在哪些类型的任务上失败得更频繁？你有什么假设吗？模型经常在哪些工具上犯错误？有些工具可能对代理来说更难使用。你可以通过更好的提示、更多的示例或微调来提高代理使用具有挑战性工具的能力。如果所有这些都失败了，你可能考虑用更容易使用的工具替换这个工具。
- en: Tool failures
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工具故障
- en: Tool failures happen when the correct tool is used, but the tool output is wrong.
    One failure mode is when a tool just gives the wrong outputs. For example, an
    image captioner returns a wrong description, or an SQL query generator returns
    a wrong SQL query.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用正确的工具但工具输出错误时，会发生工具故障。一种故障模式是工具只给出错误的输出。例如，一个图像标题生成器返回错误的描述，或者一个SQL查询生成器返回错误的SQL查询。
- en: If the agent generates only high-level plans and a translation module is involved
    in translating from each planned action to executable commands, failures can happen
    because of translation errors.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 如果智能体只生成高级计划并且有一个翻译模块参与将每个计划动作翻译成可执行命令，由于翻译错误可能会发生失败。
- en: Tool failures can also happen because the agent doesn’t have access to the right
    tools for the task. An obvious example is when the task involves retrieving the
    current stock prices from the internet, and the agent doesn’t have access to the
    internet.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 工具故障也可能发生，因为代理没有访问到完成任务所需的正确工具。一个明显的例子是当任务涉及从互联网检索当前股票价格时，代理没有访问互联网。
- en: Tool failures are tool-dependent. Each tool needs to be tested independently.
    Always print out each tool call and its output so that you can inspect and evaluate
    them. If you have a translator, create benchmarks to evaluate it.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 工具故障是工具依赖的。每个工具都需要独立测试。始终打印出每个工具调用及其输出，以便您可以检查和评估它们。如果您有一个翻译器，创建基准来评估它。
- en: Detecting missing tool failures requires an understanding of what tools should
    be used. If your agent frequently fails on a specific domain, this might be because
    it lacks tools for this domain. Work with human domain experts and observe what
    tools they would use.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 识别缺失的工具故障需要了解应该使用哪些工具。如果您的代理在特定领域频繁失败，这可能是因为它缺乏该领域的工具。与人类领域专家合作，观察他们会使用哪些工具。
- en: Efficiency
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 效率
- en: 'An agent might generate a valid plan using the right tools to accomplish a
    task, but it might be inefficient. Here are a few things you might want to track
    to evaluate an agent’s efficiency:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可能使用正确的工具生成一个有效的计划来完成一项任务，但它可能效率低下。以下是一些您可能想要跟踪以评估代理效率的事项：
- en: How many steps does the agent need, on average, to complete a task?
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理平均需要多少步才能完成任务？
- en: How much does the agent cost, on average, to complete a task?
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理平均完成一项任务的成本是多少？
- en: How long does each action typically take? Are there any actions that are especially
    time-consuming or expensive?
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个动作通常需要多长时间？是否有任何特别耗时或昂贵的动作？
- en: You can compare these metrics with your baseline, which can be another agent
    or a human operator. When comparing AI agents to human agents, keep in mind that
    humans and AI have very different modes of operations, so what’s considered efficient
    for humans might be inefficient for AI, and vice versa. For example, visiting
    100 web pages might be inefficient for a human agent who can visit only one page
    at a time, but trivial for an AI agent that can visit all the web pages at once.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将这些指标与基线进行比较，基线可以是另一个代理或人类操作员。当比较人工智能代理和人类代理时，请记住，人类和人工智能的操作模式非常不同，因此对人类来说被认为是高效的，可能对人工智能来说效率低下，反之亦然。例如，访问100个网页可能对一次只能访问一个页面的人类代理来说效率低下，但对于一次可以访问所有网页的人工智能代理来说则是微不足道的。
- en: In this chapter, we’ve discussed in detail how RAG and agent systems function.
    Both patterns often deal with information that exceeds a model’s context limit.
    A memory system that supplements the model’s context in handling information can
    significantly enhance its capabilities. Let’s now explore how a memory system
    works.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细讨论了RAG和代理系统的工作方式。这两种模式通常处理超出模型上下文限制的信息。一个补充模型上下文以处理信息的记忆系统可以显著增强其能力。现在让我们探索记忆系统是如何工作的。
- en: Memory
  id: totrans-461
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆
- en: Memory refers to mechanisms that allow a model to retain and utilize information.
    A memory system is especially useful for knowledge-rich applications like RAG
    and multi-step applications like agents. A RAG system relies on memory for its
    augmented context, which can grow over multiple turns as it retrieves more information.
    An agentic system needs memory to store instructions, examples, context, tool
    inventories, plans, tool outputs, reflections, and more. While RAG and agents
    place greater demands on memory, it is beneficial for any AI application that
    requires retaining information.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆是指允许模型保留和利用信息的机制。记忆系统对于像RAG和多步骤应用（如代理）这样的知识丰富应用特别有用。RAG系统依赖于记忆来增强其上下文，随着它检索更多信息，上下文可以增长。代理系统需要记忆来存储指令、示例、上下文、工具清单、计划、工具输出、反思等。虽然RAG和代理对记忆的需求更大，但对于任何需要保留信息的AI应用来说，这都有益处。
- en: 'An AI model typically has three main memory mechanisms:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能模型通常具有三个主要的记忆机制：
- en: Internal knowledge
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 内部知识
- en: The model itself is a memory mechanism, as it retains the knowledge from the
    data it was trained on. This knowledge is its *internal knowledge*. A model’s
    internal knowledge doesn’t change unless the model itself is updated. The model
    can access this knowledge in all queries.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 模型本身就是一个记忆机制，因为它保留了训练数据中的知识。这些知识是它的*内部知识*。除非模型本身更新，否则模型内部的知识不会改变。模型可以在所有查询中访问这些知识。
- en: Short-term memory
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 短期记忆
- en: A model’s context is a memory mechanism. Previous messages in a conversation
    can be added to the model’s context, allowing the model to leverage them to generate
    future responses. A model’s context can be considered its *short-term memory*
    as it doesn’t persist across tasks (queries). It’s fast to access, but its capacity
    is limited. Therefore, it’s often used to store information that is most important
    for the current task.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的上下文是一种记忆机制。对话中的先前消息可以添加到模型的上下文中，使模型可以利用它们生成未来的响应。模型的上下文可以被认为是其*短期记忆*，因为它不会在任务（查询）之间持续存在。它访问速度快，但容量有限。因此，它通常用于存储对当前任务最重要的信息。
- en: Long-term memory
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 长期记忆
- en: External data sources that a model can access via retrieval, such as in a RAG
    system, are a memory mechanism. This can be considered the model’s *long-term
    memory*, as it can be persisted across tasks. Unlike a model’s internal knowledge,
    information in the long-term memory can be deleted without updating the model.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以通过检索访问的外部数据源，例如在RAG系统中，是一种记忆机制。这可以被认为是模型的*长期记忆*，因为它可以在多个任务中持续存在。与模型的内部知识不同，长期记忆中的信息可以在不更新模型的情况下被删除。
- en: Humans have access to similar memory mechanisms. How to breathe is your internal
    knowledge. You typically don’t forget how to breathe unless you’re in serious
    trouble. Your short-term memory contains information immediately relevant to what
    you’re doing, such as the name of a person you just met. Your long-term memory
    is augmented with books, computers, notes, etc.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 人类可以访问类似的记忆机制。如何呼吸是你的内部知识。除非你遇到严重麻烦，否则你通常不会忘记如何呼吸。你的短期记忆包含与你当前活动立即相关的信息，例如你刚刚遇到的人的名字。你的长期记忆通过书籍、计算机、笔记等得到增强。
- en: Which memory mechanism to use for your data depends on its frequency of use.
    Information essential for all tasks should be incorporated into the model’s internal
    knowledge via training or finetuning. Information that is rarely needed should
    reside in its long-term memory. Short-term memory is reserved for immediate, context-specific
    information. These three memory mechanisms are illustrated in [Figure 6-16](#ch06_figure_16_1730157386529338).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 为您的数据选择哪种记忆机制取决于其使用频率。对于所有任务都至关重要的信息应通过训练或微调纳入模型内部知识。很少需要的信息应存储在其长期记忆中。短期记忆用于即时、特定上下文的信息。这三个记忆机制在[图6-16](#ch06_figure_16_1730157386529338)中得到了说明。
- en: '![A diagram of a memory model'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '![记忆模型的图示'
- en: Description automatically generated](assets/aien_0616.png)
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](assets/aien_0616.png)
- en: Figure 6-16\. The hierarchy of information for an agent.
  id: totrans-474
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-16\. 代理的信息层次。
- en: 'Memory is essential for humans to operate. As AI applications have evolved,
    developers have quickly realized that memory is important for AI models, too.
    Many memory management tools for AI models have been developed, and many model
    providers have incorporated external memory. Augmenting an AI model with a memory
    system has many benefits. Here are just a few of them:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆对于人类操作至关重要。随着AI应用的演变，开发者迅速意识到记忆对于AI模型也同样重要。已经开发了多种AI模型的记忆管理工具，许多模型提供商已经集成了外部记忆。为AI模型添加记忆系统有许多好处。以下只是其中的一些：
- en: Manage information overflow within a session
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在会话内管理信息溢出
- en: During the process of executing a task, an agent acquires a lot of new information,
    which can exceed the agent’s maximum context length. The excess information can
    be stored in a memory system with long-term memories.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行任务的过程中，代理会获取大量新信息，这可能会超过代理的最大上下文长度。多余的信息可以存储在具有长期记忆的记忆系统中。
- en: Persist information between sessions
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在会话之间持久化信息
- en: An AI coach is practically useless if every time you want the coach’s advice,
    you have to explain your whole life story. An AI assistant would be annoying to
    use if it keeps forgetting your preferences. Having access to your conversation
    history can allow an agent to personalize its actions to you. For example, when
    you ask for book recommendations, if the model remembers that you’ve previously
    loved *The Three-Body Problem*, it can suggest similar books.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每次你想得到教练的建议时都必须解释你整个生活故事，那么一个AI教练实际上是没有用的。如果一个AI助手总是忘记你的偏好，那么使用它将会很烦人。能够访问你的对话历史可以让代理根据你的偏好个性化其行为。例如，当你要求推荐书籍时，如果模型记得你之前非常喜欢*《三体问题》*，它就可以推荐类似的书。
- en: Boost a model’s consistency
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 提高模型的一致性
- en: If you ask me a subjective question twice, like rating a joke between 1 and
    5, I’m much more likely to give consistent answers if I remember my previous answer.
    Similarly, if an AI model can reference its previous answers, it can calibrate
    its future answers to be consistent.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问我两次主观问题，比如在1到5之间对笑话进行评分，如果我记得之前的答案，我更有可能给出一致的答案。同样，如果一个AI模型可以参考其之前的答案，它就可以调整其未来的答案以保持一致性。
- en: Maintain data structural integrity
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 维护数据结构完整性
- en: Because text is inherently unstructured, the data stored in the context of a
    text-based model is unstructured. You can put structured data in the context.
    For example, you can feed a table into the context line-by-line, but there’s no
    guarantee that the model will understand that this is supposed to be a table.
    Having a memory system capable of storing structured data can help maintain the
    structural integrity of your data. For example, if you ask an agent to find potential
    sales leads, this agent can leverage an Excel sheet to store the leads. An agent
    can also leverage a queue to store the sequence of actions to be performed.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文本本质上是未结构化的，基于文本的模型上下文中存储的数据也是未结构化的。你可以将结构化数据放入上下文中。例如，你可以逐行将表格输入到上下文中，但无法保证模型会理解这应该是一个表格。拥有能够存储结构化数据的记忆系统可以帮助保持数据的结构完整性。例如，如果你要求一个智能体寻找潜在的销售线索，这个智能体可以利用Excel表格来存储线索。智能体还可以利用队列来存储要执行的操作序列。
- en: 'A memory system for AI models typically consists of two functions:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: AI模型的记忆系统通常包括两个功能：
- en: 'Memory management: managing what information should be stored in the short-term
    and long-term memory.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存管理：管理应该存储在短期和长期内存中的信息。
- en: 'Memory retrieval: retrieving information relevant to the task from long-term
    memory.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存检索：从长期记忆中检索与任务相关的信息。
- en: 'Memory retrieval is similar to RAG retrieval, as long-term memory is an external
    data source. In this section, I’ll focus on memory management. Memory management
    typically consists of two operations: *add* and *delete* memory. If memory storage
    is limited, deletion might not be necessary. This might work for long-term memory
    because external memory storage is relatively cheap and easily extensible. However,
    short-term memory is limited by the model’s maximum context length and, therefore,
    requires a strategy for what to add and what to delete.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 内存检索类似于RAG检索，因为长期记忆是一个外部数据源。在本节中，我将重点关注内存管理。内存管理通常包括两个操作：*添加*和*删除*内存。如果内存存储有限，删除可能不是必要的。这可能适用于长期记忆，因为外部内存存储相对便宜且易于扩展。然而，短期记忆受模型最大上下文长度的限制，因此需要制定添加和删除的策略。
- en: Long-term memory can be used to store the overflow from short-term memory. This
    operation depends on how much space you want to allocate for short-term memory.
    For a given query, the context input into the model consists of both its short-term
    memory and information retrieved from its long-term memory. A model’s short-term
    capacity is, therefore, determined by how much of the context should be allocated
    for information retrieved from long-term memory. For example, if 30% of the context
    is reserved, then the model can use at most 70% of the context limit for short-term
    memory. When this threshold is reached, the overflow can be moved to long-term
    memory.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 长期记忆可以用来存储短期记忆的溢出内容。这个操作取决于你为短期记忆分配多少空间。对于一个特定的查询，模型输入的上下文包括其短期记忆和从长期记忆中检索到的信息。因此，模型的短期容量取决于应该为从长期记忆中检索到的信息分配多少上下文。例如，如果保留30%的上下文，那么模型最多可以使用70%的上下文限制作为短期记忆。当达到这个阈值时，溢出内容可以被移动到长期记忆中。
- en: Like many components previously discussed in this chapter, memory management
    isn’t unique to AI applications. Memory management has been a cornerstone of all
    data systems, and many strategies have been developed to use memory efficiently.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 就像本章之前讨论的许多组件一样，内存管理并不仅限于AI应用。内存管理一直是所有数据系统的基石，已经开发了许多策略来高效地使用内存。
- en: The simplest strategy is FIFO, first in, first out. The first to be added to
    the short-term memory will be the first to be moved to the external storage. As
    a conversation gets longer, API providers like OpenAI might start removing the
    beginning of the conversation. Frameworks like LangChain might allow the retention
    of N last messages or N last tokens. In a long conversation, this strategy assumes
    that the early messages are less relevant to the current discussion. However,
    this assumption can be fatally wrong. In some conversations, the earliest messages
    might carry the most information, especially when the early messages state the
    purpose of the conversation.^([15](ch06.html#id1357)) While FIFO is straightforward
    to implement, it can cause the model to lose track of important information.^([16](ch06.html#id1358))
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的策略是先进先出（FIFO），最先被添加到短期记忆中的信息将最先被移动到外部存储。随着对话的延长，API提供商如OpenAI可能会开始删除对话的开头部分。像LangChain这样的框架可能允许保留最后N条消息或N个最后标记。在长时间的对话中，这种策略假设早期消息与当前讨论的相关性较低。然而，这个假设可能是致命的。在某些对话中，最早的消息可能包含最重要的信息，尤其是当早期消息说明了对话的目的时。[15](ch06.html#id1357)
    虽然 FIFO 的实现简单，但它可能导致模型丢失重要信息。[16](ch06.html#id1358)
- en: More-sophisticated strategies involve removing redundancy. Human languages contain
    redundancy to enhance clarity and compensate for potential misunderstandings.
    If there’s a way to automatically detect redundancy, the memory footprint will
    be reduced significantly.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的策略涉及删除冗余。人类语言包含冗余以增强清晰度并弥补可能出现的误解。如果能够自动检测冗余，内存占用将显著减少。
- en: One way to remove redundancy is by using a summary of the conversation. This
    summary can be generated using the same or another model. Summarization, together
    with tracking named entities, can take you a long way. [Bae et al. (2022)](https://arxiv.org/abs/2210.08750)
    took this a step further. After obtaining the summary, the authors wanted to construct
    a new memory by joining the memory with the key information that the summary missed.
    The authors developed a classifier that, for each sentence in the memory and each
    sentence in the summary, determines if only one, both, or neither should be added
    to the new memory.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 删除冗余的一种方法是通过使用对话的摘要。这个摘要可以使用相同的或另一个模型生成。摘要与跟踪命名实体相结合，可以带你走很长的路。[Bae等人 (2022)](https://arxiv.org/abs/2210.08750)
    将这一方法进一步发展。在获得摘要后，作者们希望通过将摘要与遗漏的关键信息相结合来构建一个新的记忆。作者们开发了一个分类器，该分类器对于记忆中的每一句话和摘要中的每一句话，都会确定是否只添加一条、两条或都不添加到新的记忆中。
- en: '[Liu et al. (2023)](https://arxiv.org/abs/2311.08719v1), on the other hand,
    used a reflection approach. After each action, the agent is asked to do two things:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '[刘等人 (2023)](https://arxiv.org/abs/2311.08719v1) 另一方面，使用了一种反射方法。在每个动作之后，要求智能体做两件事：'
- en: Reflect on the information that has just been generated.
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反思刚刚生成的信息。
- en: Determine if this new information should be inserted into the memory, should
    merge with the existing memory, or should replace some other information, especially
    if the other information is outdated and contradicts new information.
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定新信息是否应该插入到记忆中，是否应该与现有记忆合并，或者是否应该替换其他信息，尤其是如果其他信息过时且与新的信息矛盾。
- en: When encountering contradicting pieces of information, some people opt to keep
    the newer ones. Some people ask AI models to judge which one to keep. How to handle
    contradiction depends on the use case. Having contradictions can cause an agent
    to be confused but can also help it draw from different perspectives.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 当遇到相互矛盾的信息时，有些人选择保留较新的信息。有些人会要求AI模型判断保留哪一条。如何处理矛盾取决于具体的使用场景。存在矛盾可能会导致智能体感到困惑，但也可能帮助它从不同的角度进行思考。
- en: Summary
  id: totrans-497
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Given the popularity of RAG and the potential of agents, early readers have
    mentioned that this is the chapter they’re most excited about.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RAG的流行和智能体的潜力，早期读者提到这是他们最兴奋的章节。
- en: This chapter started with RAG, the pattern that emerged first between the two.
    Many tasks require extensive background knowledge that often exceeds a model’s
    context window. For example, code copilots might need access to entire codebases,
    and research assistants may need to analyze multiple books. Originally developed
    to overcome a model’s context limitations, RAG also enables more efficient use
    of information, improving response quality while reducing costs. From the early
    days of foundation models, it was clear that the RAG pattern would be immensely
    valuable for a wide range of applications, and it has since been rapidly adopted
    across both consumer and enterprise use cases.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从RAG开始，这是两个模式中首先出现的模式。许多任务需要大量的背景知识，这通常超出了模型上下文窗口的范围。例如，代码合著者可能需要访问整个代码库，而研究助理可能需要分析多本书。最初是为了克服模型的上下文限制而开发的，RAG还使信息的使用更加高效，在降低成本的同时提高响应质量。从基础模型早期开始，就很明显，RAG模式将对广泛的领域具有巨大的价值，并且它已经迅速被应用于消费和企业用例。
- en: RAG employs a two-step process. It first retrieves relevant information from
    external memory and then uses this information to generate more accurate responses.
    The success of a RAG system depends on the quality of its retriever. Term-based
    retrievers, such as Elasticsearch and BM25, are much lighter to implement and
    can provide strong baselines. Embedding-based retrievers are more computationally
    intensive but have the potential to outperform term-based algorithms.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: RAG采用两步过程。它首先从外部记忆中检索相关信息，然后使用这些信息生成更准确的响应。RAG系统的成功取决于其检索器的质量。基于术语的检索器，如Elasticsearch和BM25，实现起来更简单，可以提供强大的基线。基于嵌入的检索器计算量更大，但有可能优于基于术语的算法。
- en: Embedding-based retrieval is powered by vector search, which is also the backbone
    of many core internet applications such as search and recommender systems. Many
    vector search algorithms developed for these applications can be used for RAG.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 基于嵌入的检索由向量搜索驱动，这也是许多核心互联网应用（如搜索和推荐系统）的骨干。为这些应用开发的许多向量搜索算法也可用于RAG。
- en: The RAG pattern can be seen as a special case of agent where the retriever is
    a tool the model can use. Both patterns allow a model to circumvent its context
    limitation and stay more up-to-date, but the agentic pattern can do even more
    than that. An agent is defined by its environment and the tools it can access.
    In an AI-powered agent, AI is the planner that analyzes its given task, considers
    different solutions, and picks the most promising one. A complex task can require
    many steps to solve, which requires a powerful model to plan. A model’s ability
    to plan can be augmented with reflection and a memory system to help it keep track
    of its progress.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: RAG模式可以看作是一种特殊的代理，其中检索器是模型可以使用的工具。这两种模式都允许模型绕过其上下文限制并保持更及时的信息，但代理模式可以做得更多。代理由其环境和可以访问的工具定义。在人工智能驱动的代理中，人工智能是规划者，它分析给定的任务，考虑不同的解决方案，并选择最有希望的方案。一个复杂任务可能需要许多步骤来解决，这需要一个强大的模型来规划。模型的规划能力可以通过反思和记忆系统来增强，以帮助它跟踪其进度。
- en: The more tools you give a model, the more capabilities the model has, enabling
    it to solve more challenging tasks. However, the more automated the agent becomes,
    the more catastrophic its failures can be. Tool use exposes agents to many security
    risks discussed in [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551).
    For agents to work in the real world, rigorous defensive mechanisms need to be
    put in place.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 你给模型提供的工具越多，模型的能力就越强，使其能够解决更具挑战性的任务。然而，代理的自动化程度越高，其失败可能就越灾难性。工具的使用使代理面临许多在[第5章](ch05.html#ch05a_prompt_engineering_1730156991195551)中讨论的安全风险。为了让代理在现实世界中工作，需要实施严格的防御机制。
- en: Both RAG and agents work with a lot of information, which often exceeds the
    maximum context length of the underlying model. This necessitates the introduction
    of a memory system for managing and using all the information a model has. This
    chapter ended with a short discussion on what this component looks like.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: RAG和代理都处理大量的信息，这通常超过了底层模型的上下文长度限制。这需要引入一个记忆系统来管理和使用模型拥有的所有信息。本章最后简要讨论了该组件的外观。
- en: RAG and agents are both prompt-based methods, as they influence the model’s
    quality solely through inputs without modifying the model itself. While they can
    enable many incredible applications, modifying the underlying model can open up
    even more possibilities. How to do so will be the topic of the next chapter.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: RAG和代理都是基于提示的方法，因为它们仅通过输入影响模型的质量，而不修改模型本身。虽然它们可以启用许多令人难以置信的应用，但修改底层模型可以打开更多的可能性。如何做到这一点将是下一章的主题。
- en: ^([1](ch06.html#id1227-marker)) The model used was a type of [recurrent neural
    network](https://en.wikipedia.org/wiki/Recurrent_neural_network) known as [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)
    (Long Short-Term Memory). LSTM was the dominant architecture of deep learning
    for natural language processing (NLP) before the transformer architecture took
    over in 2018.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#id1227-marker)) 使用的模型是一种称为[LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)（长短期记忆）的[循环神经网络](https://en.wikipedia.org/wiki/Recurrent_neural_network)类型。在2018年transformer架构接管之前，LSTM是自然语言处理（NLP）深度学习的占主导地位架构。
- en: ^([2](ch06.html#id1228-marker)) Around the same time, another paper, also from
    Facebook, “How Context Affects Language Models’ Factual Predictions” ([Petroni
    et al., *arXiv*, May 2020](https://arxiv.org/abs/2005.04611)), showed that augmenting
    a pre-trained language model with a retrieval system can dramatically improve
    the model’s performance on factual questions.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.html#id1228-marker)) 大约在同一时间，另一篇来自Facebook的论文，“上下文如何影响语言模型的事实预测”([Petroni等人，*arXiv*，2020年5月](https://arxiv.org/abs/2005.04611))，显示通过检索系统增强预训练语言模型可以显著提高模型在事实问题上的性能。
- en: ^([3](ch06.html#id1229-marker)) Thanks to Chetan Tekur for the example.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.html#id1229-marker)) 感谢Chetan Tekur提供的例子。
- en: ^([4](ch06.html#id1230-marker)) Parkinson’s Law is usually expressed as “Work
    expands so as to fill the time available for its completion.” I have a similar
    theory that an application’s context expands to fill the context limit supported
    by the model it uses.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.html#id1230-marker)) 帕金森定律通常表述为“工作会扩展以填满完成它所需的时间。”我有一个类似的理论，即应用程序的上下文会扩展以填满模型支持的上下文限制。
- en: '^([5](ch06.html#id1238-marker)) Information retrieval was described as early
    as the 1920s in Emanuel Goldberg’s patents for a “statistical machine” to search
    documents stored on films. See [“The History of Information Retrieval Research”](https://oreil.ly/-JJYn)
    (Sanderson and Croft, *Proceedings of the IEEE, 100: Special Centennial Issue,*
    April 2012).'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.html#id1238-marker)) 信息检索最早在20世纪20年代由Emanuel Goldberg的“统计机器”专利中描述，用于搜索存储在胶片上的文档。参见[“信息检索研究的历史”](https://oreil.ly/-JJYn)（Sanderson和Croft，《IEEE会议录，100：百年特刊》，2012年4月）。
- en: '^([6](ch06.html#id1245-marker)) For those interested in learning more about
    BM25, I recommend this paper by the BM25 authors: [“The Probabilistic Relevance
    Framework: BM25 and Beyond”](https://oreil.ly/aDmhb) (Robertson and Zaragoza,
    *Foundations and Trends in Information Retrieval* 3 No. 4, 2009)'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.html#id1245-marker)) 对于那些想了解更多关于BM25的人来说，我推荐这篇由BM25作者撰写的论文：“概率相关性框架：BM25及其超越”（Robertson和Zaragoza，《信息检索基础与趋势》第3卷第4期，2009年）。
- en: ^([7](ch06.html#id1246-marker)) [Aravind Srinivas, the CEO of Perplexity](https://x.com/AravSrinivas/status/1737886080555446552),
    tweeted that “Making a genuine improvement over BM25 or full-text search is hard”.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06.html#id1246-marker)) [Perplexity的CEO Aravind Srinivas](https://x.com/AravSrinivas/status/1737886080555446552)在推特上表示，“在BM25或全文搜索之上做出真正的改进是困难的”。
- en: ^([8](ch06.html#id1251-marker)) A RAG retrieval workflow shares many similar
    steps with the traditional recommender system.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch06.html#id1251-marker)) RAG检索工作流程与传统推荐系统有许多类似的步骤。
- en: ^([9](ch06.html#id1296-marker)) Some teams have told me that their retrieval
    systems work best when the data is organized in a question-and-answer format.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch06.html#id1296-marker)) 一些团队告诉我，当数据以问答格式组织时，他们的检索系统工作得最好。
- en: '^([10](ch06.html#id1313-marker)) *Artificial Intelligence: A Modern Approach*
    (1995) defines an agent as anything that can be viewed as perceiving its environment
    through sensors and acting upon that environment through actuators.'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch06.html#id1313-marker)) *《人工智能：现代方法》*（1995年）将代理定义为任何可以通过传感器感知其环境并通过执行器对环境采取行动的东西。
- en: ^([11](ch06.html#id1315-marker)) A complaint in the early days of agents is
    that agents are only good for burning through your API credits.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch06.html#id1315-marker)) 代理在早期的一个抱怨是，代理只适合耗尽你的API信用额度。
- en: ^([12](ch06.html#id1324-marker)) Because most agentic workflows are sufficiently
    complex to involve multiple components, most agents are multi-agent.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch06.html#id1324-marker)) 因为大多数代理工作流程足够复杂，需要涉及多个组件，所以大多数代理是多代理的。
- en: ^([13](ch06.html#id1330-marker)) Chameleon ([Lu et al., 2023](https://arxiv.org/abs/2304.09842))
    calls this translator a program generator.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch06.html#id1330-marker)) 难民 ([Lu et al., 2023](https://arxiv.org/abs/2304.09842))
    把这个翻译器称为程序生成器。
- en: ^([14](ch06.html#id1335-marker)) This reminds me of the actor-critic (AC) agent
    method ([Konda and Tsitsiklis, 1999](https://oreil.ly/UziTE)) in reinforcement
    learning.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch06.html#id1335-marker)) 这让我想起了强化学习中的演员-评论家（AC）代理方法 ([Konda and Tsitsiklis,
    1999](https://oreil.ly/UziTE))。
- en: ^([15](ch06.html#id1357-marker)) For human conversations, the opposite might
    be true if the first few messages are pleasantries.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch06.html#id1357-marker)) 对于人类对话来说，如果前几条消息是客套话，情况可能正好相反。
- en: ^([16](ch06.html#id1358-marker)) Usage-based strategies, such as removing the
    least frequently used information, is more challenging, since you’ll need a way
    to know when a model uses a given piece of information.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch06.html#id1358-marker)) 基于使用情况的策略，例如删除最不常用的信息，更具挑战性，因为你需要一种方式来知道模型何时使用某个特定的信息。
