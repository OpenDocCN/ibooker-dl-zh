- en: 'Chapter 5\. The “Hello World” of TinyML: Building an Application'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A model is just one part of a machine learning application. Alone, it’s just
    a blob of information; it can’t do much at all. To use our model, we need to wrap
    it in code that sets up the necessary environment for it to run, provides it with
    inputs, and uses its outputs to generate behavior. [Figure 5-1](#basic_architecture)
    shows how the model, on the right hand side, fits into a basic TinyML application.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will build an embedded application that uses our sine model
    to create a tiny light show. We’ll set up a continuous loop that feeds an `x`
    value into the model, runs inference, and uses the result to switch an LED on
    and off, or to control an animation if our device has an LCD display.
  prefs: []
  type: TYPE_NORMAL
- en: This [application](https://oreil.ly/mqkw3) has already been written. It’s a
    C++ 11 program whose code is designed to show the smallest possible implementation
    of a full TinyML application, avoiding any complex logic. This simplicity makes
    it a helpful tool for learning how to use TensorFlow Lite for Microcontrollers,
    since you can see exactly what code is necessary and very little else. It also
    makes it a useful template. After reading this chapter, you’ll understand the
    general structure of a TensorFlow Lite for Microcontrollers program, and you can
    reuse the same structure in your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter walks through the application code and explains how it works. The
    next chapter will provide detailed instructions for building and deploying it
    to several devices. If you’re not familiar with C++, don’t panic. The code is
    relatively simple, and we explain everything in detail. By the time we’re done,
    you should feel comfortable with all the code that’s required to run a model,
    and you might even learn a little C++ along the way.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing a basic TinyML application architecture](Images/timl_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. A basic TinyML application architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember, since TensorFlow is an actively developed open source project, there
    might be some minor differences between the code printed here and the code online.
    Don’t worry—even if a few lines of code change, the basic principles remain the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: Walking Through the Tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before getting our hands dirty with application code, it’s often a good idea
    to write some tests. Tests are short snippets of code that demonstrate a particular
    piece of logic. Since they are made of working code, we can run them to prove
    that the code does what it’s supposed to. After we have written them, tests are
    often run automatically as a way to continually verify that a project is still
    doing what we expect despite any changes we might make to its code. They’re also
    very useful as working examples of how to do things.
  prefs: []
  type: TYPE_NORMAL
- en: The `hello_world` example has a test, defined in [*hello_world_test.cc*](https://oreil.ly/QW0SS),
    that loads our model and uses it to run inference, checking that its predictions
    are what we expect. It contains the exact code needed to do this, and nothing
    else, so it will be a great place to start learning TensorFlow Lite for Microcontrollers.
    In this section, we walk through the test and explain what each and every part
    of it does. After we’re done reading the code, we can run the test to prove that
    it’s correct.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now walk through it, section by section. If you’re at a computer, it might
    be helpful to open up [*hello_world_test.cc*](https://oreil.ly/s0f6Q) and follow
    along.
  prefs: []
  type: TYPE_NORMAL
- en: Including Dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first part, below the license header (which specifies that anybody can
    use or share this code under the [Apache 2.0](https://oreil.ly/Xa5_x) open source
    license), looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `#include` directive is a way for C++ code to specify other code that it
    depends on. When a code file is referenced with an `#include`, any logic or variables
    it defines will be available for us to use. In this section, we use `#include`
    to import the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: '*tensorflow/lite/micro/examples/hello_world/sine_model_data.h*'
  prefs: []
  type: TYPE_NORMAL
- en: The sine model we trained, converted, and transformed into C++ using `xxd`
  prefs: []
  type: TYPE_NORMAL
- en: '*tensorflow/lite/micro/kernels/all_ops_resolver.h*'
  prefs: []
  type: TYPE_NORMAL
- en: A class that allows the interpreter to load the operations used by our model
  prefs: []
  type: TYPE_NORMAL
- en: '*tensorflow/lite/micro/micro_error_reporter.h*'
  prefs: []
  type: TYPE_NORMAL
- en: A class that can log errors and output to help with debugging
  prefs: []
  type: TYPE_NORMAL
- en: '*tensorflow/lite/micro/micro_interpreter.h*'
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Lite for Microcontrollers interpreter, which will run our model
  prefs: []
  type: TYPE_NORMAL
- en: '*tensorflow/lite/micro/testing/micro_test.h*'
  prefs: []
  type: TYPE_NORMAL
- en: A lightweight framework for writing tests, which allows us to run this file
    as a test
  prefs: []
  type: TYPE_NORMAL
- en: '*tensorflow/lite/schema/schema_generated.h*'
  prefs: []
  type: TYPE_NORMAL
- en: The schema that defines the structure of TensorFlow Lite FlatBuffer data, used
    to make sense of the model data in *sine_model_data.h*
  prefs: []
  type: TYPE_NORMAL
- en: '*tensorflow/lite/version.h*'
  prefs: []
  type: TYPE_NORMAL
- en: The current version number of the schema, so we can check that the model was
    defined with a compatible version
  prefs: []
  type: TYPE_NORMAL
- en: We’ll talk more about some of these dependencies as we dig into the code.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By convention, C++ code designed to be used with `#include` directives is written
    as two files: a *.cc* file, known as the *source file*, and a *.h* file, known
    as the *header file*. Header files define the interface that allows the code to
    connect to other parts of the program. They contain things like variable and class
    declarations, but very little logic. Source files implement the actual logic that
    performs computation and makes things happen. When we `#include` a dependency,
    we specify its header file. For example, the test we’re walking through includes
    [*micro_interpreter.h*](https://oreil.ly/60uYt). If we look at that file, we can
    see that it defines a class but doesn’t contain much logic. Instead, its logic
    is contained within [*micro_interpreter.cc*](https://oreil.ly/twN7J).'
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up the Test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next part of the code is used by the TensorFlow Lite for Microcontrollers
    testing framework. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In C++, you can define specially named chunks of code that can be reused by
    including their names elsewhere. These chunks of code are called *macros*. The
    two statements here, `TF_LITE_MICRO_TESTS_BEGIN` and `TF_LITE_MICRO_TEST`, are
    the names of macros. They are defined in the file [*micro_test.h*](https://oreil.ly/NoGm4).
  prefs: []
  type: TYPE_NORMAL
- en: These macros wrap the rest of our code in the necessary apparatus for it to
    be executed by the TensorFlow Lite for Microcontrollers testing framework. We
    don’t need to worry about how exactly this works; we just know that we can use
    these macros as shortcuts to set up a test.
  prefs: []
  type: TYPE_NORMAL
- en: The second macro, named `TF_LITE_MICRO_TEST`, accepts an argument. In this case,
    the argument being passed in is `LoadModelAndPerformInference`. This argument
    is the test name, and when the tests are run, it will be output along with the
    test results so that we can see whether the test passed or failed.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Ready to Log Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The remaining code in the file is the actual logic of our test. Let’s take
    a look at the first portion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the first line, we define a `MicroErrorReporter` instance. The `MicroErrorReporter`
    class is defined in [*micro_error_reporter.h*](https://oreil.ly/AkZrm). It provides
    a mechanism for logging debug information during inference. We’ll be calling it
    to print debug information, and the TensorFlow Lite for Microcontrollers interpreter
    will use it to print any errors it encounters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’ve probably noticed the `tflite::` prefix before each of the type names,
    such as `tflite::MicroErrorReporter`. This is a *namespace*, which is just a way
    to help organize C++ code. TensorFlow Lite defines all of its useful stuff under
    the namespace `tflite`, which means that if another library happens to implement
    classes with the same name, they won’t conflict with those that TensorFlow Lite
    provides.
  prefs: []
  type: TYPE_NORMAL
- en: The first declaration seems straightforward, but what about the funky-looking
    second line, with the `*` and `&` characters? Why are we declaring an `ErrorReporter`
    when we already have a `MicroErrorReporter`?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To explain what is happening here, we need to know a little background information.
  prefs: []
  type: TYPE_NORMAL
- en: '`MicroErrorReporter` is a subclass of the `ErrorReporter` class, which provides
    a template for how this sort of debug logging mechanism should work in TensorFlow
    Lite. `MicroErrorReporter` overrides one of `ErrorReporter`’s methods, replacing
    it with logic that is specifically written for use on microcontrollers.'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code line, we create a variable called `error_reporter`, which
    has the type `ErrorReporter`. It’s also a pointer, indicated by the * used in
    its declaration.
  prefs: []
  type: TYPE_NORMAL
- en: A pointer is a special type of variable that, instead of holding a value, holds
    a reference to a location in memory where a value can be found. In C++, a pointer
    of a certain class (such as `ErrorReporter`) can point to a value that is one
    of its child classes (such as `MicroErrorReporter`).
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, `MicroErrorReporter` overrides one of the methods of
    `ErrorReporter`. Without going into too much detail, the process of overriding
    this method has the side effect of obscuring some of its other methods.
  prefs: []
  type: TYPE_NORMAL
- en: To still have access to the non overridden methods of `ErrorReporter`, we need
    to treat our `MicroErrorReporter` instance as if it were actually an `ErrorReporter`.
    We achieve this by creating an `ErrorReporter` pointer and pointing it at the
    `micro_error_reporter` variable. The ampersand (`&`) in front of `micro_error_reporter`
    in the assignment means that we are assigning its pointer, not its value.
  prefs: []
  type: TYPE_NORMAL
- en: Phew! This sounds complicated. Don’t panic if you found it difficult to follow;
    C++ can be a little unwieldy. For our purposes, all we need to know is that that
    we should use `error_reporter` to print debug information, and that it’s a pointer.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping Our Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The reason we immediately set up a mechanism for printing debug information
    is so that we can log any problems that occur in the rest of the code. We rely
    on this in the next piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the first line, we take our model data array (defined in the file [*sine_model_data.h*](https://oreil.ly/m68Wj))
    and pass it into a method named `GetModel()`. This method returns a `Model` pointer,
    which is assigned to a variable named `model`. As you might have anticipated,
    this variable represents our model.
  prefs: []
  type: TYPE_NORMAL
- en: The type `Model` is a *struct*, which in C++ is very similar to class. It’s
    defined in [*schema_generated.h*](https://oreil.ly/SGNtU), and it holds our model’s
    data and allows us to query information about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as `model` is ready, we call a method that retrieves the model’s version
    number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We then compare the model’s version number to `TFLITE_SCHEMA_VERSION`, which
    indicates the version of the TensorFlow Lite library we are currently using. If
    the numbers match, our model was converted with a compatible version of the TensorFlow
    Lite Converter. It’s good practice to check the model version, because a mismatch
    might result in strange behavior that is tricky to debug.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding line of code, `version()` is a method that belongs to `model`.
    Notice the arrow (->) that points from `model` to `version()`. This is C++’s *arrow
    operator*, and it’s used whenever we want to access the members of an object to
    which we have a pointer. If we had the object itself (and not just a pointer),
    we would use a dot (`.`) to access its members.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the version numbers don’t match, we’ll carry on anyway, but we’ll log a
    warning using our `error_reporter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We call the `Report()` method of `error_reporter` to log this warning. Since
    `error_reporter` is also a pointer, we use the -> operator to access `Report()`.
  prefs: []
  type: TYPE_NORMAL
- en: The `Report()` method is designed to behave similarly to a commonly used C++
    method, `printf()`, which is used to log text. As its first parameter, we pass
    in a string that we want to log. This string contains two `%d` format specifiers,
    which act as placeholders where variables will be inserted when the message is
    logged. The next two parameters we pass in are the model version and the TensorFlow
    Lite schema version. These will be inserted into the string, in order, to replace
    the `%d` characters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `Report()` method supports different format specifiers that work as placeholders
    for different types of variables. `%d` should be used as a placeholder for integers,
    `%f` should be used as a placeholder for floating-point numbers, and `%s` should
    be used as a placeholder for strings.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AllOpsResolver
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far so good! Our code can log errors, and we’ve loaded our model into a
    handy struct and checked that it is a compatible version. We’ve been moving a
    little slowly, given that we’re reviewing some C++ concepts along the way, but
    things are starting to make sense. Next up, we create an instance of `AllOpsResolver`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This class, defined in [*all_ops_resolver.h*](https://oreil.ly/O0qgy), is what
    allows the TensorFlow Lite for Microcontrollers interpreter to access *operations*.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.xhtml#chapter_get_up_to_speed), you learned that a machine
    learning model is composed of various mathematical operations that are run successively
    to transform input into output. The `AllOpsResolver` class knows all of the operations
    that are available to TensorFlow Lite for Microcontrollers and is able to provide
    them to the interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a Tensor Arena
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We almost have all the ingredients ready to create an interpreter. The final
    thing we need to do is allocate an area of working memory that our model will
    need while it runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As the comment says, this area of memory will be used to store the model’s input,
    output, and intermediate tensors. We call it our *tensor arena*. In our case,
    we’ve allocated an array that is 2,048 bytes in size. We specify this with the
    expression `2 × 1024`.
  prefs: []
  type: TYPE_NORMAL
- en: So, how large should our tensor arena be? That’s a good question. Unfortunately,
    there’s not a simple answer. Different model architectures have different sizes
    and numbers of input, output, and intermediate tensors, so it’s difficult to know
    how much memory we’ll need. The number doesn’t need to be exact—we can reserve
    more memory than we need—but since microcontrollers have limited RAM, we should
    keep it as small as possible so there’s space for the rest of our program.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this through trial and error. That’s why we express the array size
    as *`n`* `× 1024`: so that it’s easy to scale the number up and down (by changing
    *`n`*) while keeping it a multiple of eight. To find the correct array size, start
    fairly high so that you can be sure it works. The highest number used in this
    book’s examples is `70 × 1024`. Then, reduce the number until your model no longer
    runs. The last number that worked is the correct one!'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Interpreter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve declared `tensor_arena`, we’re ready to set up the interpreter.
    Here’s how that looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we declare a `MicroInterpreter` named `interpreter`. This class is the
    heart of TensorFlow Lite for Microcontrollers: a magical piece of code that will
    execute our model on the data we provide. We pass in most of the objects we’ve
    created so far to its constructor, and then make a call to `AllocateTensors()`.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we set aside an area of memory by defining an array
    called `tensor_arena`. The `AllocateTensors()` method walks through all of the
    tensors defined by the model and assigns memory from the `tensor_arena` to each
    of them. It’s critical that we call `AllocateTensors()` before attempting to run
    inference, because otherwise inference will fail.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the Input Tensor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After we’ve created an interpreter, we need to provide some input for our model.
    To do this, we write our input data to the model’s input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: To grab a pointer to an input tensor, we call the interpreter’s `input()` method.
    Since a model can have multiple input tensors, we need to pass an index to the
    `input()` method that specifies which tensor we want. In this case, our model
    has only one input tensor, so its index is `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow Lite, tensors are represented by the `TfLiteTensor` struct, which
    is defined in [*c_api_internal.h*](https://oreil.ly/Qvhre). This struct provides
    an API for interacting with and learning about tensors. In the next chunk of code,
    we use this functionality to verify that our tensor looks and feels correct. Because
    we’ll be using tensors a lot, let’s walk through this code to become familiar
    with how the `TfLiteTensor` struct works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing you’ll notice is a couple of macros: `TFLITE_MICRO_EXPECT_NE`
    and `TFLITE_MICRO_EXPECT_EQ`. These macros are part of the TensorFlow Lite for
    Microcontrollers testing framework, and they allow us to make *assertions* about
    the values of variables, proving that they have certain expected values.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, the macro `TF_LITE_MICRO_EXPECT_NE` is designed to assert that
    the two variables it is called with are not equal (hence the `_NE` part of its
    name, which stands for Not Equal). If the variables are not equal, the code will
    continue to execute. If they are equal, an error will be logged, and the test
    will be marked as having failed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we check is that our input tensor actually exists. To do this,
    we assert that it is *not equal* to a `nullptr`, which is a special C++ value
    representing a pointer that is not actually pointing at any data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The next thing we check is the *shape* of our input tensor. As discussed in
    [Chapter 3](ch03.xhtml#chapter_get_up_to_speed), all tensors have a shape, which
    is a way of describing their dimensionality. The input to our model is a scalar
    value (meaning a single number). However, due to [the way Keras layers accept
    input](https://oreil.ly/SFiRV), this value must be provided inside of a 2D tensor
    containing one number. For an input of 0, it should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note how the input scalar, 0, is wrapped inside of two vectors, making this
    a 2D tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `TfLiteTensor` struct contains a `dims` member that describes the dimensions
    of the tensor. The member is a struct of type `TfLiteIntArray`, also defined in
    *c_api_internal.h*. Its `size` member represents the number of dimensions that
    the tensor has. Since the input tensor should be 2D, we can assert that the value
    of `size` is `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can further inspect the `dims` struct to ensure the tensor’s structure is
    what we expect. Its `data` variable is an array with one element for each dimension.
    Each element is an integer representing the size of that dimension. Because we
    are expecting a 2D tensor containing one element in each dimension, we can assert
    that both dimensions contain a single element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can now be confident that our input tensor has the correct shape. Finally,
    since tensors can consist of a variety of different types of data (think integers,
    floating-point numbers, and Boolean values), we should make sure that our input
    tensor has the correct type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tensor struct’s `type` variable informs us of the data type of the tensor.
    We’ll be providing a 32-bit floating-point number, represented by the constant
    `kTfLiteFloat32`, and we can easily assert that the type is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Perfect—our input tensor is now guaranteed to be the correct size and shape
    for our input data, which will be a single floating-point value. We’re ready to
    run inference!
  prefs: []
  type: TYPE_NORMAL
- en: Running Inference on an Input
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run inference, we need to add a value to our input tensor and then instruct
    the interpreter to invoke the model. Afterward, we will check whether the model
    successfully ran. Here’s how that looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'TensorFlow Lite’s `TfLiteTensor` struct has a `data` variable that we can use
    to set the contents of our input tensor. You can see this being used here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `data` variable is a `TfLitePtrUnion`—it’s a *union*, which is a special
    C++ data type that allows you to store different data types at the same location
    in memory. Since a given tensor can contain one of many different types of data
    (for example, floating-point numbers, integers, or Booleans), a union is the perfect
    type to help us store it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `TfLitePtrUnion` union is declared in [*c_api_internal.h*](https://oreil.ly/v4h7K).
    Here’s what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You can see that there are a bunch of members, each representing a certain type.
    Each member is a pointer, which can point at a place in memory where the data
    should be stored. When we call `interpreter.AllocateTensors()`, like we did earlier,
    the appropriate pointer is set to point at the block of memory that was allocated
    for the tensor to store its data. Because each tensor has a specific data type,
    only the pointer for the corresponding type will be set.
  prefs: []
  type: TYPE_NORMAL
- en: This means that to store data, we can use whichever is the appropriate pointer
    in our `TfLitePtrUnion`. For example, if our tensor is of type `kTfLiteFloat32`,
    we’ll use `data.f`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the pointer points at a block of memory, we can use square brackets (`[]`)
    after the pointer name to instruct our program where to store the data. In our
    example, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The value we’re assigning is written as `0.`, which is shorthand for `0.0`.
    By specifying the decimal point, we make it clear to the C++ compiler that this
    value should be a floating-point number, not an integer.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that we assign this value to `data.f[0]`. This means that we’re
    assigning it as the first item in our block of allocated memory. Given that there’s
    only one value, this is all we need to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we’ve set up the input tensor, it’s time to run inference. This is a
    one-liner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: When we call `Invoke()` on the `interpreter`, the TensorFlow Lite interpreter
    runs the model. The model consists of a graph of mathematical operations which
    the interpreter executes to transform the input data into an output. This output
    is stored in the model’s output tensors, which we’ll dig into later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Invoke()` method returns a `TfLiteStatus` object, which lets us know whether
    inference was successful or there was a problem. Its value can either be `kTfLiteOk`
    or `kTfLiteError`. We check for an error and report it if there is one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we assert that the status must be `kTfLiteOk` in order for our test
    to pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: That’s it—inference has been run! Next up, we grab the output and make sure
    it looks good.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like the input, our model’s output is accessed through a `TfLiteTensor`, and
    getting a pointer to it is just as simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is, like the input, a floating-point scalar value nestled inside
    a 2D tensor. For the sake of our test, we double-check that the output tensor
    has the expected size, dimensions, and type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Yep, it all looks good. Now, we grab the output value and inspect it to make
    sure that it meets our high standards. First we assign it to a `float` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Each time inference is run, the output tensor will be overwritten with new values.
    This means that if you want to keep an output value around in your program while
    continuing to run inference, you’ll need to copy it from the output tensor, like
    we just did.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use `TF_LITE_MICRO_EXPECT_NEAR` to prove that the value is close to
    the value we’re expecting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As we saw earlier, `TF_LITE_MICRO_EXPECT_NEAR` asserts that the difference between
    its first argument and its second argument is less than the value of its third
    argument. In this statement, we’re testing that the output is within 0.05 of 0,
    which is the mathematical sine of the input, 0.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are two reasons why we expect a number that is *near* to what we want,
    but not an exact value. The first is that our model only *approximates* the real
    sine value, so we know that it will not be exactly correct. The second is because
    floating-point calculations on computers have a margin of error. The error can
    vary from computer to computer: for example, a laptop’s CPU might come up with
    slightly different results to an Arduino. By having flexible expectations, we
    make it more likely that our test will pass on any platform.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If this test passes, things are looking good. The remaining tests run inference
    a few more times, just to further prove that our model is working. To run inference
    again, all we need to do is assign a new value to our input tensor, call `interpreter.Invoke()`,
    and read the output from our output tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note how we’re reusing the same `input` and `output` tensor pointer. Because
    we already have the pointers, we don’t need to call `interpreter.input(0)` or
    `interpreter.output(0)` again.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point in our tests we’ve proven that TensorFlow Lite for Microcontrollers
    can successfully load our model, allocate the appropriate input and output tensors,
    run inference, and return the expected results. The final thing to do is indicate
    the end of the tests by using a macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: And with that, we’re done walking through the tests. Next, let’s run them!
  prefs: []
  type: TYPE_NORMAL
- en: Running the Tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though this code is eventually destined to run on microcontrollers, we
    can still build and run our tests on our development machine. This makes it much
    easier to write and debug code. Compared with microcontrollers, a personal computer
    has far more convenient tools for logging output and stepping through code, which
    makes it a lot simpler to figure out any bugs. In addition, deploying code to
    a device takes time, so it’s a lot quicker to just run our code locally.
  prefs: []
  type: TYPE_NORMAL
- en: A good workflow for building embedded applications (or, honestly, any kind of
    software) is to write as much of the logic as you can in tests that can be run
    on a normal development machine. There’ll always be some parts that require the
    actual hardware to run, but the more you can test locally, the easier your life
    will be.
  prefs: []
  type: TYPE_NORMAL
- en: Practically, this means that we should try to write the code that preprocesses
    inputs, runs inference with the model, and processes any outputs in a set of tests
    before trying to get it working on-device. In [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example),
    we walk through a speech recognition application that is much more complex than
    this example. You’ll see how we’ve written detailed unit tests for each of its
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Grabbing the code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Until now, between Colab and GitHub, we’ve been doing everything in the cloud.
    To run our tests, we need to pull down the code to our development computer and
    compile it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do all this, we need the following software tools:'
  prefs: []
  type: TYPE_NORMAL
- en: A terminal emulator, such as Terminal in macOS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bash shell (the default in macOS prior to Catalina and most Linux distributions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Git](https://git-scm.com/) (installed by default in macOS and most Linux distributions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make, version 3.82 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After you have all the tools, open up a terminal and enter the command that
    follows to download the TensorFlow source code, which includes the example code
    we’re working with. It will create a directory containing the source code in whatever
    location you run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, change into the *tensorflow* directory that was just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Great stuff—we’re now ready to run some code!
  prefs: []
  type: TYPE_NORMAL
- en: Using Make to run the tests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you saw from our list of tools, we use a program called *Make* to run the
    tests. Make is a tool for automating build tasks in software. It’s been in use
    since 1976, which in computing terms is almost forever. Developers use a special
    language, written in files called *Makefiles*, to instruct Make how to build and
    run code. TensorFlow Lite for Microcontrollers has a Makefile defined in [*micro/tools/make/Makefile*](https://oreil.ly/6Kvx5);
    there’s more information about it in [Chapter 13](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers).
  prefs: []
  type: TYPE_NORMAL
- en: 'To run our tests using Make, we can issue the following command, making sure
    we’re running it from the root of the *tensorflow* directory we downloaded with
    Git. We first specify the Makefile to use, followed by the *target*, which is
    the component that we want to build:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The Makefile is set up so that in order to run tests, we provide a target with
    the prefix `test_` followed by the name of the component that we want to build.
    In our case, that component is *`hello_world_test`*, so the full target name is
    *`test_hello_world_test`*.
  prefs: []
  type: TYPE_NORMAL
- en: Try running this command. You should start to see a ton of output fly past!
    First, some necessary libraries and tools will be downloaded. Next, our test file,
    along with all of its dependencies, will be built. Our Makefile has instructed
    the C++ compiler to build the code and create a binary, which it will then run.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll need to wait a few moments for the process to complete. When the text
    stops zooming past, the last few lines should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Nice! This output shows that our test passed as expected. You can see the name
    of the test, `LoadModelAndPerformInference`, as defined at the top of its source
    file. Even if it’s not on a microcontroller yet, our code is successfully running
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see what happens when tests fail, let’s introduce an error. Open up the
    test file, *hello_world_test.cc*. It will be at this path, relative to the root
    of the directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the test fail, let’s provide a different input to the model. This will
    cause the model’s output to change, so the assertion that checks the value of
    our output will fail. Find the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the assigned value, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now save the file, and use the following command to run the test again (remember
    to do this from the root of the *tensorflow* directory):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The code will be rebuilt, and the test will run. The final output you see should
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The output contains some useful information about why the test failed, including
    the file and line number where the failure took place (`hello_world_test.cc:94`).
    If this were caused by a real bug, this output would be helpful in tracking down
    the issue.
  prefs: []
  type: TYPE_NORMAL
- en: Project File Structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the help of our test, you’ve learned how to use the TensorFlow Lite for
    Microcontrollers library to run inference in C++. Next, we’re going to walk through
    the source code of an actual application.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, the program we’re building consists of a continuous loop
    that feeds an `x` value into the model, runs inference, and uses the result to
    produce some sort of visible output (like a pattern of flashing LEDs), depending
    on the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Because the application is complex and spans multiple files, let’s take a look
    at its structure and how it all fits together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The root of the application is in *tensorflow/lite/micro/examples/hello_world*.
    It contains the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: BUILD
  prefs: []
  type: TYPE_NORMAL
- en: A file that lists the various things that can be built using the application’s
    source code, including the main application binary and the tests we walked through
    earlier. We don’t need to worry too much about it at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Makefile.inc
  prefs: []
  type: TYPE_NORMAL
- en: A Makefile that contains information about the build targets within our application,
    including *hello_world_test*, which is the test we ran earlier, and *hello_world*,
    the main application binary. It defines which source files are part of them.
  prefs: []
  type: TYPE_NORMAL
- en: README.md
  prefs: []
  type: TYPE_NORMAL
- en: A readme file containing instructions on building and running the application.
  prefs: []
  type: TYPE_NORMAL
- en: constants.h, constants.cc
  prefs: []
  type: TYPE_NORMAL
- en: A pair of files containing various *constants* (variables that don’t change
    during the lifetime of a program) that are important for defining the program’s
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: create_sine_model.ipynb
  prefs: []
  type: TYPE_NORMAL
- en: The Jupyter notebook used in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: hello_world_test.cc
  prefs: []
  type: TYPE_NORMAL
- en: A test that runs inference using our model.
  prefs: []
  type: TYPE_NORMAL
- en: main.cc
  prefs: []
  type: TYPE_NORMAL
- en: The entry point of the program, which runs first when the application is deployed
    to a device.
  prefs: []
  type: TYPE_NORMAL
- en: main_functions.h, main_functions.cc
  prefs: []
  type: TYPE_NORMAL
- en: A pair of files that define a `setup()` function, which performs all the initialization
    required by our program, and a `loop()` function, which contains the program’s
    core logic and is designed to be called repeatedly in a loop. These functions
    are called by *main.cc* when the program starts.
  prefs: []
  type: TYPE_NORMAL
- en: output_handler.h, output_handler.cc
  prefs: []
  type: TYPE_NORMAL
- en: A pair of files that define a function we can use to display an output each
    time inference is run. The default implementation, in *output_handler.cc*, prints
    the result to the screen. We can override this implementation so that it does
    different things on different devices.
  prefs: []
  type: TYPE_NORMAL
- en: output_handler_test.cc
  prefs: []
  type: TYPE_NORMAL
- en: A test that proves that the code in *output_handler.h* and *output_handler.cc*
    is working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: sine_model_data.h, sine_model_data.cc
  prefs: []
  type: TYPE_NORMAL
- en: A pair of files that define an array of data representing our model, as exported
    using `xxd` in the first part of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to these files, the directory contains the following subdirectories
    (and perhaps more):'
  prefs: []
  type: TYPE_NORMAL
- en: '*arduino/*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*disco_f76ng/*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*sparkfun_edge/*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because different microcontroller platforms have different capabilities and
    APIs, our project structure allows us to provide device-specific versions of source
    files that will be used instead of the defaults if the application is built for
    that device. For example, the *arduino* directory contains custom versions of
    *main.cc*, *constants.cc*, and *output_handler.cc* that tailor the application
    to work with Arduino. We dig into these custom implementations later.
  prefs: []
  type: TYPE_NORMAL
- en: Walking Through the Source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how the application’s source is structured, let’s dig into
    the code. We’ll begin with [*main_functions.cc*](https://oreil.ly/BYS5k), where
    most of the magic happens, and branch out into the other files from there.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A lot of this code will look very familiar from our earlier adventures in *hello_world_test.cc*.
    If we’ve covered something already, we won’t go into depth on how it works; we’d
    rather focus mainly on the things you haven’t seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with main_functions.cc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This file contains the core logic of our program. It begins like this, with
    some familiar `#include` statements and some new ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We saw a lot of these in *hello_world_test.cc*. New to the scene are *constants.h*
    and `output_handler.h`, which we learned about in the list of files earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of the file sets up the global variables that will be used within
    *main_functions.cc*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: You’ll notice that these variables are wrapped in a `namespace`. This means
    that even though they will be accessible from anywhere within *main_functions.cc*,
    they won’t be accessible from any other files within the project. This helps prevent
    problems if two different files happen to define variables with the same name.
  prefs: []
  type: TYPE_NORMAL
- en: All of these variables should look familiar from the tests. We set up variables
    to hold all of our familiar TensorFlow objects, along with a `tensor_arena`. The
    only new thing is an `int` that holds `inference_count`, which will keep track
    of how many inferences our program has performed.
  prefs: []
  type: TYPE_NORMAL
- en: The next part of the file declares a function named `setup()`. This function
    will be called when the program first starts, but never again after that. We use
    it to do all of the one-time housekeeping work that needs to happen before we
    start running inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of `setup()` is almost exactly the same as in our tests. We
    set up logging, load our model, set up the interpreter, and allocate memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Familiar territory so far. After this point, though, things get a little different.
    First, we grab pointers to both the input *and* output tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: You might be wondering how we can interact with the output before inference
    has been run. Well, remember that `TfLiteTensor` is just a struct that has a member,
    `data`, pointing to an area of memory that has been allocated to store the output.
    Even though no output has been written yet, the struct and its `data` member still
    exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to end the `setup()` function, we set our `inference_count` variable
    to `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: At this point, all of our machine learning infrastructure is set up and ready
    to go. We have all the tools required to run inference and get the results. The
    next thing to define is our application logic. What is the program actually going
    to *do*?
  prefs: []
  type: TYPE_NORMAL
- en: Our model was trained to predict the sine of any number from 0 to 2π, which
    represents the full cycle of a sine wave. To demonstrate our model, we could just
    feed in numbers in this range, predict their sines, and then output the values
    somehow. We could do this in a sequence so that we show the model working across
    the entire range. This sounds like a good plan!
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we need to write some code that runs in a loop. First, we declare
    a function called `loop()`, which is what we’ll be walking through next. The code
    we place in this function will be run repeatedly, over and over again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'First in our `loop()` function, we must determine what value to pass into the
    model (let’s call it our `x` value). We determine this using two constants: `kXrange`,
    which specifies the maximum possible `x` value as 2π, and `kInferencesPerCycle`,
    which defines the number of inferences that we want to perform as we step from
    0 to 2π. The next few lines of code calculate the `x` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The first two lines of code just divide `inference_count` (which is the number
    of inferences we’ve done so far) by `kInferencesPerCycle` to obtain our current
    “position” within the range. The next line multiplies that value by `kXrange`,
    which represents the maximum value in the range (2π). The result, `x_val`, is
    the value we’ll be passing into our model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`static_cast<float>()` is used to convert `inference_count` and `kInferencesPerCycle`,
    which are both integer values, into floating-point numbers. We do this so that
    we can correctly perform division. In C++, if you divide two integers, the result
    is an integer; any fractional part of the result is dropped. Because we want our
    `x` value to be a floating-point number that includes the fractional part, we
    need to convert the numbers being divided into floating points.'
  prefs: []
  type: TYPE_NORMAL
- en: The two constants we use, `kInferencesPerCycle` and `kXrange`, are defined in
    the files *constants.h* and *constants.cc*. It’s a C++ convention to prefix the
    names of constants with a `k`, so they’re easily identifiable as constants when
    using them in code. It can be useful to define constants in a separate file so
    they can be included and used in any place that they are needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of our code should look nice and familiar; we write our `x` value
    to the model’s input tensor, run inference, and then grab the result (let’s call
    it our `y` value) from the output tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We now have a sine value. Since it takes a small amount of time to run inference
    on each number, and this code is running in a loop, we’ll be generating a sequence
    of sine values over time. This will be perfect for controlling some blinking LEDs
    or an animation. Our next job is to output it somehow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following line calls the `HandleOutput()` function, defined in *output_handler.cc*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We pass in our `x` and `y` values, along with our `ErrorReporter` instance,
    which we can use to log things. To see what happens next, let’s explore *output_handler.cc*.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Output with output_handler.cc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The file *output_handler.cc* defines our `HandleOutput()` function. Its implementation
    is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: All this function does is use the `ErrorReporter` instance to log the `x` and
    `y` values. This is just a bare-minimum implementation that we can use to test
    the basic functionality of our application, for example by running it on our development
    computer.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal, though, is to deploy this application to several different microcontroller
    platforms, using each platform’s specialized hardware to display the output. For
    each individual platform we’re planning to deploy to, such as Arduino, we provide
    a custom replacement for *output_handler.cc* that uses the platform’s APIs to
    control output—for example, by lighting some LEDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, these replacement files are located in subdirectories
    with the name of each platform: *arduino/*, *disco_f76ng/*, and *sparkfun_edge/*.
    We’ll dive into the platform-specific implementations later. For now, let’s jump
    back into *main_functions.cc*.'
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up main_functions.cc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last thing we do in our `loop()` function is increment our `inference_count`
    counter. If it has reached the maximum number of inferences per cycle defined
    in `kInferencesPerCycle`, we reset it to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The next time our loop iterates, this will have the effect of either moving
    our `x` value along by a step or wrapping it around back to 0 if it has reached
    the end of the range.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now reached the end of our `loop()` function. Each time it runs, a new
    `x` value is calculated, inference is run, and the result is output by `HandleOutput()`.
    If `loop()` is continually called, it will run inference for a progression of
    `x` values in the range 0 to 2π and then repeat.
  prefs: []
  type: TYPE_NORMAL
- en: But what is it that makes the `loop()` function run over and over again? The
    answer lies in the file *main.cc*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding main.cc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [C++ standard](https://oreil.ly/BfmkW) specifies that every C++ program
    contain a global function named `main()`, which will be run when the program starts.
    In our program, this function is defined in the file *main.cc*. The existence
    of this `main()` function is the reason *main.cc* represents the entry point of
    our program. The code in `main()` will be run any time the microcontroller starts
    up.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file *main.cc* is very short and sweet. First, it contains an `#include`
    statement for *main_functions.h*, which will bring in the `setup()` and `loop()`
    functions defined there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, it declares the `main()` function itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: When `main()` runs, it first calls our `setup()` function. It will do this only
    once. After that, it enters a `while` loop that will continually call the `loop()`
    function, over and over again.
  prefs: []
  type: TYPE_NORMAL
- en: This loop will keep running indefinitely. Yikes! If you’re from a server or
    web programming background, this might not sound like a great idea. The loop will
    block our single thread of execution, and there’s no way to exit the program.
  prefs: []
  type: TYPE_NORMAL
- en: However, when writing software for microcontrollers, this type of endless loop
    is actually pretty common. Because there’s no multitasking, and only one application
    will ever run, it doesn’t really matter that the loop goes on and on. We just
    continue making inferences and outputting data for as long as the microcontroller
    is connected to power.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now walked through our entire microcontroller application. In the next
    section, we’ll try out the application code by running it on our development machine.
  prefs: []
  type: TYPE_NORMAL
- en: Running Our Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To give our application code a test run, we first need to build it. Enter the
    following Make command to create an executable binary for our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'When the build completes, you can run the application binary by using the following
    command, depending on your operating system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: If you can’t find the correct path, list the directories in *tensorflow/lite/micro/tools/make/gen/*.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you run the binary, you should hopefully see a bunch of output scrolling
    past, looking something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Very exciting! These are the logs written by the `HandleOutput()` function in
    *output_handler.cc*. There’s one log per inference, and the `x_value` gradually
    increments until it reaches 2π, at which point it goes back to 0 and starts again.
  prefs: []
  type: TYPE_NORMAL
- en: As soon as you’ve had enough excitement, you can press Ctrl-C to terminate the
    program.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’ll notice that the numbers are output as values with power-of-two exponents,
    like `1.4137159*2^1`. This is an efficient way to log floating-point numbers on
    microcontrollers, which often don’t have hardware support for floating-point operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the original value, just pull out your calculator: for example, `1.4137159*2^1`
    evaluates to `2.8274318`. If you’re curious, the code that prints these numbers
    is in [*debug_log_numbers.cc*](https://oreil.ly/sb06c).'
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve now confirmed the program works on our development machine. In the next
    chapter, we’ll get it running on some microcontrollers!
  prefs: []
  type: TYPE_NORMAL
