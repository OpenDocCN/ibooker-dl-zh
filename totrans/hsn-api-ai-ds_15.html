<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 12. Using APIs with Artificial Intelligence"><div class="chapter" id="chapter_12">
<h1><span class="label">Chapter 12. </span>Using APIs with Artificial Intelligence</h1>

<blockquote data-type="epigraph" epub:type="epigraph">
  <p>More AI means more APIs.</p>
  <p data-type="attribution">Frank Kilcommins, SmartBear</p>
</blockquote>

<p>In technology circles, AI and APIs <a data-type="indexterm" data-primary="Kilcommins, Frank" id="id2185"/>are sometimes treated as separate specialties. But they are closely related, and getting closer all the time.
In this chapter, you will learn about the ways that AI and APIs overlap, some of the skills you should develop, and how to build APIs that are compatible with AI. Then, you will set up your Part III portfolio project, which you will use in the remaining chapters.</p>






<section data-type="sect1" data-pdf-bookmark="The Overlap of AI and APIs"><div class="sect1" id="id125">
<h1>The Overlap of AI and APIs</h1>

<p>To begin with, APIs are <a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="APIs and" data-tertiary="overlap of AI and APIs" id="ch12dtsrc"/><a data-type="indexterm" data-primary="APIs" data-secondary="AI and" data-tertiary="overlap of AI and APIs" id="ch12dtsrc2"/><a data-type="indexterm" data-primary="data acquisition" data-secondary="APIs as data sources" data-tertiary="AI" id="id2186"/>important data sources—along with databases and files—for training AI models. <a data-type="indexterm" data-primary="REST (Representational State Transfer) APIs" data-secondary="AI model deployment" id="id2187"/>Once a model is trained, a REST API is a common method to make it available for users. You will train a machine learning model in <a data-type="xref" href="ch13.html#chapter_13">Chapter 13</a>, and deploy it with a REST API.</p>

<p>In the same way, cloud-based<a data-type="indexterm" data-primary="deploying to the cloud" data-secondary="AI models" data-tertiary="APIs used for deployment" id="id2188"/> AI tools are advanced machine learning models that are deployed using APIs. AI tools such as generative AI, natural language processing, and others are often cloud hosted and made available as APIs. You will call an Anthropic large language model (LLM) through a REST API in <a data-type="xref" href="ch14.html#chapter_14">Chapter 14</a>.</p>

<p>An emerging area of overlap between<a data-type="indexterm" data-primary="generative AI" data-secondary="apps calling APIs" id="id2189"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="API design tips" data-tertiary="apps calling APIs" id="id2190"/><a data-type="indexterm" data-primary="retrieval augmented generation (RAG)" id="id2191"/><a data-type="indexterm" data-primary="RAG (retrieval augmented generation)" id="id2192"/><a data-type="indexterm" data-primary="knowledge gap of AI models" data-secondary="retrieval augmented generation helping overcome" id="id2193"/> AI and APIs is calling APIs directly from <em>generative AI</em> applications, which are built using LLMs and interact with users via natural language. One type of these applications is called retrieval augmented generation (RAG). In a RAG application, the program calls APIs and other data sources and then feeds the retrieved information to the LLM along with the user prompt. This helps overcome the knowledge gap, in which an LLM only has information that it was trained upon.</p>

<p>Another type of generative AI application<a data-type="indexterm" data-primary="API endpoints" data-secondary="generative AI calling APIs" data-tertiary="agentic applications" id="id2194"/><a data-type="indexterm" data-primary="endpoints of APIs" data-secondary="generative AI calling APIs" data-tertiary="agentic applications" id="id2195"/><a data-type="indexterm" data-primary="agents in AI" data-secondary="agentic AI apps" data-tertiary="determining API endpoints to use" id="id2196"/> uses LLMs to determine what API endpoints to use, which we will call <em>agentic</em> applications in this book. The LLMs make this decision by interpreting definitions from OAS files, Python code, or API documentation. You will create agentic AI applications that call APIs in <a data-type="xref" href="ch14.html#chapter_14">Chapter 14</a> with LangChain and in <a data-type="xref" href="ch15.html#chapter_15">Chapter 15</a> with ChatGPT.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id2197">
<h1>API Perspectives: Bill Doerrfield on the Overlap of AI and APIs</h1>
<p>Bill Doerrfield is the editor-in-chief at <a href="https://nordicapis.com">Nordic APIs</a>, an <a data-type="indexterm" data-primary="Doerrfield, Bill" data-secondary="on overlap of AI and APIs" id="id2198"/><a data-type="indexterm" data-primary="APIs" data-secondary="AI and" data-tertiary="overlap of AI and APIs per Bill Doerrfield" id="id2199"/><a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="APIs and" data-tertiary="overlap of AI and APIs per Bill Doerrfield" id="id2200"/><a data-type="indexterm" data-primary="Nordic APIs community" id="id2201"/>international community of API practitioners and enthusiasts. Bill has been covering the API economy for more than a decade and therefore has a good perspective on how AI is impacting APIs.</p>

<p><em>How has AI affected the field of APIs in recent years?</em></p>

<p>I see the rise of AI and APIs as intrinsically tied. And this isn’t just new to the era of ChatGPT and LLMs. We’ve seen APIs powering software-as-a-service products related to image recognition, object classification, natural language processing, speech-to-text, text-to-speech, chatbot frameworks, and more for years now. But more recently, as APIs have become the lingua franca for new AI services, we’re seeing an interest in APIs as well.</p>

<p><em>What future trends do you see with the intersection of APIs and AI?</em></p>

<p>AI agents are progressively becoming more aware of APIs, and I see APIs as eventually connecting the dots behind the scenes of many complicated user-facing workflows. The next hurdle after that will be giving AI agents the power to not only integrate with data and make POST calls, but also access proprietary data and functionality that is typically monetized and perform transactions on behalf of the user.</p>

<p><em>Based on the rising importance of AI, what skills should API developers and designers be looking to develop?</em></p>

<p>API developers and designers should assume that anything that is publicly accessible will be tapped by an AI. RAG-based AI agents could also circumvent the need for APIs entirely, essentially like robotic process automation using screen scraping. So, we need to make APIs more accessible and defined for an LLM to research and learn about and for an agent to actually integrate with. Otherwise, developers will utilize these other means.</p>

<p>Developers should stay up-to-date with evolving trends in the API space, such as the OpenAPI Specification and its Arazzo Specification. They should consider using AI to enhance the API consumer’s experience when possible. And API designers should also consider ensuring that their services are accessible where developers are already working, such as within their IDEs or AI assistants.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Designing APIs to Use with Generative AI and LLMs"><div class="sect1" id="id126">
<h1>Designing APIs to Use with Generative AI and LLMs</h1>

<p>So, how should you design an API <a data-type="indexterm" data-primary="designing APIs for data scientists" data-secondary="generative AI and LLM use" id="ch12dsgn"/><a data-type="indexterm" data-primary="building APIs" data-secondary="design tips" data-tertiary="generative AI and LLM use" id="ch12dsgn2"/>so that a generative AI application can use it, as Doerrfield recommends? This field is changing rapidly, but here are some initial tips that apply to  LangChain (<a data-type="xref" href="ch14.html#chapter_14">Chapter 14</a>) and ChatGPT (<a data-type="xref" href="ch15.html#chapter_15">Chapter 15</a>).</p>

<p>First, you need to consider whether an API or endpoint is appropriate to use with an agentic generative AI application without additional safeguards in place. The providers of LLMs provide warnings such as that LLMs should not be used “on their own in high-risk situations” (Anthropic Claude 3) and that they “may sometimes provide inaccurate information” (Google NotebookLM). ChatGPT’s documentation simply admonishes the user to “check important info.”</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Researchers and machine learning engineers are exploring additional methods to address risks of using LLMs for API calls and performing other business tasks. <a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="human approval of LLM recommendations" id="id2202"/><a data-type="indexterm" data-primary="human approval of LLM recommendations" id="id2203"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="human approval of LLM recommendations" id="id2204"/><a data-type="indexterm" data-primary="generative AI" data-secondary="human approval of LLM recommendations" id="id2205"/>Some potential safeguards include requiring a human to approve tasks recommended by LLMs before executing, combining multiple AI agents to review tasks before executing, reviewing and filtering inputs and outputs to the models, and reviewing logs of the functioning of the system. In addition, foundational practices of API management and security are required when LLMs use APIs—just as they are when conventional software uses APIs. <a data-type="indexterm" data-primary="security" data-secondary="permissions for systems with LLMs" id="id2206"/><a data-type="indexterm" data-primary="permissions for systems with LLMs" id="id2207"/>A key security practice is to restrict the permissions provided to systems that include LLMs.</p>
</div>

<p>For APIs that are appropriate, here are some design tips based on my own experience and on Blobr’s <a href="https://oreil.ly/jxllA">“Is Your API AI-ready? Our Guidelines and Best Practices”</a>:<a data-type="indexterm" data-primary="“Is Your API AI-ready? Our Guidelines and Best Practices” (Blobr)" data-primary-sortas="Is Your API AI-ready" id="id2208"/><a data-type="indexterm" data-primary="Blobr" id="id2209"/></p>
<dl>
<dt>Limit the size of the data results.</dt>
<dd>
<p>This is important for cost and accuracy. <a data-type="indexterm" data-primary="datasets" data-secondary="size limited for APIs feeding LLMs" id="id2210"/><a data-type="indexterm" data-primary="tokens as unit of charge for services" id="id2211"/><a data-type="indexterm" data-primary="costs per token for AI services" id="id2212"/>From a cost perspective, model providers charge for processing <em>tokens</em>, which are chunks of text. The size of these tokens differs, but the bottom line is the same: the more data a model processes, the greater the cost. In addition to the cost, developers using ChatGPT have found that it struggles to perform calculations from very large datasets returned by APIs. If you are using a model to perform calculations, limiting the size of the data results improves its accuracy.</p>

<p>There are a few ways to accomplish this. Rather than returning all fields related to an entity, return only the critical fields. If an API returns child records in a collection (e.g., <code>product.orders</code>), exclude these from the results and make them available in a separate endpoint. Add parameters, filters, and pagination to narrow down the specific records in the API call.</p>
</dd>
<dt>Make data structures consistent throughout the API.</dt>
<dd>
<p>The more predictable the API is,<a data-type="indexterm" data-primary="datasets" data-secondary="data returned from API conforming" id="id2213"/> the more accurately an AI can use it. By re-using schemas inside your APIs and defining them in your OAS file, you will help the LLM know what to expect in the results. You used Pydantic in the API you created in <a data-type="xref" href="part01.html#part_1">Part I</a>, which enforced standard schemas and published them in your OAS file.</p>
</dd>
<dt>Provide a software development kit (SDK).</dt>
<dd>
<p>Providing an SDK is a way to <a data-type="indexterm" data-primary="SDK (software development kit)" data-secondary="API design tip" data-tertiary="for generative AI and LLMs" id="id2214"/>provide a subset of endpoints and customized API calls that are appropriate for an AI application. The SDK can also include detailed explanations of the API calls and parameters that assist an LLM in understanding its usage. In <a data-type="xref" href="ch14.html#chapter_14">Chapter 14</a>, you’ll use the swcpy SDK with LangChain and LangGraph.</p>
</dd>
<dt>Customize your OpenAPI Specification (OAS).</dt>
<dd>
<p>Some methods of using <a data-type="indexterm" data-primary="OpenAPI Specification (OAS) file" data-secondary="customizing for generative AI and LLMs" id="id2215"/><a data-type="indexterm" data-primary="endpoints of APIs" data-secondary="generative AI calling APIs" data-tertiary="design tips" id="id2216"/><a data-type="indexterm" data-primary="API endpoints" data-secondary="generative AI calling APIs" data-tertiary="design tips" id="id2217"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="API design tips" id="id2218"/><a data-type="indexterm" data-primary="generative AI" data-secondary="API design tips" id="id2219"/>generative AI support reading the OAS file to infer API endpoints to call. You can create a customized OAS file with AI-appropriate endpoints and detailed descriptions of each endpoint and parameter that assist the LLM in inferring their meaning. Endpoints in the OAS file should have unique and clear operation IDs.</p>
</dd>
<dt>Provide a separate endpoint for summary statistics.</dt>
<dd>
<p>If users ask the AI questions<a data-type="indexterm" data-primary="endpoints of APIs" data-secondary="generative AI calling APIs" data-tertiary="separate endpoint for summary statistics" id="id2220"/><a data-type="indexterm" data-primary="API endpoints" data-secondary="generative AI calling APIs" data-tertiary="separate endpoint for summary statistics" id="id2221"/><a data-type="indexterm" data-primary="summary information from generative AI and LLMs" id="id2222"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="API design tips" data-tertiary="separate endpoint for summary statistics" id="id2223"/><a data-type="indexterm" data-primary="generative AI" data-secondary="separate endpoint for summary statistics" id="id2224"/> about summary information and counts, the LLM’s behavior can be erratic. It may try to perform a scan of every record in the API, it may just look at the record identifiers and infer this is the count, or it may try something completely different. Providing dedicated endpoints takes some of the guesswork out.</p>
</dd>
<dt>Provide a search endpoint that doesn’t rely on a record identifier.</dt>
<dd>
<p>LLMs are more comfortable<a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="searching without a key" id="id2225"/> using language than numbers. They like to search based on the information that users are likely to ask them.<a data-type="indexterm" data-startref="ch12dsgn" id="id2226"/><a data-type="indexterm" data-startref="ch12dsgn2" id="id2227"/></p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id2228">
<h1>Arazzo to Define Multistep Processes</h1>
<p>Because AI agents are nondeterministic,<a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="APIs and" data-tertiary="ensuring AI using APIs correctly" id="id2229"/><a data-type="indexterm" data-primary="APIs" data-secondary="AI and" data-tertiary="ensuring AI using APIs correctly" id="id2230"/><a data-type="indexterm" data-primary="APIs" data-secondary="AI and" data-tertiary="Arazzo Specification" id="id2231"/><a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="APIs and" data-tertiary="Arazzo Specification" id="id2232"/><a data-type="indexterm" data-primary="Arazzo Specification" id="id2233"/><a data-type="indexterm" data-primary="OpenAPI Specification (OAS) file" data-secondary="Arazzo Specification" data-tertiary="sequences of API calls" id="id2234"/><a data-type="indexterm" data-primary="API endpoints" data-secondary="ensuring AI using APIs correctly" id="id2235"/><a data-type="indexterm" data-primary="endpoints of APIs" data-secondary="ensuring AI using APIs correctly" id="id2236"/> it can be difficult to ensure that they use APIs correctly, especially when multiple API calls are required to complete a business process. One current method of encouraging AI applications to use multiple API calls together is to add information to the descriptions of each API endpoint in the OAS file or tool functions. These descriptions can explain to AI models how one endpoint relates to another. But it is still up to the model to use them correctly.</p>

<p>An emerging trend that Doerrfield mentioned that may help with this challenge is the <a href="https://oreil.ly/qXKCU">Arazzo Specification</a>. Arazzo defines “sequences of calls and their dependencies” so that multiple API endpoints can be used together to complete a business outcome. If a set of related calls is defined in Arazzo, it could be a <span class="keep-together">deterministic</span> building block that could be used by AI applications, adding reliability to API usage. <a data-type="indexterm" data-primary="OpenAPI Specification (OAS) file" data-secondary="Arazzo Specification" data-tertiary="working with OpenAI" id="id2237"/>Arazzo works with OpenAPI, so it is a natural fit for applications and tools that use OAS today.</p>

<p>Keep an eye on Arazzo to see how AI frameworks add formal support for it in the future. And it may be worth experimenting with LLMs today to see how they can use it today.<a data-type="indexterm" data-startref="ch12dtsrc" id="id2238"/><a data-type="indexterm" data-startref="ch12dtsrc2" id="id2239"/></p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Defining Artificial Intelligence"><div class="sect1" id="id127">
<h1>Defining Artificial Intelligence</h1>
<blockquote>
  <p>Artificial Intelligence is technology that enables computers and machines to simulate human learning, comprehension, problem solving, decision making, creativity and autonomy.</p>
  <p data-type="attribution">“What Is Artificial Intelligence (AI)?”, Cole Stryker and Eda Kavlakoglu, IBM Corporation, 2024</p>
</blockquote>

<p>Aside <a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="definition" id="id2240"/> from that formal definition of AI, an informal definition today is that AI is a computer program that can have humanlike conversations and complete humanlike tasks. <a data-type="indexterm" data-primary="expert systems as AI" id="id2241"/>AI includes <em>expert systems</em>, which have been around for decades. These are complicated rules-based systems that can perform humanlike tasks. <a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="about AI" id="id2242"/>Modern AI focuses on <em>machine learning</em>, in which researchers direct the training of models but don’t explicitly program them. Generative AI using LLMs is one major application of machine learning.</p>

<p><a data-type="xref" href="#general_ID_diagram_ch12">Figure 12-1</a> demonstrates how these terms relate to one another.</p>

<figure><div id="general_ID_diagram_ch12" class="figure">
<img src="assets/haad_1201.png" alt="Diagram of AI terminology" width="578" height="578"/>
<h6><span class="label">Figure 12-1. </span>Diagram of AI terminology</h6>
</div></figure>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Generative AI and Large Language Models (LLMs)"><div class="sect1" id="id210">
<h1>Generative AI and Large Language Models (LLMs)</h1>

<p>Although machine learning<a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="about" id="id2243"/><a data-type="indexterm" data-primary="generative AI" data-secondary="about" id="id2244"/> is used in many different applications, most of the attention from the general public in recent years has been given to generative AI. The ability of these applications to generate text, music, and videos based on text prompts has led to rapid adoption in applications such as OpenAI’s ChatGPT, Microsoft’s Copilot, Google’s Gemini, and many additions to other software applications.</p>

<p>Despite the impressive capabilities of these applications, generative AI also has many risks and limitations associated with it. Providers of some popular models include warnings about bias, hallucinations, mistakes, and harmful content. These are major risks that should be taken seriously by developers who use them. Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch14.html#chapter_14">14</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch15.html#chapter_15">15</a> have more details on the risks and limitations of the models demonstrated in those chapters.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Creating Agentic AI Applications"><div class="sect1" id="id128">
<h1>Creating Agentic AI Applications</h1>

<p>As Doerrfield mentions, AI agents<a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="agents" data-seealso="agents in AI" id="id2245"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="AI agents" data-seealso="agents in AI" id="id2246"/><a data-type="indexterm" data-primary="generative AI" data-secondary="AI agents" data-seealso="agents in AI" id="id2247"/><a data-type="indexterm" data-primary="agents in AI" data-secondary="about" id="id2248"/><a data-type="indexterm" data-primary="agents in AI" data-secondary="agentic AI apps" id="id2249"/> are at the forefront of AI research and development.  An <em>agent</em> is software that controls application flow using an LLM. The more autonomously the LLM controls the system, the more <em>agentic</em> the system is.</p>

<p>Creating AI agents using LLMs is a new field, and a variety of different tools have been released to create agents or orchestrate multiple agents to perform tasks. <a data-type="xref" href="#frameworks_table_ch12">Table 12-1</a> lists several open source frameworks for developing agents and LLM-based applications.<a data-type="indexterm" data-primary="agents in AI" data-secondary="agentic AI apps" data-tertiary="AI agent frameworks" id="id2250"/><a data-type="indexterm" data-primary="Autogen" id="id2251"/><a data-type="indexterm" data-primary="CrewAI" id="id2252"/><a data-type="indexterm" data-primary="LangChain" data-secondary="about" id="id2253"/><a data-type="indexterm" data-primary="LangGraph" data-secondary="about" id="id2254"/><a data-type="indexterm" data-primary="LlamaIndex" id="id2255"/><a data-type="indexterm" data-primary="PydanticAI" data-secondary="Python support" id="id2256"/><a data-type="indexterm" data-primary="Vercel AI SDK" id="id2257"/></p>
<table id="frameworks_table_ch12">
<caption><span class="label">Table 12-1. </span>AI agent frameworks</caption>
<thead>
<tr>
<th>Software</th>
<th>Programming languages supported</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Autogen</p></td>
<td><p>Python, dotnet</p></td>
</tr>
<tr>
<td><p>CrewAI</p></td>
<td><p>Python</p></td>
</tr>
<tr>
<td><p>LangChain/LangGraph</p></td>
<td><p>Python</p></td>
</tr>
<tr>
<td><p>LlamaIndex</p></td>
<td><p>Python, Typescript</p></td>
</tr>
<tr>
<td><p>PydanticAI</p></td>
<td><p>Python</p></td>
</tr>
<tr>
<td><p>Vercel AI SDK</p></td>
<td><p>Typescript</p></td>
</tr>
</tbody>
</table>

<p>You will use LangChain and LangGraph in <a data-type="xref" href="ch14.html#chapter_14">Chapter 14</a> to call APIs.</p>
<aside data-type="sidebar" epub:type="sidebar" class="less_space pagebreak-before"><div class="sidebar" id="id2258">
<h1>AI Perspectives: Samuel Colvin on Agentic AI</h1>
<p>Samuel Colvin is the founder of Pydantic,<a data-type="indexterm" data-primary="agents in AI" data-secondary="Samuel Colvin on agents and agent frameworks" id="id2259"/><a data-type="indexterm" data-primary="Colvin, Samuel" id="id2260"/><a data-type="indexterm" data-primary="Pydantic" data-secondary="founder Samuel Colvin" id="id2261"/><a data-type="indexterm" data-primary="PydanticAI" data-secondary="Samuel Colvin on agents and agent frameworks" id="id2262"/> the open source Python library that you have been using throughout this book for data validation and serialization. Pydantic is used in many of the model providers’ SDKs and Python-based AI agent frameworks. I talked to Colvin about agents and Pydantic’s own agent framework, 
<span class="keep-together">PydanticAI</span>.</p>

<p><em>What is the importance of AI agents and agent frameworks in the next few years of AI development?</em></p>

<p>I’m confident that in time, people won’t be using the AI providers’ SDKs directly. There will be some libraries on top that make opinionated decisions about what people want to do. If you look at web frameworks as a parallel, technically they are constraints on what you can do [to create web applications], but they are constraints that everyone is happy with because you get to write much more high-level code. I’m hopeful that PydanticAI is a big part of that.</p>

<p><em>What role do you see PydanticAI playing in creating AI applications?</em></p>

<p>My hope is that PydanticAI can become the default in terms of how you actually take GenAI and put it into production, because we have had more Python experience but also just work harder on the high-level developer experience and the kinds of checks you need when you’re trying to put that stuff into production. For example, for PydanticAI we have put enormous effort into making it type safe, including the dependency system.</p>

<p><em>With the AI era that we’re moving into, what are the skills or tools or frameworks you think developers should be learning right now?</em></p>

<p>I think that there are bits of intuition about how LLMs work, nuances of the peculiarities of how they behave, that are worth getting to grips with. And good fundamental Python programming techniques: there is a temptation to think I can skip learning the fundamentals of good programming techniques because I’m doing GenAI. If anything, some of those things are even more important. The fact that you have a non-deterministic system that you’re interacting with means you need to be even more sure that your fundamental unit tests for the bits that should be deterministic are rock solid.</p>

<p>Two more standards that Colvin recommends developers becoming <a data-type="indexterm" data-primary="OpenTelemetry" id="id2263"/><a data-type="indexterm" data-primary="logging" data-secondary="AI applications via OpenTelemetry" id="id2264"/><a data-type="indexterm" data-primary="observability in AI via OpenTelemetry" id="id2265"/><a data-type="indexterm" data-primary="Model Context Protocol (MCP)" id="id2266"/><a data-type="indexterm" data-primary="MCP (Model Context Protocol)" id="id2267"/><a data-type="indexterm" data-primary="context for LLMs" id="id2268"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="context for" id="id2269"/>familiar with are <a href="https://oreil.ly/OTEL">OpenTelemetry</a> for observability and logging of AI applications, and <a href="https://oreil.ly/mcptL">Model Context Protocol (MCP)</a> for providing context to LLMs.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Introducing Your Part III Portfolio Project"><div class="sect1" id="id211">
<h1>Introducing Your Part III Portfolio Project</h1>

<p>You will create a portfolio project that demonstrates your ability to work with API and AI. Here is an overview of the work ahead of you:<a data-type="indexterm" data-primary="portfolio projects" data-secondary="Part III project introduction" id="id2270"/><a data-type="indexterm" data-primary="portfolio projects" data-secondary="AI project introduction" id="id2271"/><a data-type="indexterm" data-primary="portfolio projects" data-secondary="machine learning project introduction" id="id2272"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="portfolio project introduction" id="id2273"/><a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="portfolio project introduction" id="id2274"/></p>

<ul>
<li>
<p><a data-type="xref" href="ch13.html#chapter_13">Chapter 13</a>: Deploying a machine learning API</p>
</li>
<li>
<p><a data-type="xref" href="ch14.html#chapter_14">Chapter 14</a>: Using APIs with LangChain</p>
</li>
<li>
<p><a data-type="xref" href="ch15.html#chapter_15">Chapter 15</a>: Using ChatGPT to call your API</p>
</li>
</ul>

<p>Each of these tasks will enable you to showcase your API and AI skills in a unique way.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Getting Started with Your GitHub Codespace"><div class="sect1" id="id392">
<h1>Getting Started with Your GitHub Codespace</h1>

<p>You will continue to use GitHub Codespaces for all the code you develop in Part III. If you didn’t create a GitHub account yet, do that now.</p>








<section data-type="sect2" data-pdf-bookmark="Cloning the Part III Repository"><div class="sect2" id="cloning_part_3_sect">
<h2>Cloning the Part III Repository</h2>

<p>All of the <a data-type="xref" href="part03.html#part_3">Part III</a> code examples<a data-type="indexterm" data-primary="GitHub" data-secondary="cloning the repository" data-tertiary="cloning Part III" id="id2275"/><a data-type="indexterm" data-primary="resources online" data-secondary="chapter code" data-tertiary="Part III code" id="id2276"/><a data-type="indexterm" data-primary="GitHub" data-secondary="Codespaces" data-tertiary="URL of" id="id2277"/> are contained in  <a href="https://github.com/handsonapibook/api-book-part-three">this book’s GitHub repository</a>.</p>

<p>To clone the repository, log in to GitHub and go the <a href="https://github.com/new/import">GitHub Import Repository page</a>. Enter the following information in the fields on this page:<a data-type="indexterm" data-primary="Codespaces (GitHub)" data-secondary="URL of" data-tertiary="Part III" id="id2278"/></p>

<ul>
<li>
<p>The URL for your source repository: <strong><code>https://github.com/handsonapibook/api-book-part-three</code></strong></p>
</li>
<li>
<p>Your username for your source code repository: Leave blank.</p>
</li>
<li>
<p>Your access token or password for your source code repository: Leave blank.</p>
</li>
<li>
<p>Repository name: <strong><code>ai-project</code></strong></p>
</li>
<li>
<p>Public: Select this so that you can share the results of the work you are doing.</p>
</li>
</ul>

<p>Click Begin Import. The import process will begin, and the message “Preparing your new repository” will be displayed. After several minutes, you will receive an email notifying you that your import has finished. Follow the link to your new cloned repository.</p>
</div></section>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Launching Your GitHub Codespace"><div class="sect2" id="id212">
<h2>Launching Your GitHub Codespace</h2>

<p>In your new repository, click the<a data-type="indexterm" data-primary="GitHub" data-secondary="Codespaces" data-tertiary="launching your Codespace" id="id2279"/><a data-type="indexterm" data-primary="Codespaces (GitHub)" data-secondary="launching your Codespace" data-tertiary="Part III" id="id2280"/><a data-type="indexterm" data-primary="GitHub" data-secondary="cloning the repository" data-tertiary="launching your Part III Codespace" id="id2281"/> Code button and select the Codespaces tab. Click “Create codespace on main.” You should see a page with the status “Setting up your codespace”. Your Codespace window will be opened as the setup continues. When the setup completes, your display will look similar to <a data-type="xref" href="#codespace_setup_complete_ch12">Figure 12-2</a>.</p>

<figure><div id="codespace_setup_complete_ch12" class="figure">
<img src="assets/haad_1202.png" alt="GitHub Codespace for Part 3" width="1022" height="503"/>
<h6><span class="label">Figure 12-2. </span>GitHub Codespace for Part III</h6>
</div></figure>

<p>Your Codespace is now created with the cloned repository. This is the environment you will be using for Part III of this book. Open the <a href="https://oreil.ly/nLbqH">GitHub Codespaces page</a> and scroll down the page to find this new Codespace, click the ellipsis to the right of the name, and select Rename. Enter the name <strong><code>Part 3 Portfolio project codespace</code></strong> and click Save. You should see the message “Your codespace <em>Part 3 Portfolio project codespace</em> has been updated.” Click the ellipsis again and then click the ribbon next to “Auto-delete codespace” to turn off auto-deletion.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>To save space on the page, <a data-type="indexterm" data-primary="Codespaces (GitHub)" data-secondary="terminal window" data-tertiary="trimming prompt directory listing" id="id2282"/><a data-type="indexterm" data-primary="GitHub" data-secondary="Codespaces" data-tertiary="terminal window prompt trimmed" id="id2283"/><a data-type="indexterm" data-primary="terminal window of GitHub Codespace" data-secondary="trimming prompt directory listing" id="id2284"/>I have trimmed the directory listing in the terminal prompt of my Codespace. You can do this in your Codespace by editing the <em>/home/codespace/.bashrc</em> file in VS Code. Find the <code>export PROMPT_DIRTRIM</code> statement and set it to <code>export PROMPT_DIRTRIM=1</code>. To load the values the first time, execute this terminal command: <code>source ~/.bashrc</code>.</p>
</div>
</div></section>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Additional Resources"><div class="sect1" id="id129">
<h1>Additional Resources</h1>

<p>To view a 100-point scorecard for AI compatibility of APIs, read Blobr’s <a href="https://oreil.ly/JLaK1">“Is Your API AI-ready? Our Guidelines and Best Practices”</a>.<a data-type="indexterm" data-primary="APIs" data-secondary="AI and" data-tertiary="API compatibility scorecard online" id="id2285"/><a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="APIs and" data-tertiary="API compatibility scorecard online" id="id2286"/><a data-type="indexterm" data-primary="“Is Your API AI-ready? Our Guidelines and Best Practices” (Blobr)" data-primary-sortas="Is Your API AI-ready" id="id2287"/><a data-type="indexterm" data-primary="Blobr" id="id2288"/></p>

<p>To see an example of working around the limitations of a GPT, read <a href="https://oreil.ly/khh0J">“Syntax Sunday: Custom API Wrapper for GPTs” by Kade Halabuza</a>.<a data-type="indexterm" data-primary="GPT limitation workaround online" id="id2289"/><a data-type="indexterm" data-primary="“Syntax Sunday: Custom API Wrapper for GPTs” (Halabuza)" data-primary-sortas="Syntax Sunday" id="id2290"/><a data-type="indexterm" data-primary="Halabuza, Kade" id="id2291"/></p>

<p>To learn more about possible futures of AI and APIs, read <a href="https://oreil.ly/t2lxH">“AI + APIs — What 12 Experts Think The Future Holds” by Peter Schroeder</a>.<a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="APIs and" data-tertiary="future of AI and APIs" id="id2292"/><a data-type="indexterm" data-primary="APIs" data-secondary="AI and" data-tertiary="future of AI and APIs" id="id2293"/><a data-type="indexterm" data-primary="“AI + APIs — What 12 Experts Think The Future Holds” (Schroeder)" data-primary-sortas="AI APIs" id="id2294"/><a data-type="indexterm" data-primary="Schroeder, Peter" id="id2295"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id393">
<h1>Summary</h1>

<p>This chapter introduced the basics of AI and explained how it relates to APIs.</p>

<p>In <a data-type="xref" href="ch13.html#chapter_13">Chapter 13</a>, you will create a machine learning model and deploy it using FastAPI.</p>
</div></section>
</div></section></div>
</div>
</body></html>