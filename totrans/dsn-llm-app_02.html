<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 1. Introduction"><div class="chapter" id="chapter_llm-introduction">
<h1><span class="label">Chapter 1. </span>Introduction</h1>


<p>AI is no longer the realm of science fiction novels<a data-type="indexterm" data-primary="LLMs (Large Language Models)" id="xi_LLMsLargeLanguageModels1452"/> and dystopian Hollywood movies. It is fast becoming an integral part of people’s lives. Most of us interact with AI on a daily basis, often without even realizing it.</p>

<p>Current progress in AI has to a large extent been driven by advances in language modeling. Large language models (LLMs) represent one of the most significant technological advances in recent times, marking a new epoch in the world of tech. Similar inflection points in the past include the advent of the computer that ushered in the digital revolution, the birth of the internet and the World Wide Web that laid the foundation for a hyperconnected world, and the emergence of the smartphone that reshaped human communication. The ongoing AI revolution is poised to make a similar transformative impact.</p>

<p>LLMs belong to a class of models referred to as generative AI<a data-type="indexterm" data-primary="generative AI" id="id374"/>. The distinguishing factor is the ability of these models to generate responses to user queries<a data-type="indexterm" data-primary="prompting" id="id375"/>, called <em>prompts</em>. Generative AI encompasses models that generate images, videos, speech, music, and of course text. While there is an increasing focus on bringing all these modalities together into a single model, in this book we will stick to language and LLMs.</p>

<p>In this chapter, we will introduce language models and define what makes a language model <em>large</em>. We will provide a brief history of LLMs, contextualizing their place within the field of natural language processing (NLP)<a data-type="indexterm" data-primary="natural language processing (NLP)" id="id376"/><a data-type="indexterm" data-primary="NLP (natural language processing)" id="id377"/> and their evolution. We will highlight the impact LLMs are already having in the world and showcase key use cases, while discussing their strengths and limitations. We will also introduce LLM prompting and show how to interact with an LLM effectively, either through a user interface or through an API. Finally, we will end this chapter with a quick tutorial on building a <em>Chat with my PDF</em> chatbot prototype<a data-type="indexterm" data-primary="“Chat with your PDF” chatbot prototype" data-primary-sortas="Chat with your PDF chatbot prototype" id="id378"/>. We will then discuss the limitations of the prototype and the factors limiting its suitability for production use cases, thus setting the stage for the rest of the book.</p>






<section data-type="sect1" data-pdf-bookmark="Defining LLMs"><div class="sect1" id="id2">
<h1>Defining LLMs</h1>

<p>A model<a data-type="indexterm" data-primary="models" id="id379"/> is an approximation of a real-world concept or phenomenon. A faithful model will be able to make predictions about the concept it is approximating. A language model<a data-type="indexterm" data-primary="language models" id="id380"/><a data-type="indexterm" data-primary="models" data-secondary="language" id="id381"/> approximates human language and is built by training over a large body of text, thus imbuing it with various properties of language, including aspects of grammar (syntax) and meaning (semantics).</p>

<p>One way to train a language model is to teach it to predict the next token<a data-type="indexterm" data-primary="tokens and tokenization" id="id382"/> (this is equivalent to a word or a subword, but we will ignore this distinction for now) in a known text sequence. The model is trained over a large number of such sequences<a data-type="indexterm" data-primary="parameters" id="id383"/>, and its <em>parameters</em> are updated iteratively such that it gets better at its predictions.</p>

<p>For example, consider the following text sequence appearing in a training dataset:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">After</code> <code class="n">a</code> <code class="n">physical</code> <code class="n">altercation</code> <code class="k">with</code> <code class="n">the</code> <code class="n">patrons</code> <code class="n">of</code> <code class="n">a</code> <code class="n">restaurant</code><code class="p">,</code> <code class="n">Alex</code> <code class="n">was</code> <code class="n">feeling</code>
<code class="n">extremely</code> <code class="n">pleased</code> <code class="k">with</code> <code class="n">himself</code><code class="o">.</code> <code class="n">He</code> <code class="n">walked</code> <code class="n">out</code> <code class="k">with</code> <code class="n">a</code> <code class="n">swagger</code> <code class="ow">and</code> <code class="n">confidence</code>
<code class="n">that</code> <code class="n">betrayed</code> <code class="n">his</code> <code class="n">insecurities</code><code class="o">.</code> <code class="n">Smiling</code> <code class="kn">from</code> <code class="nn">ear</code> <code class="n">to</code> <code class="n">ear</code><code class="p">,</code> <code class="n">he</code> <code class="n">noticed</code> <code class="n">rain</code> <code class="n">drops</code>
<code class="n">grazing</code> <code class="n">his</code> <code class="n">face</code> <code class="ow">and</code> <code class="n">proceeded</code> <code class="n">to</code> <code class="n">walk</code> <code class="n">toward</code> <code class="n">the</code> <code class="n">hostel</code><code class="o">.</code></pre>

<p>and the language model predicts the next word that comes after “…​ and proceeded to walk toward the <em><em/>_</em>”</p>

<p>There are a large number of valid continuations to this text sequence. It could be “building” or “shelter,” but it could also be “embankment” or “catacomb.” However, it is definitely not “the” or “is,” because that would break the rules of the English language. After training on a sufficiently large body of text, the model learns that neither “the” nor “is” are valid continuations. Thus, you can see how a simple task like learning to predict the next word in a text sequence can lead the model to learning the grammar of the language in its parameters, as well as even more complex skills.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In practice, language models don’t exactly output a single word or subword as the next token in a text sequence. They output a probability distribution<a data-type="indexterm" data-primary="probability distribution, language model prediction" id="id384"/> over the entire vocabulary. (We will explore how this vocabulary is defined and constructed in <a data-type="xref" href="ch03.html#chapter-LLM-tokenization">Chapter 3</a>). A well-trained model will have high probabilities for valid continuations and very low probabilities for invalid continuations.</p>
</div>

<p><a data-type="xref" href="#next-token-prediction">Figure 1-1</a> describes the model training process in a nutshell. The output of the model prediction is a probability distribution over the entire vocabulary of the language. This is compared to the original sequence, and the parameters of the model are updated according to an algorithm so that it makes better predictions in the future. This is repeated over a very large dataset. We will describe the model training process in detail in the next three chapters.</p>

<figure><div id="next-token-prediction" class="figure">
<img src="assets/dllm_0101.png" alt="next-token-prediction" width="600" height="166"/>
<h6><span class="label">Figure 1-1. </span>Model training using next token prediction</h6>
</div></figure>

<p>Is there a limit to what a model can learn from next-token prediction alone? This is a very important question that determines how powerful LLMs can eventually be. There is plenty of disagreement in the research community, with <a href="https://oreil.ly/sUAcl">some researchers</a> arguing next-token prediction is enough to achieve human-level intelligence in models, and <a href="https://oreil.ly/7QG-l">others pointing out</a> the shortfalls of this paradigm. We will come back to this question throughout the book, and especially in <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>, where we will discuss skills like reasoning.</p>

<p>Modern-day language models<a data-type="indexterm" data-primary="language models" id="xi_languagemodels14827"/><a data-type="indexterm" data-primary="models" data-secondary="language" id="xi_modelslanguage14827"/> are based on neural networks<a data-type="indexterm" data-primary="neural networks" id="id385"/>. Several types of neural network architectures are used to train LLMs, the most prominent being the Transformer. We will learn more about neural networks, Transformers, and other architectures in detail in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>.</p>

<p>Language models can be trained to model not just human languages but also programming languages like Python or Java. In fact, the Transformer architecture<a data-type="indexterm" data-primary="Transformer architecture" id="id386"/> and the next-token prediction objective<a data-type="indexterm" data-primary="next-token prediction objective" id="id387"/> can be applied to  sequences that are not languages in the traditional sense at all, such as representations of chess moves, DNA sequences, or airline schedules.</p>

<p>For example, Adam Karvonen<a data-type="indexterm" data-primary="Karvonen, Adam" id="id388"/><a data-type="indexterm" data-primary="Chess-GPT" id="id389"/> trained <a href="https://oreil.ly/oluZN">Chess-GPT</a>, a model trained only on chess games represented in portable game notation (PGN) strings. PGN strings for chess look like “1. e4 d5 2. exd5 Qxd5…” and so on. Even without providing the rules of the game explicitly, and just training the model to predict the next character in the PGN sequence, the model was able to learn the rules of the game including moves like castling, check, and checkmate; and it could even win chess games against experts.
This shows the power of the next-token prediction objective and the Transformer architecture that forms the basis of the model. In <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>,  we will learn how to train our own Chess-GPT from scratch.</p>

<p>Another such example is the <a href="https://oreil.ly/31DXq">Geneformer</a>, a model trained<a data-type="indexterm" data-primary="Geneformer" id="id390"/> on millions of single-cell transcriptomes (representations of RNA molecules in a single cell), which can be used for making predictions in network biology, including disease progression, gene-dosage sensitivity, and therapeutic candidates.</p>

<p>Therefore, I encourage you to think beyond the realm of human language when brainstorming novel use cases for language models. If you have a concept or phenomenon that can be encoded in a discrete sequence using a finite vocabulary (we will more formally define vocabulary in <a data-type="xref" href="ch03.html#chapter-LLM-tokenization">Chapter 3</a>), then we can potentially train a useful model on it.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Is there something special about the structure of language that makes it amenable to be modeled using the next-token prediction objective? Or is the word “language” in language models just a historical accident and any stream of tokens can be modeled using this paradigm? While this is still a <a href="https://oreil.ly/nJiQW">topic of debate</a> in the research community, directly modeling speech, video, etc. using this paradigm hasn’t been as effective, perhaps showing that the discrete nature of text and the structure provided by language, be it a human language like English, a programming language like Python, or a domain-specific code like DNA sequences, is crucial to modeling success.</p>
</div>

<p>Around 2019, researchers realized that increasing the size of the language model (typically measured by the number of parameters) predictably improved performance, with no saturation point in sight. This led to Kaplan et al.’s work on LLM scaling laws (see the following sidebar), which derives a mathematical formula describing the relationship between the amount of computation (henceforth referred to as “compute”) for training the model, the training dataset size, and the model size. Ever since then, companies and organizations have been training increasingly larger models<a data-type="indexterm" data-startref="xi_languagemodels14827" id="id391"/><a data-type="indexterm" data-startref="xi_modelslanguage14827" id="id392"/>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id393">
<h1>Scaling Laws for Language Models</h1>
<p>In early 2020, <a href="https://oreil.ly/29GZV">Kaplan et al.</a> from OpenAI published a study establishing the scaling laws of language models<a data-type="indexterm" data-primary="scaling laws" id="id394"/> that ushered in the LLM era. They found a power-law relationship between the performance of the language model (measured by model loss; we will describe that in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>) and the size of the dataset used to train the model, the amount of compute used to train the model, and the size of the model itself, measured in terms of number of parameters. Simply put, the larger the model size, compute size, and amount of training data, the better 
<span class="keep-together">the model</span>.</p>

<p>More specifically, they found that for a fixed compute budget, increasing the size of the training dataset and the model in tandem improves the performance of the resulting LLM, but the dataset size needs to increase only by 1.8x for every 5.5x increase in model size to maintain an optimal level of performance. This is because larger models are more sample-efficient, meaning they need relatively fewer training examples to learn. Thus, models from that era mainly focused on increasing model sizes as much as possible.</p>

<p>However, in 2022, <a href="https://oreil.ly/igLNZ">Hoffmann et al.</a> from DeepMind pointed out that Kaplan et al. underestimated the impact of data size, resulting in language models from that era being significantly undertrained. They showed that to optimize performance of an LLM at a fixed compute budget (called compute-optimal), the training data size needs to increase at the same proportion as the model size. This led to the newer generation of models being trained on more data.</p>

<p>Note that both these scaling laws apply to compute-optimal LLMs<a data-type="indexterm" data-primary="compute-optimal LLMs" id="id395"/>, where you start with a fixed compute budget and ask, “What is the best LLM I can train with this budget?” But sometimes you are bottlenecked by other criteria, like the size of the model. Smaller models are faster to run and more energy efficient. In this case, one can significantly increase the training data size and continue seeing (albeit relatively smaller) performance gains, at the same model size. This is the trend driving more recent LLMs, especially in the open-source space.</p>
</div></aside>

<p>There is no accepted convention about when a language model is considered “large.”
In fact, as the largest models get even larger, some models that would have been designated as LLMs only a couple of years ago are now termed small language models (SLMs)<a data-type="indexterm" data-primary="small language models (SLMs)" id="id396"/>. In this book, we will remain generous and continue to refer to all language models over a billion parameters as “large.”</p>

<p>Another way in which a “large” language model differs from smaller ones is the emergent capabilities it possesses. First hypothesized by <a href="https://oreil.ly/RQfii">Wei et al.</a>, emergent capabilities are those capabilities exhibited by larger models but not smaller ones.</p>

<p>According to this theory, for tasks that require these capabilities, the performance of smaller models is close to random. However, when the model size reaches a threshold, the performance suddenly starts to increase with size. Examples include multi-digit arithmetic operations, arithmetic and logical reasoning, etc. This also suggests that certain capabilities that are completely absent in current models could be exhibited by future larger models.</p>

<p>These thresholds are not absolute, and as we see more advances in language modeling, data quality improvements, etc.,  we can expect the thresholds to come down.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a href="https://oreil.ly/OXk6I">Schaeffer et al.</a> claim that the sudden jump in performance for certain tasks at a particular model size threshold is just an artifact of the evaluation metrics used to judge performance. This happens because many metrics do not assign partial credit and only reward fully solving the task, so model improvements might not be tracked. On the other hand, one could argue that for tasks like multi-step arithmetic, getting the answer partially right is just as useless as getting it completely wrong.</p>
</div>

<p>The question of what abilities are emergent is still being explored in the research community. In <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>, we will discuss its implications for selecting the right model for our desired use case.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Unfortunately the phrase “emergent properties” has multiple meanings<a data-type="indexterm" data-primary="emergent properties" id="id397"/> in the literature. In some papers, the phrase is used to describe those capabilities that the model is not explicitly trained for. In this book, we will stick to <a href="https://oreil.ly/bkVoj">Wei et al.’s definition</a>.</p>
</div>

<p>To understand how current LLMs came to be, it is instructive to walk through a brief history of them. As more historical details are out of scope for the book, we will provide links to external resources for further reading throughout the section.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="A Brief History of LLMs"><div class="sect1" id="id3">
<h1>A Brief History of LLMs</h1>

<p>To present the history of LLMs<a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="history" id="xi_LLMsLargeLanguageModelshistory110231"/>, we need to start from the history of NLP<a data-type="indexterm" data-primary="natural language processing (NLP)" id="xi_naturallanguageprocessingNLP110273"/><a data-type="indexterm" data-primary="NLP (natural language processing)" id="xi_NLPnaturallanguageprocessing110273"/>, the field that LLMs originated from. For a more detailed history of NLP, refer<a data-type="indexterm" data-primary="Jurafsky, Daniel" id="id398"/> to Daniel Jurafsky’s seminal book, <a href="https://oreil.ly/zzU9R"><em>Speech and Language Processing</em>, 2nd edition</a>.</p>








<section data-type="sect2" data-pdf-bookmark="Early Years"><div class="sect2" id="id4">
<h2>Early Years</h2>

<p>The field traces its origins to the 1950s, driven by demand for  <em>machine translation</em>, the task<a data-type="indexterm" data-primary="machine translation" id="id399"/> of automatically translating from one language to another. The early days were dominated by symbolic approaches;  these were rule-based algorithms based on <a href="https://oreil.ly/ELKSe">linguistic theories</a> influenced by the works of linguists like Noam Chomsky<a data-type="indexterm" data-primary="Chomsky, Noam" id="id400"/>.</p>

<p>In the mid-1960s, Joseph Weizenbaum<a data-type="indexterm" data-primary="Weizenbaum, Joseph" id="id401"/><a data-type="indexterm" data-primary="ELIZA" id="id402"/> released ELIZA, a chatbot program that applied pattern matching using <a href="https://oreil.ly/rIAWY">regular expressions</a> on the user’s input and selected response templates to generate an output. ELIZA consisted of several scripts, the most famous one being DOCTOR, that simulated a psychotherapist. This variant would respond by rephrasing the user’s input in the form of a question, similar to how a therapist would. The rephrasing was performed by filling in predefined templates with pattern-matched words from the input.</p>

<p>As an example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">User</code><code class="p">:</code> <code class="s1">'I am not feeling well'</code></pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">ELIZA</code><code class="p">:</code> <code class="s1">'Do you believe it is normal to be not feeling well?'</code></pre>

<p>You can try chatting with <a href="https://oreil.ly/5g0e_">ELIZA online</a>. Even in the era of ChatGPT<a data-type="indexterm" data-primary="ChatGPT" id="id403"/><a data-type="indexterm" data-primary="OpenAI" data-secondary="ChatGPT" id="id404"/>, ELIZA can hold a somewhat convincing conversation, despite the fact that it is just rules-based.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id405">
<h1>Exercise</h1>
<p>Read through the <a href="https://oreil.ly/MkKmC">classic paper</a> by Weizenbaum introducing ELIZA. In general, I recommend developing a habit of reading papers from yesteryears, as they are a good source of inspiration. Ideas get recycled all the time, and infeasible ideas from the past become feasible today due to advances in technology.</p>

<p>Additionally, go through the code for the <a href="https://oreil.ly/Y1pSc">Python implementation</a> of ELIZA by Wade Brainerd to understand how powerful carefully constructed rule-based systems can be. Even in the era of LLMs, you should not shy away from using rule-based approaches for your language tasks where it makes sense!</p>
</div></aside>

<p>Rule-based systems are brittle, hard to construct, and a maintenance nightmare. As the decades rolled by, the limitations of symbolic approaches became more and more evident, and the relative effectiveness of statistical approaches ensured that they became more commonplace. NLP researcher <a href="https://oreil.ly/AmtvE">Frederick Jelinek</a><a data-type="indexterm" data-primary="Jelinek, Frederick" id="id406"/> famously quipped, “Every time I fire a linguist, the performance of the speech recognizer goes up.”</p>

<p>Machine learning–based approaches<a data-type="indexterm" data-primary="machine learning" id="id407"/> became more widely used in the 1990s and 2000s. Traditional machine learning relied on human-driven feature engineering and feature selection, the process of identifying features (characteristics of the input) that are predictive to solve a task. These features could be statistical, like the average word length, or linguistic, like parts of speech. To learn more about traditional statistical NLP, I recommend reading Christopher Manning’s book, <a href="https://oreil.ly/MIC70"><em>Foundations of Statistical Natural Language Processing</em></a><a data-type="indexterm" data-primary="Manning, Christopher" id="id408"/>.</p>

<p>The relevance of linguistics<a data-type="indexterm" data-primary="linguistics" id="id409"/> to modern-day NLP application development is a point of debate. Many university courses on NLP have completely dropped content related to linguistics. Even though I don’t directly use linguistics in my work, I find that I rely on them to develop intuitions about model behavior more than I expect. As such, I recommend Emily<a data-type="indexterm" data-primary="Bender, Emily" id="id410"/> Bender’s books on <a href="https://oreil.ly/hWR8S">syntax</a> and <a href="https://oreil.ly/7liiS">semantics</a> to understand the basics of this field.</p>

<p>The 2010s saw the advent of deep learning<a data-type="indexterm" data-primary="deep learning" id="id411"/> and its widespread impact on NLP. Deep learning is characterized by multi-layer neural network models that learn informative features by themselves given only raw input, thus removing the need for cumbersome feature engineering. Deep learning forms the foundation for modern NLP and LLMs. To dig deeper into  the principles of deep learning and neural networks, I recommend <a href="https://oreil.ly/0gv0D">Goodfellow et al.’s book</a>. For more hands-on deep learning training, I recommend Zhang et al.’s <a href="https://oreil.ly/YN_3Y"><em>Dive into Deep Learning</em></a>.</p>

<p>During the early years of deep learning, it was customary to construct a task-specific architecture to solve each task. Some of the types of neural network architectures used include multi-layer perceptrons, convolutional neural networks, recurrent 
<span class="keep-together">neural</span> networks, and recursive neural networks<a data-type="indexterm" data-startref="xi_naturallanguageprocessingNLP110273" id="id412"/><a data-type="indexterm" data-startref="xi_NLPnaturallanguageprocessing110273" id="id413"/>. To learn more about this era of NLP, I recommend <a href="https://oreil.ly/MCOp4"><em>Neural Network Methods for Natural Language Processing</em></a> by Yoav<a data-type="indexterm" data-primary="Goldberg, Yoav" id="id414"/> Goldberg (Springer Cham).</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Modern LLM Era"><div class="sect2" id="id5">
<h2>The Modern LLM Era</h2>

<p>In 2017, the <a href="https://oreil.ly/AAuvL">Transformer architecture</a> was<a data-type="indexterm" data-primary="Transformer architecture" id="id415"/> invented, quickly followed by the invention of efficient <em>transfer learning</em> techniques pioneered by <a href="https://oreil.ly/E15Yn">Howard et al.</a> among others and Transformer-based language models like <a href="https://oreil.ly/-Yhwz">BERT</a>. These advances removed the need for constructing complex task-specific architectures. Instead, one could use the same Transformer model to train a variety of tasks. This new paradigm divided the training<a data-type="indexterm" data-primary="pre-training of data" id="id416"/><a data-type="indexterm" data-primary="fine-tuning models" id="id417"/> step into two stages: <em>pre-training</em> and <em>fine-tuning</em>. An initial large-scale pre-training step initialized the Transformer model with general language capabilities. Subsequently, the pre-trained model could be trained on more concrete tasks, like information extraction or sentiment detection, using a process called fine-tuning. We will cover fine-tuning extensively throughout the book.</p>

<p>While academia and open-source collectives have made crucial and critical contributions to language modeling, large tech companies like OpenAI, Google, Meta, and Anthropic have taken the lead in training and releasing progressively larger LLMs. OpenAI<a data-type="indexterm" data-primary="OpenAI" id="id418"/> in particular has played a pioneering role in advancing language modeling technology.  The trajectory of the evolution of LLMs in the modern era can be traced through the advances ushered in by each version of the GPT (Generative Pre-trained Transformer)<a data-type="indexterm" data-primary="Generative Pre-trained Transformer (GPT) models" id="id419"/> family of models trained by OpenAI:</p>
<dl>
<dt><a href="https://oreil.ly/dFPSE">GPT-1</a></dt>
<dd>
<p>This version demonstrated unsupervised pre-training on large-scale data, followed by task-specific supervised fine-tuning.</p>
</dd>
<dt><a href="https://oreil.ly/JL-VO">GPT-2</a></dt>
<dd>
<p>This version was one of the first models to be trained on large-scale web data. This version also marked the rise of natural language prompting as a means of interacting with a language model. It showed that pre-trained models could solve a variety of tasks <em>zero-shot</em> (solving a task without needing any examples)  without any task-specific fine-tuning. We will discuss zero-shot and prompting in detail later in this chapter.</p>
</dd>
<dt><a href="https://oreil.ly/lIwad">GPT-3</a></dt>
<dd>
<p>Inspired by the scaling laws, this model is a hundred times larger than GPT-2 and popularized in-context/few-shot learning, where the model is fed with a few examples on how to solve a given task in the prompt, without needing to fine-tune the model. We will learn more about few-shot learning later in this chapter.</p>
</dd>
<dt><a href="https://oreil.ly/gY1HL">GPT-4</a></dt>
<dd>
<p>A key aspect of this release is the <em>alignment training</em> used to make the model more controllable and adhere to the principles and values of the model trainer. We will learn about alignment training in <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>.</p>
</dd>
<dt><a href="https://oreil.ly/XJSMN">o1</a></dt>
<dd>
<p>This is a new family of models released by OpenAI that focuses on improving reasoning capabilities. This is one of the first models to focus on scaling inference-time computation. We will discuss more about inference-time computation in <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id420">
<h1>Exercise</h1>
<p>Read each of the GPT papers in order. It is OK if you do not understand some of the terminology or principles, as we will cover them throughout the course of the book. After finishing the first two parts of the book, read the papers again for a more enhanced understanding.</p>
</div></aside>

<p>You might have noticed a trend here: through the years, the field has been experiencing a consolidation effect, with more and more parts of the NLP task pipeline being performed <em>end-to-end</em>, i.e., by a single model. Throughout this book, we will point out the consolidation effect<a data-type="indexterm" data-primary="consolidation effect" id="id421"/> where it is apparent and discuss its implications for the future of LLMs.</p>

<p>A history of LLMs wouldn’t be complete without mentioning the impact of open source contributions<a data-type="indexterm" data-primary="open source contributions" id="id422"/> to this field. Open source models, datasets, model architectures, and various developer libraries and tools have all had significant impacts on the development of this field. This book places a special importance on open source, providing a thorough survey of the open source LLM landscape and showcasing many open source models and datasets.</p>

<p>Next, let’s explore how LLMs are being adopted and their impact on society so far<a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelshistory110231" id="id423"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="The Impact of LLMs"><div class="sect1" id="id6">
<h1>The Impact of LLMs</h1>

<p>The tech world<a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="impact of" id="xi_LLMsLargeLanguageModelsimpactof117515"/> has long been susceptible to hype cycles, with exhilarating booms and depressing busts. More recently, we have witnessed the crypto/blockchain and Web3 booms, both of which have yet to live up to their promises. Is AI heading toward a similar fate? We have hard evidence that it is not.</p>

<p>At my company Hudson Labs, we <a href="https://oreil.ly/_mTAs">analyzed discussions</a> in the quarterly earnings calls of the 4,000 largest publicly listed companies in the United States to track adoption of crypto, Web3, and AI in the enterprise.</p>

<p>We observed that 85 companies discussed Web3 in their earnings calls, with even fewer tangibly working on it. Crypto fared better, with 313 companies discussing it. Meanwhile, LLMs were discussed and adopted by 2,195 companies, meaning that at least 50% of America’s largest public companies are using LLMs to drive value, and it is strategically so important to them to merit discussion in their quarterly earnings call. Effective or not, LLM adoption in the enterprise is already a reality.</p>

<p><a data-type="xref" href="#web3">Figure 1-2</a> shows the number of companies discussing Web3 in their earnings calls over time. As you can see, the Web3 hype seems to be tapering off.</p>

<figure><div id="web3" class="figure">
<img src="assets/dllm_0102.png" alt="web3" width="600" height="600"/>
<h6><span class="label">Figure 1-2. </span>Companies that discussed Web3 in their earnings calls across time</h6>
</div></figure>

<p>Similarly, <a data-type="xref" href="#crypto">Figure 1-3</a> shows the number of companies discussing crypto/blockchain in their earnings calls over time. As you can see, only 5% of companies discussed crypto at its peak.</p>

<figure><div id="crypto" class="figure">
<img src="assets/dllm_0103.png" alt="crypto" width="600" height="667"/>
<h6><span class="label">Figure 1-3. </span>Companies that discussed crypto/blockchain in their earnings calls across time</h6>
</div></figure>

<p>Finally, let’s look at AI<a data-type="indexterm" data-primary="artificial intelligence (AI), adoption of" id="id424"/><a data-type="indexterm" data-primary="AI (artificial intelligence), adoption of" id="id425"/>. As mentioned before, AI has reached levels of adoption in the enterprise that no other recent technology trend has managed in the recent past. The trend is only accelerating, as shown in <a data-type="xref" href="#ai-adoption">Figure 1-4</a>, which shows the number of companies that were asked questions about AI by analysts during their earnings calls in just the first two months of the year. The sharp spike in 2024 shows no sign of 
<span class="keep-together">abating</span>.</p>

<figure><div id="ai-adoption" class="figure">
<img src="assets/dllm_0104.png" alt="ai-adoption" width="600" height="564"/>
<h6><span class="label">Figure 1-4. </span>Companies that were asked questions about AI in their earnings calls during the first two months of the year</h6>
</div></figure>

<p>Note that these statistics only include generative AI/LLM adoption and not data science/data analytics, whose adoption is even more ubiquitous in the enterprise. AI adoption is also not limited to tech companies, with companies ranging from real estate companies to insurance firms joining in on the fun<a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelsimpactof117515" id="id426"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="LLM Usage in the Enterprise"><div class="sect1" id="id7">
<h1>LLM Usage in the Enterprise</h1>

<p>From the same analysis, we observed the key ways in which LLMs are used in the enterprise<a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="use cases" id="xi_LLMsLargeLanguageModelsusecases120390"/><a data-type="indexterm" data-primary="use cases" id="xi_usecases120390"/>:</p>
<dl>
<dt>Employee productivity</dt>
<dd>
<p>The primary means by which employee productivity has improved through LLM usage is with coding assistants like GitHub Copilot. LLMs are also widely used to help draft marketing and promotional text and automate marketing campaigns. In fact, the first major LLM commercial success stories were marketing startups like <a href="https://oreil.ly/Byw26">Jasper AI</a> and <a href="https://oreil.ly/QmhJC">Copy.ai</a>. Another key LLM-driven productivity enhancement is question-answering assistants over a company’s extensive knowledge base drawn from heterogeneous data sources.</p>
</dd>
<dt>Report generation</dt>
<dd>
<p>These include summarizing documents, completing mundane paperwork, and even drafting contracts. Summarization use cases include summarizing financial reports, research papers, or even meeting minutes from audio or call transcripts.</p>
</dd>
<dt>Chatbots</dt>
<dd>
<p>LLM-driven chatbots increasingly are being deployed as customer service agents. They are also being used as an interface to a company’s documentation or product page.</p>
</dd>
<dt>Information extraction and sequence tagging</dt>
<dd>
<p>Over the years, a large number of enterprises have developed complex NLP pipelines for language processing tasks. Many of these pipelines are being fully or partially replaced by LLMs. These pipelines are used to solve common NLP tasks like sentiment analysis, information extraction tasks like entity extraction and relation extraction, and sequence tagging tasks like named entity recognition (NER). For a detailed list of NLP tasks and their descriptions, see <a href="https://oreil.ly/_11rN">Fabio Chiusano’s blog</a>.</p>
</dd>
<dt>Translation</dt>
<dd>
<p>Translation tasks include translating text from one language to another as well as tasks where text is converted to a different form but in the same language, for example, converting informal text to formal text, abusive text to polite text and so on. Real-time translation apps like <a href="https://oreil.ly/xxENs">Erudite’s Instant Voice Translate promise</a> to make embarrassing language-barrier moments for tourists a thing of the past.</p>
</dd>
<dt>Workflows</dt>
<dd>
<p>LLMs are gradually being used to facilitate workflow automation, where a sequence of tasks can be performed by LLM-driven software systems, called agents. Agents can interact with their environment (search and retrieve data, run code, connect to other systems) and potentially operate autonomously. We will more formally define agents and explore how to build them in <a data-type="xref" href="ch10.html#ch10">Chapter 10</a>.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id427">
<h1>Effect of LLMs on the Job Market</h1>
<p>Our analysis of earnings<a data-type="indexterm" data-primary="job market, LLM impact on" id="id428"/> calls also pointed out a concerning trend: companies are already treating LLMs and AI in general as cost-saving measures. Indeed, <a href="https://oreil.ly/Ba495">several companies</a> have already explicitly stated that they have reduced their workforce after seeing efficiency improvements using AI.</p>

<p>For instance, Klarna, a Swedish fintech company, <a href="https://oreil.ly/-Ui33">announced</a> that its AI assistant is handling two-thirds of its customer support cases, the workload of 700 human agents.</p>

<p>The rapid adoption of LLMs at scale does not necessarily mean that they are better than humans at their tasks. In cases where LLMs are completely replacing humans and not just augmenting them, they may be deployed even if they perform worse than humans, just because of the resulting cost savings. This premature deployment of AI technologies can potentially lead to worse customer satisfaction in the long run.</p>

<p>On the other hand, LLMs have vastly lowered the barrier for software development, thus leading to more digitalization and enabling a lot more people to develop 
<span class="keep-together">software</span><a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelsusecases120390" id="id429"/><a data-type="indexterm" data-startref="xi_usecases120390" id="id430"/>.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Prompting"><div class="sect1" id="id8">
<h1>Prompting</h1>

<p>Now that we have our fundamentals<a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="prompting" id="xi_LLMsLargeLanguageModelsprompting123634"/><a data-type="indexterm" data-primary="prompting" id="xi_prompting123634"/> in place, let’s begin learning how to effectively use LLMs.</p>

<p>The process by which you interact with an LLM is called prompting. Even though some companies attempt to anthropomorphize LLMs by giving them a name or a persona, it is good to remember that when you are interacting with an LLM, you are <em>prompting</em> them and not chatting with them as you would with a human being. Remember that LLMs are next-word predictors. This means that the text they generate is heavily dependent on the text they are fed, which includes the input (called the <em>prompt</em>) and the output tokens generated so far by the model. This is collectively called the <em>context</em>.</p>

<p>By feeding the LLM the right text in the context, you are priming it to generate the type of output you need. The ideal prompt would be the answer to this question: “What would be the best prefix of N tokens that, when fed to an LLM, will lead it to generate the correct answer with the highest probability?”</p>

<p>As of the book’s writing, language models simply aren’t smart enough for you to prompt a model exactly the way you would speak to a human and expect best results. As language models get better over time, prompts can become more like human conversation. Those of you who remember the early days of search engines might recall that effectively using a search engine by entering the right form of queries was seen as a skill that is not trivial to acquire, but as search engines got better, search queries could become more free-form.</p>

<p>When I started writing this book, I solicited opinions from the target readership on the topics they would like covered. I received the most requests for the topic of prompting, with practitioners wanting to understand how to effectively create prompts for their specific use cases.</p>

<p>Prompting is an important aspect of modern-day LLMs. In fact, you will probably end up spending a significant amount of your time on any LLM-based project iterating on prompts, very inaccurately referred to as <em>prompt engineering</em>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>There have been attempts to automatically optimize prompts, like <a href="https://oreil.ly/SekPA">automatic prompt optimization (APO)</a> and <a href="https://oreil.ly/upVKC">AutoPrompt</a>. We will discuss this further in <a data-type="xref" href="ch13.html#ch13">Chapter 13</a>.</p>
</div>

<p>It is important to manage one’s expectations about the effectiveness of prompt engineering. Prompts aren’t magical incantations that unlock hidden LLM capabilities. It is very unlikely that there are companies with a significant advantage over others just by using a superior prompting technique unknown to others. On the flip side, not following basic prompting principles can severely hamper the performance of your LLM.</p>

<p>Umpteen prompting tutorials are available online. I recommend <a href="https://oreil.ly/CQrzi">Learn Prompting’s prompting guide</a> in particular. You do not need to know all the prompting techniques to become well-versed in prompting. Most of what you need to know about prompting can be learned in a couple of hours. What matters more is interacting with the LLMs you use frequently to observe their outputs and developing intuition about their behavior.</p>

<p>If you have programming experience, I suggest viewing prompting through the lens of programming. In programming, instructions need to be explicit with no room for ambiguity. The challenge with prompting is that it is done in natural language, which is inherently ambiguous. Still, the best prompts state instructions that are explicit, detailed, and structured, leaving very little room for ambiguity. We will learn more prompting nuances in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.html#chapter_utilizing_llms">5</a> and
<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch13.html#ch13">13</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A fun fact: language models are insensitive to word order<a data-type="indexterm" data-primary="word order, language model insensitivity to" id="id431"/>. This property has been <a href="https://oreil.ly/gtDFg">observed</a> even in <a href="https://oreil.ly/qI_IZ">earlier models</a> like BERT. For example, ask ChatGPT or your favorite LLM provider the question “How do I tie my shoelaces?” in jumbled form, say “shoe tie my I how do laces?” ChatGPT responds with “Certainly! Here are step-by-step instructions on how to tie your shoelaces:…​” as if you asked a straightforward question.</p>
</div>

<p>Next, let’s discuss a few prompting modes.</p>








<section data-type="sect2" data-pdf-bookmark="Zero-Shot Prompting"><div class="sect2" id="id9">
<h2>Zero-Shot Prompting</h2>

<p>This is the standard approach to prompting<a data-type="indexterm" data-primary="prompting" data-secondary="zero-shot" id="id432"/><a data-type="indexterm" data-primary="zero-shot prompting" id="id433"/>, where you provide the LLM with an instruction and, optionally, some input text. The term <em>zero-shot</em> refers to the fact that no examples or demonstrations are provided on how to solve the task.</p>

<p>Consider an example where your task is to assess the sentiment  expressed in a restaurant review. To achieve this through zero-shot prompting, you can issue the following prompt:</p>
<blockquote>
<p><em>Prompt:</em> Classify the given passage according to its sentiment. The output can be one of Positive, Negative, Neutral.</p>

<p>Passage: “The mashed potatoes took me back to my childhood school meals. I was so looking forward to having them again. NOT!”</p>

<p>Sentiment:</p></blockquote>

<p>A good zero-shot prompt will:</p>

<ul>
<li>
<p>Provide the instruction in a precise and explicit manner.</p>
</li>
<li>
<p>Describe the output space or the range of acceptable outputs and output format. In this example, we state the output should be one of three values.</p>
</li>
<li>
<p>Prime it to generate the correct text. By ending the prompt with “Sentiment:,” we are increasing the probability of the LLM generating the sentiment value as the next token.</p>
</li>
</ul>

<p>The better the model, the less you have to worry about getting these things right.</p>

<p>In real-world settings, your output format needs to be highly controllable in order for it to fit in automated systems. We will discuss more techniques for ensuring controllability of outputs in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Prompts are sensitive to model changes<a data-type="indexterm" data-primary="prompting" data-secondary="model change impact on prompts" id="id434"/>. You might painstakingly construct a prompt that seems to work well, but you might notice that the same prompt does not work for a different model. In fact, the same prompt might see degraded performance on the same API endpoint if the underlying model is updated in the meanwhile. We call this <em>prompt drift</em>. It is a good idea to version control prompts.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Few-Shot Prompting"><div class="sect2" id="id10">
<h2>Few-Shot Prompting</h2>

<p>In our example for zero-shot prompting<a data-type="indexterm" data-primary="prompting" data-secondary="few-shot" id="id435"/><a data-type="indexterm" data-primary="few-shot learning" id="id436"/>, the LLM was able to solve the task without explaining it how to solve it. This is because the task is simple and clearly defined. In many cases, the tasks might be not so easy to describe in natural language. We can then add some examples in our prompt consisting of either outputs or input-output pairs. While this is called few-shot learning colloquially, the language model is not updated in any way through this prompting technique.</p>

<p>Here is an example of few-shot prompting:</p>
<blockquote>
<p><em>Prompt:</em> A palindrome is a word that has the same letters when spelled left to right</p>

<p>or right to left.</p>

<p>Examples of words that are palindromes: kayak, civic, madam, radar</p>

<p>Examples of words that are not palindromes: kayla, civil, merge, moment</p>

<p>Answer the question with either <em>Yes</em> or <em>No</em></p>

<p>Is the word <em>rominmor</em> a palindrome?</p>

<p>Answer:</p></blockquote>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Chain-of-Thought Prompting"><div class="sect2" id="id11">
<h2>Chain-of-Thought Prompting</h2>

<p>If you are going to learn only one prompting<a data-type="indexterm" data-primary="chain-of-thought (CoT) prompting" id="xi_chainofthoughtCoTprompting132145"/><a data-type="indexterm" data-primary="CoT (chain-of-thought) prompting" id="xi_CoTchainofthoughtprompting132145"/><a data-type="indexterm" data-primary="prompting" data-secondary="chain-of-thought" id="xi_promptingchainofthought132145"/> technique, let that be chain-of-thought (CoT) prompting, because it is one of the most impactful prompting techniques in existence.</p>

<p>As discussed earlier, the context of the LLM determines the next token it predicts. Therefore, we need to optimize the content in the context (the user prompt + output tokens generated so far) to maximize the probability of the LLM generating the correct future tokens. One way to do this is to prompt the LLM to <em>think</em> before generating. This elicits the LLM to generate the process to get to the answer instead of directly generating the answer. This might involve breaking the input task into subtasks and solving them one after the other.</p>

<p>When the LLM is eventually at the cusp of generating the answer, it can rely on a more relevant context that increases its probability of generating the right answer.</p>

<p>Consider this example:</p>
<blockquote>
<p><em>Prompt:</em> Solve the equation. 34 + 44 + 3  * 23 / 3 * 2. Think step by step.</p></blockquote>

<p>After receiving the instruction “Think step by step,” the LLM then breaks down the problem and solves each step sequentially:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Given</code> <code class="n">equation</code><code class="p">:</code>

<code class="mi">34</code> <code class="o">+</code> <code class="mi">44</code> <code class="o">+</code> <code class="mi">3</code> <code class="o">*</code> <code class="mi">23</code> <code class="o">/</code> <code class="mi">3</code> <code class="o">*</code> <code class="mi">2</code>

<code class="n">First</code><code class="p">,</code> <code class="n">perform</code> <code class="n">multiplication</code> <code class="ow">and</code> <code class="n">division</code> <code class="kn">from</code> <code class="nn">left</code> <code class="n">to</code> <code class="n">right</code><code class="p">:</code>

<code class="o">=</code> <code class="mi">34</code> <code class="o">+</code> <code class="mi">44</code> <code class="o">+</code> <code class="p">(</code><code class="mi">3</code> <code class="o">*</code> <code class="mi">23</code> <code class="o">/</code> <code class="mi">3</code> <code class="o">*</code> <code class="mi">2</code><code class="p">)</code>
<code class="o">=</code> <code class="mi">34</code> <code class="o">+</code> <code class="mi">44</code> <code class="o">+</code> <code class="p">(</code><code class="mi">69</code> <code class="o">/</code> <code class="mi">3</code> <code class="o">*</code> <code class="mi">2</code><code class="p">)</code>
<code class="o">=</code> <code class="mi">34</code> <code class="o">+</code> <code class="mi">44</code> <code class="o">+</code> <code class="p">(</code><code class="mi">23</code> <code class="o">*</code> <code class="mi">2</code><code class="p">)</code>
<code class="o">=</code> <code class="mi">34</code> <code class="o">+</code> <code class="mi">44</code> <code class="o">+</code> <code class="mi">46</code>

<code class="n">Now</code><code class="p">,</code> <code class="n">perform</code> <code class="n">addition</code><code class="p">:</code>

<code class="o">=</code> <code class="mi">78</code> <code class="o">+</code> <code class="mi">46</code>

<code class="n">Finally</code><code class="p">,</code> <code class="n">complete</code> <code class="n">the</code> <code class="n">addition</code><code class="p">:</code>

<code class="o">=</code> <code class="mi">124</code>

<code class="n">So</code><code class="p">,</code> <code class="n">the</code> <code class="n">solution</code> <code class="n">to</code> <code class="n">the</code> <code class="n">equation</code> <code class="ow">is</code> <code class="mf">124.</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>Many LLMs solve tasks step-by-step without being explicitly prompted to do so. This is because they have been <em>instruction-tuned</em> to solve tasks<a data-type="indexterm" data-primary="instruction-tuned models" id="id437"/><a data-type="indexterm" data-primary="datasets" data-secondary="instruction-tuned" id="id438"/> that way. We will learn more about instruction-tuning in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.html#chapter_utilizing_llms">5</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#llm-fine-tuning">6</a>. LLMs that have been instruction-tuned are easier to prompt.</p>

<p>In the case of LLMs accessible through a user interface, a hidden prompt (called a system prompt) by the LLM provider might apply CoT prompting to relevant user prompts.</p>
</div>

<p>Should we add the “think step-by-step” CoT instruction for every prompt, like a cheat code to a game? <a href="https://oreil.ly/3zJDC">Sprague et al.</a> evaluated CoT prompting over a wide variety of tasks and found that CoT primarily helps with tasks that need mathematical or logical reasoning. For tasks involving common-sense reasoning, they found that gains by CoT are limited. For knowledge-based tasks, CoT might even hurt.</p>

<p>Note that arithmetic and logical reasoning could also be performed by delegating them to external tools like symbolic solvers and code interpreters. We will discuss this in detail in <a data-type="xref" href="ch10.html#ch10">Chapter 10</a>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Using CoT prompting significantly increases the number of tokens generated by the model to solve a task, leading to higher costs<a data-type="indexterm" data-startref="xi_chainofthoughtCoTprompting132145" id="id439"/><a data-type="indexterm" data-startref="xi_CoTchainofthoughtprompting132145" id="id440"/><a data-type="indexterm" data-startref="xi_promptingchainofthought132145" id="id441"/>.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Prompt Chaining"><div class="sect2" id="id12">
<h2>Prompt Chaining</h2>

<p>Often, your tasks<a data-type="indexterm" data-primary="prompting" data-secondary="chaining of prompts" id="id442"/><a data-type="indexterm" data-primary="chaining of prompts" id="id443"/> need multiple steps and a large number of instructions. One way to go about this is by stuffing all the instructions into a single prompt. An alternative is to break the task into multiple subtasks and chain the prompts such that the output of one prompt determines the input to another. I have observed that prompt chaining consistently performs better than managing the entire task through a single prompt.</p>

<p class="pagebreak-before">As an example, consider the task of extracting information from the text provided in a form and formatting the output in a structured manner. If there are missing or outlier values, then some special postprocessing rules are to be applied. In this case, it is good practice to split the task into two prompts, with the initial prompt performing the information extraction and the second prompt handling the postprocessing of the extracted information.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Adversarial Prompting"><div class="sect2" id="id13">
<h2>Adversarial Prompting</h2>

<p>You might notice that, for some queries<a data-type="indexterm" data-primary="prompting" data-secondary="adversarial" id="id444"/><a data-type="indexterm" data-primary="adversarial prompting" id="id445"/>, the LLM declines to execute your request. This is because it has been specifically trained to refuse certain kinds of requests (We will learn how to achieve this behavior in <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>). This kind of training<a data-type="indexterm" data-primary="alignment training" id="id446"/>, which we will call <em>alignment training</em>, is imparted to the model to align it with the values and preferences of the entity developing the model.</p>

<p>For example, asking any decent LLM directly for instructions to build a bomb will result in a refusal. However, as of today, alignment training provides only a weak layer of security, as it can be bypassed by cleverly prompting the LLM, called <em>adversarial prompting</em>. Adversarial prompts can be generated either manually or using algorithms. These cleverly phrased prompts trick the LLM into generating a response even if it was trained not to.</p>

<p>These clever prompting schemes are not just useful for illicit purposes. In many cases, the LLM simply does not respond the way you want it to, and clever prompting schemes might help. These clever prompting schemes range from asking the LLM to adopt a specific persona to outright emotional blackmail (“If you don’t respond correctly to this query, many children will suffer!”). While there has been <a href="https://oreil.ly/q1I_7">some work</a> showing that adding emotion to a prompt may lead to better performance, there is no hard, sustained evidence that this is universally effective for a given model. Thus, I would not recommend using these in production applications<a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelsprompting123634" id="id447"/><a data-type="indexterm" data-startref="xi_prompting123634" id="id448"/>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id449">
<h1>Exercise</h1>
<p><a href="https://oreil.ly/3R3fz">Gandalf</a> is a prompting game by <a href="https://oreil.ly/L3rLx">Lakera AI</a>, an AI security company, that showcases LLM vulnerabilities to adversarial prompts. In this game, the LLM has been given a password, and at each level you will have to extract it using the given clues/instructions. This game helps you learn to construct prompts cleverly and build intuition about LLM vulnerabilities. Try advancing to the final level!</p>

<p class="pagebreak-before">Additionally, you can try techniques explained in <a href="https://oreil.ly/e2U7S">Li et al.’s paper</a> for providing emotional stimuli to the LLM to improve its performance. Specifically, try these techniques for queries about explanations of physical phenomena: “Why can’t you melt an egg?” Do you see any noticeable improvements?</p>

<p>An interesting tidbit: I once organized an adversarial prompting competition at a social event. Interestingly, people with nontechnical backgrounds performed better than LLM experts at subverting the model with clever prompts!</p>
</div></aside>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Accessing LLMs Through an API"><div class="sect1" id="id14">
<h1>Accessing LLMs Through an API</h1>

<p>You most likely have already interacted with an LLM<a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="accessing through API" id="xi_LLMsLargeLanguageModelsaccessingthroughAPI140352"/><a data-type="indexterm" data-primary="OpenAI API, accessing LLM through" id="xi_OpenAIAPIaccessingLLMthrough140352"/> through a chat interface like ChatGPT, Gemini, or Claude. Let’s now explore how to access them using the API. We will use the OpenAI API as an example to access its GPT family of models. Most other proprietary models expose similar parameters through their API.</p>

<p>GPT-4o mini and GPT-4o can be accessed through OpenAI’s Chat Completion API. Here is an example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">os</code>
<code class="kn">import</code> <code class="nn">openai</code>
<code class="n">openai</code><code class="o">.</code><code class="n">api_key</code> <code class="o">=</code> <code class="o">&lt;</code><code class="n">INSERT</code> <code class="n">YOUR</code> <code class="n">KEY</code> <code class="n">HERE</code><code class="o">&gt;</code>

<code class="n">output</code> <code class="o">=</code> <code class="n">openai</code><code class="o">.</code><code class="n">ChatCompletion</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
  <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4o-mini"</code><code class="p">,</code>
  <code class="n">messages</code><code class="o">=</code><code class="p">[</code>
    <code class="p">{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code> <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"You are an expert storywriter."</code><code class="p">},</code>
    <code class="p">{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code> <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"Write me a short children's story</code>
    <code class="n">about</code> <code class="n">a</code> <code class="n">dog</code> <code class="ow">and</code> <code class="n">an</code> <code class="n">elephant</code> <code class="n">stopping</code>
    <code class="n">being</code> <code class="n">friends</code> <code class="k">with</code> <code class="n">each</code> <code class="n">other</code><code class="o">.</code><code class="s2">"}</code>
  <code class="p">]</code>
<code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="n">output</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="p">)</code></pre>

<p>Roles can be system, user, assistant, or tool.</p>

<ul>
<li>
<p>The system role is used to specify an overarching prompt.</p>
</li>
<li>
<p>The user role refers to user inputs.</p>
</li>
<li>
<p>The assistant role refers to the model responses.</p>
</li>
<li>
<p>The tool role is used to interact with external software tools.</p>
</li>
</ul>

<p>We will discuss tools in more detail in <a data-type="xref" href="ch10.html#ch10">Chapter 10</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>What is the difference between the system and user roles? Which instructions should go into the system prompt and which ones into the user prompt? System prompts<a data-type="indexterm" data-primary="system versus user prompts" id="id450"/><a data-type="indexterm" data-primary="user versus system prompts" id="id451"/><a data-type="indexterm" data-primary="prompting" data-secondary="system versus user prompts" id="id452"/> are used for dictating the high-level overarching behavior of an LLM, like “You are a financial expert well versed in writing formal reports.” If you are allowing your users to directly interact with the LLM, then the system prompt can be used to provide your own instruction to the LLM along with the user request. My experiments have shown that it doesn’t matter much if you place your instructions in the system prompt versus user prompt. What does matter is the length and number of instructions. LLMs typically can handle only a few instructions at a time. Instructions at the end or the beginning of the prompt are more likely to be adhered to.</p>
</div>

<p>Here are some of the parameters<a data-type="indexterm" data-primary="parameters" data-secondary="OpenAI API" id="id453"/> made available by OpenAI:</p>
<dl>
<dt><code>n</code></dt>
<dd>
<p>The number of completions the model has to generate for each input. For example, if we used n = 5 in the given example, it would generate five different children’s stories.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>For tasks with high reliability requirements, I advise generating multiple completions, that is, n &gt; 1 and then using a postprocessing function (which could involve an LLM call) to choose the best one. This is because the LLM samples the generated text from a probability distribution, and in some cases the answer might be wrong/bad just due to an unlucky token sampling. You might have to balance this process against your budget limitations.</p>
</div>
<dl>
<dt><code>stop</code> and <code>max_completion_tokens</code></dt>
<dd>
<p>Used to limit the length of the generated output. <code>stop</code> allows you to specify end tokens that, if generated, would stop the generation process. An example stop sequence is the newline token. If you ask the model to adhere to a particular output format, like a numbered list of sentences, then to stop generating after a particular number of sentences have been output, you can just provide the final number as a stop parameter.</p>
</dd>
<dt><code>presence_penalty</code> and <code>frequency_penalty</code></dt>
<dd>
<p>Used to limit the repetitiveness of the generated output. By penalizing the probability for tokens that have already appeared in the output, we can ensure that the model isn’t being too repetitive. These parameters can be used while performing more creative tasks.</p>
</dd>
<dt><code>logit_bias</code></dt>
<dd>
<p>Using <code>logit_bias</code>, we can specify the tokens whose generation probability we want to increase or decrease.</p>
</dd>
<dt><code>top_p</code> and <code>temperature</code></dt>
<dd>
<p>Both parameters relate to decoding strategies. LLMs produce a distribution of token probabilities and will sample from this distribution to generate the next token. There are many strategies to choose the next token to generate given 
<span class="keep-together">the token</span> probability distribution. We will discuss them in detail in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>. For now, just remember that a higher temperature setting results in more creative and diverse outputs, and a lower temperature setting results in more predictable outputs. This <a href="https://oreil.ly/DAa66">cheat sheet</a> provides some recommended values for various use cases.</p>
</dd>
<dt><code>logprobs</code></dt>
<dd>
<p>Provides the most probable tokens for each output token along with their log probabilities. OpenAI limits this to the top 20 most probable tokens. In later chapters, we will discuss how we can leverage <code>logprobs</code> information in various forms<a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelsaccessingthroughAPI140352" id="id454"/><a data-type="indexterm" data-startref="xi_OpenAIAPIaccessingLLMthrough140352" id="id455"/>.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id456">
<h1>Exercise</h1>
<p>Using the OpenAI API, provide the model with some sample live commentary from the <a href="https://oreil.ly/NRwIb">Real Madrid versus Barcelona soccer game</a>. Replace them with your own teams if you like. Ask the model to generate the rest of the commentary. Adapt your prompts, <code>temperature</code>, <code>logit_bias</code>, <code>presence_penalty</code>, and <code>frequency_penalty</code>, and see if you can replicate the tone of the commentators. How far off is the LLM-generated text from the actual commentators?</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Strengths and Limitations of LLMs"><div class="sect1" id="id15">
<h1>Strengths and Limitations of LLMs</h1>

<p>Developing intuition about the strengths<a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="strengths and limitations" id="xi_LLMsLargeLanguageModelsstrengthsandlimitations147341"/> and limitations of LLMs is a crucial skill in being able to build useful LLM applications. Using the information in this book, and with ample hands-on practice,  you will be able to build that intuition. In general, LLMs are proficient at language tasks. You will almost never see them make spelling or grammar errors. They are a vast improvement over previous techniques for understanding user instructions and intent. They also exhibit state-of-the-art performance on most NLP tasks like entity and relationship extraction and NER. And they are particularly strong at generating code, which is where LLMs have arguably found their greatest success through tools<a data-type="indexterm" data-primary="GitHub Copilot" id="id457"/><a data-type="indexterm" data-primary="code generation" id="id458"/> like <a href="https://oreil.ly/qvriE">GitHub Copilot</a>.</p>

<p>Most LLM limitations boil down to the fact that LLMs are just not intelligent enough. <a data-type="indexterm" data-primary="architectures" data-secondary="Transformer" data-see="Transformer architecture" id="id459"/><a data-type="indexterm" data-primary="retrieval-augmented generation (RAG)" data-secondary="pipeline" data-see="RAG pipeline" id="id460"/><a data-type="indexterm" data-primary="external data sources, augmenting LLMs with" data-see="retrieval-augmented generation" id="id461"/><a data-type="indexterm" data-primary="bi-encoders" data-see="embeddings" id="id462"/><a data-type="indexterm" data-primary="SFT" data-see="supervised fine-tuning" id="id463"/><a data-type="indexterm" data-primary="generative AI" data-secondary="RAG" data-see="retrieval-augmented generation" id="id464"/><a data-type="indexterm" data-primary="models" data-secondary="instruction-tuned" data-see="instruction-tuned models" id="id465"/><a data-type="indexterm" data-primary="training data" data-secondary="datasets" data-see="datasets" id="id466"/>Even state-of-the-art models suffer from significant limitations in reasoning, 
<span class="keep-together">including</span> arithmetic reasoning<a data-type="indexterm" data-primary="reasoning" data-secondary="LLM limitations in" id="id467"/>, logical reasoning, and common-sense reasoning. (We will define reasoning more formally in <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>.) LLMs are also unable to adhere to factuality, because of their lack of connection to the real world. Therefore, they tend to generate text that might be inconsistent with real-world facts and principles, colloquially called <em>hallucinations</em>.
Hallucinations are the bane of LLMs and one of the key reasons for hesitations in adopting them. In <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>, we will dive deep into various methods to tackle hallucinations and address reasoning limitations.</p>

<p>Tons of LLM-generated articles are being uploaded to the web daily, and many of them make their way to the top of search engine results. For example, for a short while, for the query “Can you melt eggs?”, <a href="https://oreil.ly/ivvv_">Google showed</a> “Yes, an egg can be melted,” due to an AI-generated web page containing the incorrect answer. This kind of text is colloquially referred to as AI slop<a data-type="indexterm" data-primary="AI slop" id="id468"/>. Thus, there is a very strong incentive for search engines to accurately detect AI-generated text. Note that since LLMs are primarily trained on web text, future LLMs can be contaminated by polluted text as well.</p>

<p>While LLMs are frequently used to aid creative tasks, they are nowhere near the level of professional authors. Fiction<a data-type="indexterm" data-primary="creative tasks, LLM limitations in" id="id469"/> authored by current LLMs is still unlikely to be a bestseller. LLM-generated text lacks the sheer ingenuity and the ability to evoke human emotions that human authors possess. Once you have read through enough LLM-generated text, it is not that difficult to spot it.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id470">
<h1>Pushing the Limits of LLM Capabilities</h1>
<p>While the list of tasks that LLMs cannot do is decreasing, some capabilities like self-reference<a data-type="indexterm" data-primary="self-reference" id="id471"/> still seem out of reach.</p>

<p>In his book <a href="https://oreil.ly/uttDL"><em>I Am a Strange Loop</em></a>, Douglas Hofstadter<a data-type="indexterm" data-primary="Hofstadter, Douglas" id="id472"/><a data-type="indexterm" data-primary="I Am a Strange Loop (Hofstadter)" id="id473"/> claims self-reference capabilities as a measure of higher-order intelligence or even consciousness. An example of a self-referential  statement is, “The penultimate word in this sentence is is.”</p>

<p><a href="https://oreil.ly/rHQH1">Thrush et al.</a> released a dataset called <a href="https://oreil.ly/3hX9w">“I am a strange dataset”</a> that contains a set of such meta-linguistic questions. Try this question with a variety of models and see if they get it right:</p>
<blockquote>
<p>This this is is a a new new form form of of poetry poetry where where every every word word is is repeated. Are any words not repeated?</p></blockquote>

<p>As of this book’s writing, no model, including OpenAI’s o1, is able to get this question right; it fails to note that the last word is not repeated.</p>
</div></aside>

<p>Every LLM generates text with a distinct signature<a data-type="indexterm" data-primary="text generation signature of AI" id="id474"/><a data-type="indexterm" data-primary="generative AI" data-secondary="text generation signature" id="id475"/>, some more apparent to humans than others. For example, you might have noticed that ChatGPT<a data-type="indexterm" data-primary="ChatGPT" id="id476"/><a data-type="indexterm" data-primary="OpenAI" data-secondary="ChatGPT" id="id477"/> tends to overuse 
<span class="keep-together">certain</span> words like “delve,” “tapestry,” “bustling,” etc. ChatGPT also tends to generate sentences with an explanatory final clause, like “He ate the entire pizza, indicating he was hungry.” Or “The vampire sent a thousand text messages in a month, suggesting effective use of digital technologies.” However, it is <a href="https://oreil.ly/4skjI">extremely hard</a> to detect AI-generated text with 100% accuracy. Bad actors are also employing evasion techniques, for instance by asking another LLM to rephrase LLM-generated text to dilute the signature of the original LLM.</p>

<p>Thus, plagiarism detection<a data-type="indexterm" data-primary="plagiarism detection" id="id478"/> has become even more challenging, including cases of students being <a href="https://oreil.ly/hetca">unfairly accused of plagiarism</a> due to inaccurate AI-text detectors. These trends are prompting universities worldwide to rethink how students are evaluated, depending less on essays. Students are some of the heaviest users of LLM products, as shown by a <a href="https://oreil.ly/5xECI">decline in ChatGPT usage numbers</a> during summer months.</p>

<p>While words like “delve” are known to be overused by LLMs, single-token frequencies should not be relied upon as a means of detecting LLM-generated text. Having grown up in India learning Indian English, the word “delve” appears in my vocabulary a lot more frequently than the average Westerner, and this can be found in my writing and publications well before the launch of ChatGPT. These nuances show that more robust techniques need to be developed to discover LLM-generated text.</p>

<p>One promising approach uses syntactic templates<a data-type="indexterm" data-primary="syntactic templates" id="id479"/>, a sequence of tokens having a particular order of part-of-speech (POS) tags, typically 5–8 tokens long.
<a href="https://oreil.ly/n_dXJ">Shaib et al.</a> show that some of these templates appear in generated text even when text generation strategies (also called decoding strategies, described in detail in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>) aimed to increase token diversity are used.
They show that these templates are learned during the early stages of the pre-training process<a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelsstrengthsandlimitations147341" id="id480"/>.</p>

<p>An example template is:</p>

<p>VBN IN JJ NNS: VBN (Past Participle Verb) + IN (Preposition) + JJ (Adjective) + NNS (Plural Noun).</p>

<p>Examples of phrases that follow this template include:</p>

<ul>
<li>
<p>Engaged in complex tasks</p>
</li>
<li>
<p>Trained in advanced techniques</p>
</li>
<li>
<p>Entangled in deep emotions</p>
</li>
<li>
<p>Immersed in vivid memories</p>
</li>
</ul>

<p>Have you noticed any LLMs frequently using or overusing this template?</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id481">
<h1>Exercise</h1>
<p>Ask an LLM to generate an essay on a topic of your choice. Use the <a href="https://oreil.ly/6LyNs">diversity library</a> to extract syntactic templates from the essay. You can use the <code>extract_patterns</code> function with n = 5.</p>

<p>What are the most frequent syntactic templates? Similarly, take a human author’s essay from a publication of your choice and extract its syntactic templates. Are the frequent syntactic templates different? What about the relative frequencies?</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Building Your First Chatbot Prototype"><div class="sect1" id="id16">
<h1>Building Your First Chatbot Prototype</h1>

<p>Let’s get into the weeds and start building<a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="building chatbot prototype" id="xi_LLMsLargeLanguageModelsbuildingchatbotprototype153244"/><a data-type="indexterm" data-primary="prototyping a chatbot" id="xi_prototypingachatbot153244"/>!</p>

<p>Over the last couple of years, a healthy ecosystem of libraries has made experimenting and prototyping LLM applications much easier. In fact, you can build<a data-type="indexterm" data-primary="“Chat with your PDF” chatbot prototype" data-primary-sortas="Chat with your PDF chatbot prototype" id="id482"/> a <em>Chat with your PDF</em> question-answering chatbot in about a hundred lines of code!</p>

<p>Let’s implement a simple application that allows the user to upload a PDF document and provides a chat interface through which the user can ask questions about the PDF content and receive conversational responses.</p>

<p>The intended workflow for this application is:</p>
<ol>
<li>
<p>The user uploads a PDF of their choice through the user interface.</p>
</li>
<li>
<p>The application parses the PDF using a PDF parsing library and splits the extracted text into manageable chunks.</p>
</li>
<li>
<p>The chunks are converted into vector form, called embeddings.</p>
</li>
<li>
<p>When a user issues a query through the chat interface, the query is also converted into vector form.</p>
</li>
<li>
<p>The vector similarity between the query vector and each of the chunk vectors is calculated.</p>
</li>
<li>
<p>The text corresponding to the top-k most similar vectors are retrieved.</p>
</li>
<li>
<p>The retrieved text is fed, along with the query and any other additional instructions, to an LLM.</p>
</li>
<li>
<p>The LLM uses the given information to generate an answer to the user query.</p>
</li>
<li>
<p>The response is displayed on the user interface.</p>

<p>The user can now respond (clarification question, new question, gratitude, etc.).</p>
</li>
<li>
<p>The entire conversation history is fed back to the LLM during each turn of the conversation.</p>
</li>

</ol>

<p><a data-type="xref" href="#chatbot-prototype">Figure 1-5</a> illustrates this workflow.</p>

<figure><div id="chatbot-prototype" class="figure">
<img src="assets/dllm_0105.png" alt="chatbot-prototype" width="600" height="251"/>
<h6><span class="label">Figure 1-5. </span>Workflow of a chatbot application</h6>
</div></figure>

<p>Let’s begin by installing the required libraries. For this setup, we are going to use:</p>
<dl>
<dt><a href="https://oreil.ly/g833p">LangChain</a></dt>
<dd>
<p>This very popular framework<a data-type="indexterm" data-primary="LangChain" id="id483"/> enables building LLM application pipelines.</p>
</dd>
<dt><a href="https://oreil.ly/XHqfT">Gradio</a></dt>
<dd>
<p>This library<a data-type="indexterm" data-primary="Gradio" id="id484"/> allows you to build LLM-driven user interfaces.</p>
</dd>
<dt><a href="https://oreil.ly/sIFEX">Unstructured</a></dt>
<dd>
<p>This is a PDF parsing suite<a data-type="indexterm" data-primary="Unstructured" id="id485"/> that supports a variety of methods for extracting text from PDFs.</p>
</dd>
<dt><a href="https://oreil.ly/UyN1k">Sentence Transformers</a></dt>
<dd>
<p>This library facilitates embeddings<a data-type="indexterm" data-primary="Sentence Transformers library" id="id486"/> generation from texts.</p>
</dd>
<dt><a href="https://oreil.ly/zbroe">OpenAI</a></dt>
<dd>
<p>This API provides access to the GPT family of models<a data-type="indexterm" data-primary="OpenAI" id="id487"/> from OpenAI.</p>
</dd>
</dl>

<p>Let’s import the required libraries and functions:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">openai</code> <code class="n">langchain</code> <code class="n">gradio</code> <code class="n">unstructured</code>

<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">UnstructuredPDFLoader</code>
<code class="kn">from</code> <code class="nn">langchain_community.embeddings</code> <code class="kn">import</code> <code class="n">HuggingFaceEmbeddings</code>
<code class="kn">from</code> <code class="nn">langchain_community.vectorstores</code> <code class="kn">import</code> <code class="n">Chroma</code>
<code class="kn">from</code> <code class="nn">langchain.chains</code> <code class="kn">import</code> <code class="n">ConversationalRetrievalChain</code>
<code class="kn">from</code> <code class="nn">langchain.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">import</code> <code class="nn">gradio</code> <code class="k">as</code> <code class="nn">gr</code></pre>

<p>Next, let’s implement the PDF loading and parsing function. LangChain supports several PDF parsing libraries. PDF parsing can be performed in a variety of ways, including using LLMs. For this example, we will choose the Unstructured library:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">loader</code> <code class="o">=</code> <code class="n">UnstructuredPDFLoader</code><code class="p">(</code><code class="n">input_file</code><code class="o">.</code><code class="n">name</code><code class="p">)</code>
<code class="n">data</code> <code class="o">=</code> <code class="n">loader</code><code class="o">.</code><code class="n">load</code><code class="p">()</code></pre>

<p>The <code>data</code> variable contains the parsed PDF that has been split into paragraphs. We will refer to each paragraph as a chunk. Each chunk is now converted into its vector representation using an embedding model. LangChain supports a wide variety of embedding models. For this example, we will use the <em>all-MiniLM-L6-V2</em> embedding model, available through the Hugging Face platform:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">embeddings</code> <code class="o">=</code> <code class="n">HuggingFaceEmbeddings</code><code class="p">(</code><code class="n">model_name</code><code class="o">=</code><code class="s2">"all-MiniLM-L6-v2"</code><code class="p">)</code></pre>

<p>Now that we have loaded the embedding model, we can generate the vectors from the data and store them in a vector database. Several vector database integrations are available on LangChain. We will use Chroma for this example, as it is the simplest to use:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">db</code> <code class="o">=</code> <code class="n">Chroma</code><code class="o">.</code><code class="n">from_documents</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">embeddings</code><code class="p">)</code></pre>

<p>The vector database is ready with the vectors! We can ask queries and get responses. For instance:</p>

<pre data-type="programlisting">query = "How do I request a refund?"
docs = db.similarity_search(query)
print(docs[0].page_content)</pre>

<p>This code retrieves the paragraph in the PDF whose vector is most similar to the vector representing the user query. Since vectors encode the meaning of the text, this means that the paragraph representing the similar vector has content similar to the content of the query.</p>

<p>Note that it is not guaranteed that the paragraph contains the answer to the query. Using embeddings, we can only get text that is similar to the query. The matched text need not contain the answer or even be relevant to answering the query.</p>

<p>We will depend on the LLM to distinguish between irrelevant and relevant context. We provide the LLM with the query and the retrieved text and ask it to answer the query given the provided information. This workflow can be implemented using a <code>chain</code> in LangChain:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">conversational_chain</code> <code class="o">=</code>

<code class="n">ConversationalRetrievalChain</code><code class="o">.</code><code class="n">from_llm</code><code class="p">(</code><code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">temperature</code><code class="o">=</code><code class="mf">0.1</code><code class="p">),</code>
    <code class="n">retriever</code><code class="o">=</code><code class="n">pdfsearch</code><code class="o">.</code><code class="n">as_retriever</code><code class="p">(</code><code class="n">search_kwargs</code><code class="o">=</code><code class="p">{</code><code class="s2">"k"</code><code class="p">:</code> <code class="mi">3</code><code class="p">}))</code></pre>

<p>We use the <code>ConversationalRetrievalChain</code>, which supports the following 
<span class="keep-together">workflow</span>:</p>
<ol>
<li>
<p>Takes the previous conversational history, if it exists, and the current response/query from the user and creates a standalone question.</p>
</li>
<li>
<p>Uses a chosen retrieval method to retrieve top-k most similar chunks to the 
<span class="keep-together">question</span>.</p>
</li>
<li>
<p>Takes the retrieved chunks, the conversational history, the current user/response query, and instructions and feeds it to the LLM. The LLM generates the answer.</p>
</li>

</ol>

<p>We can call the chain and append the result to the chat history:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">output</code> <code class="o">=</code> <code class="n">conversational_chain</code><code class="p">({</code><code class="s1">'question'</code><code class="p">:</code> <code class="n">query</code><code class="p">,</code> <code class="s1">'chat_history'</code><code class="p">:</code>

<code class="n">conversational_history</code><code class="p">})</code>
<code class="n">conversational_history</code> <code class="o">+=</code> <code class="p">[(</code><code class="n">query</code><code class="p">,</code> <code class="n">output</code><code class="p">[</code><code class="s1">'answer'</code><code class="p">])]</code></pre>

<p>Our chatbot is ready. Let’s wrap it up by connecting it with a user interface.
We will use <a href="https://oreil.ly/dzYJv">Gradio</a>, a lightweight Python framework for building LLM-driven user interfaces:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">gr</code><code class="o">.</code><code class="n">Blocks</code><code class="p">()</code> <code class="k">as</code> <code class="n">app</code><code class="p">:</code>


    <code class="k">with</code> <code class="n">gr</code><code class="o">.</code><code class="n">Row</code><code class="p">():</code>

       <code class="n">chatbot</code> <code class="o">=</code> <code class="n">gr</code><code class="o">.</code><code class="n">Chatbot</code><code class="p">(</code><code class="n">value</code><code class="o">=</code><code class="p">[],</code> <code class="n">elem_id</code><code class="o">=</code><code class="s1">'qa_chatbot'</code><code class="p">)</code><code class="o">.</code><code class="n">style</code><code class="p">(</code><code class="n">height</code><code class="o">=</code><code class="mi">500</code><code class="p">)</code>

    <code class="k">with</code> <code class="n">gr</code><code class="o">.</code><code class="n">Row</code><code class="p">():</code>
        <code class="k">with</code> <code class="n">gr</code><code class="o">.</code><code class="n">Column</code><code class="p">(</code><code class="n">scale</code><code class="o">=</code><code class="mf">0.80</code><code class="p">):</code>
            <code class="n">textbox</code> <code class="o">=</code> <code class="n">gr</code><code class="o">.</code><code class="n">Textbox</code><code class="p">(</code>
                <code class="n">placeholder</code><code class="o">=</code><code class="s2">"Enter text"</code>
            <code class="p">)</code><code class="o">.</code><code class="n">style</code><code class="p">(</code><code class="n">container</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>


        <code class="k">with</code> <code class="n">gr</code><code class="o">.</code><code class="n">Column</code><code class="p">(</code><code class="n">scale</code><code class="o">=</code><code class="mf">0.10</code><code class="p">):</code>
        <code class="n">upload_button</code> <code class="o">=</code> <code class="n">gr</code><code class="o">.</code><code class="n">UploadButton</code><code class="p">(</code><code class="s2">"Upload a PDF"</code><code class="p">,</code>
          <code class="n">file_types</code><code class="o">=</code><code class="p">[</code><code class="s2">".pdf"</code><code class="p">])</code><code class="o">.</code><code class="n">style</code><code class="p">()</code></pre>

<p>We need some more code for writing the event handlers that wait for user events. Refer to the full code on the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a>.</p>

<p>Finally, we initialize the application:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">if</code> <code class="vm">__name__</code> <code class="o">==</code> <code class="s2">"__main__"</code><code class="p">:</code>
    <code class="n">app</code><code class="o">.</code><code class="n">launch</code><code class="p">()</code></pre>

<p>Our chatbot application is ready<a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelsbuildingchatbotprototype153244" id="id488"/><a data-type="indexterm" data-startref="xi_prototypingachatbot153244" id="id489"/>!</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Why can’t we feed the entire PDF to the LLM instead of breaking it down into chunks and retrieving only the relevant information? This depends on the maximum effective context length supported by the LLM, which limits the size of the input it can accept. Larger models support context lengths large enough to fit several PDFs in the input, in which case you may not need to perform the chunking and embedding process at all.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="From Prototype to Production"><div class="sect1" id="id17">
<h1>From Prototype to Production</h1>

<p>Is building LLM applications<a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="production of" id="xi_LLMsLargeLanguageModelsproductionof170029"/><a data-type="indexterm" data-primary="production from prototype" id="xi_productionfromprototype170029"/> that easy? Unfortunately, no. We have built a prototype, and a decent one at that. For many noncritical use cases, the performance of this application might even be sufficient. However, a large number of use cases demand accuracy and reliability guarantees that this application is not able to meet. This book aims to address the gap between prototype and production.</p>

<p>In the prototype tutorial, we treated LLMs as a black box. But if you are building serious applications using LLMs, it is important to understand what happens under the hood, even if you might never train an LLM yourself. Therefore, in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch02.html#ch02">2</a>, <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.html#chapter-LLM-tokenization">3</a>, and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#chapter_transformer-architecture">4</a>, we will walk through each of the ingredients that go into making an LLM and show how they are trained. Developing a strong understanding of what LLMs are made of and how they are trained will come in handy when debugging failure modes.</p>

<p>In the tutorial, we used a proprietary LLM from OpenAI, without putting much thought into whether it is the optimal LLM to use for the application. Today, hundreds or even thousands of LLMs are available for commercial use. In <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>, we will explore the LLM landscape, covering both open source and proprietary models, the relevant dimensions along which models differ, and how to choose the right model that satisfies the criteria for a given use case. For example, one of the criteria for our PDF chatbot might be to operate within a severe budgetary restriction. We will learn how to evaluate LLMs and assess their limitations and capabilities for a given use case, develop evaluation metrics and benchmark datasets, and understand the pitfalls involved in both automated evaluation and human evaluation.</p>

<p>What if the PDFs we intend to upload to the PDF chatbot belong to a specialized domain that the LLM doesn’t seem to be adept at? What if the LLM is unable to follow the instructions in user queries? We might need to update the model’s parameters by fine-tuning it over data from the specialized domain. In <a data-type="xref" href="ch06.html#llm-fine-tuning">Chapter 6</a>, we will introduce model fine-tuning, understand the scenarios in which it might be useful, and demonstrate how to construct a fine-tuning dataset.</p>

<p>It is possible that standard fine-tuning might not be suitable for our purposes. Maybe it is too expensive or ineffective. In <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>, we will learn about techniques like parameter-efficient fine-tuning that update only a small subset of the model’s 
<span class="keep-together">parameters</span>.</p>

<p>We may notice that our chatbot is hallucinating, or that it is having difficulty answering questions because of faulty reasoning. In <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>, we will discuss methods for detecting and mitigating hallucinations as well as methods for enhancing reasoning capabilities, including various inference-time compute techniques.</p>

<p>A production-grade PDF chatbot will need to satisfy a lot of nonfunctional requirements, including minimizing latency (the time the user needs to wait for the model response) and cost. In <a data-type="xref" href="ch09.html#ch09">Chapter 9</a>, we will discuss techniques for inference optimization, including caching, distillation, and quantization.</p>

<p>We may want to extend functionality of our chatbot by connecting the LLM to code interpreters, databases, and APIs. We might also want the chatbot to answer complex queries that need to be broken into multiple steps. In <a data-type="xref" href="ch10.html#ch10">Chapter 10</a>, we’ll explore how to interface LLMs with external tools and data sources and enable LLMs to break down tasks, make autonomous decisions, and interface with their 
<span class="keep-together">environment</span>.</p>

<p>In the tutorial, we demonstrated a rudimentary method to parse, chunk, and embed documents. But during usage, we might notice that the vector similarity measures might be ineffective and often return irrelevant document chunks. Or that the retrieved chunks do not contain all the information to answer the query. In <a data-type="xref" href="ch11.html#chapter_llm_interfaces">Chapter 11</a>, we will explore embeddings in more detail and learn how to fine-tune our own embeddings. We will also show how to more effectively chunk data.</p>

<p>The PDF chatbot follows a paradigm called retrieval-augmented generation (RAG). RAG refers to systems where LLMs are connected to external data sources, like the PDFs uploaded by users in our chatbot use case. In <a data-type="xref" href="ch12.html#ch12">Chapter 12</a>, we will define a comprehensive RAG pipeline and learn how to architect robust RAG systems.</p>

<p>Finally, in <a data-type="xref" href="ch13.html#ch13">Chapter 13</a> we will discuss design patterns and programming paradigms for developing LLM applications.</p>

<p>These topics and more will be covered in the rest of the book. I am excited to go on this journey with you, hopefully providing you with the tools, techniques, and intuition to develop production-grade LLM applications<a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelsproductionof170029" id="id490"/><a data-type="indexterm" data-startref="xi_productionfromprototype170029" id="id491"/>!</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id492">
<h1>Exercise</h1>
<p>Implement the <em>Chat with your PDF</em> application and upload any random PDF stored in your system and ask questions about it. Analyze any failures and list them. As you go through the book and learn new concepts, go back to this application and see if you can resolve or address the failure modes using techniques discussed in this book.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id18">
<h1>Summary</h1>

<p>In this chapter, we introduced language models, provided a brief history, and discussed the impact they are already having on the world<a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModels1452" id="id493"/>. We showed how to effectively interact with the model using various prompting techniques. We also gave an overview of the strengths and limitations of language models. We showed how easy it is to build prototype applications and highlighted the challenges involved in taking them to production. In the next chapter, we will begin our journey into the world of LLMs by introducing the ingredients that go into making an LLM.</p>
</div></section>
</div></section></div>
</div>
</body></html>