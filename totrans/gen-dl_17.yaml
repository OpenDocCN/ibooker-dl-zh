- en: Chapter 13\. Multimodal Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have analyzed generative learning problems that focus solely on
    one modality of data: either text, images, or music. We have seen how GANs and
    diffusion models can generate state-of-the-art images and how Transformers are
    pioneering the way for both text and image generation. However, as humans, we
    have no difficulties crossing modalities—for example, writing a description of
    what is happening in a given photograph, creating digital art to depict a fictional
    fantasy world in a book, or matching a film score to the emotions of a given scene.
    Can we train machines to do the same?'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Multimodal learning* involves training generative models to convert between
    two or more different kinds of data. Some of the most impressive generative models
    introduced in the last two years have been multimodal in nature. In this chapter
    we will explore how they work in detail and consider how the future of generative
    modeling will be shaped by large multimodal models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll explore four different vision-language models: DALL.E 2 from OpenAI;
    Imagen from Google Brain; Stable Diffusion from Stability AI, CompVis, and Runway;
    and Flamingo from DeepMind.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The aim of this chapter is to concisely explain how each model works, without
    going into the fine detail of every design decision. For more information, refer
    to the individual papers for each model, which explain all of the design choices
    and architecture decisions in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-image generation focuses on producing state-of-the-art images from a
    given text prompt. For example, given the input “A head of broccoli made out of
    modeling clay, smiling in the sun,” we would like the model to be able to output
    a image that accurately matches the text prompt, as shown in [Figure 13-1](#dalle_example).
  prefs: []
  type: TYPE_NORMAL
- en: This is clearly a highly challenging problem. Text understanding and image generation
    are difficult to solve in their own right, as we have seen in previous chapters
    of this book. Multimodal modeling such as this presents an additional challenge,
    because the model must also learn how to cross the bridge between the two domains
    and learn a shared representation that allows it to accurately convert from a
    block of text to a high-fidelity image without loss of information.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. An example of text-to-image generation by DALL.E 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Moreover, in order to be successful the model must be able to combine concepts
    and styles that it may never have seen before. For example, there are no Michelangelo
    frescos containing people wearing virtual reality headsets, but we would like
    our model to be able to create such an image if we ask it to. Equally, it would
    be desirable for the model to accurately infer how objects in the generated image
    relate to each other, based on the text prompt. For example, a picture of “an
    astronaut riding a doughnut through space” should look very different from one
    of “an astronaut eating a doughnut in a crowded space.” The model must learn how
    words are given meaning through context and how to convert explicit textual relationships
    between entities to images that imply the same meaning.
  prefs: []
  type: TYPE_NORMAL
- en: DALL.E 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first model we shall explore is *DALL.E 2*, a model designed by OpenAI for
    text-to-image generation. The first version of this model, DALL.E,^([1](ch13.xhtml#idm45387001413744))
    was released in February 2021 and sparked a new wave of interest in generative
    multimodal models. In this section, we shall investigate the workings of the second
    iteration of the model, DALL.E 2,^([2](ch13.xhtml#idm45387001411696)) released
    just over a year later in April 2022.
  prefs: []
  type: TYPE_NORMAL
- en: DALL.E 2 is an extremely impressive model that has furthered our understanding
    of AI’s ability to solve these types of multimodal problems. It not only has ramifications
    academically, but also forces us to ask big questions relating to the role of
    AI in creative processes that previously were thought to be unique to humans.
    We will start by exploring how DALL.E 2 works, building on key foundational ideas
    that we have already explored earlier in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand how DALL.E 2 works, we must first survey its overall architecture,
    as shown in [Figure 13-2](#dalle_arch).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. The DALL.E 2 architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are three distinct parts to consider: the *text encoder*, the *prior*,
    and the *decoder*. Text is first passed through the text encoder to produce a
    text embedding vector. This vector is then transformed by the prior to produce
    an image embedding vector. Finally, this is passed through the decoder, along
    with the original text, to produce the generated image. We will step through each
    component in turn, to get a complete picture of how DALL.E 2 works in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: The Text Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The aim of the text encoder is to convert the text prompt into an embedding
    vector that represents the conceptual meaning of the text prompt within a latent
    space. As we have seen in previous chapters, converting discrete text to a continuous
    latent space vector is essential for all downstream tasks, because we can continue
    to manipulate the vector further depending on our particular goal.
  prefs: []
  type: TYPE_NORMAL
- en: In DALL.E 2, the authors do not train the text encoder from scratch, but instead
    make use of an existing model called *Contrastive Language–Image Pre-training*
    (CLIP), also produced by OpenAI. Therefore, to understand the text encoder, we
    must first understand how CLIP works.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[CLIP](https://openai.com/blog/clip)^([3](ch13.xhtml#idm45387001394816)) was
    unveiled in a paper published by OpenAI in February 2021 (just a few days after
    the first DALL.E paper) that described it as “a neural network that efficiently
    learns visual concepts from natural language supervision.”'
  prefs: []
  type: TYPE_NORMAL
- en: It uses a technique called *contrastive learning* to match images with text
    descriptions. The model is trained on a dataset of 400 million text–image pairs
    scraped from the internet—some example pairs are shown in [Figure 13-3](#clip_training_set).
    For comparison, there are 14 million hand-annotated images in ImageNet. Given
    an image and a list of possible text descriptions, its task is to find the one
    that actually matches the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-3\. Examples of text–image pairs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The key idea behind contrastive learning is simple. We train two neural networks:
    a *text encoder* that converts text to a text embedding and an *image encoder*
    that converts an image to an image embedding. Then, given a batch of text–image
    pairs, we compare all text and image embedding combinations using *cosine similarity*
    and train the networks to maximize the score between matching text–image pairs
    and minimize the score between incorrect text–image pairs. This process is shown
    in [Figure 13-4](#clip_arch).'
  prefs: []
  type: TYPE_NORMAL
- en: CLIP Is Not Generative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that CLIP is not itself a generative model—it cannot produce images or
    text. Is it closer to a discriminative model, because the final output is a prediction
    about which text description from a given set most closely matches a given image
    (or the other way around, which image most closely matches a given text description).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. The CLIP training process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Both the text encoder and the image encoder are Transformers—the image encoder
    is a Vision Transformer (ViT), introduced in [“ViT VQ-GAN”](ch10.xhtml#Vit_VQ-GAN),
    which applies the same concept of attention to images. The authors tested other
    model architectures, but found this combination to produce the best results.
  prefs: []
  type: TYPE_NORMAL
- en: What makes CLIP especially interesting is the way it can be used for *zero-shot
    prediction* on tasks that it has never been exposed to. For example, suppose we
    want to use CLIP to predict the label of a given image in the ImageNet dataset.
    We can first convert the ImageNet labels into sentences by using a template (e.g.,
    “a photo of a <label>”), as shown in [Figure 13-5](#text_encode_imagenet).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-5\. Converting labels in a new dataset to captions, in order to produce
    CLIP text embeddings
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To predict the label of a given image, we can pass it through the CLIP image
    encoder and calculate the cosine similarity between the image embedding and all
    possible text embeddings in order to find the label with the maximum score, as
    shown in [Figure 13-6](#cosine_sim_clip).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-6\. Using CLIP to predict the content of an image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that we do not need to retrain either of the CLIP neural networks for
    it to be readily applicable to new tasks. It uses language as the common domain
    through which any set of labels can be expressed.
  prefs: []
  type: TYPE_NORMAL
- en: Using this approach, it is possible to show that CLIP performs well across a
    wide range of image dataset labeling challenges ([Figure 13-7](#clip_labelling)).
    Other models that have been trained on a specific dataset to predict a given set
    of labels often fail when applied to different datasets with the same labels because
    they are highly optimized to the individual datasets on which they were trained.
    CLIP is much more robust, as it has learned a deep conceptual understanding of
    full text descriptions and images, rather than just excelling at the narrow task
    of assigning a single label to a given image in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1307.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-7\. CLIP performs well on a wide range of image labeling datasets
    (source: [Radford et al., 2021](https://arxiv.org/abs/2103.00020))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As mentioned, CLIP is measured on its discriminative ability, so how does it
    help us to build generative models such as DALL.E 2?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is that we can take the trained text encoder and use it as one part
    of a larger model such as DALL.E 2, with frozen weights. The trained encoder is
    simply a generalized model for converting text to a text embedding, which should
    be useful for downstream tasks such as generating images. The text encoder is
    able to capture a rich conceptual understanding of the text, as it has been trained
    to be as similar as possible to its matching image embedding counterpart, which
    is produced only from the paired image. It is therefore the first part of the
    bridge that we need to be able to cross over from the text domain to the image
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: The Prior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next stage of the process involves converting the text embedding into a
    CLIP image embedding. The DALL.E 2 authors tried two different methods for training
    the prior model:'
  prefs: []
  type: TYPE_NORMAL
- en: An autoregressive model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A diffusion model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They found that the diffusion approach outperformed the autoregressive model
    and was more computationally efficient. In this section, we’ll look at both and
    see how they differ.
  prefs: []
  type: TYPE_NORMAL
- en: Autoregressive prior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An autoregressive model generates output sequentially, by placing an ordering
    on the output tokens (e.g., words, pixels) and conditioning the next token on
    previous tokens. We have seen in previous chapters how this is used in recurrent
    neural networks (e.g., LSTMs), Transformers, and PixelCNN.
  prefs: []
  type: TYPE_NORMAL
- en: The autoregressive prior of DALL.E 2 is an encoder-decoder Transformer. It is
    trained to reproduce the CLIP image embedding given a CLIP text embedding, as
    shown in [Figure 13-8](#ar_prior). Note that there are some additional components
    to the autoregressive model mentioned in the original paper that we omit here
    for conciseness.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-8\. A simplified diagram of the autoregressive prior of DALL.E 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model is trained on the CLIP text–image pair dataset. You can think of
    it as the second part of the bridge that we need in order to jump from the text
    domain to the image domain: we are converting a vector from the text embedding
    latent space to the image embedding latent space.'
  prefs: []
  type: TYPE_NORMAL
- en: The input text embedding is processed by the encoder of the Transformer to produce
    another representation that is fed to the decoder, alongside the current generated
    output image embedding. The output is generated one element at a time, using teacher
    forcing to compare the predicted next element to the actual CLIP image embedding.
  prefs: []
  type: TYPE_NORMAL
- en: The sequential nature of the generation means that the autoregressive model
    is less computationally efficient than the other method tried by the authors,
    which we’ll look at next.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion prior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we saw in [Chapter 8](ch08.xhtml#chapter_diffusion), diffusion models are
    fast becoming the go-to choice for generative modeling practitioners, alongside
    Transformers. In DALL.E 2 a decoder-only Transformer is used as the prior, trained
    using a diffusion process.
  prefs: []
  type: TYPE_NORMAL
- en: The training and generation process is shown in [Figure 13-9](#diffusion_prior).
    Again, this is a simplified version; the original paper contains full details
    of how the diffusion model is structured.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-9\. A simplified diagram of the diffusion prior training and generation
    process of DALL.E 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: During training, each CLIP text and image embedding pair are first concatenated
    into a single vector. Then, the image embedding is noised over 1,000 timesteps
    until it is indistinguishable from random noise. The diffusion prior is then trained
    to predict the denoised image embedding at the previous timestep. The prior has
    access to the text embedding throughout, so it is able to condition its predictions
    on this information, gradually transforming the random noise into a predicted
    CLIP image embedding. The loss function is the average mean-squared error across
    denoising steps.
  prefs: []
  type: TYPE_NORMAL
- en: To generate new image embeddings, we sample a random vector, prepend the relevant
    text embedding, and pass it through the trained diffusion prior multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: The Decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final part of DALL.E 2 is the decoder. This is the part of the model that
    generates the final image conditioned on the text prompt and the predicted image
    embedding output by the prior.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture and training process of the decoder borrows from an earlier
    OpenAI paper, published in December 2021, which presented a generative model called
    Guided Language to Image Diffusion for Generation and Editing (GLIDE).^([4](ch13.xhtml#idm45387001327744))
  prefs: []
  type: TYPE_NORMAL
- en: GLIDE is able to generate realistic images from text prompts, in much the same
    way that DALL.E 2 can. The difference is that GLIDE does not make use of CLIP
    embeddings, but instead works directly with the raw text prompt, training the
    entire model from scratch, as shown in [Figure 13-10](#glide_dalle).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-10\. A comparison between DALL.E 2 and GLIDE—GLIDE trains the entire
    generative model from scratch, whereas DALL.E 2 makes use of CLIP embeddings to
    carry information forward from the initial text prompt
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s see how GLIDE works first.
  prefs: []
  type: TYPE_NORMAL
- en: GLIDE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GLIDE is trained as a diffusion model, with U-Net architecture for the denoiser
    and Transformer architecture for the text encoder. It learns to undo the noise
    added to an image, guided by the text prompt. Finally, an *Upsampler* is trained
    to scale the generated image to 1,024 × 1,024 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: GLIDE trains the 3.5 billion (B) parameter model from scratch—2.3B parameters
    for the visual part of the model (U-Net and Upsampler) and 1.2B for the Transformer.
    It is trained on 250 million text–image pairs.
  prefs: []
  type: TYPE_NORMAL
- en: The diffusion process is shown in [Figure 13-11](#glide_diffusion). A Transformer
    is used to create an embedding of the input text prompt, which is then used to
    guide the U-Net throughout the denoising process. We explored the U-Net architecture
    in [Chapter 8](ch08.xhtml#chapter_diffusion); it’s a perfect model choice when
    the overall size of the image should stay the same (e.g., for style transfer,
    denoising, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-11\. The GLIDE diffusion process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The DALL.E 2 decoder still uses the U-Net denoiser and Transformer text encoder
    architectures, but additionally has the predicted CLIP image embeddings to condition
    on. This is the key difference between GLIDE and DALL.E 2, as shown in [Figure 13-12](#decoder_diffusion).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-12\. The DALL.E 2 decoder additionally conditions on the image embedding
    produced by the prior
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with all diffusion models, to generate a new image, we simply sample some
    random noise and run this through the U-Net denoiser multiple times, conditioned
    on the Transformer text encoding and image embedding. The output is a 64 × 64–pixel
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Upsampler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final part of the decoder is the Upsampler (two separate diffusion models).
    The first diffusion model transforms the image from 64 × 64 to 256 × 256 pixels.
    The second transforms it again, from 256 × 256 to 1,024 × 1,024 pixels, as shown
    in [Figure 13-13](#decoder_upsampler).
  prefs: []
  type: TYPE_NORMAL
- en: Upsampling is useful because it means we do not have to build large upstream
    models to handle high-dimensional images. We can work with small images until
    the final stages of the process, when we apply the Upsamplers. This saves on model
    parameters and ensures a more efficient upstream training process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-13\. The first Upsampler diffusion model converts the image from 64
    × 64 pixels to 256 × 256 pixels while the second converts from 256 × 256 pixels
    to 1,024 × 1,024 pixels
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This concludes the DALL.E 2 model explanation! In summary, DALL.E 2 makes use
    of the pre-trained CLIP model to immediately produce a text embedding of the input
    prompt. Then it converts this into an image embedding using a diffusion model
    called the prior. Lastly, it implements a GLIDE-style diffusion model to generate
    the output image, conditioned on the predicted image embedding and Transformer-encoded
    input prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Examples from DALL.E 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Examples of more images generated by DALL.E 2 can be found on [the official
    website](https://openai.com/dall-e-2). The way that the model is able to combine
    complex, disparate concepts in a realistic, believable way is astonishing and
    represents a significant leap forward for AI and generative modeling.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, the authors show how the model can be used for additional purposes
    other than text-to-image generation. One of these applications is creating variations
    of a given image, which we explore in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Image variations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed previously, to generate images using the DALL.E 2 decoder we sample
    an image consisting of pure random noise and then gradually reduce the amount
    of noise using the denoising diffusion model, conditioned on the provided image
    embedding. Selecting different initial random noise samples will result in different
    images.
  prefs: []
  type: TYPE_NORMAL
- en: In order to generate variations of a given image, we therefore just need to
    establish its image embedding to feed to the decoder. We can obtain this using
    the original CLIP image encoder, which is explicitly designed to convert an image
    into its CLIP image embedding. This process is shown in [Figure 13-14](#dalle_image_variations).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1314.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-14\. DALL.E 2 can be used for generating variations of a given image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Importance of the prior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another avenue explored by the authors is establishing the importance of the
    prior. The purpose of the prior is to provide the decoder with a useful representation
    of the image to be generated, making use of the pre-trained CLIP model. However,
    it is feasible that this step isn’t necessary—perhaps we could just pass the text
    embedding directly to the decoder instead of the image embedding, or ignore the
    CLIP embeddings completely and condition only on the text prompt. Would this impact
    the quality of the generations?
  prefs: []
  type: TYPE_NORMAL
- en: 'To test this, the authors tried three different approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Feed the decoder only with the text prompt (and a zero vector for the image
    embedding).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the decoder with the text prompt and the text embedding (as if it were
    an image embedding).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the decoder with the text prompt and the image embedding (i.e., the full
    model).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example results are shown in [Figure 13-15](#dalle_prior_importance). We can
    see that when the decoder is starved of image embedding information, it can only
    produce a rough approximation of the text prompt, missing key information such
    as the calculator. Using the text embedding as if it were an image embedding performs
    slightly better, though it is not able to capture the relationship between the
    hedgehog and the calculator. Only the full model with the prior produces an image
    that accurately reflects all of the information contained within the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1315.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-15\. The prior provides the model with additional context and helps
    the decoder to produce more accurate generations (source: [Ramesh et al., 2022](https://arxiv.org/pdf/2204.06125.pdf))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the DALL.E 2 paper, the authors also highlight several known limitations
    of the model. Two of these (attribute binding and text generation) are shown in
    [Figure 13-16](#dalle_limitations).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1316.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-16\. Two limitations of DALL.E 2 lie in its ability to bind attributes
    to objects and reproduce textual information—top prompt: “A red cube on top of
    a blue cube”; bottom prompt: “A sign that says deep learning” (source: [Ramesh
    et al., 2022](https://arxiv.org/pdf/2204.06125.pdf))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Attribute binding* is the ability of a model to understand the relationship
    between words in a given text prompt, and in particular how attributes relate
    to objects. For example, the prompt “A red cube on top of a blue cube” must appear
    visually distinct from “A blue cube on top of a red cube.” DALL.E struggles somewhat
    with this, compared to earlier models such as GLIDE, though the overall quality
    of generations is better and more diverse.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, DALL.E 2 is not able to accurately reproduce text—this is probably due
    to the fact that the CLIP embeddings do not capture spellings, but instead only
    contain a higher-level representation of the text. These representations can be
    decoded into text with partial success (e.g., individual letters are mostly correct),
    but not with enough compositional understanding to form full words.
  prefs: []
  type: TYPE_NORMAL
- en: Imagen
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just over a month after OpenAI released DALL.E 2, the Google Brain team released
    their own text-to-image model called Imagen.^([5](ch13.xhtml#idm45387001262144))
    Many of the core themes that we have already explored in this chapter are also
    relevant to Imagen: for example, it uses a text encoder and a diffusion model
    decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll explore the overall architecture of Imagen and compare
    it with DALL.E 2.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An overview of the Imagen architecture is shown in [Figure 13-17](#imagen_arch).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1317.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-17\. The Imagen architecture (source: [Saharia et al., 2022](https://arxiv.org/abs/2205.11487))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The frozen text encoder is the pre-trained T5-XXL model, a large encoder-decoder
    Transformer. Unlike CLIP, this was trained only on text and not images, so it
    is not a multimodal model. However, the authors found that it still functions
    extremely well as a text encoder for Imagen and that scaling this model has more
    impact on overall performance than scaling the diffusion model decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Like DALL.E 2’s, Imagen’s the decoding diffusion model is based on a U-Net architecture,
    conditioned on text embeddings. There are several architectural improvements made
    to the standard U-Net architecture, to produce what the authors call the *Efficient
    U-Net*. This model uses less memory, converges faster, and has better sample quality
    than previous U-Net models.
  prefs: []
  type: TYPE_NORMAL
- en: The Upsampler super-resolution models that take the generated image from 64
    × 64 to 1,024 × 1,024 pixels are also diffusion models that continue to use the
    text embeddings to guide the upsampling process.
  prefs: []
  type: TYPE_NORMAL
- en: DrawBench
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An additional contribution of the Imagen paper is *DrawBench*—a suite of 200
    text prompts for text-to-image evaluation. The text prompts cover 11 categories,
    such as *Counting* (ability to generate a specified number of objects), *Description*
    (ability to generate complex and long text prompts describing objects), and *Text*
    (ability to generate quoted text). To compare two models, the DrawBench text prompts
    are passed through each model and the outputs given to a panel of human raters
    for evaluation across two metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Alignment
  prefs: []
  type: TYPE_NORMAL
- en: Which image more accurately describes the caption?
  prefs: []
  type: TYPE_NORMAL
- en: Fidelity
  prefs: []
  type: TYPE_NORMAL
- en: Which image is more photorealistic (looks more real)?
  prefs: []
  type: TYPE_NORMAL
- en: The results from the DrawBench human evaluation are shown in [Figure 13-18](#imagen_evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: Both DALL.E 2 and Imagen are remarkable models that have made significant contributions
    to the field of text-to-image generation. Whilst Imagen outperforms DALL.E 2 on
    many of the DrawBench benchmarks, DALL.E 2 provides additional functionalities
    that are not present in Imagen. For example, because DALL.E 2 utilizes CLIP (a
    multimodal text–image model), it is able to accept images as input to generate
    image embeddings. This means DALL.E 2 is able to provide image editing and image
    variation capabilities. This is not possible with Imagen; the text encoder is
    a pure text model, so there is no way to input an image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1318.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-18\. Comparison of Imagen and DALL.E 2 on DrawBench across alignment
    and image fidelity (source: [Saharia et al., 2022](https://arxiv.org/abs/2205.11487))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Examples from Imagen
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Example Imagen generations are shown in [Figure 13-19](#imagen_generations).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1319.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-19\. Example Imagen generations (source: [Saharia et al., 2022](https://arxiv.org/abs/2205.11487))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Stable Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last text-to-image diffusion model that we shall explore is *Stable Diffusion*,
    released in August 2022 by [Stability AI](https://stability.ai), in collaboration
    with the [Computer Vision and Learning research group at Ludwig Maximilian University
    of Munich](https://ommer-lab.com) and [Runway](https://runwayml.com). It is different
    from DALL.E 2 and Imagen in that its code and model weights have been released
    publicly, through [Hugging Face](https://oreil.ly/BTrWI). This means that anyone
    can interact with the model on their own hardware, without having to use proprietary
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main architectural difference between Stable Diffusion and the text-to-image
    models discussed previously is that it uses *latent diffusion* as its underlying
    generative model. Latent diffusion models (LDMs) were introduced by Rombach et
    al. in December 2021, in the paper “High-Resolution Image Synthesis with Latent
    Diffusion Models.”^([6](ch13.xhtml#idm45387001217376)) The key idea from the paper
    is to wrap the diffusion model within an autoencoder, so that the diffusion process
    operates on a latent space representation of the image rather than the image itself,
    as shown in [Figure 13-20](#stablediffusion).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1320.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-20\. The Stable Diffusion architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This breakthrough means that the denoising U-Net model can be kept relatively
    lightweight, in comparison to U-Net models that operate on full images. The autoencoder
    handles the heavy lifting of encoding the image detail into latent space and decoding
    the latent space back to a high-resolution image, leaving the diffusion model
    to work purely in a latent, conceptual space. This gives a significant speed and
    performance boost to the training process.
  prefs: []
  type: TYPE_NORMAL
- en: The denoising process can also optionally be guided by a text prompt that has
    been passed through a text encoder. The first version of Stable Diffusion utilized
    the pre-trained CLIP model from OpenAI (the same as in DALL.E 2), but Stable Diffusion
    2 has a custom trained CLIP model called [OpenCLIP](https://oreil.ly/RaCbu), which
    has been trained from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Examples from Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 13-21](#stablediffusionexamples) shows some example outputs from Stable
    Diffusion 2.1—you can try your own prompts through the model hosted on [Hugging
    Face](https://oreil.ly/LpGW4).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1321.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-21\. Example outputs from Stable Diffusion 2.1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Exploring the Latent Space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’d like to explore the latent space of the Stable Diffusion model, I highly
    recommended the [walkthrough](https://oreil.ly/4sNe5) on the Keras website.
  prefs: []
  type: TYPE_NORMAL
- en: Flamingo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have looked at three different kinds of text-to-image models. In this
    section, we’ll explore a multimodal model that generates text given a stream of
    text and visual data. Flamingo, introduced in a paper by DeepMind in April 2022,^([7](ch13.xhtml#idm45387001198400))
    is a family of visual language models (VLMs) that act as a bridge between pre-trained
    vision-only and language-only models.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll run through the architecture of Flamingo models and compare
    them to the text-to-image models we have seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The overall architecture of Flamingo is shown in [Figure 13-22](#flamingo_arch).
    For conciseness, we shall explore the core components of this model—the Vision
    Encoder, the Perceiver Resampler, and the Language Mode—in just enough detail
    to highlight the key ideas that make Flamingo unique. I highly recommend reading
    the original research paper for a thorough review of each part of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1322.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-22\. The Flamingo architecture (source: [Alayrac et al., 2022](https://arxiv.org/pdf/2204.14198.pdf))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Vision Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first difference between a Flamingo model and pure text-to-image models
    such as DALL.E 2 and Imagen is that Flamingo can accept a combination of text
    and visual data interleaved. Here, *visual data* includes videos as well as images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The job of the Vision Encoder is to convert the vision data within the input
    into embedding vectors (similar to the image encoder in CLIP). The Vision Encoder
    in Flamingo is a pre-trained Normalizer-Free ResNet (NFNet), as introduced by
    Brock et al. in 2021^([8](ch13.xhtml#idm45387001185712))—in particular, an NFNet-F6
    (the NFNet models range from F0 to F6, increasing in size and power). This is
    one key difference between the CLIP image encoder and the Flamingo Vision Encoder:
    the former uses a ViT architecture, whereas the latter uses a ResNet architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: The Vision Encoder is trained on image-text pairs using the same contrastive
    objective as introduced in the CLIP paper. After training, the weights are frozen
    so that any further training of the Flamingo model does not affect the weights
    of the Vision Encoder.
  prefs: []
  type: TYPE_NORMAL
- en: The output from the Vision Encoder is a 2D grid of features that then gets flattened
    to a 1D vector before being passed to the Perceiver Resampler. Video is handled
    by sampling at 1 frame per second and passing each snapshot through the Vision
    Encoder independently to produce several feature grids; learned temporal encodings
    are then added in before flattening the features and concatenating the results
    into a single vector.
  prefs: []
  type: TYPE_NORMAL
- en: The Perceiver Resampler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Memory requirements in a traditional encoder Transformer (e.g., BERT) scale
    quadratically with input sequence length, which is why input sequences are normally
    capped at a set number of tokens (e.g., 512 in BERT). However, the output from
    the Vision Encoder is a vector of variable length (due to the variable input image
    resolution and the variable number of video frames) and is therefore potentially
    very long.
  prefs: []
  type: TYPE_NORMAL
- en: The Perceiver architecture is specifically designed to efficiently handle long
    input sequences. Instead of performing self-attention on the full input sequence,
    it works with a fixed-length latent vector and only uses the input sequence for
    cross-attention. Specifically, in the Flamingo Perceiver Resampler, the *key*
    and *value* are a concatenation of the input sequence and latent vector and the
    *query* is the latent vector alone. A diagram of the Vision Encoder and Perceiver
    Resampler process for video data is shown in [Figure 13-23](#flamingo_perceiver).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1323.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-23\. The Perceiver Resampler applied to video input (source: [Alayrac
    et al., 2022](https://arxiv.org/pdf/2204.14198.pdf))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The output of the Perceiver Resampler is a fixed-length latent vector that gets
    passed to the Language Model.
  prefs: []
  type: TYPE_NORMAL
- en: The Language Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Language Model consists of several stacked blocks, in the style of a decoder
    Transformer, that output a predicted text continuation. In fact, the majority
    of the Language Model is from a pre-trained DeepMind model called *Chinchilla*.
    The Chinchilla paper, published in March 2022,^([9](ch13.xhtml#idm45387001171728))
    showcases a language model that is designed to be considerably smaller than its
    peers (e.g., 70B parameters for Chinchilla compared to 170B for GPT-3), while
    using significantly more tokens for training. The authors show that the model
    outperforms larger models on a range of tasks, highlighting the importance of
    optimizing the trade-off between training a larger model and using a larger number
    of tokens during training.
  prefs: []
  type: TYPE_NORMAL
- en: A key contribution of the Flamingo paper is to show how Chinchilla can be adapted
    to work with additional vision data (`X`) that is interspersed with the language
    data (`Y`). Let’s first explore how the language and vision input are combined
    to produce the input to the Language Model ([Figure 13-24](#flamingo_masked_cross_attention)).
  prefs: []
  type: TYPE_NORMAL
- en: First the text is processed by replacing vision data (e.g., images) with an
    `<image>` tag and the text is divided into *chunks* using the `<EOC>` (end of
    chunk) tag. Each chunk contains at most one image, which is always at the start
    of the chunk—i.e., the subsequent text is assumed to relate only to that image.
    The beginning of the sequence is also marked with the `<BOS>` (beginning of sentence)
    tag.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the sequence is tokenized and each token is given an index (`phi`) corresponding
    to the preceding image index (or `0` if there is no preceding image in the chunk).
    This way, the text tokens (`Y`) can be forced to only cross-attend to the image
    tokens (`X`) that correspond to their particular chunk, through masking. For example,
    in [Figure 13-24](#flamingo_masked_cross_attention) the first chunk contains no
    images, so all image tokens from the Perceiver Resampler are masked. The second
    chunk contains image 1, so these tokens are allowed to interact with the image
    tokens from image 1\. Likewise, the final chunk contains image 2, so these tokens
    are allowed to interact with the image tokens from image 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1324.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-24\. Masked cross-attention (XATTN), combining vision and text data—light
    blue entries are masked and dark blue entries are nonmasked (source: [Alayrac
    et al., 2022](https://arxiv.org/pdf/2204.14198.pdf))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can now see how this masked cross-attention component fits into the overall
    architecture of the Language Model ([Figure 13-25](#flamingo_language_model)).
  prefs: []
  type: TYPE_NORMAL
- en: The blue LM layer components are frozen layers from Chinchilla—these are not
    updated during the training process. The purple `GATED XATTN-DENSE` layers are
    trained as part of Flamingo and include the masked cross-attention components
    that blend the language and vision information, as well as subsequent feed-forward
    (dense) layers.
  prefs: []
  type: TYPE_NORMAL
- en: The layer is *gated* because it passes the output from the cross-attention and
    feed-forward components through two distinct tanh gates, which are both initialized
    to zero. Therefore, when the network is initialized, there is no contribution
    from the `GATED XATTN-DENSE` layers—the language information is just passed straight
    through. The `alpha` gating parameters are learned by the network, to gradually
    blend in information from the vision data as training progresses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1325.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-25\. A Flamingo Language Model block, comprising a frozen language
    model layer from Chinchilla and a `GATED XATTN-DENSE` layer (source: [Alayrac
    et al., 2022](https://arxiv.org/pdf/2204.14198.pdf))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Examples from Flamingo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flamingo can be used for a variety of purposes, including image and video understanding,
    conversational prompting, and visual dialogue. In [Figure 13-26](#flamingo_examples)
    we can see a few examples of what Flamingo is capable of.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1326.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-26\. Examples of inputs and outputs obtained from the 80B parameter
    Flamingo model (source: [Alayrac et al., 2022](https://arxiv.org/pdf/2204.14198.pdf))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how in each example, Flamingo is blending information from the text and
    the images in true multimodal style. The first example uses images in place of
    words and is able to suggest an appropriate book to continue the prompt. The second
    example shows frames from a video, and Flamingo correctly identifies the consequence
    of the action. The last three examples all demonstrate how Flamingo can be used
    interactively, to provide additional information through dialogue or probe with
    further questioning.
  prefs: []
  type: TYPE_NORMAL
- en: It is astonishing to see a machine being able to answer complex questions across
    such a wide range of modalities and input tasks. In the paper, the authors quantify
    Flamingo’s ability across a set of benchmark tasks and find that across many benchmarks,
    Flamingo is able to surpass the performance of models that have been tailored
    to specifically tackle the one task in question. This highlights how large multimodal
    models can be rapidly adapted to a wide range of tasks and paves the way for the
    development of AI agents that aren’t just tied to a single task, but instead are
    truly general agents that can be guided by the user at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we have explored four different state-of-the-art multimodal
    models: DALL.E 2, Imagen, Stable Diffusion, and Flamingo.'
  prefs: []
  type: TYPE_NORMAL
- en: DALL.E 2 is a large-scale text-to-image model from OpenAI that can generate
    realistic images across a range of styles given a text prompt. It works by combining
    pre-trained models (e.g., CLIP) with diffusion model architectures from previous
    works (GLIDE). It also has additional capabilities, such as being able to edit
    images through text prompting and provide variations of a given image. While it
    does have some limitations, such as inconsistent text rendering and attribute
    binding, DALL.E 2 is an incredibly powerful AI model that has helped to propel
    the field of generative modeling into a new era.
  prefs: []
  type: TYPE_NORMAL
- en: Another model that has surpassed previous benchmarks is Imagen from Google Brain.
    This model shares many similarities with DALL.E 2, such as a text encoder and
    a diffusion model decoder. One of the key differences between the two models is
    that the Imagen text encoder is trained on pure text data, whereas the training
    process for the DALL.E 2 text encoder involves image data (through the contrastive
    CLIP learning objective). The authors show that this approach leads to state-of-the-art
    performance across a range of tasks, through their DrawBench evaluation suite.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion is an open source offering from Stability AI, CompVis, and
    Runway. It is a text-to-image model whose model weights and code are freely available,
    so you can run it on your own hardware. Stable Diffusion is particularly fast
    and lightweight due to the use of a latent diffusion model that operates on the
    latent space of an autoencoder, rather than the images themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, DeepMind’s Flamingo is a visual language model—that is, it accepts
    a stream of interleaved text and visual data (images and video) and is able to
    continue the prompt with additional text, in the style of a decoder Transformer.
    The key contribution is showing how the visual information can be fed to the Transformer
    via a Visual Encoder and Perceiver Resampler that encode the visual input features
    into a small number of visual tokens. The Language Model itself is an extension
    of DeepMind’s earlier Chinchilla model, adapted to blend in visual information.
  prefs: []
  type: TYPE_NORMAL
- en: All four are remarkable examples of the power of multimodal models. In the future,
    it is highly likely that generative modeling will become more multimodal and AI
    models will be able to easily cross modalities and tasks through interactive language
    prompting.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch13.xhtml#idm45387001413744-marker)) Aditya Ramesh et al., “Zero-Shot
    Text-to-Image Generation,” February 24, 2021, [*https://arxiv.org/abs/2102.12092*](https://arxiv.org/abs/2102.12092).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch13.xhtml#idm45387001411696-marker)) Aditya Ramesh et al., “Hierarchical
    Text-Conditional Image Generation with CLIP Latents,” April 13, 2022, [*https://arxiv.org/abs/2204.06125*](https://arxiv.org/abs/2204.06125).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch13.xhtml#idm45387001394816-marker)) Alec Radford et al., “Learning Transferable
    Visual Models From Natural Language Supervision,” February 26, 2021, [*https://arxiv.org/abs/2103.00020*](https://arxiv.org/abs/2103.00020).
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch13.xhtml#idm45387001327744-marker)) Alex Nichol et al., “GLIDE: Towards
    Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,”
    December 20, 2021, [*https://arxiv.org/abs/2112.10741*](https://arxiv.org/abs/2112.10741).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch13.xhtml#idm45387001262144-marker)) Chitwan Saharia et al., “Photorealistic
    Text-to-Image Diffusion Models with Deep Language Understanding,” May 23, 2022,
    [*https://arxiv.org/abs/2205.11487*](https://arxiv.org/abs/2205.11487).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch13.xhtml#idm45387001217376-marker)) Robin Rombach et al., “High Resolution
    Image Synthesis with Latent Diffusion Models,” December 20, 2021, [*https://arxiv.org/abs/2112.10752*](https://arxiv.org/abs/2112.10752).
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch13.xhtml#idm45387001198400-marker)) Jean-Baptiste Alayrac et al., “Flamingo:
    A Visual Language Model for Few-Shot Learning,” April 29, 2022, [*https://arxiv.org/abs/2204.14198*](https://arxiv.org/abs/2204.14198).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch13.xhtml#idm45387001185712-marker)) Andrew Brock et al., “High-Performance
    Large-Scale Image Recognition Without Normalization,” February 11, 2021, [*https://arxiv.org/abs/2102.06171*](https://arxiv.org/abs/2102.06171).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch13.xhtml#idm45387001171728-marker)) Jordan Hoffmann et al., “Training
    Compute-Optimal Large Language Models,” March 29, 2022, [*https://arxiv.org/abs/2203.15556v1*](https://arxiv.org/abs/2203.15556v1).
  prefs: []
  type: TYPE_NORMAL
