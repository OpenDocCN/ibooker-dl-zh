- en: Chapter 13\. Multimodal Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have analyzed generative learning problems that focus solely on
    one modality of data: either text, images, or music. We have seen how GANs and
    diffusion models can generate state-of-the-art images and how Transformers are
    pioneering the way for both text and image generation. However, as humans, we
    have no difficulties crossing modalities—for example, writing a description of
    what is happening in a given photograph, creating digital art to depict a fictional
    fantasy world in a book, or matching a film score to the emotions of a given scene.
    Can we train machines to do the same?'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Multimodal learning* involves training generative models to convert between
    two or more different kinds of data. Some of the most impressive generative models
    introduced in the last two years have been multimodal in nature. In this chapter
    we will explore how they work in detail and consider how the future of generative
    modeling will be shaped by large multimodal models.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll explore four different vision-language models: DALL.E 2 from OpenAI;
    Imagen from Google Brain; Stable Diffusion from Stability AI, CompVis, and Runway;
    and Flamingo from DeepMind.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The aim of this chapter is to concisely explain how each model works, without
    going into the fine detail of every design decision. For more information, refer
    to the individual papers for each model, which explain all of the design choices
    and architecture decisions in detail.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-image generation focuses on producing state-of-the-art images from a
    given text prompt. For example, given the input “A head of broccoli made out of
    modeling clay, smiling in the sun,” we would like the model to be able to output
    a image that accurately matches the text prompt, as shown in [Figure 13-1](#dalle_example).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This is clearly a highly challenging problem. Text understanding and image generation
    are difficult to solve in their own right, as we have seen in previous chapters
    of this book. Multimodal modeling such as this presents an additional challenge,
    because the model must also learn how to cross the bridge between the two domains
    and learn a shared representation that allows it to accurately convert from a
    block of text to a high-fidelity image without loss of information.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1301.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. An example of text-to-image generation by DALL.E 2
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Moreover, in order to be successful the model must be able to combine concepts
    and styles that it may never have seen before. For example, there are no Michelangelo
    frescos containing people wearing virtual reality headsets, but we would like
    our model to be able to create such an image if we ask it to. Equally, it would
    be desirable for the model to accurately infer how objects in the generated image
    relate to each other, based on the text prompt. For example, a picture of “an
    astronaut riding a doughnut through space” should look very different from one
    of “an astronaut eating a doughnut in a crowded space.” The model must learn how
    words are given meaning through context and how to convert explicit textual relationships
    between entities to images that imply the same meaning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: DALL.E 2
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first model we shall explore is *DALL.E 2*, a model designed by OpenAI for
    text-to-image generation. The first version of this model, DALL.E,^([1](ch13.xhtml#idm45387001413744))
    was released in February 2021 and sparked a new wave of interest in generative
    multimodal models. In this section, we shall investigate the workings of the second
    iteration of the model, DALL.E 2,^([2](ch13.xhtml#idm45387001411696)) released
    just over a year later in April 2022.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: DALL.E 2 is an extremely impressive model that has furthered our understanding
    of AI’s ability to solve these types of multimodal problems. It not only has ramifications
    academically, but also forces us to ask big questions relating to the role of
    AI in creative processes that previously were thought to be unique to humans.
    We will start by exploring how DALL.E 2 works, building on key foundational ideas
    that we have already explored earlier in this book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: DALL.E 2是一个非常令人印象深刻的模型，进一步增进了我们对AI解决这类多模态问题能力的理解。它不仅在学术上具有影响力，还迫使我们提出与AI在创造性过程中的角色有关的重大问题，这些问题以前被认为是人类独有的。我们将从探索DALL.E
    2的工作方式开始，建立在本书前面已经探讨过的关键基本思想之上。
- en: Architecture
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: To understand how DALL.E 2 works, we must first survey its overall architecture,
    as shown in [Figure 13-2](#dalle_arch).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解DALL.E 2的工作原理，我们必须首先了解其整体架构，如[图13-2](#dalle_arch)所示。
- en: '![](Images/gdl2_1302.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1302.png)'
- en: Figure 13-2\. The DALL.E 2 architecture
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-2\. DALL.E 2架构
- en: 'There are three distinct parts to consider: the *text encoder*, the *prior*,
    and the *decoder*. Text is first passed through the text encoder to produce a
    text embedding vector. This vector is then transformed by the prior to produce
    an image embedding vector. Finally, this is passed through the decoder, along
    with the original text, to produce the generated image. We will step through each
    component in turn, to get a complete picture of how DALL.E 2 works in practice.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个不同的部分需要考虑：*文本编码器*、*先验*和*解码器*。文本首先通过文本编码器传递，以产生文本嵌入向量。然后，该向量通过先验进行转换，以产生图像嵌入向量。最后，这通过解码器传递，连同原始文本，以生成图像。我们将依次逐个步骤地介绍每个组件，以全面了解DALL.E
    2在实践中的工作方式。
- en: The Text Encoder
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本编码器
- en: The aim of the text encoder is to convert the text prompt into an embedding
    vector that represents the conceptual meaning of the text prompt within a latent
    space. As we have seen in previous chapters, converting discrete text to a continuous
    latent space vector is essential for all downstream tasks, because we can continue
    to manipulate the vector further depending on our particular goal.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码器的目的是将文本提示转换为表示文本提示概念含义的嵌入向量，该向量位于潜在空间内。正如我们在前几章中所看到的，将离散文本转换为连续潜在空间向量对于所有下游任务都是至关重要的，因为我们可以根据特定目标进一步操纵向量。
- en: In DALL.E 2, the authors do not train the text encoder from scratch, but instead
    make use of an existing model called *Contrastive Language–Image Pre-training*
    (CLIP), also produced by OpenAI. Therefore, to understand the text encoder, we
    must first understand how CLIP works.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在DALL.E 2中，作者并不是从头开始训练文本编码器，而是利用了一个名为*对比语言-图像预训练*（CLIP）的现有模型，也是由OpenAI制作的。因此，要理解文本编码器，我们必须首先了解CLIP的工作原理。
- en: CLIP
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIP
- en: '[CLIP](https://openai.com/blog/clip)^([3](ch13.xhtml#idm45387001394816)) was
    unveiled in a paper published by OpenAI in February 2021 (just a few days after
    the first DALL.E paper) that described it as “a neural network that efficiently
    learns visual concepts from natural language supervision.”'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIP](https://openai.com/blog/clip)^([3](ch13.xhtml#idm45387001394816))是OpenAI于2021年2月发布的一篇论文中公布的（就在第一篇DALL.E论文发布几天后），该论文将其描述为“一种能够有效地从自然语言监督中学习视觉概念的神经网络。”'
- en: It uses a technique called *contrastive learning* to match images with text
    descriptions. The model is trained on a dataset of 400 million text–image pairs
    scraped from the internet—some example pairs are shown in [Figure 13-3](#clip_training_set).
    For comparison, there are 14 million hand-annotated images in ImageNet. Given
    an image and a list of possible text descriptions, its task is to find the one
    that actually matches the image.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '它使用一种称为*对比学习*的技术将图像与文本描述进行匹配。该模型在从互联网上抓取的4亿个文本-图像对数据集上进行训练——一些示例对显示在[图13-3](#clip_training_set)中。作为比较，ImageNet中有1400万个手动注释的图像。给定一幅图像和一组可能的文本描述，它的任务是找到实际与图像匹配的描述。 '
- en: '![](Images/gdl2_1303.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1303.png)'
- en: Figure 13-3\. Examples of text–image pairs
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-3\. 文本-图像对的示例
- en: 'The key idea behind contrastive learning is simple. We train two neural networks:
    a *text encoder* that converts text to a text embedding and an *image encoder*
    that converts an image to an image embedding. Then, given a batch of text–image
    pairs, we compare all text and image embedding combinations using *cosine similarity*
    and train the networks to maximize the score between matching text–image pairs
    and minimize the score between incorrect text–image pairs. This process is shown
    in [Figure 13-4](#clip_arch).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习背后的关键思想很简单。我们训练两个神经网络：一个*文本编码器*，将文本转换为文本嵌入，以及一个*图像编码器*，将图像转换为图像嵌入。然后，给定一批文本-图像对，我们使用*余弦相似度*比较所有文本和图像嵌入组合，并训练网络，以最大化匹配文本-图像对之间的分数，并最小化不正确的文本-图像对之间的分数。这个过程在[图13-4](#clip_arch)中显示。
- en: CLIP Is Not Generative
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP不是生成模型
- en: Note that CLIP is not itself a generative model—it cannot produce images or
    text. Is it closer to a discriminative model, because the final output is a prediction
    about which text description from a given set most closely matches a given image
    (or the other way around, which image most closely matches a given text description).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，CLIP本身不是生成模型——它不能生成图像或文本。它更接近于判别模型，因为最终输出是关于给定图像最接近哪个文本描述（或反之亦然，哪个图像最接近给定文本描述）的预测。
- en: '![](Images/gdl2_1304.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1304.png)'
- en: Figure 13-4\. The CLIP training process
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-4\. CLIP训练过程
- en: Both the text encoder and the image encoder are Transformers—the image encoder
    is a Vision Transformer (ViT), introduced in [“ViT VQ-GAN”](ch10.xhtml#Vit_VQ-GAN),
    which applies the same concept of attention to images. The authors tested other
    model architectures, but found this combination to produce the best results.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码器和图像编码器都是Transformer——图像编码器是Vision Transformer（ViT），在[“ViT VQ-GAN”](ch10.xhtml#Vit_VQ-GAN)中介绍，它将注意力的相同概念应用于图像。作者测试了其他模型架构，但发现这种组合产生了最好的结果。
- en: What makes CLIP especially interesting is the way it can be used for *zero-shot
    prediction* on tasks that it has never been exposed to. For example, suppose we
    want to use CLIP to predict the label of a given image in the ImageNet dataset.
    We can first convert the ImageNet labels into sentences by using a template (e.g.,
    “a photo of a <label>”), as shown in [Figure 13-5](#text_encode_imagenet).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP特别有趣的地方在于它可以用于对从未接触过的任务进行*零样本预测*。例如，假设我们想使用CLIP来预测ImageNet数据集中给定图像的标签。我们可以首先通过使用模板（例如“一张<标签>的照片”）将ImageNet标签转换为句子，如[图13-5](#text_encode_imagenet)所示。
- en: '![](Images/gdl2_1305.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1305.png)'
- en: Figure 13-5\. Converting labels in a new dataset to captions, in order to produce
    CLIP text embeddings
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-5\. 将新数据集中的标签转换为标题，以生成CLIP文本嵌入
- en: To predict the label of a given image, we can pass it through the CLIP image
    encoder and calculate the cosine similarity between the image embedding and all
    possible text embeddings in order to find the label with the maximum score, as
    shown in [Figure 13-6](#cosine_sim_clip).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测给定图像的标签，我们可以通过CLIP图像编码器传递图像，并计算图像嵌入与所有可能文本嵌入之间的余弦相似度，以找到得分最高的标签，如[图13-6](#cosine_sim_clip)所示。
- en: '![](Images/gdl2_1306.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1306.png)'
- en: Figure 13-6\. Using CLIP to predict the content of an image
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-6\. 使用CLIP预测图像内容
- en: Notice that we do not need to retrain either of the CLIP neural networks for
    it to be readily applicable to new tasks. It uses language as the common domain
    through which any set of labels can be expressed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们无需重新训练CLIP神经网络，即可将其应用于新任务。它使用语言作为一个通用领域，通过它可以表达任何一组标签。
- en: Using this approach, it is possible to show that CLIP performs well across a
    wide range of image dataset labeling challenges ([Figure 13-7](#clip_labelling)).
    Other models that have been trained on a specific dataset to predict a given set
    of labels often fail when applied to different datasets with the same labels because
    they are highly optimized to the individual datasets on which they were trained.
    CLIP is much more robust, as it has learned a deep conceptual understanding of
    full text descriptions and images, rather than just excelling at the narrow task
    of assigning a single label to a given image in a dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，可以证明CLIP在各种图像数据集标签挑战中表现良好（[图13-7](#clip_labelling)）。其他模型通常在应用于具有相同标签的不同数据集时失败，因为它们高度优化于它们训练的个别数据集。CLIP更加稳健，因为它学习了对完整文本描述和图像的深刻概念理解，而不仅仅擅长于将单个标签分配给给定数据集中的图像的狭窄任务。
- en: '![](Images/gdl2_1307.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1307.png)'
- en: 'Figure 13-7\. CLIP performs well on a wide range of image labeling datasets
    (source: [Radford et al., 2021](https://arxiv.org/abs/2103.00020))'
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-7\. CLIP在各种图像标签数据集上表现良好（来源：[Radford等人，2021](https://arxiv.org/abs/2103.00020)）
- en: As mentioned, CLIP is measured on its discriminative ability, so how does it
    help us to build generative models such as DALL.E 2?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CLIP是根据其区分能力来衡量的，那么它如何帮助我们构建生成模型，如DALL.E 2呢？
- en: The answer is that we can take the trained text encoder and use it as one part
    of a larger model such as DALL.E 2, with frozen weights. The trained encoder is
    simply a generalized model for converting text to a text embedding, which should
    be useful for downstream tasks such as generating images. The text encoder is
    able to capture a rich conceptual understanding of the text, as it has been trained
    to be as similar as possible to its matching image embedding counterpart, which
    is produced only from the paired image. It is therefore the first part of the
    bridge that we need to be able to cross over from the text domain to the image
    domain.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，我们可以将训练好的文本编码器作为DALL.E 2等更大模型的一部分，冻结权重。训练好的编码器只是一个将文本转换为文本嵌入的通用模型，对于生成图像等下游任务应该是有用的。文本编码器能够捕捉文本的丰富概念理解，因为它经过训练，使其尽可能与其匹配的图像嵌入对应物相似，后者仅由配对图像产生。因此，它是我们需要能够从文本领域跨越到图像领域的桥梁的第一部分。
- en: The Prior
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先验
- en: 'The next stage of the process involves converting the text embedding into a
    CLIP image embedding. The DALL.E 2 authors tried two different methods for training
    the prior model:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下一阶段的过程涉及将文本嵌入转换为CLIP图像嵌入。DALL.E 2的作者尝试了两种不同的方法来训练先验模型：
- en: An autoregressive model
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自回归模型
- en: A diffusion model
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型
- en: They found that the diffusion approach outperformed the autoregressive model
    and was more computationally efficient. In this section, we’ll look at both and
    see how they differ.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 他们发现扩散方法优于自回归模型，并且在计算效率上更高。在本节中，我们将看看两者的区别。
- en: Autoregressive prior
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自回归先验
- en: An autoregressive model generates output sequentially, by placing an ordering
    on the output tokens (e.g., words, pixels) and conditioning the next token on
    previous tokens. We have seen in previous chapters how this is used in recurrent
    neural networks (e.g., LSTMs), Transformers, and PixelCNN.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型按顺序生成输出，通过对输出标记（例如单词、像素）进行排序，并将下一个标记的生成条件放在前面的标记上。我们已经在之前的章节中看到了这在循环神经网络（例如LSTMs）、Transformer和PixelCNN中的应用。
- en: The autoregressive prior of DALL.E 2 is an encoder-decoder Transformer. It is
    trained to reproduce the CLIP image embedding given a CLIP text embedding, as
    shown in [Figure 13-8](#ar_prior). Note that there are some additional components
    to the autoregressive model mentioned in the original paper that we omit here
    for conciseness.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: DALL.E 2的自回归先验是一个编码器-解码器Transformer。它经过训练，可以在给定CLIP文本嵌入的情况下重现CLIP图像嵌入，如[图13-8](#ar_prior)所示。请注意，原始论文中提到了一些自回归模型的附加组件，为了简洁起见，我们在这里省略了。
- en: '![](Images/gdl2_1308.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1308.png)'
- en: Figure 13-8\. A simplified diagram of the autoregressive prior of DALL.E 2
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-8\. DALL.E 2的自回归先验的简化图
- en: 'The model is trained on the CLIP text–image pair dataset. You can think of
    it as the second part of the bridge that we need in order to jump from the text
    domain to the image domain: we are converting a vector from the text embedding
    latent space to the image embedding latent space.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在CLIP文本-图像对数据集上进行训练。您可以将其视为我们需要的桥梁的第二部分，以便从文本领域跳转到图像领域：我们正在将一个向量从文本嵌入潜在空间转换为图像嵌入潜在空间。
- en: The input text embedding is processed by the encoder of the Transformer to produce
    another representation that is fed to the decoder, alongside the current generated
    output image embedding. The output is generated one element at a time, using teacher
    forcing to compare the predicted next element to the actual CLIP image embedding.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文本嵌入由变压器的编码器处理，产生另一个表示，传递给解码器，同时传递当前生成的输出图像嵌入。输出是逐个元素生成的，使用教师强制来比较预测的下一个元素与实际的CLIP图像嵌入。
- en: The sequential nature of the generation means that the autoregressive model
    is less computationally efficient than the other method tried by the authors,
    which we’ll look at next.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的顺序性意味着自回归模型在计算效率上不如作者尝试的其他方法，接下来我们将看一下这些方法。
- en: Diffusion prior
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩散先验
- en: As we saw in [Chapter 8](ch08.xhtml#chapter_diffusion), diffusion models are
    fast becoming the go-to choice for generative modeling practitioners, alongside
    Transformers. In DALL.E 2 a decoder-only Transformer is used as the prior, trained
    using a diffusion process.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第8章](ch08.xhtml#chapter_diffusion)中看到的，扩散模型正迅速成为生成建模从业者的首选之一，与变压器并列。在DALL.E
    2中，一个仅使用解码器的变压器作为先验，通过扩散过程进行训练。
- en: The training and generation process is shown in [Figure 13-9](#diffusion_prior).
    Again, this is a simplified version; the original paper contains full details
    of how the diffusion model is structured.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和生成过程如[图13-9](#diffusion_prior)所示。再次强调，这是一个简化版本；原始论文包含了扩散模型结构的所有细节。
- en: '![](Images/gdl2_1309.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1309.png)'
- en: Figure 13-9\. A simplified diagram of the diffusion prior training and generation
    process of DALL.E 2
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-9。DALL.E 2扩散先验训练和生成过程的简化图示
- en: During training, each CLIP text and image embedding pair are first concatenated
    into a single vector. Then, the image embedding is noised over 1,000 timesteps
    until it is indistinguishable from random noise. The diffusion prior is then trained
    to predict the denoised image embedding at the previous timestep. The prior has
    access to the text embedding throughout, so it is able to condition its predictions
    on this information, gradually transforming the random noise into a predicted
    CLIP image embedding. The loss function is the average mean-squared error across
    denoising steps.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，每个CLIP文本和图像嵌入对首先被连接成一个单一向量。然后，图像嵌入在1,000个时间步长内被加入噪声，直到它与随机噪声无法区分。然后扩散先验被训练以预测上一个时间步长的去噪图像嵌入。先验在整个过程中都可以访问文本嵌入，因此能够根据这些信息对其预测进行条件化，逐渐将随机噪声转换为预测的CLIP图像嵌入。损失函数是去噪步骤中的平均均方误差。
- en: To generate new image embeddings, we sample a random vector, prepend the relevant
    text embedding, and pass it through the trained diffusion prior multiple times.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成新的图像嵌入，我们随机采样一个向量，将相关文本嵌入前置，并通过训练好的扩散先验多次传递。
- en: The Decoder
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器
- en: The final part of DALL.E 2 is the decoder. This is the part of the model that
    generates the final image conditioned on the text prompt and the predicted image
    embedding output by the prior.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: DALL.E 2的最后部分是解码器。这是模型的一部分，根据文本提示和先验输出的预测图像嵌入生成最终图像。
- en: The architecture and training process of the decoder borrows from an earlier
    OpenAI paper, published in December 2021, which presented a generative model called
    Guided Language to Image Diffusion for Generation and Editing (GLIDE).^([4](ch13.xhtml#idm45387001327744))
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的架构和训练过程借鉴了早前OpenAI发表的一篇论文，该论文于2021年12月发表，介绍了一种名为Guided Language to Image
    Diffusion for Generation and Editing (GLIDE)的生成模型。^([4](ch13.xhtml#idm45387001327744))
- en: GLIDE is able to generate realistic images from text prompts, in much the same
    way that DALL.E 2 can. The difference is that GLIDE does not make use of CLIP
    embeddings, but instead works directly with the raw text prompt, training the
    entire model from scratch, as shown in [Figure 13-10](#glide_dalle).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GLIDE能够从文本提示中生成逼真的图像，这与DALL.E 2的工作方式非常相似。不同之处在于GLIDE不使用CLIP嵌入，而是直接使用原始文本提示进行训练，从头开始训练整个模型，如[图13-10](#glide_dalle)所示。
- en: '![](Images/gdl2_1310.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1310.png)'
- en: Figure 13-10\. A comparison between DALL.E 2 and GLIDE—GLIDE trains the entire
    generative model from scratch, whereas DALL.E 2 makes use of CLIP embeddings to
    carry information forward from the initial text prompt
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-10。DALL.E 2和GLIDE之间的比较—GLIDE从头开始训练整个生成模型，而DALL.E 2利用CLIP嵌入将信息从初始文本提示传递下去
- en: Let’s see how GLIDE works first.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看GLIDE是如何工作的。
- en: GLIDE
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GLIDE
- en: GLIDE is trained as a diffusion model, with U-Net architecture for the denoiser
    and Transformer architecture for the text encoder. It learns to undo the noise
    added to an image, guided by the text prompt. Finally, an *Upsampler* is trained
    to scale the generated image to 1,024 × 1,024 pixels.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: GLIDE作为一个扩散模型进行训练，使用U-Net架构作为去噪器，使用变压器架构作为文本编码器。它学会了根据文本提示消除添加到图像中的噪声。最后，一个*上采样器*被训练以将生成的图像缩放到1,024×1,024像素。
- en: GLIDE trains the 3.5 billion (B) parameter model from scratch—2.3B parameters
    for the visual part of the model (U-Net and Upsampler) and 1.2B for the Transformer.
    It is trained on 250 million text–image pairs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GLIDE从头开始训练35亿（B）参数模型—模型的视觉部分（U-Net和上采样器）有23亿参数，变压器有12亿参数。它在2.5亿文本-图像对上进行训练。
- en: The diffusion process is shown in [Figure 13-11](#glide_diffusion). A Transformer
    is used to create an embedding of the input text prompt, which is then used to
    guide the U-Net throughout the denoising process. We explored the U-Net architecture
    in [Chapter 8](ch08.xhtml#chapter_diffusion); it’s a perfect model choice when
    the overall size of the image should stay the same (e.g., for style transfer,
    denoising, etc.).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散过程如[图13-11](#glide_diffusion)所示。使用Transformer创建输入文本提示的嵌入，然后用于引导U-Net进行去噪过程。我们在[第8章](ch08.xhtml#chapter_diffusion)中探讨了U-Net架构；当图像的整体大小应保持不变时（例如，用于风格转移、去噪等），这是一个完美的模型选择。
- en: '![](Images/gdl2_1311.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1311.png)'
- en: Figure 13-11\. The GLIDE diffusion process
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-11。GLIDE扩散过程
- en: The DALL.E 2 decoder still uses the U-Net denoiser and Transformer text encoder
    architectures, but additionally has the predicted CLIP image embeddings to condition
    on. This is the key difference between GLIDE and DALL.E 2, as shown in [Figure 13-12](#decoder_diffusion).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: DALL.E 2解码器仍然使用U-Net去噪器和Transformer文本编码器架构，但另外还有预测的CLIP图像嵌入来进行条件。这是GLIDE和DALL.E
    2之间的关键区别，如[图13-12](#decoder_diffusion)所示。
- en: '![](Images/gdl2_1312.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1312.png)'
- en: Figure 13-12\. The DALL.E 2 decoder additionally conditions on the image embedding
    produced by the prior
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-12。DALL.E 2解码器还额外依赖于先验产生的图像嵌入
- en: As with all diffusion models, to generate a new image, we simply sample some
    random noise and run this through the U-Net denoiser multiple times, conditioned
    on the Transformer text encoding and image embedding. The output is a 64 × 64–pixel
    image.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有扩散模型一样，要生成新图像，我们只需对一些随机噪声进行多次U-Net去噪，条件是Transformer文本编码和图像嵌入。输出是一个64×64像素的图像。
- en: Upsampler
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上采样器
- en: The final part of the decoder is the Upsampler (two separate diffusion models).
    The first diffusion model transforms the image from 64 × 64 to 256 × 256 pixels.
    The second transforms it again, from 256 × 256 to 1,024 × 1,024 pixels, as shown
    in [Figure 13-13](#decoder_upsampler).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的最后部分是上采样器（两个单独的扩散模型）。第一个扩散模型将图像从64×64转换为256×256像素。第二个再次转换，从256×256到1,024×1,024像素，如[图13-13](#decoder_upsampler)所示。
- en: Upsampling is useful because it means we do not have to build large upstream
    models to handle high-dimensional images. We can work with small images until
    the final stages of the process, when we apply the Upsamplers. This saves on model
    parameters and ensures a more efficient upstream training process.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样很有用，因为这意味着我们不必构建处理高维图像的大型上游模型。我们可以在整个过程的最后阶段之前使用小图像，然后应用上采样器。这节省了模型参数，并确保更高效的上游训练过程。
- en: '![](Images/gdl2_1313.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1313.png)'
- en: Figure 13-13\. The first Upsampler diffusion model converts the image from 64
    × 64 pixels to 256 × 256 pixels while the second converts from 256 × 256 pixels
    to 1,024 × 1,024 pixels
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-13。第一个Upsampler扩散模型将图像从64×64像素转换为256×256像素，而第二个将图像从256×256像素转换为1,024×1,024像素
- en: This concludes the DALL.E 2 model explanation! In summary, DALL.E 2 makes use
    of the pre-trained CLIP model to immediately produce a text embedding of the input
    prompt. Then it converts this into an image embedding using a diffusion model
    called the prior. Lastly, it implements a GLIDE-style diffusion model to generate
    the output image, conditioned on the predicted image embedding and Transformer-encoded
    input prompt.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是DALL.E 2模型的解释！总之，DALL.E 2利用预训练的CLIP模型立即生成输入提示的文本嵌入。然后使用称为先验的扩散模型将其转换为图像嵌入。最后，它实现了一个GLIDE风格的扩散模型，以生成输出图像，条件是预测的图像嵌入和Transformer编码的输入提示。
- en: Examples from DALL.E 2
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DALL.E 2的示例
- en: Examples of more images generated by DALL.E 2 can be found on [the official
    website](https://openai.com/dall-e-2). The way that the model is able to combine
    complex, disparate concepts in a realistic, believable way is astonishing and
    represents a significant leap forward for AI and generative modeling.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[官方网站](https://openai.com/dall-e-2)上找到DALL.E 2生成的更多图像示例。该模型能够以令人惊讶的方式将复杂、不同的概念结合在一起，以一种现实、可信的方式，这代表了AI和生成建模的重大进步。
- en: In the paper, the authors show how the model can be used for additional purposes
    other than text-to-image generation. One of these applications is creating variations
    of a given image, which we explore in the following section.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，作者展示了该模型可以用于除文本到图像生成之外的其他目的。其中一个应用是创建给定图像的变化，我们将在下一节中探讨。
- en: Image variations
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像变化
- en: As discussed previously, to generate images using the DALL.E 2 decoder we sample
    an image consisting of pure random noise and then gradually reduce the amount
    of noise using the denoising diffusion model, conditioned on the provided image
    embedding. Selecting different initial random noise samples will result in different
    images.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用DALL.E 2解码器生成图像时，我们对由纯随机噪声组成的图像进行采样，然后逐渐减少噪声量，使用依赖于提供的图像嵌入的去噪扩散模型。选择不同的初始随机噪声样本将导致不同的图像。
- en: In order to generate variations of a given image, we therefore just need to
    establish its image embedding to feed to the decoder. We can obtain this using
    the original CLIP image encoder, which is explicitly designed to convert an image
    into its CLIP image embedding. This process is shown in [Figure 13-14](#dalle_image_variations).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成给定图像的变化，我们只需要建立其图像嵌入以供解码器使用。我们可以使用原始的CLIP图像编码器来获得这个，它专门设计用于将图像转换为其CLIP图像嵌入。这个过程如[图13-14](#dalle_image_variations)所示。
- en: '![](Images/gdl2_1314.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1314.png)'
- en: Figure 13-14\. DALL.E 2 can be used for generating variations of a given image
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-14。DALL.E 2可用于生成给定图像的变化
- en: Importance of the prior
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先验的重要性
- en: Another avenue explored by the authors is establishing the importance of the
    prior. The purpose of the prior is to provide the decoder with a useful representation
    of the image to be generated, making use of the pre-trained CLIP model. However,
    it is feasible that this step isn’t necessary—perhaps we could just pass the text
    embedding directly to the decoder instead of the image embedding, or ignore the
    CLIP embeddings completely and condition only on the text prompt. Would this impact
    the quality of the generations?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 作者探索的另一条途径是建立先验的重要性。先验的目的是为解码器提供一个有用的图像表示，利用预训练的CLIP模型。然而，这一步骤可能是不必要的——也许我们可以直接将文本嵌入传递给解码器，而不是图像嵌入，或者完全忽略CLIP嵌入，只根据文本提示进行条件化。这会影响生成的质量吗？
- en: 'To test this, the authors tried three different approaches:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这一点，作者尝试了三种不同的方法：
- en: Feed the decoder only with the text prompt (and a zero vector for the image
    embedding).
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只将文本提示（以及图像嵌入的零向量）提供给解码器。
- en: Feed the decoder with the text prompt and the text embedding (as if it were
    an image embedding).
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本提示和文本嵌入（就像它是图像嵌入一样）提供给解码器。
- en: Feed the decoder with the text prompt and the image embedding (i.e., the full
    model).
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本提示和图像嵌入（即完整模型）提供给解码器。
- en: Example results are shown in [Figure 13-15](#dalle_prior_importance). We can
    see that when the decoder is starved of image embedding information, it can only
    produce a rough approximation of the text prompt, missing key information such
    as the calculator. Using the text embedding as if it were an image embedding performs
    slightly better, though it is not able to capture the relationship between the
    hedgehog and the calculator. Only the full model with the prior produces an image
    that accurately reflects all of the information contained within the prompt.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 示例结果显示在[图13-15](#dalle_prior_importance)中。我们可以看到，当解码器缺乏图像嵌入信息时，它只能产生文本提示的粗略近似，缺少关键信息，如计算器。将文本嵌入视为图像嵌入稍微好一些，尽管它无法捕捉刺猬和计算器之间的关系。只有带有先验的完整模型才能产生准确反映提示中包含的所有信息的图像。
- en: '![](Images/gdl2_1315.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1315.png)'
- en: 'Figure 13-15\. The prior provides the model with additional context and helps
    the decoder to produce more accurate generations (source: [Ramesh et al., 2022](https://arxiv.org/pdf/2204.06125.pdf))'
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-15。先验为模型提供了额外的上下文，并帮助解码器产生更准确的生成物（来源：[Ramesh等人，2022](https://arxiv.org/pdf/2204.06125.pdf)）
- en: Limitations
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制
- en: In the DALL.E 2 paper, the authors also highlight several known limitations
    of the model. Two of these (attribute binding and text generation) are shown in
    [Figure 13-16](#dalle_limitations).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在DALL.E 2论文中，作者还强调了模型的几个已知限制。其中两个（属性绑定和文本生成）显示在[图13-16](#dalle_limitations)中。
- en: '![](Images/gdl2_1316.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1316.png)'
- en: 'Figure 13-16\. Two limitations of DALL.E 2 lie in its ability to bind attributes
    to objects and reproduce textual information—top prompt: “A red cube on top of
    a blue cube”; bottom prompt: “A sign that says deep learning” (source: [Ramesh
    et al., 2022](https://arxiv.org/pdf/2204.06125.pdf))'
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-16。DALL.E 2的两个限制在于其将属性绑定到对象和再现文本信息的能力——顶部提示：“一个红色立方体放在一个蓝色立方体上”；底部提示：“一个写着深度学习的标志”（来源：[Ramesh等人，2022](https://arxiv.org/pdf/2204.06125.pdf)）
- en: '*Attribute binding* is the ability of a model to understand the relationship
    between words in a given text prompt, and in particular how attributes relate
    to objects. For example, the prompt “A red cube on top of a blue cube” must appear
    visually distinct from “A blue cube on top of a red cube.” DALL.E struggles somewhat
    with this, compared to earlier models such as GLIDE, though the overall quality
    of generations is better and more diverse.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*属性绑定*是模型理解给定文本提示中单词之间关系的能力，特别是属性如何与对象相关联。例如，提示“一个红色立方体放在一个蓝色立方体上”在视觉上必须与“一个蓝色立方体放在一个红色立方体上”有明显区别。与之前的模型（如GLIDE）相比，DALL.E在这方面有些困难，尽管生成的整体质量更好且更多样化。'
- en: Also, DALL.E 2 is not able to accurately reproduce text—this is probably due
    to the fact that the CLIP embeddings do not capture spellings, but instead only
    contain a higher-level representation of the text. These representations can be
    decoded into text with partial success (e.g., individual letters are mostly correct),
    but not with enough compositional understanding to form full words.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DALL.E 2无法准确再现文本——这可能是因为CLIP嵌入不捕捉拼写，而只包含文本的更高级表示。这些表示可以部分成功地解码为文本（例如，单个字母大多正确），但没有足够的组合理解来形成完整的单词。
- en: Imagen
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Imagen
- en: 'Just over a month after OpenAI released DALL.E 2, the Google Brain team released
    their own text-to-image model called Imagen.^([5](ch13.xhtml#idm45387001262144))
    Many of the core themes that we have already explored in this chapter are also
    relevant to Imagen: for example, it uses a text encoder and a diffusion model
    decoder.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI发布DALL.E 2一个多月后，Google Brain团队发布了他们自己的文本到图像模型称为Imagen。我们在本章中已经探讨的许多核心主题也与Imagen相关：例如，它使用文本编码器和扩散模型解码器。
- en: In the next section, we’ll explore the overall architecture of Imagen and compare
    it with DALL.E 2.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探讨Imagen的整体架构，并将其与DALL.E 2进行比较。
- en: Architecture
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: An overview of the Imagen architecture is shown in [Figure 13-17](#imagen_arch).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Imagen架构的概述显示在[图13-17](#imagen_arch)中。
- en: '![](Images/gdl2_1317.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1317.png)'
- en: 'Figure 13-17\. The Imagen architecture (source: [Saharia et al., 2022](https://arxiv.org/abs/2205.11487))'
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-17。Imagen架构（来源：[Saharia等人，2022](https://arxiv.org/abs/2205.11487)）
- en: The frozen text encoder is the pre-trained T5-XXL model, a large encoder-decoder
    Transformer. Unlike CLIP, this was trained only on text and not images, so it
    is not a multimodal model. However, the authors found that it still functions
    extremely well as a text encoder for Imagen and that scaling this model has more
    impact on overall performance than scaling the diffusion model decoder.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结文本编码器是预训练的T5-XXL模型，一个大型编码器-解码器Transformer。与CLIP不同，这个模型仅在文本上进行训练，而不是图像，因此它不是一个多模态模型。然而，作者发现它仍然在Imagen中作为文本编码器表现非常出色，并且扩展这个模型对整体性能的影响比扩展扩散模型解码器更大。
- en: Like DALL.E 2’s, Imagen’s the decoding diffusion model is based on a U-Net architecture,
    conditioned on text embeddings. There are several architectural improvements made
    to the standard U-Net architecture, to produce what the authors call the *Efficient
    U-Net*. This model uses less memory, converges faster, and has better sample quality
    than previous U-Net models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与DALL.E 2一样，Imagen的解码扩散模型基于U-Net架构，以文本嵌入为条件。对标准U-Net架构进行了几项架构改进，以产生作者称之为*高效U-Net*的模型。该模型使用更少的内存，收敛更快，并且比以前的U-Net模型具有更好的样本质量。
- en: The Upsampler super-resolution models that take the generated image from 64
    × 64 to 1,024 × 1,024 pixels are also diffusion models that continue to use the
    text embeddings to guide the upsampling process.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成的图像从64×64像素升级到1,024×1,024像素的上采样器超分辨率模型也是扩散模型，继续使用文本嵌入来指导上采样过程。
- en: DrawBench
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DrawBench
- en: 'An additional contribution of the Imagen paper is *DrawBench*—a suite of 200
    text prompts for text-to-image evaluation. The text prompts cover 11 categories,
    such as *Counting* (ability to generate a specified number of objects), *Description*
    (ability to generate complex and long text prompts describing objects), and *Text*
    (ability to generate quoted text). To compare two models, the DrawBench text prompts
    are passed through each model and the outputs given to a panel of human raters
    for evaluation across two metrics:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Imagen论文的另一个贡献是*DrawBench*——一个包含200个文本提示的套件，用于文本到图像的评估。文本提示涵盖11个类别，如*计数*（生成指定数量的对象的能力）、*描述*（生成描述对象的复杂和长文本提示的能力）和*文本*（生成引用文本的能力）。为了比较两个模型，DrawBench文本提示通过每个模型，并将输出交给一组人类评分员进行评估，评估涵盖两个指标：
- en: Alignment
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐
- en: Which image more accurately describes the caption?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 哪张图更准确地描述了标题？
- en: Fidelity
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 保真度
- en: Which image is more photorealistic (looks more real)?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 哪张图更逼真（看起来更真实）？
- en: The results from the DrawBench human evaluation are shown in [Figure 13-18](#imagen_evaluation).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: DrawBench人类评估的结果显示在[图13-18](#imagen_evaluation)中。
- en: Both DALL.E 2 and Imagen are remarkable models that have made significant contributions
    to the field of text-to-image generation. Whilst Imagen outperforms DALL.E 2 on
    many of the DrawBench benchmarks, DALL.E 2 provides additional functionalities
    that are not present in Imagen. For example, because DALL.E 2 utilizes CLIP (a
    multimodal text–image model), it is able to accept images as input to generate
    image embeddings. This means DALL.E 2 is able to provide image editing and image
    variation capabilities. This is not possible with Imagen; the text encoder is
    a pure text model, so there is no way to input an image.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: DALL.E 2和Imagen都是在文本到图像生成领域做出了重大贡献的显著模型。虽然Imagen在许多DrawBench基准测试中表现优于DALL.E
    2，但DALL.E 2提供了Imagen中没有的额外功能。例如，因为DALL.E 2利用了CLIP（一个多模态文本-图像模型），它能够接受图像作为输入来生成图像嵌入。这意味着DALL.E
    2能够提供图像编辑和图像变化的功能。这在Imagen中是不可能的；文本编码器是一个纯文本模型，因此无法输入图像。
- en: '![](Images/gdl2_1318.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1318.png)'
- en: 'Figure 13-18\. Comparison of Imagen and DALL.E 2 on DrawBench across alignment
    and image fidelity (source: [Saharia et al., 2022](https://arxiv.org/abs/2205.11487))'
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-18\. 在对齐和图像保真度方面比较Imagen和DALL.E 2（来源：[Saharia等人，2022](https://arxiv.org/abs/2205.11487)）
- en: Examples from Imagen
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Imagen的示例
- en: Example Imagen generations are shown in [Figure 13-19](#imagen_generations).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 示例Imagen生成显示在[图13-19](#imagen_generations)中。
- en: '![](Images/gdl2_1319.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1319.png)'
- en: 'Figure 13-19\. Example Imagen generations (source: [Saharia et al., 2022](https://arxiv.org/abs/2205.11487))'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-19\. 示例Imagen生成（来源：[Saharia等人，2022](https://arxiv.org/abs/2205.11487)）
- en: Stable Diffusion
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定扩散
- en: The last text-to-image diffusion model that we shall explore is *Stable Diffusion*,
    released in August 2022 by [Stability AI](https://stability.ai), in collaboration
    with the [Computer Vision and Learning research group at Ludwig Maximilian University
    of Munich](https://ommer-lab.com) and [Runway](https://runwayml.com). It is different
    from DALL.E 2 and Imagen in that its code and model weights have been released
    publicly, through [Hugging Face](https://oreil.ly/BTrWI). This means that anyone
    can interact with the model on their own hardware, without having to use proprietary
    APIs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨的最后一个文本到图像扩散模型是*稳定扩散*，由[Stability AI](https://stability.ai)于2022年8月发布，与[慕尼黑路德维希·马克西米利安大学计算机视觉与学习研究小组](https://ommer-lab.com)和[Runway](https://runwayml.com)合作。它与DALL.E
    2和Imagen不同，因为它的代码和模型权重已经通过[Hugging Face](https://oreil.ly/BTrWI)公开发布。这意味着任何人都可以在自己的硬件上与模型互动，而无需使用专有API。
- en: Architecture
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: The main architectural difference between Stable Diffusion and the text-to-image
    models discussed previously is that it uses *latent diffusion* as its underlying
    generative model. Latent diffusion models (LDMs) were introduced by Rombach et
    al. in December 2021, in the paper “High-Resolution Image Synthesis with Latent
    Diffusion Models.”^([6](ch13.xhtml#idm45387001217376)) The key idea from the paper
    is to wrap the diffusion model within an autoencoder, so that the diffusion process
    operates on a latent space representation of the image rather than the image itself,
    as shown in [Figure 13-20](#stablediffusion).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散和之前讨论的文本到图像模型之间的主要架构差异在于它使用*潜在扩散*作为其基础生成模型。潜在扩散模型（LDMs）是由Rombach等人在2021年12月提出的，在论文“使用潜在扩散模型进行高分辨率图像合成”中。^([6](ch13.xhtml#idm45387001217376))
    该论文的关键思想是将扩散模型包装在一个自动编码器中，使得扩散过程在图像的潜在空间表示上运行，而不是在图像本身上运行，如[图13-20](#stablediffusion)所示。
- en: '![](Images/gdl2_1320.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1320.png)'
- en: Figure 13-20\. The Stable Diffusion architecture
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-20\. 稳定扩散架构
- en: This breakthrough means that the denoising U-Net model can be kept relatively
    lightweight, in comparison to U-Net models that operate on full images. The autoencoder
    handles the heavy lifting of encoding the image detail into latent space and decoding
    the latent space back to a high-resolution image, leaving the diffusion model
    to work purely in a latent, conceptual space. This gives a significant speed and
    performance boost to the training process.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这一突破意味着去噪U-Net模型相对轻量化，与操作完整图像的U-Net模型相比。自动编码器处理将图像细节编码到潜在空间并将潜在空间解码回高分辨率图像的繁重工作，使扩散模型纯粹在潜在的概念空间中工作。这为训练过程带来了显著的速度和性能提升。
- en: The denoising process can also optionally be guided by a text prompt that has
    been passed through a text encoder. The first version of Stable Diffusion utilized
    the pre-trained CLIP model from OpenAI (the same as in DALL.E 2), but Stable Diffusion
    2 has a custom trained CLIP model called [OpenCLIP](https://oreil.ly/RaCbu), which
    has been trained from scratch.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪过程也可以选择由通过文本编码器传递的文本提示引导。稳定扩散的第一个版本使用了OpenAI的预训练CLIP模型（与DALL.E 2中相同），但稳定扩散2使用了一个名为[OpenCLIP](https://oreil.ly/RaCbu)的自定义训练的CLIP模型，该模型是从头开始训练的。
- en: Examples from Stable Diffusion
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稳定扩散的示例
- en: '[Figure 13-21](#stablediffusionexamples) shows some example outputs from Stable
    Diffusion 2.1—you can try your own prompts through the model hosted on [Hugging
    Face](https://oreil.ly/LpGW4).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-21](#stablediffusionexamples)展示了稳定扩散2.1的一些示例输出—您可以通过[Hugging Face](https://oreil.ly/LpGW4)上托管的模型尝试自己的提示。'
- en: '![](Images/gdl2_1321.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1321.png)'
- en: Figure 13-21\. Example outputs from Stable Diffusion 2.1
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-21\. 稳定扩散2.1的示例输出
- en: Exploring the Latent Space
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索潜在空间
- en: If you’d like to explore the latent space of the Stable Diffusion model, I highly
    recommended the [walkthrough](https://oreil.ly/4sNe5) on the Keras website.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想探索稳定扩散模型的潜在空间，我强烈推荐在Keras网站上进行的[演练](https://oreil.ly/4sNe5)。
- en: Flamingo
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flamingo
- en: So far we have looked at three different kinds of text-to-image models. In this
    section, we’ll explore a multimodal model that generates text given a stream of
    text and visual data. Flamingo, introduced in a paper by DeepMind in April 2022,^([7](ch13.xhtml#idm45387001198400))
    is a family of visual language models (VLMs) that act as a bridge between pre-trained
    vision-only and language-only models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过三种不同类型的文本到图像模型。在本节中，我们将探索一种多模态模型，它可以根据文本和视觉数据流生成文本。Flamingo是DeepMind在2022年4月发表的一篇论文中介绍的，^([7](ch13.xhtml#idm45387001198400))是一系列视觉语言模型（VLMs），作为预训练的仅视觉和仅语言模型之间的桥梁。
- en: In this section, we’ll run through the architecture of Flamingo models and compare
    them to the text-to-image models we have seen so far.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍Flamingo模型的架构，并将其与我们迄今为止看到的文本到图像模型进行比较。
- en: Architecture
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: The overall architecture of Flamingo is shown in [Figure 13-22](#flamingo_arch).
    For conciseness, we shall explore the core components of this model—the Vision
    Encoder, the Perceiver Resampler, and the Language Mode—in just enough detail
    to highlight the key ideas that make Flamingo unique. I highly recommend reading
    the original research paper for a thorough review of each part of the model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo的整体架构显示在[图13-22](#flamingo_arch)中。为了简洁起见，我们将仅探讨该模型的核心组件—视觉编码器、感知器重采样器和语言模式—以足够的细节来突出使Flamingo独特的关键思想。我强烈建议阅读原始研究论文，对模型的每个部分进行彻底审查。
- en: '![](Images/gdl2_1322.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1322.png)'
- en: 'Figure 13-22\. The Flamingo architecture (source: [Alayrac et al., 2022](https://arxiv.org/pdf/2204.14198.pdf))'
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-22\. Flamingo架构（来源：[Alayrac等人，2022](https://arxiv.org/pdf/2204.14198.pdf)）
- en: The Vision Encoder
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉编码器
- en: The first difference between a Flamingo model and pure text-to-image models
    such as DALL.E 2 and Imagen is that Flamingo can accept a combination of text
    and visual data interleaved. Here, *visual data* includes videos as well as images.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo模型与纯文本到图像模型（如DALL.E 2和Imagen）之间的第一个区别是，Flamingo可以接受交错的文本和视觉数据的组合。这里，*视觉数据*包括视频和图像。
- en: 'The job of the Vision Encoder is to convert the vision data within the input
    into embedding vectors (similar to the image encoder in CLIP). The Vision Encoder
    in Flamingo is a pre-trained Normalizer-Free ResNet (NFNet), as introduced by
    Brock et al. in 2021^([8](ch13.xhtml#idm45387001185712))—in particular, an NFNet-F6
    (the NFNet models range from F0 to F6, increasing in size and power). This is
    one key difference between the CLIP image encoder and the Flamingo Vision Encoder:
    the former uses a ViT architecture, whereas the latter uses a ResNet architecture.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉编码器的工作是将输入中的视觉数据转换为嵌入向量（类似于CLIP中的图像编码器）。Flamingo中的视觉编码器是一个预训练的无归一化ResNet（NFNet），由Brock等人在2021年介绍^([8](ch13.xhtml#idm45387001185712))—具体来说，是一个NFNet-F6（NFNet模型从F0到F6，大小和功率逐渐增加）。这是CLIP图像编码器和Flamingo视觉编码器之间的一个关键区别：前者使用ViT架构，而后者使用ResNet架构。
- en: The Vision Encoder is trained on image-text pairs using the same contrastive
    objective as introduced in the CLIP paper. After training, the weights are frozen
    so that any further training of the Flamingo model does not affect the weights
    of the Vision Encoder.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉编码器是使用与CLIP论文中引入的对比目标相同的图像-文本对进行训练的。训练后，权重被冻结，以便对Flamingo模型的任何进一步训练不会影响视觉编码器的权重。
- en: The output from the Vision Encoder is a 2D grid of features that then gets flattened
    to a 1D vector before being passed to the Perceiver Resampler. Video is handled
    by sampling at 1 frame per second and passing each snapshot through the Vision
    Encoder independently to produce several feature grids; learned temporal encodings
    are then added in before flattening the features and concatenating the results
    into a single vector.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉编码器的输出是一个特征的2D网格，然后在传递给Perceiver Resampler之前被展平为1D向量。视频通过每秒采样1帧处理，并将每个快照独立通过视觉编码器传递以产生几个特征网格；然后在展平特征之前添加了学习的时间编码，并将结果连接成一个单一向量。
- en: The Perceiver Resampler
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Perceiver Resampler
- en: Memory requirements in a traditional encoder Transformer (e.g., BERT) scale
    quadratically with input sequence length, which is why input sequences are normally
    capped at a set number of tokens (e.g., 512 in BERT). However, the output from
    the Vision Encoder is a vector of variable length (due to the variable input image
    resolution and the variable number of video frames) and is therefore potentially
    very long.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 传统编码器Transformer（例如BERT）中的内存需求随着输入序列长度呈二次方增长，这就是为什么输入序列通常被限制在一定数量的标记上（例如BERT中的512个）。然而，视觉编码器的输出是一个长度可变的向量（由于可变的输入图像分辨率和可变的视频帧数），因此可能非常长。
- en: The Perceiver architecture is specifically designed to efficiently handle long
    input sequences. Instead of performing self-attention on the full input sequence,
    it works with a fixed-length latent vector and only uses the input sequence for
    cross-attention. Specifically, in the Flamingo Perceiver Resampler, the *key*
    and *value* are a concatenation of the input sequence and latent vector and the
    *query* is the latent vector alone. A diagram of the Vision Encoder and Perceiver
    Resampler process for video data is shown in [Figure 13-23](#flamingo_perceiver).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Perceiver架构专门设计用于高效处理长输入序列。它不是对整个输入序列执行自注意力，而是使用固定长度的潜在向量，并仅将输入序列用于交叉注意力。具体来说，在Flamingo
    Perceiver Resampler中，*key*和*value*是输入序列和潜在向量的串联，而*query*仅是潜在向量本身。图13-23显示了视频数据的视觉编码器和Perceiver
    Resampler过程的图示。
- en: '![](Images/gdl2_1323.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/gdl2_1323.png)
- en: 'Figure 13-23\. The Perceiver Resampler applied to video input (source: [Alayrac
    et al., 2022](https://arxiv.org/pdf/2204.14198.pdf))'
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-23。应用于视频输入的Perceiver Resampler（来源：[Alayrac等人，2022](https://arxiv.org/pdf/2204.14198.pdf)）
- en: The output of the Perceiver Resampler is a fixed-length latent vector that gets
    passed to the Language Model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Perceiver Resampler的输出是一个固定长度的潜在向量，传递给语言模型。
- en: The Language Model
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型
- en: The Language Model consists of several stacked blocks, in the style of a decoder
    Transformer, that output a predicted text continuation. In fact, the majority
    of the Language Model is from a pre-trained DeepMind model called *Chinchilla*.
    The Chinchilla paper, published in March 2022,^([9](ch13.xhtml#idm45387001171728))
    showcases a language model that is designed to be considerably smaller than its
    peers (e.g., 70B parameters for Chinchilla compared to 170B for GPT-3), while
    using significantly more tokens for training. The authors show that the model
    outperforms larger models on a range of tasks, highlighting the importance of
    optimizing the trade-off between training a larger model and using a larger number
    of tokens during training.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型由几个堆叠的块组成，以解码器Transformer的风格输出预测的文本延续。事实上，语言模型的大部分来自一个名为*Chinchilla*的预训练DeepMind模型。2022年3月发表的Chinchilla论文^([9](ch13.xhtml#idm45387001171728))展示了一个设计得比同行要小得多的语言模型（例如，Chinchilla的参数为70B，而GPT-3的参数为170B），同时在训练中使用了更多标记。作者表明，该模型在一系列任务上优于更大的模型，突出了在训练更大的模型和在训练期间使用更多标记之间优化权衡的重要性。
- en: A key contribution of the Flamingo paper is to show how Chinchilla can be adapted
    to work with additional vision data (`X`) that is interspersed with the language
    data (`Y`). Let’s first explore how the language and vision input are combined
    to produce the input to the Language Model ([Figure 13-24](#flamingo_masked_cross_attention)).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo论文的一个关键贡献是展示了Chinchilla如何适应与插入语言数据（`Y`）一起工作的额外视觉数据（`X`）。让我们首先探讨语言和视觉输入是如何结合以产生语言模型的输入的（[图13-24](#flamingo_masked_cross_attention)）。
- en: First the text is processed by replacing vision data (e.g., images) with an
    `<image>` tag and the text is divided into *chunks* using the `<EOC>` (end of
    chunk) tag. Each chunk contains at most one image, which is always at the start
    of the chunk—i.e., the subsequent text is assumed to relate only to that image.
    The beginning of the sequence is also marked with the `<BOS>` (beginning of sentence)
    tag.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，文本通过用`<image>`标记替换视觉数据（例如图像），并使用`<EOC>`（块结束）标记将文本分成*块*。每个块最多包含一个图像，该图像始终位于块的开头，即假定后续文本仅与该图像相关。序列的开头也用`<BOS>`（句子开头）标记。
- en: Next, the sequence is tokenized and each token is given an index (`phi`) corresponding
    to the preceding image index (or `0` if there is no preceding image in the chunk).
    This way, the text tokens (`Y`) can be forced to only cross-attend to the image
    tokens (`X`) that correspond to their particular chunk, through masking. For example,
    in [Figure 13-24](#flamingo_masked_cross_attention) the first chunk contains no
    images, so all image tokens from the Perceiver Resampler are masked. The second
    chunk contains image 1, so these tokens are allowed to interact with the image
    tokens from image 1\. Likewise, the final chunk contains image 2, so these tokens
    are allowed to interact with the image tokens from image 2.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，序列被标记化，每个标记被赋予一个索引（`phi`），对应于前面的图像索引（如果在块中没有前置图像，则为`0`）。这样，文本标记（`Y`）可以被强制只与对应于其特定块的图像标记（`X`）进行交叉关注，通过掩蔽。例如，在[图13-24](#flamingo_masked_cross_attention)中，第一个块不包含图像，因此Perceiver
    Resampler的所有图像标记都被掩盖。第二个块包含图像1，因此这些标记可以与图像1的图像标记进行交互。同样，最后一个块包含图像2，因此这些标记可以与图像2的图像标记进行交互。
- en: '![](Images/gdl2_1324.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1324.png)'
- en: 'Figure 13-24\. Masked cross-attention (XATTN), combining vision and text data—light
    blue entries are masked and dark blue entries are nonmasked (source: [Alayrac
    et al., 2022](https://arxiv.org/pdf/2204.14198.pdf))'
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-24。掩蔽的交叉关注（XATTN），结合视觉和文本数据——浅蓝色条目被掩盖，深蓝色条目未被掩盖（来源：[Alayrac等人，2022](https://arxiv.org/pdf/2204.14198.pdf))
- en: We can now see how this masked cross-attention component fits into the overall
    architecture of the Language Model ([Figure 13-25](#flamingo_language_model)).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到这个掩蔽的交叉关注组件如何融入语言模型的整体架构中（[图13-25](#flamingo_language_model)）。
- en: The blue LM layer components are frozen layers from Chinchilla—these are not
    updated during the training process. The purple `GATED XATTN-DENSE` layers are
    trained as part of Flamingo and include the masked cross-attention components
    that blend the language and vision information, as well as subsequent feed-forward
    (dense) layers.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色的LM层组件是来自Chinchilla的冻结层，这些层在训练过程中不会更新。紫色的`GATED XATTN-DENSE`层作为Flamingo的一部分进行训练，包括混合语言和视觉信息的掩蔽交叉关注组件，以及随后的前馈（密集）层。
- en: The layer is *gated* because it passes the output from the cross-attention and
    feed-forward components through two distinct tanh gates, which are both initialized
    to zero. Therefore, when the network is initialized, there is no contribution
    from the `GATED XATTN-DENSE` layers—the language information is just passed straight
    through. The `alpha` gating parameters are learned by the network, to gradually
    blend in information from the vision data as training progresses.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该层是*门控*的，因为它通过两个不同的tanh门传递来自交叉关注和前馈组件的输出，这两个门都初始化为零。因此，当网络初始化时，`GATED XATTN-DENSE`层没有贡献——语言信息直接通过。`alpha`门控参数由网络学习，随着训练的进行逐渐混合视觉数据的信息。
- en: '![](Images/gdl2_1325.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1325.png)'
- en: 'Figure 13-25\. A Flamingo Language Model block, comprising a frozen language
    model layer from Chinchilla and a `GATED XATTN-DENSE` layer (source: [Alayrac
    et al., 2022](https://arxiv.org/pdf/2204.14198.pdf))'
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-25。Flamingo语言模型块，包括来自Chinchilla的冻结语言模型层和一个`GATED XATTN-DENSE`层（来源：[Alayrac等人，2022](https://arxiv.org/pdf/2204.14198.pdf)）
- en: Examples from Flamingo
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自Flamingo的例子
- en: Flamingo can be used for a variety of purposes, including image and video understanding,
    conversational prompting, and visual dialogue. In [Figure 13-26](#flamingo_examples)
    we can see a few examples of what Flamingo is capable of.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo可以用于各种目的，包括图像和视频理解，对话提示和视觉对话。在[图13-26](#flamingo_examples)中，我们可以看到Flamingo的一些示例。
- en: '![](Images/gdl2_1326.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1326.png)'
- en: 'Figure 13-26\. Examples of inputs and outputs obtained from the 80B parameter
    Flamingo model (source: [Alayrac et al., 2022](https://arxiv.org/pdf/2204.14198.pdf))'
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-26。从80B参数Flamingo模型获得的输入和输出示例（来源：[Alayrac等人，2022](https://arxiv.org/pdf/2204.14198.pdf)）
- en: Notice how in each example, Flamingo is blending information from the text and
    the images in true multimodal style. The first example uses images in place of
    words and is able to suggest an appropriate book to continue the prompt. The second
    example shows frames from a video, and Flamingo correctly identifies the consequence
    of the action. The last three examples all demonstrate how Flamingo can be used
    interactively, to provide additional information through dialogue or probe with
    further questioning.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在每个示例中，Flamingo以真正的多模式风格混合文本和图像信息。第一个示例使用图像代替文字，并能够建议一个适当的书籍来继续提示。第二个示例展示了视频中的帧，Flamingo正确地识别了行动的后果。最后三个示例都展示了Flamingo如何交互使用，通过对话提供额外信息或通过进一步提问进行探究。
- en: It is astonishing to see a machine being able to answer complex questions across
    such a wide range of modalities and input tasks. In the paper, the authors quantify
    Flamingo’s ability across a set of benchmark tasks and find that across many benchmarks,
    Flamingo is able to surpass the performance of models that have been tailored
    to specifically tackle the one task in question. This highlights how large multimodal
    models can be rapidly adapted to a wide range of tasks and paves the way for the
    development of AI agents that aren’t just tied to a single task, but instead are
    truly general agents that can be guided by the user at inference time.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 看到一台机器能够回答如此广泛的模态和输入任务范围内的复杂问题，真是令人惊讶。在论文中，作者们量化了Flamingo在一组基准任务上的能力，并发现在许多基准测试中，Flamingo能够超越专门针对特定任务的模型的性能。这突显了大型多模型可以迅速适应各种任务，并为开发不仅仅局限于单一任务的AI代理铺平了道路，而是真正可以在推理时由用户引导的通用代理。
- en: Summary
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter we have explored four different state-of-the-art multimodal
    models: DALL.E 2, Imagen, Stable Diffusion, and Flamingo.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了四种不同的最先进多模型：DALL.E 2，Imagen，Stable Diffusion和Flamingo。
- en: DALL.E 2 is a large-scale text-to-image model from OpenAI that can generate
    realistic images across a range of styles given a text prompt. It works by combining
    pre-trained models (e.g., CLIP) with diffusion model architectures from previous
    works (GLIDE). It also has additional capabilities, such as being able to edit
    images through text prompting and provide variations of a given image. While it
    does have some limitations, such as inconsistent text rendering and attribute
    binding, DALL.E 2 is an incredibly powerful AI model that has helped to propel
    the field of generative modeling into a new era.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: DALL.E 2是来自OpenAI的大规模文本到图像模型，可以根据文本提示生成各种风格的逼真图像。它通过将预训练模型（例如CLIP）与先前作品（GLIDE）中的扩散模型架构相结合来工作。它还具有额外的功能，例如能够通过文本提示编辑图像并提供给定图像的变体。尽管它存在一些限制，例如不一致的文本渲染和属性绑定，但DALL.E
    2是一个非常强大的AI模型，已经帮助推动生成建模领域进入一个新时代。
- en: Another model that has surpassed previous benchmarks is Imagen from Google Brain.
    This model shares many similarities with DALL.E 2, such as a text encoder and
    a diffusion model decoder. One of the key differences between the two models is
    that the Imagen text encoder is trained on pure text data, whereas the training
    process for the DALL.E 2 text encoder involves image data (through the contrastive
    CLIP learning objective). The authors show that this approach leads to state-of-the-art
    performance across a range of tasks, through their DrawBench evaluation suite.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个超越先前基准的模型是Google Brain的Imagen。这个模型与DALL.E 2有许多相似之处，例如文本编码器和扩散模型解码器。两个模型之间的一个关键区别是Imagen文本编码器是在纯文本数据上训练的，而DALL.E
    2文本编码器的训练过程涉及图像数据（通过对比CLIP学习目标）。作者表明，这种方法在各种任务中取得了最先进的性能，通过他们的DrawBench评估套件。
- en: Stable Diffusion is an open source offering from Stability AI, CompVis, and
    Runway. It is a text-to-image model whose model weights and code are freely available,
    so you can run it on your own hardware. Stable Diffusion is particularly fast
    and lightweight due to the use of a latent diffusion model that operates on the
    latent space of an autoencoder, rather than the images themselves.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散是来自Stability AI、CompVis和Runway的开源产品。这是一个文本到图像的模型，其模型权重和代码都是免费提供的，因此您可以在自己的硬件上运行它。稳定扩散特别快速和轻量，因为它使用了一个在自动编码器的潜在空间上运行的潜在扩散模型，而不是图像本身。
- en: Finally, DeepMind’s Flamingo is a visual language model—that is, it accepts
    a stream of interleaved text and visual data (images and video) and is able to
    continue the prompt with additional text, in the style of a decoder Transformer.
    The key contribution is showing how the visual information can be fed to the Transformer
    via a Visual Encoder and Perceiver Resampler that encode the visual input features
    into a small number of visual tokens. The Language Model itself is an extension
    of DeepMind’s earlier Chinchilla model, adapted to blend in visual information.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，DeepMind的Flamingo是一种视觉语言模型，即它接受交错的文本和视觉数据流（图像和视频），并能够继续通过附加文本提示的方式进行文本输出，类似解码器Transformer的风格。其关键贡献在于展示了如何通过视觉编码器和感知器重采样器将视觉信息输入到Transformer中，将视觉输入特征编码为少量的视觉标记。语言模型本身是DeepMind早期Chinchilla模型的扩展，经过调整以融入视觉信息。
- en: All four are remarkable examples of the power of multimodal models. In the future,
    it is highly likely that generative modeling will become more multimodal and AI
    models will be able to easily cross modalities and tasks through interactive language
    prompting.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这四个都是多模态模型强大性能的显著例子。未来，生成建模很可能会变得更加多模态化，AI模型将能够通过交互式语言提示轻松跨越模态和任务。
- en: ^([1](ch13.xhtml#idm45387001413744-marker)) Aditya Ramesh et al., “Zero-Shot
    Text-to-Image Generation,” February 24, 2021, [*https://arxiv.org/abs/2102.12092*](https://arxiv.org/abs/2102.12092).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 阿迪蒂亚·拉梅什等人，“零样本文本到图像生成”，2021年2月24日，https://arxiv.org/abs/2102.12092。
- en: ^([2](ch13.xhtml#idm45387001411696-marker)) Aditya Ramesh et al., “Hierarchical
    Text-Conditional Image Generation with CLIP Latents,” April 13, 2022, [*https://arxiv.org/abs/2204.06125*](https://arxiv.org/abs/2204.06125).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 阿迪蒂亚·拉梅什等人，“具有CLIP潜在特征的分层文本条件图像生成”，2022年4月13日，https://arxiv.org/abs/2204.06125。
- en: ^([3](ch13.xhtml#idm45387001394816-marker)) Alec Radford et al., “Learning Transferable
    Visual Models From Natural Language Supervision,” February 26, 2021, [*https://arxiv.org/abs/2103.00020*](https://arxiv.org/abs/2103.00020).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 亚历克斯·拉德福德等人，“从自然语言监督中学习可转移的视觉模型”，2021年2月26日，https://arxiv.org/abs/2103.00020。
- en: '^([4](ch13.xhtml#idm45387001327744-marker)) Alex Nichol et al., “GLIDE: Towards
    Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,”
    December 20, 2021, [*https://arxiv.org/abs/2112.10741*](https://arxiv.org/abs/2112.10741).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '亚历克斯·尼科尔等人，“GLIDE: 朝向逼真图像生成和编辑的文本引导扩散模型”，2021年12月20日，https://arxiv.org/abs/2112.10741。'
- en: ^([5](ch13.xhtml#idm45387001262144-marker)) Chitwan Saharia et al., “Photorealistic
    Text-to-Image Diffusion Models with Deep Language Understanding,” May 23, 2022,
    [*https://arxiv.org/abs/2205.11487*](https://arxiv.org/abs/2205.11487).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 奇特万·萨哈里亚等人，“具有深度语言理解的逼真文本到图像扩散模型”，2022年5月23日，https://arxiv.org/abs/2205.11487。
- en: ^([6](ch13.xhtml#idm45387001217376-marker)) Robin Rombach et al., “High Resolution
    Image Synthesis with Latent Diffusion Models,” December 20, 2021, [*https://arxiv.org/abs/2112.10752*](https://arxiv.org/abs/2112.10752).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 罗宾·隆巴赫等人，“使用潜在扩散模型进行高分辨率图像合成”，2021年12月20日，https://arxiv.org/abs/2112.10752。
- en: '^([7](ch13.xhtml#idm45387001198400-marker)) Jean-Baptiste Alayrac et al., “Flamingo:
    A Visual Language Model for Few-Shot Learning,” April 29, 2022, [*https://arxiv.org/abs/2204.14198*](https://arxiv.org/abs/2204.14198).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '让-巴蒂斯特·阿拉拉克等人，“Flamingo: 一种用于少样本学习的视觉语言模型”，2022年4月29日，https://arxiv.org/abs/2204.14198。'
- en: ^([8](ch13.xhtml#idm45387001185712-marker)) Andrew Brock et al., “High-Performance
    Large-Scale Image Recognition Without Normalization,” February 11, 2021, [*https://arxiv.org/abs/2102.06171*](https://arxiv.org/abs/2102.06171).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch13.xhtml#idm45387001185712-marker)) Andrew Brock等人，“无归一化的高性能大规模图像识别”，2021年2月11日，[*https://arxiv.org/abs/2102.06171*](https://arxiv.org/abs/2102.06171)。
- en: ^([9](ch13.xhtml#idm45387001171728-marker)) Jordan Hoffmann et al., “Training
    Compute-Optimal Large Language Models,” March 29, 2022, [*https://arxiv.org/abs/2203.15556v1*](https://arxiv.org/abs/2203.15556v1).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch13.xhtml#idm45387001171728-marker)) Jordan Hoffmann等人，“训练计算优化的大型语言模型”，2022年3月29日，[*https://arxiv.org/abs/2203.15556v1*](https://arxiv.org/abs/2203.15556v1)。
