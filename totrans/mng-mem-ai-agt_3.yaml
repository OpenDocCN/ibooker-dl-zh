- en: Chapter 3\. Some Economics of Agents, Model Usage, and Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs serve as the cognitive engines that power AI agents—just like how different
    car engines are built for different purposes, from ATVs to school buses. Extending
    this framework, outlined in [*What Are AI Agents? When and How to Use LLM Agents*](https://learning.oreilly.com/library/view/what-are-ai/9781098159726/)
    (O’Reilly), selecting the right LLM for your agent isn’t just about performance—it’s
    also about a trade-off between capabilities and economics. The foundation model
    you choose will directly affect not only what your agent can do but also how much
    it costs to run, how efficiently it operates, and ultimately, whether your project
    makes financial sense. Understanding these economic trade-offs is crucial for
    anyone building AI agents, whether you’re a startup watching every dollar or an
    enterprise looking to scale.
  prefs: []
  type: TYPE_NORMAL
- en: The Economics of Agent Adoption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing to note about AI agents is that as they expand in their capacity
    to access and work with more tools—through protocols like Model Context Protocol
    (MCP) or through functional calls within LLMs themselves—the end user benefits
    more from the agent. But as with everything in economics, there is a trade-off.
    As the complexity of the task an agent is provided increases, so too does the
    marginal cost of a given action—whether we measure that cost by the hours it takes
    to complete a task, the cost of API calls, or the ability of the underlying agent
    to solve the niche tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chart shown in [Figure 3-1](#ch03_figure_1_1758256567876034) plots task
    complexity (*x*-axis) against marginal benefit and marginal cost (*y*-axis). As
    complexity rises, the extra value an AI model adds (marginal benefit) falls while
    the extra cost or risk (marginal cost) climbs. Their intersection (*) marks the
    economically optimal point: any task with complexity to the left of * still delivers
    a positive net benefit (MB > MC), making AI deployment worthwhile; tasks to the
    right cost more than they’re worth.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating how current AI capabilities relate task complexity with
    marginal benefit and cost, showing the economically optimal point for AI deployment
    where benefits outweigh costs.](assets/mmai_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Current AI capabilities
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: But there’s more. When breakthrough models or new integration capabilities arrive—think
    GPT-5 or a cutting-edge open source foundation model—the marginal cost curve shifts
    downward and to the right. In practical terms, each additional unit of task complexity
    now carries a lower cost, so the intersection with the marginal benefit curve
    moves farther right. That expands the zone where MB > MC, meaning more complex
    tasks become economically viable for AI automation. In other words, as model efficiency
    improves, your “deployment frontier” widens—AI can add positive net benefit to
    a broader range of projects ([Figure 3-2](#ch03_figure_2_1758256567876062)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating how the introduction of new AI models shifts the intersection
    of marginal cost (MC) and marginal value (MV), expanding the frontier for economically
    viable AI automation.](assets/mmai_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. When new AI models emerge, the frontier shifts
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Model Selection Matrix: A Multifactor Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building an AI agent requires a corresponding LLM to run it. The choice of an
    LLM is not as simple as it may seem. Defaulting to the latest model, such as Claude
    Sonnet 4 or GPT-5, may be a solid heuristic for quick, general projects, but when
    it comes to specialized applications or applications that may become the core
    of your business or product, much more thought, planning, and explanation are
    required. Do you only need text-based agents, or do you need multimodal agents
    that can ingest voice or vision inputs? How many users will use your agents? What
    are your latency needs? These questions are just table stakes.
  prefs: []
  type: TYPE_NORMAL
- en: Making these decisions effectively requires understanding how different models
    perform across several key dimensions. Some factors—like quality and accuracy—seem
    obvious, but others, like the relationship between context window size and cost,
    can catch teams off guard when they reach production scale.
  prefs: []
  type: TYPE_NORMAL
- en: Core Performance Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to AI agents, there’s a general trade-off between speed and reasoning
    agents’ output and reasoning. Many in the industry have started distinguishing
    between what they call “System 1” versus “System 2” thinking—borrowing theory
    of the human mind developed in the parallel domain of behavioral economics by
    Daniel Kahneman and Amos Tversky.^([1](ch03.html#id72)) System 1 processes are
    fast, automatic, and instinctual: the mental heuristics and quick judgments we
    use to navigate daily life without expending too much cognitive energy. System
    2 processes are deliberate and require conscious effort, the kind of rational
    deliberation and critical thinking we associate with complex problem solving.'
  prefs: []
  type: TYPE_NORMAL
- en: Quality and accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In LLMs, techniques like chain-of-thought prompting essentially force the model
    into System 2 mode, where it reexamines its logic iteratively. This reasoning
    process can significantly improve accuracy and reliability, but it can come at
    a cost: more processing time and higher compute expenses. The trade-off is real—System
    2 thinking can deliver better results but costs more to run.'
  prefs: []
  type: TYPE_NORMAL
- en: The good news? Clever engineering strategies can sometimes recover most of the
    quality gains of chain-of-thought processing, with two to three times less latency
    or token usage.^([2](ch03.html#id73)) Techniques like early exit inference and
    speculative decoding are making it possible to get closer to System 2 quality
    without the full System 2 cost.
  prefs: []
  type: TYPE_NORMAL
- en: Cost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most well-known LLMs (OpenAI’s GPTs, Claude’s Sonnet, Google’s Gemini) provide
    proprietary APIs for pay-as-you-go usage. Prices vary by model and are constantly
    being revised based on the market and the latest model, but do *not* let the cheap
    per-token cost of most models fool you. The average project usually requires the
    user to pass in the context of their code, writing, or documents to prime the
    model—accruing context windows in the hundreds of thousands—if not millions—of
    tokens *per day*. These costs can quickly add up. [Figure 3-3](#ch03_figure_3_1758256567876080)
    shows the cost per million tokens of 21 LLMs from some of the most popular providers.
    As the size and quality of the model increase, so too do the prices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Bar chart comparing input and output token pricing for 21 AI models across
    Google, OpenAI, and Anthropic, illustrating higher costs with increased model
    quality.](assets/mmai_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. AI model pricing comparison
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In fact, there is a significant discrepancy between the cost of content you
    provide a model and the cost of content it provides you. Ingress costs among LLMs
    are relatively cheap, but the egress costs from the LLM back to you can be substantial.
    The more you ask the model to return, the more your costs will rise. So for those
    companies looking to position themselves as central to their product, it bears
    careful consideration of how much these models will be used and how much you can
    afford to spend.
  prefs: []
  type: TYPE_NORMAL
- en: The Rise of the Multimodel Strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As agentic systems become more complex, organizations and industry experts
    are increasingly abandoning a one-size-fits-all approach to model selection. Instead,
    a more sophisticated multimodel strategy is emerging as a best practice. This
    involves using a combination of different LLMs rather than sticking to a single
    LLM vendor. In this multimodel strategy, model selection is context dependent
    and optimized to the specific requirements of a particular task within a larger
    workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Specialized roles for different models
  prefs: []
  type: TYPE_NORMAL
- en: A common and effective pattern is to create a hierarchy of models. A highly
    capable but more expensive and potentially slower model, such as o3 Pro or Claude
    Opus 4, is used for the most complex cognitive tasks, such as high-level planning,
    complex reasoning, or decomposing a difficult problem into subtasks. Once the
    plan is formulated, the execution of the individual, more straightforward subtasks
    is delegated to a fleet of smaller, faster, and more cost-effective models, such
    as Claude Sonnet 4, Gemini Flash 2.5, or GPT-5 mini. This approach allows the
    system to achieve a balance between reasoning and cost-efficient execution.
  prefs: []
  type: TYPE_NORMAL
- en: Scalable orchestration
  prefs: []
  type: TYPE_NORMAL
- en: For large enterprises deploying hundreds or even thousands of agents, relying
    on a single, powerful, monolithic model for every operation can be prohibitively
    expensive and difficult to scale. A more scalable architecture may involve using
    sparse models, such as mixture-of-experts (MoE) models like Mixtral, or orchestrating
    a large number of specialized lightweight agents. This allows the system to scale
    cost-effectively while maintaining high performance.
  prefs: []
  type: TYPE_NORMAL
- en: A Framework for Empirical Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In such a fast-changing industry, many look to industry benchmarks as useful
    signals for which models they should use. While public benchmarks such as these
    are a useful starting point for comparing models, they cannot guarantee how a
    model will perform on your specific project or proprietary use case. It is therefore
    essential for all agent and LLM developers and architects to develop their own
    domain-specific evaluation frameworks to test models against their unique data
    and requirements. Here’s how.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Use-Case-Specific Test Criteria
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step to developing an internal evaluation framework is to define
    clear, objective, and measurable criteria for what constitutes success for the
    agent’s task—after all, you cannot manage what you cannot measure. These evaluation
    criteria should go beyond simply checking for a single “right answer” and should
    instead assess the quality of the entire agentic process. Key areas for evaluation
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: Task completion
  prefs: []
  type: TYPE_NORMAL
- en: 'The most fundamental metric: did the agent successfully achieve its high-level
    goal? Specifically, did a piece of substantive work get completed in a timely
    and relatively correct manner?'
  prefs: []
  type: TYPE_NORMAL
- en: Tool correctness and efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Did the agent select the correct tools for the job? Did it invoke them with
    the correct parameters? Was the tool usage efficient, or were there redundant
    or unnecessary calls?
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning coherence and relevance
  prefs: []
  type: TYPE_NORMAL
- en: Was the agent’s internal chain of thought logical? Did its reasoning steps directly
    contribute to solving the problem, or were they irrelevant?
  prefs: []
  type: TYPE_NORMAL
- en: The “LLM-as-a-Judge” Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take it from a practitioner: evaluating thousands of agent outputs is impractical
    and does not scale. A powerful technique that has emerged to address this challenge
    is the “LLM-as-a-judge” approach. This method involves using a powerful, state-of-the-art
    LLM to act as an impartial evaluator. The judge LLM is given the agent’s output,
    the original prompt, a set of ground-truth data, and a detailed evaluation rubric
    or scoring template. It is then prompted to assess the agent’s performance against
    the defined criteria and provide a score and a qualitative explanation for its
    judgment. To improve the reliability of this method, it is common to use advanced
    prompting techniques like chain-of-thought prompting, which instructs the judge
    LLM to think step-by-step through its evaluation, making its reasoning more transparent
    and less prone to random variability. This methodology enables the scalable, repeatable,
    and nuanced evaluation of agent performance, although it requires careful prompt
    engineering and awareness of potential biases in the judge model itself.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#id72-marker)) Daniel Kahneman, *Thinking, Fast and Slow* (Farrar,
    Straus, and Giroux, 2011).
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch03.html#id73-marker)) Yaniv Leviathan, Matan Kalman, and Yossi Matias,
    “Fast Inference from Transformers via Speculative Decoding,” in *Proceedings of
    the 40th International Conference on Machine Learning*, PMLR 202, 2023; Mostafa
    Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen
    Lai, et al., “LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding,”
    in *Proceedings of the 62nd Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), 12622–12642*, 2024.'
  prefs: []
  type: TYPE_NORMAL
