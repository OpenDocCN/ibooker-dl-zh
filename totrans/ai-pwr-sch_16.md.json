["```py\n1 = |a| = |b|\ncos(a, b) = (a . b) / |a| x |b|\ncos(a, b) = (a . b) / (1 x 1)\ncos(a, b) = a . b\n```", "```py\nvector_a = [5.0, 3.0]\nvector_b = [6.0, 2.0]\n\nunit_vector_a  #1\n  = unit_normalize(vector_a)  #1\n  = unit_normalize([5.0, 3.0])  #1\n  = [5.0 / sqrt(5.0^2 + 3.0^2), 3.0 / sqrt(5.0^2 + 3.0^2)]  #1\n  = [0.8575,  0.5145]  #1\n  #1\nunit_vector_b  #1\n  = unit_normalize(vector_b)  #1\n  = unit_normalize([6.0, 2.0])  #1\n  = [6.0 / sqrt(6.0^2 + 2.0^2), 2.0 / sqrt(6.0^2 + 2.0^2)]  #1\n  = [0.9487, 0.3162]  #1\n\ncos(vector_a, vector_b)  #2\n  = cos([5.0, 3.0], [6.0, 2.0])  #2\n  = (5.0 x 6.0 + 3.0 x 2.0) /  #2\n    (sqrt(5.0^2 + 3.0^2) x sqrt(6.0^2 + 2.0^2))  #2\n  = 0.9762  #3\n\ndot_product(unit_vector_a, unit_vector_b)  #4\n  = dot_product([0.8575, 0.5145], [0.9487, 0.3162])  #4\n  = (0.8575 x 0.9487) + (0.5145 x 0.3162)  #4\n  = 0.9762  #3\n```", "```py\noutdoors_collection = engine.create_collection(\"outdoors\")\noutdoors_dataframe = load_outdoors_data(\"data/outdoors/posts.csv\")\noutdoors_collection.write(outdoors_dataframe)\n```", "```py\nroot\n |-- id: *integer* (nullable = true) \n |-- accepted_answer_id: *integer* (nullable = true) \n |-- parent_id: *integer* (nullable = true) \n |-- creation_date: *timestamp* (nullable = true) \n |-- score: *integer* (nullable = true) \n |-- view_count: *integer* (nullable = false) \n |-- body: *string* (nullable = true) \n |-- owner_user_id: *string* (nullable = true) \n |-- title: *string* (nullable = true) \n |-- tags: *array* (nullable = true) \n |    |-- element: *string* (containsNull = true) \n |-- answer_count: *integer* (nullable = true) \n |-- post_type: *string* (nullable = true) \n |-- url: *string* (nullable = true)\n```", "```py\n[{\"id\": \"18825\",\n  \"accepted_answer_id\": 18826, #1\n  \"body\": \"If I wanted to learn how to tie certain knots, \n  ↪or learn about new knots and what they're used for, \n  ↪what are some good resources to look up?\",\n  \"title\": \"What's a good resource for learning to tie knots for climbing?\",\n  \"post_type\": \"question\"},  #2\n {\"id\": \"24440\",  #3\n  \"parent_id\": 18825,  #3\n  \"body\": \"Knots and Ropes for Climbers by Duane Raleigh is a fantastic\n  ↪illustrated resource tailored specifically to climbers. The ABoK\n  ↪is great, but a but beyond the pale of what the average rock...\",\n\n \"post_type\": \"answer\"},   #4\n {\"id\": \"18826\",  #5\n  \"parent_id\": 18825,  #3\n  \"body\": \"Animated Knots By Grog Arguably the best resource online for knot\n  ↪tying is Animated Knots by Grog , it's used by virtually every avid\n  ↪knot tyer I've known. They have excellent step-by-step animatio...\",\n  \"post_type\": \"answer\"}]  #4\n```", "```py\ndef search_questions(query, verbose=False):\n  request = {\"query\": query,\n             \"query_fields\": [\"title\", \"body\"],  #1\n             \"limit\": 5,\n             \"return_fields\": [\"id\", \"url\", \"post_type\", \"title\",\n                               \"body\", \"accepted_answer_id\", \"score\"],\n             \"filters\": [(\"post_type\", \"question\")],\n             \"order_by\": [(\"score\", \"desc\"), (\"title\", \"asc\")]}\n  response = outdoors_collection.search(**request)\n  display_questions(query, response, verbose)\n\nsearch_questions(\"climbing knots\")\n```", "```py\nQuery: climbing knots\n\nRanked Questions:\nQuestion 21855: What are the four climbing knots used by Jim Bridwell?\nQuestion 18825: What's a good resource for learning to tie knots for clim...\nQuestion 18814: How to tie a figure eight on a bight?\nQuestion 9183: Can rock climbers easily transition to canyoning?\nQuestion 22477: Tradeoffs between different stopper knots\n```", "```py\nsearch_questions(\"What is DEET?\")\n```", "```py\nQuery What is DEET?:\n\nRanked Questions:\nQuestion 20403: What is bushcrafting?\nQuestion 20977: What is \"catskiing\"?\nQuestion 1660: What is Geocaching?\nQuestion 17374: What is a tent skirt and what is its purpose?\nQuestion 913: What is a buff?\n```", "```py\nfrom sentence_transformers import SentenceTransformer\ntransformer = SentenceTransformer(\"roberta-base-nli-stsb-mean-tokens\")\n```", "```py\nphrases = [\"it's raining hard\", \"it is wet outside\",  #1\n           \"cars drive fast\", \"motorcycles are loud\"]  #1\nembeddings = transformer.encode(phrases, convert_to_tensor=True)  #2\nprint(\"Number of embeddings:\", len(embeddings))\nprint(\"Dimensions per embedding:\", len(embeddings[0]))\nprint(\"The embedding feature values of \\\"it's raining hard\\\":\")\nprint(embeddings[0])\n```", "```py\nNumber of embeddings: 4\nDimensions per embedding: 768\nThe embedding feature values of \"it's raining hard\":\ntensor( 1.1609e-01, -1.8422e-01,  4.1023e-01,  2.8474e-01,  5.8746e-01,\n        7.4418e-02, -5.6910e-01, -1.5300e+00, -1.4629e-01,  7.9517e-01,\n        5.0953e-01,  3.5076e-01, -6.7288e-01, -2.9603e-01, -2.3220e-01,\n        ...\n        5.1413e-01,  3.0842e-01, -1.1862e-01,  5.9565e-02, -5.5944e-01,\n        9.9763e-01, -2.2970e-01, -1.3132e+00])\n```", "```py\ndef normalize_embedding(embedding): #1\n  normalized = numpy.divide(embedding, numpy.linalg.norm(embedding))\n  return list(map(float, normalized))\n\nnormalized_embeddings = list(map(normalize_embedding, embeddings))\nsimilarities = sentence_transformers.util.dot_score(normalized_embeddings,\n                                                    normalized_embeddings)\nprint(\"The shape of the resulting similarities:\", similarities.shape)\n```", "```py\nThe shape of the resulting similarities: torch.Size([4, 4])\n```", "```py\ndef rank_similarities(phrases, similarities, name=None):\n  a_phrases = []\n  b_phrases = []\n  scores = []\n  for a in range(len(similarities) - 1):  #1\n    for b in range(a + 1, len(similarities)): #2\n      a_phrases.append(phrases[a])\n      b_phrases.append(phrases[b])\n      scores.append(float(similarities[a][b]))  #3\n\n  dataframe = pandas.DataFrame({\"score\": scores, \"phrase a\": a_phrases,\n                                \"phrase b\": b_phrases})\n  dataframe[\"idx\"] = dataframe.index  #4\n  dataframe = dataframe.reindex(columns=[\"idx\", \"score\",\n                                         \"phrase a\", \"phrase b\"])\n\n  return dataframe.sort_values(by=[\"score\"],  #5\n                               ascending=False,  #5\n                               ignore_index=True)  #5\n\ndataframe = rank_similarities(phrases, similarities)\ndisplay(HTML(dataframe.to_html(index=False)))\n```", "```py\nidx  score     phrase a           phrase b\n0    0.669060  it's raining hard  it is wet outside\n5    0.590783  cars drive fast    motorcycles are loud\n1    0.281166  it's raining hard  cars drive fast\n2    0.280800  it's raining hard  motorcycles are loud\n4    0.204867  it is wet outside  motorcycles are loud\n3    0.138172  it is wet outside  cars drive fast\n```", "```py\nnlp = spacy.load(\"en_core_web_sm\")  #1\nphrases = []  #2\nsources = []  #3\nmatcher = Matcher(nlp.vocab) #4\nnountags = [\"NN\", \"NNP\", \"NNS\", \"NOUN\"]  #5\nverbtags = [\"VB\", \"VBD\", \"VBG\", \"VBN\", ) #6\n            \"VBP\", \"VBZ\", \"VERB\"]  #6\n\nmatcher.add(\"noun_phrases\", [[{\"TAG\": {\"IN\": nountags},  #7\n                               \"IS_ALPHA\": True,  #7\n                               \"OP\": \"+\"}]])  #7\nmatcher.add(\"verb_phrases\", [[{\"TAG\": {\"IN\": verbtags},\n                               \"IS_ALPHA\": True, \"OP\": \"+\",\n                               \"LEMMA\":{\"NOT_IN\":[\"be\"]}}]])  #8\nfor doc, _ in tqdm.tqdm(nlp.pipe(yield_tuple(dataframe,  #9\n                                   source_field=\"body\",  #9\n                                   total=total),)  #9\n                                 batch_size=40,  #9\n                                 n_threads=4,  #9\n                                 as_tuples=True), #9\n                        total=total): \n  matches = matcher(doc)\n  for _, start, end in matches:  #10\n    span = doc[start:end]  #10\n    phrases.append(normalize(span))  #10\n    sources.append(span.text)  #10\n\nconcepts = {}\nlabels = {}\nfor i, phrase in phrases: #11\n  if phrase not in concepts:  #11\n    concepts[phrase] = 0  #11\n    labels[phrase] = sources[i]  #11\n  concepts[phrase] += 1  #11\n```", "```py\ncollection = engine.get_collection(\"outdoors\")\nconcepts, labels = get_concepts(collection, source_field=\"body\",\n                                load_from_cache=True)\ntopcons = {key: value for (key, value)\n                      in concepts.items() if value > 5}\nprint(\"Total number of labels:\", len(labels.keys()))\nprint(\"Total number of concepts:\", len(concepts.keys()))\nprint(\"Concepts with greater than 5 term frequency:\", len(topcons.keys()))\nprint(json.dumps(topcons, indent=2))\n```", "```py\nTotal number of labels: 124366\nTotal number of concepts: 124366\nConcepts with greater than 5 term frequency: 12375\n{\n  \"have\": 32782,\n  \"do\": 26869,\n  \"use\": 16793,\n  ...\n  \"streamside vegetation\": 6,\n  \"vehicle fluid\": 6,\n  \"birdshot\": 6\n}\n```", "```py\ndef get_embeddings(texts, model, cache_name, ignore_cache=False):\n  ...  #1\n    embeddings = model.encode(texts)\n  ...  #1\n  return embeddings\n\nminimum_frequency = 6  #2\nphrases = [key for (key, tf) in concepts.items() if tf >= minimum_frequency]\ncache_name = \"outdoors_embeddings\"\nembeddings = get_embeddings(phrases, transformer, \n                            cache_name, ignore_cache=False)\n\nprint(f\"Number of embeddings: {len(embeddings)}\")\nprint(f\"Dimensions per embedding: {len(embeddings[0])}\")\n```", "```py\nNumber of embeddings: 12375\nDimensions per embedding: 768\n```", "```py\nnormalized_embeddings = list(map(normalize_embedding, embeddings))\nsimilarities = sentence_transformers.util.dot_score(  #1\n                 normalized_embeddings[0:250],  #1\n                 normalized_embeddings[0:250])  #1\ncomparisons = rank_similarities(phrases, similarities)  #2\ndisplay(HTML(comparisons[:10].to_html(index=False)))\n```", "```py\nidx     score      phrase a   phrase b\n31096   0.928151   protect    protection\n13241   0.923570   climbing   climber\n18096   0.878894   camp       camping\n...\n7354    0.782962   climb      climber\n1027    0.770643   go         leave\n4422    0.768611   keep       stay\n```", "```py\nfrom plotnine import *\ncandidate_synonyms = comparisons[comparisons[\"score\"] > 0.0]\n{\n  ggplot(comparisons, aes(\"idx\", \"score\")) +\n    geom_violin(color=\"blue\") +\n    scale_y_continuous(limits=[-0.4, 1.0],\n                       breaks=[-0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0])\n}\n```", "```py\nimport nmslib\n\nconcepts_index = nmslib.init(method=\"hnsw\",  #1\n                             space=\"negdotprod\")  #1\nnormalized_embeddings = list(map(normalize_embedding, embeddings))\nconcepts_index.addDataPointBatch(normalized_embeddings)  #2\nconcepts_index.createIndex(print_progress=True)   #3\n\nids, _ = concepts_index.knnQuery( #4\n           normalized_embeddings[25], k=10)  #4\nmatches = [labels[phrases[i]].lower() for i in ids] #5\ndisplay(matches)\n```", "```py\n['bag', 'bag ratings', 'bag cover', 'bag liner', 'garbage bags', 'wag bags',\n 'bag cooking', 'airbag', 'paper bag', 'tea bags']\n```", "```py\ndef embedding_search(index, query, phrases, k=20,  #1\n                     min_similarity=0.75):  #1\n  matches = []\n  query_embedding = transformer.encode(query) #2\n  query_embedding = normalize_embedding(query_embedding)\n  ids, distances = index.knnQuery(query_embedding, k=k)\n  for i in range(len(ids)):\n    similarity = distances[i] * -1  #3\n    if similarity >= min_similarity: #4\n      matches.append((phrases[ids[i]], similarity))\n  if not len(matches):\n    matches.append((phrases[ids[1]], distances[1] * -1))  #5\n  return matches\n\ndef semantic_suggest(prefix, phrases):\n  matches = embedding_search(concepts_index, prefix, phrases)\n  print_labels(prefix, matches)\n\nsemantic_suggest(\"mountain hike\", phrases)\nsemantic_suggest(\"dehyd\", phrases)\n```", "```py\nResults for: mountain hike\n\n1.000 | mountain hike\n0.975 | mountain hiking\n0.847 | mountain trail\n0.787 | mountain guide\n0.779 | mountain terrain\n0.775 | mountain climbing\n0.768 | mountain ridge\n0.754 | winter hike\n\nResults for: \"dehyd\"\n\n0.941 | dehydrate\n0.931 | dehydration\n0.852 | rehydration\n...\n0.812 | hydrate\n0.788 | hydration pack\n0.776 | hydration system\n```", "```py\noutdoors_dataframe = load_dataframe(\"data/outdoors/posts.csv\")\ntitles = outdoors_dataframe.rdd.map(lambda x: x.title).collect()  #1\ntitles = list(filter(None, titles))  #1\nembeddings = get_embeddings(titles, cache_name)  #2\n\nprint(f\"Number of embeddings: {len(embeddings)}\")\nprint(f\"Dimensions per embedding: {len(embeddings[0])}\")\n```", "```py\nNumber of embeddings: 5331\nDimensions per embedding: 768\n```", "```py\nimport nmslib\ntitles_index = nmslib.init(method=\"hnsw\", space=\"negdotprod\")\nnormalized_embeddings = list(map(normalize_embedding, embeddings))\ntitles_index.addDataPointBatch(normalized_embeddings)\ntitles_index.createIndex(print_progress=True)\n```", "```py\ndef semantic_search(query, phrases):\n  results = embedding_search(titles_index, query, phrases,  #1\n                             k=5, min_similarity=0.6)  #1\n  print_labels(query, results)\n\nsemantic_search(\"mountain hike\", titles)\n```", "```py\nResults for: mountain hike\n\n0.723 | How is elevation gain and change measured for hiking trails?\n0.715 | How do I Plan a Hiking Trip to Rocky Mountain National Park, CO\n0.698 | Hints for hiking the west highland way\n0.694 | New Hampshire A.T. Section Hike in May? Logistics and Trail Condi...\n0.678 | Long distance hiking trail markings in North America or parts the...\n```", "```py\ndef display_results(query, search_results):\n  print_labels(query, [(d[\"title\"], d[\"score\"])\n                       for d in search_results])\n\ndef index_outdoor_title_embeddings(): #1\n  create_view_from_collection(engine.get_collection(\"outdoors\"),\n                              \"outdoors\")\n  outdoors_dataframe = spark.sql(\"\"\"SELECT id, title FROM outdoors\n                                    WHERE title IS NOT NULL\"\"\")\n  ids = outdoors_dataframe.rdd.map(lambda x: x.id).collect()\n  titles = outdoors_dataframe.rdd.map(lambda x: x.title).collect()\n  embeddings = list(\n    map(normalize_embedding,  #2\n        get_embeddings(titles, cache_name)))  #2\n  embeddings_dataframe = spark.createDataFrame(\n    zip(ids, titles, embeddings),\n    schema=[\"id\", \"title\", \"title_embedding\"])\n\n  collection = engine.create_collection(\"outdoors_with_embeddings\")\n  collection.write(embeddings_dataframe)\n  return collection\n\ndef semantic_search_with_engine(collection, query, limit=10):  #3\n  query_vector = transformer.encode(query) #4\n  query_vector = normalize_embedding(query_vector)  #4\n  request = {\"query\": query_vector,\n             \"query_fields\": [\"title_embedding\"],\n             \"return_fields\": [\"title\", \"score\", \"title_embedding\"],\n             \"quantization_size\": \"FLOAT32\",  #5\n             \"limit\": limit}\n  response = collection.search(**request)\n  return response[\"docs\"]\n\nembeddings_collection = index_outdoor_title_embeddings()\n\nquery = \"mountain hike\"\nsearch_results = semantic_search_with_engine(embeddings_collection, query)\ndisplay_results(query, search_results)\n```", "```py\n0.723 | How is elevation gain and change measured for hiking trails?\n0.715 | How do I Plan a Hiking Trip to Rocky Mountain National Park, CO\n0.698 | Hints for hiking the west highland way\n0.694 | New Hampshire A.T. Section Hike in May? Logistics and Trail Condi...\n0.678 | Long distance hiking trail markings in North America or parts the...\n```", "```py\n[ -1.2345679,  2.2345679, 100.45679 ]  #4 bytes = 32 bits\n[ -1.234,      2.234,     100.44 ]     #2 bytes = 16 bits\n```", "```py\nfrom sentence_transformers.quantization import quantize_embeddings\n\nmodel = SentenceTransformer(\n          \"mixedbread-ai/mxbai-embed-large-v1\",  #1\n          similarity_fn_name=SimilarityFunction.DOT_PRODUCT,\n          truncate_dim=1024)  #2\n\ndef index_full_precision_embeddings(doc_embeddings, name):\n  index = faiss.IndexFlatIP(doc_embeddings.shape[1])  #3\n  index.add(doc_embeddings)  #4\n  faiss.write_index(index, name)  #5\n  return index\n\ndef get_outdoors_embeddings(model):\n  outdoors_dataframe = load_dataframe(\"data/outdoors/posts.csv\")\n  post_texts = [post[\"title\"] + \" \" + post[\"body\"]\n                for post in outdoors_dataframe.collect()]\n  return numpy.array(\n    get_embeddings(post_texts, model, \"outdoors_mrl_normed\"))\n\ndoc_embeddings = get_outdoors_embeddings(model)  #6\nfull_index = index_full_precision_embeddings(  #7\n               doc_embeddings, \"full_embeddings\")  #7\n```", "```py\ndef get_test_queries():\n  return [\"tent poles\", \"hiking trails\", \"mountain forests\",\n          \"white water\", \"best waterfalls\", \"mountain biking\",\n          \"snowboarding slopes\", \"bungee jumping\", \"public parks\"]\n\nqueries = get_test_queries()  #1\nquery_embeddings = model.encode(queries,  #2\n                     convert_to_numpy=True,  #2\n                     normalize_embeddings=True)  #2\n\nfull_results = time_and_execute_search( #3\n                 full_index, \"full_embeddings\",  #3\n                 query_embeddings, k=25)  #3\ndisplay_statistics(full_results)  #4\n```", "```py\nfull_embeddings search took: 7.621 ms\nfull_embeddings index size: 75.6 MB\nRecall: 1.0\n```", "```py\ndef evaluate_search(full_index, optimized_index,  #1\n                    optimized_index_name,  #1\n                    query_embeddings,  #1\n                    optimized_query_embeddings,  #1\n                    k=25, display=True, log=False):  #1\n  full_results = time_and_execute_search(  #2\n                   full_index, \"full_embeddings\",  #2\n                   query_embeddings, k=k)  #2\n  optimized_results = time_and_execute_search(  #2\n                        optimized_index,  #2\n                        optimized_index_name,  #2\n                        optimized_query_embeddings, k=k)  #2\n  if display:\n    display_statistics(optimized_results, full_results)\n  return optimized_results, full_results\n\ndef evaluate_rerank_search(full_index, optimized_index,  #3\n                           query_embeddings,  #3\n                           optimized_embeddings,  #3\n                           k=50, limit=25):  #3\n  results, full_results = evaluate_search(\n                            full_index,\n                            optimized_index, None,\n                            query_embeddings,\n                            optimized_embeddings,\n                            display=False, k=k)\n\n  doc_embeddings = get_outdoors_embeddings(model)  #4\n  rescore_scores, rescore_ids = [], []\n  for i in range(len(results[\"results\"])):\n    embedding_ids = results[\"faiss_ids\"][i]\n    top_k_embeddings = [doc_embeddings[id]  #5\n                        for id in embedding_ids]  #5\n    query_embedding = query_embeddings[i] #5\n    scores = query_embedding @ \\ #5\n             numpy.array(top_k_embeddings).T  #5\n    indices = scores.argsort()[::-1][:limit]  #6\n    top_k_indices = embedding_ids[indices]  #6\n    top_k_scores = scores[indices]  #6\n    rescore_scores.append(top_k_scores)  #6\n    rescore_ids.append(top_k_indices)  #6\n\n  results = generate_search_results(rescore_scores, rescore_ids)  #7\n  recall = calculate_recall(full_results[\"results\"], results)  #8\n  print(f\"Reranked recall: {recall}\")\n```", "```py\ndef index_int8_embeddings(doc_embeddings, name):\n  int8_embeddings = quantize_embeddings(  #1\n                      doc_embeddings, precision=\"int8\")  #1\n  print(\"Int8 embeddings shape:\", int8_embeddings.shape)\n  index = faiss.IndexFlatIP(int8_embeddings.shape[1]) #2\n  index.add(int8_embeddings)  #3\n  faiss.write_index(index, name) #4\n  return index\n\nint8_index_name = \"int8_embeddings\"\nint8_index = index_int8_embeddings(doc_embeddings, int8_index_name)\n\nquantized_queries = quantize_embeddings(  #5\n  query_embeddings,  #5\n  calibration_embeddings=doc_embeddings,  #5\n  precision=\"int8\")  #5\n\nevaluate_search(full_index, int8_index,  #6\n                int8_index_name, query_embeddings,  #6\n                quantized_queries)  #6\nevaluate_rerank_search(full_index, int8_index,  #7\n          query_embeddings, quantized_queries)  #7\n```", "```py\nInt8 embeddings shape: (18456, 1024)\nint8_embeddings search took: 9.070 ms (38.65% improvement)\nint8_embeddings index size: 18.91 MB (74.99% improvement)\nRecall: 0.9289\nReranked recall: 1.0\n```", "```py\ndef index_binary_embeddings(doc_embeddings,\n                            binary_index_name):\n  binary_embeddings = quantize_embeddings( #1\n    doc_embeddings,  #1\n    precision=\"binary\").astype(numpy.uint8)  #1\n  print(\"Binary embeddings shape:\", binary_embeddings.shape)\n  index = faiss.IndexBinaryFlat(  #2\n    binary_embeddings.shape[1] * 8)  #2\n  index.add(binary_embeddings)  #3\n  faiss.write_index_binary(index, binary_index_name)  #4\n  return index\n\nbinary_index_name = \"binary_embeddings\"\nbinary_index = index_binary_embeddings(\n  doc_embeddings, binary_index_name)\n\nquantized_queries = quantize_embeddings(  #5\n  query_embeddings,  #5\n  calibration_embeddings=doc_embeddings, #6\n  precision=\"binary\").astype(numpy.uint8)  #7\n\nevaluate_search(full_index, binary_index,  #8\n  binary_index_name,  #8\n  query_embeddings, quantized_queries)  #8\nevaluate_rerank_search(full_index, binary_index,  #8\n  query_embeddings, quantized_queries)  #8\n```", "```py\nBinary embeddings shape: (18456, 128)\nbinary_embeddings search took: 1.232 ms (83.38% improvement)\nbinary_embeddings index size: 2.36 MB (96.87% improvement)\nRecall: 0.6044\nReranked recall: 1.0\n```", "```py\ndef index_pq_embeddings(doc_embeddings, index_name, num_subvectors=16):\n  dimensions = doc_embeddings.shape[1]  #1\n  M = num_subvectors   #2\n  num_bits = 8  #3\n  index = faiss.IndexPQ(dimensions, M, num_bits)  #4\n  index.train(doc_embeddings) #5\n  index.add(doc_embeddings) #6\n  faiss.write_index(index, index_name)  #7\n  return index\n\npq_index_name = \"pq_embeddings\"\npq_index = index_pq_embeddings(doc_embeddings,\n                               pq_index_name)\n\nevaluate_search(full_index, pq_index, pq_index_name,  #8\n                query_embeddings, query_embeddings)  #8\nevaluate_rerank_search(full_index, pq_index,  #8\n                       query_embeddings,  #8\n                       query_embeddings)  #8\n```", "```py\npq_embeddings search took: 2.092 ms (75.22% improvement)\npq_embeddings index size: 1.34 MB (98.22% improvement)\nRecall: 0.3333\nReranked recall: 0.6800\n```", "```py\npq_embeddings search took: 4.061 ms (43.99% improvement)\npq_embeddings index size: 2.23 MB (97.05% improvement)\nRecall: 0.5778\nReranked recall: 0.9911\n```", "```py\ndef get_mrl_embeddings(embeddings, num_dimensions):\n  mrl_embeddings = numpy.array(  #1\n    list(map(lambda e: e[:num_dimensions], embeddings)))  #1\n  return mrl_embeddings\n\ndef index_mrl_embeddings(doc_embeddings, num_dimensions, mrl_index_name):\n  mrl_doc_embeddings = get_mrl_embeddings(doc_embeddings, num_dimensions)\n  print(f\"{mrl_index_name} embeddings shape:\", mrl_doc_embeddings.shape)\n  mrl_index = index_full_precision_embeddings(  #2\n    mrl_doc_embeddings, mrl_index_name)  #2\n  return mrl_index\n\nprint(f\"Original embeddings shape:\", doc_embeddings.shape)\noriginal_dimensions = doc_embeddings.shape[1]  #3\n\nfor num_dimensions in [original_dimensions//2,  #4\n                       original_dimensions//4,  #5\n                       original_dimensions//8]:  #6\n\n  mrl_index_name = f\"mrl_embeddings_{num_dimensions}\"\n  mrl_index = index_mrl_embeddings(doc_embeddings,\n                                   num_dimensions,\n                                   mrl_index_name)\n  mrl_queries = get_mrl_embeddings(query_embeddings,\n                                   num_dimensions)\n\n  evaluate_search(full_index, mrl_index, mrl_index_name,  #7\n                  query_embeddings, mrl_queries) #7\n  evaluate_rerank_search(full_index,   #8\n    mrl_index, query_embeddings, mrl_queries)  #8\n```", "```py\nOriginal embeddings shape: (18456, 1024)\n\nmrl_embeddings_512 embeddings shape: (18456, 512)\nmrl_embeddings_512 search took: 3.586 ms (49.15% improvement)\nmrl_embeddings_512 index size: 37.8 MB (50.0% improvement)\nRecall: 0.7022\nReranked recall: 1.0\n\nmrl_embeddings_256 embeddings shape: (18456, 256)\nmrl_embeddings_256 search took: 1.845 ms (73.45% improvement)\nmrl_embeddings_256 index size: 18.9 MB (75.0% improvement)\nRecall: 0.4756\nReranked recall: 0.9689\n\nmrl_embeddings_128 embeddings shape: (18456, 128)\nmrl_embeddings_128 search took: 1.061 ms (84.35% improvement)\nmrl_embeddings_128 index size: 9.45 MB (87.5% improvement)\nRecall: 0.2489\nReranked recall: 0.64\n```", "```py\ndef index_binary_ivf_mrl_embeddings(reduced_mrl_doc_embeddings,\n                                    binary_index_name):\n  binary_embeddings = quantize_embeddings( #1\n    reduced_mrl_doc_embeddings,  #1\n    calibration_embeddings=reduced_mrl_doc_embeddings,  #1\n    precision=\"binary\").astype(numpy.uint8)  #1\n\n  dimensions = reduced_mrl_doc_embeddings.shape[1]  #2\n  quantizer = faiss.IndexBinaryFlat(dimensions)  #2\n\n  num_clusters = 256  #3\n  index = faiss.IndexBinaryIVF(  #3\n            quantizer, dimensions, num_clusters)  #3\n  index.nprobe = 4  #3\n\n  index.train(binary_embeddings) #4\n  index.add(binary_embeddings)  #4\n  faiss.write_index_binary(index, binary_index_name)  #4\n  return index\n\nmrl_dimensions = doc_embeddings.shape[1] // 2  #5\nreduced_mrl_doc_embeddings =  get_mrl_embeddings(  #5\n  doc_embeddings, mrl_dimensions)  #5\n\nbinary_ivf_mrl_index_name = \"binary_ivf_mrl_embeddings\"\nbinary_ivf_mrl_index = index_binary_ivf_mrl_embeddings(\n  reduced_mrl_doc_embeddings, mrl_dimensions,\n  binary_ivf_mrl_index_name)\n\nmrl_queries = get_mrl_embeddings(query_embeddings,  #6\n                                 mrl_dimensions)  #6\nquantized_queries = quantize_embeddings(mrl_queries,  #7\n  calibration_embeddings=reduced_mrl_doc_embeddings,  #7\n  precision=\"binary\").astype(numpy.uint8)  #7\n\nevaluate_search(full_index, binary_ivf_mrl_index,  #8\n  binary_ivf_mrl_index_name,  #8\n  query_embeddings, quantized_queries)  #8\nevaluate_rerank_search(  #9\n  full_index, binary_ivf_mrl_index,  #9\n  query_embeddings, quantized_queries)  #9\n```", "```py\nbinary_ivf_mrl_embeddings search took: 0.064 ms (99.09% improvement)\nbinary_ivf_mrl_embeddings index size: 1.35 MB (98.22% improvement)\nRecall: 0.3511\nReranked recall: 0.7244\n```", "```py\n{\n   'query': [0, ..., 1],\n   'query_fields': ['binary_embedding'],\n   'quantization_size': 'BINARY',\n   'order_by': [('score', 'desc')],\n   'limit': 25,\n     'rerank_query': {\n       'query': [-0.01628, ..., 0.02110],\n       'query_fields': ['full_embedding'],\n       'quantization_size': 'FLOAT32',\n       'order_by': [('score', 'desc')],\n       'rerank_count': 50\n     }\n }\n```", "```py\nfrom sentence_transformers import CrossEncoder\ncross_encoder = \\  #1\n  CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")  #1\n\nquery = \"mountain hike\"\nsearch_results = semantic_search_with_engine(  #2\n  embeddings_collection, query, limit=50)  #2\npairs_to_score = [[query, doc[\"title\"]]  #3\n                  for doc in search_results]  #3\ncross_scores = cross_encoder.predict(pairs_to_score,  #4\n                 activation_fct=torch.nn.Sigmoid())  #5\nreranked_results = rerank(search_results, cross_scores) #6\ndisplay_results(query, reranked_results[:10])\n```", "```py\n0.578 | What constitutes mountain exposure when hiking or scrambling?\n0.337 | ... hiking trails... in... Rocky Mountain National Park...\n0.317 | Where in the US can I find green mountains to hike...?\n0.213 | Appropriate terms... hiking, trekking, mountaineering...\n0.104 | Camping on top of a mountain\n0.102 | ... Plan a Hiking Trip to Rocky Mountain National Park, CO\n0.093 | What considerations... for... a hiking ascent of Mount...\n0.073 | Are there any easy hiking daytrips up mountains...\n0.053 | First time snow hiking\n0.049 | Advice for first Grand Canyon Hike for Eastern Hikers\n```"]