- en: 4 The evolution of created content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Creating and detecting synthetic media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using generative AI for content creation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the ongoing debates around the use of copyrighted content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an image that was circulated widely on Twitter, Pope Francis is walking down
    a street, wearing a cross around his neck and his typical white zucchetto. More
    unusually, the octogenarian is sporting an eye-catching white puffer coat that
    bears a strong resemblance to one sold by the designer brand Balenciaga (for $3,350
    retail). The pope’s “drip,” or style, was the talk of the internet. The only problem?
    The image wasn’t real—it was created by a construction worker in Chicago, who
    was tripping on shrooms while using the AI image-generation tool Midjourney, and
    thought it would be funny to see Pope Francis dripped out [[1]](https://www.buzzfeednews.com/article/chrisstokelwalker/pope-puffy-jacket-ai-midjourney-image-creator-interview).
  prefs: []
  type: TYPE_NORMAL
- en: Although the “Balenciaga Pope” meme was harmless fun, it fooled many users.
    Model and author Chrissy Teigen tweeted, “I thought the pope’s puffer jacket was
    real and didn’t give it a second thought. no way am I surviving the future of
    technology” [[2]](https://twitter.com/chrissyteigen/status/1639802312632975360).
    But the future of technology is here, and AI-generated media is quickly becoming
    indistinguishable from the forms it imitates. In this chapter, we’ll discuss the
    methods, risks, opportunities, and legal landscape of synthetic media, one of
    the foremost applications for LLMs and other types of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of synthetic media
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic media, or more specifically, AI-generated media, is an umbrella term
    for content that has been created or altered with the help of AI. It’s sometimes
    used synonymously with “deepfake” visual technology, but synthetic content (as
    shown in figure 4.1) is much broader and can span text, image, video, voice, and
    data. The term *deepfak**e*—a portmanteau of “deep learning” and “fake”—was coined
    by a Reddit user in 2017 who used face-swapping technology to alter pornographic
    videos [[3]](https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained).
    Deepfakes narrowly refer to faking a particular person’s physical characteristics
    or voice, most often to “fake” others into believing an event happened.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic media or, more specifically, AI-generated media, is an umbrella term
    for content that has been created or altered with the help of AI, which spans
    text, image, video, voice, and data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F01_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 The landscape of synthetic media
  prefs: []
  type: TYPE_NORMAL
- en: Initially, deepfakes referred to a form of synthetic media in which a person
    in an image or video is replaced with someone else, but it has since expanded
    to include synthetic media applications, such as realistic-looking images of people
    who don’t exist, synthetic audio or video recordings that mimic a target, or targeted
    propaganda that resemble real news articles. Deepfakes have generally had a negative
    connotation, with prominent examples including a fake video of President Biden
    announcing a draft to send American soldiers to Ukraine (see [http://mng.bz/p1Q2](http://mng.bz/p1Q2));
    Mark Zuckerberg saying “whoever controls the data, controls the future” in an
    edited video (see [http://mng.bz/OPVo](http://mng.bz/OPVo)); or Donald Trump’s
    viral deepfake asking Belgium to exit the Paris climate agreement (see [http://mng.bz/YR8K](http://mng.bz/YR8K)).
    In fact, 9 out of 10 Americans believe that deepfakes could cause more harm than
    good [[4]](https://thesentinel.ai/media/Deepfakes%202020:%20The%20Tipping%20Point,%20Sentinel.pdf).
    As we’ll discuss, there are a number of potentially beneficial applications and
    use cases, so people in the space have been increasingly using the term *AI-generated
    media*, or *AI-generated synthetic media*, to move away from the negative connotation
    of the term *deepfakes*.
  prefs: []
  type: TYPE_NORMAL
- en: Popular techniques for creating synthetic media
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve previously discussed how large language models (LLMs) are used to generate
    text. Here, we’ll explore two commonly used techniques to alter or create images
    and videos (since videos are just sequences of images). The first technique, autoencoders,
    uses neural networks to compress and decompress images. You may remember the encoder-decoder
    framework from chapter 1, where text is encoded into a numeric representation
    for use by the model and then decoded back into a readable output. Similarly,
    an image can be fed into an encoder, which creates a compressed version of the
    same file. This compressed version of the file, also referred to as latent features
    or latent representation, contains a set of patterns that represent the characteristics
    of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we passed an image of someone’s face through the encoder. Then,
    the latent features could include facial characteristic patterns such as expression,
    face angle, skin tone, and so on. These features are then passed into a decoder,
    which reconstructs the image based on the latent features. Autoencoders are often
    used in face-swapping technology, where the same encoder is used to create latent
    features from both faces, and then separate decoders are used to create the images
    from the latent features to best rebuild the original image. In figure 4.2, the
    same encoder creates the latent representations of Original Face A and Original
    Face B. Then, the decoder trained to rebuild Face B is fed the facial latent features
    of Face A (same encoder) to generate a seamless blend of the two faces. For example,
    the decoder can map characteristics such as the eyes, nose, mouth, and lighting
    to mix the two faces.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F02_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 Deepfake creation through the use of autoencoders with a single encoder
    and two decoders
  prefs: []
  type: TYPE_NORMAL
- en: The second technique for generating synthetic media is Generative Adversarial
    Networks (GANs), which consist of two neural networks—a generator and a discriminator.
    For example, suppose there is a shop that buys authentic artworks that they later
    resell. But there is a criminal who sells fake artworks to make money. Initially,
    the criminal might make mistakes when trying to sell fake artworks, so the shop
    owner might be able to identify that it’s not an authentic artwork. Then, the
    criminal will likely learn what characteristics of the artwork the shop owner
    is looking at to determine if it’s real or not, so the criminal can use that knowledge
    to improve the process by which artworks can be sold as fake to eventually be
    successful. At the same time, when the shop owner accidentally buys and tries
    to resell some of the fake artworks, they would get feedback from customers or
    experts that some of their art pieces are counterfeit, so the shop owner also
    has to learn how to better distinguish between the fake and real artworks.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure 4.3, the goal of the criminal (generator) is to create fake
    artworks that are indistinguishable from real ones, while the goal of the shop
    owner (discriminator) is to be able to distinguish between real and fake artworks—this
    competitive feedback loop is the main idea behind GANs. The generator exists to
    create new data, such as images, and the discriminator verifies the authenticity
    of an image by comparing it to the training dataset to determine the difference
    between a fake and a real image. The ultimate goal of a generative network is
    to create images that are indistinguishable from authentic images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F03_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 Creation of GANs using a generator and discriminator
  prefs: []
  type: TYPE_NORMAL
- en: The good and the bad of synthetic media
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Samsung NEXT’s “Synthetic Media Landscape” report, they argue that “this
    technology will transform the way we produce, consume, and distribute media.”
    They claim that synthetic media is the third evolutionary stage of media. The
    first, old media, made possible through broadcasting, enabled *mass distribution*
    for a select few through TV, radio, and print. The second, new media, made possible
    through the internet, enabled *democratized distribution* for everyone through
    social media. The third, synthetic media, made possible through AI and deep learning,
    will *democratize media creation* and creativity for everyone. Samsung’s report
    highlights an important point here—synthetic media will democratize content creation
    [[5]](https://www.syntheticmedialandscape.com/). Now, anyone can produce high-quality
    content at low costs. This could democratize small-scale creators who could use
    synthetic media technology in the image/video synthesis space to bring their imagination
    to life without access to large film budgets. As we’ll discuss in the next section,
    we believe that synthetic media will usher in a new wave of creativity and art.
  prefs: []
  type: TYPE_NORMAL
- en: Another potential benefit of synthetic media is its ability to anonymize photos
    and videos to enhance privacy. In an HBO documentary about anti-gay and lesbian
    purges, *Welcome to Chechnya*, the film uses deepfake technology to guard the
    identities of the volunteers who told their stories to protect them from prosecution
    [[6]](https://www.nytimes.com/2020/07/01/movies/deepfakes-documentary-welcome-to-chechnya.xhtml).
    Similarly, we could also use synthetic media technology to anonymize our faces
    in images and videos on cameras in public spaces, retail stores, and social media
    accounts. Face anonymization can be used for privacy protection while preserving
    data utility.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, AI-generated media can also be a cause for concern. We can
    use the same technology to generate content (text, video, image, or speech) that
    is adversarial in nature. Malicious actors can disseminate intentionally misleading
    and adversarial narratives, which can disrupt discourse, create divisions, and
    undermine our trust in scientific, social, political, and economic institutions.
    The phenomenon of “seeing is believing” can also enable altered or inauthentic
    images and videos to spread more quickly. In this vein, in an article titled,
    “Deep Fakes: A Looming Challenge for Privacy, Democracy, and Social Security,”
    researchers identify a notable danger that they have termed *the liar’s dividend*.
    Here, the idea is that as the general public becomes more aware of how convincingly
    synthetic media can be generated, they may become more skeptical of the authenticity
    of traditional real documentary evidence [[7]](https://doi.org/10.2139/ssrn.3213954).
    We’ll discuss dis/misinformation, and its implications on individuals and society,
    in detail in chapter 5\.'
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic media has also infamously been used for celebrity pornographic videos,
    revenge porn or cybersexual harassment, and fraud and espionage. Deepfakes can
    be used to impersonate an authorized decision-maker for financial transactions
    and various cybersecurity problems, such as showing an executive committing a
    crime or creating fake financial statements. Finally, celebrities can also be
    synthetically generated for brand advertisements, which can result in a loss of
    intellectual property (IP) revenue. Later in this chapter, we’ll talk about IP
    and copyright problems related to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI or genuine: Detecting synthetic media'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are various ongoing efforts to detect AI-generated media. In early 2023,
    OpenAI released a work-in-progress classifier to distinguish between machine-generated
    and human-written text to help mitigate concerns about running automated misinformation
    campaigns, among other problems. They acknowledged that their “classifier is not
    fully reliable” by correctly identifying AI-written text 26% of the time (true
    positives) and incorrectly labeling the human-written text as AI-written text
    9% of the time (false positives). As of July 20, 2023, the classifier was taken
    down due to its low accuracy rate [[8]](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text).
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have explored various techniques to detect machine-generated or
    manipulated images, videos, and speech, including digital, physical, and semantic
    analysis. In the Media Forensics (MediFor) program from the Defense Advanced Research
    Projects Agency (DARPA), researchers produced manipulation indicators by looking
    for inconsistencies in pixel representation and the physical environment, in combination
    with the semantic interpretation of the media [[9]](https://www.darpa.mil/program/media-forensics).
    Are there any pixel-level errors? That is, are there blurred edges or replicated
    pixels? For the physical environment, they look to see if the laws of physics
    are violated—are the shadows, reflections, lighting, and so on consistent with
    the laws of nature? Finally, they look at semantic integrity, which helps determine
    if the contextual information related to the piece of content is contradictory
    or inconsistent. So, they look for whether the image has been placed out of context
    or repurposed, and whether there are any date and time inaccuracies [[10]](https://www.youtube.com/watch?v=Crfm3vGoBsM).
    This program was followed by DARPA’s Semantic Forensics (SemaFor) with the goal
    of not only detecting manipulated media but also characterizing if the media was
    generated or manipulated for malicious purposes, and attributing the origination
    of the content to an individual or organization [[11]](https://www.darpa.mil/news-events/2021-03-02).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there have been numerous studies on detecting face swapping by analyzing
    photo response nonuniformity (PRNU) [[12]](https://www.researchgate.net/profile/Zeno-Geradts/publication/329814168_Detection_of_Deepfake_Video_Manipulation/links/5c1bdf7da6fdccfc705da03e/Detection-of-Deepfake-Video-Manipulation.pdf)
    and inconsistent artifacts in images and videos, such as facial characteristics
    or physiological signals [[13]](https://arxiv.org/pdf/1806.02877.pdf) and image
    quality [[14]](https://ieeexplore.ieee.org/abstract/document/8987375). These techniques
    are promising but often limited, with solutions consisting of only detecting facial
    manipulations in a curated dataset. One study showed that entire generated faces
    can be detected via irregular pupil shapes, but the assumption of pupil shape
    regularity doesn’t always hold [[15]](https://arxiv.org/pdf/2109.00162.pdf). Other
    techniques to detect deepfakes include physiological analysis in videos to estimate
    whether the individual’s breathing and heart rate are normal [[16]](https://doi.org/10.1007/978-3-030-87664-7_12),
    and biometric analysis to analyze a specific individual’s mannerisms, including
    movement and style of speech, which can then be compared to distinguish fake from
    real [[17]](https://doi.org/10.3390/jimaging9010018). Biometric analysis has also
    been applied to deepfake audio detection, where audio analysis has proven to be
    effective in detecting deepfakes [[18]](https://arxiv.org/pdf/2209.14098.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Because of their adversarial nature, no single magic bullet can detect *all*
    the deepfakes *all* the time, and a majority of detection techniques tend to have
    a low generalization capability—if they encounter a novel manipulation type that
    hasn’t been seen in the training dataset, then their performances drop significantly
    [[17]](https://doi.org/10.3390/jimaging9010018). While there has been significant
    progress in deepfake detection and notable solutions for addressing certain artifacts
    of synthetic media generation, we hope that efforts to raise awareness will motivate
    researchers to solve the shortcomings of current datasets used for testing these
    techniques, as well as developing techniques to perform well across various kinds
    of deepfake manipulation and generation. At some point, it will likely become
    extremely difficult, perhaps impossible, to confidently detect manipulated media
    at scale purely based on specific image characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: While technical solutions are certainly crucial to countering AI-generated and
    manipulated media, they don’t solve the problem in its entirety. Media literacy
    efforts to educate and inform the public are also essential steps to effectively
    respond to this problem. For visual deepfakes, such as images and videos, we can
    use artifacts of the generated images to help distinguish them from real images.
  prefs: []
  type: TYPE_NORMAL
- en: 'While there isn’t a single tell-tale sign, image manipulations often use facial
    transformations, where we can pay attention to cheeks, forehead, eyes, eyebrows,
    lips, and facial hair. We can ask questions like these: Is the agedness of the
    skin consistent with the agedness of other facial features? Is the skin tone uneven?
    Are the shadows expected? Do facial hair transformations look natural? Is there
    not enough or too much glare with glasses? Does the person blink enough or too
    much? Do lip movements look natural? AI-generated images have also historically
    generated too many fingers on hands, given that hands are less visible than faces
    in many human images, which is what these models are trained on. In videos, the
    facial expressions or movements may not exactly line up with the voice. Generally,
    we’re looking for distortions with visual deepfakes. Additionally, media literacy
    efforts should emphasize understanding the source and context behind the content
    shared. Understanding the content’s origination, credibility, and context can
    help us decipher how much attention it should receive.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as discussed in chapter 3, appropriate legislation to govern the use
    of the technology and how it’s distributed will be fundamental to the responsible
    use and dissemination of synthetic media. The United States alone has introduced
    several synthetic media bills, especially concerning pornographic content and
    manipulation of the democratic process [[19]](https://www.malwarebytes.com/blog/news/2020/01/deepfakes-laws-and-proposals-flood-us).
    In parallel, social media companies, including Facebook, Twitter, Reddit, YouTube,
    and TikTok, have developed content-moderation policies to ban any deepfakes with
    malicious intent on their platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative AI: Transforming creative workflows'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In June 2022, *Cosmopolitan* fashion magazine unveiled the first cover made
    entirely by generative AI [[20]](https://www.cosmopolitan.com/lifestyle/a40314356/dall-e-2-artificial-intelligence-cover/).
    Synthetic media has opened up a new realm of possibilities for content creators.
    It has transformed creative work by eliminating monotonous tasks, increasing productivity
    and efficiency, and enabling people to express their creativity in new and unprecedented
    ways. From marketing and virtual influencers to art and film, we’ll unpack several
    creative applications of synthetic media in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Marketing applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Marketing applications are perhaps the most common commercial use case for generative
    AI. There are countless examples of how individuals and brands are using synthetic
    media to create content for marketing purposes, accelerating the delivery of personalized
    content while adhering to a brand’s style and tone. They range from creating social
    media and blog posts to developing marketing videos and visual branding. Jasper
    (see [www.jasper.ai/](https://www.jasper.ai/)), an AI content platform based on
    a collection of third-party models (including OpenAI’s GPT-3.5) and their own,
    is focused on content creation for businesses. It can produce various types of
    customer-facing content, including social media posts, website copy, emails, blogs,
    ads, and imagery. Jasper also can move between different formats, tones, and languages.
    The Jasper website boasts that they are “trusted by 100,000+ teams globally at
    innovative companies.”
  prefs: []
  type: TYPE_NORMAL
- en: Some brands are using DALL-E 2 and other image-generation tools for advertising.
    DALL-E 2 is an OpenAI model that can generate realistic images and art given a
    natural language description [[21]](https://openai.com/product/dall-e-2). Heinz
    put together a marketing campaign, “AI Ketchup,” based on OpenAI’s DALL-E 2—*EVEN
    A.I. KNOWS THAT KETCHUP IS HEINZ* [[22]](https://www.youtube.com/watch?v=LFmpVy6eGXs).
    As shown in figure 4.4, when we asked DALL-E 2 to create a series of generic ketchup-inspired
    pieces, the pictures overwhelmingly represented elements of Heinz’s signature
    branding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F04_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4 From left to right, prompts to DALL-E 2: an impressionist painting
    of a ketchup bottle, a five-year-old’s drawing of a ketchup bottle, and an astronaut
    in space holding a ketchup bottle'
  prefs: []
  type: TYPE_NORMAL
- en: Nestlé used DALL-E’s Outpainting feature, which helps users extend an image
    beyond its original borders by adding visual elements in the same style (see [http://mng.bz/z0JX](http://mng.bz/z0JX)).
    They advertised an extended version of Johannes Vermeer’s famous painting, *The
    Milkmaid*, generated by DALL-E’s Outpainting feature, which was used to help sell
    Nestlé’s yogurt and dessert brand, La Laitière. The ad, created by Ogilvy Paris
    (see [http://mng.bz/G98R](http://mng.bz/G98R)), a creative communications agency,
    extends the world of the original painting to show the kitchen maid preparing
    La Laitière–inspired treats [[23]](https://www.adweek.com/creativity/nestle-brand-is-latest-to-venture-into-brave-new-world-of-ai-art-direction/).
    Going back to the earlier example of an astronaut holding a ketchup bottle, we
    asked DALL-E Outpainting to extend the image, as shown in figure 4.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F05_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 The result of DALL-E’s Outpainting feature given the prompt “a burger
    in outer space without ketchup”
  prefs: []
  type: TYPE_NORMAL
- en: Creative agencies aren’t the only ones using generative AI for marketing applications—Ryan
    Reynolds, a Canadian American actor, asked ChatGPT to write a commercial for Mint
    Mobile in his voice using a joke, a curse word, and a callout to Mint’s holiday
    promo [[24]](https://www.fastcompany.com/90833253/ryan-reynolds-used-chatgpt-to-make-a-mint-mobile-ad-and-the-results-were-mildly-terrifying).
  prefs: []
  type: TYPE_NORMAL
- en: As of May 2023, 19-year-old Miquela Sousa has 2.8 million followers on Instagram
    and 3.6 million followers on TikTok. More famously known as Lil Miquela, she is
    one of *TIME Magazine*’s 25 Most Influential People on the Internet and is known
    to support Black Lives Matter, reproductive rights, and LGBTQ+ causes. She has
    also appeared in Calvin Klein ads, alongside American model Bella Hadid [[25]](https://www.elle.com/uk/fashion/a27492073/bella-hadid-calvin-klein-lil-miquela/).
    But Lil Miquela isn’t real—she is the most famous example of a virtual influencer,
    created by LA-based startup, Brud. Lil Miquela’s creators closed a $125 million
    Series B round in 2019 taking a bet on virtual influencers becoming the future
    of ads, fashion, and commerce [[26]](https://techcrunch.com/2019/01/14/more-investors-are-betting-on-virtual-influencers-like-lil-miquela/).
    Generative AI has increased the creation of virtual influencers, quickly being
    adopted in the workflows of their content production pipeline. Esther Olofsson,
    a Swedish virtual influencer, uses four AI tools, including Stable Diffusion (a
    text-to-image model) to generate 3D images of Esther, and ChatGPT to generate
    her captions on Instagram. Creators of virtual influencers believe that synthetic
    media can scale their creative output and earning power, with the ability to generate
    a boundless amount of content without the real-world constraints of human influencers.
    Yet, virtual influencers also raise ethical questions for their creators, specifically
    around cultural appropriation and representation for creators who create virtual
    influencers with different demographic characteristics than their own. Virtual
    dark-skinned influencer, Shudu Gram, has been critiqued as “contrived by a white
    man who has noticed the ‘movement’ of dark-skinned women” by social theorist Patricia
    Hill Collins [[27]](https://journals.sagepub.com/doi/full/10.1177/1527476420983745)
    [[28]](https://www.newyorker.com/culture/culture-desk/shudu-gram-is-a-white-mans-digital-projection-of-real-life-black-womanhood).
  prefs: []
  type: TYPE_NORMAL
- en: Artwork creation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Artistic creation is another area that has been disrupted by generative AI.
    In 2018, the *Portrait of Edmond Belamy* was the first widely covered sale of
    an AI-generated artwork. The fictional portrait, created by Obvious, a Paris-based
    collective, was sold for a whopping $432,500 [[29]](https://news.artnet.com/market/first-ever-artificial-intelligence-portrait-painting-sells-at-christies-1379902).
  prefs: []
  type: TYPE_NORMAL
- en: While algorithms have been used to generate art since the 1960s [[30]](https://www.researchgate.net/publication/311104742_Algorithmic_Art_and_Its_Art-Historical_Relationships),
    AI-generated art can produce art (image, film/video, and music) without an explicit
    set of programming instructions that have been provided by human artists. AI tools
    such as DALL-E 2, Stable Diffusion, Midjourney, and WOMBO Dream can be used to
    quickly create artworks given any descriptive text input. Although some artists
    have expressed concerns about copyright problems with these tools (explored later
    in section 4.3), they have also been a source of creative inspiration for many.
    Creators have used DALL-E to create fan art, comic books, and design sneakers
    (someone made a pair for Sam Altman, cofounder of OpenAI, after he tweeted them
    [[31]](https://twitter.com/sama/status/1539670012536844289)). Tattoo artists are
    using DALL-E to generate tattoo designs together with their clients, while animation
    studios are using DALL-E to design characters and environments [[32]](https://www.technologyreview.com/2022/12/16/1065005/generative-ai-revolution-art/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another well-known AI art generator tool is Google’s DeepDream, which takes
    an image as an input and outputs abstract, psychedelic art. The core idea behind
    generating these psychedelic images is to ask the network: “whatever you see there,
    I want more of it!” (see [http://mng.bz/0lYl](http://mng.bz/0lYl)). In practice,
    this means that the model amplifies any patterns that it sees in the image. Figure
    4.6 illustrates this idea by using the example image from DALL-E Outpainting (refer
    to figure 4.5) as a base image for DeepDream.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F06_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 DeepDream applied to figure 4.5 with the input prompt, “a portrait
    of a beautiful female knight in silver armor with intricate golden details”
  prefs: []
  type: TYPE_NORMAL
- en: Filmmakers have also been provided with new tools for creative possibilities.
    Generative AI is changing the way films are conceptualized, developed, and produced.
    The Writers Guild of America (WGA) is the first labor organization to take on
    generative AI—“The challenge is we want to make sure that these technologies are
    tools used by writers and not tools used to replace writers,” says John August,
    a member of the WGA’s 2023 negotiating committee [[33]](https://www.hollywoodreporter.com/business/business-news/writers-strike-ai-chatgpt-1235478681/).
    Filmmakers can generate scripts, storyboards, and scenes—as previously discussed,
    independent filmmakers can use generative AI to create compelling stories and
    visual elements without the need for a large budget, while studios can draw inspiration
    from these tools and use them to streamline content. Generative AI can also be
    used for improved visual effects by creating enhanced characters and environments
    without a manual labor-intensive process.
  prefs: []
  type: TYPE_NORMAL
- en: 'A controversial application is the ability to render the dead digitally. In
    the 2016 film, *Rogue One: A Star Wars Story*, filmmakers used face-swapping technology
    to digitally recreate the character played by the late Peter Cushing, who died
    in 1994 [[34]](https://www.polygon.com/2016/12/27/14092060/rogue-one-star-wars-grand-moff-tarkin-princess-leia).
    As for the ethics of digitally resurrecting dead actors, John Knoll, *Rogue One:
    A Star Wars Story*’s visual effects supervisor, said, “We weren’t doing anything
    that I think Peter Cushing would’ve objected to. I think this work was done with
    a great deal of affection and care. We know that Peter Cushing was very proud
    of his involvement in Star Wars and had said as much, and that he regretted that
    he never got a chance to be in another Star Wars film because George [Lucas] had
    killed off his character” [[35]](https://www.theguardian.com/film/2017/jan/16/rogue-one-vfx-jon-knoll-peter-cushing-ethics-of-digital-resurrections).
    Filmmakers are also using generative AI to accelerate the postproduction workflow
    with assistance in editing footage, applying visual effects, sound design, and
    more. Finally, as with every industry, filmmakers can use generative AI for creative
    inspiration.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI has also been a source of inspiration for architects and designers—one
    such example is the project, *This House Does Not Exist* (see [https://thishousedoesnotexist.org](https://thishousedoesnotexist.org)),
    which generates AI renderings of homes and buildings that don’t currently exist.
    AI-generated tools are making strides in architecture, with designers using them
    to rapidly iterate solutions that can then be augmented and tested using existing
    tools [[36]](https://www.elledecor.com/life-culture/a42711299/generative-ai-design-architecture/).
  prefs: []
  type: TYPE_NORMAL
- en: In a similar vein, musicians are also exploring how humans and machines can
    collaborate, rather than compete. Pianist David Dolan performed with a semiautonomous
    AI system at the Stockholm University of the Arts, showing how generative AI may
    creatively supplement music [[37]](https://www.youtube.com/watch?v=sIFbvgmYBA0).
    The AI system was designed and overseen by Kingston University researcher Oded
    Ben-Tal, who says that musicians can use AI with pianists to improvise outside
    of their skillset or draw inspiration from AI compositions, for now [[38]](https://www.wired.co.uk/article/generative-ai-music).
  prefs: []
  type: TYPE_NORMAL
- en: Musician Holly Herndon also used AI to clone her voice, dubbed Holly+, which
    she uses to sing in languages and styles she is unable to [[39]](https://www.youtube.com/watch?v=5cbCYwgQkTE).
    Holly+ is free to use by anyone, with Herndon and her team developing tools for
    anyone to be able to make art with her image and voice (see [https://holly.plus/](https://holly.plus/)).
    Sir Paul McCartney and The Beatles released a new tune, “Now and Then,” in November
    2023, by using generative AI to resurrect the voice of fellow bandmate, John Lennon
    [[40]](https://www.cnn.com/2023/06/13/entertainment/paul-mccartney-ai-beatles-song/index.xhtml).
    While these tools present an opportunity for musicians, some are worried about
    AI-generated music flooding streaming platforms and competing with real musicians.
    There are, of course, copyright concerns as well, which we’ll discuss in the next
    section. Universal Media Group, which backs superstars such as Taylor Swift and
    Nicki Minaj, urged Spotify and Apple Music to prohibit AI tools from scraping
    copyrighted songs [[41]](https://www.ft.com/content/aec1679b-5a34-4dad-9fc9-f4d8cdd124b9).
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an ongoing debate about whether AI-generated art should be considered
    art in the same way that human-generated art is, whether artists will be replaced,
    and, more broadly, what this means for creativity. In defense of AI-generated
    art, artists argue that the AI tool is a medium of conveying the significance
    or meaning that lies in the human mind, similar to a brush and a palette or a
    camera. Anna Ridler, an artist known for her work with GANs, believes that the
    idea of replacing artists comes from undermining the artistic process—she says:'
  prefs: []
  type: TYPE_NORMAL
- en: 'AI can’t handle concepts: collapsing moments in time, memory, thoughts, emotions—all
    of that is a real human skill, that makes a piece of art rather than something
    that visually looks pretty. [[42]](https://www.theguardian.com/technology/2022/nov/12/when-ai-can-make-art-what-does-it-mean-for-creativity-dall-e-midjourney)'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of replacing artists, AI-generated art can be understood as a collaboration
    between humans and machines.
  prefs: []
  type: TYPE_NORMAL
- en: Intellectual property in the LLM era
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While synthetic media pushes the boundaries of art, the tools and models used
    to create it are testing the boundaries of the legal system. In the following
    section, we’ll take a look at the relevant policy governing the collection of
    open web data, including text and images, and the generation of synthetic media
    using models trained on those collections.
  prefs: []
  type: TYPE_NORMAL
- en: Copyright law and fair use
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pablo Picasso, one of the most renowned painters of the 20th century, allegedly
    said, “Good artists copy; great artists steal” [[43]](https://www.bbc.com/culture/article/20141112-great-artists-steal).
    It’s common practice in the literary and fine arts to imitate the styles of others,
    and it’s often seen as a prerequisite for creative success. Of course, such imitation
    has its limits, which are encoded into law as intellectual property (IP). The
    conception of IP as a type of property over which one could claim legal ownership
    dates back to England in the 17th century [[44]](https://www.eff.org/issues/intellectual-property/the-term).
    In the United States, Section 8 of Article I of the Constitution reads that Congress
    shall have the power
  prefs: []
  type: TYPE_NORMAL
- en: to promote the progress of science and useful arts, by securing for limited
    times to authors and inventors the exclusive right to their respective writings
    and discoveries. [[45]](https://www.archives.gov/founding-docs/constitution-transcript)
  prefs: []
  type: TYPE_NORMAL
- en: While there are several different types of IP protections—patents for inventions,
    trademarks for corporate logos and symbols, trade secrets for proprietary information
    such as the formula for Coca-Cola—the most contentious legal questions for generative
    AI are about the potential copyright infringements in model training and model
    generations.
  prefs: []
  type: TYPE_NORMAL
- en: Copyrights are exclusive rights to a work of creative expression, whether that’s
    an image, a text, a movie, or a song. Typically, the owner of the copyright is
    the only one authorized to copy, distribute, display, or perform the work for
    a limited period of time, after which the work enters the public domain (in the
    United States, the copyright dates from the time that the work is created, and
    the standard term lasts until 70 years after the death of the creator) [[46]](https://www.copyright.gov/help/faq/faq-duration.xhtml).
    The US Copyright Office has stated their policy to be that text, images, and other
    media generated by AI aren’t eligible for copyright protections, although works
    by humans that have AI-generated elements might be, as long as there is sufficient
    human creativity involved [[47]](https://www.federalregister.gov/documents/2023/03/16/2023-05321/copyright-registration-guidance-works-containing-material-generated-by-artificial-intelligence).
    The most pressing current legal question around LLMs, as well as generative image
    models, isn’t whether their work is copyrightable, but whether they are actually
    violating existing copyrights of artists and writers whose works comprise their
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Copyrights are exclusive rights to a work of creative expression, whether that’s
    an image, a text, a movie, or a song.
  prefs: []
  type: TYPE_NORMAL
- en: Despite copyrights offering exclusive rights for use, these rights are by no
    means absolute. *Fair use* is the legal doctrine that outlines when it’s acceptable
    to use copyrighted material without requiring the permission of the holder of
    the copyright [[48]](https://www.copyright.gov/fair-use/). For example, courts
    have typically considered parody to be fair use, which is why “Weird Al” Yankovic
    can commercially sell melodic duplicates (e.g., “Eat It” and “Like a Surgeon”)
    of copyrighted songs with his own comical lyrics (though Yankovic states on his
    website that he gets permission from the original writers anyway to maintain relationships
    he has built over the years). [[49]](https://www.weirdal.com/archives/faq/) As
    defined in the US Copyright Act of 1976, fair use hinges on four factors, as shown
    in figure 4.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F07_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 The four factors that determine fair use of copyrighted materials
    [[50]](https://www.law.cornell.edu/uscode/text/17/107)
  prefs: []
  type: TYPE_NORMAL
- en: The first factor, “the purpose and character of the use,” refers to how and
    why the copyrighted material is used. Commercial use is less likely to be deemed
    fair as compared to nonprofit or educational purposes—for example, a college professor
    could distribute printouts of a painting for an art history lecture, but you might
    get in trouble for selling T-shirts with that same painting printed on them. “Transformative
    use” is another case that falls under this first factor. Essentially, US courts
    have found that when the character of the use is *transformative*, adding a new
    element that fundamentally changes the work, that isn’t a copyright violation.
    Transformative use also hinges on the derivate work being used for a purpose that
    is different from the consumption or enjoyment of the original work and is an
    important defense for companies that develop LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The second factor, “the nature of the copyrighted work,” refers to the varying
    degrees of protection that different types of materials enjoy. Because the original
    intent of copyright was to incentivize free and creative expression, the use of
    more “creative” works, such as songs, plays, and novels, is more likely to be
    deemed fair use as compared to factual or technical copyrighted works. In other
    words, you could argue that referencing lines of poetry in a new verse is fair
    use, but it’d be harder to do the same for a piece of investigative reporting.
  prefs: []
  type: TYPE_NORMAL
- en: The third factor assesses how much of the original source material was reused.
    If it’s a substantial portion or nearly all of it, that is less likely to be deemed
    fair use than a small amount.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth and final factor refers to if and how the use of copyrighted material
    will affect the market for that work. If an unauthorized seller is distributing
    a new movie online, for example, that would pose a serious threat to the digital
    sales or streaming revenue for that movie. Uses that hurt the market for the original
    work are unlikely to be considered fair [[48]](https://www.copyright.gov/fair-use/).
  prefs: []
  type: TYPE_NORMAL
- en: 'If all of this seems a little blurry, that’s because it is—none of these single
    factors are hard-and-fast rules, and they are all weighed against each other if
    a copyright suit is brought. Before turning to the lawsuits that have been brought
    already against developers of LLMs, though, let’s first examine a case that hinges
    similarly on the use of vast amounts of copyrighted text from the internet: *Authors
    Guild v. Google* [[51]](https://www.copyright.gov/fair-use/summaries/authorsguild-google-2dcir2015.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: In 2015, Google collaborated with several major research libraries to digitize
    their collections of books—some 20 million volumes. The tech giant accessed the
    books through partnerships, scanned them, and allowed people to search them for
    text snippets, all without the permission of the copyright owners, and without
    paying licensing fees. The case made it to the Second Circuit Court of Appeals,
    which concurred with a lower court’s opinion that Google’s digitization efforts
    constituted fair use because the search functionality gave the public access to
    information *about* the books that they wouldn’t otherwise have, and because even
    though Google used the full text of the books, they only returned the snippets
    of matching text, rather than making the entire books available. This concept
    of using the entirety of source material for a fundamentally different tool is
    analogous to the training of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the LLMs we’ve discussed thus far would seem to be protected by
    fair use because the model is a very different work than any of the documents,
    and thus the use of the materials is transformative. Complicating matters, users
    have shown that it’s occasionally possible to get LLMs to regurgitate text verbatim.
    It’s difficult to show examples of “memorizing” source material consistently,
    due to the probabilistic nature of LLMs. Because of the lack of understanding
    as to exactly what LLMs learn, even their developers are unlikely to be able to
    say for sure when the model will reproduce phrases or texts word for word. Still,
    under the precedent of *Authors Guild v. Google*, the odds seem considerably in
    favor of LLMs being considered fair use.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs aren’t the only generative models making a splash in copyright—as mentioned
    previously, there are impressive generative models capable of creating all types
    of synthetic media, including images, audio, and videos. Some of the most popular
    models, including Midjourney and Stable Diffusion, are text-to-image models: users
    can describe what they want their picture to look like, and the model will generate
    it for them.'
  prefs: []
  type: TYPE_NORMAL
- en: Just like LLMs, generative image models train on huge amounts of data collected
    from the internet. As with text datasets such as Common Crawl, there are common
    image datasets, such as LAION-5B, a dataset of 5.8 billion images compiled by
    the nonprofit Large-scale Artificial Intelligence Open Network (LAION). LAION-5B
    is used by Stability AI, the developer of Stable Diffusion, and other companies;
    it’s made up of images that are publicly available online, including stock photos
    and editorial photography. One German photographer, upon discovering that some
    of his stock images were used in LAION-5B, requested that they be removed; LAION
    responded that to fulfill such a request would be impossible because the database
    contained only links to images, so nothing was stored, and they could not readily
    identify which images were from his portfolio. German copyright law—like in many
    countries—does allow data mining if the data is “lawfully accessible” and deleted
    later, but the emergence of generative models has brought the problem under greater
    scrutiny [[52]](https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead).
    Stability AI later announced that they would honor opt-out requests from artists
    whose work was included in the LAION dataset [[53]](https://arstechnica.com/information-technology/2022/12/stability-ai-plans-to-let-artists-opt-out-of-stable-diffusion-3-image-training/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Stability is also currently being sued by Getty Images for using more than
    12 million photographs from the Getty collection [[52]](https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead),
    [[54]](https://fingfx.thomsonreuters.com/gfx/legaldocs/byvrlkmwnve/GETTY%20IMAGES%20AI%20LAWSUIT%20complaint.pdf).
    In the complaint, the plaintiffs write:'
  prefs: []
  type: TYPE_NORMAL
- en: At great expense, over the course of nearly three decades, Getty Images has
    curated a collection of hundreds of millions of premium quality visual assets . . . Many
    of these images were created by Getty Images staff photographers as works made-for-hire,
    others have been acquired by Getty Images from third parties with an assignment
    of its associated copyrights, and the remainder have been licensed to Getty Images
    by its hundreds of content partners or hundreds of thousands of contributing photographers,
    who rely on the licensing income Getty Images generates for them. [[55]](https://fingfx.thomsonreuters.com/gfx/legaldocs/byvrlkmwnve/GETTY%20IMAGES%20AI%20LAWSUIT%20complaint.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'The subtext is clear: generative AI models pose an existential threat to Getty
    and stock photography as an industry. Getty hopes to be compensated for their
    contributions and perceived copyright infringement, but as with the large text
    datasets, it’s difficult to ascertain how much information the model retains from
    any single image, and, again, the use by Stability AI would seem to be transformative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amusingly, Getty might have a stronger case due to an artifact of the training
    data: the complaint further alleges the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Often, the output generated by Stable Diffusion contains a modified version
    of a Getty Images watermark, creating confusion as to the source of the images
    and falsely implying an association with Getty Images. While some of the output
    generated through the use of Stable Diffusion is aesthetically pleasing, other
    output is of much lower quality and at times ranges from the bizarre to the grotesque.
    Stability AI’s incorporation of Getty Images’ marks into low quality, unappealing,
    or offensive images dilutes those marks in further violation of federal and state
    trademark laws. [[55]](https://fingfx.thomsonreuters.com/gfx/legaldocs/byvrlkmwnve/GETTY%20IMAGES%20AI%20LAWSUIT%20complaint.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion or possibly its users could be found in violation of trademarks
    if the Getty Images watermark appears on images, although Stability AI will undoubtedly
    move quickly to address this behavior. Altogether, this is a relatively untested
    area of law.
  prefs: []
  type: TYPE_NORMAL
- en: Things get even dicier when a model has not only learned an image captured by
    a human artist but also encoded the style of that artist. In addition to generating
    photorealistic renderings, generative models such as Midjourney and Stable Diffusion
    are also capable of producing artwork in particular styles, as discussed in section
    Generative AI in Creative Workflows. Style isn’t generally copyrightable, but
    it’s easy to see how artists might think such imitation could devalue or diminish
    their work. Sarah Andersen, a prominent cartoonist who publishes webcomics under
    the “Sarah’s Scribbles” collection, wrote a *New York Times* opinion essay about
    her experience of alt-right internet trolls co-opting her comics by editing the
    words and frames to change their meaning. Figure 4.8 shows an example of artwork
    generated by an AI tool in her artistic style—with clearly garbled text, but some
    visual elements of Andersen’s work present. “When I checked the website [https://haveibeentrained.com](https://haveibeentrained.com),
    a site created to allow people to search LAION data sets, so much of my work was
    on there that it filled up my entire desktop screen,” Andersen attested, and worried
    that the AI tools would be used to twist her creations again [[56]](https://www.nytimes.com/2022/12/31/opinion/sarah-andersen-how-algorithim-took-my-work.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Andersen is one of three plaintiffs, with Karla Ortiz and Kelly McKernan, in
    a class-action lawsuit brought against Midjourney, Stability AI, and DeviantArt.
    Like Andersen, McKernan and Ortiz similarly found that the tools could generate
    images in their styles in a way that felt personally invasive. “They trained these
    models with our work. They took away our right to decide whether they wanted to
    be a part of this or not,” said Ortiz [[56]](https://www.nytimes.com/2022/12/31/opinion/sarah-andersen-how-algorithim-took-my-work.xhtml)
    [[57]](https://www.buzzfeednews.com/article/pranavdixit/ai-art-generators-lawsuit-stable-diffusion-midjourney).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F08_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 An AI-created image using an open source image-generation model with
    the prompt “Sarah Andersen webcomic”
  prefs: []
  type: TYPE_NORMAL
- en: While it remains to be seen how Andersen, Ortiz, and McKernen’s suit will play
    out, these tools continue to be used by people around the world to generate and
    experiment with novel art forms. The permissive structure of fair use means that
    any substantial changes to the status quo would require a new precedent for use
    in training AI models. Yet, at the same time, many of the datasets and models
    that we’re talking about are already open source, meaning that anyone can either
    train their own model or make a new version of an existing one. Regardless of
    whether any particular company changes its dataset construction procedure, or
    ends up paying damages or licensing fees, AI-generated art, from comics to music
    to poetry, is here to stay.
  prefs: []
  type: TYPE_NORMAL
- en: Open source and licenses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve mentioned already that due to the enormous scale of data and compute required
    to produce LLMs, the exercise has thus far largely been left to a few major tech
    companies and some well-funded startups. That is already changing due to the open
    source community. *Open source* refers to the source code of software being open
    and available to the public for reuse and modification. More than that, open source
    is a movement, whose advocates believe that open source software is a public good
    and leads to better software through more collaboration and participation, as
    well as lower barriers to entry. Similarly, the open data movement proponents
    suggest that when data is widely accessible, the public will be more informed,
    so data collected or produced by government and nonprofit organizations, scientific
    research, and other entities should be freely available to use and build on.
  prefs: []
  type: TYPE_NORMAL
- en: Open source refers to the source code of software being open and available to
    the public for reuse and modification.
  prefs: []
  type: TYPE_NORMAL
- en: 'If anything, generative image models have been ahead of LLMs in this respect.
    Because of a keen interest in computer vision models, academics have compiled
    large image datasets since Fei-Fei Li, a professor of computer science at Stanford
    University, began a project called ImageNet. In 2006, Li had the prophetic idea
    that the biggest gains to be made in computer vision weren’t necessarily from
    new, better algorithms, but from better (and bigger) data. She began creating
    a database, ImageNet, that would eventually be composed of millions of images
    depicting hundreds of things: animals, household objects, land formations, and
    many other categories. After much initial skepticism, ImageNet became a standard
    against which all computer vision models measured their results. Not only did
    it kick-start the object detection problem (which is now considered “solved” on
    ImageNet, as state-of-the-art models can perform nearly perfectly), but it ushered
    in an era of sharing benchmark datasets for training and testing models. Of ImageNet’s
    influence, Li said, “There is a lot of mushrooming and blossoming of all kinds
    of datasets, from videos to speech to games to everything.” Of course, it was
    also a proof point for her original hypothesis, which was later also borne out
    by the success of LLMs [[58]](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world).'
  prefs: []
  type: TYPE_NORMAL
- en: Across problem domains from natural language to images and videos, then, it
    pays to be greedy for data. Like later datasets, ImageNet was assembled from pictures
    from the internet and then labeled by workers on Amazon Mechanical Turk, a crowdsourcing
    platform. By writing a minimal amount of code, people can compile text and image
    data by programmatically accessing web pages and copying their contents. This
    practice is called *web scraping*, which has been repeatedly found to be legal
    [[59]](https://techcrunch.com/2022/04/18/web-scraping-legal-court/) as long as
    the data is publicly available—so almost anything that you would see by browsing
    online. Any website that is indexed by search engines, for example, is scraped
    by bots. Some of the companies that operate websites that are frequent data sources
    for LLMs, including Reddit, Twitter, and Stack Overflow, have publicly stated
    plans to charge AI developers to use that data, though it’s unclear what this
    would look like in practice—most likely, they would sell datasets that obviate
    the need for scraping [[60]](https://www.zdnet.com/article/stack-overflow-joins-reddit-and-twitter-in-charging-ai-companies-for-training-data/).
    People who maintain websites can add a robots.txt file, which is essentially a
    set of instructions for a bot, to tell the bot which pages it can scrape and which
    it shouldn’t. In practice, robots.txt files are only advisories, and malicious
    programs can easily ignore them.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are few legal restrictions for publicly available web content,
    both code and data have licenses. Some open source licenses explicitly allow all
    types of derivative uses. The MIT License, for example, is a permissive software
    license—in fact, the most popular license on GitHub—that allows for reuse within
    proprietary software [[61]](https://github.blog/2015-03-09-open-source-license-usage-on-github-com/).
    Other licenses allow reuse only for noncommercial purposes; still others might
    allow reuse with attribution, or several other conditions. Code and data licenses
    are legally enforceable [[62]](https://www.nytimes.com/2008/08/14/technology/14commons.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Code licenses are a central question in a class-action lawsuit brought by software
    developers against Microsoft, GitHub, and OpenAI over the LLM tool Copilot. Copilot
    is based on a variant of OpenAI’s GPT-3 model that is tailored especially for
    writing code, and it’s trained on thousands of GitHub repositories. Like the copyright
    question, there is litigation over the use of this code for training LLMs; it’s
    unclear how relying on licensing instead of fair use would work. The plaintiffs
    in the case argue that the use amounts to “software piracy on an unprecedented
    scale,” while the defendants say that it’s the plaintiffs who are undermining
    the principles of open source by requesting “an injunction and multi-billion dollar
    windfall” for “software that they willingly share” [[63]](https://www.theverge.com/2023/1/28/23575919/microsoft-openai-github-dismiss-copilot-ai-copyright-lawsuit).
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, companies such as Hugging Face are bullish on open source principles,
    building and hosting models and datasets that are free to use [[64]](https://huggingface.co/).
    People unaffiliated with any of the prestigious AI labs are nonetheless able to
    access and, in some cases, improve upon state-of-the-art results in this ecosystem
    of rapid iteration and sharing. This carries with it certain risks because any
    limits put in to reduce certain harms can be removed by downstream users. It will
    be harder to prevent the creation of copycat content or enforce existing copyrights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, there are reasons to be hopeful that these problems won’t stifle creativity,
    but foster it. Cory Doctorow, an internet activist and author, has long been critical
    of copyright, pointing out that while the terms of these rights have gotten longer
    and broader over time, creators haven’t reaped the profits—companies that purchase
    their copyrights have [[65]](https://doctorow.medium.com/copyright-wont-solve-creators-generative-ai-problem-92d7adbcc6e6).
    Skeptical of broadening copyright even further to prevent generative models from
    accessing those works for their training, Doctorow wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fundamentally, machine learning systems ingest a lot of works, analyze them,
    find statistical correlations between them, and then use those to make new works.
    It’s a math-heavy version of what every creator does: analyze how the works they
    admire are made, so they can make their own new works. If you go through the pages
    of an art-book analyzing the color schemes or ratios of noses to foreheads in
    paintings you like, you are not infringing copyright. We should not create a new
    right to decide who is allowed to think hard about your creative works and learn
    from them—such a right would make it impossible for the next generation of creators
    to (lawfully) learn their craft. [[65]](https://doctorow.medium.com/copyright-wont-solve-creators-generative-ai-problem-92d7adbcc6e6)'
  prefs: []
  type: TYPE_NORMAL
- en: People may reasonably disagree over whether and how large-scale models should
    be trained on copyrighted data. It’s certain that we’ll get more clarity from
    a legal perspective as these cases continue to progress and as precedents are
    established. But earlier artists also worried over the invention of photography
    that no one would continue to paint or purchase paintings because they could no
    longer compete with the camera in the depiction of reality. Instead, artists continued
    to paint, but they conveyed scenes with their own interpretations and expressions
    [[66]](https://www.thecollector.com/how-photography-transformed-art/). It seems
    possible that generative models will become another medium, without ever entirely
    fulfilling the human need for beauty nor replacing the human impulse toward creativity.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic media, or more specifically, AI-generated media, is an umbrella term
    for content that has been created or altered with the help of AI, which spans
    text, image, video, voice, and data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term *deepfak**e*—a portmanteau of “deep learning” and “fake”—is sometimes
    used synonymously with visual synthetic media, but it often has a negative connotation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders use neural networks to compress and decompress images, and they
    are often used in face-swapping technology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs consist of two neural networks—a generator and a discriminator. The generator
    exists to create new data, such as images, and the discriminator verifies the
    authenticity of an image by comparing it to the training dataset to determine
    the difference between a fake and a real image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthetic media is democratizing content creation and creativity for everyone
    while ushering in a new wave of creativity and art.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI has also infamously been used to create mis/disinformation content,
    celebrity pornographic videos, revenge porn or cybersexual harassment, and fraud
    and espionage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A holistic approach to detect AI-generated media encompassing technical solutions,
    media literacy and education, and appropriate legislation to govern the use of
    the technology is essential to countering deepfakes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI tools have transformed creative work by eliminating monotonous
    tasks, increasing productivity and efficiency, and enabling people to express
    their creativity in new and unprecedented ways.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Companies that develop LLMs have been accused of infringing on others’ intellectual
    property, specifically copyrights, via the training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the United States, fair use of copyrighted material is allowed without permission,
    and fair use is determined by four factors as established in the Copyright Act
    of 1976.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although there are pending lawsuits, it seems that most of the activity in the
    generative AI space would be considered fair use under current precedent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Open source* refers to the practice of making the source code of software
    accessible to the public to modify and reuse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The open source and open data movements have accelerated developments and continue
    to drive progress in AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
