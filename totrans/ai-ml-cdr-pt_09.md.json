["```py\ndef tokenize(text):\n    tokens = text.lower().split()\n    return tokens\n\ndef create_word_dictionary(word_list):\n    # Create an empty dictionary\n    word_dict = {}\n    word_dict[\"UNK\"] = 0\n    # Counter for unique values\n    counter = 1\n\n    # Iterate through the list and assign numbers to unique words\n    for word in word_list:\n        if word not in word_dict:\n            word_dict[word] = counter\n            counter += 1\n\n    return word_dict\n\n```", "```py\ndata=\"In the town of Athy one Jeremy Lanigan \\n \n      Battered away til he hadnt a pound. ...\"\n\ntokens = tokenize(data)\nword_index = create_word_dictionary(tokens)\n\ntotal_words = len(tokenizer.word_index) + 1\n\n```", "```py\ndef text_to_sequence(sentence, word_dict):\n    # Convert sentence to lowercase and split into words\n    words = sentence.lower().strip().split()\n\n    # Convert each word to its corresponding number\n    number_sequence = [word_dict[word] for word in words]\n\n    return number_sequence\n\ndef pad_sequences(sequences, max_length=None):\n    # If max_length is not specified, find the length of the longest sequence\n    if max_length is None:\n        max_length = max(len(seq) for seq in sequences)\n\n    # Pad each sequence with zeros at the beginning\n    padded_sequences = []\n    for seq in sequences:\n        # Calculate number of zeros needed\n        num_zeros = max_length – len(seq)\n        # Create padded sequence\n        padded_seq = [0] * num_zeros + list(seq)\n        padded_sequences.append(padded_seq)\n\n    return padded_sequences\n```", "```py\ncorpus = data.lower().split(\"\\n\")\n\ninput_sequences = []\nfor line in corpus:\n    token_list = text_to_sequence(line, word_index)\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n\nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = pad_sequences(input_sequences, max_sequence_len)\n```", "```py\ndef split_sequences(sequences):\n    # Create xs by removing the last element from each sequence\n    xs = [seq[:–1] for seq in sequences]\n\n    # Create labels by taking just the last element from each sequence\n    labels = [seq[–1:] for seq in sequences]  \n    # Using [–1:] to keep it as a single-element list\n    # Alternative if you want labels as single numbers instead of lists:\n    # labels = [seq[–1] for seq in sequences]\n\n    return xs, labels\nxs, labels = split_sequences(input_sequences)\n```", "```py\ndef one_hot_encode_with_checks(value, corpus_size):\n    # Check if value is within valid range\n    if not 0 <= value < corpus_size:\n        raise ValueError(f\"Value {value} is out of range for corpus size \n                                 {corpus_size}\")\n    # Create and return one-hot encoded list\n    encoded = [0] * corpus_size\n    encoded[value] = 1\n    return encoded\n```", "```py\nclass LSTMPredictor(nn.Module):\n    def __init__(self, total_words, embedding_dim=8, hidden_dim=None):\n        super(LSTMPredictor, self).__init__()\n\n        # If hidden_dim not specified, use max_sequence_len-1 as in TF version\n        if hidden_dim is None:\n            hidden_dim = max_sequence_len–1\n\n        # Embedding layer\n        self.embedding = nn.Embedding(total_words, embedding_dim)\n\n        # Bidirectional LSTM\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            bidirectional=True,\n            batch_first=True\n        )\n\n        # Final dense layer (accounting for bidirectional LSTM)\n        self.fc = nn.Linear(hidden_dim * 2, total_words)\n\n        # Softmax activation\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        # Embedding layer\n        x = self.embedding(x)\n\n        # LSTM layer\n        lstm_out, _ = self.lstm(x)\n\n        # Take the output from the last time step\n        lstm_out = lstm_out[:, –1, :]\n\n        # Dense layer\n        out = self.fc(lstm_out)\n\n        # Softmax activation\n        out = self.softmax(out)\n\n        return out\n```", "```py\n# Training setup\ntotal_words = len(word_index)\nmodel = LSTMPredictor(total_words)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n```", "```py\ninput_text = \"In the town of Athy\"\n```", "```py\n# Convert text to lowercase and split into words\nwords = input_text.lower().strip().split()\n\n# Convert words to numbers using the word dictionary, use 0 for unknown words\nnumber_sequence = [word_dict.get(word, 0) for word in words]\n\n# Pad the sequence\npadded_sequence = [0] * (sequence_length – len(number_sequence)) \n                      + number_sequence\n\n```", "```py\ninput_tensor = torch.LongTensor([padded_sequence])\n```", "```py\n# Get prediction\nwith torch.no_grad():  # No need to track gradients for prediction\n    output = model(input_tensor)\n```", "```py\n# Get the predicted word index (highest probability)\npredicted_idx = torch.argmax(output[0]).item()\n\n```", "```py\n{'UNK': 0, 'in': 1, 'the': 2, 'town': 3, 'of': 4, 'Athy': 5, \n `'``one``'``:` `6``,` 'Jeremy': 7, 'Lanigan': 8,\n\n```", "```py\n# Create reverse dictionary to convert number back to word\nreverse_dict = {v: k for k, v in word_dict.items()}\n\n# Convert predicted index to word\npredicted_word = reverse_dict[predicted_idx]\n\n```", "```py\nTop 5 predictions:\none: 0.9999\nyoull: 0.0000\ndidnt: 0.0000\ncreature: 0.0000\nnelly: 0.0000\n```", "```py\nTop 5 predictions:\nhis: 0.7782\ngo: 0.1393\nbellows,: 0.0605\naccident: 0.0090\ntil: 0.0048\n```", "```py\ndef generate_sequence(model, initial_text, word_dict, \n                      sequence_length, num_words=10):\n    # Set model to evaluation mode\n    model.eval()\n\n    # Start with the initial text\n    current_text = initial_text\n    generated_sequence = initial_text\n\n    # Create reverse dictionary for converting numbers back to words\n    reverse_dict = {v: k for k, v in word_dict.items()}\n\n    print(f\"Initial text: {initial_text}\")\n\n    for i in range(num_words):\n        # Convert current text to lowercase and split into words\n        words = current_text.lower().strip().split()\n\n        # Take the last 'sequence_length' words if we exceed it\n        if len(words) > sequence_length:\n            words = words[-sequence_length:]\n\n        # Convert words to numbers using the word dictionary, use 0 for unknown\n        number_sequence = [word_dict.get(word, 0) for word in words]\n\n        # Pad the sequence\n        padded_sequence = [0] * (sequence_length – len(number_sequence)) \n                                                 + number_sequence\n\n        # Convert to PyTorch tensor and add batch dimension\n        input_tensor = torch.LongTensor([padded_sequence])\n\n        # Get prediction\n        with torch.no_grad():\n            output = model(input_tensor)\n\n        # Get the predicted word index (highest probability)\n        predicted_idx = torch.argmax(output[0]).item()\n\n        # Convert predicted index to word\n        predicted_word = reverse_dict[predicted_idx]\n\n        # Add the predicted word to the sequence\n        generated_sequence += \" \" + predicted_word\n\n        # Update current text for next prediction\n        current_text = generated_sequence\n\n        # Print progress\n        print(f\"Generated word {i+1}: {predicted_word}\")\n\n        # Optionally print top 5 predictions for each step\n        _, top_indices = torch.topk(output[0], 5)\n        print(f\"\\nTop 5 predictions for step {i+1}:\")\n        for idx in top_indices:\n            word = reverse_dict[idx.item()]\n            probability = output[0][idx].item()\n            print(f\"{word}: {probability:.4f}\")\n        print(\"\\n\" + \"-\"*50 + \"\\n\")\n\n    return generated_sequence\n\n# Example usage:\ninitial_text = \"sweet Jeremy saw Dublin\"\ngenerated_text = generate_sequence(\n    model=model,\n    initial_text=initial_text,\n    word_dict=word_index,\n    sequence_length=max_sequence_len,\n    num_words=10\n)\n\nprint(\"\\nFinal generated sequence:\")\nprint(generated_text)\n\n```", "```py\nsweet jeremy saw dublin his right leg acres of the nolans, dolans, daughter, of\n\n```", "```py\nInitial text: sweet Jeremy saw Dublin\nGenerated word 1: his\n\nTop 5 predictions for step 1:\nhis: 0.7782\ngo: 0.1393\nbellows,: 0.0605\naccident: 0.0090\ntil: 0.0048\n```", "```py\nGenerated word 2: right\n\nTop 5 predictions for step 2:\nright: 0.7678\npipes,: 0.1376\ncreature: 0.0458\ndidnt: 0.0136\nyoull: 0.0113\n```", "```py\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/learning-datasets/ \\\n    irish-lyrics-eof.txt-O /tmp/irish-lyrics-eof.txt\n```", "```py\ndata = open('/tmp/irish-lyrics-eof.txt').read()\ncorpus = data.lower().split(\"\\n\")\n```", "```py\nInitial text: in the town of Athy\nUsing device: cuda\nGenerated word 1: one\n\nTop 5 predictions for step 1:\none: 0.8318\nis: 0.1648\nshe: 0.0016\nthee: 0.0013\nthat: 0.0003\n\n--------------------------------------------------\n\nGenerated word 2: my\n\nTop 5 predictions for step 2:\nmy: 0.9377\nof: 0.0622\nis: 0.0001\nthat: 0.0000\none: 0.0000\n```", "```py\nin the town of athy one my heart\nwas they were the a reflections\non me all the frivolity;\nof me and me and me and the\nthere was my heart was\non the a over the frivolity;\n```", "```py\nsweet jeremy saw dublin she of his on the frivolity; of a heart is the ground\n\n```", "```py\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=0.01,              # Peak learning rate\n    epochs=20000,             # Total epochs\n    steps_per_epoch=1,        # Steps per epoch \n    pct_start=0.1,           # Percentage of training spent increasing lr\n    div_factor=10.0,         # Initial lr = max_lr/10\n    final_div_factor=1000.0  # Final lr = initial_lr/1000\n)\n```", "```py\nsweet jeremy saw dublin one evening two white ever we once to raise you, \ntis young i was told my heart as found has\n\nyou know nothing jon snow you should laugh all the while at me curious style, \ntwould set your heart a bubblin will lámh. you that\n\nin the town of athy one jeremy lanigan do lámh. pretty generation her soul, \nfell on the stony ground red we were feeble was down\n\n```", "```py\nwindow_size=10\nsentences=[]\nalltext=[]\ndata = open('/tmp/irish-lyrics-eof.txt').read()\ncorpus = data.lower()\nwords = corpus.split(\" \")\nrange_size = len(words)-max_sequence_len\nfor i in range(0, range_size):\n    thissentence=\"\"\n    for word in range(0, window_size-1):\n        word = words[i+word]\n        thissentence = thissentence + word\n        thissentence = thissentence + \" \"\n    sentences.append(thissentence)\n```", "```py\nyou know nothing jon snow\ntell the loved ones and the friends\nwe would neer see again.\nand the way of their guff again\nand high tower might ask,\nnot see night unseen\n```"]