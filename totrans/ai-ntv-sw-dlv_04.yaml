- en: Chapter 4\. Deploying to Test Environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 3](ch03.html#chapter_3_the_build_and_pre_deployment_testing_steps_of_cont_1749354010266208),
    we explored the fundamentals of continuous integration, focusing on early steps
    in a CI/CD pipeline: mainly, building and pre-deployment testing. We walked through
    an example pipeline triggered when a PR is opened, as shown in [Figure 4-1](#chapter_4_figure_1_1749354010437513).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ansd_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. A CI pipeline
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This pipeline built and packaged the code, conducted static code analysis, and
    executed early, fast tests including unit and lightweight integration tests, providing
    build and test feedback to the PR. These steps ensure that the code in the pull
    request is merge-ready, providing confidence that the merged code would function
    as intended and would not introduce any regressions. Assuming that the code changes
    in the PR prove ready, the developer can merge the PR.
  prefs: []
  type: TYPE_NORMAL
- en: With our new code merged, the next step is getting ready for production by deploying
    into test environments and then running a battery of tests. AI and ML tools are
    being integrated into the deployment process. These tools help teams make better
    deployment decisions, identify potential issues proactively, and streamline the
    verification process. Rather than adding complexity, well-implemented AI actually
    reduces the cognitive load on developers while improving deployment reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Between the CI steps and the production release, we are primarily focused on
    testing. We want to learn if the release is ready for our users. If it is safe
    to release, we want to get it to our users quickly, to enhance the user experience
    and potentially drive increased customer engagement and loyalty. If our software
    has a problem, we need to detect and address it quickly. That dynamic is a barrier
    to releasing valuable updates, and the longer the time between the introduction
    of the defect and when it is brought back to the development team, the less likely
    the work will be fresh in the mind of the developers involved. They will have
    to spend more time and effort familiarizing themselves with those sections of
    code, making the remediation more expensive. If a developer is already deep into
    their next task, that task may be interrupted and become more expensive to complete
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: When the release is ready, we will deploy the release into one or more environments
    where we can test against running code. It is in these pre-production environments
    that we bridge the gap between development and real-world usage, ensuring our
    software not only functions correctly but is ready for real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-2](#chapter_4_figure_2_1749354010437547) gives a high-level depiction
    of our entire delivery process. Increasingly, AI is being embedded throughout
    this pipeline to strengthen testing and deployment decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ansd_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. High-level delivery process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this chapter we’ll look at steps to provision infrastructure, deploy to
    one or more pre-production environments, and test against the software. In addition,
    we will cover key best practices including:'
  prefs: []
  type: TYPE_NORMAL
- en: Using IaC to create the lower environments that are consistent with production,
    but smaller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using “production-like” deployments to move your application consistently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting testing to the deployment pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting where to apply AI to deployments and where to remain cautious
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This stage in the pipeline is a pivotal one, where development and operations
    concerns intersect. By understanding and implementing these best practices, you’ll
    be well-equipped to determine the optimal number and types of test environments
    for your specific project requirements, regardless of project size or complexity.
    You will understand how to balance development velocity and operational stability,
    ensuring your software undergoes thorough testing and is release-ready.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a Unified Deployment Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we continue to navigate our delivery pipeline en route to production, we
    need to consider the deployment steps and deployment environments that will be
    necessary. For a delivery process that is predictable and reliable, we need deployment
    steps and environments that are predictable and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll cover best practices to give us the predictability and
    reliability we’re after, from test to production. In [Chapter 8](ch08.html#chapter_8_feature_management_and_experimentation_1749354011197288)
    we will cover production releases and production environments in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Consistently to Every Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automation is the foundation of DevOps, and a key function of our delivery pipeline
    is to automate both the setup of our pre-production environments and the deployments
    to those environments. Just as we need to validate our software before releasing
    it into the wild, we need to have measures in place to validate how we deploy
    our software.
  prefs: []
  type: TYPE_NORMAL
- en: We do this by consistently using the same methods to deploy to pre-production
    environments as we do to deploy to production. This consistency tests our deployment
    methods and minimizes the risk of unexpected issues when repeating these steps
    to deploy our software into production environments.
  prefs: []
  type: TYPE_NORMAL
- en: The following best practices help provide the predictability we’re after.
  prefs: []
  type: TYPE_NORMAL
- en: Use consistent tooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s not unusual for developers to spin up their own lightweight deployment
    processes using simple tooling to deploy to test environments, while the operations
    teams focus on processes geared to production deployments using enterprise tooling.
    This inconsistency between processes leads to changes being communicated on an
    “as broken basis,” where developers will update their process and forget to notify
    operations until something breaks.
  prefs: []
  type: TYPE_NORMAL
- en: This approach should be avoided, as it limits the effectiveness of testing in
    nonproduction environments and leads to duplicated effort in automation scripting.
    Instead, adopt a unified toolset for all deployments.
  prefs: []
  type: TYPE_NORMAL
- en: One way to encourage consistency is to offer developers easy, premade template
    pipelines known as “golden pipelines” or “paved roads.” We will examine this in
    more detail in [Chapter 10](ch10.html#chapter_10_a_platform_engineering_approach_to_modern_devops_1749354009763489).
    At a minimum, your developers and operations teams need to agree on a common set
    of tools for performing deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Use consistent pipeline steps and deployment strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether you’re using your CI/CD tool or custom deployment scripts, the sequence
    of actions should remain consistent across environments. Advanced deployment strategies
    like canary or blue-green deployments are typically selected based on derisking
    production deployments. If your production environment utilizes these strategies,
    replicate them in your pre-production environments. Similarly, if you were to
    use feature flags to release individual features in production, use feature flags
    to roll out the features in test environments. This consistency minimizes the
    chance of introducing discrepancies or oversights during deployment.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll cover production deployment and these progressive deployment strategies
    more thoroughly in Chapters [7](ch07.html#chapter_7_deploying_to_production_1749354011062634)
    and [8](ch08.html#chapter_8_feature_management_and_experimentation_1749354011197288).
    For now, note that the steps and strategies you use should be replicated at every
    level. While a test environment may be smaller due to cost or resource constraints,
    deploy as if it were a production environment. For example, a rolling deployment
    in production might deploy two nodes at a time to 10 targets, while in a test
    environment you could deploy one node at a time to 3 targets. This approach ensures
    that your production deployment steps and strategy are thoroughly tested with
    each version deployed to the test environment.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.html#chapter_7_deploying_to_production_1749354011062634),
    we will examine in depth how AI techniques can verify that a deployment is not
    causing trouble in its new environment. Those same approaches should be used in
    lower environments to validate that they are working and protect our tests from
    being run against a faulty install.
  prefs: []
  type: TYPE_NORMAL
- en: Use parameterization for differences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inevitably, variations will exist between environments. Target names, service
    URLs, and passwords may differ. Instead of creating unique deployment scripts
    for each environment, leverage variables to accommodate these differences. This
    allows you to maintain a single, adaptable script or pipeline that can be tailored
    to the specific environment at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: By being consistent in deployment, you’ll create a robust and reliable delivery
    pipeline that instills confidence in your team’s ability to release software seamlessly
    and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerate pipeline creation with AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#chapter_3_the_build_and_pre_deployment_testing_steps_of_cont_1749354010266208),
    we discussed automatic pipeline creation. Templating remains a good pattern—you
    want your AI to leverage your organization’s templates and pull in the correct
    adjustments and variables for your project and team. Whether you or an AI are
    creating or maintaining pipelines, the less that pipeline code needs to be managed,
    the better.
  prefs: []
  type: TYPE_NORMAL
- en: Leverage Infrastructure as Code for deployment consistency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We want consistent and predictable environments to deliver our release to production.
    IaC gives us an approach to not only achieve consistency but also control our
    configuration with as much care and control as we do our code resources. At its
    core, IaC treats infrastructure configuration like software code.
  prefs: []
  type: TYPE_NORMAL
- en: Engineers make changes to the IaC code locally and test them in their development
    environment. These changes are then committed to the VCS, just like application
    code. By managing our IaC, we leverage these features of our VCS and CI/CD pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The as-code nature has made IaC a DevOps area that quickly benefited from large
    language models. AI coding assistants generate and explain IaC code well, lowering
    the barriers to entry for developers and infrastructure professionals adopting
    new IaC languages. For major cloud providers with access to performance data from
    environments or DevOps platforms that combine cloud cost features with IaC management,
    future code generation tools will likely incorporate the following runtime optimizations
    based on live workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration and code review
  prefs: []
  type: TYPE_NORMAL
- en: Version control enables multiple team members to work on files simultaneously
    and manage conflicts. We can define and enforce policies to require code reviews
    of our infrastructure configuration changes.
  prefs: []
  type: TYPE_NORMAL
- en: Branching and experimentation
  prefs: []
  type: TYPE_NORMAL
- en: Version control allows you to create branches for experimenting with different
    configurations without affecting the main production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Traceability and auditability
  prefs: []
  type: TYPE_NORMAL
- en: A VCS provides a complete history of changes to your configuration settings.
    The commit messages and change history help you understand why your systems evolve,
    and audit trails are important in supporting compliance with security frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Rollback and recovery
  prefs: []
  type: TYPE_NORMAL
- en: If an infrastructure configuration change causes problems, you can quickly revert
    to a previous working version, minimizing downtime and impact on your systems.
    In addition, in the case of a catastrophic failure, you can use your version-controlled
    configurations to restore your systems to a known working state.
  prefs: []
  type: TYPE_NORMAL
- en: Automated testing
  prefs: []
  type: TYPE_NORMAL
- en: Delivery pipelines can run automated tests on the IaC code, including syntax
    checks, security scans, and compliance tests. The changes are then applied to
    the staging environment for integration testing, and finally, they’re promoted
    to production, typically using a careful rollout strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs: []
  type: TYPE_NORMAL
- en: Version control can help enforce security policies and controls around configuration
    changes, ensuring that only authorized personnel can make modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a scenario all too familiar to many in the tech industry: an application
    works flawlessly in development and runs smoothly in staging, but descends into
    chaos when deployed to production. This discrepancy often stems from inconsistencies
    in infrastructure configurations across environments. With IaC configuration definitions
    you can ensure that every environment, from development to production, is provisioned
    identically.'
  prefs: []
  type: TYPE_NORMAL
- en: This methodical process ensures that your infrastructure evolves in a controlled,
    predictable manner. It eliminates the “worked in QA” problem by removing unexpected
    differences between environments. By treating your infrastructure with the same
    respect and rigor as your application code, you gain consistency, reliability,
    and agility.
  prefs: []
  type: TYPE_NORMAL
- en: IaC offers several advantages beyond control and consistency. With a single
    command, you can spin up new environments that are exact replicas of your existing
    infrastructure. This not only makes your processes repeatable but also serves
    as accurate, living documentation. Because environments are easily created and
    destroyed, you can tear them down when not in use, saving resources and reducing
    costs, with the confidence that they can be recreated effortlessly.
  prefs: []
  type: TYPE_NORMAL
- en: To implement IaC effectively, you need the right tools, and several popular
    options are available. Terraform and its more open fork, OpenTofu, use a cloud-agnostic
    approach. If you’re all in on a particular cloud provider, native tools like AWS
    CloudFormation or Azure Resource Manager might be more appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Leverage Git Workflows with GitOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GitOps is a newer and increasingly popular approach to deploying software that
    builds on the capabilities of code repositories. With a GitOps approach, you describe
    the desired state of infrastructure in version-controlled configuration. This
    description is declarative. GitOps tools include an agent that regularly reconciles
    the actual environment and the desired state described in Git-controlled configurations.
    Instead of running a script to directly deploy software, you instigate a software
    deployment by updating the configuration in your code repository. This approach
    and GitOps tools are typically used in Kubernetes environments to orchestrate
    containerized applications across clusters of machines.
  prefs: []
  type: TYPE_NORMAL
- en: With this approach, you rely on your code repository to enforce security, provide
    governance, and implement your organization’s policies, such as requiring oversight
    through code reviews and approvals. Your updates are traceable and auditable.
    You can collaborate, experiment, and roll back the configuration updates used
    to deploy software. Once you make an update and merge it, the GitOps reconciliation
    agent does the rest, picking up the updates and implementing the changes to the
    target environment.
  prefs: []
  type: TYPE_NORMAL
- en: The approach has gained traction because managing the intricate configurations
    describing complex orchestrated cloud systems is an application well suited to
    code repository capabilities. In addition, GitOps addresses the problem of environment
    drift; that is, the environment is changed operationally from the desired state.
    The reconciliation agent automatically detects and remediates, guarding against
    inconsistencies in environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'While a GitOps approach is powerful, deploying with a GitOps approach within
    a CI/CD delivery pipeline is more complicated than simply pushing your app updates
    with a script. With GitOps, your pipeline must automate the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve configuration from your code repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the configuration to reference the latest version of your application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge the updated configuration back to Git.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GitOps reconciliation then takes it from there.
  prefs: []
  type: TYPE_NORMAL
- en: You may also encounter complexities with applications that are geographically
    replicated across multiple clusters. Maintaining consistency and synchronizing
    across clusters can be difficult due to many GitOps reconcilers being optimized
    for deploying applications to a single cluster. You may need to balance the need
    for a single source of truth with the reality that certain configurations will
    need to be tailored for specific clusters. Commercial GitOps tools often provide
    orchestration and visibility in these more complicated scenarios, extending what
    open source provides.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these challenges, the benefits in terms of collaboration, traceability,
    and automated reconciliation make GitOps a compelling choice for organizations
    that extensively leverage Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery, Deployment, and Test in the CI/CD Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have an understanding of the importance of predictable and reliable
    deployment steps and environments, let’s return to our delivery pipeline. With
    our new code merged, we now want to deploy it into one or more environments where
    we can test against running code. [Figure 4-3](#chapter_4_figure_3_1749354010437567)
    shows an example pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ansd_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Testing our code in pre-production environments
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this section, we’ll focus on the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Code trigger
  prefs: []
  type: TYPE_NORMAL
- en: The pull request is reviewed, approved, and merged into the main branch. In
    this pipeline, the PR merge triggers the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Continuous integration
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline repeats the continuous integration steps we reviewed in the last
    chapter, checking out, building, and executing continuous integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Provision infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline provisions the pre-production environments required for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Deploy to one or more pre-production environments
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline deploys the app to one or more pre-production environments.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Tests against the deployed app
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline tests against the deployed software. Various types of tests can
    be run, depending on the type of software and the priorities of your organization.
    We’ll look at a number of different types of tests in the following section. The
    pipeline can be configured to run multiple types of tests in parallel or sequentially.
    Some tests can reuse the same pre-production environments, while others may necessitate
    pre-production environments tailored to the requirements of the tests. Generally,
    faster tests are prioritized over slower tests.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Deploy to production
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to deploy, or promote, to the production environment. Depending
    on your delivery process, the decision to deploy to production can be automated
    or require manual approval. We’ll look at promotion strategies and steps to deploy
    to production in [Chapter 7](ch07.html#chapter_7_deploying_to_production_1749354011062634).
  prefs: []
  type: TYPE_NORMAL
- en: Types of Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Test environments are crucial for running tests, but the choice of tests depends
    heavily on the type of application being developed, the intended users, software
    architecture, and budget and time constraints. For example, in general, testing
    priorities for a website will differ significantly from those of embedded software
    or a web API. Testing priorities will vary between software services in a highly
    regulated industry versus software that must be intuitive and compelling to a
    large retail user base. Your selection of tests and their frequency can substantially
    impact application quality, infrastructure costs, and overall delivery speed.
  prefs: []
  type: TYPE_NORMAL
- en: AI-powered testing platforms increasingly use ML to optimize testing strategies.
    These platforms analyze historical test data, code changes, application architecture,
    and past deployment issues to intelligently select and prioritize tests. For example,
    AI-driven test selection tools identify the most impactful tests to execute for
    each code change, significantly speeding up test cycles. Vendors such as Harness,
    Tricentis SeaLights, and CloudBees Launchable are using AI and ML techniques to
    optimize test selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are common types of tests that occur during this phase:'
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end or functional tests
  prefs: []
  type: TYPE_NORMAL
- en: These tests are the most straightforward test type, simulating real-world user
    scenarios and validating the entire application flow from start to finish, to
    determine if the software does what is expected. These tests may be automated
    or performed manually. Modern teams automate more. Selenium is a commonly used
    open source test automation framework that many commercial tools also build upon.
    ML has been present in these tools for quite some time, but we are increasingly
    seeing a shift toward an AI-first approach, which we’ll dive into shortly.
  prefs: []
  type: TYPE_NORMAL
- en: AI-powered testing
  prefs: []
  type: TYPE_NORMAL
- en: AI can automatically generate test cases, identify edge cases, and learn from
    previous test runs to focus on areas most likely to have issues. AI testing is
    likely to complement or be a part of your end-to-end (functional) test programs.
  prefs: []
  type: TYPE_NORMAL
- en: API tests
  prefs: []
  type: TYPE_NORMAL
- en: A form of functional testing is API testing, which validates that an API works
    as expected. In distributed systems, services interact over APIs, so ensuring
    that APIs are performing well is important. Common API testing frameworks include
    SoapUI, Postman, Insomnia, and Swagger. AI-enhanced API testing goes beyond simple
    validation to intelligently explore API behaviors and edge cases. These systems
    can automatically generate API test scenarios by analyzing API documentation or
    actual usage patterns.
  prefs: []
  type: TYPE_NORMAL
- en: User experience tests
  prefs: []
  type: TYPE_NORMAL
- en: Developers, testers, and product managers may evaluate new features to make
    sure they are easy and intuitive to use. While this may test the same systems
    as end-to-end testing, the focus is on assessing usability.
  prefs: []
  type: TYPE_NORMAL
- en: User acceptance tests
  prefs: []
  type: TYPE_NORMAL
- en: These tests are typically done as a final check to ensure that the software
    meets the end user’s needs, that it meets the requirements, and that it functions
    as expected. User acceptance tests can include many other types of tests, from
    end-to-end to user experience and performance. These tests are done from the end
    user’s perspective with the purpose of providing a final and formal acceptance
    of the software release.
  prefs: []
  type: TYPE_NORMAL
- en: Accessibility tests
  prefs: []
  type: TYPE_NORMAL
- en: These tests ensure that our software is usable for people with disabilities
    such as visual, hearing, or cognitive impairments in order to serve our users
    and comply with legal, contractual, and regulatory requirements. Open source accessibility
    scanners include Lighthouse and Pa11Y. Companies, including accessiBe, are beginning
    to offer AI-augmented testing and remediation tooling as well.
  prefs: []
  type: TYPE_NORMAL
- en: Localization tests
  prefs: []
  type: TYPE_NORMAL
- en: Localization testing is important for software targeting a global audience.
    It involves a comprehensive assessment of the product’s linguistic accuracy, cultural
    appropriateness, and functional correctness within the specific target locale.
    This includes verifying translations, adapting visuals to cultural sensitivities,
    and ensuring the software functions correctly with local formats and regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tests
  prefs: []
  type: TYPE_NORMAL
- en: These tests simulate workloads to assess the speed, responsiveness, and stability
    of the application under different conditions. The tests help identify performance
    bottlenecks and ensure the application can handle expected traffic. This type
    of testing is critical for applications with seasonal peaks to ensure that the
    release can withstand peak demand. Apache JMeter, Gatling, and Grafana k6 are
    often used for performance testing. AI can leverage the data from performance
    testing to recommend resilience tests to run. These AI-powered performance testing
    systems can now detect performance anomalies with much greater accuracy than traditional
    threshold-based approaches. The systems establish baseline performance patterns
    and identify subtle deviations that might indicate looming issues. More advanced
    platforms can even pinpoint the specific components or code changes responsible
    for performance degradation by correlating test results with code changes and
    architecture maps.
  prefs: []
  type: TYPE_NORMAL
- en: Resilience tests
  prefs: []
  type: TYPE_NORMAL
- en: In modern distributed systems, a production system has many components. The
    one certainty is that something is going to break somewhere. Resilience testing,
    also known as chaos testing, evaluates if the software can remain useful when
    services it relies on fail. We’ll return to resilience testing in [Chapter 6](ch06.html#chapter_6_chaos_engineering_and_service_reliability_1749354010916149).
  prefs: []
  type: TYPE_NORMAL
- en: Security tests
  prefs: []
  type: TYPE_NORMAL
- en: These tests identify vulnerabilities and weaknesses in the application that
    could be exploited by attackers. They help ensure the security and integrity of
    the application. Dynamic application security testing (DAST) is a specific type
    of security testing that automates penetration testing, inspecting your running
    application for security flaws. DAST attempts to attack your applications like
    a malicious user would. ZAP is a commonly used free tool, while commercial offerings
    from Veracode and Checkmarx are popular as well. We’ll return to security testing
    in [Chapter 5](ch05.html#chapter_5_securing_applications_and_the_software_supply_chai_1749354010735711).
  prefs: []
  type: TYPE_NORMAL
- en: While the test types outlined above are commonly used, it’s important to note
    that there is no one-size-fits-all approach to software testing, and terminology
    can vary across organizations. The specific tests you choose and how you categorize
    them will depend on your unique development process, application architecture,
    and risk tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Intent-Based Functional and End-to-End Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional approaches to automated functional and end-to-end testing often
    rely heavily on scripted tests or simplistic record-and-playback methods. While
    convenient initially, these tests quickly become brittle and difficult to maintain,
    breaking whenever minor UI changes occur. This fragility creates a high maintenance
    burden, slows down development, and frequently results in teams abandoning automated
    testing entirely or limiting its scope.
  prefs: []
  type: TYPE_NORMAL
- en: An emerging AI-first approach to testing, known as intent-based testing, aims
    to overcome these challenges. Instead of explicitly scripting or manually recording
    each test step, teams express the intent of their test scenarios, describing the
    outcome they expect rather than the exact sequence of actions to achieve it. AI-native
    testing tools then dynamically generate and execute these tests by interacting
    with your application, much like a human user would.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, instead of recording precise clicks and form inputs for an e-commerce
    checkout process, you could simply describe the goal: “Purchase a product using
    a credit card.” The AI would automatically determine the most appropriate paths
    through your application, interacting with buttons, forms, and workflows intelligently.'
  prefs: []
  type: TYPE_NORMAL
- en: An important benefit is improved resilience of the tests—addressing the challenge
    of UI-based tests being brittle. If the UI changes later, the AI adapts to the
    new layout or modified interactions, significantly reducing maintenance overhead.
    Test automation tools have tried to automatically repair tests for many years,
    using techniques from tracking DOM objects to implementing ML. Shifting to understand
    the intent behind the test, and attempting to regenerate the entire script in
    response to a UI overhaul, brings a new level of recoverability.
  prefs: []
  type: TYPE_NORMAL
- en: These tools may also help compensate for the shift from professional testers
    toward asking developers to own these tests. The tools can recommend additional
    tests and assertions related to the existing tests, which may help an optimistic
    developer remember to check for corner cases and bad user behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced use cases for AI include migrating tests written in traditional tools
    such as Selenium and Playwright into intent-based testing tools, and generating
    and running not just individual tests but also entire test cases.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional Testing Versus a Hollowing-Out-the-Middle Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In traditional software development, testing is often compartmentalized with
    dedicated environments for each type. This ensures, for example, that manual user
    experience testing is never impacted by concurrent automated performance tests.
    However, this isolation comes at a cost: a proliferation of test environments
    is expensive and can be time-consuming to manage. When a single new release must
    clear numerous stages, the approach becomes increasingly unsustainable in the
    face of accelerating release cadences and growing application complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: As you try to accelerate your release cadences and your application becomes
    more complex, it becomes increasingly unsustainable to test across many stages,
    with each stage requiring a new environment. [Figure 4-4](#chapter_4_figure_4_1749354010437583)
    illustrates this staged approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ansd_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. Traditional testing through several pre-production environments
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On the other hand, a more modern testing approach challenges this model. This
    approach is sometimes referred to as “hollowing out the middle.” Instead of multiple,
    sequential test passes across multiple environments, there are fewer environments
    where tests run concurrently. This practice advocates shifting testing both to
    the left *and to the right*.
  prefs: []
  type: TYPE_NORMAL
- en: We introduced shift-left security in [Chapter 3](ch03.html#chapter_3_the_build_and_pre_deployment_testing_steps_of_cont_1749354010266208).
    By moving SAST, SCA, dependency scanning, and secrets detection into pre-deployment
    steps, our sample pipeline exemplified shift-left. We incorporated these crucial
    tests early such that passing them is a prerequisite to merging code. Unit and
    other early testing, completed as part of the merge workflow, also represent a
    shift-left approach. This helps catch issues sooner, reducing the need for extensive
    downstream testing.
  prefs: []
  type: TYPE_NORMAL
- en: A shift-right approach advocates executing some types of tests, traditionally
    late-cycle test types, against the new release in the live, production environment.
    Instead of provisioning and moving a release from one or more pre-production environments
    and using these isolated environments to test, we deploy the app straight to production
    and validate there. For example, load testing can be difficult to execute well
    and the environments may need to be large. Deploying to a portion of prod, applying
    load to that targeted infrastructure, and measuring the impact using production
    observability tooling can be a viable alternative to traditional load testing.
    [Figure 4-5](#chapter_4_figure_5_1749354010437606) illustrates this approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ansd_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. A “hollow-out-the-middle” approach to testing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that removing the need for pre-production environments that closely
    mirror production can save costs and maintenance toil, but how can extensive testing
    in a production environment be safe? Shift-right relies on new tools and production
    deployment practices. With advanced traffic management, observability tools, and
    containerization, many organizations have found that these tests can in fact be
    performed in the production environment with minimal side effects. Beyond significantly
    cutting infrastructure expenses, this approach has the advantage of yielding more
    accurate results. We’ll discuss these new tools and production deployment practices
    in [Chapter 7](ch07.html#chapter_7_deploying_to_production_1749354011062634).
  prefs: []
  type: TYPE_NORMAL
- en: Hollowing out the middle optimizes testing and is one modern strategy that organizations
    are taking to fuel faster delivery. By redesigning our approach to how we move
    our software between environments, we can similarly accelerate our delivery process.
    In [“Promotion Between Environments”](#chapter_4_promotion_between_environments_1749354010446697),
    we’ll look at how and why we should promote our releases between environments.
  prefs: []
  type: TYPE_NORMAL
- en: Promotion Between Environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at a typical delivery process that required
    moving our software through multiple stages of testing, with each stage of testing
    conducted in a separate pre-production environment. In this process we want to
    promote our release as quickly and intelligently as possible, meaning our new
    version of software should advance to the next environment and stage without any
    undue delay.
  prefs: []
  type: TYPE_NORMAL
- en: AI is beginning to play an increasing role in this promotion process, analyzing
    test results, performance data, and deployment history to make intelligent decisions
    about when and how to promote releases. These systems can evaluate multiple metrics
    simultaneously, detect subtle patterns that might indicate risks, and become increasingly
    accurate over time through ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, our promotion process is simple: if the tests in one stage pass, our
    release is immediately promoted to the next environment, and that environment
    is ready and available for the next round of testing. The promotion decision is
    automatic and instant and simply based on whether the previous stage of testing
    passed. In practice, release promotion, even between test environments, becomes
    a bottleneck in many delivery processes. This can be attributed to several factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Promotion decision is by committee
  prefs: []
  type: TYPE_NORMAL
- en: Promotion decisioning is not automated and requires a group review and approval
    of test results.
  prefs: []
  type: TYPE_NORMAL
- en: Promotion relies on tedious manual steps
  prefs: []
  type: TYPE_NORMAL
- en: Manual intervention to trigger the next deployment creates bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Insufficient number of testing environments
  prefs: []
  type: TYPE_NORMAL
- en: If the next environment is occupied with testing another version, the new version
    must wait.
  prefs: []
  type: TYPE_NORMAL
- en: In this section we’ll look at mitigations to address these issues. The practices
    we’ll introduce help us move our release from one pre-production environment to
    the next, and also apply to promoting our app into production. However, the final
    release into production has some special considerations, which will be addressed
    in more depth in [Chapter 7](ch07.html#chapter_7_deploying_to_production_1749354011062634).
  prefs: []
  type: TYPE_NORMAL
- en: From Decisions by Committee to Automated Decisions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human decision making, whether it’s a committee huddle or a trusted individual’s
    call, inevitably introduces delays in promoting your release from one stage to
    the next. Team members need to be alerted, then take time to analyze testing results
    before reaching a decision and taking action. While this might not always be labor-intensive,
    it undoubtedly slows things down.
  prefs: []
  type: TYPE_NORMAL
- en: While traditional automation has relied on simple pass/fail criteria, AI systems
    offer more sophisticated decision-making capabilities. Modern AI promotion engines
    can evaluate hundreds of metrics simultaneously, looking beyond simple test results
    to analyze system behavior holistically. These systems might consider factors
    like performance trends, error types, user impact assessments, and even code change
    risk levels based on past deployment patterns. By weighting these factors appropriately,
    AI can make more nuanced decisions than traditional rule-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Our aim is to streamline this process by automating the decision to promote
    your release. We’ll revisit this topic in detail in [Chapter 7](ch07.html#chapter_7_deploying_to_production_1749354011062634).
  prefs: []
  type: TYPE_NORMAL
- en: From Manual to Automated Promotion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you’ve automated the decision-making process, the actual promotion of your
    build becomes significantly easier. The key is to ensure that the deployment to
    the next environment is triggered immediately after the decision to proceed has
    been made, eliminating unnecessary wait times.
  prefs: []
  type: TYPE_NORMAL
- en: How you implement this automation depends on your chosen continuous delivery
    tooling. Some tools offer end-to-end pipelines with simple, built-in triggers
    for seamless promotion between stages. Others allow you to call another pipeline
    or job as a step within your current pipeline, offering flexibility but potentially
    requiring more configuration. While the ease of implementation varies, achieving
    this level of automation is almost always possible.
  prefs: []
  type: TYPE_NORMAL
- en: GitOps-style deployments, however, often present a unique challenge in this
    area, as we discussed in “Leverage Git Workflows with GitOps.” To execute the
    deployment, we need to automate the Git changes to the GitOps configurations instead
    of relying on manual updates. To do so, we will typically automate the pull request
    step and its approval directly within our CI/CD pipeline. We maintain Git as the
    source of truth that GitOps is known for while automating each step of our release
    promotion.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine a scenario where your pipeline has determined that a build
    is ready for promotion to the User Acceptance Testing (UAT) environment. When
    our pipeline is set up to generate the necessary pull request, trigger any required
    approvals, and (once approved) merge the changes into the main branch, our pipeline
    initiates the GitOps deployment to the UAT environment seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Break the Environment Bottleneck
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A final challenge in automating promotion between stages and environments in
    your delivery process is determining the “right” number of environments that you’ll
    need. Having too many environments becomes a financial burden due to the cost
    of maintaining their underlying infrastructure, while having too few creates bottlenecks
    and delays in moving releases toward delivery, as the process waits on resources
    to become available.
  prefs: []
  type: TYPE_NORMAL
- en: Ephemeral environments present a common solution to this dilemma. This approach
    involves creating environments on demand when needed for testing and promptly
    dismantling them once tests are complete. In the pre-cloud era, environment creation
    was a laborious process, often taking days. Now, thanks to programmable cloud
    infrastructure, environments can be spun up and torn down in minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as Code Management (IaCM) tools simplify ephemeral environments.
    These specialized CI/CD platforms automate the provisioning, configuration, and
    deployment of infrastructure resources using code. Unlike traditional CI/CD tools
    focused on applications, IaCM tools manage the underlying infrastructure. With
    IaCM tools, you define your desired infrastructure state using declarative code
    templates, making configurations more manageable, maintainable, and version-controlled.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, to achieve our goal of “production-like” test environments, the same
    template should be used to create both pre-production test environments and production
    environments, with adjustments made only to variables. When your pipelines seamlessly
    integrate with IaCM tools, deploying to a “Test” stage automatically triggers
    the creation of a corresponding “Test” environment. Once this environment is provisioned
    and configured with necessary details like IP addresses, passwords, and other
    environment-specific variables, the deployment and testing processes can proceed.
    Upon completion, the IaCM tool efficiently dismantles the environment, freeing
    up resources.
  prefs: []
  type: TYPE_NORMAL
- en: While this strategy offers significant benefits in terms of consistency, flexibility,
    and cost reduction, it’s important to note that the environment creation and teardown
    process can add a few minutes to the overall test cycle. Therefore, ephemeral
    environments might not be the ideal solution for pipelines targeting extremely
    rapid delivery cadences, such as those measured in minutes. However, for delivery
    cycles measured in hours, days, or weeks, ephemeral environments provide a powerful
    way to break bottlenecks, improve consistency, and optimize infrastructure costs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we continued to navigate our delivery process, focusing on the
    continuous delivery steps that follow continuous integration. These are primarily
    testing steps, and we reviewed the types of tests that are important for validating
    all aspects of our software. We discussed the importance of reliable and predictable
    pre-production environments to testing and the best practices to give us these.
    By automating all aspects of promoting your release between testing stages, including
    promotion decision making, we can dramatically accelerate the delivery of our
    software.
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing testing, there’s only one step left to get our latest software
    release into the hands of users: actually deploying to production. We’ll return
    to this step in [Chapter 7](ch07.html#chapter_7_deploying_to_production_1749354011062634).
    Before we get there, we’ll take the next few chapters to discuss how we can fortify
    our releases to be more secure, more resilient, and more reliable.'
  prefs: []
  type: TYPE_NORMAL
