<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="chap-prob">5 Probability distributions in machine learning</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">The role of probability distributions in machine learning</li>
<li class="co-summary-bullet">Working with binomial, multinomial, categorical, Bernoulli, beta, and Dirichlet distributions</li>
<li class="co-summary-bullet">The significance of entropy and cross-entropy in machine learning</li>
</ul>
<p class="body"><a id="marker-149"/>Life often requires us to estimate the chances of an event occurring or make a decision in the face of uncertainty. Probability and statistics form the common toolbox to use in such circumstances. In machine learning, we take large feature vectors as inputs. As stated earlier, we can view these feature vectors as points in a high-dimensional space. For instance, gray-level images of size <span class="math">224 × 224</span> can be viewed as points in a <span class="math">50, 176</span>-dimensional space, with each pixel corresponding to a specific dimension. Inputs with common characteristics, such as images of animals, will correspond to a cluster of points in that space. Probability distributions provide an effective tool for analyzing such loosely structured point distributions in arbitrarily high-dimensional spaces. Instead of simply developing a machine that emits a class given an input, we can fit a probability distribution to the clusters of input points (or a transformed version of them) satisfying some property of interest. This often lends more insight into the problem we are trying to solve.</p>
<p class="body">For instance, suppose we are trying to design a recommendation system. We could design one or more classifiers that emit yes/no decisions about whether to recommend product X to person Y. On the other hand, we can fit probability distributions to specific groups of people. Doing so can lead to the discovery of significant overlap between the point clusters representing various groups—for instance, people who drink black coffee and start-up founders. We may not know the explanation or even the direction in which causality (if any) flows in the relationship. But we see the correlation and may design a better recommendation system using it.</p>
<p class="body">Another situation in which probabilistic models are used in machine learning is when the problem involves a very large number of (perhaps infinitely many) classes. For instance, suppose we are creating a machine that not only recognizes cats in an image but also labels each pixel as belonging or not belonging to a cat. Effectively, the machine segments the image pixels into foreground versus background. This is called <i class="fm-italics">semantic segmentation</i>. It is hard to cast this problem as a classification problem: we typically design a system that emits a probability of being foreground for each pixel.</p>
<p class="body">Probabilistic models are also used in unsupervised and minimally supervised learning: for instance, in <i class="fm-italics">variational autoencoders</i> VAEs), which we discuss in chapter <a class="url" href="../Text/14.xhtml#ch-ae-vae">14</a>.</p>
<p class="body">This chapter introduces the fundamental notion of probability and discusses probability distributions (including multivariates), with specific examples, in a machine learning-centric way. As usual, we emphasize the geometrical view of multivariate statistics. An equally important goal of this chapter is to familiarize you with PyTorch <code class="fm-code-in-text">distributions</code>, the PyTorch statistical package, which we use throughout the book. All the distributions discussed are accompanied by code snippets from PyTorch <code class="fm-code-in-text">distributions</code>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The complete PyTorch code for this chapter is available at <a class="url" href="http://mng.bz/8NVg">http://mng</a> <a class="url" href="http://mng.bz/8NVg">.bz/8NVg</a> in the form of fully functional and executable Jupyter notebooks.<a id="marker-150"/></p>
<h2 class="fm-head" id="sec-prob_frequentist">5.1 Probability: The classical frequentist view</h2>
<p class="body">Consider a mythical city called Statsville. Suppose we choose a random adult inhabitant of Statsville. What are the chances of this person’s height being greater than <span class="math">6</span> ft? Less than <span class="math">3</span> ft? Between <span class="math">5</span> ft <span class="math">5</span> in. and <span class="math">6</span> ft? What are the chances of this person’s weight being between <span class="math">50</span> and <span class="math">70</span> kg (physicists would rather use the term <i class="fm-italics">mass</i> here, but we have chosen to stick to the more common word <i class="fm-italics">weight</i>)? Greater than 100 kg? What is the probability of the person’s home being exactly <span class="math">6</span> km from the city center? What are the chances of the person’s weight being in the <span class="math">50</span>–<span class="math">70</span> kg range <i class="fm-italics">and</i> their height being in the <span class="math">5</span> ft <span class="math">5</span> in. to <span class="math">6</span> ft range? What are the chances of the person’s weight being greater than <span class="math">90</span> kg <i class="fm-italics">and</i> their home being within <span class="math">5</span> km of the city center?</p>
<p class="body">All these questions can be answered in the frequentist paradigm by adopting the following approach:</p>
<p class="fm-quote">Count the size of the population belonging to the desired event satisfying the criterion or criteria of interest): for instance, the number of Statsville adults taller than 6 ft. Divide that by the total size of the population (here, the number of adults in Statsville). This is the probability (chance) of that criterion/criteria being satisfied.</p>
<p class="body">Formally,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\text{Probability of an event} &amp;= \frac{\text{Size of population belonging to that event}}{\text{Total size of population}}\nonumber\\[6pt]
&amp;= \frac{\text{Number of favorable outcomes}}{\text{Number of possible outcomes}}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="106" src="../../OEBPS/Images/eq_05-01-2.png" width="560"/></p>
</div>
<p class="fm-equation-caption">Equation 5.1 <span class="calibre" id="eq-probability"/></p>
<p class="body">For instance, suppose there are 100,000 adults in the city. Of them, 25,000 are <span class="math">6</span> ft or taller. Then the size of the population satisfying the event of interest (aka the number of favorable outcomes) is 25,000. The total population size (aka the number of possible outcomes) is 100,000. So,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\text{Probability of a random adult Statsville resident being taller than 6 ft}\\
&amp; = \frac{\text{Number of adult Statsville residents taller than 6 ft}}{\text{Total number of adult Statsville residents}}=\frac{25000}{100000}=0.25\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="103" src="../../OEBPS/Images/eq_05-01-a-2.png" width="579"/></p>
</div>
<p class="body">Since the total population is always a superset of the population belonging to any event, the denominator is always greater than or equal to the numerator. Consequently, <i class="fm-italics">probabilities are always lesser than or equal to <span class="math">1</span></i>.<a id="marker-151"/></p>
<h3 class="fm-head1" id="sec-RVs">5.1.1 Random variables</h3>
<p class="body">When we talk about probability, a relevant question is, “The probability of what?” The simplest answer is, “The probability of the occurrence of an event.” For example, in the previous subsection, we discussed the probability of the event that the height of an adult Statsville resident is less than <span class="math">6</span> ft. A little thought reveals that an event always corresponds to a numerical entity of interest taking a particular value or lying in a particular range of values. This entity is called a <i class="fm-italics">random variable</i>. For instance, the height of adult Statsville residents can be a random variable, and we can talk about the probability of it being less than <span class="math">6</span> ft, or the weight of adult Statsville residents can be a random variable, and we can talk about the probability of it being less than <span class="math">60</span> kg. When predicting the performance of stock markets, the Dow Jones index maybe a random variable: we can talk about the probability of this random variable crossing 19,000. And when discussing the spread of a virus, the total number of infected people may be a random variable, and we can talk about the probability of it being less than 2,000, and so on.</p>
<p class="body">The defining characteristic of a random variable is that every allowed value (or range of values) is associated with a probability (of the random variable taking that value or value range). For instance, we may allow a set of only three weight ranges for adults of Statsville: <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span>, less than <span class="math">60</span> kg; <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span>, between <span class="math">60</span> and <span class="math">90</span> kg; and <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>, greater than <span class="math">90</span> kg. Then we can have a corresponding random variable <i class="timesitalic">X</i> representing the quantized weight. It can take one of only three values: <span class="math"><i class="fm-italics">X</i> = 1</span> (corresponding to the weight in <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span>), <span class="math"><i class="fm-italics">X</i> = 2</span> (corresponding to the weight in <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span>), or <span class="math"><i class="fm-italics">X</i> = 3</span> (corresponding to the weight in <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>). Each value comes with a fixed probability: for example, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 1) = 0.25</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 2) = = 0.5</span>, and <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 3) = 0.25</span>, respectively, in the example from section <a class="url" href="#sec-prob_frequentist">5.1</a>. Such random variables that take values from a countable set are known as <i class="fm-italics">discrete</i> random variables.</p>
<p class="body">Random variables can also be <i class="fm-italics">continuous</i>. For a continuous random variable <i class="timesitalic">X</i>, we associate a probability with its value being in an infinitesimally small range, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i> ≤ <i class="fm-italics">X</i> &lt; <i class="fm-italics">x</i> + <i class="fm-italics">δx</i>)</span>, with <span class="math"><i class="fm-italics">δx</i> → 0</span>. This is called <i class="fm-italics">probability density</i> and is explained in more detail in section <a class="url" href="#sec-cont-rv">5.6</a>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> In this book, we always use uppercase letters to denote random variables. Usually, the same letter in lowercase refers to a specific value of the random variable: for example, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = <i class="fm-italics">x</i>)</span> denotes the probability of random variable <i class="timesitalic">X</i> taking the value <i class="timesitalic">x</i> and <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i><span class="cambria">∈</span>{<i class="fm-italics">x</i>, <i class="fm-italics">x</i> + <i class="fm-italics">δx</i>})</span> denotes the probability of random variable <i class="timesitalic">X</i> taking a value between <i class="timesitalic">x</i> and <span class="math"><i class="fm-italics">x</i> + <i class="fm-italics">δx</i></span>. Also note that sometimes we use the letter <i class="timesitalic">X</i> to denote a data set. This popular but confusing convention is rampant in the literature—generally, the usage is obvious from the context.</p>
<h3 class="fm-head1" id="population-histograms">5.1.2 Population histograms</h3>
<p class="body"><a id="marker-152"/>Histograms help us to visualize discrete random variables. Let’s continue with our Statsville example. We are only interested in three weight ranges for Statsville adults: <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span>: less than <span class="math">60</span> kg; <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span>: between <span class="math">60</span> and <span class="math">90</span> kg; and <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>: greater than <span class="math">90</span> kg. Suppose the counts of Statsville adults in these weight ranges are as shown in table 5.1.</p>
<p class="fm-table-caption">Table 5.1 Frequency table for the weights of adults in the city of Statsville</p>
<table border="1" class="contenttable-1-table" id="tab-hist-wt" width="100%">
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head"><span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span>: Less than 60 kg</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head"><span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span>: Between 60 and 90 kg</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head"><span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>: More than 90 kg</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">25,000</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">50,000</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">25,000</p>
</td>
</tr>
</tbody>
</table>
<p class="body">The same information can be visualized by the histogram shown in figure <a class="url" href="#fig-statsville-weight-histogram">5.1</a>. The <i class="timesitalic">X</i>-axis of the histogram corresponds to possible values of the discrete random variable from section <a class="url" href="#sec-RVs">5.1.1</a>. The <i class="timesitalic">Y</i>-axis shows the size of the population in the corresponding weight range. There are 25,000 people in range <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span>, 50,000 people in range <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span>, and 25,000 people in range <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>. Together, these categories account for the entire adult population of Statsville—every adult belongs to one category or another. This can be verified by adding the <i class="timesitalic">Y</i>-axis values for all the categories: <span class="math">25, 000 + 50, 000 + 25, 000 = 100, 000</span>, the adult population of Statsville.</p>
<h2 class="fm-head" id="sec-prob-distr">5.2 Probability distributions</h2>
<p class="body">Figure <a class="url" href="#fig-statsville-weight-histogram">5.1</a> and its equivalent, table <a class="url" href="#tab-hist-wt">5.1</a>, can easily be converted to probabilities, as shown in table <a class="url" href="#tab-hist-prob">5.2</a>. The table shows the probabilities corresponding to allowed values of the discrete random variable <i class="timesitalic">X</i> representing the quantized weight of a randomly chosen adult resident of Statsville. Table <a class="url" href="#tab-hist-prob">5.2</a> represents what is formally known as a <i class="fm-italics">probability distribution</i>: a mathematical function that takes a random variable as input and outputs the probability of it taking any allowed value. It must be defined over all possible values of the random variable.</p>
<p class="body"><a id="marker-153"/>Note that the set of ranges <span class="math">{<i class="fm-italics">S</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">S</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">S</i><sub class="fm-subscript">3</sub>}</span> is exhaustive in the sense that all possible values of <i class="timesitalic">X</i> belong to one range or other—we cannot have a weight that does not belong to any of them. In set-theoretical terms, the union of these ranges, <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub> ∪ <i class="fm-italics">S</i><sub class="fm-subscript">2</sub> ∪ <i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>, covers a space that contains the entire population (all possible values of <i class="timesitalic">X</i>).</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="636" id="fig-statsville-weight-histogram" src="../../OEBPS/Images/CH05_F01_Chaudhury.png" width="891"/></p>
<p class="figurecaption">Figure 5.1 Histogram depicting the weights of adults in Statsville, corresponding to table <a class="url" href="#tab-hist-wt">5.1</a></p>
</div>
<p class="fm-table-caption">Table 5.2 Probability distribution for quantized weights of Statsville adults</p>
<table border="1" class="contenttable-1-table" id="tab-hist-prob" width="100%">
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head"><span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span>: Less than 60 kg</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head"><span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span>: Between 60 and 90 kg</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head"><span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>: More than 90 kg</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 1) = 25,000/100,000 = 0.25</span></td>
<td class="contenttable-1-td"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 2) = 50,000/100,000 = 0.5</span></td>
<td class="contenttable-1-td"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 3) = 25,000/100,000 = 0.25</span></td>
</tr>
</tbody>
</table>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The set-theoretic operator <span class="math">∪</span> denotes set union.</p>
<p class="body">The ranges are also mutually exclusive in that any given observation of <i class="timesitalic">X</i> can belong to only a single range, never more. In set-theoretic terms, the intersection of any pair of ranges is null: <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub> ∩ <i class="fm-italics">S</i><sub class="fm-subscript">2</sub> = <i class="fm-italics">S</i><sub class="fm-subscript">1</sub> ∩ <i class="fm-italics">S</i><sub class="fm-subscript">3</sub> = <i class="fm-italics">S</i><sub class="fm-subscript">2</sub> ∩ <i class="fm-italics">S</i><sub class="fm-subscript">3</sub> = <i class="fm-italics">ϕ</i></span>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The set-theoretic operator <span class="math">∩</span> denotes set intersection.</p>
<p class="body">For a set of exhaustive and mutually exclusive events, the function ielding the probabilities of these events is a probability distribution. For instance, the probability distribution in our tiny example comprises three probabilities: <span class="math"><i class="fm-italics">P</i>(<i class="fm-italics">X</i> = 1) = 0.25</span>, <span class="math"><i class="fm-italics">P</i>(<i class="fm-italics">X</i> = 2) = 0.5</span>, and <span class="math"><i class="fm-italics">P</i>(<i class="fm-italics">X</i> = 3) = 0.25</span>. This is shown in figure <a class="url" href="#fig-statsville-weight-probability-distribution">5.2</a>, which is a three-point graph.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="651" id="fig-statsville-weight-probability-distribution" src="../../OEBPS/Images/CH05_F02_Chaudhury.png" width="891"/></p>
<p class="figurecaption">Figure 5.2 Probability distribution graph for the weights of adults in Statsville, corresponding to table <a class="url" href="#tab-hist-prob">5.2</a>. Event <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub> ≡ <i class="fm-italics">X</i> = 1 <span class="cambria">⟹</span></span> a weight in the range <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span>, Event <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub> ≡ <i class="fm-italics">X</i> = 2 <span class="cambria">⟹</span></span> a weight in the range <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span>, and Event <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub> ≡ <i class="fm-italics">x</i> = 3 <span class="cambria">⟹</span></span> a weight in the range <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>.</p>
</div>
<h2 class="fm-head" id="basic-concepts-of-probability-theory">5.3 Basic concepts of probability theory</h2>
<p class="body"><a id="marker-154"/>In this section, we briefly touch on impossible and certain events; the sum of probabilities of exhaustive, mutually exclusive events; and independent events.</p>
<h3 class="fm-head1" id="probabilities-of-impossible-and-certain-events">5.3.1 Probabilities of impossible and certain events</h3>
<p class="body">The probability of an impossible event is zero (for example, the probability that the sun will rise in the west). The probability of an event that occurs with certitude is <span class="math">1</span> the probability that the sun will rise in the east). Improbable events such as this author beating Roger Federer in competitive tennis) have low but nonzero probabilities, like <span class="math">0.001</span>. Highly probable events (such as Roger Federer beating this author in competitive tennis) have probabilities close to but not exactly equal to <span class="math">1</span>, like <span class="math">0.999</span>.</p>
<h3 class="fm-head1" id="exhaustive-and-mutually-exclusive-events">5.3.2 Exhaustive and mutually exclusive events</h3>
<p class="body">Consider the events <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span> corresponding to the quantized weight of a Statsville adults from section <a class="url" href="#sec-prob-distr">5.2</a> belonging to the range <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span>, or <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>, respectively equivalently, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span> is the event corresponding to <span class="math"><i class="fm-italics">X</i> = 1</span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span> is the event corresponding to <span class="math"><i class="fm-italics">X</i> = 2</span>, and <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span> is the event corresponding to <span class="math"><i class="fm-italics">X</i> = 3</span>). The events are exhaustive: their union covers the entire population space. This means all quantized weights of Statsville adults belong to one of the ranges <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>. The events are also mutually exclusive: their mutual intersections are null, meaning no member of the population can belong to more than one range. For example, if a weight belongs to <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span>, it cannot belong to <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span> or <span class="math"><i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span>. For such events, the following holds true:</p>
<p class="fm-equation">The sum of the probabilities of mutually exclusive events yields the probability of one or the other of them occurring.</p>
<p class="body">For instance, for events <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span>,</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub> or <i class="fm-italics">E</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) + <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="fm-equation-caption">Equation 5.2 <span class="calibre" id="eq-prob-sum"/></p>
<p class="body">the <i class="fm-italics">sum rule</i> says that</p>
<p class="fm-equation">The sum of the probabilities of an exhaustive, mutually exclusive set of events is always <span class="math">1</span>.</p>
<p class="body">For example,</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) + <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>) + <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub> or <i class="fm-italics">E</i><sub class="fm-subscript">2</sub> or <i class="fm-italics">E</i><sub class="fm-subscript">3</sub>) = 1</span></p>
<p class="body">This is intuitively obvious. We are merely saying that <i class="fm-italics">we can say with certainty that either <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span> or <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span> or <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span> will occur</i>.</p>
<p class="body">In general, given a set of exhaustive, mutually exclusive events <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">E<sub class="fm-subscript">n</sub></i>,</p><!--<p class="FM-Equation"><span class="times">$$\sum_{i=1}^{i=n} p\left(E_{i}\right) = 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="63" src="../../OEBPS/Images/eq_05-03.png" width="103"/></p>
</div>
<p class="fm-equation-caption">Equation 5.3 <span class="calibre" id="eq-discrete-prob-sum"/></p>
<h3 class="fm-head1" id="independent-events">5.3.3 Independent events</h3>
<p class="body"><a id="marker-155"/>Consider the two events <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub> ≡ “weight of an adult inhabitant of Statsville is less than 60 kg”</span> and <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub> ≡ “home of an adult inhabitant of Statsville is within 5 km of the city center”</span>. These events do not influence each other at all. The knowledge that a member of the population weighs less than <span class="math">60</span> kg does not reveal anything about the distance of their home from the city center and vice versa. We say <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span> and <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span> are <i class="fm-italics">independent events</i>. Formally,</p>
<p class="fm-equation">A set of events are independent if the occurrence of one does not affect the probability of the occurrence of another.</p>
<h2 class="fm-head" id="sec-joint-prob">5.4 Joint probabilities and their distributions</h2>
<p class="body">Given an adult Statsville resident, let <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span> be, as before, the event that their weight is less than <span class="math">60</span> kg. The corresponding probability is <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>)</span>. Also, let <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span> be the event that the distance of their home from the city center is less than <span class="math">5</span> km. The corresponding probability is <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span>. Now consider the probability that a resident weights less than <span class="math">60</span> kg <i class="fm-italics">and</i> their home is less than <span class="math">5</span> km from the city center. This probability, denoted <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span>, is called a <i class="fm-italics">joint probability</i>. Formally,</p>
<p class="fm-equation">The joint probability of a set of events is the probability of all those events occurring together.</p>
<p class="body">The <i class="fm-italics">product rule</i> says that the joint probability of independent events can be obtained by multiplying their individual probabilities. Thus, for the current example, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>)<i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span>.</p>
<p class="body">Let’s continue our discussion of joint probabilities with a slightly more elaborate example. We have consolidated the weight categories, corresponding populations, and probability distributions in table <a class="url" href="#tab-hist-prob-wt">5.3</a> for quick reference. Similarly, we quantize the distance of residents’ homes from the city center into three ranges: <span class="math"><i class="fm-italics">D</i><sub class="fm-subscript">1</sub> ≡ less than 5 km</span>, <span class="math"><i class="fm-italics">D</i><sub class="fm-subscript">2</sub> ≡ between 5 and 15 km</span>, and <span class="math"><i class="fm-italics">D</i><sub class="fm-subscript">3</sub> ≡ greater than 15 km</span>. Table <a class="url" href="#tab-hist-prob-dist">5.4</a> shows the corresponding population and probability distributions. The joint probability distribution of the events <span class="math">{<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">E</i><sub class="fm-subscript">3</sub>}</span> and <span class="math">{<i class="fm-italics">G</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">3</sub>}</span> is shown in table <a class="url" href="#tab-joint-prob-distr">5.5</a>.<a id="marker-156"/></p>
<p class="fm-table-caption">Table 5.3 Population probability distribution table for the weights of adult residents of Statsville. <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span> are exhaustive, mutually exclusive events, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) + <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>) + <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>) = 1</span>.</p>
<table border="1" class="contenttable-1-table" id="tab-hist-prob-wt" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="33.33%"/>
<col class="contenttable-0-col" span="1" width="33.33%"/>
<col class="contenttable-0-col" span="1" width="33.33%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">Less than 60 kg (range <i class="fm-italics">S</i><sub class="fm-subscript">1</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Between 60 and 90 kg (range <i class="fm-italics">S</i><sub class="fm-subscript">2</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">More than 90 kg (range <i class="fm-italics">S</i><sub class="fm-subscript">3</sub>)</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Event <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub> ≡ <i class="fm-italics">weight</i> <span class="cambria">∈</span> <i class="fm-italics">S</i><sub class="fm-subscript">1</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Event <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub> ≡ <i class="fm-italics">weight</i> <span class="cambria">∈</span> <i class="fm-italics">S</i><sub class="fm-subscript">2</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Event <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub> ≡ <i class="fm-italics">weight</i> <span class="cambria">∈</span> <i class="fm-italics">S</i><sub class="fm-subscript">3</sub></span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Population size = 25,000</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Population size = 50,000</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Population size = 25,000</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">S</i><sub class="fm-subscript">1</sub>) = 25,000/100,000 = 0.25</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">S</i><sub class="fm-subscript">2</sub>) = 50,000/100,000 = 0.5</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">S</i><sub class="fm-subscript">3</sub>) = 25,000/100,000 = 0.25</span></p>
</td>
</tr>
</tbody>
</table>
<p class="fm-table-caption">Table 5.4 Population probability distribution table for the distance of adult Statsville residents’ homes from the city center. <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">3</sub></span> are exhaustive, mutually exclusive events, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">1</sub>) + <i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">2</sub>) + <i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">3</sub>) = 1</span>.</p>
<table border="1" class="contenttable-1-table" id="tab-hist-prob-dist" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="33.33%"/>
<col class="contenttable-0-col" span="1" width="33.33%"/>
<col class="contenttable-0-col" span="1" width="33.33%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">Less than 5 km (range <i class="fm-italics">D</i><sub class="fm-subscript">1</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Between 5 and 15 km (range <i class="fm-italics">D</i><sub class="fm-subscript">2</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Greater than 15 km (range <i class="fm-italics">D</i><sub class="fm-subscript">3</sub>)</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Event <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub> ≡ <i class="fm-italics">distance</i> <span class="cambria">∈</span> <i class="fm-italics">D</i><sub class="fm-subscript">1</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Event <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">2</sub> ≡ <i class="fm-italics">distance</i> <span class="cambria">∈</span> <i class="fm-italics">D</i><sub class="fm-subscript">2</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Event <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">3</sub> ≡ <i class="fm-italics">distance</i> <span class="cambria">∈</span> <i class="fm-italics">D</i><sub class="fm-subscript">3</sub></span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Population size = 20,000</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Population size = 60,000</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Population size = 20,000</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">1</sub>) = 20,000/100,000 = 0.20</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">1</sub>) = 60,000/100,000 = 0.6</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">1</sub>) = 20,000/100,000 = 0.20</span></p>
</td>
</tr>
</tbody>
</table>
<p class="fm-table-caption">Table 5.5 Joint probability distribution of independent events. The sum of all elements in the table is <span class="math">1</span>.</p>
<table border="1" class="contenttable-1-table" id="tab-joint-prob-distr" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="33.33%"/>
<col class="contenttable-0-col" span="1" width="22.22%"/>
<col class="contenttable-0-col" span="1" width="22.22%"/>
<col class="contenttable-0-col" span="1" width="22.22%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">  </p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Less than 60 kg (<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Between 60 and 90 kg (<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">More than 90 kg (<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>)</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Less than 5 km</b> (<span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.05</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.5 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.1</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.05</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Between 5 and 15 km</b> (<span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">2</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.6</span></p>
<p class="fm-table-body"><span class="math">= 0.15</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.5 × 0.6</span></p>
<p class="fm-table-body"><span class="math">= 0.3</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.6</span></p>
<p class="fm-table-body"><span class="math">= 0.15</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">More than 15 km</b> (<span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">3</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">3</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.05</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">3</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.5 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.1</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">3</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.05</span></p>
</td>
</tr>
</tbody>
</table>
<p class="body">We can make the following statements about table <a class="url" href="#tab-joint-prob-distr">5.5</a>:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The sum total of all elements in table <a class="url" href="#tab-joint-prob-distr">5.5</a> is <span class="math">1</span>. In other words, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">i</sub></i>, <i class="fm-italics">G<sub class="fm-subscript">j</sub></i>)</span> is a proper probability distribution indicating the probabilities of event <i class="timesitalic">E<sub class="fm-subscript">i</sub></i> and event <i class="timesitalic">G<sub class="fm-subscript">j</sub></i> occurring together: here, <span class="math">(<i class="fm-italics">i</i>, <i class="fm-italics">j</i>) <span class="cambria">∈</span> {1,2,3} × {1,2,3}</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">i</sub></i>, <i class="fm-italics">G<sub class="fm-subscript">j</sub></i>) = <i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">i</sub></i>)<i class="fm-italics">p</i>(<i class="fm-italics">G<sub class="fm-subscript">j</sub></i>)</span> <span class="math"><span class="cambria">∀</span>(<i class="fm-italics">i</i>, <i class="fm-italics">j</i>) <span class="cambria">∈</span> {1,2,3} × {1,2,3}</span>. This is because the events are independent.</p>
</li>
</ul>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The symbol <span class="math">×</span> denotes the <i class="fm-italics">Cartesian product</i>. The Cartesian product of two sets <span class="math">{1,2,3} × {1,2,3}</span> is the set <span class="math">{(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),</span><span class="math">(3,2),(3,3)}</span>. And the symbol <span class="cambria">∀</span> indicates “for all.” Read <span class="math"><span class="cambria">∀</span>(<i class="fm-italics">i</i>, <i class="fm-italics">j</i>) <span class="cambria">∈</span> {1,2,3} × {1,2,3}</span> as follows: for all pairs <span class="math">(<i class="fm-italics">i</i>, <i class="fm-italics">j</i>)</span> in the Cartesian product, <span class="math">{1,2,3} × {1,2,3}</span>.</p>
<p class="body">In general, given a set of independent events <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">E<sub class="fm-subscript">n</sub></i>, the joint probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">E</i><sub class="fm-subscript">2</sub>,⋯, <i class="fm-italics">E<sub class="fm-subscript">n</sub></i>)</span> of all the events occurring together is the product of their individual probabilities of occurring:</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left(E_{1}, E_{2}, \cdots, E_{n}\right) =  p\left(E_{1}\right) p\left(E_{n}\right) \cdots p\left(E_{1}\right) = \prod_{i=1}^{i=n} p\left(E_{i}\right) \end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_05-04.png" width="438"/></p>
</div>
<p class="fm-equation-caption">Equation 5.4 <span class="calibre" id="eq-joint-prob-indep"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The symbol <span class="math">∏</span> stands for “product.”</p>
<h3 class="fm-head1" id="sec-marginal-prob">5.4.1 Marginal probabilities</h3>
<p class="body"><a id="marker-157"/>Suppose we do not have the individual probabilities <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">i</sub></i>)</span> and <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G<sub class="fm-subscript">j</sub></i>)</span>. All we have is the joint probability distribution: that is, table <a class="url" href="#tab-joint-prob-distr">5.5</a>. Can we find the individual probabilities from them? If so, how?</p>
<p class="body">To answer this question, consider a particular row or column in table <a class="url" href="#tab-joint-prob-distr">5.5</a>—say, the top row. In this row, the <i class="timesitalic">E</i> values iterate over all possibilities (the entire space of <i class="timesitalic">E</i>s), but <i class="timesitalic">G</i> is fixed at <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span>. If <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span> is to occur, there are only three possibilities: it occurs with <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, or <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span>. The corresponding joint probabilities are <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span>, and <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span>. If we add them, we get the probability of <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span> occurring with <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span> or <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, or <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span>: that is, event <span class="math">(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span> or <span class="math">(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span> or <span class="math">(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span>. Thus we have considered all situations under which <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span> can occur. The sum represents the probability of event <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span> occurring. Thus, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span> can be obtained by adding all the probabilities in the row corresponding to <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span> and writing it in the margin (this is why it is called the <i class="fm-italics">marginal</i> probability). Similarly, by adding all the probabilities in the middle column, we obtain the probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>)</span>, and so forth. Table <a class="url" href="../Text/05.xhtml#tab-jmarginal-prob">5.6</a> shows table <a class="url" href="#tab-joint-prob-distr">5.5</a> updated with marginal probabilities.</p>
<p class="fm-table-caption">Table 5.6 Joint probability distribution with marginal probabilities shown</p>
<table border="1" class="contenttable-1-table" id="tab-jmarginal-prob" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head"/>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Less than 60 kg (<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Between 60 and 90 kg (<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">More than 90 kg (<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Marginals for G’s</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Less than 5 km</b> (<span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.05</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.5 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.1</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.05</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">1</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.05 + 0.1 + 0.05</span></p>
<p class="fm-table-body"><span class="math">= 0.2</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Between 5 and 15 km</b> (<span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">2</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.6</span></p>
<p class="fm-table-body"><span class="math">= 0.15</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.5 × 0.6</span></p>
<p class="fm-table-body"><span class="math">= 0.3</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.6</span></p>
<p class="fm-table-body"><span class="math">= 0.15</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="fm-table-body"><span class="math">0.15 + 0.3 + 0.15</span></p>
<p class="fm-table-body">= <span class="math">0.6</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">More than 15 km</b> (<span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">3</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">3</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.05</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">3</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.5 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.1</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">3</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.25 × 0.2</span></p>
<p class="fm-table-body"><span class="math">= 0.05</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G</i><sub class="fm-subscript">3</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.05 + 0.1 + 0.05</span></p>
<p class="fm-table-body"><span class="math">= 0.2</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Marginals for</b> <i class="timesitalic">E</i><b class="fm-bold">s</b></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.05 + 0.15 + 0.05</span></p>
<p class="fm-table-body"><span class="math">0.05 = 0.25</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.1 + 0.3 + 0.1</span></p>
<p class="fm-table-body"><span class="math">= 0.5</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.05 + 0.15 +</span></p>
<p class="fm-table-body"><span class="math">= 0.25</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"/>
</td>
</tr>
</tbody>
</table>
<p class="body">In general, given a set of exhaustive, mutually exclusive events <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">E<sub class="fm-subscript">n</sub></i>, another event <i class="timesitalic">G</i>, and joint probabilities <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i>)</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">G</i>)</span>, <span class="math">⋯</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">n</sub></i>, <i class="fm-italics">G</i>)</span>,</p><!--<p class="Body"><span class="times">$$p\left(G\right) = \sum_{i=1}^{i=n} p\left(E_{i}, G\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_05-05.png" width="164"/></p>
</div>
<p class="fm-equation-caption">Equation 5.5 <span class="calibre" id="eq-marginal-prob"/></p>
<p class="body">By summing over all possible values of <i class="timesitalic">E<sub class="fm-subscript">i</sub></i>s, we factor out the <i class="timesitalic">E</i>s. This is because the <i class="timesitalic">E</i>s are mutually exclusive and exhaustive; summing over them results in a certain event that is factored out (remember, the probability of a certain event is <span class="math">1</span>).</p>
<h3 class="fm-head1" id="sec-joint-prob-depend">5.4.2 Dependent events and their joint probability distribution</h3>
<p class="body">So far, the events we have considered jointly are “weights” and “distance of a resident’s home from the city center.” These are independent of each other—their joint is the product of their individual probabilities. Now, let’s discuss a different situation where the variables are connected and knowing the value of one does help us predict the other. For instance, the weights and heights of adult residents of Statsville are not independent: typically, taller people weigh more, and vice versa.</p>
<p class="body"><a id="marker-158"/>As usual, we use a toy example to understand the idea. We quantize heights into three ranges, <span class="math"><i class="fm-italics">H</i><sub class="fm-subscript">1</sub> ≡ less than 5 ft 5 in.</span>, <span class="math"><i class="fm-italics">H</i><sub class="fm-subscript">2</sub> ≡ between 5 ft 5 in. and 6 ft</span>, and <span class="math"><i class="fm-italics">H</i><sub class="fm-subscript">3</sub> ≡ greater than 6 ft</span>. Let <i class="timesitalic">z</i> be the random variable corresponding to height. We have three possible events with respect to height: <span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">1</sub> ≡ <i class="fm-italics">z</i> <span class="cambria">∈</span> <i class="fm-italics">H</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">2</sub> ≡ <i class="fm-italics">z</i> <span class="cambria">∈</span> <i class="fm-italics">H</i><sub class="fm-subscript">2</sub></span>, and <span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">3</sub> ≡ <i class="fm-italics">z</i> <span class="cambria">∈</span> <i class="fm-italics">H</i><sub class="fm-subscript">3</sub></span>. The joint probability distribution of height and weight is shown in table <a class="url" href="#tab-joint-prob-distr-dep">5.7</a>.</p>
<p class="fm-table-caption">Table 5.7 Joint probability distribution of dependent events</p>
<table border="1" class="contenttable-1-table" id="tab-joint-prob-distr-dep" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="33.33%"/>
<col class="contenttable-0-col" span="1" width="22.22%"/>
<col class="contenttable-0-col" span="1" width="22.22%"/>
<col class="contenttable-0-col" span="1" width="22.22%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head"/>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Less than 60 kg (<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Between 60 and 90 kg (<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">More than 90 kg (<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>)</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Less than</b> <span class="math">5</span> ft <span class="math">5</span> <b class="fm-bold">in. (<span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">1</sub></span>)</b></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) </span><span class="math">= 0.25</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) </span><span class="math">= 0</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) </span><span class="math">= 0</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Between <span class="math">5</span> ft <span class="math">5</span> in. and <span class="math">6</span> ft</b> (<span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">2</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) </span><span class="math">= 0.</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) </span><span class="math">= 0.5</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) </span><span class="math">= 0</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">More than</b> <b class="fm-bold"><span class="math">6</span> ft (<span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">3</sub></span>)</b>.</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) </span><span class="math">= 0</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) </span><span class="math">= 0</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) </span><span class="math">= 0.25</span></p>
</td>
</tr>
</tbody>
</table>
<p class="body">Note the following about table <a class="url" href="#tab-joint-prob-distr-dep">5.7</a>:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The sum total of all elements in table <a class="url" href="#tab-joint-prob-distr-dep">5.7</a> is <span class="math">1</span>. In other words, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">i</sub></i>, <i class="fm-italics">F<sub class="fm-subscript">j</sub></i>)</span> is a proper probability distribution indicating the probabilities of event <i class="timesitalic">E<sub class="fm-subscript">i</sub></i> and event <i class="timesitalic">F<sub class="fm-subscript">j</sub></i> occurring together. Here <span class="math">(<i class="fm-italics">i</i>, <i class="fm-italics">j</i>) <span class="cambria">∈</span> {1,2,3} × {1,2,3}</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">i</sub></i>, <i class="fm-italics">F<sub class="fm-subscript">j</sub></i>) = 0 <i class="fm-italics">if</i> <i class="fm-italics">i</i> ≠ <i class="fm-italics">j</i> <span class="cambria">∀</span>(<i class="fm-italics">i</i>, <i class="fm-italics">j</i>) <span class="cambria">∈</span> {1,2,3} × {1,2,3}</span>. This essentially means the events are perfectly correlated: the occurrence of <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span> implies the occurrence of <span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">1</sub></span> and vice versa, the occurrence of <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span> implies the occurrence of <span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">2</sub></span> and vice versa, and the occurrence of <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span> implies the occurrence of <span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">3</sub></span> and vice versa. In other words, every adult resident of Statsville who weighs less than <span class="math">60</span> kg is also shorter than <span class="math">5</span> ft <span class="math">5</span> in., and so on. (In life, such perfect correlations rarely exist; but Statsville is a mythical town.)</p>
</li>
</ul>
<h2 class="fm-head" id="geometrical-view-sample-point-distributions-for-dependent-and-independent-variables">5.5 Geometrical view: Sample point distributions for dependent and independent variables</h2>
<p class="body"><a id="marker-159"/>Let’s look at a graphical view of the point distributions corresponding to tables <a class="url" href="#tab-joint-prob-distr">5.5</a> and <a class="url" href="#tab-joint-prob-distr-dep">5.7</a>. There is a fundamental difference in how the point distributions look for independent and dependent variables; it is connected to principal component analysis (PCA) and dimensionality reduction, which we discussed in section <a class="url" href="../Text/04.xhtml#sec-pca">4.4</a>.</p>
<p class="body">We use a rectangular bucket-based technique to visualize joint <span class="math">2</span>D discrete events. For instance, we have three weight-related events, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span>, and three distance-related events, <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">G</i><sub class="fm-subscript">3</sub></span>. Hence the joint distribution has <span class="math">3 × 3 = 9</span> possible events <span class="math">(<i class="fm-italics">E<sub class="fm-subscript">i</sub></i>, <i class="fm-italics">G<sub class="fm-subscript">j</sub></i>)</span>, <span class="math"><span class="cambria">∀</span>(<i class="fm-italics">i</i>, <i class="fm-italics">j</i>) <span class="cambria">∈</span> {1,2,3} × {1,2,3}</span>, as shown in table <a class="url" href="#tab-joint-prob-distr">5.5</a>. Each of these nine events is represented by a small rectangle (bucket for the joint event); altogether, we have a <span class="math">3 × 3</span> grid of rectangular buckets. To visualize the sample point distribution, we have drawn 1,000 samples from the joint distribution. A joint event sample is placed at a random location within its bucket (that is, all points within the bucket have an equal probability of being selected). Notice that the concentration of points is greater inside high-probability buckets and vice versa.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="517" id="fig-joint-point-distr" src="../../OEBPS/Images/CH05_F03a_Chaudhury.png" width="537"/></p>
<p class="figurecaption">(a)</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="580" id="fig-joint-point-distr-dep" src="../../OEBPS/Images/CH05_F03b_Chaudhury.png" width="600"/></p>
<p class="figurecaption">(b)</p>
</div>
<p class="fm-table-caption" id="fig5_3">Figure 5.3 Graphical visualization of joint probability distributions. Rectangles represent buckets of different discrete events. (a) From table <a class="url" href="#tab-joint-prob-distr">5.5</a> independent events). The probabilities of all nine events are non-negligible, and all nine rectangles have a relatively high concentration of sample points. Not suitable for PCA. (b) From table <a class="url" href="#tab-joint-prob-distr-dep">5.7</a> (non-independent events). Events <span class="math">(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>)</span>, <span class="math">(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>)</span>, and <span class="math">(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>)</span> have very high probabilities, and other events have negligible probabilities. Sample points are concentrated along the rectangles on the diagonal. Suitable for PCA.</p>
<p class="body">Graphical views of the point distribution for the independent table <a class="url" href="#tab-joint-prob-distr">5.5</a>) and non-independent table <a class="url" href="#tab-joint-prob-distr-dep">5.7</a>) joint variable pairs are shown in figures <a class="url" href="#fig5_3">5.3a</a> and <a class="url" href="#fig5_3">5.3b</a>, respectively. We see that <i class="fm-italics">the sample point distribution for the independent events is spread somewhat symmetrically over the domain</i>, while <i class="fm-italics">that for the dependent events is spread narrowly around a particular line</i> (in this case, the diagonal). This holds true in general and for higher dimensions, too. You should have this mental picture with respect to independent versus non-independent point distributions. If we sample independent events (uncorrelated), all possible combinations of events <span class="math">{<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">1</sub>}</span>, <span class="math">{<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">2</sub>}</span>, <span class="math">{<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">3</sub>}</span>, <span class="math">⋯</span>, <span class="math">{<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">G</i><sub class="fm-subscript">3</sub>}</span> have a non-negligible probability of occurrence (see table <a class="url" href="#tab-joint-prob-distr">5.5</a>), which is equivalent to saying that none of the events have a very high probability of occurring remember that probabilities sum to <span class="math">1</span>, so if some events have very low probabilities [close to zero], other events must have high probabilities [near one] to compensate). This precludes the concentration of points in a small region of the space. All buckets will have many points. In other words, the joint probability samples of independent events are diffused throughout the population space (see figure <a class="url" href="#fig5_3">5.3a</a>, for instance).</p>
<p class="body">On the other hand, if the events are correlated, the joint probability samples are concentrated in certain high-probability regions of the joint space. For instance, in table <a class="url" href="#tab-joint-prob-distr-dep">5.7</a>, events <span class="math">(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>)</span>, <span class="math">(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>)</span>, <span class="math">(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>)</span> are far more likely than the other combinations. Hence, the sample points are concentrated along the corresponding diagonal (see figure <a class="url" href="#fig5_3">5.3b</a>).</p>
<p class="body">If this does not remind you of PCA (section <a class="url" href="../Text/04.xhtml#sec-pca">4.4</a>), you should re-read that section. Dependent events such as that shown in figure <a class="url" href="#fig5_3">5.3a</a> are good candidates for dimensionality reduction: the two dimensions essentially carry the same information, and if we know one, we can derive the other. We can drop one of the highly correlated dimensions without losing significant information.</p>
<h2 class="fm-head" id="sec-cont-rv">5.6 Continuous random variables and probability density</h2>
<p class="body"><a id="marker-160"/>So far, we have quantized our random variables and made them discrete. For instance, weight has been quantized into three buckets—less than <span class="math">60</span> kg, between <span class="math">60</span> and <span class="math">90</span> kg, and greater than <span class="math">90</span> kg—and probabilities have been assigned to each bucket. What if we want to know probabilities at a more granular level like <span class="math">0 to 10</span> kg, <span class="math">10 to 20</span> kg, <span class="math">20 to 30</span> kg, and so on? Well, we have to create more buckets. Each bucket covers a narrower range of values (a smaller portion of the population space), but there are more of them. In all cases, following the frequentist approach, we count the number of adult Statsvilleans in each bucket, divide that by the total population size, and call that the probability of belonging to that bucket.</p>
<p class="body">What if we want even further granularity? We create even more buckets, each covering an even smaller portion of the population space. In the limit, we have an infinite number of buckets, each covering an infinitesimally small portion of the population space. Together they still cover the population space—a very large number of very small pieces can cover arbitrary regions. At this limit, the probability distribution function is called a <i class="fm-italics">probability density function</i>. Formally,</p>
<p class="fm-equation">The probability density function <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> for a continuous random variable <i class="timesitalic">X</i> is defined as the probability that <i class="timesitalic">X</i> lies between <i class="timesitalic">x</i> and <span class="math"><i class="fm-italics">x</i> + <i class="fm-italics">δx</i></span> with <span class="math"><i class="fm-italics">δx</i> → 0</span></p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>) = lim<sub class="fm-subscript"><i class="fm-italics1">δx</i>→0</sub> <i class="fm-italics">probability</i>(<i class="fm-italics">x</i> ≤ <i class="fm-italics">X</i> &lt; <i class="fm-italics">x</i> + <i class="fm-italics">δx</i>)</span></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> It is slightly unfortunate that the typical symbol for a random variable, <i class="timesitalic">X</i>, collides with that for a dataset (collection of data vectors), also <i class="timesitalic">X</i>. But the context is usually enough to tell them apart.</p>
<p class="body">There is a bit of theoretical nuance here. We are saying that <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> is the probability of the random variable <i class="timesitalic">X</i> lying between <i class="timesitalic">x</i> and <span class="math"><i class="fm-italics">x</i> + <i class="fm-italics">δx</i></span>. This is not exactly the same as saying that <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> is the probability that <i class="timesitalic">X</i> is <i class="fm-italics">equal</i> to <i class="timesitalic">x</i>. But because <i class="timesitalic">δx</i> is infinitesimally small, they amount to the same thing.</p>
<p class="body">Consider the set of events <span class="math"><i class="fm-italics">E</i> = lim<sub class="fm-subscript"><i class="fm-italics1">δx</i>→0</sub> {<i class="fm-italics">x</i> ≤ <i class="fm-italics">X</i> &lt; <i class="fm-italics">x</i> + <i class="fm-italics">δx</i>}</span> for all possible values of <i class="timesitalic">x</i>. All possible values of <i class="timesitalic">x</i> range from negative infinity to infinity: <span class="math"><i class="fm-italics">x</i> <span class="cambria">∈</span> [−∞,∞]</span>. There are infinite such events, each of which is infinitesimally narrow, but together they cover the entire domain <span class="math"><i class="fm-italics">x</i> <span class="cambria">∈</span> [−∞,∞]</span>. In other words, they are exhaustive. They are also mutually exclusive because <i class="timesitalic">x</i> cannot belong to more than one of them at the same time. They are continuous counterparts of the discrete events <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span> that we have seen before.</p>
<p class="body">The fact that the set of events <span class="math"><i class="fm-italics">E</i> = lim<sub class="fm-subscript"><i class="fm-italics1">δx</i> → 0</sub>{<i class="fm-italics">x</i> ≤ <i class="fm-italics">X</i> &lt; <i class="fm-italics">x</i> + <i class="fm-italics">δx</i>}</span> in continuous space is exhaustive and mutually exclusive means we can apply equation <a class="url" href="../Text/05.xhtml#eq-discrete-prob-sum">5.3</a> but the sum will be replaced by an integral as the variable is continuous.<a id="marker-161"/></p>
<p class="body">The sum rule in a continuous domain is expressed as</p><!--<p class="FM-Equation"><span class="times">$$\int\displaylimits_{x = -\infty}^{\infty} p\left(x\right) dx = 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_05-06.png" width="138"/></p>
</div>
<p class="fm-equation-caption">Equation 5.6 <span class="calibre" id="eq-continuous-prob-sum"/></p>
<p class="body">Equation <a class="url" href="#eq-continuous-prob-sum">5.6</a> is the continuous analog of equation <a class="url" href="../Text/05.xhtml#eq-discrete-prob-sum">5.3</a>. It physically means we can say with certainty that <i class="timesitalic">x</i> lies somewhere in the interval <span class="math">−∞</span> to <span class="math">∞</span>.</p>
<p class="body">The random variable can also be multidimensional (that is, a vector). Then the probability density function is denoted as <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>.</p>
<p class="body">The sum rule for a continuous multidimensional probability density function is</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\int\displaylimits_{\vec{x} \in D} p\left(\vec{x}\right) d\vec{x} = 1
\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="68" src="../../OEBPS/Images/eq_05-07.png" width="130"/></p>
</div>
<p class="fm-equation-caption">Equation 5.7 <span class="calibre" id="eq-continuous-vec-prob-sum"/></p>
<p class="body">where <i class="timesitalic">D</i> is the domain of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>—that is, the space containing all possible values of the vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>.</p>
<p class="body">For instance, the <span class="math">2</span>D vector <!--<span class="times">$\begin{bmatrix}x\\y\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="31" src="../../OEBPS/Images/eq_05-07-a.png" width="20"/></span> has the <i class="timesitalic">XY</i> plane as its domain. Note that the integral in equation <a class="url" href="#eq-continuous-vec-prob-sum">5.7</a> is a <i class="fm-italics">multidimensional</i> integral (for example, for <span class="math">2</span>D <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, it is <span class="math"><span class="cambria">∬</span><sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><span class="cambria">∈</span><i class="fm-italics1">D</i></sub> <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) <i class="fm-italics">d</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = 1</span>).</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> For simplicity of notation, we usually use a single integral sign to denote multidimensional integrals. The vector sign in the domain (for example, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <span class="cambria">∈</span> <i class="fm-italics">D</i></span>), as well the vector sign in <span class="math"><i class="fm-italics">d</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span>, indicates multiple dimensions.</p>
<p class="body">You may remember from elementary integral calculus that equation <a class="url" href="#eq-continuous-prob-sum">5.6</a> corresponds to the area under the curve for <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> (or <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>). In higher dimensions, equation <a class="url" href="#eq-continuous-vec-prob-sum">5.7</a> corresponds to the volume under the hypersurface for <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. Thus, <i class="fm-italics">the total area under a univariate probability density curve is always 1</i>. And in higher dimensions, <i class="fm-italics">the volume under the hypersurface for a multivariate probability density function is always 1</i>.</p>
<h2 class="fm-head" id="sec-distr-properties">5.7 Properties of distributions: Expected value, variance, and covariance</h2>
<p class="body">Toward the beginning of this chapter, we stated that generative machine learning models are often developed by fitting a distribution from a known family to the available training data. Thus, we postulate a parameterized distribution from a known family and estimate the exact parameters that best fit the training data. Most distribution families are parameterized in terms of intuitive properties like the mean, variance, and so on. Understanding these concepts and their geometric significance is essential for understanding the models based on them.</p>
<p class="body">In this section, we explain a few properties/parameters common to all distributions. Later, when we discuss individual distributions, we connect them to the parameters of those distributions. We also show how to programmatically obtain the values of these for each individual distribution via the PyTorch <code class="fm-code-in-text">distributions</code> package.<a id="marker-162"/></p>
<h3 class="fm-head1" id="expected-value-aka-mean">5.7.1 Expected value (aka mean)</h3>
<p class="body">If we sample a random variable with a given distribution many times and take the average of the sampled values, what value do we expect to end up with? The average will be closer to the values with higher probabilities (as these appear more often during sampling). If we sample enough times, for a given probability distribution, this average always settles down to a fixed value for that distribution: the <i class="fm-italics">expected value</i> of the distribution.</p>
<p class="body">Formally,</p>
<p class="fm-equation"><!--[](</span>)</span> -->given a discrete distribution <i class="timesitalic">D</i> where a discrete random variable <i class="timesitalic">X</i> can take any value from the sets <span class="math">{<i class="fm-italics">x</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">x</i><sub class="fm-subscript">2</sub>,⋯, <i class="fm-italics">x<sub class="fm-subscript">n</sub></i>}</span> with respective probabilities <span class="math">{<i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">1</sub>), <i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">2</sub>)⋯, <i class="fm-italics">p</i>(<i class="fm-italics">x<sub class="fm-subscript">n</sub></i>)}</span>, the expected value is given by the formula</p><!--<p class="Body"><span class="times">$$\mathbb{E}\left(X\right) = \lim_{N \to \infty}
\frac{1}{N}\sum_{k=1}^{N}  x_{k} \rightarrow D   = \sum_{i=1}^{n} p\left(x_{i}\right) x_{i}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_05-08.png" width="292"/></p>
</div>
<p class="fm-equation-caption">Equation 5.8 <span class="calibre" id="eq-discrete-univar-expected-val"/></p>
<p class="fm-equation">where <span class="math"><i class="fm-italics">x<sub class="fm-subscript">k</sub></i> → <i class="fm-italics">D</i></span> denotes the <i class="timesitalic">k</i>th sample drawn from the distribution <i class="timesitalic">D</i>. Overall, equation <a class="url" href="#eq-discrete-univar-expected-val">5.8</a> says that <i class="fm-italics">the average or expected value of a very large number of samples drawn from the distribution approaches the probability-weighted sum of all possible sample values</i>. When we sample, the higher-probability values appear more frequently than the lower-probability values, so the average over a large number of samples is pulled closer to the higher-probability values.</p>
<p class="body">For multivariate random variables:</p>
<p class="fm-equation"><!--[](</span>)</span> -->Given a discrete distribution where a discrete multidimensional random variable <i class="timesitalic">X</i> can take any value from the sets <span class="math">{<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">1</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">2</sub>,⋯, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i>}</span> with respective probabilities <span class="math">{<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">1</sub>), <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">2</sub>),⋯, <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i>)}</span>, the expected value is given by the formula</p><!--<p class="Body"><span class="times">$$\mathbb{E}\left(X\right) = \sum_{i=1}^{n} p\left(\vec{x}_{i}\right) \vec{x}_{i}
%\tag{\protect\normalfont\normalsize 5.9}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_05-09.png" width="138"/></p>
</div>
<p class="fm-equation-caption">Equation 5.9 <span class="calibre" id="eq-discrete-multivar-expected-val"/></p>
<p class="body">For continuous random variables (note how the sum is replaced by an integral):</p>
<p class="fm-equation"><!--[](</span>)</span> -->The expected value of a continuous random variable <i class="timesitalic">X</i> that takes values from <span class="math">−∞</span> to <span class="math">∞</span> (that is, <span class="math"><i class="fm-italics">x</i> <span class="cambria">∈</span> { − ∞, ∞}</span>) is</p><!--<p class="Body"><span class="times">$$\mathbb{E}\left(X\right) =
\begin{cases}
&amp;\int\displaylimits_{x=-\infty}^{\infty} x \; p\left(x\right) dx \Rightarrow \text{for continuous univariate distributions} \\[6pt]
&amp;\int\displaylimits_{\vec{x} \in D} \vec{x} \; p\left(\vec{x}\right) d\vec{x} \Rightarrow \text{for continuous multivariate distributions}
\end{cases}
%\tag{\protect\normalfont\normalsize 5.10}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="77" src="../../OEBPS/Images/eq_05-10.png" width="487"/></p>
</div>
<p class="fm-equation-caption">Equation 5.10 <span class="calibre" id="eq-continuous-expected-val"/></p>
<p class="fm-head2" id="expected-value-and-center-of-mass-in-physics">Expected value and center of mass in physics</p>
<p class="body"><a id="marker-163"/>In physics, we have the concept of the center of mass or centroid. If we have a set of points, each with a mass, the entire point set can be replaced by a single point. This point is called the <i class="fm-italics">centroid</i>. The position of the centroid is the weighted average of the positions of the individual points, weighted by their individual masses. If we mentally think of the probabilities of individual points as masses, the notion of expected value in statistics corresponds to the notion of centroid in physics.</p>
<p class="fm-head2" id="expected-value-of-an-arbitrary-function-of-a-random-variable">Expected value of an arbitrary function of a random variable</p>
<p class="body">So far, we have seen the expected value of the random variable itself. The notion can be extended to functions of the random variable.</p>
<p class="body">The expected value of a function of a random variable is the probability-weighted sum of the values of that function at all possible values of the random variable. Formally,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\mathbb{E}\left(\,f\left(X\right)\right) &amp;= \sum_{i=1}^{n} f\left(x_{i}\right) p\left(x_{i}\right) \Rightarrow \text{for discrete univariate distributions} \nonumber\\
\mathbb{E}\left(\,f\left(X\right)\right) &amp;= \sum_{i=1}^{n} f\left(\vec{x}_{i}\right) p\left(\vec{x}_{i}\right) \Rightarrow
\text{for discrete multivariate distributions} \nonumber\\
\mathbb{E}\left(\,f\left(X\right)\right)
&amp;=  \int\displaylimits_{x=-\infty}^{\infty} f\left(x\right) \; p\left(x\right) dx \Rightarrow \text{for continuous univariate distributions} \nonumber\\
\mathbb{E}\left(\,f\left(X\right)\right) &amp;=
\int\displaylimits_{\vec{x} \in D} f\left( \vec{x}\right) \; p\left(\vec{x}\right) d\vec{x} \Rightarrow \text{for continuous multivariate distributions}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="252" src="../../OEBPS/Images/eq_05-11.png" width="581"/></p>
</div>
<p class="fm-equation-caption">Equation 5.11 <span class="calibre" id="eq-func-rv-expected-val"/></p>
<p class="fm-head2" id="expected-value-and-dot-product">Expected value and dot product</p>
<p class="body">In equation <a class="url" href="02.xhtml#eq-dot-product">2.6</a>, we looked at the dot product between two vectors. Further, in section <a class="url" href="02.xhtml#subsubsec-dotproduct_as_agreement">2.5.6.2</a>, we saw that the dot product between two vectors measures the agreement between the two vectors. If both point in the same direction, the dot product is larger. In this section, we show that the expected value of a function of a random variable can be viewed as a dot product between a vector representing the probability and another vector representing the function itself.</p>
<p class="body">First let’s consider the discrete case. Our random variable can take values <i class="timesitalic">x<sub class="fm-subscript">i</sub></i>, <span class="math"><i class="fm-italics">i</i> <span class="cambria">∈</span> {1, <i class="fm-italics">n</i>}</span>. Now, imagine a vector <!--<span class="times">$\vec{f} = \begin{bmatrix} f\left(x_{1}\right) \\[-3pt] f\left(x_{2}\right)\\[-3pt]
\cdots\\[-3pt] f\left(x_{n}\right)
\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="87" src="../../OEBPS/Images/eq_05-11-a.png" width="90"/></span> and a vector <!--<span class="times">$\vec{p} =
\begin{bmatrix} p\left(x_{1}\right) \\[-3pt] p\left(x_{2}\right)\\[-3pt]
\cdots\\[-3pt] p\left(x_{n}\right)
\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="87" src="../../OEBPS/Images/eq_05-11-b.png" width="86"/></span>. From equation <a class="url" href="#eq-func-rv-expected-val">5.11</a>, we see that the expected value of the function <span class="math"><span class="segoe">𝔼</span>(<i class="fm-italics">f</i>(<i class="fm-italics">X</i>))</span> of random variable <i class="timesitalic">X</i> is the same as <span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_f.png" width="22"/></span><sup class="fm-superscript">T</sup><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span></i> = <i class="fm-italics"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_f.png" width="22"/></span></i> ⋅ <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span></span>. This is high when <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_f.png" width="22"/></span> and <span class="times"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span> are aligned; thus, the expected value of the function of the random variable is high when the high function values coincide with high probabilities of the random variable and vice versa. In the continuous case, these vectors have an infinite number of components and the summation is replaced by an integral, but the idea stays the same.</p>
<p class="fm-head2" id="expected-value-of-linear-combinations-of-random-variables">Expected value of linear combinations of random variables</p>
<p class="body">The expected value is a linear operator. This means the expected value of a linear combination of random variables is a linear combination (with the same weights) of the expected values of the random variables. Formally,</p>
<p class="fm-equation"><span class="math"><span class="segoe">𝔼</span>(<i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">X</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">X</i><sub class="fm-subscript">2</sub> ⋯ <i class="fm-italics">α<sub class="fm-subscript">n</sub>X<sub class="fm-subscript">n</sub></i>) = <i class="fm-italics">α</i><sub class="fm-subscript">1</sub><span class="segoe">𝔼</span>(<i class="fm-italics">X</i><sub class="fm-subscript">1</sub>) + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><span class="segoe">𝔼</span>(<i class="fm-italics">X</i><sub class="fm-subscript">2</sub>) + ⋯ <i class="fm-italics">α<sub class="fm-subscript">n</sub></i><span class="segoe">𝔼</span>(<i class="fm-italics">X<sub class="fm-subscript">n</sub></i>)</span></p>
<p class="fm-equation-caption">Equation 5.12</p>
<h3 class="fm-head1" id="sec-var-covar-std">5.7.2 Variance, covariance, and standard deviation</h3>
<p class="body"><a id="marker-164"/>When we draw a very large number of samples from a given point distribution, we often like to know the spread of the point set. The spread is not merely a matter of measuring the largest distance between two points in the distribution. Rather, we want to know how densely packed the points are. If most of the points fit within a very small ball, then even if one or two points are far from the ball, we call that a <i class="fm-italics">small spread</i> or <i class="fm-italics">high packing density</i>.</p>
<p class="body">Why is this important in machine learning? Let’s start with a few informal examples. If we discover that the points are tightly packed in a small region around a single point, we may want to replace the entire distribution with that point without causing much error. Or if the points are packed tightly around a single straight line, we can replace the entire distribution with that line. Doing so gives us a simpler lower-dimensional) representation and often leads to a view of the data that is more amenable to understanding the big picture. This is because small variations about a particular point or direction are usually caused by noise, while large variations are caused by meaningful things. By eliminating small variations and focusing on the large ones, we capture the main information content. (This could be why older people tend to be better at forming big-picture views: perhaps there are too few neurons in their heads to retain the huge amount of memory data they have accumulated over the years. Their brain performs dimensionality reduction.) This is the basic idea behind PCA and dimensionality reduction, which we saw in section <a class="url" href="../Text/04.xhtml#sec-pca">4.4</a>.</p>
<p class="body">Variance—or its square root, standard deviation—measures how densely packed around the expected value the points in the distribution are: that is, the spread of the point distribution. Formally, the variance of a probability distribution is defined as follows:</p><!--<p class="Body"><span class="times">$$var\left(X\right) =
\begin{cases}
&amp;\sum_{i=1}^{n}\left(x_{i} - \mu\right)^{2} p\left(x_{i}\right) \Rightarrow \text{    for a discrete n point distribution} \\[6pt]
&amp;\int\displaylimits_{x=-\infty}^{\infty} \left(x -
\mu\right)^{2} p\left(x\right) dx \Rightarrow \text{    for a continuous distribution}
\end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_05-13.png" width="548"/></p>
</div>
<p class="fm-equation-caption">Equation 5.13 <span class="calibre" id="eq-variance"/></p>
<p class="body">By comparing equation <a class="url" href="../Text/05.xhtml#eq-variance">5.13</a> to equations <a class="url" href="#eq-continuous-expected-val">5.10</a> and <a class="url" href="#eq-func-rv-expected-val">5.11</a>, we see that the variance is the expected value of the distance <span class="math">(<i class="fm-italics">x</i> − <i class="fm-italics">μ</i>)<sup class="fm-superscript">2</sup></span> of sample points <i class="timesitalic">x</i> from the mean <i class="timesitalic">μ</i>. So if the more probable (more frequently occurring) sample points lie within a short distance of the mean, the variance is small, and vice versa. That is to say, the variance measures how tightly packed the points are around the mean.</p>
<p class="fm-head2" id="covariance-variance-in-higher-dimensions">Covariance: Variance in higher dimensions</p>
<p class="body"><a id="marker-165"/>Extending the notion of the expected value from the univariate case to the multivariate case was straightforward. In the univariate case, we take a probability-weighted average of a scalar quantity, <i class="timesitalic">x</i>. The resulting expected value is a scalar, <span class="math"><i class="fm-italics">μ</i> = ∫<sub class="fm-subscript"><i class="fm-italics1">x</i> = −∞</sub><sup class="fm-superscript">∞</sup> <i class="fm-italics">x</i> <i class="fm-italics">p</i>(<i class="fm-italics">x</i>)<i class="fm-italics">dx</i></span>. In the multivariate case, we take the probability-weighted average of a vector quantity, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. The resulting expected value is a vector, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> = ∫<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><span class="cambria">∈</span><i class="fm-italics1">D</i></sub> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)<i class="fm-italics">d</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span>.</p>
<p class="body">Extending the notion of variance to the multivariate case is not as straightforward. This is because we can traverse the multidimensional random vector’s domain (the space over which the vector is defined) in an infinite number of possible directions—think how many possible directions we can walk on a <span class="math">2</span>D plane—and the spread or packing density can be different for each direction. For instance, in figure <a class="url" href="#fig5_3">5.3b</a>, the spread along the main diagonal is much larger than the spread in a perpendicular direction.</p>
<p class="fm-equation">The covariance of a multidimensional point distribution is a matrix that allows us to easily measure the spread or packing density in any desired direction. It also allows us to easily figure out the direction in which the maximum spread occurs and what that spread is.</p>
<p class="body">Consider a multivariate random variable <i class="timesitalic">X</i> that takes vector values <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. Let <i class="timesitalic">l̂</i> be an arbitrary direction (as always, we use overhead hats to denote unit-length vectors signifying directions) in which we want to measure the packing density of <i class="timesitalic">X</i>. We discussed in sections <a class="url" href="02.xhtml#subsec-dotprod-ml">2.5.2</a> and <a class="url" href="02.xhtml#subsection-dot_product">2.5.6</a> that the dot product of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> in the direction <i class="timesitalic">l̂</i> (that is, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>l̂</i></span>) is the projection or component (effective value) of <i class="timesitalic">x</i> along <i class="timesitalic">l̂</i>. Thus the spread or packing density of the random vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> in direction <i class="timesitalic">l̂</i> is the same as the spread of the dot product (aka component or projection) <span class="math"><i class="fm-italics">l̂<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span>. This projection <span class="math"><i class="fm-italics">l̂<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> is a scalar quantity: we can use the univariate formula to measure its variance.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> In this context, we can use <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>l̂</i></span> and <span class="math"><i class="fm-italics">l̂<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> interchangeably. The dot product is symmetric.</p>
<p class="body">The expected value of the projection is</p><!--<p class="Body"><span class="times">$$\vec{\mu}_{l} = \int\displaylimits_{\vec{x} \in D} \left(\hat{l}^{{\kern1pt}T} \vec{x}\right) \; p\left(\vec{x}\right) d\vec{x}
= \hat{l}^{{\kern1pt}T}  \int\displaylimits_{\vec{x} \in D} \vec{x} \; p\left(\vec{x}\right) d\vec{x}
=  \hat{l}^{{\kern1pt}T} \vec{\mu}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="67" src="../../OEBPS/Images/eq_05-13-a.png" width="374"/></p>
</div>
<p class="body">The variance is given by</p><!--<p class="Body"><span class="times">$$var\left(\hat{l}^{{\kern1pt}T}\vec{x}\right) =
\int\displaylimits_{\vec{x} \in D} \left( \hat{l}^{{\kern1pt}T} \vec{x}
- \hat{l}^{{\kern1pt}T} \vec{\mu}\right)^{2} d\vec{x}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="63" src="../../OEBPS/Images/eq_05-13-b.png" width="251"/></p>
</div>
<p class="body">Now, since the transpose of a scalar is the same scalar, we can write the square term within the integral as the product of the scalar <span class="math"><i class="fm-italics">l̂<sup class="fm-superscript">T</sup></i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> - <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>)</span> and its transpose:</p><!--<p class="Body"><span class="times">$$var\left(\hat{l}^{{\kern1pt}T}\vec{x}\right)
=  \int\displaylimits_{\vec{x} \in D} \left( \hat{l}^{{\kern1pt}T}
\vec{x} - \hat{l}^{{\kern1pt}T} \vec{\mu}\right)  \left(
\hat{l}^{{\kern1pt}T} \vec{x} - \hat{l}^{{\kern1pt}T}
\vec{\mu}\right)^{T}   d\vec{x} =
\int\displaylimits_{\vec{x} \in D} \hat{l}^{{\kern1pt}T} \left( \vec{x}
- \vec{\mu}\right)  \left( \hat{l}^{{\kern1pt}T}\left( \vec{x}
-  \vec{\mu}\right)\right)^{T} \; d\vec{x}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_05-13-c.png" width="605"/></p>
</div>
<p class="body">Using equation <a class="url" href="02.xhtml#eq-mat-prod-transpose">2.10</a>,</p><!--<p class="Body"><span class="times">$$var\left(\hat{l}^{{\kern1pt}T}\vec{x}\right)
= \int\displaylimits_{\vec{x} \in D} \hat{l}^{{\kern1pt}T} \left(
\vec{x} - \vec{\mu}\right) \left( \vec{x} -  \vec{\mu}\right)^{T} \left(
\hat{l}^{{\kern1pt}T} \right)^{T} \; d\vec{x}
= \int\displaylimits_{\vec{x} \in D} \hat{l}^{{\kern1pt}T} \left(
\vec{x} - \vec{\mu}\right)  \left( \vec{x}
-  \vec{\mu}\right)^{T}  \hat{l}\; d\vec{x}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="63" src="../../OEBPS/Images/eq_05-13-d.png" width="569"/></p>
</div>
<p class="body">Since</p><!--<p class="Body"><span class="times"><i class="fm-italics">l̂</em></span> is independent of <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span>, we can take it out of the integral. Hence, <span class="times">$$var\left(\hat{l}^{{\kern1pt}T}\vec{x}\right)
=  \hat{l}^{{\kern1pt}T} \left(\; \int\displaylimits_{\vec{x} \in D}\left( \vec{x} - \vec{\mu}\right)  \left( \vec{x}
-  \vec{\mu}\right)^{T} d\vec{x} \right)  \hat{l} =
\hat{l}^{{\kern1pt}T} {\mathbb{C}\left({X}\right)} \hat{l}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="82" src="../../OEBPS/Images/eq_05-13-e.png" width="410"/></p>
</div>
<p class="body">where</p><!--<p class="Body"><span class="times">$$\mathbb{C}\left(X\right) =
\begin{cases}
&amp;\sum_{i=1}^{n}
\left(\vec{x}-\vec{\mu}\right)\left(\vec{x}-\vec{\mu}\right)^{T}\Rightarrow\text{    for discrete $n$ point distributions}\\
&amp;\int\displaylimits_{\vec{x} \in D}\left( \vec{x} -
\vec{\mu}\right)  \left( \vec{x} -  \vec{\mu}\right)^{T} d\vec{x}\Rightarrow\text{    for continuous distributions}
\end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_05-14.png" width="542"/></p>
</div>
<p class="fm-equation-caption">Equation 5.14 <span class="calibre" id="eq-covariance"/></p>
<p class="body"><a id="marker-166"/>For simplicity, we drop the <i class="timesitalic">X</i> in parentheses and simply write <span class="math">ℂ(<i class="fm-italics">X</i>)</span> as <span class="math">ℂ</span>. An equivalent way of looking at the covariance matrix of a <i class="timesitalic">d</i>-dimensional random variable <i class="timesitalic">X</i> taking vector values <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is as follows:</p><!--<p class="Body"><span class="times">$$\mathbb{C} =
\begin{bmatrix}
\sigma_{11} &amp; \sigma_{12} &amp; \sigma_{13} \cdots  &amp;
\sigma_{1d}\\
\sigma_{21} &amp; \sigma_{22} &amp; \sigma_{23} \cdots  &amp;
\sigma_{2d}\\
\vdots\\
\sigma_{d1} &amp; \sigma_{d2} &amp; \sigma_{d3} \cdots  &amp;
\sigma_{dd}\\
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="137" src="../../OEBPS/Images/eq_05-15.png" width="233"/></p>
</div>
<p class="fm-equation-caption">Equation 5.15 <span class="calibre" id="eq-covar-eltwise"/></p>
<p class="body">where</p><!--<p class="Body"><span class="times">$$\sigma_{i,j} =
\begin{cases}
\int\displaylimits_{x_{i} \in D_{i}} \int\displaylimits_{x_{j} \in D_{j}} \left( x_{i} - \mu_{i}\right)  \left(x_{j} -  \mu_{j}\right) dx_{i} dx_{j}\Rightarrow\text{    for continuous distributions}\\
\sum_{i=1}^{n} \sum_{j=1}^{n} \left(x_{i} -
\mu_{i}\right)\left(x_{j}-\mu_{j}\right)\Rightarrow\text{    for discrete $n$ point distributions}
\end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="68" src="../../OEBPS/Images/eq_05-15-a.png" width="596"/></p>
</div>
<p class="body">is the co-variance of the <i class="timesitalic">i</i>th and <i class="timesitalic">j</i>th dimensions of the random vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>.</p>
<p class="body"><span class="math">ℂ(<i class="fm-italics">X</i>)</span> or <span class="math">ℂ</span> is the <i class="fm-italics">covariance matrix</i> of the random variable <i class="timesitalic">X</i>. A little thought reveals that equations <a class="url" href="#eq-covariance">5.14</a> and <a class="url" href="#eq-covar-eltwise">5.15</a> are equivalent.</p>
<p class="body">The following things are noteworthy:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">From equation <a class="url" href="#eq-covariance">5.14</a>, <span class="math">ℂ</span> is the sum of the products of <span class="math"><i class="fm-italics">d</i> × 1</span> vectors <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>−<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>)</span> and their transpose <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>−<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i></span>, <span class="math">1 × <i class="fm-italics">d</i></span> vectors. Hence, <span class="math">ℂ</span> is a <span class="math"><i class="fm-italics">d</i> × <i class="fm-italics">d</i></span> matrix.</p>
</li>
<li class="fm-list-bullet">
<p class="list">This matrix is independent of the direction, <i class="timesitalic">l̂</i>, in which we are measuring the variance or spread. We can precompute <span class="math">ℂ</span>; then, when we need to measure the variance in any direction <i class="timesitalic">l̂</i>, we can evaluate the quadratic form <span class="math"><i class="fm-italics">l̂<sup class="fm-superscript">T</sup></i> ℂ<i class="fm-italics">l̂</i></span> to obtain the variance in that direction. Thus <span class="math">ℂ</span> is a generic property of the distribution, much like <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>. <span class="math">ℂ</span> is called the <i class="fm-italics">covariance</i> of the distribution.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Covariance is the multivariate peer of the univariate entity variance.</p>
</li>
</ul>
<p class="body">That covariance is the multivariate analog of variance is evident by comparing the expressions in equations <a class="url" href="../Text/05.xhtml#eq-variance">5.13</a> and <a class="url" href="#eq-covariance">5.14</a>.</p>
<p class="fm-head2" id="variance-and-expected-value">Variance and expected value</p>
<p class="body">As outlined previously, the variance is the expected value of the distance <span class="math">(<i class="fm-italics">x</i> − <i class="fm-italics">μ</i>)<sup class="fm-superscript">2</sup></span> of sample points <i class="timesitalic">x</i> from the mean <i class="timesitalic">μ</i>. This can be easily seen by comparing equations <a class="url" href="../Text/05.xhtml#eq-variance">5.13</a>, <a class="url" href="#eq-continuous-expected-val">5.10</a>, and <a class="url" href="#eq-func-rv-expected-val">5.11</a> and leads to the following formula (where we use the principle of the expected value of linear combinations):</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">var</i>(<i class="fm-italics">X</i>) = <span class="segoe">𝔼</span>((<i class="fm-italics">X</i> − <i class="fm-italics">μ</i>)<sup class="fm-superscript">2</sup>) = <span class="segoe">𝔼</span>(<i class="fm-italics">X</i><sup class="fm-superscript">2</sup>) − <span class="segoe">𝔼</span>(2<i class="fm-italics">μX</i>) + <span class="segoe">𝔼</span>(<i class="fm-italics">μ</i><sup class="fm-superscript">2</sup>)</span></p>
<p class="body">Since <i class="timesitalic">μ</i> is a constant, we can take it out of the expected value (a special case of the principal of the expected value of linear combinations). Thus we get</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">var</i>(<i class="fm-italics">X</i>) = <span class="segoe">𝔼</span>(<i class="fm-italics">X</i><sup class="fm-superscript">2</sup>) − 2<i class="fm-italics">μ</i><span class="segoe">𝔼</span>(<i class="fm-italics">X</i>) + <i class="fm-italics">μ</i><sup class="fm-superscript">2</sup><span class="segoe">𝔼</span>(1)</span></p>
<p class="body">But <span class="math"><i class="fm-italics">μ</i> = <span class="segoe">𝔼</span>(<i class="fm-italics">X</i>)</span>. Also, the expected value of a constant is that constant. So, <span class="math"><span class="segoe">𝔼</span>(1) = 1</span>.</p>
<p class="body">Hence,</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">var</i>(<i class="fm-italics">X</i>) = <span class="segoe">𝔼</span>(<i class="fm-italics">X</i><sup class="fm-superscript">2</sup>) − 2<i class="fm-italics">μ</i><sup class="fm-superscript">2</sup> + <i class="fm-italics">μ</i><sup class="fm-superscript">2</sup><span class="segoe">𝔼</span>(1) = <span class="segoe">𝔼</span>(<i class="fm-italics">X</i><sup class="fm-superscript">2</sup>) − <i class="fm-italics">μ</i><sup class="fm-superscript">2</sup></span></p>
<p class="body">or</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">var</i>(<i class="fm-italics">X</i>) = <span class="segoe">𝔼</span>(<i class="fm-italics">X</i><sup class="fm-superscript">2</sup>) − <span class="segoe">𝔼</span>(<i class="fm-italics">X</i>)<sup class="fm-superscript">2</sup></span></p>
<p class="fm-equation-caption">Equation 5.16 <span class="calibre" id="eq-mean-var"/></p>
<h2 class="fm-head" id="sec-sampling">5.8 Sampling from a distribution</h2>
<p class="body"><a id="marker-167"/>Drawing a sample from the probability distribution of a random variable yields an arbitrary value from the set of possible values. If we draw many samples, the higher-probability values show up more often than lower-probability values. The sampled points form a cloud in the domain of possible values, and the region where the probabilities are higher is more densely populated than lower-probability regions. In other words, in a sample point cloud, higher-probability values are overrepresented. Thus a collection of sample points is often referred to as a <i class="fm-italics">sample point cloud</i>. The hope, of course, is that the sample point cloud is a good representation of the entire population so that analyzing the points in the cloud will yield insights about the entire population. In univariate cases, the sample value is a scalar and represented by a point on the number line. In multivariate cases, the sample value is a vector and represented as a point in a higher-dimensional space.</p>
<p class="body">It is often useful to compute aggregate statistics (such as the mean and variance) to describe the population. If we know a distribution, we can use closed-form expressions to obtain these properties. Many standard distributions and closed-form equations for obtaining their means and variance are discussed in section <a class="url" href="#sec-famous-distr">5.9</a>. But often, we don’t know the underlying distribution. Under those circumstances, the sample mean and sample variance can be used. Given a set of <i class="timesitalic">n</i> samples <span class="math"><i class="fm-italics">X</i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">1</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">2</sub>⋯<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span> from any distribution, the sample mean and variance are computed as</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{\mu}_{n} = \frac{1}{n} \sum\displaylimits_{i=1}^{n}
\vec{x}_{i} \nonumber\\[2pt]
&amp;\sigma_{n}^{2} = \frac{1}{n} \sum\displaylimits_{i=1}^{n}  \left(
\vec{x}_{i} - \vec{\mu}_{n} \right)^{2}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="122" src="../../OEBPS/Images/eq_05-16-a.png" width="170"/></p>
</div>
<p class="body">In some situations, like Gaussian distributions (which we discuss shortly), it can be theoretically proved that the sample mean and variance are optimal (the best possible guesses of the true mean and variance, given the sampled data). Also, the sample mean approaches the true mean as the number of samples increases, and with enough samples, we get a pretty good approximation of the true mean. In the next subsection, we learn more about how much is “enough.”</p>
<p class="fm-head2" id="law-of-large-numbers-how-many-samples-are-enough">Law of large numbers: How many samples are enough?</p>
<p class="body"><a id="marker-168"/>Informally speaking, the law of large numbers says that if we draw a large number of sample values from a probability distribution, their average should be close to the expected value of the distribution. In the limit, the average over an infinite number of samples will match the mean.</p>
<p class="body">In practice, we cannot draw an infinite number of samples, so there is no guarantee that the sample mean will coincide with the expected value (true mean) in real-life sampling. But if the number of samples is large, they will not be too different. This is not a matter of mere theory. Casinos design games where the probability of the house winning a bet against the guest is slightly higher than the probability of the guest winning. The expected value of the outcome is that the casino wins rather than the guest. Over the very large number of bets placed in a casino, this is exactly what happens—and that is why casinos make money on the whole, even though they may lose individual bets.</p>
<p class="body">How many samples is “a large number of samples?” Well, it is not defined precisely. But one thing is known: if the variance is larger, more samples need to be drawn to make the law of large numbers apply.</p>
<p class="body">Let’s illustrate this with an example. Consider a betting game. Suppose that the famous soccer club FC Barcelona, for unknown reasons, has agreed to play a very large number of matches against the Machine Learning Experts’ Soccer Club of Silicon Valley. We can place a bet of $100 on a team. If that team wins, we get back $200: that is, we make $100. If that team loses, we lose the bet: that is, we make –$100. The betting game is happening in a country where nobody knows anything about the reputations of these clubs. A bettor bets on FC Barcelona in the first game and wins $100. Based on this one observation, can the bettor say that by betting on Barcelona, they expect to win $100 every time? Obviously not.</p>
<p class="body">But suppose the bettor places <span class="math">100</span> bets and wins $100 <span class="math">99</span> times and loses $100 once. Now the bettor can expect with some confidence that they will win $100 (or close to it) by betting on Barcelona. Based on these observations, the sample mean winnings from a bet on FC Barcelona are <span class="math">0.99 × (100) + 0.01 × (−100) = 98</span>. The sample standard deviation is <span class="math">√(.99 × (98 - 100)<sup class="fm-superscript">2</sup> + 0.01 × (98 - (-100))<sup class="fm-superscript">2</sup>) = 19.8997</span>. Relative to the sample mean, the sample standard deviation is <span class="math">19.8997/98 = 0.203</span>.</p>
<p class="body">Next, consider the same game, except now FC Barcelona is playing the Real Madrid football club. Since the two teams are evenly matched (the theoretical win probability of Barcelona is <span class="math">0.5</span>), the results are no longer one-sided. Suppose that after <span class="math">100</span> games, FC Barcelona has won <span class="math">60</span> times and Real Madrid has won <span class="math">40</span> times. The sample mean winnings on a Barcelona bet are <span class="math">0.6 × (100) + 0.4 × (−100) = 20</span>. The sample standard deviation is <span class="math">√(.6 × (20 - 100)<sup class="fm-superscript">2</sup> + 0.4 × (20 - (-100))<sup class="fm-superscript">2</sup>) = 97.9795</span>. Relative to the sample mean, the sample standard deviation is <span class="math">97.9795/20 = 4.89897</span>. This is a much larger number than the previous <span class="math">0.203</span>. In this case, even after <span class="math">100</span> trials, a bettor cannot be very confident in predicting that the expected win is the sample mean, $<span class="math">20</span>.</p>
<p class="body">The overall intuition is as follows:</p>
<p class="fm-equation">If we take a sufficiently large number of samples, their average is close to the expected value. The exact definition of what constitutes a “sufficiently large” number of samples is not known. However, the larger the variance (relative to the mean), the more samples are needed.</p>
<h2 class="fm-head" id="sec-famous-distr">5.9 Some famous probability distributions</h2>
<p class="body"><a id="marker-169"/>In this section, we introduce some probability distributions and density functions often used in deep learning. We will use PyTorch code snippets to demonstrate how to set up, sample, and compute properties like expected values, variance/covariance, and so on for each distribution. Note the following:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">In the code snippets, for every distribution, we evaluate the probability using</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">A PyTorch <code class="fm-code-in-text">distributions</code> function call</p>
</li>
<li class="fm-list-bullet">
<p class="list">A raw evaluation from the formula (to understand the math)</p>
</li>
</ul>
<p class="body">Both should yield the same result. In practice, you should use the PyTorch <code class="fm-code-in-text">distributions</code> function call instead of the raw formula.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In the code snippets, for every distribution,</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">We evaluate the theoretical mean and variance using a PyTorch <code class="fm-code-in-text">distributions</code> function call.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We evaluate the sample mean and variance.</p>
</li>
</ul>
<p class="body">When the sample set is large enough, the sample mean and theoretical mean should be close. Ditto for variance.</p>
</li>
</ul>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for these distributions, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/8NVg">http://mng.bz/8NVg</a>.</p>
<p class="body">Another point to remember: In machine learning, we often work with the logarithm of the probability. Since the popular distributions are exponential, this leads to simpler computations. With that, let’s dive into the probability distributions.</p>
<h3 class="fm-head1" id="sec-uniform-distr">5.9.1 Uniform random distributions</h3>
<p class="body"><a id="marker-170"/>Consider a continuous random variable <i class="timesitalic">x</i> that can take any value from a fixed compact range, say <span class="math">[<i class="fm-italics">a</i>, <i class="fm-italics">b</i>]</span>, <i class="fm-italics">with equal probability, while the probability of x taking a value outside the range is zero</i>. The corresponding <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> is a uniform probability distribution. Formally stated,</p><!--<p class="Body"><span class="times">$$p\left(x\right)  =
  \begin{cases}
   \dfrac{1}{b -a} &amp; \text{if } x \in  \left[a, b \right] \\[5pt]
   0   &amp; \text{otherwise}
  \end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="84" src="../../OEBPS/Images/eq_05-17.png" width="222"/></p>
</div>
<p class="fm-equation-caption">Equation 5.17 <span class="calibre" id="eq-uniform-random-univar"/></p>
<p class="body">Equation <a class="url" href="#eq-uniform-random-univar">5.17</a> means <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> is constant, <span class="math">1/<i class="fm-italics">b-a</i></span>, for <i class="timesitalic">x</i> between <i class="timesitalic">a</i> and <i class="timesitalic">b</i> and zero for other values of <i class="timesitalic">x</i>. Note how the value of the constant is cleverly chosen to make the total area under the curve <span class="math">1</span>. This equation is depicted graphically in figure <a class="url" href="#fig-univar-uniform-distr">5.4</a>, and listing 5.1 shows the PyTorch code for the log probability of a univariate uniform random distribution.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="456" id="fig-univar-uniform-distr" src="../../OEBPS/Images/CH05_F04_Chaudhury.png" width="600"/></p>
<p class="figurecaption">Figure 5.4 Univariate (single-variable) uniform random probability density function. Probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> is constant, <span class="math">0.05</span>, in the interval <span class="math">[−10,10]</span> and zero everywhere outside the interval. Thus it depicts equation <a class="url" href="#eq-uniform-random-univar">5.17</a> with <span class="math"><i class="fm-italics">b</i> = 10, <i class="fm-italics">a</i> = −10</span>. The area under the curve is the area of the shaded rectangle of width <span class="math">20</span> and height <span class="math">0.05</span>, <span class="math">20 × 0.05 = 1</span>. The thin rectangle depicts an infinitesimally small interval corresponding to event <span class="math"><i class="fm-italics">E</i> = {<i class="fm-italics">x</i> ≤ <i class="fm-italics">X</i> &lt; <i class="fm-italics">x</i> + <i class="fm-italics">δx</i>}</span>. If we draw a random sample <i class="timesitalic">x</i> from this distribution, the probability that the value of the sample is between, say, <span class="math">4</span> and <span class="math">4 + <i class="fm-italics">δx</i></span>, with <span class="math"><i class="fm-italics">δx</i> → 0</span>, is <span class="math"><i class="fm-italics">p</i>(4) = 0.05</span>. The probability that the value of the sample is between, say, <span class="math">15</span> and <span class="math">15 + <i class="fm-italics">δx</i></span>, with <span class="math"><i class="fm-italics">δx</i> → 0</span>, is <span class="math"><i class="fm-italics">p</i>(15) = 0</span>.</p>
</div>
<p class="fm-code-listing-caption" id="lst-ufm-dist-logprob">Listing 5.1 Log probability of a univariate uniform random distribution</p>
<pre class="programlisting">from torch.distributions import Uniform                      <span class="fm-combinumeral">①</span>

a = torch.tensor([1.0], dtype=torch.float)                   <span class="fm-combinumeral">②</span>
b = torch.tensor([5.0], dtype=torch.float) 

ufm_dist = Uniform(a, b)                                     <span class="fm-combinumeral">③</span>

X = torch.tensor([2.0], dtype=torch.float)                   <span class="fm-combinumeral">④</span>

def raw_eval(X, a, b):
    return torch.log(1 / (b - a))

log_prob = ufm_dist.log_prob(X)                              <span class="fm-combinumeral">⑤</span>

raw_eval_log_prob = raw_eval(X, a, b)                        <span class="fm-combinumeral">⑥</span>

assert torch.isclose(log_prob, raw_eval_log_prob, atol=1e-4) <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports a PyTorch uniform distribution</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sets the distribution parameters</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates a uniform distribution object</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates a single-point test dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Evaluates the probability using PyTorch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Evaluates the probability using the formula</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Asserts that the probabilities match</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the uniform distribution, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/E2Jr">http://mng.bz/E2Jr</a>.</p>
<p class="fm-head2" id="expected-value-of-a-uniform-distribution">Expected value of a uniform distribution</p>
<p class="body"><a id="marker-171"/>We do this for the univariate case, although the computations can be easily extended to the multivariate case. Substituting the probability density function from equation <a class="url" href="#eq-uniform-random-univar">5.17</a> into the expression for the expected value for a continuous variable, equation <a class="url" href="#eq-continuous-expected-val">5.10</a>,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\mathbb{E}_{uniform}\left(X\right) &amp;=
\int\displaylimits_{-\infty}^{\infty} x \; p\left(x\right)  \; dx =
\int\displaylimits_{a}^{b} x  \left(\frac{1}{b-a}\right)   \; dx
\nonumber\\[-3pt]
&amp;= \frac{1}{\left(b-a\right)} \int\displaylimits_{a}^{b} x  \; dx
=  \frac{1}{\left(b-a\right)} \lceil  \frac{x^{2}}{2} \rceil ^{b}_{a} =
\frac{\left(b^{2} - a^{2}\right)}{2\left(b - a\right)} \nonumber
\\[-3pt]
&amp;=  \frac{\left(a+b\right)}{2}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="184" src="../../OEBPS/Images/eq_05-18.png" width="438"/></p>
</div>
<p class="fm-equation-caption">Equation 5.18 <span class="calibre" id="eq-continuous-uniform-expected-val"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The limits of integration changed because <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> is zero outside the interval <span class="math">[<i class="fm-italics">a</i>, <i class="fm-italics">b</i>]</span>.</p>
<p class="body">Overall, equation <a class="url" href="#eq-continuous-uniform-expected-val">5.18</a> agrees with our intuition. The expected value is right in the middle of the uniform interval, as shown in figure <a class="url" href="#fig-univar-uniform-distr-with-expectedvalue">5.5</a>.</p>
<p class="fm-head2" id="variance-of-a-uniform-distribution">Variance of a uniform distribution</p>
<p class="body">If we look at figure <a class="url" href="#fig-univar-uniform-distr-with-expectedvalue">5.5</a>, it is intuitively obvious that the packing density of the samples is related to the width of the rectangle. The smaller the width, the tighter the packing and the smaller the variance, and vice versa. Let’s see if the math supports that intuition:</p><!--<p class="Body"><span class="times">$$\begin{aligned} var_{uniform}\left(x\right)
&amp;=  \int\displaylimits_{x=-\infty}^{\infty} \left(x - \mu\right)^{2} p\left(x\right) dx =  \nonumber\\
&amp;= \int\displaylimits_{x=-\infty}^{\infty} \left(x -
\frac{a+b}{2}\right)^{2} \frac{1}{\left(b-a\right)} dx =  \nonumber\\
&amp;= \frac{\left(b-a\right)^{2}}{12}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="189" src="../../OEBPS/Images/eq_05-19.png" width="356"/></p>
</div>
<p class="fm-equation-caption">Equation 5.19 <span class="calibre" id="eq-uniform-var"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="457" id="fig-univar-uniform-distr-with-expectedvalue" src="../../OEBPS/Images/CH05_F05_Chaudhury.png" width="600"/></p>
<p class="figurecaption">Figure 5.5 Univariate (single-variable) uniform random probability density function. The solid line in the middle indicates the expected value. Interactive visualizations (where ou can change the parameters and observe how the graph changes as a result) can be found at <a class="url" href="http://mng.bz/E2Jr">http://mng.bz/E2Jr</a>.</p>
</div>
<p class="body"><a id="marker-172"/>Figure <a class="url" href="#fig-univar-uniform-distr-with-expectedvalue">5.5</a> shows that the variance in equation <a class="url" href="#eq-uniform-var">5.19</a> is proportional to the square of the width of the rectangle: that is, <span class="math">(<i class="fm-italics">b</i> − <i class="fm-italics">a</i>)<sup class="fm-superscript">2</sup></span>.</p>
<p class="body">Here is the PyTorch code for the mean and variance of a uniform random distribution.</p>
<p class="fm-code-listing-caption" id="lst-ufm-dist-meanvar">Listing 5.2 Mean and variance of a uniform random distribution</p>
<pre class="programlisting">num_samples = 100000                              <span class="fm-combinumeral">①</span>

    <span class="fm-combinumeral">②</span>
samples  = ufm_dist.sample([num_samples])         <span class="fm-combinumeral">③</span>


sample_mean = samples.mean()                      <span class="fm-combinumeral">④</span>
dist_mean = ufm_dist.mean                         <span class="fm-combinumeral">⑤</span>
assert torch.isclose(sample_mean, dist_mean, atol=0.2)

sample_var = ufm_dist.sample([num_samples]).var() <span class="fm-combinumeral">⑥</span>
dist_var = ufm_dist.variance                      <span class="fm-combinumeral">⑦</span>
assert torch.isclose(sample_var, dist_var, atol=0.2)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Number of sample points</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math">100000 × 1</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Obtains samples from ufm_dist instantiated in listing <a class="url" href="#lst-ufm-dist-logprob">5.1</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Sample mean</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Mean via PyTorch function</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Sample variance</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Variance via PyTorch function</p>
<p class="fm-head2" id="multivariate-uniform-distribution">Multivariate uniform distribution</p>
<p class="body">Uniform distributions also can be multivariate. In that case, the random variable is a vector, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> not a single value, but a sequence of values). Its domain is a multidimensional volume instead of the <i class="timesitalic">X</i>-axis, and the graph has more than two dimensions. For example, this is a two-variable uniform random distribution:</p><!--<p class="Body"><span class="times">$$p\left(x, y\right)  =
  \begin{cases}
   \frac{1}{\left(b_{1} -a_{1}\right)\left(b_{2} -a_{2}\right)} &amp;
\text{if } \left(x, y\right) \in  \left[a_{1}, b_{1} \right] \times
\left[a_{2}, b_{2} \right]   \\
   0   &amp; \text{otherwise}
  \end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="72" src="../../OEBPS/Images/eq_05-20.png" width="420"/></p>
</div>
<p class="fm-equation-caption">Equation 5.20 <span class="calibre" id="eq-uniform-random-bivar"/></p>
<p class="body"><a id="marker-173"/>Here, <span class="math">(<i class="fm-italics">x</i>, <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span>) <span class="cambria">∈</span> [<i class="fm-italics">a</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">b</i><sub class="fm-subscript">1</sub>] × [<i class="fm-italics">a</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">b</i><sub class="fm-subscript">2</sub>]</span> indicates a rectangular domain on the two-dimensional <i class="timesitalic">XY</i> plane where <i class="timesitalic">x</i> lies between <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">1</sub></span> and <span class="math"><i class="fm-italics">b</i><sub class="fm-subscript">1</sub></span> and <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> lies between <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">2</sub></span> and <span class="math"><i class="fm-italics">b</i><sub class="fm-subscript">2</sub></span>. Equation <a class="url" href="#eq-uniform-random-bivar">5.20</a> is shown graphically in figure <a class="url" href="#fig-bivar-uniform-distr">5.6</a>. In the general multidimensional case,:</p><!--<p class="Body"><span class="times">$$p\left(\vec{x}\right)  =
  \begin{cases}
   \frac{1}{V} &amp; \text{if } \vec{x} \in  D   \\
   0   &amp; \text{otherwise}
  \end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_05-21.png" width="188"/></p>
</div>
<p class="fm-equation-caption">Equation 5.21 <span class="calibre" id="eq-uniform-random-multivar"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="433" id="fig-bivar-uniform-distr" src="../../OEBPS/Images/CH05_F06_Chaudhury.png" width="600"/></p>
<p class="figurecaption">Figure 5.6 Bivariate uniform random probability density The probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>, <i class="fm-italics">y</i>)</span> is constant, <span class="math">0.0025</span>, in the domain <span class="math">(<i class="fm-italics">x</i>, <i class="fm-italics">y</i>) <span class="cambria">∈</span> [−10,10] × [−10,10]</span> and zero everywhere outside the interval. The volume of the box of width <span class="math">20 × 20</span> and height <span class="math">0.0025</span>, <span class="math">20 * 20 * 0.0025 = 1</span>.</p>
</div>
<p class="body">Here, <i class="timesitalic">V</i> is the volume of the hyperdimensional box with base <i class="timesitalic">D</i>. Equation <a class="url" href="#eq-uniform-random-multivar">5.21</a> means <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is constant for <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> in the domain <i class="timesitalic">D</i> and zero for other values of <i class="timesitalic">x</i>. When nonzero, it has a constant value, the inverse of the volume <i class="timesitalic">V</i>: this makes the total volume under the density function <span class="math">1</span>.</p>
<h3 class="fm-head1" id="sec-gaussian-distr">5.9.2 Gaussian (normal) distribution</h3>
<p class="body">This is probably the most famous distribution in the world. Let’s consider, one more time, the weights of adult residents of Statsville. If Statsville is anything like a real city, the likeliest weight is around <span class="math">75</span> kg: the largest percentage of the population will weigh this much. Weights near this value (say <span class="math">70</span> or <span class="math">80</span> kg) will also be quite likely, although slightly less likely than <span class="math">75</span> kg. Weights further away from <span class="math">75</span> kg are still less likely, and so on. The further we go from <span class="math">75</span> kg, the lower the percentage of the population with that weight. <i class="fm-italics">Outlier</i> values like <span class="math">40</span> and <span class="math">110</span> kg are unlikely. Informally speaking, a Gaussian probability density function looks like a <i class="fm-italics">bell-shaped curve</i>. The central value has the highest probability. The probability falls gradually as we move away from the center. In theory, however, it never disappears completely (the function <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> never becomes equal to <span class="math">0</span>), although it becomes almost zero for all practical purposes. This behavior is described in mathematics as <i class="fm-italics">asymptotically approaching zero</i>. Figure <a class="url" href="#fig-univar-gaussian-distr">5.7</a> shows a Gaussian probability density function. Formally,<a id="marker-174"/></p><!--<p class="Body"><span class="times">$$p\left(x\right) =   \frac{1}{{\sqrt
{2\pi } \sigma}}e^{{\frac{ - \left( {x - \mu } \right)^2 } {2\sigma ^2
}}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="56" src="../../OEBPS/Images/eq_05-22.png" width="176"/></p>
</div>
<p class="fm-equation-caption">Equation 5.22 <span class="calibre" id="eq-univar-normal"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="457" id="fig-univar-gaussian-distr" src="../../OEBPS/Images/CH05_F07_Chaudhury.png" width="607"/></p>
<p class="figurecaption">Figure 5.7 Univariate Gaussian random probability density function, <span class="math"><i class="fm-italics">μ</i> = 0</span> and <span class="math"><i class="fm-italics">σ</i> = 4</span>. The bell-shaped curve is highest at the center and decreases more and more as we move away from the center, approaching zero asymptotically. The value <span class="math"><i class="fm-italics">x</i> = 0</span> has the highest probability, corresponding to the center of the probability density function. Note that the curve is symmetric. Thus, for instance, the probability of a random sample being in the vicinity of <span class="math">−5</span> is the same as that of <span class="math">5</span> (<span class="math">0.04</span>): that is, <span class="math"><i class="fm-italics">p</i>(−5) = <i class="fm-italics">p</i>(5) = 0.04</span>. An interactive visualization (where you can change the parameters and observe how the graph changes as a result) can be found at <a class="url" href="http://mng.bz/NYJX">http://mng.bz/NYJX</a>.</p>
</div>
<p class="body">Here, <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i> are parameters; <i class="timesitalic">μ</i> corresponds to the center (for example, in figure <a class="url" href="#fig-univar-gaussian-distr">5.7</a>, <span class="math"><i class="fm-italics">μ</i> = 0</span>). The parameter <i class="timesitalic">σ</i> controls the width of the bell. A larger <i class="timesitalic">σ</i> implies that <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> falls more slowly as we move away from the center.</p>
<p class="body">The Gaussian (normal) probability density function is so popular that we have a special symbol for it: <span class="math"><span class="cambria">𝒩</span>(<i class="fm-italics">x</i>, <i class="fm-italics">μ</i>, <i class="fm-italics">σ</i><sup class="fm-superscript">2</sup>)</span>. It can be proved (but doing so is exceedingly tedious, so we will skip the proof here) that</p><!--<p class="Body"><span class="times">$$\int\displaylimits_{x
= -\infty}^{\infty} \mathcal{N}(x; \, \mu,\,\sigma^{2}) dx =
\int\displaylimits_{x = -\infty}^{\infty}
\frac{1}{{\sqrt {2\pi } \sigma}}e^{{\frac{ - \left( {x - \mu } \right)^2
} {2\sigma ^2 }}} dx = 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="76" src="../../OEBPS/Images/eq_05-22-a.png" width="380"/></p>
</div>
<p class="body">This establishes that <span class="math"><span class="cambria">𝒩</span>(<i class="fm-italics">x</i>;<i class="fm-italics">μ</i>, <i class="fm-italics">σ</i><sup class="fm-superscript">2</sup>)</span> is a true probability (satisfying the sum rule in equation <a class="url" href="#eq-continuous-vec-prob-sum">5.7</a>).</p>
<p class="fm-code-listing-caption" id="lst-univar-normal-dist-logprob">Listing 5.3 Log probability of a univariate normal distribution</p>
<pre class="programlisting">from torch.distributions import Normal        <span class="fm-combinumeral">①</span>

mu = torch.tensor([0.0], dtype=torch.float)   <span class="fm-combinumeral">②</span>
sigma = torch.tensor([5.0], dtype=torch.float)

uvn_dist = Normal(mu, sigma)                  <span class="fm-combinumeral">③</span>

X = torch.tensor([0.0], dtype=torch.float)    <span class="fm-combinumeral">④</span>

def raw_eval(X, mu, sigma):
    K = 1 / (math.sqrt(2 * math.pi) * sigma)
    E = math.exp( -1 * (X - mu) ** 2 * (1 / (2 * sigma ** 2)))
    return math.log(K * E)

log_prob = uvn_dist.log_prob(X)               <span class="fm-combinumeral">⑤</span>
raw_eval_log_prob = raw_eval(X, mu, sigma)    <span class="fm-combinumeral">⑥</span>
assert log_prob == raw_eval_log_prob          <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports a PyTorch univariate normal distribution</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sets the distribution params</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates a univariate normal distribution object</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates a single-point test dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Evaluates the probability using PyTorch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Evaluates the probability using the formula</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Asserts that the probabilities match</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for this normal distribution, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/NYJX">http://mng.bz/NYJX</a>.<a id="marker-175"/></p>
<p class="fm-head2" id="multivariate-gaussian">Multivariate Gaussian</p>
<p class="body">A Gaussian distribution can also be multivariate. Then the random variable <i class="timesitalic">x</i> is a vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, as usual. The parameter <i class="timesitalic">μ</i> also becomes a vector <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>, and the parameter <i class="timesitalic">σ</i> becomes a matrix <span class="timesbold">Σ</span>. As in the univariate case, these parameters are related to the expected value and variance. The Gaussian multivariate probability distribution function is</p><!--<p class="Body"><span class="times">$$p\left(\vec{x}\right) =
\mathcal{N}\left(\vec{x}; \, \vec{\mu}, \boldsymbol{\Sigma}\right) =
\frac{1}{\left(2\pi \det \boldsymbol{\Sigma} \right)^{\frac{1}{2}}} e^{-\frac{1}{2}\left(\vec{x} - \vec{\mu}\right)^{T}
\boldsymbol{\Sigma}^{-1} \left(\vec{x} - \vec{\mu}\right)}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="52" src="../../OEBPS/Images/eq_05-23.png" width="395"/></p>
</div>
<p class="fm-equation-caption">Equation 5.23 <span class="calibre" id="eq-multivar-normal"/></p>
<p class="body">Equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a> describes the probability density function for the random vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> to lie within the infinitesimally small volume with dimensions <span class="math"><i class="fm-italics">δ</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> around the point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. (Imagine a tiny box (cuboid) whose sides are successive elements of <span class="math"><i class="fm-italics">δ</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span>, with the top-left corner of the box at <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>.) The vector <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> and the matrix <span class="timesbold">Σ</span> are parameters. As in the univariate case, <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> corresponds to the most likely value of the random vector. Figure <a class="url" href="#fig-multivar-gaussian-distr">5.8</a> shows the Gaussian normal) distribution with two variables in three dimensions. The shape of the base of the bell is controlled by the parameter <span class="timesbold">Σ</span>.</p>
<p class="fm-code-listing-caption" id="lst-multivar-normal-dist-logprob">Listing 5.4 Log probability of a multivariate normal distribution</p>
<pre class="programlisting">from torch.distributions import MultivariateNormal <span class="fm-combinumeral">①</span>

mu = torch.tensor([0.0, 0.0], dtype=torch.float)   <span class="fm-combinumeral">②</span>
C = torch.tensor([[5.0, 0.0], [0.0, 5.0]], dtype=torch.float)

mvn_dist = MultivariateNormal(mu, C)               <span class="fm-combinumeral">③</span>

X = torch.tensor([0.0, 0.0], dtype=torch.float)    <span class="fm-combinumeral">④</span>

def raw_eval(X, mu, C):
    K = (1 / (2 * math.pi * math.sqrt(C.det())))
    X_minus_mu = (X - mu).reshape(-1, 1)
    E1 = torch.matmul(X_minus_mu.T, C.inverse())
    E = math.exp(-1 / 2. * torch.matmul(E1, X_minus_mu))
    return math.log(K * E)

log_prob = mvn_dist.log_prob(X)                    <span class="fm-combinumeral">⑤</span>
raw_eval_log_prob = raw_eval(X, mu, C)             <span class="fm-combinumeral">⑥</span>
assert log_prob == raw_eval_log_prob               <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports a PyTorch multivariate normal distribution</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sets the distribution params</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates a multivariate normal distribution object</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates a single point test dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Evaluates the probability using PyTorch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Evaluates the probability using the formula</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Asserts that the probabilities match</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="717" id="fig-multivar-gaussian-distr" src="../../OEBPS/Images/CH05_F08_Chaudhury.jpg" width="944"/></p>
<p class="figurecaption">Figure 5.8 Bivariate Gaussian random probability density function. It is a bell-shaped surface: highest at the center and decreasing as we move away from the center, approaching zero asymptotically. <span class="math"><i class="fm-italics">x</i> = 0</span>, <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = 0</span> has the highest probability, corresponding to the center of the probability density function. The bell has a circular base, and the <span class="timesbold">Σ</span> matrix is a scalar multiple of the identity matrix <span class="math">𝕀</span>. An interactive visualization (where you can change the parameters and observe how the graph changes as a result) can be found at <a class="url" href="http://mng.bz/NYJX">http://mng.bz/NYJX</a>.</p>
</div>
<p class="fm-head2" id="expected-value-of-a-gaussian-distribution">Expected value of a Gaussian distribution</p>
<p class="body"><a id="marker-176"/>Substituting the probability density function from equation <a class="url" href="../Text/05.xhtml#eq-univar-normal">5.22</a> into the expression for the expected value of a continuous variable, equation <a class="url" href="#eq-continuous-expected-val">5.10</a>, we get</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\mathbb{E}_{gaussian}\left(X\right) &amp;=
\int\displaylimits_{-\infty}^{\infty} x \; \frac{1}{{\sqrt {2\pi }
\sigma}}e^{{\frac{ - \left( {x - \mu } \right)^2 } {2\sigma ^2 }}} dx\\
&amp;=  \frac{1}{\sqrt{\pi}} \int\displaylimits_{-\infty}^{\infty}
\frac{\left(x-\mu\right)}{\sqrt {2}\sigma}e^{{\frac{ - \left( {x - \mu }
\right)^2 } {2\sigma ^2 }}} dx + \mu
\int\displaylimits_{-\infty}^{\infty}  \frac{1}{{\sqrt {2\pi }
\sigma}}e^{{\frac{ - \left( {x - \mu } \right)^2 } {2\sigma ^2 }}} dx\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="139" src="../../OEBPS/Images/eq_05-23-a.png" width="488"/></p>
</div>
<p class="body">Substituting <!--<span class="times"><i class="fm-italics">y</em> = –(<i class="fm-italics">x–μ</em>)/√2<i class="fm-italics">σ</em></span>--><span class="infigure"><img alt="" class="calibre5" height="30" src="../../OEBPS/Images/eq_05-23-b.png" width="77"/></span></p><!--<p class="Body"><span class="times">$$\mathbb{E}_{gaussian}\left(X\right)
=  \frac{\sqrt{2}\sigma}{\sqrt{\pi}}  \int\displaylimits_{-\infty}^{\infty}  e^{-y^{2}}dy + \mu \int\displaylimits_{-\infty}^{\infty} p\left(x\right) \; dx$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="68" src="../../OEBPS/Images/eq_05-23-c.png" width="363"/></p>
</div>
<p class="body">Substituting <span class="math"><i class="fm-italics">u</i> = <i class="timesitalic">y</i><sup class="fm-superscript">2</sup></span> and using equation <a class="url" href="#eq-continuous-prob-sum">5.6</a></p><!--<p class="Body"><span class="times">$$\mathbb{E}_{gaussian}\left(X\right) =
\frac{\sqrt{2}\sigma}{2\sqrt{\pi}}
\int\displaylimits_{\infty}^{\infty}e^{-u}du  + \mu$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_05-23-d.png" width="259"/></p>
</div>
<p class="body">Note that the limits of the integral in the first term are identical. This is because <span class="math"><i class="fm-italics">u</i> = <i class="timesitalic">y</i><sup class="fm-superscript">2</sup> →∞</span> whether <span class="math"><i class="timesitalic">y</i> → ∞</span> or <span class="math"><i class="timesitalic">y</i> → −∞</span>. But an integral with the same lower and upper limits is zero. Thus the first term is zero. Hence,</p>
<p class="fm-equation"><span class="math"><span class="segoe">𝔼</span><i class="fm-italics"><sub class="fm-subscript">gaussian</sub></i>(<i class="fm-italics">X</i>) = <i class="fm-italics">μ</i></span></p>
<p class="fm-equation-caption">Equation 5.24 <span class="calibre" id="eq-gaussian-univar-expected-val"/></p>
<p class="body"><a id="marker-177"/>Intuitively, this makes perfect sense. The probability density <!--<span class="times">$p\left(x\right) = \frac{1}{{\sqrt {2\pi }
\sigma}}e^{{\frac{ - \left( {x - \mu } \right)^2 } {2\sigma ^2
}}}$</span>--><span class="infigure"><img alt="" class="calibre5" height="39" src="../../OEBPS/Images/eq_05-24-a2.png" width="147"/></span> peaks (maximizes) at <span class="math"><i class="fm-italics">x</i> = <i class="fm-italics">μ</i></span>. At this <i class="timesitalic">x</i>, the exponent becomes zero, which makes the term <!--<span class="times">$e^{\frac{ - \left( {x - \mu }
\right)^2 } {2\sigma ^2 }}$</span>--><span class="infigure"><img alt="" class="calibre5" height="27" src="../../OEBPS/Images/eq_05-24-b2.png" width="51"/></span> attain its maximum possible value of <span class="math">1</span>. This is right in the middle of the bell, as shown in figure <a class="url" href="#fig-univar-normal-distr-with-expectedvalue">5.10</a>. And, of course, the expected value coincides with the middle value if the density is symmetric and peaks in the middle. Analogously, in the multivariate case, the Gaussian multidimensional random variable <i class="timesitalic">X</i> that takes vector values <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> in the <i class="timesitalic">d</i>-dimensional domain <span class="math">ℝ<i class="fm-italics"><sup class="fm-superscript">d</sup></i></span> (that is, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <span class="cambria">∈</span> ℝ<i class="fm-italics"><sup class="fm-superscript">d</sup></i></span>) has an expected value</p>
<p class="fm-equation"><span class="math"><span class="segoe">𝔼</span><i class="fm-italics"><sub class="fm-subscript">gaussian</sub></i>(<i class="fm-italics">X</i>) = <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span></span></p>
<p class="fm-equation-caption">Equation 5.25 <span class="calibre" id="eq-gaussian-multivar-expected-val"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="455" id="fig-univar-normal-distr-with-expectedvalue" src="../../OEBPS/Images/CH05_F09_Chaudhury.png" width="607"/></p>
<p class="figurecaption">Figure 5.9 Univariate (single-variable) normal (Gaussian) random probability density function, <span class="math"><i class="fm-italics">μ</i> = 0</span> and <span class="math"><i class="fm-italics">σ</i> = 4</span>. The solid line in the middle indicates the expected value.</p>
</div>
<p class="fm-head2" id="sec-gaussian-variance">Variance of a Gaussian distribution</p>
<p class="body">The variance of the Gaussian distribution is obtained by substituting equation <a class="url" href="../Text/05.xhtml#eq-univar-normal">5.22</a> in the integral form of equation <a class="url" href="../Text/05.xhtml#eq-variance">5.13</a>. The mathematical derivation is shown in the book’s appendix; here we only state the result.</p>
<p class="body">The variance of a Gaussian distribution with probability density function <!--<span class="times">$p\left(x\right) =   \frac{1}{{\sqrt
{2\pi } \sigma}}e^{{\frac{ - \left( {x - \mu } \right)^2 } {2\sigma ^2
}}}$</span>--><span class="infigure"><img alt="" class="calibre5" height="39" src="../../OEBPS/Images/eq_05-24-c.png" width="150"/></span> is <span class="math"><i class="fm-italics">σ</i><sup class="fm-superscript">2</sup></span>, and the standard deviation is the square root of that (<i class="timesitalic">σ</i>). This makes intuitive sense. <i class="timesitalic">σ</i> appears in the denominator of a negative exponent in the expression for the probability density function <!--<span class="times">$p\left(x\right) =   \frac{1}{{\sqrt
{2\pi } \sigma}}e^{{\frac{ - \left( {x - \mu } \right)^2 } {2\sigma ^2
}}}$</span>--><span class="infigure"><img alt="" class="calibre5" height="38" src="../../OEBPS/Images/eq_05-24-d.png" width="146"/></span>. As such, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> is an increasing function of <i class="timesitalic">σ</i>: that is, for a given <i class="timesitalic">x</i> and <i class="timesitalic">μ</i>, a larger <i class="timesitalic">σ</i> implies a larger <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span>. In other words, a larger <i class="timesitalic">σ</i> implies that the probability decays more slowly as we move away from the center: a fatter bell curve, a bigger spread, and hence a larger variance. Figure <a class="url" href="../Text/05.xhtml#fig-multi-univar-gauss">5.10</a> depicts this.<a id="marker-178"/></p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="475" id="fig-multi-univar-gauss-mus" src="../../OEBPS/Images/CH05_F10a_Chaudhury.png" width="607"/></p>
<p class="figurecaption">(a) Different <i class="timesitalic">μ</i>s but the same <i class="timesitalic">σ</i>s.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="480" id="fig-multi-univar-gauss-musb" src="../../OEBPS/Images/CH05_F10b_Chaudhury.png" width="607"/></p>
<p class="figurecaption">(b) The same <i class="timesitalic">μ</i>s but different <i class="timesitalic">σ</i>s.</p>
</div>
<p class="fm-table-caption" id="fig-multi-univar-gauss">Figure 5.10 Gaussian densities with varying <i class="timesitalic">μ</i>s and <i class="timesitalic">σ</i>s. Changing <i class="timesitalic">μ</i> shifts the center of the curve. A larger <i class="timesitalic">σ</i> (variance) implies a fatter bell <span class="math">⇒</span> more spread. Note that fatter curves are smaller in height as the total area under the curve must be <span class="math">1</span>.</p>
<p class="fm-code-listing-caption" id="lst-univar-normal-dist-meanvar">Listing 5.5 Mean and variance of a univariate Gaussian</p>
<pre class="programlisting">num_samples = 100000                              <span class="fm-combinumeral">①</span>

    <span class="fm-combinumeral">②</span>
samples  = uvn_dist.sample([num_samples])         <span class="fm-combinumeral">③</span>


sample_mean = samples.mean()                      <span class="fm-combinumeral">④</span>
dist_mean = uvn_dist.mean                         <span class="fm-combinumeral">⑤</span>
assert torch.isclose(sample_mean, dist_mean, atol=0.1)

sample_var = uvn_dist.sample([num_samples]).var() <span class="fm-combinumeral">⑥</span>
dist_var = uvn_dist.variance                      <span class="fm-combinumeral">⑦</span>
assert torch.isclose(sample_var, dist_var, atol=0.1)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Number of sample points</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math">100000 × 1</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Obtains samples from uvn_dist<br class="calibre20"/>
  instantiated in listing <a class="url" href="#lst-univar-normal-dist-logprob">5.3</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Sample mean</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Mean via PyTorch function</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Sample variance</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Variance via PyTorch function</p>
<p class="fm-head2" id="sec-gaussian-covar">Covariance of a multivariate Gaussian distribution and geometry of the bell surface</p>
<p class="body">Comparing equation <a class="url" href="../Text/05.xhtml#eq-univar-normal">5.22</a> for a univariate Gaussian probability density with equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a> for a multivariate Gaussian probability density, we intuitively feel that the matrix <span class="timesbold">Σ</span> is the multivariate peer of the univariate variance <span class="math"><i class="fm-italics">σ</i><sup class="fm-superscript">2</sup></span>. Indeed it is. Formally, for a multivariate Gaussian random variable with a probability distribution given in equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a>, the covariance matrix is given by the equation</p>
<p class="fm-equation"><span class="math">ℂ<i class="fm-italics"><sub class="fm-subscript">gaussian</sub></i>(<i class="fm-italics">X</i>) = <b class="fm-bold">Σ</b></span></p>
<p class="fm-equation-caption">Equation 5.26</p>
<p class="body">As shown in table <a class="url" href="#fig-gaussian-multivar-pointclouds">5.11</a>, <span class="math">Σ</span> regulates the shape of the base of the bell-shaped probability density function.</p>
<p class="body">It is easy to see that the exponent in equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a> is a quadratic form (introduced in section <a class="url" href="../Text/04.xhtml#sec-quadratic-form">4.2</a>). As such, it defines a hyper-ellipse, as shown in figure <a class="url" href="#fig-gaussian-multivar-pointclouds">5.11</a> and section <a class="url" href="02.xhtml#sec-hyper-ellipse">2.17</a>. All the properties of quadratic forms and hyper-ellipses apply here.<a id="marker-179"/></p>
<p class="fm-code-listing-caption" id="lst-multivar-normal-dist-meanvar">Listing 5.6 Mean and variance of a multivariate normal distribution</p>
<pre class="programlisting">num_samples = 100000                              <span class="fm-combinumeral">①</span>

    <span class="fm-combinumeral">②</span>
samples  = mvn_dist.sample([num_samples])         <span class="fm-combinumeral">③</span>


sample_mean = samples.mean()                      <span class="fm-combinumeral">④</span>
dist_mean = mvn_dist.mean                         <span class="fm-combinumeral">⑤</span>
assert torch.allclose(sample_mean, dist_mean, atol=1e-1)

sample_var = mvn_dist.sample([num_samples]).var() <span class="fm-combinumeral">⑥</span>
dist_var = mvn_dist.variance                      <span class="fm-combinumeral">⑦</span>
assert torch.allclose(sample_var, dist_var, atol=1e-1)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Number of sample points</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math">100000 × 1</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Obtains samples from uvn_dist<br class="calibre20"/>
  instantiated in listing <a class="url" href="#lst-multivar-normal-dist-logprob">5.4</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Sample mean</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Mean via PyTorch function</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Sample variance</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Variance via PyTorch function</p>
<p class="body">Let’s look at the geometric properties of the Gaussian covariance matrix <span class="math">Σ</span>. Consider a <span class="math">2</span>D version of equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a>. We rewrite <!--<span class="times">$\vec{x} =
\begin{bmatrix}
x\\y
\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="38" src="../../OEBPS/Images/eq_05-26-a2.png" width="51"/></span> and <!--<span class="times">$\vec{\mu}
= \begin{bmatrix}
\mu_{x}\\\mu_{y}
\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="38" src="../../OEBPS/Images/eq_05-26-b2.png" width="62"/></span>—<span class="math">2</span>D vectors both. Also <!--<span class="times">$\Sigma^{-1} =
\begin{bmatrix}
\sigma_{11} &amp; \sigma_{12}\\
\sigma_{21} &amp; \sigma_{22}
\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="33" src="../../OEBPS/Images/eq_05-26-c2.png" width="131"/></span>—a <span class="math">2 × 2</span> matrix. The probability density function from equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a> becomes</p><!--<p class="Body"><span class="times">$$p\left(x, y\right) =
\mathcal{N}\left(x, y; \, \vec{\mu}, \boldsymbol{\Sigma}\right) =
\frac{1}{\left(2\pi \det \boldsymbol{\Sigma} \right)^{\frac{1}{2}}} e^{-\frac{1}{2}\left(\sigma_{11} x^{2} + \left(\sigma_{11}  +
\sigma_{12} \right)x y +\sigma_{22} y^{2} \right)}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_05-27.png" width="489"/></p>
</div>
<p class="fm-equation-caption">Equation 5.27 <span class="calibre" id="eq-bivar-normal"/></p>
<p class="body">(Use what you learned in chapter <a class="url" href="../Text/03.xhtml#chapter-intro-vec-mat">3</a> to satisfy yourself that equation <a class="url" href="#eq-bivar-normal">5.27</a> is a <span class="math">2</span>D analog of equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a>.)</p>
<p class="body">If we plot the surface <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>, <i class="timesitalic">y</i>)</span> against <span class="math">(<i class="fm-italics">x</i>, <i class="timesitalic">y</i>)</span>, it looks like a bell in <span class="math">3</span>D space. The shape of the bell’s base, on the <span class="math">(<i class="fm-italics">x</i>, <i class="timesitalic">y</i>)</span> plane, is governed by the <span class="math">2 × 2</span> matrix <span class="timesbold">Σ</span>. In particular,</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">If <span class="timesbold">Σ</span> is a diagonal matrix with equal diagonal elements, the bell is symmetric in all directions, and its base is circular.</p>
</li>
<li class="fm-list-bullet">
<p class="list">If <span class="timesbold">Σ</span> is a diagonal matrix with unequal diagonal elements, the base of the bell is elliptical. The axes of the ellipse are aligned with the coordinate axes.</p>
</li>
<li class="fm-list-bullet">
<p class="list">For a general <span class="timesbold">Σ</span> matrix, the base of the bell is elliptical. The axes of the ellipse are not necessarily aligned with the coordinate axes.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The eigenvectors of <span class="timesbold">Σ</span> yield the axes of the elliptical base of the bell surface.</p>
</li>
</ul>
<p class="body">Now, if we sample the distribution from equation <a class="url" href="#eq-bivar-normal">5.27</a>, we get a set of points <span class="math">(<i class="fm-italics">x</i>, <i class="timesitalic">y</i>)</span> on the base plane of the surface shown in figure <a class="url" href="#fig-multivar-gaussian-distr">5.8</a>. The taller the <i class="timesitalic">z</i> coordinate (depicting <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>, <i class="timesitalic">y</i>)</span>) of the surface at a point <span class="math">(<i class="fm-italics">x</i>, <i class="timesitalic">y</i>)</span>, the greater its probability of being selected in the sampling. If we draw a large number of samples, the corresponding point cloud will look more or less like the base of the bell surface.</p>
<p class="body">Figure <a class="url" href="#fig-gaussian-multivar-pointclouds">5.11</a> shows various point clouds formed by sampling Gaussian distributions with different covariance matrices <span class="timesbold">Σ</span>. Compare it to figure <a class="url" href="../Text/05.xhtml#fig-multi-univar-gauss">5.10</a>.</p>
<p class="fm-head2" id="geometry-of-sampled-point-clouds-covariance-and-direction-of-maximum-or-minimum-spread">Geometry of sampled point clouds: Covariance and direction of maximum or minimum spread</p>
<p class="body"><a id="marker-180"/>We have seen that if a multivariate distribution has a covariance matrix <span class="math">ℂ</span>, its variance (spread) in any specific direction <i class="timesitalic">l̂</i> is <span class="math"><i class="fm-italics">l̂<sup class="fm-superscript">T</sup></i> ℂ<i class="fm-italics">l̂</i></span>. What is the direction of maximum spread?</p>
<p class="body">Asking this is the same as asking “What direction <i class="timesitalic">l̂</i> maximizes the quadratic form <span class="math"><i class="fm-italics">l̂<sup class="fm-superscript">T</sup></i> ℂ<i class="fm-italics">l̂</i></span>?” In section <a class="url" href="../Text/04.xhtml#sec-quadratic-form">4.2</a>, we saw that a quadratic form like this is maximized or minimized when the direction <i class="timesitalic">l̂</i> is aligned with the eigenvector corresponding to the maximum or minimum eigenvalue of the matrix <span class="math">ℂ</span>. Thus, <i class="fm-italics">the maximum spread of a distribution occurs along the eigenvector of the covariance matrix corresponding to its maximum eigenvalue</i>. This led to the PCA technique in section <a class="url" href="../Text/04.xhtml#sec-pca">4.4</a>.</p>
<p class="body">Next, we discuss the covariance of the Gaussian distribution and geometry of the point cloud formed by sampling a multivariate Gaussian a large number of times. You may want to take a look at figure <a class="url" href="#fig-gaussian-multivar-pointclouds">5.11</a>, which shows various point clouds formed by sampling Gaussian distributions with different covariance matrices <span class="timesbold">Σ</span>.</p>
<div class="figure3">
<p class="figure1"><img alt="(a)" class="calibre17" height="578" id="fig-gaussian-multivar-pointclouds-circ" src="../../OEBPS/Images/CH05_F11a_Chaudhury.png" width="607"/></p>
<p class="figurecaption">(a) <span class="infigure"><img alt="" class="calibre5" height="33" src="../../OEBPS/Images/fig_05-11_a.png" width="68"/></span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="(b)" class="calibre17" height="578" id="fig-gaussian-multivar-pointclouds-elliptic1" src="../../OEBPS/Images/CH05_F11b_Chaudhury.png" width="607"/></p>
<p class="figurecaption">(b) <span class="infigure"><img alt="" class="calibre5" height="33" src="../../OEBPS/Images/fig_05-11_b.png" width="106"/></span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="(c)" class="calibre17" height="578" id="fig-gaussian-multivar-pointclouds-elliptic2" src="../../OEBPS/Images/CH05_F11c_Chaudhury.png" width="607"/></p>
<p class="figurecaption">(c) <span class="infigure"><img alt="" class="calibre5" height="33" src="../../OEBPS/Images/fig_05-11_c.png" width="80"/></span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="(d)" class="calibre17" height="577" id="fig-gaussian-multivar-pointclouds-elliptic3" src="../../OEBPS/Images/CH05_F11d_Chaudhury.png" width="608"/></p>
<p class="figurecaption">(d) <span class="infigure"><img alt="" class="calibre5" height="33" src="../../OEBPS/Images/fig_05-11_d.png" width="127"/></span></p>
</div>
<p class="fm-table-caption" id="fig-gaussian-multivar-pointclouds">Figure 5.11 Point clouds formed by sampling multivariate Gaussians with the same <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> = [0,0]<i class="fm-italics"><sup class="fm-superscript">T</sup></i></span> but different <span class="math">Σ</span>s. These point clouds correspond to the bases of the bell curves for multivariate Gaussian probability densities. All the point clouds except (a) may be replaced by a univariate Gaussian after rotation to align the coordinate axes with the eigenvectors of <span class="math">Σ</span> (dimensionality reduction). See sections <a class="url" href="../Text/04.xhtml#sec-pca">4.4</a>, <a class="url" href="../Text/04.xhtml#sec-svd">4.5</a>, and <a class="url" href="../Text/04.xhtml#sec-lsa">4.6</a> for details. Interactive contour plots for the base of the bell curve can be found at <a class="url" href="http://mng.bz/NYJX">http://mng.bz/NYJX</a>.<a id="marker-181"/></p>
<p class="fm-head2" id="multivariate-gaussian-point-clouds-and-hyper-ellipses">Multivariate Gaussian point clouds and hyper-ellipses</p>
<p class="body">The numerator of the exponential term in equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a>, <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>−<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i><b class="fm-bold">Σ</b><sup class="fm-superscript">−1</sup>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>−<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>)</span>, is a quadratic form as we discussed in section <a class="url" href="../Text/04.xhtml#sec-quadratic-form">4.2</a>. It should also remind you of the hyper-ellipse we looked at in section <a class="url" href="02.xhtml#sec-hyper-ellipse">2.17</a>, equation <a class="url" href="02.xhtml#eq-hyper-ellipse">2.33</a>, and equation <a class="url" href="../Text/04.xhtml#eq-hyper-ellipse-again">4.1</a>.</p>
<p class="body">Now consider the plot of <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> against <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. This is a hypersurface in <span class="math"><i class="fm-italics">n</i> + 1</span>-dimensional space, where the random variable <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is <i class="timesitalic">n</i>-dimensional. For instance, if the random Gaussian variable <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is <span class="math">2</span>D, the <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>))</span> plot in <span class="math">3</span>D is as shown in figure <a class="url" href="#fig-multivar-gaussian-distr">5.8</a>. It is a bell-shaped surface. The hyper-ellipse corresponding to the quadratic form in the numerator of the probability density function in equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a> governs the shape and size of the base of this bell.</p>
<p class="body">If the matrix <span class="timesbold">Σ</span> is diagonal (with equal diagonal elements), the base is <i class="fm-italics">circular</i>—this is the special case shown in figure <a class="url" href="#fig-multivar-gaussian-distr">5.8</a>. Otherwise, the base of the bell is elliptic. The eigenvectors of the covariance matrix <span class="timesbold">Σ</span> correspond to the directions of the axes of the elliptical base. The eigenvalues correspond to the lengths of the axes.</p>
<h3 class="fm-head1" id="sec-binomial-distr">5.9.3 Binomial distribution</h3>
<p class="body">Suppose we have a database containing photos of people. Also, suppose we know that <span class="math">20%</span> of the photos contain a celebrity and the remaining <span class="math">80%</span> do not. If we randomly select three photos from this database, what is the probability that two of them contain a celebrity? This is the kind of problem the binomial distribution deals with.</p>
<p class="body">In a computer vision-centric machine learning setting, we would probably inspect the selected photos and try to predict whether they contained a celebrity. But for now, let’s restrict ourselves to the simpler task of blindly predicting the chances from aggregate statistics.</p>
<p class="body">If we select a single photo, the probability of it containing a celebrity is <span class="math"><i class="fm-italics">π</i> = 0.2</span>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> This has nothing to do with the natural number <i class="timesitalic">π</i> denoting the ratio of the circumference to the diameter of a circle. We are just reusing the symbol <i class="timesitalic">π</i> following popular convention.</p>
<p class="body">The probability of the photo not containing a celebrity is <span class="math">1 − <i class="fm-italics">π</i> = 0.8</span>. From that, we can compute the probability of, say, the first two sampled photos containing a celebrity but the last one containing a non-celebrity: that is, the event <span class="math">{<i class="fm-italics">S</i>, <i class="fm-italics">S</i>, <i class="fm-italics">F</i>}</span> (where S denotes success in finding a celebrity and F denotes failure in finding a celebrity). Using equation <a class="url" href="../Text/05.xhtml#eq-joint-prob-indep">5.4</a>, the probability of the event <span class="math">{<i class="fm-italics">S</i>, <i class="fm-italics">S</i>, <i class="fm-italics">F</i>}</span> is <span class="math"><i class="fm-italics">π</i> × <i class="fm-italics">π</i> × (1−<i class="fm-italics">π</i>) = 0.2 × 0.2 × 0.8</span>. However, many other combinations are also possible.<a id="marker-182"/></p>
<p class="body">All the possible combinations that can occur in three trials are shown in table <a class="url" href="#tab-binomial-distr">5.8</a>. In the table, event ids <span class="math">3</span>, <span class="math">5</span>, and <span class="math">6</span> correspond to two successes and one failure. They occur with probabilities <span class="math">0.8 × 0.2 × 0.2</span>, <span class="math">0.2 × 0.8 × 0.2</span>, and <span class="math">0.2 × 0.2 × 0.8</span>, respectively. If any one of them occurs, we have two celebrity photos in three trials. Thus, using equation <a class="url" href="../Text/05.xhtml#eq-discrete-prob-sum">5.3</a>, the overall probability of selecting two celebrity photos in three trials is the sum of these event probabilities: <span class="math">0.8 × 0.2 × 0.2 + 0.2 × 0.8 × 0.2 + 0.2 × 0.2 × 0.8 = 0.096</span>.</p>
<p class="fm-table-caption">Table 5.8 All possible combinations of three trials</p>
<table border="1" class="contenttable-1-table" id="tab-binomial-distr" width="100%">
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">Event Id</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Event</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Probability</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">{<i class="fm-italics">F</i>, <i class="fm-italics">F</i>, <i class="fm-italics">F</i>}</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">(1−<i class="fm-italics">π</i>) × (1−<i class="fm-italics">π</i>) × (1−<i class="fm-italics">π</i>) = 0.8 × 0.8 × 0.8</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">1</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">{<i class="fm-italics">F</i>, <i class="fm-italics">F</i>, <i class="fm-italics">S</i>}</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">(1−<i class="fm-italics">π</i>) × (1−<i class="fm-italics">π</i>) × <i class="fm-italics">π</i> = 0.8 × 0.8 × 0.2</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">2</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">{<i class="fm-italics">F</i>, <i class="fm-italics">S</i>, <i class="fm-italics">F</i>}</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">(1−<i class="fm-italics">π</i>) × <i class="fm-italics">π</i> × (1−<i class="fm-italics">π</i>) = 0.8 × 0.2 × 0.8</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">3</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">{<i class="fm-italics">F</i>, <i class="fm-italics">S</i>, <i class="fm-italics">S</i>}</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">(1−<i class="fm-italics">π</i>) × <i class="fm-italics">π</i> × <i class="fm-italics">π</i> = 0.8 × 0.2 × 0.2</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">4</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">{<i class="fm-italics">S</i>, <i class="fm-italics">F</i>, <i class="fm-italics">F</i>}</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">π</i> × (1−<i class="fm-italics">π</i>) × (1−<i class="fm-italics">π</i>) = 0.2 × 0.8 × 0.8</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">5</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">{<i class="fm-italics">S</i>, <i class="fm-italics">F</i>, <i class="fm-italics">S</i>}</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">π</i> × (1−<i class="fm-italics">π</i>) × <i class="fm-italics">π</i> = 0.2 × 0.8 × 0.2</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">6</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">{<i class="fm-italics">S</i>, <i class="fm-italics">S</i>, <i class="fm-italics">F</i>}</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">π</i> × <i class="fm-italics">π</i> × (1−<i class="fm-italics">π</i>) = 0.2 × 0.2 × 0.8</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">7</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">{<i class="fm-italics">S</i>, <i class="fm-italics">S</i>, <i class="fm-italics">S</i>}</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">π</i> × <i class="fm-italics">π</i> × <i class="fm-italics">π</i> = 0.2 × 0.2 × 0.2</span></p>
</td>
</tr>
</tbody>
</table>
<p class="body">In the general case, with more than three trials, it would be impossibly tedious to enumerate all the possible combinations of <i class="fm-italics">success</i> and <i class="fm-italics">failure</i> that can occur in a set of <i class="timesitalic">n</i> trials. Fortunately, we can derive a formula. But before doing that, let’s state the task of a binomial distribution in more general terms:</p>
<p class="fm-quote">Given a process that has a binary outcome (success or failure) in any given trial, and given that the probability of success in a trial is a known constant (say, <i class="timesitalic">π</i>), a binomial distribution deals with the probability of observing <i class="timesitalic">k</i> successes in <i class="timesitalic">n</i> trials of the process.</p>
<p class="body">Imagine events with <i class="timesitalic">n</i> successive items, where each individual item can be either <i class="timesitalic">S</i> or <i class="timesitalic">F</i>. Table <a class="url" href="#tab-binomial-distr">5.8</a> shows such events with <span class="math"><i class="fm-italics">n</i> = 3</span>. Each item has two possible values (<i class="timesitalic">S</i> or <i class="timesitalic">F</i>), and there are <i class="timesitalic">n</i> items. Hence, altogether there can be <span class="math">2 × 2 × ⋯2 = 2<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span> possible events.</p>
<p class="body">We are only interested in events with <i class="timesitalic">k</i> occurrences of <i class="timesitalic">S</i> (and therefore <span class="math">(<i class="fm-italics">n</i> − <i class="fm-italics">k</i>)</span> occurrences of <i class="timesitalic">F</i>). How many of the <i class="timesitalic">n</i> events are like that? Well, asking this is the same as asking how many ways we can choose <i class="timesitalic">k</i> slots from a total of <i class="timesitalic">n</i> possible slots. Another way to pose the same question is, “How many different orderings of <i class="timesitalic">n</i> items exist, where each item is either <i class="timesitalic">S</i> or <i class="timesitalic">F</i> and the total count of <i class="timesitalic">S</i> is <i class="timesitalic">k</i>?” The answer, from combination theory, is<a id="marker-183"/></p><!--<p class="Body"><span class="times">$$\binom{n}{k} =
\frac{n!}{k!\left(n-k\right)!}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="52" src="../../OEBPS/Images/eq_05-27-a.png" width="123"/></p>
</div>
<p class="body">Each of these events has a probability of <span class="math"><i class="fm-italics">π<sup class="fm-superscript">k</sup></i> × (1−<i class="fm-italics">π</i>)<sup class="fm-superscript"><i class="fm-italics1">n</i> − <i class="fm-italics1">k</i></sup></span>. Hence, the overall probability of <i class="timesitalic">k</i> successes in <i class="timesitalic">n</i> trials is <span class="infigure"><img alt="" class="calibre5" height="28" src="../../OEBPS/Images/eq_05-27-b-2.png" width="137"/></span><!--<span class="math"><i class="fm-italics">π<sup class="fm-superscript">k</sup></i> × (1−<i class="fm-italics">π</i>)<sup class="fm-superscript"><i class="fm-italics1">n</i> − <i class="fm-italics1">k</i></sup></span><span class="inFigure"><img alt="" src="imgs/equations/eq_05-27-b1.png" /></span><span class="times"><i class="fm-italics">π<sub class="FM-Subscript">k</sub></em> × (1−<i class="fm-italics">π</em>)<sup class="FM-Superscript"><i class="fm-italics">n</em> − <i class="fm-italics">k</em></sup></span>-->.</p>
<p class="body">Formally, if <i class="timesitalic">X</i> is a random variable denoting the number of successes in <i class="timesitalic">n</i> trials, with the probability of success in any single trial being some constant value <i class="timesitalic">π</i>,</p><!--<p class="Body"><span class="times">$$p\left(X = k\right) = \binom{n}{k} \pi^{k} \times
\left(1-\pi\right)^{n-k}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_05-28.png" width="235"/></p>
</div>
<p class="fm-equation-caption">Equation 5.28 <span class="calibre" id="eq-binom-distr"/></p>
<p class="body">What values can <i class="timesitalic">k</i> take? Of course, we cannot have more than <i class="timesitalic">n</i> successes in <i class="timesitalic">n</i> trials; therefore, the maximum possible value of <i class="timesitalic">k</i> is <i class="timesitalic">n</i>. All integer values between <span class="math">0</span> and <i class="timesitalic">n</i> are possible:</p><!--<p class="Body"><span class="times">$$\sum_{k=0}^{k=n} p\left(X = k\right) =
\sum_{k=0}^{k=n}  \binom{n}{k} \pi^{k} \times
\left(1-\pi\right)^{n-k}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_05-28-a.png" width="285"/></p>
</div>
<p class="body">The right-hand side is an expression for the generic term in the famous binomial expansion of <span class="math">(<i class="fm-italics">a</i> + <i class="fm-italics">b</i>)<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span> with <span class="math"><i class="fm-italics">a</i> = <i class="fm-italics">π</i></span> and <span class="math"><i class="fm-italics">b</i> = 1 − <i class="fm-italics">π</i></span>. Hence, we get</p><!--<p class="Body"><span class="times">$$\sum_{k=0}^{k=n} p\left(X = k\right) =
\sum_{k=0}^{k=n}  \binom{n}{k} \pi^{k} \times \left(1-\pi\right)^{n-k} =
\left(\pi + 1 - \pi\right)^{n} = 1^{n} = 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_05-29.png" width="451"/></p>
</div>
<p class="fm-equation-caption">Equation 5.29 <span class="calibre" id="eq-eq-binom-sum"/></p>
<p class="body">This agrees with intuition, since given <i class="timesitalic">n</i>, <i class="timesitalic">k</i> can only take values <span class="math">0</span>, <span class="math">1</span>, <span class="math">⋯</span>, <i class="timesitalic">n</i>; the sum of the probabilities on the left-hand side of equation <a class="url" href="#eq-eq-binom-sum">5.29</a> corresponds to a certain event with probability <span class="math">1</span>.</p>
<p class="body">Also, plugging <span class="math"><i class="fm-italics">n</i> = 3</span>, <span class="math"><i class="fm-italics">k</i> = 2</span>, and <span class="math"><i class="fm-italics">π</i> = 0.2</span> into equation <a class="url" href="#eq-binom-distr">5.28</a> yields <span class="math">3!/2!1! (0.2)<sup class="fm-superscript">2</sup>, (0.8)<sup class="fm-superscript">3-2</sup> = 0.096</span>: exactly what we get from explicit enumeration.</p>
<p class="fm-code-listing-caption" id="lst-binom-dist-logprob">Listing 5.7 Log probability of a binomial distribution</p>
<pre class="programlisting">from torch.distributions import Binomial                     <span class="fm-combinumeral">①</span>

num_trials = 3                                               <span class="fm-combinumeral">②</span>
p = torch.tensor([0.2], dtype=torch.float)

binom_dist = Binomial(num_trials, probs=p)                   <span class="fm-combinumeral">③</span>

X = torch.tensor([1], dtype=torch.float)                     <span class="fm-combinumeral">④</span>

def nCk(n, k):
    f = math.factorial
    return f(n) * 1. / (f(k) * f(n-k))

def raw_eval(X, n, p):
    result = nCk(n, X) * (p ** X) * (1 - p) ** (n - X)
    return torch.log(result)

log_prob = binom_dist.log_prob(X)                            <span class="fm-combinumeral">⑤</span>
raw_eval_log_prob = raw_eval(X, num_trials, p)               <span class="fm-combinumeral">⑥</span>
assert torch.isclose(log_prob, raw_eval_log_prob, atol=1e-4) <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports a PyTorch binomial distribution</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sets the distribution params</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates a binomial distribution object</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates a single point test dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Evaluates the probability using PyTorch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Evaluates the probability using formula</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Asserts that the probabilities match</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the binomial distribution, executable via Jupyter Notebook, can be found at <a href="http://mng.bz/DRJ0" style="text-decoration: none; color: #4080C0;">http://mng.bz/DRJ0</a>.</p>
<p class="fm-head2" id="expected-value-of-a-binomial-distribution">Expected value of a binomial distribution</p>
<p class="body"><a id="marker-184"/>We have seen that the binomial distribution deals with a random variable <i class="timesitalic">X</i> that depicts the number of successes in <i class="timesitalic">n</i> trials, where the probability of success in a given trial is a constant <i class="timesitalic">π</i> (again, this has nothing to do with the <i class="timesitalic">π</i> denoting the ratio of the circumference to the diameter of a circle). This <i class="timesitalic">X</i> can take any integer value <span class="math">0</span> to <i class="timesitalic">n</i>. Hence,</p><!--<p class="Body"><span class="times">$$\mathbb{E}\left(X\right) = \sum_{k=0}^{k=n} k \; p\left(X = k\right) = \sum_{k=0}^{k=n}  k \; \binom{n}{k} \pi^{k}
\left(1-\pi\right)^{n-k}=\sum_{k=0}^{k=n}  k
\;  \frac{n!}{k!\left(n-k\right)!}\pi^{k} \times
\left(1-\pi\right)^{n-k}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_05-29-a.png" width="594"/></p>
</div>
<p class="body">We can drop the first term, which has the multiplier <span class="math"><i class="fm-italics">k</i> = 0</span>. Thus we get</p><!--<p class="Body"><span class="times">$$\mathbb{E}\left(X\right) =
\sum_{k=1}^{k=n}  \frac{n!}{\left(k - 1\right)!\left(n-k\right)!}\pi^{k}
\times \left(1-\pi\right)^{n-k}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_05-29-b.png" width="321"/></p>
</div>
<p class="body">We can factor <span class="math"><i class="fm-italics">n</i>! = <i class="fm-italics">n</i>(<i class="fm-italics">n</i> − 1)!</span> and <span class="math"><i class="fm-italics">π<sup class="fm-superscript">k</sup></i> = <i class="fm-italics">π</i> <i class="fm-italics">π</i><sup class="fm-superscript"><i class="fm-italics1">k</i> − 1</sup></span>. Also, <span class="math"><i class="fm-italics">n</i> − <i class="fm-italics">k</i> = (<i class="fm-italics">n</i> − 1) − (<i class="fm-italics">k</i> − 1)</span>. This gives us</p><!--<p class="Body"><span class="times">$$\mathbb{E}\left(X\right) =
\sum_{k=1}^{k=n}  \frac{n \; \left(n -1\right)!}{\left(k - 1\right)!\left(\left(n -1\right)-\left(k-1\right)\right)!} \pi \;
\pi^{k-1} \times \left(1-\pi\right)^{n-k}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_05-29-c.png" width="435"/></p>
</div>
<p class="body">Substituting <i class="timesitalic">j</i> for <span class="math"><i class="fm-italics">k</i> − 1</span> and <i class="timesitalic">m</i> for <span class="math"><i class="fm-italics">n</i> − 1</span>, we get</p><!--<p class="Body"><span class="times">$$\mathbb{E}\left(X\right) = n  \pi \;
\sum_{j=0}^{j=m}    \frac{m!}{j! \left(m-j\right)!}\pi^{j} \times
\left(1-\pi\right)^{m-j}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_05-30.png" width="313"/></p>
</div>
<p class="fm-equation-caption">Equation 5.30</p>
<p class="body">The quantity within the summation is similar to that in equation <a class="url" href="#eq-eq-binom-sum">5.29</a> (should sum to <span class="math">1</span>). This leaves us with</p>
<p class="fm-equation"><span class="math"><span class="segoe">𝔼</span><i class="fm-italics"><sub class="fm-subscript">binomial</sub></i> (<i class="fm-italics">X</i>) = <i class="fm-italics">nπ</i></span></p>
<p class="fm-equation-caption">Equation 5.31 <span class="calibre" id="eq-binomial-expected-value"/></p>
<p class="body">Equation <a class="url" href="#eq-binomial-expected-value">5.31</a> says that if <i class="timesitalic">π</i> is the probability of success in a single trial, then the expected number of successes in <i class="timesitalic">n</i> trials is <span class="math"><i class="fm-italics">n</i><i class="fm-italics">π</i></span>. For instance, if the probability of success in a single trial is <span class="math">0.2</span>, then the expected number of successes in <span class="math">100</span> trials is <span class="math">20</span>—which is almost intuitively obvious.</p>
<p class="fm-head2" id="variance-of-a-binomial-distribution">Variance of a binomial distribution</p>
<p class="body">The variance of a binomial random variable depicting the number of successes in <i class="timesitalic">n</i> trials where the probability of success in a given trial is a constant <i class="timesitalic">π</i> is</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">var<sub class="fm-subscript">binomial</sub></i> = <i class="fm-italics">nπ</i> (1 − <i class="fm-italics">π</i>)</span></p>
<p class="fm-equation-caption">Equation 5.32 <span class="calibre" id="eq-binomial-var"/></p>
<p class="body">The proof follows the same lines as that of the expected value.</p>
<p class="fm-code-listing-caption" id="lst-binom-dist-meanvar">Listing 5.8 Mean and variance of a binomial distribution</p>
<pre class="programlisting">num_samples = 100000                                <span class="fm-combinumeral">①</span>

   <span class="fm-combinumeral">②</span>
samples  = binom_dist.sample([num_samples])         <span class="fm-combinumeral">③</span>


sample_mean = samples.mean()                        <span class="fm-combinumeral">④</span>
dist_mean = binom_dist.mean                         <span class="fm-combinumeral">⑤</span>
assert torch.isclose(sample_mean, dist_mean, atol=0.2)

sample_var = binom_dist.sample([num_samples]).var() <span class="fm-combinumeral">⑥</span>
dist_var = binom_dist.variance                      <span class="fm-combinumeral">⑦</span>
assert torch.isclose(sample_var, dist_var, atol=0.2)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Number of sample points</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math">100000 × 1</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Obtains samples from ufm_dist instantiated in listing <a class="url" href="#lst-binom-dist-logprob">5.7</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Sample mean</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Mean via PyTorch function</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Sample variance</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Variance via PyTorch function</p>
<h3 class="fm-head1" id="sec-multinomial-distr">5.9.4 Multinomial distribution</h3>
<p class="body"><a id="marker-185"/>Consider again the example problem we discussed in section <a class="url" href="#sec-binomial-distr">5.9.3</a>. We have a database of photos of people. But instead of two classes, celebrity and non-celebrity, we have four classes:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Photos of Albert Einstein (class 1): <span class="math">10%</span> of the photos</p>
</li>
<li class="fm-list-bullet">
<p class="list">Photos of Marie Curie (class 2): <span class="math">42%</span> of the photos</p>
</li>
<li class="fm-list-bullet">
<p class="list">Photos of Carl Friedrich Gauss (class 3): <span class="math">4%</span> of the photos</p>
</li>
<li class="fm-list-bullet">
<p class="list">Other photos (class 4): <span class="math">44%</span> of the photos</p>
</li>
</ul>
<p class="body">If we randomly select a photo from the database (that is, perform a random trial),</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The probability of selecting class 1 (picking an Einstein photo) is <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = 0.1</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The probability of selecting class 2 (picking a Marie Curie photo) is <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.42</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The probability of selecting class 3 (picking a Gauss photo) is <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">3</sub> = 0.04</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The probability of selecting class 4 (picking a photo of none of the above) is <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">4</sub> = 0.44</span>.</p>
</li>
</ul>
<p class="body">Notice that <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">π</i><sub class="fm-subscript">2</sub> + <i class="fm-italics">π</i><sub class="fm-subscript">3</sub> + <i class="fm-italics">π</i><sub class="fm-subscript">4</sub> = 1</span>. This is because the classes are mutually exclusive and exhaustive, so exactly one of these classes must occur in every trial.</p>
<p class="body">Given all this, let’s ask the question: “What is the probability that in a set of <span class="math">10</span> random trials, class <span class="math">1</span> occurs <span class="math">1</span> time, class <span class="math">2</span> occurs <span class="math">2</span> times, class <span class="math">3</span> occurs <span class="math">1</span> time, and class <span class="math">4</span> occurs the remaining <span class="math">6</span> times?” This is the kind of problem multinomial distributions deal with.</p>
<p class="body">Formally,</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Let <span class="math"><i class="fm-italics">C</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">C</i><sub class="fm-subscript">2</sub>, ⋯, <i class="fm-italics">C<sub class="fm-subscript">m</sub></i></span> be a set of <i class="timesitalic">m</i> classes such that in any random trial, exactly one of these classes will be selected with the respective probabilities <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">π</i><sub class="fm-subscript">2</sub>, ⋯, <i class="fm-italics">π<sub class="fm-subscript">m</sub></i></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Let <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">X</i><sub class="fm-subscript">2</sub>, ⋯, <i class="fm-italics">X<sub class="fm-subscript">m</sub></i></span> be a set of random variables. <i class="timesitalic">X<sub class="fm-subscript">i</sub></i> corresponds to the number of occurrences of class <i class="timesitalic">C<sub class="fm-subscript">i</sub></i> in a set of <i class="timesitalic">n</i> trials.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Then the multinomial probability function depicting the probability that class <span class="math"><i class="fm-italics">C</i><sub class="fm-subscript">1</sub></span> is selected <span class="math"><i class="fm-italics">k</i><sub class="fm-subscript">1</sub></span> times, class <span class="math"><i class="fm-italics">C</i><sub class="fm-subscript">2</sub></span> is selected <span class="math"><i class="fm-italics">k</i><sub class="fm-subscript">2</sub></span> times, and class <span class="math"><i class="fm-italics">C</i><sub class="fm-subscript">3</sub></span> is selected <i class="timesitalic">k<sub class="fm-subscript">m</sub></i> times is</p>
</li>
</ul><!--<p class="Body"><span class="times">$$p\left(X_{1} = k_{1}, X_{2} = k_{2},
\cdots, X_{m}=k_{m}\right) =
\frac{n!}{k_{1}!\,k_{2}!\,\cdots,k_{m}!}\pi_{1}^{k_{1}} \,
\pi_{2}^{k_{2}}\, \cdots \, \pi_{m}^{k_{m}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="51" src="../../OEBPS/Images/eq_05-33.png" width="500"/></p>
</div>
<p class="fm-equation-caption">Equation 5.33 <span class="calibre" id="eq-multinomial-distr"/></p>
<p class="body-ind">where</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\sum_{i=1}^{m} k_{i} = n\\
&amp;\sum_{i=1}^{m} \pi_{i} = 1\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="118" src="../../OEBPS/Images/eq_05-33-a.png" width="81"/></p>
</div>
<p class="body">We can verify that for <span class="math"><i class="fm-italics">m</i> = 2</span>, this becomes the binomial distribution (equation <a class="url" href="#eq-binom-distr">5.28</a>). A noteworthy point is that if we look at any one of the <i class="timesitalic">m</i> variables <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">X<sub class="fm-subscript">m</sub></i> individually, its distribution is binomial.</p>
<p class="body"><a id="marker-186"/>Let’s work out the final probability for the example we started with: the probability that in a set of <span class="math">10</span> random trials, class 1 occurs <span class="math">1</span> time, class <span class="math">2</span> occurs <span class="math">2</span> times, class <span class="math">3</span> occurs <span class="math">1</span> time, and class <span class="math">4</span> occurs the remaining <span class="math">6</span> times. This is</p><!--<p class="Body"><span class="times">$$p\left(X_{1} = 1, X_{2} = 2, X_{3} = 1, X_{4} = 6\right) = \frac{10!}{1!\, 2! \, 1! \, 6!}\left(0.1\right)^{1}\,\left(0.42\right)^{2}\,\left(0.04\right)^{1}\,\left(0.44\right)^{6}
= 0.0129$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="51" src="../../OEBPS/Images/eq_05-33-b.png" width="651"/></p>
</div>
<p class="fm-code-listing-caption" id="lst-multinom-dist-logprob">Listing 5.9 Log probability of a multinomial distribution</p>
<pre class="programlisting">from torch.distributions import Multinomial                  <span class="fm-combinumeral">①</span>

num_trials = 10                                              <span class="fm-combinumeral">②</span>
P = torch.tensor([0.1, 0.42, 0.04, 0.44], dtype=torch.float)

multinom_dist = Multinomial(num_trials, probs=P)             <span class="fm-combinumeral">③</span>

X = torch.tensor([1, 2, 1, 6], dtype=torch.float)            <span class="fm-combinumeral">④</span>

def raw_eval(X, n, P):
    f = math.factorial
    result = f(n)
    for p, x in zip(P, X):
        result *= (p ** x) / f(x)
    return math.log(result)

log_prob = multinom_dist.log_prob(X)                         <span class="fm-combinumeral">⑤</span>
raw_eval_log_prob = raw_eval(X, num_trials, P)               <span class="fm-combinumeral">⑥</span>
assert torch.isclose(log_prob, raw_eval_log_prob, atol=1e-4) <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports a PyTorch multinomial distribution</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sets the distribution params</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates a multinomial dist object</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates a single-point test dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Evaluates the probability using PyTorch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Evaluates the probability using formula</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Asserts that the probabilities match</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the multinomial distribution, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/l1gz">http://mng.bz/l1gz</a>.</p>
<p class="fm-head2" id="expected-value-of-a-multinomial-distribution">Expected value of a multinomial distribution</p>
<p class="body"><a id="marker-187"/>Each of the random variables <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">X<sub class="fm-subscript">m</sub></i> individually subscribes to a binomial distribution. Accordingly, following the binomial distribution expected value formula from equation <a class="url" href="#eq-binomial-expected-value">5.31</a>,</p>
<p class="fm-equation"><span class="math"><span class="segoe">𝔼</span><i class="fm-italics"><sub class="fm-subscript">multinomial</sub></i>(<i class="fm-italics">X<sub class="fm-subscript">i</sub></i>) = <i class="fm-italics">nπ<sub class="fm-subscript">i</sub></i></span></p>
<p class="fm-equation-caption">Equation 5.34 <span class="calibre" id="eq-multinomial-expected-value"/></p>
<p class="fm-head2" id="variance-of-a-multinomial-distribution">Variance of a multinomial distribution</p>
<p class="body">The variation of the random variables <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">X<sub class="fm-subscript">m</sub></i>, following the binomial distribution variance formula from equation <a class="url" href="#eq-binomial-var">5.32</a>, is</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">var<sub class="fm-subscript">multinomial</sub></i>(<i class="fm-italics">X<sub class="fm-subscript">i</sub></i>) = <i class="fm-italics">nπ<sub class="fm-subscript">i</sub></i>(1−<i class="fm-italics">π<sub class="fm-subscript">i</sub></i>)</span></p>
<p class="fm-equation-caption">Equation 5.35 <span class="calibre" id="eq-multinomial-var"/></p>
<p class="body">If each of the <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">X<sub class="fm-subscript">m</sub></i> is a scalar, then we can think of a random vector <!--<span class="times">$X=\begin{bmatrix}X_{1}\\X_{2}\\ \vdots\\X_{m}\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="87" src="../../OEBPS/Images/eq_05-35-a.png" width="34"/></span>. The expected value of such a random variable is</p><!--<p class="FM-Equation"><span class="times">$$\mathbb{E}_{multinomial}\left(X\right) =
\begin{bmatrix} n  \pi_{1}\\n  \pi_{1}\\ \vdots \\n  \pi_{m}\\
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="136" src="../../OEBPS/Images/eq_05-35-b.png" width="178"/></p>
</div>
<p class="fm-equation">and the covariance is</p><!--<p class="FM-Equation"><span class="times">$$\mathbb{C}_{multinomial}\left(X\right) =
\begin{bmatrix}
\sigma_{11} &amp; \sigma_{12} &amp; \cdots \sigma_{1m}\\[-3pt]
\sigma_{21} &amp; \sigma_{22} &amp; \cdots \sigma_{2m}\\[-3pt]
&amp; &amp; \cdots\\[-3pt]
\sigma_{m1} &amp; \sigma_{m2} &amp; \cdots \sigma_{mm}\\
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="113" src="../../OEBPS/Images/eq_05-36.png" width="300"/></p>
</div>
<p class="fm-equation-caption">Equation 5.36</p>
<p class="body">where the diagonal terms are like the binomial variance <span class="math"><i class="fm-italics">σ<sub class="fm-subscript">ii</sub></i> = <i class="fm-italics">nπ<sub class="fm-subscript">i</sub></i>(1−<i class="fm-italics">π<sub class="fm-subscript">i</sub></i>) <span class="cambria">∀</span><i class="fm-italics">i</i> <span class="cambria">∈</span> [1, <i class="fm-italics">m</i>]</span> and the off-diagonal terms are <span class="math"><i class="fm-italics">σ<sub class="fm-subscript">ij</sub></i> = −<i class="fm-italics">nπ<sub class="fm-subscript">i</sub>π<sub class="fm-subscript">j</sub></i> <span class="cambria">∀</span>(<i class="fm-italics">i</i>, <i class="fm-italics">j</i>) <span class="cambria">∈</span> [1, <i class="fm-italics">m</i>] × [1, <i class="fm-italics">m</i>]</span>. The cross-covariance terms in the diagonal are negative because an increase in one element implies a decrease in the others.</p>
<p class="fm-code-listing-caption" id="lst-multinom-dist-meanvar">Listing 5.10 Mean and variance of a multinomial distribution</p>
<pre class="programlisting">num_samples = 100000                                         <span class="fm-combinumeral">①</span>

   <span class="fm-combinumeral">②</span>
samples = multinom_dist.sample([num_samples])                <span class="fm-combinumeral">③</span>

sample_mean = samples.mean(axis=0)                           <span class="fm-combinumeral">④</span>
dist_mean = multinom_dist.mean                               <span class="fm-combinumeral">⑤</span>
assert torch.allclose(sample_mean, dist_mean, atol=0.2)

sample_var = multinom_dist.sample([num_samples]).var(axis=0) <span class="fm-combinumeral">⑥</span>
dist_var = multinom_dist.variance                            <span class="fm-combinumeral">⑦</span>
assert torch.allclose(sample_var, dist_var, atol=0.2)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Number of sample points</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math">100000 × 1</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Obtains samples from ufm_dist instantiated in listing <a class="url" href="#lst-multinom-dist-logprob">5.9</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Sample mean</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Mean via PyTorch function</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Sample variance</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Variance via PyTorch function</p>
<h3 class="fm-head1" id="sec-bernoulli-distr">5.9.5 Bernoulli distribution</h3>
<p class="body"><a id="marker-188"/>A Bernoulli distribution is a special case of a binomial distribution where <span class="math"><i class="fm-italics">n</i> = 1</span>: that is, a single success-or-failure trial is performed. The probability of success is <i class="timesitalic">π</i>, and the probability of failure is <span class="math">1 − <i class="fm-italics">π</i></span>.</p>
<p class="body">In other words, let <i class="timesitalic">X</i> be a discrete random variable that takes the value <span class="math">1</span> (success) with probability <i class="timesitalic">π</i> and the value <span class="math">0</span> (failure) with probability <span class="math">1 − <i class="fm-italics">π</i></span>. The distribution of <i class="timesitalic">X</i> is the Bernoulli distribution:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 1) = <i class="fm-italics">π</i></span><br class="calibre20"/>
<span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 0) = 1 - <i class="fm-italics">π</i></span></p>
<p class="fm-code-listing-caption" id="lst-bern-dist-logprob">Listing 5.11 Log probability of a Bernoulli distribution</p>
<pre class="programlisting">from torch.distributions import Bernoulli                    <span class="fm-combinumeral">①</span>

p = torch.tensor([0.3], dtype=torch.float)                   <span class="fm-combinumeral">②</span>

bern_dist = Bernoulli(p)                                     <span class="fm-combinumeral">③</span>
X = torch.tensor([1], dtype=torch.float)                     <span class="fm-combinumeral">④</span>
def raw_eval(X, p):
    prob = p if X == 1 else 1-p
    return math.log(prob)

log_prob = bern_dist.log_prob(X)                             <span class="fm-combinumeral">⑤</span>
raw_eval_log_prob = raw_eval(X, p)                           <span class="fm-combinumeral">⑥</span>
assert torch.isclose(log_prob, raw_eval_log_prob, atol=1e-4) <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports a PyTorch Bernoulli distribution</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sets the distribution params</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates a Bernoulli distribution object</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates a single-point test dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Evaluates the probability using PyTorch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Evaluates the probability using the formula</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Asserts that the probabilities match</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the Bernoulli distribution, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/BRwq">http://mng.bz/BRwq</a>.</p>
<p class="fm-head2" id="expected-value-of-a-bernoulli-distribution">Expected value of a Bernoulli distribution</p>
<p class="body">If there are only two classes, <i class="fm-italics">success</i> and <i class="fm-italics">failure</i>, we cannot speak directly of an expected value. If we run, say, <span class="math">100</span> trials and get <span class="math">30</span> <i class="fm-italics">successes</i> and <span class="math">70</span> <i class="fm-italics">failures</i>, the average is <span class="math">0.3</span> <i class="fm-italics">success</i>, which is not a valid outcome. We cannot have fractional <i class="fm-italics">success</i> or <i class="fm-italics">failure</i> in this binary system.</p>
<p class="body">We can, however, talk about the expected value of a Bernoulli distribution if we introduce an artificial construct. We assign numerical values to these binary entities: <i class="fm-italics">success</i> <span class="math">= 1</span> and <i class="fm-italics">failure</i> <span class="math">= 0</span>. Then the expected value of <i class="timesitalic">X</i> is</p><!--<p class="FM-Equation"><span class="times"><span class="segoe">𝔼</span>(<i class="fm-italics">X</em>) = ∑<sub class="FM-Subscript"><i class="fm-italics">x</em><span class="cambria">∈</span>{0,1}</sub> <i class="fm-italics">xp</em>(<i class="fm-italics">x</em>) = 1 ⋅ <i class="fm-italics">π</em> + (1−<i class="fm-italics">π</em>) ⋅ 0 = <i class="fm-italics">π</em></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_05-37.png" width="332"/></p>
</div>
<p class="fm-equation-caption">Equation 5.37 <span class="calibre" id="eq-bernoulli-distr-expectedvalue"/></p>
<p class="fm-head2" id="variance-of-a-bernoulli-distribution">Variance of a Bernoulli distribution</p>
<p class="body">Similarly, if we assign numerical values to these binary entities—<i class="fm-italics">success</i> <span class="math">= 1</span> and <i class="fm-italics">failure</i> <span class="math">= 0</span>—the variance of the Bernoulli distribution is</p><!--<p class="FM-Equation"><span class="times"><i class="fm-italics">var</em>(<i class="fm-italics">X</em>) = ∑<sub class="FM-Subscript"><i class="fm-italics">x</em><span class="cambria">∈</span>{0,1}</sub> (<i class="fm-italics">x</em> − <span class="segoe">𝔼</span>(<i class="fm-italics">X</em>))<sup class="FM-Superscript">2</sup><i class="fm-italics">p</em>(<i class="fm-italics">x</em>) = (1−<i class="fm-italics">π</em>)<sup class="FM-Superscript">2</sup><i class="fm-italics">π</em> + (0−<i class="fm-italics">π</em>)<sup class="FM-Superscript">2</sup>(1−<i class="fm-italics">π</em>) = <i class="fm-italics">π</em>(1−<i class="fm-italics">π</em>)</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_05-38.png" width="579"/></p>
</div>
<p class="fm-equation-caption">Equation 5.38</p>
<p class="fm-code-listing-caption">Listing 5.12 Mean and variance of a Bernoulli distribution</p>
<pre class="programlisting">num_samples = 100000                               <span class="fm-combinumeral">①</span>

   <span class="fm-combinumeral">②</span>
samples = bern_dist.sample([num_samples])          <span class="fm-combinumeral">③</span>

sample_mean = samples.mean()                       <span class="fm-combinumeral">④</span>
dist_mean = bern_dist.mean                         <span class="fm-combinumeral">⑤</span>
assert torch.isclose(sample_mean, dist_mean, atol=0.2)

sample_var = bern_dist.sample([num_samples]).var() <span class="fm-combinumeral">⑥</span>
dist_var = bern_dist.variance                      <span class="fm-combinumeral">⑦</span>
assert torch.isclose(sample_var, dist_var, atol=0.2)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Number of sample points</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math">100000 × 1</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Obtains samples from ufm_dist instantiated in listing <a class="url" href="#lst-bern-dist-logprob">5.11</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Sample mean</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Mean via PyTorch function</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Sample variance</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Variance via PyTorch function</p>
<h3 class="fm-head1" id="sec-categorical-distr">5.9.6 Categorical distribution and one-hot vectors</h3>
<p class="body"><a id="marker-189"/>Consider again the example problem introduced in section <a class="url" href="#sec-multinomial-distr">5.9.4</a>. We have a database with four classes of photos:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Photos of Albert Einstein (class 1): <span class="math">10%</span></p>
</li>
<li class="fm-list-bullet">
<p class="list">Photos of Marie Curie (class 2): <span class="math">42%</span></p>
</li>
<li class="fm-list-bullet">
<p class="list">Photos of Carl Friedrich Gauss (class 3): <span class="math">4%</span></p>
</li>
<li class="fm-list-bullet">
<p class="list">Other photos (class 4): <span class="math">44%</span></p>
</li>
</ul>
<p class="body">If we randomly select a photo from the database,</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The probability of selecting class 1 is <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = 0.1</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The probability of selecting class 2 is <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.42</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The probability of selecting class 3 is <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">3</sub> = 0.04</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The probability of selecting class 4 is <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">4</sub> = 0.44</span>.</p>
</li>
</ul>
<p class="body">As before, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">π</i><sub class="fm-subscript">2</sub> + <i class="fm-italics">π</i><sub class="fm-subscript">3</sub> + <i class="fm-italics">π</i><sub class="fm-subscript">4</sub> = 1</span> because the classes are mutually exclusive and exhaustive so exactly one class must occur in each trial.</p>
<p class="body">In multinomial distribution, we performed <i class="timesitalic">n</i> trials and asked how many times each specific class would occur. What if we perform only one trial? Then we get categorical distribution.</p>
<p class="body">Categorical distribution is a special case of multinomial distribution (with the number of trials <span class="math"><i class="fm-italics">n</i> = 1</span>). It is also an extension of the Bernoulli distribution where instead of just two classes, <i class="fm-italics">success</i> and <i class="fm-italics">failure</i>, we can have an arbitrary number of classes.</p>
<p class="body">Formally,</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Let <span class="math"><i class="fm-italics">C</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">C</i><sub class="fm-subscript">2</sub>, ⋯, <i class="fm-italics">C<sub class="fm-subscript">m</sub></i></span> be a set of <i class="timesitalic">m</i> classes such that in any random trial, exactly one of these classes will be selected, with the respective probabilities <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">π</i><sub class="fm-subscript">2</sub>, ⋯, <i class="fm-italics">π<sub class="fm-subscript">m</sub></i></span>. We sometimes refer to the probabilities of all the classes together as a vector <!--<p class="Body"><span class="times">$\vec{\pi}=\begin{bmatrix}\pi_{1}\\\pi_{2}\\\vdots\\\pi_{m}\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="80" src="../../OEBPS/Images/eq_05-38-a2.png" width="64"/></span></p>
</li>
<li class="fm-list-bullet">
<p class="list">Let <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">X</i><sub class="fm-subscript">2</sub>, ⋯, <i class="fm-italics">X<sub class="fm-subscript">m</sub></i></span> be a set of random variables. <i class="timesitalic">X<sub class="fm-subscript">i</sub></i> corresponds to the number of occurrences of class <i class="timesitalic">C<sub class="fm-subscript">i</sub></i> in a set of <i class="timesitalic">n</i> trials.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Then the categorical probability function depicts the probability of each of the classes <span class="math"><i class="fm-italics">C</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">C</i><sub class="fm-subscript">2</sub></span>, and so on, in a single trial.</p>
</li>
</ul>
<p class="fm-head2" id="one-hot-vector">One-hot vector</p>
<p class="body"><a id="marker-190"/>We can use a one-hot vector to compactly express the outcome of a single trial of categorical distribution. This is a vector with <i class="timesitalic">m</i> elements. Exactly a single element is <span class="math">1</span>; all other elements are <span class="math">0</span>. The <span class="math">1</span> indicates which of the <i class="timesitalic">m</i> possible classes occurred in that specific trial. For instance, in the example with the database of photos, if a Marie Curie photo comes up in a given trial, the corresponding one-hot vector is <!--<span class="times">$\vec{x}=\begin{bmatrix} 0 \\1\\0\\0\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="77" src="../../OEBPS/Images/eq_05-38-b2.png" width="51"/></span>.</p>
<p class="fm-head2" id="probability-of-a-categorical-distribution">Probability of a categorical distribution</p>
<p class="body">We can think of a one-hot vector <i class="timesitalic">X</i> as a random variable with a categorical distribution. Note that each individual class follows a Bernoulli distribution. The probability of class <i class="timesitalic">C<sub class="fm-subscript">i</sub></i> occurring in any given trial is</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">C<sub class="fm-subscript">i</sub></i>) = <i class="fm-italics">π<sub class="fm-subscript">i</sub></i></span></p>
<p class="body">We can express the probability distribution of all the classes together compactly</p><!--<p class="Body"><span class="times">$$p\left(X=\vec{x}\right) =\pi_{1}^{x_{1}}
\pi_{2}^{x_{2}} \cdots \pi_{m}^{x_{m}} = \prod_{i=1}^{i=m}
\pi_{i}^{x_{i}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_05-39.png" width="263"/></p>
</div>
<p class="fm-equation-caption">Equation 5.39 <span class="calibre" id="eq-prob-categorical-distr"/></p>
<p class="body">where <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is a one-hot vector. Note that all but one of the powers in equation <a class="url" href="#eq-prob-categorical-distr">5.39</a> is 0; hence the corresponding factor evaluates to 1. The remaining power is <span class="math">1</span>. Hence the overall probability always evaluates to <i class="timesitalic">π<sub class="fm-subscript">i</sub></i>, where <i class="timesitalic">i</i> is the index of the class that occurred in the trial.</p>
<p class="fm-head2" id="expected-value-of-a-categorical-distribution">Expected value of a categorical distribution</p>
<p class="body">Since we are talking about classes, expected value and variance do not make sense in this context. We encountered a similar situation with the Bernoulli distribution. We assigned numerical values to each class and somewhat artificially defined the expected value and variance. A similar idea can also be applied here: we can talk about the expected value and variance of the one-hot vector (which consists of numerical values <span class="math">0</span> and <span class="math">1</span>). But it remains an artificial construct.</p>
<p class="body">Given a random variable <i class="timesitalic">X</i> whose instances are one-hot vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> following a categorical distribution with <i class="timesitalic">m</i> classes with respective probabilities <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">π</i><sub class="fm-subscript">2</sub>, ⋯, <i class="fm-italics">π<sub class="fm-subscript">m</sub></i></span>,</p><!--<p class="Body"><span class="times">$$\mathbb{E}\left(X\right) = \vec{\pi} =
\begin{bmatrix}
\pi_{1}\\
\pi_{2}\\
\vdots\\
\pi_{m}
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="133" src="../../OEBPS/Images/eq_05-40.png" width="125"/></p>
</div>
<p class="fm-equation-caption">Equation 5.40</p>
<p class="body">We skip the variance of a categorical distribution.</p>
<h2 class="fm-head" id="summary-4">Summary</h2>
<p class="body"><a id="marker-191"/>In this chapter, we first looked at probability and statistics from a machine learning point of view. We also introduced the PyTorch <code class="fm-code-in-text">distributions</code> package and illustrated each concept with PyTorch <code class="fm-code-in-text">distributions</code> code samples immediately following the math.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The probability of a specific event type is defined as the fraction of the total population of all possible events occupied by events of that specific type.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A random variable is a variable that can assume any value from a predefined range of possible values. Random variables can be discrete or continuous. A probability is associated with a discrete random variable taking a specific value. A probability is also associated with a continuous random variable taking a value in an infinitesimally small range around a specific value, called its probability density at that value.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The sum rule of probabilities states that the sum of the probabilities of a set of mutually exclusive events is the probability of one or another of them occurring. If the set of events is exhaustive that is, among them, they cover the entire space of possible events), then their sum is 1 because one or another of them must occur. For continuous random variables, integrating the probability density function over the domain of possible values yields 1.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The joint probability of a set of events is the probability of all those events occurring together. If the events are independent, the joint probability is the product of their individual probabilities.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Drawing a sample from the probability distribution of a random variable returns an arbitrary value from the set of possible values. If we draw many samples, the higher-probability values show up more often than the lower-probability values. The sampled points occupy a region (called the sample point cloud) in the domain of possible values. In a sample point cloud, the region where the probabilities are higher is more densely populated than lower-probability regions.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The expected value of a random variable is the average of the values of points in a very large (approaching infinity) sample cloud. It is equal to the weighted sum of all possible values of the random variable, where the weight for each value is its probability of occurrence. For continuous random variables, this boils down to integration—over the domain of possible values—of the product of the random variable’s value and the probability density. The physical significance of the expected value is that it is a single-point representation of the entire distribution.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The variance of a random variable is the square root of the average squared distances of the sample point values from the mean in a very large (approaching infinity) sample cloud. It is equal to the weighted sum of the squared distances of all possible values of the random variable from the mean. The weight for each value is its probability of occurrence. For continuous random variables, this boils down to integration—over the domain of possible values—of the product of the squared distance of the random variable’s value from the mean and the probability density. Physically, the variance is a measure of the spread of the points in the distribution around its mean. In the multivariate case, this spread depends on the direction. Since there are infinite possible directions in a space with two or more dimensions, we cannot speak of a single variance value. Instead, we compute a covariance matrix with which to compute the spread along any specified direction. The eigenvector corresponding to the largest eigenvalue of this covariance matrix yields the direction of maximum spread. That eigenvalue yields the maximum spread. The eigenvector corresponding to the next-largest eigenvalue yields the orthogonal direction with the next-highest spread, and so forth.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Principal component analysis (PCA) is a technique in multivariate statistics to identify the directions of the maximum spread of data. It uses the eigenvectors and eigenvalues of the covariance matrix.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The Gaussian distribution is the most important probability distribution. The Gaussian random variable has one value with the highest probability of occurrence. The probability decreases smoothly with increasing distance from that highest probability value. The probability density function is continuous and looks like a bell-shaped surface. The center of the bell is the highest probability value, which also happens to be the expected value of the Gaussian random variable. The covariance matrix determines the shape of the base of the bell surface. It is circular when the covariance matrix is diagonal, with equal values on the diagonal; it is elliptical in general, with the axes of the ellipse along the eigenvectors of the covariance matrix.<br class="calibre20"/>
      The sample point cloud of a Gaussian distribution is elliptical. It corresponds to the base of the bell-shaped probability density function. The longest spread corresponds to the ellipse’s major axis, which corresponds to the eigenvector corresponding to the largest eigenvalue of the covariance matrix. In the GitHub repository, we have provided an interactive visualizer for observing the shapes of Gaussian distributions in one and two dimensions as you change the parameter values. Take a look at the interactive visualization section at <a class="url" href="http://mng.bz/NYJX">http://mng.bz/NYJX</a>.<a id="marker-192"/></p>
</li>
</ul>
</div></body></html>