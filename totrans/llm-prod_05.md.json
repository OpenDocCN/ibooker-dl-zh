["```py\n$ docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:23.09-py3\n```", "```py\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport torch_tensorrt\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokens = tokenizer(\"The cat is on the table.\", return_tensors=\"pt\")[\n    \"input_ids\"\n].cuda()\nmodel = GPT2LMHeadModel.from_pretrained(\n    \"gpt2\", use_cache=False, return_dict=False, torchscript=True\n).cuda()\nmodel.eval()\n\ntraced_model = torch.jit.trace(model, tokens)      #1\n\ncompile_settings = {     #2\n    \"inputs\": [\n        torch_tensorrt.Input(\n            # For static size\n            shape=[1, 7],\n            # For dynamic sizing:\n            # min_shape=[1, 3],\n            # opt_shape=[1, 128],\n            # max_shape=[1, 1024],\n            dtype=torch.int32,  # Datatype of input tensor.\n            # Allowed options torch.(float|half|int8|int32|bool)\n        )\n    ],\n    \"truncate_long_and_double\": True,\n    \"enabled_precisions\": {torch.half},      #3\n    \"ir\": \"torchscript\",\n}\ntrt_model = torch_tensorrt.compile(traced_model, **compile_settings)\n\ntorch.jit.save(trt_model, \"trt_model.ts\")      #4\n\ntrt_model = torch.jit.load(\"trt_model.ts\")      #5\ntokens.half()\ntokens = tokens.type(torch.int)\nlogits = trt_model(tokens)\nresults = torch.softmax(logits[-1], dim=-1).argmax(dim=-1)\nprint(tokenizer.batch_decode(results))\n```", "```py\n# ['\\n was a the way.\\n']\n```", "```py\n$ pip install --upgrade-strategy eager optimum[exporters,onnxruntime]\n```", "```py\n↪ $ optimum-cli export onnx --model WizardLM/WizardCoder-1B-V1.0 ./models_onnx --optimize O1\n```", "```py\nfrom fastapi import FastAPI, Depends, HTTPException, status, Request\nfrom fastapi.security import OAuth2PasswordBearer\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nimport uvicorn\n\napi_keys = [\"1234567abcdefg\"]        #1\nAPI_KEY_NAME = \"access_token\"\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\nlimiter = Limiter(key_func=get_remote_address)\n\napp = FastAPI()\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\nasync def get_api_key(api_key: str = Depends(oauth2_scheme)):\n    if api_key not in api_keys:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API Key\",\n        )\n\n@app.get(\"/hello\", dependencies=[Depends(get_api_key)])\n@limiter.limit(\"5/minute\")\nasync def hello(request: Request):\n    return {\"message\": \"Hello World\"}\n```", "```py\nimport argparse\nimport asyncio\nfrom typing import AsyncGenerator\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import Response, StreamingResponse\nimport uvicorn\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TextIteratorStreamer,\n)\nfrom threading import Thread\n\napp = FastAPI()\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")     #1\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nstreamer = TextIteratorStreamer(tokenizer)\n\nasync def stream_results() -> AsyncGenerator[bytes, None]:\n    for response in streamer:\n        await asyncio.sleep(1)                   #2\n        yield (response + \"\\n\").encode(\"utf-8\")\n\n@app.post(\"/generate\")\nasync def generate(request: Request) -> Response:\n    \"\"\"Generate LLM Response\n\n    The request should be a JSON object with the following fields:\n    - prompt: the prompt to use for the generation.\n    \"\"\"\n    request_dict = await request.json()\n    prompt = request_dict.pop(\"prompt\")\n    inputs = tokenizer([prompt], return_tensors=\"pt\")\n    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)\n\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)    #3\n    thread.start()\n\n    return StreamingResponse(stream_results())\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()                      #4\n    parser.add_argument(\"--host\", type=str, default=None)\n    parser.add_argument(\"--port\", type=int, default=8000)\n    args = parser.parse_args()\n\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"debug\")\n```", "```py\n$ feast init feast_example\n$ cd feast_example/feature_repo\n```", "```py\nimport pandas as pd\nfrom datasets import load_dataset\nimport datetime\n\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\ndef save_qa_to_parquet(path):\n    squad = load_dataset(\"squad\", split=\"train[:5000]\")    #1\n    ids = squad[\"id\"]                             #2\n    questions = squad[\"question\"]\n    answers = [answer[\"text\"][0] for answer in squad[\"answers\"]]\n    qa = pd.DataFrame(           #3\n        zip(ids, questions, answers),\n        columns=[\"question_id\", \"questions\", \"answers\"],\n    )\n\n    qa[\"embeddings\"] = qa.questions.apply(lambda x: model.encode(x))   #4\n    qa[\"created\"] = datetime.datetime.utcnow()\n    qa[\"datetime\"] = qa[\"created\"].dt.floor(\"h\")\n    qa.to_parquet(path)              #5\n\nif __name__ == \"__main__\":\n    path = \"./data/qa.parquet\"\n    save_qa_to_parquet(path)\n```", "```py\nfrom feast import Entity, FeatureView, Field, FileSource, ValueType\nfrom feast.types import Array, Float32, String\nfrom datetime import timedelta\n\npath = \"./data/qa.parquet\"\n\nquestion = Entity(name=\"question_id\", value_type=ValueType.STRING)\n\nquestion_feature = Field(name=\"questions\", dtype=String)\n\nanswer_feature = Field(name=\"answers\", dtype=String)\n\nembedding_feature = Field(name=\"embeddings\", dtype=Array(Float32))\n\nquestions_view = FeatureView(\n    name=\"qa\",\n    entities=[question],\n    ttl=timedelta(days=1),\n    schema=[question_feature, answer_feature, embedding_feature],\n    source=FileSource(\n        path=path,\n        event_timestamp_column=\"datetime\",\n        created_timestamp_column=\"created\",\n        timestamp_field=\"datetime\",\n    ),\n    tags={},\n    online=True,\n)\n```", "```py\n$ feast apply\n```", "```py\n$ feast materialize-incremental 2023-11-30T00:00:00 --views qa\n```", "```py\nimport pandas as pd\nfrom feast import FeatureStore\n\nstore = FeatureStore(repo_path=\".\")\n\npath = \"./data/qa.parquet\"\nids = pd.read_parquet(path, columns=[\"question_id\"])\n\nfeature_vectors = store.get_online_features(\n    features=[\"qa:questions\", \"qa:answers\", \"qa:embeddings\"],\n    entity_rows=[{\"question_id\": _id} for _id in ids.question_id.to_list()],\n).to_df()\nprint(feature_vectors.head())\n```", "```py\nimport os\nimport tiktoken\nfrom datasets import load_dataset\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom pinecone import Pinecone, ServerlessSpec\nfrom sentence_transformers import SentenceTransformer\n\nfrom tqdm.auto import tqdm\nfrom uuid import uuid4\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")            #1\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")              #2\n\npc = Pinecone(api_key=PINECONE_API_KEY)\n\nclass WikiDataIngestion:\n    def __init__(\n        self,\n        index,\n        wikidata=None,\n        embedder=None,\n        tokenizer=None,\n        text_splitter=None,\n        batch_limit=100,\n    ):\n        self.index = index\n        self.wikidata = wikidata or load_dataset(\n            \"wikipedia\", \"20220301.simple\", split=\"train[:10000]\"\n        )\n        self.embedder = embedder or OpenAIEmbeddings(\n            model=\"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY\n        )\n        self.tokenizer = tokenizer or tiktoken.get_encoding(\"cl100k_base\")\n        self.text_splitter = (\n            text_splitter\n            or RecursiveCharacterTextSplitter(\n                chunk_size=400,\n                chunk_overlap=20,\n                length_function=self.token_length,\n                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n            )\n        )\n        self.batch_limit = batch_limit\n\n    def token_length(self, text):\n        tokens = self.tokenizer.encode(text, disallowed_special=())\n        return len(tokens)\n\n    def get_wiki_metadata(self, page):\n        return {\n            \"wiki-id\": str(page[\"id\"]),\n            \"source\": page[\"url\"],\n            \"title\": page[\"title\"],\n        }\n\n    def split_texts_and_metadatas(self, page):\n        basic_metadata = self.get_wiki_metadata(page)\n        texts = self.text_splitter.split_text(page[\"text\"])\n        metadatas = [\n            {\"chunk\": j, \"text\": text, **basic_metadata}\n            for j, text in enumerate(texts)\n        ]\n        return texts, metadatas\n\n    def upload_batch(self, texts, metadatas):\n        ids = [str(uuid4()) for _ in range(len(texts))]\n        embeddings = self.embedder.embed_documents(texts)\n        self.index.upsert(vectors=zip(ids, embeddings, metadatas))\n\n    def batch_upload(self):\n        batch_texts = []\n        batch_metadatas = []\n\n        for page in tqdm(self.wikidata):\n            texts, metadatas = self.split_texts_and_metadatas(page)\n\n            batch_texts.extend(texts)\n            batch_metadatas.extend(metadatas)\n\n            if len(batch_texts) >= self.batch_limit:\n                self.upload_batch(batch_texts, batch_metadatas)\n                batch_texts = []\n                batch_metadatas = []\n\n        if len(batch_texts) > 0:\n            self.upload_batch(batch_texts, batch_metadatas)\n\nif __name__ == \"__main__\":\n    index_name = \"pincecone-llm-example\"\n\n    if index_name not in pc.list_indexes().names():      #3\n        pc.create_index(\n            name=index_name,\n            metric=\"cosine\",\n            dimension=1536,          #4\n            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n        )\n\n    index = pc.Index(index_name)         #5\n    print(index.describe_index_stats())\n\n    embedder = None           #6\n    if not OPENAI_API_KEY:\n        embedder = SentenceTransformer(\n            \"sangmini/msmarco-cotmae-MiniLM-L12_en-ko-ja\"\n        )                                                      #7\n        embedder.embed_documents = lambda *args, **kwargs: embedder.encode(\n            *args, **kwargs\n        ).tolist()\n\n    wiki_data_ingestion = WikiDataIngestion(index, embedder=embedder)    #8\n    wiki_data_ingestion.batch_upload()\n    print(index.describe_index_stats())\n\n    query = \"Did Johannes Gutenberg invent the printing press?\"     #9\n    embeddings = wiki_data_ingestion.embedder.embed_documents(query)\n    results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n    print(results)\n```", "```py\n$ python -m vllm.entrypoints.api_server --model IMJONEZZ/ggml-openchat-8192-q4_0\n```", "```py\n$ curl http://localhost:8000/generate -d '{\"prompt\": \"Which pokemon is \n↪ the best?\", \"use_beam_search\": true, \"n\": 4, \"temperature\": 0}'\n```", "```py\n$ gcloud container clusters create <NAME>\n```", "```py\n$ eksctl create cluster\n```", "```py\n$ az group create --name=<GROUP_NAME> --location=westus\n$ az aks create --resource-group=<GROUP_NAME> --name=<CLUSTER_NAME>\n```", "```py\nresourceLimits:\n  - resourceType: 'cpu'\n    minimum: 10\n    maximum: 100\n  - resourceType: 'memory'\n    maximum: 1000\n  - resourceType: 'nvidia-tesla-t4'\n    maximum: 40\n  - resourceType: 'nvidia-tesla-a100'\n    maximum: 16\n  - resourceType: 'nvidia-a100-80gb'\n    maximum: 8\nmanagement:\n  autoRepair: true\n  autoUpgrade: true\nshieldedInstanceConfig:\n  enableSecureBoot: true\n  enableIntegrityMonitoring: true\ndiskSizeGb: 100\n```", "```py\n$ gcloud container clusters update <CLUSTER_NAME> --enable-autoprovisioning --autoprovisioning-config-file <FILE_NAME>\n```", "```py\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llm-application\nspec:\n  replicas: 5\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 3\n  selector:\n    matchLabels:\n      app: llm-app\n  template:\n    metadata:\n      labels:\n        app: llm-app\n    spec:\n      containers:\n      - name: llm-gpu-app-container\n        image: llm-gpu-application:v2\n        resources:\n          limits:\n            nvidia.com/gpu: 8\n```", "```py\napiVersion: machinelearning.seldon.io/v1alpha2\nkind: SeldonDeployment\nmetadata:\n  name: example-seldon-inference-graph\nspec:\n  name: example-deployment\n  predictors:\n  - componentSpecs:\n    - spec:\n        containers:\n        - name: encoder\n          image: encoder_image:latest\n        - name: LLM\n          image: llm_image:latest\n        - name: classifier\n          image: classifier_image:latest\n        - name: combiner\n          image: combiner_image:latest\n    graph:\n      name: encoder\n      type: MODEL\n      endpoint:\n        type: REST\n      children:\n        - name: combiner\n          type: COMBINER\n          children:\n            - name: LLM\n              type: MODEL\n              endpoint:\n                type: REST\n              children: []\n            - name: classifier\n              type: MODEL\n              endpoint:\n                type: REST\n              children: []\n    name: example\n    replicas: 1\n```", "```py\nimport os\nimport pandas as pd\n\nimport whylogs as why\nfrom langkit import llm_metrics\nfrom datasets import load_dataset\n\nOUTPUT_DIR = \"logs\"\n\nclass LoggingApp:\n    def __init__(self):\n        \"\"\"\n        Sets up a logger that collects profiles and writes them\n        locally every 5 minutes. By setting the schema with langkit\n        we get useful metrics for LLMs.\n        \"\"\"\n        self.logger = why.logger(\n            mode=\"rolling\",\n            interval=5,\n            when=\"M\",\n            base_name=\"profile_\",\n            schema=llm_metrics.init(),\n        )\n        self.logger.append_writer(\"local\", base_dir=OUTPUT_DIR)\n\n    def close(self):\n        self.logger.close()\n\n    def consume(self, text):\n        self.logger.log(text)\n\ndef driver(app):\n    \"\"\"Driver function to run the app manually\"\"\"\n    data = load_dataset(\n        \"shahules786/OA-cornell-movies-dialog\",\n        split=\"train\",\n        streaming=True,\n    )\n    data = iter(data)\n    for text in data:\n        app.consume(text)\n\nif __name__ == \"__main__\":\n    app = LoggingApp()       #1\n    driver(app)\n    app.close()\n\n    pd.set_option(\"display.max_columns\", None)      #2\n\n    all_files = [                #3\n        f for f in os.listdir(OUTPUT_DIR) if f.startswith(\"profile_\")\n    ]\n    path = os.path.join(OUTPUT_DIR, all_files[0])\n    result_view = why.read(path).view()\n    print(result_view.to_pandas().head())\n```", "```py\n# ...\n# column        udf/flesch_reading_ease:cardinality/est\n# conversation                               425.514743\n# ...\n# column        udf/jailbreak_similarity:cardinality/est\n# conversation                               1172.226702\n# ...\n# column        udf/toxicity:types/string  udf/toxicity:types/tensor\n# conversation                          0                          0\n```", "```py\nimport time\nfrom locust import HttpUser, task, events\n\nstat_file = open(\"stats.csv\", \"w\")      #1\nstat_file.write(\"Latency,TTFT,TPS\\n\")\n\nclass StreamUser(HttpUser):\n    @task\n    def generate(self):\n        token_count = 0           #2\n        start = time.time()\n\n        with self.client.post(       #3\n            \"/generate\",\n            data='{\"prompt\": \"Salt Lake City is a\"}',\n            catch_response=True,\n            stream=True,\n        ) as response:\n            first_response = time.time()\n            for line in response.iter_lines(decode_unicode=True):\n                token_count += 1\n\n        end = time.time()          #4\n        latency = end - start\n        ttft = first_response - start\n        tps = token_count / (end - first_response)\n\n        stat_file.write(f\"{latency},{ttft},{tps}\\n\")     #5\n\n# Close stats file when Locust quits\n@events.quitting.add_listener\ndef close_stats_file(environment):\n    stat_file.close()\n```", "```py\n$ locust -f locustfile.py\n> locust.main: Starting web interface at http://0.0.0.0:8089 (accepting\n↪ connections from all network interfaces)\n> locust.main: Starting Locust 2.17.0\n```", "```py\n$ locust -f locustfile.py --host http://0.0.0.0:8000 --csv=llm --\n↪ headless -u 50 -r 1 -t 10m\n```", "```py\nimport json\nimport grpc\nfrom mlserver.codecs.string import StringRequestCodec\nimport mlserver.grpc.converters as converters\nimport mlserver.grpc.dataplane_pb2_grpc as dataplane\nimport mlserver.types as types\n\nmodel_name = \"grpc_model\"\ninputs = {\"message\": \"I'm using gRPC!\"}\n\ninputs_bytes = json.dumps(inputs).encode(\"UTF-8\")     #1\ninference_request = types.InferenceRequest(\n    inputs=[\n        types.RequestInput(\n            name=\"request\",\n            shape=[len(inputs_bytes)],\n            datatype=\"BYTES\",\n            data=[inputs_bytes],\n            parameters=types.Parameters(content_type=\"str\"),\n        )\n    ]\n)\n\nserialized_request = converters.ModelInferRequestConverter.from_types(   #2\n    inference_request, model_name=model_name, model_version=None\n)\n\ngrpc_channel = grpc.insecure_channel(\"localhost:8081\")      #3\ngrpc_stub = dataplane.GRPCInferenceServiceStub(grpc_channel)\nresponse = grpc_stub.ModelInfer(serialized_request)\nprint(response)\n\ndeserialized_response = converters.ModelInferResponseConverter.to_types(    #4\n    response\n)\njson_text = StringRequestCodec.decode_response(deserialized_response)\noutput = json.loads(json_text[0])\nprint(output)\n```", "```py\n$ sky gpunode -p 8888 -c jupyter-vm --gpus l4:2 --cloud gcp --region us-west1\n```"]