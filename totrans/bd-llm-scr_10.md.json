["```py\nimport torch\nfrom chapter04 import GPTModel\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,          #1\n\n    \"context_length\": 256,       #2\n    \"emb_dim\": 768,           #3\n    \"n_heads\": 12,            #4\n    \"n_layers\": 12,           #5\n    \"drop_rate\": 0.1,         #6\n    \"qkv_bias\": False         #7\n}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\nmodel.eval()\n```", "```py\nimport os\nimport urllib.request\n\nfile_path = \"the-verdict.txt\"\n\nurl = (\n    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/\"\n    \"main/ch02/01_main-chapter-code/the-verdict.txt\"\n)\n\nif not os.path.exists(file_path):\n    with urllib.request.urlopen(url) as response:\n        text_data = response.read().decode('utf-8')\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(text_data)\nelse:\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text_data = file.read()\n```", "```py\nfrom previous_chapters import create_dataloader_v1\n\ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntorch.manual_seed(123)\ntrain_loader = create_dataloader_v1(\n    text_data[:split_idx],\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=True,\n    shuffle=True,\n    num_workers=0\n)\nval_loader = create_dataloader_v1(\n    text_data[split_idx:],\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=False,\n    shuffle=False,\n    num_workers=0\n)\n```", "```py\nn_epochs = 15\ninitial_lr = 0.0001\npeak_lr = 0.01\n```", "```py\ntotal_steps = len(train_loader) * n_epochs\nwarmup_steps = int(0.2 * total_steps)       #1\nprint(warmup_steps)\n```", "```py\noptimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\nlr_increment = (peak_lr - initial_lr) / warmup_steps    #1\n\nglobal_step = -1\ntrack_lrs = []\n\nfor epoch in range(n_epochs):    #2\n    for input_batch, target_batch in train_loader:\n        optimizer.zero_grad()\n        global_step += 1\n\n        if global_step < warmup_steps:             #3\n            lr = initial_lr + global_step * lr_increment\n        else:\n            lr = peak_lr\n\n        for param_group in optimizer.param_groups:    #4\n            param_group[\"lr\"] = lr\n        track_lrs.append(optimizer.param_groups[0][\"lr\"])   #5\n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Step\")\ntotal_training_steps = len(train_loader) * n_epochs\nplt.plot(range(total_training_steps), track_lrs);\nplt.show()\n```", "```py\nimport math\n\nmin_lr = 0.1 * initial_lr\ntrack_lrs = []\nlr_increment = (peak_lr - initial_lr) / warmup_steps\nglobal_step = -1\n\nfor epoch in range(n_epochs):\n    for input_batch, target_batch in train_loader:\n        optimizer.zero_grad()\n        global_step += 1\n\n        if global_step < warmup_steps:                     #1\n            lr = initial_lr + global_step * lr_increment  \n        else:                                                #2\n            progress = ((global_step - warmup_steps) / \n                        (total_training_steps - warmup_steps))\n            lr = min_lr + (peak_lr - min_lr) * 0.5 * (\n                1 + math.cos(math.pi * progress)\n            )\n\n        for param_group in optimizer.param_groups:\n            param_group[\"lr\"] = lr\n        track_lrs.append(optimizer.param_groups[0][\"lr\"])\n```", "```py\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Step\")\nplt.plot(range(total_training_steps), track_lrs)\nplt.show()\n```", "```py\nfrom chapter05 import calc_loss_batch\n\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\nloss = calc_loss_batch(input_batch, target_batch, model, device)\nloss.backward()\n```", "```py\ndef find_highest_gradient(model):\n    max_grad = None\n    for param in model.parameters():\n        if param.grad is not None:\n            grad_values = param.grad.data.flatten()\n            max_grad_param = grad_values.max()\n            if max_grad is None or max_grad_param > max_grad:\n                max_grad = max_grad_param\n    return max_grad\nprint(find_highest_gradient(model))\n```", "```py\ntensor(0.0411)\n```", "```py\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\nprint(find_highest_gradient(model))\n```", "```py\ntensor(0.0185)\n```", "```py\nfrom chapter05 import evaluate_model, generate_and_print_sample\n\ndef train_model(model, train_loader, val_loader, optimizer, device,\n                n_epochs, eval_freq, eval_iter, start_context, tokenizer,\n                warmup_steps, initial_lr=3e-05, min_lr=1e-6):\n\n    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n    tokens_seen, global_step = 0, -1\n\n    peak_lr = optimizer.param_groups[0][\"lr\"]   #1\n    total_training_steps = len(train_loader) * n_epochs     #2\n    lr_increment = (peak_lr - initial_lr) / warmup_steps    #3\n\n    for epoch in range(n_epochs):\n        model.train()\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad()\n            global_step += 1\n\n            if global_step < warmup_steps:   #4\n                lr = initial_lr + global_step * lr_increment  \n            else:\n                progress = ((global_step - warmup_steps) / \n                            (total_training_steps - warmup_steps))\n                lr = min_lr + (peak_lr - min_lr) * 0.5 * (\n                    1 + math.cos(math.pi * progress))\n\n            for param_group in optimizer.param_groups:   #5\n                param_group[\"lr\"] = lr\n            track_lrs.append(lr)\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward()\n\n            if global_step >= warmup_steps:         #6\n                torch.nn.utils.clip_grad_norm_(\n                    model.parameters(), max_norm=1.0\n                )\n #7\n            optimizer.step() \n            tokens_seen += input_batch.numel()\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader,\n                    device, eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, \"\n                      f\"Val loss {val_loss:.3f}\"\n                )\n\n        generate_and_print_sample(\n            model, tokenizer, device, start_context\n        )\n\n    return train_losses, val_losses, track_tokens_seen, track_lrs\n```", "```py\nimport tiktoken\n\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\npeak_lr = 0.001\noptimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\nn_epochs = 15\ntrain_losses, val_losses, tokens_seen, lrs = train_model(\n    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\n    eval_freq=5, eval_iter=1, start_context=\"Every effort moves you\",\n    tokenizer=tokenizer, warmup_steps=warmup_steps, \n    initial_lr=1e-5, min_lr=1e-5\n)\n```", "```py\nEp 1 (Iter 000000): Train loss 10.934, Val loss 10.939\nEp 1 (Iter 000005): Train loss 9.151, Val loss 9.461 \nEvery effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nEp 2 (Iter 000010): Train loss 7.949, Val loss 8.184 \nEp 2 (Iter 000015): Train loss 6.362, Val loss 6.876 \nEvery effort moves you,,,,,,,,,,,,,,,,,,, the,,,,,,,,, the,,,,,,,,,,, \nthe,,,,,,,, \n... \nEp 15 (Iter 000130): Train loss 0.041, Val loss 6.915 \nEvery effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n```"]