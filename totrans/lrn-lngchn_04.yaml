- en: Chapter 4\. Using LangGraph to Add Memory to Your Chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580),
    you learned how to provide your AI chatbot application with up-to-date and relevant
    context. This enables your chatbot to generate accurate responses based on the
    user’s input. But that’s not enough to build a production-ready application. How
    can you enable your application to actually “chat” back and forth with the user,
    while remembering prior conversations and relevant context?
  prefs: []
  type: TYPE_NORMAL
- en: Large language models are *stateless*, which means that each time the model
    is prompted to generate a new response it has no memory of the prior prompt or
    model response. In order to provide this historical information to the model,
    we need a robust memory system that will keep track of previous conversations
    and context. This historical information can then be included in the final prompt
    sent to the LLM, thus giving it “memory.” [Figure 4-1](#ch04_figure_1_1736545668257395)
    illustrates this.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a brain  Description automatically generated](assets/lelc_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. Memory and retrieval used to generate context-aware answers from
    an LLM
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to build this essential memory system using
    LangChain’s built-in modules to make this development process easier.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Chatbot Memory System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two core design decisions behind any robust memory system:'
  prefs: []
  type: TYPE_NORMAL
- en: How state is stored
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How state is queried
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A simple way to build a chatbot memory system that incorporates effective solutions
    to these design decisions is to store and reuse the history of all chat interactions
    between the user and the model. The state of this memory system can be:'
  prefs: []
  type: TYPE_NORMAL
- en: Stored as a list of messages (refer to [Chapter 1](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)
    to learn more about messages)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updated by appending recent messages after each turn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appended into the prompt by inserting the messages into the prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 4-2](#ch04_figure_2_1736545668257433) illustrates this simple memory
    system.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a memory  Description automatically generated](assets/lelc_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. A simple memory system utilizing chat history in prompts to generate
    model answers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here’s a code example that illustrates a simple version of this memory system
    using LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note how the incorporation of the previous conversation in the chain enabled
    the model to answer the follow-up question in a context-aware manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this is simple and it works, when taking your application to production,
    you’ll face some more challenges related to managing memory at scale, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to update the memory after every interaction, atomically (i.e.,
    don’t record only the question or only the answer in the case of failure).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ll want to store these memories in durable storage, such as a relational
    database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ll want to control how many and which messages are stored for later, and
    how many of these are used for new interactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ll want to inspect and modify this state (for now, just a list of messages)
    outside a call to an LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll now introduce some better tooling, which will help with this and all later
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LangGraph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the remainder of this chapter and the following chapters, we’ll start to
    make use of [LangGraph](https://oreil.ly/TKCb6), an open source library authored
    by LangChain. LangGraph was designed to enable developers to implement multiactor,
    multistep, stateful cognitive architectures, called *graphs*. That’s a lot of
    words packed into a short sentence; let’s take them one at a time. [Figure 4-3](#ch04_figure_3_1736545668257457)
    illustrates the multiactor aspect.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a computer  Description automatically generated](assets/lelc_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. From single-actor applications to multiactor applications
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A team of specialists can build something together that none of them could
    build alone. The same is true of LLM applications: an LLM prompt (great for answer
    generation and task planning and many more things) is much more powerful when
    paired up with a search engine (best at finding current facts), or even when paired
    with different LLM prompts. We have seen developers build some amazing applications,
    like [Perplexity](https://oreil.ly/bVlu7) or [Arc Search](https://oreil.ly/NPOlF),
    when they combine those two building blocks (and others) in novel ways.'
  prefs: []
  type: TYPE_NORMAL
- en: 'And just as a human team needs more coordination than one person working by
    themselves, an application with multiple actors needs a coordination layer to
    do these things:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the actors involved (the nodes in a graph) and how they hand off work
    to each other (the edges in that graph).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schedule execution of each actor at the appropriate time—in parallel if needed—with
    deterministic results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 4-4](#ch04_figure_4_1736545668257478) illustrates the multistep dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer screen  Description automatically generated](assets/lelc_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. From multiactor to multistep applications
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As each actor hands off work to another (for example, an LLM prompt asking a
    search tool for the results of a given search query), we need to make sense of
    the back-and-forth between multiple actors. We need to know what order it happens
    in, how many times each actor is called, and so on. To do this, we can model the
    interaction between the actors as happening across multiple discrete steps in
    time. When one actor hands off work to another actor, it results in the scheduling
    of the next step of the computation, and so on, until no more actors hand off
    work to others, and the final result is reached.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-5](#ch04_figure_5_1736545668257500) illustrates the stateful aspect.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. From multistep to stateful applications
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Communication across steps requires tracking some state—otherwise, when you
    call the LLM actor the second time, you’d get the same result as the first time.
    It is very helpful to pull this state out of each of the actors and have all actors
    collaborate on updating a single central state. With a single central state, we
    can:'
  prefs: []
  type: TYPE_NORMAL
- en: Snapshot and store the central state during or after each computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pause and resume execution, which makes it easy to recover from errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement human-in-the-loop controls (more on this in [Chapter 8](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each *graph* is then made up of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: State
  prefs: []
  type: TYPE_NORMAL
- en: The data received from outside the application, modified and produced by the
    application while it’s running.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes
  prefs: []
  type: TYPE_NORMAL
- en: Each step to be taken. Nodes are simply Python/JS functions, which receive the
    current state as input and can return an update to that state (that is, they can
    add to it and modify or remove existing data).
  prefs: []
  type: TYPE_NORMAL
- en: Edges
  prefs: []
  type: TYPE_NORMAL
- en: The connections between nodes. Edges determine the path taken from the first
    node to the last, and they can be fixed (that is, after Node B, always visit node
    D) or conditional (evaluate a function to decide the next node to visit after
    node C).
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph offers utilities to visualize these graphs and numerous features to
    debug their workings while in development. These graphs can then easily be deployed
    to serve production workloads at high scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you followed the instructions in [Chapter 1](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004),
    you’ll already have LangGraph installed. If not, you can install it by running
    one of the following commands in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To help get you familiar with using LangGraph, we’ll create a simple chatbot
    using LangGraph, which is a great example of the LLM call architecture with a
    single use of an LLM. This chatbot will respond directly to user messages. Though
    simple, it does illustrate the core concepts of building with LangGraph.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a StateGraph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start by creating a `StateGraph`. We’ll add a node to represent the LLM call:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The first thing you do when you define a graph is define the state of the graph.
    The *state* consists of the shape, or schema, of the graph state, as well as reducer
    functions that specify how to apply updates to the state. In this example, the
    state is a dictionary with a single key: `messages`. The `messages` key is annotated
    with the `add_messages` reducer function, which tells LangGraph to append new
    messages to the existing list, rather than overwrite it. State keys without an
    annotation will be overwritten by each update, storing the most recent value.
    You can write your own reducer functions, which are simply functions that receive
    as arguments—argument 1 is the current state, and argument 2 is the next value
    being written to the state—and should return the next state, that is, the result
    of merging the current state with the new value. The simplest example is a function
    that appends the next value to a list and returns that list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So now our graph knows two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Every `node` we define will receive the current `State` as input and return
    a value that updates that state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`messages` will be *appended* to the current list, rather than directly overwritten.
    This is communicated via the prebuilt [`add_messages`](https://oreil.ly/sK-Ry)
    function in the `Annotated` syntax in the Python example or the reducer function
    for the JavaScript example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, add the `chatbot` node. Nodes represent units of work. They are typically
    just functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This node receives the current state, does one LLM call, and then returns an
    update to the state containing the new message produced by the LLM. The `add_messages`
    reducer appends this message to the messages already in the state.
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally let’s add the edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This does a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: It tells the graph where to start its work each time you run it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This instructs the graph where it should exit (this is optional, as LangGraph
    will stop execution once there’s no more nodes to run).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It compiles the graph into a runnable object, with the familiar `invoke` and
    `stream` methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also draw a visual representation of the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The graph we just made looks like [Figure 4-6](#ch04_figure_6_1736545668257524).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a chatbot  Description automatically generated](assets/lelc_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. A simple chatbot
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can run it with the familiar `stream()` method you’ve seen in earlier chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14] const input = {messages: [new HumanMessage(''hi!)]} for await (const
    chunk of await graph.stream(input)) {   console.log(chunk) } [PRE15] { "chatbot":
    { "messages": [AIMessage("How can I help you?")] } } [PRE16]`  [PRE17] from langgraph.checkpoint.memory
    import MemorySaver  graph = builder.compile(checkpointer=MemorySaver()) [PRE18]
    import {MemorySaver} from ''@langchain/langgraph''  const graph = builder.compile({
    checkpointer: new MemorySaver() }) [PRE19] thread1 = {"configurable": {"thread_id":
    "1"}} result_1 = graph.invoke(     { "messages": [HumanMessage("hi, my name is
    Jack!")] },      thread1 ) // { "chatbot": { "messages": [AIMessage("How can I
    help you, Jack?")] } }  result_2 = graph.invoke(     { "messages": [HumanMessage("what
    is my name?")] },      thread1 ) // { "chatbot": { "messages": [AIMessage("Your
    name is Jack")] } } [PRE20] const thread1 = {configurable: {thread_id: ''1''}}
    const result_1 = await graph.invoke(   { "messages": [new HumanMessage("hi, my
    name is Jack!")] },   thread1 ) // { "chatbot": { "messages": [AIMessage("How
    can I help you, Jack?")] } }  const result_2 = await graph.invoke(   { "messages":
    [new HumanMessage("what is my name?")] },   thread1 ) // { "chatbot": { "messages":
    [AIMessage("Your name is Jack")] } } [PRE21] graph.get_state(thread1) [PRE22]
    await graph.getState(thread1) [PRE23] graph.update_state(thread1, [HumanMessage(''I
    like LLMs!)]) [PRE24]`*JavaScript*    [PRE25]    This would add one more message
    to the list of messages in the state, to be used the next time you invoke the
    graph on this thread.[PRE26]`# Modifying Chat History    In many cases, the chat
    history messages aren’t in the best state or format to generate an accurate response
    from the model. To overcome this problem, we can modify the chat history in three
    main ways: trimming, filtering, and merging messages.    ## Trimming Messages    LLMs
    have limited *context windows*; in other words, there is a maximum number of tokens
    that LLMs can receive as a prompt. As such, the final prompt sent to the model
    shouldn’t exceed that limit (particular to each mode), as models will either refuse
    an overly long prompt or truncate it. In addition, excessive prompt information
    can distract the model and lead to hallucination.    An effective solution to
    this problem is to limit the number of messages that are retrieved from chat history
    and appended to the prompt. In practice, we need only to load and store the most
    recent messages. Let’s use an example chat history with some preloaded messages.    Fortunately,
    LangChain provides the built-in `trim_messages` helper that incorporates various
    strategies to meet these requirements. For example, the trimmer helper enables
    specifying how many tokens we want to keep or remove from chat history.    Here’s
    an example that retrieves the last `max_tokens` in the list of messages by setting
    a strategy parameter to `"last"`:    *Python*    [PRE27]    *JavaScript*    [PRE28]    *The
    output:*    [PRE29]    Note the following:    *   The parameter `strategy` controls
    whether to start from the beginning or the end of the list. Usually, you’ll want
    to prioritize the most recent messages and cut older messages if they don’t fit.
    That is, start from the end of the list. For this behavior, choose the value `last`.
    The other available option is `first`, which would prioritize the oldest messages
    and cut more recent messages if they don’t fit.           *   The `token_counter`
    is an LLM or chat model, which will be used to count tokens using the tokenizer
    appropriate to that model.           *   We can add the parameter `include_system=True`
    to ensure that the trimmer keeps the system message.           *   The parameter
    `allow_partial` determines whether to cut the last message’s content to fit within
    the limit. In our example, we set this to `false`, which completely removes the
    message that would send the total over the limit.           *   The parameter
    `start_on="human"` ensures that we never remove an `AIMessage` (that is, a response
    from the model) without also removing a corresponding `HumanMessage` (the question
    for that response).              ## Filtering Messages    As the list of chat
    history messages grows, a wider variety of types, subchains, and models may be
    utilized. LangChain’s `filter_messages` helper makes it easier to filter the chat
    history messages by type, ID, or name.    Here’s an example where we filter for
    human messages:    *Python*    [PRE30]    *JavaScript*    [PRE31]    *The output:*    [PRE32]    Let’s
    try another example where we filter to exclude users and IDs, and include message
    types:    *Python*    [PRE33]    *JavaScript*    [PRE34]    The `filter_messages`
    helper can also be used imperatively or declaratively, making it easy to compose
    with other components in a chain:    *Python*    [PRE35]    *JavaScript*    [PRE36]    ##
    Merging Consecutive Messages    Certain models don’t support inputs, including
    consecutive messages of the same type (for instance, Anthropic chat models). LangChain’s
    `merge_message_runs` utility makes it easy to merge consecutive messages of the
    same type:    *Python*    [PRE37]    *JavaScript*    [PRE38]    *The output:*    [PRE39]    Notice
    that if the contents of one of the messages to merge is a list of content blocks,
    then the merged message will have a list of content blocks. And if both messages
    to merge have string contents, then those are concatenated with a newline character.    The
    `merge_message_runs` helper can be used imperatively or declaratively, making
    it easy to compose with other components in a chain:    *Python*    [PRE40]    *JavaScript*    [PRE41]    #
    Summary    This chapter covered the fundamentals of building a simple memory system
    that enables your AI chatbot to remember its conversations with a user. We discussed
    how to automate the storage and updating of chat history using LangGraph to make
    this easier. We also discussed the importance of modifying chat history and explored
    various strategies to trim, filter, and summarize chat messages.    In [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774),
    you’ll learn how to enable your AI chatbot to do more than just chat back: for
    instance, your new model will be able to make decisions, pick actions, and reflect
    on its past outputs.[PRE42]``'
  prefs: []
  type: TYPE_NORMAL
