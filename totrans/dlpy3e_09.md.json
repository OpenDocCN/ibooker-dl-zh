["```py\n# Some input tensor\nx = ...\n# Saves a reference to the original input. This is called the residual.\nresidual = x\n# This computation block can potentially be destructive or noisy, and\n# that's fine.\nx = block(x)\n# Adds the original input to the layer's output. The final output will\n# thus always preserve full information about the original input.\nx = add([x, residual]) \n```", "```py\nimport keras\nfrom keras import layers\n\ninputs = keras.Input(shape=(32, 32, 3))\nx = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n# Sets aside the residual\nresidual = x\n# This is the layer around which we create a residual connection: it\n# increases the number of output filers from 32 to 64\\. We use\n# padding=\"same\" to avoid downsampling due to padding.\nx = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n# The residual only had 32 filters, so we use a 1 x 1 Conv2D to project\n# it to the correct shape.\nresidual = layers.Conv2D(64, 1)(residual)\n# Now the block output and the residual have the same shape and can be\n# added.\nx = layers.add([x, residual]) \n```", "```py\ninputs = keras.Input(shape=(32, 32, 3))\nx = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n# Sets aside the residual\nresidual = x\n# This is the block of two layers around which we create a residual\n# connection: it includes a 2 x 2 max pooling layer. We use\n# padding=\"same\" in both the convolution layer and the max pooling\n# layer to avoid downsampling due to padding.\nx = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\nx = layers.MaxPooling2D(2, padding=\"same\")(x)\n# We use strides=2 in the residual projection to match the downsampling\n# created by the max pooling layer.\nresidual = layers.Conv2D(64, 1, strides=2)(residual)\n# Now the block output and the residual have the same shape and can be\n# added.\nx = layers.add([x, residual]) \n```", "```py\ninputs = keras.Input(shape=(32, 32, 3))\nx = layers.Rescaling(1.0 / 255)(inputs)\n\n# Utility function to apply a convolutional block with a residual\n# connection, with an option to add max pooling\ndef residual_block(x, filters, pooling=False):\n    residual = x\n    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n    if pooling:\n        x = layers.MaxPooling2D(2, padding=\"same\")(x)\n        # If we use max pooling, we add a strided convolution to\n        # project the residual to the expected shape.\n        residual = layers.Conv2D(filters, 1, strides=2)(residual)\n    elif filters != residual.shape[-1]:\n        # If we don't use max pooling, we only project the residual if\n        # the number of channels has changed.\n        residual = layers.Conv2D(filters, 1)(residual)\n    x = layers.add([x, residual])\n    return x\n\n# First block\nx = residual_block(x, filters=32, pooling=True)\n# Second block. Note the increasing filter count in each block.\nx = residual_block(x, filters=64, pooling=True)\n# The last block doesn't need a max pooling layer, since we will apply\n# global average pooling right after it.\nx = residual_block(x, filters=128, pooling=False)\n\nx = layers.GlobalAveragePooling2D()(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs) \n```", "```py\n>>> model.summary()\nModel: \"functional\"\n┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)         ┃ Output Shape       ┃    Param # ┃ Connected to        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_2        │ (None, 32, 32, 3)  │          0 │ -                   │\n│ (InputLayer)         │                    │            │                     │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ rescaling (Rescaling)│ (None, 32, 32, 3)  │          0 │ input_layer_2[0][0] │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ conv2d_6 (Conv2D)    │ (None, 32, 32, 32) │        896 │ rescaling[0][0]     │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ conv2d_7 (Conv2D)    │ (None, 32, 32, 32) │      9,248 │ conv2d_6[0][0]      │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ max_pooling2d_1      │ (None, 16, 16, 32) │          0 │ conv2d_7[0][0]      │\n│ (MaxPooling2D)       │                    │            │                     │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ conv2d_8 (Conv2D)    │ (None, 16, 16, 32) │        128 │ rescaling[0][0]     │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ add_2 (Add)          │ (None, 16, 16, 32) │          0 │ max_pooling2d_1[0]… │\n│                      │                    │            │ conv2d_8[0][0]      │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ conv2d_9 (Conv2D)    │ (None, 16, 16, 64) │     18,496 │ add_2[0][0]         │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ conv2d_10 (Conv2D)   │ (None, 16, 16, 64) │     36,928 │ conv2d_9[0][0]      │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ max_pooling2d_2      │ (None, 8, 8, 64)   │          0 │ conv2d_10[0][0]     │\n│ (MaxPooling2D)       │                    │            │                     │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ conv2d_11 (Conv2D)   │ (None, 8, 8, 64)   │      2,112 │ add_2[0][0]         │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ add_3 (Add)          │ (None, 8, 8, 64)   │          0 │ max_pooling2d_2[0]… │\n│                      │                    │            │ conv2d_11[0][0]     │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ conv2d_12 (Conv2D)   │ (None, 8, 8, 128)  │     73,856 │ add_3[0][0]         │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ conv2d_13 (Conv2D)   │ (None, 8, 8, 128)  │    147,584 │ conv2d_12[0][0]     │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ conv2d_14 (Conv2D)   │ (None, 8, 8, 128)  │      8,320 │ add_3[0][0]         │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ add_4 (Add)          │ (None, 8, 8, 128)  │          0 │ conv2d_13[0][0],    │\n│                      │                    │            │ conv2d_14[0][0]     │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ global_average_pool… │ (None, 128)        │          0 │ add_4[0][0]         │\n│ (GlobalAveragePooli… │                    │            │                     │\n├──────────────────────┼────────────────────┼────────────┼─────────────────────┤\n│ dense (Dense)        │ (None, 1)          │        129 │ global_average_poo… │\n└──────────────────────┴────────────────────┴────────────┴─────────────────────┘\n Total params: 297,697 (1.14 MB)\n Trainable params: 297,697 (1.14 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nnormalized_data = (data - np.mean(data, axis=...)) / np.std(data, axis=...) \n```", "```py\nx = ...\n# Because the output of the Conv2D layer gets normalized, the layer\n# doesn't need its own bias vector.\nx = layers.Conv2D(32, 3, use_bias=False)(x)\nx = layers.BatchNormalization()(x) \n```", "```py\nx = layers.Conv2D(32, 3, activation=\"relu\")(x)\nx = layers.BatchNormalization()(x) \n```", "```py\n# Note the lack of activation here.\nx = layers.Conv2D(32, 3, use_bias=False)(x)\nx = layers.BatchNormalization()(x)\n# We place the activation after the BatchNormalization layer.\nx = layers.Activation(\"relu\")(x) \n```", "```py\nimport keras\n\ninputs = keras.Input(shape=(180, 180, 3))\n# Don't forget input rescaling!\nx = layers.Rescaling(1.0 / 255)(inputs)\n# The assumption that underlies separable convolution, \"Feature\n# channels are largely independent,\" does not hold for RGB images! Red,\n# green, and blue color channels are actually highly correlated in\n# natural images. As such, the first layer in our model is a regular\n# `Conv2D` layer. We'll start using `SeparableConv2D` afterward.\nx = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n\n# We apply a series of convolutional blocks with increasing feature\n# depth. Each block consists of two batch-normalized depthwise\n# separable convolution layers and a max pooling layer, with a residual\n# connection around the entire block.\nfor size in [32, 64, 128, 256, 512]:\n    residual = x\n\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n\n    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n    residual = layers.Conv2D(\n        size, 1, strides=2, padding=\"same\", use_bias=False\n    )(residual)\n    x = layers.add([x, residual])\n\n# In the original model, we used a Flatten layer before the Dense\n# layer. Here, we go with a GlobalAveragePooling2D layer.\nx = layers.GlobalAveragePooling2D()(x)\n# Like in the original model, we add a dropout layer for\n# regularization.\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs) \n```"]