- en: Interpreting what ConvNets learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释卷积神经网络学习的内容
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter10_interpreting-what-convnets-learn](https://deeplearningwithpython.io/chapters/chapter10_interpreting-what-convnets-learn)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter10_interpreting-what-convnets-learn](https://deeplearningwithpython.io/chapters/chapter10_interpreting-what-convnets-learn)
- en: 'A fundamental problem when building a computer vision application is that of
    *interpretability*: *Why* did your classifier think a particular image contained
    a fridge, when all you can see is a truck? This is especially relevant to use
    cases where deep learning is used to complement human expertise, such as medical
    imaging use cases. This chapter will get you familiar with a range of different
    techniques for visualizing what ConvNets learn and understanding the decisions
    they make.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建计算机视觉应用时，一个基本问题是**可解释性**：为什么你的分类器认为某个图像包含冰箱，而你只能看到一辆卡车？这在深度学习用于补充人类专业知识的应用场景中尤为重要，例如医学影像应用场景。本章将使你熟悉一系列不同的技术，用于可视化卷积神经网络学习的内容以及理解它们的决策。
- en: 'It’s often said that deep learning models are “black boxes”: they learn representations
    that are difficult to extract and present in a human-readable form. Although this
    is partially true for certain types of deep learning models, it’s definitely not
    true for ConvNets. The representations learned by ConvNets are highly amenable
    to visualization, in large part because they’re *representations of visual concepts*.
    Since 2013, a wide array of techniques has been developed for visualizing and
    interpreting these representations. We won’t survey all of them, but we’ll cover
    three of the most accessible and useful ones:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常说深度学习模型是“黑盒”：它们学习到的表示难以提取并以人类可读的形式呈现。虽然这在某些类型的深度学习模型中部分正确，但对于卷积神经网络来说绝对不是这样。卷积神经网络学习到的表示高度适合可视化，这在很大程度上是因为它们是**视觉概念的表示**。自2013年以来，已经开发出大量技术来可视化和解释这些表示。我们不会对所有这些技术进行综述，但我们将介绍三种最易于获取和最有用的技术：
- en: '*Visualizing intermediate ConvNet outputs (intermediate activations)* — Useful
    for understanding how successive ConvNet layers transform their input, and for
    getting a first idea of the meaning of individual ConvNet filters'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化中间卷积神经网络输出（中间激活）*——有助于理解连续的卷积神经网络层如何转换它们的输入，以及获得对单个卷积神经网络滤波器意义的初步了解'
- en: '*Visualizing ConvNets filters* — Useful for understanding precisely what visual
    pattern or concept each filter in a ConvNet is receptive to'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化卷积神经网络滤波器*——有助于理解卷积神经网络中的每个滤波器对精确的视觉模式或概念的反应'
- en: '*Visualizing heatmaps of class activation in an image* — Useful for understanding
    which parts of an image were identified as belonging to a given class, thus allowing
    you to localize objects in images'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化图像中类别激活的热图*——有助于理解图像的哪些部分被识别为属于给定类别，从而允许你在图像中定位对象'
- en: For the first method — activation visualization — you’ll use the small ConvNet
    that you trained from scratch on the dogs-versus-cats classification problem in
    chapter 8\. For the next two methods, you’ll use a pretrained Xception model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一种方法——激活可视化——你将使用你在第8章从头开始训练的小型卷积神经网络，用于处理狗与猫的分类问题。对于接下来的两种方法，你将使用预训练的Xception模型。
- en: Visualizing intermediate activations
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化中间激活
- en: 'Visualizing intermediate activations consists of displaying the values returned
    by various convolution and pooling layers in a model, given a certain input (the
    output of a layer is often called its *activation*, the output of the activation
    function). This gives a view into how an input is decomposed into the different
    filters learned by the network. You want to visualize feature maps with three
    dimensions: width, height, and depth (channels). Each channel encodes relatively
    independent features, so the proper way to visualize these feature maps is by
    independently plotting the contents of every channel as a 2D image. Let’s start
    by loading the model that you saved in section 8.2:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化中间激活包括显示模型中各种卷积和池化层在给定输入（层的输出通常称为其**激活**，激活函数的输出称为**激活值**）下返回的值。这提供了了解输入如何分解为网络学习到的不同滤波器的视角。你想要可视化的特征图有三个维度：宽度、高度和深度（通道）。每个通道编码相对独立的特点，因此可视化这些特征图的正确方法是独立地将每个通道的内容作为二维图像绘制出来。让我们先加载你在第8.2节中保存的模型：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, you’ll get an input image — a picture of a cat, not part of the images
    the network was trained on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将获得一个输入图像——一张猫的图片，这不是网络训练图像的一部分。
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 10.1](#listing-10-1): Preprocessing a single image'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单10.1](#listing-10-1)：预处理单个图像'
- en: Let’s display the picture (see figure 10.1).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示这张图片（见图10.1）。
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 10.2](#listing-10-2): Displaying the test picture'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单10.2](#listing-10-2)：显示测试图片'
- en: '![](../Images/33eaa28cbb2d03a5c08f20e412a915d0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33eaa28cbb2d03a5c08f20e412a915d0.png)'
- en: '[Figure 10.1](#figure-10-1): The test cat picture'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.1](#figure-10-1)：测试猫图片'
- en: To extract the feature maps you want to look at, you’ll create a Keras model
    that takes batches of images as input and outputs the activations of all convolution
    and pooling layers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要提取你想要查看的特征图，你需要创建一个Keras模型，该模型以图像批次作为输入，并输出所有卷积和池化层的激活。
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 10.3](#listing-10-3): Instantiating a model that returns layer activations'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单10.3](#listing-10-3)：实例化返回层激活的模型'
- en: 'When fed an image input, this model returns the values of the layer activations
    in the original model, as a list. This is the first time you’ve encountered a
    multi-output model in this book in practice since you learned about them in chapter
    7: until now, the models you’ve seen have had exactly one input and one output.
    This one has one input and nine outputs — one output per layer activation.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入图像时，此模型返回原始模型中层激活的值，作为一个列表。这是你在第7章学习了多输出模型后，在本书中第一次在实践中遇到多输出模型：到目前为止，你看到的模型都有且只有一个输入和一个输出。这个模型有一个输入和九个输出——每个层的激活对应一个输出。
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 10.4](#listing-10-4): Using the model to compute layer activations'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单10.4](#listing-10-4)：使用模型计算层激活'
- en: 'For instance, this is the activation of the first convolution layer for the
    cat image input:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这是猫图像输入的第一卷积层的激活：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It’s a 178 × 178 feature map with 32 channels. Let’s try plotting the sixth
    channel of the activation of the first layer of the original model (see figure
    10.2).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个178 × 178的特征图，有32个通道。让我们尝试绘制原始模型第一层激活的第六通道（见图10.2）。
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 10.5](#listing-10-5): Visualizing the sixth channel'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单10.5](#listing-10-5)：可视化第六通道'
- en: '![](../Images/b376db7cebc68395f5678bdead7f2d5d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b376db7cebc68395f5678bdead7f2d5d.png)'
- en: '[Figure 10.2](#figure-10-2): Sixth channel of the activation of the first layer
    on the test cat picture'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.2](#figure-10-2)：测试猫图片第一层的激活第六通道'
- en: This channel appears to encode a diagonal edge detector, but note that your
    own channels may vary because the specific filters learned by convolution layers
    aren’t deterministic.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个通道似乎编码了一个对角线边缘检测器，但请注意，你自己的通道可能会有所不同，因为卷积层学习的特定滤波器不是确定的。
- en: Now let’s plot a complete visualization of all the activations in the network
    (see figure 10.3). We’ll extract and plot every channel in each of the layer activations,
    and we’ll stack the results in one big grid, with channels stacked side by side.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制网络中所有激活的完整可视化（见图10.3）。我们将提取并绘制每个层激活中的每个通道，并将结果堆叠在一个大网格中，通道并排堆叠。
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 10.6](#listing-10-6): Visualizing every channel in every intermediate
    activation'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单10.6](#listing-10-6)：可视化每个中间激活中的每个通道'
- en: '![](../Images/449cd612e2702fb4072369056fb379ca.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/449cd612e2702fb4072369056fb379ca.png)'
- en: '[Figure 10.3](#figure-10-3): Every channel of every layer activation on the
    test cat picture'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.3](#figure-10-3)：测试猫图片上每个层的每个通道的激活'
- en: 'There are a few things to note here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些需要注意的事项：
- en: The first layer acts as a collection of various edge detectors. At that stage,
    the activations retain almost all of the information present in the initial picture.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层充当各种边缘检测器的集合。在那个阶段，激活保留了初始图片中几乎所有信息。
- en: As you go higher, the activations become increasingly abstract and less visually
    interpretable. They begin to encode higher-level concepts such as “cat ear” and
    “cat eye.” Higher representations carry increasingly less information about the
    visual contents of the image and increasingly more information related to the
    class of the image.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着你向上移动，激活变得越来越抽象，视觉上可解释性越来越低。它们开始编码更高级的概念，如“猫耳”和“猫眼”。更高层的表示携带越来越少的关于图像视觉内容的信息，以及越来越多的与图像类别相关的信息。
- en: 'The sparsity of the activations increases with the depth of the layer: in the
    first layer, all filters are activated by the input image, but in the following
    layers, more and more filters are blank. This means the pattern encoded by the
    filter isn’t found in the input image.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活稀疏度随着层深度的增加而增加：在第一层，所有滤波器都由输入图像激活，但在后续层中，越来越多的滤波器变为空白。这意味着滤波器编码的图案在输入图像中没有找到。
- en: 'We have just observed an important universal characteristic of the representations
    learned by deep neural networks: the features extracted by a layer become increasingly
    abstract with the depth of the layer. The activations of higher layers carry less
    and less information about the specific input being seen and more and more information
    about the target (in this case, the class of the image: cat or dog). A deep neural
    network effectively acts as an *information distillation pipeline*, with raw data
    going in (in this case, RGB pictures) and being repeatedly transformed so that
    irrelevant information is filtered out (for example, the specific visual appearance
    of the image) and useful information is magnified and refined (for example, the
    class of the image).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚观察到了深度神经网络学习到的表示的一个重要普遍特征：层中提取的特征随着层深度的增加而变得越来越抽象。更高层的激活携带越来越少关于特定输入的信息，而越来越多关于目标（在这种情况下，图像的类别：猫或狗）的信息。深度神经网络有效地充当一个*信息蒸馏管道*，原始数据进入（在这种情况下，RGB图片），并被反复转换，以便过滤掉无关信息（例如，图像的具体视觉外观），并放大和细化有用信息（例如，图像的类别）。
- en: 'This is analogous to the way humans and animals perceive the world: after observing
    a scene for a few seconds, a human can remember which abstract objects were present
    in it (bicycle, tree) but can’t remember the specific appearance of these objects.
    In fact, if you tried to draw a generic bicycle from memory, chances are you couldn’t
    get it even remotely right, even though you’ve seen thousands of bicycles in your
    lifetime (see, for example, figure 10.4). Try it right now: this effect is absolutely
    real. Your brain has learned to completely abstract its visual input — to transform
    it into high-level visual concepts while filtering out irrelevant visual details
    — making it tremendously difficult to remember how things around you look.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这与人类和动物感知世界的方式类似：观察场景几秒钟后，人类可以记住其中包含哪些抽象对象（自行车、树），但无法记住这些对象的具体外观。事实上，如果你试图凭记忆画一辆通用的自行车，你很可能连大致的样子都画不出来，尽管你一生中见过成千上万辆自行车（例如，见图10.4）。现在试试看：这种效果绝对是真实的。你的大脑已经学会了完全抽象其视觉输入——将其转化为高级视觉概念，同时过滤掉无关的视觉细节——这使得记住周围事物的外观变得极其困难。
- en: '![](../Images/8118eccf021ef647fb2a0c668c897a7c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8118eccf021ef647fb2a0c668c897a7c.png)'
- en: '[Figure 10.4](#figure-10-4): Left: Attempts to draw a bicycle from memory.
    Right: What a schematic bicycle should look like.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.4](#figure-10-4)：左：尝试凭记忆画一辆自行车。右：一个示意图自行车应该看起来像什么。'
- en: Visualizing ConvNet filters
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化ConvNet滤波器
- en: Another easy way to inspect the filters learned by ConvNets is to display the
    visual pattern that each filter is meant to respond to. This can be done with
    *gradient ascent in input space*, applying *gradient descent* to the value of
    the input image of a ConvNet so as to *maximize* the response of a specific filter,
    starting from a blank input image. The resulting input image will be one that
    the chosen filter is maximally responsive to.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 检查ConvNets学习到的滤波器的一个简单方法是通过显示每个滤波器旨在响应的视觉模式。这可以通过在输入空间中进行*梯度上升*来实现，对ConvNet的输入图像的值应用*梯度下降*，以*最大化*特定滤波器的响应，从空白输入图像开始。结果输入图像将是选择滤波器响应最大的图像。
- en: 'Let’s try this with the filters of the Xception model. The process is simple:
    we’ll build a loss function that maximizes the value of a given filter in a given
    convolution layer, and then we’ll use stochastic gradient descent to adjust the
    values of the input image so as to maximize this activation value. This will be
    your second example of a low-level gradient descent loop (the first one was in
    chapter 2). We will show it for TensorFlow, PyTorch, and Jax.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用Xception模型的滤波器来尝试这个方法。过程很简单：我们将构建一个损失函数，该函数最大化给定卷积层中给定滤波器的值，然后我们将使用随机梯度下降来调整输入图像的值，以最大化这个激活值。这将是你的第二个低级梯度下降循环的例子（第一个在第2章中）。我们将为TensorFlow、PyTorch和Jax展示它。
- en: First, let’s instantiate the Xception model trained on the ImageNet dataset.
    We can once again use the KerasHub library, exactly as we did in chapter 8.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实例化在ImageNet数据集上训练的Xception模型。我们可以再次使用KerasHub库，就像我们在第8章中所做的那样。
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 10.7](#listing-10-7): Instantiating the Xception convolutional base'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表10.7](#listing-10-7)：实例化Xception卷积基'
- en: We’re interested in the convolutional layers of the model — the `Conv2D` and
    `SeparableConv2D` layers. We’ll need to know their names so we can retrieve their
    outputs. Let’s print their names, in order of depth.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型的卷积层感兴趣——`Conv2D`和`SeparableConv2D`层。我们需要知道它们的名称，以便检索它们的输出。让我们按深度顺序打印它们的名称。
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Listing 10.8](#listing-10-8): Printing the names of all convolutional layers
    in Xception'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表10.8](#listing-10-8)：打印Xception中所有卷积层的名称'
- en: You’ll notice that the `SeparableConv2D` layers here are all named something
    like `block6_sepconv1`, `block7_sepconv2`, etc. — Xception is structured into
    blocks, each containing several convolutional layers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这里的`SeparableConv2D`层都命名为类似`block6_sepconv1`、`block7_sepconv2`等——Xception被结构化为块，每个块包含几个卷积层。
- en: 'Now let’s create a second model that returns the output of a specific layer
    — a “feature extractor” model. Because our model is a Functional API model, it
    is inspectable: you can query the `output` of one of its layers and reuse it in
    a new model. No need to copy the entire Xception code.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建第二个模型，该模型返回特定层的输出——“特征提取器”模型。由于我们的模型是功能API模型，它是可检查的：你可以查询其某个层的`output`并在新模型中重用它。无需复制整个Xception代码。
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 10.9](#listing-10-9): A feature extractor model returning a specific
    output'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表10.9](#listing-10-9)：返回特定输出的特征提取器模型'
- en: To use this model, we can simply call it on some input data, but we should be
    careful to apply our model-specific image preprocessing so that our images are
    scaled to the same range as the Xception pretraining data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此模型，我们只需在输入数据上调用它即可，但我们应该小心应用我们模型特有的图像预处理，以确保我们的图像被缩放到与Xception预训练数据相同的范围。
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Listing 10.10](#listing-10-10): Using the feature extractor'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表10.10](#listing-10-10)：使用特征提取器'
- en: 'Let’s use our feature extractor model to define a function that returns a scalar
    value quantifying how much a given input image “activates” a given filter in the
    layer. This is the loss function that we’ll maximize during the gradient ascent
    process:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的特征提取器模型定义一个函数，该函数返回一个标量值，量化给定输入图像在层中激活给定滤波器的程度。这是我们将在梯度上升过程中最大化的损失函数：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A non-obvious trick to help the gradient-ascent process go smoothly is to normalize
    the gradient tensor by dividing it by its L2 norm (the square root of the sum
    of the squares of the values in the tensor). This ensures that the magnitude of
    the updates done to the input image is always within the same range.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有助于梯度上升过程顺利进行的非明显技巧是将梯度张量通过除以其L2范数（张量中值的平方和的平方根）进行归一化。这确保了对输入图像所做的更新幅度始终在相同的范围内。
- en: Let’s set up the gradient ascent step function. Anything that involves gradients
    requires calling backend-level APIs, such as `GradientTape` in TensorFlow, `.backward()`
    in PyTorch, and `jax.grad()` in JAX. Let’s line up all the code snippets for each
    of the three backends, starting with TensorFlow.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置梯度上升步骤函数。任何涉及梯度的操作都需要调用后端API，例如TensorFlow中的`GradientTape`、PyTorch中的`.backward()`和JAX中的`jax.grad()`。让我们排列出三个后端中每个后端的代码片段，从TensorFlow开始。
- en: Gradient ascent in TensorFlow
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow中的梯度上升
- en: 'For TensorFlow, we can just open a `GradientTape` scope and compute the loss
    inside of it to retrieve the gradients we need. We’ll use a `@tf.function` decorator
    to speed up computation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于TensorFlow，我们只需打开一个`GradientTape`作用域，在其中计算损失以检索所需的梯度。我们将使用`@tf.function`装饰器来加速计算：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Listing 10.11](#listing-10-11): Loss maximization via stochastic gradient
    ascent: TensorFlow'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表10.11](#listing-10-11)：通过随机梯度上升进行损失最大化：TensorFlow'
- en: Gradient ascent in PyTorch
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch中的梯度上升
- en: In the case of PyTorch, we use `loss.backward()` and `image.grad` to obtain
    the gradients of the loss with respect to the input image, like this.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch的情况下，我们使用`loss.backward()`和`image.grad`来获取相对于输入图像的损失梯度，如下所示。
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[Listing 10.12](#listing-10-12): Loss maximization via stochastic gradient
    ascent: PyTorch'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表10.12](#listing-10-12)：通过随机梯度上升进行损失最大化：PyTorch'
- en: No need to reset the gradients since the image tensor is recreated at each iteration.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像张量在每个迭代中都会被重新创建，因此无需重置梯度。
- en: Gradient ascent in JAX
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX中的梯度上升
- en: In the case of JAX, we use `jax.grad()` to obtain a function that returns the
    gradients of the loss with respect to the input image.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在JAX的情况下，我们使用`jax.grad()`来获取一个函数，该函数返回相对于输入图像的损失梯度。
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 10.13](#listing-10-13): Loss maximization via stochastic gradient
    ascent: JAX'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表10.13](#listing-10-13)：通过随机梯度上升进行损失最大化：JAX'
- en: The filter visualization loop
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤器可视化循环
- en: Now you have all the pieces. Let’s put them together into a Python function
    that takes a filter index as input and returns a tensor representing the pattern
    that maximizes the activation of the specified filter in our target layer.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经拥有了所有部件。让我们将它们组合成一个 Python 函数，该函数接受一个过滤器索引作为输入，并返回一个表示在目标层中最大化指定过滤器激活的模式张量。
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Listing 10.14](#listing-10-14): Function to generate filter visualizations'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.14](#listing-10-14)：生成过滤器可视化的函数'
- en: The resulting image tensor is a floating-point array of shape `(200, 200, 3)`,
    with values that may not be integers within `[0, 255]`. Hence, you need to post-process
    this tensor to turn it into a displayable image. You do so with the following
    straightforward utility function.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的图像张量是一个形状为 `(200, 200, 3)` 的浮点数组，其值可能不在 `[0, 255]` 的整数范围内。因此，您需要后处理这个张量，将其转换为可显示的图像。您可以使用以下简单的实用函数来完成此操作。
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Listing 10.15](#listing-10-15): Utility function to convert a tensor into
    a valid image'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.15](#listing-10-15)：将张量转换为有效图像的实用函数'
- en: 'Let’s try it (see figure 10.5):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试（见图 10.5）：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/ab3bf7e0bbde66c050003a258e160303.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab3bf7e0bbde66c050003a258e160303.png)'
- en: '[Figure 10.5](#figure-10-5): Pattern that the second channel in layer `block3_sepconv1`
    responds to maximally'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.5](#figure-10-5)：层 `block3_sepconv1` 第二通道响应最大的模式'
- en: It seems that filter 2 in layer `block3_sepconv1` is responsive to a horizontal
    lines pattern, somewhat water-like or fur-like.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，层 `block3_sepconv1` 中的过滤器 2 对水平线图案有响应，有点像水或毛发的样子。
- en: 'Now the fun part: you can start visualizing every filter in the layer — and
    even every filter in every layer in the model (see figure 10.6).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候进行有趣的部分了：您可以从可视化层中的每个过滤器开始——甚至可以可视化模型中每一层的每个过滤器（见图 10.6）。
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Listing 10.16](#listing-10-16): Generating a grid of all filter response patterns'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.16](#listing-10-16)：生成所有过滤器响应模式的网格'
- en: '![](../Images/293a80383ac80c0fb7f099520dd61ead.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/293a80383ac80c0fb7f099520dd61ead.png)'
- en: '[Figure 10.6](#figure-10-6): Some filter patterns for layers `block2_sepconv1`,
    `block4_sepconv1`, and `block8_sepconv1`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.6](#figure-10-6)：`block2_sepconv1`、`block4_sepconv1` 和 `block8_sepconv1`
    层的一些过滤器模式'
- en: 'These filter visualizations tell you a lot about how ConvNet layers see the
    world: each layer in a ConvNet learns a collection of filters such that their
    inputs can be expressed as a combination of the filters. This is similar to how
    the Fourier transform decomposes signals onto a bank of cosine functions. The
    filters in these ConvNet filter banks get increasingly complex and refined as
    you go higher in the model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些过滤器可视化向您展示了卷积神经网络层如何观察世界：卷积神经网络中的每一层都学习一组过滤器，使得其输入可以表示为这些过滤器的组合。这类似于傅里叶变换将信号分解到一系列余弦函数中。随着你在模型中向上移动，这些卷积神经网络过滤器库中的过滤器变得越来越复杂和精细：
- en: The filters from the first layers in the model encode simple directional edges
    and colors (or colored edges, in some cases).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型第一层的过滤器编码简单的方向边缘和颜色（或在某些情况下，彩色边缘）。
- en: The filters from layers a bit further up the stack, such as `block4_sepconv1`,
    encode simple textures made from combinations of edges and colors.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 栈中更高层的层，如 `block4_sepconv1`，编码由边缘和颜色组合而成的简单纹理。
- en: 'The filters in higher layers begin to resemble textures found in natural images:
    feathers, eyes, leaves, and so on.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高层中的过滤器开始类似于自然图像中找到的纹理：羽毛、眼睛、叶子等等。
- en: Visualizing heatmaps of class activation
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化类别激活的热图
- en: Here’s one last visualization technique — one that is useful for understanding
    which parts of a given image led a ConvNet to its final classification decision.
    This is helpful for “debugging” the decision process of a ConvNet, particularly
    in the case of a classification mistake (a problem domain called *model interpretability*).
    It can also allow you to locate specific objects in an image.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是最后一种可视化技术——一种有助于理解给定图像的哪些部分导致卷积神经网络做出最终分类决策的技术。这对于“调试”卷积神经网络的决策过程非常有用，尤其是在分类错误的情况下（一个被称为
    *模型可解释性* 的领域问题）。它还可以让您在图像中定位特定对象。
- en: This general category of techniques is called *class activation map* (CAM) visualization,
    and it consists of producing heatmaps of class activation over input images. A
    class activation heatmap is a 2D grid of scores associated with a specific output
    class, computed for every location in any input image, indicating how important
    each location is with respect to the class under consideration. For instance,
    given an image fed into a dogs-versus-cats ConvNet, CAM visualization would allow
    you to generate a heatmap for the class “cat,” indicating how cat-like different
    parts of the image are, and also a heatmap for the class “dog,” indicating how
    dog-like parts of the image are. The specific implementation we’ll use is the
    one described in Selvaraju et al.^([[1]](#footnote-1))
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这种一般的技术类别被称为 *类别激活图*（CAM）的可视化，它包括在输入图像上生成类别激活的热图。一个类别激活热图是与特定输出类别相关联的分数的 2D
    网格，为任何输入图像中的每个位置计算，指示每个位置相对于考虑的类别的相对重要性。例如，给定一个输入到狗与猫的 ConvNet 的图像，CAM 可视化将允许你为“猫”类别生成热图，指示图像的不同部分有多像猫，以及为“狗”类别生成热图，指示图像的不同部分有多像狗。我们将使用的具体实现是
    Selvaraju 等人描述的（[[1]](#footnote-1))。
- en: Grad-CAM consists of taking the output feature map of a convolution layer, given
    an input image, and weighting every channel in that feature map by the gradient
    of the class with respect to the channel. Intuitively, one way to understand this
    trick is that you’re weighting a spatial map of “how intensely the input image
    activates different channels” by “how important each channel is with regard to
    the class,” resulting in a spatial map of “how intensely the input image activates
    the class.”
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Grad-CAM 包括取一个卷积层的输出特征图，给定一个输入图像，并按类别相对于通道的梯度对该特征图中的每个通道进行加权。直观地说，理解这个技巧的一种方式是，你通过“每个通道相对于类别的相对重要性”来加权“输入图像激活不同通道的强度”的空间图，从而得到一个“输入图像激活类别的强度”的空间图。
- en: Let’s demonstrate this technique using the pretrained Xception model. Consider
    the image of two African elephants shown in figure 10.7, possibly a mother and
    her calf, strolling in the savanna. We can start by downloading this image and
    converting it to a NumPy array, as shown in figure 10.7.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用预训练的 Xception 模型来演示这项技术。考虑图 10.7 中显示的两只非洲象的图像，可能是一只母象和她的幼崽，在草原上漫步。我们可以从下载这张图像并将其转换为
    NumPy 数组开始，如图 10.7 所示。
- en: '![](../Images/be45d44f9634de90e89e016dbd9ed545.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/be45d44f9634de90e89e016dbd9ed545.png)'
- en: '[Figure 10.7](#figure-10-7): Test picture of African elephants'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.7](#figure-10-7)：非洲象的测试图片'
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Listing 10.17](#listing-10-17): Preprocessing an input image for Xception'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 10.17](#listing-10-17)：为 Xception 预处理输入图像'
- en: So far, we have only used KerasHub to instantiate a pretrained feature extractor
    network using the backbone class. For Grad-CAM, we need the entire Xception model
    including the classification head — recall that Xception was trained on the ImageNet
    dataset with ~1 million labeled images belonging to 1,000 different classes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了 KerasHub 来实例化一个使用骨干类预训练的特征提取网络。对于 Grad-CAM，我们需要整个 Xception 模型，包括分类头——回想一下，Xception
    是在 ImageNet 数据集上训练的，该数据集包含约 100 万张标签图像，属于 1,000 个不同的类别。
- en: 'KerasHub provides a high-level *task* API for common end-to-end workflows like
    image classification, text classification, image generation, and so on. A task
    wraps preprocessing, a feature extraction network, and a task-specific head into
    a single class that is easy to use. Let’s try it out:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: KerasHub 提供了一个高级 *任务* API，用于常见的端到端工作流程，如图像分类、文本分类、图像生成等。一个任务将预处理、特征提取网络和特定任务的头封装成一个易于使用的单个类。让我们试试看：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The top five classes predicted for this image are as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此图像预测的前五个类别如下：
- en: African elephant (with 90% probability)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非洲象（90% 的概率）
- en: Tusker (with 5% probability)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非洲象（5% 的概率）
- en: Indian elephant (with 2% probability)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚洲象（2% 的概率）
- en: Triceratops and Mexican hairless dog with less than 0.1% probability
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三角龙和墨西哥无毛犬，概率低于 0.1%
- en: 'The network has recognized the image as containing an undetermined quantity
    of African elephants. The entry in the prediction vector that was maximally activated
    is the one corresponding to the “African elephant” class, at index 386:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 网络已识别出图像包含不确定数量的非洲象。预测向量中激活程度最高的条目是对应于“非洲象”类别的，索引为 386：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: To visualize which parts of the image are the most African elephant–like, let’s
    set up the Grad-CAM process.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化图像中最像非洲象的部分，让我们设置 Grad-CAM 的过程。
- en: 'You will note that we didn’t need to preprocess our image before calling the
    task model. That’s because the KerasHub `ImageClassifier` is preprocessing inputs
    for us as part of `predict()`. Let’s preprocess the image ourselves so we can
    use the preprocessed inputs directly:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，在调用任务模型之前，我们不需要对图像进行预处理。这是因为 KerasHub 的 `ImageClassifier` 在 `predict()`
    方法中为我们预处理输入。让我们自己预处理图像，以便我们可以直接使用预处理后的输入：
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: First, we create a model that maps the input image to the activations of the
    last convolutional layer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个模型，将输入图像映射到最后一层卷积层的激活。
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[Listing 10.18](#listing-10-18): Returning the last convolutional output'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.18](#listing-10-18)：返回最后一层卷积输出'
- en: Second, we create a model that maps the activations of the last convolutional
    layer to the final class predictions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们创建一个模型，将最后一层卷积层的激活映射到最终的类别预测。
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Listing 10.19](#listing-10-19): Going from the last convolutional output to
    final predictions'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.19](#listing-10-19)：从最后一层卷积输出到最终预测'
- en: Then, we compute the gradient of the top predicted class for our input image
    with respect to the activations of the last convolution layer. Once again, having
    to compute gradients means we have to use backend APIs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算输入图像相对于最后一层卷积层激活的顶级预测类梯度。再次强调，需要计算梯度意味着我们必须使用后端 API。
- en: 'Getting the gradient of the top class: TensorFlow version'
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取顶级类梯度：TensorFlow 版本
- en: Let’s start with the TensorFlow version, once again using `GradientTape`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 TensorFlow 版本开始，再次使用 `GradientTape`。
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Listing 10.20](#listing-10-20): Computing the top class gradients with TensorFlow'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.20](#listing-10-20)：使用 TensorFlow 计算顶级类梯度'
- en: 'Getting the gradient of the top class: PyTorch version'
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取顶级类梯度：PyTorch 版本
- en: Next, here’s the PyTorch version, using `.backward()` and `.grad`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这是 PyTorch 版本，使用 `.backward()` 和 `.grad`。
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Listing 10.21](#listing-10-21): Computing the top class gradients with PyTorch'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.21](#listing-10-21)：使用 PyTorch 计算顶级类梯度'
- en: 'Getting the gradient of the top class: JAX version'
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取顶级类梯度：JAX 版本
- en: Finally, let’s do JAX. We define a separate loss computation function that takes
    the final layer’s output and returns the activation channel corresponding to the
    top predicted class. We use this activation value as our loss, allowing us to
    compute the gradient.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来做 JAX。我们定义一个单独的损失计算函数，它接受最终层的输出并返回对应于顶级预测类的激活通道。我们使用这个激活值作为我们的损失，从而计算梯度。
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[Listing 10.22](#listing-10-22): Computing the top class gradients with Jax'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.22](#listing-10-22)：使用 Jax 计算顶级类梯度'
- en: Displaying the class activation heatmap
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 显示类别激活热图
- en: Now, we apply pooling and importance weighting to the gradient tensor to obtain
    our heatmap of class activation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将池化和重要性权重应用于梯度张量，以获得我们的类别激活热图。
- en: '[PRE29]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[Listing 10.23](#listing-10-23): Gradient pooling and channel importance weighting'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.23](#listing-10-23)：梯度池化和通道重要性权重'
- en: For visualization purposes, you’ll also normalize the heatmap between 0 and
    1. The result is shown in figure 10.8.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化目的，你还需要将热图归一化到 0 到 1 之间。结果如图 10.8 所示。
- en: '[PRE30]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[Listing 10.24](#listing-10-24): Heatmap post-processing'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.24](#listing-10-24)：热图后处理'
- en: '![](../Images/c32d227d3557544e6058ccb7c8a6ab71.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/c32d227d3557544e6058ccb7c8a6ab71.png)'
- en: '[Figure 10.8](#figure-10-8): Standalone class activation heatmap'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.8](#figure-10-8)：独立的类别激活热图'
- en: Finally, let’s generate an image that superimposes the original image on the
    heatmap you just obtained (see figure 10.9).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们生成一个图像，将原始图像叠加到你刚刚获得的热图上（见图 10.9）。
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[Listing 10.25](#listing-10-25): Superimposing the heatmap with the original
    picture'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.25](#listing-10-25)：将热图与原始图像叠加'
- en: '![](../Images/7ce2ad2a03c71201ecc93b19562ffa7b.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/7ce2ad2a03c71201ecc93b19562ffa7b.png)'
- en: '[Figure 10.9](#figure-10-9): African elephant class activation heatmap over
    the test picture'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.9](#figure-10-9)：测试图片上的非洲象类别激活热图'
- en: 'This visualization technique answers two important questions:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化技术回答了两个重要问题：
- en: Why did the network think this image contained an African elephant?
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么网络认为这张图片包含非洲象？
- en: Where is the African elephant located in the picture?
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图片中非洲象在哪里？
- en: 'In particular, it’s interesting to note that the ears of the elephant calf
    are strongly activated: this is probably how the network can tell the difference
    between African and Indian elephants.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 特别值得注意的是，小象的耳朵激活非常强烈：这可能是网络区分非洲象和亚洲象的方式。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: ConvNets process images by applying a set of learned filters. Filters from earlier
    layers detect edges and basic textures, while filters from later layers detect
    increasingly abstract concepts.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络通过应用一系列学习到的过滤器来处理图像。早期层的过滤器检测边缘和基本纹理，而后期层的过滤器检测越来越抽象的概念。
- en: You can visualize both the pattern that a filter detects and a filter’s response
    map across an image.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以可视化过滤器检测到的模式和过滤器在整个图像中的响应图。
- en: You can use the Grad-CAM technique to visualize what area(s) in an image were
    responsible for a classifier’s decision.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 Grad-CAM 技术来可视化图像中哪些区域负责了分类器的决策。
- en: Together, these techniques make ConvNets highly interpretable.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些技术共同使得卷积神经网络具有高度的可解释性。
- en: Footnotes
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: 'Ramprasaath R. Selvaraju, et al., “Grad-CAM: Visual Explanations from Deep
    Networks via Gradient-Based Localization,” arxiv (2019), [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391).
    [[↩]](#footnote-link-1)'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Ramprasaath R. Selvaraju 等人，“Grad-CAM: 通过基于梯度的定位从深度网络中获取视觉解释，” arxiv (2019)，[https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)。[[↩]](#footnote-link-1)'
