- en: Interpreting what ConvNets learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter10_interpreting-what-convnets-learn](https://deeplearningwithpython.io/chapters/chapter10_interpreting-what-convnets-learn)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A fundamental problem when building a computer vision application is that of
    *interpretability*: *Why* did your classifier think a particular image contained
    a fridge, when all you can see is a truck? This is especially relevant to use
    cases where deep learning is used to complement human expertise, such as medical
    imaging use cases. This chapter will get you familiar with a range of different
    techniques for visualizing what ConvNets learn and understanding the decisions
    they make.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s often said that deep learning models are “black boxes”: they learn representations
    that are difficult to extract and present in a human-readable form. Although this
    is partially true for certain types of deep learning models, it’s definitely not
    true for ConvNets. The representations learned by ConvNets are highly amenable
    to visualization, in large part because they’re *representations of visual concepts*.
    Since 2013, a wide array of techniques has been developed for visualizing and
    interpreting these representations. We won’t survey all of them, but we’ll cover
    three of the most accessible and useful ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Visualizing intermediate ConvNet outputs (intermediate activations)* — Useful
    for understanding how successive ConvNet layers transform their input, and for
    getting a first idea of the meaning of individual ConvNet filters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Visualizing ConvNets filters* — Useful for understanding precisely what visual
    pattern or concept each filter in a ConvNet is receptive to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Visualizing heatmaps of class activation in an image* — Useful for understanding
    which parts of an image were identified as belonging to a given class, thus allowing
    you to localize objects in images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the first method — activation visualization — you’ll use the small ConvNet
    that you trained from scratch on the dogs-versus-cats classification problem in
    chapter 8\. For the next two methods, you’ll use a pretrained Xception model.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing intermediate activations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Visualizing intermediate activations consists of displaying the values returned
    by various convolution and pooling layers in a model, given a certain input (the
    output of a layer is often called its *activation*, the output of the activation
    function). This gives a view into how an input is decomposed into the different
    filters learned by the network. You want to visualize feature maps with three
    dimensions: width, height, and depth (channels). Each channel encodes relatively
    independent features, so the proper way to visualize these feature maps is by
    independently plotting the contents of every channel as a 2D image. Let’s start
    by loading the model that you saved in section 8.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, you’ll get an input image — a picture of a cat, not part of the images
    the network was trained on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.1](#listing-10-1): Preprocessing a single image'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s display the picture (see figure 10.1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.2](#listing-10-2): Displaying the test picture'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33eaa28cbb2d03a5c08f20e412a915d0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 10.1](#figure-10-1): The test cat picture'
  prefs: []
  type: TYPE_NORMAL
- en: To extract the feature maps you want to look at, you’ll create a Keras model
    that takes batches of images as input and outputs the activations of all convolution
    and pooling layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.3](#listing-10-3): Instantiating a model that returns layer activations'
  prefs: []
  type: TYPE_NORMAL
- en: 'When fed an image input, this model returns the values of the layer activations
    in the original model, as a list. This is the first time you’ve encountered a
    multi-output model in this book in practice since you learned about them in chapter
    7: until now, the models you’ve seen have had exactly one input and one output.
    This one has one input and nine outputs — one output per layer activation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.4](#listing-10-4): Using the model to compute layer activations'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, this is the activation of the first convolution layer for the
    cat image input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It’s a 178 × 178 feature map with 32 channels. Let’s try plotting the sixth
    channel of the activation of the first layer of the original model (see figure
    10.2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.5](#listing-10-5): Visualizing the sixth channel'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b376db7cebc68395f5678bdead7f2d5d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 10.2](#figure-10-2): Sixth channel of the activation of the first layer
    on the test cat picture'
  prefs: []
  type: TYPE_NORMAL
- en: This channel appears to encode a diagonal edge detector, but note that your
    own channels may vary because the specific filters learned by convolution layers
    aren’t deterministic.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s plot a complete visualization of all the activations in the network
    (see figure 10.3). We’ll extract and plot every channel in each of the layer activations,
    and we’ll stack the results in one big grid, with channels stacked side by side.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.6](#listing-10-6): Visualizing every channel in every intermediate
    activation'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/449cd612e2702fb4072369056fb379ca.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 10.3](#figure-10-3): Every channel of every layer activation on the
    test cat picture'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few things to note here:'
  prefs: []
  type: TYPE_NORMAL
- en: The first layer acts as a collection of various edge detectors. At that stage,
    the activations retain almost all of the information present in the initial picture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you go higher, the activations become increasingly abstract and less visually
    interpretable. They begin to encode higher-level concepts such as “cat ear” and
    “cat eye.” Higher representations carry increasingly less information about the
    visual contents of the image and increasingly more information related to the
    class of the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The sparsity of the activations increases with the depth of the layer: in the
    first layer, all filters are activated by the input image, but in the following
    layers, more and more filters are blank. This means the pattern encoded by the
    filter isn’t found in the input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have just observed an important universal characteristic of the representations
    learned by deep neural networks: the features extracted by a layer become increasingly
    abstract with the depth of the layer. The activations of higher layers carry less
    and less information about the specific input being seen and more and more information
    about the target (in this case, the class of the image: cat or dog). A deep neural
    network effectively acts as an *information distillation pipeline*, with raw data
    going in (in this case, RGB pictures) and being repeatedly transformed so that
    irrelevant information is filtered out (for example, the specific visual appearance
    of the image) and useful information is magnified and refined (for example, the
    class of the image).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is analogous to the way humans and animals perceive the world: after observing
    a scene for a few seconds, a human can remember which abstract objects were present
    in it (bicycle, tree) but can’t remember the specific appearance of these objects.
    In fact, if you tried to draw a generic bicycle from memory, chances are you couldn’t
    get it even remotely right, even though you’ve seen thousands of bicycles in your
    lifetime (see, for example, figure 10.4). Try it right now: this effect is absolutely
    real. Your brain has learned to completely abstract its visual input — to transform
    it into high-level visual concepts while filtering out irrelevant visual details
    — making it tremendously difficult to remember how things around you look.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8118eccf021ef647fb2a0c668c897a7c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 10.4](#figure-10-4): Left: Attempts to draw a bicycle from memory.
    Right: What a schematic bicycle should look like.'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing ConvNet filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another easy way to inspect the filters learned by ConvNets is to display the
    visual pattern that each filter is meant to respond to. This can be done with
    *gradient ascent in input space*, applying *gradient descent* to the value of
    the input image of a ConvNet so as to *maximize* the response of a specific filter,
    starting from a blank input image. The resulting input image will be one that
    the chosen filter is maximally responsive to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try this with the filters of the Xception model. The process is simple:
    we’ll build a loss function that maximizes the value of a given filter in a given
    convolution layer, and then we’ll use stochastic gradient descent to adjust the
    values of the input image so as to maximize this activation value. This will be
    your second example of a low-level gradient descent loop (the first one was in
    chapter 2). We will show it for TensorFlow, PyTorch, and Jax.'
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s instantiate the Xception model trained on the ImageNet dataset.
    We can once again use the KerasHub library, exactly as we did in chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.7](#listing-10-7): Instantiating the Xception convolutional base'
  prefs: []
  type: TYPE_NORMAL
- en: We’re interested in the convolutional layers of the model — the `Conv2D` and
    `SeparableConv2D` layers. We’ll need to know their names so we can retrieve their
    outputs. Let’s print their names, in order of depth.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.8](#listing-10-8): Printing the names of all convolutional layers
    in Xception'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll notice that the `SeparableConv2D` layers here are all named something
    like `block6_sepconv1`, `block7_sepconv2`, etc. — Xception is structured into
    blocks, each containing several convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s create a second model that returns the output of a specific layer
    — a “feature extractor” model. Because our model is a Functional API model, it
    is inspectable: you can query the `output` of one of its layers and reuse it in
    a new model. No need to copy the entire Xception code.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.9](#listing-10-9): A feature extractor model returning a specific
    output'
  prefs: []
  type: TYPE_NORMAL
- en: To use this model, we can simply call it on some input data, but we should be
    careful to apply our model-specific image preprocessing so that our images are
    scaled to the same range as the Xception pretraining data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.10](#listing-10-10): Using the feature extractor'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use our feature extractor model to define a function that returns a scalar
    value quantifying how much a given input image “activates” a given filter in the
    layer. This is the loss function that we’ll maximize during the gradient ascent
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: A non-obvious trick to help the gradient-ascent process go smoothly is to normalize
    the gradient tensor by dividing it by its L2 norm (the square root of the sum
    of the squares of the values in the tensor). This ensures that the magnitude of
    the updates done to the input image is always within the same range.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set up the gradient ascent step function. Anything that involves gradients
    requires calling backend-level APIs, such as `GradientTape` in TensorFlow, `.backward()`
    in PyTorch, and `jax.grad()` in JAX. Let’s line up all the code snippets for each
    of the three backends, starting with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient ascent in TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For TensorFlow, we can just open a `GradientTape` scope and compute the loss
    inside of it to retrieve the gradients we need. We’ll use a `@tf.function` decorator
    to speed up computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.11](#listing-10-11): Loss maximization via stochastic gradient
    ascent: TensorFlow'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient ascent in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of PyTorch, we use `loss.backward()` and `image.grad` to obtain
    the gradients of the loss with respect to the input image, like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.12](#listing-10-12): Loss maximization via stochastic gradient
    ascent: PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: No need to reset the gradients since the image tensor is recreated at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient ascent in JAX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of JAX, we use `jax.grad()` to obtain a function that returns the
    gradients of the loss with respect to the input image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.13](#listing-10-13): Loss maximization via stochastic gradient
    ascent: JAX'
  prefs: []
  type: TYPE_NORMAL
- en: The filter visualization loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now you have all the pieces. Let’s put them together into a Python function
    that takes a filter index as input and returns a tensor representing the pattern
    that maximizes the activation of the specified filter in our target layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.14](#listing-10-14): Function to generate filter visualizations'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting image tensor is a floating-point array of shape `(200, 200, 3)`,
    with values that may not be integers within `[0, 255]`. Hence, you need to post-process
    this tensor to turn it into a displayable image. You do so with the following
    straightforward utility function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.15](#listing-10-15): Utility function to convert a tensor into
    a valid image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it (see figure 10.5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ab3bf7e0bbde66c050003a258e160303.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 10.5](#figure-10-5): Pattern that the second channel in layer `block3_sepconv1`
    responds to maximally'
  prefs: []
  type: TYPE_NORMAL
- en: It seems that filter 2 in layer `block3_sepconv1` is responsive to a horizontal
    lines pattern, somewhat water-like or fur-like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the fun part: you can start visualizing every filter in the layer — and
    even every filter in every layer in the model (see figure 10.6).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.16](#listing-10-16): Generating a grid of all filter response patterns'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/293a80383ac80c0fb7f099520dd61ead.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 10.6](#figure-10-6): Some filter patterns for layers `block2_sepconv1`,
    `block4_sepconv1`, and `block8_sepconv1`'
  prefs: []
  type: TYPE_NORMAL
- en: 'These filter visualizations tell you a lot about how ConvNet layers see the
    world: each layer in a ConvNet learns a collection of filters such that their
    inputs can be expressed as a combination of the filters. This is similar to how
    the Fourier transform decomposes signals onto a bank of cosine functions. The
    filters in these ConvNet filter banks get increasingly complex and refined as
    you go higher in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: The filters from the first layers in the model encode simple directional edges
    and colors (or colored edges, in some cases).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The filters from layers a bit further up the stack, such as `block4_sepconv1`,
    encode simple textures made from combinations of edges and colors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The filters in higher layers begin to resemble textures found in natural images:
    feathers, eyes, leaves, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing heatmaps of class activation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here’s one last visualization technique — one that is useful for understanding
    which parts of a given image led a ConvNet to its final classification decision.
    This is helpful for “debugging” the decision process of a ConvNet, particularly
    in the case of a classification mistake (a problem domain called *model interpretability*).
    It can also allow you to locate specific objects in an image.
  prefs: []
  type: TYPE_NORMAL
- en: This general category of techniques is called *class activation map* (CAM) visualization,
    and it consists of producing heatmaps of class activation over input images. A
    class activation heatmap is a 2D grid of scores associated with a specific output
    class, computed for every location in any input image, indicating how important
    each location is with respect to the class under consideration. For instance,
    given an image fed into a dogs-versus-cats ConvNet, CAM visualization would allow
    you to generate a heatmap for the class “cat,” indicating how cat-like different
    parts of the image are, and also a heatmap for the class “dog,” indicating how
    dog-like parts of the image are. The specific implementation we’ll use is the
    one described in Selvaraju et al.^([[1]](#footnote-1))
  prefs: []
  type: TYPE_NORMAL
- en: Grad-CAM consists of taking the output feature map of a convolution layer, given
    an input image, and weighting every channel in that feature map by the gradient
    of the class with respect to the channel. Intuitively, one way to understand this
    trick is that you’re weighting a spatial map of “how intensely the input image
    activates different channels” by “how important each channel is with regard to
    the class,” resulting in a spatial map of “how intensely the input image activates
    the class.”
  prefs: []
  type: TYPE_NORMAL
- en: Let’s demonstrate this technique using the pretrained Xception model. Consider
    the image of two African elephants shown in figure 10.7, possibly a mother and
    her calf, strolling in the savanna. We can start by downloading this image and
    converting it to a NumPy array, as shown in figure 10.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be45d44f9634de90e89e016dbd9ed545.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 10.7](#figure-10-7): Test picture of African elephants'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.17](#listing-10-17): Preprocessing an input image for Xception'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have only used KerasHub to instantiate a pretrained feature extractor
    network using the backbone class. For Grad-CAM, we need the entire Xception model
    including the classification head — recall that Xception was trained on the ImageNet
    dataset with ~1 million labeled images belonging to 1,000 different classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'KerasHub provides a high-level *task* API for common end-to-end workflows like
    image classification, text classification, image generation, and so on. A task
    wraps preprocessing, a feature extraction network, and a task-specific head into
    a single class that is easy to use. Let’s try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The top five classes predicted for this image are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: African elephant (with 90% probability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tusker (with 5% probability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indian elephant (with 2% probability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triceratops and Mexican hairless dog with less than 0.1% probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The network has recognized the image as containing an undetermined quantity
    of African elephants. The entry in the prediction vector that was maximally activated
    is the one corresponding to the “African elephant” class, at index 386:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: To visualize which parts of the image are the most African elephant–like, let’s
    set up the Grad-CAM process.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will note that we didn’t need to preprocess our image before calling the
    task model. That’s because the KerasHub `ImageClassifier` is preprocessing inputs
    for us as part of `predict()`. Let’s preprocess the image ourselves so we can
    use the preprocessed inputs directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: First, we create a model that maps the input image to the activations of the
    last convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.18](#listing-10-18): Returning the last convolutional output'
  prefs: []
  type: TYPE_NORMAL
- en: Second, we create a model that maps the activations of the last convolutional
    layer to the final class predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.19](#listing-10-19): Going from the last convolutional output to
    final predictions'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we compute the gradient of the top predicted class for our input image
    with respect to the activations of the last convolution layer. Once again, having
    to compute gradients means we have to use backend APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting the gradient of the top class: TensorFlow version'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start with the TensorFlow version, once again using `GradientTape`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.20](#listing-10-20): Computing the top class gradients with TensorFlow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting the gradient of the top class: PyTorch version'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, here’s the PyTorch version, using `.backward()` and `.grad`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.21](#listing-10-21): Computing the top class gradients with PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting the gradient of the top class: JAX version'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, let’s do JAX. We define a separate loss computation function that takes
    the final layer’s output and returns the activation channel corresponding to the
    top predicted class. We use this activation value as our loss, allowing us to
    compute the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.22](#listing-10-22): Computing the top class gradients with Jax'
  prefs: []
  type: TYPE_NORMAL
- en: Displaying the class activation heatmap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, we apply pooling and importance weighting to the gradient tensor to obtain
    our heatmap of class activation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.23](#listing-10-23): Gradient pooling and channel importance weighting'
  prefs: []
  type: TYPE_NORMAL
- en: For visualization purposes, you’ll also normalize the heatmap between 0 and
    1. The result is shown in figure 10.8.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.24](#listing-10-24): Heatmap post-processing'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c32d227d3557544e6058ccb7c8a6ab71.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 10.8](#figure-10-8): Standalone class activation heatmap'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s generate an image that superimposes the original image on the
    heatmap you just obtained (see figure 10.9).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 10.25](#listing-10-25): Superimposing the heatmap with the original
    picture'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ce2ad2a03c71201ecc93b19562ffa7b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 10.9](#figure-10-9): African elephant class activation heatmap over
    the test picture'
  prefs: []
  type: TYPE_NORMAL
- en: 'This visualization technique answers two important questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Why did the network think this image contained an African elephant?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where is the African elephant located in the picture?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In particular, it’s interesting to note that the ears of the elephant calf
    are strongly activated: this is probably how the network can tell the difference
    between African and Indian elephants.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ConvNets process images by applying a set of learned filters. Filters from earlier
    layers detect edges and basic textures, while filters from later layers detect
    increasingly abstract concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can visualize both the pattern that a filter detects and a filter’s response
    map across an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the Grad-CAM technique to visualize what area(s) in an image were
    responsible for a classifier’s decision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these techniques make ConvNets highly interpretable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Footnotes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ramprasaath R. Selvaraju, et al., “Grad-CAM: Visual Explanations from Deep
    Networks via Gradient-Based Localization,” arxiv (2019), [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391).
    [[↩]](#footnote-link-1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
