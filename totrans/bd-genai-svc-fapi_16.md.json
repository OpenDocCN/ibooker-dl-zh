["```py\nproject/\n│\n├── host.json\n├── main.py\n├── app.py\n└── requirements.txt\n```", "```py\n$ pip install azure-functions\n```", "```py```", "````py Quindi, crea il *file host.json* seguendo l'[Esempio 12-1](#deployment_function_azure_host).    ##### Esempio 12-1\\. Configurazioni dell'host di Azure Functions (host.json)    ``` { `\"version\"``:` `\"2.0\"``,` ```py````", "```py```", "````` `\"routePrefix\"``:` `\"\"` ```py` `}` ``` `}` `` `}` `` ```py ```` ```py`` `````", "``````py` ```   ```py``````", "```py```", "```py```", "```py # app.py  import azure.functions as func from fastapi import FastAPI  app = FastAPI()  @app.post(\"/generate/text\", response_model_exclude_defaults=True) async def serve_text_to_text_controller(prompt): ... ```", "```py # function.py  import azure.functions as func from main import app as fastapi_app  app = func.AsgiFunctionApp(     app=fastapi_app, http_auth_level=func.AuthLevel.ANONYMOUS ) ```", "```py $ func start >> Found the following functions: ```", "```py ```", "```py```", "````` ```py`Puoi quindi provare gli URL corrispondenti ai gestori nell'applicazione inviando richieste HTTP sia ai percorsi semplici che a quelli parametrizzati:    ``` http://localhost:7071/generate/text http://localhost:7071/<other-paths> ```py    Una volta pronta, puoi distribuire la tua funzione serverless FastAPI wrapped nel cloud Azure ed eseguire il seguente comando:    ``` $ func azure functionapp publish <FunctionAppName> ```py    Il comando `publish` pubblicherà quindi i file del progetto dalla directory del progetto a `<FunctionAppName>` come pacchetto di distribuzione ZIP.    Dopo la distribuzione, puoi testare diversi percorsi sull'URL distribuito:    ``` http://<FunctionAppName>.azurewebsites.net/generate/text http://<FunctionAppName>.azurewebsites.net/<other-paths> ```py    ###### Avvertenze    Il provider Cloud scelto potrebbe non supportare il servizio di un server FastAPI all'interno del suo runtime di funzione. In questo caso, potresti voler cercare delle opzioni di distribuzione alternative. Altrimenti, dovrai migrare la logica dei tuoi endpoint al framework web supportato del runtime di funzione e creare funzioni separate per ogni endpoint.    Come vedi, distribuire i tuoi servizi FastAPI come funzioni cloud è semplice e ti permette di delegare la gestione e la scalabilità dei tuoi servizi aiprovider cloud.    Tieni presente che se decidi di servire un modello GenAI nel tuo servizio, le funzioni Cloud non sono adatte all'implementazione a causa dei loro brevi periodi di timeout (10 minuti). Al contrario, dovresti utilizzare un'API del provider del modello nei tuoi servizi in modo da avere un accesso affidabile e scalabile al modello senza essere vincolato da limiti di tempo di esecuzione.```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "````  ```py `` `## Distribuzione su piattaforme di app gestite    Oltre alle funzioni o alle macchine virtuali del cloud, puoi caricare la tua base di codice sotto forma di file ZIP sulle piattaforme di app gestite dai provider del cloud. Le piattaforme di app gestite ti permettono di delegare al provider del cloud diversi compiti relativi alla manutenzione e alla gestione dei tuoi servizi. In cambio, paghi solo per le risorse di calcolo gestite dal provider del cloud che servono alla tua applicazione. I sistemi del provider del cloud assegnano e ottimizzano le risorse in base alle esigenze della tua applicazione.    Esempi di tali servizi sono Azure App Services, AWS Elastic Beanstalk, Google App Engine o la piattaforma app Digital Ocean.    Esistono anche piattaforme di terze parti come Heroku, Hugging Face Spaces, railway.app, render.com o fly.io per distribuire i tuoi servizi direttamente dal codice contenuto nei repository, che ti sottraggono alcune decisioni in modo che tu possa distribuire più velocemente e più facilmente. Sotto il cofano, le piattaforme di app gestite da terze parti possono utilizzare l'infrastruttura dei principali provider Cloud come Azure, Google o AWS.    Il vantaggio principale della distribuzione su piattaforme di app gestite è la facilità e la velocità di distribuzione, networking, scalabilità e manutenzione dei tuoi servizi. Queste piattaforme ti forniscono gli strumenti necessari per proteggere, monitorare, scalare e gestire i tuoi servizi senza doverti preoccupare dell'allocazione delle risorse sottostanti, della sicurezza o degli aggiornamenti del software. Possono permetterti di configurare bilanciatori di carico, certificati SSL, mappature dei domini, monitoraggio e ambienti di staging, in modo che tu possa concentrarti maggiormente sullo sviluppo dell'applicazione piuttosto che sul carico di lavoro di distribuzione del progetto.    Poiché queste piattaforme seguono il modello di pagamento platform-as-a-service (PaaS), ti verrà addebitata una tariffa più alta rispetto all'utilizzo della tua infrastruttura o di risorse di livello inferiore come VM bare-bone o opzioni di calcolo serverless.I servizi alternativi possono utilizzare il modello di pagamento infrastructure-as-a-service (IaaS), che spesso è più conveniente.    Personalmente, trovo che le piattaforme app gestite siano un modo conveniente per distribuire le mie applicazioni senza troppi problemi. Se sto lavorando a un prototipo e ho bisogno di rendere disponibili i miei servizi agli utenti il più velocemente possibile, le piattaforme app gestite sono la mia prima opzione. Tuttavia, tieni presente che se hai bisogno di accedere all'hardware GPU per eseguire i servizi diinferenza, dovrai affidarti a macchine virtuali dedicate, a server on-premises o a servizi di piattaforme AI specializzate per servire i tuoi modelli. Le piattaforme app possono solo fornire CPU, memoria e storage su disco per servire i servizi backend o leapplicazioni frontend.    ###### Suggerimento    Tra le piattaforme AI gestite dai provider cloud ci sono Azure Machine Learning Studio o Azure AI, Google Cloud Vertex AI Platform, AWS Bedrock e SageMaker o IBM Watson Studio.    Esistono anche piattaforme di terze parti per ospitare i tuoi modelli, come Hugging Face Inference Endpoints, Weights & Biases Platform o Replicate.    Il deploy dai repository di codice richiede spesso l'aggiunta di alcuni file di configurazione alla radice del progetto, a seconda della piattaforma di app su cui verrà effettuato il deploy. Il processo dipende anche dal fatto che la piattaforma di app supporti il runtime dell'applicazione, le librerie e le versioni del framework che stai utilizzando, per cui il successo del deploy non è sempre garantito. Inoltre, è spesso difficile migrare ai runtime o alle versioni supportate.    A causa di questi problemi imprevisti, molti ingegneri stanno passando alle tecnologie di containerizzazione come Docker o Podman per impacchettare e distribuire i loro servizi. Queste applicazioni containerizzate possono poi essere distribuite direttamente su qualsiasi piattaforma di app che supporti i container, con la garanzia che l'applicazione verrà eseguita indipendentemente dalle risorse sottostanti, dal runtime o dalle versioni delle dipendenze.    Il deploy dei servizi con i container è oggi una delle strategie più affidabili per inviare le tue applicazioni in produzione e renderle accessibili agli utenti.    ## Distribuire con i container    Un *container* è un ambiente isolato progettato per la creazione e l'esecuzione di applicazioni. I container possono eseguire i tuoi servizi in modo rapido e affidabile in qualsiasi ambiente informatico, confezionando il tuo codice con tutte le dipendenze necessarie.    I container si basano su un metodo di virtualizzazione del sistema operativo che consente loro di essere eseguiti su hardware fisico, nel cloud, su macchine virtuali o su piùsistemi operativi.    ###### Suggerimento    Analogamente alle piattaforme di app gestite e alle funzioni serverless, puoi configurare i container in modo che si riavviino automaticamente e si auto-riparino, se la tua applicazione esce per qualsiasi motivo.    A differenza delle macchine virtuali, le cui tecnologie sottostanti si basano sulla virtualizzazione, i container si basano sulla containerizzazione.    La containerizzazione impacchetta le applicazioni e le loro dipendenze in unità leggere e isolate che condividono il kernel del sistema operativo host. D'altra parte, la virtualizzazione permette di eseguire più sistemi operativi su una singola macchina fisica grazie agli hypervisor. Pertanto, a differenza delle macchine virtuali, i container non virtualizzano le risorse hardware, ma vengono eseguiti su una piattaforma di runtime per container che astrae le risorse, rendendoli leggeri (cioè con pochi megabyte da memorizzare) e più veloci delle macchine virtuali, poiché non richiedono un sistema operativo separato per container.    ###### Nota    In sostanza, la virtualizzazione consiste nell'astrarre le risorse hardware della macchina host, mentre la containerizzazione consiste nell'astrarre il kernel del sistema operativo e nell'eseguire tutti i componenti dell'applicazione all'interno di un'unità isolata chiamata *container*.    La[Figura 12-3](#deployment_container_architecture) mette a confronto le architetture dei sistemi di virtualizzazione e di containerizzazione.  ![bgai 1203](assets/bgai_1203.png)  ###### Figura 12-3\\. Confronto tra le architetture dei sistemi di containerizzazione e virtualizzazione    Il vantaggio principale dell'utilizzo dei container è la loro *portabilità*, *velocità di avvio*, *compattezza* e *affidabilità* in diversi ambienti informatici, in quanto non richiedono un sistema operativo guest e un livello software di hypervisor.    Questo li rende perfetti per distribuire i tuoi servizi con risorse, sforzi di distribuzione e spese generali minime. Si avviano più velocemente di una macchina virtuale e anche la scalabilità è più semplice. Puoi aggiungere altri container per *scalare orizzontalmente* i tuoiservizi.    Per aiutarti a containerizzare le tue applicazioni, puoi affidarti a piattaforme come Docker che sono state testate a fondo dalle comunità MLOps e DevOps.` `` ```  ```py````", "```py```", "````py````", "```py```", "````py````", "```py```", "````py````", "```py```", "````py````", "```py```", "``````py``````", "```````py```````", "`````` # La containerizzazione con Docker    Docker è una piattaforma di containerizzazione utilizzata per costruire, spedire ed eseguire container. Al momento in cui scriviamo, Docker detiene circa [il 22% della quota di mercato](https://oreil.ly/A5x63) delle piattaforme di virtualizzazione con oltre 9 milioni di sviluppatori e [11 miliardi di download mensili di immagini](https://oreil.ly/8-wx4), il che la rende la piattaforma di containerizzazione più popolare. Molti ambienti server e fornitori di cloud supportano Docker all'interno di molte varianti di server Linux e Windows.    È probabile che se devi distribuire i tuoi servizi GenAI, l'opzione più semplice e diretta sia quella di utilizzare Docker per containerizzare la tua applicazione. Tuttavia, per trovarti a tuo agio con Docker, devi comprenderne l'architettura e i sottosistemi sottostanti, come lo storage e il networking.    ## Architettura di Docker    Il sistema Docker è composto da un motore, un client e un server:    Motore Docker      Il motore è costituito da diversi componenti, tra cui un client e un server che girano sullo stesso sistema operativo.      Client Docker      Docker viene fornito con uno *strumento CLI* chiamato `docker` e un'applicazione con interfaccia grafica (GUI) chiamata *Docker Desktop*. Utilizzando l'implementazione client-server, il client Docker può comunicare con l'istanza server locale o remota utilizzando un'API REST per gestire i container eseguendo comandi come l'esecuzione, l'arresto e la terminazione dei container. Puoi anche utilizzare il client per prelevare le immagini da un registro di immagini.      Server Docker      Il server è un *demone* chiamato `dockerd`. Il demone Docker risponde alle richieste HTTP del client tramite l'API REST e può interagire con altri demoni. È anche responsabile del monitoraggio del ciclo di vita dei container.      La piattaforma Docker ti permette anche di creare e configurare oggetti come *reti*, *volumi di archiviazione*, *plug-in* e oggetti di servizio per supportare le tue implementazioni.    La cosa più importante è che per containerizzare le tue applicazioni con Docker, dovrai creare delle immagini Docker.    Un'*immagine Docker* è un pacchetto portatile contenente software e funge da ricetta per la creazione e l'esecuzione dei container delle tue applicazioni. In sostanza, un container è un'istanza in memoria di un'immagine.    ###### Suggerimento    L'immagine di un contenitore è *immutabile*, quindi una volta creata non è più possibile modificarla. Puoi solo aggiungere e non sottrarre un'immagine. Dovrai crearne una nuova se vuoi applicare delle modifiche.    Le immagini Docker sono il primo passo verso la containerizzazione dei tuoi servizi, come imparerai nella prossima sezione.    ## Creazione di immagini Docker    Immaginiamo di avere un piccolo servizio GenAI che utilizza FastAPI, come mostrato nell'[Esempio 12-4](#docker_app), che vogliamo containerizzare.    ##### Esempio 12-4\\. Un semplice servizio GenAI FastAPI    ```py # main.py  from fastapi import FastAPI from models import generate_text ![1](assets/1.png)  app = FastAPI()  @app.post(\"/generate\") def generate_text(prompt: str):     return generate_text(prompt) ```    [![1](assets/1.png)](#co_deployment_of_ai_services_CO1-1)      Supponiamo che la funzione `generate_text` chiami un fornitore di modelli API o un server di modelli esterno.      Per creare questa applicazione in un'immagine contenitore, dovrai scrivere le istruzioni in un file di testo chiamato *Dockerfile*. All'interno di questo Dockerfile, puoi specificare i seguenticomponenti:    *   L'immagine *di base* da cui creare una nuova immagine, che fornisce il sistema operativo e l'ambiente su cui vengono costruiti i livelli applicativi aggiuntivi.           *   Comandi per aggiornare il sistema operativo guest e installare software aggiuntivo           *   Costruisci gli artefatti da includere, come ad esempio il codice dell'applicazione.           *   Servizi da esporre, come la configurazione dello storage e della rete.           *   Il comando da eseguire all'avvio del contenitore              L['esempio 12-5](#containers_dockerfile) illustra come costruire l'immagine di un'applicazione in un file Docker.    ##### Esempio 12-5\\. Profilo Docker per containerizzare un'applicazione FastAPI    ```py ARG PYTHON_VERSION=3.12 FROM python:${PYTHON_VERSION}-slim as base ![1](assets/1.png)  WORKDIR /code ![2](assets/2.png)  COPY requirements.txt . ![3](assets/3.png)  RUN pip install --no-cache-dir --upgrade -r requirements.txt ![4](assets/4.png)  COPY . . ![5](assets/5.png)  EXPOSE 8000 ![6](assets/6.png)  CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] ![7](assets/7.png) ```    [![1](assets/1.png)](#co_deployment_of_ai_services_CO2-1)      Usa l'immagine slim ufficiale di Python 3.12 come immagine `base`.^([1](ch12.html#id1291))      [![2](assets/2.png)](#co_deployment_of_ai_services_CO2-2)      Imposta la directory di lavoro all'interno del contenitore su `/code`.      [![3](assets/3.png)](#co_deployment_of_ai_services_CO2-3)      Copia il file `requirements.txt` dall'host alla directory corrente del contenitore.      [![4](assets/4.png)](#co_deployment_of_ai_services_CO2-4)      Installa le dipendenze di Python elencate in `requirements.txt` senza utilizzare la cache.      [![5](assets/5.png)](#co_deployment_of_ai_services_CO2-5)      Copia tutti i file dalla directory corrente dell'host alla directory corrente del contenitore.      [![6](assets/6.png)](#co_deployment_of_ai_services_CO2-6)      Informa il demone Docker che l'applicazione all'interno del contenitore è in ascolto su `8000` in fase di runtime. Il comando `EXPOSE` non mappa o consente automaticamente l'accesso alle porte.^([2](ch12.html#id1292))      [![7](assets/7.png)](#co_deployment_of_ai_services_CO2-7)      Eseguire il server `uvicorn` con il modulo applicativo e la configurazione host/porta, quando viene lanciato il container.      In questo capitolo non tratteremo l'intera [specifica del Dockerfile](https://oreil.ly/8fJ6l), ma notiamo come ogni comando modifica la struttura dell'immagine che ti permette di eseguire tutti i servizi GenAI all'interno di un container.    Puoi utilizzare il comando `docker build` per creare l'immagine riportata nell'[Esempio 12-5](#containers_dockerfile):    ```py $ docker build -t genai-service . ```   `Nota i passaggi elencati nell'output: quando ogni passaggio viene eseguito, viene aggiunto un nuovo livello all'immagine che stai costruendo.    Una volta ottenuta l'immagine di un container, puoi utilizzare i registri dei container per archiviare, condividere e scaricare le immagini.`  ```py``````", "```````py```````", "```````py` ## Registri dei contenitori    Per archiviare e distribuire le immagini in un ambiente a controllo di versione, puoi utilizzare i *registri dei container*, che includono sia la versione pubblica che quella privata.    *Docker Hub* è un registro di container gestito in modalità software-as-a-service (SaaS) per archiviare e distribuire le immagini create dall'utente.    Docker Hub è pubblico per impostazione predefinita, ma puoi anche utilizzare registri privati autogestiti o di provider cloud come Azure Container Registry (ACR), AWS Elastic Container Registry (ECR) o Google Cloud Artifact Registry.    Puoi vedere l'architettura completa della piattaforma Docker nella [Figura 12-4](#docker_architecture).  ![bgai 1204](assets/bgai_1204.png)  ###### Figura 12-4\\. Architettura di sistema della piattaforma Docker    Come puoi vedere nella [Figura 12-4](#docker_architecture), il demone Docker gestisce i contenitori e le immagini. Crea i contenitori dalle immagini e comunica con il client Docker, gestendo i comandi per costruire ed eseguire le immagini. Il demone Docker può anche prelevare le immagini da un registro (ad esempio, Docker Hub) che contiene immagini come Ubuntu, Redis o PostgreSQL.    Utilizzando il registro di Docker Hub, puoi accedere ad altre immagini fornite, oltre a distribuire e controllare la tua versione. I registri come Docker Hub svolgono un ruolo cruciale nella scalabilità dei tuoi servizi, poiché le piattaforme di orchestrazione dei container come Kubernetes hanno bisogno di accedere ai registri per estrarre ed eseguire istanze multiple di container dalle immagini.    Puoi prelevare le immagini pubbliche da Docker Hub utilizzando il comando `docker pull`:    ``` $ docker image pull python:3.12-slim bookworm: Pulling from library/python ```py Digest: sha256:3f1d6c17773a45c97bd8f158d665c9709d7b29ed7917ac934086ad96f92e4510 `` Status: Downloaded newer image `for` python:3.12-slim `docker.io/library/python:3.12-slim` `` ``` ```py   ``````py``` ``````py`` ``````py` Quando effettui il push e il pull delle immagini, dovrai specificare un *tag* utilizzando la sintassi `<name>:<tag>`. Se non fornisci un tag, il motore di Docker utilizzerà il tag `latest` per impostazione predefinita.    Oltre al pulling, puoi anche memorizzare le tue immagini nei registri dei container. Per prima cosa, devi creare e taggare la tua immagine con un'etichetta di versione e l'URL del repository dell'immagine:    ``` $ docker build -t genai-service:latest . $ docker image tag genai-service:latest docker.io/myrepo/genai-service:latest ```py   ``````py ````` Una volta che l'immagine è stata costruita e contrassegnata, puoi inviarla al registro dei container di Docker Hub utilizzando il comando `docker push`. Potrebbe essere necessario effettuare il login per autenticarsi con l'hub:    ```py $ docker login $ docker image push docker.io/myrepo/genai-service:latest `195be5f8be1d: Pushed` ```   ```py` ``` ``###### Avvertenze    Fai attenzione a non sovrascrivere il tag di un'immagine in molti repository. Ad esempio, un'immagine creata e taggata `genai:latest` in un repository può essere sovrascritta da un'altra immagine taggata `genai:latest`.    Ora che l'immagine è memorizzata nel registro di sistema, puoi richiamarla su un altro computer o in un secondo momento per eseguire l'immagine senza doverla ricostruire.^([3](ch12.html#id1297)) o in un secondo momento per eseguire l'immagine senza doverla ricostruire.`` ```py ```` ```py`` ``````py ``````py` ``````py`` ``````py```  ``````py```````", "```````py```````", "```` ```py````", "```py```", "````py````", "```py```", "``` ```", "```````py```````", "```````py````` ## Filesystem del container e livelli di Docker    Quando costruisce l'immagine, Docker utilizza un filesystem speciale chiamato `Unionfs` (stackable unification filesystem) per unire il contenuto di diverse directory (cioè *rami* o, nella terminologia di Docker, *livelli*), mantenendo il loro contenuto fisico separato.    Utilizzando `Unionfs`, le directory di filesystem distinti possono essere combinate e sovrapposte per formare un singolo filesystem virtuale coerente, come mostrato nella [Figura 12-5](#docker_unionfs).  ![bgai 1205](assets/bgai_1205.png)  ###### Figura 12-5\\. File system virtuale unificato da più file system    Utilizzando il sito `Unionfs`, Docker può aggiungere o rimuovere rami mentre costruisci il filesystem del tuo container a partire da un'immagine.    Per illustrare il meccanismo dell'architettura a strati nei container, esaminiamo l'immagine dell'[Esempio 12-5](#containers_dockerfile).    Quando costruisci l'immagine usando l'[Esempio 12-5](#containers_dockerfile), stai stratificando un'immagine base di Python 3.12 in esecuzione su una distribuzione Linux in cima al filesystem root. Successivamente, aggiungi *requirements.txt* in cima all'immagine base di Python e poi installi le dipendenze in cima al livello *requirements.txt*. Poi aggiungi un nuovo livello copiando il contenuto della cartella del progetto nel contenitore, sovrapponendolo a tutto il resto. Infine, quando avvii il contenitore con il comando `uvicorn`, aggiungi un ultimo livello scrivibile come parte del filesystem del contenitore. Di conseguenza, l'ordine dei livelli diventa importante quando si costruiscono immagini Docker.    La[Figura 12-6](#docker_branches) mostra l'architettura del filesystem a strati.  ![bgai 1206](assets/bgai_1206.png)  ###### Figura 12-6\\. Architettura del filesystem Unionfs a strati    Nell'[Esempio 12-5](#containers_dockerfile), ogni passaggio di comando crea un'immagine nella cache mentre il processo di compilazione finalizza l'immagine del contenitore. Per eseguire i comandi, vengono creati dei contenitori intermedi che vengono poi eliminati automaticamente. L'immagine sottostante nella cache viene mantenuta sull'host di compilazione e non viene rimossa. Queste immagini temporanee vengono sovrapposte all'immagine precedente e combinate in un'unica immagine una volta completati tutti i passaggi. Questa ottimizzazione consente alle future compilazioni di riutilizzare queste immagini per accelerare i tempi di compilazione.    Alla fine, il contenitore comprenderà uno o più livelli di immagine e un livello finale effimero del contenitore (cioè che non sarà persistito) quando il contenitore verrà distrutto.    ## Stoccaggio Docker    In questa sezione imparerai a conoscere i vari meccanismi di archiviazione di Docker. Durante lo sviluppo dei tuoi servizi come container, puoi utilizzare questi strumenti per gestire la persistenza dei dati, la condivisione dei dati tra i container e il mantenimento dello stato tra i riavvii dei container.    Quando lavori con i container, la tua applicazione potrebbe aver bisogno di scrivere dati sul disco, che persisteranno in uno storage *effimero*. Lo storage effimero è uno storage temporaneo di breve durata che viene cancellato una volta che il container viene fermato, riavviato o rimosso. Se riavvii il tuo container, noterai che i dati precedentemente persistenti non sono più disponibili. Sotto il cofano, Docker scrive i dati runtime in un livello container effimero scrivibile nel filesystem virtuale del container.    ###### Avvertenze    Se ti affidi alla configurazione di archiviazione predefinita del contenitore, perderai tutti i dati generati dall'applicazione e i file di log che hai scritto su disco durante il runtime del contenitore.    Per evitare la perdita dei dati e dei log del runtime dell'applicazione, hai a disposizione diverse opzioni di archiviazione che ti permettono di persistere i dati durante la vita di un container. Durante lo sviluppo, puoi utilizzare *volumi* o *mount bind* per persistere i dati nel filesystem del sistema operativo host o affidarti a database locali per la persistenza dei dati.    La[Tabella 12-1](#docker_storage_options) mostra le opzioni di montaggio dello storage di Docker.      Tabella 12-1\\. Supporti di archiviazione di Docker   | Immagazzinamento | Descrizione | Casi d'uso | | --- | --- | --- | | Volumi | Soluzione di archiviazione ottimizzata per l'I/O e preferita. Gestita da Docker e memorizzata in una posizione specifica sull'host, ma disaccoppiata dalla struttura del filesystem dell'host. | Se hai bisogno di archiviare e condividere i dati tra più contenitori.Se non hai bisogno di modificare file o directory dell'host. | | Montaggi vincolati | Montano i file o le directory dell'host nel contenitore, ma hanno funzionalità limitate rispetto ai volumi. | Se vuoi che sia i container che i processi host accedano e modifichino i file e le directory dell'host. Ad esempio, durante lo sviluppo e i test locali. | | Supporti temporanei (tmpfs) | Memorizza i dati nella memoria dell'host (RAM) e non li scrive mai nel container o nel filesystem dell'host. | Se hai bisogno di un'archiviazione temporanea ad alte prestazioni per dati sensibili o non statistici che non persistono dopo l'arresto del contenitore. |    [La Figura 12-7](#docker_storage_mounts) mostra i diversi tipi di supporti.  ![bgai 1207](assets/bgai_1207.png)  ###### Figura 12-7\\. Supporti di archiviazione di Docker    Ora analizzeremo in dettaglio ogni opzione di archiviazione in modo che tu possa simulare il tuo ambiente di produzione a livello locale con i container Docker utilizzando l'archiviazione appropriata. Quando distribuisci i container in produzione all'interno di un ambiente cloud, puoi utilizzare un database o un'offerta di archiviazione cloud per la persistenza dei dati invece dei volumi Docker o dei mount bind per centralizzare l'archiviazione su più container.    ### Volumi di Docker    Docker ti permette di creare *volumi* isolati per conservare i dati delle applicazioni tra i runtime dei container. Per creare un volume, puoi eseguire il seguente comando:    ```py $ docker volume create -n data ```    Una volta creati, puoi utilizzare i volumi per conservare i dati tra un'esecuzione e l'altra del contenitore. I volumi ti permettono anche di persistere i dati quando utilizzi i contenitori di database e di memoria.    ###### Avvertenze    Il riavvio di un contenitore di database con le nuove variabili d'ambiente potrebbe non essere sufficiente per ripristinare le nuove impostazioni.    Alcuni sistemi di database potrebbero richiedere di ricreare il volume del contenitore se devi aggiornare le impostazioni come lecredenziali dell'utente amministratore.    Per impostazione predefinita, tutti i volumi creati verranno memorizzati nel filesystem della macchina host fino a quando non li rimuoverai esplicitamente con il comando `docker volume remove`.    ### Montaggi vincolati    Oltre ai volumi, puoi anche utilizzare le mappature del filesystem tramite i *mount bind* dei volumi che mappano le directory che risiedono sul filesystem dell'host sul filesystem del container, come mostrato nella [Figura 12-8](#docker_bind_mounts).  ![bgai 1208](assets/bgai_1208.png)  ###### Figura 12-8\\. Bind mount tra il filesystem host e un container    Il montaggio avviene all'avvio del container. Con le directory montate, puoi accedervi direttamente dall'interno delcontainer.Puoi leggere e persistere i dati nelle directory montate durante l'esecuzione e l'arresto del container.    Per eseguire un container con un volume bind mount, puoi utilizzare il seguente comando:    ```py $ docker run -v src:/app genai-service ```   ``In questo caso, il flag `-v` ti permette di mappare la directory dell'host in una directory del contenitore utilizzando la sintassi `<host_dir>:<container_dir>`.    ###### Avvertenze    La funzionalità del comando `COPY` che utilizzi in un file Docker è diversa da quella del montaggio delle directory.    Il primo crea una copia separata di una directory host nel contenitore durante il processo di creazione dell'immagine, mentre il secondo ti permette di accedere e aggiornare la directory host mappata dall'interno delcontenitore.    Questo significa che se non stai attento, puoi modificare o cancellare involontariamente tutti i file originali sul computer host in modo permanente, dall'interno del contenitore.    I volumi di montaggio Bind possono essere ancora utili in un ambiente di sviluppo locale: mentre modifichi il codice sorgente dei tuoi servizi, potrai osservare in tempo reale l'impatto delle modifiche sui container applicativi in esecuzione.``  ```py```````", "```` ### Supporti temporanei (tmpfs)    Se hai dei dati non persistenti, come le cache dei modelli o i file sensibili che non hai bisogno di archiviare in modo permanente, dovresti prendere in considerazione l'utilizzo dei *montaggi* temporanei *tmpfs*.    Questo montaggio temporaneo persisterà i dati nella memoria host (RAM) solo durante il runtime del contenitore e aumenta le prestazioni del contenitore evitando le scritture nel livello scrivibile del contenitore.    Quando containerizzi le applicazioni GenAI, puoi utilizzare dei mount temporanei per memorizzare i risultati nella cache, i calcoli intermedi del modello, i file temporanei e i log specifici della sessione che non ti serviranno una volta che il container si sarà fermato.    ###### Suggerimento    Il livello scrivibile del container è strettamente legato alla macchina host attraverso un driver di archiviazione per implementare il filesystem union. Pertanto, la scrittura sul livello scrivibile del container riduce le prestazioni a causa di questo ulteriore livello di astrazione.    Al contrario, puoi utilizzare i volumi di dati per l'archiviazione persistente che scrive direttamente sul filesystem host o i mount tmpfs per l'archiviazione temporanea in memoria.    A differenza dei mount e dei volumi bind, non è possibile condividere il mount tmpfs tra i container e la funzionalità è disponibile solo sui sistemi Linux. Inoltre, se modifichi i permessi delle directory sui mount tmpfs, questi possono essere ripristinati al riavvio del container.    Ecco alcuni altri casi di utilizzo dei montaggi tmpfs:    *   Memorizzare temporaneamente cache di dati, risposte API, log, dati di test, file di configurazione e artefatti del modello AI in memoria           *   Evitare le scritture di I/O sui dischi quando si lavora con le API di libreria che richiedono oggetti di tipo file           *   Simulare l'I/O ad alta velocità con l'accesso e la scrittura rapida dei file           *   Prevenire le scritture su disco eccessive o non necessarie se hai bisogno didirectory temporanee.              Per impostare un montaggio tmpfs, puoi utilizzare il seguente comando:    ```py $ docker run --tmpfs /cache genai-service ```   ``In questo caso, stai impostando un mount tmpfs sulla directory `/cache` per le cache dei modelli, che cesseranno di esistere una volta che il contenitore si sarà fermato.``  ```py````", "```py```", "``` $ docker run --user genai-service ```", "``````py``````", "```py ARG USERNAME=fastapi ![1](assets/1.png) ARG USER_UID=1001 ARG USER_GID=1002  RUN groupadd --gid $USER_GID $USERNAME \\ ![2](assets/2.png)     && adduser \\     --disabled-password \\     --shell \"/sbin/nologin\" \\ ![3](assets/3.png)     --gecos \"\" \\     --home \"/nonexistent\" \\     --no-create-home \\ ![4](assets/4.png)     --uid \"${UID}\" \\     --gid $USER_GID     $USERNAME ![5](assets/5.png)  USER $USERNAME ![6](assets/6.png)  CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] ```", "```py total 12 drw-r--r-- `2` root root `4096` Oct  `1` `10`:00 myscripts ```", "```py```", "````py` ````", "```` Puoi leggere i permessi `drwxr-xr-x` per la directory `myscripts` utilizzando la seguente ripartizione:    *   `d`: Specifica che `myscripts` è una directory; altrimenti mostrerebbe una `-`.           *   `rwx`: Proprietario `root` L'utente può (r)leggere, (w)scrivere ed elaborare i file di questadirectory.           *   `r--`: I membri del gruppo `root` possono eseguire operazioni di sola lettura ma non possono scrivere o eseguire alcun file.           *   `r--`: Tutti gli altri possono leggere il file ma non possono scriverlo o eseguirlo.^([4](ch12.html#id1317))              Se vuoi impostare la proprietà o i permessi della directory `myscripts`, puoi utilizzare i comandi `chmod` o `chown` nei sistemi Linux.    Usa il comando `chown` per cambiare il proprietario della directory sull'host in modo da poter modificare i file nell'editor di codice:    ```py # Set file or directory ownership $ sudo chown -R username:groupname mydir ```   ```py````", "```py # Set execute permissions using flags $ sudo chmod -R +x myscripts `# Set execute permissions in a numeric form` $ sudo chmod -R `755` myscripts ```", "```py```", "```py```", "```py total 12 drwxr-xr-x `2` root root `4096` Oct  `1` `10`:00 myscripts ```", "```py```", "````` *   `rwx`: Proprietario `root` L'utente può ancora (r)leggere, (w)scrivere ed elaborare i file in questadirectory.           *   `r-x`: I membri del gruppo `root` possono eseguire operazioni di (r)lettura e di (x)calcolo ma non possono modificare alcun file.           *   `r-x`: Chiunque altro non può modificare i file della directory `myscripts` ma può leggerli ed eseguirli.              Puoi utilizzare l'[Esempio 12-7](#docker_permissions_execute) per impostare i permessi quando crei delle directory all'interno di un'immagine.    ##### Esempio 12-7\\. Creare la cartella degli script e consentire l'esecuzione dei file (solo contenitori Ubuntu/Debian)    ```py RUN mkdir -p scripts `COPY` scripts scripts `` `RUN` chmod -R +x scripts `` ```   ```py` ``` ``Le istruzioni riportate nell'[Esempio 12-7](#docker_permissions_execute) ti permetteranno di configurare i permessi per eseguire i file nella directory `scripts` dall'interno del contenitore.    ###### Avvertenze    Quando si utilizzano i volumi dei container, bisogna fare attenzione ai mount bindings perché sostituiscono i permessi all'interno del container con quelli del filesystem host.    I problemi più frustranti quando si lavora con i container sono legati ai permessi del filesystem. Pertanto, sapere come impostare e correggere i permessi dei file ti farà risparmiare ore di sviluppo quando lavori con container che producono o modificano artefatti sulla macchina host.`` ```py ```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "```` ```py````", "```py` ```", "```py```", "``` ```", "```````py``` ``````py```````", "``````py``````", "```````py```````", "``````py``````", "```````py```````", "``````py``````", "```````py`````` ## Network+ di Docker    Il networking di Docker è uno dei concetti più difficili da comprendere nei progetti multicontainer. Questa sezione spiega come funziona il networking di Docker e come impostare i container locali per comunicare, simulando gli ambienti di produzione durante lo sviluppo.    Spesso, quando si esegue il deploy in ambienti di produzione nel cloud, si configura il networking utilizzando le soluzioni del cloud provider. Tuttavia, se hai bisogno di collegare i container in un ambiente di sviluppo per i test locali o per il deploy su risorse on-premises, allora ti sarà utile capire come funziona il networking di Docker.    Se stai sviluppando servizi GenAI che interagiscono con sistemi esterni come i database, è probabile che utilizzerai più container: uno per la tua applicazione e uno per l'esecuzione di ciascun database o sistema esterno.    Docker viene fornito con un sottosistema di rete che permette ai container di connettersi tra loro sullo stesso host o su host diversi. Puoi anche connettere i container tramite host rivolti a internet.    Quando crei dei container con il comando `docker run`, per impostazione predefinita avranno il networking abilitato su una *rete bridge* in modo da poter effettuare connessioni in uscita, ma non esporranno o pubblicheranno le loro porte al mondo esterno.    ###### Avvertenze    Con le impostazioni predefinite, Docker interagisce con i kernel del sistema operativo per configurare le *regole del firewall* (ad esempio, le regole `iptables` e `ip6tables` su Linux) per implementare l'isolamento della rete, la pubblicazione delle porte e ilfiltraggio.    Poiché Docker può ignorare queste regole del firewall, se una porta dell'host come `8000` è chiusa, Docker può forzarne l'apertura ed esporla all'esterno della macchina host quando si esegue un container con il flag `-p 8000:8000`. Per evitare questa esposizione, una soluzione è quella di eseguire i container utilizzando `-p 127.0.0.1:8000:8000`.    Per far funzionare il sottosistema di rete, Docker utilizza i *driver di rete*, come mostrato nella [Tabella 12-3](#docker_networking_drivers).      Tabella 12-3\\. Driver di rete di Docker   | Autista | Descrizione | Caso d'uso | | --- | --- | --- | | Ponte (predefinito) | Collega i container in esecuzione sullo stesso host del demone Docker. Le reti definite dall'utente possono sfruttare un server DNS incorporato. | Controlla la comunicazione dei container in reti Docker isolate con una semplice configurazione. | | Ospite | Rimuove il livello di isolamento tra i container e il sistema host, in modo che tutte le connessioni TCP/UDP siano accessibili direttamente tramite la rete dell'host, ad esempio localhost, senza la necessità di pubblicare le porte. | Semplificare l'accesso al container dalla rete host (ad esempio, localhost) o quando un container deve gestire un'ampia gamma di porte. | | Nessuno | Disattiva tutti i servizi di rete e isola i container in esecuzione all'interno dell'ambiente Docker. | Isolare i container da qualsiasi processo Docker e non Docker per motivi di sicurezza. Debug della rete o simulazione di interruzioni. Isolamento delle risorse e container transitori per processi di breve durata. | | Sovrapposizione | Collega i container tra più host/motore o in un cluster *Docker Swarm*.**Nota:** il motore Docker ha una modalità *swarm* che consente l'orchestrazione dei container tramite *cluster* di demoni/motore Docker. | Elimina la necessità di un routing a livello di sistema operativo quando si collegano i container tra gli host Docker. | | Macvlan | Assegna gli indirizzi mac ai contenitori come se fossero dispositivi fisici.Una configurazione errata può portare a un degrado involontario della rete a causa dell'esaurimento degli indirizzi IP, con conseguente diffusione delle VLAN (numero elevato di indirizzi mac) o modalità promiscua (sovrapposizione di indirizzi). | Utilizzato in sistemi o applicazioni legacy che monitorano il traffico di rete e che si aspettano di essere collegati direttamente a una rete fisica. | | IPVlan | Ti dà il controllo totale sull'indirizzamento dei container IPv4 e IPv6, fornendo un facile accesso ai servizi esterni senza bisogno di mappature delle porte. | Configurazione di rete avanzata che bypassa il tradizionale bridge di Linux per isolare, migliorare le prestazioni e semplificare la topologia di rete. |    Per assicurarti che i tuoi container possano comunicare tra loro, potrebbe essere necessario specificare le impostazioni e i driver di rete. Puoi selezionare un driver di rete adatto al tuo caso d'uso in base alla [Tabella 12-3](#docker_networking_drivers).    ###### Nota    Alcuni di questi driver potrebbero non essere disponibili a seconda della piattaforma/host OS su cui stai eseguendo Docker (host Windows, Linux o macOS).    I driver di rete più comunemente utilizzati sono bridge, host e nessuno. Probabilmente non avrai bisogno di utilizzare altri driver (ad esempio, overlay, Macvlan, IPVlan) a meno che tu non abbia bisogno di configurazioni di rete più avanzate.    La[Figura 12-9](#docker_networking_drivers_viz) visualizza le funzionalità dei driver bridge, host, none, overlay, Macvlan e IPVlan.  ![bgai 1209](assets/bgai_1209.png)  ###### Figura 12-9\\. Driver di rete di Docker    Analizziamo questi driver di rete in modo più dettagliato.    ### Driver di rete bridge    Il driver di rete bridge collega i container creando una rete bridge predefinita `docker0` e associando i container ad essa e all'interfaccia di rete principale dell'host, a meno che non sia specificato diversamente. In questo modo i container potranno accedere alla rete dell'host (e a internet) e tu potrai accedere ai container.    Puoi visualizzare le reti utilizzando il comando `docker network ls`:    ```py $ docker network ls NETWORK ID     NAME      DRIVER    SCOPE ```` 72ec0b2e6034   bridge    bridge    `local` ```py 53ec40b3c639   host      host      `local` `` 64368b7baa5f   none      null      `local` `` ``` ```py` ```   ```py```````", "``````py``` ``````", "``` $ docker network create genai-net ```", "``````py` ###### Nota    Quando crei reti definite dall'utente, Docker utilizza gli strumenti del sistema operativo host per gestire l'infrastruttura di rete sottostante, come l'aggiunta o la rimozione di dispositivi bridge e la configurazione delle regole di `iptables` su Linux.    Una volta creata la rete, puoi elencare le reti utilizzando il comando `docker network ls`:    ``` $ docker network ls NETWORK ID     NAME         DRIVER    SCOPE ```py 72ec0b2e6034   bridge       bridge    `local` `` 6aa21632e77e   genai-net    bridge    `local` `` ``` ```py   ``````", "````` ```py` La topologia della rete avrà ora l'aspetto della [Figura 12-10](#docker_networking_isolated).  ![bgai 1210](assets/bgai_1210.png)  ###### Figura 12-10\\. Reti a ponte isolate    Quando esegui i container, ora puoi collegarli alla rete creata utilizzando il flag `--network genai-net`:    ``` $ docker run --network genai-net genai-service $ docker run --network genai-net postgresql ```py   ``` ``###### Avvertenze    Su Linux, c'è un limite di 1.000 container che possono connettersi a una singola rete bridge a causa delle restrizioni del kernel Linux. Collegare più container a una singola rete bridge può renderla instabile e interrompere la comunicazione tra container.    Entrambi i container possono ora accedere l'uno all'altro sulla rete `genai-net` meglio isolata e definita dall'utente, con *risoluzione DNS* automatica tra i container.`` ```py ```` ```py`` `````", "``````py`  ``````", "````` ```py` #### DNS integrato    Docker sfrutta un server DNS incorporato con reti definite dall'utente, come mostrato nella [Figura 12-11](#docker_networking_bridge_dns), per mappare gli indirizzi IP interni in modo che i container possano raggiungerne uno per nome.  ![bgai 1211](assets/bgai_1211.png)  ###### Figura 12-11\\. DNS incorporato    Ad esempio, se chiami il tuo contenitore di applicazioni `genai-service` e il tuo contenitore di database `db`, allora il tuo contenitore `genai-service` può comunicare con il database chiamando il nome host `db`.    ###### Avvertenze    Non è possibile accedere al contenitore `db` dall'esterno della rete Docker bridge con il suo nome, poiché il server DNS incorporato non è visibile alla macchina host.    Invece, puoi esporre la porta del contenitore `5432` e accedere al contenitore `db` utilizzando la rete dell'host (ad esempio, tramite `localhost:5432`).    Parliamo ora di come pubblicare le porte dei container nell'ambiente esterno, come ad esempio il computer host.    #### Porte di pubblicazione    Quando esegui i container in una rete, questi espongono automaticamente le porte l'uno all'altro.    Se hai bisogno di accedere ai container dalla macchina host o da processi non Docker su reti diverse, dovrai esporre le porte dei container pubblicandole con il flag `--publish` o `-p`:    ``` $ docker run -p 127.0.0.1:8000:8000 myimage ```py   ``Questo comando ti permette di creare un contenitore con la porta esposta `8000` mappata sulla porta `8000` del computer host (ad esempio, localhost) utilizzando la sintassi `<host_port>:​<con⁠tainer_port>`.    Se non specifichi una porta del container, Docker pubblicherà e mapperà la porta `80` per impostazione predefinita.    ###### Avvertenze    Controlla sempre due volte le porte che vuoi esporre ed evita di pubblicare porte di container che sono già in uso sul tuo computer host. Altrimenti si creeranno *conflitti di porte* che porteranno le richieste a servizi in conflitto, il che richiederà molto tempo perla risoluzione dei problemi.    Se l'utilizzo delle reti bridge e delle mappature delle porte ti crea molti problemi, puoi anche utilizzare il driver di rete *host* per collegare i tuoi container, anche se senza gli stessi vantaggi di isolamento e sicurezza delle reti bridge.`` ```` ```py`` `````", "``````py`` ``````", "``` ```", "```py```", "```py```", "```py```", "````` ### Driver di rete host    Un driver di rete *host* è utile nei casi in cui vuoi migliorare le prestazioni, quando vuoi evitare la mappatura delle porte dei container o quando uno dei tuoi container deve gestire un gran numero di porte.    L'esecuzione di un container con il driver host è semplice come l'utilizzo del flag `--net=host` con il comando `docker run`:    ```py $ docker run --net=host genai-service ```   `Nel networking host, i container condividono lo spazio dei nomi di rete della macchina host, il che significa che i container non saranno isolati dall'host Docker. Pertanto, ai container non verrà assegnato un proprio indirizzo IP.    ###### Avvertenze    Non appena abiliti il driver di rete host, le porte pubblicate in precedenza verranno scartate, in quanto i container non avranno un proprio indirizzo IP.    Il driver di rete host è più performante perché non necessita di una *traduzione degli indirizzi di rete* (NAT) per mappare gli indirizzi IP da uno spazio dei nomi (container) a un altro (macchina host) ed evita di creare un *proxy di terra dell'utente* (cioè un port forwarding) per ogni porta. Tuttavia, la rete host è supportata solo dai container Linux e non Windows. Inoltre, i container non avranno accesso alle interfacce di rete dell'host e non potranno quindi effettuare il bind agli indirizzi IP dell'host, il che comporta una maggiore complessità nella configurazione di rete necessaria.`  ```py` ### Nessun driver di rete    Se vuoi isolare completamente lo stack di rete di un container, puoi utilizzare il flag `--network none` all'avvio del container. All'interno del container viene creato solo il dispositivo di loopback, un'interfaccia di rete virtuale che il container utilizza per comunicare con se stesso. Puoi specificare il driver di rete none utilizzando il seguente comando:    ``` $ docker run --network none genai-service ```py   `Questi sono alcuni casi in cui è utile isolare i contenitori:    *   Applicazioni che gestiscono dati altamente sensibili o processi critici           *   Dove c'è un rischio più elevato di attacchi basati sulla rete o di malware           *   Eseguire il debug della rete e simulare le interruzioni di rete eliminando le interferenze esterne.           *   L'esecuzione di container stand-alone senza dipendenze esterne può essere eseguitain modo indipendente           *   Gestione di contenitori transitori per processi di breve durata per ridurre al minimo l'esposizione della rete.              In generale, usa il driver di rete none se hai bisogno di isolare i container da qualsiasi processo Docker e non Docker per motivi di sicurezza.` ```` ```py`` `````", "``````py`  ``````", "```````py```````", "````` ```py`````", "```````py```````", "``````py``````", "```````py``` ## Abilitazione del driver della GPU    Se hai una scheda grafica NVIDIA con il toolkit CUDA e i driver necessari installati, puoi usare il flag `--gpus=all` per abilitare il supporto GPU per i tuoi container in Docker.^([5](ch12.html#id1336))    Per verificare che il tuo sistema abbia i driver necessari e che supporti la GPU in Docker, esegui il seguente comando per eseguire il benchmark della tua GPU:    ``` $ docker run --rm -it \\              --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody \\              -gpu \\              -benchmark  > Windowed mode > Simulation data stored in video memory > Single precision floating point simulation > 1 Devices used for simulation MapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM MapSMtoArchName for SM 8.9 is undefined.  Default to use Ampere GPU Device 0: \"Ampere\" with compute capability 8.9  > Compute 8.9 CUDA device: [NVIDIA GeForce RTX 4090] 131072 bodies, total time for 10 iterations: 75.182 ms = 2285.102 billion interactions per second = 45702.030 single-precision GFLOP/s at 20 flops per interaction ```py    ###### Suggerimento    Puoi anche usare lo strumento `nvidia-smi` dell'interfaccia di gestione del sistema NVIDIA per gestire e monitorarei dispositivi GPU NVIDIA.    I framework di deep learning come `tensorflow` o `pytorch` possono rilevare e utilizzare automaticamente il dispositivo GPU quando si eseguono le applicazioni in un container abilitato per le GPU. Questo include le librerie Hugging Face come `transformers` che consentono di auto-ospitare i modelli linguistici.    Se utilizzi il pacchetto `transformers`, assicurati di installare anche la libreria `accelerate`:    ``` $ pip install accelerate ```py    Ora puoi spostare il modello sulla GPU prima che venga caricato nella CPU utilizzando `device_map='cuda'`, come mostrato nell'[Esempio 12-8](#docker_gpu).    ##### Esempio 12-8\\. Trasferimento dei modelli Hugging Face alla GPU    ``` from transformers import pipeline  pipe = pipeline(     \"text-generation\",     model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",     device_map=\"cuda\" ) ```py    Dovresti essere in grado di eseguire le previsioni sulla GPU passando il flag `--gpus=all` a `docker run`.    ## Docker Compose    In ambienti multicontainer, puoi utilizzare lo strumento *Docker Compose* per definire ed eseguire i container delle applicazioni per un'esperienza di sviluppo e distribuzione semplificata.    L'uso di Docker Compose può aiutarti a semplificare la gestione di diversi container, reti, volumi, variabili e segreti con un unico *file di configurazione YAML*. Questo semplifica il complesso compito di orchestrare e coordinare i vari container, rendendo più facile la gestione e la replica dei tuoi servizi in diversi ambienti applicativi utilizzando le variabili d'ambiente. Puoi anche condividere il file YAML con altri utenti in modo che possano replicare il tuo ambiente di container. Inoltre, memorizza le configurazioni per evitare di ricreare i container quando riavvii i servizi.    L['esempio 12-9](#docker_compose) mostra un esempio di file di configurazione YAML.    ##### Esempio 12-9\\. File di configurazione YAML di Docker Compose    ``` # compose.yaml  services: ![1](assets/1.png)   server:     build: . ![2](assets/2.png)     ports:       - \"8000:8000\"     environment:       SHOW_DOCS_IN_PRODUCTION: $SHOW_DOCS_IN_PRODUCTION       ALLOWED_CORS_ORIGINS: $ALLOWED_CORS_ORIGINS     secrets:        - openai_api_token ![3](assets/3.png)     volumes:       - ./src/app:/code/app     networks:       - genai-net ![4](assets/4.png)    db:     image: postgres:12.2-alpine     ports:       - \"5433:5432\"     volumes:       - db-data:/etc/data     networks:       - genai-net  volumes:   db-data:     name: \"my-app-data\"  networks:   genai-net:     name: \"genai-net\"     driver: bridge  secrets:   openai_api_token:     environment: OPENAI_API_KEY ```py    [![1](assets/1.png)](#co_deployment_of_ai_services_CO4-1)      Crea i container insieme ai volumi, alle reti e ai segreti associati.      [![2](assets/2.png)](#co_deployment_of_ai_services_CO4-2)      Usa il Dockerfile che si trova nella stessa directory del file Compose per creare l'immagine `server`.      [![3](assets/3.png)](#co_deployment_of_ai_services_CO4-3)      Utilizza i segreti di Docker per mascherare dati sensibili come le chiavi API all'interno dell'ambiente shell del container.      [![4](assets/4.png)](#co_deployment_of_ai_services_CO4-4)      Crea una rete bridge `genai-net` e collegaad essa i contenitori `server` e `db` .      ###### Suggerimento    Se hai degli oggetti Docker come volumi e reti che gestisci tu stesso, puoi etichettarli con `external: true` nel file di composizione in modo che Docker Compose non li gestisca.    Una volta che hai un file `compose.yaml`, puoi utilizzare semplici comandi di composizione per gestire i tuoi contenitori:    ``` # Start services defined in compose.yaml $ docker compose up `# Stop and remove running services (won't remove created volumes and networks)` $ docker compose down ```py `# Monitor output of running containers` $ docker compose logs `` `# List all running services with their status` $ docker compose ps `` ``` ```py   ``````py`````` ```py```````", "``````py```` Puoi usare questi comandi per avviare/arrestare i servizi e visualizzare i loro log o lo stato dei container. Inoltre, puoi modificare il file Compose mostrato nell'[Esempio 12-9](#docker_compose) per utilizzare `watch` in modo che i servizi vengano aggiornati automaticamente quando modifichi e salvi il codice.    L['esempio 12-10](#docker_compose_watch) mostra come utilizzare l'istruzione `watch` su una determinata directory.    ##### Esempio 12-10\\. Abilitazione di Docker Compose `watch` su una determinata directory    ```py services: `server``:` ``````", "``````py` `develop``:` ``````", "````` `-` `action``:` `sync` ```py` `path``:` `./src` ``` `target``:` `/code` ```py ```` ```py`` `````", "``````py` ``````", "```   ```", "`````` ```py````` ```py```` Ogni volta che un file cambia nella cartella `./src` sul computer host, Compose sincronizza il suo contenuto con `/code` e aggiorna l'applicazione in esecuzione (servizio server) senza riavviarla.    Puoi quindi eseguire il processo `watch` utilizzando `docker compose watch`:    ```py $ docker compose watch `[`+`]` Running `2`/2 ````` ✔ Container project-server-1  Created     `0`.0s ```py` ✔ Container project-db-1      Recreated   `0`.1s ``` Attaching to db-1, server-1 `` ⦿ watch enabled `...` `` ```py ```` ```py`` ```   ```py``` ````` ```py` Docker Compose `watch` consente una granularità maggiore rispetto a quella praticata con i montaggi bind, come mostrato nell'[Esempio 12-9](#docker_compose). Ad esempio, ti permette di ignorare file specifici o intere directory all'interno dell'albero osservato per evitare problemi di prestazioni I/O.    Oltre a utilizzare Docker Compose `watch`, puoi unire e sovrascrivere più file Compose per creare una configurazione composita adatta a specifici ambienti di compilazione. In genere, il file `compose.yml` contiene le configurazioni di base, che possono essere sovrascritte da un file opzionale `compose.override.yml`. Ad esempio, come mostrato nell'[Esempio 12-11](#compose_override), puoi iniettare le impostazioni dell'ambiente locale, montare i volumi locali e creare un nuovo servizio di database.    ##### Esempio 12-11\\. Unire e sovrascrivere i file Compose per le configurazioni di compilazione specifiche dell'ambiente    ``` # compose.yml  services: ![1](assets/1.png)   server:       ports:         - 8000:8000       # ...       command: uvicorn main:app  # compose.override.yml  services: ![2](assets/2.png)   server:     environment:       - LLM_API_KEY=$LLM_API_KEY       - DATABASE_URL=$DATABASE_URL     volumes:       - ./code:/code     command: uvicorn main:app --reload    database:     image: postgres:latest     environment:       - POSTGRES_DB=genaidb       - POSTGRES_USER=genaiuser       - POSTGRES_PASSWORD=secretPassword!     volumes:       - db_data:/var/lib/postgresql/data  networks:   app-network:  volumes:   db_data: ```py    [![1](assets/1.png)](#co_deployment_of_ai_services_CO5-1)      Il file Compose di base contiene le istruzioni per eseguire la versione di produzione dell'applicazione.      [![2](assets/2.png)](#co_deployment_of_ai_services_CO5-2)      Sovrascrive le istruzioni di base sostituendo il comando di avvio del container, inietta variabili locali e aggiunge configurazioni di volume e di rete con un servizio di database locale.      Per utilizzare questi file, esegui il seguente comando:    ``` $ docker compose up ```py   `Docker Compose unirà automaticamente le configurazioni di entrambi i file Compose, applicando le impostazioni specifiche dell'ambiente dal file Compose sovrascritto.` ```` ```py`` ``````", "``````py` ``````", "``````py``` ``````", "```` ```py````", "```py` ```", "```py```", "```  ```", "```````py```````", "````` ```py`````", "```````py```` ```py```````", "```````py``` ## Abilitazione dell'accesso alla GPU in Docker Compose    Per accedere ai dispositivi GPU con i servizi gestiti da Docker Compose, dovrai aggiungere le istruzioni al file composto (vedi [Esempio 12-12](#DockerComposeapp)).    ##### Esempio 12-12\\. Aggiunta delle configurazioni della GPU al servizio app di Docker Compose    ``` services:   app:     # ...     deploy:       resources:         reservations:           devices:             - driver: nvidia               count: 1 ![1](assets/1.png)               capabilities: [gpu] ```py    [![1](assets/1.png)](#co_deployment_of_ai_services_CO6-1)      Limita il numero di dispositivi GPU accessibili dal servizio app.      Queste istruzioni ti daranno un controllo più granulare su come i tuoi servizi devono utilizzare le risorse della GPU.    ## Ottimizzare le immagini di Docker    Se le tue immagini Docker crescono di dimensioni, saranno anche più lente da eseguire, costruire e testare in produzione. Inoltre, passerai molto tempo in fase di sviluppo ad iterare lo sviluppo dell'immagine.    In questo caso, è importante comprendere le strategie di ottimizzazione delle immagini, compreso il modo in cui utilizzare il meccanismo di stratificazione di Docker per mantenere le immagini leggere ed efficienti da eseguire, in particolare con i carichi di lavoro GenAI.    Questi sono alcuni modi per ridurre le dimensioni delle immagini e velocizzare il processo di creazione:    *   Utilizzo di immagini di base minime           *   Evitare i runtime di inferenza su GPU           *   Esternalizzare i dati dell'applicazione           *   Ordinamento a strati e caching           *   Utilizzo di costruzioni in più fasi              L'implementazione di queste ottimizzazioni, come mostrato nella [Tabella 12-4](#build_optimization_impact), può ridurre le dimensioni tipiche delle immagini da diversi gigabyte a meno di 1 GB. Allo stesso modo, i tempi di creazione possono ridursi da diversi minuti in media a meno di un minuto.      Tabella 12-4\\. Impatto dell'ottimizzazione della creazione su un'immagine tipica^([a](ch12.html#id1343))   | Fase di ottimizzazione | Tempo di costruzione (secondi) | Dimensione dell'immagine (GB) | | --- | --- | --- | | Iniziale | 352.9 | 1.42 | | Utilizzo di immagini di base minime | 38.5 | 1.38 | | Usa la cache | 24.4 | 1.38 | | Ordinamento dei livelli | 17.9 | 1.38 | | Costruzioni in più fasi | 10.3 | 0.034 (34 MB) | | ^([a](ch12.html#id1343-marker)) Fonte: [warpbuild.com](https://www.warpbuild.com) |    Esaminiamo ciascuno di essi in modo più dettagliato, con esempi di codice per maggiore chiarezza.    ### Usa un'immagine di base minima    Le immagini di base ti permettono di partire da un'immagine preconfigurata in modo da non dover installare tutto da zero, compreso l'interprete Python. Tuttavia, alcune immagini di base disponibili su Docker Hub potrebbero non essere adatte per le distribuzioni di produzione. Al contrario, vorrai selezionare l'immagine di base giusta con un'impronta minima del sistema operativo da cui lavorare per ottenere build più veloci e dimensioni dell'immagine ridotte, possibilmente con dipendenze Python preinstallate e supporto per l'installazione dei vari pacchetti.    Le immagini base Alpine utilizzano una distribuzione Alpine Linux leggera, progettata per essere piccola e sicura, che contiene solo gli strumenti *minimi* essenziali per l'esecuzione della tua applicazione, ma non supporta l'installazione di molti pacchetti Python. D'altra parte, le immagini base slim possono utilizzare altre distribuzioni Linux come Debian o CentOS, che contengono gli strumenti essenziali *necessari* per l'esecuzione di applicazioni che le rendono più grandi delle immagini base Alpine.    ###### Suggerimento    Usa le immagini base slim se ti interessa il tempo di costruzione e le immagini base alpine se ti interessa la dimensione dell'immagine.    Puoi utilizzare le immagini di base di `slim`, come `python:3.12-slim`, o anche le immagini di base Alpine, come `python:3.12-alpine`, che possono avere una dimensione di 71,4 MB. Un'immagine Alpine \"bare-bones\" può addirittura scendere a 12,1 MB. Il comando seguente mostra un elenco di immagini base estratte dal repository di Docker:    ``` $ docker image ls REPOSITORY  TAG         IMAGE ID       CREATED         SIZE ```py` alpine      `3`.20        3463e98c969d   `4` weeks ago     `12`.1MB ``` python      `3`.12-alpine c6de2e87f545   `6` days ago      `71`.4MB ``python      `3`.12-slim   1ba4bc34383e   `6` days ago      186MB`` ```py ```` ```py   ``` `` `###### Suggerimento    Le immagini di dimensioni standard contengono in genere una distribuzione Linux completa, come Ubuntu o Debian, con una serie di pacchetti e dipendenze preinstallate, che le rendono adatte allo sviluppo locale ma forse non agli ambienti di produzione.` `` ```py  ``````py```````", "`````` ```py``````", "```````py` ``````py```````", "```` ### Evita i runtime di inferenza su GPU    Nei carichi di lavoro di intelligenza artificiale in cui stai servendo modelli ML/GenAI, potresti dover installare framework di deep learning, dipendenze e librerie per GPU che possono far esplodere improvvisamente l'ingombro delle tue immagini. Ad esempio, per fare inferenze su una GPU usando la libreria `transformers`, dovrai installare 3 GB di pacchetti NVIDIA per l'inferenza su GPU, 1,6 GB per `torch` per eseguire l'inferenza.    Purtroppo non è possibile ridurre le dimensioni dell'immagine se devi utilizzare una GPU per eseguire un'inferenza.Tuttavia, se puoi evitare l'inferenza tramite GPU e affidarti solo alla CPU, puoi ridurre le dimensioni dell'immagine fino a 10 volte utilizzando il runtime Open Neural Network Exchange (ONNX) con quantizzazione del modello.    Come discusso nel [Capitolo 10](ch10.html#ch10), puoi utilizzare la quantizzazione INT8 con un modello ONNX per beneficiare della compressione del modello senza perdere molto in qualità dell'output.    Per passare dal runtime di inferenza GPU al runtime ONNX per i modelli trasformatori Hugging Face, puoi usare il pacchetto `transformers[onnx]`:    ```py $ pip install transformers[onnx] ```    Puoi quindi esportare qualsiasi checkpoint del modello del trasformatore Hugging Face con le configurazioni predefinite nel formato ONNX con `transformers.onnx`:    ```py $ python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/ ```    Questo comando esporta il checkpoint del modello `distilbert/distilbert-base-uncased` come un grafico ONNX memorizzato in `onnx/model.onnx`, che può essere eseguito con qualsiasi acceleratore di modelli Hugging Face che supporti lo standard ONNX, come mostrato nell'[Esempio 12-13](#docker_onnx).    ##### Esempio 12-13\\. Inferenza del modello utilizzando il runtime ONNX con quantizzazione    ```py from onnxruntime import InferenceSession from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") session = InferenceSession(\"onnx/model.onnx\")  inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\") ![1](assets/1.png) output = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs)) ```    [![1](assets/1.png)](#co_deployment_of_ai_services_CO7-1)      Il runtime ONNX si aspetta come input gli array `numpy`.      Utilizzando una tecnica come quella illustrata nell'[Esempio 12-13](#docker_onnx), è possibile ridurre le dimensioni delle immagini da 5 a 10 GB a circa 0,5 GB, il che rappresenta un'enorme riduzione dell'ingombro, significativamente più conveniente e scalabile.    ### Esternalizzare i dati dell'applicazione    Uno dei principali fattori che contribuiscono alle dimensioni dell'immagine è la copia dei modelli e dei dati dell'applicazione nell'immagine durante la creazione. Questo approccio aumenta sia il tempo di creazione che le dimensioni dell'immagine.    Un approccio migliore consiste nell'utilizzare volumi durante lo sviluppo locale e soluzioni di archiviazione esterne per scaricare e caricare i modelli all'avvio dell'applicazione in produzione. Negli ambienti di orchestrazione di container Kubernetes, puoi anche utilizzare volumi persistenti per l'archiviazione dei modelli.    ###### Suggerimento    Se il contenitore dell'applicazione impiega molto tempo per scaricare i dati e gli artefatti del modello da un'origine esterna, i controlli sullo stato di salute possono fallire e la piattaforma di hosting può chiudere prematuramente i contenitori. In questi casi, configura le sonde di controllo dello stato di salute in modo che attendano più a lungo o, come ultima risorsa, inserisci il modello nell'immagine.    ### Ordinamento dei livelli e caching    Docker utilizza un filesystem a livelli per creare livelli in un'immagine per ogni istruzione del file Docker. Questi livelli sono come una pila, con ogni livello che aggiunge altro contenuto in cima ai livelli precedenti. Ogni volta che un livello viene modificato, quel livello (e altri livelli) dovrà essere ricostruito per far sì che le modifiche appaiano nell'immagine (cioè, la cache di compilazione deve essere invalidata).    Viene creato un livello (cioè un'istantanea del filesystem) se l'istruzione sta scrivendo o cancellando dei file nel filesystem union del contenitore.    ###### Suggerimento    Le istruzioni di Dockerfile che modificano il filesystem come `ENV`, `COPY`, `ADD` e `RUN` contribuiscono a creare nuovi livelli nel processo di compilazione, aumentando di fatto le dimensioni dell'immagine. D'altra parte, istruzioni come `WORKDIR`, `ENTRYPOINT`, `LABEL` e `CMD` che aggiornano solo i metadati dell'immagine non creano alcun livello e alcuna cache di compilazione.    Dopo la creazione, ogni livello viene memorizzato nella cache per essere riutilizzato in tutte le ricostruzioni dell'immagine, se le istruzioni e i file da cui dipende non sono stati modificati dall'ultima compilazione. Per questo motivo, l'ideale è scrivere un file Docker che ti permetta di fermare, distruggere, ricostruire e sostituire i contenitori con una configurazione minima.    Ci sono alcune tecniche che puoi utilizzare per ridurre al minimo e ottimizzare il più possibile questi livelli.    #### Ordinamento dei livelli per evitare l'invalidazione frequente della cache    Poiché le modifiche ai livelli precedenti possono invalidare la cache di compilazione e quindi ripetere i passaggi, dovresti ordinare i tuoi Dockerfile da quelli più stabili (ad esempio, le installazioni) a quelli che cambiano più frequentemente o sono volatili (ad esempio, il codice dell'applicazione, i file di configurazione).    Seguendo questo ordine, posiziona le istruzioni più stabili ma costose (ad esempio, il download di modelli o l'installazione di dipendenze pesanti) all'inizio del file Docker e le operazioni volatili e veloci (ad esempio, la copia del codice dell'applicazione) in fondo.    Immagina che il tuo file Dockerfile abbia questo aspetto:    ```py FROM python:3.12-slim as base `# Changes to the` ``` `COPY` . . `` `RUN` pip install requirements.txt `` ```py ```   ```py``` ````", "```py` In questo caso stai creando un livello copiando la tua cartella di lavoro contenente il codice dell'applicazione nell'immagine prima di scaricare e installare le dipendenze.    Se uno qualsiasi dei file sorgente cambia, Docker builder invaliderà la cache causando la ripetizione dell'installazione delle dipendenze, che è costosa e può richiedere diversi minuti per essere completata, se non viene memorizzata nella cache da `pip`.    Per evitare di ripetere passaggi costosi, puoi ordinare logicamente le istruzioni del tuo file Docker per ottimizzare la cache del livello, riordinando istruzioni come queste:    ```", "```py `RUN` pip install requirements.txt `` `COPY` . . `` ```", "```py   ```", "```py ```", "```py`` ```", "```py  ```", "```py`` ```", "```py` ```", "```py #### Riduci al minimo i livelli    Per mantenere le dimensioni delle immagini ridotte, dovrai ridurre al minimo i livelli di immagine.    Una tecnica semplice per ottenere questo risultato è quella di combinare più istruzioni `RUN` in una sola. Ad esempio, invece di scrivere più installazioni `RUN apt-get`, puoi combinarle in un unico comando `RUN` con `&&`:    ```", "```py   ``In questo modo si evita di aggiungere livelli non necessari e si prevengono i problemi di cache con `apt-get update` che utilizza la tecnica del *cache busting*.    Poiché il costruttore può potenzialmente saltare l'aggiornamento dell'indice dei pacchetti, causando il fallimento delle installazioni o l'utilizzo di pacchetti obsoleti, l'utilizzo di `&&` assicura che vengano installati i pacchetti più recenti se l'indice dei pacchetti viene aggiornato.    ###### Suggerimento    Puoi anche usare il flag `--no-cache` quando utilizzi `docker build` per evitare gli hit della cache e garantire un download fresco delle immagini di base e delle dipendenze a ogni compilazione.``  ```", "```py **/.DS_Store **/__pycache__ **/.mypy_cache **/.venv **/.env **/.git ```", "```py RUN --mount=type=cache,target=/root/.cache/huggingface && \\     pip install transformers && \\     python -c \"from transformers import AutoModel; \\  AutoModel.from_pretrained('bert-base-uncased')\" ```", "```py` #### Usa una cache esterna    Se stai costruendo e distribuendo container utilizzando una pipeline CI/CD, puoi trarre vantaggio da una cache esterna ospitata in una posizione remota. Una cache esterna può accelerare drasticamente il processo di compilazione nelle pipeline CI/CD dove i costruttori sono spesso effimeri e i minuti di compilazione sono preziosi.    Per utilizzare una cache esterna, puoi specificare le opzioni `--cache-to` e `--cache-from` con il comando `docker buildx build`:    ```", "```py   `Oltre all'ordinamento dei livelli e all'ottimizzazione della cache, puoi utilizzare le costruzioni multi-stadio per ridurre significativamente le dimensioni delle immagini.` ```", "```py`` ```", "```py ```", "```py` ```", "```py``  ```", "```py```", "````py``` ````", "```````py`` ``````py```````", "``` # Stage 1: Base `FROM` `python:3.11.0-slim` `as` `base` ```", "```` `RUN` pip install transformers `&&` `\\`     python -c `\"from transformers import AutoModel; \\`  `AutoModel.from_pretrained('bert-base-uncased')\"` ```py `RUN` --mount`=``type``=`cache,target`=`/root/.cache/pip `\\`     --mount`=``type``=`bind,source`=`requirements.txt,target`=`requirements.txt `\\`     python -m pip install -r requirements.txt ``` ```py` ````", "```py   ```", "```py```", "````py ````", "`````` ```py``````", "``` # Stage 2: Production `FROM` `base` `as` `production` ```", "```````py `RUN` apt-get update `&&` apt-get install -y ``````py``` `COPY` --from`=`base /opt/venv /opt/venv ``````py`` `COPY` --from`=`base /root/.cache/huggingface /root/.cache/huggingface ``````py` `WORKDIR` `/code` ``````py `COPY` . . ````` `EXPOSE` `8000` ```py` `ENV` `BUILD_ENV``=`PROD ``` `CMD` `[``\"uvicorn\"``,` `\"main:app\"``,` `\"--host\"``,` `\"0.0.0.0\"``,` `\"--port\"``,` `\"8000\"``]` ```py ```` ```py`` ``````py ``````py` ``````py`` ``````py``` ``````py```` ```py   ``````py```` ```py`````` ```py````` L'ultima fase copia l'ambiente virtuale Python della fase di produzione con i pacchetti installati, aggiunge diversi strumenti di sviluppo e avvia il server con la funzione di ricarica a caldo:    ```py # Stage 3: Development `FROM` `production` `as` `development` ``````py` `COPY` --from`=`production /opt/venv /opt/venv ``````py `COPY` ./requirements_dev.txt ./ ````` `RUN` pip install --no-cache-dir --upgrade -r requirements_dev.txt ```py` `ENV` `BUILD_ENV``=`DEV ``` `CMD` `[``\"uvicorn\"``,` `\"main:app\"``,` `\"--host\"``,` `\"0.0.0.0\"``,` `\"--port\"``,` `\"8000\"``,` `\"--reload\"``]` ```py ```` ```py`` ``````py ``````py` ```   ```py` ``` ``Utilizzando un unico file Docker, siamo stati in grado di creare tre fasi distinte e di utilizzarle a nostro piacimento tramite il comando `--target development` quando necessario.`` ```py ```` ```py````` ```py`````` ```py```````", "``````py````` ```py``````", "``````py``````", "``````py``````", "``````py``````", "``````py``````", "``` ```", "```py```", "````py````", "```py```", "````py````", "```py` ```", "```py```", "````py````", "```py```", "````` ```py`## Docker init    Ora hai una conoscenza approfondita del processo di containerizzazione con la piattaforma Docker e delle relative best practice.    Se hai bisogno di aggiungere Docker a un progetto esistente, puoi usare il comando `docker init`, che ti guiderà attraverso una procedura guidata per creare tutti i file di distribuzione Docker necessari nella tua directory di lavoro corrente:    ``` $ docker init >> Answer a few questions in the terminal...  project/ │ ├── .dockerignore ├── compose.yaml ├── Dockerfile └── README.Docker.md ... # other application files ```py    Questo ti fornirà un ottimo punto di partenza su cui potrai lavorare per includere ulteriori passaggi di configurazione, dipendenze o servizi, a seconda delle necessità.    ###### Suggerimento    Ti consiglio di usare `docker init` quando inizi, perché ogni file generato aderirà alle migliori pratiche, tra cui l'uso di `dockerignore`, l'ottimizzazione dei livelli di immagine, l'uso di bind e cache mount per l'installazione dei pacchetti e il passaggio a utenti non root.    Una volta che hai un'immagine ottimizzata e un set di container funzionanti, puoi scegliere qualsiasi provider Cloud o soluzione self-hosting per inviare le immagini ai registri e distribuire i tuoi nuovi servizi GenAI.```` ```py`` `````", "``````py``````", "```````py ``````py```````", "```````py` ``````py```````", "```````py`` ``````py```````", "```````py``` ``````py```````", "```````py```` ```py```````", "```````py```````", "``````py``````", "```````py`````` ```py```````", "```````py```````", "``` ```", "```py```", "````py````", "```py```", "````py` ````", "```````py```````", "```````py`` ``````py```````", "```````py```````", "``` ```", "```py```", "````py````", "```py```", "````py````", "```py```", "````py````", "```py```", "````py````", "```py`  ```", "```py ``# Riassunto    In questo capitolo abbiamo esaminato varie strategie per distribuire i tuoi servizi GenAI, ad esempio su macchine virtuali, come funzioni cloud, con piattaforme di servizi app gestiti o tramite container. In questo capitolo abbiamo spiegato come la virtualizzazione si differenzia dalla containerizzazione e perché potresti voler distribuire i tuoi servizi come container.    Poi hai imparato a conoscere la piattaforma di containerizzazione Docker e come puoi usarla per creare immagini autocontenute delle tue applicazioni che possono essere eseguite come contenitori.    Abbiamo parlato dei meccanismi di archiviazione e di rete di Docker che ti permettono di conservare i dati utilizzando il filesystem union nei container e di come collegare i container con diversi driver di rete.    Infine, ti sono state presentate varie tecniche di ottimizzazione per ridurre i tempi di creazione e le dimensioni delle immagini per distribuire i tuoi servizi GenAI nel modo più efficiente possibile.    Con i servizi containerizzati, puoi inviarli ai registri dei container per condividerli, distribuirli ed eseguirli su qualsiasi cloud o ambiente di hosting di tua scelta.    ^([1](ch12.html#id1291-marker)) Le immagini Python di base Slim bilanciano le dimensioni e la compatibilità della distribuzione Linux con una gamma più ampia di pacchetti Python rispetto alle immagini Python di base Alpine che riducono al minimo le dimensioni ma richiedono configurazioni extra.    ^([2](ch12.html#id1292-marker)) Puoi utilizzare il flag `-p` o `--publish` durante l'esecuzione del container per mappare e abilitare l'accesso al container tramite una porta.    ^([3](ch12.html#id1297-marker)) Le immagini create su una macchina possono essere eseguite solo su altre macchine con la stessa architettura di processore.    ^([4](ch12.html#id1317-marker)) Puoi comunque eseguire i file eseguibili con il solo permesso `r` utilizzando il comando `bash script.sh` invece di `./script.sh`.    ^([5](ch12.html#id1336-marker)) Consulta la documentazione NVIDIA su come installare il toolkit CUDA e i driver grafici più recenti per il tuo sistema.`` ```", "```py` ```", "```py```", "````py````", "```py```", "````py````", "```py```", "````py````", "```py```", "````py````", "```py```", "``````py``````", "```````py```````", "```````"]