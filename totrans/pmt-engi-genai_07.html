<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Introduction to Diffusion Models for &#10;Image Generation"><div class="chapter" id="intro_image_07">
<h1><span class="label">Chapter 7. </span>Introduction to Diffusion Models for 
<span class="keep-together">Image Generation</span></h1>


<p>This chapter introduces the most popular diffusion models for AI image generation. You’ll learn the benefits and limitations of each of the top models, so that you can be confident in choosing between them based on the task at hand.</p>

<p>Introduced in 2015, <em>diffusion models</em> are a class of <a data-type="indexterm" data-primary="diffusion models" id="id1131"/><a data-type="indexterm" data-primary="image generation" data-secondary="diffusion models" id="iggdffm"/>generative models that have shown spectacular results for generating images from text. The <a data-type="indexterm" data-primary="DALL-E 2" id="id1132"/>release of <a href="https://oreil.ly/dalle2">DALL-E 2</a> in 2022 marked a great leap forward in the quality of generated images from diffusion models, with open source <a href="https://oreil.ly/gjNJ_">Stable Diffusion</a>, and community favorite <a href="https://oreil.ly/j51L0">Midjourney</a> quickly following to forge a competitive category. With <a data-type="indexterm" data-primary="DALL-E 3" id="id1133"/>the integration of <a href="https://oreil.ly/dalle3">DALL-E 3</a> into ChatGPT, the lines will continue to blur between text and image generation. However, advanced users will likely continue to require direct access to the underlying image generation model, to get the best results.</p>

<p>Diffusion models are trained <a data-type="indexterm" data-primary="diffusion models" data-secondary="training" id="id1134"/><a data-type="indexterm" data-primary="diffusion models" data-secondary="denoising" id="id1135"/><a data-type="indexterm" data-primary="denoising" id="id1136"/>by many steps of <a href="https://oreil.ly/OrAHA">adding random noise</a> to an image and then predicting how to reverse the diffusion process by <em>denoising</em> (removing noise). The approach comes from physics, where it has been used for simulating how particles <em>diffuse</em> (spread out) through a medium. The predictions are conditioned on the description of the image, so if the resulting image doesn’t match, the neural network weights of the model are adjusted to make it better at predicting the image from the description. When trained, the model is able to take random noise and turn it into an image that matches the description provided in the prompt.</p>

<p class="pagebreak-before"><a data-type="xref" href="#figure-7-1">Figure 7-1</a> illustrates the denoising process, as demonstrated by Binxu Wang in <a href="https://oreil.ly/57szp">“Mathematical Foundation of Diffusion Generative Models”</a>.</p>

<figure><div id="figure-7-1" class="figure">
<img src="assets/pega_0701.png" alt="pega 0701" width="600" height="218"/>
<h6><span class="label">Figure 7-1. </span>Diffusion schematics</h6>
</div></figure>

<p>These models were trained on large datasets of billions of images scraped from the internet (and accompanying captions) and can therefore replicate most popular art styles or artists. This has been the source of much controversy, as copyright holders seek to <a href="https://oreil.ly/a4Fyp">enforce their legal claims</a>, while model creators argue in favor of fair use.</p>

<p>A diffusion model is not simply a “complex collage tool” that regurgitates replicas of copyrighted images: it’s only a few gigabytes in size and therefore can’t possibly contain copies of all its training data. When researchers attempted to reproduce 350,000 images from Stable Diffusion’s training data, they only succeeded with 109 of them (<a href="https://oreil.ly/SGn9B">Carlini et al.</a>, 2023).</p>

<p>What the model is doing is more analogous to a human artist looking at every image on the internet <a data-type="indexterm" data-primary="diffusion models" data-secondary="vector representation" id="id1137"/><a data-type="indexterm" data-primary="diffusion models" data-secondary="latent space" id="id1138"/><a data-type="indexterm" data-primary="Stable Diffusion" id="id1139"/><a data-type="indexterm" data-primary="diffusion models" data-secondary="Stable Diffusion" id="id1140"/><a data-type="indexterm" data-primary="vector representation, diffusion models" id="id1141"/><a data-type="indexterm" data-primary="latent space" data-secondary="diffusion models" id="id1142"/>and learning the patterns that define every subject and style. These patterns are encoded as a <em>vector representation</em> (a list of numbers) referring to a location in <em>latent space</em>: a map of all possible combinations of images that could be generated by the model. The prompt input by the user is first encoded into vectors; the diffusion model then generates an image matching these vectors, before the resulting image is decoded back into pixels for the user.</p>

<p class="pagebreak-before"><a data-type="xref" href="#figure-7-2">Figure 7-2</a> illustrates the encoding and decoding process, from Ian Stenbit’s <a href="https://oreil.ly/qOpis">“A Walk Through Latent Space with Stable Diffusion”</a>.</p>

<figure><div id="figure-7-2" class="figure">
<img src="assets/pega_0702.png" alt="pega 0702" width="600" height="263"/>
<h6><span class="label">Figure 7-2. </span>Encoding and decoding process</h6>
</div></figure>

<p>These vectors, also referred <a data-type="indexterm" data-primary="embeddings" data-secondary="diffusion models" id="id1143"/><a data-type="indexterm" data-primary="diffusion models" data-secondary="vector representation" data-tertiary="embeddings" id="id1144"/><a data-type="indexterm" data-primary="vector representation, diffusion models" data-secondary="embeddings" id="id1145"/>to as <em>embeddings</em>, act as a location or address for a point in the model’s map of every image, and as such images that are similar will be closer together in latent space. The latent space is continuous, and you can travel between two points (interpolate) and still get valid images along the way. For example, if you interpolate from a picture of a dog to a bowl of fruit, the intermediate images will be coherent-looking images, demonstrating a progressive shift between the two concepts.</p>

<p><a data-type="xref" href="#figure-7-3">Figure 7-3</a> contains a grid, also from Ian Stenbit, showing the <a href="https://oreil.ly/cjm8A">intermediate steps between four images</a>: a dog (top left), a bowl of fruit (top right), the Eiffel Tower (bottom left), and a skyscraper (bottom right).</p>

<figure><div id="figure-7-3" class="figure">
<img src="assets/pega_0703.png" alt="pega 0703" width="600" height="596"/>
<h6><span class="label">Figure 7-3. </span>A random walk through latent space</h6>
</div></figure>

<p>Within the domain of diffusion models, prompt engineering can be seen as navigating the latent space, searching for an image that matches your vision, out of all of the possible images available. There are many techniques and best practices for locating the right combination of words to conjure up your desired image, and an active community of AI artists and researchers have worked to build a set of tools to help. Each model and method has its own quirks and behaviors depending on its architecture, training method, and the data on which it was trained. The three main organizations responsible for building the most popular text-to-image diffusion models have all taken radically different approaches in terms of business models and functionality, and as such there is a greater diversity of choice in diffusion models than there is in the OpenAI-dominated <a data-type="indexterm" data-primary="image generation" data-secondary="diffusion models" data-startref="iggdffm" id="id1146"/> LLM space.</p>






<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="OpenAI DALL-E"><div class="sect1" id="id110">
<h1>OpenAI DALL-E</h1>

<p>In January 2021, OpenAI released the <a data-type="indexterm" data-primary="OpenAI" data-secondary="DALL-E" data-see="DALL-E" id="id1147"/><a data-type="indexterm" data-primary="DALL-E" id="id1148"/><a data-type="indexterm" data-primary="diffusion models" data-secondary="DALL-E" data-see="DALL-E" id="id1149"/><a data-type="indexterm" data-primary="image generation" data-secondary="diffusion models" data-tertiary="DALL-E" id="igdfdll"/><a data-type="indexterm" data-primary="GPT-3, DALL-E and" id="id1150"/>text-to-image model DALL-E, its name being a play on surrealist artist Salvador Dali and the Pixar animated robot WALL-E. The model was based on a modified version of OpenAI’s remarkable GPT-3 text model, which had been released seven months before. DALL-E was a breakthrough in generative AI, demonstrating artistic abilities most people thought were impossible for a computer to possess. <a data-type="xref" href="#figure-7-4">Figure 7-4</a> shows an example of the <a href="https://oreil.ly/dalle1">first version</a> of DALL-E’s capabilities.</p>

<figure><div id="figure-7-4" class="figure">
<img src="assets/pega_0704.png" alt="pega 0704" width="600" height="340"/>
<h6><span class="label">Figure 7-4. </span>DALL-E capabilities</h6>
</div></figure>

<p>The DALL-E model was not open sourced nor <a data-type="indexterm" data-primary="DALL-E Mini" id="id1151"/><a data-type="indexterm" data-primary="Craiyon" id="id1152"/>released to the public, but it inspired multiple researchers and hobbyists to attempt to replicate the research. The most popular of these models was DALL-E Mini, released in July 2021 (renamed Craiyon a year later at the request of OpenAI), and although it gained a cult following on social media, the quality was considerably poorer than the official DALL-E model. OpenAI published a <a href="https://oreil.ly/EqdtP">paper announcing DALL-E 2</a> in April 2022, and the quality was significantly higher, attracting a waitlist of one million people. <a data-type="xref" href="#figure-7-5">Figure 7-5</a> shows an example of the now iconic astronaut riding a horse image from the paper that captured the public’s imagination.</p>

<figure><div id="figure-7-5" class="figure">
<img src="assets/pega_0705.png" alt="pega 0705" width="600" height="600"/>
<h6><span class="label">Figure 7-5. </span>DALL-E 2 image quality</h6>
</div></figure>

<p>Access was limited to waitlist users until September 2022, due to concerns about AI ethics and safety. Generation of images containing people was initially banned, as were a long list of <a data-type="indexterm" data-primary="DALL-E 2" id="id1153"/>sensitive words. Researchers identified DALL-E 2 <a href="https://oreil.ly/ot4vw">adding the words <em>black</em> or <em>female</em></a> to some image prompts like a photo of a doctor in a hamfisted attempt to address bias inherited from the dataset (images of doctors on the internet are disproportionally of white males).</p>

<p>The team added inpainting and outpainting to <a data-type="indexterm" data-primary="DALL-E 2" data-secondary="Outpainting" id="id1154"/>the user interface in August 2022, which was a further leap forward, garnering attention in the press and on social media. These features allowed users to generate only selected parts of an image or to <em>zoom out</em> by generating around the border of an existing image. However, users have little control over the parameters of the model and could not fine-tune it on their own data. The model would generate garbled text on some images and struggled with <a data-type="indexterm" data-primary="image generation" data-secondary="deformed body parts" id="id1155"/><a data-type="indexterm" data-primary="deformed body parts in images" id="id1156"/>realistic depictions of people, generating disfigured or deformed hands, feet, and <a data-type="indexterm" data-primary="image generation" data-secondary="diffusion models" data-tertiary="DALL-E" data-startref="igdfdll" id="id1157"/>eyes, as demonstrated in <a data-type="xref" href="#figure-7-6">Figure 7-6</a>.</p>

<figure><div id="figure-7-6" class="figure">
<img src="assets/pega_0706.png" alt="pega 0706" width="600" height="190"/>
<h6><span class="label">Figure 7-6. </span>Deformed hands and eyes</h6>
</div></figure>

<p>Google’s Imagen demonstrated impressive <a data-type="indexterm" data-primary="Imagen" id="id1158"/><a data-type="indexterm" data-primary="diffusion models" data-secondary="Imagen" id="id1159"/><a data-type="indexterm" data-primary="image generation" data-secondary="diffusion models" data-tertiary="Imagen" id="id1160"/>results and was introduced in a paper in May 2022 (<a href="https://oreil.ly/sFaeW">Ho et al.</a>, 2022), but the model was not made available to the general public, citing AI ethics and safety concerns. Competitors like Midjourney (July 2022) moved quickly and capitalized on huge demand from people who had seen impressive demos of DALL-E on social media but were stuck on the waitlist. The open source release <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="open source" id="id1161"/><a data-type="indexterm" data-primary="diffusion models" data-secondary="Stable Diffusion" id="id1162"/>of Stable Diffusion (August 2022) broke what had seemed to be an unassailable lead for OpenAI just a few months before. Although the rollout of the more advanced <a href="https://oreil.ly/dalle3">DALL-E 3 model</a> as a feature of ChatGPT has helped OpenAI regain lost ground, and Google has gotten into the game with <a href="https://oreil.ly/XzQrU">Gemini 1.5</a>, there remains everything to play for.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Midjourney"><div class="sect1" id="id111">
<h1>Midjourney</h1>

<p>In July 2022, just three months after <a data-type="indexterm" data-primary="diffusion models" data-secondary="Midjourney" id="dffmjry"/><a data-type="indexterm" data-primary="Midjourney" id="mdjrny"/><a data-type="indexterm" data-primary="image generation" data-secondary="diffusion models" data-tertiary="Midjourney" id="ggffmjy"/>the release of DALL-E 2, Midjourney put its v3 model in open beta. This was a uniquely good time to launch an image generation model, because the demonstrations of what DALL-E 2 could do from early users looked like magic, and yet access was initially limited. Eager early-adopters flocked to Midjourney, and its notable fantasy aesthetic gained a cult following among the gaming and digital art crowds, showcased in the <a href="https://oreil.ly/dqshh">now famous image</a>, which won first prize in a digital art competition, in <a data-type="xref" href="#figure-7-7">Figure 7-7</a>.</p>

<figure><div id="figure-7-7" class="figure">
<img src="assets/pega_0707.png" alt="pega 0707" width="600" height="363"/>
<h6><span class="label">Figure 7-7. </span>Théâtre d’Opéra Spatial</h6>
</div></figure>

<p>Midjourney was one of the first viable image models that had a business model and commercial license, making it suitable for more than just experimentation. The subscription model was favored by many artists accustomed to paying monthly for other software like Adobe Photoshop. It also helped the creative process to not be charged per image generated, particularly in the early days when you’d have to try multiple images before you found one that was high-enough quality. If you were a paying customer of Midjourney, you owned the rights to any image generated, unlike DALL-E, where OpenAI was retaining the copyright.</p>

<p>Unique to Midjourney is its heavy community focus. To use the tool, you must sign into a <a href="https://oreil.ly/JKZzD">Discord server</a> (<a data-type="xref" href="#figure-7-8">Figure 7-8</a>) and submit your prompt in an open channel or direct message. Given that all image generations are shared in open channels by default, and private mode is available only on the <a href="https://oreil.ly/OV46r">most expensive plan</a>, the vast majority of images created through Midjourney are available for others to learn from. This led to rapid copying and iteration between users, making it easy for novices to quickly learn from others. As early as July 2022, the Discord community was nearing 1 million people (shown in <a data-type="xref" href="#figure-7-8">Figure 7-8</a>), and a year later, there were more than 13 million members.</p>

<figure><div id="figure-7-8" class="figure">
<img src="assets/pega_0708.png" alt="pega 0708" width="600" height="327"/>
<h6><span class="label">Figure 7-8. </span>Midjourney’s Discord server, July 2022</h6>
</div></figure>

<p>When you find an image you like, you can click a button to <em>upscale</em> the image (make it higher resolution) for use. Many have speculated that this procedure acts as training data for reinforcement learning, similar to <a href="https://oreil.ly/3ISZk">reinforcement learning from human feedback</a> (RLHF), the method touted as the key to success of ChatGPT. In addition, the team regularly asks for ratings of images generated by newer models in order to improve the performance. Midjourney released v4 of its model in November 2022, followed by v5 in March 2023 and v6 in December 2023. The quality is significantly improved: hands and eyes issues identified in <a data-type="xref" href="#figure-7-6">Figure 7-6</a> have largely gone away, and the model has a larger stylistic range, demonstrated in <a data-type="xref" href="#figure-7-9">Figure 7-9</a>.</p>

<p>Input:</p>

<pre data-type="programlisting">a group of best friends women eating salads and laughing
while high fiving in a coffee shop, cinematic lighting</pre>

<p><a data-type="xref" href="#figure-7-9">Figure 7-9</a> shows the output.</p>

<figure><div id="figure-7-9" class="figure">
<img src="assets/pega_0709.png" alt="pega 0709" width="600" height="600"/>
<h6><span class="label">Figure 7-9. </span>Women eating salads and laughing</h6>
</div></figure>

<p>Remarkably, the Midjourney team has remained small, with just <a href="https://oreil.ly/YrmA_">11 employees</a> as of March 2023. The founder of Midjourney, David Holz, formerly of hardware startup Leap Motion, <a href="https://oreil.ly/jeFYV">confirmed in an interview</a> that the company was already profitable as of August 2022. What is even more remarkable is that without the billions of dollars of funding that OpenAI enjoys, the team has built significant functionality over what’s available in DALL-E, including negative prompting (removing concepts from an image), weighted terms (increasing the prevalance of other concepts), and their <em>describe</em> feature (reverse engineering the prompt from an uploaded image). However, there is no API available; the only way to access the model is through Discord, which has likely acted as a drag on <a data-type="indexterm" data-primary="diffusion models" data-secondary="Midjourney" data-startref="dffmjry" id="id1163"/><a data-type="indexterm" data-primary="Midjourney" data-startref="mdjrny" id="id1164"/><a data-type="indexterm" data-primary="image generation" data-secondary="diffusion models" data-tertiary="Midjourney" data-startref="ggffmjy" id="id1165"/>mainstream adoption.</p>
</div></section>






<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Stable Diffusion"><div class="sect1" id="id112">
<h1>Stable Diffusion</h1>

<p>While DALL-E 2’s waitlist continued to <a data-type="indexterm" data-primary="diffusion models" data-secondary="Stable Diffusion" id="dffsbff"/><a data-type="indexterm" data-primary="Stable Diffusion" id="stbff"/><a data-type="indexterm" data-primary="image generation" data-secondary="diffusion models" data-tertiary="Stable Diffusion" id="gngfmdsd"/>build, researchers from the CompVis Group at LMU Munich and applied research company Runway ML received a donation of computing power from Stability AI to train Stable Diffusion. The model shocked the generative AI world when it was released open source in August 2022, because the results were comparable to DALL-E 2 and Midjourney, but it could be run for free on your own computer (assuming you had a modest GPU with 8GB VRAM). Stable Diffusion had one of the <a href="https://oreil.ly/pwPGX">fastest climbs in GitHub stars of any software</a>, rising to 33,600 stars in its first 90 days (<a data-type="xref" href="#figure-7-10">Figure 7-10</a>).</p>

<figure><div id="figure-7-10" class="figure">
<img src="assets/pega_0710.png" alt="pega 0710" width="600" height="421"/>
<h6><span class="label">Figure 7-10. </span>GitHub developer adoption of Stable Diffusion</h6>
</div></figure>

<p>The move to open source the model was controversial, and raised concerns about AI ethics and safety. Indeed, many of the initial use cases were to generate AI porn, as evidenced by the not safe for work (NSFW) models <a data-type="indexterm" data-primary="AI porn" id="id1166"/><a data-type="indexterm" data-primary="porn, AI" id="id1167"/><a data-type="indexterm" data-primary="Civitai" id="id1168"/>shared on platforms like <a href="https://civitai.com">Civitai</a>. However, the ability for hobbyists and tinkerers to modify and extend the model, as well as fine-tune it on their own data, led to rapid evolution and improvement of the model’s functionality. The decision to surface all of the model’s parameters to users, such as Classifier Free Guidance (how closely to follow a prompt), Denoising (how much noise to add to the base image for the model to remove during inference), and Seed (the random noise to start denoising from), has led to more creativity and innovative artwork. The accessibility and reliability of open source have also enticed several small businesses to build on top of Stable Diffusion, such as Pieter Level’s <a href="https://photoai.com">PhotoAI</a> and <a href="http://interiorai.com">InteriorAI</a> (together raking in more than $100,000 in monthly revenue), and Danny Postma’s <a href="https://www.headshotpro.com">Headshot Pro</a>. As well as matching DALL-E’s inpainting and <a data-type="indexterm" data-primary="Headshot Pro" id="id1169"/>outpainting functionality, open source contributions have also kept pace with Midjourney’s features, such as negative prompts, weighted terms, and the ability to reverse engineer prompts from images. In addition, advanced functionality like ControlNet (matching the posture or composition of an image) <a data-type="indexterm" data-primary="ControlNet" id="id1170"/><a data-type="indexterm" data-primary="Segment Anything" id="id1171"/>and Segment Anything (clicking on an element to generate a mask for inpainting), have been quickly added as extensions for use with Stable Diffusion (both released in April 2023), most commonly accessed via <a href="https://oreil.ly/0inw3">AUTOMATIC1111’s web UI</a> (<a data-type="xref" href="#figure-7-11">Figure 7-11</a>).</p>

<figure><div id="figure-7-11" class="figure">
<img src="assets/pega_0711.png" alt="pega 0711" width="600" height="464"/>
<h6><span class="label">Figure 7-11. </span>AUTOMATIC1111’s web UI for Stable Diffusion</h6>
</div></figure>

<p>Version 1.5 of Stable Diffusion was released in October 2022 and is still in use today. Therefore, it will form the basis for the ControlNet examples in <a data-type="xref" href="ch10.html#building_applications_10">Chapter 10</a>, the advanced section for image generation in this book. The weights for Stable Diffusion were released on Hugging Face, introducing a generation of AI engineers to the open source AI model hub. Version 2.0 of Stable Diffusion came out a month later in November 2022, trained on a more aesthetic subset of the original <a href="https://oreil.ly/K5vX2">LAION-5B dataset</a> (a large-scale dataset of image and text pairs for research purposes), with NSFW (not safe for work) images filtered out. Power users of Stable Diffusion complained of censorship as well as a degradation in model performance,  <a href="https://oreil.ly/2mgh5">speculating</a> that NSFW images in the training set were necessary to generate realistic human anatomy.</p>

<p>Stability AI <a href="https://oreil.ly/BT-k5">raised over $100 million</a> and has continued to develop newer models, including <a href="https://oreil.ly/UCQ3I">DeepFloyd</a>, a model better able to generate real text on images (an issue that plagues other models) and the current favorite <a href="https://oreil.ly/gcT4t">Stable Diffusion XL 1.0</a> (abbreviated to SDXL). This model has overcome the misgivings of the community over censorship in version 2.0, not least due to the impressive results of this more powerful model, which <a data-type="indexterm" data-primary="diffusion models" data-secondary="Stable Diffusion" data-startref="dffsbff" id="id1172"/><a data-type="indexterm" data-primary="Stable Diffusion" data-startref="stbff" id="id1173"/>has 6.6 billion parameters, compared with 0.98 billion for the v1.5 model.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Google Gemini"><div class="sect1" id="id113">
<h1>Google Gemini</h1>

<p>Google long threatened to be a competitor <a data-type="indexterm" data-primary="diffusion models" data-secondary="Google Gemini" id="id1174"/><a data-type="indexterm" data-primary="Google Gemini" id="id1175"/><a data-type="indexterm" data-primary="image generation" data-secondary="diffusion models" data-tertiary="Google Gemini" id="id1176"/>in the space with their <a href="https://oreil.ly/K8oWv">Imagen</a> model (not <a href="https://oreil.ly/341QB">released publicly</a>), and indeed ex-Googlers have since founded a promising new image model <a href="https://ideogram.ai">Ideogram</a>, released in August 2023. They finally entered the image generation game with Gemini in December 2023, though quickly faced criticism over a clumsy attempt to <a href="https://oreil.ly/u-Glg">promote diversity</a>. It remains to be seen whether Google’s internal politics will prevent them from capitalizing on their significant resources.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Text to Video"><div class="sect1" id="id114">
<h1>Text to Video</h1>

<p>Much of the attention in the image space is also likely to shift toward <em>text-to-video</em>, <em>image-to-video</em>, and even <em>video-to-video</em>, as the Stable Diffusion community <a href="https://oreil.ly/l7KHB">extends the capabilities</a> of the model to generate <a data-type="indexterm" data-primary="diffusion models" data-secondary="Stable Diffusion" data-tertiary="text-to-video" id="id1177"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="text-to-video" id="id1178"/><a data-type="indexterm" data-primary="text-to-video, Stable Diffusion" id="id1179"/><a data-type="indexterm" data-primary="diffusion models" data-secondary="Stable Diffusion" data-tertiary="image-to-video" id="id1180"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="image-to-video" id="id1181"/><a data-type="indexterm" data-primary="image-to-video, Stable Diffusion" id="id1182"/><a data-type="indexterm" data-primary="diffusion models" data-secondary="Stable Diffusion" data-tertiary="video-to-video" id="id1183"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="video-to-video" id="id1184"/><a data-type="indexterm" data-primary="video-to-video, Stable Diffusion" id="id1185"/>consistent images frame by frame, including promising open source <a data-type="indexterm" data-primary="AnimateDiff" id="id1186"/>projects such as <a href="https://oreil.ly/CsJgT">AnimateDiff</a>. In addition, one of the cocreators of Stable Diffusion, RunwayML, has <a data-type="indexterm" data-primary="RunwayML" id="id1187"/><a data-type="indexterm" data-primary="Gen-2 model" id="id1188"/><a data-type="indexterm" data-primary="Stable Video Diffusion" id="id1189"/><a data-type="indexterm" data-primary="Stable Video Turbo" id="id1190"/><a data-type="indexterm" data-primary="Sora" id="id1191"/>become the leading pioneer in text-to-video, and is starting to get usable results with their <a href="https://oreil.ly/vS0mA">Gen-2 model</a>. <a href="https://oreil.ly/UuApM">Stable Video Diffusion</a> was released in  November 2023, capable of turning text into short video clips or animating existing images, and <a href="https://oreil.ly/uMAkh">Stable Diffusion Turbo</a> can generate images in near real time. The release of <a href="https://oreil.ly/sora">Sora</a> in February 2024 shows that OpenAI isn’t sleeping on this space either. Although we don’t cover text-to-video prompting techniques explicitly, everything you learn about prompting for image generation also applies directly to video.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Model Comparison"><div class="sect1" id="id183">
<h1>Model Comparison</h1>

<p>As demand for AI image generation <a data-type="indexterm" data-primary="diffusion models" data-secondary="model comparison" id="id1192"/>increases and competition heats up, new entrants will emerge, and the major players will diversify. In our own workflows we already find ourselves using different models for different reasons. DALL-E 3 is great at composition, and the integration with ChatGPT is convenient. Midjourney still has the best aesthetics, both for fantasy and photorealism. Stable Diffusion being open source makes it the most flexible and extendable model, and is what most AI businesses build their products on top of. Each model has evolved toward a distinct style and set of capabilities, as can be discerned when comparing the same prompt <a data-type="indexterm" data-primary="image generation" data-secondary="diffusion models" data-tertiary="Stable Diffusion" data-startref="gngfmdsd" id="id1193"/>across multiple models, as in <a data-type="xref" href="#figure-7-12">Figure 7-12</a>.</p>

<p id="comparing_image_models">Input:</p>

<pre data-type="programlisting">a corgi on top of the Brandenburg Gate</pre>

<p><a data-type="xref" href="#figure-7-12">Figure 7-12</a> shows the output.</p>

<figure><div id="figure-7-12" class="figure">
<img src="assets/pega_0712.png" alt="pega 0712" width="600" height="221"/>
<h6><span class="label">Figure 7-12. </span>A corgi on top of the Brandenburg Gate</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id357">
<h1>Summary</h1>

<p>In this chapter, you were introduced to diffusion models for AI image generation. These models, such as DALL-E, Stable Diffusion, and Midjourney, use random noise and denoising techniques to generate images based on text descriptions. They have been trained on large datasets and can replicate various art styles. However, there is controversy surrounding copyright issues. You learned how prompt engineering principles apply to image generation when navigating the latent space to find the desired image.</p>

<p>In this chapter, you explored the different approaches taken by organizations like OpenAI, Stability AI, and Midjourney in developing text-to-image models. OpenAI’s DALL-E gained popularity for its artistic abilities, but access was limited, and the quality of replicated models was poorer. Midjourney, on the other hand, capitalized on the demand for DALL-E alternatives and gained a cult following with its v3 and v4 models. It had a subscription-based pricing model and a strong community focus. Stable Diffusion, on the other hand, gained attention for its comparable results to DALL-E and Midjourney, but with the advantage of being open source and free to run on personal computers. By reading this chapter, you  also gained insights into the history of AI image generation and the advancements made by organizations like OpenAI, Midjourney, and Stable Diffusion.</p>

<p>In the next chapter, you will learn practical tips for handling image generation with AI. The chapter will equip you with the necessary knowledge and techniques to create visually stunning and unique images. From format modifiers to art-style replication, you will discover the power of prompt engineering in creating captivating and original visual content. Get ready to unleash your creativity and take your image generation skills to the next level.</p>
</div></section>
</div></section></div></div></body></html>