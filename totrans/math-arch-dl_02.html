<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="chapter-intro-vec-mat">3 Classifiers and vector calculus</h1>
<p class="body"><a id="marker-83"/>We took a first look at the core concept of machine learning in section <a class="url" href="../Text/01.xhtml#sec-cat_brain">1.3</a>. Then, in section <a class="url" href="02.xhtml#subsec-hyper-planes-classifiers">2.8.2</a>, we examined classifiers as a special case. But so far, we have skipped the topic of error minimization: given one or more training examples, how do we adjust the weights and biases to make the machine closer to the desired ideal? We will study this topic in this chapter by discussing the concept of gradients.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The complete PyTorch code for this chapter is available at <a class="url" href="http://mng.bz/4Zya">http://mng.bz</a> <a class="url" href="http://mng.bz/4Zya">/4Zya</a> in the form of fully functional and executable Jupyter notebooks.</p>
<h2 class="fm-head" id="sec3_1">3.1 Geometrical view of image classification</h2>
<p class="body">To fix our ideas, consider a machine that classifies whether an image contains a car or a giraffe. Such classifiers, with only two classes, are known as <i class="fm-italics">binary classifiers</i>. The first question is how to represent the input.</p>
<h3 class="fm-head1" id="input-representation">3.1.1 Input representation</h3>
<p class="body">The car-versus-giraffe scenario belongs to a special class of problems where we are analyzing a visual scene. Here, the inputs are the brightness levels of various points in the 3D scene projected onto a 2D image plane. Each element of the image represents a point in the actual scene and is referred to as a <i class="fm-italics">pixel</i>. The image is a two-dimensional array representing the collection of pixel values at a given instant in time. It is usually scaled to a fixed size, say <span class="math">224 × 224</span>. As such, the image can be viewed as a matrix:</p><!--<p class="Body"><span class="times">$$X = \begin{bmatrix}
X_{0,0}   &amp; X_{0,1}   &amp; \cdots &amp; X_{0, 223}\\[-2pt]
X_{1,0}   &amp; X_{1,1}   &amp; \cdots &amp; X_{1, 223}\\[-2pt]
\vdots    &amp; \vdots    &amp; \vdots &amp; \vdots    \\[-2pt]
X_{223,0} &amp; X_{223,1} &amp; \cdots &amp; X_{223, 223}
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="140" src="../../OEBPS/Images/eq_03-00-a.png" width="304"/></p>
</div>
<p class="body">Each element of the matrix, <i class="timesitalic">X<sub class="fm-subscript">i, j</sub></i>, is a pixel color value in the range <span class="math">[0,255]</span>.</p>
<p class="fm-head2" id="image-rasterization">Image rasterization</p>
<p class="body"><a id="marker-84"/>In the previous chapters, we have always seen a <i class="fm-italics">vector</i> as the input to a machine learning system. The vector representation of the input allowed us to view it as a point in a high-dimensional space. This led to many geometric insights about classification. But here, our input is an image, which is akin to a <i class="fm-italics">matrix</i> rather than a vector. Can we represent an image (matrix) as a vector?</p>
<p class="body">The answer is yes. A matrix can always be converted into a vector by a process called <i class="fm-italics">rasterization</i>. During rasterization, we iterate over the elements of the matrix from left to right and top to bottom, storing successive encountered elements into a vector. The result is the rasterized vector. It has the same elements as the original matrix, but they are organized differently. The length of the rasterized vector is equal to the product of the row count and column count of the matrix. The rasterized vector for the earlier matrix <i class="timesitalic">X</i> has <span class="math">224 × 224 = 50176</span> elements</p><!--<p class="Body"><span class="times">$\vec{x} = \begin{bmatrix}
x_{0} = X_{0,0}\\[-2pt]
x_{1} = X_{0,1}\\[-2pt]
\vdots\\[-2pt]
x_{223} = X_{0, 223}\\[-2pt]
x_{224} = X_{1, 0}\\[-2pt]
x_{225} = X_{1, 1}\\[-2pt]
\vdots\\[-2pt]
x_{50175} = X_{223, 223}
\end{bmatrix}^{\phantom{h}}_{\phantom{h}}$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="267" src="../../OEBPS/Images/eq_03-00-b.png" width="179"/></p>
</div>
<p class="body">where <span class="math"><i class="fm-italics">x<sub class="fm-subscript">i</sub></i> <span class="cambria">∈</span> [0,255]</span> are values of the image pixels. Thus, a <span class="math">224 × 224</span> input image can be viewed as a vector (equivalently, a point) in a <span class="math">50, 176</span>-dimensional space.</p>
<h3 class="fm-head1" id="classifiers-as-decision-boundaries">3.1.2 Classifiers as decision boundaries</h3>
<p class="body">We see that input images can be converted to vectors via rasterization. Each vector can be viewed as a point in a high-dimensional space. But the points corresponding to any given object or class, say <i class="fm-italics">giraffe</i> or <i class="fm-italics">car</i>, are not distributed randomly all over the space. Rather, they occupy a small portion subspace) in the vast high-dimensional space of inputs. This is because there is always inherent commonality in members of a class. For instance, all giraffes are predominantly yellow with a bit of black, and cars have a somewhat fixed shape. This causes the pixel values in images containing the same object to have somewhat similar values. Overall, this means points belonging to a class loosely form a <i class="fm-italics">cluster</i>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Geometrically speaking, a classifier is a hypersurface that separates the point clusters for the classes we want to recognize. This surface forms a <i class="fm-italics">decision boundary</i>—the decision about which class a specific input point belongs to is made by looking at which side of the surface the point belongs to.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="600" id="fig-difficult_classifier_diagram" src="../../OEBPS/Images/CH03_F01a_Chaudhury.png" width="394"/></p>
<p class="figurecaption">(a) Car vs. giraffe classifier</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="600" id="fig-" src="../../OEBPS/Images/CH03_F01b_Chaudhury.png" width="480"/></p>
<p class="figurecaption">(b) Horse vs. zebra classifier</p>
</div>
<p class="figurecaption" id="fig-classifier_diagram">Figure 3.1 Geometric depiction of a classification problem. In the multidimensional input space, each data instance corresponds to a point. In figure <a class="url" href="#fig-classifier_diagram">3.1a</a>, the points marked <i class="fm-italics">c</i> denote cars, and points marked <i class="fm-italics">g</i> denote giraffes. This is a simple case: the points form reasonably distinct clusters, so the classification can be done with a relatively simple surface, a hyperplane. The exact parameters of the hyperplane—orientation and position—are determined via training. In figure <a class="url" href="#fig-classifier_diagram">3.1b</a>, the points marked <i class="fm-italics">h</i> denote horses, and those marked <i class="fm-italics">z</i> denote zebras. This case is a bit more difficult: the classification has to be done with a curved (nonplanar) surface, a hypersphere. The parameters of the hypersphere—radius and center—are determined via training.<a id="marker-85"/></p>
<p class="body">Figure <a class="url" href="#fig-classifier_diagram">3.1a</a> shows an example of a rasterized space for the giraffe and car classification problem. The points corresponding to a giraffe are marked <i class="fm-italics">g</i>, and those corresponding to a car are marked <i class="fm-italics">c</i>. This is a simple case. Here, the classifier surface (aka decision boundary) that separates the cluster of points corresponding to <i class="fm-italics">car</i> from those corresponding to <i class="fm-italics">giraffe</i> is a hyperplane, depicted in figure <a class="url" href="#fig-classifier_diagram">3.1a</a>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> We often call surfaces <i class="fm-italics">hypersurfaces</i> and planes <i class="fm-italics">hyperplanes</i> in greater than three dimensions.</p>
<p class="body">Figure <a class="url" href="#fig-classifier_diagram">3.1b</a> shows a more difficult example: horse and zebra classification in images. Here the points corresponding to horses are marked <i class="fm-italics">h</i> and those corresponding to zebras are marked <i class="fm-italics">z</i>. In this example, we need a nonlinear (curved) surface (such as a hypersphere) to separate the two classes.</p>
<h3 class="fm-head1" id="modeling-in-a-nutshell">3.1.3 Modeling in a nutshell</h3>
<p class="body"><a id="marker-86"/>Unfortunately, in the typical scenario, we do not know the separating surface. In fact, we do not even know all the points belonging to a class of interest. All we know is a <i class="fm-italics">sampled</i> set of inputs <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i></span> (training inputs) and corresponding classes <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_y2.png" width="15"/></span><sub class="fm-subscript">i</sub></i> (the ground truth). The complete set of training inputs plus ground truth—<span class="math">{<i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">i</sub>, <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_y2.png" width="15"/></span><sub class="fm-subscript">i</sub></i>}</span> for a large set of <i class="timesitalic">i</i> values—is called the <i class="fm-italics">training data</i>. When we want to teach a baby to recognize a car, we show the baby several example cars and say “This is a car.” The training data plays the same role for a neural network.</p>
<p class="body">From only this training dataset <span class="math">{<i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">i</sub>, <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_y2.png" width="15"/></span><sub class="fm-subscript">i</sub></i>} <span class="cambria">∀</span><i class="fm-italics"><sub class="fm-subscript">i</sub> <span class="cambria">∈</span></i> [1, <i class="fm-italics">n</i>]</span>, we have to identify a good enough approximation of the general classifying surface that when presented with a random scene, we can map it to an input point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, check which side of the surface that point lies on, and identify the class (car or giraffe). This process of developing a best guess for a surface that forms a decision boundary between various classes of interest is called <i class="fm-italics">modeling the classifier</i>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The ground truth labels (<i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_y2.png" width="15"/></span><sub class="fm-subscript">i</sub></i>) for the training images <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i></span> are often created manually. This process of manually generating labels for the training images is one of the most painful aspects of machine learning, and significant research effort is going on at the moment to mitigate it.</p>
<p class="body">As indicated in section <a class="url" href="../Text/01.xhtml#sec-cat_brain">1.3</a>, modeling has two steps:</p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Model architecture selection</i>: Choose the parametric model function <span class="math"><i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span>. This function takes an input vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> and emits the class <i class="timesitalic">y</i>. It has a set of parameters <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="timesitalic">b</i>, which are unknown at first. This function is typically chosen from a bank of well-known functions that are tried and tested; for simple problems, we may choose a linear model, and for more complex problems, we choose nonlinear models. The model designer makes the choice based on their understanding of the problem. Remember, at this point the parameters are still unknown—we have only decided on the <i class="fm-italics">function family</i> for the model.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Model training</i>: Estimate the parameters <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> such that <i class="timesitalic">ϕ</i> emits the known correct output (as closely as possible) on the training data inputs. This is typically done via an iterative process. For each training data instance <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">i</sub></span>, we evaluate <span class="math"><i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">i</sub></i> = <i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i </sub></i>;<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span>. This emitted output is compared with the corresponding known outputs <i class="timesitalic">ȳ<sub class="fm-subscript">i</sub></i>. Their difference, <span class="math"><i class="fm-italics">e<sub class="fm-subscript">i</sub></i> = ||<i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">i</sub></i> − <i class="fm-italics">ȳ<sub class="fm-subscript">i</sub></i>||</span>, is called the <i class="fm-italics">training error</i>. The sum of training errors over all training data is the aggregate training error. We iteratively adjust the parameters <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="timesitalic">b</i> such that the aggregate training error keeps going down. This means at each iteration, we adjust the parameters so the model output <span class="math"><i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">i</sub></i></span> moves a little closer to the target output <i class="timesitalic">ȳ<sub class="fm-subscript">i</sub></i> for all <i class="timesitalic">i</i>. Exactly how to adjust the parameters to reduce the error forms the bulk of this chapter and will be introduced in section <a class="url" href="../Text/03.xhtml#sec-grad">3.3</a>.</p>
</li>
</ol>
<p class="body">The function <span class="math"><i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span> represents the decision boundary hypersurface. For example, in the binary classification problem depicted in figure <a class="url" href="#fig-classifier_diagram">3.1</a>, <span class="math"><i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span> may represent a plane (shown by the dashed line). Points on one side of the plane are classified as cars, while points on the other side are classified as giraffes. Here,</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>) = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i></span></p>
<p class="body">From equation <a class="url" href="02.xhtml#eq-plane-1">2.14</a> we know this equation represents a plane.</p>
<p class="body">In figure <a class="url" href="#fig-classifier_diagram">3.1b</a>, a good planar separation does not exist—we need a nonlinear separator, such as the spherical separator shown with dashed lines. Here,</p><!--<p class="Body"><span class="times">$$\phi\left(\vec{x};\vec{w}, b\right) =\vec{x}^{T}\begin{bmatrix} w_{0} &amp; 0      &amp; \cdots &amp; 0\\ 0     &amp; w_{1}  &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 0     &amp; 0      &amp; \cdots &amp; w_{n}
\end{bmatrix}\vec{x} + b = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="148" src="../../OEBPS/Images/eq_03-00-c.png" width="364"/></p>
</div>
<p class="body">This equation represents a sphere.</p>
<p class="body"><a id="marker-87"/>It should be noted that in typical real-life cases, the separating surface does not correspond to any known geometric surface (see figure <a class="url" href="#fig-real_life_classifier_diagram">3.2</a>). But in this chapter, we will continue to use simple examples to bring out the underlying concepts.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="600" id="fig-real_life_classifier_diagram" src="../../OEBPS/Images/CH03_F02_Chaudhury.png" width="480"/></p>
<p class="figurecaption">Figure 3.2 In real-life problems, the surface is often not a well-known surface like a plane or sphere. And often, the classification is not perfect—some points fall on the wrong side of the separator.</p>
</div>
<h3 class="fm-head1" id="sec-sign-sep-surface">3.1.4 Sign of the surface function in binary classification</h3>
<p class="body">In the special case of binary classifiers, the <i class="fm-italics">sign</i> of the expression <span class="math"><i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>;<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span> representing the decision boundary has a special significance. To see this, consider a line in a <span class="math">2</span>D plane corresponding to the equation<a id="marker-88"/></p>
<p class="fm-equation"><span class="math"><i class="fm-italics">y</i> + 2<i class="fm-italics">x</i> + 1 = 0</span></p>
<p class="body">All points <i class="fm-italics">on</i> the line have <span class="math"><i class="fm-italics">x</i>, <i class="fm-italics">y</i></span> coordinate values satisfying this equation. The line divides the 2D plane into two half planes. All points on one half plane have <span class="math"><i class="fm-italics">x</i>, <i class="fm-italics">y</i></span> values such that <span class="math"><i class="fm-italics">y</i> + 2<i class="fm-italics">x</i> + 1</span> is negative. All points in the other half plane have <span class="math"><i class="fm-italics">x</i>, <i class="fm-italics">y</i></span> values such that <span class="math"><i class="fm-italics">y</i> + 2<i class="fm-italics">x</i> + 1</span> is positive. This is shown in figure <a class="url" href="#fig-line_sign">3.3</a>. This idea can be extended to other surfaces and higher dimensions. Thus, in binary classification, once we have estimated an optimal decision surface <span class="math"><i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span>, given any input vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, we can compute the sign of <span class="math"><i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span> to predict the class.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="599" id="fig-line_sign" src="../../OEBPS/Images/CH03_F03_Chaudhury.jpg" width="817"/></p>
<p class="figurecaption">Figure 3.3 Given a point <span class="math">(<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>, <i class="timesitalic">y</i><sub class="fm-subscript">0</sub>)</span> and a separator <span class="math"><i class="timesitalic">y</i> + 2<i class="fm-italics">x</i> + 1 = 0</span>, we can tell which side of the separator the point lies on from the sign of <span class="math"><i class="timesitalic">y</i><sub class="fm-subscript">0</sub> + 2<i class="fm-italics">x</i><sub class="fm-subscript">0</sub> + 1</span>.</p>
</div>
<h2 class="fm-head" id="error-aka-loss-function">3.2 Error, aka loss function</h2>
<p class="body">As stated earlier, during training, we adjust the parameters <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> so that the error keeps going down. Let’s derive a quantitative expression for this error aka loss function). Later, we will see how to minimize it.</p>
<p class="body">Overall, training data consists of a set of labeled inputs (training data instances paired with known ground truths):</p><!--<p class="Body"><span class="times">$$\begin{aligned} \vec{x}^{\left(0\right)}, \;\;\; &amp;\bar{y}^{\left(0\right)})\\[-2pt] \vec{x}^{\left(1\right)},  \;\;\;
&amp;\bar{y}^{\left(1\right)})\\[-2pt]
\vdots, \;\;\;  &amp;\vdots\\[-2pt] \vec{x}^{\left(N\right)},  \;\;\;
&amp;\bar{y}^{\left(N\right)})\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="128" src="../../OEBPS/Images/eq_03-00-d.png" width="108"/></p>
</div>
<p class="body">Now we define a <i class="fm-italics">loss function</i>. On a specific training data instance, the loss function effectively measures the error made by the machine on that particular training data—input-target pair <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>, <i class="fm-italics">y</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)</span>. Although there are many sophisticated error functions more suitable for this problem, for now, let’s use a squared error function for the sake of simplicity (introduced in section <a class="url" href="02.xhtml#subsection-vector_length">2.5.4</a>). The squared error on the <i class="timesitalic">i</i>th training data element is the squared difference between the output yielded by the model and the expected or target output:</p><!--<p class="Body"><span class="times">(<i class="fm-italics">e</em><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>)<sup class="FM-Superscript">2</sup> = (<i class="fm-italics">ϕ</em>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>;<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_w.png" /></span>, <i class="fm-italics">b</em>)−<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>)<sup class="FM-Superscript">2</sup></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="47" src="../../OEBPS/Images/eq_03-01.png" width="252"/></p>
</div>
<p class="fm-equation-caption">Equation 3.1 <span class="calibre" id="eq-err-one-input-instance"/></p>
<p class="body">The total loss (aka squared error) during training is</p><!--<p class="Body"><span class="times">$$L\left(\vec{w}, b\right) = E^{2}\left(\vec{w}, b\right) = \sum_{i=0}^{i=N}\left(e^{\left(i\right)}\right)^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_03-02.png" width="267"/></p>
</div>
<p class="fm-equation-caption">Equation 3.2 <span class="calibre" id="eq-err-total"/></p>
<p class="body">Note that this total error is not a function of any specific training data instance. Rather, it is the <i class="fm-italics">overall error over the entire training data set</i>. This is what we minimize by adjusting <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and <i class="timesitalic">b</i>. To be precise, we estimate the <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and <i class="timesitalic">b</i> that will minimize <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span>.</p>
<h2 class="fm-head" id="sec-grad">3.3 Minimizing loss functions: Gradient vectors</h2>
<p class="body"><a id="marker-89"/>The goal of training is to estimate the weights and bias parameter <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> that will minimize <i class="timesitalic">L</i>. This is usually done by an iterative process. We start with random values of <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> and adjust these values so that the loss <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>) = <i class="fm-italics">E</i><sup class="fm-superscript">2</sup>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span> declines rapidly. Doing this many times is likely to take us close to the optimal values for <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span>. This is the essential idea behind the process of training a model. It is important to note that we are minimizing the total error: this prevents us from over-indexing on any particular training instance. If the training data is a well-sampled set, the parameters <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> that minimize loss over the training dataset will also work well during inferencing.</p>
<p class="body">How do we “adjust” <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> so that the value of loss <span class="math"><i class="fm-italics">L</i> = <i class="fm-italics">E</i><sup class="fm-superscript">2</sup></span> declines? This is where gradients come in. For any function <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span>, the gradient with respect to <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span>—that is, <span class="math">∇<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics1">b</i></sub><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span>—indicates the direction along which the maximum change in <i class="timesitalic">L</i> occurs. The gradient is the analog of a derivative in 1D calculus. Intuitively, going down along the direction of the gradient of a function seems like the best strategy for minimizing the function value.</p>
<p class="body">Geometrically speaking, if we start at an arbitrary point on the surface corresponding to <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span> and move along the direction of the gradient <span class="math">∇<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics1">b</i></sub><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span>, we will go toward the minimum at the fastest rate (this is discussed in detail throughout the rest of this section). Hence, during training, we iteratively move toward the minimum by taking steps along <span class="math">∇<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics1">b</i></sub><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span>. Note that <i class="fm-italics">the gradient is with respect to weights, not the input</i>. The overall algorithm is shown in algorithm <a class="url" href="../Text/03.xhtml#alg-supervised_training_detailed">3.2</a>.</p>
<div class="calibre3">
<p class="fm-algorithm-caption" id="alg-supervised_training_detailed">Algorithm 3.2 Training a supervised model (overall idea)</p>
<p class="algorithm-body">Initialize <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> with random values</p>
<p class="algorithm-body"><b class="fm-bold">while</b> <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span>) &gt; <i class="fm-italics">threshold</i> <b class="fm-bold">do</b></p><!--<p class="algorithm-body"><span class="times">$\begin{bmatrix}
\vec{w}\\b
\end{bmatrix} = \begin{bmatrix}
\vec{w}\\b
\end{bmatrix} - \mu \nabla_{\vec{w}, b}L\left(\vec{w}, b\right)$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="71" src="../../OEBPS/Images/eq_03-02-a.png" width="210"/></p>
</div>
<p class="algorithm-body">    Recompute <i class="timesitalic">L</i> on new <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span>.</p>
<p class="algorithm-body"><b class="fm-bold">end</b> <b class="fm-bold">while</b></p>
<p class="algorithm-body"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><sub class="fm-subscript">*</sub> ← <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i><sub class="fm-subscript">*</sub> ← <i class="fm-italics">b</i></span></p>
</div>
<p class="body"><a id="marker-90"/>Note the following points:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">In each iteration, we are adjusting <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> along the gradient of the error function. We will see in section <a class="url" href="../Text/03.xhtml#sec-grad">3.3</a> that this is the direction of maximum change for <i class="timesitalic">L</i>. Thus, <i class="timesitalic">L</i> is reduced at a maximal rate.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">μ</i> is the learning rate: larger values imply longer steps, and smaller values imply shorter steps. The simplest approach, outlined in algorithm <a class="url" href="../Text/03.xhtml#alg-supervised_training_detailed">3.2</a>, takes equal-sized steps everywhere. In later chapters, we will study more sophisticated approaches where we try to sense how close to the minimum we are and vary the step size accordingly:</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">We take longer steps when far from the minimum, to progress quickly.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We take shorter steps when near the minimum, to avoid overshooting it.</p>
</li>
</ul>
</li>
<li class="fm-list-bullet">
<p class="list">Mathematically, we should keep iterating until the loss becomes minimal (that is, the gradient of the loss is zero). But in practice, we simply iterate until the accuracy is good enough for the purpose at hand.</p>
</li>
</ul>
<h3 class="fm-head1" id="sec-gradient">3.3.1 Gradients: A machine learning-centric introduction</h3>
<p class="body">In machine learning, we model the output as a parametric function of the inputs. We define a loss function that quantifies the difference between the model output and the known ideal output on the set of training inputs. Then we try to obtain the parameter values that will minimize this loss. This effectively identifies the parameters that will result in the model function emitting outputs as close as possible to the ideal on the set of training inputs.</p>
<p class="body">The loss function depends on <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> (the model inputs), <i class="timesitalic">ȳ</i> (the known ideal outputs on the training data—aka ground truth), and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> (the parameters). Here only the behavior of the loss function with respect to the parameters is of interest to us, so we are ignoring everything else and denoting the loss function as a function of the parameters as <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> For the sake of brevity, here we use the symbol <i class="timesitalic">w</i> to denote all parameters—weight as well as bias.</p>
<p class="body">The core question we are trying to answer is this: given a loss <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> and current parameter values <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, what is the optimal change in the parameters <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> that maximally reduces the loss? Equivalently, we want to determine <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> that will make <span class="math"><i class="fm-italics">δL</i> = <i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>) - <i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> as negative as possible. Toward that goal, we will study the relationship between the loss function <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i>)</span> and change in parameter values <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> in several scenarios of increasing complexity.<a class="url" href="#fn9" id="fnref9"><sup class="fm-superscript">1</sup></a></p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre13" height="483" id="fig-line_tangent" src="../../OEBPS/Images/CH03_F04a_Chaudhury.png" width="653"/></p>
<p class="figurecaption">(a) Line: <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i>) = 2<i class="fm-italics">w</i> + 1</span>, <span class="math"><i class="fm-italics">dL</i>/<i class="fm-italics">dw</i> = <i class="fm-italics">m</i></span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre13" height="556" id="fig-parabola_tangent" src="../../OEBPS/Images/CH03_F04b_Chaudhury.png" width="652"/></p>
<p class="figurecaption">(b) Parabola: <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i>) = <i class="fm-italics">w</i><sup class="fm-superscript">2</sup></span>, <span class="math"><i class="fm-italics">dL</i>/<i class="fm-italics">dw</i> = 2<i class="fm-italics">w</i></span></p>
</div>
<p class="figurecaption" id="fig-tangent-1d">Figure 3.4 <i class="timesitalic">δL</i> in terms of <i class="timesitalic">δw</i> in one dimension, illustrated with two example curves: a straight line and a parabola. In general, <span class="math"><i class="fm-italics">δL</i> = (<i class="fm-italics">dL</i>/<i class="fm-italics">dw)</i> <i class="fm-italics">δw</i></span>. To decrease loss, <i class="timesitalic">δw</i> must have the opposite sign of the derivative <span class="math"><i class="fm-italics">dL</i>/<i class="fm-italics">dw</i></span>. In (a), this implies we always have to move left (decrease <i class="timesitalic">w</i>) to decrease <i class="timesitalic">L</i>. In (b), if we are in the left half (e.g., point Q), the derivative is negative, and we have to move to the right to decrease <i class="timesitalic">L</i>. But if we are in the right half, the derivative is positive, and we have to move to the left to decrease <i class="timesitalic">L</i>. Geometrically, this is equivalent to following the tangent “downward.”<a id="marker-92"/></p>
<p class="fm-head2" id="one-dimensional-loss-functions">One-dimensional loss functions</p>
<p class="body"><a id="marker-91"/>For simplicity, we begin by examining this topic in one dimension—meaning there is a single parameter <i class="timesitalic">w</i>. The first example we will study is the simplest possible case: a linear one-dimensional loss function, shown in figure <a class="url" href="#fig-line_tangent">3.5</a>. A linear loss function in one dimension can be written as <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i>) = <i class="fm-italics">mw</i> + <i class="fm-italics">c</i></span>. If we change the parameter <i class="timesitalic">w</i> by a small amount <i class="timesitalic">δw</i>, what is the corresponding change in loss <i class="timesitalic">δL</i>? We have <span class="math"><i class="fm-italics">δL</i> = <i class="fm-italics">L</i>(<i class="fm-italics">w</i> + <i class="fm-italics">δw</i>) − <i class="fm-italics">L</i>(<i class="fm-italics">w</i>) = (<i class="fm-italics">m</i>(<i class="fm-italics">w</i> + <i class="fm-italics">δw</i>)+<i class="fm-italics">c</i>) − (<i class="fm-italics">m</i>(<i class="fm-italics">w</i>)+<i class="fm-italics">c</i>) = <i class="fm-italics">m</i> <i class="fm-italics">δw</i></span> which gives us <span class="math"><i class="fm-italics">δL</i>/<i class="fm-italics">δw</i> = <i class="fm-italics">m</i></span>, a constant. By definition, the derivative <span class="math"><i class="fm-italics">dL</i>/<i class="fm-italics">dw</i> = lim<sub class="fm-subscript"><i class="fm-italics1">δw</i>→0</sub> <i class="fm-italics">δL</i>/<i class="fm-italics">δw</i></span>, which leads to <span class="math"><i class="fm-italics">dL</i>/<i class="fm-italics">dw</i> = <i class="fm-italics">m</i></span>. Thus, for the straight line <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i>) = <i class="fm-italics">mw</i> + <i class="fm-italics">c</i></span>, the rate of change of <i class="timesitalic">L</i> with respect to <i class="timesitalic">w</i> is constant everywhere and equals the slope <i class="timesitalic">m</i>. Putting all this together, we get <span class="math"><i class="fm-italics">δL</i> = <i class="fm-italics">m δw</i> = <i class="fm-italics">dL</i>/<i class="fm-italics">dw</i> <i class="fm-italics">δw</i></span>.</p>
<p class="body">Let’s now study a slightly more complex, non-linear but still one dimensional case—a parabolic loss function illustrated in figure <a class="url" href="#fig-parabola_tangent">3.4</a>. This parabola can be written as <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i>) = <i class="fm-italics">w</i><sup class="fm-superscript">2</sup></span>. If we change the parameter <i class="timesitalic">w</i> by a small amount <i class="timesitalic">δw</i>, what is the corresponding change in in loss <i class="timesitalic">δL</i>? We have <span class="math"><i class="fm-italics">δL</i> = <i class="fm-italics">L</i>(<i class="fm-italics">w</i> + <i class="fm-italics">δw</i>) − <i class="fm-italics">L</i>(<i class="fm-italics">w</i>) = (<i class="fm-italics">w</i> + <i class="fm-italics">δw</i>)<sup class="fm-superscript">2</sup> − <i class="fm-italics">w</i><sup class="fm-superscript">2</sup> = (2<i class="fm-italics">wδw</i> + <i class="fm-italics">δw</i><sup class="fm-superscript">2</sup>)</span>. For infinitesimally small <i class="timesitalic">δw</i>, <span class="math"><i class="fm-italics">δw</i><sup class="fm-superscript">2</sup></span> becomes negligibly small and we get <span class="math">lim<sub class="fm-subscript"><i class="fm-italics1">δw</i>→0</sub> <i class="fm-italics">δL</i> = lim<sub class="fm-subscript"><i class="fm-italics1">δw</i>→0</sub> (2<i class="fm-italics">wδw</i> + <i class="fm-italics">δw</i><sup class="fm-superscript">2</sup>) = 2<i class="fm-italics">wδw</i></span> and <span class="math"><i class="fm-italics">dL</i>/<i class="fm-italics">dw</i> = lim<sub class="fm-subscript"><i class="fm-italics1">δw</i>→0</sub> <i class="fm-italics">δL</i>/<i class="fm-italics">δw</i> = 2w</span>. Combining all these we get the same equation as the linear case <span class="math"><i class="fm-italics">δL</i> = <i class="fm-italics">dL</i>/<i class="fm-italics">dw</i> <i class="fm-italics">δw</i></span>. Of course, in case of the straight line this expression holds for all <i class="timesitalic">δw</i> while in the non-linear curves the expression holds only for small <i class="timesitalic">δw</i>.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title"><span class="times2"><i class="fm-italics1">δL</i> and <i class="fm-italics1">δw</i></span></p>
<p class="fm-sidebar-text">In general, for all one-dimensional loss functions <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i>)</span>, the change <i class="timesitalic">δL</i> caused by a change <i class="timesitalic">δw</i> in parameters can be expressed as follows:</p><!--<p class="FM-Equation"><span class="times"><i class="fm-italics">δL</em> = <i class="fm-italics">dL</em>/<i class="fm-italics">dw</em> <i class="fm-italics">δw</em></span></p>-->
<p class="sidebarafigures"><img alt="" class="calibre2" height="54" src="../../OEBPS/Images/eq_03-03.png" width="104"/></p>
<p class="sidebaracaptions">Equation 3.3 <span id="eq-derivative_total_change"/></p>
<p class="fm-sidebar-text">To decrease <i class="timesitalic">L</i>, <i class="timesitalic">δL</i> must be negative. From equation <a class="url" href="#eq-derivative_total_change">3.3</a>, we can see that this requires <i class="timesitalic">δw</i> (change in <i class="timesitalic">w</i>) and <span class="math"><i class="fm-italics">dL</i>/<i class="fm-italics">dw</i></span> (derivative) to have opposite signs.</p>
</div>
<p class="body">Geometrically speaking, the loss function represents a curve with the loss <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i>)</span> plotted along the <i class="timesitalic">Y</i> axis against the parameter <i class="timesitalic">w</i> plotted along the <i class="timesitalic">X</i> axis (see figure <a class="url" href="#fig-tangent-1d">3.4</a> for examples). The tangent at any point can be viewed as the local approximation to the curve itself for an infinitesimally small neighborhood around the point. The derivative at any point represents the slope of the tangent to the curve at that point.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Equation <a class="url" href="#eq-derivative_total_change">3.3</a> basically tells us that to reduce the loss value, we have to follow the tangent, moving to the right (i.e., positive <i class="timesitalic">δw</i>) if the derivative is negative and moving to the left (i.e., negative <i class="timesitalic">δw</i>) if the derivative is positive.</p>
<p class="fm-head2" id="multidimensional-loss-functions">Multidimensional loss functions</p>
<p class="body">If there are many tunable parameters, our loss function will be a function of many variables, which implies that we have a high-dimensional vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and a loss function <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span>. Our goal is to compute the change <i class="timesitalic">δL</i> in <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> caused by a small vector displacement <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>.</p>
<p class="body"><a id="marker-93"/>We immediately note a fundamental difference from the one-dimensional case: the parameter change is a vector, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>, which has not only a magnitude denoted <span class="infigure">||<img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/>||</span> but also a direction denoted by the unit vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_bw_hat.png" width="26"/></span>. We can take a step of the same size in the <i class="timesitalic">w</i> space, and the change in <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> will be different for different directions. The situation is illustrated in figure <a class="url" href="#fig-surface_gradient">3.5</a>, which shows an example loss function <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) ≡ <i class="fm-italics">L</i>(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>) = 2<i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + 3<i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup></span> for two independent variables <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span> and <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span>. Let’s examine how this loss function changes with a few concrete examples.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="391" id="fig-surface_gradient" src="../../OEBPS/Images/CH03_F05_Chaudhury.png" width="520"/></p>
<p class="figurecaption">Figure 3.5 Plot for surface <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) ≡ <i class="fm-italics">L</i>(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>) = 2<i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + 3<i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup></span> against <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> <span class="math">≡ (<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>)</span>. From an example point <span class="math"><i class="fm-italics">P</i> ≡ (<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>=3, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>=4, <i class="fm-italics">L</i> = 66)</span> on the surface, we can travel in many directions to reduce <i class="timesitalic">L</i>. Some of these are shown byarrows. The maximum reduction occurs when we travel along the dark arrow: this happens when <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> is changed along <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> = [-12, -24]<i class="fm-italics"><sup class="fm-superscript">T</sup></i></span>, which is the negative of the gradient of <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> at <i class="timesitalic">P</i>.</p>
</div>
<p class="body">Suppose we are at <!--<span class="times">$\vec{w} =\begin{bmatrix} w_{0} = 3\\w_{1} = 4\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_03-03-a.png" width="100"/></span>. The corresponding value of <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> is <span class="math">2 ∗ 3<sup class="fm-superscript">2</sup> + 3 ∗ 4<sup class="fm-superscript">2</sup> = 66</span>. Now, suppose we undergo a small displacement from this point: <!--<span class="times">$\vec{\delta w} = \begin{bmatrix} 0.0003\\0.0004\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_03-03-b.png" width="116"/></span>. The new value is <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>) = <i class="fm-italics">L</i>(3.0003, 4.0004) = 2 ∗ 3.0003<sup class="fm-superscript">2</sup> + 3 ∗ 4.0004<sup class="fm-superscript">2</sup> ≈ 66.0132066</span>. Thus this displacement vector <!--<span class="times">$\vec{\delta w} = \begin{bmatrix} 0.0003\\[-3pt]0.0004\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_03-03-b.png" width="116"/></span> causes a change <span class="math"><i class="fm-italics">δL</i> = 66.01320066 – 66 = 0.01320066</span> in <i class="timesitalic">L</i>.</p>
<p class="body">On the other hand, if the displacement is <!--<span class="times">$\vec{\delta w} = \begin{bmatrix} 0.0004\\[-3pt]0.0003\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_03-03-c.png" width="115"/></span>, we get <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>) = <i class="fm-italics">L</i>(3.0004, 4.0003) = 2 ∗ 3.0004<sup class="fm-superscript">2</sup> + 3 ∗ 4.0003<sup class="fm-superscript">2</sup> ≈ 66.0120006</span>. Thus, this displacement causes a change <span class="math"><i class="fm-italics">δL</i> = 66.0120006 − 66 = 0.0120006</span> in <i class="timesitalic">L</i>. The displacement <!--<span class="times">$\vec{\delta w} = \begin{bmatrix} 0.0003\\[-3pt]0.0004\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_03-03-b.png" width="116"/></span> and <!--<span class="times">$\vec{\delta w} = \begin{bmatrix} 0.0004\\[-3pt]0.0003\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_03-03-c.png" width="115"/></span> have the same length <!--<span class="times">$\sqrt{0.0003^{2} + 0.0004^{2}}=\break\sqrt{0.0004^{2} + 0.0003^{2}} = 0.0005$</span>--><span class="infigure"><img alt="" class="calibre5" height="22" src="../../OEBPS/Images/eq_03-03-d.png" width="421"/></span> but different directions. The change they cause to the function value is different. This exemplifies our thesis that in multivariable loss function, the change in the loss function depends not only on the magnitude but also on the direction of the displacement in the parameter space.</p>
<p class="body">What is the general relationship between the displacement vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> in the parameter space and the overall change in loss <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span>? To examine this question, we need to know what a partial derivative is.</p>
<p class="fm-head2" id="partial-derivatives">Partial derivatives</p>
<p class="body"><a id="marker-94"/>The derivative <span class="math"><i class="fm-italics">dL</i>/<i class="fm-italics">dw</i></span> of a function <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i>)</span> indicates the rate of change of the function with respect to <i class="timesitalic">w</i>. But if <i class="timesitalic">L</i> is a function of many variables, how does it change if only one of those variables is changed? This question leads to the notion of partialderivatives.</p>
<p class="body">The <i class="fm-italics">partial derivative</i> of a function of many variables is a derivative taken with respect to exactly one variable, treating all other variables as constants. For instance, given <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) ≡ <i class="fm-italics">L</i>(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>) = 2<i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + 3<i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup></span>, the partial derivatives with respect to <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span> , <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span> are</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\frac{\partial L}{\partial w_{0}} = 4w_{0}\\[4pt]
&amp;\frac{\partial L}{\partial w_{1}} = 6w_{1}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="108" src="../../OEBPS/Images/eq_03-03-e.png" width="92"/></p>
</div>
<p class="fm-head2" id="total-change-in-a-multidimensional-function">Total change in a multidimensional function</p>
<p class="body">Partial derivatives estimate the change in a function if a single variable changes and the others stay constant. How do we estimate the change in a function’s value if all the variables change together?</p>
<p class="body">The total change can be estimated by taking a weighted combination of the partial derivatives. Let <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> denote the point and the displacement vector, respectively:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\vec{w} &amp;=
\begin{bmatrix} w_{0}\\[-1pt] w_{1}\\[-1pt]
\vdots\\[-1pt] w_{n}
\end{bmatrix}\\
%\noalign{\eject}
\vec{\delta w} &amp;=
\begin{bmatrix}
\delta w_{0}\\
\delta w_{1}\\
\vdots \\
\delta w_{n}
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="296" src="../../OEBPS/Images/eq_03-03-f.png" width="97"/></p>
</div>
<p class="body">Then</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\delta L\left(\vec{w}\right) &amp;= L\left(\vec{w} + \vec{\delta w}\right) - L\left(\vec{w}\right) \nonumber\\[3pt]
&amp;= \frac{\partial L}{\partial w_{0}}\delta w_{0} +
\frac{\partial L}{\partial w_{1}}\delta w_{1} +
\cdots +
\frac{\partial L}{\partial w_{n}}\delta w_{n}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="104" src="../../OEBPS/Images/eq_03-04.png" width="370"/></p>
</div>
<p class="fm-equation-caption">Equation 3.4 <span class="calibre" id="eq-partial_deriv_total_change"/></p>
<p class="body"><a id="marker-95"/>Equation <a class="url" href="#eq-partial_deriv_total_change">3.4</a> essentially says that the total change in <i class="timesitalic">L</i> is obtained by adding up the changes caused by displacements in individual variables. The rate of change of <i class="timesitalic">L</i> with respect to the change in <i class="timesitalic">w<sub class="fm-subscript">i</sub></i> only is <span class="math"><i class="fm-italics">∂L</i>/<i class="fm-italics">∂w<sub class="fm-subscript">i</sub></i></span>. The displacement along the variable <i class="timesitalic">w<sub class="fm-subscript">i</sub></i> is <i class="timesitalic">δw<sub class="fm-subscript">i</sub></i>. Hence, the change caused by the <i class="timesitalic">i</i>th element of the displacement is <span class="math"><i class="fm-italics">∂L</i>/<i class="fm-italics">∂w<sub class="fm-subscript">i</sub></i> <i class="fm-italics">δw<sub class="fm-subscript">i</sub></i></span>— this follows from equation <a class="url" href="#eq-derivative_total_change">3.3</a>. The total change is obtained by adding the changes caused by individual elements of the displacement vector: that is, summing over all <i class="timesitalic">i</i> from <span class="math">0</span> to <i class="timesitalic">n</i>. This leads to equation <a class="url" href="#eq-partial_deriv_total_change">3.4</a>. Thus equation <a class="url" href="#eq-partial_deriv_total_change">3.4</a> is simply the multidimensional version of equation <a class="url" href="#eq-derivative_total_change">3.3</a>.</p>
<p class="fm-head2" id="gradients">Gradients</p>
<p class="body">It would be nice to be able to represent equation <a class="url" href="#eq-partial_deriv_total_change">3.4</a> compactly. To do this, we define a quantity called a <i class="fm-italics">gradient</i>: the vector of all the partial derivatives.</p>
<p class="body">Given an <i class="timesitalic">n</i>-dimensional function <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span>, its gradient is defined as</p><!--<p class="Body"> <span class="times">$$\nabla L\left(\vec{w}\right) = \begin{bmatrix}
\frac{\partial L}{\partial w_{0}}\\
\frac{\partial L}{\partial w_{1}}\\
\vdots \\
\frac{\partial L}{\partial w_{n}}
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="151" src="../../OEBPS/Images/eq_03-05.png" width="125"/></p>
</div>
<p class="fm-equation-caption">Equation 3.5 <span class="calibre" id="eq-gradient-def"/></p>
<p class="body">Using gradients, we can rewrite equation <a class="url" href="#eq-partial_deriv_total_change">3.4</a> as</p><!--<p class="Body"> <span class="times">$$\begin{aligned}
\delta L\left(\vec{w}\right) &amp;= L\left(\vec{w} + \vec{\delta w}\right) - L\left(\vec{w}\right)\nonumber\\
&amp;= \frac{\partial L}{\partial w_{0}}\delta w_{0} +
\frac{\partial L}{\partial w_{1}}\delta w_{1} +
\cdots +
\frac{\partial L}{\partial w_{n}}\delta w_{n}\nonumber\\
&amp;= \left(\nabla L\left(\vec{w}\right)\right)^{T}\vec{\delta w}
=  \nabla L\left(\vec{w}\right) \cdot \vec{\delta w}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="126" src="../../OEBPS/Images/eq_03-06.png" width="384"/></p>
</div>
<p class="fm-equation-caption">Equation 3.6 <span class="calibre" id="eq-gradient_total_change"/></p>
<p class="body">Equation <a class="url" href="#eq-gradient_total_change">3.6</a> tells us that the total change, <i class="timesitalic">δL</i> in <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span>, caused by displacement <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> from <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> in parameter space is the dot product between the gradient vector <span class="math">∇<i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> and the displacement vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>. This is the exact multidimensional analog of equation <a class="url" href="#eq-derivative_total_change">3.3</a>.</p>
<p class="body">Recall from section <a class="url" href="02.xhtml#subsubsec-dotproduct_as_agreement">2.5.6.2</a> that the dot product of two vectors (of fixed magnitude) attains a maximum value when the vectors are aligned in direction. This yields a physical interpretation of the gradient vector: its direction is the direction in parameter space <i class="fm-italics">along which the multidimensional function is changing fastest</i>. It is the multidimensional counterpart of the derivative. This is why, in machine learning, when we want to minimize the loss function, we change the parameter values along the direction of the gradient vector of the loss function.</p>
<p class="fm-head2" id="the-gradient-is-zero-at-the-minimum">The gradient is zero at the minimum</p>
<p class="body"><a id="marker-96"/>any <i class="fm-italics">optimum</i> (that is, maximum or minimum) of a function is a point of inflection. This means the function turns around at the optimum point. In other words, the gradient direction on one side of the optimum is the opposite of that on the other side. If we try to travel smoothly from positive values to negative values, we must cross zero somewhere in between. Thus, the gradient is zero at the exact point of inflection maximum or minimum). This is easiest to see in <span class="math">2</span>D and is depicted in figure <a class="url" href="#fig-gradient_zero_at_minima">3.6</a>. However, the idea is general: it works in higher dimensions, too. The fact that the gradient becomes zero at the optimum is often used to algebraically compute the optimum. The following example illustrates this.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="331" id="fig-gradient_zero_at_minima" src="../../OEBPS/Images/CH03_F06_Chaudhury.png" width="529"/></p>
<p class="figurecaption">Figure 3.6 The minimum is always a point of inflection, meaning the function turns around at that point. If we consider any two points <span class="math"><i class="fm-italics">P</i><sub class="fm-subscript">−</sub></span> and <span class="math"><i class="fm-italics">P</i> <sub class="fm-subscript">+</sub></span> on both sides of the minimum, the gradient is positive on one side and negative on the other. Assuming the gradient changes smoothly, it must be zero in between, at the minimum.</p>
</div>
<p class="body">Consider the simple example function <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>) = √<i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup></span>. Its optimum occurs when its gradient is zero:</p><!--<p class="Body"><span class="times">$$\nabla_{\vec{w}}L =
\begin{bmatrix}
\frac{\partial L}{\partial w_{0}}\\
\frac{\partial L}{\partial w_{1}}
\end{bmatrix} =
\frac{1}{2 \sqrt{w_{0}^{2} + w_{1}^{2}}}
\begin{bmatrix} 2w_{0}\\ 2w_{1}
\end{bmatrix}=
\begin{bmatrix} 0\\0
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="81" src="../../OEBPS/Images/eq_03-06-a.png" width="344"/></p>
</div>
<p class="body">The solution is <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub> = 0, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> = 0</span> The function attains its minimum value at the origin, which agrees with our intuition.</p>
<h3 class="fm-head1" id="subsec-level-surf">3.3.2 Level surface representation and loss minimization</h3>
<p class="body"><a id="marker-97"/>In figure <a class="url" href="#fig-surface_gradient">3.5</a>, we plotted the loss function <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> against the parameter values <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>. In this section, we study a different way of visualizing loss surfaces. This will lend further insight into gradients and minimization.</p>
<p class="body">We will continue with our simple example function from the last subsection. Consider a field <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>) = √(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup>)</span>. Its domain is the infinite <span class="math">2</span>D plane defined by the axes <span class="math"><i class="fm-italics">W</i><sub class="fm-subscript">0</sub></span> and <span class="math"><i class="fm-italics">W</i><sub class="fm-subscript">1</sub></span>. Note that the function has constant values along concentric circles centered on the origin. For instance, at all points on the circumference of the circle <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup> = 1</span>, the function has a constant function value of <span class="math">1</span>. At all points on the circumference of the circle <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup> = 25</span>, the function has a constant function value of <span class="math">5</span>. Such constant function value curves on the domain are called <i class="fm-italics">level contours</i> in <span class="math">2</span>D. This is shown as a heat map in figure <a class="url" href="#fig-circle-field-2D">3.7</a>. The idea of level contours can be generalized to higher dimensions where we have level surfaces or level hypersurfaces. Note that while the <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> in figure <a class="url" href="#fig-surface_gradient">3.5</a> was on an <span class="math">(<i class="fm-italics">n</i>+1)</span>-dimensional space (where <i class="timesitalic">n</i> is the dimensionality of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>), the level surface/contour representation is in <i class="timesitalic">n</i>-dimensional space. At any point on the domain, what is the direction along which the biggest <i class="fm-italics">change</i> in the function value occurs? The answer is <i class="fm-italics">along the direction of the gradient</i>. The magnitude of the change corresponds to the magnitude of the gradient. In the current example, say we are at a point <span class="math">(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>)</span>. There exists a level contour through this point: the circle with origin at the center passing through <span class="math">(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>)</span>. If we move along the circumference of this circle—that is, along the tangent to this circle—the function value does not change. In other words, at any point, the tangent to the level contour through that point is the direction of <i class="fm-italics">minimal</i> change. On the other hand, <i class="fm-italics">if we move perpendicular to the tangent, maximum change in the function value occurs</i>. The perpendicular to the tangent is known as a <i class="fm-italics">normal</i>. This is the direction of the gradient. <i class="fm-italics">The gradient at any point on the domain is always normal to the level contour through that point, indicating the direction of maximum change in the function value</i>. In figure <a class="url" href="#fig-circle-field-2D">3.7</a>, the gradients are all parallel to the radii of the concentric circles.</p>
<p class="body">Recall that while training a machine learning model, we essentially define a loss function in terms of a tunable set of parameters and try to minimize the loss by adjusting (tuning) the parameters. We start at a random point and iteratively progress toward the minimum. Geometrically, this can be viewed as starting at an arbitrary point on the domain and continuing to move in a direction that minimizes the function value. Of course, we would like to progress to the minimum of the function value in as few iterations as possible. In figure <a class="url" href="#fig-circle-field-2D">3.7</a>, the minimum is at the origin, which is also the center of all the concentric circles. Wherever we start, we will have to always travel radially inward to reach the minimum <span class="math">(0,0)</span> of the function <span class="math">√(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup>)</span>.</p>
<p class="body">In higher dimensions, level contours become level surfaces. Given any function <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> with <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>] <span class="cambria">∈</span> ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span>, we define level surfaces as <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) = <i class="fm-italics">constant</i></span>. If we move along the level surface, the change in <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> is minimal (<span class="math">0</span>). The gradient of a function at any point is normal to the level surface through that point. This is the direction along which the function value is changing fastest. Moving along the gradient, we pass from one level surface to another, as shown in figure <a class="url" href="#fig-grad_sphere">3.8</a>. Here the function is <span class="math">3</span>D: <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) = <i class="fm-italics">L</i>(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">2</sub><sup class="fm-superscript">2</sup></span>. The level surfaces <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">2</sub><sup class="fm-superscript">2</sup> = <i class="fm-italics">constant</i></span> for various values of the constant are concentric spheres, with the origin as their center. The gradient vector at any point is along the outward-pointing radius of the sphere through that point.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre22" height="446" id="fig-circle-field-2D" src="../../OEBPS/Images/CH03_F07_Chaudhury.png" width="415"/></p>
<p class="figurecaption">Figure 3.7 The domain of <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>) = √(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup>)</span> shown as a heat map of function values. Gradients point radially outward, as shown by the arrowed line. The intensity of the heat map changes fastest along the gradient (that is, radii). This is the direction to follow to rapidly reach lower values of the function represented by the heat map.<a id="marker-98"/></p>
</div>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="486" id="fig-grad_sphere" src="../../OEBPS/Images/CH03_F08_Chaudhury.png" width="594"/></p>
<p class="figurecaption">Figure 3.8 Gradient example in 3D: the function <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">2</sub><sup class="fm-superscript">2</sup></span>. The levelsurfaces <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) = <i class="fm-italics">constant</i></span> are concentric spheres with the origin as their center. One such surface is partially shown in the diagram. <span class="math">∇<i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) = <i class="fm-italics">k</i>[<i class="fm-italics">w</i><sub class="fm-subscript">0</sub> <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> <i class="fm-italics">w</i><sub class="fm-subscript">2</sub>]<i class="fm-italics"><sup class="fm-superscript">T</sup></i></span>—the gradient points radially outward. along the gradient, we go from one level surface to another, corresponding to maximum change in <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span>. Moving along any direction orthogonal to the gradient, we stay on the same level surface (sphere), which corresponds to zero change in the function value. <span class="math"><i class="fm-italics">D<sub class="fm-subscript">θ</sub></i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> denotes the directional derivative along the displacement direction making angle <i class="timesitalic">θ</i> with the gradient. If <i class="timesitalic">l̂</i> denotes this displacement direction, <span class="math"><i class="fm-italics">D<sub class="fm-subscript">θ</sub></i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) = ∇<i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) ⋅ <i class="fm-italics">l̂</i></span>.</p>
</div>
<p class="body">Another example is shown in figure <a class="url" href="#fig-grad_cylinder">3.9</a>. Here the function is <span class="math">3</span>D: <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) = <i class="fm-italics">f</i>(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup></span>. The level surfaces <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup> = <i class="fm-italics">constant</i></span> for various values of the constant are coaxial cylinders, with <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">2</sub></span> as the axis. The gradient vector at any point is along the outward-pointing radius of the planar circle belonging to the cylinder through that point.<a id="marker-99"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="498" id="fig-grad_cylinder" src="../../OEBPS/Images/CH03_F09_Chaudhury.png" width="594"/></p>
<p class="figurecaption">Figure 3.9 Gradient example in 3D: the function <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup></span>. The level surfaces <span class="math"><i class="fm-italics">f</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) = <i class="fm-italics">constant</i></span> are coaxial cylinders. One such surface is partially shown in the diagram: <span class="math">∇<i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) = <i class="fm-italics">k</i>[<i class="fm-italics">w</i><sub class="fm-subscript">0</sub> <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> 0]<i class="fm-italics"><sup class="fm-superscript">T</sup></i></span>. The gradient is normal to the curved surface of the cylinder along the outward radiusof the circle. Moving along the gradient, we go from one level surface to another, corresponding to themaximum change in <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span>. Moving along any direction orthogonal to the gradient, we stay on thesame level surface (cylinder), which corresponds to zero change in the function value.</p>
</div>
<p class="body">So far, we have studied the change in loss value resulting from infinitesimally small displacements in the parameter space. In practice, the programmatic displacements undergone during parameter updates while training are small, but not infinitesimally so. Is there any way to improve the approximation in these cases? This is discussed in the following section.</p>
<h2 class="fm-head" id="sec-local_approx_loss">3.4 Local approximation for the loss function</h2>
<p class="body">Equation <a class="url" href="#eq-gradient_total_change">3.6</a> expresses the change <i class="timesitalic">δL</i> in the loss value corresponding to displacement <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> in the parameter space. The equation is exactly true if and only if the loss function is linear or the magnitude of the displacement is infinitesimally small. In practice, we adjust parameter values by small—but not infinitesimally small—amounts. Under these circumstances, equation <a class="url" href="#eq-gradient_total_change">3.6</a> is only approximately true: the larger the magnitude of <span class="math">||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>||</span>, the worse the approximation.</p>
<p class="body">A Taylor series offers a way to approximate a multidimensional function in the local neighborhood of any point by expressing it in terms of the displacements in the parameter space. It is an infinite series, meaning the equation is exactly true (zero approximation) only when we have summed an infinite number of terms. Of course, we cannot add an infinite number of terms with a computer program. But we can improve the accuracy of the approximation as much as we like by including more and more terms. In practice, we include at most up to the second term. Anything beyond that is redundant because the improvement is too small to be realized by the floating point system of current computers. First we will study a Taylor series in one dimension.</p>
<h3 class="fm-head1" id="sec-taylor-onedim">3.4.1 1<i class="fm-italics">D</i> Taylor series recap</h3>
<p class="body"><a id="marker-100"/>Suppose we are trying to describe the curve <span class="math"><i class="fm-italics">L</i>(<i class="fm-italics">w</i>)</span> in the neighborhood of a particular point <i class="timesitalic">w</i>. If we stay infinitesimally close to <i class="timesitalic">w</i>, then, as described in section <a class="url" href="../Text/03.xhtml#sec-grad">3.3</a>, we can approximate the curve with a straight line:</p><!--<p class="Body"><span class="times">$$L\left( w + \delta w\right)
= L\left(w\right) + \frac{dL}{dw}\delta w$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_03-06-b.png" width="229"/></p>
</div>
<p class="body">But in the general case, if we are describing a continuous (smooth) function in the neighborhood of a specific point, we use a Taylor series. A Taylor series allows us to describe a function in the neighborhood of a specific point in terms of the value of the function and its derivatives at that point. Doing so is relatively simple in 1D:</p><!--<p class="Body"><span class="times">$$L\left(w+\delta w\right) = L\left(w\right) +
\frac{\left(\delta w\right)}{1!}\frac{ dL }{dw} + \frac{ \left(\delta w\right)^{2} } {2!} \frac{d^{2} L }{dw^{2}} + \cdots$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_03-07.png" width="453"/></p>
</div>
<p class="fm-equation-caption">Equation 3.7 <span class="calibre" id="eq-taylor-onedim"/></p>
<p class="body">Note that the terms become progressively smaller (since they involve higher and higher powers of a small number <i class="timesitalic">δw</i>). Hence, although the series goes on to infinity, in practice we entail a negligible loss in accuracy by dropping higher-order terms. We often use the first-order approximation (or, at most, second-order). Equation <a class="url" href="#eq-taylor-onedim">3.7</a> can be rewritten as</p><!--<p class="Body"><span class="times">$$\delta L = L\left(w+\delta w\right) - L\left(w\right) = \frac{\left(\delta w\right)}{1!}\frac{ dL
}{dw} + \frac{ \left(\delta w\right)^{2} } {2!} \frac{d^{2} L }{dw^{2}}
+ \cdots$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_03-07-a.png" width="453"/></p>
</div>
<p class="body">Note that the second term has <span class="math">(<i class="fm-italics">δw</i>)<sup class="fm-superscript">2</sup></span> as a factor, which is nearly zero at small values of the displacement <i class="timesitalic">δw</i>. So, for really small <i class="timesitalic">δw</i>, we include only the first term. Then we get <span class="math"><i class="fm-italics">δL</i> = (<i class="fm-italics">δw</i>/1!) (<i class="fm-italics">dL</i>/<i class="fm-italics">dw)</i></span>, which is the same as equation <a class="url" href="#eq-derivative_total_change">3.3</a>. If <i class="timesitalic">δw</i> is a bit larger and we want greater accuracy, we can include the second-order term. In practice, as mentioned earlier, that is hardly ever done.</p>
<p class="body">A handy example of a Taylor series is the expansion of the exponential function <i class="timesitalic">e<sup class="fm-superscript">x</sup></i> near <span class="math"><i class="fm-italics">x</i> = 0</span></p><!--<p class="Body"><span class="times">$$e^{t} = e^{0+t} = 1 + t + \frac{t^{2}}{2!} +
\frac{t^{3}}{3!} \cdots$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_03-07-b.png" width="453"/></p>
</div>
<p class="body">where we use the fact that <span class="math"><i class="fm-italics">d<sup class="fm-superscript">n</sup></i>/<i class="fm-italics">dx<sup class="fm-superscript">n</sup></i> (<i class="fm-italics">e<sup class="fm-superscript">x</sup></i>)|<sub class="fm-subscript"><i class="fm-italics1">x</i> = 0</sub> = <i class="fm-italics">e<sup class="fm-superscript">x</sup></i>|<sub class="fm-subscript"><i class="fm-italics1">x</i> = 0</sub> = 1</span> for all <i class="timesitalic">n</i>.</p>
<h3 class="fm-head1" id="sec-taylor-multidim">3.4.2 Multidimensional Taylor series and the Hessian matrix</h3>
<p class="body"><a id="marker-101"/>In equation <a class="url" href="#eq-taylor-onedim">3.7</a>, we express a function of one variable in a small neighborhood around a point in terms of the derivatives. Can we do a similar thing in higher dimensions? Yes. We simply need to replace the first derivative with the gradient. We replace the second derivative with its multidimensional counterpart: the Hessian matrix. The multidimensional Taylor series is as follows</p><!--<p class="Body"><span class="times">$$L\left(\vec{w}+\vec{\delta w}\right) = L\left(\vec{w}\right) + \frac{1}{1!}\left(\vec{\delta w}\right)^{T}
\nabla L\left(\vec{w}\right) +  \frac{1}{2!}\left(\vec{\delta w}\right)^{T}  H \left( L\left(\vec{w}\right)\right)  \left(\vec{\delta w}\right) + \cdots$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="52" src="../../OEBPS/Images/eq_03-08.png" width="590"/></p>
</div>
<p class="fm-equation-caption">Equation 3.8 <span class="calibre" id="eq-taylor-multidim"/></p>
<p class="body">where <span class="math"><i class="fm-italics">H</i>(<i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>))</span>, called the <i class="fm-italics">Hessian matrix</i>, is defined as</p><!--<p class="Body"><span class="times">$$H \left( L\left(\vec{w}\right)\right) =
\begin{bmatrix}
\frac{\partial^{2} L}{\partial w_{1}^{2}} &amp;\frac{\partial^{2} L}{\partial w_{1} \partial w_{2}} &amp;\cdots &amp;\frac{\partial^{2} L}{\partial w_{1} \partial w_{n}}\\
\frac{\partial^{2} L}{\partial w_{2} \partial w_{1}}
&amp;\frac{\partial^{2} L}{\partial w_{2}^{2}} &amp;\cdots
&amp;\frac{\partial^{2} L}{\partial w_{1} \partial w_{n}}\\
\vdots &amp;\vdots &amp;\vdots\\
\frac{\partial^{2} L}{\partial w_{n} \partial w_{1}}
&amp;\frac{\partial^{2} L}{\partial w_{n} \partial w_{2}} &amp;\cdots
&amp;\frac{\partial^{2} L}{\partial w_{n}^{2}}\\
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="161" src="../../OEBPS/Images/eq_03-09.png" width="364"/></p>
</div>
<p class="fm-equation-caption">Equation 3.9 <span class="calibre" id="eq-hessian"/></p>
<p class="body">The Hessian matrix is symmetric since <!--<span class="times">$\frac{\partial^{2} L}{\partial w_{i} \partial w_{j}} = \frac{\partial^{2} L}{\partial w_{j} \partial w_{i}}$</span>--><span class="infigure"><img alt="" class="calibre5" height="33" src="../../OEBPS/Images/eq_03-09-a.png" width="115"/></span>. Also, note that the Taylor expansion assumes that the function is continuous in the neighborhood.</p>
<p class="body">Equation <a class="url" href="../Text/03.xhtml#eq-taylor-multidim">3.8</a> allows us to compute the value of <i class="timesitalic">L</i> in a small neighborhood around point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> in the parameter space. If we displace from <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> by the vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>, we arrive at <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span></span>. The loss there is <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>)</span>, which is expressed by equation <a class="url" href="../Text/03.xhtml#eq-taylor-multidim">3.8</a> in terms of the loss <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> at the original point and the displacement <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>.</p><!--<p class="Body">This leads to <span class="times">$$\delta L
=  L\left(\vec{w}+\vec{\delta w}\right) - L\left(\vec{w}\right) =
\frac{1}{1!}\left(\vec{\delta w}\right)^{T} \nabla L\left(\vec{w}\right)
+  \frac{1}{2!}\left(\vec{\delta w}\right)^{T}  H \left( L\left(\vec{w}\right)\right)  \left(\vec{\delta w}\right) +
\cdots$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_03-10.png" width="634"/></p>
</div>
<p class="fm-equation-caption">Equation 3.10 <span class="calibre" id="eq-gradient_total_change_2nddeg"/></p>
<p class="body">Note that the first term is same as equation <a class="url" href="#eq-gradient_total_change">3.6</a> and the second term has squares of the displacement. Since the square of a small quantity is even smaller, for very small displacements, the second term disappears, and we essentially get back equation <a class="url" href="#eq-gradient_total_change">3.6</a>. This is called <i class="fm-italics">first-order approximation</i>. For slightly larger displacements, we can include the second term, involving Hessians to improve the approximation. As stated earlier, this is hardly ever done in practice.</p>
<h2 class="fm-head" id="sec-gradient-descent">3.5 PyTorch code for gradient descent, error minimization,and model training</h2>
<p class="body">In this section, we study PyTorch examples in which models are trained by minimizing errors via gradient descent. Before we present the code, we briefly recap the main ideas from a practical point of view. Complete code for this section can be found at <a class="url" href="http://mng.bz/4Zya">http://mng.bz/4Zya</a>.)</p>
<h3 class="fm-head1" id="pytorch-code-for-linear-models">3.5.1 PyTorch code for linear models</h3>
<p class="body">If the true underlying function we are trying to predict is very simple, linear models suffice. Otherwise, we require nonlinear models. Here we will look at the linear model. In machine learning, we identify the input and output variables pertaining to the problem at hand and cast the problem as generating outputs from input variables. All the inputs are represented together by the vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. Sometimes there are multiple outputs, and sometimes there is a single output. Accordingly, we have an output vector <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> or an output scalar <i class="timesitalic">y</i>. Let’s denote the function that generates the output from the input vector as <i class="timesitalic">f</i>: that is, <span class="math"><i class="timesitalic">y</i> = <i class="fm-italics">f</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>.</p>
<p class="body">In real-life problems, we do not know <i class="timesitalic">f</i>. The crux of machine learning is to estimate <i class="timesitalic">f</i> from a set of observed inputs <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i></span> and their corresponding outputs <i class="timesitalic">y<sub class="fm-subscript">i</sub></i>. Each observation can be depicted as a pair <span class="math"><span class="segoe">⟨</span><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i>, <i class="fm-italics">y<sub class="fm-subscript">i</sub></i><span class="segoe">⟩</span></span>. We model the unknown function <i class="timesitalic">f</i> with a known function <i class="timesitalic">ϕ</i>. <i class="timesitalic">ϕ</i> is a parameterized function. Although the nature of <i class="timesitalic">ϕ</i> is known, its parameter values are unknown. These parameter values are “learned” via training. This means we estimate the parameter values such that the overall error on the observations is minimized.</p>
<p class="body"><a id="marker-102"/>If <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> denotes the current set of parameters (weights, bias), then the model will output <span class="math"><i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span> on the observed input <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i></span>. Thus the error on this <i class="timesitalic">i</i>th observation is <span class="math"><i class="fm-italics">e<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">2</sup> = (<i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)−<i class="timesitalic">y<sub class="fm-subscript">i</sub></i>)<sup class="fm-superscript">2</sup></span>. We can batch several observations and add up the errors into a batch error <span class="math"><i class="fm-italics">L</i> = Σ<sub class="fm-subscript"><i class="fm-italics1">i</i> = 0</sub><sup class="fm-superscript"><i class="fm-italics1">i</i> = <i class="fm-italics1">N</i></sup> (<i class="fm-italics">e<sup class="fm-superscript">(i)</sup></i>)<sup class="fm-superscript">2</sup></span>.</p>
<p class="body">The error is a function of the parameter set <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>. The question is, how do we adjust <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> so that the error <span class="math"><i class="fm-italics">e<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">2</sup></span> decreases? We know a function’s value changes most when we move along the direction of the gradient of the parameters. Hence, we adjust the parameters <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> as follows:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\begin{bmatrix} \vec{w}\\[-3pt]b \end{bmatrix} = \begin{bmatrix}
\vec{w}\\[-3pt]b \end{bmatrix} - \mu \nabla_{\vec{w}, b}L\left(\vec{w}, b\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="78" src="../../OEBPS/Images/eq_03-10-a.png" width="211"/></p>
</div>
<p class="body">Each adjustment reduces the error. Starting from a random set of parameter values and doing this a sufficiently large number of times ields the desired model.</p>
<p class="body">A simple and popular model <i class="timesitalic">ϕ</i> is the linear function (the predicted value is the dot product between the input vector and parameters vector plus bias): <span class="math"><i class="fm-italics">ỹ<sub class="fm-subscript">i</sub></i> = <i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>) = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i> = ∑<i class="fm-italics"><sub class="fm-subscript">j</sub>w<sub class="fm-subscript">j</sub>x<sub class="fm-subscript">j</sub></i> + <i class="fm-italics">b</i></span>. Our initial implementation (listing <a class="url" href="#listing3-1">3.1</a>) simply mimics this formula. For more complicated models <i class="timesitalic">ϕ</i> (with millions of parameters and nonlinearities), we cannot obtain closed-form gradients like this. In such cases, we use a technique called autograd (automatic gradient computation), which does not required closed form gradients. This is discussed in the next section.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> In real-world problems, we will not know the true underlying function mapping inputs to outputs. But here, for the sake of gaining insight, we will assume known output functions and perturb them with noise to make them slightly more realistic.</p>
<p class="fm-code-listing-caption" id="listing3-1">Listing 3.1 PyTorch linear model (closed-form gradient formula needed)</p>
<pre class="programlisting">x = 10 * torch.randn(N)                              <span class="fm-combinumeral">①</span>
  = 1.5 * x + 2.73 _obs = y + (0.5 * torch.randn(N)) <span class="fm-combinumeral">②</span>

for step in range(num_steps):
    y_pred = w*x + b                                 <span class="fm-combinumeral">③</span>
    mean_squared_error = torch.mean(
            (y_pred - y_obs) ** 2)                   <span class="fm-combinumeral">④</span>

    w_grad = torch.mean(2 * ((y_pred - y_obs)* x))
    b_grad = torch.mean(2 * (y_pred - y_obs))        <span class="fm-combinumeral">⑤</span>

    w = w - learning_rate * w_grad
    b = b - learning_rate * b_grad                   <span class="fm-combinumeral">⑥</span>

print("True function: y = 1.5*x + 2.73")
print("Learned function: y_pred = {}*x + {}".format(w[0], b[0]))</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates random input values</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generates output values by applying a simple known function to the input and then adds noise. Let’s see if our learned function matches the known underlying function.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Our model, initialized with arbitrary parameter values</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Model error is the (squared) difference between the observed and predicted values.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculates the gradient of the error using calculus. Possible only with such simple models.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Adjusts the weight, bias along the gradient of error</p>
<p class="body">The output is as follows:</p>
<pre class="programlisting">True function: y = 1.5*x + 2.73
Learned function: y_pred = 1.50059*x + 2.746823</pre>
<h3 class="fm-head1" id="automatic-gradient-computation-in-pytorch">3.5.2 Autograd: PyTorch automatic gradient computation</h3>
<p class="body"><a id="marker-103"/>In the PyTorch code in listing <a class="url" href="#listing3-1">3.1</a>, for this specific model architecture, we computed the gradient using calculus. This approach does not scale to more complex models with millions of weights and perhaps nonlinear complex functions. For scalability, we use an <i class="fm-italics">automatic differentiation</i> software library like PyTorch Autograd. Users of the library need not worry about how to compute the gradients—they just construct the model function. Once the function is specified, PyTorch figures out how to compute its gradient through the Autograd technology.</p>
<p class="body">To use Autograd, we explicitly tell PyTorch to track gradients for a variable by setting <code class="fm-code-in-text">requires_grad = True</code> when creating the variable. PyTorch remembers a computation graph that is updated every time we create an expression using tracked variables. Figure <a class="url" href="#fig-auto-grad">3.10</a> shows an example of a computation graph.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre23" height="864" id="fig-auto-grad" src="../../OEBPS/Images/CH03_F10_Chaudhury.png" width="860"/></p>
<p class="figurecaption">Figure 3.10 Autograd analysis<a id="marker-104"/></p>
</div>
<p class="body">The following listing, which implements a linear model in PyTorch, relies on PyTorch’s Autograd for gradient computation. Note that this method does not require the closed-form gradient.</p>
<p class="fm-code-listing-caption" id="listing3-2">Listing 3.2 Linear modeling with PyTorch</p>
<pre class="programlisting">def update_parameters(params, learning_rate):            <span class="fm-combinumeral">①</span>

  with torch.no_grad():                                  <span class="fm-combinumeral">②</span>
    for i, p in enumerate(params):
      params[i] = p - learning_rate * p.grad

    for i in range(len(params)):
      params[i].requires_grad = True                     <span class="fm-combinumeral">③</span>

x = 10 * torch.randn(N)                                  <span class="fm-combinumeral">④</span>
y  = 1.5 * x + 2.73

y_obs = y + (0.5 * torch.randn(N))                       <span class="fm-combinumeral">⑤</span>

w = torch.randn(1, requires_grad=True)
b = torch.randn(1, requires_grad=True)
params = [b, w]                                          <span class="fm-combinumeral">⑥</span>

for step in range(num_steps):
  y_pred = params[0] + params[1] * x

  mean_squared_error = torch.mean((y_pred - y_obs) ** 2) <span class="fm-combinumeral">⑦</span>

  mean_squared_error.backward()                          <span class="fm-combinumeral">⑧</span>

  update_parameters(params, learning_rate)               <span class="fm-combinumeral">⑨</span>

print("True function: y = 1.5*x + 2.73")
print("Learned function: y_pred = {}*x + {}"
      .format(params[1].data[0], params[0].data.[0]))</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Updates parameters: adjusts the weight, bias along the gradient of error</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Doesn’t track gradients during parameter updates</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Restores gradient tracking</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Generates random training input</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Generates training output: applies a simple known function to the input and then adds noise. Let’s see if our learned function matches the known underlying function.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Our model, initialized with arbitrary parameter values</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> The model error is the (squared) difference<br class="calibre20"/>
  between the observed and predicted values.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Backpropagates: computes the partial<br class="calibre20"/>
  derivatives of the error with respect to<br class="calibre20"/>
  each variable and stores them in the “grad” field within the variable</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Updates parameters using those partial derivatives</p>
<p class="body">The output is as follows:</p>
<pre class="programlisting">True function: y = 1.5*x + 2.73 
Learned function: y_pred = 1.50059*x + 2.74783</pre>
<h3 class="fm-head1" id="nonlinear-models-in-pytorch">3.5.3 Nonlinear Models in PyTorch</h3>
<p class="body"><a id="marker-105"/>In listings <a class="url" href="#listing3-1">3.1</a> and <a class="url" href="#listing3-2">3.2</a>, we fit a linear model to a data distribution that we know to be linear. From the output, we can see that those models converged to a pretty good approximation of the underlying output function. This is also shown graphically in figure <a class="url" href="#fig-gradients-pytorch-linear">3.11</a>. But what happens if the underlying output function is nonlinear?</p>
<p class="body">First, listing <a class="url" href="#listing3-3">3.3</a> tries to use a linear model on a nonlinear data distribution. As expected (and demonstrated via the output as well as figure <a class="url" href="#fig-gradients-pytorch-non-linear-using-linear">3.12</a>), this model does not do well, because we are using an inadequate model architecture. Further training will not help.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="480" id="fig-gradients-pytorch-linear" src="../../OEBPS/Images/CH03_F11_Chaudhury.png" width="743"/></p>
<p class="figurecaption">Figure 3.11 Linear approximation of linear data. By step 1,000, the model has more or less converged to the true underlying function.</p>
</div>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="475" id="fig-gradients-pytorch-non-linear-using-linear" src="../../OEBPS/Images/CH03_F12_Chaudhury.png" width="768"/></p>
<p class="figurecaption">Figure 3.12 Linear approximation of nonlinear data. Clearly the model is not converging to anything close to the desired/true function. Our model architecture is inadequate.<a id="marker-106"/></p>
</div>
<p class="fm-code-listing-caption" id="listing3-3">Listing 3.3 Linear approximation of nonlinear data</p>
<pre class="programlisting">x = 10 * torch.rand(N, 1)                    <span class="fm-combinumeral">①</span>

y = x**2 - x + 2.0 
y_obs = y + (0.5 * torch.rand(N, 1) - 0.25)  <span class="fm-combinumeral">②</span>

w = torch.rand(1, requires_grad=True)
b = torch.rand(1, requires_grad=True)
params = [b, w]
for step in range(num_steps):
  y_pred = params[0] + params[1] * x         <span class="fm-combinumeral">③</span>
  mean_squared_error = torch.mean((y_pred - y_obs) ** 2)
  mean_squared_error.backward()
  update_parameters(params, learning_rate)

print("True function: y = 1.5*x + 2.73")
print("Learned function: y_pred = {}*x + {}"
      .format(params[1].data[0], params[0].data[0]))</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates random input training data</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generates training output: applies a known nonlinear function to the input and then perturbs it with noise</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Trains a linear model as in listing 3.2</p>
<p class="body">Here is the output:</p>
<pre class="programlisting">True function: y=x^2 -x + 2 
Learned function: y_pred = 8.79633331299*x + -13.4027605057</pre>
<p class="body">Next, listing <a class="url" href="#listing3-4">3.4</a> tries a nonlinear model. As expected (and demonstrated via the output as well as figure <a class="url" href="#fig-gradients-pytorch-nonlinear">3.13</a>), the nonlinear model does well. In real-life problems, we usually assume nonlinearity and choose a model architecture accordingly.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="457" id="fig-gradients-pytorch-nonlinear" src="../../OEBPS/Images/CH03_F13_Chaudhury.png" width="768"/></p>
<p class="figurecaption">Figure 3.13 If we use a nonlinear model, it more or less converges to the true underlying function.</p>
</div>
<p class="fm-code-listing-caption" id="listing3-4">Listing 3.4 Nonlinear modeling with PyTorch<a id="marker-107"/></p>
<pre class="programlisting">params = [w0, w1, w2]
for step in range(num_steps):
    y_pred = params[0] + params[1] * x + params[2] * (x**2)
    mean_squared_error = torch.mean((y_pred -y_obs) ** 2)
    mean_squared_error.backward()
    update_parameters(params, learning_rate)

print("True function: y= 2 - x + x^2")
print("Learned function: y_pred = {} + {}*x + {}*x^2"
      .format(params[0].data[0],
              params[1].data[0],
              params[2].data[0]))</pre>
<p class="body">Here is the output:</p>
<pre class="programlisting">True function: y= 2 - x + x^2 
Learned function: y_pred = 1.87116754055+-0.953767299652*x+0.996278882027*x^2</pre>
<h3 class="fm-head1" id="a-linear-model-for-the-cat-brain-in-pytorch">3.5.4 A linear model for the cat brain in PyTorch</h3>
<p class="body"><a id="marker-108"/>In section <a class="url" href="02.xhtml#python-overdet">2.12.5</a>, we solved the cat-brain problem directly via pseudo-inverse. Now, let’s train a PyTorch model over the same dataset. As expected, the model parameters will converge to a solution close to that obtained by the pseudo-inverse technique this being a simple training dataset); but in the following listing, we demonstrate our first somewhat sophisticated PyTorch model.</p>
<p class="fm-code-listing-caption" id="listing3-5">Listing 3.5 Our first realistic PyTorch model (solves the cat-brain problem)</p>
<pre class="programlisting">X = torch.tensor([[0.11, 0.09], ... [0.63, 0.24]])       <span class="fm-combinumeral">①</span>

X = torch.column_stack((X, torch.ones(15)))              <span class="fm-combinumeral">①</span>
    <span class="fm-combinumeral">②</span>
y = torch.tensor([-0.8, ... 0.37])                       <span class="fm-combinumeral">①</span>

class LinearModel(torch.nn.Module):
    def __init__(self, num_features):
        super(LinearModel, self).__init__()


        self.w = torch.nn.Parameter(                     <span class="fm-combinumeral">③</span>
            torch.randn(num_features, 1))

    def forward(self, X):
        y_pred  = torch.mm(X, self.w)                    <span class="fm-combinumeral">④</span>
        return y_pred

model =  LinearModel(num_features=num_unknowns)

loss_fn = torch.nn.MSELoss(reduction='sum')              <span class="fm-combinumeral">⑤</span>

optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) <span class="fm-combinumeral">⑥</span>

for step in range(num_steps):
    y_pred = model(X)
    loss = loss_fn(y_pred, y)
    optimizer.zero_grad()                                <span class="fm-combinumeral">⑦</span>
    loss.backward()                                      <span class="fm-combinumeral">⑧</span>

    optimizer.step()                                     <span class="fm-combinumeral">⑨</span>

solution_gd = torch.squeeze(model.w.data)
print("The solution via gradient descent is {}".format(solution_gd))</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> <i class="timesitalic">X</i>, <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> created (see section <a class="url" href="02.xhtml#subsec-over-under-determined-linsys">2.12.3</a>) as per equation <a class="url" href="02.xhtml#eq-lin-model">2.22</a> It is easy to verify that the solution to equation <a class="url" href="02.xhtml#eq-lin-model">2.22</a> is roughly <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub> = 1</span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub> = 1</span>, <span class="math"><i class="fm-italics">b</i> = −1</span>. But the equations are not consistent: no one solution perfectly fits all of them. We expect the learned model to be close to <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">x</i><sub class="fm-subscript">0</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">1</sub> − 1</span>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Adds a column of all 1s to augment the data matrix <i class="timesitalic">X</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Parameter is a type (subclass) of Torch Tensor suitable for model parameters (weights+bias).</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Linear model: <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = <i class="fm-italics">X</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>(<i class="timesitalic">X</i></span> is augmented, and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> includes bias)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Ready-made class for computing squared error loss/</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Ready-made class for updating weights using the gradient of error</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Zeros out all partial derivatives</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Computes partial derivatives via autograd</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Updates the parameters using gradients computed in the backward() step</p>
<p class="body">The output is as follows:</p>
<pre class="programlisting">The solution via gradient descent is [ 1.0766 0.8976 -0.9581]</pre>
<h2 class="fm-head" id="sec-locglob-minima">3.6 Convex and nonconvex functions, and global and local minima</h2>
<p class="body"><a id="marker-109"/>A convex surface (see figure <a class="url" href="#fig-convex_diagram">3.14</a>) has a single optimum (maximum/minimum): the global one.<a class="url" href="#fn10" id="fnref10"><sup class="fm-superscript">2</sup></a> Wherever we are on such a surface, if we keep moving along the gradient in parameter space, we will eventually reach the global minimum. On the other hand, on a nonconvex surface, we might get stuck in a local minimum. For instance, in figure <a class="url" href="#fig-nonconvex_diagram">3.14b</a>, if we start at the point marked with the arrowed line indicating a gradient and move downward following the gradient, we will arrive at a local minimum. At the minimum, the gradient is zero, and we will never move out of that point.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre13" height="316" src="../../OEBPS/Images/CH03_F14a_Chaudhury.png" width="519"/></p>
<p class="figurecaption" id="fig-convex_diagram">(a) A convex function</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre13" height="329" id="fig-nonconvex_diagram" src="../../OEBPS/Images/CH03_F14b_Chaudhury.png" width="671"/></p>
<p class="figurecaption">(b) A nonconvex function</p>
</div>
<p class="fm-table-caption" id="fig-convex-nonconvex_diagram">Figure 3.14 Convex vs. nonconvex functions. Convex functions have only a global optimum (minimum or maximum), nolocal optimum. Following the gradient downward is guaranteed to reach the global minimum. Friendly error functions are convex. A nonconvex function has one or more local optima. Following the gradient may reach a local minimum and never discover the global minimum. Unfriendly error functions are nonconvex.</p>
<p class="body">There was a time when researchers put a lot of effort into trying to avoid local minima. Special techniques (such as simulated annealing) were developed to avoid them. However, neural networks typically do not do anything special to deal with local minima and nonconvex functions. Often, the local minimum is good enough. Or we can retrain by starting from a different random point, which may help us escape the local minimum.</p>
<h2 class="fm-head" id="sec-conv-functions">3.7 Convex sets and functions</h2>
<p class="body">In section <a class="url" href="../Text/03.xhtml#sec-locglob-minima">3.6</a>, we briefly encountered convex functions and how convexity tells us whether a function has local minima. In this section, we look at convex functions in more detail. In particular, we learn how to tell whether a given function is convex. We also discuss some important properties of convex functions that will come in handy later—for instance, when we study Jensen’s inequality in probability and statistics, in the appendix. We will mostly illustrate the ideas in <span class="math">2</span>D space, but they can be easily extended to higher dimensions.<a id="marker-110"/></p>
<h3 class="fm-head1" id="convex-sets">3.7.1 Convex sets</h3>
<p class="body">Informally speaking, a set of points is said to be convex if and only if the straight line joining any pair of points in the set lies entirely within the set. For example, if we join any pair of points in the shaded region on the left-hand side of figure <a class="url" href="#fig-convex-set">3.15</a> with a straight line, all points on that line will also be in the shaded region. This is illustrated by points A and B in the figure. The complete set of points in any such region constitutes a convex set.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="414" id="fig-convex-set" src="../../OEBPS/Images/CH03_F15_Chaudhury.png" width="1000"/></p>
<p class="figurecaption">Figure 3.15 Convex and nonconvex sets. The points in the left-hand shaded region form a convex set.The line joining any pair of points in that shaded region lies entirely in the shaded region: for example, AB.The points in the right-hand shaded region form a nonconvex set. For instance, the line joining points C and D passes through a nonshaded region even though both end points belong to a shaded region.</p>
</div>
<p class="body">Conversely, a set of points is nonconvex if it contains at least one pair of points whose joining line contains a point not belonging to the set. For instance, the shaded region on the right-hand side of figure <a class="url" href="#fig-convex-set">3.15</a> contains a pair of points C and D whose joining line passes through points not belonging to the shaded region.</p>
<p class="body">The boundary of a convex set is always a convex curve.</p>
<h3 class="fm-head1" id="convex-curves-and-surfaces">3.7.2 Convex curves and surfaces</h3>
<p class="body">Consider a function <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>)</span>. Let’s pick any two points on the curve <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">g</i>(<i class="fm-italics">x</i>)</span>: <span class="math"><i class="fm-italics">A</i> ≡ (<i class="fm-italics">x</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">y</i><sub class="fm-subscript">1</sub> = <i class="fm-italics">g</i>(<i class="fm-italics">x</i><sub class="fm-subscript">1</sub>))</span> and <span class="math"><i class="fm-italics">B</i> ≡ (<i class="fm-italics">x</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">y</i><sub class="fm-subscript">2</sub> = <i class="fm-italics">g</i>(<i class="fm-italics">x</i><sub class="fm-subscript">2</sub>))</span>. Now consider the line segment <i class="timesitalic">L</i> joining <i class="timesitalic">A</i> and <i class="timesitalic">B</i>. From section <a class="url" href="02.xhtml#sec-multidim-line-eq">2.8.1</a> (equation <a class="url" href="02.xhtml#eq-collinearity">2.12</a> and figure <a class="url" href="02.xhtml#fig-multi-dim-lineeq">2.8</a>), we know that all points <i class="timesitalic">C</i> on <i class="timesitalic">L</i> can be expressed as a weighted average of the coordinates of <i class="timesitalic">A</i> and <i class="timesitalic">B</i>, with the sum of weights being <span class="math">1</span>. Thus, <span class="math"><i class="fm-italics">C</i> ≡ (<i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">y</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">y</i><sub class="fm-subscript">2</sub>)</span>, where <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub> = 1</span>. Compare <i class="timesitalic">C</i> with its corresponding point <i class="timesitalic">D</i> on the curve, which has the same <i class="timesitalic">X</i> coordinate:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">D</i> ≡ (<i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">g</i>(<i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub>))</span>.</p>
<p class="body">If and only if <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>)</span> is a convex function, <i class="timesitalic">C</i> will always be above <i class="timesitalic">D</i>, or</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">y</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">y</i><sub class="fm-subscript">2</sub> = <i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">g</i>(<i class="fm-italics">x</i><sub class="fm-subscript">1</sub>) + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">g</i>(<i class="fm-italics">x</i><sub class="fm-subscript">2</sub>) ≥ <i class="fm-italics">g</i>(<i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="body">Viewed another way, if we drop a perpendicular to the <i class="timesitalic">X</i>-axis from any point on the secant line joining a pair of points on the curve, that perpendicular will cut the curve at a lower point (that is, smaller in its <i class="timesitalic">Y</i>-coordinate).</p>
<p class="body">This is illustrated on the left-hand side of figure <a class="url" href="#fig-convex-wt-avg">3.19</a> with the function <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">x</i><sup class="fm-superscript">2</sup></span> known to be convex) and <span class="math"><i class="fm-italics">A</i> ≡ (−3,9)</span> and <span class="math"><i class="fm-italics">B</i> ≡ (5,25)</span>, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub> = 0.3</span>, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">2</sub> = 0.7</span>. It can be seen that the weighted average point <i class="timesitalic">C</i> on the line lies above the corresponding point on the curve <i class="timesitalic">D</i>. The right-hand side illustrates the nonconvex function <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">x</i><sup class="fm-superscript">3</sup></span>, with <span class="math"><i class="fm-italics">A</i> ≡ (−8,−512)</span> and <span class="math"><i class="fm-italics">B</i> ≡ (5,125)</span>, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub> = 0.3</span>, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">2</sub> = 0.7</span>. The figure shows one weighted average point (<i class="timesitalic">C</i>) on the line joining points <i class="timesitalic">A</i> and <i class="timesitalic">B</i> on the curve: <i class="timesitalic">C</i> lies below point <i class="timesitalic">D</i> on the curve, which has the same <i class="timesitalic">X</i>-coordinate.<a id="marker-111"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="458" id="fig-convex-wt-avg" src="../../OEBPS/Images/CH03_F16_Chaudhury.png" width="1000"/></p>
<p class="figurecaption">Figure 3.16 Convex and nonconvex curves. <i class="timesitalic">A</i> and <i class="timesitalic">B</i> are a pair of points on the curve. <span class="math"><i class="fm-italics">C</i> = 0.3<i class="fm-italics">A</i> + 0.7<i class="fm-italics">B</i></span> is a weighted average of the coordinates of A and B, with weights summing to <span class="math">1</span>. <i class="timesitalic">C</i> lies on the line joining <i class="timesitalic">A</i> and <i class="timesitalic">B</i>. The left-hand curve is convex: <i class="timesitalic">C</i> lies above the corresponding curve point <i class="timesitalic">D</i>. The right-hand curve is nonconvex: <i class="timesitalic">C</i> lies below the corresponding curve point <i class="timesitalic">D</i>.</p>
</div>
<p class="body">We need not restrict ourselves to two points. We can take the weighted average of an arbitrary number of points on the curve, with the weights summing to one. The point corresponding to the weighted average will lie above the curve (that is, above the point on the curve with the same <i class="timesitalic">X</i>-coordinate). The idea also extends to higher dimensions, as discussed next.</p>
<p class="fm-head2" id="definition-1">Definition 1</p>
<p class="body">In general, a multidimensional function <span class="math"><i class="fm-italics">g</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is convex if and only if</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Given an arbitrary set of points on the function surface (curve, if the function is <span class="math">1</span>D), <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">1</sub>, <i class="fm-italics">g</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">1</sub>))</span>, <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">2</sub>, <i class="fm-italics">g</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">2</sub>))</span>, <span class="math">⋯</span>, <span class="math">(<i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">n</sub></i>, <i class="fm-italics">g</i>(<i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">n</sub></i>))</span>,</p>
</li>
<li class="fm-list-bullet">
<p class="list">And given an arbitrary set of <i class="timesitalic">n</i> weights <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">α<sub class="fm-subscript">n</sub></i> that sum to <span class="math">1</span> (that is, <span class="math">Σ<i class="fm-italics1"><sub class="fm-subscript">i</sub><sup class="fm-superscript">n</sup></i><sub class="fm-subscript">= 1</sub> <i class="fm-italics">α<sub class="fm-subscript">i</sub></i> = 1</span>),</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">The weighted sum of the function outputs exceeds or equals the function output on the weighted sums</i>:</p>
</li>
</ul><!--<p class="FM-Equation"><span class="times">$$\sum_{i=1}^{n} \alpha_{i} g\left(\vec{x}_{i}\right) \geq g\left(\sum_{i=1}^{n} \alpha_{i}  \vec{x}_{i}\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_03-11.png" width="206"/></p>
</div>
<p class="fm-equation-caption">Equation 3.11 <span class="calibre" id="eq-convexity-wt-avg"/></p>
<p class="body"><a id="marker-112"/>A little thought will reveal that definition 1 implies that convex curves always curl upward and/or rightward everywhere. This leads to another equivalent definition of convexity.</p>
<p class="fm-head2" id="definition-2">Definition 2</p>
<p class="body">In general, a multidimensional function <span class="math"><i class="fm-italics">g</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is convex if and only if</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">A <span class="math">1</span>D function <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>)</span> is convex if and only if its curvature is positive everywhere:</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\frac{d^{2}g}{dx^{2}} \geq 0 \;\;\;\;\forall x$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="51" src="../../OEBPS/Images/eq_03-12.png" width="107"/></p>
</div>
<p class="fm-equation-caption">Equation 3.12 <span class="calibre" id="eq-convexity-2nd-deriv"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">A multidimensional function <span class="math"><i class="fm-italics">g</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is convex if and only if its Hessian matrix (see section <a class="url" href="#sec-taylor-multidim">3.4.2</a>, equation <a class="url" href="../Text/03.xhtml#eq-hessian">3.9</a>) is positive semi-definite that is, all the eigenvalues of the Hessian matrix are greater than or equal to zero). This is just the multidimensional analog of equation <a class="url" href="#eq-convexity-2nd-deriv">3.12</a>.</p>
</li>
</ul>
<p class="body">One subtle point to note is that if the second derivative is negative everywhere or the Hessian is negative semi-definite, the curve or surface is said to be <i class="fm-italics">concave</i>. This is different from nonconvex curves, where the second derivative is positive in some places and negative in others. The negative of a concave function is a convex function. But the negative of a nonconvex function is again nonconvex.</p>
<p class="body">A function that curves upward everywhere always lies above its tangent. This leads to another equivalent definition of a convex function.</p>
<p class="fm-head2" id="definition-3">Definition 3</p>
<p class="body">In general, a multidimensional function <span class="math"><i class="fm-italics">g</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is convex if and only if</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">A function <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>)</span> is convex if and only if all the points on the curve <span class="math"><i class="fm-italics">S</i> ≡ (<i class="fm-italics">x</i>, <i class="fm-italics">g</i>(<i class="fm-italics">x</i>))</span> lie above the tangent line <i class="timesitalic">T</i> at any point <i class="timesitalic">A</i> on <i class="timesitalic">S</i>, with <i class="timesitalic">S</i> touching <i class="timesitalic">T</i> only at <i class="timesitalic">A</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A function <span class="math"><i class="fm-italics">g</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is convex if and only if all the points on the surface <span class="math"><i class="fm-italics">S</i> ≡ (<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">g</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>))</span> lie above the tangent plane <i class="timesitalic">T</i> at any point <i class="timesitalic">A</i> on <i class="timesitalic">S</i>, with <i class="timesitalic">S</i> touching <i class="timesitalic">T</i> only at <i class="timesitalic">A</i>.</p>
</li>
</ul>
<p class="body">This is illustrated in figure <a class="url" href="#fig-convex-tangent">3.17</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre23" height="407" id="fig-convex-tangent" src="../../OEBPS/Images/CH03_F17_Chaudhury.png" width="983"/></p>
<p class="figurecaption">Figure 3.17 The left-hand curve is convex. If we draw a tangent line at any point <i class="timesitalic">A</i> on the curve, the entire curve is <i class="fm-italics">above</i> the tangent line, touching it only at <i class="timesitalic">A</i>. The right-hand cuve is nonconvex: part of the curve lies above the tangent and part of it below.</p>
</div>
<h3 class="fm-head1" id="convexity-and-the-taylor-series">3.7.3 Convexity and the Taylor series</h3>
<p class="body"><a id="marker-113"/>In section <a class="url" href="#sec-taylor-onedim">3.4.1</a>, equation <a class="url" href="#eq-taylor-onedim">3.7</a>, we saw the one-dimensional Taylor expansion for a function in the neighborhood of a point <i class="timesitalic">x</i>. If we retain the terms in the Taylor expansion only up to the first derivative and ignore all subsequent terms, that is equivalent to approximating the function at <i class="timesitalic">x</i> with its tangent at <i class="timesitalic">x</i> (see figure <a class="url" href="#fig-convex-tangent">3.17</a>). This is the linear approximation to the curve. If we retain one more term (that is, up to the second derivative), we get the quadratic approximation to the curve. If the second derivative of the function is always positive (as in convex functions), the quadratic approximation to the function will always be greater than or equal to the linear approximation. In other words, locally, the curve will curve so that it lies above the tangent. This connects the second derivative definition (definition 2) with the tangent definition (definition 3) of convexity.</p>
<h3 class="fm-head1" id="examples-of-convex-functions">3.7.4 Examples of convex functions</h3>
<p class="body">The function <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">x</i><sup class="fm-superscript">2</sup></span> is convex. The easiest way to verify this is to compute <span class="math"><i class="fm-italics">d</i><sup class="fm-superscript">2</sup><i class="fm-italics">g</i>/<i class="fm-italics">dx</i><sup class="fm-superscript">2</sup> = <i class="fm-italics">d</i>2<i class="fm-italics">x</i>/<i class="fm-italics">dx</i> = 2</span>, which is always positive. In fact, any even power of <i class="timesitalic">x</i>, <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">x</i><sup class="fm-superscript">2<i class="fm-italics1">n</i></sup></span> for an integer <i class="timesitalic">n</i>, such as <span class="math"><i class="fm-italics">x</i><sup class="fm-superscript">4</sup></span> or <span class="math"><i class="fm-italics">x</i><sup class="fm-superscript">6</sup></span>, is convex. <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">e<sup class="fm-superscript">x</sup></i></span> is also convex. This can be easily verified by taking its second derivative. <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">logx</i></span> is concave. Hence, <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = −<i class="fm-italics">logx</i></span> is convex.</p>
<p class="body">Multiplication by a positive scalar preserves convexity. The sum of convex functions is also a convex function.</p>
<h2 class="fm-head" id="summary-2">Summary</h2>
<p class="body">We would like to leave you with the following mental pictures from this chapter:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Inputs for a machine learning problem can be viewed as vectors or, equivalently, points in a high-dimensional feature space. Classification is nothing but separating clusters of points belonging to individual classes in this space.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A classifier is can be viewed geometrically as the hypersurface (aka decision boundary) in the high-dimensional feature space, separating the point clusters corresponding to individual classes. During training, we collect sample inputs with known classes and identify the surface that best separates the corresponding points. During inferencing, given an unknown input, we determine which side of the decision boundary this point lies in—this tells us the class.</p>
</li>
<li class="fm-list-bullet">
<p class="list">For two-class classifiers (aka binary classifiers), if we plug in the point in the function for the classifier hypersurface, the sign of the corresponding output yields the class.</p>
</li>
<li class="fm-list-bullet">
<p class="list">To compute the hypersurface decision boundary that best separates the training data, we first choose a parametric function family to model this surface (for example, a hyperplane for simple problems). Then we estimate the optimal parameter values that best separate the training data, usually in an iterative fashion.</p>
</li>
<li class="fm-list-bullet">
<p class="list">To estimate the parameter values that optimally separates the training data, we define a loss function that measures the difference between the model output and the known desired output over the entire training dataset. Then, starting from random initial values, we iteratively adjust the parameter values so that the loss value decreases progressively.</p>
</li>
<li class="fm-list-bullet">
<p class="list">At every iteration, the adjustment to the parameter values that optimally reduces the loss is estimated by computing the gradient of the loss function.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The gradient of a multidimensional function identifies the direction in the parameter space corresponding to the maximum change in the function. Thus, the gradient of the loss function identifies the direction in which we can adjust the parameters to maximally decrease the loss.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The gradient is zero at the maximum or minimum point of a function, which is always a point of inflection. This can be used to recognize when we have reached the minimum. However, in practice, in machine learning we often do an early stop: terminate training iterations when the loss is sufficiently low.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A multidimensional Taylor series can be used to create local approximations to a smooth function in the neighborhood of a point. The function is expressed in terms of the displacement from the point, the first-order derivatives (gradient), second-order derivatives Hessian matrix), and so on. This can be used to make higher-accuracy approximations to the change in loss value resulting from a displacement in the parameter space.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Loss functions can be<i class="fm-italics">convex</i> or <i class="fm-italics">nonconvex</i>. In a convex function, there is no local minimum, only a single global minimum. Hence, gradient descent is guaranteed to converge to the global minimum. A nonconvex function can have both a local and a global minimum. So, gradient-based descent may get stuck in a local minimum.<a id="marker-114"/></p>
</li>
</ul>
<hr class="calibre14"/>
<p class="fm-footnote" id="fn9"><sup class="footnotenumber">1</sup>  If the change in a quantity such as <i class="timesitalic">w</i> is infinitesimally small, we use the symbol <i class="timesitalic">dw</i> to denote the change. If the change is small but not infinitesimally so, we use the symbol <i class="timesitalic">δw</i>. <a class="url" href="#fnref9">↩</a></p>
<p class="fm-footnote" id="fn10"><sup class="footnotenumber">2</sup>  Although the theory applies to either optimum, maximum or minimum, for brevity’s sake, here we will only talk in terms of the minimum <a class="url" href="#fnref10">↩</a></p>
</div></body></html>