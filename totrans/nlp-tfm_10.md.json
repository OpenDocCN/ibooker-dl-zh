["```py\nimport pandas as pd\n\ndataset_url = \"https://git.io/nlp-with-transformers\"\ndf_issues = pd.read_json(dataset_url, lines=True)\nprint(f\"DataFrame shape: {df_issues.shape}\")\n```", "```py\nDataFrame shape: (9930, 26)\n```", "```py\ncols = [\"url\", \"id\", \"title\", \"user\", \"labels\", \"state\", \"created_at\", \"body\"]\ndf_issues.loc[2, cols].to_frame()\n```", "```py\n[\n   {\n      \"id\":2659267025,\n      \"node_id\":\"MDU6TGFiZWwyNjU5MjY3MDI1\",\n      \"url\":\"https://api.github.com/repos/huggingface...\",\n      \"name\":\"DeepSpeed\",\n      \"color\":\"4D34F7\",\n      \"default\":false,\n      \"description\":\"\"\n   }\n]\n```", "```py\ndf_issues[\"labels\"] = (df_issues[\"labels\"]\n                       .apply(lambda x: [meta[\"name\"] for meta in x]))\ndf_issues[[\"labels\"]].head()\n```", "```py\ndf_issues[\"labels\"].apply(lambda x : len(x)).value_counts().to_frame().T\n```", "```py\ndf_counts = df_issues[\"labels\"].explode().value_counts()\nprint(f\"Number of labels: {len(df_counts)}\")\n# Display the top-8 label categories\ndf_counts.to_frame().head(8).T\n```", "```py\nNumber of labels: 65\n```", "```py\nlabel_map = {\"Core: Tokenization\": \"tokenization\",\n             \"New model\": \"new model\",\n             \"Core: Modeling\": \"model training\",\n             \"Usage\": \"usage\",\n             \"Core: Pipeline\": \"pipeline\",\n             \"TensorFlow\": \"tensorflow or tf\",\n             \"PyTorch\": \"pytorch\",\n             \"Examples\": \"examples\",\n             \"Documentation\": \"documentation\"}\n\ndef filter_labels(x):\n    return [label_map[label] for label in x if label in label_map]\n\ndf_issues[\"labels\"] = df_issues[\"labels\"].apply(filter_labels)\nall_labels = list(label_map.values())\n```", "```py\ndf_counts = df_issues[\"labels\"].explode().value_counts()\ndf_counts.to_frame().T\n```", "```py\ndf_issues[\"split\"] = \"unlabeled\"\nmask = df_issues[\"labels\"].apply(lambda x: len(x)) > 0\ndf_issues.loc[mask, \"split\"] = \"labeled\"\ndf_issues[\"split\"].value_counts().to_frame()\n```", "```py\nfor column in [\"title\", \"body\", \"labels\"]:\n    print(f\"{column}: {df_issues[column].iloc[26][:500]}\\n\")\n```", "```py\ntitle: Add new CANINE model\n\nbody: #  New model addition\n\n## Model description\n\nGoogle recently proposed a new **C**haracter **A**rchitecture with **N**o\n tokenization **I**n **N**eural **E**ncoders architecture (CANINE). Not only\n the title is exciting:\n\nPipelined NLP systems have largely been superseded by end-to-end neural\n modeling, yet nearly all commonly-used models still require an explicit\n tokenization step. While recent tokenization approaches based on data-derived\n subword lexicons are less brittle than manually en\n\nlabels: ['new model']\n\n```", "```py\ndf_issues[\"text\"] = (df_issues\n                     .apply(lambda x: x[\"title\"] + \"\\n\\n\" + x[\"body\"], axis=1))\n```", "```py\nlen_before = len(df_issues)\ndf_issues = df_issues.drop_duplicates(subset=\"text\")\nprint(f\"Removed {(len_before-len(df_issues))/len_before:.2%} duplicates.\")\n```", "```py\nRemoved 1.88% duplicates.\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n(df_issues[\"text\"].str.split().apply(len)\n .hist(bins=np.linspace(0, 500, 50), grid=False, edgecolor=\"C0\"))\nplt.title(\"Words per issue\")\nplt.xlabel(\"Number of words\")\nplt.ylabel(\"Number of issues\")\nplt.show()\n```", "```py\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nmlb.fit([all_labels])\nmlb.transform([[\"tokenization\", \"new model\"], [\"pytorch\"]])\n```", "```py\narray([[0, 0, 0, 1, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0]])\n```", "```py\nfrom skmultilearn.model_selection import iterative_train_test_split\n\ndef balanced_split(df, test_size=0.5):\n    ind = np.expand_dims(np.arange(len(df)), axis=1)\n    labels = mlb.transform(df[\"labels\"])\n    ind_train, _, ind_test, _ = iterative_train_test_split(ind, labels,\n                                                           test_size)\n    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:,0]]\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\ndf_clean = df_issues[[\"text\", \"labels\", \"split\"]].reset_index(drop=True).copy()\ndf_unsup = df_clean.loc[df_clean[\"split\"] == \"unlabeled\", [\"text\", \"labels\"]]\ndf_sup = df_clean.loc[df_clean[\"split\"] == \"labeled\", [\"text\", \"labels\"]]\n\nnp.random.seed(0)\ndf_train, df_tmp = balanced_split(df_sup, test_size=0.5)\ndf_valid, df_test = balanced_split(df_tmp, test_size=0.5)\n```", "```py\nfrom datasets import Dataset, DatasetDict\n\nds = DatasetDict({\n    \"train\": Dataset.from_pandas(df_train.reset_index(drop=True)),\n    \"valid\": Dataset.from_pandas(df_valid.reset_index(drop=True)),\n    \"test\": Dataset.from_pandas(df_test.reset_index(drop=True)),\n    \"unsup\": Dataset.from_pandas(df_unsup.reset_index(drop=True))})\n```", "```py\nnp.random.seed(0)\nall_indices = np.expand_dims(list(range(len(ds[\"train\"]))), axis=1)\nindices_pool = all_indices\nlabels = mlb.transform(ds[\"train\"][\"labels\"])\ntrain_samples = [8, 16, 32, 64, 128]\ntrain_slices, last_k = [], 0\n\nfor i, k in enumerate(train_samples):\n    # Split off samples necessary to fill the gap to the next split size\n    indices_pool, labels, new_slice, _ = iterative_train_test_split(\n        indices_pool, labels, (k-last_k)/len(labels))\n    last_k = k\n    if i==0: train_slices.append(new_slice)\n    else: train_slices.append(np.concatenate((train_slices[-1], new_slice)))\n\n# Add full dataset as last slice\ntrain_slices.append(all_indices), train_samples.append(len(ds[\"train\"]))\ntrain_slices = [np.squeeze(train_slice) for train_slice in train_slices]\n```", "```py\nprint(\"Target split sizes:\")\nprint(train_samples)\nprint(\"Actual split sizes:\")\nprint([len(x) for x in train_slices])\n```", "```py\nTarget split sizes:\n[8, 16, 32, 64, 128, 223]\nActual split sizes:\n[10, 19, 36, 68, 134, 223]\n```", "```py\ndef prepare_labels(batch):\n    batch[\"label_ids\"] = mlb.transform(batch[\"labels\"])\n    return batch\n\nds = ds.map(prepare_labels, batched=True)\n```", "```py\nfrom collections import defaultdict\n\nmacro_scores, micro_scores = defaultdict(list), defaultdict(list)\n```", "```py\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfor train_slice in train_slices:\n    # Get training slice and test data\n    ds_train_sample = ds[\"train\"].select(train_slice)\n    y_train = np.array(ds_train_sample[\"label_ids\"])\n    y_test = np.array(ds[\"test\"][\"label_ids\"])\n    # Use a simple count vectorizer to encode our texts as token counts\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(ds_train_sample[\"text\"])\n    X_test_counts = count_vect.transform(ds[\"test\"][\"text\"])\n    # Create and train our model!\n    classifier = BinaryRelevance(classifier=MultinomialNB())\n    classifier.fit(X_train_counts, y_train)\n    # Generate predictions and evaluate\n    y_pred_test = classifier.predict(X_test_counts)\n    clf_report = classification_report(\n        y_test, y_pred_test, target_names=mlb.classes_, zero_division=0,\n        output_dict=True)\n    # Store metrics\n    macro_scores[\"Naive Bayes\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n    micro_scores[\"Naive Bayes\"].append(clf_report[\"micro avg\"][\"f1-score\"])\n```", "```py\nimport matplotlib.pyplot as plt\n\ndef plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):\n    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n\n    for run in micro_scores.keys():\n        if run == current_model:\n            ax0.plot(sample_sizes, micro_scores[run], label=run, linewidth=2)\n            ax1.plot(sample_sizes, macro_scores[run], label=run, linewidth=2)\n        else:\n            ax0.plot(sample_sizes, micro_scores[run], label=run,\n                     linestyle=\"dashed\")\n            ax1.plot(sample_sizes, macro_scores[run], label=run,\n                     linestyle=\"dashed\")\n\n    ax0.set_title(\"Micro F1 scores\")\n    ax1.set_title(\"Macro F1 scores\")\n    ax0.set_ylabel(\"Test set F1 score\")\n    ax0.legend(loc=\"lower right\")\n    for ax in [ax0, ax1]:\n        ax.set_xlabel(\"Number of training samples\")\n        ax.set_xscale(\"log\")\n        ax.set_xticks(sample_sizes)\n        ax.set_xticklabels(sample_sizes)\n        ax.minorticks_off()\n    plt.tight_layout()\n    plt.show()\n```", "```py\nplot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes\")\n```", "```py\nfrom transformers import pipeline\n\npipe = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n```", "```py\nmovie_desc = \"The main characters of the movie madacascar \\\nare a lion, a zebra, a giraffe, and a hippo. \"\nprompt = \"The movie is about [MASK].\"\n\noutput = pipe(movie_desc + prompt)\nfor element in output:\n    print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n```", "```py\nToken animals:  0.103%\nToken lions:    0.066%\nToken birds:    0.025%\nToken love:     0.015%\nToken hunting:  0.013%\n```", "```py\noutput = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\nfor element in output:\n    print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n```", "```py\nToken animals:  0.103%\nToken cars:     0.001%\n```", "```py\nmovie_desc = \"In the movie transformers aliens \\\ncan morph into a wide range of vehicles.\"\n\noutput = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\nfor element in output:\n    print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n```", "```py\nToken cars:     0.139%\nToken animals:  0.006%\n```", "```py\nfrom transformers import pipeline\n\npipe = pipeline(\"zero-shot-classification\", device=0)\n```", "```py\nsample = ds[\"train\"][0]\nprint(f\"Labels: {sample['labels']}\")\noutput = pipe(sample[\"text\"], all_labels, multi_label=True)\nprint(output[\"sequence\"][:400])\nprint(\"\\nPredictions:\")\n\nfor label, score in zip(output[\"labels\"], output[\"scores\"]):\n    print(f\"{label}, {score:.2f}\")\n```", "```py\nLabels: ['new model']\nAdd new CANINE model\n\n#  New model addition\n\n## Model description\n\nGoogle recently proposed a new **C**haracter **A**rchitecture with **N**o\ntokenization **I**n **N**eural **E**ncoders architecture (CANINE). Not only the\ntitle is exciting:\n\n> Pipelined NLP systems have largely been superseded by end-to-end neural\nmodeling, yet nearly all commonly-used models still require an explicit tokeni\n\nPredictions:\nnew model, 0.98\ntensorflow or tf, 0.37\nexamples, 0.34\nusage, 0.30\npytorch, 0.25\ndocumentation, 0.25\nmodel training, 0.24\ntokenization, 0.17\npipeline, 0.16\n\n```", "```py\ndef zero_shot_pipeline(example):\n    output = pipe(example[\"text\"], all_labels, multi_label=True)\n    example[\"predicted_labels\"] = output[\"labels\"]\n    example[\"scores\"] = output[\"scores\"]\n    return example\n\nds_zero_shot = ds[\"valid\"].map(zero_shot_pipeline)\n```", "```py\ndef get_preds(example, threshold=None, topk=None):\n    preds = []\n    if threshold:\n        for label, score in zip(example[\"predicted_labels\"], example[\"scores\"]):\n            if score >= threshold:\n                preds.append(label)\n    elif topk:\n        for i in range(topk):\n            preds.append(example[\"predicted_labels\"][i])\n    else:\n        raise ValueError(\"Set either `threshold` or `topk`.\")\n    return {\"pred_label_ids\": list(np.squeeze(mlb.transform([preds])))}\n```", "```py\ndef get_clf_report(ds):\n    y_true = np.array(ds[\"label_ids\"])\n    y_pred = np.array(ds[\"pred_label_ids\"])\n    return classification_report(\n        y_true, y_pred, target_names=mlb.classes_, zero_division=0,\n        output_dict=True)\n```", "```py\nmacros, micros = [], []\ntopks = [1, 2, 3, 4]\nfor topk in topks:\n    ds_zero_shot = ds_zero_shot.map(get_preds, batched=False,\n                                    fn_kwargs={'topk': topk})\n    clf_report = get_clf_report(ds_zero_shot)\n    micros.append(clf_report['micro avg']['f1-score'])\n    macros.append(clf_report['macro avg']['f1-score'])\n```", "```py\nplt.plot(topks, micros, label='Micro F1')\nplt.plot(topks, macros, label='Macro F1')\nplt.xlabel(\"Top-k\")\nplt.ylabel(\"F1-score\")\nplt.legend(loc='best')\nplt.show()\n```", "```py\nmacros, micros = [], []\nthresholds = np.linspace(0.01, 1, 100)\nfor threshold in thresholds:\n    ds_zero_shot = ds_zero_shot.map(get_preds,\n                                    fn_kwargs={\"threshold\": threshold})\n    clf_report = get_clf_report(ds_zero_shot)\n    micros.append(clf_report[\"micro avg\"][\"f1-score\"])\n    macros.append(clf_report[\"macro avg\"][\"f1-score\"])\n```", "```py\nplt.plot(thresholds, micros, label=\"Micro F1\")\nplt.plot(thresholds, macros, label=\"Macro F1\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"F1-score\")\nplt.legend(loc=\"best\")\nplt.show()\n```", "```py\nbest_t, best_micro = thresholds[np.argmax(micros)], np.max(micros)\nprint(f'Best threshold (micro): {best_t} with F1-score {best_micro:.2f}.')\nbest_t, best_macro = thresholds[np.argmax(macros)], np.max(macros)\nprint(f'Best threshold (micro): {best_t} with F1-score {best_macro:.2f}.')\n```", "```py\nBest threshold (micro): 0.75 with F1-score 0.46.\nBest threshold (micro): 0.72 with F1-score 0.42.\n```", "```py\nds_zero_shot = ds['test'].map(zero_shot_pipeline)\nds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs={'topk': 1})\nclf_report = get_clf_report(ds_zero_shot)\nfor train_slice in train_slices:\n    macro_scores['Zero Shot'].append(clf_report['macro avg']['f1-score'])\n    micro_scores['Zero Shot'].append(clf_report['micro avg']['f1-score'])\n```", "```py\nplot_metrics(micro_scores, macro_scores, train_samples, \"Zero Shot\")\n```", "```py\nfrom transformers import set_seed\nimport nlpaug.augmenter.word as naw\n\nset_seed(3)\naug = naw.ContextualWordEmbsAug(model_path=\"distilbert-base-uncased\",\n                                device=\"cpu\", action=\"substitute\")\n\ntext = \"Transformers are the most popular toys\"\nprint(f\"Original text: {text}\")\nprint(f\"Augmented text: {aug.augment(text)}\")\n```", "```py\nOriginal text: Transformers are the most popular toys\nAugmented text: transformers'the most popular toys\n```", "```py\ndef augment_text(batch, transformations_per_example=1):\n    text_aug, label_ids = [], []\n    for text, labels in zip(batch[\"text\"], batch[\"label_ids\"]):\n        text_aug += [text]\n        label_ids += [labels]\n        for _ in range(transformations_per_example):\n            text_aug += [aug.augment(text)]\n            label_ids += [labels]\n    return {\"text\": text_aug, \"label_ids\": label_ids}\n```", "```py\nds_train_sample = ds_train_sample.map(augment_text, batched=True,\n    remove_columns=ds_train_sample.column_names).shuffle(seed=42)\n```", "```py\nplot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes + Aug\")\n```", "```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\nmodel_ckpt = \"miguelvictor/python-gpt2-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModel.from_pretrained(model_ckpt)\n\ndef mean_pooling(model_output, attention_mask):\n    # Extract the token embeddings\n    token_embeddings = model_output[0]\n    # Compute the attention mask\n    input_mask_expanded = (attention_mask\n                           .unsqueeze(-1)\n                           .expand(token_embeddings.size())\n                           .float())\n    # Sum the embeddings, but ignore masked tokens\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    # Return the average as a single vector\n    return sum_embeddings / sum_mask\n\ndef embed_text(examples):\n    inputs = tokenizer(examples[\"text\"], padding=True, truncation=True,\n                       max_length=128, return_tensors=\"pt\")\n    with torch.no_grad():\n        model_output = model(**inputs)\n    pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n    return {\"embedding\": pooled_embeds.cpu().numpy()}\n```", "```py\ntokenizer.pad_token = tokenizer.eos_token\nembs_train = ds[\"train\"].map(embed_text, batched=True, batch_size=16)\nembs_valid = ds[\"valid\"].map(embed_text, batched=True, batch_size=16)\nembs_test = ds[\"test\"].map(embed_text, batched=True, batch_size=16)\n```", "```py\nembs_train.add_faiss_index(\"embedding\")\n```", "```py\ni, k = 0, 3 # Select the first query and 3 nearest neighbors\nrn, nl = \"\\r\\n\\r\\n\", \"\\n\" # Used to remove newlines in text for compact display\n\nquery =  np.array(embs_valid[i][\"embedding\"], dtype=np.float32)\nscores, samples = embs_train.get_nearest_examples(\"embedding\", query, k=k)\n\nprint(f\"QUERY LABELS: {embs_valid[i]['labels']}\")\nprint(f\"QUERY TEXT:\\n{embs_valid[i]['text'][:200].replace(rn, nl)} [...]\\n\")\nprint(\"=\"*50)\nprint(f\"Retrieved documents:\")\nfor score, label, text in zip(scores, samples[\"labels\"], samples[\"text\"]):\n    print(\"=\"*50)\n    print(f\"TEXT:\\n{text[:200].replace(rn, nl)} [...]\")\n    print(f\"SCORE: {score:.2f}\")\n    print(f\"LABELS: {label}\")\n```", "```py\nQUERY LABELS: ['new model']\nQUERY TEXT:\nImplementing efficient self attention in T5\n\n#  New model addition\nMy teammates and I (including @ice-americano) would like to use efficient self\nattention methods such as Linformer, Performer and [...]\n\n==================================================\nRetrieved documents:\n==================================================\nTEXT:\nAdd Linformer model\n\n#  New model addition\n## Model description\n### Linformer: Self-Attention with Linear Complexity\nPaper published June 9th on ArXiv: https://arxiv.org/abs/2006.04768\nLa [...]\nSCORE: 54.92\nLABELS: ['new model']\n==================================================\nTEXT:\nAdd FAVOR+ / Performer attention\n\n#  FAVOR+ / Performer attention addition\nAre there any plans to add this new attention approximation block to\nTransformers library?\n## Model description\nThe n [...]\nSCORE: 57.90\nLABELS: ['new model']\n==================================================\nTEXT:\nImplement DeLighT: Very Deep and Light-weight Transformers\n\n#  New model addition\n## Model description\nDeLight, that delivers similar or better performance than transformer-based\nmodels with sign [...]\nSCORE: 60.12\nLABELS: ['new model']\n\n```", "```py\ndef get_sample_preds(sample, m):\n    return (np.sum(sample[\"label_ids\"], axis=0) >= m).astype(int)\n\ndef find_best_k_m(ds_train, valid_queries, valid_labels, max_k=17):\n    max_k = min(len(ds_train), max_k)\n    perf_micro = np.zeros((max_k, max_k))\n    perf_macro = np.zeros((max_k, max_k))\n    for k in range(1, max_k):\n        for m in range(1, k + 1):\n            _, samples = ds_train.get_nearest_examples_batch(\"embedding\",\n                                                             valid_queries, k=k)\n            y_pred = np.array([get_sample_preds(s, m) for s in samples])\n            clf_report = classification_report(valid_labels, y_pred,\n                target_names=mlb.classes_, zero_division=0, output_dict=True)\n            perf_micro[k, m] = clf_report[\"micro avg\"][\"f1-score\"]\n            perf_macro[k, m] = clf_report[\"macro avg\"][\"f1-score\"]\n    return perf_micro, perf_macro\n```", "```py\nvalid_labels = np.array(embs_valid[\"label_ids\"])\nvalid_queries = np.array(embs_valid[\"embedding\"], dtype=np.float32)\nperf_micro, perf_macro = find_best_k_m(embs_train, valid_queries, valid_labels)\n```", "```py\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\nax0.imshow(perf_micro)\nax1.imshow(perf_macro)\n\nax0.set_title(\"micro scores\")\nax0.set_ylabel(\"k\")\nax1.set_title(\"macro scores\")\nfor ax in [ax0, ax1]:\n    ax.set_xlim([0.5, 17 - 0.5])\n    ax.set_ylim([17 - 0.5, 0.5])\n    ax.set_xlabel(\"m\")\nplt.show()\n```", "```py\nk, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\nprint(f\"Best k: {k}, best m: {m}\")\n```", "```py\nBest k: 15, best m: 5\n```", "```py\nembs_train.drop_index(\"embedding\")\ntest_labels = np.array(embs_test[\"label_ids\"])\ntest_queries = np.array(embs_test[\"embedding\"], dtype=np.float32)\n\nfor train_slice in train_slices:\n    # Create a Faiss index from training slice\n    embs_train_tmp = embs_train.select(train_slice)\n    embs_train_tmp.add_faiss_index(\"embedding\")\n    # Get best k, m values with validation set\n    perf_micro, _ = find_best_k_m(embs_train_tmp, valid_queries, valid_labels)\n    k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\n    # Get predictions on test set\n    _, samples = embs_train_tmp.get_nearest_examples_batch(\"embedding\",\n                                                           test_queries,\n                                                           k=int(k))\n    y_pred = np.array([get_sample_preds(s, m) for s in samples])\n    # Evaluate predictions\n    clf_report = classification_report(test_labels, y_pred,\n        target_names=mlb.classes_, zero_division=0, output_dict=True,)\n    macro_scores[\"Embedding\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n    micro_scores[\"Embedding\"].append(clf_report[\"micro avg\"][\"f1-score\"])\n```", "```py\nplot_metrics(micro_scores, macro_scores, train_samples, \"Embedding\")\n```", "```py\nimport torch\nfrom transformers import (AutoTokenizer, AutoConfig,\n                          AutoModelForSequenceClassification)\n\nmodel_ckpt = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], truncation=True, max_length=128)\nds_enc = ds.map(tokenize, batched=True)\nds_enc = ds_enc.remove_columns(['labels', 'text'])\n```", "```py\nds_enc.set_format(\"torch\")\nds_enc = ds_enc.map(lambda x: {\"label_ids_f\": x[\"label_ids\"].to(torch.float)},\n                    remove_columns=[\"label_ids\"])\nds_enc = ds_enc.rename_column(\"label_ids_f\", \"label_ids\")\n```", "```py\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args_fine_tune = TrainingArguments(\n    output_dir=\"./results\", num_train_epochs=20, learning_rate=3e-5,\n    lr_scheduler_type='constant', per_device_train_batch_size=4,\n    per_device_eval_batch_size=32, weight_decay=0.0,\n    evaluation_strategy=\"epoch\", save_strategy=\"epoch\",logging_strategy=\"epoch\",\n    load_best_model_at_end=True, metric_for_best_model='micro f1',\n    save_total_limit=1, log_level='error')\n```", "```py\nfrom scipy.special import expit as sigmoid\n\ndef compute_metrics(pred):\n    y_true = pred.label_ids\n    y_pred = sigmoid(pred.predictions)\n    y_pred = (y_pred>0.5).astype(float)\n\n    clf_dict = classification_report(y_true, y_pred, target_names=all_labels,\n                                     zero_division=0, output_dict=True)\n    return {\"micro f1\": clf_dict[\"micro avg\"][\"f1-score\"],\n            \"macro f1\": clf_dict[\"macro avg\"][\"f1-score\"]}\n```", "```py\nconfig = AutoConfig.from_pretrained(model_ckpt)\nconfig.num_labels = len(all_labels)\nconfig.problem_type = \"multi_label_classification\"\n```", "```py\nfor train_slice in train_slices:\n    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,\n                                                               config=config)\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer,\n        args=training_args_fine_tune,\n        compute_metrics=compute_metrics,\n        train_dataset=ds_enc[\"train\"].select(train_slice),\n        eval_dataset=ds_enc[\"valid\"],)\n\n    trainer.train()\n    pred = trainer.predict(ds_enc[\"test\"])\n    metrics = compute_metrics(pred)\n    macro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"macro f1\"])\n    micro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"micro f1\"])\n```", "```py\nplot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (vanilla)\")\n```", "```py\nprompt = \"\"\"\\\nTranslate English to French:\nthanks =>\n\"\"\"\n```", "```py\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], truncation=True,\n                     max_length=128, return_special_tokens_mask=True)\n\nds_mlm = ds.map(tokenize, batched=True)\nds_mlm = ds_mlm.remove_columns([\"labels\", \"text\", \"label_ids\"])\n```", "```py\nfrom transformers import DataCollatorForLanguageModeling, set_seed\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n                                                mlm_probability=0.15)\n```", "```py\nset_seed(3)\ndata_collator.return_tensors = \"np\"\ninputs = tokenizer(\"Transformers are awesome!\", return_tensors=\"np\")\noutputs = data_collator([{\"input_ids\": inputs[\"input_ids\"][0]}])\n\npd.DataFrame({\n    \"Original tokens\": tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]),\n    \"Masked tokens\": tokenizer.convert_ids_to_tokens(outputs[\"input_ids\"][0]),\n    \"Original input_ids\": original_input_ids,\n    \"Masked input_ids\": masked_input_ids,\n    \"Labels\": outputs[\"labels\"][0]}).T\n```", "```py\ndata_collator.return_tensors = \"pt\"\n```", "```py\nfrom transformers import AutoModelForMaskedLM\n\ntraining_args = TrainingArguments(\n    output_dir = f\"{model_ckpt}-issues-128\", per_device_train_batch_size=32,\n    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\", save_strategy=\"no\",\n    num_train_epochs=16, push_to_hub=True, log_level=\"error\", report_to=\"none\")\n\ntrainer = Trainer(\n        model=AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\"),\n        tokenizer=tokenizer, args=training_args, data_collator=data_collator,\n        train_dataset=ds_mlm[\"unsup\"], eval_dataset=ds_mlm[\"train\"])\n\ntrainer.train()\n```", "```py\ntrainer.push_to_hub(\"Training complete!\")\n```", "```py\ndf_log = pd.DataFrame(trainer.state.log_history)\n\n(df_log.dropna(subset=[\"eval_loss\"]).reset_index()[\"eval_loss\"]\n .plot(label=\"Validation\"))\ndf_log.dropna(subset=[\"loss\"]).reset_index()[\"loss\"].plot(label=\"Train\")\n\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()\n```", "```py\nmodel_ckpt = f'{model_ckpt}-issues-128'\nconfig = AutoConfig.from_pretrained(model_ckpt)\nconfig.num_labels = len(all_labels)\nconfig.problem_type = \"multi_label_classification\"\n\nfor train_slice in train_slices:\n    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,\n                                                               config=config)\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args_fine_tune,\n        compute_metrics=compute_metrics,\n        train_dataset=ds_enc[\"train\"].select(train_slice),\n        eval_dataset=ds_enc[\"valid\"],\n    )\n\n    trainer.train()\n    pred = trainer.predict(ds_enc['test'])\n    metrics = compute_metrics(pred)\n    # DA refers to domain adaptation\n    macro_scores['Fine-tune (DA)'].append(metrics['macro f1'])\n    micro_scores['Fine-tune (DA)'].append(metrics['micro f1'])\n```", "```py\nplot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (DA)\")\n```"]