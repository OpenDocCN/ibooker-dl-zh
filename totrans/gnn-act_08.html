<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">6</span></span> <span class="chapter-title-text">Dynamic graphs: Spatiotemporal GNNs</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Introducing memory into your deep learning models</li>
<li class="readable-text" id="p3">Understanding the different ways to model temporal relations using graph neural networks </li>
<li class="readable-text" id="p4">Implementing dynamic graph neural networks</li>
<li class="readable-text" id="p5">Evaluating your temporal graph neural network models </li>
</ul>
</div>
<div class="readable-text" id="p6">
<p>So far, all of our models and data have been single snapshots in time. In practice, the world is dynamic and in constant flux. Objects can move physically, following a trajectory in front of our eyes, and we’re able to predict their future positions based on these observed trajectories. Traffic flow, weather patterns, and the spread of diseases across networks of people are all examples where more information can be gained when modeled with spatiotemporal graphs instead of static graphs. </p>
</div>
<div class="readable-text intended-text" id="p7">
<p>Models that we build today might quickly lose performance and accuracy as we deploy them in the real world. These are problems intrinsic to any deep learning (and machine learning) model, known as <em>out-of-distribution (OOD) generalization, </em>that is, how well models generalize to entirely unseen data. </p>
</div>
<div class="readable-text intended-text" id="p8">
<p>In this chapter, we consider how to make models that are suitable for dynamic events. While this doesn’t mean they can deal with OOD data, our dynamic models will be able to make predictions about unseen events in the future using the recent past. </p>
</div>
<div class="readable-text intended-text" id="p9">
<p>To build our dynamic graph-based learning model, we’ll consider the problem of pose estimation. <em>Pose estimation</em> relates to those classes of problems that predict how bodies (human, animal, or robotic) move over time. In this chapter, we’ll consider a body walking and build several models that learn how to predict the next step from a series of video frames. To do this, we’ll first explain the problem in more detail and how to understand this as a relational problem before jumping in to see how graph-based learning approaches this problem. As with the rest of our book, further technical details are left to section 6.5 at the end of the chapter.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>We’ll use much of the material that we’ve already covered in the book. If you’ve skipped ahead to this chapter, make sure you have a good understanding of the concepts described in the “Building on what you’ve learned” sidebar. </p>
</div>
<div class="readable-text print-book-callout" id="p11">
<p><span class="print-book-callout-head">Note</span>  Code from this chapter can be found in notebook form at the GitHub repository (<a href="https://mng.bz/4a8D">https://mng.bz/4a8D</a>). </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p12">
<h5 class="callout-container-h5 readable-text-h5">Building on what you’ve learned</h5>
</div>
<div class="readable-text" id="p13">
<p>To introduce temporal updates into our GNN, we can build on some of the concepts that we’ve learned in previous chapters. As a quick refresher, we’ve summarized some of the main important features from each chapter: </p>
</div>
<ul>
<li class="readable-text" id="p14"> <em>Message passing</em>—In chapter 2, you learned that the main method used by GNNs to learn from relational data is by combining message passing with artificial neural networks. Each layer of a GNN can be understood as one step of message passing. </li>
<li class="readable-text" id="p15"> <em>Graph convolutional networks (GCNs)</em>—In chapter 3, you saw that message passing itself can be understood as the relational form of the convolution operator (as in convolutional neural networks [CNNs]), and this is the central idea behind GCNs. Messages can also be averaged across neighborhoods by only sampling a subset of nearest neighbors. This is used for GraphSAGE and can considerably reduce the total compute needed.  </li>
<li class="readable-text" id="p16"> <em>Attention</em>—In chapter 4, we showed how the aggregation function for message passing doesn’t need to be restricted to only summing, averaging, or max operations (though the operation must be permutation invariant). Attention allows for a weighting to be learned during training to give more flexible message-passing aggregation functions. Using a graph attention network (GAT) is the basic form of adding attention to message passing.  </li>
<li class="readable-text" id="p17"> <em>Generative models</em>—While discriminative models seek to learn separations between data classes, generative models attempt to learn the underlying data-generating process. The autoencoder is one of the most popular frameworks for designing generative models, where data is passed through a neural network bottleneck to create a low-dimensional representation of the data, also called the latent space. These are commonly implemented as graph autoencoders (GAEs) or variational graph autoencoders (VGAEs) for graphs, as we discussed in chapter 5. </li>
</ul>
</div>
<div class="readable-text" id="p19">
<h2 class="readable-text-h2"><span class="num-string">6.1</span> Temporal models: Relations through time</h2>
</div>
<div class="readable-text" id="p20">
<p>Almost every data problem will, in some way, also be a dynamic problem. In many cases, we can ignore changes in time and build models that are suitable for snapshots of the data that we’ve collected. For example, image segmentation methods rarely consider video footage to train models.</p>
</div>
<div class="readable-text intended-text" id="p21">
<p>In chapter 3, we used a GCN to predict suitable products to recommend to customers using data on a customer-purchaser network. We used a toy dataset that had been collected over a period of several years. However, in reality, we’ll often have constant streams of data and want to make up-to-date predictions that account for both customer and cultural habit changes. Similarly, when we applied a GAT to a fraud-detection problem, the data we used was a single snapshot of financial records that was collected over a period of several years. However, we didn’t account for how financial behaviors changed over time in our model. Again, we would likely want to use this information to predict where an individual’s spending behavior abruptly changes to help us detect fraudulent activity. </p>
</div>
<div class="readable-text intended-text" id="p22">
<p>These are just a few of the many different dynamic problems that we’re faced with every day (see figure 6.1). GNNs are unique in that they can model both dynamic and relational changes. This is very important as many of the networks that operate around us are also moving in time. Take, for example, a social network. Our friendships change, mature, and sadly (or fortunately!) weaken over time. We might become stronger friends with work colleagues or friends of friends and see friends from our hometown less frequently. Making predictions for social networks need to account for this. </p>
</div>
<div class="readable-text intended-text" id="p23">
<p>As another example, we often make predictions about which way to go and when we might arrive based on our knowledge of the roads, traffic patterns, and how much of a rush we’re in. A dynamic GNN can also be used to help make use of this data, by treating the road network as a graph and making temporal predictions on how this network will change. Finally, we can consider predicting how two or more objects move together, that is, by estimating their future trajectories. While this might seem less useful than making friends or getting to work on time, predicting trajectories of interacting bodies, such as molecules, cells, objects, or even stars, is vital to many sciences as well as for robotic planning. Again, dynamic GNNs can help us both predict these trajectories and infer new equations or rules that explain them. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p24">
<img alt="figure" height="649" src="../Images/6-1.png" width="974"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.1</span> Examples of different dynamic problems</h5>
</div>
<div class="readable-text intended-text" id="p25">
<p>These examples are just the tip of the iceberg for applications where we need to model temporal changes. In fact, we’re sure that you can think of many others. Given the importance of knowing how to combine relational learning with temporal learning, we’ll cover three different methods for building dynamic models, two of which use GNNs: a recurrent neural network (RNN) model, a GAT model, and a neural relational inference (NRI) model. We’ll build machine learning models that “learn to walk” by estimating how a human pose changes over time. These models are often deployed in, for example, medical consultations, remote home security services, and filmmaking. The models are also a great toy problem for us to learn to walk before we can run. In that spirit, let’s first learn more about the data and build our first benchmark model.</p>
</div>
<div class="readable-text" id="p26">
<h2 class="readable-text-h2"><span class="num-string">6.2</span> Problem definition: Pose estimation</h2>
</div>
<div class="readable-text" id="p27">
<p>In this chapter, we’ll solve a “dynamic relational” problem with one set of data: preprocessed segmentation of a body walking. This is a useful dataset to explore these techniques, as a moving body is a textbook example of an interacting system: our foot moves because our knee moves because a leg moves, and our arms and torso will all move too. This means that there is a temporal component to our problem.</p>
</div>
<div class="readable-text intended-text" id="p28">
<p>In a nutshell, our pose estimation problem is about path prediction. More precisely, we want to know where, for example, a foot will move having followed the rest of the body for some number of previous timesteps. This type of object tracking is something that we do every day, for example, when we play sports, catch something that’s falling, or watch a television show. We learn this skill as a child and often take it for granted. However, as you’ll see, teaching a machine to perform this object tracking was a significant challenge up until the emergence of spatiotemporal GNNs.</p>
</div>
<div class="readable-text intended-text" id="p29">
<p>The skills that we’ll use for path prediction are important for many other tasks. Predicting events in the future is useful when we want to predict the next purchase of a customer or understand how weather patterns will change based on geospatial data. </p>
</div>
<div class="readable-text intended-text" id="p30">
<p>We’ll be using the Carnegie Mellon University (CMU) Motion Capture Database (<a href="http://mocap.cs.cmu.edu/">http://mocap.cs.cmu.edu/</a>), which contains many examples of different dynamic poses, including walking, running, jumping, and performing sports moves, as well as multiple people interacting [1]. Throughout this chapter, we’ll use the same dataset of subject #35 walking. At each timestep, the subject has 41 sensors that each follow a single joint, ranging from the toes up to the neck. An example of the data from this database is shown in figure 6.2. These sensors track the movement of part of the body across snapshots of their motion. In this chapter, we won’t follow the entire motion and consider only a small subset of the motion. We’ll use the first 49 frames for our training and validation datasets and 99 frames for our test set. In total, there are 31 different examples of this subject walking. We’ll discuss more about the structure of our data in the next section.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p31">
<img alt="figure" height="274" src="../Images/6-2.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.2</span> Snapshots in time (t = time in seconds) of a human subject walking. The dots represent sensors placed on key joints on the human’s body. These snapshots are across 30 seconds. To represent these figures as a graph, the sensor placements (joints) can be represented as nodes, and the body’s connections between the joints are the edges.</h5>
</div>
<div class="readable-text" id="p32">
<h3 class="readable-text-h3"><span class="num-string">6.2.1</span> Setting up the problem</h3>
</div>
<div class="readable-text" id="p33">
<p>Our aim is to predict the dynamics for all the individual joints. Clearly, we can construct this as a graph because all the joints are connected through edges, as shown previously in figure 6.2. Therefore, it makes sense to use GNNs to solve this problem. However, we’ll first compare another approach, which doesn’t account for the graph data, to benchmark our GNN models.</p>
</div>
<div class="readable-text" id="p34">
<h4 class="readable-text-h4">Downloading the data</h4>
</div>
<div class="readable-text" id="p35">
<p>We’ve included the steps to download and preprocess the data in our code repository. The data is contained within a zip file where each of the different trials is saved as an advanced systems format (.asf) file. These .asf files are basically just text files that contain the label for each sensor and their xyz coordinates at each timestep. In the following listing, we show a snippet of the text.</p>
</div>
<div class="browsable-container listing-container" id="p36">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.1</span> Example of the sensor data text files </h5>
<div class="code-area-container">
<pre class="code-area">   1
   root 4.40047 17.8934 -21.0986 -0.943965 -8.37963 -7.42612
   lowerback 11.505 1.60479 4.40928
   upperback 0.47251 2.84449 2.26157
   thorax -5.8636 1.30424 -0.569129
   lowerneck -15.9456 -3.55911 -2.36067
   upperneck 19.9076 -4.57025 1.03589</pre>
</div>
</div>
<div class="readable-text" id="p37">
<p>Here, the first number is the frame number, and <code>root</code> is specific to the sensors and can be ignored. <code>lowerback</code>, <code>upperback</code>, <code>thorax</code>, <code>lowerneck</code>, and <code>upperneck</code> denote the positions of the sensors. In total, there are 31 sensors mapping the movement of a man walking. To convert this sensor data into trajectories, we need to calculate the change in position for each sensor. This becomes quite a complicated task, as we need to account for both translational movements and angular rotations for the various sensors between each frame. Here, we’ll use the same data files as in the NRI paper [2]. We can use these to map out the trajectories of each individual sensor in x, y, and z, or look at how the sensors are moving in two dimensions to get intuition about how the entire body is moving. Examples of this are shown in figure 6.3, where we focus on the movement of a foot sensor in x, y, and z, as well as the overall movement of the body over time (with the sensor shown as solid black stars). <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p38">
<img alt="figure" height="457" src="../Images/6-3.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.3</span> Preconstructed spatial trajectories of sensors</h5>
</div>
<div class="readable-text" id="p39">
<p>Along with the spatial data, we can also calculate the velocity data. This data is provided as separate files for each of the movie frames. An example of the change in velocity data is shown in figure 6.4. As you can see, the velocity data varies around a smaller range. Both spatial and velocity data will be used as the features in our machine learning problem. Here, we now have six features across 50 frames for each of our 31 sensors and across 33 different trials. We can understand this as a multivariate time series problem. We’re trying to predict the future evolution of a six-dimensional (three spatial and three velocity) object (each sensor). Our first approach will treat these as independent, looking to predict future positions and velocity based on past sensor data. We’ll then switch to treating this as a graph, where we can couple all sensors together. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p40">
<img alt="figure" height="438" src="../Images/6-4.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.4</span> Preconstructed velocity data of sensors</h5>
</div>
<div class="readable-text" id="p41">
<p>Currently, this is a relational problem, but we’re only considering the node data and not the edge data. Where there is node data and no edge data, we have to be careful not to make too many assumptions. For example, if we chose to connect nodes based on their distance from one another, then we might end up with a very strange-looking skeleton, as shown in figure 6.5. Luckily, we have the edge data as well, which has been built using the CMU dataset and is included in the data provided. This serves as a cautionary tale that GNNs are only as powerful as the graphs they’re trained on and that we must take care to ensure that the graph structure is correct. However, if edge data is entirely lacking, then we can attempt to infer the edge data from the node data itself. While we won’t be doing this here, note that the NRI model we’ll be using has this capability. </p>
</div>
<div class="browsable-container figure-container" id="p43">
<img alt="figure" height="1083" src="../Images/6-5.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.5</span> Sensor networks showing the error of wrongly inferring graph structures. The nodes are human skeletal connections. The left figure shows a network with edges inferred from node proximity (closest nodes connected to one another). This figure does not reflect a real human skeleton. The true set of edges is shown in the right figure.</h5>
</div>
<div class="readable-text intended-text" id="p42">
<p>We now have all of our data loaded. In total, we have three datasets (training, validation, testing) that each contain 31 individual sensor positions. Each of these sensors contain six features (spatial coordinates) and are connected by an adjacency matrix that is constant in time. The sensor graph is undirected, and the edges are unweighted. The training and validation sets contain 49 frames, and the test sets contain 99 frames. <span class="aframe-location"/></p>
</div>
<div class="readable-text" id="p44">
<h3 class="readable-text-h3"><span class="num-string">6.2.2</span> Building models with memory</h3>
</div>
<div class="readable-text" id="p45">
<p>Now that our problem is defined and our data is loaded, let’s consider how we might approach the problem of predicting the joint dynamics. First, we need to think about what the underlying aim is. At its core, we’ll be involved in sequence prediction, just like autocomplete on a phone or search tool. These types of problems are often approached using networks, such as transformers, for which we use an attention mechanism as in chapter 4. However, before attention-based networks, many deep learning practitioners instead approached sequence prediction tasks by introducing memory into their models [3]. This makes intuitive sense: if we want to predict the future, we need to remember the past. </p>
</div>
<div class="readable-text intended-text" id="p46">
<p>Let’s build a simple model that predicts the next location for all the individual sensors using past events. Essentially, this means we’ll build a model that predicts the position of nodes without edge data. An example of what we’ll be attempting is shown in figure 6.6. Here, we’ll start by preprocessing and preparing our data to be passed to a model that can predict how the data evolves over time. This allows us to predict the changes in the pose given a few input frames. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p47">
<img alt="figure" height="472" src="../Images/6-6.png" width="894"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.6</span> Predicting future positions using only sensor data</h5>
</div>
<div class="readable-text" id="p48">
<p>To introduce memory to our neural networks, we’ll start by considering a recurrent neural network (RNN). Similar to convolutional and attention neural networks, RNNs are a broad class of architectures that are fundamental tools for researchers and practitioners alike. For more information about RNNs, see, for example <em>Machine Learning with TensorFlow</em> (Manning, 2020, <a href="https://mng.bz/VVOW">https://mng.bz/VVOW</a>). RNNs can be considered as multiple individual networks that link together. These repeating subnetworks allow for past information to be “remembered” and the effect from past data to affect future predictions. After initializing, each subnetwork takes in input data as well as the output of the last subnetwork, and these are used to make new predictions. In other words, each subnetwork takes input and information from the recent past to build inferences about the data. However, a vanilla RNN will only ever remember the preceding step. They have <em>very</em> short-term memory. To improve the effect of the past on the future, we need something stronger. </p>
</div>
<div class="readable-text intended-text" id="p49">
<p>Long short-term memory (LSTM) networks are another extremely popular neural network architecture for modeling and predicting temporal or sequential information. These networks are special cases of RNN that similarly link multiple subnetworks together. The difference is that LSTMs introduce more complex dependencies in the subnetwork structure. LSTMs are particularly useful for sequential data as they resolve the problem of vanishing gradients that is observed for RNNs. Put simply, <em>vanishing gradients</em> refers to where the gradient that we use to train our neural network using gradient descent approaches zero. This is especially likely to happen when we train an RNN that has many layers. (We won’t go into the reasons for this here, but if you’re interested, read <em>Deep Learning with Python </em>(Manning, 2024, <a href="https://mng.bz/xKag">https://mng.bz/xKag</a>) for more information. </p>
</div>
<div class="readable-text intended-text" id="p50">
<p>Gated recurrent unit networks (GRUs) also resolve the problem of vanishing gradients by allowing new information to be added to the memory store about the recent past. This is achieved through a gating structure, where gates within the model architecture help to control the flow of information. These gates also add a new design element to how we can build and adapt our neural networks. We won’t consider LSTM here as it’s outside the scope of the book, but again we recommend that you check out <em>Deep Learning with Python </em>(Manning, 2024, <a href="https://mng.bz/xKag">https://mng.bz/xKag</a>) for more information. </p>
</div>
<div class="readable-text" id="p51">
<h4 class="readable-text-h4">Constructing a recurrent neural network</h4>
</div>
<div class="readable-text" id="p52">
<p>Let’s now look at how to use an RNN to predict the trajectories of the body sensors over time, which will act as one of our baselines for future performance gains. We won’t go into the details of RNNs and GRU architectures but additional information is provided at the end of the chapter in section 6.5. </p>
</div>
<div class="readable-text intended-text" id="p53">
<p>The idea for this model is that our RNN will predict the future positions for sensors without taking into account relational data. When we start to introduce our graph models, we’ll see how this can be improved.</p>
</div>
<div class="readable-text intended-text" id="p54">
<p>We’ll use the same standard training loop for deep learning, as shown in figure 6.7. Once we define our model and define a training and test loop, we use these to train and then test the model. As always, we’ll keep the training and testing data completely separate and include a validation set of data to make sure our model isn’t over-fitting during training. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p55">
<img alt="figure" height="113" src="../Images/6-7.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.7</span> Standard process for training a deep learning model that we’ll follow throughout this chapter</h5>
</div>
<div class="readable-text" id="p56">
<p>The training loop used here is fairly standard, so we’ll describe it first. In the training loop definition shown in listing 6.2, we follow the same convention as in previous chapters, looping through model prediction and loss updates over a fixed number of epochs. Here, our loss will be contained in our criterion function, which we define as a simple mean standard error (MSE) loss. We will use a learning rate scheduler, which will reduce the learning rate parameter after our validation loss starts to plateau. We initialize the best loss as infinity and lower the learning rate after the validation loss is less than our best loss for <code>N</code> steps. </p>
</div>
<div class="browsable-container listing-container" id="p57">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.2</span> Training loop</h5>
<div class="code-area-container">
<pre class="code-area">   num_epochs = 200  
   train_losses = []
   valid_losses = []

   pbar = tqdm(range(num_epochs))

   for epoch in pbar:

        train_loss = 0.0 <span class="aframe-location"/> #1
        valid_loss = 0.0  #1

        modelRNN.train() <span class="aframe-location"/> #2
        for i, (inputs, labels) in enumerate(trainloader):
             inputs = inputs.to(device)
             labels = labels.to(device)

        optimizer.zero_grad() <span class="aframe-location"/> #3

        outputs = modelRNN(inputs) <span class="aframe-location"/> #4
        loss = criterion(outputs, labels)  #4
        loss.backward()  #4
        optimizer.step()  #4

        train_loss += loss.item() * inputs.size(0) <span class="aframe-location"/> #5

        modelRNN.eval()  <span class="aframe-location"/> #6
        with torch.no_grad():
             for i, (inputs, labels) in enumerate(validloader):
                  inputs = inputs.to(device)
                  labels = labels.to(device)

                  outputs = modelRNN(inputs) 
                  loss = criterion(outputs, labels)
                  valid_loss += loss.item() * inputs.size(0)

         if valid_loss &lt; best_loss: <span class="aframe-location"/> #7
              best_loss = valid_loss
              counter = 0
         else:
              counter += 1

         scheduler.step(best_loss) <span class="aframe-location"/> #8

         if counter == early_stop:
         print(f"\n\nEarly stopping \
initiated, no change \
after {early_stop} steps")
         break

        train_loss = train_loss/len(trainloader.dataset) <span class="aframe-location"/> #9
        valid_loss = valid_loss/len(validloader.dataset)  #9

        train_losses.append(train_loss)  #9
        valid_losses.append(valid_loss)  #9</pre>
<div class="code-annotations-overlay-container">
     #1 Initializes loss and accuracy variables
     <br/>#2 Begins the training loop
     <br/>#3 Zeros the parameter gradients
     <br/>#4 Forward + backward + optimize
     <br/>#5 Updates training loss, multiplying by the number of samples in the current mini-batch
     <br/>#6 Begins the validation loop
     <br/>#7 Checks for early stopping
     <br/>#8 Steps the scheduler
     <br/>#9 Calculates and stores losses
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p58">
<p>Both layers are trained (using our training loop in listing 6.3) for a specific task. For both the RNN and the GRU, the format for the data will be the individual trials or videos, the frame timestamp, the number of sensors, and the features of the sensors. By providing the data broken up into individual snapshots of time, the model is able to use the temporal aspects to learn from. Here, we use the RNN to predict the future position for each individual sensor, given the 40 previous frames. For all of our calculations, we’ll normalize the data based on the node features (position and velocity) using min-max scaling. </p>
</div>
<div class="readable-text intended-text" id="p59">
<p>After we finish our training loop, we test our network. As always, we don’t want to update the parameters of our network, so we make sure that there is no backpropagated gradient (by selecting <code>torch.no_grad()</code>). Note that we choose a sequence length of 40 so that our testing loop is able to see the first 40 frames and then attempt to infer the final 10 frames. </p>
</div>
<div class="browsable-container listing-container" id="p60">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.3</span> Testing loop</h5>
<div class="code-area-container">
<pre class="code-area">   model.eval()  <span class="aframe-location"/> #1
   predictions = [] 
   test_losses = [] 
   seq_len = 40 

   with torch.no_grad():
        for i, (inputs, targets) in enumerate(testloader):
             inputs = inputs.to(device)
             targets = targets.to(device)

             preds = []
             for _ in range(seq_len):
                  output = model(inputs)
                  preds.append(output)

             inputs = torch.cat([inputs[:, 1:]\
, output.unsqueeze(1)], dim=1) <span class="aframe-location"/>\ #2

        preds = torch.cat(preds, dim=1) <span class="aframe-location"/> #3
           loss = criterion(preds, targets)  #3
           test_losses.append(loss.item())  #3

           predictions.append(preds.detach().cpu().numpy())

    predictions = np.concatenate(predictions, axis=0) <span class="aframe-location"/> #4
   test_loss = np.mean(test_losses) <span class="aframe-location"/> #5</pre>
<div class="code-annotations-overlay-container">
     #1 Sets the model to evaluation mode
     <br/>#2 Updates inputs for the next prediction
     <br/>#3 Computes the loss for this sequence
     <br/>#4 Converts predictions to a NumPy array for easier manipulation
     <br/>#5 Computes the average test loss
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p61">
<p>Once our models are defined, we can next use the training loop given in listing 6.3 to train our model. At this point, you might be wondering how we’ll amend the training loop to correctly account for the temporal element when backpropagating. The good news is that this is handled automatically by PyTorch. We find that the RNN model is able to predict the future positions with 70% accuracy for the validation data and 60% accuracy for the test data.</p>
</div>
<div class="readable-text intended-text" id="p62">
<p>We also tried a GRU model to predict the future steps taken and found this model is able to get an accuracy of 75% using the validation data. This is quite low but not as low as it might be given the simplicity of the model and the little amount of information that we’ve passed it. However, when we test the model performance on our test data, we can see that performance falls to 65%. A few example outputs from our model are shown in figure 6.8. Clearly, the model quickly degrades, and the estimated pose position starts to vary widely. For better accuracy, we’ll need to use some of the relational inductive biases in the pose data. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p63">
<img alt="figure" height="241" src="../Images/6-8.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.8</span> Predicting future movements using an RNN. Here, figures on the left represent the true data, and those on the right represent the predicted data.</h5>
</div>
<div class="readable-text" id="p64">
<h2 class="readable-text-h2"><span class="num-string">6.3</span> Dynamic graph neural networks</h2>
</div>
<div class="readable-text" id="p65">
<p>To predict the future evolution of the graph, we need to restructure our data to account for temporal data. Specifically, dynamic GNNs connect different sequential snapshots of the graph’s evolution and learn to predict future evolutions [4–6]. One method for doing so is to combine them into a single graph. This temporal graph now contains both per-timestep data and the temporal connections encoded as nodes with temporal edges. We’ll first approach the task of pose estimation by taking a naive approach to modeling graph evolution. We’ll look at how we can combine our temporal data into one large graph and then predict the future evolution by masking the nodes of interest. We’ll use the same GAT network that you saw in chapter 3. Then, in section 6.4, we’ll show another method for solving the pose estimation problem by instead encoding each snapshot of the graph and predicting the evolution using a combination of variational autoencoders (VAEs) and RNNs, which is the NRI method [2]. </p>
</div>
<div class="readable-text" id="p66">
<h3 class="readable-text-h3"><span class="num-string">6.3.1</span> Graph attention network for dynamic graphs</h3>
</div>
<div class="readable-text" id="p67">
<p>We’ll look at how to convert our pose estimation problem into a graph-based problem. To do this, we need to construct an adjacency matrix that accounts for temporal information. First, we need to load our data in as a PyTorch Geometric (PyG) data object. We’ll use the same location and velocity data that we used to train our RNN. The difference here is that we’ll construct a single graph that contains all the data. The code snippet in listing 6.4 shows how we initialize our dataset. We pass the paths for where the location and velocity data are as well as where the edge data is located. We also pass whether we need to transform our data and the mask and window size that we’ll predict over. </p>
</div>
<div class="browsable-container listing-container" id="p68">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.4</span> Loading the data as a graph</h5>
<div class="code-area-container">
<pre class="code-area">   class PoseDataset(Dataset):
        def __init__(self, loc_path, 
                          vel_path, 
                          edge_path, 
                          mask_path, 
                          mask_size, 
                          transform=True):

       self.locations = np.load(loc_path) <span class="aframe-location"/> #1
        self.velocities = np.load(vel_path)  #1
        self.edges = np.load(edge_path)

        self.transform=transform
        self.mask_size = mask_size <span class="aframe-location"/> #2
        self.window_size = self.locations\
.shape[1] - self.mask_size <span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 Loads the data from .npy files
     <br/>#2 Determines the mask size
     <br/>#3 Determines the window size
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p69">
<p>For all our dataset objects, we need a <code>get</code> method inside the class to describe how to retrieve this data, which is shown in listing 6.5. This method combines the location and velocity data into node features. We also provide an option to transform the data using a <code>normalize_array</code> function. </p>
</div>
<div class="browsable-container listing-container" id="p70">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.5</span> Set up node features using location and velocity data</h5>
<div class="code-area-container">
<pre class="code-area">   def __getitem__(self, idx):
        nodes = np.concatenate((self.locations[idx], 
   self.velocities[idx]), axis=2) <span class="aframe-location"/> #1
        nodes = nodes.reshape(-1, nodes.shape[-1]) <span class="aframe-location"/> #2

        if self.transform: <span class="aframe-location"/> #3
             nodes, node_min, node_max\
    = normalize_array(nodes) 

        total_timesteps = self.window_size + self.mask_size <span class="aframe-location"/> #4
        edge_index = np.repeat(self.\
edges[None, :], total_timesteps, axis=0) 

         N_dims = self.locations.shape[2]
        shift = np.arange(total_\
   timesteps)[:, None, None]*N_dims <span class="aframe-location"/> #5
         edge_index += shift
         edge_index = edge_index.reshape(2, -1)  <span class="aframe-location"/> #6

         x = torch.tensor(nodes, dtype=torch.float) <span class="aframe-location"/> #7
         edge_index = torch.tensor\
(edge_index, dtype=torch.long) 
          mask_indices = np.arange(        <span class="aframe-location"/> #8
               self.window_size * self.\
locations.shape[2],                        
               total_timesteps * \
self.locations.shape[2]                    
                    )                      
           mask_indices = torch.tensor(mask_indices, dtype=torch.long)

           if self.transform:
                  trnsfm_data = [node_min, node_max]
                  return Data(x=x, 
                       edge_index=edge_index, 
                       mask_indices=mask_indices,  
                       trnsfm=trnsfm_data
                        )
            return Data(x=x, edge_index=\
edge_index, mask_indices=mask_indices)</pre>
<div class="code-annotations-overlay-container">
     #1 Concatenates location and velocity data for each node
     <br/>#2 Determines the mask size
     <br/>#3 Applies normalization if transform is True
     <br/>#4 Repeats the edges for the total number of timesteps (past + future)
     <br/>#5 Applies the shift to the edge indices
     <br/>#6 Flattens the edge indices into two dimensions
     <br/>#7 Converts everything to PyTorch tensors
     <br/>#8 Calculates the indices of the masked nodes
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p71">
<p>We next want to combine all nodes across the different timesteps into one large graph containing all individual frames. This gives an adjacency matrix that covers all different timesteps. (For further details on the idea of temporal adjacency matrices, see section 6.5 at the end of this chapter.) To do this for our pose estimation data, we first construct the adjacency matrix for each timestep, as shown in listing 6.6 and included in listing 6.5. </p>
</div>
<div class="readable-text intended-text" id="p72">
<p>As shown in figure 6.9, the process begins by representing the graph data across multiple timesteps, where each timestep is treated as a distinct layer (Step 1). All nodes have node feature data (not shown in the figure). For our application, the node feature data consists of location and velocity information.</p>
</div>
<div class="readable-text intended-text" id="p73">
<p>Nodes within a timestep are connected to each other using intra-timestep edges, that is, connections between nodes on the same timestep layer (Step 2). These edges ensure that each graph at a specific timestep is internally consistent. The nodes are not yet connected across timesteps.</p>
</div>
<div class="readable-text intended-text" id="p74">
<p>To incorporate temporal relationships, inter-timestep edges (i.e., connections between nodes on different timestep layers) are added to connect corresponding nodes across adjacent timesteps (Step 3). These edges allow information to flow between nodes in different timesteps, enabling temporal modeling of the graph data.</p>
</div>
<div class="readable-text intended-text" id="p75">
<p>In preparation for predicting future values, the nodes in the last timestep are masked to represent unknown data (Step 4). These masked nodes are treated as the target of the prediction task. Their values are unknown, but they can be inferred by leveraging the features and relationships of the unmasked nodes in earlier timesteps.</p>
</div>
<div class="readable-text intended-text" id="p76">
<p>The inference process (Step 5) involves using the known features of unmasked nodes from previous timesteps (t = 0 and t = 1) to predict the features of the masked nodes in t = 2. Dotted arrows illustrate how information flows from unmasked nodes to masked nodes, showing the dependency of the predictions on earlier graph data. This transforms the task into a node prediction problem, where the goal is to estimate the features of the masked nodes based on the relationships and features of the unmasked nodes.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p77">
<img alt="figure" height="1426" src="../Images/6-9.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.9</span> Illustration of the spatiotemporal graph construction and inference process. Step 1 shows the sequence of graphs across timesteps with nodes representing entities at each timestep. Step 2 highlights intra-timestep edges (solid lines) connecting nodes within the same graph layer. Step 3 introduces inter-timestep edges (dotted lines) that encode temporal dependencies by linking corresponding nodes across adjacent timesteps. In Step 4, nodes at the final timestep are masked (gray) to represent unknown values for prediction. Step 5 demonstrates the inference process (dashed arrows), where information from unmasked nodes in earlier timesteps is used to estimate the features of masked nodes. The legend clarifies the types of nodes and edges used in the graph representation.</h5>
</div>
<div class="browsable-container listing-container" id="p78">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.6</span> Constructing the adjacency matrix</h5>
<div class="code-area-container">
<pre class="code-area">       total_timesteps = self.\
window_size + self.mask_size <span class="aframe-location"/> #1
       edge_index = np.repeat(self.edges[None, :],\
 total_timesteps, axis=0) 

       shift = np.arange(total_timesteps)[:, None, \
None] * num_nodes_per_timestep <span class="aframe-location"/> #2
       edge_index += shift <span class="aframe-location"/> #3
       edge_index = edge_index.reshape(2, -1) <span class="aframe-location"/> #4</pre>
<div class="code-annotations-overlay-container">
     #1 Repeats the edges for the total number of timesteps (past + future)
     <br/>#2 Creates a shift for each timestep
     <br/>#3 Applies the shift to the edge indices
     <br/>#4 Flattens the edge indices into two dimensions
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p79">
<p>Now that we have the adjacency matrix, the next step is to build a model that can predict future timesteps. Here, we’ll use a GAT model, introduced in chapter 4 [7]. We choose this GNN because it can be more expressive than other GNNs, and we want something that is able to account for the different temporal and spatial information. The model architecture is provided in listing 6.7. </p>
</div>
<div class="browsable-container listing-container" id="p81">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.7</span> Defining the GAT model</h5>
<div class="code-area-container">
<pre class="code-area">  class GAT(torch.nn.Module):
        def __init__(self, n_feat,
                      hidden_size=32,
                      num_layers=3,
                      num_heads=1,
                      dropout=0.2,
                      mask_size=10):
             super(GAT, self).__init__()

             self.num_layers = num_layers
             self.heads = num_heads
             self.n_feat = n_feat
             self.hidden_size = hidden_size
             self.gat_layers = torch.nn.ModuleList()
             self.batch_norms = torch.nn.ModuleList()
             self.dropout = nn.Dropout(dropout)
             self.mask_size = mask_size

             gat_layer = GATv2Conv(self.n_feat,\
 self.hidden_size, heads=num_heads) <span class="aframe-location"/> #1
             self.gat_layers.append(gat_layer)  #1
             middle_size = self.hidden_size*num_heads 
             batch_layer = nn.BatchNorm1d\
(num_features=middle_size) <span class="aframe-location"/> #2
             self.batch_norms.append(batch_layer) #2

             for _ in range(num_layers-2): <span class="aframe-location"/> #3
                  gat_layer = GATv2Conv(input_size,\
 self.hidden_size, heads=num_heads) 
                  self.gat_layers.append(gat_layer) 
                  batch_layer = nn.BatchNorm1d(num_features\
=middle_size)                                          <span class="aframe-location"/> #4
                  self.batch_norms.append(batch_layer) 

             gat_layer = GATv2Conv(middle_size, self.n_feat)
             self.gat_layers.append(gat_layer) <span class="aframe-location"/> #5

        def forward(self, data):
             x, edge_index = data.x, data.edge_index
             for i in range(self.num_layers):
                  x = self.gat_layers[i](x, edge_index)
                  if i &lt; self.num_layers - 1: <span class="aframe-location"/> #6
                       x = self.batch_norms[i](x)  #6
                       x = torch.relu(x)  #6
                       x = self.dropout(x)  #6

             n_nodes = edge_index.max().item() + 1 <span class="aframe-location"/> #7
             x = x.view(-1, n_nodes, self.n_feat)
             return x[-self.mask_size:].view(-1, self.n_feat)</pre>
<div class="code-annotations-overlay-container">
     #1 First GAT layer
     <br/>#2 BatchNorm layer for the first GAT layer
     <br/>#3 Intermediate GAT layers
     <br/>#4 BatchNorm layers for intermediate GAT layers
     <br/>#5 Last GAT layer
     <br/>#6 Don’t apply batch normalization and dropout to the output of the last GAT layer.
     <br/>#7 Only outputs the last frame
     <br/>
</div>
</div>
</div>
<div class="readable-text intended-text" id="p80">
<p>This model follows the basic structure outlined in chapter 4. We define the number of layers and heads for our model as well as the relevant input size, which depends on the number of features that we’re predicting. Each of our GAT layers has a hidden size and we include dropout and batch normalization to improve performance. We then loop through the number of layers in our model, ensuring that the dimensions are correct to match our target output. We also define our forward function, which predicts the node features for the masked nodes. By unwrapping each timestep into a larger graph, we start to introduce temporal effects as additional network structures that our model can learn. </p>
</div>
<div class="readable-text" id="p82">
<p>With both model and dataset defined, let’s start training our model and see how it performs. Recall that the RNN and GRU achieved 60% and 65% in test accuracy, respectively. In listing 6.8, we show the training loop for our GAT model. This training loop follows the same structure as that used in previous chapters. We use the MSE as our loss functions and set the learning rate to 0.0005. We calculate the node features of the masked nodes using our GAT and then compare these to the true data, which is stored in <code>data</code>. We first train our model and then compare the model predictions using our validation set. Note that because of the multiple graph sequences we’re now predicting, this training loop takes more time than previous models. On a V100 GPU through Google Colab, this took under an hour to train. </p>
</div>
<div class="browsable-container listing-container" id="p83">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.8</span> GAT training loop</h5>
<div class="code-area-container">
<pre class="code-area">   lr = 0.001
   criterion = torch.nn.MSELoss()                           <span class="aframe-location"/> #1
   optimizer = torch.optim.Adam(model.parameters(), lr=lr)  

   for epoch in tqdm(range(epochs), ncols=300):
        model.train()
        train_loss = 0.0
        for data in train_dataset:
             optimizer.zero_grad()
             out = model(data) <span class="aframe-location"/> #2

        loss = criterion(out, \
data.y.reshape(out.shape[0], -1)) <span class="aframe-location"/> #3
        loss.backward() 
        optimizer.step()
        train_loss += loss.item()

        model.eval() <span class="aframe-location"/> #4
        val_loss = 0.0  #4
        with torch.no_grad():  #4
             for val_data in val_dataset:  #4
               val_out = model(val_data) <span class="aframe-location"/> #5
                  val_loss += criterion(out, \
data.y.reshape(out.shape[0],\
 -1)).item() <span class="aframe-location"/> #6

        val_loss /= len(val_dataset)
        train_loss /= len(train_dataset)</pre>
<div class="code-annotations-overlay-container">
     #1 Initializes loss and optimizer with learning rate
     <br/>#2 Generates the model’s predictions for the input
     <br/>#3 Computes the loss between the outputs and the targets
     <br/>#4 Validation loop
     <br/>#5 Generates the model’s predictions for the input
     <br/>#6 Computes the loss between the outputs and the targets
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p84">
<p>Finally, we test our trained model using the test set and code shown in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p85">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.9</span> GAT test loop</h5>
<div class="code-area-container">
<pre class="code-area">   test_loss = 0
   for test_data in test_dataset:
        test_out = model(test_data) <span class="aframe-location"/> #1
        test_loss += criterion(out,\
 data.y.reshape(out.shape[0], -1)).item() <span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 Generates the model’s predictions for the input
     <br/>#2 Computes the loss between the outputs and the targets
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p86">
<p>We find that this naive approach is unable to predict the poses. Our overall test accuracy is 55%, and the predicted graphs look very different from our expectation of the pose’s appearance. This is due to the large amount of data that we’re now holding in a single graph. We’re compressing both node features and temporal data into one graph, and we’re not emphasizing the temporal property when defining our model. There are ways to improve this, such as by using temporal encodings to extract the edge data that is unused, as in the temporal GAT (TGAT) model. TGAT treats edges as dynamic rather than static, such that each edge also encodes a timestamp. </p>
</div>
<div class="readable-text intended-text" id="p87">
<p>However, without this time data, our model has become too expressive such that the overall structure of the pose has diverged significantly from the original structure, as shown with the predicted poses in figure 6.10. Next, we’ll investigate how to combine the best of both approaches into a GNN that uses RNN-based predictions by learning on each graph snapshot. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p88">
<img alt="figure" height="195" src="../Images/6-10.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.10</span> Output from the GAT model</h5>
</div>
<div class="readable-text" id="p89">
<h2 class="readable-text-h2"><span class="num-string">6.4</span> Neural relational inference</h2>
</div>
<div class="readable-text" id="p90">
<p>Our RNN focused entirely on the temporal data but ignored the underlying relational data. This resulted in a model that was able to move in the right direction on average but didn’t really alter the individual sensor positions very well. On the other hand, our GAT model ignored temporal data by encoding all individual temporal graphs into a single graph and attempting node prediction on the unknown future graphs. The model caused the sensors to move dramatically, and our resulting graphs looked very unlike how we would expect a human to move. </p>
</div>
<div class="readable-text intended-text" id="p91">
<p>Neural relational inference (NRI), as mentioned earlier, is a slightly different approach that uses a more complex encoding framework to combine the best of both RNN and GNNs [2]. The architecture for this model is shown in figure 6.11. Specifically, NRI uses an autoencoder structure to embed the information at each timestep. Therefore, the embedding architecture is applied to the entire graph in a similar way to GAE, which we discussed in chapter 5. This encoded graph data is then updated using an RNN. One key point is that NRI evolves the latent representation of the embeddings.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p92">
<img alt="figure" height="309" src="../Images/6-11.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.11</span> Schematic for NRI (Source: Kipf et al. [2]). The model consists of an encoder and decoder layer and several message-passing steps. However, here the messages are passed in the encoder from node to edge, back from edge to node, and then back from node to edge again. For the decoder, messages are passed from node to edge and then from edge to node. The final step takes the latent representation and is used to predict the next step in the temporal evolution of the body.</h5>
</div>
<div class="readable-text" id="p93">
<p>Let’s explore how this model applies to our problem of pose estimation so that we can best understand the different components in the model. We’ll use the same format of masking some data during training and then using the test day to identify these masked nodes. Recall that this is equivalent to inferring the future frames in our video. However, we now need to change both the model architecture and the loss. We need to change the model architecture to account for the new autoencoder structure, and we need to adjust the loss to include minimizing the reconstruction loss as well as the Kullbeck-Liebler divergence (KL divergence). For more information on the NRI model and relevant changes, see section 6.5 at the end of the chapter. </p>
</div>
<div class="readable-text intended-text" id="p94">
<p>The code for the base class of an NRI model is provided in listing 6.10. As is clear in the code, we need to define an encoder and decoder when calling this class. Along with the encoder and decoder, there are some other model-specific details we need to be aware of. First, we need to define the number of variables. This relates to the number nodes in our graph rather than the number of features for each node. In our case, this will be 31, corresponding to each of the different sensors tracking a joint position. We also need to define the different types of edges between the nodes. This will be either 1 or 0, representing whether an edge exists. </p>
</div>
<div class="readable-text intended-text" id="p95">
<p>We’ll assume that the way the nodes, or sensors, connect doesn’t change, that is, that the graph structure is static. Note that this model also allows for dynamic graphs where the connectivity changes over time, for example, when different players move around a basketball court. The total number of players is fixed but the number of players that can be passed to changes. In fact, this model was also used to predict how different players would pass using footage from the NBA. </p>
</div>
<div class="readable-text intended-text" id="p96">
<p>Finally, this model needs some hyperparameters to be set, including the Gumbel temperature and the prior variance. <em>Gumbel temperature</em> controls the tradeoff between exploration and exploitation when performing discrete sampling. Here, we need to use a discrete probability distribution to predict the edge type. We discuss this in more detail in section 6.5. <em>Prior variance</em> reflects how uncertain we are on the connectivity of the graph before we start. We need to set this because the model assumes we <em>don’t</em> know the connectivity. In fact, the model learns the connectivity that best helps it to improve its predictions. This is exactly what we’re setting when we call the <code>_initialize_log_prior</code> function. We’re telling the model what our best guess is for a likely connectivity pattern. For example, if we were to apply this model to a sports team, we might use a Gaussian distribution with a high mean for edges between players that frequently pass to each other or even to players on the same team. </p>
</div>
<div class="readable-text intended-text" id="p97">
<p>To demonstrate our model, we’re instead going to assume a uniform prior, which means that all edges are as likely as all others, or in everyday terms “we don’t know.” The prior variance sets our uncertainty bound for each of the edges. In the following listing, we set it to be 5 × 10<sup>–5</sup> for numerical stability, but given that our prior is uniform, it shouldn’t have much effect.</p>
</div>
<div class="browsable-container listing-container" id="p98">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.10</span> Base class for the NRI model</h5>
<div class="code-area-container">
<pre class="code-area">   class BaseNRI(nn.Module):
        def __init__(self, num_vars, encoder, decoder,
                num_edge_types=2,
                gumbel_temp=0.5, 
                prior_variance=5e-5):
           super(BaseNRI, self).__init__()
           self.num_vars = num_vars <span class="aframe-location"/> #1
           self.encoder = encoder <span class="aframe-location"/> #2
           self.decoder = decoder <span class="aframe-location"/> #3
           self.num_edge_types = num_edge_types 
           self.gumbel_temp = gumbel_temp <span class="aframe-location"/> #4
           self.prior_variance = prior_variance <span class="aframe-location"/> #5

           self.log_prior = self._initialize_log_prior()

        def _initialize_log_prior(self): 
             prior = torch.zeros(self.num_edge_types)
             prior.fill_(1.0 / self.num_edge_types) <span class="aframe-location"/> #6
             log_prior = torch.log(prior)\
   .unsqueeze(0).unsqueeze(0) <span class="aframe-location"/> #7
             return log_prior.cuda(non_blocking=True)</pre>
<div class="code-annotations-overlay-container">
     #1 Number of variables in the mode
     <br/>#2 Encoder neural network
     <br/>#3 Decoder neural network
     <br/>#4 Gumbel temperature for sampling categorical variables
     <br/>#5 Prior variance
     <br/>#6 Fills the prior tensor with uniform probabilities
     <br/>#7 Takes the log and adds two singleton dimensions
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p99">
<p>As we discovered in chapter 5, VAEs have a two-component loss—the reconstruction error and the error in representing the distributional properties of the data—captured by the KL-divergence. The total loss function is given in listing 6.11.</p>
</div>
<div class="readable-text intended-text" id="p100">
<p>Our encoder is passed edge embeddings and then outputs log probabilities of an edge type. The Gumbel-Softmax function converts these discrete logits into a differentiable continuous distribution. The decoder takes this distribution and the edge representations and then converts these back into node data. At this point, we’re ready to use the standard loss machinery for VAEs, so we calculate the reconstruction loss as MSE and the KL divergence. For further insight into VAE losses and how the KL divergence is calculated, revisit chapter 5. </p>
</div>
<div class="browsable-container listing-container" id="p101">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.11</span> Loss for the NRI model</h5>
<div class="code-area-container">
<pre class="code-area">   def calculate_loss(self, inputs,
       is_train=False,
       teacher_forcing=True,
       return_edges=False,
       return_logits=False):

       encoder_results = self.encoder(inputs)
       logits = encoder_results['logits']
       hard_sample = not is_train
       edges = F.gumbel_softmax\
               (logits.view(-1, self.num_edge_types),
               tau=self.gumbel_temp,
               hard=hard_sample).view\
                       (logits.shape) <span class="aframe-location"/> #1

       output = self.decoder(inputs[:, :-1], edges)

       if len(inputs.shape) == 3: \
target = inputs[:, 1:] 
       else:
           Target = inputs[:, 1:, :, :]

       loss_nll = F.mse_loss(\
output, target) / (2 * \
self.prior_variance) <span class="aframe-location"/> #2

       probs = F.softmax(logits, dim=-1)
       log_probs = torch.log(probs + 1e-16) <span class="aframe-location"/> #3
       loss_kl = (probs * \
(log_probs - torch.log(\
torch.tensor(1.0 / <span class="aframe-location"/> #4
       self.num_edge_types)))).\
sum(-1).mean() 

        loss = loss_nll + loss_kl

        return loss, loss_nll, loss_kl, logits, output</pre>
<div class="code-annotations-overlay-container">
     #1 Calculates Gumbel-Softmax using PyTorch's functional API, imported as F in code
     <br/>#2 Negative log likelihood (NLL) for Gaussian distribution
     <br/>#3 Adds a small constant to avoid taking the logarithm of zero
     <br/>#4 KL divergence with a uniform categorical distribution
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p102">
<p>Finally, we need our model to be able to predict the future trajectories of the sensors. The code for predicting the future state of the graph is given in listing 6.12. This is a relatively simple function once we have our encoder and decoder trained. We pass the encoder the current graph, and this returns a latent representation of whether an edge exists. We then convert these probabilities into a suitable distribution using Gumbel-Softmax and pass this to our decoder. The output from the decoder is our predictions. We can either get the predictions directly or get both predictions and whether an edge exists. </p>
</div>
<div class="browsable-container listing-container" id="p103">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.12</span> Predicting the future</h5>
<div class="code-area-container">
<pre class="code-area">   def predict_future(self, inputs, prediction_steps, 
      return_edges=False, 
      return_everything=False): #1
       encoder_dict = self.encoder(inputs)<span class="aframe-location"/> #1
       logits = encoder_dict['logits'] 
       edges = nn.functional.gumbel_softmax( <span class="aframe-location"/> #2
           logits.view(-1, \
           self.num_edge_types),  
           tau=self.gumbel_temp,\
           hard=True).view(logits.shape\ 
           ) 
       tmp_predictions, decoder_state =\
          self.decoder( <span class="aframe-location"/> #3
          inputs[:, :-1],  #3
          edges,  #3
          return_state=True  #3
       )  #3
       predictions = self.decoder( <span class="aframe-location"/> #4
          inputs[:, -1].unsqueeze(1),   #4
          edges,   #4
          prediction_steps=prediction_steps,   #4
          teacher_forcing=False,  #4
          state=decoder_state  #4
          ) #4
       if return_everything: <span class="aframe-location"/> #5
           predictions = torch.cat([\  #4
              tmp_predictions,\  #5
              Predictions\  #5
              ], dim=1)  #5

       return (predictions, edges)\
          if return_edges else predictions <span class="aframe-location"/> #6</pre>
<div class="code-annotations-overlay-container">
     #1 Runs the encoder to get logits for edge types
     <br/>#2 Applies Gumbel-Softmax to the edges
     <br/>#3 Runs the decoder to get the initial predictions and decoder state
     <br/>#4 Uses the last input and decoder state to predict future steps
     <br/>#5 Concatenates initial and future predictions if needed
     <br/>#6 Returns predictions and edges if specified
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p104">
<p>This is the basis of the NRI model. We have an encoder that converts our initial node data into edge probabilities. The edge probabilities get passed to our decoder, and the decoder predicts future trajectories conditional on the most likely graph representation. Our encoder will be a simple multilayer perceptron (MLP) that works on graph data. Our decoder needs to be able to make future predictions, so we’ll use an RNN to do this, specifically the same GRU model we discussed in section 6.2.2. Let’s next meet our encoder and decoder networks so we can apply our model to the data and see how it performs.</p>
</div>
<div class="readable-text" id="p105">
<h3 class="readable-text-h3"><span class="num-string">6.4.1</span> Encoding pose data</h3>
</div>
<div class="readable-text" id="p106">
<p>Now that we know the different parts of our NRI model, let’s define our encoder. This encoder will act as the bottleneck to make our problem simpler. After encoding, we’ll be left with a low-dimensional representation of the edge data, so we don’t need to worry about temporal data at this stage. However, by providing our temporal data together, we’re transferring temporal structure into our latent space. Specifically, the encoder takes the temporal patterns and relationships from the input data and preserves this in the compressed, low-dimensional representations. This makes it easier to decode from, making our pose prediction problem easier to solve. </p>
</div>
<div class="readable-text intended-text" id="p107">
<p>There are several subsets to implementing the encoder. First, we pass the input data, which comprises the different sensors at different frames, across different experiments. The encoder then takes this data, <em>x</em>, and performs a message-passing step to transform edge data into node data and then back into edge data. The edge data is then converted to node data again before being encoded in the latent space. This is equivalent to three message-passing steps, from edges to nodes, edges to edges, and edges to nodes again. The repeated transformations are useful for information aggregation through repeated message passing and capturing high-order interactions in the graph. By repeatedly transforming between nodes and edges, the model becomes aware of both local and global structure information. </p>
</div>
<div class="readable-text intended-text" id="p108">
<p>Throughout this book, we’ve explored how to use message passing to convert node or edge features into complex representations of nodes or edges. These are at the core of all GNN methods. The NRI model is slightly different from the methods that we’ve explored before because messages are passed between nodes and edges, rather than node to node or edge to edge. To make explicit what these steps are doing, we’ll depart from PyG and code our model in plain PyTorch instead.</p>
</div>
<div class="readable-text intended-text" id="p109">
<p>In listing 6.13, we show the base class for our encoder, which requires several key features. First, note that we haven’t described the actual neural network that will be used to encode the data. We’ll introduce this shortly. Instead, we have two message-passing functions, <code>edge2node</code> and <code>node2edge</code>, as well as an encoding function, <code>one_hot_recv</code>. </p>
</div>
<div class="browsable-container listing-container" id="p110">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.13</span> Encoder base class</h5>
<div class="code-area-container">
<pre class="code-area">   class BaseEncoder(nn.Module):
       def __init__(self, num_vars):
           super(BaseEncoder, self).__init__()
           self.num_vars = num_vars
           edges = torch.ones(num_vars)\
 - torch.eye(num_vars) <span class="aframe-location"/> #1
           self.send_edges, self.\
recv_edges = torch.where(edges) <span class="aframe-location"/> #2

           one_hot_recv = torch.nn.functional.one_hot( <span class="aframe-location"/> #3
              self.recv_edges,  #3
              num_classes=num_vars  #3
                                                ) #3
           self.edge2node_mat = \
nn.Parameter(one_hot_recv.\
float().T, requires_grad=False) <span class="aframe-location"/> #4

       def node2edge(self, node_embeddings):
           send_embed = \
node_embeddings[:, self.send_edges] <span class="aframe-location"/> #5
           recv_embed = \
node_embeddings[:, self.recv_edges] 
           return torch.\
cat([send_embed, recv_embed], dim=2) <span class="aframe-location"/> #6

       def edge2node(self, edge_embeddings):
           incoming = torch.\
matmul(self.edge2node_mat, edge_embeddings) <span class="aframe-location"/> #7
           return incoming / (self.num_vars - 1) <span class="aframe-location"/> #8</pre>
<div class="code-annotations-overlay-container">
     #1 Creates a matrix representing edges between variables
     <br/>#2 Finds the indices where edges exist
     <br/>#3 Creates a one-hot representation for receiving edges
     <br/>#4 Creates a parameter tensor for edge-to-node transformation
     <br/>#5 Extracts sender and receiver embeddings
     <br/>#6 Concatenates sender and receiver embeddings
     <br/>#7 Multiplies edge embeddings with edge-to-node matrix
     <br/>#8 Normalizes the incoming embeddings
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p111">
<p>The first step in our encoder class is to build an adjacency matrix. Here, we assume that the graph is fully connected, such that all nodes are connected to all other nodes but not to themselves. The <code>node2edge</code> function takes node embedding data and identifies the direction that these messages have been sent. Figure 6.12 shows an example of how we’re building the adjacency matrix.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p112">
<img alt="figure" height="172" src="../Images/6-12.png" width="876"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.12</span> Example of creating an adjacency matrix for a fully connected graph with three nodes. The matrix on the left represents a fully connected graph, the matrix in the middle represents the identity matrix, and the matrix on the right shows the final adjacency matrix after subtracting the identity matrix. This results in a graph where each node is connected to every other node with no self-loops.</h5>
</div>
<div class="readable-text" id="p113">
<p>The next function call then determines which nodes are sending or receiving data by returning two vectors that contain rows and columns for connected nodes. Recall that in an adjacency matrix, the rows represent receiving nodes and the columns represent sending nodes. The output is then</p>
</div>
<div class="browsable-container listing-container" id="p114">
<div class="code-area-container">
<pre class="code-area">send_edges = tensor([0, 0, 1, 1, 2, 2])
recv_edges = tensor([1, 2, 0, 2, 0, 1])</pre>
</div>
</div>
<div class="readable-text" id="p115">
<p>We can interpret this as saying that the node at row 0 sends data to nodes at columns 1 and 2, and so on. This allows us to extract edges between nodes. Once we construct our node embeddings, we then use the sending and receiving data to convert our node data to edges. This is the principle of the <code>node2edge</code> function. </p>
</div>
<div class="readable-text intended-text" id="p116">
<p>The next function we need is how to build <code>edge2node</code> based on our <code>edge_ embeddings</code>. We first construct an <code>edge2node</code> matrix. Here, we’re using a one-hot encoding method that converts our receiving edges into a one-hot encoded representation. Specifically, we create a matrix where each row denotes whether that category (receiving node) exists. For our simple three-node case, the one-hot encoding method for the receiving edges is shown in figure 6.13. </p>
</div>
<div class="readable-text intended-text" id="p117">
<p>We then transpose this to switch rows and columns, so that the dimension will be (number of nodes, number of edges), and we convert it into a PyTorch parameter so that we can differentiate over it. Once we have our <code>edge2node</code> matrix, we multiple this by our edge embeddings. Our edge embeddings will be of shape (number of edges, embedding size) so that multiplying the <code>edge2node</code> matrix by the edge embeddings gives us an object of shape (number of nodes, embedding size). These are our new node embeddings! Finally, we normalize this matrix by the number of possible nodes for numerical stability. </p>
</div>
<div class="readable-text intended-text" id="p118">
<p>This section is key to understanding the message-passing step in the model. (For further information on message passing, revisit chapter 2 and 3.) As discussed there, once we have a principled way to pass messages between nodes, edges, or some combination of both, we then apply neural networks to these embeddings to get nonlinear representations. To do so, we need to define our embedding architecture. The code for the complete encoder is given in listing 6.14. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p119">
<img alt="figure" height="632" src="../Images/6-13.png" width="936"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.13</span> The one-hot encoding matrix representing incoming edges for each node in a fully connected graph with three nodes is shown on the left. Each row corresponds to an edge, and each column corresponds to a node. A 1 in position (i, j) indicates that edge i is directed toward node j. This matrix is used to transform edge embeddings to node embeddings in the <code>edge2node</code> function of the encoder base class, enabling the model to aggregate information from incoming edges for each node. In this graph structure, nodes 0, 1, and 2 each send messages to the other two nodes, resulting in a total of six directed edges. The diagram of the three-node graph is shown on the right.</h5>
</div>
<div class="readable-text" id="p120">
<p>The <code>RefMLPEncoder</code> is shown in listing 6.14. This encoder uses four MLPs for message processing, each featuring exponential linear unit (ELU) activation and batch normalization (defined in <code>RefNRIMLP</code>, shown in the chapter’s code repository). </p>
</div>
<div class="readable-text print-book-callout" id="p121">
<p><span class="print-book-callout-head">Note</span>  The exponential linear unit (ELU) is an activation function that is useful in smoothing outputs across multiple layers and preventing vanishing gradients. In contrast to ReLUs, ELUs have a smoother gradient built in for negative inputs and allows for negative outputs.</p>
</div>
<div class="readable-text" id="p122">
<p>The final part of the network (<code>self.fc_out</code>) is a sequence of linear layers with ELU activations between them, ending with a linear layer that outputs the desired embeddings or predictions. The final layer of this sequence is a fully connected layer.</p>
</div>
<div class="browsable-container listing-container" id="p123">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.14</span> NRI MLP encoder</h5>
<div class="code-area-container">
<pre class="code-area">   class RefMLPEncoder(BaseEncoder):
       def __init__(self, 
               num_vars=31, 
               input_size=6, 
               input_time_steps=50, 
               encoder_mlp_hidden=256, 
               encoder_hidden=256, 
               num_edge_types=2, 
               encoder_dropout=0.):
           super(RefMLPEncoder, self).__init__(num_vars)
           inp_size = input_size * input_time_steps
           hidden_size = encoder_hidden
           num_layers = 3
           self.input_time_steps = input_time_steps

           self.mlp1 = RefNRIMLP\
(inp_size, hidden_size, \
hidden_size, encoder_dropout) <span class="aframe-location"/> #1
           self.mlp2 = RefNRIMLP\
(hidden_size*2, hidden_size,\
 hidden_size, encoder_dropout) 
           self.mlp3 = RefNRIMLP\
(hidden_size, hidden_size,\
 hidden_size, encoder_dropout) 
           mlp4_inp_size = hidden_size * 2
           self.mlp4 = RefNRIMLP\
(mlp4_inp_size, hidden_size,\
 hidden_size, encoder_dropout)

           layers = [nn.Linear\
(hidden_size, encoder_mlp_hidden), \
nn.ELU(inplace=True)] <span class="aframe-location"/> #2
           layers += [nn.Linear\
(encoder_mlp_hidden, \
encoder_mlp_hidden),\ 
   nn.ELU(inplace=True)] \
   * (num_layers - 2) 
           layers.append(nn.\
Linear(encoder_mlp_hidden, \
num_edge_types)) 
           self.fc_out = nn.Sequential(*layers) 
           self.init_weights()</pre>
<div class="code-annotations-overlay-container">
     #1 Defines MLP layers. RefNRIMLP is a 2-layer fully connected ELU net with batch norm.
     <br/>#2 Defines the final fully connected layer
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p124">
<p>Here, we define architectural details related to the encoder. As discussed earlier, there are 31 sensors that we represent using the <code>num_vars</code> variable. The number of features is 6, which is the <code>input_size</code> for our network. The number of timesteps for our training and validation set is still 50, and our encoder network size will be 256. The number of <code>edge_types</code> is 2, and we assume no dropout of the weights. We then initialize our networks, which are typical MLPs, described in our shared repository. The networks include a batch normalization layer and two fully connected layers. Once the network is defined, we also pre-initialize the weights, as shown in listing 6.15. Here, we loop through all the different layers and then initialize the weights using the Xavier initialization approach. This ensures that the gradients in the layers are all approximately of similar scale, which reduces the risk of our loss rapidly diverging—known as blow-up. This is an important step when combining multiple networks with different architectures as we do here. We also set the initial bias to 0.1, which further helps with the stability of training. </p>
</div>
<div class="browsable-container listing-container" id="p125">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.15</span> Weight initialization</h5>
<div class="code-area-container">
<pre class="code-area">    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear): <span class="aframe-location"/> #1
                nn.init.xavier_normal_(m.weight.data) <span class="aframe-location"/> #2
                m.bias.data.fill_(0.1) <span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 Only applies to linear layers
     <br/>#2 Initializes weights using Xavier normal initialization
     <br/>#3 Sets bias to 0.1
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p126">
<p>Finally, we need to define our forward pass method, as shown in listing 6.16. This is where our message-passing step occurs. </p>
</div>
<div class="browsable-container listing-container" id="p127">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.16</span> Encoder forward pass</h5>
<div class="code-area-container">
<pre class="code-area">   def forward(self, inputs, state=None, return_state=False):
       if inputs.size(1) &gt; self.input_time_steps:
           inputs = inputs[:, -self.input_time_steps:]
       elif inputs.size(1) &lt; self.input_time_steps:
           begin_inp = inputs[:, 0:1].expand(
           -1, 
           self.input_time_steps-inputs.size(1),
           -1, -1
           )
           inputs = torch.cat([begin_inp, inputs], dim=1) #1

       x = inputs.transpose(1, 2).contiguous() <span class="aframe-location"/> #1
       x = x.view(inputs.size(0), inputs.size(2), -1) 

       x = self.mlp1(x) <span class="aframe-location"/> #2
       x = self.node2edge(x) <span class="aframe-location"/> #3
       x = self.mlp2(x) <span class="aframe-location"/> #4

       x = self.edge2node(x) <span class="aframe-location"/> #5
       x = self.mlp3(x)

       x = self.node2edge(x) <span class="aframe-location"/> #6
       x = self.mlp4(x)

       result =  self.fc_out(x) <span class="aframe-location"/> #7
       result_dict = {
          'logits': result,
          'state': inputs,
           }
       return result_dict</pre>
<div class="code-annotations-overlay-container">
     #1 New shape: [num_sims, num_atoms, num_timesteps*num_dims]
     <br/>#2 Passes through first MLP layer (two-layer ELU network per node)
     <br/>#3 Converts node embeddings to edge embeddings
     <br/>#4 Passes through the second MLP layer
     <br/>#5 Converts edge embeddings back to node embeddings
     <br/>#6 Converts node embeddings to edge embeddings again
     <br/>#7 Final fully connected layer to get the logits
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p128">
<p>Our encoder lets our model transform different sets of frames of our sensor graphs into latent representation of edge probabilities. Next, let’s explore how to construct a decoder that transforms the latent edge probabilities into trajectory using the recent sensor data. </p>
</div>
<div class="readable-text" id="p129">
<h3 class="readable-text-h3"><span class="num-string">6.4.2</span> Decoding pose data using a GRU</h3>
</div>
<div class="readable-text" id="p130">
<p>To transform the latent representations into future frames, we need to account for the temporal evolution of the trajectories. To do so, we train a decoder network. Here, we’ll follow the original structure of the NRI paper [2] and use a GRU as our RNN. </p>
</div>
<div class="readable-text intended-text" id="p131">
<p>We introduced the concept of a GRU in section 6.2.2 earlier. As a quick reminder, gated recurrent units (GRUs) are a type of RNN that uses a gated process to allow RNNs to capture long-term behaviors in the data. They are composed of two types of gates—reset gates and update gates. </p>
</div>
<div class="readable-text intended-text" id="p132">
<p>For the NRI model, we’ll apply GRUs to our edges, rather than across the entire graph. The update gates will be used to determine how much of the node’s hidden state should be updated, given the receiving data, and the reset gate decides how much should be erased or “forgotten.” To put it another way, we’ll use a GRU to predict what the future state of a node should be based on the edge type probabilities from our encoder network. </p>
</div>
<div class="readable-text intended-text" id="p133">
<p>Let’s look at how we construct this step-by-step. The initialization code for our decoder is given in listing 6.17. First, we note some of the variables passed to this network. We again define the number of variables or nodes in our graphs, 31, and the number of input features, 6. We assume there is no dropout of the weights and the hidden size for each layer is 64. Again, we need to make clear that our decoder should be predicting two different types of edges. We’ll also skip the first edge type when making predictions as this denotes that there is no edge. </p>
</div>
<div class="readable-text intended-text" id="p134">
<p>Once we have the input parameters defined, we can introduce the network architecture. The first layer is a simple linear network that needs to have twice the input dimension to account for the mean and variance provided by our encoder, and we define this network for each of the edge types. We then define a second layer to further increase the expressivity of our network. The output from these two linear layers is passed to our RNN, which is a GRU. Here, we have to use a custom GRU to account for both node data and edge data. The output from the GRU is passed to three more neural network layers to provide the future predictions. Finally, we need to define our <code>edge2node</code> matrix and sending and receiving nodes, as we did with our encoder.</p>
</div>
<div class="browsable-container listing-container" id="p135">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.17</span> RNN decoder</h5>
<div class="code-area-container">
<pre class="code-area">   class GraphRNNDecoder(nn.Module):
       def __init__(self, 
           num_vars=31, 
           input_size=6, 
           decoder_dropout=0., 
           decoder_hidden=64, 
           num_edge_types=2, 
           skip_first=True):
           super(GraphRNNDecoder, self).__init__()
           self.num_vars = num_vars
           self.msg_out_shape = decoder_hidden
           self.skip_first_edge_type = skip_first
           self.dropout_prob = decoder_dropout
           self.edge_types = num_edge_types

           self.msg_fc1 = nn.ModuleList\
([nn.Linear(2 * decoder_hidden,\
 decoder_hidden) for _ in \
range(self.edge_types)]) <span class="aframe-location"/> #1
           self.msg_fc2 = nn.ModuleList\
([nn.Linear(decoder_hidden, decoder_hidden)\
 for _ in range(self.edge_types)])

           self.custom_gru = CustomGRU\
(input_size, decoder_hidden) <span class="aframe-location"/> #2

           self.out_fc1 = nn.Linear\
(decoder_hidden, decoder_hidden) <span class="aframe-location"/> #3
           self.out_fc2 = nn.Linear(decoder_hidden, decoder_hidden)
           self.out_fc3 = nn.Linear(decoder_hidden, input_size)

           self.num_vars = num_vars
           edges = np.ones(num_vars) - np.eye(num_vars)
           self.send_edges = np.where(edges)[0]
           self.recv_edges = np.where(edges)[1]
           self.edge2node_mat = \
                torch.FloatTensor\
                (encode_onehot(self.recv_edges))
           self.edge2node_mat = self.edge2node_mat.cuda(non_blocking=True)</pre>
<div class="code-annotations-overlay-container">
     #1 Edge-related layers
     <br/>#2 GRU layers
     <br/>#3 Fully connected layers
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p136">
<p>In listing 6.18, we provide the architecture for our GRU. The first overall architecture for this network is the same structure as a typical GRU. We define three hidden layers which represent the reset gates defined by <code>hidden_r</code> and <code>input_r</code>, the update gates defined by <code>hidden_i</code> and <code>input_i</code>, and the activation networks defined by <code>hidden_h</code> and <code>input_h</code>. The forward network, however, needs to account for the aggregated messages from the message-passing output of our encoder. This is shown in the forward pass. We’ll pass the edge probabilities in <code>agg_msgs</code>, along with the input node data, and these combine to return future predictions. This can be seen in the <code>predict_future</code> code in our base NRI class: </p>
</div>
<div class="browsable-container listing-container" id="p137">
<div class="code-area-container">
<pre class="code-area">      predictions = self.decoder(inputs[:, -1].unsqueeze(1), edges,
      prediction_steps=prediction_steps, teacher_forcing=False, 
      state=decoder_state)</pre>
</div>
</div>
<div class="readable-text" id="p138">
<p>Our decoder gets passed the last time frame of our graphs. The edge data that is output from our encoder is also passed to the decoder. </p>
</div>
<div class="browsable-container listing-container" id="p139">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.18</span> Custom GRU network</h5>
<div class="code-area-container">
<pre class="code-area">   class CustomGRU(nn.Module):
       def __init__(self,input_size, n_hid,num_vars=31):
           super(CustomGRU, self).__init__()
           self.num_vars = num_vars
           self.hidden_r = nn.Linear
(n_hid, n_hid, bias=False) <span class="aframe-location"/> #1
           self.hidden_i = nn.Linear\
(n_hid, n_hid, bias=False) 
           self.hidden_h = nn.Linear\
(n_hid, n_hid, bias=False) 

           self.input_r = nn.Linear\
(input_size, n_hid, bias=True) <span class="aframe-location"/> #2
           self.input_i = nn.Linear(\
input_size, n_hid, bias=True) 
           self.input_n = nn.Linear\
(input_size, n_hid, bias=True) 

       def forward(self, inputs, agg_msgs, hidden):
           inp_r = self.input_r(inputs)\
.view(inputs.size(0), self.num_vars, -1)
           inp_i = self.input_i(inputs)\
.view(inputs.size(0), self.num_vars, -1)
           inp_n = self.input_n(inputs)\
.view(inputs.size(0), self.num_vars, -1)

           r = torch.sigmoid(inp_r + \
self.hidden_r(agg_msgs)) <span class="aframe-location"/> #3
           i = torch.sigmoid(inp_i + \
self.hidden_i(agg_msgs)) <span class="aframe-location"/> #4
           n = torch.tanh(inp_n + \
r*self.hidden_h(agg_msgs)) <span class="aframe-location"/> #5
           hidden = (1 - i)*n + i*hidden <span class="aframe-location"/> #6

           return hidden</pre>
<div class="code-annotations-overlay-container">
     #1 Defines hidden layer transformations for reset, input, and new gates
     <br/>#2 Defines input layer transformations for reset, input, and new gates
     <br/>#3 Computes reset gate activations
     <br/>#4 Computes input gate activations
     <br/>#5 Computes new gate activations
     <br/>#6 Updates hidden state
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p140">
<p>The output from the decoder network is then the future prediction timesteps. To better understand this, let’s look at the forward pass method for our decoder, given in listing 6.19. Our forward pass is given the inputs and sampled edges to build a prediction. There are also four additional arguments that help control the behavior. First, we define a <code>teacher_forcing</code> variable. Teaching forcing is a typical method used when training sequential models, such as RNNs. If this is true, we use the ground truth (the real graph) to predict the next time frame. When this is false, we use the output from the model’s previous timestep. This makes sure that the model isn’t led astray by incorrect predictions during training. Next, we include a <code>return_state</code> variable, which allows us to access the hidden representations given by the decoder network. We use this when we predict the future graph evolution, as shown here: </p>
</div>
<div class="browsable-container listing-container" id="p141">
<div class="code-area-container">
<pre class="code-area">     tmp_predictions, decoder_state = \
        self.decoder(inputs[:, :-1], edges, 
        return_state=True)
     predictions = self.decoder\
        (inputs[:, -1].unsqueeze(1), edges, 
        prediction_steps=prediction_steps, \
        teacher_forcing=False, state=decoder_state)</pre>
</div>
</div>
<div class="readable-text" id="p142">
<p>Let’s now discuss the prediction process. First, we predict a temporary prediction set. Then, we use the hidden representations to predict as many steps in the future as is needed. This is particularly useful when we want to predict more than one timestep, as we show in the testing phase of this model. This is controlled by the <code>prediction_steps</code> variable, which tells us how many times to loop through our RNN, that is, how many timesteps in the future we want to predict. Finally, we have a <code>state</code> variable, which is used to control the information being passed to our decoder. When it’s left empty, we initialize a tensor of zeros so that there is no information being passed. Otherwise, we’ll use information from previous timesteps. </p>
</div>
<div class="browsable-container listing-container" id="p143">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.19</span> Decoder forward pass</h5>
<div class="code-area-container">
<pre class="code-area">     def forward(self, inputs, sampled_edges,
         teacher_forcing=False,
         return_state=False,
         prediction_steps=-1,
         state=None):

         batch_size, time_steps, num_vars, num_feats = inputs.size()
         pred_steps = prediction_steps if \
            prediction_steps &gt; 0 else time_steps <span class="aframe-location"/> #1

         if len(sampled_edges.shape) == 3: <span class="aframe-location"/> #2
             sampled_edges = sampled_edges.unsqueeze(1) 
             sampled_edges = sampled_edges.expand\
                (batch_size, pred_steps, -1, -1) 

         if state is None: <span class="aframe-location"/> #3
             hidden = torch.zeros(batch_size,  #3
                Num_vars,  #3
                Self.msg_out_shape,  #3
                device=inputs.device)  #3
         else:  #3
             hidden = state  #3
             teacher_forcing_steps = time_steps <span class="aframe-location"/> #4

         pred_all = []
         for step in range(pred_steps): <span class="aframe-location"/> #5
         if step == 0 or (teacher_forcing \
            and step &lt; teacher_forcing_steps): 
             ins = inputs[:, step, :] 
         else: 
             ins = pred_all[-1] 

         pred, hidden = self.single_step_forward( <span class="aframe-location"/> #6
              ins,   #6
              sampled_edges[:, step, :],   #6
              hidden  #6
              )  #6
              pred_all.append(pred)

         preds = torch.stack(pred_all, dim=1)

         return (preds, hidden) if return_state else preds <span class="aframe-location"/> #7</pre>
<div class="code-annotations-overlay-container">
     #1 Determines the number of prediction steps
     <br/>#2 Expands the sampled_edges tensor if needed
     <br/>#3 Initializes the hidden state if not provided
     <br/>#4 Determines the number of steps to apply teacher forcing to
     <br/>#5 Decides the input for this step based on teacher forcing
     <br/>#6 Performs a single forward step using the ins calculated from inputs or pred_all (see the previous comment)
     <br/>#7 Returns predictions and the hidden state
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p144">
<p>To predict timesteps into the future, we make an additional forward pass that is based on a single timestep, as defined in listing 6.20. This is where our network performs additional message-passing steps. We take our receiver nodes and sending nodes, which are defined from the edge probabilities from our encoder. We ignore the first edges, as these are unconnected nodes, and the network then loops through the different networks for the different edge types to get all edge-dependent messages from the network. This is the critical step that makes our predictions dependent on the graph data. Our GRU then takes the messages from the connected node to inform its predictions of the trajectories. At this step, we’re learning to predict how the body is walking from what we’ve learned about how the body is connected. The output is both the predicted trajectories of the sensors on the body as well as the network data for why it made these predictions, encoded in the hidden weights. This completes the NRI model for estimating poses. </p>
</div>
<div class="browsable-container listing-container" id="p145">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.20</span> Decoder single step forward</h5>
<div class="code-area-container">
<pre class="code-area">     def single_step_forward(self, inputs, rel_type, hidden):
         receivers = hidden[:, self.recv_edges, :] <span class="aframe-location"/> #1
         senders = hidden[:, self.send_edges, :]  #1

         pre_msg = torch.cat([receivers, senders], dim=-1) <span class="aframe-location"/> #2

         all_msgs = torch.zeros(
             pre_msg.size(0), 
             pre_msg.size(1), 
             self.msg_out_shape, 
             device=inputs.device
             )

         start_idx = 1 if self.skip_first_edge_type else 0
         norm = float(len(self.msg_fc2) - start_idx)

         for i in range(start_idx, len(self.msg_fc2)): <span class="aframe-location"/> #3
             msg = torch.tanh(self.msg_fc1[i](pre_msg))  #3
             msg = F.dropout(msg, p=self.dropout_prob)  #3
             msg = torch.tanh(self.msg_fc2[i](msg))  #3
             msg = msg * rel_type[:, :, i:i+1]  #3
             all_msgs += msg / norm  #3

         agg_msgs = all_msgs.transpose(-2, -1) <span class="aframe-location"/> #4
         agg_msgs = agg_msgs.matmul(self.edge2node_mat) 
         agg_msgs = agg_msgs.transpose\
            (-2, -1) / (self.num_vars - 1) 

         hidden = self.custom_gru(inputs, agg_msgs, hidden) <span class="aframe-location"/> #5

         pred = F.dropout(F.relu\
           (self.out_fc1(hidden)), \
           p=self.dropout_prob) <span class="aframe-location"/> #6
         pred = F.dropout(F.relu\
         (self.out_fc2(pred)), \
         p=self.dropout_prob) 
         pred = self.out_fc3(pred) 

         pred = inputs + pred   
         return pred, hidden</pre>
<div class="code-annotations-overlay-container">
     #1 Node-to-edge step
     <br/>#2 Message of size: [batch, num_edges, 2*msg_out]
     <br/>#3 Runs a separate MLP for every edge type
     <br/>#4 Sums all the messages per node
     <br/>#5 GRU-style gated aggregation
     <br/>#6 Builds output MLP
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p146">
<h3 class="readable-text-h3"><span class="num-string">6.4.3</span> Training the NRI model</h3>
</div>
<div class="readable-text" id="p147">
<p>Now that we’ve defined the different parts of our model, let’s train the model and see how it performs. To train our model, we’ll take the following steps:</p>
</div>
<ol>
<li class="readable-text" id="p148"> Train an encoder that converts sensor data into a representation of edge probabilities, indicating whether a sensor is connected to another or not. </li>
<li class="readable-text" id="p149"> Train a decoder to predict future trajectories, conditional on the probability of there being an edge connecting the different sensors. </li>
<li class="readable-text" id="p150"> Run the decoder to predict the future trajectories using a GRU, which is passed the edge probabilities. </li>
<li class="readable-text" id="p151"> Reduce the loss based on the reconstructed poses. This loss has two components: the reconstruction loss and the KL divergence. </li>
<li class="readable-text" id="p152"> Repeat steps 1 through 4 until training converges. </li>
</ol>
<div class="readable-text" id="p153">
<p>This is also shown in figure 6.14, and the training loop is given in listing 6.21.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p154">
<img alt="figure" height="439" src="../Images/6-14.png" width="1007"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.14</span> Pipeline for the NRI model</h5>
</div>
<div class="browsable-container listing-container" id="p155">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.21</span> NRI training loop</h5>
<div class="code-area-container">
<pre class="code-area">   pbar = tqdm(range(start_epoch, num_epochs + 1), desc='Epochs')
   for epoch in pbar:
       model.train() <span class="aframe-location"/> #1
       model.train_percent = epoch / num_epochs
       total_training_loss = 0
       for batch in train_data_loader:
           inputs = batch['inputs'].cuda(non_blocking=True)
           loss, _, _, _, _ = model.\
              calculate_loss(inputs, 
              is_train=True, 
              return_logits=True)
           loss.backward() <span class="aframe-location"/> #2
           optimizer.step() 
           optimizer.zero_grad() <span class="aframe-location"/> #3
           total_training_loss += loss.item()

      if training_scheduler is not None:
          training_scheduler.step()

      total_nll, total_kl = 0, 0
      for batch in val_data_loader:
          inputs = batch['inputs'].cuda(non_blocking=True)
            , loss_nll, loss_kl, _, _ = model.calculate_loss(inputs,
            is_train=False, 
            teacher_forcing=True, 
            return_logits=True)
          total_kl += loss_kl.sum().item()
          total_nll += loss_nll.sum().item()

          total_kl /= len(val_data)
          total_nll /= len(val_data)
          total_loss = total_kl + total_nll
          tuning_loss = total_nll 

      if tuning_loss &lt; best_val_result:
          best_val_epoch, best_val_result = epoch, tuning_loss</pre>
<div class="code-annotations-overlay-container">
     #1 Training loop
     <br/>#2 Update the weights.
     <br/>#3 Zero gradients for the validation pass
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p156">
<p>We’ll train for 50 epochs with a learning rate of 0.0005, a learning rate scheduler that reduces the learning rate by a factor of 0.5 after 500 forward passes, and a batch size of 8. Most of the training is based on the <code>calculate_loss</code> method call, which we defined earlier in listing 6.14. We find that our model loss falls along with the validation loss, reaching a validation loss of 1.21 based on the negative log likelihood (<code>nll</code>). This looks good but let’s see how it performs on the test data, where it needs to predict multiple steps into the future. To do so, we need to define a new function, given in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p157">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.22</span> Evaluating future predictions</h5>
<div class="code-area-container">
<pre class="code-area">def eval_forward_prediction(model, 
  dataset, 
  burn_in, 
  forward_steps, 
  gpu=True, batch_size=8, 
  return_total_errors=False):

  dataset.return_edges = False

  data_loader = DataLoader\
    (dataset, batch_size=\
    batch_size, pin_memory=gpu)
  model.eval()
  total_se = 0
  batch_count = 0
  all_errors = []

  for batch_ind, batch in enumerate(data_loader):
    inputs = batch['inputs']
    with torch.no_grad():
      model_inputs = inputs[:, :burn_in]
      gt_predictions = inputs[:, burn_in:burn_in+forward_steps]
      model_inputs = model_inputs.cuda(non_blocking=True)
      model_preds = model.predict_future(
          model_inputs,
          forward_pred_steps
          ).cpu()
      batch_count += 1
      if return_total_errors:
          all_errors.append(
            F.mse_loss(
              model_preds, 
              gt_predictions,
              reduction='none'
             ).view(
               model_preds.size(0), 
               model_preds.size(1), -1
             ).mean(dim=-1)
          )
      else:
          total_se += F.mse_loss(
            model_preds, 
            gt_predictions,
            reduction='none'
          ).view(
            model_preds.size(0),
            model_preds.size(1),
            -1
          ).mean(dim=-1).sum(dim=0)

  if return_total_errors:
         return torch.cat(all_errors, dim=0)
     else:
            return total_se / len(dataset)</pre>
</div>
</div>
<div class="readable-text" id="p158">
<p>This function loads our test data and then calculates the MSE for our predictions given different time horizons. When we test our model, we find that it’s able to predict the next timestep with an MSE of 0.00008. Even better, it predicts 40 timesteps into the future with an accuracy of 94%. This is significantly better than our LSTM and GAT models, which achieved 65% and 55%, respectively. The reduction in accuracy over future timesteps is shown in figure 6.15, and the example output is given in figure 6.16.<span class="aframe-location"/> <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p159">
<img alt="figure" height="818" src="../Images/6-15.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.15</span> Reduction in accuracy as we predict into the future</h5>
</div>
<div class="browsable-container figure-container" id="p160">
<img alt="figure" height="539" src="../Images/6-16.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.16</span> Example output from the NRI model</h5>
</div>
<div class="readable-text" id="p161">
<p>We’ve covered all the core components for the NRI model, with the full working code provided in the GitHub repository (<a href="https://mng.bz/4a8D">https://mng.bz/4a8D</a>). The accuracy is impressive and highlights the power of combining generative and graph-based methods with temporal models. This is shown in figure 6.15, where we see good agreement with the predicted pose and the resulting estimated pose.</p>
</div>
<div class="readable-text intended-text" id="p162">
<p>Furthermore, this method is robust at not just predicting graphs but also learning the underlying structure even when all the graph data isn’t available. In this problem, we knew what interaction network to expect. However, there are many instances where we don’t know the interaction network. One example is particles that are moving in a confined space. When they are within some interaction radius, then they will influence each other, but not when they are farther away. This is true of organisms from cells to sports players. In fact, the majority of the world involves interacting agents with secret interaction networks. NRI models provide a tool to not only predict the behavior and movement of these agents but also learn about their interaction patterns with other agents. Indeed, the original NRI paper demonstrated this using video tracking data of basketball games and showed that the model can learn typical patterns between ball, ball handler, screener, and defensive matchups for the different players. (For more information, refer to Kipf et al. [2].) </p>
</div>
<div class="readable-text" id="p163">
<h2 class="readable-text-h2"><span class="num-string">6.5</span> Under the hood</h2>
</div>
<div class="readable-text" id="p164">
<p>In this chapter, we showed how to tackle temporal or dynamic problems. Here, we go into more detail for some of the key model components that we used. </p>
</div>
<div class="readable-text" id="p165">
<h3 class="readable-text-h3"><span class="num-string">6.5.1</span> Recurrent neural networks</h3>
</div>
<div class="readable-text" id="p166">
<p>In figure 6.16, we showed a schematic for RNN models. The main difference for RNN models compared to all the other models that we’ve seen is that the model can cope with sequential data. This means that each timestep has a hidden layer, and output from this hidden layer is combined with new input at subsequent timesteps. In figure 6.17, this is shown in two ways. First, on the left side, we show the temporal updates as a single self-loop denoted by Whh. To get a better understanding of what this self-loop is doing, we’ve “unfolded” the model in time so that we can explicitly see how our model updates. Here, we change our input, output, and hidden layers (x, y, h) to be temporal variables (xt, yt, ht). At our initial step, t, we update our current hidden layer with input data from xt and the weights from our previous hidden layer ht–1 and then use this to output yt. The weights from ht are then passed to ht+1 along with the new input at xt+1 to infer yt+1. </p>
</div>
<div class="readable-text intended-text" id="p167">
<p>One of the key features for this model is that when we backpropagate to update our weights, we need to backpropagate through time (BPTT). This is a specific feature for all RNNs. However, most modern deep learning packages make this very straightforward to do and hide all the difficult computational details for the practitioner. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p168">
<img alt="figure" height="466" src="../Images/6-17.png" width="874"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.17</span> Structure for an RNN. Temporal updates as a single self-loop denoted by Whh (left). An unfolded model in time showing the model updates (right). Here, we change our input, output, and hidden layers (x, y, h) to be temporal variables (x<sub>t</sub>, y<sub>t</sub>, h<sub>t</sub>). At our initial step, t, we update our current hidden layer with input data from x<sub>t</sub> and the weights from our previous hidden layer h<sub>t–1</sub> and then use this to output y<sub>t</sub>. The weights from ht are then passed to h<sub>t+1</sub> along with the new input at x<sub>t+1</sub> to infer y<sub>t+1</sub>. </h5>
</div>
<div class="readable-text intended-text" id="p169">
<p>Let’s see how to implement an RNN using PyTorch. This is as straightforward as defining a neural network class and then introducing specific RNN layers within the network. For example, in listing 6.23, we show the code for defining a network with a single RNN layer. This is a very basic definition of an RNN, given there is only one hidden layer. However, it’s useful to see this example to get some solid intuition on how a model can be trained. For each timestep, our input is passed both to the hidden layer and the output. When we perform a forward pass, the output goes back to output and the hidden layer. Finally, we need to initialize our hidden layer with something, so we’re using a fully connected layer. </p>
</div>
<div class="browsable-container listing-container" id="p170">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.23</span> Defining an RNN</h5>
<div class="code-area-container">
<pre class="code-area">   class PoseEstimationRNN(nn.Module):
       def __init__(self, input_size, hidden_size, output_size, num_layers):
           super(PoseEstimationRNN, self).__init__()

           self.hidden_size = hidden_size
           self.num_layers = num_layers

           self.rnn = nn.RNN\
(input_size, hidden_size, \
num_layers, batch_first=True) <span class="aframe-location"/> #1
           self.fc = nn.Linear(hidden_size, output_size) <span class="aframe-location"/> #2

       def forward(self, x):    
           h0 = torch.zeros(self.num_layers,\ #3
             x.size(0), self.hidden_size) <span class="aframe-location"/> #3
           H0 = h0.to(x.device) 

           out, _ = self.rnn(x, h0) <span class="aframe-location"/> #4
           out = self.fc(out[:, -10:, :])<span class="aframe-location"/> #5
           return out</pre>
<div class="code-annotations-overlay-container">
     #1 RNN layer
     <br/>#2 Fully connected layer
     <br/>#3 Sets the initial hidden and cell states
     <br/>#4 Forward propagates the RNN
     <br/>#5 Passes the output of the last timestep to the fully connected layer
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p171">
<p>In practice, we often want to use more complicated RNNs. This includes extensions to RNNs such as LSTM networks or GRU networks. We can even stack RNNs, LSTMs, and GRUs together using our deep learning library of choice. A GRU is similar to an RNN in that it’s useful for sequences of data. They were specifically designed to resolve one of the key drawbacks of RNNs, the vanishing gradient problem. It uses two gates, which determine both how much past information to keep (the update gates) and how much to forget or throw away (the reset gates). We show an example design for a GRU in figure 6.18. Here, <em>z</em><em><sub>t</sub></em> denotes the update gates, and r<em><sub>t</sub></em> denotes the reset gates. The <em>~h</em><em><sub>t</sub></em> term is known as the candidate activation and reflects a candidate for the new state of the representations, while the <em>h</em><em><sub>t</sub></em> term is the actual hidden state.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p172">
<img alt="figure" height="549" src="../Images/6-18.png" width="692"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.18</span> Design of the GRU layer, where r<em><sub>t</sub></em> represents the reset gate, z<em><sub>t</sub></em> is the update gate, ~h<em><sub>t</sub></em> is the candidate function, and h<em><sub>t</sub></em> is the final actual hidden state</h5>
</div>
<div class="readable-text" id="p173">
<p>In listing 6.24, we show how to build a model with GRU layers. Here, the majority of the implementation is handled by PyTorch, where the layer is imported from the standard PyTorch library. The rest of the model definition is a typical neural network. </p>
</div>
<div class="browsable-container listing-container" id="p174">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.24</span> GRU</h5>
<div class="code-area-container">
<pre class="code-area">   class PoseEstimationGRU(nn.Module):
       def __init__(self, input_size, hidden_size, output_size, num_layers):
           super(PoseEstimationGRU, self).__init__()
           self.hidden_size = hidden_size
           self.num_layers = num_layers
           self.gru = nn.GRU\
(input_size, hidden_size, \
num_layers, batch_first=True) <span class="aframe-location"/> #1
           self.fc = nn.Linear(hidden_size, output_size) <span class="aframe-location"/> #2

        def forward(self, x):

            h0 = torch.zeros\
(self.num_layers, \
x.size(0), self.hidden_size) <span class="aframe-location"/> #3
            h0 = h0.to(x.device)  #3
            out, _ = self.gru(x, h0) <span class="aframe-location"/> #4
            out = self.fc(out[:, -10:, :]) <span class="aframe-location"/> #5
            return out</pre>
<div class="code-annotations-overlay-container">
     #1 GRU layer
     <br/>#2 Fully connected layer
     <br/>#3 Sets the initial hidden state
     <br/>#4 Forward propagates the GRU
     <br/>#5 Passes the output of the last timestep to the fully connected layer
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p175">
<h3 class="readable-text-h3"><span class="num-string">6.5.2</span> Temporal adjacency matrices</h3>
</div>
<div class="readable-text" id="p176">
<p>When considering temporal graphs, we might start with two nodes connected by one edge, then at each subsequent timestep, another few nodes and/or edges are added. This results in several distinct graphs, each with a differently sized adjacency matrix. </p>
</div>
<div class="readable-text intended-text" id="p177">
<p>This might present a difficulty when designing our GNN. First, we have different sized graphs at each timestep. This means we won’t be able to use node embeddings because the number of nodes will keep changing across input data. One method is to use graph embeddings at each timestep to store the entire graph as a low-dimensional representation. This method is at the heart of many temporal approaches, where graph embeddings are evolved in time rather than the actual graph. We can even use more complex transformations on our graph, such as using an autoencoder model as in our NRI model. </p>
</div>
<div class="readable-text intended-text" id="p178">
<p>Alternatively, we can transform all the individual graphs at each timestep into one single larger graph by creating a temporal adjacency matrix. This involves wrapping each timestep into a single graph that spans both per-timestep data as well as dynamic temporal data. Temporal adjacency matrices can be useful if a graph is small and we’re only interested in a few timesteps in the future. However, they can often become very large and difficult to work with. On the other hand, using temporal embedding methods can often involve multiple complicated subcomponents and become difficult to train. Unfortunately, there is no one-size-fits-all temporal graph, and the best approach is almost always problem specific. </p>
</div>
<div class="readable-text" id="p179">
<h3 class="readable-text-h3"><span class="num-string">6.5.3</span> Combining autoencoders with RNNs</h3>
</div>
<div class="readable-text" id="p180">
<p>In this section, to build intuition around the NRI model, we’ll summarize its components and illustrate its application in predicting graph structures and node trajectories. To start, in figure 6.19, we repeat the schematic for the NRI model. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p181">
<img alt="figure" height="302" src="../Images/6-19.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 6.19</span> Schematic for NRI (Source: Kipf et al. [2]). The model consists of an encoder and decoder layer and several message-passing steps. However, here the messages are passed in the encoder from node to edge, back from edge to node, and then back from node to edge again. For the decoder, messages are passed from node to edge and then from edge to node. The final step takes the latent representation and is used to predict the next step in the temporal evolution of the body.</h5>
</div>
<div class="readable-text intended-text" id="p182">
<p>In this model, there are two key components. First, we train an encoder to encode the graphs from each frame into the latent space. Explicitly, we use the encoder to predict the probability distribution, q<sub>j</sub>(z|x) over the latent interactions (z), given the initial graphs (x). Once we’ve trained the encoder, we then use the decoder to convert samples from this probability distribution into trajectories using the latent encoding as well as previous timesteps. In practice, we use the encoder-decoder structure to infer the trajectories of nodes with different interaction types (or edges). </p>
</div>
<div class="readable-text intended-text" id="p183">
<p>In this chapter, we’ve only considered two edge types: where there is or isn’t a physical connection between sensors. However, this method can be scaled to consider many different connections, all changing with time. Additionally, the decoder model needs an RNN to effectively capture the temporal data in our graph. To build some intuition around the NRI model, let’s repeat the process once more. </p>
</div>
<ol>
<li class="readable-text" id="p184"> <em>Input </em>—Node data. </li>
<li class="readable-text" id="p185"> <em>Encoding </em>— 
    <ol style="list-style: lower-alpha">
<li> The encoder receives the node data.  </li>
<li> The encoder converts the node data into edge data. </li>
<li> The encoder represents the edge data in a latent space. </li>
</ol></li>
<li class="readable-text" id="p186"> <em>Latent space </em>—The latent space represents probabilities of different edge types. Here, we have two edge types (connected and not connected), though multiple edge types are possible for more complex relationships. We always need to include at least two types as otherwise the model would assume all the nodes are connected or, worse, none of them are. </li>
<li class="readable-text" id="p187"> <em>Decoding </em>— 
    <ol style="list-style: lower-alpha">
<li> The decoder takes the edge type probabilities from the latent space.  </li>
<li> The decoder learns to reconstruct the future graph state based on these probabilities. </li>
</ol></li>
<li class="readable-text" id="p188"> <em>Prediction </em>—The model predicts future trajectories by learning to predict graph connectivity. </li>
</ol>
<div class="readable-text" id="p189">
<p>Note that this model gives us graph and trajectory predictions simultaneously! While this might not be helpful for our problem, for cases where we don’t know the underlying graph structure such as social media networks or sports teams, this can provide ways to discover new interaction patterns in a system. </p>
</div>
<div class="readable-text" id="p190">
<h3 class="readable-text-h3"><span class="num-string">6.5.4</span> Gumbel-Softmax</h3>
</div>
<div class="readable-text" id="p191">
<p>In the NRI model, there is an additional step before calculating both of these losses, which is calculating the probability of an edge using Gumbel-Softmax. The key reason we need to introduce Gumbel-Softmax is that our autoencoder is learning to predict the adjacency matrix representing our edges, that is, the network connectivity, rather than the nodes and their features. Therefore, the end predictions for the autoencoder have to be discrete. However, we’re also inferring a probability. Gumbel-Softmax is a popular approach whenever probability data needs to be made discrete. </p>
</div>
<div class="readable-text intended-text" id="p192">
<p>Here, we have two discrete types of edges, that is, whether something is or isn’t connected. This means that our data is <em>categorical </em>—each edge is either in category 0 (isn’t connected) or category 1 (connected). Gumbel-Softmax is used to draw and score samples from a categorical distribution. In practice, Gumbel-Softmax will approximate the output from our encoder, which comes in the form of log probabilities or <em>logits</em>, as a Gumbel distribution, which is an extreme value distribution. This approximates the continuous distribution of our data as a discrete one (edge types) and allows us to then apply a loss function to the distribution. </p>
</div>
<div class="readable-text intended-text" id="p193">
<p>The temperature of a Gumbel distribution, one of our hyperparameters, reflects the “sharpness” of the distribution, similar to how variance controls the sharpness of a Gaussian distribution. In this chapter, we used a temperature of 0.5, which is about medium sharpness. We also specify <code>Hard</code> as a hyperparameter, which denotes whether one or more categories exist. As discussed, we want it to have two categories when training to represent whether an edge exists. This allows us to approximate the distribution as a continuous one, and then we can backpropagate this through our network as a loss. However, when testing, we can set <code>Hard</code> to <code>True</code>, which means that there is only one category. This makes the distribution fully discrete, meaning we can’t optimize using the loss, as discrete variables are nondifferentiable by definition. This is a useful control to make sure that our test loop doesn’t propagate any gradients.</p>
</div>
<div class="readable-text" id="p194">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p195"> While some systems can use single snapshots of data to make predictions, others need to consider changes in time to avoid errors or vulnerabilities. </li>
<li class="readable-text" id="p196"> Spatiotemporal GNNs consider previous timesteps to model how graphs evolve over time. </li>
<li class="readable-text" id="p197"> Spatiotemporal GNNs can solve pose-estimation problems where we predict the next position of the body given some data on how the body position was in the recent past. In this case, nodes represent sensors placed on body joints, and edges represent the body connections between joints. </li>
<li class="readable-text" id="p198"> Adjacency matrices can be adapted to consider temporal information by concatenating different adjacency matrices along the diagonal. </li>
<li class="readable-text" id="p199"> Memory can be introduced into models, including GNNs, such as by using a recurrent neural network (RNN) or a gated recurrent unit network (GRU). </li>
<li class="readable-text" id="p200"> The neural relational inference (NRI) model combines recurrent networks such as a GRU with autoencoder GNNs. These models can infer temporal patterns, even where adjacency information is unknown. </li>
</ul>
</div></body></html>