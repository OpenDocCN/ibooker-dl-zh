- en: 1 Introducing AI-powered search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is AI-powered search?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding user intent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How AI-powered search works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content and behavioral intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecting an AI-powered search engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The search box has become the default user interface for interacting with data
    in most modern applications. If you think of every major app or website you use
    daily, one of the first things you likely do on each visit is enter a query to
    find the content or actions most relevant to you.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re not explicitly searching, you may instead be consuming streams of
    content customized to your tastes and interests. Whether these be video recommendations,
    items for purchase, prioritized emails, news articles, or other content, you’re
    likely still looking at filtered or ranked results and given the option to either
    page through or explicitly filter the content with your own query.
  prefs: []
  type: TYPE_NORMAL
- en: For most people, the phrase “search engine” brings up thoughts of a website
    like Google, Bing, or Baidu that enables queries based on a crawl of the entire
    public internet. However, the reality is that search is now available in nearly
    all our digital interactions every day across the numerous websites and applications
    we use.
  prefs: []
  type: TYPE_NORMAL
- en: These search engines are far from static. We’re seeing commercial technologies
    like OpenAI’s ChatGPT, Anthropic’s Claude, and Google’s Gemini, as well as hundreds
    of other more open large language models (LLMs), like Meta’s Llama and Mistral’s
    Mixtral, with source code and model weights published for public use. These all
    serve as models of the world’s information that can generate interpretations and
    responses to arbitrary queries. These models are being actively integrated into
    major search engines and will continue to heavily influence the evolution of AI-powered
    search.
  prefs: []
  type: TYPE_NORMAL
- en: While the expected response from a search box may have historically been to
    return “ten blue links”—a list of ranked documents for a user to investigate further
    to find information in response to their query—expectations for the intelligence
    level of search technologies have skyrocketed in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: Users today expect search technology to be
  prefs: []
  type: TYPE_NORMAL
- en: '*Domain-aware*—Search technology should understand the entities, terminology,
    categories, and attributes of each specific use case and corpus of documents,
    not just use generic statistics on strings of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contextual and personalized*—It should be able to take into account user context
    (location, last search, profile, previous interactions, user recommendations,
    and user classification), query context (other keywords, similar searches), and
    domain context (inventory, business rules, domain-specific terminology) to better
    interpret user intent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Conversational*—It should be able to interact in natural language and guide
    users through a multi-step discovery process while learning and remembering relevant
    new information along the way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multi-modal*—It should be able to resolve queries issued by text, voice, images,
    video, or other content types, and to use those queries to also search across
    the other content types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Intelligent*—It should be able to deliver predictive type-ahead and to understand
    what users mean (spelling correction, phrase and attribute detection, intent classification,
    conceptual searching) to deliver the right answers at the right time and to constantly
    get smarter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Assistive*—It should move beyond delivering just links to delivering answers,
    summaries, explanations, and available actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many of these capabilities are enabled by LLMs, while others are driven by analyzing
    user behavior and building domain-specific personalization profiles, knowledge
    graphs, and ranking models.
  prefs: []
  type: TYPE_NORMAL
- en: Search interfaces are also evolving to include more chatbot and conversational
    information discovery sessions as LLMs become more ubiquitous, but even today’s
    best models struggle with hallucinating (making up bad answers) and going off
    the rails unless tethered to an actual information source, such as a search engine
    index, to reliably find and return information from trusted sources. *Retrieval
    augmented generation (RAG)*, the technique of using a search engine or vector
    database as a knowledge source to provide LLMs accurate and up-to-date information
    as context, is one of the most reliable techniques for improving the accuracy
    of generative AI models today.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of AI-powered search is to use automated machine learning techniques
    to deliver on all these desired capabilities. While many organizations start with
    basic text search and spend many years trying to manually optimize synonym lists,
    business rules, ontologies, field weights, and countless other aspects of their
    search configuration, some are beginning to realize that most of this process
    can be automated.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, you’ll learn to implement many key AI-powered search techniques,
    such as
  prefs: []
  type: TYPE_NORMAL
- en: Using LLMs for query interpretation, embeddings, question answering, and results
    summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning LLMs for search and question answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting and using user signals for crowdsourced relevance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signals-boosting models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge graph learning from both signals and content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic knowledge graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query intent classification and query-sense disambiguation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personalized search and recommendations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine-learned ranking (learning to rank)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click models for implicit relevance feedback
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding bias in ranking models through active learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid search and multimodal search across text, images, and mixed content types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic search using both knowledge graphs and LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book is an example-driven guide through the most applicable machine learning
    algorithms and techniques commonly used to build intelligent search systems. We’ll
    not only walk through key concepts but will also provide reusable code examples
    to cover data collection and processing techniques, as well as the self-learning
    query interpretation and relevance strategies employed to deliver AI-powered search
    capabilities across today’s leading organizations—hopefully soon to include your
    own!
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 What is AI-powered search?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prior to November 2022, when OpenAI released ChatGPT to the world as a generalizable
    algorithm that non-technical users could talk with to solve many problems, the
    definition of “artificial intelligence” was a bit nebulous to the general public.
    It was understood to include things like self-driving cars, autonomous robots,
    and other futuristic technologies that made computers appear to be intelligent,
    but AI appeared to many to be more of a marketing buzzword than a well-defined
    term. A more concrete definition has existed in the software industry for years,
    however.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of software development, the term *artificial intelligence* generally
    describes any computer program that can perform a task that previously required
    human intelligence. That program often includes machine learning techniques, allowing
    it to learn from data and improve its performance over time. That said, even rules-based
    systems that do not involve machine learning techniques but generate human-like
    feedback have also traditionally been considered “AI” systems. We’ll adopt this
    more general definition of AI in this book, though we’ll be primarily discussing
    the machine-learning aspects of AI.
  prefs: []
  type: TYPE_NORMAL
- en: The term *search* (or *search engine*) is likewise considered by the general
    public to refer to web search engines like Google or Bing. In software development,
    the term is also used to describe any technology that enables users to query for
    and find information. Search typically involves at least two critical steps—finding
    documents that match a query (*matching*) and then ordering those documents by
    relevance to the query (*ranking*). Search can also include many preprocessing
    steps to better understand the query, and postprocessing steps to extract answers
    or summarize results from the matched documents. Search is often the primary way
    users find information, whether conducting general web search, product search,
    enterprise search, video/image search, or any of hundreds of other common use
    cases for finding and ranking information. It is also the primary way generative
    AI systems quickly find updated factual content to use as context for their prompts.
  prefs: []
  type: TYPE_NORMAL
- en: But what is AI-powered search, and how does it differ from traditional “search”?
    Many buzzwords such as “AI”, “machine learning”, “data science”, and “deep learning”
    are often thrown around interchangeably, and it’s important to understand the
    distinctions and how they overlap with AI-powered search. Figure 1.1 demonstrates
    the important relationships between these related areas.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F01_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 AI-powered search includes all the technologies and techniques at
    the intersection of the fields of search and AI. These overlap heavily with and
    use the fields of data science, machine learning, and deep learning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Machine learning* is a subset of AI that focuses on using data to train models
    to perform tasks based on insights learned from the training data. Deep learning
    is a further subset of machine learning that focuses on training artificial neural
    networks—algorithms that partially mimic the structure of the human brain—to learn
    to solve complex problems. In figure 1.1, notice that deep learning is a fully
    contained subset of machine learning, which is then a fully contained subset of
    artificial intelligence. Data science is a discipline that overlaps heavily with
    AI and search, but it also contains other distinct focus areas, so it is not completely
    a superset or subset of either.'
  prefs: []
  type: TYPE_NORMAL
- en: Our focus in this book is specifically on the intersection of search (also known
    as *information retrieval*) and AI, and in particular on the application of machine
    learning and deep learning techniques to improve the relevance of search results
    and to automate the process of tuning search relevance. Building AI-powered search
    involves many well-known machine learning techniques, but also many that are specific
    to information retrieval and the search domain. Figure 1.2 provides a categorized
    list of some key AI-powered search techniques we’ll cover in this book, broken
    down by whether they are deep learning techniques, other machine learning techniques
    not requiring deep learning, or other artificial intelligence techniques not requiring
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F02_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 Specific AI-powered search techniques, broken down by whether they
    are deep learning techniques, other machine learning techniques not requiring
    deep learning, or other artificial intelligence techniques not requiring machine
    learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the AI-only category, question-answering systems, virtual assistants, chatbots,
    and rules-based relevancy are all examples of AI techniques that are often built
    using machine learning, but which do not *require* machine learning. Many have
    built chatbots based entirely on rules to understand different user utterances
    and intents, and likewise question-answering systems can be built solely on rules
    and ontologies. That said, machine learning is often used to learn these kinds
    of rules and ontologies, so the lines between these categories are often blurred.
  prefs: []
  type: TYPE_NORMAL
- en: When algorithms begin to use data to train models, we enter into the machine
    learning subcategory of AI-powered search. We use behavioral signals from search
    engine users (clicks, likes, add-to-carts, purchases, etc.) to build models that
    can learn to better rank documents. This can include signals-boosting models (top
    documents per query or category), collaborative filtering models that generate
    recommendations or personalize search results, and ranking classifiers (learning
    to rank) that learn from content and behavioral signals to better rank results.
    Machine learning is also used to learn knowledge graphs, which are graphs of entities,
    concepts, and their relationships that can be used to better understand the domain
    and to better interpret user queries. Semantic search (search on meaning, not
    just keywords) can be enabled by such knowledge graphs, along with traditional
    natural language processing approaches, query intent classification, document
    clustering, and other techniques driven by user queries, documents, and user behavioral
    signals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in the deep learning subcategory of AI-powered search, we see the
    use of neural networks to build models that can understand user queries and documents,
    as well as rank and summarize search results. Here, text is used to train LLMs
    to understand the meaning of words and phrases, to generate answers to questions,
    and to generate summaries of documents. LLMs are a type of *foundation model*
    that can interpret text content and are often trained on massive amounts of text
    from the internet. Foundation models can also be trained on other types of content
    beyond just text (images, audio, video) to enable multimodal search across those
    content types: text-to-image search, text-to-audio, image-to-video, and so on.
    LLMs are also used to generate *embeddings*, which are vector representations
    of content that represent the content’s meaning. Since a search engine’s primary
    job is to find and rank content similar to an incoming query, these embeddings
    enable a sophisticated ability to search on a query’s meaning and significantly
    improve query understanding and ranking. Further fine-tuning of foundation models
    on specific goals or domain-specific datasets will also make them significantly
    better at understanding the nuances of those domains or use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models compress a large amount of human knowledge (often much of
    the internet), providing them with a broad understanding across most domains.
    This compression of knowledge, however, is a lossy compression—the original data
    is not stored, and specific facts and concepts can be easily confused. Foundation
    models are well known to hallucinate answers to questions, making them generally
    unreliable for answering factual questions. As a result, in addition to search
    engines using foundation models to improve query understanding and ranking, we’re
    also seeing them used heavily for RAG—where search serves as a knowledge source
    that foundation models can rely on for accurate and up-to-date information as
    context for generative AI tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll cover each of these AI-powered search techniques in detail throughout
    this book. But first, let’s discuss the goals of AI-powered search and how it
    differs from traditional search.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Understanding user intent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To deliver AI-powered search, we’ll need a cohesive understanding of the dimensions
    involved in interpreting user intent and returning content matching that intent.
    Within the field of information retrieval, search engines and recommendation engines
    are the two most popular technologies employed to deliver the relevant content
    required to satisfy users’ information need. Many organizations think of search
    engines and recommendation engines as separate technologies solving different
    use cases. Commonly, different teams within the same organization—often with different
    skill sets—work independently on separate search engines and recommendation engines.
    In this section, we’ll discuss why separating search and recommendations into
    independent functions and teams can often lead to less-than-ideal outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 What is a search engine?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A search engine is typically thought of as a technology for explicitly entering
    queries and receiving a response (figure 1.3). It is usually exposed to end users
    through a text box into which a user can enter keywords or questions. The results
    are often returned in a list, alongside additional filtering options that enable
    further refinement of the initial query. Using this mechanism, search is used
    as a tool for direct discovery of relevant content. When a user is finished with
    their search session, they can usually issue a new query and start with a blank
    slate, ignoring the context of previous searches.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F03_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 A typical search experience, with a user entering a query and seeing
    search results with filtering options to support further refinement of the search
    results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A search engine is one of the most cross-functional kinds of systems within
    the software engineering world. Most underlying search engine technology is designed
    to operate in a massively scalable way, serving large volumes of queries against
    millions, billions, or even trillions of documents, and delivering results in
    hundreds of milliseconds or less. In many cases, real-time processing and near-real-time
    searching on newly ingested data is required, and all of this must be parallelizable
    across numerous servers to scale out and meet such high-performance requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing search engines also requires substantial work building search-specific
    data structures like an inverted index or ANN-based vector store, an understanding
    of linear algebra and vector similarity scoring, experience with text analysis
    and natural language processing, and knowledge of numerous search-specific types
    of data models and capabilities (spell checking, autosuggest, faceting, text highlighting,
    embeddings, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: For a search engine to fully interpret user intent, it’s critical that you combine
    a thorough understanding of your content, your users, and your domain. We’ll revisit
    why this is important after briefly discussing the related topic of recommendation
    engines.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 What do recommendation engines offer?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most people think of recommendation engines (or “recommendation systems”) as
    systems that don’t accept direct user input and instead deliver content based
    upon what the engine learns about them, calculating best matches for their interests
    and behaviors. These interests are inferred in a variety of ways through user
    preferences, user behavior, viewed content, and so on. This lack of direct user
    input for recommendation engines stands in direct contrast with search engines,
    which are traditionally thought of as technology that requires explicit user-driven
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: If you routinely visit Amazon.com or any other major e-commerce website, you
    are no doubt familiar with recommendation engine sections stating that “based
    on your interest in this item, you may also like . . .” or otherwise just recommending
    a list of items based upon your collective browsing and purchase history, like
    the example in figure 1.4\. These recommendations often drive significant revenue
    for companies, and they help customers discover relevant, personalized, and related
    content that often complements what they are searching for explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F04_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 Recommendations based upon users expressing interest in similar items
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Recommendation algorithms can roughly be divided into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Content-based recommenders*—These match based on attributes of items or users'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Behavior-based recommenders*—These match based upon the overlap of interactions
    from similar users with similar items'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multimodal recommenders*—These perform hybrid matching based on both similar
    content-based attributes and overlapping behavior-based interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.2.3 The personalization spectrum between search and recommendations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key difference between search engines and recommendation engines is that
    search engines are typically guided by users and match the users’ explicitly entered
    queries, whereas recommendation engines typically accept no direct user input
    and instead recommend—based upon already-known or inferred knowledge—what a user
    may want to see next.
  prefs: []
  type: TYPE_NORMAL
- en: But these two systems are really two sides of the same coin, and treating them
    as separate systems creates a false dichotomy. The goal, in both cases, is to
    understand what a user is looking for and to deliver relevant results to meet
    that user’s information need. A broad range of personalization capabilities lies
    within the spectrum between search and recommendation systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming you have both explicit queries and a user-specific personalization
    profile available when trying to find content for your end users, you can do any
    of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Traditional keyword search*—Ignore the profile and only use explicit inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Personalized search*—Use the profile implicitly along with other explicit
    user input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*User-guided recommendations*—Use the profile explicitly and provide the user
    with the ability to adjust it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Traditional recommendations*—Use the profile explicitly with no ability for
    a user to adjust it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 1.5 shows this personalization spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F05_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 The personalization spectrum, showing traditional keyword search
    and traditional recommendations as two ends of a larger continuum
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While the two ends of this personalization spectrum represent the extremes,
    they are also the two most common approaches. Unfortunately, one of the biggest
    mistakes we see in many organizations is teams built around the belief that search
    and recommendations are separate problems. This often leads to data science teams
    building complicated personalization and segmentation models only capable of recommendations
    and not search, and engineering teams building large-scale keyword matching engines
    that can’t easily take advantage of the robust models built by the recommendations
    teams.
  prefs: []
  type: TYPE_NORMAL
- en: More often than not, the recommendation teams are staffed by data scientists
    with minimal information retrieval background, and the search teams are often
    staffed by engineers with minimal data science background. Due to Conway’s law
    (“organizations which design systems … are constrained to produce designs which
    are copies of the communication structures of these organizations”), this ultimately
    results in challenges solving problems along the personalization spectrum (particularly
    in the middle) that need the best from both teams. In this book, we focus on the
    shared techniques that make it possible for search to become smarter and for recommendations
    to become more flexible through a unified approach. AI-powered search platforms
    need to be able to continuously learn from both your users and your content and
    then enable your users to guide the results so they continue to improve.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.4 Semantic search and knowledge graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We presented search and recommendations as a personalization spectrum in figure
    1.5, with personalized search and user-guided recommendations in between, but
    there’s one more dimension that is critical for building a good AI-powered search
    system—a deep understanding of the given domain. It’s not enough to match on keywords
    and to recommend content based upon how users collectively interact with documents.
    The engine must also learn as much as it can about the domain. This includes
  prefs: []
  type: TYPE_NORMAL
- en: Learning all the important domain-specific phrases, synonyms, and related terms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying entities in documents and queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a knowledge graph that relates those entities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disambiguating the many nuanced meanings represented by domain-specific terminology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being able to effectively parse, interpret, and conceptually match the nuanced
    intent of users within your domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 1.6 shows an example of semantic parsing of a query, with the goal being
    to search for “things” (known entities) instead of “strings” (just text matching).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F06_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 Semantic parsing of a query, demonstrating an understanding of the
    entities (“things”) represented by query terms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To make their searches smarter, many companies spend considerable money employing
    large teams to manually create dictionaries and knowledge graphs to identify the
    relationships between entities in their users’ queries. This book focuses on a
    more scalable approach: building an AI-powered search engine that can automatically
    learn these relationships continuously. We also dive into additional techniques
    for semantic search, including dense vector search on embeddings and generative
    search using LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.5 Understanding the dimensions of user intent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ve discussed the important roles of traditional keyword search, recommendations,
    and the personalization spectrum in between. We also discussed the need for semantic
    search to provide domain-specific understanding of your content and your users’
    queries. All of these are key pillars of a singular, larger goal: fully understanding
    user intent. Figure 1.7 demonstrates the interplay between each of these key pillars
    of user intent.'
  prefs: []
  type: TYPE_NORMAL
- en: The top-left circle in figure 1.7 represents *content understanding*—the ability
    to find the right content based on keywords, language patterns, and known attribute
    matching. The top-right circle represents *user understanding*—the ability to
    understand each user’s specific preferences and use those to return more personalized
    results. Finally, the lower circle represents *domain understanding*—the ability
    to interpret words, phrases, concepts, entities, and nuanced interpretations and
    relationships between each of these within your own domain-specific context.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F07_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7 The dimensions of user intent: a combination of content understanding,
    user understanding, and domain understanding'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A query only in the content understanding circle represents traditional *keyword
    search*, enabling matching on keywords but without using any domain or user-specific
    context. A query only in the user understanding circle would be recommendations
    from collaborative filtering, with no ability for the user to override the inputs
    and no understanding of the domain or content of the underlying documents. A query
    only in the domain understanding circle might be a structured query on known tags,
    categories, or entities, or even a browse-like interface that allowed for exploration
    of a *knowledge graph* of these domain-specific entities and their relationships,
    but without any user-specific personalization or ability to find arbitrary terms,
    phrases, and content.
  prefs: []
  type: TYPE_NORMAL
- en: 'When traditional keyword search and recommendations overlap, we get *personalized
    search* or guided recommendations. When traditional keyword search and knowledge
    graphs overlap, we get *semantic search*: a smart, domain-specific search experience.
    Finally, when recommendations and knowledge graphs overlap, we get smarter *domain-aware
    recommendations* that match on crowdsourced user interactions across similar documents
    and also on a domain-specific understanding of the important attributes of those
    documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The holy grail for AI-powered search is to harness the intersection of all
    three categories: semantic search, personalized search, and domain-aware recommendations.
    That is to say, to truly understand user intent, we need all of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An expert understanding of the domain the user is searching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An expert understanding of the user and their preferences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An expert ability to match and rank arbitrary queries against any content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI-powered search starts with the three pillars of user intent (content, domain,
    and user), and then employs intelligent algorithms to constantly learn and improve
    in each of these areas. This learning includes techniques like automatically learning
    ranking criteria, automatically learning user preferences, and automatically learning
    knowledge graphs and language models of the represented domain. At the end of
    the day, a balanced combination of these three approaches provides the key to
    optimal understanding of users and their query intent, which is the end goal of
    our AI-powered search system.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 How does AI-powered search work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We laid out our end goal of matching user intent through content understanding,
    user understanding, and domain understanding. With that background established,
    let’s wrap up this chapter with an overview of the actual components needed to
    deliver an AI-powered search platform. Search intelligence typically matures along
    a predictable progression iteratively over time, as shown in figure 1.8\. Basic
    keyword search is a typical starting point for organizations. Once in production,
    they realize their search relevancy needs to be improved, and they start manually
    tuning field weights, boosts, text and language analysis, and introducing additional
    features and functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F08_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 The typical search intelligence progression, from basic keyword search
    to a full self-learning search platform
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Eventually, they realize they need to inject domain understanding into their
    search capabilities, at which point organizations begin to invest in synonym lists,
    taxonomies, lists of known entities, and domain-specific business rules. While
    these all help, organizations eventually also discover that relevant search is
    very much dependent upon successfully interpreting user queries and understanding
    user intent, so they begin investing in techniques for query classification, semantic
    query parsing, knowledge graphs, personalization, and other attempts to correctly
    interpret user queries.
  prefs: []
  type: TYPE_NORMAL
- en: Because these tasks yield improvements, this success often results in the creation
    of large teams investing significant time manually tuning lists and parameters,
    and eventually organizations may realize that it is possible (and more expedient)
    to automate as much of that process as possible through learning from user signals,
    user testing (A/B testing, offline relevancy simulations, and active learning),
    and building of machine-learned relevancy models. The end goal is to completely
    automate each of these steps along the search intelligence progression and enable
    the engine to be self-learning.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 The core search foundation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in building a search platform is almost always to get traditional
    keyword search working (the “content understanding” part in figure 1.7). Teams
    often spend years tuning and improving this step, and a whole discipline called
    *relevance engineering* has arisen that has historically focused significant effort
    into understanding content; improving content for search; adjusting boosts, query
    parameters, and query functions; and otherwise trying to maximize the relevance
    of the traditional search experience. For a deep dive into this world of relevance
    engineering and tuning traditional keyword search relevance, we recommend the
    book *Relevant Search* by Doug Turnbull and John Berryman (Manning, 2016).
  prefs: []
  type: TYPE_NORMAL
- en: As relevance engineers become more sophisticated, their work often moves into
    the realms of user understanding and recommendations, as well as into domain-understanding
    and semantic search. The rise of large language models has made it easy in recent
    years to implement out-of-the-box semantic search, but getting to the next level
    in optimizing the relevance and matching requires much more sophisticated approaches,
    as you’ll learn throughout this book. Our focus in *AI-Powered Search* will be
    on automating the process of learning and optimizing search relevance so it operates
    as a continuous feedback loop. We essentially want to automate much of the relevance
    engineer’s job, relying on algorithms, where possible, to continually learn optimal
    matching and ranking strategies.
  prefs: []
  type: TYPE_NORMAL
- en: So, what characteristics differentiate a well-tuned search engine from an AI-powered
    search engine? A well-tuned search engine is the foundation upon which AI-powered
    search is built, but AI-powered search goes far beyond that, continuously learning
    and improving through reflected intelligence. *Reflected intelligence* is the
    idea of using continual feedback loops of user input, content updates, and user
    interactions with content to continually learn and improve the quality of your
    search application.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 Reflected intelligence through feedback loops
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Feedback loops are critical to building an AI-powered search solution. Imagine
    if your entire education (elementary school through to your highest degree) had
    consisted of nothing more than you reading textbooks: no teachers to ask questions,
    no exams to test your knowledge and provide feedback, and no classmates or others
    with which to interact, study, or collaborate. You would have probably hit endless
    walls where you were unable to fully grasp certain concepts or even understand
    what you were reading, and you would have understood many ideas incorrectly and
    never had the opportunity to realize this or to adjust your assumptions.'
  prefs: []
  type: TYPE_NORMAL
- en: Search engines often operate this same way. Smart engineers push data to the
    search engine and tune certain features and feature weights, but the engine just
    reads those configurations and acts upon them the same way every time for repeated
    user queries. Search engines are the perfect kind of system for interactive learning,
    however, when we introduce feedback loops.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.9 shows the typical flow of information through a search feedback loop.
    First, a user issues a query. This query executes a search, which returns results,
    such as a specific answer, a list of answers, or a list of links to pages, to
    an end user. Once presented with the list, the user then takes one or more actions.
    These actions usually start with clicks on documents, but those clicks can ultimately
    lead to adding an item to a shopping cart and purchasing it (e-commerce), giving
    the item a thumbs up or thumbs down (media consumption website), liking or commenting
    on the result (social media website), or any number of other context-specific
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F09_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 Reflected intelligence through feedback loops
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These actions can then be used to generate an improved relevance ranking model
    for future searches. Your search application can automatically adjust the ranking
    of future search results, delivering an improved search experience for the next
    user’s search.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.3 Signals boosting, collaborative filtering, and learning to rank
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The searches, clicks, likes, add to carts, purchases, comments, and other interactions
    with your search application are critical data that you need to capture. We collectively
    refer to these data points as *signals*. Signals provide a constant stream of
    feedback to your search application, recording every meaningful interaction with
    your end users. These digital moments can then be used by machine learning algorithms
    to generate models to power user understanding, content understanding, and domain
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.10 shows the data flow for the collection and processing of signals
    in a typical AI-powered search application. You can see signals being collected
    for each search, as well as resulting clicks and purchases. Unique signals can
    also be recorded for any other kind of user interaction (add-to-cart, facet click,
    bookmark, hover, or even page dwell time).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F10_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 Signal collection and processing data flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Signals are one of the two sources of data that power the intelligence engine
    of an AI-powered search application, with the other being content. Many AI-powered
    search algorithms incorporate signals feedback loops to build reflected intelligence
    models. Some of these key types of reflected intelligence algorithms include
  prefs: []
  type: TYPE_NORMAL
- en: Popularized relevance—*Signals-boosting* algorithms create models that use aggregated
    signals to boost the rankings of the most important documents for your most popular
    queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personalized relevance—*Collaborative filtering* algorithms create models using
    matrix factorization or similar techniques that use signals to generate recommendations
    and user profiles to personalize search results for each user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized relevance—*Learning to rank* algorithms train *ranking classifiers*
    to perform machine-learned ranking based on relevance judgments generated from
    user-signals-based click models. This process learns a set of features and ranking
    weights that can be applied generally to all queries—even ones that have not been
    previously seen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These algorithms enable your search application to learn from user interactions
    and to automatically adjust rankings for future search results, delivering an
    improved search experience for the next users’ searches.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.4 Content and domain intelligence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While signals provide a constant stream of usage and feedback data to your search
    application, your content is also a rich source of information that can be incorporated
    in your feedback loops. For example, if someone searches for a particular keyword,
    the other keywords and top categories in the documents returned serve as valuable
    data points. Those data points can be used to tag or categorize the query and
    can be shown to other end users (as facets, for example), leading to further interactions
    that generate signals from which the engine can learn.
  prefs: []
  type: TYPE_NORMAL
- en: The content of your documents forms a representative textual model of your domain.
    Entities, domain-specific terminology, and the sentences contained within your
    documents serve as a rich, semantic graph. That graph can be utilized to drive
    powerful conceptual and semantic search that better understands your domain. We’ll
    dive more deeply into understanding your content in chapter 2, and into semantic
    search capabilities using this rich semantic knowledge graph (SKG) in chapter
    5\.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, LLMs have revolutionized how search engines can interpret queries
    and responses. LLMs are deep neural networks trained on massive amounts of text
    data. They can recognize, translate, summarize, predict, and generate new data
    based on incoming prompts and any additional context provided. Often, LLMs are
    trained on text, receive a prompt as text, and return a response as text, though
    similar multimodal models can also be trained on images, audio, other data, or
    all the above. LLMs often contain billions of parameters within the neural network,
    and this number is likely to continue to grow in the future so long as model performance
    continues to improve with more parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Today’s most successful LLMs are based on the Transformer architecture, introduced
    by Google researchers in 2017, which applies the concept of “attention” to language
    learning (“Attention is All You Need”, Ashish Vaswani et al.). Massive amounts
    of textual data are fed into a neural network, and a representation of the words
    and their relationships within each context are modeled using unsupervised learning.
    Once the model is built, it’s able to interpret an incoming string of text, a
    *prompt*, as a context and to encode the context into embeddings, which are numerical
    vector representations of the meaning of the prompt. In addition to being able
    to encode prompts into embeddings, Transformers also contain a decoder layer,
    which can convert embeddings back into text. Transformers can be used to solve
    many kinds of problems, from similarity search on embeddings (text search, image
    search, etc.), to question answering, to classification, to summarization of content,
    and even to generation of new content (writing, code, poems, images, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers are context-sensitive. An LLM tuned for question answering might
    respond to the prompt “What is the difference between a capital and capitol?”
    with the answer “A capital is a city or town that serves as the seat of government
    for a state or country. A capitol is a building in which a state legislature meets.”
    However, the same LLM may respond to the question “What is the difference between
    a capital and lowercase word?” with the following context-based answer: “The difference
    between a capital and lowercase word is that a capital letter is used at the beginning
    of a sentence or proper nouns, while a lowercase letter is used for all other
    letters in a word.”'
  prefs: []
  type: TYPE_NORMAL
- en: Many LLMs are open sourced, but for optimal output quality, LLMs benefit from
    being fine-tuned for the task at hand with domain-specific content and prompts.
    Fine-tuning is the act of taking a pretrained model, which already has a strong
    general understanding of language and general concepts, and “teaching” it about
    new content and tasks. The original pretrained models are often referred to as
    *foundation models*, as they form the foundation upon which the domain-specific
    fine-tuning will be applied. The process of fine-tuning usually takes a small
    fraction of the time necessary to train the original LLM. Some LLMs have been
    trained on so much data and such a wide variety of data (such as a comprehensive
    web crawl of the internet) that they can perform quite well without retraining,
    but retraining for the task at hand almost always improves performance.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.5 Generative AI and retrieval augmented generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative AI is accelerating at a rapid pace, and search engines both benefit
    from it and serve as a key component of generative AI systems. LLMs (and other
    foundation models) serve as reasoning engines, having enough knowledge of the
    world to interpret language and generally reason about most concepts, but without
    the ability to reliably recall factual information without the risk of hallucinating
    (making up false information).
  prefs: []
  type: TYPE_NORMAL
- en: As a result, search engines are used in retrieval augmented generation (RAG)
    pipelines as a knowledge source for LLMs, allowing relevant context to be retrieved
    and passed to the LLM to ensure it has up-to-date and accurate data from which
    to answer. This entire book is effectively about using AI to optimize the “retrieval”
    part of RAG, and we’ll cover the “generative” part in chapter 15\.
  prefs: []
  type: TYPE_NORMAL
- en: While RAG makes search engines a critical component of generative AI systems,
    LLMs also serve as critical components of search engines. LLMs can be used to
    interpret queries, generate embeddings for vector search, generate summaries of
    search results, and even generate answers to questions directly from search results.
  prefs: []
  type: TYPE_NORMAL
- en: The transition from traditional information retrieval to these new *generative
    search* capabilities is shown in figure 1.11\. For decades, traditional search
    has returned a list of search results (“ten blue links”), showing the top-ranked
    documents most relevant for a query. For queries on entities and well-known topics,
    search engines often show precalculated info boxes with summary information or
    show predetermined answers to known questions. Search engines often also extract
    words, sentences, or paragraph snippets out of search results to answer questions
    instead of forcing users to open and read the search results to find the answer.
    This process is known as *extractive question answering*, and it is a more targeted
    form of search, since it additionally searches and ranks answers found within
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F11_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 The transition from traditional information retrieval to generative
    search
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, there’s a fine line between extracting answers from search results
    and synthesizing new content to return in the results, and this is where we transition
    into the realm of generative search. *Results summarization* is the process of
    rewriting search results into a more concise and readable format, often combining
    information from multiple sources and even providing citations for the sources
    within the summarized response. *Abstractive question answering* is the process
    of generating answers to questions by synthesizing information from one or more
    ranked search results into an answer to a user’s question. The difference between
    extractive question answering and abstractive question answering is that extractive
    question answering finds relevant content within documents to return as answers
    (“extracting it”), whereas abstractive question answering writes a synthesized
    response by interpreting results and generating an answer that may look different
    than what’s written in any of the documents. *New content generation* is also
    possible within a generative search experience, such as responding to queries
    with creative new prose, code, poems, images, or other content, based on the keywords
    or prompts being submitted by users.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, generative AI and AI-powered search are tightly intertwined. Generative
    AI is a critical component of “AI-powered search” (powering answer generation
    and results summarization), and AI-powered search is a critical component of “search-powered
    AI” (RAG). Both heavily utilize LLMs and other foundation models, and both are
    critical components of intelligent and accurate AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.6 Curated vs. black-box AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like LLMs, many modern AI techniques rely heavily on deep learning based on
    artificial neural networks. Unfortunately, it is often challenging for a human
    to understand the specific factors that go into any particular prediction or output
    from a deep learning model due to the internal complexity of the learned model.
  prefs: []
  type: TYPE_NORMAL
- en: This sometimes results in a “black-box AI” system, where the results may be
    correct or impressive, but they are not easy to debug or correct when the model
    makes an incorrect judgment. An entire field of *explainable AI* (sometimes called
    *interpretable AI* or *transparent AI*) has arisen out of a need to be able to
    understand, curate, and trust these models.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we’ll cover deep learning approaches to search, such as dense
    vector search on embeddings, question answering, synthetic training data generation,
    and results summarization using LLMs. We’ll mostly focus our efforts, however,
    on creating intelligence that can be expressed in human terms and then corrected
    and augmented by human intelligence. You can think of this as “AI-assisted human
    curation”, or as “human-assisted AI”, but either way, the overriding philosophy
    of this book is to use AI to automate the process of search intelligence while
    keeping the human in the loop with the ability to take control and augment or
    override the system.
  prefs: []
  type: TYPE_NORMAL
- en: As a learning exercise, this approach also leads to a deeper, intuitive understanding
    of how search ranking and relevance work, and how you can integrate many different
    AI-driven approaches without forfeiting control of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.7 Architecture for an AI-powered search engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The architecture for an AI-powered search engine often requires numerous building
    blocks to be assembled to form a smart end-to-end system. You start with a core
    search engine like Apache Solr, OpenSearch, or one of the other search engines
    or vector databases identified in appendix B. You then feed your searchable content
    into the engine, running various transformations to make it more useful. These
    index-time transformations might include changes like these:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the meaning of your documents into embeddings using LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the document, adding the classification as a field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing field values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting entities from text, adding entities in separate fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering content, adding clusters as a field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting and annotating phrases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pulling in additional data from a knowledge graph, external API, or other data
    source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing part of speech (POS) detection and other natural language processing
    steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting facts (such as RDF triples)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying other machine learning models or ETL rules to enrich the document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the data is in the engine, your goal is to make it available for searching.
    This requires query pipelines, which can interpret incoming queries; identify
    concepts, phrases, and entities; correct misspellings; expand the query to include
    related terms, synonyms, concepts, or embedding representations; and then rewrite
    the query so your core engine can find the most relevant results. Individual search
    documents may then be returned to the end user, summaries of results may be generated
    from language models, or answers may be explicitly extracted from the results.
  prefs: []
  type: TYPE_NORMAL
- en: Much of this query intelligence requires a robust understanding of your domain,
    however. This requires running batch jobs on your content and user signals to
    learn patterns and derive domain-specific intelligence. What are the most common
    misspellings from your users, and what do they choose as the correct spelling
    among multiple candidates? When a user searches for specific queries, which documents
    should be boosted as the most popular? For unknown queries, what is the ideal
    ranking among all the attributes or features available for matching?
  prefs: []
  type: TYPE_NORMAL
- en: We need access to most of these answers at query time (either precomputed or
    quickly computable) because we expect queries to return within milliseconds to
    seconds. This requires a job processing framework (we use Apache Spark in this
    book) and a workflow scheduling mechanism to keep the jobs running in sequence.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll also need a mechanism for collecting the constant stream of incoming
    user signals (capturing them on the frontend application and then storing them
    in your search engine or other backend datastore).
  prefs: []
  type: TYPE_NORMAL
- en: The signals will then be used to generate all kinds of models—from signals boosting
    models that boost the most popular items for top queries, to learning to rank
    models that apply a generalizable ranking function to all queries, to personalization
    models that output user-specific recommendations and personalization preferences
    for each user or segment of users.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI-powered search is way more than just using the latest LLM to interpret queries.
    It’s about engineering an end-to-end system for continuous learning. Ultimately,
    you’ll end up with a system that receives constant streams of document changes
    and user signals, continually processes those streams to improve models, and then
    constantly adjusts future search results and measures the effect of changes in
    order to deliver more intelligent results. That is the key behind AI-powered search:
    implementing a process of continual learning and improvement based upon real user
    interactions, updating content patterns, and evolving models to optimally understand
    current user intent and to deliver an ever-improving search experience.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Expectations for search sophistication are evolving with the rise of LLMs, with
    end users expecting search to now be domain-aware, contextual and personalized,
    conversational, multimodal, intelligent, and assistive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search and recommendations are the two extreme ends of a continuous personalization
    spectrum within information retrieval, and it’s important to consider the opportunities
    in between to optimize relevance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correctly interpreting user intent requires simultaneous understanding of your
    content, your user and their preferences, and the knowledge domain in which your
    platform operates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimal search relevance lies at the intersection of personalized search (traditional
    keyword search plus collaborative recommendations), semantic search (traditional
    keyword search plus knowledge graphs), and domain-aware recommendations (collaborative
    recommendations plus knowledge graphs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AI-powered search operates on and learns from two key types of data: content
    and user signals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search and generative AI go hand in hand. Generative search capabilities, such
    as RAG, are a critical component of modern generative AI systems (to prevent hallucinations);
    and generative AI capabilities, such as results summarization, are critical components
    of modern search engines (to return better answers).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflected intelligence—the use of feedback loops to continually collect signals,
    tune results, and measure improvements—is the engine that enables AI-powered search
    to learn and constantly improve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
