- en: Chapter 12\. Memory Augmented Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬12ç« ã€‚è®°å¿†å¢å¼ºç¥ç»ç½‘ç»œ
- en: '[Mostafa Samir](https://mostafa-samir.github.io)'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mostafa Samir](https://mostafa-samir.github.io)'
- en: So far weâ€™ve seen how effective an RNN can be at solving a complex problem like
    machine translation. However, weâ€™re still far from reaching its full potential!
    In [ChapterÂ 9](ch09.xhtml#ch07) we mentioned that itâ€™s theoretically proven that
    the RNN architecture is a universal functional representer; a more precise statement
    of the same result is that RNNs areÂ *Turing complete*. This simply means that
    given proper wiring and adequate parameters, an RNN can learn to solve any computable
    problem, which is basically any problem that can be solved by a computer algorithm
    or, equivalently, a Turing machine.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°RNNåœ¨è§£å†³åƒæœºå™¨ç¿»è¯‘è¿™æ ·çš„å¤æ‚é—®é¢˜æ—¶æœ‰å¤šä¹ˆæœ‰æ•ˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ç¦»å…¶å……åˆ†æ½œåŠ›è¿˜æœ‰å¾ˆè¿œï¼åœ¨[ç¬¬9ç« ](ch09.xhtml#ch07)ä¸­ï¼Œæˆ‘ä»¬æåˆ°RNNæ¶æ„åœ¨ç†è®ºä¸Šè¢«è¯æ˜æ˜¯ä¸€ä¸ªé€šç”¨çš„åŠŸèƒ½è¡¨è¾¾å™¨ï¼›åŒä¸€ç»“æœçš„æ›´ç²¾ç¡®é™ˆè¿°æ˜¯RNNæ˜¯*Turingå®Œå…¨*çš„ã€‚è¿™åªæ˜¯æ„å‘³ç€ï¼Œç»™å®šé€‚å½“çš„æœ‰çº¿å’Œå……åˆ†çš„å‚æ•°ï¼ŒRNNå¯ä»¥å­¦ä¼šè§£å†³ä»»ä½•å¯è®¡ç®—é—®é¢˜ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä»»ä½•å¯ä»¥é€šè¿‡è®¡ç®—æœºç®—æ³•æˆ–ç­‰æ•ˆåœ°å›¾çµæœºè§£å†³çš„é—®é¢˜ã€‚
- en: Neural Turing Machines
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¥ç»å›¾çµæœº
- en: Though theoretically possible, itâ€™s extremely difficult to achieve that kind
    of universality in practice. This difficulty stems from the fact that weâ€™re looking
    at an immensely huge search space of possible wirings and parameter values of
    RNNs, a space so vastly large for gradient descent to find an appropriate solution
    for any arbitrary problem. However, in this chapter weâ€™ll start exploring some
    approaches at the edge of research that will allow us to start tapping into that
    potential.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡åœ¨ç†è®ºä¸Šå¯èƒ½ï¼Œä½†åœ¨å®è·µä¸­å®ç°è¿™ç§æ™®éæ€§éå¸¸å›°éš¾ã€‚è¿™ç§å›°éš¾æºäºæˆ‘ä»¬æ­£åœ¨çœ‹ä¸€ä¸ªå·¨å¤§çš„å¯èƒ½æ€§æœ‰çº¿å’ŒRNNå‚æ•°å€¼çš„æœç´¢ç©ºé—´ï¼Œè¿™ä¸ªç©ºé—´å¯¹äºæ¢¯åº¦ä¸‹é™æ¥æ‰¾åˆ°ä»»æ„é—®é¢˜çš„é€‚å½“è§£å†³æ–¹æ¡ˆæ¥è¯´æ˜¯å¦‚æ­¤ä¹‹å¤§ã€‚ç„¶è€Œï¼Œåœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å¼€å§‹æ¢ç´¢ä¸€äº›æ¥è¿‘ç ”ç©¶è¾¹ç¼˜çš„æ–¹æ³•ï¼Œè¿™å°†ä½¿æˆ‘ä»¬å¼€å§‹åˆ©ç”¨è¿™ç§æ½œåŠ›ã€‚
- en: 'Letâ€™s think for a while about a very simple reading comprehension question
    like the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ€è€ƒä¸€ä¸‹ä¸€ä¸ªéå¸¸ç®€å•çš„é˜…è¯»ç†è§£é—®é¢˜ï¼Œå°±åƒä¸‹é¢è¿™æ ·ï¼š
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The answer is so trivial: itâ€™s two. But what actually happened in our brains
    that allowed us to come up with the answer so trivially? If we thought about how
    we could solve that comprehension question using a simple computer program, our
    approach would probably go like this:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆæ˜¯å¦‚æ­¤å¾®ä¸è¶³é“ï¼šæ˜¯ä¸¤ã€‚ä½†æ˜¯æˆ‘ä»¬çš„å¤§è„‘å®é™…ä¸Šå‘ç”Ÿäº†ä»€ä¹ˆï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå¦‚æ­¤è½»æ¾åœ°å¾—å‡ºç­”æ¡ˆï¼Ÿå¦‚æœæˆ‘ä»¬è€ƒè™‘å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªç®€å•çš„è®¡ç®—æœºç¨‹åºæ¥è§£å†³é‚£ä¸ªç†è§£é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯èƒ½ä¼šæ˜¯è¿™æ ·çš„ï¼š
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It turns out that our brains tackle the same task in a similar way to that simple
    computer program. Once we start reading, we start allocating memory (just as our
    computer program) and store the pieces of information we receive. We start by
    storing that location of Mary, which after the first sentence is the hallway.
    In the second sentence we store the objects Mary is carrying, and by now itâ€™s
    only a glass of milk. Once we see the third sentence, our brain modifies the first
    memory location to point to the office. By the end of the fourth sentence, the
    second memory location is modified to include both the milk and the apple. When
    we finally encounter the question, our brains quickly query the second memory
    location and count the information there, which turns out to be two. In neuroscience
    and cognitive psychology, such a system of transient storing and manipulation
    of information is called a *working memory*, and itâ€™s the main inspiration behind
    the line of research weâ€™ll be discussing in the rest of this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®è¯æ˜ï¼Œæˆ‘ä»¬çš„å¤§è„‘ä»¥ä¸é‚£ä¸ªç®€å•çš„è®¡ç®—æœºç¨‹åºç±»ä¼¼çš„æ–¹å¼å¤„ç†ç›¸åŒçš„ä»»åŠ¡ã€‚ä¸€æ—¦æˆ‘ä»¬å¼€å§‹é˜…è¯»ï¼Œæˆ‘ä»¬å°±å¼€å§‹åˆ†é…å†…å­˜ï¼ˆå°±åƒæˆ‘ä»¬çš„è®¡ç®—æœºç¨‹åºä¸€æ ·ï¼‰ï¼Œå¹¶å­˜å‚¨æˆ‘ä»¬æ”¶åˆ°çš„ä¿¡æ¯ç‰‡æ®µã€‚æˆ‘ä»¬é¦–å…ˆå­˜å‚¨ç›ä¸½çš„ä½ç½®ï¼Œç¬¬ä¸€å¥è¯åæ˜¯èµ°å»Šã€‚åœ¨ç¬¬äºŒå¥ä¸­ï¼Œæˆ‘ä»¬å­˜å‚¨ç›ä¸½æºå¸¦çš„ç‰©å“ï¼Œåˆ°ç›®å‰ä¸ºæ­¢åªæœ‰ä¸€æ¯ç‰›å¥¶ã€‚ä¸€æ—¦æˆ‘ä»¬çœ‹åˆ°ç¬¬ä¸‰å¥ï¼Œæˆ‘ä»¬çš„å¤§è„‘ä¿®æ”¹ç¬¬ä¸€ä¸ªå†…å­˜ä½ç½®ï¼ŒæŒ‡å‘åŠå…¬å®¤ã€‚åˆ°ç¬¬å››å¥ç»“æŸæ—¶ï¼Œç¬¬äºŒä¸ªå†…å­˜ä½ç½®è¢«ä¿®æ”¹ä¸ºåŒ…æ‹¬ç‰›å¥¶å’Œè‹¹æœã€‚å½“æˆ‘ä»¬æœ€ç»ˆé‡åˆ°é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬çš„å¤§è„‘è¿…é€ŸæŸ¥è¯¢ç¬¬äºŒä¸ªå†…å­˜ä½ç½®ï¼Œå¹¶è®¡ç®—é‚£é‡Œçš„ä¿¡æ¯ï¼Œç»“æœæ˜¯ä¸¤ä¸ªã€‚åœ¨ç¥ç»ç§‘å­¦å’Œè®¤çŸ¥å¿ƒç†å­¦ä¸­ï¼Œè¿™ç§ä¿¡æ¯çš„ç¬æ—¶å­˜å‚¨å’Œæ“ä½œç³»ç»Ÿè¢«ç§°ä¸º*å·¥ä½œè®°å¿†*ï¼Œè¿™æ˜¯æˆ‘ä»¬å°†åœ¨æœ¬ç« å‰©ä½™éƒ¨åˆ†è®¨è®ºçš„ç ”ç©¶çº¿çš„ä¸»è¦çµæ„Ÿæ¥æºã€‚
- en: In 2014, Graves et al. from Google DeepMind started this line of work in a paper
    called [â€œNeural Turing Machinesâ€](https://arxiv.org/abs/1410.5401)Â in which they
    introduced a new neural architecture with the same name, a *Neural Turing Machine*
    (NTM), that consists of a controller neural network (usually an RNN) with an external
    memory that resembles the brainâ€™s working memory. For the close resemblance between
    the working memory model and the computer model we just saw, [FigureÂ 12-1](#architecture_of_a_modern_day_computer)
    shows that the same resemblance holds for the NTM architecture, with the external
    memory in place of the RAM, the read/write heads in place of the read/write buses,
    and the controller network in place of the CPU, except for the fact that the controller
    learns its program, unlike the CPU, which is fed its program. [FigureÂ 12-1](#architecture_of_a_modern_day_computer)
    has a single read head and a single write head, but an NTM can have several in
    practice.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2014å¹´ï¼Œæ¥è‡ªGoogle DeepMindçš„Gravesç­‰äººåœ¨ä¸€ç¯‡åä¸º[â€œç¥ç»å›¾çµæœºâ€](https://arxiv.org/abs/1410.5401)çš„è®ºæ–‡ä¸­å¼€å§‹äº†è¿™ä¸€å·¥ä½œçº¿ï¼Œä»–ä»¬ä»‹ç»äº†ä¸€ç§åŒåçš„æ–°ç¥ç»æ¶æ„ï¼Œ*ç¥ç»å›¾çµæœº*ï¼ˆNTMï¼‰ï¼Œå®ƒç”±ä¸€ä¸ªæ§åˆ¶å™¨ç¥ç»ç½‘ç»œï¼ˆé€šå¸¸æ˜¯RNNï¼‰å’Œä¸€ä¸ªç±»ä¼¼å¤§è„‘å·¥ä½œè®°å¿†çš„å¤–éƒ¨å­˜å‚¨å™¨ç»„æˆã€‚ç”±äºå·¥ä½œè®°å¿†æ¨¡å‹å’Œæˆ‘ä»¬åˆšåˆšçœ‹åˆ°çš„è®¡ç®—æœºæ¨¡å‹ä¹‹é—´çš„å¯†åˆ‡ç›¸ä¼¼æ€§ï¼Œ[å›¾12-1](#architecture_of_a_modern_day_computer)æ˜¾ç¤ºäº†NTMæ¶æ„çš„ç›¸åŒç›¸ä¼¼æ€§ï¼Œå¤–éƒ¨å­˜å‚¨å™¨å–ä»£äº†RAMï¼Œè¯»/å†™å¤´å–ä»£äº†è¯»/å†™æ€»çº¿ï¼Œæ§åˆ¶å™¨ç½‘ç»œå–ä»£äº†CPUï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯æ§åˆ¶å™¨å­¦ä¹ å…¶ç¨‹åºï¼Œè€ŒCPUåˆ™è¢«è¾“å…¥å…¶ç¨‹åºã€‚[å›¾12-1](#architecture_of_a_modern_day_computer)æœ‰ä¸€ä¸ªè¯»å¤´å’Œä¸€ä¸ªå†™å¤´ï¼Œä½†åœ¨å®è·µä¸­ï¼ŒNTMå¯ä»¥æœ‰å‡ ä¸ªã€‚
- en: '![](Images/fdl2_1201.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1201.png)'
- en: Figure 12-1\. Comparing the architecture of a modern-day computer, which is
    fed its program (left) to an NTM that learns its program (right)
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾12-1ã€‚æ¯”è¾ƒç°ä»£è®¡ç®—æœºçš„æ¶æ„ï¼Œå®ƒè¢«è¾“å…¥å…¶ç¨‹åºï¼ˆå·¦ï¼‰åˆ°ä¸€ä¸ªå­¦ä¹ å…¶ç¨‹åºçš„NTMï¼ˆå³ï¼‰
- en: 'If we thought about NTMs in light of our earlier discussion of RNNâ€™s Turing
    completeness, weâ€™ll find that augmenting the RNN with an external memory for transient
    storage prunes a large portion out of that search space, as we now donâ€™t care
    about exploring RNNs that can both process and store the information; weâ€™re just
    looking for the RNNs that can process the information stored outside of them.
    This pruning of the search space allows us to start tapping into some of the RNN
    potentials that were locked away before augmenting it with a memory, evident by
    the variety of tasks that the NTM could learn: from copying input sequences after
    seeing them, to emulating N-gram models, to performing a priority sort on data.
    Weâ€™ll even see by the end of the chapter how an extension to the NTM can learn
    to do reading comprehension tasks like the one we saw earlier, with nothing more
    than a gradient-based search.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Attention-Based Memory Access
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be able to train an NTM with a gradient-based search method, we need to make
    sure that the whole architecture is differentiable so that we can compute the
    gradient of some output loss with respect to the modelâ€™s parameters that process
    the input. This property is called *end-to-end-differentiable*, with one end being
    the inputs and the other the outputs. If we attempted to access the NTMâ€™s memory
    in the same way a digital computer accesses its RAM, via discrete values of addresses,
    the discreteness of the addresses would introduce discontinuities in gradients
    of the output, and hence we would lose the ability to train the model with a gradient-based
    method. We need a continuous way to access the memory while being able to â€œfocusâ€
    on a specific location in it. This kind of continuous focusing can be achieved
    via attention methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of generating a discrete memory address, we let each head generate
    a normalized softmax attention vector with the same size as the number of memory
    locations. With this attention vector, weâ€™ll be accessing all the memory locations
    at the same time in a blurry manner, with each value in the vector telling us
    how much weâ€™re going to focus on the corresponding location, or how likely weâ€™re
    going to access it. For example, to read a vector at a time step *t* out of our
    <math alttext="upper N times upper W"><mrow><mi>N</mi> <mo>Ã—</mo> <mi>W</mi></mrow></math>
    Â NTMâ€™s memory matrix denoted by <math alttext="upper M Subscript t"><msub><mi>M</mi>
    <mi>t</mi></msub></math> Â (where *N*Â is the number of locations and <math alttext="upper
    W"><mi>W</mi></math> Â is the size of the location), we generate an attention vector,
    or a weighting vector <math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>
    Â of size *N*, and our read vector can be calculated via the product:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold r Subscript t Baseline equals upper M Subscript t Superscript
    down-tack Baseline w Subscript t"><mrow><msub><mi>ğ«</mi> <mi>t</mi></msub> <mo>=</mo>
    <msubsup><mi>M</mi> <mi>t</mi> <mi>âŠ¤</mi></msubsup> <msub><mi>w</mi> <mi>t</mi></msub></mrow></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold r Subscript t Baseline equals upper M Subscript t Superscript
    down-tack Baseline w Subscript t"><mrow><msub><mi>ğ«</mi> <mi>t</mi></msub> <mo>=</mo>
    <msubsup><mi>M</mi> <mi>t</mi> <mi>âŠ¤</mi></msubsup> <msub><mi>w</mi> <mi>t</mi></msub></mrow></math>
- en: where <math alttext="Superscript down-tack"><msup><mi>âŠ¤</mi></msup></math> Â denotes
    the matrix transpose operation.Â [FigureÂ 12-2](#demo_of_blurry_attention_based_reading)
    shows how with the weights attending to a specific location, we can retrieve a
    read vector that approximately contains the same information as the content of
    that memory location.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1202.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. A demonstration of how a blurry attention-based reading can retrieve
    a vector containing approximately the same information as in the focused-on location
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A similar attention weighting method is used for the write head:Â a weighting
    vector <math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>
    is generated and used for erasing specific information from the memory,Â as specified
    by the controller in an erase vector <math alttext="e Subscript t"><msub><mi>e</mi>
    <mi>t</mi></msub></math> Â that has <math alttext="upper W"><mi>W</mi></math> values
    between 0 and 1 specifying what to erase and what to keep. Then we use the same
    weighting for writing to the erased memory matrix some new information, also specified
    by the controller in a write vector <math alttext="v Subscript t"><msub><mi>v</mi>
    <mi>t</mi></msub></math> containing <math alttext="upper W"><mi>W</mi></math>
    Â values:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å†™å¤´ä¹Ÿä½¿ç”¨ç±»ä¼¼çš„æ³¨æ„åŠ›åŠ æƒæ–¹æ³•ï¼šç”Ÿæˆä¸€ä¸ªæƒé‡å‘é‡<math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>ï¼Œç”¨äºä»å­˜å‚¨å™¨ä¸­æ“¦é™¤ç‰¹å®šä¿¡æ¯ï¼Œç”±æ§åˆ¶å™¨åœ¨æ“¦é™¤å‘é‡<math
    alttext="e Subscript t"><msub><mi>e</mi> <mi>t</mi></msub></math>ä¸­æŒ‡å®šï¼Œè¯¥å‘é‡æœ‰<math
    alttext="upper W"><mi>W</mi></math>ä¸ªå€¼åœ¨0å’Œ1ä¹‹é—´æŒ‡å®šè¦æ“¦é™¤å’Œè¦ä¿ç•™çš„å†…å®¹ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„æƒé‡å°†ä¸€äº›æ–°ä¿¡æ¯å†™å…¥å·²æ“¦é™¤çš„å­˜å‚¨å™¨çŸ©é˜µï¼Œä¹Ÿç”±æ§åˆ¶å™¨åœ¨åŒ…å«<math
    alttext="upper W"><mi>W</mi></math>ä¸ªå€¼çš„å†™å…¥å‘é‡<math alttext="v Subscript t"><msub><mi>v</mi>
    <mi>t</mi></msub></math>ä¸­æŒ‡å®šï¼š
- en: <math alttext="upper M Subscript t Baseline equals upper M Subscript t minus
    1 Baseline ring left-parenthesis upper E minus w Subscript t Baseline e Subscript
    t Superscript down-tack Baseline right-parenthesis plus w Subscript t Baseline
    normal v Subscript t Superscript down-tack"><mrow><msub><mi>M</mi> <mi>t</mi></msub>
    <mo>=</mo> <msub><mi>M</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>âˆ˜</mo> <mrow><mo>(</mo> <mi>E</mi> <mo>-</mo> <msub><mi>w</mi> <mi>t</mi></msub>
    <msubsup><mi>e</mi> <mi>t</mi> <mi>âŠ¤</mi></msubsup> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mi>w</mi> <mi>t</mi></msub> <msubsup><mi mathvariant="normal">v</mi> <mi>t</mi>
    <mi>âŠ¤</mi></msubsup></mrow></math>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper M Subscript t Baseline equals upper M Subscript t minus
    1 Baseline ring left-parenthesis upper E minus w Subscript t Baseline e Subscript
    t Superscript down-tack Baseline right-parenthesis plus w Subscript t Baseline
    normal v Subscript t Superscript down-tack"><mrow><msub><mi>M</mi> <mi>t</mi></msub>
    <mo>=</mo> <msub><mi>M</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>âˆ˜</mo> <mrow><mo>(</mo> <mi>E</mi> <mo>-</mo> <msub><mi>w</mi> <mi>t</mi></msub>
    <msubsup><mi>e</mi> <mi>t</mi> <mi>âŠ¤</mi></msubsup> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mi>w</mi> <mi>t</mi></msub> <msubsup><mi mathvariant="normal">v</mi> <mi>t</mi>
    <mi>âŠ¤</mi></msubsup></mrow></math>
- en: where <math alttext="upper E"><mi>E</mi></math> Â is a matrix of ones and <math
    alttext="ring"><mo>âˆ˜</mo></math> is element-wise multiplication.Â Similar to the
    reading case, the weighting <math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>
    tells us where to focus our erasing (the first term of the equation) and writing
    operations (the second term).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<math alttext="upper E"><mi>E</mi></math>æ˜¯ä¸€ä¸ªå…¨ä¸º1çš„çŸ©é˜µï¼Œ<math alttext="ring"><mo>âˆ˜</mo></math>æ˜¯é€å…ƒç´ ä¹˜æ³•ã€‚ä¸è¯»å–æƒ…å†µç±»ä¼¼ï¼Œæƒé‡<math
    alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>å‘Šè¯‰æˆ‘ä»¬åœ¨å“ªé‡Œé›†ä¸­æˆ‘ä»¬çš„æ“¦é™¤ï¼ˆæ–¹ç¨‹çš„ç¬¬ä¸€é¡¹ï¼‰å’Œå†™å…¥æ“ä½œï¼ˆæ–¹ç¨‹çš„ç¬¬äºŒé¡¹ï¼‰ã€‚
- en: NTM Memory Addressing Mechanisms
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NTMå­˜å‚¨å™¨å¯»å€æœºåˆ¶
- en: Now that we understand how NTMs access their memories in a continuous manner
    via attention weighting, weâ€™re left with how these weightings are generated and
    what forms of memory addressing mechanisms they represent. We can understand that
    by exploring what NTMs are expected to do with their memories, and based on the
    model they are mimicking (the Turning machine), we expect them to be able to access
    a location by the value it contains, and to be able to go forward or backward
    from a given location.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬äº†è§£äº†NTMå¦‚ä½•é€šè¿‡æ³¨æ„åŠ›åŠ æƒä»¥è¿ç»­æ–¹å¼è®¿é—®å…¶å­˜å‚¨å™¨ï¼Œæˆ‘ä»¬è¿˜éœ€è¦äº†è§£è¿™äº›æƒé‡æ˜¯å¦‚ä½•ç”Ÿæˆçš„ä»¥åŠå®ƒä»¬ä»£è¡¨ä»€ä¹ˆå½¢å¼çš„å­˜å‚¨å™¨å¯»å€æœºåˆ¶ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¢ç´¢NTMè¢«æœŸæœ›å¦‚ä½•å¤„ç†å…¶å­˜å‚¨å™¨ä»¥åŠåŸºäºå…¶æ¨¡å‹ï¼ˆå›¾çµæœºï¼‰çš„é¢„æœŸæ¥ç†è§£è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æœŸæœ›å®ƒä»¬èƒ½å¤Ÿé€šè¿‡å…¶åŒ…å«çš„å€¼è®¿é—®ä½ç½®ï¼Œå¹¶èƒ½å¤Ÿä»ç»™å®šä½ç½®å‘å‰æˆ–å‘åç§»åŠ¨ã€‚
- en: 'The first mode of behavior can be achieved with an access mechanism that weâ€™ll
    call *content-based addressing*. In this form of addressing, the controller emits
    the value that itâ€™s looking for, which weâ€™ll call a key <math alttext="k Subscript
    t"><msub><mi>k</mi> <mi>t</mi></msub></math> , then it measures its similarity
    to the information stored in each location and focuses the attention on the most
    similar one. This kind of weighting can be calculated via:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ç§è¡Œä¸ºæ¨¡å¼å¯ä»¥é€šè¿‡æˆ‘ä»¬å°†ç§°ä¹‹ä¸º*åŸºäºå†…å®¹çš„å¯»å€*çš„è®¿é—®æœºåˆ¶å®ç°ã€‚åœ¨è¿™ç§å¯»å€å½¢å¼ä¸­ï¼Œæ§åˆ¶å™¨å‘å‡ºå®ƒæ­£åœ¨å¯»æ‰¾çš„å€¼ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºå…³é”®<math alttext="k
    Subscript t"><msub><mi>k</mi> <mi>t</mi></msub></math>ï¼Œç„¶åæµ‹é‡å®ƒä¸å­˜å‚¨åœ¨æ¯ä¸ªä½ç½®çš„ä¿¡æ¯çš„ç›¸ä¼¼æ€§ï¼Œå¹¶å°†æ³¨æ„åŠ›é›†ä¸­åœ¨æœ€ç›¸ä¼¼çš„ä½ç½®ä¸Šã€‚è¿™ç§åŠ æƒå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¡ç®—ï¼š
- en: '*C*(*M*,*k*, *Î²*) = <math alttext="StartFraction exp left-parenthesis beta
    script upper D left-parenthesis upper M comma k right-parenthesis right-parenthesis
    Over sigma-summation Underscript i equals 0 Overscript upper N Endscripts exp
    left-parenthesis beta script upper D left-parenthesis upper M left-bracket i right-bracket
    comma k right-parenthesis right-parenthesis EndFraction"><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><mi>Î²</mi><mi>ğ’Ÿ</mi><mo>(</mo><mi>M</mi><mo>,</mo><mi>k</mi><mo>)</mo><mo>)</mo></mrow>
    <mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>N</mi></msubsup>
    <mo form="prefix">exp</mo><mrow><mo>(</mo><mi>Î²</mi><mi>ğ’Ÿ</mi><mrow><mo>(</mo><mi>M</mi><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow><mo>,</mo><mi>k</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></math>'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*C*(*M*,*k*, *Î²*) = <math alttext="StartFraction exp left-parenthesis beta
    script upper D left-parenthesis upper M comma k right-parenthesis right-parenthesis
    Over sigma-summation Underscript i equals 0 Overscript upper N Endscripts exp
    left-parenthesis beta script upper D left-parenthesis upper M left-bracket i right-bracket
    comma k right-parenthesis right-parenthesis EndFraction"><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><mi>Î²</mi><mi>ğ’Ÿ</mi><mo>(</mo><mi>M</mi><mo>,</mo><mi>k</mi><mo>)</mo><mo>)</mo></mrow>
    <mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>N</mi></msubsup>
    <mo form="prefix">exp</mo><mrow><mo>(</mo><mi>Î²</mi><mi>ğ’Ÿ</mi><mrow><mo>(</mo><mi>M</mi><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow><mo>,</mo><mi>k</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></math>'
- en: where *D* is some similarity measure, like the cosine similarity. The equation
    is nothing more than a normalized softmax distribution over the similarity scores.
    There is, however, an extra parameter <math alttext="beta"><mi>Î²</mi></math> that
    is used to attenuate the attention weights if needed. We call that the *key strength*.
    The main idea behind that parameter is that for some tasks, the key emitted by
    the controller may not be close to any of the information in the memory, which
    would result in seemingly uniform attention weights. [FigureÂ 12-3](#indecisive_key_with_unit_strength_results)
    shows how the key strength allows the controller to learn how to attenuate such
    uniform attention to be more focused on a single location that is the most probable;
    the controller then learns what value of the strength to emit with each possible
    key it emits.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*D*æ˜¯ä¸€äº›ç›¸ä¼¼åº¦åº¦é‡ï¼Œæ¯”å¦‚ä½™å¼¦ç›¸ä¼¼åº¦ã€‚è¯¥æ–¹ç¨‹å®é™…ä¸Šåªæ˜¯ç›¸ä¼¼åº¦åˆ†æ•°ä¸Šçš„å½’ä¸€åŒ–softmaxåˆ†å¸ƒã€‚ç„¶è€Œï¼Œè¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„å‚æ•°<math alttext="beta"><mi>Î²</mi></math>ï¼Œç”¨äºéœ€è¦æ—¶å‡å¼±æ³¨æ„åŠ›æƒé‡ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º*å…³é”®å¼ºåº¦*ã€‚è¯¥å‚æ•°èƒŒåçš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œå¯¹äºæŸäº›ä»»åŠ¡ï¼Œæ§åˆ¶å™¨å‘å‡ºçš„å…³é”®å¯èƒ½ä¸å­˜å‚¨å™¨ä¸­çš„ä»»ä½•ä¿¡æ¯éƒ½ä¸æ¥è¿‘ï¼Œè¿™å°†å¯¼è‡´çœ‹ä¼¼å‡åŒ€çš„æ³¨æ„åŠ›æƒé‡ã€‚[å›¾12-3](#indecisive_key_with_unit_strength_results)å±•ç¤ºäº†å…³é”®å¼ºåº¦å¦‚ä½•ä½¿æ§åˆ¶å™¨å­¦ä¼šå¦‚ä½•å‡å¼±è¿™ç§å‡åŒ€çš„æ³¨æ„åŠ›ï¼Œæ›´åŠ ä¸“æ³¨äºæœ€æœ‰å¯èƒ½çš„å•ä¸ªä½ç½®ï¼›ç„¶åæ§åˆ¶å™¨å­¦ä¹ å‘å‡ºæ¯ä¸ªå¯èƒ½çš„å…³é”®æ—¶è¦å‘å‡ºçš„å¼ºåº¦å€¼ã€‚
- en: '![](Images/fdl2_1203.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1203.png)'
- en: Figure 12-3\. An indecisive key with unit strength results in a nearly uniform
    attention vector; increasing the strength for keys like that focuses the attention
    on the most probable location
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾12-3ã€‚ä¸€ä¸ªçŠ¹è±«ä¸å†³çš„å…³é”®å…·æœ‰å•ä½å¼ºåº¦ï¼Œå¯¼è‡´å‡ ä¹å‡åŒ€çš„æ³¨æ„åŠ›å‘é‡ï¼›å¢åŠ åƒè¿™æ ·çš„å…³é”®çš„å¼ºåº¦ä¼šå°†æ³¨æ„åŠ›é›†ä¸­åœ¨æœ€å¯èƒ½çš„ä½ç½®ä¸Š
- en: 'To move forward and backward in the memory, we first need to know where we
    are we standing now, and such information is located in the access weighting from
    the last time step <math alttext="w Subscript t minus 1"><msub><mi>w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    . So to preserve the information about our current location with the new content-based
    weighting <math alttext="w Subscript t Superscript c"><msubsup><mi>w</mi> <mi>t</mi>
    <mi>c</mi></msubsup></math> Â we just got, we interpolate between the two weighting
    using a scalarÂ  <math alttext="g Subscript t"><msub><mi>g</mi> <mi>t</mi></msub></math>
    Â that lies between 0 and 1:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨å†…å­˜ä¸­å‰è¿›å’Œåé€€ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦çŸ¥é“æˆ‘ä»¬ç°åœ¨ç«™åœ¨å“ªé‡Œï¼Œè¿™æ ·çš„ä¿¡æ¯ä½äºä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„è®¿é—®åŠ æƒ<math alttext="w Subscript t minus
    1"><msub><mi>w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>ä¸­ã€‚å› æ­¤ï¼Œä¸ºäº†ä¿ç•™å…³äºæˆ‘ä»¬å½“å‰ä½ç½®çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„æ ‡é‡<math
    alttext="g Subscript t"><msub><mi>g</mi> <mi>t</mi></msub></math>æ¥æ’å€¼æ–°çš„åŸºäºå†…å®¹çš„åŠ æƒ<math
    alttext="w Subscript t Superscript c"><msubsup><mi>w</mi> <mi>t</mi> <mi>c</mi></msubsup></math>å’Œæˆ‘ä»¬åˆšåˆšå¾—åˆ°çš„åŠ æƒä¹‹é—´ï¼š
- en: <math alttext="w Subscript t Superscript g Baseline equals g Subscript t Baseline
    w Subscript t Superscript c Baseline plus left-parenthesis 1 minus g Subscript
    t Baseline right-parenthesis w Subscript t minus 1"><mrow><msubsup><mi>w</mi>
    <mi>t</mi> <mi>g</mi></msubsup> <mo>=</mo> <msub><mi>g</mi> <mi>t</mi></msub>
    <msubsup><mi>w</mi> <mi>t</mi> <mi>c</mi></msubsup> <mo>+</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>g</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <msub><mi>w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="w Subscript t Superscript g Baseline equals g Subscript t Baseline
    w Subscript t Superscript c Baseline plus left-parenthesis 1 minus g Subscript
    t Baseline right-parenthesis w Subscript t minus 1"><mrow><msubsup><mi>w</mi>
    <mi>t</mi> <mi>g</mi></msubsup> <mo>=</mo> <msub><mi>g</mi> <mi>t</mi></msub>
    <msubsup><mi>w</mi> <mi>t</mi> <mi>c</mi></msubsup> <mo>+</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>g</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <msub><mi>w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
- en: We callÂ  <math alttext="g Subscript t"><msub><mi>g</mi> <mi>t</mi></msub></math>
    Â the *interpolation gate*, and itâ€™s also emitted by the controller to control
    the kind of information we want to use in the current time step. When the gateâ€™s
    value is close to 1, we favor the addressing given by content lookup. However,
    when itâ€™s close to 0, we tend to pass the information about our current location
    through and ignore the content-based addressing. The controller learns to use
    this gate so that, for example, it could set it to 0 when iteration through consecutive
    locations is desired and information about the current location is crucial. The
    type of information the controller chooses to gate through is denoted by the *gated
    weighting*Â  <math alttext="w Subscript t Superscript g"><msubsup><mi>w</mi> <mi>t</mi>
    <mi>g</mi></msubsup></math> .
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç§°<math alttext="g Subscript t"><msub><mi>g</mi> <mi>t</mi></msub></math>ä¸º*æ’å€¼é—¨*ï¼Œå®ƒä¹Ÿç”±æ§åˆ¶å™¨å‘å‡ºï¼Œç”¨äºæ§åˆ¶æˆ‘ä»¬åœ¨å½“å‰æ—¶é—´æ­¥ä¸­æƒ³è¦ä½¿ç”¨çš„ä¿¡æ¯ç±»å‹ã€‚å½“é—¨çš„å€¼æ¥è¿‘1æ—¶ï¼Œæˆ‘ä»¬æ›´å€¾å‘äºåŸºäºå†…å®¹æŸ¥æ‰¾ç»™å‡ºçš„å¯»å€ã€‚ç„¶è€Œï¼Œå½“å®ƒæ¥è¿‘0æ—¶ï¼Œæˆ‘ä»¬å€¾å‘äºä¼ é€’å…³äºæˆ‘ä»¬å½“å‰ä½ç½®çš„ä¿¡æ¯ï¼Œå¹¶å¿½ç•¥åŸºäºå†…å®¹çš„å¯»å€ã€‚æ§åˆ¶å™¨å­¦ä¼šä½¿ç”¨è¿™ä¸ªé—¨ï¼Œä¾‹å¦‚ï¼Œå½“éœ€è¦è¿­ä»£é€šè¿‡è¿ç»­ä½ç½®æ—¶ï¼Œä»¥åŠå½“å‰ä½ç½®çš„ä¿¡æ¯è‡³å…³é‡è¦æ—¶ï¼Œå¯ä»¥å°†å…¶è®¾ç½®ä¸º0ã€‚æ§åˆ¶å™¨é€‰æ‹©é€šè¿‡çš„ä¿¡æ¯ç±»å‹ç”±*é—¨æ§åŠ æƒ*
    <math alttext="w Subscript t Superscript g"><msubsup><mi>w</mi> <mi>t</mi> <mi>g</mi></msubsup></math>è¡¨ç¤ºã€‚
- en: 'To start moving around the memory we need a way to take our current gated weighting
    and shift the focus from one location to another. This can be done by convoluting
    the gated weighting with a *shift weighting* <math alttext="s Subscript t"><msub><mi>s</mi>
    <mi>t</mi></msub></math> ,Â also emitted by the controller. This shift weighting
    is a normalized softmax attention vector of sizeÂ  <math alttext="n plus 1"><mrow><mi>n</mi>
    <mo>+</mo> <mn>1</mn></mrow></math> ,Â whereÂ  <math alttext="n"><mi>n</mi></math>
    Â is an even integer specifying the number of possible shifts around the focused-on
    location in the gated weighting; for example,Â if it has a size of 3, then there
    are two possible shifts around a location: one forward and one backward. [FigureÂ 12-4](#shift_weighting_focused_on_the_rights)
    shows how a shift weighting can move around the focused-on location in gated weighting.
    The shifting occurs by convoluting the gated weighting by the shift weighting
    in pretty much the same way we convoluted images with feature maps back in [ChapterÂ 7](ch07.xhtml#convolutional_neural_networks).
    The only exception is how we handle the case when the shift weightings go outside
    the gated weighting. Instead of using padding like we did before, we use a rotational
    convolution operator where overflown weights get applied to the values at the
    other end of the gated weighting, as shown in the middle panel of [FigureÂ 12-4](#shift_weighting_focused_on_the_rights).
    This operation can be expressed element-wise as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹åœ¨å†…å­˜ä¸­ç§»åŠ¨ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥è·å–å½“å‰çš„é—¨æ§åŠ æƒï¼Œå¹¶å°†ç„¦ç‚¹ä»ä¸€ä¸ªä½ç½®è½¬ç§»åˆ°å¦ä¸€ä¸ªä½ç½®ã€‚è¿™å¯ä»¥é€šè¿‡å°†é—¨æ§åŠ æƒä¸ç”±æ§åˆ¶å™¨å‘å‡ºçš„*ç§»ä½åŠ æƒ* <math
    alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math> è¿›è¡Œå·ç§¯æ¥å®ç°ï¼Œè¿™ä¸ªç§»ä½åŠ æƒæ˜¯ä¸€ä¸ªå¤§å°ä¸º<math
    alttext="n plus 1"><mrow><mi>n</mi> <mo>+</mo> <mn>1</mn></mrow></math>çš„å½’ä¸€åŒ–softmaxæ³¨æ„åŠ›å‘é‡ï¼Œå…¶ä¸­<math
    alttext="n"><mi>n</mi></math>æ˜¯ä¸€ä¸ªæŒ‡å®šé—¨æ§åŠ æƒä¸­ç„¦ç‚¹å‘¨å›´å¯èƒ½ç§»åŠ¨çš„æ¬¡æ•°çš„å¶æ•°æ•´æ•°ï¼›ä¾‹å¦‚ï¼Œå¦‚æœå®ƒçš„å¤§å°ä¸º3ï¼Œåˆ™åœ¨ä¸€ä¸ªä½ç½®å‘¨å›´æœ‰ä¸¤ä¸ªå¯èƒ½çš„ç§»ä½ï¼šä¸€ä¸ªå‘å‰ï¼Œä¸€ä¸ªå‘åã€‚[å›¾12-4](#shift_weighting_focused_on_the_rights)å±•ç¤ºäº†ç§»ä½åŠ æƒå¦‚ä½•åœ¨é—¨æ§åŠ æƒä¸­ç§»åŠ¨ç„¦ç‚¹ã€‚ç§»ä½æ˜¯é€šè¿‡å°†é—¨æ§åŠ æƒä¸ç§»ä½åŠ æƒè¿›è¡Œå·ç§¯æ¥å®ç°çš„ï¼Œè¿™ä¸æˆ‘ä»¬åœ¨[ç¬¬7ç« ](ch07.xhtml#convolutional_neural_networks)ä¸­ç”¨ç‰¹å¾å›¾å·ç§¯å›¾åƒçš„æ–¹å¼å‡ ä¹ç›¸åŒã€‚å”¯ä¸€çš„ä¾‹å¤–æ˜¯å½“ç§»ä½åŠ æƒè¶…å‡ºé—¨æ§åŠ æƒæ—¶æˆ‘ä»¬å¦‚ä½•å¤„ç†çš„æƒ…å†µã€‚ä¸ä¹‹å‰ä½¿ç”¨å¡«å……ä¸åŒï¼Œæˆ‘ä»¬ä½¿ç”¨æ—‹è½¬å·ç§¯è¿ç®—ç¬¦ï¼Œå…¶ä¸­æº¢å‡ºçš„æƒé‡è¢«åº”ç”¨äºé—¨æ§åŠ æƒå¦ä¸€ç«¯çš„å€¼ï¼Œå¦‚[å›¾12-4](#shift_weighting_focused_on_the_rights)çš„ä¸­é—´é¢æ¿æ‰€ç¤ºã€‚è¿™ä¸ªæ“ä½œå¯ä»¥é€å…ƒç´ åœ°è¡¨ç¤ºä¸ºï¼š
- en: <math alttext="ModifyingAbove w With tilde Subscript t Baseline left-bracket
    i right-bracket equals sigma-summation Underscript j equals 0 Overscript StartAbsoluteValue
    s Subscript t Baseline EndAbsoluteValue Endscripts w Subscript t Superscript g
    Baseline left-bracket left-parenthesis i plus StartFraction StartAbsoluteValue
    s Subscript t Baseline EndAbsoluteValue minus 1 Over 2 EndFraction minus j right-parenthesis
    mod upper N right-bracket s Subscript t Baseline left-bracket j right-bracket"><mrow><msub><mover
    accent="true"><mi>w</mi> <mo>Ëœ</mo></mover> <mi>t</mi></msub> <mrow><mo>[</mo>
    <mi>i</mi> <mo>]</mo></mrow> <mo>=</mo> <msubsup><mo>âˆ‘</mo> <mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow>
    <mrow><mrow><mo>|</mo></mrow><msub><mi>s</mi> <mi>t</mi></msub> <mrow><mo>|</mo></mrow></mrow></msubsup>
    <msubsup><mi>w</mi> <mi>t</mi> <mi>g</mi></msubsup> <mfenced separators="" open="["
    close="]"><mfenced separators="" open="(" close=")"><mi>i</mi> <mo>+</mo> <mfrac><mrow><mrow><mo>|</mo></mrow><msub><mi>s</mi>
    <mi>t</mi></msub> <mrow><mo>|</mo><mo>-</mo><mn>1</mn></mrow></mrow> <mn>2</mn></mfrac>
    <mo>-</mo> <mi>j</mi></mfenced> <mo form="prefix">mod</mo> <mi>N</mi></mfenced>
    <msub><mi>s</mi> <mi>t</mi></msub> <mfenced open="[" close="]"><mi>j</mi></mfenced></mrow></math>![](Images/fdl2_1204.png)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12-4\. A shift weighting focused on the right shifts the gated weighting
    one location to the right (left). Rotational convolution on a left-focused shift
    weighting, shifting the gated weighting to the left (middle). A nonsharp centered
    shift weighting keeps the gated weighting intact but disperses it (right).
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With the introduction of the shifting operation, our headsâ€™ weightings can
    now move around the memory freely forward and backward. However, a problem occurs
    if at any time the shift weighting is not sharp enough. Because of the nature
    of the convolution operation, a nonsharp shift weighting (as in the right panel
    of [FigureÂ 12-4](#shift_weighting_focused_on_the_rights)) disperses the original
    gated weightings around its surroundings and results in a less-focused shifted
    weighting. To overcome that blurring effect, we run the shifted weightings through
    one last operation: a sharpening operation. The controller emits one last scalarÂ 
    <math alttext="gamma Subscript t Baseline greater-than-or-equal-to 1"><mrow><msub><mi>Î³</mi>
    <mi>t</mi></msub> <mo>â‰¥</mo> <mn>1</mn></mrow></math> Â that sharpens the shifted
    weightings via:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="w Subscript t Baseline equals StartFraction w overTilde Subscript
    t Superscript gamma Super Subscript t Superscript Baseline Over sigma-summation
    Underscript i equals 0 Overscript upper N Endscripts w overTilde Subscript t Baseline
    left-bracket i right-bracket Superscript gamma Super Subscript t Superscript Baseline
    EndFraction"><mrow><msub><mi>w</mi> <mi>t</mi></msub> <mo>=</mo> <mfrac><msubsup><mover
    accent="true"><mi>w</mi> <mo>Ëœ</mo></mover> <mi>t</mi> <msub><mi>Î³</mi> <mi>t</mi></msub></msubsup>
    <mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>N</mi></msubsup>
    <msub><mover accent="true"><mi>w</mi> <mo>Ëœ</mo></mover> <mi>t</mi></msub> <msup><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow>
    <msub><mi>Î³</mi> <mi>t</mi></msub></msup></mrow></mfrac></mrow></math>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="w Subscript t Baseline equals StartFraction w overTilde Subscript
    t Superscript gamma Super Subscript t Superscript Baseline Over sigma-summation
    Underscript i equals 0 Overscript upper N Endscripts w overTilde Subscript t Baseline
    left-bracket i right-bracket Superscript gamma Super Subscript t Superscript Baseline
    EndFraction"><mrow><msub><mi>w</mi> <mi>t</mi></msub> <mo>=</mo> <mfrac><msubsup><mover
    accent="true"><mi>w</mi> <mo>Ëœ</mo></mover> <mi>t</mi> <msub><mi>Î³</mi> <mi>t</mi></msub></msubsup>
    <mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>N</mi></msubsup>
    <msub><mover accent="true"><mi>w</mi> <mo>Ëœ</mo></mover> <mi>t</mi></msub> <msup><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow>
    <msub><mi>Î³</mi> <mi>t</mi></msub></msup></mrow></mfrac></mrow></math>
- en: 'Starting from interpolation down to the final weighting vector out of sharpening,
    this process constitutes the second addressing mechanism of NTMs: the *location-based
    mechanism*. Using a combination of both addressing mechanisms, an NTM is able
    to utilize its memory to learn to solve various tasks. One of these tasks that
    would allow us to get a deeper look into the NTM in action is the copy task shown
    in [FigureÂ 12-5](#viz_of_an_ntm_trained_on_the_copy_task). In this task, we present
    the model with a sequence of random binary vectors that terminate with a special
    end symbol. We then request the same input sequence to be copied to the output.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1205.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: Figure 12-5\. An NTM trained on the copy task^([1](ch12.xhtml#idm45934167096448))
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The visualization shows how at the input time, the NTM starts writing the inputs
    step-by-step into consecutive locations in the memory. In the output time, the
    NTM goes back at the first written vector and iterates through the next locations
    to read and return the previously written input sequence. The original NTM paper
    contains several other visualizations of NTMs trained on different problems that
    are worth checking. These visualizations demonstrate the architectureâ€™s ability
    to utilize the addressing mechanisms to adapt to and learn to solve various tasks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ll suffice with our current understanding of NTMs and skip its implementation.
    Instead, we will spend the rest of the chapter exploring the drawbacks of NTMs
    and how the novel architecture of the differentiable neural computer (DNC) was
    able to overcome these drawbacks. Weâ€™ll conclude our discussion by implementing
    that novel architecture on simple reading comprehension tasks like the one we
    saw earlier.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Differentiable Neural Computers
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite the power of NTMs, they have a few limitations regarding their memory
    mechanisms. The first of these limitations is that NTMs have no way to ensure
    that no interference or overlap between written data would occur. This is due
    to the nature of the â€œdifferentiableâ€ writing operation in which we write new
    data everywhere in the memory to some extent specified by the attention. Usually,
    the attention mechanisms learn to focus the write weightings strongly on a single
    memory location, and the NTM converges to a mostly interference-free behavior,
    but thatâ€™s not guaranteed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'However, even when the NTM converges to an interference-free behavior, once
    a memory location has been written to, thereâ€™s no way to reuse that location again,
    even when the data stored in it becomes irrelevant. The inability to free and
    reuse memory locations is the second limitation of the NTM architecture. This
    results in new data being written to new locations that are likely to be contiguous,
    as we saw with the copy task. This contiguous writing fashion is the only way
    for an NTM to record any temporal information about the data being written: consecutive
    data is stored in consecutive locations. If the write head jumped to another place
    in the memory while writing some consecutive data, a read head wonâ€™t be able to
    recover the temporal link between the data written before and after the jump:
    this constitutes the third limitation of NTMs.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'In October 2016, Graves et al. from DeepMind published in *Nature* a paper
    titled, [â€œHybrid Computing Using a Neural Network with Dynamic External Memory,â€](http://go.nature.com/2peM8m2)
    in which they introduced a new memory-augmented neural architecture called *differentiable
    neural computer* (DNC) that improves on NTMs and addresses the limitations we
    just discussed. Similar to NTMs, DNCs consists of a controller that interacts
    with an external memory. The memory consists of *N* words of size <math alttext="upper
    W"><mi>W</mi></math> , making up an <math alttext="upper N times upper W"><mrow><mi>N</mi>
    <mo>Ã—</mo> <mi>W</mi></mrow></math> Â matrix weâ€™ll be callingÂ  *M*. The controller
    takes in an input vector of size <math alttext="upper X"><mi>X</mi></math> and
    the *R*Â vectors of size <math alttext="upper W"><mi>W</mi></math> Â read from memory
    in the previous step, where *R*Â is the number of read heads. The controller then
    processes them through a neural network and returns two pieces of information:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: An *interface vector* that contains all the necessary information to query the
    memory (i.e., write and read from it)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *pre-output* vector of size *Y*
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The external memory then takes in the interface vector, performs the necessary
    writing through a single write head, then reads *R*Â new vectors from the memory.
    It returns the newly read vectors to the controller to be added with the pre-output
    vector, producing the final output vector of size *Y*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[FigureÂ 12-6](#overview_of_dncs_architecture_and_operation) summarizes the
    operation of the DNC that we just described. We can see that unlike NTMs, DNCs
    keep other data structures alongside the memory itself to keep track of the state
    of the memory. As weâ€™ll shortly see, with these data structures and some clever
    new attention mechanisms, DNCs are able to successfully overcome NTMâ€™s limitations.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1206.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Figure 12-6\. An overview of DNCâ€™s architecture and operation
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To make the whole architecture differentiable, DNCs access the memory through
    weight vectors of size *N*Â whose elements determine how much the heads focus on
    each memory location. There are *R* weightings for the read headsÂ  <math alttext="normal
    w Subscript t Superscript r comma 1 Baseline comma ellipsis comma normal w Subscript
    t Superscript r comma upper R"><mrow><msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msubsup> <mo>,</mo> <mo>â‹¯</mo>
    <mo>,</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>R</mi></mrow></msubsup></mrow></math>
    whereÂ  <math alttext="t"><mi>t</mi></math> denotes the time step.Â On the other
    hand, thereâ€™s one write weightingÂ  <math alttext="normal w Subscript t Superscript
    w"><msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup></math>
    Â for the single write head. Once we obtain these weightings, we can modify the
    memory matrix and get updated via:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿æ•´ä¸ªæ¶æ„å¯å¾®åˆ†ï¼ŒDNCsé€šè¿‡å¤§å°ä¸º*N*çš„æƒé‡å‘é‡è®¿é—®å†…å­˜ï¼Œå…¶å…ƒç´ ç¡®å®šå¤´éƒ¨åœ¨æ¯ä¸ªå†…å­˜ä½ç½®ä¸Šçš„å…³æ³¨ç¨‹åº¦ã€‚è¯»å¤´æœ‰*R*ä¸ªæƒé‡ <math alttext="normal
    w Subscript t Superscript r comma 1 Baseline comma ellipsis comma normal w Subscript
    t Superscript r comma upper R"><mrow><msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msubsup> <mo>,</mo> <mo>â‹¯</mo>
    <mo>,</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>R</mi></mrow></msubsup></mrow></math>ï¼Œå…¶ä¸­
    <math alttext="t"><mi>t</mi></math> è¡¨ç¤ºæ—¶é—´æ­¥ã€‚å¦ä¸€æ–¹é¢ï¼Œå•ä¸ªå†™å¤´æœ‰ä¸€ä¸ªå†™æƒé‡ <math alttext="normal
    w Subscript t Superscript w"><msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mi>w</mi></msubsup></math>ã€‚ä¸€æ—¦æˆ‘ä»¬è·å¾—è¿™äº›æƒé‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿®æ”¹å†…å­˜çŸ©é˜µå¹¶è¿›è¡Œæ›´æ–°ï¼š
- en: <math alttext="upper M Subscript t Baseline equals upper M Subscript t minus
    1 Baseline ring left-parenthesis upper E minus normal w Subscript t Superscript
    w Baseline e Subscript t Superscript down-tack Baseline right-parenthesis plus
    normal w Subscript t Superscript w Baseline normal v Subscript t Superscript down-tack"><mrow><msub><mi>M</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>M</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>âˆ˜</mo> <mrow><mo>(</mo> <mi>E</mi> <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <msubsup><mi>e</mi> <mi>t</mi> <mi>âŠ¤</mi></msubsup>
    <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mi>w</mi></msubsup> <msubsup><mi mathvariant="normal">v</mi> <mi>t</mi> <mi>âŠ¤</mi></msubsup></mrow></math>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper M Subscript t Baseline equals upper M Subscript t minus
    1 Baseline ring left-parenthesis upper E minus normal w Subscript t Superscript
    w Baseline e Subscript t Superscript down-tack Baseline right-parenthesis plus
    normal w Subscript t Superscript w Baseline normal v Subscript t Superscript down-tack"><mrow><msub><mi>M</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>M</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>âˆ˜</mo> <mrow><mo>(</mo> <mi>E</mi> <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <msubsup><mi>e</mi> <mi>t</mi> <mi>âŠ¤</mi></msubsup>
    <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mi>w</mi></msubsup> <msubsup><mi mathvariant="normal">v</mi> <mi>t</mi> <mi>âŠ¤</mi></msubsup></mrow></math>
- en: and <math alttext="e Subscript t Baseline comma normal v Subscript t Baseline"><mrow><msub><mi>e</mi>
    <mi>t</mi></msub> <mo>,</mo> <msub><mi mathvariant="normal">v</mi> <mi>t</mi></msub></mrow></math>
    Â are the *erase* and *write* vectors we saw earlier with NTMs, coming from the
    controller through the interface vector as instructions about what to erase from
    and write to the memory.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œ <math alttext="e Subscript t Baseline comma normal v Subscript t Baseline"><mrow><msub><mi>e</mi>
    <mi>t</mi></msub> <mo>,</mo> <msub><mi mathvariant="normal">v</mi> <mi>t</mi></msub></mrow></math>
    æ˜¯æˆ‘ä»¬ä¹‹å‰åœ¨NTMsä¸­çœ‹åˆ°çš„*æ“¦é™¤*å’Œ*å†™å…¥*å‘é‡ï¼Œé€šè¿‡æ§åˆ¶å™¨é€šè¿‡æ¥å£å‘é‡ä¼ é€’æŒ‡ä»¤ï¼ŒæŒ‡ç¤ºè¦ä»å†…å­˜ä¸­æ“¦é™¤å’Œå†™å…¥çš„å†…å®¹ã€‚
- en: 'As soon as we get the updated memory matrixÂ  <math alttext="upper M Subscript
    t"><msub><mi>M</mi> <mi>t</mi></msub></math> , we can read out the new read vectorsÂ 
    <math alttext="normal r Subscript t Superscript 1 Baseline comma normal r Subscript
    t Superscript 2 Baseline comma ellipsis comma normal r Subscript t Superscript
    upper R"><mrow><msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>1</mn></msubsup>
    <mo>,</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>2</mn></msubsup>
    <mo>,</mo> <mo>â‹¯</mo> <mo>,</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi>
    <mi>R</mi></msubsup></mrow></math> Â using the following equation for each read
    weighting:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬è·å¾—æ›´æ–°åçš„å†…å­˜çŸ©é˜µ <math alttext="upper M Subscript t"><msub><mi>M</mi> <mi>t</mi></msub></math>ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹ç¨‹å¼è¯»å–æ–°çš„è¯»å‘é‡
    <math alttext="normal r Subscript t Superscript 1 Baseline comma normal r Subscript
    t Superscript 2 Baseline comma ellipsis comma normal r Subscript t Superscript
    upper R"><mrow><msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>1</mn></msubsup>
    <mo>,</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>2</mn></msubsup>
    <mo>,</mo> <mo>â‹¯</mo> <mo>,</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi>
    <mi>R</mi></msubsup></mrow></math>ï¼Œå¯¹æ¯ä¸ªè¯»æƒé‡ä½¿ç”¨ä»¥ä¸‹æ–¹ç¨‹å¼ï¼š
- en: <math alttext="normal r Subscript t Superscript i Baseline equals upper M Subscript
    t Superscript down-tack Baseline normal w Subscript t Superscript r comma i"><mrow><msubsup><mi
    mathvariant="normal">r</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <msubsup><mi>M</mi>
    <mi>t</mi> <mi>âŠ¤</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></math>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal r Subscript t Superscript i Baseline equals upper M Subscript
    t Superscript down-tack Baseline normal w Subscript t Superscript r comma i"><mrow><msubsup><mi
    mathvariant="normal">r</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <msubsup><mi>M</mi>
    <mi>t</mi> <mi>âŠ¤</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></math>
- en: Up until now, it seems that thereâ€™s nothing different from how NTMs write to
    and read from memory. However, the differences will start to show up when we discuss
    the attention mechanisms DNCs use to obtain their access weightings. While they
    both share the content-based addressing mechanismÂ *C*(*M*, *k*, *Î²*)Â defined earlier,
    DNCs use more sophisticated mechanisms to attend more efficiently to the memory.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä¼¼ä¹ä¸NTMså†™å…¥å’Œè¯»å–å†…å­˜çš„æ–¹å¼æ²¡æœ‰ä»€ä¹ˆä¸åŒã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬è®¨è®ºDNCsç”¨äºè·å–è®¿é—®æƒé‡çš„æ³¨æ„æœºåˆ¶æ—¶ï¼Œå·®å¼‚å°†å¼€å§‹æ˜¾ç°ã€‚è™½ç„¶å®ƒä»¬éƒ½å…±äº«ä¹‹å‰å®šä¹‰çš„åŸºäºå†…å®¹çš„å¯»å€æœºåˆ¶
    *C*(*M*, *k*, *Î²*)ï¼Œä½†DNCsä½¿ç”¨æ›´å¤æ‚çš„æœºåˆ¶æ›´æœ‰æ•ˆåœ°å…³æ³¨å†…å­˜ã€‚
- en: Interference-Free Writing in DNCs
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNCä¸­çš„æ— å¹²æ‰°å†™å…¥
- en: The first limitation we discussed for NTMs was their inability to ensure an
    interference-free writing behavior. An intuitive way to address this issue is
    to design the architecture to focus strongly on a single, free memory location
    and not wait for NTM to learn to do so. To keep track of which locations are free
    and which are busy, we need to introduce a new data structure that can hold this
    kind of information. Weâ€™ll call it the *usage vector.*
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¨è®ºNTMsçš„ç¬¬ä¸€ä¸ªé™åˆ¶æ˜¯å®ƒä»¬æ— æ³•ç¡®ä¿æ— å¹²æ‰°çš„å†™å…¥è¡Œä¸ºã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„ç›´è§‚æ–¹æ³•æ˜¯è®¾è®¡æ¶æ„ï¼Œå¼ºçƒˆå…³æ³¨å•ä¸ªç©ºé—²å†…å­˜ä½ç½®ï¼Œè€Œä¸æ˜¯ç­‰å¾…NTMå­¦ä¹ å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è·Ÿè¸ªå“ªäº›ä½ç½®æ˜¯ç©ºé—²çš„ï¼Œå“ªäº›æ˜¯å¿™ç¢Œçš„ï¼Œæˆ‘ä»¬éœ€è¦å¼•å…¥ä¸€ç§æ–°çš„æ•°æ®ç»“æ„æ¥ä¿å­˜è¿™ç§ä¿¡æ¯ã€‚æˆ‘ä»¬å°†å…¶ç§°ä¸º*ä½¿ç”¨å‘é‡*ã€‚
- en: The usage vector <math alttext="normal u Subscript t"><msub><mi mathvariant="normal">u</mi>
    <mi>t</mi></msub></math> Â is a vector of size *N*,Â where each element holds a
    value between 0 and 1 that represents how much of the corresponding memory location
    is used; with 0 indicating a completely free location, and 1 indicating a completely
    used one.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å‘é‡ <math alttext="normal u Subscript t"><msub><mi mathvariant="normal">u</mi>
    <mi>t</mi></msub></math> æ˜¯ä¸€ä¸ªå¤§å°ä¸º*N*çš„å‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ çš„å€¼ä»‹äº0å’Œ1ä¹‹é—´ï¼Œè¡¨ç¤ºç›¸åº”å†…å­˜ä½ç½®çš„ä½¿ç”¨ç¨‹åº¦ï¼›0è¡¨ç¤ºå®Œå…¨ç©ºé—²çš„ä½ç½®ï¼Œ1è¡¨ç¤ºå®Œå…¨ä½¿ç”¨çš„ä½ç½®ã€‚
- en: 'The usage vector initially contains zerosÂ  <math alttext="normal u 0 equals
    bold 0"><mrow><msub><mi mathvariant="normal">u</mi> <mn>0</mn></msub> <mo>=</mo>
    <mn mathvariant="bold">0</mn></mrow></math> and gets updated with the usage information
    across the steps. Using this information, itâ€™s clear that the location to which
    the weights should attend most strongly is the one with the least usage value.
    To obtain such weighting, we need to first sort the usage vector and obtain the
    list of location indices in ascending order of the usage; we call such a list
    a *free list* and denote it byÂ  <math alttext="phi Subscript t"><msub><mi>Ï†</mi>
    <mi>t</mi></msub></math> . Using that free list, we can construct an intermediate
    weighting called the *allocation weightingÂ  <math alttext="normal a Subscript
    t"><msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub></math>* that would
    determine which memory location should be allocated for new data. We calculateÂ 
    <math alttext="normal a Subscript t"><msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub></math>
    Â using:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å‘é‡æœ€åˆåŒ…å«é›¶ <math alttext="normal u 0 equals bold 0"><mrow><msub><mi mathvariant="normal">u</mi>
    <mn>0</mn></msub> <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> å¹¶éšç€æ­¥éª¤çš„è¿›è¡Œè€Œæ›´æ–°ä½¿ç”¨ä¿¡æ¯ã€‚åˆ©ç”¨è¿™äº›ä¿¡æ¯ï¼Œå¾ˆæ˜æ˜¾æƒé‡åº”è¯¥æœ€å¼ºçƒˆåœ°å…³æ³¨çš„ä½ç½®æ˜¯å…·æœ‰æœ€å°ä½¿ç”¨å€¼çš„ä½ç½®ã€‚ä¸ºäº†è·å¾—è¿™æ ·çš„æƒé‡ï¼Œæˆ‘ä»¬éœ€è¦é¦–å…ˆå¯¹ä½¿ç”¨å‘é‡è¿›è¡Œæ’åºï¼Œå¹¶æŒ‰ç…§ä½¿ç”¨é‡çš„å‡åºè·å¾—ä½ç½®ç´¢å¼•åˆ—è¡¨ï¼›æˆ‘ä»¬ç§°è¿™æ ·çš„åˆ—è¡¨ä¸º*è‡ªç”±åˆ—è¡¨*ï¼Œå¹¶ç”¨
    <math alttext="phi Subscript t"><msub><mi>Ï†</mi> <mi>t</mi></msub></math> è¡¨ç¤ºã€‚åˆ©ç”¨è¿™ä¸ªè‡ªç”±åˆ—è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªç§°ä¸º*åˆ†é…æƒé‡
    <math alttext="normal a Subscript t"><msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub></math>*
    çš„ä¸­é—´æƒé‡ï¼Œç”¨äºç¡®å®šåº”è¯¥ä¸ºæ–°æ•°æ®åˆ†é…å“ªä¸ªå†…å­˜ä½ç½®ã€‚æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¡ç®— <math alttext="normal a Subscript t"><msub><mi
    mathvariant="normal">a</mi> <mi>t</mi></msub></math>ï¼š
- en: <math alttext="normal a Subscript t Baseline left-bracket phi Subscript t Baseline
    left-bracket j right-bracket right-bracket equals left-parenthesis 1 minus normal
    u Subscript t Baseline left-bracket phi Subscript t Baseline left-bracket j right-bracket
    right-bracket right-parenthesis product Underscript i equals 1 Overscript j minus
    1 Endscripts normal u Subscript t Baseline left-bracket phi Subscript t Baseline
    left-bracket i right-bracket right-bracket where j element-of 1 comma ellipsis
    comma upper N"><mrow><msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <msub><mi>Ï†</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow>
    <mo>]</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>Ï†</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>]</mo></mrow>
    <mo>)</mo></mrow> <msubsup><mo>âˆ</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi mathvariant="normal">u</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>Ï†</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <mi>i</mi> <mo>]</mo></mrow> <mo>]</mo></mrow> <mtext>where</mtext> <mi>j</mi>
    <mo>âˆˆ</mo> <mn>1</mn> <mo>,</mo> <mo>â‹¯</mo> <mo>,</mo> <mi>N</mi></mrow></math>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal a Subscript t Baseline left-bracket phi Subscript t Baseline
    left-bracket j right-bracket right-bracket equals left-parenthesis 1 minus normal
    u Subscript t Baseline left-bracket phi Subscript t Baseline left-bracket j right-bracket
    right-bracket right-parenthesis product Underscript i equals 1 Overscript j minus
    1 Endscripts normal u Subscript t Baseline left-bracket phi Subscript t Baseline
    left-bracket i right-bracket right-bracket where j element-of 1 comma ellipsis
    comma upper N"><mrow><msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <msub><mi>Ï†</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow>
    <mo>]</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>Ï†</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>]</mo></mrow>
    <mo>)</mo></mrow> <msubsup><mo>âˆ</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi mathvariant="normal">u</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>Ï†</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <mi>i</mi> <mo>]</mo></mrow> <mo>]</mo></mrow> <mtext>where</mtext> <mi>j</mi>
    <mo>âˆˆ</mo> <mn>1</mn> <mo>,</mo> <mo>â‹¯</mo> <mo>,</mo> <mi>N</mi></mrow></math>
- en: 'This equation may look incomprehensible at first glance. A good way to understand
    it is to work through it with a numerical example, for example, whenÂ  <math alttext="normal
    u Subscript t Baseline equals left-bracket 1 comma 0.7 comma 0.2 comma 0.4 right-bracket"><mrow><msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo> <mrow><mo>[</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>7</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>2</mn>
    <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>4</mn> <mo>]</mo></mrow></mrow></math> .
    Weâ€™ll leave the details for you to go through. In the end, you should arrive at
    the allocation weighting beingÂ  <math alttext="normal a Subscript t Baseline equals
    left-bracket 0 comma 0.024 comma 0.8 comma 0.12 right-bracket"><mrow><msub><mi
    mathvariant="normal">a</mi> <mi>t</mi></msub> <mo>=</mo> <mrow><mo>[</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>024</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo>
    <mn>8</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>12</mn> <mo>]</mo></mrow></mrow></math>
    . As we go through the calculations, weâ€™ll begin to understand how this formula
    works: the <math alttext="1 minus normal u Subscript t Baseline left-bracket phi
    Subscript t Baseline left-bracket j right-bracket right-bracket"><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <msub><mi>Ï†</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow>
    <mo>]</mo></mrow></mrow></math> makes the location weight proportional to how
    free it is. By noticing that the productÂ  <math alttext="product Underscript i
    equals 1 Overscript j minus 1 Endscripts normal u Subscript t Baseline left-bracket
    phi Subscript t Baseline left-bracket j right-bracket right-bracket"><mrow><msubsup><mo>âˆ</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>Ï†</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>]</mo></mrow></mrow></math>
    gets smaller and smaller as we iterate through the free list (because we keep
    multiplying small values between 0 and 1), we can see that this product decreases
    the location weight even more as we go from the least used location to the most
    used one, which finally results in the least used location having the largest
    weight, while the most used one gets the smallest weight. So weâ€™re able to guarantee
    the ability to focus on a single location by design without the the need to hope
    for the model to learn it on its own from scratch; this means more reliability
    as well as faster training time.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹ä¹ä¸€çœ‹å¯èƒ½éš¾ä»¥ç†è§£ã€‚ç†è§£å®ƒçš„ä¸€ä¸ªå¥½æ–¹æ³•æ˜¯é€šè¿‡ä¸€ä¸ªæ•°å€¼ç¤ºä¾‹æ¥é€æ­¥è§£é‡Šï¼Œä¾‹å¦‚ï¼Œå½“ <math alttext="normal u Subscript
    t Baseline equals left-bracket 1 comma 0.7 comma 0.2 comma 0.4 right-bracket"><mrow><msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo> <mrow><mo>[</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>7</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>2</mn>
    <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>4</mn> <mo>]</mo></mrow></mrow></math> æ—¶ã€‚æˆ‘ä»¬å°†ç•™ä¸‹ç»†èŠ‚è®©æ‚¨è‡ªè¡ŒæŸ¥çœ‹ã€‚æœ€ç»ˆï¼Œæ‚¨åº”è¯¥å¾—å‡ºåˆ†é…æƒé‡ä¸º
    <math alttext="normal a Subscript t Baseline equals left-bracket 0 comma 0.024
    comma 0.8 comma 0.12 right-bracket"><mrow><msub><mi mathvariant="normal">a</mi>
    <mi>t</mi></msub> <mo>=</mo> <mrow><mo>[</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>.</mo> <mn>024</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>8</mn> <mo>,</mo>
    <mn>0</mn> <mo>.</mo> <mn>12</mn> <mo>]</mo></mrow></mrow></math> ã€‚å½“æˆ‘ä»¬è¿›è¡Œè®¡ç®—æ—¶ï¼Œæˆ‘ä»¬å°†å¼€å§‹ç†è§£è¿™ä¸ªå…¬å¼çš„å·¥ä½œåŸç†ï¼š
    <math alttext="1 minus normal u Subscript t Baseline left-bracket phi Subscript
    t Baseline left-bracket j right-bracket right-bracket"><mrow><mn>1</mn> <mo>-</mo>
    <msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>Ï†</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>]</mo></mrow></mrow></math>
    ä½¿å¾—ä½ç½®æƒé‡ä¸å…¶è‡ªç”±ç¨‹åº¦æˆæ¯”ä¾‹ã€‚é€šè¿‡æ³¨æ„åˆ°ä¹˜ç§¯ <math alttext="product Underscript i equals 1 Overscript
    j minus 1 Endscripts normal u Subscript t Baseline left-bracket phi Subscript
    t Baseline left-bracket j right-bracket right-bracket"><mrow><msubsup><mo>âˆ</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>Ï†</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>]</mo></mrow></mrow></math>
    éšç€æˆ‘ä»¬åœ¨è‡ªç”±åˆ—è¡¨ä¸­è¿­ä»£å˜å¾—è¶Šæ¥è¶Šå°ï¼ˆå› ä¸ºæˆ‘ä»¬ä¸æ–­åœ°å°†ä»‹äº0å’Œ1ä¹‹é—´çš„å°å€¼ç›¸ä¹˜ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªä¹˜ç§¯ä¼šä½¿ä½ç½®æƒé‡åœ¨ä»æœ€ä¸å¸¸ç”¨çš„ä½ç½®åˆ°æœ€å¸¸ç”¨çš„ä½ç½®æ—¶å‡å°å¾—æ›´å¤šï¼Œæœ€ç»ˆå¯¼è‡´æœ€ä¸å¸¸ç”¨çš„ä½ç½®å…·æœ‰æœ€å¤§çš„æƒé‡ï¼Œè€Œæœ€å¸¸ç”¨çš„ä½ç½®å…·æœ‰æœ€å°çš„æƒé‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡è®¾è®¡æ¥ä¿è¯èƒ½å¤Ÿä¸“æ³¨äºå•ä¸ªä½ç½®ï¼Œè€Œæ— éœ€å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿè‡ªè¡Œä»å¤´å¼€å§‹å­¦ä¹ ï¼›è¿™æ„å‘³ç€æ›´å¯é ä»¥åŠæ›´å¿«çš„è®­ç»ƒæ—¶é—´ã€‚
- en: 'With the allocation weightingÂ  <math alttext="normal a Subscript t"><msub><mi
    mathvariant="normal">a</mi> <mi>t</mi></msub></math> and lookup weightingÂ  <math
    alttext="normal c Subscript t Superscript w"><msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mi>w</mi></msubsup></math> Â we get from the content-based addressing
    mechanism <math alttext="normal c Subscript t Superscript w Baseline equals script
    upper C left-parenthesis upper M Subscript t minus 1 Baseline comma k Subscript
    t Superscript w Baseline comma beta Subscript t Superscript w Baseline right-parenthesis"><mrow><msubsup><mi
    mathvariant="normal">c</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>=</mo> <mi>ğ’</mi>
    <mrow><mo>(</mo> <msub><mi>M</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msubsup><mi>k</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>,</mo> <msubsup><mi>Î²</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mo>)</mo></mrow></mrow></math> ,Â whereÂ  <math
    alttext="k Subscript t Superscript w Baseline comma beta Subscript t Superscript
    w"><mrow><msubsup><mi>k</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>,</mo> <msubsup><mi>Î²</mi>
    <mi>t</mi> <mi>w</mi></msubsup></mrow></math> Â are the lookup key and the lookup
    strength we receive through the interface vector, we can now construct our final
    write weighting:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä»åŸºäºå†…å®¹çš„å¯»å€æœºåˆ¶ä¸­è·å¾—çš„åˆ†é…æƒé‡ <math alttext="normal a Subscript t"><msub><mi mathvariant="normal">a</mi>
    <mi>t</mi></msub></math> å’ŒæŸ¥æ‰¾æƒé‡ <math alttext="normal c Subscript t Superscript
    w"><msubsup><mi mathvariant="normal">c</mi> <mi>t</mi> <mi>w</mi></msubsup></math>ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥æ„å»ºæˆ‘ä»¬çš„æœ€ç»ˆå†™å…¥æƒé‡ï¼š
- en: <math alttext="normal w Subscript t Superscript w Baseline equals g Subscript
    t Superscript w Baseline left-bracket g Subscript t Superscript a Baseline normal
    a Subscript t Baseline plus left-parenthesis 1 minus g Subscript t Superscript
    a Baseline right-parenthesis normal c Subscript t Superscript w Baseline right-bracket"><mrow><msubsup><mi
    mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>=</mo> <msubsup><mi>g</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mfenced separators="" open="[" close="]"><msubsup><mi>g</mi>
    <mi>t</mi> <mi>a</mi></msubsup> <msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi>g</mi> <mi>t</mi>
    <mi>a</mi></msubsup> <mo>)</mo></mrow> <msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mi>w</mi></msubsup></mfenced></mrow></math>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal w Subscript t Superscript w Baseline equals g Subscript
    t Superscript w Baseline left-bracket g Subscript t Superscript a Baseline normal
    a Subscript t Baseline plus left-parenthesis 1 minus g Subscript t Superscript
    a Baseline right-parenthesis normal c Subscript t Superscript w Baseline right-bracket"><mrow><msubsup><mi
    mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>=</mo> <msubsup><mi>g</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mfenced separators="" open="[" close="]"><msubsup><mi>g</mi>
    <mi>t</mi> <mi>a</mi></msubsup> <msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi>g</mi> <mi>t</mi>
    <mi>a</mi></msubsup> <mo>)</mo></mrow> <msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mi>w</mi></msubsup></mfenced></mrow></math>
- en: whereÂ  <math alttext="g Subscript t Superscript w"><msubsup><mi>g</mi> <mi>t</mi>
    <mi>w</mi></msubsup></math> and <math alttext="g Subscript t Superscript a"><msubsup><mi>g</mi>
    <mi>t</mi> <mi>a</mi></msubsup></math> Â are values between 0 and 1 and are called
    the write and allocation gates, which we also get from the controller through
    the interface vector. These gates control the writing operation, withÂ  <math alttext="g
    Subscript t Superscript w"><msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup></math>
    Â determining if any writing is going to happen in the first place, and <math alttext="g
    Subscript t Superscript a"><msubsup><mi>g</mi> <mi>t</mi> <mi>a</mi></msubsup></math>
    Â specifying whether weâ€™ll write to a new location using the allocation weighting
    or modify an existing value specified by the lookup weighting.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ <math alttext="g Subscript t Superscript w"><msubsup><mi>g</mi> <mi>t</mi>
    <mi>w</mi></msubsup></math> å’Œ <math alttext="g Subscript t Superscript a"><msubsup><mi>g</mi>
    <mi>t</mi> <mi>a</mi></msubsup></math> æ˜¯ä»‹äº0å’Œ1ä¹‹é—´çš„å€¼ï¼Œç§°ä¸ºå†™å…¥é—¨å’Œåˆ†é…é—¨ï¼Œæˆ‘ä»¬ä¹Ÿä»æ§åˆ¶å™¨é€šè¿‡æ¥å£å‘é‡ä¸­è·å–ã€‚è¿™äº›é—¨æ§åˆ¶å†™å…¥æ“ä½œï¼Œå…¶ä¸­
    <math alttext="g Subscript t Superscript w"><msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup></math>
    ç¡®å®šé¦–å…ˆæ˜¯å¦ä¼šå‘ç”Ÿä»»ä½•å†™å…¥ï¼Œè€Œ <math alttext="g Subscript t Superscript a"><msubsup><mi>g</mi>
    <mi>t</mi> <mi>a</mi></msubsup></math> æŒ‡å®šæˆ‘ä»¬æ˜¯å¦å°†ä½¿ç”¨åˆ†é…æƒé‡å†™å…¥æ–°ä½ç½®ï¼Œæˆ–è€…ä¿®æ”¹ç”±æŸ¥æ‰¾æƒé‡æŒ‡å®šçš„ç°æœ‰å€¼ã€‚
- en: DNC Memory Reuse
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNCå†…å­˜é‡ç”¨
- en: What if while we calculate the allocation weighting we find that all locations
    are used, or in other words,Â  <math alttext="normal u Subscript t Baseline equals
    bold 1"><mrow><msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo>
    <mn mathvariant="bold">1</mn></mrow></math> ? This means that the allocation weightings
    will turn out all zeros and no new data can be allocated to memory. This raises
    the need for the ability to free and reuse the memory.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—åˆ†é…æƒé‡æ—¶ï¼Œå¦‚æœæˆ‘ä»¬å‘ç°æ‰€æœ‰ä½ç½®éƒ½è¢«ä½¿ç”¨ï¼Œæˆ–è€…æ¢å¥è¯è¯´ï¼Œ<math alttext="normal u Subscript t Baseline
    equals bold 1"><mrow><msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo>
    <mn mathvariant="bold">1</mn></mrow></math>ï¼Ÿè¿™æ„å‘³ç€åˆ†é…æƒé‡å°†å…¨éƒ¨å˜ä¸ºé›¶ï¼Œæ²¡æœ‰æ–°æ•°æ®å¯ä»¥åˆ†é…åˆ°å†…å­˜ä¸­ã€‚è¿™å°±å¼•å‘äº†é‡Šæ”¾å’Œé‡ç”¨å†…å­˜çš„éœ€æ±‚ã€‚
- en: 'In order to know which locations can be freed and which cannot, we construct
    a *retention vector*Â  <math alttext="psi Subscript t"><msub><mi>Ïˆ</mi> <mi>t</mi></msub></math>
    Â of size *N* that specifies how much of each location should be retained and not
    get freed. Each element of this vector takes a value between 0 and 1, with 0 indicating
    that the corresponding location can be freed, and 1 indicating that it should
    be retained. This vector is calculated using:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†çŸ¥é“å“ªäº›ä½ç½®å¯ä»¥è¢«é‡Šæ”¾ï¼Œå“ªäº›ä¸èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§å°ä¸º*N*çš„*ä¿ç•™å‘é‡* <math alttext="psi Subscript t"><msub><mi>Ïˆ</mi>
    <mi>t</mi></msub></math>ï¼ŒæŒ‡å®šæ¯ä¸ªä½ç½®åº”è¯¥ä¿ç•™å¤šå°‘ï¼Œä¸åº”è¯¥è¢«é‡Šæ”¾ã€‚è¯¥å‘é‡çš„æ¯ä¸ªå…ƒç´ å–å€¼ä»‹äº0å’Œ1ä¹‹é—´ï¼Œ0è¡¨ç¤ºç›¸åº”ä½ç½®å¯ä»¥è¢«é‡Šæ”¾ï¼Œ1è¡¨ç¤ºåº”è¯¥ä¿ç•™ã€‚è¿™ä¸ªå‘é‡æ˜¯é€šè¿‡ä»¥ä¸‹å…¬å¼è®¡ç®—çš„ï¼š
- en: <math alttext="psi Subscript t Baseline equals product Underscript i equals
    1 Overscript upper R Endscripts left-parenthesis bold 1 minus f Subscript t Superscript
    i Baseline normal w Subscript t minus 1 Superscript r comma i Baseline right-parenthesis"><mrow><msub><mi>Ïˆ</mi>
    <mi>t</mi></msub> <mo>=</mo> <msubsup><mo>âˆ</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>R</mi></msubsup> <mrow><mo>(</mo> <mn mathvariant="bold">1</mn> <mo>-</mo>
    <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="psi Subscript t Baseline equals product Underscript i equals
    1 Overscript upper R Endscripts left-parenthesis bold 1 minus f Subscript t Superscript
    i Baseline normal w Subscript t minus 1 Superscript r comma i Baseline right-parenthesis"><mrow><msub><mi>Ïˆ</mi>
    <mi>t</mi></msub> <mo>=</mo> <msubsup><mo>âˆ</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>R</mi></msubsup> <mrow><mo>(</mo> <mn mathvariant="bold">1</mn> <mo>-</mo>
    <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>)</mo></mrow></mrow></math>
- en: This equation is basically saying that the degree to which a memory location
    should be freed is proportional to how much is read from it in the last time steps
    by the various read heads (represented by the values of the read weightings <math
    alttext="normal w Subscript t minus 1 Superscript r comma i"><msubsup><mi mathvariant="normal">w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></math>
    ). However, continuously freeing a memory location once its data is read is not
    generally preferable as we might still need the data afterward. We let the controller
    decide when to free and when to retain a location after reading by emitting a
    set of *R* free gates <math alttext="f Subscript t Superscript 1 Baseline comma
    ellipsis comma f Subscript t Superscript upper R"><mrow><msubsup><mi>f</mi> <mi>t</mi>
    <mn>1</mn></msubsup> <mo>,</mo> <mo>â‹¯</mo> <mo>,</mo> <msubsup><mi>f</mi> <mi>t</mi>
    <mi>R</mi></msubsup></mrow></math> that have a value between 0 and 1\. This determines
    how much freeing should be done based on the fact that the location was just read
    from. The controller will then learn how to use these gates to achieve the behavior
    it desires.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹åŸºæœ¬ä¸Šæ˜¯è¯´ï¼Œåº”è¯¥é‡Šæ”¾å†…å­˜ä½ç½®çš„ç¨‹åº¦ä¸æœ€åå‡ ä¸ªæ—¶é—´æ­¥ä¸­å„è¯»å¤´è¯»å–çš„é‡æˆæ­£æ¯”ï¼ˆç”±è¯»å–æƒé‡çš„å€¼è¡¨ç¤ºï¼‰ã€‚ç„¶è€Œï¼Œä¸€æ—¦è¯»å–æ•°æ®ï¼Œè¿ç»­é‡Šæ”¾å†…å­˜ä½ç½®é€šå¸¸ä¸æ˜¯é¦–é€‰ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½ä¹‹åä»éœ€è¦è¿™äº›æ•°æ®ã€‚æˆ‘ä»¬è®©æ§åˆ¶å™¨å†³å®šä½•æ—¶åœ¨è¯»å–åé‡Šæ”¾æˆ–ä¿ç•™ä½ç½®ï¼Œé€šè¿‡å‘å‡ºä¸€ç»„*R*è‡ªç”±é—¨ï¼Œå…¶å€¼ä»‹äº0å’Œ1ä¹‹é—´ã€‚è¿™å†³å®šäº†åŸºäºåˆšåˆšä»ä¸­è¯»å–çš„ä½ç½®åº”è¯¥è¿›è¡Œå¤šå°‘é‡Šæ”¾ã€‚æ§åˆ¶å™¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨è¿™äº›é—¨æ¥å®ç°å…¶æ‰€éœ€çš„è¡Œä¸ºã€‚
- en: 'Once the retention vector is obtained, we can use it to update the usage vector
    to reflect any freeing or retention made via:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è·å¾—ä¿ç•™å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥æ›´æ–°ä½¿ç”¨å‘é‡ï¼Œä»¥åæ˜ é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œçš„ä»»ä½•é‡Šæ”¾æˆ–ä¿ç•™ï¼š
- en: <math alttext="normal u Subscript t Baseline equals left-parenthesis normal
    u Subscript t minus 1 Baseline plus normal w Subscript t minus 1 Superscript w
    Baseline minus normal u Subscript t minus 1 Baseline ring normal w Subscript t
    minus 1 Superscript w Baseline right-parenthesis ring psi Subscript t"><mrow><msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="(" close=")"><msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup> <mo>-</mo> <msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>âˆ˜</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup></mfenced> <mo>âˆ˜</mo> <msub><mi>Ïˆ</mi> <mi>t</mi></msub></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal u Subscript t Baseline equals left-parenthesis normal
    u Subscript t minus 1 Baseline plus normal w Subscript t minus 1 Superscript w
    Baseline minus normal u Subscript t minus 1 Baseline ring normal w Subscript t
    minus 1 Superscript w Baseline right-parenthesis ring psi Subscript t"><mrow><msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="(" close=")"><msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup> <mo>-</mo> <msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>âˆ˜</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup></mfenced> <mo>âˆ˜</mo> <msub><mi>Ïˆ</mi> <mi>t</mi></msub></mrow></math>
- en: 'This equation can be read as follows: a location will be used if it has been
    retained (its value inÂ  <math alttext="psi Subscript t Baseline almost-equals
    1"><mrow><msub><mi>Ïˆ</mi> <mi>t</mi></msub> <mo>â‰ˆ</mo> <mn>1</mn></mrow></math>
    ) and either itâ€™s already in use or has just been written to (indicated by its
    value inÂ  <math alttext="normal u Subscript t minus 1 Baseline plus normal w Subscript
    t minus 1 Superscript w"><mrow><msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup></mrow></math> ). Subtracting the element-wise productÂ  <math
    alttext="normal u Subscript t minus 1 Baseline ring normal w Subscript t minus
    1 Superscript w"><mrow><msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>âˆ˜</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup></mrow></math> brings the whole expression back between 0
    and 1 to be a valid usage value in case the addition between the previous usage
    got the write weighting past 1.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹å¯ä»¥è§£è¯»å¦‚ä¸‹ï¼šå¦‚æœä¸€ä¸ªä½ç½®å·²è¢«ä¿ç•™ï¼ˆåœ¨Ïˆtä¸­çš„å€¼å‡ ä¹ç­‰äº1ï¼‰ï¼Œå¹¶ä¸”å®ƒå·²ç»åœ¨ä½¿ç”¨ä¸­æˆ–åˆšåˆšè¢«å†™å…¥ï¼ˆåœ¨ut-1åŠ wt-1wä¸­çš„å€¼è¡¨ç¤ºï¼‰ï¼Œåˆ™è¯¥ä½ç½®å°†è¢«ä½¿ç”¨ã€‚é€å…ƒç´ ç›¸ä¹˜ut-1wt-1wçš„å·®å€¼å°†æ•´ä¸ªè¡¨è¾¾å¼å¸¦å›0å’Œ1ä¹‹é—´ï¼Œä»¥ä¾¿åœ¨å…ˆå‰ä½¿ç”¨ä¹‹é—´çš„åŠ æ³•ä½¿å†™å…¥æƒé‡è¶…è¿‡1æ—¶æˆä¸ºæœ‰æ•ˆä½¿ç”¨å€¼ã€‚
- en: By doing this usage update step before calculating the allocation, we can introduce
    some free memory for possible new data. Weâ€™re also able to use and reuse a limited
    amount of memory efficiently and overcome the second limitation of NTMs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨è®¡ç®—åˆ†é…ä¹‹å‰æ‰§è¡Œæ­¤ä½¿ç”¨æ›´æ–°æ­¥éª¤ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºå¯èƒ½çš„æ–°æ•°æ®å¼•å…¥ä¸€äº›ç©ºé—²å†…å­˜ã€‚æˆ‘ä»¬è¿˜èƒ½å¤Ÿæœ‰æ•ˆåœ°ä½¿ç”¨å’Œé‡å¤ä½¿ç”¨æœ‰é™æ•°é‡çš„å†…å­˜ï¼Œå¹¶å…‹æœNTMsçš„ç¬¬äºŒä¸ªé™åˆ¶ã€‚
- en: Temporal Linking of DNC Writes
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNCå†™å…¥çš„æ—¶é—´é“¾æ¥
- en: With the dynamic memory management mechanisms that DNCs use, each time a memory
    location is requested for allocation, weâ€™re going to get the most unused location,
    and thereâ€™ll be no positional relation between that location and the location
    of the previous write. With this type of memory access, NTMâ€™s way of preserving
    temporal relation with contiguity is not suitable. Weâ€™ll need to keep an explicit
    record of the order of the written data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨DNCä½¿ç”¨çš„åŠ¨æ€å†…å­˜ç®¡ç†æœºåˆ¶ï¼Œæ¯æ¬¡è¯·æ±‚åˆ†é…å†…å­˜ä½ç½®æ—¶ï¼Œæˆ‘ä»¬å°†è·å¾—æœ€ä¸å¸¸ç”¨çš„ä½ç½®ï¼Œå¹¶ä¸”è¯¥ä½ç½®ä¸å…ˆå‰å†™å…¥çš„ä½ç½®ä¹‹é—´æ²¡æœ‰ä½ç½®å…³ç³»ã€‚ä½¿ç”¨è¿™ç§ç±»å‹çš„å†…å­˜è®¿é—®ï¼ŒNTMä¿æŒæ—¶é—´å…³ç³»ä¸è¿ç»­æ€§çš„æ–¹å¼ä¸é€‚ç”¨ã€‚æˆ‘ä»¬éœ€è¦æ˜ç¡®è®°å½•å†™å…¥æ•°æ®çš„é¡ºåºã€‚
- en: 'This explicit recording is achieved in DNCs via two additional data structures
    alongside the memory matrix and the usage vector. The first is called a *precedence
    vector*Â  <math alttext="normal p Subscript t"><msub><mi mathvariant="normal">p</mi>
    <mi>t</mi></msub></math> , an *N*-sized vector considered to be a probability
    distribution over the memory locations, with each value indicating how likely
    the corresponding location was the last one written to. The precedence is initially
    set to <math alttext="normal p 0 equals bold 0"><mrow><msub><mi mathvariant="normal">p</mi>
    <mn>0</mn></msub> <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> and gets
    updated in the following steps via:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å†…å­˜çŸ©é˜µå’Œä½¿ç”¨å‘é‡æ—è¾¹çš„ä¸¤ä¸ªé¢å¤–æ•°æ®ç»“æ„ï¼Œåœ¨DNCä¸­å®ç°äº†è¿™ç§æ˜ç¡®è®°å½•ã€‚ç¬¬ä¸€ä¸ªè¢«ç§°ä¸º*ä¼˜å…ˆå‘é‡*ptï¼Œæ˜¯ä¸€ä¸ªNå¤§å°çš„å‘é‡ï¼Œè¢«è®¤ä¸ºæ˜¯å¯¹å†…å­˜ä½ç½®çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ¯ä¸ªå€¼è¡¨ç¤ºç›¸åº”ä½ç½®æœ€åä¸€æ¬¡å†™å…¥çš„å¯èƒ½æ€§æœ‰å¤šå¤§ã€‚ä¼˜å…ˆçº§æœ€åˆè®¾ç½®ä¸ºp0=0ï¼Œå¹¶é€šè¿‡ä»¥ä¸‹æ­¥éª¤è¿›è¡Œæ›´æ–°ã€‚
- en: <math alttext="normal p Subscript t Baseline equals left-parenthesis 1 minus
    sigma-summation Underscript i equals 1 Overscript upper N Endscripts normal w
    Subscript t Superscript w Baseline left-bracket i right-bracket right-parenthesis
    normal p Subscript t minus 1 Baseline plus normal w Subscript t Superscript w"><mrow><msub><mi
    mathvariant="normal">p</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="(" close=")"><mn>1</mn> <mo>-</mo> <msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow></mfenced> <msub><mi mathvariant="normal">p</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup></mrow></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal p Subscript t Baseline equals left-parenthesis 1 minus
    sigma-summation Underscript i equals 1 Overscript upper N Endscripts normal w
    Subscript t Superscript w Baseline left-bracket i right-bracket right-parenthesis
    normal p Subscript t minus 1 Baseline plus normal w Subscript t Superscript w"><mrow><msub><mi
    mathvariant="normal">p</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="(" close=")"><mn>1</mn> <mo>-</mo> <msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow></mfenced> <msub><mi mathvariant="normal">p</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup></mrow></math>
- en: Updating is done by first resetting the previous values of the precedence with
    a reset factor that is proportionate to how much writing was just made to the
    memory (indicated by the summation of the write weightingâ€™s components). Then
    the value of write weighting is added to the reset value so that a location with
    a large write weighting (that is the most recent location written to) would also
    get a large value in the precedence vector.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ–°æ˜¯é€šè¿‡é¦–å…ˆä½¿ç”¨ä¸åˆšåˆšå†™å…¥å†…å­˜çš„å†™å…¥é‡æˆæ¯”ä¾‹çš„é‡ç½®å› å­é‡ç½®ä¼˜å…ˆå€¼çš„å…ˆå‰å€¼æ¥å®Œæˆçš„ï¼ˆç”±å†™å…¥æƒé‡çš„åˆ†é‡æ±‚å’Œè¡¨ç¤ºï¼‰ã€‚ç„¶åå°†å†™å…¥æƒé‡çš„å€¼æ·»åŠ åˆ°é‡ç½®å€¼ä¸­ï¼Œä»¥ä¾¿å…·æœ‰è¾ƒå¤§å†™å…¥æƒé‡ï¼ˆå³æœ€è¿‘å†™å…¥çš„ä½ç½®ï¼‰çš„ä½ç½®ä¹Ÿä¼šåœ¨ä¼˜å…ˆå‘é‡ä¸­è·å¾—è¾ƒå¤§çš„å€¼ã€‚
- en: 'The second data structure we need to record temporal information is the *link
    matrix*Â  <math alttext="normal upper L Subscript t"><msub><mi mathvariant="normal">L</mi>
    <mi>t</mi></msub></math> . The link matrix is anÂ  <math alttext="upper N times
    upper N"><mrow><mi>N</mi> <mo>Ã—</mo> <mi>N</mi></mrow></math> Â matrix in which
    the elementÂ  <math alttext="normal upper L Subscript t Baseline left-bracket i
    comma j right-bracket"><mrow><msub><mi mathvariant="normal">L</mi> <mi>t</mi></msub>
    <mrow><mo>[</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>]</mo></mrow></mrow></math>
    Â has a value between 0,1, indicating how likely it is that locationÂ *i*Â was written
    after location *j*. This matrix is also initialized to zeros, and the diagonal
    elements are kept at zero throughout the timeÂ  <math alttext="normal upper L Subscript
    t Baseline left-bracket i comma i right-bracket equals 0"><mrow><msub><mi mathvariant="normal">L</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>i</mi> <mo>,</mo> <mi>i</mi> <mo>]</mo></mrow>
    <mo>=</mo> <mn>0</mn></mrow></math> , as itâ€™s meaningless to track if a location
    was written after itself when the previous data has already been overwritten and
    lost. However, each other element in the matrix is updated using:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦è®°å½•æ—¶é—´ä¿¡æ¯çš„ç¬¬äºŒä¸ªæ•°æ®ç»“æ„æ˜¯*é“¾æ¥çŸ©é˜µ* <math alttext="normal upper L Subscript t"><msub><mi
    mathvariant="normal">L</mi> <mi>t</mi></msub></math>ã€‚é“¾æ¥çŸ©é˜µæ˜¯ä¸€ä¸ª<math alttext="upper
    N times upper N"><mrow><mi>N</mi> <mo>Ã—</mo> <mi>N</mi></mrow></math>çŸ©é˜µï¼Œå…¶ä¸­å…ƒç´ <math
    alttext="normal upper L Subscript t Baseline left-bracket i comma j right-bracket"><mrow><msub><mi
    mathvariant="normal">L</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <mi>i</mi> <mo>,</mo>
    <mi>j</mi> <mo>]</mo></mrow></mrow></math>çš„å€¼åœ¨0å’Œ1ä¹‹é—´ï¼Œè¡¨ç¤ºä½ç½®*i*åœ¨ä½ç½®*j*ä¹‹åè¢«å†™å…¥çš„å¯èƒ½æ€§æœ‰å¤šå¤§ã€‚è¯¥çŸ©é˜µä¹Ÿåˆå§‹åŒ–ä¸ºé›¶ï¼Œå¹¶ä¸”å¯¹è§’å…ƒç´ åœ¨æ•´ä¸ªæ—¶é—´å†…ä¿æŒä¸ºé›¶<math
    alttext="normal upper L Subscript t Baseline left-bracket i comma i right-bracket
    equals 0"><mrow><msub><mi mathvariant="normal">L</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <mi>i</mi> <mo>,</mo> <mi>i</mi> <mo>]</mo></mrow> <mo>=</mo> <mn>0</mn></mrow></math>ï¼Œå› ä¸ºè·Ÿè¸ªä¸€ä¸ªä½ç½®åœ¨è‡ªèº«ä¹‹åè¢«å†™å…¥æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œå½“å…ˆå‰çš„æ•°æ®å·²ç»è¢«è¦†ç›–å’Œä¸¢å¤±æ—¶ã€‚ç„¶è€Œï¼ŒçŸ©é˜µä¸­çš„æ¯ä¸ªå…¶ä»–å…ƒç´ éƒ½ä¼šä½¿ç”¨ä»¥ä¸‹æ–¹å¼è¿›è¡Œæ›´æ–°ï¼š
- en: <math alttext="normal upper L Subscript t Baseline left-bracket i comma j right-bracket
    equals left-parenthesis 1 minus normal w Subscript t Superscript w Baseline left-bracket
    i right-bracket minus normal w Subscript t Superscript w Baseline left-bracket
    j right-bracket right-parenthesis normal upper L Subscript t minus 1 Baseline
    left-bracket i comma j right-bracket plus normal w Subscript t Superscript w Baseline
    left-bracket i right-bracket normal p Subscript t minus 1 Baseline left-bracket
    j right-bracket"><mrow><msub><mi mathvariant="normal">L</mi> <mi>t</mi></msub>
    <mrow><mo>[</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>=</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow>
    <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>)</mo></mrow> <msub><mi mathvariant="normal">L</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mrow><mo>[</mo> <mi>i</mi>
    <mo>,</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow>
    <msub><mi mathvariant="normal">p</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow></mrow></math>
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper L Subscript t Baseline left-bracket i comma j right-bracket
    equals left-parenthesis 1 minus normal w Subscript t Superscript w Baseline left-bracket
    i right-bracket minus normal w Subscript t Superscript w Baseline left-bracket
    j right-bracket right-parenthesis normal upper L Subscript t minus 1 Baseline
    left-bracket i comma j right-bracket plus normal w Subscript t Superscript w Baseline
    left-bracket i right-bracket normal p Subscript t minus 1 Baseline left-bracket
    j right-bracket"><mrow><msub><mi mathvariant="normal">L</mi> <mi>t</mi></msub>
    <mrow><mo>[</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>=</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow>
    <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>)</mo></mrow> <msub><mi mathvariant="normal">L</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mrow><mo>[</mo> <mi>i</mi>
    <mo>,</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow>
    <msub><mi mathvariant="normal">p</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow></mrow></math>
- en: 'The equation follows the same pattern we saw with other update rules: first
    the link element is reset by a factor proportional to how much writing had been
    done on locations <math alttext="i comma j"><mrow><mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></math>
    . Then the link is updated by the correlation (represented here by multiplication)
    between the write weighting at locationÂ *i* and the previous precedence value
    of locationÂ *j*. This eliminates NTMâ€™s third limitation; now we can keep track
    of temporal information no matter how the write head hops around the memory.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹éµå¾ªæˆ‘ä»¬çœ‹åˆ°çš„å…¶ä»–æ›´æ–°è§„åˆ™ç›¸åŒçš„æ¨¡å¼ï¼šé¦–å…ˆï¼Œé“¾æ¥å…ƒç´ ä¼šæ ¹æ®åœ¨ä½ç½®*i*å’Œ*j*ä¸Šå†™å…¥äº†å¤šå°‘è¿›è¡Œé‡ç½®ã€‚ç„¶åï¼Œé“¾æ¥ä¼šæ ¹æ®ä½ç½®*i*çš„å†™å…¥æƒé‡å’Œä½ç½®*j*çš„å…ˆå‰ä¼˜å…ˆå€¼ä¹‹é—´çš„ç›¸å…³æ€§ï¼ˆåœ¨è¿™é‡Œè¡¨ç¤ºä¸ºä¹˜æ³•ï¼‰è¿›è¡Œæ›´æ–°ã€‚è¿™æ¶ˆé™¤äº†NTMçš„ç¬¬ä¸‰ä¸ªé™åˆ¶ï¼›ç°åœ¨æˆ‘ä»¬å¯ä»¥è·Ÿè¸ªæ—¶é—´ä¿¡æ¯ï¼Œæ— è®ºå†™å¤´å¦‚ä½•åœ¨å†…å­˜ä¸­è·³è·ƒã€‚
- en: Understanding the DNC Read Head
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£DNCè¯»å¤´
- en: 'Once the write head has finished updating the memory matrix and the associated
    data structures, the read head is now ready to work. Its operation is simple:
    it needs to be able to look up values in the memory and be able to iterate forward
    and backward in temporal ordering between data. The lookup ability can simply
    be achieved with content-based addressing: for each read headÂ *i*,Â we calculate
    an intermediate weightingÂ  <math alttext="normal c Subscript t Superscript r comma
    i Baseline equals script upper C left-parenthesis upper M Subscript t Baseline
    comma k Subscript t Superscript r comma i Baseline comma beta Subscript t Superscript
    r comma i Baseline right-parenthesis"><mrow><msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup> <mo>=</mo> <mi>ğ’</mi>
    <mrow><mo>(</mo> <msub><mi>M</mi> <mi>t</mi></msub> <mo>,</mo> <msubsup><mi>k</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup> <mo>,</mo> <msubsup><mi>Î²</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup> <mo>)</mo></mrow></mrow></math>
    ,Â whereÂ  <math alttext="k Subscript t Superscript r comma 1 Baseline comma ellipsis
    comma k Subscript t Superscript r comma upper R"><mrow><msubsup><mi>k</mi> <mi>t</mi>
    <mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msubsup> <mo>,</mo> <mo>â‹¯</mo> <mo>,</mo>
    <msubsup><mi>k</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>R</mi></mrow></msubsup></mrow></math>
    and <math alttext="beta Subscript t Superscript r comma 1 Baseline comma ellipsis
    comma beta Subscript t Superscript r comma upper R"><mrow><msubsup><mi>Î²</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msubsup> <mo>,</mo> <mo>â‹¯</mo>
    <mo>,</mo> <msubsup><mi>Î²</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>R</mi></mrow></msubsup></mrow></math>
    Â are two sets of *R*Â read keys and strengths received from the controller in the
    interface vector.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve forward and backward iterations, we need to make the weightings
    go a step ahead or back from the location they recently read from. We can achieve
    that for the forward iteration by multiplying the link matrix by the last read
    weightings. This shifts the weights from the last read location to the location
    of the last write specified by the link matrix and constructs an intermediate
    forward weighting for each read headÂ *i*: <math alttext="normal f Subscript t
    Superscript i Baseline equals normal upper L Subscript t Baseline normal w Subscript
    t minus 1 Superscript r comma i"><mrow><msubsup><mi mathvariant="normal">f</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <msub><mi mathvariant="normal">L</mi>
    <mi>t</mi></msub> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></math> . Similarly,
    we construct an intermediate backward weighting by multiplying the transpose of
    the link matrix by the last read weightingsÂ  <math alttext="normal b Subscript
    t Superscript i Baseline equals normal upper L Subscript t minus 1 Superscript
    down-tack Baseline normal w Subscript t minus 1 Superscript r comma i"><mrow><msubsup><mi
    mathvariant="normal">b</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <msubsup><mi
    mathvariant="normal">L</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mi>âŠ¤</mi></msubsup>
    <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></math> .'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now construct the new read weightings for each read using the following
    rule:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal w Subscript t Superscript r comma i Baseline equals pi
    Subscript t Superscript i Baseline left-bracket 1 right-bracket normal b Subscript
    t Superscript i Baseline plus pi Subscript t Superscript i Baseline left-bracket
    2 right-bracket normal c Subscript t Superscript i Baseline plus pi Subscript
    t Superscript i Baseline left-bracket 3 right-bracket normal f Subscript t Superscript
    i"><mrow><msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>=</mo> <msubsup><mi>Ï€</mi> <mi>t</mi> <mi>i</mi></msubsup> <mrow><mo>[</mo>
    <mn>1</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">b</mi> <mi>t</mi>
    <mi>i</mi></msubsup> <mo>+</mo> <msubsup><mi>Ï€</mi> <mi>t</mi> <mi>i</mi></msubsup>
    <mrow><mo>[</mo> <mn>2</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>+</mo> <msubsup><mi>Ï€</mi> <mi>t</mi> <mi>i</mi></msubsup>
    <mrow><mo>[</mo> <mn>3</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">f</mi>
    <mi>t</mi> <mi>i</mi></msubsup></mrow></math>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal w Subscript t Superscript r comma i Baseline equals pi
    Subscript t Superscript i Baseline left-bracket 1 right-bracket normal b Subscript
    t Superscript i Baseline plus pi Subscript t Superscript i Baseline left-bracket
    2 right-bracket normal c Subscript t Superscript i Baseline plus pi Subscript
    t Superscript i Baseline left-bracket 3 right-bracket normal f Subscript t Superscript
    i"><mrow><msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>=</mo> <msubsup><mi>Ï€</mi> <mi>t</mi> <mi>i</mi></msubsup> <mrow><mo>[</mo>
    <mn>1</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">b</mi> <mi>t</mi>
    <mi>i</mi></msubsup> <mo>+</mo> <msubsup><mi>Ï€</mi> <mi>t</mi> <mi>i</mi></msubsup>
    <mrow><mo>[</mo> <mn>2</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>+</mo> <msubsup><mi>Ï€</mi> <mi>t</mi> <mi>i</mi></msubsup>
    <mrow><mo>[</mo> <mn>3</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">f</mi>
    <mi>t</mi> <mi>i</mi></msubsup></mrow></math>
- en: 'whereÂ  <math alttext="pi Subscript t Superscript 1 Baseline comma ellipsis
    comma pi Subscript t Superscript upper R"><mrow><msubsup><mi>Ï€</mi> <mi>t</mi>
    <mn>1</mn></msubsup> <mo>,</mo> <mo>â‹¯</mo> <mo>,</mo> <msubsup><mi>Ï€</mi> <mi>t</mi>
    <mi>R</mi></msubsup></mrow></math> Â are called the *read modes*. Each of these
    are a softmax distribution over three elements that come from the controller on
    the interface vector. Its three values determine the emphasis the read head should
    put on each read mechanism: backward, lookup, and forward, respectively. The controller
    learns to use these modes to instruct the memory on how data should be read.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The DNC Controller Network
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that weâ€™ve figured out the internal workings of the external memory in
    the DNC architecture, weâ€™re left with understanding how the controller that coordinates
    all the memory operations work. The controllerâ€™s operation is simple: in its heart
    thereâ€™s a neural network (recurrent or feed-forward) that takes in the input step
    along with the read-vectors from the last step and outputs a vector whose size
    depends on the architecture we chose for the network. Letâ€™s denote that vector
    byÂ *N*(Ï‡[t]),Â where *N*Â denotes whatever function is computed by the neural network,
    andÂ  <math alttext="chi Subscript t"><msub><mi>Ï‡</mi> <mi>t</mi></msub></math>
    Â denotes the concatenation of the input step and the last read vectorsÂ  <math
    alttext="chi Subscript t Baseline equals left-bracket x Subscript t Baseline semicolon
    normal r Subscript t minus 1 Superscript 1 Baseline semicolon ellipsis semicolon
    normal r Subscript t minus 1 Superscript upper R Baseline right-bracket"><mrow><msub><mi>Ï‡</mi>
    <mi>t</mi></msub> <mo>=</mo> <mrow><mo>[</mo> <msub><mi>x</mi> <mi>t</mi></msub>
    <mo>;</mo> <msubsup><mi mathvariant="normal">r</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mn>1</mn></msubsup> <mo>;</mo> <mo>â‹¯</mo> <mo>;</mo> <msubsup><mi mathvariant="normal">r</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mi>R</mi></msubsup> <mo>]</mo></mrow></mrow></math>
    . This concatenation of the last read vectors serves a similar purpose as the
    hidden state in a regular LSTM: to condition the output on the past.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: From that vector emitted from the neural network, we need two pieces of information.
    The first one is the interface vectorÂ Î¶[t]. As we saw, the interface vector holds
    all the information for the memory to carry out its operation. We can look at
    theÂ Î¶[t]Â vector as a concatenation of the individual elements we encountered before,
    as depicted in [FigureÂ 12-7](#interface_vector_decomposed).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1207.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Figure 12-7\. The interface vector decomposed to its individual components
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By summing up the sizes along the components, we can consider theÂ  <math alttext="zeta
    Subscript t"><msub><mi>Î¶</mi> <mi>t</mi></msub></math> Â vector as one big vector
    of size <math alttext="upper R times upper W plus 3 upper W plus 5 upper R plus
    3"><mrow><mi>R</mi> <mo>Ã—</mo> <mi>W</mi> <mo>+</mo> <mn>3</mn> <mi>W</mi> <mo>+</mo>
    <mn>5</mn> <mi>R</mi> <mo>+</mo> <mn>3</mn></mrow></math> . So in order to obtain
    that vector from the network output, we construct a learnable <math alttext="StartAbsoluteValue
    script upper N EndAbsoluteValue times left-parenthesis upper R times upper W plus
    3 upper W plus 5 upper R plus 3 right-parenthesis"><mrow><mo>|</mo> <mi>ğ’©</mi>
    <mo>|</mo> <mo>Ã—</mo> <mo>(</mo> <mi>R</mi> <mo>Ã—</mo> <mi>W</mi> <mo>+</mo> <mn>3</mn>
    <mi>W</mi> <mo>+</mo> <mn>5</mn> <mi>R</mi> <mo>+</mo> <mn>3</mn> <mo>)</mo></mrow></math>
    Â weights matrixÂ  <math alttext="upper W Subscript zeta"><msub><mi>W</mi> <mi>Î¶</mi></msub></math>
    ,Â whereÂ  <math alttext="StartAbsoluteValue script upper N EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>ğ’©</mi> <mo>|</mo></mrow></math> Â is the size of the networkâ€™s output, such
    that:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="zeta Subscript t Baseline equals upper W Subscript zeta Baseline
    script upper N left-parenthesis chi Subscript t Baseline right-parenthesis"><mrow><msub><mi>Î¶</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>W</mi> <mi>Î¶</mi></msub> <mi>ğ’©</mi> <mrow><mo>(</mo>
    <msub><mi>Ï‡</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="zeta Subscript t Baseline equals upper W Subscript zeta Baseline
    script upper N left-parenthesis chi Subscript t Baseline right-parenthesis"><mrow><msub><mi>Î¶</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>W</mi> <mi>Î¶</mi></msub> <mi>ğ’©</mi> <mrow><mo>(</mo>
    <msub><mi>Ï‡</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: 'Before passing thatÂ  <math alttext="zeta Subscript t"><msub><mi>Î¶</mi> <mi>t</mi></msub></math>
    Â vector to the memory, we need to make sure that each component has a valid value.
    For example, all the gates as well as the erase vector must have values between
    0 and 1, so we pass them through a sigmoid function to ensure that requirement:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="e Subscript t Baseline equals sigma left-parenthesis e Subscript
    t Baseline right-parenthesis comma f Subscript t Superscript i Baseline equals
    sigma left-parenthesis f Subscript t Superscript i Baseline right-parenthesis
    comma g Subscript t Superscript a Baseline equals sigma left-parenthesis g Subscript
    t Superscript a Baseline right-parenthesis comma g Subscript t Superscript w Baseline
    equals sigma left-parenthesis g Subscript t Superscript w Baseline right-parenthesis
    where sigma left-parenthesis z right-parenthesis equals StartFraction 1 Over 1
    plus e Superscript negative z Baseline EndFraction"><mrow><msub><mi>e</mi> <mi>t</mi></msub>
    <mo>=</mo> <mi>Ïƒ</mi> <mrow><mo>(</mo> <msub><mi>e</mi> <mi>t</mi></msub> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <mi>Ïƒ</mi>
    <mrow><mo>(</mo> <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>a</mi></msubsup> <mo>=</mo> <mi>Ïƒ</mi>
    <mrow><mo>(</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>a</mi></msubsup> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>=</mo> <mi>Ïƒ</mi>
    <mrow><mo>(</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>)</mo></mrow>
    <mtext>where</mtext> <mi>Ïƒ</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="e Subscript t Baseline equals sigma left-parenthesis e Subscript
    t Baseline right-parenthesis comma f Subscript t Superscript i Baseline equals
    sigma left-parenthesis f Subscript t Superscript i Baseline right-parenthesis
    comma g Subscript t Superscript a Baseline equals sigma left-parenthesis g Subscript
    t Superscript a Baseline right-parenthesis comma g Subscript t Superscript w Baseline
    equals sigma left-parenthesis g Subscript t Superscript w Baseline right-parenthesis
    where sigma left-parenthesis z right-parenthesis equals StartFraction 1 Over 1
    plus e Superscript negative z Baseline EndFraction"><mrow><msub><mi>e</mi> <mi>t</mi></msub>
    <mo>=</mo> <mi>Ïƒ</mi> <mrow><mo>(</mo> <msub><mi>e</mi> <mi>t</mi></msub> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <mi>Ïƒ</mi>
    <mrow><mo>(</mo> <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>a</mi></msubsup> <mo>=</mo> <mi>Ïƒ</mi>
    <mrow><mo>(</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>a</mi></msubsup> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>=</mo> <mi>Ïƒ</mi>
    <mrow><mo>(</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>)</mo></mrow>
    <mtext>where</mtext> <mi>Ïƒ</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow></math>
- en: 'Also, all the lookup strengths need to have a value larger than or equal to
    1, so we pass them through a *oneplusÂ *function first:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="beta Subscript t Superscript r comma i Baseline equals normal
    o normal n normal e normal p normal l normal u normal s left-parenthesis beta
    Subscript t Superscript r comma i Baseline right-parenthesis comma beta Subscript
    t Superscript w Baseline equals normal o normal n normal e normal p normal l normal
    u normal s left-parenthesis beta Subscript t Superscript w Baseline right-parenthesis
    where normal o normal n normal e normal p normal l normal u normal s left-parenthesis
    z right-parenthesis equals 1 plus log left-parenthesis 1 plus e Superscript z
    Baseline right-parenthesis"><mrow><msubsup><mi>Î²</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>=</mo> <mi>oneplus</mi> <mrow><mo>(</mo> <msubsup><mi>Î²</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>)</mo></mrow> <mo>,</mo> <msubsup><mi>Î²</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mo>=</mo> <mi>oneplus</mi> <mrow><mo>(</mo> <msubsup><mi>Î²</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mo>)</mo></mrow> <mtext>where</mtext> <mi>oneplus</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>+</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>+</mo> <msup><mi>e</mi> <mi>z</mi></msup> <mo>)</mo></mrow></mrow></math>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="beta Subscript t Superscript r comma i Baseline equals normal
    o normal n normal e normal p normal l normal u normal s left-parenthesis beta
    Subscript t Superscript r comma i Baseline right-parenthesis comma beta Subscript
    t Superscript w Baseline equals normal o normal n normal e normal p normal l normal
    u normal s left-parenthesis beta Subscript t Superscript w Baseline right-parenthesis
    where normal o normal n normal e normal p normal l normal u normal s left-parenthesis
    z right-parenthesis equals 1 plus log left-parenthesis 1 plus e Superscript z
    Baseline right-parenthesis"><mrow><msubsup><mi>Î²</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>=</mo> <mi>oneplus</mi> <mrow><mo>(</mo> <msubsup><mi>Î²</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>)</mo></mrow> <mo>,</mo> <msubsup><mi>Î²</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mo>=</mo> <mi>oneplus</mi> <mrow><mo>(</mo> <msubsup><mi>Î²</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mo>)</mo></mrow> <mtext>where</mtext> <mi>oneplus</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>+</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>+</mo> <msup><mi>e</mi> <mi>z</mi></msup> <mo>)</mo></mrow></mrow></math>
- en: 'And finally, the read modes must have a valid softmax distribution:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="pi Subscript t Superscript i Baseline equals normal s normal
    o normal f normal t normal m normal a normal x left-parenthesis pi Subscript t
    Superscript i Baseline right-parenthesis where normal s normal o normal f normal
    t normal m normal a normal x left-parenthesis z right-parenthesis equals StartFraction
    e Superscript z Baseline Over sigma-summation Underscript j Endscripts e Superscript
    z Super Subscript j Superscript Baseline EndFraction"><mrow><msubsup><mi>Ï€</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo> <msubsup><mi>Ï€</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mtext>where</mtext> <mi>softmax</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mi>z</mi></msup> <mrow><msub><mo>âˆ‘</mo> <mi>j</mi></msub> <msup><mi>e</mi> <msub><mi>z</mi>
    <mi>j</mi></msub></msup></mrow></mfrac></mrow></math>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="pi Subscript t Superscript i Baseline equals normal s normal
    o normal f normal t normal m normal a normal x left-parenthesis pi Subscript t
    Superscript i Baseline right-parenthesis where normal s normal o normal f normal
    t normal m normal a normal x left-parenthesis z right-parenthesis equals StartFraction
    e Superscript z Baseline Over sigma-summation Underscript j Endscripts e Superscript
    z Super Subscript j Superscript Baseline EndFraction"><mrow><msubsup><mi>Ï€</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo> <msubsup><mi>Ï€</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mtext>where</mtext> <mi>softmax</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mi>z</mi></msup> <mrow><msub><mo>âˆ‘</mo> <mi>j</mi></msub> <msup><mi>e</mi> <msub><mi>z</mi>
    <mi>j</mi></msub></msup></mrow></mfrac></mrow></math>
- en: 'By these transformations, the interface vector is now ready to be passed to
    the memory; and while it guides the memory in its operations, weâ€™ll be needing
    a second piece of information from the neural network, the *pre-output* vectorÂ 
    <math alttext="v Subscript t"><msub><mi>v</mi> <mi>t</mi></msub></math> . This
    is a vector of the same size of the final output vector, but itâ€™s not the final
    output vector. By using another learnableÂ  <math alttext="StartAbsoluteValue script
    upper N EndAbsoluteValue times upper Y"><mrow><mo>|</mo> <mi>ğ’©</mi> <mo>|</mo>
    <mo>Ã—</mo> <mi>Y</mi></mrow></math> Â weights matrix <math alttext="upper W Subscript
    y"><msub><mi>W</mi> <mi>y</mi></msub></math> , we can obtain the pre-output via:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="v Subscript t Baseline equals upper W Subscript y Baseline script
    upper N left-parenthesis chi Subscript t Baseline right-parenthesis"><mrow><msub><mi>v</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>W</mi> <mi>y</mi></msub> <mi>ğ’©</mi> <mrow><mo>(</mo>
    <msub><mi>Ï‡</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="v Subscript t Baseline equals upper W Subscript y Baseline script
    upper N left-parenthesis chi Subscript t Baseline right-parenthesis"><mrow><msub><mi>v</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>W</mi> <mi>y</mi></msub> <mi>ğ’©</mi> <mrow><mo>(</mo>
    <msub><mi>Ï‡</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: 'This pre-output vector gives us the ability to condition our final output not
    just on the network output, but also on the recently read vectorsÂ  <math alttext="normal
    r Subscript t"><msub><mi mathvariant="normal">r</mi> <mi>t</mi></msub></math>
    Â from memory. Via a third learnable <math alttext="left-parenthesis upper R times
    upper W right-parenthesis times upper Y"><mrow><mo>(</mo> <mi>R</mi> <mo>Ã—</mo>
    <mi>W</mi> <mo>)</mo> <mo>Ã—</mo> <mi>Y</mi></mrow></math> Â weights matrix <math
    alttext="upper W Subscript r"><msub><mi>W</mi> <mi>r</mi></msub></math> , we can
    get the final output as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y Subscript t Baseline equals v Subscript t Baseline plus upper
    W Subscript r Baseline left-bracket normal r Subscript t Superscript 1 Baseline
    semicolon ellipsis semicolon normal r Subscript t Superscript upper R Baseline
    right-bracket"><mrow><msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>v</mi>
    <mi>t</mi></msub> <mo>+</mo> <msub><mi>W</mi> <mi>r</mi></msub> <mrow><mo>[</mo>
    <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>1</mn></msubsup> <mo>;</mo>
    <mo>â‹¯</mo> <mo>;</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mi>R</mi></msubsup>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y Subscript t Baseline equals v Subscript t Baseline plus upper
    W Subscript r Baseline left-bracket normal r Subscript t Superscript 1 Baseline
    semicolon ellipsis semicolon normal r Subscript t Superscript upper R Baseline
    right-bracket"><mrow><msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>v</mi>
    <mi>t</mi></msub> <mo>+</mo> <msub><mi>W</mi> <mi>r</mi></msub> <mrow><mo>[</mo>
    <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>1</mn></msubsup> <mo>;</mo>
    <mo>â‹¯</mo> <mo>;</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mi>R</mi></msubsup>
    <mo>]</mo></mrow></mrow></math>
- en: Given that the controller knows nothing about the memory except for the word
    sizeÂ  <math alttext="upper W"><mi>W</mi></math> , an already learned controller
    can be scaled to a larger memory with more locations without any need for retraining.
    Also, the fact that we didnâ€™t specify any particular structure for the neural
    network or any particular loss function makes DNC a universal architecture that
    can be applied to a variety of tasks and learning problems.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the DNC in Action
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to see DNCâ€™s operation in action is to train it on a simple task that
    would allow us to look at the weightings and the parametersâ€™ values and visualize
    them in an interpretable way. For this simple task, weâ€™ll use the copy problem
    we already saw with NTMs, but in a slightly modified form.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Instead of trying to copy a single sequence of binary vectors, our task here
    will be to copy a series of such sequences. In [FigureÂ 12-8](#single_v_series_of_input_sequences),Â (a)
    shows the single sequence input. After processing such single sequence input and
    copying the same sequence to the output, the DNC would have finished its program,
    and its memory would be reset in a way that will not allow us to see how it can
    dynamically manage it. Instead weâ€™ll treat a series of such sequences, shown in
    [FigureÂ 12-8](#single_v_series_of_input_sequences)Â (b), as a single input.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1208.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 12-8\. Single sequence input versus a series of input sequences
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[FigureÂ 12-9](#viz_of_the_dnc_operation_on_copy_problem) shows a visualization
    of the DNC operation after being trained on a series of length 4 where each sequence
    contains five binary vectors and an end mark. The DNC used here has only 10 memory
    locations, so thereâ€™s no way it can store all 20 vectors in the input. A feed-forward
    controller is used to ensure that nothing would be stored in a recurrent state,
    and only one read head is used to make the visualization more clear. These constraints
    should force the DNC to learn how to deallocate and reuse memory to successfully
    copy the whole input, and indeed it does.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: We can see in that visualization how the DNC is writing each vector of the five
    in a sequence into a single memory location. As soon as the end mark is seen,
    the read head starts reading from these locations in the exact same order of writing.
    We can see how both the allocation and free gates alternate in activation between
    writing and reading phases of each sequence in the series. From the usage vector
    chart at the bottom, we can also see how after a memory location is written to,
    its usage becomes exactly 1, and how it drops to 0 just after reading from that
    location, indicating that it was freed and can be reused again.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1209.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: Figure 12-9\. Visualization of the DNC operation on the copy problem
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This visualization is part of the open source implementation of the DNC architecture
    by [Mostafa Samir](https://oreil.ly/TtKJ8). In the next section weâ€™ll learn the
    important tips and tricks that will allow us to implement a simpler version of
    DNC on the reading comprehension tasks.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the DNC in PyTorch
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing the DNC architecture is essentially a direct application of the
    math we just discussed. So with the full implementation in the code repository
    associated with the book, weâ€™ll just be focusing on the tricky parts and introduce
    some new PyTorch practice while weâ€™re at it.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'The main part of the implementation resides in the *mem_ops.py* file where
    all of the attention and access mechanisms are implemented. This file is then
    imported to be used with the controller. Two operations that might be a little
    tricky to implement are the link matrix update and the allocation weighting calculation.
    Both of these operations can be naively implemented with `for` loops, but using
    `for` loops in creating a computational graph is generally not a good idea. Letâ€™s
    take the link matrix update operation first and see how it looks with a loop-based
    implementation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After that computational graph is fully defined, itâ€™s then fed with concrete
    values and executed. With that in mind, we can see, as depicted in [FigureÂ 12-10](#computational_graph_of_the_link_matrix),
    how in most of the iterations of the `for` loop, a new set of nodes representing
    the loop body gets added in the computational graph. So forÂ  *N*Â memory locations,
    we end up with <math alttext="upper N squared minus upper N"><mrow><msup><mi>N</mi>
    <mn>2</mn></msup> <mo>-</mo> <mi>N</mi></mrow></math> Â identical copies of the
    same nodes, each for each iteration, each taking up a chunk of our RAM and needing
    its own time to be processed before the next can be. When *N*Â is a small number,
    say 5, we get 20 identical copies, which is not so bad. However, if we want to
    use a larger memory, like with <math alttext="upper N equals 256"><mrow><mi>N</mi>
    <mo>=</mo> <mn>256</mn></mrow></math> , we get 65,280 identical copies of the
    nodes, which is catastrophic for both the memory usage and the execution time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1210.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: Figure 12-10\. The computational graph of the link matrix update operation built
    with the `for` loop implementation
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One possible way to overcome such an issue is *vectorization*. In vectorization,
    we take an array operation that is originally defined in terms of individual elements
    and rewrite it as an operation on the whole array at once. For the link matrix
    update, we can rewrite the operation as:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal upper L Subscript t Baseline equals left-bracket left-parenthesis
    1 minus normal w Subscript t Superscript w Baseline circled-plus normal w Subscript
    t Superscript w Baseline right-parenthesis ring normal upper L Subscript t minus
    1 Baseline plus normal w Subscript t Superscript w Baseline normal p Subscript
    t minus 1 Baseline right-bracket ring left-parenthesis 1 minus upper I right-parenthesis"><mrow><msub><mi
    mathvariant="normal">L</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="[" close="]"><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo>
    <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>âŠ•</mo>
    <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup></mfenced>
    <mo>âˆ˜</mo> <msub><mi mathvariant="normal">L</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <msub><mi mathvariant="normal">p</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mfenced>
    <mo>âˆ˜</mo> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mi>I</mi></mfenced></mrow></math>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper L Subscript t Baseline equals left-bracket left-parenthesis
    1 minus normal w Subscript t Superscript w Baseline circled-plus normal w Subscript
    t Superscript w Baseline right-parenthesis ring normal upper L Subscript t minus
    1 Baseline plus normal w Subscript t Superscript w Baseline normal p Subscript
    t minus 1 Baseline right-bracket ring left-parenthesis 1 minus upper I right-parenthesis"><mrow><msub><mi
    mathvariant="normal">L</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="[" close="]"><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo>
    <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>âŠ•</mo>
    <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup></mfenced>
    <mo>âˆ˜</mo> <msub><mi mathvariant="normal">L</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <msub><mi mathvariant="normal">p</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mfenced>
    <mo>âˆ˜</mo> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mi>I</mi></mfenced></mrow></math>
- en: 'WhereÂ  <math alttext="upper I"><mi>I</mi></math> Â is the identity matrix, and
    the product <math alttext="normal w Subscript t Superscript w Baseline normal
    p Subscript t minus 1"><mrow><msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mi>w</mi></msubsup> <msub><mi mathvariant="normal">p</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
    Â is an outer product. To achieve this vectorization, we define a new operator,
    the pairwise-addition of vectors, denoted byÂ  <math alttext="circled-plus"><mo>âŠ•</mo></math>
    . This new operator is simply defined as:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="u circled-plus v equals Start 3 By 3 Matrix 1st Row 1st Column
    u 1 plus v 1 2nd Column  ellipsis 3rd Column u 1 plus v Subscript n Baseline 2nd
    Row 1st Column  ellipsis 2nd Column  ellipsis 3rd Column  ellipsis 3rd Row 1st
    Column u Subscript n Baseline plus v 1 2nd Column  ellipsis 3rd Column u Subscript
    n Baseline plus v Subscript n EndMatrix"><mrow><mi>u</mi> <mo>âŠ•</mo> <mi>v</mi>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><msub><mi>u</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>v</mi> <mn>1</mn></msub></mrow></mtd> <mtd><mo>â‹¯</mo></mtd>
    <mtd><mrow><msub><mi>u</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></mtd></mtr>
    <mtr><mtd><mo>â‹®</mo></mtd> <mtd><mo>â‹±</mo></mtd> <mtd><mo>â‹®</mo></mtd></mtr> <mtr><mtd><mrow><msub><mi>u</mi>
    <mi>n</mi></msub> <mo>+</mo> <msub><mi>v</mi> <mn>1</mn></msub></mrow></mtd> <mtd><mo>â‹¯</mo></mtd>
    <mtd><mrow><msub><mi>u</mi> <mi>n</mi></msub> <mo>+</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="u circled-plus v equals Start 3 By 3 Matrix 1st Row 1st Column
    u 1 plus v 1 2nd Column  ellipsis 3rd Column u 1 plus v Subscript n Baseline 2nd
    Row 1st Column  ellipsis 2nd Column  ellipsis 3rd Column  ellipsis 3rd Row 1st
    Column u Subscript n Baseline plus v 1 2nd Column  ellipsis 3rd Column u Subscript
    n Baseline plus v Subscript n EndMatrix"><mrow><mi>u</mi> <mo>âŠ•</mo> <mi>v</mi>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><msub><mi>u</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>v</mi> <mn>1</mn></msub></mrow></mtd> <mtd><mo>â‹¯</mo></mtd>
    <mtd><mrow><msub><mi>u</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></mtd></mtr>
    <mtr><mtd><mo>â‹®</mo></mtd> <mtd><mo>â‹±</mo></mtd> <mtd><mo>â‹®</mo></mtd></mtr> <mtr><mtd><mrow><msub><mi>u</mi>
    <mi>n</mi></msub> <mo>+</mo> <msub><mi>v</mi> <mn>1</mn></msub></mrow></mtd> <mtd><mo>â‹¯</mo></mtd>
    <mtd><mrow><msub><mi>u</mi> <mi>n</mi></msub> <mo>+</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'This operator adds a little bit to the memory requirements of the implementation,
    but not as much as the case in the loop-based implementation. With this vectorized
    reformulation of the update rule, we rewrite a more memory- and time-efficient
    implementation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A similar process could be made for the allocation weightings rule. Instead
    of having a single rule for each element in the weighting vector, we can decompose
    it into a few operations that work on the whole vector at once:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: While sorting the usage vector to get the free list, we also grab the sorted
    usage vector itself.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We calculate the cumulative product vector of the sorted usage. Each element
    of that vector is the same as the product term in our original element-wise rule.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We multiply the cumulative product vector by (1 â€“ the sorted usage vector).
    The resulting vector is the allocation weighting but in the sorted order, not
    the original order of the memory location.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each element of that out-of-order allocation weighting, we take its value
    and put it in the corresponding index in the free list. The resulting vector is
    now the correct allocation weighting that we want.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[FigureÂ 12-11](#fig0744) summarizes this process with a numerical example.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1211.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: Figure 12-11\. The vectorized process of calculating the allocation weightings
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It may seem that we still need loops for the sorting operation in step 1 and
    for reordering the weights in step 4, but fortunately PyTorch provides symbolic
    operations that would allow us to carry out these operations without the need
    for a Python loop.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'For sorting weâ€™ll be using `torch.topk`. This operation takes a tensor and
    a number <math alttext="k"><mi>k</mi></math> , and returns both the sorted top
    <math alttext="k"><mi>k</mi></math> values in descending order and the indices
    of these values. To get the sorted usage vector in ascending order, we need to
    get the top *N*Â values of the negative of the usage vector. We can bring back
    the sorted values to their original signs by multiplying the resulting vector
    by <math alttext="negative 1"><mrow><mo>-</mo> <mn>1</mn></mrow></math> :'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For reordering the allocation weights, we first create an empty tensor array
    of size N to be the container of the weights in their correct order, and then
    put the values in their correct places using the instance method `scatter(indices,
    values)`. This method takes in its second argument a tensor, and scatters the
    values along its first dimension across the array, with the first argument being
    a list of indices of the locations to which we want to scatter the corresponding
    values. In our case here, the first argument is the free list, and the second
    is the out-of-order allocation weightings. Once we get the array with the weights
    in the correct places, we use another instance method, `pack()`, to wrap up the
    whole array into a `Tensor` object:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The last part of the implementation that requires looping is the controller
    loop itselfâ€”the loop that goes over each step of the input sequence to process
    it. Because vectorization works only when operations are defined element-wise,
    the controllerâ€™s loop canâ€™t be vectorized. Fortunately, PyTorch still provides
    us with a method to escape Pythonâ€™s `for` loops and their massive performance
    hit; this method is the *symbolic loop.Â *A symbolic loop works like most of our
    symbolic operations: Â instead of unrolling the actual loop into the graph, it
    defines a node that would be executed as a loop when the graph is executed.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ll leave the symbolic loop implementation in PyTorch up to the reader. More
    information on how you can use symbolic loops in PyTorch can be found in the torch.fx
    [documentation](https://oreil.ly/qtgBt).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow implementation of our symbolic loop can be found in the *train_babi.py*
    file in the code repository.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Teaching a DNC to Read and Comprehend
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in the chapter, when we were talking about neural n-grams, we said that
    itâ€™s not of the complexity of an AI that can answer questions after reading a
    story. Now we have reached the point where we can build such a system because
    this is exactly what DNCs do when applied on the bAbI dataset.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'The bAbI dataset is a synthetic dataset consisting of 20 sets of stories, questions
    on those stories, and their answers. Each set represents a specific and unique
    task of reasoning and inference from text. In the version weâ€™ll use, each task
    contains 10,000 questions for training and 1,000 questions for testing. For example,
    the following story (from which the passage we saw earlier was adapted) is from
    the *lists-and-sets* task where the answers to the questions are lists/sets of
    objects mentioned in the story:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is taken directly from the dataset, and as you can see, a story is organized
    into numbered sentences that start from 1\. Each question ends with a question
    mark, and the words that directly follow the question mark are the answers. If
    an answer consists of more than one word, the words are separated by commas. The
    numbers that follow the answers are supervisory signals that point to the sentences
    that contain the answersâ€™ words.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: To make the tasks more challenging, weâ€™ll discard these supervisory signals
    and let the system learn to read the text and figure out the answer on its own.
    Following the DNC paper, weâ€™ll preprocess our dataset by removing all the numbers
    and punctuation except for â€œ?â€ and â€œ.â€, bringing all the words to lowercase, and
    replacing the answer words with dashes â€œ-â€ in the input sequence. After this we
    get 159 unique words and marks (lexicons) across all the tasks, so weâ€™ll encode
    each lexicon as a one-hot vector of size 159, no embeddings, just the plain words
    directly. Finally, we combine all of the 200,000 training questions to train the
    model jointly on them, and we keep each taskâ€™s test questions separate to test
    the trained model afterward on each task individually. This whole process is implemented
    in the *preprocess.py* file in the code repository.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: To train the model, we randomly sample a story from the encoded training data,
    pass it through the DNC with an LSTM controller, and get the corresponding output
    sequence. We then measure the loss between the output sequence and the desired
    sequence using the softmax cross-entropy loss, but only on the steps that contain
    answers. All the other steps are ignored by weighting the loss with a weights
    vector that has 1 at the answerâ€™s steps and 0 elsewhere. This process is implemented
    in the *train_babi.py* file.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: After the model is trained, we test its performance on the remaining test questions.
    Our metric will be the percentage of questions the model failed to answer in each
    task. An answer to a question is the word with the largest softmax value in the
    output, or the most probable word. A question is considered to be answered correctly
    if all of its answerâ€™s words are the correct words. If the model failed to answer
    more than 5% of a taskâ€™s questions, we consider that the model failed on that
    task. The testing procedure is found in the *test_babi.py* file.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'After training the model for about 500,000 iterations (cautionâ€”it takes a long
    time!), we can see that itâ€™s performing pretty well on most of the tasks. At the
    same time, itâ€™s performing badly on more difficult tasks like *pathfinding*, where
    the task is to answer questions about how to get from one place to another. The
    following report compares our modelâ€™s results to the mean values reported in the
    original DNC paper:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, weâ€™ve explored the cutting edge of deep learning research with
    NTMs and DNCs, culminating with the implementation of a model that can solve an
    involved reading comprehension task.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter of this book, weâ€™ll begin to explore a very different space
    of problems known as reinforcement learning. Weâ€™ll build an intuition for this
    new class of tasks and develop an algorithmic foundation to tackle these problems
    using the deep learning tools weâ€™ve developed thus far.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch12.xhtml#idm45934167096448-marker)) Source: [Graves et al. â€œNeural
    Turing Machines.â€ (2014)](https://arxiv.org/abs/1410.5401)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
