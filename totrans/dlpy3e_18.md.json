["```py\n!pip install keras-tuner -q \n```", "```py\nimport keras\nfrom keras import layers\n\ndef build_model(hp):\n    # Sample hyperparameter values from the hp object. After sampling,\n    # these values (such as the \"units\" variable here) are just regular\n    # Python constants.\n    units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n    model = keras.Sequential(\n        [\n            layers.Dense(units, activation=\"relu\"),\n            layers.Dense(10, activation=\"softmax\"),\n        ]\n    )\n    # Different kinds of hyperparameters are available: Int, Float,\n    # Boolean, Choice.\n    optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n    model.compile(\n        optimizer=optimizer,\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    # The function returns a compiled model.\n    return model \n```", "```py\nimport keras_tuner as kt\n\nclass SimpleMLP(kt.HyperModel):\n    # Thanks to the object-oriented approach, we can configure model\n    # constants as constructor arguments (instead of hardcoding them in\n    # the model-building function).\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n\n    # The build method is identical to our prior build_model standalone\n    # function.\n    def build(self, hp):\n        units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n        model = keras.Sequential(\n            [\n                layers.Dense(units, activation=\"relu\"),\n                layers.Dense(self.num_classes, activation=\"softmax\"),\n            ]\n        )\n        optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n        model.compile(\n            optimizer=optimizer,\n            loss=\"sparse_categorical_crossentropy\",\n            metrics=[\"accuracy\"],\n        )\n        return model\n\nhypermodel = SimpleMLP(num_classes=10) \n```", "```py\ntuner = kt.BayesianOptimization(\n    # Specifies the model-building function (or hypermodel instance)\n    build_model,\n    # Specifies the metric that the tuner will seek to optimize. Always\n    # specify validation metrics, since the goal of the search process\n    # is to find models that generalize!\n    objective=\"val_accuracy\",\n    # Maximum number of different model configurations (\"trials\") to\n    # try before ending the search\n    max_trials=20,\n    # To reduce metrics variance, you can train the same model multiple\n    # times and average the results. executions_per_trial is how many\n    # training rounds (executions) to run for each model configuration\n    # (trial).\n    executions_per_trial=2,\n    # Where to store search logs\n    directory=\"mnist_kt_test\",\n    # Whether to overwrite data in the directory to start a new search.\n    # Set this to True if you've modified the model-building function\n    # or to False to resume a previously started search with the same\n    # model-building function.\n    overwrite=True,\n) \n```", "```py\n>>> tuner.search_space_summary()\nSearch space summary\nDefault search space size: 2\nunits (Int)\n{\"default\": None,\n \"conditions\": [],\n \"min_value\": 128,\n \"max_value\": 1024,\n \"step\": 128,\n \"sampling\": None}\noptimizer (Choice)\n{\"default\": \"rmsprop\",\n \"conditions\": [],\n \"values\": [\"rmsprop\", \"adam\"],\n \"ordered\": False}\n```", "```py\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255\nx_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n# Reserves these for later\nx_train_full = x_train[:]\ny_train_full = y_train[:]\n# Sets aside a validation set\nnum_val_samples = 10000\nx_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:]\ny_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:]\ncallbacks = [\n    # Uses a large number of epochs (you don't know in advance how many\n    # epochs each model will need) and uses an EarlyStopping callback\n    # to stop training when you start overfitting\n    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n]\n# This takes the same arguments as fit() (it simply passes them down to\n# fit() for each new model).\ntuner.search(\n    x_train,\n    y_train,\n    batch_size=128,\n    epochs=100,\n    validation_data=(x_val, y_val),\n    callbacks=callbacks,\n    verbose=2,\n) \n```", "```py\ntop_n = 4\n# Returns a list of HyperParameters objects, which you can pass to the\n# model-building function\nbest_hps = tuner.get_best_hyperparameters(top_n) \n```", "```py\ndef get_best_epoch(hp):\n    model = build_model(hp)\n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            # Note the very high patience value.\n            monitor=\"val_loss\", mode=\"min\", patience=10\n        )\n    ]\n    history = model.fit(\n        x_train,\n        y_train,\n        validation_data=(x_val, y_val),\n        epochs=100,\n        batch_size=128,\n        callbacks=callbacks,\n    )\n    val_loss_per_epoch = history.history[\"val_loss\"]\n    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n    print(f\"Best epoch: {best_epoch}\")\n    return best_epoch \n```", "```py\ndef get_best_trained_model(hp):\n    best_epoch = get_best_epoch(hp)\n    model = build_model(hp)\n    model.fit(\n        x_train_full, y_train_full, batch_size=128, epochs=int(best_epoch * 1.2)\n    )\n    return model\n\nbest_models = []\nfor hp in best_hps:\n    model = get_best_trained_model(hp)\n    model.evaluate(x_test, y_test)\n    best_models.append(model) \n```", "```py\nbest_models = tuner.get_best_models(top_n) \n```", "```py\n# Uses four different models to compute initial predictions\npreds_a = model_a.predict(x_val)\npreds_b = model_b.predict(x_val)\npreds_c = model_c.predict(x_val)\npreds_d = model_d.predict(x_val)\n# This new prediction array should be more accurate than any of the\n# initial ones.\nfinal_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d) \n```", "```py\npreds_a = model_a.predict(x_val)\npreds_b = model_b.predict(x_val)\npreds_c = model_c.predict(x_val)\npreds_d = model_d.predict(x_val)\n# These weights (0.5, 0.25, 0.1, 0.15) are assumed to be learned\n# empirically.\nfinal_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d \n```", "```py\nmodel = keras.Sequential(\n    [\n        keras.layers.Input(shape=(16000,)),\n        keras.layers.Dense(64000, activation=\"relu\"),\n        keras.layers.Dense(8000, activation=\"sigmoid\"),\n    ]\n) \n```", "```py\nhalf_kernel_0 = kernel[:, :32000]\nhalf_bias_0 = bias[:32000]\n\nhalf_kernel_1 = kernel[:, 32000:]\nhalf_bias_1 = bias[32000:]\n\nwith keras.device(\"gpu:0\"):\n    half_output_0 = keras.ops.matmul(inputs, half_kernel_0) + half_bias_0\n\nwith keras.device(\"gpu:1\"):\n    half_output_1 = keras.ops.matmul(inputs, half_kernel_1) + half_bias_1 \n```", "```py\nkeras.distribution.set_distribution(keras.distribution.DataParallel()) \n```", "```py\nkeras.distribution.list_devices() \n```", "```py\nkeras.distribution.set_distribution(\n    keras.distribution.DataParallel([\"gpu:0\", \"gpu:1\"])\n) \n```", "```py\ngpu:0   |   gpu:4\n--------|---------\ngpu:1   |   gpu:5\n--------|---------\ngpu:2   |   gpu:6\n--------|---------\ngpu:3   |   gpu:7 \n```", "```py\ndevice_mesh = keras.distribution.DeviceMesh(\n    # We assume eight devices, organized as a 2 Ã— 4 grid.\n    shape=(2, 4),\n    # It's convenient to give your axes meaningful names!\n    axis_names=[\"data\", \"model\"],\n) \n```", "```py\ndevices = [f\"gpu:{i}\" for i in range(8)]\ndevice_mesh = keras.distribution.DeviceMesh(\n    shape=(2, 4),\n    axis_names=[\"data\", \"model\"],\n    devices=devices,\n) \n```", "```py\n{\n    # None means \"replicate the variable along this dimension.\"\n    \"sequential/dense_1/kernel\": (None, \"model\"),\n    # \"model\" means \"shard the variable along this dimension across the\n    # devices of the model axis of the device mesh.\"\n    \"sequential/dense_1/bias\": (\"model\",),\n    ...\n} \n```", "```py\nfor v in model.variables:\n    print(v.path) \n```", "```py\nsequential/dense/kernel\nsequential/dense/bias\nsequential/dense_1/kernel\nsequential/dense_1/bias \n```", "```py\nlayout_map = keras.distribution.LayoutMap(device_mesh)\nlayout_map[\"sequential/dense/kernel\"] = (None, \"model\")\nlayout_map[\"sequential/dense/bias\"] = (\"model\",)\nlayout_map[\"sequential/dense_1/kernel\"] = (None, \"model\")\nlayout_map[\"sequential/dense_1/bias\"] = (\"model\",) \n```", "```py\nmodel_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    # This argument tells Keras to use the mesh axis named \"data\" for\n    # data parallelism.\n    batch_dim_name=\"data\",\n)\nkeras.distribution.set_distribution(model_parallel) \n```", "```py\n>>> model.layers[0].kernel.value.sharding\nNamedSharding(\n    mesh=Mesh(\"data\": 2, \"model\": 4),\n    spec=PartitionSpec(None, \"model\")\n)\n```", "```py\nimport jax\n\nvalue = model.layers[0].kernel.value\njax.debug.visualize_sharding(value.shape, value.sharding) \n```", "```py\nmodel.compile(..., steps_per_execution=8) \n```", "```py\nimport keras\n\nkeras.config.set_dtype_policy(\"float16\") \n```", "```py\nimport keras\n\nkeras.config.set_dtype_policy(\"mixed_float16\") \n```", "```py\noptimizer = keras.optimizers.Adam(learning_rate=1e-3, loss_scale_factor=10) \n```", "```py\noptimizer = keras.optimizers.LossScaleOptimizer(\n    keras.optimizers.Adam(learning_rate=1e-3)\n) \n```", "```py\nfrom keras import ops\n\nx = ops.array([[0.1, 0.9], [1.2, -0.8]])\nkernel = ops.array([[-0.1, -2.2], [1.1, 0.7]]) \n```", "```py\ndef abs_max_quantize(value):\n    # Max of absolute value of the tensor\n    abs_max = ops.max(ops.abs(value), keepdims=True)\n    # Scale is max of int range divided by max of tensor (1e-7 is to\n    # avoid dividing by 0).\n    scale = ops.divide(127, abs_max + 1e-7)\n    # Scales the value\n    scaled_value = value * scale\n    # Rounding and clipping first is more accurate than directly\n    # casting.\n    scaled_value = ops.clip(ops.round(scaled_value), -127, 127)\n    # Casts to int8\n    scaled_value = ops.cast(scaled_value, dtype=\"int8\")\n    return scaled_value, scale\n\nint_x, x_scale = abs_max_quantize(x)\nint_kernel, kernel_scale = abs_max_quantize(kernel) \n```", "```py\nint_y = ops.matmul(int_x, int_kernel)\ny = ops.cast(int_y, dtype=\"float32\") / (x_scale * kernel_scale) \n```", "```py\n>>> y\narray([[ 0.9843736,  0.3933239],\n       [-1.0151455, -3.1965137]])\n>>> ops.matmul(x, kernel)\narray([[ 0.98      ,  0.40999997],\n       [-1\\.        , -3.2       ]])\n```", "```py\n# Instantiates a model (or any quantizable layer)\nmodel = ...\n# Boom!\nmodel.quantize(\"int8\")\n# Now predict() and call() will run (partially) in int8!\npredictions = model.predict(...) \n```"]