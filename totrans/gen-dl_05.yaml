- en: Part II. Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Part II](#part_methods) we will dive into the six families of generative
    models, including the theory behind how they work and practical examples of how
    to build each type of model.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.xhtml#chapter_vae) we shall take a look at our first generative
    deep learning model, the *variational autoencoder*. This technique will allow
    us to not only generate realistic faces, but also alter existing images—for example,
    by adding a smile or changing the color of someone’s hair.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 4](ch04.xhtml#chapter_gan) explores one of the most successful generative
    modeling techniques of recent years, the *generative adversarial network*. We
    shall see the ways that GAN training has been fine-tuned and adapted to continually
    push the boundaries of what generative modeling is able to achieve.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.xhtml#chapter_autoregressive) we will delve into several
    examples of *autoregressive models*, including LSTMs and PixelCNN. This family
    of models treats the generation process as a sequence prediction problem—it underpins
    today’s state-of-the-art text generation models and can also be used for image
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](ch06.xhtml#chapter_flow) we will cover the family of *normalizing
    flow models*, including RealNVP. This model is based on a change of variables
    formula, which allows the transformation of a simple distribution, such as a Gaussian
    distribution, into a more complex distribution in way that preserves tractability.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.xhtml#chapter_energy_based_models) introduces the family of
    *energy-based models*. These models train a scalar energy function to score the
    validity of a given input. We will explore a technique for training energy-based
    models called contrastive divergence and a technique for sampling new observations
    called Langevin dynamics.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in [Chapter 8](ch08.xhtml#chapter_diffusion) we shall explore the family
    of *diffusion models*. This technique is based on the idea of iteratively adding
    noise to an image and then training a model to remove the noise, giving us the
    ability to transform pure noise into realistic samples.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of [Part II](#part_methods) you will have built practical examples
    of generative models from each of the six generative modeling families and be
    able to explain how each works from a theoretical perspective.
  prefs: []
  type: TYPE_NORMAL
