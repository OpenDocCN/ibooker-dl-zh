- en: 2 Clustering techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Clustering techniques and salient use cases in the industry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple k-means, hierarchical, and density-based spatial clustering algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of algorithms in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A case study on cluster analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplicity is the ultimate sophistication.—Leonardo da Vinci
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nature loves simplicity and teaches us to follow the same path. Most of the
    time, our decisions are simple choices. Simple solutions are easier to comprehend,
    less time-consuming, and painless to maintain and ponder over. The machine learning
    world is no different. An elegant machine learning solution is not the one that
    is the most complicated algorithm available but the one that solves the business
    problem. A robust machine learning solution is easy enough to readily decipher
    and pragmatic enough to implement. Clustering solutions are generally easier to
    understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we defined unsupervised learning and discussed the
    various unsupervised algorithms available. We will cover each of those algorithms
    as we work through this book; in this second chapter, we focus on the first of
    these: clustering algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: We will define clustering first and then study the different types of clustering
    techniques. We will examine the mathematical foundation, accuracy measurements,
    and pros and cons of each algorithm. We will implement three of these algorithms
    using Python code on a dataset to complement theoretical knowledge. The chapter
    ends with the various use cases of clustering techniques in the pragmatic business
    scenario to prepare for the actual business world. This technique is followed
    throughout the book—we study the concepts first, implement the actual code to
    enhance the Python skills, and then dive into real-world business problems.
  prefs: []
  type: TYPE_NORMAL
- en: We study basic clustering algorithms in this chapter, which are k-means clustering,
    hierarchical clustering, and density-based spatial clustering of applications
    with noise (DBSCAN) clustering. These clustering algorithms are generally the
    starting points whenever we want to study clustering. In the later chapters of
    the book, we will explore more complex algorithms like spectrum clustering, Gaussian
    mixture models, time series clustering, fuzzy clustering, and others. If you have
    a good understanding of k-means clustering, hierarchical clustering, and DBSCAN,
    you can skip to the next chapter. Still, it is advisable to read this chapter
    once—you might find something useful to refresh your concepts!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first understand what we mean by clustering. Good luck on your journey
    to master unsupervised learning–based clustering techniques!
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use the latest version of Python in this chapter. A basic understanding of
    Python and code execution is expected. You are advised to refresh your knowledge
    of object-oriented programming and Python.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, we use Jupyter Notebook to execute the code. Jupyter offers
    flexibility in execution and debugging. It is quite user-friendly and is platform
    or operating-system agnostic. So, if you are using Windows, macOS, or Linux, Jupyter
    should work just fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the datasets and code files are checked into the GitHub repository at [https://mng.bz/lYq2](https://mng.bz/lYq2).
    You need to install the following Python libraries to execute the code: `numpy`,
    `pandas`, `matplotlib`, `scipy`, and `sklearn`. CPU is good enough for execution,
    but if you face some computing lags and would like to speed up the execution,
    switch to GPU or Google Collaboratory (Colab). Google Colab offers free computation
    for machine learning solutions. I recommend studying more about Google Colab and
    how to use it for training machine learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider this scenario: a group of children is asked to group the items in
    a room into different segments. Each child can use their own logic. Some might
    group the objects based on weight; other children might use material or color;
    while yet others might use all three: weight, material, and color. There are many
    permutations, and they depend on the parameters used for grouping. Here, a child
    is segmenting or clustering objects based on the chosen logic.'
  prefs: []
  type: TYPE_NORMAL
- en: Formally put, *clustering* is used to group objects with similar attributes
    in the same segments and objects with different attributes in different segments.
    The resultant clusters share similarities within themselves while they are more
    heterogeneous between each other. We can understand this better by looking at
    figure 2.1\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F01_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 Clustering is grouping objects with similar attributes into logical
    segments. The grouping is based on a similar trait shared by different observations,
    and hence they are gathered into a group. We are using shape as a variable for
    clustering here.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Cluster analysis is not one individual algorithm or solution; rather it is used
    as a problem-solving mechanism in practical business scenarios. It is a class
    of algorithms under unsupervised learning and an iterative process following a
    logical approach and qualitative business inputs. It results in the generation
    of a thorough understanding of the data and the logical patterns in it, pattern
    discovery, and information retrieval. As an unsupervised approach, clustering
    does not need a target variable. It performs segmenting by analyzing underlying
    patterns in the dataset, which are generally multidimensional and, hence, difficult
    to analyze with traditional methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, we want the clustering algorithms to have the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: The output clusters should be easy to explain and comprehend, usable, and make
    business sense. The number of clusters should not be too few or too many. For
    example, it is not ideal to have only two clusters, and the division is not clear
    and decisive. On the other hand, if we have 20 clusters, handling them will become
    a challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm should not be too sensitive to outliers or missing values or the
    noise in the dataset. Generally put, a good solution will be able to handle multiple
    data types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is advisable for a data analyst/scientist to have a good grip on the business
    domain, althougha good clustering solution may allow analysts with less domain
    understanding to train the clustering algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm should be independent of the order of the input parameters. If
    the order matters, the clustering is biased on the order and hence will add more
    confusion to the process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we generate new datasets continuously, the clusters should be scalable to
    newer training examples and should not be a time-consuming process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As one could imagine, the clustering output will depend on the attributes used
    for grouping. In figure 2.2, there can be two logical groupings for the same dataset,
    and both are equally valid. Hence, it is prudent that the attributes or *variables*
    for clustering are chosen wisely, and often that decision depends on the business
    problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F02_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 Using different attributes for clustering results in different clusters
    for the same dataset. Hence, choosing the correct set of attributes defines the
    final set of results we will achieve.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Along with the attributes used in clustering, the actual technique used also
    makes a big difference. There are quite a few (in fact, more than 80) clustering
    techniques. For the interested audience, we provide a list of all the clustering
    algorithms in the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering can be achieved using a variety of algorithms. These algorithms use
    different methodologies to define similarity between objects—for example, density-based
    clustering, centroid-based clustering, distribution-based methods, and others.
    Multiple techniques, such as Euclidean distance, Manhattan distance, etc., are
    available to measure the distance between objects. The choice of distance measurement
    leads to different similarity scores. We will study these similarity measurement
    parameters in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, we can identify two broad clustering methods: *hard clustering*
    and *soft clustering* (see figure 2.3). When the decision is quite clear that
    an object belongs to a certain class or cluster, it is referred to as hard clustering.
    In hard clustering, an algorithm is quite sure of an object’s class. On the other
    hand, soft clustering assigns a likelihood score for an object belonging to a
    particular cluster. So, a soft clustering method will not put an object into a
    cluster; rather, an object can belong to multiple clusters. Soft clustering sometimes
    is also called *fuzzy* clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F03_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 Hard clustering has distinct clusters, whereas in the case of soft
    clustering, a data point can belong to multiple clusters, and we get a likelihood
    score for a data point to belong to a cluster. The figure on the left is hard
    clustering, and the one on the right is soft clustering.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can broadly classify the clustering techniques as shown in table 2.1\. The
    methods described are not the only ones available. We can have graph-based models,
    overlapping clustering, subspace models, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.1 Classification of clustering methodologies, brief descriptions, and
    examples
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Serial no. | Clustering methodology | Brief description of the method | Example
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | Centroid-based clustering  | Distance from a defined centroid  | k-means  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | Density-based models  | Data points are connected in dense regions in
    a vector space  | DBSCAN, OPTICS  |'
  prefs: []
  type: TYPE_TB
- en: '| 3  | Connectivity-based clustering  | Distance connectivity is the modus
    operandi  | Hierarchical clustering, balanced iterative reducing and clustering
    using hierarchies  |'
  prefs: []
  type: TYPE_TB
- en: '| 4  | Distribution models  | Modeling is based on statistical distributions  |
    Gaussian mixture models  |'
  prefs: []
  type: TYPE_TB
- en: '| 5  | Deep learning models  | Unsupervised neural network based  | Self-organizing
    maps  |'
  prefs: []
  type: TYPE_TB
- en: 'Generally, the six most popular algorithms used in clustering in the industry
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering (with variants like k-medians, k-medoids)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agglomerative clustering or hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian mixture models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balanced iterative reducing and clustering using hierarchies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple other algorithms are available, like Chinese whisper, canopy clustering,
    SUBCLU, FLAME, and others. We will study the first three algorithms in this chapter
    and some of the advanced ones in subsequent chapters in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Use these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN clustering is a centroid-based clustering technique. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clustering is a supervised learning technique with a fixed target variable.
    True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between hard clustering and soft clustering?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2.3 Centroid-based clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Centroid-based algorithms measure the similarity of the objects based on their
    distance to the centroid of the clusters (for more information on centroids, see
    the appendix). The distance is measured between a specific data point to the centroid
    for the cluster. The smaller the distance, the higher the similarity. We can understand
    the concept by looking at figure 2.4\. The figure on the right side represents
    the respective centroids for each of the group of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: TIP  To get more clarity on the concept of centroid and other mathematical concepts,
    refer to the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F04_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 Centroid-based clustering methods create a centroid for the respective
    clusters, and the similarity is measured based on the distance from the centroid.
    In this case, we have five centroids; hence, we have five distinct clusters.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In clustering, distance plays a central role as many algorithms use it as a
    metric to measure the similarity. In centroid-based clustering, distance is measured
    between points and between centroids. There are multiple ways to measure the distance.
    The most widely used are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Euclidean distance*—This is the most common distance metric used. It represents
    the straight-line distance between the two points in space and is the shortest
    path between the two points. For example, if we want to calculate the distance
    between points *P*[1] and *P*[2] where coordinates are (*x*[1], *y*[1]) for *P*[1]
    and (*x*[2], *y*[2]) for *P*[2], Euclidean distance is given by equation 2.1\.
    The geometric representation is shown in figure 2.5:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Distance = √(*y*[2] – *y*[1])[²] + (*x*[2] – *x*[1])[²]
  prefs: []
  type: TYPE_NORMAL
- en: '*Chebyshev distance*—Named after Russian mathematician Pafnuty Chebyshev, this
    is defined as the distance between two points such that their differences are
    maximum value along any coordinate dimension. Mathematically, we can represent
    Chebyshev distance in equation 2.2 and as shown in figure 2.5:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Chebyshev distance = max (|*y*[2] – *y*[1]|, |*x*[2] – *x*[1]|)
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F05_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 Euclidean distance, Chebyshev distance, Manhattan distance, and cosine
    distance are the primary distance metrics used. Note how the distance is different
    for two points using these metrics. In Euclidean distance, the direct distance
    is measured between two points, as shown by the first figure on the left.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Manhattan distance*—This is a very easy concept. It simply calculates the
    distance between two points in a grid-like path, and the distance is hence measured
    along the axes at right angles. Hence, sometimes it is also referred to as city
    block distance or the taxicab metric. Mathematically, we can represent the Manhattan
    distance in equation 2.3 and as shown in figure 2.5:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2.3)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Manhattan distance = (|*y*[2] – *y*[1]| + |*x*[2] – *x*[1]|)
  prefs: []
  type: TYPE_NORMAL
- en: Manhattan distance is in L1 norm form while Euclidean distance is in L2 norm
    form. Refer to the appendix to study the L1 norm and L2 norm in detail. If we
    have a high number of dimensions or variables in the dataset, Manhattan distance
    is a better choice than Euclidean distance. This is due to the *curse of dimensionality,*
    which we will study in chapter 3.
  prefs: []
  type: TYPE_NORMAL
- en: '*Cosine distance*—Cosine distance is used to measure the similarity between
    two points in a vector-space diagram. In trigonometry, the cosine of 0 is 1 and
    the cosine of 90⁰ is 0\. Hence, if two points are similar to each other, the angle
    between them will be zero; hence, cosine will be 1, which means the two points
    are very similar to each other and vice versa. Mathematically, cosine similarity
    is shown in equation 2.4\. If we want to measure the cosine between two vectors
    *A* and *B*, then cosine is'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2.4)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Cosine distance = (*A* . *B*) / (||*A*|| ||*B*||)
  prefs: []
  type: TYPE_NORMAL
- en: TIP  If you want to refresh your knowledge on the concepts of vector factorization,
    refer to the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: Other distance-measuring metrics, such as Hamming distance, Jaccard distance,
    and others, are available. Mostly, we use Euclidean distance in our pragmatic
    business problems, but other distance metrics are also used sometimes.
  prefs: []
  type: TYPE_NORMAL
- en: Note  These distance metrics are true for other clustering algorithms too. I
    recommend testing the Python codes in the book with different distance metrics
    and comparing the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the various distance metrics, we proceed to study k-means
    clustering, which is the most widely used algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 K-means clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: K-means clustering is an easy and straightforward approach. It is arguably the
    most widely used clustering method to segment data points and create nonoverlapping
    clusters. We have to specify the number of clusters *k* we wish to create as an
    input, and the algorithm will associate each observation to exactly one of the
    k clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Note  K-means clustering is sometimes confused with the k-nearest neighbor (KNN)
    classifier. Although there is some relationship between the two, KNN is used for
    classification and regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering is quite an elegant approach and starts with some initial
    cluster centers and then iterates to assign each observation to the closest center.
    In the process, the centers are recalculated as the mean of points in the cluster.
    Let’s study the approach used in a step-by-step fashion by using the diagram in
    figure 2.6\. For the sake of simplicity, we are assuming that there are three
    clusters in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that we have all the data points, as shown in step 1\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The three centers are initialized randomly, as shown by three squares: blue,
    red, and green. This input of three is the final number of clusters we wish to
    have.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F06_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 Step 1 represents the raw dataset. In step 2, the algorithm initiates
    three random centroids as we have given the input of the number of clusters as
    three. In step 3, all the neighboring points of the centroids are assigned to
    the same cluster.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 3\. The distance of all the data points is calculated to the centers, and the
    points are assigned to the nearest center. Note that the points have attained
    blue, red, and green colors as they are nearest to those respective centers. (The
    colors are not distinguishable in the print version; hence we have grouped them
    together.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4\. The three centers are readjusted in this step. The centers are recalculated
    as the mean of the points in that cluster, as shown in figure 2.7\. We can see
    that in step 4, the three squares have changed their respective positions as compared
    to step 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F07_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 The centroids are recalculated in step 4\. In step 5, the data points
    are again reassigned new centers. In step 6, the centroids are again readjusted
    as per the new calculations.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5\. The distance of all the data points is recalculated to the new centers and
    the points are reassigned to the nearest centers again. Note that two blue data
    points have become red while a red point has become green in this step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 6\. The centers are again readjusted as they were in step 4\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 7\. The data points are again assigned a new cluster, as shown in figure 2.8\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 8\. The process will continue until convergence is achieved. In other words,
    the process continues until there is no more reassignment of the data points;
    hence, we cannot improve the clustering further, and the final clustering is achieved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F08_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 The centroids are recalculated, and this process continues until
    we can no longer improve the clustering. Then the process stops, as shown in step
    8.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The objective of k-means clustering is to ensure that the within-cluster variation
    is as small as possible while the difference between clusters is as big as possible.
    In other words, the members of the same cluster are most similar to each other,
    while members in different clusters are dissimilar. Once the results no longer
    change, we can conclude that a local optimum has been reached, and clustering
    can stop. Hence, the final clusters are homogeneous within themselves while heterogeneous
    with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is imperative to note two points here:'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering initializes the centers randomly; hence it finds a local
    optimum solution rather than a global optimum solution. Thus it is advisable to
    iterate the solution multiple times and choose the best output from all the results.
    By iteration, we mean to repeat the process multiple times, as in each of the
    iterations, the centroid chosen randomly will be different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have to input the number of final clusters *k* we wish to have, and it changes
    the output drastically. A very small value of *k* relative to the data size will
    result in redundant clusters as they will not be of any use. In other words, if
    we have a very small value of *k* relative to big-sized data, data points with
    different characteristics will be cobbled together in a few groups. Having a very
    high value of *k* will create clusters that are different from each other minutely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, having a very high number of clusters will be difficult to manage
    and refresh in the long run. Let’s study an example. If a telecom operator has
    1 million subscribers, and if we take the number of clusters as two or three,
    the resultant cluster size will be very large. This can also lead to different
    customers classified in the same segment. On the other hand, if we take the number
    of clusters as 50 or 60, due to the sheer number of clusters, the output becomes
    unmanageable to use, analyze, and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: With different values of *k*, we get different results; hence, it is necessary
    that we understand how we can choose the optimum number of clusters for a dataset.
    Now let’s examine the process to measure the accuracy of clustering solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Measuring the accuracy of clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One objective of clustering is to find the cleanest clusters. Theoretically
    (though not ideally), if we have the same number of clusters as the number of
    observations, the results will be completely accurate. In other words, if we have
    1 million customers, the purest clustering will have 1 million clusters, wherein
    each customer is in a separate cluster. But it is not the best approach and is
    not a pragmatic solution either. Clustering intends to create a group of similar
    observations in one cluster, and we use the same principle to measure the accuracy
    of our solution. Other options include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Within the cluster sum of squares (WCSS) or cohesion*—This index measures
    the variability of the data points with respect to the distance they are from
    the centroid of the cluster. This metric is the average distance of each data
    point from the cluster’s centroid, which is repeated for each data point. If the
    value is too large, it shows there is a large data spread, whereas the smaller
    value indicates that the data points are quite similar and homogeneous and hence
    the cluster is compact.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, this intracluster distance is also referred to as *inertia* for that
    cluster. It is simply the summation of all the distances. The lower the value
    of inertia, the better the cluster is.
  prefs: []
  type: TYPE_NORMAL
- en: '*Intercluster sum of squares*—This metric is used to measure the distance between
    centroids of all the clusters. To get it, we measure the distance between centroids
    of all the clusters and divide it by the number of clusters to get the average
    value. The bigger it is, the better the clustering is, indicating that clusters
    are heterogeneous and distinguishable from each other, as represented in figure
    2.9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F09_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 Intracluster vs. intercluster distance. Both are used to measure
    the purity of the final clusters and the performance of the clustering solution.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Silhouette value*—This is one of the metrics used to measure the success of
    clustering. It ranges from –1 to +1, and a higher value is better. It measures
    how a data point is similar to other data points in its own cluster as compared
    to other clusters. As a first step, for each observation we calculate the average
    distance from all the data points in the same cluster; let’s call it *x*[*i*].
    Then we calculate the average distance from all the data points in the nearest
    cluster; let’s call it *y*[*i*].We will then calculate the coefficient by equation
    2.5:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2.5)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Silhouette coefficient = (*y*[*i*] – *x*[*i*])/ max (*y*[*i*], *x*[*i*])
  prefs: []
  type: TYPE_NORMAL
- en: If the value of coefficient is –1, it means that the observation is in the wrong
    cluster. If it is 0, the observation is very close to the neighboring clusters.
    If the value of coefficient is +1, it means that the observation is at a distance
    from the neighboring clusters. Hence, we would expect to get the highest value
    for the coefficient to have a good clustering solution.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dunn index*—Thiscan also be used to measure the efficacy of the clustering.
    It uses the inter- and intradistance measurements defined in the previous intercluster
    sum of squares silhouette value sections and is given by equation 2.6:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2.6)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Dunn index = min (intercluster distance)/max (intracluster distance)
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, we would strive to maximize the value of Dunn index. To achieve it,
    the numerator should be as big as possible, implying that clusters are at a distance
    from each other, while the denominator should be as low as possible, signifying
    that the clusters are quite robust and close-packed.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Finding the optimum value of k
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Choosing the optimum number of clusters is not easy. As I said earlier, the
    finest clustering is when the number of clusters equals the number of observations,
    but as we studied in the last section, it is not practically possible. Hence,
    we should provide the number of clusters *k* as an input to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most widely used method for finding the optimum value of *k* is
    the *elbow method.* In this method, we calculate within the cluster sum of squares
    or WCSS for different values of *k*. The process is the same as discussed in the
    last section. Then, WCSS is plotted on a graph against different values of *k*.
    Wherever we observe a kink or elbow, as shown in figure 2.10, we find the optimum
    number of clusters for the dataset. Notice the sharp edge.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F10_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 The elbow method to find the optimal number of clusters. The circle
    shows the kink. However, the final number of clusters is dependent on business
    logic, and often we merge/split clusters based on this. The ease of maintaining
    the clusters also plays a crucial role.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Exercise 2.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering does not require the number of clusters as an input. True
    or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: KNN and k-means clustering are the same thing. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe one possible process to find the optimal value of *k*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: But this does not mean that it is the final number of clusters we suggest for
    the business problem. Based on the number of observations falling in each of the
    clusters, a few clusters might be combined or broken into subclusters. We also
    consider the computation cost required to create the clusters. The higher the
    number of clusters, the greater the computation cost and the time required. We
    can also find the optimum number of clusters using the silhouette coefficient
    we discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Note  It is imperative that the business logic of merging a few clusters or
    breaking a few clusters is explored. Ultimately, the solution has to be implemented
    in real-world business scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have examined the nuts and bolts of k-means clustering—the mathematical
    concepts and the process, the various distance metrics, and determining the best
    value of *k*.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4 Pros and cons of k-means clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The k-means algorithm is quite a popular and widely implemented clustering
    solution. The solution offers the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: It is simple to comprehend and relatively easier to implement as compared to
    other algorithms. The distance measurement calculation makes it quite intuitive
    to understand even by users from nonstatistics backgrounds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the number of dimensions is large, the k-means algorithm is faster than other
    clustering algorithms and creates tighter clusters. Hence, it is preferred if
    the number of dimensions is quite high.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It quickly adapts to new observations and can generalize very well to clusters
    of various shapes and sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution produces results through a series of iterations of recalculations.
    Most of the time, the Euclidean distance metric is used, which makes it less computationally
    expensive. It also ensures that the algorithm surely converges and produces results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K-means is widely used for real-life business problems. Though there are clear
    advantages of k-means clustering, we do face certain challenges with the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the optimum number of clusters is not easy. We should provide it as
    an input. With different values of *k*, the results will be completely different.
    The process of choosing the best value of *k* is explored in the previous section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution is dependent on the initial values of centroids. Since the centroids
    are initialized randomly, the output will be different with each iteration. Hence,
    it is advisable to run multiple versions of the solution and choose the best one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is quite sensitive to outliers. Outliers can mess up the final
    results, and hence it is imperative that we treat outliers before starting with
    clustering. We can also implement other variants of the k-means algorithm, like
    k-modes clustering, to deal with the problem of outliers. We discuss dealing with
    outliers in section 11.4.4 of chapter 11\. You can refer to it if you want to
    know how to deal with outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the basic principle of k-means clustering is to calculate the distance,
    the solution is not directly applicable to categorical variables. In other words,
    we cannot use categorical variables directly since we can calculate the distance
    between numeric values but cannot perform mathematical calculations on categorical
    variables. To resolve this, we can convert categorical variables to numeric ones
    using one-hot encoding. We discuss dealing with categorical variables in section
    11.4.2 of chapter 11\. You can refer to it if you want to know how to deal with
    categorical variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Despite these problems, k-means clustering is one of the most used clustering
    solutions due to its simplicity and ease of implementation. There are different
    implementations of k-means algorithms like k-median, k-medoids, etc., which are
    sometimes used to resolve the problems faced:'
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, *k-median clustering* is based on the medians of the dataset
    as compared to the centroid in k-means. This increases the amount of computation
    time as the median can be found only after the data has been sorted. But at the
    same time, k-means is sensitive to outliers whereas k-median is less affected
    by them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*K-medoids clustering* is one of the variants of the k-means algorithm. Medoids
    are similar to means, except they are always from the same dataset and are implemented
    when it is difficult to get means, like with images. A medoid can be thought of
    as the most central point in a cluster that is least dissimilar to all the other
    members in the cluster. K-medoids choose the actual observations as the centers
    as compared to k-means, where the centroids may not even be part of the data.
    It is less sensitive to outliers as compared to the k-means clustering algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other versions too, including k-means++, mini-batch k-means, and others.
    Generally, in the industry, k-means is used for most of the clustering solutions.
    You can explore other options like k-means++, mini-batch k-means, etc., if the
    results are not desirable or if the computation is taking a lot of time. Moreover,
    having different distance measurement metrics may produce different results for
    the k-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This section concludes our discussion on the k-means clustering algorithm. It
    is time to hit the lab and develop actual Python code!
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.5 K-means clustering implementation using Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now create a Python solution for k-means clustering. In this case,
    we are using the dataset from the link at GitHub at [https://mng.bz/lYq2](https://mng.bz/lYq2).
    This dataset has information about the features of four models of cars. Based
    on the features of the car, we are going to group them into different clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries and the dataset into a dataframe. Here, `vehicles.csv`
    is the input data file. If the data file is not in the same folder as the Jupyter
    notebook, you would have to provide the complete path to the file. `dropna` is
    used to remove the missing values, if any:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Perform some initial checks on the data, like shape, info, top five rows,
    distribution of classes, etc. This is to ensure that we have loaded the complete
    dataset and there is no corruption while loading the dataset. The `Shape` command
    will give the number of rows and columns in the data, `info` will describe all
    the variables and their respective types, and `head` will display the first five
    rows. The `value_counts` displays the distribution for the `class` variable. Or,
    in other words, `value_counts` returns the count of the unique values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Generate two plots for the variable `class`. The dataset has more examples
    from `car` while for `bus` and `van` it is balanced data. I used the `matplotlib`
    library to plot these graphs. The outputs of the plots are as follows (see figure
    2.11):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH02_F11_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 Two plots for the variable `class`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '4\. Check for any missing data points in the dataset. There are no missing
    data points in our dataset, as we have already dealt with them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note  We cover the methods to deal with missing values in section 11.4.3 of
    chapter 11 as dropping the missing values is generally not the best approach.
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Standardize the dataset. It is a good practice to standardize the dataset
    for clustering. It is important, as the different dimensions might be on a different
    scale, and one dimension may dominate the computation of distance if its values
    are naturally much larger than other dimensions. This is done using the `StandardScaler()`
    function. Refer to the appendix to examine different scaling techniques:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Have a quick look at the dataset by generating a scatter plot. The plot
    displays the distribution of all the data points we have created as `X_standard`
    in the last step (see figure 2.12):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH02_F12_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 A scatter plot of the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 7\. Perform k-means clustering. First, we have to select the optimum number
    of clusters using the elbow method. From the `sklearn` library, we import `KMeans`.
    In a `for` loop, we iterate for the values of clusters from 1 to 10\. In other
    words, the algorithm will create between 1 and 10 clusters and will then generate
    the results for us to choose the optimal value of *k*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code snippet, the model object contains the output of the
    k-means algorithm, which is then fit on the `X_standard` generated in the last
    step. Here, Euclidean distance has been used as a distance metric (see figure
    2.13):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH02_F13_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 K-means clustering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '8\. As we can observe, the optimal number of clusters is three. It is the point
    where we can observe a sharp kink in the graph. We will continue with k-means
    clustering with the number of clusters as three. While there is nothing special
    about the number 3 here, it is best suited for this dataset. `random_state` is
    a parameter that is used to determine random numbers for centroid initialization.
    We set it to a value to make randomness deterministic. If you want to repeat the
    same results again, use the same random state number. It acts like a seed number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '9\. Get the `centroids` for the clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '10\. Now we use the `centroids` so that they can be profiled by the `columns`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '11\. We will now create a `dataframe` only for the purpose of creating the
    `labels`, and then we convert it into categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '12\. In this step, we join the two `dataframes`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '13\. A `groupby` is done to create a data frame required for the analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '14\. Now we create a visualization for the clusters we have defined. This is
    done using the `mpl_toolkits` library. The logic is simple to understand. The
    data points are colored as per the respective labels. The rest of the steps are
    related to the display of the plot by adjusting the label, title, ticks, etc.
    Since it is not possible to plot all 18 variables in the plot, we have chosen
    3 variables to show in the plot (see figure 2.14):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can also test the preceding code with multiple other values of *k*. We have
    created the code with different values of *k*. In the interest of space, we have
    put the code for testing with different values of *k* at the GitHub location.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F14_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 K-means clustering for the vehicles dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note  Exploratory data analysis holds the key to a robust machine learning solution
    and a successful project. In the subsequent chapters, we will create detailed
    exploratory data analyses for datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we first did a small exploratory analysis of the dataset.
    This was followed by identifying the optimum number of clusters, which in this
    case comes out to be three. Then we implemented k-means clustering. You are expected
    to iterate the k-means solution with different initializations and compare the
    results, iterate with different values of *k*, and visualize to analyze the movements
    of data points.
  prefs: []
  type: TYPE_NORMAL
- en: Centroid-based clustering is one of the most recommended solutions due to its
    less complicated logic, ease of implementation, flexibility, and trouble-free
    maintenance. Whenever we require clustering as a solution, mostly we start with
    creating a k-means clustering solution that acts as a benchmark. The algorithm
    is highly popular and generally one of the first solutions utilized for clustering.
    Then we test and iterate with other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Connectivity-based clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “Birds of a feather flock together” is the principle followed in connectivity-based
    clusters. The core concept is that objects that are connected with each other
    are similar to each other. Hence, based on the connectivity between these objects,
    they are grouped into clusters. An example of such a representation is shown in
    figure 2.15, where we can iteratively group observations. As an example, we are
    initiating with all things, dividing into living and nonliving, and so on. Such
    representation is known as a *dendrogram.* Since there is a tree-like structure,
    connectivity-based clustering is sometimes referred to as hierarchical clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F15_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 Hierarchical clustering utilizes grouping similar objects iteratively.
    Such representation is known as a dendrogram.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Hierarchical clustering fits nicely into human intuition and, hence, is easy
    to comprehend. Unlike k-means clustering, in hierarchical clustering we do not
    have to input the number of final clusters, but the method does require a termination
    condition (i.e., when the clustering should stop). At the same time, hierarchical
    clustering does not suggest the optimum number of clusters. From the hierarchy/dendrogram
    generated, we have to choose the best number of clusters ourselves. We will explore
    this more when we create the Python code for it in subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16 shows an example of hierarchical clustering. Here, the first node
    is the root, which is then iteratively split into nodes and subnodes. Whenever
    a node cannot be split further, it is called a terminal node or *leaf*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F16_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 Hierarchical clustering has a root that splits into nodes and subnodes.
    A node that cannot be split further is called the leaf. In the bottom-up approach,
    a merging of the leaves will take place.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Since there is more than one process or logic to merge the observations into
    clusters, we can generate a large number of dendrograms, which is given by equation
    2.7:'
  prefs: []
  type: TYPE_NORMAL
- en: (2.7)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Number of dendrograms = (2*n* – 3)!/[2^(^(*n*)^(–2)) (*n* – 2)!]
  prefs: []
  type: TYPE_NORMAL
- en: where *n* is the number of observations or the leaves. So, if we have only two
    observations, we can have only one dendrogram. If we have 5 observations, we can
    have 105 dendrograms. Hence, based on the number of observations, we can generate
    a lot of dendrograms.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering can be further classified based on the process used
    to create the grouping of observations, which we explore next.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Types of hierarchical clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the grouping strategy, hierarchical clustering can be subdivided into
    two types: *agglomerative* clustering and *divisive* clustering (see table 2.2).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.2 Different types of hierarchical clustering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Serial no. | Agglomerative clustering | Divisive clustering |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | Bottom-up approach.  | Top-down approach.  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | Each observation creates its own cluster and then merging takes place
    as the algorithm goes up.  | We start with one cluster and then observations are
    iteratively split to create a tree-like structure.  |'
  prefs: []
  type: TYPE_TB
- en: '| 3  | Greedy approach is followed to merge (the greedy approach is described
    below).  | Greedy approach is followed to split.  |'
  prefs: []
  type: TYPE_TB
- en: '| 4  | An observation will find the best pair to merge and the process completes
    when all the observations have merged with each other.  | All the observations
    are taken at the start and then, based on division conditions, splitting takes
    place until all the observations are exhausted or the termination condition is
    met.  |'
  prefs: []
  type: TYPE_TB
- en: Let’s explore the meaning of the greedy approach first. The greedy approach
    or greedy algorithm is any algorithm that makes the best choice at each step without
    considering the effect on future states. In other words, we live in the moment
    and choose the best option from the available choices at that moment. The current
    choice is independent of the future choices, and the algorithm will solve the
    subproblems later. The greedy approach may not provide the most optimal solution
    but generally provides a locally optimal solution that is close to the best solution
    in a reasonable amount of time. Hierarchical clustering follows this greedy approach
    while merging or splitting at a node.
  prefs: []
  type: TYPE_NORMAL
- en: 'We next examine the steps followed in the hierarchical clustering approach:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in figure 2.17, let us say we have five observations in our dataset:
    1, 2, 3, 4, and 5\.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this step, observations 1 and 2 are grouped into one and 4 and 5 are grouped
    into one; 3 is not grouped in any one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this step, we group the output of 4,5 in the last step and observation 3
    into one cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output from step 3 is grouped with the output of 1,2 as a single cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F17_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 Steps followed in hierarchical clustering. From left to right, we
    have agglomerative clustering (merging of the nodes), while from right to left,
    we have divisive clustering (splitting of the nodes).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this approach, from left to right, we have an agglomerative approach, and
    from right to left, a divisive approach is represented. In an agglomerative approach,
    we merge the observations, while in a divisive approach, we split the observations.
    We can use both agglomerative and divisive approaches for hierarchical clustering.
    Divisive clustering is an exhaustive approach and sometimes might take more time
    than the other.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to k-means clustering, the distance metric used to measure plays a significant
    role here. We are aware of and understand how to measure the distance between
    data points, but there are multiple methods to define that distance, which we
    study next.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Linkage criterion for distance measurement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use Euclidean distance, Manhattan distance, Chebyshev distance, and
    others to measure the distance between two observations. At the same time, we
    can employ various methods to define that distance. Based on this input criterion,
    the resultant clusters will be different. The various methods to define the distance
    metric are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Nearest neighbors or single linkages* use the distance between the two nearest
    points in different clusters. The distance between the closest neighbors in distinct
    clusters is calculated, and this is used to determine the next split/merging.
    It is done by an exhaustive search among all the pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Farthest neighbor or complete linkage* is the opposite of the nearest neighbor
    approach. Here, instead of taking the nearest neighbors, we concentrate on the
    most distant neighbors in different clusters. In other words, the distance between
    the clusters is calculated by the greatest distance between two objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Group average linkage* calculates the average of the distances between all
    the possible pairs of objects in two different clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Ward linkage* method aims to minimize the variability of the clusters that
    are getting merged into one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use these options of distance metrics while we are developing the actual
    code for hierarchical clustering and compare the accuracies to determine the best
    distance metrics for the dataset. During algorithm training, the algorithm merges
    the observations, which will minimize the linkage criteria chosen. We can visualize
    the various linkages in figure 2.18.
  prefs: []
  type: TYPE_NORMAL
- en: Note  Such inputs to the algorithm are referred to as hyperparameters. These
    are the values we feed to the algorithm to generate the results as per our requirement,
    and they act as our control on the algorithm. An example of a hyperparameter is
    *k* in k-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F18_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 Single linkage is for closest neighbors (left); complete linkage
    is for farthest neighbors (center); and group average is for the average of the
    distance between clusters (right).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: With this, we have understood the working mechanisms in hierarchical clustering.
    But we have still not addressed the mechanism to determine the optimum number
    of clusters using hierarchical clustering, which we examine next.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.3 Optimal number of clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that in k-means clustering, we have to give the number of clusters as
    an input to the algorithm. We use the elbow method to determine the optimum number
    of clusters. In the case of hierarchical clustering, we do not have to specify
    the number of clusters to the algorithm, but we still have to identify the number
    of final clusters we wish to have. We use a dendrogram to answer that question.
  prefs: []
  type: TYPE_NORMAL
- en: Let us assume that we have 10 data points in total at the bottom of the chart,
    as shown in figure 2.19\. The clusters are merged iteratively until we get the
    one final cluster at the top. The height of the dendrogram at which two clusters
    get merged with each other represents the respective distance between the said
    clusters in the vector-space diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F19_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 Dendrogram to identify the optimum number of clusters. The distance
    between X and Y is more than between A and B and between P and Q; hence, we choose
    that as the cut to create clusters and the number of clusters chosen is five.
    The x-axis represents the clusters, while the y-axis represents the distance (dissimilarity)
    between two clusters.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: From a dendrogram, the number of clusters is given by the number of vertical
    lines being cut by a horizontal line. The *optimum* number of clusters is given
    by the number of vertical lines in the dendrogram cut by a horizontal line such
    that it intersects the tallest of the vertical lines. Or if the cut is shifted
    from one end of the vertical line to another, the length covered is the maximum.
    A dendrogram utilizes branches of clusters to show how closely various data points
    are related to each other. In a dendrogram, clusters that are located at the same
    height level are more closely related than clusters that are located at different
    height levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example shown in figure 2.19, we have shown three potential cuts: AB,
    PQ, and XY. If we take a cut above AB, it will result in two very broad clusters,
    while below PQ will result in nine clusters that will become difficult to analyze
    further.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the distance between X and Y is more than between A and B and between
    P and Q. So we can conclude that the distance between X and Y is the maximum,
    and hence, we can finalize that as the best cut. This cut intersects at five distinct
    points; hence, we should have five clusters. The height of the cut in the dendrogram
    is similar to the value of *k* in k-means clustering. In k-means clustering, *k*
    determines the number of clusters. In hierarchical clustering, the best cut determines
    the number of clusters we wish to have.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to k-means clustering, the final number of clusters is not dependent
    on the choice from the algorithm only. Business acumen and pragmatic logic play
    a vital role in determining the final number of clusters. Recall that one of the
    important attributes of clusters is their usability, which we discussed in section
    2.2\.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we also use cophenetic correlation coefficient to measure how well
    the dendrogram represents the actual pairwise distance between the points. It
    compares the cophenetic distance, which is the height at which two points merged
    first in the dendrogram, with the original dissimilarity between the points.
  prefs: []
  type: TYPE_NORMAL
- en: There is one more index known as the Calinski-Haranasz index. It measures the
    ratio of between-cluster dispersion to within-cluster dispersion. A higher value
    means better clustering, and hence we choose the optimal number of clusters to
    maximize this index.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the greedy approach used in hierarchical clustering?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete linkage is used for finding distances for closest neighbors. True or
    False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between group linkage and ward linkage?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the process to find the most optimal value of *k*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2.4.4 Pros and cons of hierarchical clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hierarchical clustering is a strong clustering technique and is quite popular,
    too. Similar to k-means, it also uses distance as a metric to measure similarity.
    At the same time, there are a few challenges with the algorithm. The advantages
    of hierarchical clustering are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the biggest advantage of hierarchical clustering is the reproducibility
    of results. Recall in k-means clustering, the process starts with random initialization
    of centroids giving different results. In hierarchical clustering, we can reproduce
    the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In hierarchical clustering, we do not have to input the number of clusters to
    segment the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation is easy to implement and comprehend. Since it follows a tree-like
    structure, it is explainable to users from nontechnical backgrounds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dendrogram generated can be interpreted to give a very good understanding
    of the data with a visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the same time, we do face some challenges with hierarchical clustering algorithms,
    which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The biggest challenge we face with hierarchical clustering is the time taken
    to converge. The time complexity for k-means is linear, while for hierarchical
    clustering it is quadratic. For example, if we have “*n*” data points, then for
    k-means clustering the time complexity will be *O*(*n*), while for hierarchical
    clustering it is *O*(*n*³).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TIP  Refer to the appendix if you want to study *O*(*n*).
  prefs: []
  type: TYPE_NORMAL
- en: Since the time complexity is *O*(*n*³), it is a time-consuming task. Moreover,
    the memory required to compute is at least *O*(*n*²), making hierarchical clustering
    quite a time-consuming and memory-intensive process. And this is the problem even
    if the dataset is medium. The computation required might not be a challenge if
    we are using high-end processors, but it surely can be a concern for regular computers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interpretation of dendrograms at times can be subjective; hence due diligence
    is required while interpreting dendrograms. The key to interpreting a dendrogram
    is to focus on the height at which any two data points are connected. It can be
    subjective, as different analysts can decipher different cuts and try to prove
    their methodology. Hence, it is advisable to interpret the results in the light
    of mathematics and marry the results with real-world business problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering cannot undo the previous steps it has done. Even if
    we feel that a connection made is not proper and should be rolled back, there
    is no mechanism to remove the connection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is very sensitive to outliers and messy datasets. The presence
    of outliers, NULL, missing values, duplicates, etc., makes a dataset messy. Hence
    the resultant output might not be proper or what we expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But despite all the challenges, hierarchical clustering is one of the most widely
    used clustering algorithms. Generally, we create both k-means clustering and hierarchical
    clustering for the same dataset to compare the results of the two. If the number
    of clusters suggested and the distribution of respective clusters look similar,
    we get more confident about the clustering methodology used.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered the theoretical background of hierarchical clustering. It is
    time to take action and jump into Python for coding.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.5 Hierarchical clustering case study using Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now create a Python solution for hierarchical clustering using the
    same dataset we used for k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the required libraries and dataset. For this, follow steps 1 to 6 we followed
    for the k-means algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F20_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.20 Hierarchical clustering using average, ward, and complete linking
    methods (top to bottom, respectively)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '2\. Next, we create hierarchical clustering using three linkage methods: average,
    ward, and complete. Then the clusters will be plotted. The input to the method
    is the `X_Standard` variable, the linkage method used, and the distance metric.
    Then, using the `matplotlib` library, we plot the dendrogram. In the following
    code snippet, simply change the method from “average” to “ward” and “complete”
    and get the respective results (see figure 2.20):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '3\. We now want to choose the number of clusters we wish to have. For this
    purpose, let’s re-create the dendrogram by subsetting the last 10 merged clusters.
    We have chosen 10 as it is generally an optimal choice; I advise you to test with
    other values too (see figure 2.21):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F21_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.21 A dendrogram subsetting the last 10 merged clusters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 4\. We observe that the most optimal distance is 10\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '5\. Cluster the data into different groups. By using the logic described in
    the last section, the number of optimal clusters is going to be four:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Plot the distinct clusters using the `matplotlib` library. In the print
    version of the book, you will not see different colors. The output of the Python
    code will have the colors; I advise that you run the code to appreciate the output.
    The same output is available in the GitHub repository (see figure 2.22):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH02_F22_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.22 A plot of the distinct clusters using the `matplotlib` library
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 7\. For different values of distance, the number of clusters will change, and
    the plot will look different. We are showing different results for distances of
    5, 15, and 20 and different numbers of clusters generated for each iteration.
    Figure 2.23 shows that we get completely different results for different values
    of distances as we move from left to right. We should be cautious when we choose
    the value of the distance, and sometimes we might have to iterate a few times
    to get the best value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F23_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.23 The number of clusters using different values of distance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using hierarchical clustering, we segment the data on the left side to the one
    on the right side of figure 2.24\. The left side represents the raw data, while
    on the right, we have a representation of the clustered dataset. In the print
    version of the book, you won’t see the different colors. The output of the Python
    code will have the colors. The same output is available at the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F24_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.24 Segmenting the data using hierarchical clustering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Hierarchical clustering is a robust method and is highly recommended. Along
    with k-means, it creates a great foundation for clustering-based solutions. Most
    of the time, at least these two techniques are used when we create clustering
    solutions, and then we move on to iterate with other methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Density-based clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have studied k-means in the earlier sections. Recall how it uses a centroid-based
    method to assign a cluster to each of the data points. If an observation is an
    outlier, the outlier point pulls the centroid toward itself and is also assigned
    a cluster like a normal observation. These outliers do not necessarily bring information
    to the cluster and can affect other data points disproportionally but are still
    made a part of the cluster. Moreover, getting clusters of arbitrary shapes, as
    shown in figure 2.25, is a challenge with the k-means algorithm. Density-based
    clustering methods solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F25_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.25 DBSCAN is highly recommended for irregular-shaped clusters. With
    k-means, we generally get spherical clusters; DBSCAN can resolve it.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the density-based method, the clusters are identified as the areas that have
    a higher density as compared to the rest of the dataset. In other words, given
    a vector-space diagram where the data points are represented, a cluster is defined
    by adjacent regions or neighboring regions of high-density points. This cluster
    will be separated from other clusters by regions of low-density points. The observations
    in the sparse areas or separating regions are considered noise or outliers in
    the dataset. A few examples of density-based clustering are shown in figure 2.25.
  prefs: []
  type: TYPE_NORMAL
- en: 'We mentioned two terms: neighborhood and density. To understand density-based
    clustering, we will study these terms in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1 Neighborhood and density
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine we represent data observations in a vector-space, and we have a point
    P. We now define the neighborhood for this point P. The representation is shown
    in figure 2.26\. For a point P we have defined an *ε*—neighborhoods for it that
    are the points equidistant from P. In a 2D space, it is represented by a circle;
    in a 3D space it is a sphere; and for a *n*-dimensional space, it is *n*-sphere
    with center P and radius *ε*. This defines the concept of *neighborhood*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F26_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.26 Representation of data points in a vector-space diagram. On the
    right-side we have a point P, and the circle drawn is of radius *ε*. So, for *ε*
    > 0, the neighborhood of P is defined by the set of points that are at less than
    or equal to *ε* distance from the point P.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now let’s explore the term *density*. Recall density is mass divided by volume
    (mass/volume). The higher the mass, the higher the density, and the lower the
    mass, the lower the density. Conversely, the lower the volume, the higher the
    density, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous context, mass is the number of points in the neighborhood. In
    figure 2.26 we can observe the effect of *ε* on the number of data points or the
    mass. When it comes to volume, in the case of 2D space, volume is π*r*², while
    for a sphere that is 3D, it is 4/3 π*r*³. For spheres of *n*-dimensions, we can
    calculate the respective volume as per the number of dimensions, which will be
    π times a numerical constant raised to the number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: So, in the two cases shown in figure 2.27, for a point P, we can get the number
    of points (mass) and volumes, and then we can calculate the respective densities.
    But the absolute values of these densities mean nothing to us; rather how they
    are similar (or different) from nearby areas is what’s important. The points that
    are in the same neighborhood and have similar densities can be grouped into one
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F27_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.27 The effect of radius *ε*. On the left side, the number of points
    is more than on the right side. So the mass of the right side is less, since it
    contains a smaller number of data points.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In an ideal case scenario, we wish to have highly dense clusters with a maximum
    number of points. In the two cases shown in figure 2.28, we have a less dense
    cluster depicted on the left and a high-dense one on the right.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F28_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.28 Denser clusters are preferred over less dense ones. Ideally, a dense
    cluster, with a maximum number of data points, is what we aim to achieve from
    clustering.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: From the preceding discussion, we can conclude that
  prefs: []
  type: TYPE_NORMAL
- en: If we *increase* the value of *ε*, we will get a *higher* volume but not necessarily
    a higher number of points (mass). It depends on the distribution of the data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we *decrease* the value of *ε*, we will get a *lower* volume but not necessarily
    a lower number of points (mass).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the fundamental points we adhere to. Hence, it is imperative that
    we choose clusters that have high density and cover the maximum number of neighboring
    points.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2 DBSCAN clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DBSCAN clustering is one of the highly recommended density-based algorithms.
    It clusters the data observations that are closely packed in a densely populated
    area but does not consider the outliers in low-density regions. Unlike k-means,
    we do not specify the number of clusters, and the algorithm is able to identify
    irregular-shaped clusters, whereas k-means generally proposes spherical-shaped
    clusters. Similar to hierarchical clustering, it works by connecting the data
    points but with the observations that satisfy the density criteria or the threshold
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Note  DBSCAN was proposed in 1996 by Martin Ester, Hans-Peter Kriegal, Jörg
    Sander, and Xiaowei Xu. The algorithm was given the Test of Time award in 2014
    at ACM SIGKDD. The paper can be accessed at [https://mng.bz/BXv1](https://mng.bz/BXv1).
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN works on the concepts of neighborhood we discussed in the last section.
    We will now dive deeper into the working methodology and building blocks of DBSCAN.
  prefs: []
  type: TYPE_NORMAL
- en: nuts and bolts of DBSCAN clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s now examine the core building blocks of DBSCAN clustering. We know it
    is a density-based clustering algorithm, and hence the neighborhood concept is
    applicable here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we have a few data observations that we need to cluster. We also locate
    a data point P. Then we can easily define two hyperparameter terms:'
  prefs: []
  type: TYPE_NORMAL
- en: The radius of the neighborhood around P, known as *ε*, which we discussed in
    the last section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum number of points we wish to have in the neighborhood of P or, in
    other words, the minimum number of points that are required to create a dense
    region. This is referred to as minimum points (minPts). It is one of the parameters
    we can input by applying a threshold on minPts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these concepts, we can classify the observations into three broad
    categories: core points, border or reachable points, and outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Core points*—Any data point x can be termed as a core point if at least minPts
    are within *ε* distance of it (including x itself), shown as squares in figure
    2.29\. They are the building blocks of our clusters and are called core points.
    We use the same value of radius (*ε*) for each point and hence the *volume* of
    each neighborhood remains constant. But the number of points will vary and hence
    the *mass* varies. Consequently, the density varies as well. Since we put a threshold
    using minPts, we are putting a limit on density. So we can conclude that core
    points fulfill the minimum density threshold requirement. It is imperative to
    note that we can choose different values of *ε* and minPts to iterate and fine-tune
    the clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Border points or reachable points*—A point that is not a core point in the
    clusters is called a border point, shown as filled circles in figure 2.29\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F29_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.29 Core points are shown as squares; border points are shown as filled
    circles, while noise is unfilled circles. Together, these three are the building
    blocks for DBSCAN clustering.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A point y is directly reachable from x if y is within *ε* distance of core point
    x. A point can only be approached from a core point, and it is the primary condition
    or rule to be followed. Only a core point can reach a noncore point, and the opposite
    is not true. In other words, a noncore point can only be reached by other core
    points; it cannot reach anyone else. In figure 2.29, border points are represented
    as dark circles.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the process better, we have to understand the term *density-reachable*
    or *connectedness*. In figure 2.30, we have two core points: X and Y. We can directly
    go from X to Y. Point Z is not in the neighborhood of X but is in the neighborhood
    of Y. So we cannot directly reach Z from X, but we can surely reach Z from X through
    Y or, in other words, using the neighborhood of Y, we can travel to Z from X.
    Conversely, we cannot go from Z to X since Z is the border point and, as described
    earlier, we cannot travel from a border point.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F30_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.30 X and Y are the core points, and we can travel from X to Y. Though
    Z is not in the immediate neighborhood of X, we can still reach Z from X through
    Y. This is the core concept of density-connected points used in DBSCAN clustering.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Outliers*—All the other points are outliers. In other words, if it is not
    a core point or is not a reachable point, it is an outlier, shown as unfilled
    circles in figure 2.29\. They are not assigned any cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: steps in DBSCAN clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The steps in DBSCAN clustering are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with assigning values for *ε* and minPts required to create a cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We start with picking a random point, let’s say P, which is not yet given any
    label (i.e., it has not been analyzed and assigned any cluster).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then analyze the neighborhood for P. If it contains a sufficient number of
    points (i.e., higher than minPts), then the condition is met to start a cluster.
    If so, we tag the point P as the *core point*. If a point cannot be recognized
    as a core point, we will assign it the tag of *outlier* or *noise*. We should
    note this point can be made a part of a different cluster later. Then we go back
    to step 2\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once this core point P is found, we start creating the cluster by adding all
    directly reachable points from P and then increase this cluster size by adding
    more points directly reachable from P. Then we add all the points to the cluster,
    which can be included using the neighborhood by iterating through all these points.
    If we add an outlier point to the cluster, the tag of the outlier point is changed
    to a border point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process continues until the density cluster is complete. We then find a
    new unassigned point and repeat the process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all the points have been assigned to a cluster or called an outlier, we
    stop our clustering process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are iterations in the process. Then, once the clustering concludes, we
    utilize business logic to either merge or split a few clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Compare and contrast the importance of DBSCAN clustering with respect to k-means
    clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A noncore point can reach a core point and vice versa is also true. True or
    False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the significance of neighborhood and minPts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the process to find the most optimal value of *k*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we are clear with the process of DBSCAN clustering. Before creating the
    Python solution, we will examine the advantages and disadvantages of the DBSCAN
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: pros and cons of DBSCAN clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DBSCAN has the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike k-means, we need not specify the number of clusters to DBSCAN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is quite a robust solution for unclean datasets. Unlike other
    algorithms, it can deal with outliers effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can determine irregular-shaped clusters too. Arguably, this is the biggest
    advantage of DBSCAN clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only the input of radius and minPts is required by the algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DBSCAN has the following challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: The differentiation in clusters is sometimes not clear using DBSCAN. Depending
    on the order of processing the observations, a point can change its cluster. In
    other words, if a border point P is accessible by more than one cluster, P can
    belong to either cluster, which is dependent on the order of processing the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the difference in densities among different areas of the datasets is very
    big, then the optimum combination of *ε* and minPts will be difficult to determine,
    and hence DBSCAN will not generate effective results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distance metric used plays a highly significant role in clustering algorithms,
    including DBSCAN. Arguably, the most common metric used is Euclidean distance,
    but if the number of dimensions is quite large, then it becomes a challenge to
    compute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is very sensitive to different values of *ε* and minPts. Sometimes
    finding the most optimum value becomes a challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: python solution for DBSCAN clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We will use the same dataset we have used for k-means and hierarchical clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the libraries and dataset up to step 6 in the k-means algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import additional libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we fit the model with a value for minimum distance and radius:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of distinct clusters is 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 3\. We are not getting any results for clustering here. In other words, there
    will not be any logical results of clustering since we have not provided the optimal
    values for minPts and *ε*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '4\. Now we will find the optimum values for *ε* (see figure 2.31). For this,
    we will calculate the distance to the nearest points for each point and then sort
    and plot the results. Wherever the curvature is maximum, it is the best value
    for *ε*. For minPts, generally minPts ≥ *d* + 1 where *d* is the number of dimensions
    in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH02_F31_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.31 Finding the optimum value of *ε*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note  See the paper at [https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf](https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf)
    for further study on how to choose the values of radius for DBSCAN.
  prefs: []
  type: TYPE_NORMAL
- en: '5\. The best value is coming up as 1.5, as observed in the point of defection.
    We will use it and set the minPts as 5, which is generally taken as a standard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Now we can observe that we are getting more than one cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '7\. Let’s plot the clusters (see figure 2.32). In the print version of the
    book, you will not see different colors. The output of the Python code will have
    the colors. The same output is available at the GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH02_F32_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.32 Plotting the clusters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We have thus created a solution using DBSCAN. I advise you to compare the results
    from all three algorithms. In real-world scenarios, we test the solution with
    multiple algorithms, iterate with hyperparameters, and then choose the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: Density-based clustering is quite an efficient solution and, to a certain extent,
    is a very effective one too. It is heavily recommended if the shape of the clusters
    is suspected to be irregular.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we conclude our discussion on DBSCAN clustering. In the next section,
    we solve a business use case on clustering. In the case study, the focus is less
    on technical concepts and more on business understanding and solution generation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Case study using clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now define a case study that employs clustering as one of the solutions.
    The objective of the case study is to give you a flavor of the practical and real-life
    business world. This case study–based approach is also followed in job-related
    interviews, wherein a case is discussed during the interview stage. I highly recommend
    you understand how we implement machine learning solutions in pragmatic business
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: A case study typically has a business problem, the dataset available, the various
    solutions that can be used, the challenges faced, and the final chosen solution.
    We also discuss the problems faced while implementing the solution in real-world
    business.
  prefs: []
  type: TYPE_NORMAL
- en: So let’s start our case study on clustering using unsupervised learning. In
    the case study, we focus on the steps we take to solve the case study and not
    on the technical algorithms, as there can be multiple technical solutions to a
    particular problem.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.1 Business context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The industry we are considering can be retail; telecom; banking, financial services,
    and insurance; aviation; healthcare; or any other industry that has a customer
    base. For any business, the objective is to generate more revenue for the business
    and ultimately increase the overall profit of the business. To increase revenue,
    the business would want to have increasingly more new customers. The business
    would also want the existing consumers to buy more and buy more often. So the
    business always strives to keep the consumers engaged and happy and to increase
    their transactional value with the business.
  prefs: []
  type: TYPE_NORMAL
- en: For this to happen, the business must have a thorough understanding of its consumer
    base; it must know their tastes, price points, category preferences, affinity,
    preferred marketing/communication channels, etc. Once the business has examined
    and understood the consumer base minutely, then
  prefs: []
  type: TYPE_NORMAL
- en: The product team can improve the product features as per the consumer’s need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pricing team can improve the price of the products by aligning them to customers’
    preferred prices. The prices can be customized for a customer, or loyalty discounts
    can be offered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The marketing team and customer relationship team can target the consumers with
    a customized offer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The teams can win back the consumers who are going to churn or stop buying from
    the business, can enhance their spending, increase the stickiness, and increase
    the customer lifetime value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, different teams can align their offerings as per the understanding
    of the consumers generated. And the end consumer will be happier, more engaged,
    and more loyal to the business, leading to more fruitful consumer engagement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The business hence should dive deep into the consumers’ data and generate an
    understanding of the base. The customer data can look like that shown in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.2 Dataset for the analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We take as an example an apparel retailer that has a loyalty program and that
    saves the customer’s transaction details. The various (not exhaustive) data sources
    are shown in figure 2.33.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F33_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.33 Data sources for an apparel retail store
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can have store details, such as store ID, store name, city, area, number
    of employees, etc. We can have an item hierarchies table, which has all the details
    of the items like price, category, etc. Then we can have customer demographic
    details like age, gender, city, and customer transactional history. Clearly, by
    joining such tables, we will be able to create a master table that will have all
    the details in one place.
  prefs: []
  type: TYPE_NORMAL
- en: Note  I advise you to develop a good skill set for SQL. It is required in almost
    all of the domains related to data—be it data science, data engineering, or data
    visualization, SQL is ubiquitous.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.34 is an example of a master table. This is not an exhaustive list
    of variables, and the number of variables can be much larger than the ones shown.
    The master table has some raw variables like Revenue, Invoices, etc., and some
    derived variables like Average Transaction Value, Average Basket Size, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F34_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.34 A master table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We could also take an example of a telecom operator. In that subscriber usage,
    call rate, revenue, days spent on the network, data usage, etc., will be the attributes
    we analyze. Hence, based on the business domain at hand, the datasets will change.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the dataset, we generally create derived attributes from it. For
    example, the average transaction value attribute is total revenue divided by the
    number of invoices. We create such attributes in addition to the raw variables
    we already have.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.3 Suggested solutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There can be multiple solutions to the problem, some of which we include in
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We can create a dashboard to depict the major key performance indicators. This
    will allow us to analyze the history and take necessary actions based on it. But
    the dashboard will only show the information that we already know (to some extent).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can perform data analysis using some of techniques we used in the solutions
    in the earlier sections. This will solve a part of the problem and, moreover,
    it is difficult to consider multiple dimensions simultaneously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can create predictive models to predict if the customers are going to shop
    in the coming months or are going to churn in the next *X* days, but this will
    not solve the problem completely. To be clear, “churn” here means that the customer
    no longer shops with the retailer in the next *X* days. Here, duration *X* is
    defined as per the business domain. For example, for the telecom domain, *X* will
    be less than in the insurance domain. This is due to the fact that people use
    mobile phones every day, whereas in the insurance domain, most customers pay the
    premium yearly. So customer interaction is less for insurance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can create customer segmentation solutions wherein we group customers based
    on their historical trends and attributes. This is the solution we will use to
    solve this business problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.6.4 Solution for the problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall figure 1.9 in chapter 1, where we discussed the steps we follow in the
    machine learning algorithm. Everything starts with defining the business problem
    and then we move on to data discovery, preprocessing, etc. For our case study
    here, we will utilize a similar strategy. We have already defined the business
    problem; data discovery is done and we have completed the exploratory data analysis
    and the preprocessing of the data. To create a segmentation solution using clustering,
    follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with finalizing the dataset we wish to feed to the clustering algorithms.
    We might have created some derived variables, treated some missing values or outliers,
    etc. In the case study, we would want to know the minimum/maximum/average values
    of transactions, invoices, items bought, etc. We would be interested to know the
    gender and age distribution. We also would like to know the mutual relationships
    between these variables, such as if women customers use the online mode more than
    male customers. All of these questions are answered as part of this step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TIP  A Python Jupyter notebook is checked in at the GitHub repository, which
    provides detailed steps and code for the exploratory data analysis and data preprocessing.
    Check it out!
  prefs: []
  type: TYPE_NORMAL
- en: 2\. We create the first solution using k-means clustering followed by hierarchical
    clustering. For each of the algorithms, iterations are done by changing hyperparameters.
    In the case study, we will choose parameters like the number of visits, total
    revenue, distinct categories purchased, online/offline transactions ratio, gender,
    age, etc., as parameters for clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. A final version of the algorithm and respective hyperparameters are chosen.
    The clusters are analyzed further in the light of business understanding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4\. More often, the clusters are merged or broken, depending on the size of
    the observations and the nature of the attributes present in them. For example,
    if the total customer base is 1 million, it will be really hard to take action
    on a cluster of size 100\. At the same time, it will be equally difficult to manage
    a cluster of size 700,000\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5\. We then analyze the clusters we finally have. The clusters distribution
    is checked for the variables, their distinguishing factors are understood, and
    we give logical names to the clusters. We can expect to see such a clustering
    output as shown in figure 2.35\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F35_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.35 Segmentation based on a few dimensions like response, life stage,
    engagement, and spending patterns. The dimensions are not exhaustive, and in a
    real-world business problem, the number of dimensions can be higher.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the example clusters shown, we have depicted spending patterns, responsiveness
    to previous campaigns, life stage, and overall engagement as a few dimensions.
    Respective subdivisions of each of these dimensions are also shown. The clusters
    will be a logical combination of these dimensions. The actual dimensions can be
    much higher.
  prefs: []
  type: TYPE_NORMAL
- en: The segmentation shown in figure 2.35 can be used for multiple domains and businesses.
    The parameters and attributes might change, the business context may be different,
    the extent of data available might vary—but the overall approach remains similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the applications we saw in the last section, let’s examine a
    few use cases here:'
  prefs: []
  type: TYPE_NORMAL
- en: Market research utilizes clustering to segment the groups of consumers into
    market segments; then the groups can be analyzed better in terms of their preferences.
    Product placement can be improved, pricing can be made tighter, and geography
    selection will be more scientific.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the bioinformatics and medical industry, clustering can be used to group
    genes into distinct categories. Groups of genes can be segmented and comparisons
    can be assessed by analyzing the attributes of the groups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering is used as an effective data preprocessing step before we create
    algorithms using supervised learning solutions. It can also be used to reduce
    the data size by focusing on the data points belonging to a cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering is utilized for pattern detection across both structured and unstructured
    datasets. We have already studied the case for a structured dataset. For text
    data, it can be used to group similar types of documents, journals, news, etc.
    We can also employ clustering to work and develop solutions for images. We will
    study unsupervised learning solutions for text and images in later chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the algorithms work on similarity measurements, clustering can be used to
    segment the incoming dataset as fraud or genuine, which can be used to reduce
    the number of criminal activities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use cases of clustering are many. We have discussed only the prominent ones.
    Clustering is one of the algorithms that changes the working methodologies and
    generates a lot of insights around the data. It is widely used across telecom;
    retail; banking, financial services, and insurance; aviation; and others.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, there are a few problems with the algorithm. We next examine
    the common problems we face with clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Common challenges faced in clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clustering is not a completely straightforward solution without any challenges.
    Like any other solution in the world, clustering too has its share of problems.
    The most common challenges we face in clustering are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Too much data*—Sometimes the magnitude of the data is quite big, and there
    are a lot of dimensions available. In such a case, it becomes difficult to manage
    the dataset. The computation power might be limited, and like any project, there
    is finite time available. To overcome the problem, we can'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to reduce the number of dimensions by finding the most significant variables
    by using a supervised learning-based regression approach or decision tree algorithm,
    etc.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the number of dimensions by employing principal component analysis or
    singular value decomposition, etc.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A noisy dataset*—“Garbage in, garbage out” is a cliché that is true for clustering
    too. If the dataset is messy, it creates a lot of problems. The problems can include'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing values (i.e., NULL, NA, ?, blanks, etc.).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers present in the dataset.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Junk values like #€¶§^ etc., present in the dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrong entries made in the data. For example, if a name is entered in the Revenue
    field, it is an incorrect entry.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss the steps and the process to resolve these problems in later chapters.
    In this chapter, we are examining how to work with categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: '*Categorical variables*—While discussing, recall the problem where k-means
    was not able to use categorical variables. We solve that problem next.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To convert categorical variables into numeric ones, we can use *one-hot encoding*.
    This technique adds additional columns equal to the number of distinct classes
    as shown in the following figure. The variable city has unique values as London
    and New Delhi. We can observe that two additional columns have been created with
    0 or 1 filled in for the values (see figure 2.36).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F36_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.36 Using one-hot encoding to convert categorical variables into numeric
    ones
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using one-hot encoding does not always ensure an effective and efficient solution.
    Imagine if the number of cities in this example is 100; then we will have 100
    additional columns in the dataset, and most of the values will be filled in with
    0\. Hence, in such a situation, it is advisable to group a few values.
  prefs: []
  type: TYPE_NORMAL
- en: '*Distance metrics*—With different distance metrics, we might get different
    results. Though there is no “one size fits all,” Euclidean distance is most often
    used for measuring distance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Subjective interpretations*—Interpretations for the clusters are quite subjective.
    By using different attributes, completely different clustering can be done for
    the same datasets. As discussed earlier, the focus should be on solving the business
    problem at hand. This holds the key to choosing the hyperparameters and the final
    algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Time-consuming*—Since a lot of dimensions are dealt with simultaneously, sometimes
    converging the algorithm takes a lot of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite all these challenges, clustering is a widely recognized and utilized
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Concluding thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning is not an easy task. But it is certainly a very engaging
    one. It does not require any target variable, and the solution identifies the
    patterns itself, which is one of the biggest advantages of unsupervised learning
    algorithms. And the implementations are already having a great effect on the business
    world. We studied one of these solution classes called clustering in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is an unsupervised learning solution that is useful for pattern identifications,
    exploratory analysis, and, of course, segmenting the data points. Organizations
    heavily use clustering algorithms and proceed to the next level of understanding
    consumer data. Better prices can be offered, more relevant offers can be suggested,
    consumer engagement can be improved, and overall customer experience becomes better.
    After all, a happy consumer is the goal of any business. Clustering can be used
    not only for structured data but for text data, images, videos, and audio too.
    Due to its capability to find patterns across multiple datasets using a large
    number of dimensions, clustering is the go-to solution whenever we want to analyze
    multiple dimensions together.
  prefs: []
  type: TYPE_NORMAL
- en: In this second chapter of this book, we introduced concepts of unsupervised-based
    clustering methods. We examined different types of clustering algorithms—k-means
    clustering, hierarchical clustering, and DBSCAN clustering—along with their mathematical
    concepts, respective use cases, and pros and cons with an emphasis on creating
    actual Python code for these datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will study dimensionality reduction techniques
    like principal component analysis and singular value decomposition. We will discuss
    the building blocks for techniques, their mathematical foundation, advantages
    and disadvantages, and use cases and perform actual Python implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.9 Practical next steps and suggested readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the online retail data from [https://mng.bz/dXqo](https://mng.bz/dXqo).
    This dataset contains all the online transactions occurring between January 12,
    2010, and September 12, 2011, for a UK-based retailer. Apply the three algorithms
    described in the chapter to identify which customers the company should target
    and why.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the IRIS dataset from [https://www.kaggle.com/uciml/iris](https://www.kaggle.com/uciml/iris).
    It includes three iris species with 50 samples, each having some properties of
    the flowers. Use k-means and DBSCAN and compare the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the dataset at UCI for clustering at [http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Study the following papers on k-means clustering, hierarchical clustering,
    and DBSCAN clustering:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mng.bz/rKqJ](https://mng.bz/rKqJ)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://mng.bz/VVEy](https://mng.bz/VVEy)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://ieeexplore.ieee.org/document/1017616](https://ieeexplore.ieee.org/document/1017616)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://ieeexplore.ieee.org/document/7100308](https://ieeexplore.ieee.org/document/7100308)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://mng.bz/xKqd](https://mng.bz/xKqd)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://mng.bz/AQno](https://mng.bz/AQno)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: DBSCAN clustering
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1810.13105.pdf](https://arxiv.org/pdf/1810.13105.pdf)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://ieeexplore.ieee.org/document/9356727](https://ieeexplore.ieee.org/document/9356727)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering is used for a variety of purposes across all industries, such as
    retail, telecom, finance, and pharma. Clustering solutions are implemented for
    customer and marketing segmentation to better understand the customer base, which
    further improves targeting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering groups objects with similar attributes into segments, aiding in data
    understanding and pattern discovery without needing a target variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using clustering, we find the underlying patterns in a dataset and identify
    the natural groupings in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be multiple clustering techniques based on the methodology. A few
    examples are k-means clustering, hierarchical clustering, DBSCAN, and fuzzy clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different clustering algorithms (k-means, hierarchical, DBSCAN) offer distinct
    pros and cons, and each is suitable for different data characteristics and purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering is categorized into hard clustering, where objects belong to a single
    cluster, and soft clustering, where objects can belong to multiple clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different clustering attributes and techniques, such as centroid-based, density-based,
    and distribution models, lead to varied clustering results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective clustering algorithms produce comprehensible, scalable, and independent
    clusters, handling outliers and multiple data types with minimal domain input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance metrics for clustering include Euclidean, Chebyshev, Manhattan, and
    cosine distances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centroid-based clustering measures similarity based on the distance to the centroid
    of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering creates nonoverlapping clusters by specifying the number
    of clusters, *k*, and assigning data points to the nearest center iteratively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The elbow method is a common technique to determine the optimal number of clusters
    in k-means clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means is based on the centroid of the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering creates clusters based on connectivity and does not
    require a predefined number of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering can be agglomerative (bottom-up) or divisive (top-down)
    and uses linkage criteria to measure distances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN identifies clusters based on point density and effectively distinguishes
    outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN does not require specifying the number of clusters and is suited for
    irregular-shaped clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring clustering accuracy involves metrics like WCSS, intercluster sum of
    squares, silhouette value, and the Dunn index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
