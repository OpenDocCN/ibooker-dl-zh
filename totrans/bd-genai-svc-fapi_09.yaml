- en: Capitolo 6\. Comunicazione in tempo realecon i modelli generativi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Questo lavoro è stato tradotto utilizzando l''AI. Siamo lieti di ricevere il
    tuo feedback e i tuoi commenti: [translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Questo capitolo esplorerà i carichi di lavoro di AI in streaming come i chatbot,
    illustrando l'uso di tecnologie di comunicazione in tempo reale come SSE e WebSocket.
    Imparerai la differenza tra queste tecnologie e come implementare lo streaming
    di modelli costruendo endpoint per interazioni testo-testo in tempo reale.
  prefs: []
  type: TYPE_NORMAL
- en: Meccanismi di comunicazione web
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nel capitolo precedente hai imparato a implementare la concorrenza nei flussi
    di lavoro dell'intelligenza artificiale sfruttando la programmazione asincrona,
    i task in background e il batching continuo. Con la concorrenza, i tuoi servizi
    diventano più resistenti per far fronte all'aumento della domanda quando più utenti
    accedono all'applicazione simultaneamente. La concorrenza risolve il problema
    di consentire agli utenti simultanei di accedere al tuo servizio e aiuta a diminuire
    i tempi di attesa, ma la generazione dei dati dell'intelligenza artificiale rimane
    un'attività che richiede molte risorse e molto tempo.
  prefs: []
  type: TYPE_NORMAL
- en: Fino a questo punto, hai costruito gli endpoint utilizzando la comunicazione
    HTTP convenzionale in cui il client invia una richiesta al server. Il server web
    elabora le richieste in arrivo e risponde tramite messaggi HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: La[Figura 6-1](#client_server_architecture) mostra l'architettura client-server.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0601](assets/bgai_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figura 6-1\. L''architettura client-server (Fonte: [scaleyourapp.com](https://scaleyourapp.com))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Poiché il protocollo HTTP è stateless, il server tratta ogni richiesta in arrivo
    in modo completamente indipendente e non correlato alle altre richieste. Ciò significa
    che più richieste in arrivo da client diversi non influenzano il modo in cui il
    server risponde a ciascuna di esse. Ad esempio, in un servizio di intelligenza
    artificiale conversazionale che non utilizza un database, ogni richiesta può fornire
    l'intera cronologia della conversazione e ricevere la risposta corretta dal server.
  prefs: []
  type: TYPE_NORMAL
- en: Il modello *HTTP richiesta-risposta* è un modello di progettazione delle API
    ampiamente adottato in tutto il web grazie alla sua semplicità. Tuttavia, questo
    approccio diventa inadeguato non appena il client o il server necessitano di aggiornamenti
    in tempo reale.
  prefs: []
  type: TYPE_NORMAL
- en: Nel modello standard HTTP richiesta-risposta, i tuoi servizi rispondono alla
    richiesta dell'utente una volta che questa è stata interamente elaborata. Tuttavia,
    se il processo di generazione dei dati è lungo e lento, i tuoi utenti aspetteranno
    a lungo e successivamente saranno inondati da molte informazioni in una volta
    sola. Immagina di chattare con un bot che impiega diversi minuti per rispondere
    e, una volta risposto, ti vengono mostrati blocchi di testo sovrabbondanti.
  prefs: []
  type: TYPE_NORMAL
- en: In alternativa, se fornisci i dati al cliente mentre vengono generati, invece
    di aspettare che l'intero processo di generazione sia completato, puoi ridurre
    i lunghi ritardi e fornire le informazioni in pezzi digeribili. Questo approccio
    non solo migliora l'esperienza dell'utente, ma mantiene anche il suo coinvolgimento
    durante l'elaborazione della sua richiesta.
  prefs: []
  type: TYPE_NORMAL
- en: Ci sono casi in cui l'implementazione di funzioni in tempo reale può essere
    eccessiva e aumentare il carico di sviluppo. Ad esempio, alcuni modelli o API
    open source non hanno la capacità di generare in tempo reale. Inoltre, l'aggiunta
    di endpoint per lo streaming dei dati può aumentare la complessità del sistema
    su entrambi i lati, il server e il client. Significa dover gestire le eccezioni
    in modo diverso e gestire le connessioni simultanee agli endpoint di streaming
    per evitare perdite di memoria. Se il client si disconnette durante uno streaming,
    potrebbe verificarsi una perdita di dati o una deriva di stato tra il server e
    il client. Inoltre, potrebbe essere necessario implementare una complessa logica
    di riconnessione e gestione dello stato per gestire i casi in cui la connessione
    cade.
  prefs: []
  type: TYPE_NORMAL
- en: Mantenere molte connessioni aperte contemporanee può anche gravare sui tuoi
    server e comportare un aumento dei costi di hosting e di infrastruttura.
  prefs: []
  type: TYPE_NORMAL
- en: Altrettanto importante è considerare la scalabilità della gestione di un gran
    numero di flussi contemporanei, i requisiti di latenza della tua applicazione
    e la compatibilità del browser con il protocollo di streaming scelto.
  prefs: []
  type: TYPE_NORMAL
- en: Nota
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Rispetto alle applicazioni web tradizionali che hanno una qualche forma di latenza
    di I/O o di elaborazione dei dati, le applicazioni di AI hanno anche una latenza
    di inferenza del modello di AI, a seconda del modello utilizzato.
  prefs: []
  type: TYPE_NORMAL
- en: Poiché questa latenza può essere significativa, i tuoi servizi di intelligenza
    artificiale devono essere in grado di gestire tempi di attesa più lunghi sia dal
    lato del server che da quello del client, compresa la gestione dell'esperienza
    dell'utente.
  prefs: []
  type: TYPE_NORMAL
- en: 'Se il tuo caso d''uso beneficia di funzionalità in tempo reale, allora hai
    a disposizione alcuni modelli di progettazione architettonica che puoi implementare:'
  prefs: []
  type: TYPE_NORMAL
- en: Sondaggio regolare/breve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sondaggio lungo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: La scelta dipende dai tuoi requisiti di esperienza utente, scalabilità, latenza,
    costi di sviluppo e manutenibilità.
  prefs: []
  type: TYPE_NORMAL
- en: Analizziamo ogni opzione in modo più dettagliato.
  prefs: []
  type: TYPE_NORMAL
- en: Sondaggio regolare/breve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Un metodo per beneficiare degli aggiornamenti in semi-tempo reale è quello di
    utilizzare il *polling regolare/breve*, come mostrato nella [Figura 6-2](#short_polling).
    In questo meccanismo di polling, il client invia periodicamente richieste HTTP
    al server per verificare la presenza di aggiornamenti a intervalli preconfigurati.
    Più brevi sono gli intervalli, più ci si avvicina agli aggiornamenti in tempo
    reale ma anche più alto sarà il traffico da gestire.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0602](assets/bgai_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 6-2\. Polling regolare/breve
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Puoi utilizzare questa tecnica se stai costruendo un servizio per generare dati
    come le immagini in batch. Il client invia semplicemente una richiesta per avviare
    il lavoro in batch e gli viene assegnato un identificativo univoco per il lavoro/la
    richiesta. Poi controlla periodicamente il server per confermare lo stato e i
    risultati del lavoro richiesto. Il server risponde con nuovi dati o fornisce una
    risposta vuota (e forse un aggiornamento dello stato) se i risultati devono ancora
    essere calcolati.
  prefs: []
  type: TYPE_NORMAL
- en: Come puoi immaginare con un polling breve, ti ritroverai con un numero eccessivo
    di richieste in entrata a cui il server deve rispondere, anche quando non ci sono
    nuove informazioni. Se hai più utenti contemporanei, questo approccio può rapidamente
    sovraccaricare il server, limitando la scalabilità della tua applicazione. Tuttavia,
    puoi ridurre il carico del server utilizzando le risposte nella cache (cioè eseguendo
    controlli di stato sul backend con una frequenza tollerabile) e implementando
    il rate limiting, che imparerai a conoscere meglio nei Capitoli [9](ch09.html#ch09)
    e [10](ch10.html#ch10).
  prefs: []
  type: TYPE_NORMAL
- en: Un potenziale caso d'uso del polling breve nei servizi di intelligenza artificiale
    è quando hai dei lavori batch o di inferenza in corso. Puoi esporre degli endpoint
    che permettano ai tuoi clienti di utilizzare il polling breve per tenersi aggiornati
    sullo stato di questi lavori e recuperare i risultati quando sono stati completati.
  prefs: []
  type: TYPE_NORMAL
- en: Un'alternativa è quella di sfruttare i sondaggi lunghi.
  prefs: []
  type: TYPE_NORMAL
- en: Sondaggio lungo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Se vuoi ridurre il carico sul server continuando a sfruttare un meccanismo di
    polling in tempo reale, puoi implementare il *polling lungo* (vedi [Figura 6-3](#long_polling)),
    una versione migliorata del polling regolare/breve.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0603](assets/bgai_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 6-3\. Polling lungo
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Con il polling lungo, sia il server che il client sono configurati per evitare
    i*timeout* (se possibile) che si verificano quando il client o il server rinunciano
    alla richiesta prolungata.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I timeout vengono osservati più spesso in un tipico ciclo di richiesta-risposta
    HTTP quando una richiesta richiede un tempo prolungato per essere risolta o quando
    ci sono problemi di rete.
  prefs: []
  type: TYPE_NORMAL
- en: Per implementare il polling lungo, il server mantiene aperte le richieste in
    arrivo (cioè sospese) fino a quando non ci sono dati disponibili da inviare. Ad
    esempio, questo può essere utile quando si dispone di un LLM con tempi di elaborazione
    imprevedibili. Il client viene istruito ad aspettare per un periodo di tempo prolungato
    ed evita di interrompere e ripetere le richiesteprematuramente.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi utilizzare il polling lungo se hai bisogno di un design semplice dell'API
    e di un'architettura dell'applicazione per l'elaborazione di lavori prolungati,
    come ad esempio le inferenze multiple dell'intelligenza artificiale. Questa tecnica
    ti permette di evitare l'implementazione di un gestore di lavori batch per tenere
    traccia dei lavori per la generazione di dati in blocco. Invece, le richieste
    del client rimangono aperte fino a quando non vengono elaborate, evitando il ciclo
    costante di richiesta-risposta con polling breve che può sovraccaricare il server.
  prefs: []
  type: TYPE_NORMAL
- en: Sebbene il polling lungo sembri simile al tipico modello di richiesta-risposta
    HTTP, si differenzia per il modo in cui il client gestisce le richieste. Nel polling
    lungo, il client riceve tipicamente un singolo messaggio per ogni richiesta. Una
    volta che il server invia una risposta, la connessione viene chiusa. Il client
    apre quindi immediatamente una nuova connessione per attendere il messaggio successivo.
    Questo processo si ripete, consentendo al client di ricevere più messaggi nel
    tempo, ma ogni ciclo di richiesta-risposta HTTP gestisce un solo messaggio.
  prefs: []
  type: TYPE_NORMAL
- en: Poiché il polling lungo mantiene una connessione aperta fino a quando non è
    disponibile un messaggio, riduce la frequenza delle richieste rispetto al polling
    breve e implementa un meccanismo di comunicazione quasi in tempo reale. Tuttavia,
    il server deve comunque mantenere le richieste non soddisfatte, che consumano
    le risorse del server. Inoltre, se ci sono più richieste aperte da parte dello
    stesso client, l'ordinamento dei messaggi può essere difficile da gestire, portando
    potenzialmente a messaggi fuori ordine.
  prefs: []
  type: TYPE_NORMAL
- en: Se non hai un requisito specifico per l'utilizzo dei meccanismi di polling,
    un'alternativa più moderna ai meccanismi di polling per la comunicazione in tempo
    reale è SSE tramite l'interfaccia Event Source.
  prefs: []
  type: TYPE_NORMAL
- en: Eventi inviati dal server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gli*eventi inviati dal server* (SSE) sono un meccanismo basato su HTTP per stabilire
    una connessione persistente e unidirezionale tra il server e il client. Mentre
    la connessione è aperta, il server può inviare continuamente aggiornamenti al
    client quando i dati diventanodisponibili.
  prefs: []
  type: TYPE_NORMAL
- en: Una volta che il client stabilisce la connessione persistente SSE con il server,
    non avrà più bisogno di ristabilirla, a differenza del lungo meccanismo di polling
    in cui il client invia ripetutamente richieste al server per mantenere una connessione
    aperta.
  prefs: []
  type: TYPE_NORMAL
- en: Se stai servendo modelli GenAI, SSE è un meccanismo di comunicazione in tempo
    reale più adatto rispetto al long polling. SSE è stato progettato specificamente
    per gestire gli eventi in tempo reale ed è più efficiente del polling lungo. A
    causa dell'apertura e della chiusura ripetuta delle connessioni, il polling lungo
    richiede molte risorse e comporta un aumento della latenza e dell'overhead. SSE,
    invece, supporta la riconnessione automatica e gli ID evento per riprendere i
    flussi interrotti, cosa che il polling lungo non ha.
  prefs: []
  type: TYPE_NORMAL
- en: 'In SSE, il client effettua una richiesta HTTP standard `GET` con un''intestazione
    `Accept:text/event-stream` e il server risponde con un codice di stato `200` e
    un''intestazione `Content-Type: text/event-stream`. Dopo questo scambio, il server
    può inviare eventi al client sulla stessa connessione.'
  prefs: []
  type: TYPE_NORMAL
- en: Anche se SSE dovrebbe essere la prima scelta per le applicazioni in tempo reale,
    puoi comunque optare per un meccanismo più semplice di polling lungo quando gli
    aggiornamenti sono poco frequenti o se il tuo ambiente non supporta le connessioni
    persistenti.
  prefs: []
  type: TYPE_NORMAL
- en: Un ultimo dettaglio importante da notare è che le connessioni SSE sono *unidirezionali*,
    cioè invii una normale richiesta HTTP al server e ricevi la risposta tramite SSE.
    Pertanto, sono adatte solo ad applicazioni che non hanno bisogno di inviare dati
    al server. Potresti aver visto SSE in azione all'interno di feed di notizie, notifiche
    e dashboard in tempo reale come i grafici dei dati azionari.
  prefs: []
  type: TYPE_NORMAL
- en: Non sorprende che l'SSE sia particolarmente indicato nelle applicazioni di chat,
    quando è necessario trasmettere le risposte dell'LLM durante una conversazione.
    In questo caso, il client può stabilire una connessione persistente separata fino
    a quando il server non trasmette completamente la risposta dell'LLM all'utente.
  prefs: []
  type: TYPE_NORMAL
- en: Nota
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ChatGPT sfrutta SSE sotto il cofano per consentire risposte in tempo reale alle
    query degli utenti.
  prefs: []
  type: TYPE_NORMAL
- en: '[La Figura 6-4](#server_sent_events) mostra il funzionamento del meccanismo
    di comunicazione SSE.'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0604](assets/bgai_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 6-4\. SSE
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Per approfondire la tua comprensione, in questo capitolo realizzeremo due mini-progetti
    con SSE: uno per lo streaming di dati da un generatore di dati simulato e l''altro
    per lo streaming di risposte LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Scoprirai maggiori dettagli sul meccanismo SSE durante i progetti sopra citati.
  prefs: []
  type: TYPE_NORMAL
- en: In sintesi, SSE è eccellente per stabilire connessioni persistenti unidirezionali,
    ma cosa succede se hai bisogno di inviare e ricevere messaggi durante una connessione
    persistente? In questo caso WebSocket può essere utile.
  prefs: []
  type: TYPE_NORMAL
- en: WebSocket
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: L'ultimo meccanismo di comunicazione in tempo reale da trattare è WebSocket.
  prefs: []
  type: TYPE_NORMAL
- en: WebSocket è un eccellente meccanismo di comunicazione in tempo reale per stabilire
    *connessioni bidirezionali* persistenti tra il client e il server per le chat
    in tempo reale, nonché per le applicazioni vocali e video con un modello di intelligenza
    artificiale. Una connessione bidirezionale significa che entrambe le parti possono
    inviare e ricevere dati in tempo reale in qualsiasi ordine, purché sia aperta
    una connessione persistente tra il client e il server. È stato progettato per
    funzionare su porte HTTP standard per garantire la compatibilità con le misure
    di sicurezza esistenti. Le applicazioni web che richiedono una comunicazione bidirezionale
    con i server traggono il massimo vantaggio da questo meccanismo in quanto possono
    evitare l'overhead e la complessità delpolling HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi utilizzare WebSocket in una varietà di applicazioni, tra cui feed sociali,
    giochi multiplayer, feed finanziari, aggiornamenti basati sulla posizione, chat
    multimediali, ecc.
  prefs: []
  type: TYPE_NORMAL
- en: A differenza di tutti gli altri meccanismi di comunicazione discussi finora,
    il protocollo WebSocket non trasferisce i dati su HTTP dopo l'handshake iniziale,
    ma implementa un meccanismo di messaggistica bidirezionale (full-duplex) su una
    singola connessione TCP. Di conseguenza, WebSocket è più veloce nella trasmissione
    dei dati rispetto a HTTP perché ha meno overhead di protocollo e opera a un livello
    inferiore nello stack di protocolli di rete. Questo perché HTTP si trova in cima
    a TCP, quindi il ritorno a TCP sarà più veloce.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: WebSocket mantiene un socket aperto sia sul client che sul server per tutta
    la durata della connessione. Si noti che questo rende anche i server statici,
    il che rende più complicato il ridimensionamento.
  prefs: []
  type: TYPE_NORMAL
- en: A questo punto ti starai chiedendo come funziona il protocollo WebSocket.
  prefs: []
  type: TYPE_NORMAL
- en: Secondo l'RFC 6455, per stabilire una connessione WebSocket, il client invia
    una richiesta HTTP di "aggiornamento" al server, chiedendo di aprire una connessione
    WebSocket.Questo viene definito l'*handshake di apertura*, che dà inizio al ciclo
    di vita della connessione WebSocket nello stato*CONNECTING*.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I tuoi servizi di AI devono essere in grado di gestire più handshake simultanei
    e di autenticarli prima di aprire una connessione. Le nuove connessioni possono
    consumare le risorse del server, quindi devono essere gestite correttamente dal
    tuo server.
  prefs: []
  type: TYPE_NORMAL
- en: La richiesta di aggiornamento HTTP deve contenere una serie di intestazioni
    necessarie, come mostrato nell'[Esempio 6-1](#websocket_handshake).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-1\. Apertura di WebSocket tramite HTTP
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Effettua una richiesta di aggiornamento HTTP all'endpoint WebSocket. Gli endpoint
    WebSocket iniziano con `ws://` invece del tipico `http://`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Richiesta di aggiornamento e apertura di una connessione WebSocket.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizza una stringa casuale di 16 byte codificata in Base64 per assicurarti
    che il server supporti il protocollo WebSocket.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Usa il sottoprotocollo `html-chat` o `text-chat` se `html-chat` non è disponibile.
    I sottoprotocolli regolano i dati da scambiare.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In produzione, utilizza sempre endpoint WebSocket `wss://` sicuri.
  prefs: []
  type: TYPE_NORMAL
- en: Il protocollo `wss://`, simile a `https://`, non solo è criptato ma anche più
    affidabile. Questo perché i dati di `ws://` non sono criptati e sono visibili
    a qualsiasi intermediario. I vecchi server proxy non conoscono WebSocket e potrebbero
    vedere intestazioni "strane" e interrompere la connessione.
  prefs: []
  type: TYPE_NORMAL
- en: D'altra parte, `wss://` è la versione sicura di WebSocket, che funziona su Transport
    Layer Security (TLS), che cripta i dati al mittente e li decripta al destinatario.
    Quindi i pacchetti di dati passano criptati attraverso i proxy, che non possono
    vedere cosa contengono e lasciarli passare.
  prefs: []
  type: TYPE_NORMAL
- en: Una volta stabilita la connessione WebSocket, i messaggi testuali o binari possono
    essere trasmessi in entrambe le direzioni sotto forma di *frame di messaggi*.
    Il ciclo di vita della connessione è ora nello stato *OPEN*.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi vedere il meccanismo di comunicazione WebSocket nella [Figura 6-5](#websockets).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0605](assets/bgai_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 6-5\. Comunicazione WS
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'I*message frame* sono un modo per impacchettare e trasmettere i dati tra il
    client e il server. Non sono un''esclusiva di WebSocket, poiché si applicano a
    tutte le connessioni tramite il protocollo TCP che costituisce la base di HTTP.
    Tuttavia, un message frame di WebSocket è costituito da diversi componenti:'
  prefs: []
  type: TYPE_NORMAL
- en: Intestazione fissa
  prefs: []
  type: TYPE_NORMAL
- en: Descrive le informazioni di base del messaggio
  prefs: []
  type: TYPE_NORMAL
- en: Lunghezza del carico utile estesa (opzionale)
  prefs: []
  type: TYPE_NORMAL
- en: Fornisce la lunghezza effettiva del payload quando la lunghezza supera i 125
    byte.
  prefs: []
  type: TYPE_NORMAL
- en: Chiave di mascheramento
  prefs: []
  type: TYPE_NORMAL
- en: Maschera i dati del payload nei frame inviati dal client al server, prevenendo
    alcuni tipi di vulnerabilità di sicurezza, in particolare l'*avvelenamento della
    cache*^([1](ch06.html#id893)) e gli attacchi *cross-protocollo*^([2](ch06.html#id894))
    attacchi
  prefs: []
  type: TYPE_NORMAL
- en: Carico utile
  prefs: []
  type: TYPE_NORMAL
- en: Contiene il contenuto effettivo del messaggio
  prefs: []
  type: TYPE_NORMAL
- en: 'A differenza delle intestazioni verbose delle richieste HTTP, i frame WebSocket
    hanno intestazioni minime che includono quanto segue:'
  prefs: []
  type: TYPE_NORMAL
- en: Cornici di testo
  prefs: []
  type: TYPE_NORMAL
- en: Utilizzato per i dati di testo codificati UTF-8
  prefs: []
  type: TYPE_NORMAL
- en: Cornici binarie
  prefs: []
  type: TYPE_NORMAL
- en: Utilizzato per i dati binari
  prefs: []
  type: TYPE_NORMAL
- en: Frammentazione
  prefs: []
  type: TYPE_NORMAL
- en: Utilizzato per frammentare i messaggi in più frame, che vengono riassemblati
    dal destinatario.
  prefs: []
  type: TYPE_NORMAL
- en: Il bello del protocollo WebSocket è anche la sua capacità di mantenere una connessione
    persistente attraverso i *frame di controllo*.
  prefs: []
  type: TYPE_NORMAL
- en: 'I*frame di controllo* sono frame speciali utilizzati per gestire la connessione:'
  prefs: []
  type: TYPE_NORMAL
- en: Telai per ping/pong
  prefs: []
  type: TYPE_NORMAL
- en: Utilizzato per controllare lo stato della connessione
  prefs: []
  type: TYPE_NORMAL
- en: Chiudi la cornice
  prefs: []
  type: TYPE_NORMAL
- en: Utilizzato per terminare la connessione con grazia
  prefs: []
  type: TYPE_NORMAL
- en: Quando è il momento di chiudere la connessione WebSocket, il client o il server
    inviano un frame di chiusura. Il frame di chiusura può specificare facoltativamente
    un codice di stato e/o un motivo per la chiusura della connessione. A questo punto,
    la connessione WebSocket entra nello stato di *CHIUSURA*.
  prefs: []
  type: TYPE_NORMAL
- en: Lo stato *CLOSING* termina quando l'altra parte risponde con un altro frame
    di chiusura, concludendo così l'intero ciclo di vita della connessione WebSocket
    nello stato *CLOSED*, come mostrato nella[Figura 6-6](#websocket_connection_lifecycle).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0606](assets/bgai_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 6-6\. Ciclo di vita della connessione WebSocket
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Come puoi vedere, l'utilizzo del meccanismo di comunicazione WebSocket può essere
    un po' eccessivo per le applicazioni semplici che non richiedono l'overhead. Per
    la maggior parte delle applicazioni GenAI, le connessioni SSE possono essere sufficienti.
  prefs: []
  type: TYPE_NORMAL
- en: Tuttavia, ci sono casi d'uso GenAI in cui WebSocket può brillare, come le applicazioni
    di chat multimediale e voice-to-voice, le applicazioni GenAI collaborative e i
    servizi di trascrizione in tempo reale basati sulla comunicazione bidirezionale.
    Per fare un po' di esperienza pratica, più avanti in questo capitolo realizzerai
    un'applicazione speech-to-text.
  prefs: []
  type: TYPE_NORMAL
- en: Ora che hai imparato a conoscere diversi meccanismi di comunicazione web unici
    per le applicazioni in tempo reale, riassumiamo rapidamente il loro confronto.
  prefs: []
  type: TYPE_NORMAL
- en: Meccanismi di comunicazione a confronto
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: La[Figura 6-7](#communication_mechanims_figure)illustra i cinque meccanismi
    di comunicazione utilizzati nello sviluppo web.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0607](assets/bgai_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 6-7\. Confronto tra i meccanismi di comunicazione web
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Come puoi vedere dalla [Figura 6-7](#communication_mechanims_figure), i modelli
    di messaggistica differiscono in ogni approccio.
  prefs: []
  type: TYPE_NORMAL
- en: La*richiesta-risposta HTTP* è il modello più comune supportato da tutti i client
    e i server web, adatto alle API RESTful e ai servizi che non richiedono aggiornamenti
    in tempo reale.
  prefs: []
  type: TYPE_NORMAL
- en: Il*polling breve/regolare* prevede che i client verifichino la presenza di dati
    a intervalli prestabiliti, il che è semplice ma può essere dispendioso in termini
    di risorse quando si scalano i servizi. Di solito viene utilizzato nelle applicazioni
    per eseguire aggiornamenti poco frequenti, come ad esempio nei cruscotti analitici.
  prefs: []
  type: TYPE_NORMAL
- en: Il*polling lungo* è più efficiente per gli aggiornamenti in tempo reale, in
    quanto mantiene aperte le connessioni fino a quando i dati non sono disponibili
    sul server. Tuttavia, può comunque esaurire le risorse del server, il che lo rende
    ideale per le funzioni quasi in tempo reale come le notifiche.
  prefs: []
  type: TYPE_NORMAL
- en: '*SSE* mantiene una singola connessione persistente solo da server a client,
    utilizzando il protocollo HTTP. È semplice da configurare, sfrutta l''API `EventSource`
    del browser e viene fornito con funzioni integrate come la riconnessione. Questi
    fattori rendono SSE adatto alle applicazioni che richiedono feed live, funzioni
    di chat e dashboard in tempo reale.'
  prefs: []
  type: TYPE_NORMAL
- en: '*WebSocket* offre una comunicazione full-duplex (a doppia faccia) con bassa
    latenza e supporto per i dati binari, ma è complesso da implementare. È ampiamente
    utilizzato nelle applicazioni che richiedono un''elevata interattività e lo scambio
    di dati in tempo reale, come i giochi multiplayer, le applicazioni di chat, gli
    strumenti collaborativi e i servizi di trascrizione in tempo reale.'
  prefs: []
  type: TYPE_NORMAL
- en: Con l'invenzione di SSE e WebSocket e la loro crescente popolarità, il polling
    breve/regolare e il polling lungo stanno diventando meccanismi in tempo reale
    meno comuni nelle applicazioni web.
  prefs: []
  type: TYPE_NORMAL
- en: La[Tabella 6-1](#communication_mechanisms_table) mette a confronto le caratteristiche,
    le sfide e le applicazioni di ciascun meccanismo.
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 6-1\. Confronto tra i meccanismi di comunicazione web
  prefs: []
  type: TYPE_NORMAL
- en: '| Meccanismo di comunicazione | Caratteristiche | Sfide | Applicazioni |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Richiesta-risposta HTTP | Modello semplice di richiesta e risposta, protocollo
    stateless, supportato da tutti i client e i server web. | Alta latenza per gli
    aggiornamenti in tempo reale, inefficiente per il trasferimento frequente di dati
    da server a client | API RESTful, servizi web in cui gli aggiornamenti in tempo
    reale non sono critici |'
  prefs: []
  type: TYPE_TB
- en: '| Sondaggi brevi/regolari | Il cliente richiede regolarmente i dati a intervalli,
    facile da implementare | Spreco di risorse quando non ci sono nuovi dati, la latenza
    dipende dagli intervalli di sondaggio | Applicazioni con aggiornamenti poco frequenti,
    semplici dashboard quasi in tempo reale, aggiornamenti sullo stato dei lavori
    inviati |'
  prefs: []
  type: TYPE_TB
- en: '| Sondaggio lungo | Più efficiente del polling breve per gli aggiornamenti
    in tempo reale, mantiene la connessione aperta fino a quando i dati sono disponibili
    | Può richiedere molte risorse al server, è complesso gestire più connessioni.
    | Notifiche in tempo reale, vecchie applicazioni di chat |'
  prefs: []
  type: TYPE_TB
- en: '| Eventi inviati dal server | Singola connessione persistente per gli aggiornamenti,
    riconnessione integrata e supporto dell''ID evento | Comunicazione unidirezionale
    solo dal server al client | Feed in diretta, applicazione di chat, dashboard di
    analisi in tempo reale |'
  prefs: []
  type: TYPE_TB
- en: '| WebSocket | Comunicazione full-duplex, bassa latenza, supporto dei dati binari
    | Più complesso da implementare e gestire, richiede il supporto di WebSocket sul
    server. | Giochi multiplayer, applicazioni di chat, strumenti di editing collaborativo,
    applicazioni per videoconferenze e webinar, applicazioni per la trascrizione e
    la traduzione in tempo reale. |'
  prefs: []
  type: TYPE_TB
- en: Dopo aver esaminato in dettaglio i meccanismi di comunicazione in tempo reale,
    approfondiamo l'argomento SSE e WebSocket implementando i nostri endpoint di streaming
    utilizzando questi due meccanismi.Nella prossima sezione scoprirai come implementare
    gli endpoint di streaming con entrambe le tecnologie.
  prefs: []
  type: TYPE_NORMAL
- en: Implementare gli endpoint SSE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nel [Capitolo 3](ch03.html#ch03) hai imparato a conoscere gli LLMs, che sono
    modelli *autoregressivi* che prevedono il token successivo in base agli input
    precedenti. Dopo ogni fase di generazione, il token di uscita viene aggiunto agli
    input e passato nuovamente attraverso il modello fino a quando non viene generato
    un token `<stop>` per interrompere il ciclo. Invece di aspettare che il ciclo
    finisca, puoi inoltrare all'utente i token di uscita mentre vengono generati come
    flusso di dati.
  prefs: []
  type: TYPE_NORMAL
- en: I provider di modelli normalmente espongono un'opzione che ti permette di impostare
    la modalità di uscita come flusso di dati utilizzando `stream=True`. Con questa
    opzione, il provider di modelli può restituirti un generatore di dati invece dell'output
    finale, che puoi passare direttamente al tuo server FastAPI per lo streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Per dimostrarlo in azione, fai riferimento all'[Esempio 6-2](#async_azure_openai_client),
    che implementa un generatore di dati asincrono utilizzando la libreria `openai`.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Per eseguire l'[Esempio 6-2](#async_azure_openai_client), dovrai creare un'istanza
    di Azure OpenAI sul portale di Azure e creare un modello di distribuzione. Prendi
    nota dell'endpoint API, della chiave e del nome del modello di distribuzione.
    Per l'[Esempio 6-2](#async_azure_openai_client), puoi utilizzare la versione `2023-05-15`
    api.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-2\. Implementazione del client di chat asincrono di Azure OpenAI per
    lo streaming delle risposte
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea un client asincrono `AzureOpenAIChatClient` per interagire con l'API OpenAI
    di Azure. Il client di chat richiede un endpoint API, un nome di distribuzione,
    una chiave e una versione per funzionare.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Definisce un metodo generatore asincrono `chat_stream` che produce ogni token
    di uscita dall'API.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Imposta `stream=True` per ricevere un flusso di output dall'API invece della
    risposta completa in una volta sola.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Esegue un loop sullo stream e restituisce ogni token di output o restituisce
    una stringa vuota se `delta.content` è vuoto. La sottostringa `data:` deve essere
    anteposta a ogni token in modo che i browser possano analizzare correttamente
    il contenuto utilizzando l'API `EventSource`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Rallenta la velocità di streaming per ridurre la contropressione sui clienti.
  prefs: []
  type: TYPE_NORMAL
- en: Nell'[Esempio 6-2](#async_azure_openai_client), crei un'istanza di`AsyncAzureOpenAI`,
    che ti permette di chattare con i modelli Azure OpenAI tramite un'API nel tuo
    ambiente Azure privato.
  prefs: []
  type: TYPE_NORMAL
- en: Impostando il prefisso `stream=True`, `AsyncAzureOpenAI` restituisce un flusso
    di dati (una funzione generatrice asincrona) invece della risposta completa del
    modello. Puoi eseguire un loop sul flusso di dati e sui token `yield` con il prefisso
    `data:` per conformarti alle specifiche SSE. In questo modo i browser potranno
    analizzare automaticamente il contenuto del flusso utilizzando l'API web `EventSource`,
    ampiamente disponibile.^([3](ch06.html#id906))
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Quando esponi endpoint di streaming, dovrai considerare la velocità con cui
    i client possono consumare i dati che gli stai inviando. Una buona pratica è quella
    di ridurre la velocità di streaming come hai visto nell'[Esempio 6-2](#async_azure_openai_client)
    per ridurre la pressione sui client. Puoi regolare il throttling testando i tuoi
    servizi con diversi client su vari dispositivi.
  prefs: []
  type: TYPE_NORMAL
- en: SSE con richiesta GET
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ora puoi implementare l'endpoint SSE passando il flusso di chat al sito `StreamingResponse`
    di FastAPI come endpoint `GET`, come mostrato nell'[Esempio 6-3](#sse_endpoint).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-3\. Implementazione di un endpoint SSE utilizzando la FastAPI `StreamingResponse`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Implementa un endpoint SSE con il metodo `GET` da utilizzare con l'API `EventSource`
    sul browser.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Passa il generatore di flussi di chat a `StreamingResponse` per inoltrare il
    flusso di output che viene generato al client. Imposta `media_type=text/event-stream`
    come da specifiche SSE in modo che i browser possano gestire correttamente la
    risposta.
  prefs: []
  type: TYPE_NORMAL
- en: Con l'endpoint `GET` configurato sul server, puoi creare un semplice modulo
    HTML sul client per consumare il flusso SSE tramite l'interfaccia `EventSource`,
    come mostrato nell'[Esempio 6-4](#sse_client).
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: L['esempio 6-4](#sse_client) non utilizza alcuna libreria JavaScript o framework
    web, ma esistono librerie che ti aiuteranno a implementare la connessione`EventSource`
    in qualsiasi framework di tua scelta come React, Vue o SvelteKit.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-4\. Implementazione di SSE sul client utilizzando l'API del browser
    `EventSource`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea un semplice input e un pulsante HTML per avviare le richieste SSE.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea un contenitore vuoto da utilizzare come lavandino per il contenuto dello
    stream.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Ascolta il pulsante `clicks` ed esegue il callback SSE.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Ripristina il modulo del contenuto e il contenitore di risposta del contenuto
    precedente.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea un nuovo oggetto `EventSource` e ascolta i cambiamenti di stato della connessione
    per gestire gli eventi.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO4-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Registra nella console quando viene aperta una connessione SSE. Gestisce ogni
    messaggio renderizzando il contenuto del messaggio nel contenitore delle risposte
    fino a quando non viene ricevuto il messaggio `[DONE]`, che segnala che la connessione
    deve essere chiusa. Inoltre, chiude la connessione se si verificano errori e registra
    l'errore nellaconsole del browser.
  prefs: []
  type: TYPE_NORMAL
- en: Con il client SSE implementato nell'[Esempio 6-4](#sse_client), puoi ora utilizzarlo
    per testare il tuo endpoint SSE. Tuttavia, devi prima servire l'HTML.
  prefs: []
  type: TYPE_NORMAL
- en: Crea una directory `pages` e inserisci il file HTML al suo interno. Poi *monta*
    la directory sul tuo server FastAPI per servire il suo contenuto come file statico,
    come mostrato nell'[Esempio 6-5](#mounting_static_files). Tramite il montaggio,
    FastAPI si occupa di mappare i percorsi API di ogni file in modo che tu possa
    accedervi con un browser dalla stessa origine del tuo server.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-5\. Montare i file HTML sul server come risorse statiche
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Monta la cartella `pages` su `/pages` per servire i suoi contenuti come risorse
    statiche. Una volta montata, puoi accedere a ogni file visitando`*<origin>*/pages/*<filename>*`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementando l'[Esempio 6-5](#mounting_static_files), servirai l'HTML dalla
    stessa origine del tuo server API, evitando così di attivare il meccanismo di
    sicurezza CORS del browser, che può bloccare le richieste in uscita che raggiungono
    il tuo server.
  prefs: []
  type: TYPE_NORMAL
- en: Ora puoi accedere alla pagina HTML visitando il sito`http://localhost:8000/pages/sse-client.html`.
  prefs: []
  type: TYPE_NORMAL
- en: Condivisione di risorse tra origini diverse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Se provi ad aprire direttamente il file HTML dell'[Esempio 6-4](#sse_client)
    nel tuo browser e clicchi sul pulsante Avvia streaming, noterai che non succede
    nulla. Puoi controllare la scheda di rete del browser per vedere cosa è successo
    alle richieste in uscita.
  prefs: []
  type: TYPE_NORMAL
- en: Dopo alcune indagini, dovresti notare che il tuo browser ha bloccato le richieste
    in uscita verso il tuo server perché i suoi controlli preliminari di*condivisione
    delle risorse in origine* (CORS) con il tuo server sono falliti.
  prefs: []
  type: TYPE_NORMAL
- en: CORS è un meccanismo di sicurezza implementato nei browser per controllare il
    modo in cui le risorse di una pagina web possono essere richieste da un altro
    dominio ed è rilevante solo quando si inviano richieste direttamente dal browser
    invece che da un server. I browser utilizzano CORS per verificare se sono autorizzati
    a inviare richieste al server da un'origine (cioè un dominio) diversa da quella
    del server.
  prefs: []
  type: TYPE_NORMAL
- en: Ad esempio, se il tuo client è ospitato su `https://example.com` e deve recuperare
    dati da un'API ospitata su `https://api.example.com`, il browser bloccherà la
    richiesta a meno che il server API non abbia abilitato CORS.
  prefs: []
  type: TYPE_NORMAL
- en: Per il momento, puoi aggirare questi errori CORS aggiungendo un middleware CORS
    sul tuo server, come puoi vedere nell'[Esempio 6-6](#cors), per consentire qualsiasi
    richiesta in arrivo dai browser.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-6\. Applicare le impostazioni CORS
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Consente le richieste in arrivo da qualsiasi origine, metodo (`GET`, `POST`,
    ecc.) eintestazione.
  prefs: []
  type: TYPE_NORMAL
- en: Streamlit evita di attivare il meccanismo CORS inviando le richieste sul suo
    server interno anche se l'interfaccia utente generata viene eseguita sul browser.
  prefs: []
  type: TYPE_NORMAL
- en: D'altra parte, la pagina di documentazione di FastAPI effettua le richieste
    dalla stessa origine del server (cioè `http://localhost:8000`), quindi le richieste
    per impostazione predefinita non attivano ilmeccanismo di sicurezza CORS.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Nell'[Esempio 6-6](#cors), configuri il middleware CORS per elaborare qualsiasi
    richiesta in arrivo, aggirando di fatto il meccanismo di sicurezza CORS per facilitare
    lo sviluppo. In produzione, dovresti consentire al tuo server di elaborare solo
    una manciata di origini, metodi e intestazioni.
  prefs: []
  type: TYPE_NORMAL
- en: Se hai seguito l'esempio [6-5](#mounting_static_files) o [6-6](#cors), ora dovresti
    essere in grado di visualizzare il flusso in arrivo dal tuo endpoint SSE (vedi
    [Figura 6-8](#sse_results)).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0608](assets/bgai_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 6-8\. Flusso in entrata dall'endpoint SSE
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulazioni! Ora hai una soluzione perfettamente funzionante in cui le risposte
    del modello vengono trasmesse direttamente al tuo cliente non appena i dati generati
    sono disponibili. Grazie all'implementazione di questa funzione, i tuoi utenti
    avranno un'esperienza più piacevole nell'interazione con il tuo chatbot, poiché
    riceveranno le risposte alle loro query in tempo reale.
  prefs: []
  type: TYPE_NORMAL
- en: La tua soluzione ha anche implementato la concurrency utilizzando un client
    asincrono per interagire con l'API OpenAI di Azure per trasmettere risposte più
    veloci ai tuoi utenti. Puoi provare a utilizzare un client sincrono per confrontare
    le differenze nella velocità di generazione. Con un client asincrono, la velocità
    di generazione può essere così elevata che riceverai un blocco di testo in una
    sola volta anche se in realtà viene trasmesso al browser.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming dei risultati LLM dei modelli Hugging Face
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ora che hai imparato a implementare gli endpoint SSE con i provider di modelli
    come Azure OpenAI, ti starai chiedendo se puoi trasmettere gli output dei modelli
    open source che hai precedentemente scaricato da Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Sebbene la libreria `transformers` di Hugging Face implementi un componente
    `TextStreamer`che puoi passare alla tua pipeline di modelli, la soluzione più
    semplice è quella di eseguire un server di inferenza separato come HF Inference
    Server per implementare lo streaming dei modelli.
  prefs: []
  type: TYPE_NORMAL
- en: L['esempio 6-7](#hf_llm_inference_server) mostra come configurare un semplice
    server di inferenza di modelli utilizzando Docker, fornendo un sito `model-id`.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-7\. Servire i modelli HF LLM tramite il server di inferenza HF
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Usa Docker per scaricare ed eseguire il container `vllm/vllm-openai` più recente
    su tutte le GPU NVIDIA disponibili.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Condividi un volume con il contenitore Docker per evitare di scaricare pesi
    a ogni esecuzione.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Imposta la variabile d'ambiente secret per accedere a modelli con accesso limitato
    come `mistralai/Mistral-7B-v0.1`.^([4](ch06.html#id909))
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Esegui il server di inferenza sulla porta localhost `8080` mappando la porta
    dell'host `8080` sulla porta esposta del contenitore Docker `8000`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO7-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Abilita la comunicazione inter-processo (IPC) tra il contenitore e l'host per
    consentire al contenitore di accedere alla memoria condivisa dell'host.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO7-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Il server di inferenza vLLM utilizza le specifiche API OpenAI per il servizio
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO7-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Scarica e utilizza il sito `mistralai/Mistral-7B-v0.1` di Hugging Face Hub.
  prefs: []
  type: TYPE_NORMAL
- en: Con il server del modello in esecuzione, puoi ora utilizzare un sito `AsyncInferenceClient`
    per generare output in formato streaming, come mostrato nell'[Esempio 6-8](#hf_llm_streaming).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-8\. Consumo del flusso di output LLM dal flusso di inferenza HF
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Sebbene [l'Esempio 6-8](#hf_llm_streaming) mostri come utilizzare il server
    di inferenza Hugging Face, puoi comunque utilizzare altri framework di model-serving
    come [vLLM](https://oreil.ly/LQAzF) che supportano lo streaming delle risposte
    del modello.
  prefs: []
  type: TYPE_NORMAL
- en: Prima di passare a parlare di WebSocket, vediamo come consumare un'altra variante
    di endpoint SSE utilizzando il metodo `POST`.
  prefs: []
  type: TYPE_NORMAL
- en: SSE con richiesta POST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: La [specifica`EventSource`](https://oreil.ly/61ovi) prevede che gli endpoint
    di `GET` sul server consumino correttamente il flusso SSE in entrata. Questo rende
    semplice l'implementazione di applicazioni in tempo reale con SSE, poiché l'interfaccia`EventSource`
    è in grado di gestire problemi come la caduta della connessione e la riconnessione
    automatica.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tuttavia, l''utilizzo delle richieste HTTP `GET` comporta delle limitazioni:
    le richieste`GET` sono normalmente meno sicure rispetto agli altri metodi di richiesta
    e più vulnerabili agli attacchi *XSS*.^([5](ch06.html#id912)) Inoltre, poiché
    le richieste `GET` non possono avere un corpo della richiesta, puoi trasferire
    i dati solo come parte dei parametri di query dell''URL al server. Il problema
    è che c''è un limite di lunghezza dell''URL che devi considerare e tutti i parametri
    di query devono essere codificati correttamente nell''URL della richiesta. Pertanto,
    non puoi semplicemente aggiungere l''intera cronologia della conversazione all''URL
    come parametro. Il tuo server deve gestire la cronologia della conversazione e
    tenere traccia del contesto della conversazione con gliendpoint SSE di `GET` .'
  prefs: []
  type: TYPE_NORMAL
- en: Una soluzione comune a questa limitazione consiste nell'implementare un endpoint
    SSE `POST` anche se la specifica SSE non lo supporta. Di conseguenza, l'implementazione
    sarà più complessa.
  prefs: []
  type: TYPE_NORMAL
- en: Per prima cosa, implementiamo l'endpoint `POST` sul server nell'[Esempio 6-9](#sse_server_post).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-9\. Implementazione dell'endpoint SSE sul server
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Con l'implementazione dell'endpoint `POST` per lo streaming degli output della
    chat, puoi ora sviluppare la logica del client per elaborare lo stream SSE.
  prefs: []
  type: TYPE_NORMAL
- en: Dovrai elaborare manualmente lo streaming in arrivo utilizzando l'interfaccia
    web `fetch` del browser, come mostrato nell'[Esempio 6-10](#sse_client_post).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-10\. Implementazione di SSE sul client utilizzando l'API del browser
    `EventSource`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Invia una richiesta `POST` al backend utilizzando l'interfaccia `fetch` del
    browser. Prepara il corpo come stringa JSON come parte della richiesta. Aggiungi
    le intestazioni per specificare il corpo della richiesta inviata e la risposta
    attesa dal server.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Accede a `reader` del flusso dal flusso del corpo della risposta.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea un'istanza di decodificatore di testo per elaborare ogni messaggio.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO8-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Esegue un ciclo infinito e legge il messaggio successivo nel flusso utilizzando`reader`.
    Se il flusso è terminato, `done=true`, quindi interrompe il ciclo; altrimenti,
    decodifica il messaggio con il decodificatore di testo e lo aggiunge al contenitore
    delle risposte`textContent` per eseguire il rendering.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO8-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Ascolta gli eventi del pulsante `click` per eseguire un callback che ripristina
    lo stato del modulo ed effettua la connessione SSE con l'endpoint del backend
    con un prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Come si può vedere dall'[Esempio 6-10](#sse_client_post), consumare il flusso
    SSE senza il parametro`EventSource` può diventare complesso.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Un'alternativa all'[Esempio 6-10](#sse_client_post) è quella di utilizzare gli
    endpoint SSE di `GET` ma di inviare preventivamente il payload di grandi dimensioni
    al server tramite una richiesta `POST`. Il server memorizza i dati e li utilizza
    quando viene stabilita la connessione SSE.
  prefs: []
  type: TYPE_NORMAL
- en: SSE supporta anche i cookie, quindi puoi affidarti ai cookie per scambiare payload
    di grandi dimensioni negli endpoint di `GET` SSE.
  prefs: []
  type: TYPE_NORMAL
- en: Se vuoi utilizzare l'endpoint SSE in produzione, la tua soluzione deve supportare
    anche la funzionalità di retry, la gestione degli errori o addirittura la possibilità
    di interrompere le connessioni.
  prefs: []
  type: TYPE_NORMAL
- en: L['esempio 6-11](#sse_retry) mostra come implementare in JavaScript una funzionalità
    di ritentativo lato client con un *ritardo di backoff esponenziale*.^([6](ch06.html#id914))
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-11\. Implementazione della funzionalità di retry lato client con backoff
    esponenziale
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Finché `maxRetries` non viene raggiunto, prova a stabilire la connessione SSE.
    Conta ogni tentativo.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizza `try` e `catch` per gestire gli errori di connessione.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO9-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Esce dalla funzione in caso di successo.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO9-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Pausa in `delay` millisecondi prima di riprovare.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO9-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Implementa il backoff esponenziale moltiplicando un fattore di backoff al valore
    del ritardo in ogni iterazione.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO9-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Lancia `error` se viene raggiunto `maxRetries`.
  prefs: []
  type: TYPE_NORMAL
- en: Ora dovresti sentirti più a tuo agio nell'implementare i tuoi endpoint SSE per
    lo streaming delle risposte del modello. SSE è il meccanismo di comunicazione
    che applicazioni come ChatGPT utilizzano per le conversazioni in tempo reale con
    il modello. Poiché SSE supporta prevalentemente flussi basati sul testo, è ideale
    per gli scenari di streaming dell'output di LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Nella prossima sezione, implementeremo la stessa soluzione utilizzando il meccanismo
    WebSocket, in modo da poter confrontare le differenze nei dettagli dell'implementazione.
    Inoltre, imparerai cosa rende WebSocket ideale per gli scenari che richiedono
    una comunicazione duplex in tempo reale, come nei servizi di trascrizione dal
    vivo.
  prefs: []
  type: TYPE_NORMAL
- en: Implementare gli endpoint WS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In questa sezione, implementerai un endpoint utilizzando il protocollo WebSocket.
    Con questo endpoint, trasmetterai gli output di LLM al client utilizzando WebSocket
    per confrontarli con la connessione SSE. Alla fine, imparerai le differenze e
    le somiglianze tra SSE e WebSocket nello streaming degli output di LLM in tempo
    reale.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming degli output di LLM con WebSocket
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FastAPI supporta WebSocket attraverso l'uso dell'interfaccia `WebSocket` del
    framework web Starlette. Poiché le connessioni WebSocket devono essere gestite,
    iniziamo con l'implementazione di un gestore di connessioni per tenere traccia
    delle connessioni attive e gestire i loro stati.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi implementare un gestore di connessioni WebSocket seguendo l'[Esempio 6-12](#websockets_manager).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-12\. Implementazione di un gestore di connessioni WebSocket
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea un `WSConnectionManager` per tenere traccia e gestire le connessioni WS
    attive.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO10-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Apri una connessione WebSocket utilizzando il metodo `accept()`. Aggiungi la
    nuova connessione all'elenco delle connessioni attive.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Quando ti disconnetti, chiudi la connessione e rimuovi l'istanza di `websocket`dall'elenco
    delle connessioni attive.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO10-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Ricevi i messaggi in arrivo come testo durante una connessione aperta.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO10-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Invia i messaggi al client utilizzando il metodo di invio pertinente.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO10-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea una singola istanza di `WSConnectionManager` da riutilizzare in tutta l'applicazione.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi anche estendere il gestore delle connessioni dell'[Esempio 6-12](#websockets_manager)
    per *trasmettere* messaggi (ad esempio, avvisi, notifiche o aggiornamenti del
    sistema in tempo reale) a tutti i client connessi. Questo è utile in applicazioni
    come le chat di gruppo o gli strumenti di editing collaborativo di lavagne e documenti.
  prefs: []
  type: TYPE_NORMAL
- en: Poiché il gestore delle connessioni mantiene un puntatore a ogni client tramite
    l'elenco `active_​con⁠nec⁠tions`, puoi trasmettere messaggi a ogni client, come
    mostrato nell'[Esempio 6-13](#websockets_broadcast).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-13\. Trasmissione di messaggi ai client connessi utilizzando il gestore
    WebSocket
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Con l'implementazione del gestore WebSocket, ora puoi sviluppare un endpoint
    WebSocket per trasmettere le risposte ai client. Tuttavia, prima di implementare
    l'endpoint, segui l'[Esempio 6-14](#chat_stream_ws) per aggiornare il metodo`chat_stream`in
    modo che produca il contenuto dello stream in un formato adatto alle connessioni
    WebSocket.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-14\. Aggiornare il metodo di streaming del client di chat per produrre
    contenuti adatti alle connessioni WebSocket
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Restituisce solo contenuti non vuoti.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO11-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Restituisce il contenuto del flusso in base al tipo di connessione (SSE o WS).
  prefs: []
  type: TYPE_NORMAL
- en: Dopo aver aggiornato il metodo `stream_chat`, puoi concentrarti sull'aggiunta
    di un endpoint WebSocket. Utilizza il metodo `@app.websocket` per decorare una
    funzione del controller che utilizza la classe `WebSocket` di FastAPI, come mostrato
    nell'[esempio 6-15](#websocket_endpoint).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-15\. Implementazione di un endpoint WS
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO12-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea un endpoint WebSocket accessibile all'indirizzo`ws://localhost:8000/generate/text/stream`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO12-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Apri la connessione WebSocket tra il client e il server.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO12-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Finché la connessione è aperta, continua a inviare o ricevere messaggi.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO12-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Gestire gli errori e registrare gli eventi importanti all'interno di `websocket_controller`
    per identificare le cause degli errori e gestire le situazioni inaspettate con
    grazia. Interrompere il ciclo infinito quando la connessione viene chiusa dal
    server o dal client.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO12-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Quando viene ricevuto il primo messaggio, passalo come prompt all'API OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO12-6)'
  prefs: []
  type: TYPE_NORMAL
- en: iterare in modo asincrono sul flusso di chat generato e inviare ogni pezzo al
    client.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO12-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Attendere un piccolo lasso di tempo prima di inviare il messaggio successivo
    per ridurre i problemi di race condition e consentire al client di avere tempo
    sufficiente per l'elaborazione del flusso.
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](assets/8.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO12-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Quando il client chiude la connessione WebSocket, viene sollevata l'eccezione
    `WebSocketDisconnect`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](assets/9.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO12-9)'
  prefs: []
  type: TYPE_NORMAL
- en: Se si verifica un errore sul lato server durante una connessione aperta, registra
    l'errore e identifica il cliente.
  prefs: []
  type: TYPE_NORMAL
- en: '[![10](assets/10.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO12-10)'
  prefs: []
  type: TYPE_NORMAL
- en: Interrompe il ciclo infinito e chiude con grazia la connessione WebSocket se
    lo stream è terminato, se c'è un errore interno o se il client ha chiuso la connessione.
    Rimuove la connessione dall'elenco delle connessioni WebSocket attive.
  prefs: []
  type: TYPE_NORMAL
- en: Ora che hai un endpoint WebSocket, sviluppiamo l'HTML del client per testare
    l'endpoint (vedi [Esempio 6-16](#ws_client)).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 6-16\. Implementazione di connessioni WebSocket lato client con gestione
    degli errori e funzionalità di backoff retry esponenziale
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO13-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Stabilisci una connessione WebSocket con il server FastAPI.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO13-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Aggiungi dei gestori di callback all'istanza di connessione WebSocket per gestire
    gli eventi di apertura, chiusura, messaggio ed errore.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO13-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Gestisce con grazia gli errori di connessione e ristabilisce la connessione
    con una funzionalità di backoff retry esponenziale utilizzando un flag `isError`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO13-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Aggiungi un ascoltatore di eventi al pulsante di streaming per inviare il primo
    messaggio al server.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO13-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Una volta stabilita la connessione, invia il prompt iniziale non vuoto come
    primo messaggio al server.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO13-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Reimposta il modulo prima di stabilire la connessione WebSocket da avviare.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_real_time_communication___span_class__keep_together__with_generative_models__span__CO13-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Aggiungi un ascoltatore di eventi al pulsante di chiusura della connessione
    per chiudere la connessione quando il pulsante viene cliccato.
  prefs: []
  type: TYPE_NORMAL
- en: Ora puoi visitare [*http://localhost:8000/pages/client-ws.html*](http://localhost:8000/pages/client-ws.html)
    per testare il tuo endpoint di streaming WebSocket (vedi [Figura 6-9](#ws_results)).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0609](assets/bgai_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 6-9\. Flusso in entrata dall'endpoint WebSocket
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ora dovresti avere un'applicazione di streaming LLM completamente funzionante
    con WebSocket. Ben fatto!
  prefs: []
  type: TYPE_NORMAL
- en: 'A questo punto ti starai chiedendo quale sia la soluzione migliore: lo streaming
    con SSE o le connessioni WS. La risposta dipende dai requisiti della tua applicazione.
    SSE è semplice da implementare ed è nativo del protocollo HTTP, quindi la maggior
    parte dei client lo supporta. Se hai bisogno solo di uno streaming unidirezionale
    verso il client, allora ti consiglio di implementare le connessioni SSE per lo
    streaming degli output di LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Le connessioni WebSocket forniscono un maggiore controllo al meccanismo di streaming
    e consentono una comunicazione duplex all'interno della stessa connessione, ad
    esempio nelle applicazioni di chat in tempo reale con più utenti e nei servizi
    LLM, speech-to-text, text-to-speech e speech-to-speech. Tuttavia, l'utilizzo di
    WebSocket richiede l'aggiornamento della connessione da HTTP al protocollo WebSocket,
    che i client legacy e i browser più vecchi potrebbero non supportare. Inoltre,
    dovrai gestire le eccezioni in modo leggermente diverso con gli endpoint WebSocket.
  prefs: []
  type: TYPE_NORMAL
- en: Gestire le eccezioni WebSocket
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: La gestione delle eccezioni WebSocket è diversa da quella delle connessioni
    HTTP tradizionali. Se fai riferimento all'[Esempio 6-15](#websocket_endpoint),
    noterai che non stai più restituendo al cliente una risposta con codici di stato,
    o `HTTPExceptions`, ma piuttosto stai mantenendo una connessione aperta dopo l'accettazione
    della connessione.
  prefs: []
  type: TYPE_NORMAL
- en: Finché la connessione è aperta, puoi inviare e ricevere messaggi. Tuttavia,
    non appena si verifica un'eccezione, devi gestirla chiudendo con grazia la connessione
    e/o inviando un messaggio di errore al client in sostituzione della risposta `HTTPException`.
  prefs: []
  type: TYPE_NORMAL
- en: Poiché il protocollo WebSocket non supporta i consueti codici di stato HTTP
    (`4xx` o `5xx`), non puoi usare i codici di stato per notificare ai client i problemi
    del lato server. Al contrario, dovresti inviare messaggi WebSocket ai client per
    notificare loro i problemi prima di chiudere qualsiasi connessione attiva dal
    server.
  prefs: []
  type: TYPE_NORMAL
- en: Durante la chiusura della connessione, puoi utilizzare diversi codici di stato
    relativi a WebSocket per specificare il motivo della chiusura. Utilizzando questi
    motivi di chiusura, puoi implementare qualsiasi comportamento di chiusura personalizzato
    sul server o sui client.
  prefs: []
  type: TYPE_NORMAL
- en: La[Tabella 6-2](#ws_status_codes) mostra alcuni codici di stato comuni che possono
    essere inviati con un frame `CLOSE`.
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 6-2\. Codici di stato comuni del protocollo WebSocket
  prefs: []
  type: TYPE_NORMAL
- en: '| Codice di stato | Descrizione |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | Chiusura normale |'
  prefs: []
  type: TYPE_TB
- en: '| 1001 | Il cliente si è allontanato o il server è andato giù |'
  prefs: []
  type: TYPE_TB
- en: '| 1002 | Un endpoint (ad esempio, client o server) ha ricevuto dati che violano
    il protocollo WS (ad esempio, pacchetti non mascherati, lunghezza del payload
    non valida). |'
  prefs: []
  type: TYPE_TB
- en: '| 1003 | Un endpoint ha ricevuto dati non supportati (ad esempio, si aspettava
    un testo e ha ricevuto un file binario). |'
  prefs: []
  type: TYPE_TB
- en: '| 1007 | Un endpoint ha ricevuto dati codificati in modo incoerente (ad esempio,
    dati non UTF-8 all''interno di un messaggio di testo). |'
  prefs: []
  type: TYPE_TB
- en: '| 1008 | Un endpoint ha ricevuto un messaggio che viola la sua politica; può
    essere utilizzato per nascondere i dettagli della chiusura per motivi di sicurezza.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1011 | Errore interno del server |'
  prefs: []
  type: TYPE_TB
- en: Puoi trovare maggiori informazioni sugli altri codici di stato WebSocket nella[sezione
    7.4](https://oreil.ly/1L_HH) del protocollo WebSocket[RFC 6455.](https://oreil.ly/1L_HH)
  prefs: []
  type: TYPE_NORMAL
- en: Progettare API per lo streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ora che hai acquisito una maggiore familiarità con le implementazioni degli
    endpoint SSE e WebSocket, desidero soffermarmi su un ultimo importante dettaglio
    relativo al loro design architettonico.
  prefs: []
  type: TYPE_NORMAL
- en: Un'insidia comune nella progettazione di API di streaming è l'esposizione di
    un numero eccessivo di endpoint di streaming. Ad esempio, se stai realizzando
    un'applicazione di chatbot, potresti esporre diversi endpoint di streaming, ognuno
    preconfigurato per gestire diversi messaggi in arrivo in una singola conversazione.
    Utilizzando questo particolare modello di progettazione dell'API, chiedi al cliente
    di passare da un endpoint all'altro, fornendo le informazioni necessarie in ogni
    passaggio e navigando tra le connessioni in streaming durante una singola conversazione.
    Questo modello di progettazione aumenta la complessità delle applicazioni sia
    di backend che di frontend, poiché gli stati della conversazione devono essere
    gestiti da entrambi i lati, evitando condizioni di gara e problemi di rete tra
    i componenti.
  prefs: []
  type: TYPE_NORMAL
- en: Un modello di progettazione dell'API più semplice consiste nel fornire un unico
    punto di ingresso al client per avviare un flusso con i tuoi modelli GenAI e utilizzare
    le intestazioni, il corpo della richiesta o i parametri della query per attivare
    la logica pertinente nel backend. Con questo design, la logica del backend è astratta
    dal client, il che semplifica la gestione dello stato sul frontend, mentre tutte
    le rotte e la logica aziendale sono implementate nel backend. Poiché il backend
    ha accesso ai database, ad altri servizi e a prompt personalizzati, può facilmenteeseguire
    operazioni CRUD e passare da un prompt all'altro o da un modello all'altro per
    calcolare una risposta. Pertanto, un endpoint può fungere da unico punto di ingresso
    per la logica di commutazione, gestire gli stati dell'applicazione e generare
    risposte personalizzate.
  prefs: []
  type: TYPE_NORMAL
- en: Riassunto
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Questo capitolo ha trattato diverse strategie per implementare la comunicazione
    in tempo reale tramite lo streaming dei dati nei tuoi servizi GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Hai imparato a conoscere diversi meccanismi di comunicazione web, tra cui il
    tradizionale modello HTTP richiesta-risposta, il polling breve/regolare, il polling
    lungo, SSE e WebSocket. Hai poi confrontato questi meccanismi in dettaglio per
    comprenderne le caratteristiche, i vantaggi, gli svantaggi e i casi d'uso, in
    particolare per i flussi di lavoro dell'intelligenza artificiale. Infine, hai
    implementato due endpoint di streaming LLM utilizzando il client asincrono Azure
    OpenAI per imparare a sfruttare imeccanismi di comunicazione in tempo reale SSE
    e WebSocket.
  prefs: []
  type: TYPE_NORMAL
- en: Nel prossimo capitolo imparerai a conoscere meglio i flussi di lavoro per lo
    sviluppo delle API durante l'integrazione dei database per i servizi di intelligenza
    artificiale, tra cui come impostare, migrare e interagire con i database. Imparerai
    anche a gestire le operazioni di archiviazione e recupero dei dati all'interno
    degli endpoint di streaming utilizzando i task in background di FastAPI.
  prefs: []
  type: TYPE_NORMAL
- en: Gli argomenti trattati nel prossimo capitolo comprenderanno l'impostazione dei
    database e la progettazione degli schemi, il lavoro con SQLAlchemy, le migrazioni
    di database e la gestione delle operazioni di database durante lo streaming degli
    output dei modelli.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch06.html#id893-marker)) Gli aggressori possono utilizzare il cache poisoning
    per iniettare dati dannosi nei sistemi di caching, che poi servono dati errati
    agli utenti o ai sistemi. Per proteggersi da questo attacco, il client e il server
    mascherano i payload per farli apparire come dati casuali prima di inviarli.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.html#id894-marker)) Questi attacchi consistono nell'ingannare un
    server per far trapelare informazioni sensibili inviando una risposta HTTP a un
    frame WebSocket.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch06.html#id906-marker)) Per maggiori dettagli sull'[interfaccia`EventSource`](https://oreil.ly/0yuKA)
    , consulta le risorse MDN.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch06.html#id909-marker)) Segui la [guida "Accesso ai modelli privati/garantiti"](https://oreil.ly/a7KeV)
    per generare un token di accesso per l'utente di Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch06.html#id912-marker)) Gli aggressori utilizzano la vulnerabilità XSS
    per inserire script dannosi nelle pagine web, che vengono poi eseguiti dai browser
    degli altri utenti.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch06.html#id914-marker)) Il backoff esponenziale riduce le possibilità
    di errori di limitazione del tasso API aumentando il ritardo dopo ogni tentativo.
  prefs: []
  type: TYPE_NORMAL
