["```py\nimport pandas as pd\nimport numpy as np\nimport random\nfrom statistics import mean, stdev\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\")\n)\n\n# Define the prompts to test\nPROMPT_A = \"Is the following email spam? Respond with spam if the email is spam \nor ham if the email is not spam. Use only spam or ham as the answers, nothing \nelse.\\n\\nSubject: {subject}\\n\\nMessage: {message}\"\nPROMPT_B = \"After considering it very carefully, do you think it's likely that \nthe email below is spam? Respond with spam if the email is spam or ham if the \nemail is not spam. Use only spam or ham as the answers, nothing else.\n\\n\\nSubject: {subject}\\n\\nMessage: {message}\"\n\n# Load the dataset and sample\ndf = pd.read_csv(\"enron_spam_data.csv\")\nspam_df = df[df['Spam/Ham'] == 'spam'].sample(n=30)\nham_df = df[df['Spam/Ham'] == 'ham'].sample(n=30)\nsampled_df = pd.concat([spam_df, ham_df])\n\n# Define Evaluation function\n\n# Run and display results\n```", "```py\n#DOCKERFILE\nFROM python:3.9-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"python\", \"serve_model.py\"] \n```", "```py\ndocker build -t my-llm-model .\ndocker run -p 5000:5000 my-llm-model\n```", "```py\n    pipeline { \n        agent any \n        stages { \n            stage('Build Image') { \n                steps { \n                    sh 'docker build -t my-llm-model .' \n                } \n            } \n            stage('Push Image') { \n                steps { \n                    sh 'docker tag my-llm-model myregistry/my-llm-model:latest' \n                    sh 'docker push myregistry/my-llm-model:latest' \n                } \n            } \n            stage('Deploy to Kubernetes') { \n                steps { \n                    sh 'kubectl apply -f deployment.yaml' \n                } \n            } \n        } \n    } \n    ```", "```py\nfrom zenml.pipelines import pipeline \nfrom zenml.steps import step \n\n@step \ndef preprocess_data(): \n    print(\"Preprocessing data for LLM training or inference.\") \n\n@step \ndef deploy_model(): \n    print(\"Deploying the containerized LLM to Kubernetes.\") \n\n@pipeline \ndef llm_pipeline(preprocess_data, deploy_model): \n    preprocess_data() \n    deploy_model() \n\npipeline_instance = llm_pipeline(preprocess_data=preprocess_data(), \n                                 deploy_model=deploy_model()) \npipeline_instance.run() \n```", "```py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass TextRequest(BaseModel):\n    text: str\n\n@app.post(\"/generate\")\nasync def generate_text(request: TextRequest):\n    # Dummy response; replace with LLM inference logic\n    generated_text = f\"Generated text based on: {request.text}\"\n    return {\"input\": request.text, \"output\": generated_text}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```", "```py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass PreprocessRequest(BaseModel):\n    text: str\n\n@app.post(\"/preprocess\")\nasync def preprocess(request: PreprocessRequest):\n    # Basic preprocessing logic\n    preprocessed_text = request.text.lower().strip()\n    return {\"original\": request.text, \"processed\": preprocessed_text}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n```", "```py\n# nginx.conf\n\nserver {\n    listen 80;\n    location /preprocess {\n        proxy_pass http://localhost:8001;\n    }\n    location /generate {\n        proxy_pass http://localhost:8002;\n    }\n}\n```", "```py\nFROM python:3.9-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\"]\n```", "```py\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: preprocessing-service\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: preprocessing\n  template:\n    metadata:\n      labels:\n        app: preprocessing\n    spec:\n      containers:\n      - name: preprocessing\n        image: myregistry/preprocessing-service:latest\n        ports:\n        - containerPort: 8001\n```", "```py\nkubectl apply -f preprocessing-deployment.yaml \n```", "```py\nimport os\nfrom langchain.vectorstores import Pinecone\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom pinecone import init, Index\n\n# Step 1\\. Set environment variables for API keys\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\nos.environ[\"PINECONE_API_KEY\"] = \"your_pinecone_api_key\"\nos.environ[\"PINECONE_ENV\"] = \"your_pinecone_environment\"\n\n# Step 2\\. Initialize Pinecone\ninit(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENV\"])\nindex_name = \"your_index_name\"\n\n# Ensure the index exists\nif index_name not in Pinecone.list_indexes():\n    print(f\"Index '{index_name}' not found. Please create it in Pinecone console.\")\n    exit()\n\n# Step 3\\. Set up the retriever\nembedding_model = OpenAIEmbeddings()\nretriever = Pinecone(index_name=index_name, embedding=embedding_model.embed_query)\n\n# Step 4\\. Define the re-ranker function\ndef rerank_documents(documents, query):\n    \"\"\"\n    Rerank documents based on a simple similarity scoring using embeddings.\n    \"\"\"\n    reranked_docs = sorted(\n        documents,\n        key=lambda doc: embedding_model.similarity(query, doc.page_content),\n        reverse=True,\n    )\n    return reranked_docs[:5]  # Return top 5 documents\n\n# Step 5\\. Set up the LLM and prompt\nllm = OpenAI(model=\"gpt-4\")\n\nprompt_template = \"\"\"\nYou are my hero. Use the following context to answer the user's question:\nContext: {context}\nQuestion: {question}\nAnswer:\n\"\"\"\nprompt = PromptTemplate(template=prompt_template, \n                        input_variables=[\"context\", \"question\"])\n```", "```py\npip install spacy torch torchvision dgl neo4j pandas\npython -m spacy download en_core_web_sm\n```", "```py\n#Step 1: Import all the relevant libraries\nimport spacy\nimport torch\nimport dgl\nimport pandas as pd\nfrom neo4j import GraphDatabase\nfrom spacy.matcher import PhraseMatcher\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.data import Data\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Step 2: Connect to Neo4j for knowledge graph management\nuri = \"bolt://localhost:7687\"\nusername = \"neo4j\"\npassword = \"your_neo4j_password\"\ndriver = GraphDatabase.driver(uri, auth=(username, password))\n\n# Step 3: Define function for entity linking and updating the knowledge graph\ndef link_entities_and_update_kg(text, graph):\n    # Process the text using spaCy to extract entities\n    doc = nlp(text)\n    entities = set([ent.text for ent in doc.ents])\n\n    # Update KG with new entities\n    with graph.session() as session:\n        for entity in entities:\n            session.run(f\"MERGE (e:Entity {{name: '{entity}'}})\")\n\n    print(f\"Entities linked and updated in the KG: {entities}\")\n\n# Step 4: Generate graph embeddings using graph convolutional networks (GCN)\ndef update_graph_embeddings(graph):\n    edges = [(0, 1), (1, 2), (2, 0)]  # Example edges for a graph\n    x = torch.tensor([[1, 2], [2, 3], [3, 4]], dtype=torch.float)\n\n    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n\n    data = Data(x=x, edge_index=edge_index)\n    gcn = GCNConv(in_channels=2, out_channels=2)\n\n    # Forward pass through the GCN\n    output = gcn(data.x, data.edge_index)\n    print(\"Updated Graph Embeddings:\", output)\n\n# Step 5: Automating the KG update process\ndef automate_kg_update(text):\n    link_entities_and_update_kg(text, driver)\n\n    # Step 5b: Update graph embeddings for the KG\n    update_graph_embeddings(driver)\n```", "```py\ndocker pull nvcr.io/nvidia/tritonserver:latest \n```", "```py\nmodel_repository/\n├── my_model/\n│   ├── 1/\n│   │   └── model.pt\n```", "```py\ndocker run --gpus all --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n  -v /path/to/model_repository:/models nvcr.io/nvidia/tritonserver:latest \\\n  tritonserver --model-repository=/models\n```", "```py\nimport tritonclient.grpc\nfrom tritonclient.grpc import service_pb2, service_pb2_grpc\n\n# Set up the Triton client\ntriton_client = tritonclient.grpc.InferenceServerClient(url=\"localhost:8001\")\n\n# Prepare the input data\ninput_data = some_input_data()\n\n# Send inference request\nresponse = triton_client.infer(model_name=\"my_model\", inputs=[input_data])\n\nprint(response)\n```", "```py\nversion: '3'\nservices:\n  model1:\n    image: model1_image\n    ports:\n      - \"5001:5001\"\n  model2:\n    image: model2_image\n    ports:\n      - \"5002:5002\"\n  model3:\n    image: model3_image\n    ports:\n      - \"5003:5003\"\n```", "```py\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: model-deployment\nspec:\n  replicas: 3  # Number of instances to scale\n  selector:\n    matchLabels:\n      app: model\n  template:\n    metadata:\n      labels:\n        app: model\n    spec:\n      containers:\n        - name: model-container\n          image: model_image:latest  # Your actual Docker image\n          ports:\n            - containerPort: 5000\n```", "```py\napiVersion: v1\nkind: Service\nmetadata:\n  name: model-service\nspec:\n  selector:\n    app: model  # Match the app label from the deployment\n  ports:\n    - protocol: TCP\n      port: 80  # External port\n      targetPort: 5000  # Port the model container is listening to\n  type: LoadBalancer\n```", "```py\nkubectl apply -f model-deployment.yaml\nkubectl apply -f model-service.yaml\nkubectl get deployments\nkubectl get services\n```", "```py\nimport asyncio\nimport faiss\nimport numpy as np\n\n# Example function to retrieve vectors from FAISS\nasync def retrieve_from_faiss(query_vector, index):\n    # Simulate a query to FAISS\n    return index.search(np.array([query_vector]), k=5)\n\nasync def batch_retrieve(query_vectors, index):\n    tasks = [\n        retrieve_from_faiss(query_vector, index)\n        for query_vector in query_vectors\n    ]\n\n    results = await asyncio.gather(*tasks)\n    return results\n\n# Initialize FAISS index\ndimension = 128  # Example dimension\nindex = faiss.IndexFlatL2(dimension)  # Use L2 distance for similarity search\n\n# Create some random query vectors\nquery_vectors = np.random.rand(10, dimension).astype('float32')\n\n# Perform asynchronous retrieval\nresults = asyncio.run(batch_retrieve(query_vectors, index))\nprint(results)\n```", "```py\nfrom whoosh.index import create_in\nfrom whoosh.fields import Schema, TEXT\nimport faiss\nimport numpy as np\n\n# Initialize FAISS index for dense retrieval\ndimension = 128\ndense_index = faiss.IndexFlatL2(dimension)\n\n# Simulate sparse retrieval with Whoosh\nschema = Schema(content=TEXT(stored=True))\nix = create_in(\"index\", schema)\nwriter = ix.writer()\n\nwriter.add_document(content=\"This is a test document.\")\nwriter.add_document(content=\"Another document for retrieval.\")\nwriter.commit()\n\n# Query for dense and sparse retrieval\ndef retrieve_dense(query_vector):\n    return dense_index.search(np.array([query_vector]), k=5)\n\ndef retrieve_sparse(query):\n    searcher = ix.searcher()\n    results = searcher.find(\"content\", query)\n    return [hit['content'] for hit in results]\n\nquery_vector = np.random.rand(1, dimension).astype('float32')\nsparse_query = \"document\"\n\n# Perform combined retrieval\ndense_results = retrieve_dense(query_vector)\nsparse_results = retrieve_sparse(sparse_query)\n\n# Combine dense and sparse results\ncombined_results = dense_results + sparse_results\nprint(\"Combined results:\", combined_results)\n```", "```py\nimport joblib\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('MiniLM')\n\n# Check if embeddings are cached\ndef get_embeddings(query):\n    cache_file = \"embedding_cache.pkl\"\n\n    # Check if cache exists\n    try:\n        embeddings_cache = joblib.load(cache_file)\n    except FileNotFoundError:\n        embeddings_cache = {}\n\n    # If query is not in cache, compute and cache the embeddings\n    if query not in embeddings_cache:\n        embedding = model.encode([query])\n        embeddings_cache[query] = embedding\n        joblib.dump(embeddings_cache, cache_file)  # Save cache to disk\n\n    return embeddings_cache[query]\n\n# Query\nquery = \"What is the capital of France?\"\nembedding = get_embeddings(query)\nprint(\"Embedding for the query:\", embedding)\n```", "```py\nimport redis\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\n# Step 1\\. Initialize Redis client\nr = redis.Redis(host='localhost', port=6379, db=0)\n\n# Step 2\\. Initialize sentence transformer model\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# Step 3\\. Function to get embeddings and cache them in Redis\ndef get_embeddings_from_cache_or_compute(query):\n    cache_key = f\"embedding:{query}\"  # Key to store the query embeddings\n\n    # Check if the embedding exists in the cache\n    cached_embedding = r.get(cache_key)\n\n    if cached_embedding:\n        print(\"Cache hit, returning cached embedding\")\n        return np.frombuffer(cached_embedding, dtype=np.float32)\n    else:\n        print(\"Cache miss, computing and storing embedding\")\n        embedding = model.encode([query])\n        r.set(cache_key, embedding.tobytes())  # Store embedding in Redis\n        return embedding\n\n# Step 4\\. Query the system\nquery = \"What is the capital of France?\"\nembedding = get_embeddings_from_cache_or_compute(query)\nprint(\"Embedding:\", embedding)\n```"]