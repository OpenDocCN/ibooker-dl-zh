- en: 4 AI-assisted testing for developers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Developing unit tests and production code with GitHub Copilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing unit tests and production code with ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to a poll on dev ecosystems conducted by JetBrains in 2022, 81% of
    respondents have a developer-to-QA ratio greater than one to one. Forty percent
    reported that they had “less than 1 QA per 10 developers,” and only 1% reported
    that they had “more QAs than developers” (see [www.jetbrains.com/lp/devecosystem-2022/testing](https://www.jetbrains.com/lp/devecosystem-2022/testing).)
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and building in quality is essential to delivering value to our
    users, yet the ratio between development and testing is nearly always imbalanced,
    and this is so for many reasons. Some organizational leaders choose to educate
    developers to build in quality with the support of quality coaches, and others
    simply don’t want to invest in roles that advocate for testing and quality. Either
    way, this situation puts pressure on everyone on a team to deliver high-quality
    applications. So, how can artificial intelligence (AI) tools help relieve this
    pressure?
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will specifically focus on how large language model (LLM) AIs,
    such as GitHub Copilot and ChatGPT, can help developers build quality into our
    work. Rather than thinking of these AI tools as replacements for developers and
    testers, we’ll learn how they can guide us as we carry out activities that build
    in quality as we develop, as well as demonstrate how we can use AI tools to improve
    quality and identify risks when testing is a limited resource to take advantage
    of.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Copilot
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll be looking at examples of GitHub Copilot in use. To familiarize
    yourself with Copilot before we try out these examples, you can find installation
    and operation instructions in appendix B.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Examining the rise of the automated developer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In April 2023, Similarweb, a market competition analysis company, reported that
    Stack Overflow’s traffic in the preceding month had dropped by 14%. According
    to Similarweb, the cause was a rise in developers switching from Stack Overflow
    to tools such as GitHub’s Copilot and ChatGPT (see [https://mng.bz/5Oy4](https://mng.bz/5Oy4)).
    Whether this is a sign of an ongoing trend of developers jumping ship or an anomaly
    that will eventually balance out, reports such as this one demonstrate the sweeping
    changes that LLM-based AI tools (LLMs) are introducing to the role of a developer.
  prefs: []
  type: TYPE_NORMAL
- en: As a tester, it’s an interesting change to witness. For many years, as test
    automation tools became more advanced, I had discussions with developers about
    whether testing could be replaced by automated testing tools. Now, with LLMs that
    are trained against billions of lines of code from public and private repositories,
    conversations have turned toward whether developer roles can be automated. For
    example, a tool such as ChatGPT can be sent a prompt such as
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Write me some Java code that sends an HTTP request to a booking API |'
  prefs: []
  type: TYPE_TB
- en: 'and it will then use its billions of trained weights and balances to return
    a working code example like this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: From just one basic prompt, we can use LLMs to create workable code (updating
    the `apiUrl` to a real API returned a positive result). So, it’s not surprising
    that there are tools that combine prompts attempting to automate development work.
    Tools such as AutoGPT ([https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT))
    and MetaGPT ([https://github.com/geekan/MetaGPT](https://github.com/geekan/MetaGPT))
    have appeared to work as autonomous agents, generating their own prompts based
    on initial questions to solve complex problems. Although these tools are in their
    infancy, it’s clear why the more hyperbolic claims of developers being automated
    out of their role are bandied about.
  prefs: []
  type: TYPE_NORMAL
- en: As someone who has spent much of his career explaining why test automation is
    not a suitable replacement, it’s tempting to enjoy the schadenfreude of seeing
    developers defending their roles in the same way, but instead, it’s more valuable
    to learn from the experiences of testers and the automation topic. Just as a tester’s
    role can’t be fully automated, neither can a developer’s role. A development role
    is more than just the code that is produced. The solutions that developers create
    are a product of analytical skills, problem-solving, and design thinking. LLM
    tools give the impression of having these skills, which is not true.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, developers find success with LLM tools by using them to enhance their
    own abilities: they may use tools such as Copilot to quickly and effectively create
    code that they want to build or seek advice from ChatGPT to solve issues or learn
    new APIs. These principles can also be applied to improving a developer’s ability
    to build quality into an application. By combining techniques such as test-driven
    design (TDD) or the power of LLMs, developers can increase their productivity
    while ensuring that their analytical and design skills lead the charge. To help
    demonstrate this symbiosis, let’s explore the following two examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Copilot to rapidly generate unit checks and production code for TDD loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emulating pairing with a simulated developer, courtesy of ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through the examples, you’ll learn to set up and use these LLM tools, as well
    as appreciate the balance that is possible to strike between the power of AI and
    the abilities of developers.
  prefs: []
  type: TYPE_NORMAL
- en: Experiences may vary
  prefs: []
  type: TYPE_NORMAL
- en: Given that Copilot relies on predictive algorithms that are frequently trained
    on newly added code and updated APIs/libraries, it’s worth highlighting that the
    output and experience you have when following the upcoming examples may be different
    from what has been recorded. Keep in mind that the goal of this chapter is not
    to replicate the examples 100%, but rather for you to become comfortable with
    using LLMs to assist our work in a way that helps us build in quality.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Pairing with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve already seen that LLMs are probabilistic in nature, and as a result, it
    can be useful to think of them as outputting simulations of roles rather than
    inhabiting specific roles. An LLM has no more awareness of being a software tester
    than it does of being a restauranteur. But with prompt engineering, we can create
    prompts that frame an LLM’s probabilistic output to simulate a role, helping us
    to create rubber ducks to interact with. This can be useful in a development capacity
    when testing resources are limited in either availability or capability. So, let’s
    take a look at a couple of sample prompts we can use to elicit feedback that can
    help us improve the quality of our work and our products.
  prefs: []
  type: TYPE_NORMAL
- en: Wait, what—rubber duck?
  prefs: []
  type: TYPE_NORMAL
- en: 'When facing a problem that we have no solution for, it can help to verbalize
    the problem to others to find an answer. By articulating the problem to another
    person, we sometimes find that the solution presents itself to us. However, there
    isn’t always an opportunity to speak with colleagues; therefore, some developers
    will verbalize their issues to a rubber duck (or another item). Although we’re
    sharing our challenges with an inanimate object, the experience is the same: verbalizing
    our challenges tricks our brains into finding solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Analyzing ideas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our first prompt is inspired by the shift-left testing mindset that some teams
    apply to help build quality into applications as early as possible. *Shift-left*
    testing means bringing testing analysis earlier into the development process,
    ideally around the point at which ideas are being discussed and details clarified.
    This process results in many questions being asked that help us identify issues
    earlier, deliver valuable features, and improve the quality of our products. Shift-left
    activities are an invaluable approach to improving quality and should be encouraged
    regardless of the use of AI in the development process. However, by introducing
    LLMs into the process, we can use prompts to generate questions that might reveal
    assumptions or misunderstandings or questions that we can simply disregard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at a sample prompt that can be used to rapidly generate questions
    that might be useful. We use delimiters to inform the LLM where the data to analyze
    is found:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a software testing expert. Analyze the text delimited by triple hashes
    and generate questions that will clarify ambiguities in the text. |'
  prefs: []
  type: TYPE_TB
- en: 'We then set a series of rules around how questions are asked and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    *   Questions will be generated by using the What, Where, Why, When, Who
    and How heuristic   *   Multiple questions can be identified for each quality
    characteristic   *   The quality characteristics we care about are: Accuracy,
    Security and Accessibility   *   Output questions in the format of Question -
    Explanation |'
  prefs: []
  type: TYPE_TB
- en: 'Few-shot prompting allows us to create an example of how we expect the output
    to look:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Example:What does relevant mean?The acceptance criteria says relevant search
    results, but how do we determine what is relevant and what isn’t? |'
  prefs: []
  type: TYPE_TB
- en: 'We then enter the data for analysis into the delimited section of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| ###As a B&B ownerI want to be able to manage user accessSo that I can allow
    different levels of access to multiple usersFeatures:   −   DB is configured with
    an Admin user   −   Guests are asked to create an account to make a booking  
    −   Admin can see the accounts created on the platform and can delete them###
    |'
  prefs: []
  type: TYPE_TB
- en: The prompt outlines instructions for an LLM to analyze a user story and acceptance
    criteria and return a list of questions for us to consider. Notice how we also
    provide context in the quality characteristics instruction. If we want to focus
    on different characteristics, we can update these as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Entering this prompt into ChatGPT returned the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Accuracy:   1.  What criteria are used to determine the accuracy of user
    access management?Explanation: The text mentions managing user access, but it
    doesn’t specify how accuracy is ensured in granting different levels of access
    to multiple users.   2.  How is the system ensuring that the right access levels
    are assigned to users?Explanation: The text refers to allowing different levels
    of access to multiple users, but it doesn’t explain the mechanism for guaranteeing
    the accuracy of access permissions.Security:   1.  Who has access to the database
    configured with an Admin user?Explanation: The text mentions configuring the database
    with an Admin user, but it doesn’t specify who has access to this Admin account
    and how its security is maintained.   2.  How is the platform protecting user
    account information and data?Explanation: The text talks about guests creating
    accounts for bookings, but it lacks details about the security measures implemented
    to safeguard user data.   3.  What security measures are in place to prevent unauthorized
    access to user accounts?Explanation: The text mentions that Admin can see and
    delete accounts, but it doesn’t explain how unauthorized access to such account
    management functionalities is prevented.Accessibility:   1.  What levels of accessibility
    are provided to different types of users?Explanation: The text mentions managing
    user access with different levels, but it doesn’t clarify what those levels are
    and how they vary based on user types.   2.  How are guests guided to create accounts
    for booking, and is the process accessible to all users?Explanation: The text
    states that guests are asked to create accounts, but it doesn’t specify how this
    process is carried out and if it meets accessibility standards.   3.  What measures
    are taken to ensure the platform’s accessibility for users with disabilities?Explanation:
    The text doesn’t provide information on how accessibility is addressed for users
    with disabilities, and it’s essential to understand the measures taken to accommodate
    their needs.   4.  Where can we find information about the platform’s accessibility
    features?Explanation: The text mentions nothing about the platform’s accessibility
    features, so it would be helpful to know where this information is available.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Reading through the response, we can see that there is an interesting collection
    of questions ranging in quality. For example, consider this question:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| How are guests guided to create accounts for booking, and is the process
    accessible to all users?Explanation: The text states that guests are asked to
    create accounts, but it doesn’t specify how this process is carried out and if
    it meets accessibility standards. |'
  prefs: []
  type: TYPE_TB
- en: This is a helpful question that encourages us to consider in more detail the
    statement “Guests are asked to create an account to make a booking, ”found in
    our user story. We could consider this question and think about how we would design
    the booking process to make it accessible to all types of users.
  prefs: []
  type: TYPE_NORMAL
- en: However, other questions are perhaps not quite as good. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| What criteria are used to determine the accuracy of user access management?Explanation:
    The text mentions managing user access, but it doesn’t specify how accuracy is
    ensured in granting different levels of access to multiple users. |'
  prefs: []
  type: TYPE_TB
- en: What makes this question problematic is its reference to the quality characteristic
    *accuracy*. Typically, accuracy would be more focused on the accuracy of data
    processed or stored (think bank interest rate calculations). Instead, the phrase
    “accuracy of user access” feels like an odd way to describe the rules set around
    users and what they can access. Ultimately, it’s up to us to evaluate each question
    for suitability and use. Some questions can encourage us to build products that
    are more closely aligned with what a user wants and help us avoid errors, whereas
    other generated questions will either make little sense or cover topics already
    considered.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll come back to this prompt and how we might use it during the development
    of a feature, but first, let’s take a look at how we can repurpose this prompt
    to review our code.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Analyzing code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Just as we can prompt an LLM to analyze written-down ideas, we can also have
    it review code and help us identify risks. Using this prompt is akin to simulating
    the role of a developer or tester that you are pairing with by having them analyze
    your work as you develop to offer suggestions for consideration. Let’s check out
    a prompt that could be of use for this type of activity. We use delimiters to
    signal where the code will be, as well as check for assumptions if no code is
    given:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a software testing expert. Analyze the code delimited by triple hashes
    and identify risks that might impact the code. If no code is provided, respond
    with “No risks could be found.” |'
  prefs: []
  type: TYPE_TB
- en: 'We then provide instructions around quality characteristics and output structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    *   Risks must be related to the quality characteristics: Performance,
    security and interoperability   *   Multiple risks can be identified for each
    quality characteristic   *   Output identified risks in the format of Risk - Quality
    Characteristic - Explanation |'
  prefs: []
  type: TYPE_TB
- en: 'Few-shot prompting allows us to give an example of output structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Example:Overflow - Security - Adding in large integers as parameters might
    cause the method to trigger a buffer overflow.Code is provided within the instructed
    delimiters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'After I sent the prompt with the sample code and quality characteristics to
    ChatGPT, the following output was returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Based on the provided code snippet, let’s analyze the potential risks that
    might affect the quality characteristics of Performance, Security, and Interoperability:  
    •   Performance - Nested Validation:   −   Risk: The nested validation checks
    (`dateCheckValidator.isValid` and `bookingDB.checkForBookingConflict`) may lead
    to performance issues, especially when dealing with a large number of bookings.  
    −   Explanation: If the system has to process a substantial amount of bookings,
    the nested validation can result in increased execution time and resource consumption.  
    •   Performance - Message Posting:   −   Risk: The message posting (`messageRequests.postMessage`)
    might introduce performance bottlenecks, affecting the overall system responsiveness.  
    −   Explanation: If the message posting process involves network communication
    or I/O operations, it could slow down the execution of the `createBooking` method.  
    •   Security - SQL Injection:   −   Risk: The code is susceptible to SQL injection
    attacks as it directly uses the `bookingToCreate` object in the database queries.  
    −   Explanation: Malicious input in the `bookingToCreate` object could manipulate
    the SQL queries and compromise the integrity and security of the database.   •  
    Security - Inadequate Authorization:   −   Risk: The code does not perform explicit
    authorization checks before creating bookings, potentially allowing unauthorized
    users to create bookings.   −   Explanation: Lack of proper authorization checks
    can lead to unauthorized access and misuse of the system’s booking creation functionality.  
    •   Interoperability - Platform Dependencies:   −   Risk: The code might have
    dependencies on specific platforms or libraries, limiting its interoperability
    with other systems or environments.   −   Explanation: If the `MessageBuilder`
    or `messageRequests` rely on platform-specific features or external libraries,
    it could cause issues when running the code in different environments.   •   Interoperability
    - Error Codes:   −   Risk: The code returns HTTP status codes directly, which
    may not be compatible with other systems that expect different error code formats.  
    −   Explanation: If external systems expect error codes in a different format
    (e.g., JSON or XML), using HTTP status codes may hinder interoperability. |'
  prefs: []
  type: TYPE_TB
- en: 'Similar to the previous prompt, which generated questions around written requirements,
    the quality of these different risks varies. For example, the risk “Performance
    - Message Posting” feels quite abstract, as we can see in its explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| If the message posting process involves network communication or I/O operations,
    it could slow down the execution of the `createBooking` method. |'
  prefs: []
  type: TYPE_TB
- en: Suggesting risks around network communication or I/O operations feels vague
    because it might refer to a range of implementations, some of which might relate
    to our work and some that might not.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, other risks are more concrete and potentially useful—for example,
    “Security - Inadequate Authorization,” which highlights the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| The code does not perform explicit authorization checks before creating bookings,
    potentially allowing unauthorized users to create bookings. |'
  prefs: []
  type: TYPE_TB
- en: This risk feels more concrete because it refers to actions that are carried
    out in our method and the absence of potentially important checks within it. Of
    course, we may carry out authorization checks elsewhere, but the information it
    has given highlighted an explicit activity that we might need to discuss further
    to improve the security of our booking feature.
  prefs: []
  type: TYPE_NORMAL
- en: Generating more ideas
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve looked at singular prompts to send to LLMs, which gives us useful
    responses to review. But what if we want to solicit more questions and risks?
    We simply ask by submitting an additional prompt, such as “Generate more questions”
    or “Identify further risks.” Be wary, though, as this has diminishing returns.
    An LLM will try to fulfill our requests at the risk of increasing hallucinations.
    So, as options start to dry out, we may see more suggestions that are less connected
    to the ideas and code we wanted feedback on in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Recognizing that a simulation is better than nothing at all
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When testing is discussed, the focus is typically on the production and execution
    of test cases. But a highly trained and experienced tester delivers value by using
    their critical and lateral thinking skills and asks questions that help view solutions
    in new ways and reveal potential problems. The prompts we’ve looked at can offer
    a simulation of that process. However, it’s important to remember that LLMs don’t
    have these critical and lateral thinking skills and that the questions and risks
    generated come from the instruction of our prompts. Instead, these types of prompts
    can offer a lightweight way to simulate the experience of pairing with testers
    or other developers when the opportunity to pair is unavailable. The key is to
    develop an eye for generated questions to determine which are of use.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Building in quality with AI assistance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve looked at prompts as singular activities, but now, let’s turn
    our attention to how the prompts we’ve recently learned and other LLM assistant
    tools can be used in conjunction with TDD to help us build in quality.
  prefs: []
  type: TYPE_NORMAL
- en: Although TDD is not strictly a testing activity when compared to other testing
    activities, TDD that is carried out correctly helps guide developers to build
    quality into products. To recap, the process of TDD is to use unit-checking tools
    to first create failing checks and then just enough production code to make the
    check *pass* (and fix any other checks that may have failed). Once all our checks
    are passing, we can refactor our production code while ensuring all our checks
    are green. Once that’s complete, we start the loop again until our work is complete,
    as demonstrated in figure 4.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F01_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 The red/green/refactor TDD cycle
  prefs: []
  type: TYPE_NORMAL
- en: What’s all this about checks?
  prefs: []
  type: TYPE_NORMAL
- en: In our Automation in Testing training, Richard Bradshaw and I make a distinction
    between human-led and tool-led testing. We call the latter type *automated checking*
    because the tools can only assert explicit actions or data that we codify into
    our automation. This distinction helps us better appreciate that automated tools
    such as unit-checking frameworks are excellent at rapidly checking small, specific
    changes in a product but are unable to tell us more about a system beyond its
    assertions. Humans, however, are slower and less deterministic in testing, although
    we are much more efficient at identifying many events happening simultaneously.
    Hence, this is why tools check and humans test. One is not better than the other,
    and hopefully, this book will demonstrate that we can have the best success when
    combining both.
  prefs: []
  type: TYPE_NORMAL
- en: This approach enables us to design products that are highly testable while ensuring
    we deliver what the business or end user desires.
  prefs: []
  type: TYPE_NORMAL
- en: Although its benefits are manifold (improving code design, making it more testable,
    and most importantly, improving its quality), some developers find it hard to
    adopt the TDD approach, believing that it slows down development as we create
    unit checks for each specific section of production code added to our system.
    However, with tools such as Copilot, we can learn how to establish a balance in
    which the tools enable us to rapidly create unit checks and build in quality.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Copilot
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes that you have installed and configured the Copilot plugin
    within your IDE. If you haven’t already completed the setup process, you can find
    installation instructions in appendix B.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Creating our first TDD loop with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To demonstrate, let’s undergo the process of creating part of a feature for
    a sample timesheet manager:'
  prefs: []
  type: TYPE_NORMAL
- en: As a timesheet user, I want to be able to track time against specific projects
    so that I can keep a history of how much time I have spent working on individual
    projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Features:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time is tracked in half hour units.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To complete a timesheet, a project and amount of time must be entered.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Only I should be able to see my tracked time.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can review the test and production code created as part of this example
    in the following GitHub repository: [https://mng.bz/6YMG](https://mng.bz/6YMG).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Preparing the work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before starting to create my timesheet manager feature, I wanted to think about
    how I would go about building it. To get help with this process and encourage
    me to think about the solution from various angles, I took the first prompt we
    explored earlier in this chapter and entered my context into it. Specifically,
    I changed the quality characteristics to accuracy and consistency and added the
    user story into the delimited section at the bottom of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a software testing expert. Analyze the text delimited by triple hashes
    and generate questions that will clarify ambiguities in the text.   *   Questions
    will be generated by using the What, Where, Why, When, Who and How heuristic  
    *   Multiple questions can be identified for each quality characteristic   *  
    The quality characteristics we care about are: Accuracy and Consistency   *  
    Output questions in the format of Question - ExplanationExample:What does relevant
    mean?The acceptance criteria say relevant search results, but how do we determine
    what is relevant and what isn’t?###As a timesheet userI want to be able to track
    time against specific projectsSo that I can keep a history of how much time I
    have spent working on individual projectsFeatures:   −   Time is tracked in half
    hour units   −   To complete a timesheet a project and amount of time must be
    entered   −   Only I should be able to see my tracked time### |'
  prefs: []
  type: TYPE_TB
- en: 'Sending this prompt to ChatGPT returned the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Accuracy:   •   What is the required precision for tracking time in half-hour
    units? - The text mentions that time is tracked in half-hour units. It’s important
    to know the level of accuracy required for this tracking.   •   How will the system
    handle invalid time entries? - Since time must be entered to complete a timesheet,
    what measures are in place to ensure that only valid time entries are accepted?  
    •   What validation mechanisms are in place to prevent users from entering incorrect
    time data? - To maintain accuracy, it’s essential to know how the system will
    validate and handle potential data entry errors.   •   How will the system handle
    time zone differences for users in different locations? - This is important to
    ensure that the tracked time is consistent and accurate for users in various time
    zones.Consistency:   •   Where will the history of tracked time be stored and
    accessed? - To ensure consistency, it’s important to know where the data will
    be stored and how users can access it.   •   What are the rules for displaying
    and formatting tracked time information? - Consistency in displaying the tracked
    time across the system is crucial for a seamless user experience.   •   Who is
    responsible for verifying and maintaining the consistency of tracked time data?
    – It’s essential to assign responsibility for data integrity and consistency to
    avoid discrepancies.   •   When and how often will the timesheet data be synchronized
    or updated? - Ensuring consistency in real-time updates or scheduled syncs is
    vital to avoid discrepancies between users’ data.   •   How will the system handle
    potential conflicts if multiple users try to update the same project’s timesheet
    simultaneously? - Maintaining consistency in data updates when multiple users
    are involved is critical to prevent data loss or inaccuracies.   •   What security
    measures are in place to protect the data and ensure its integrity? - Consistency
    also involves safeguarding the data from unauthorized access or tampering. |'
  prefs: []
  type: TYPE_TB
- en: What are quality characteristics?
  prefs: []
  type: TYPE_NORMAL
- en: Quality means very different things to different people and is a multifaceted
    concept. Therefore, when attempting to understand what quality means to a stakeholder
    or end user, we break out ways of thinking about it into different characteristics.
    For example, quality characteristics might include look and feel, usability, compliance,
    and much more. Different projects will have different quality characteristics
    that we prioritize, and it’s up to us as teams to identify what quality characteristics
    are of priority to us and our users.
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, I read each of the questions sent back to me and noted the ones
    that stood out as raising points I hadn’t considered. Let’s go through some of
    them and explore my thinking behind each one:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| How will the system handle invalid time entries? / What validation mechanisms
    are in place to prevent users from entering incorrect time data? |'
  prefs: []
  type: TYPE_TB
- en: These two questions felt very similar, but they highlighted that I will, at
    some point, need to add date–time handling into the code to prevent problems around
    invalid entries.
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| How will the system handle time zone differences for users in different locations?
    |'
  prefs: []
  type: TYPE_TB
- en: It’s a good question, but not relevant to this work as I plan to just track
    the amount of time carried out and not when it was carried out (at least for now).
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| What are the rules for displaying and formatting tracked time information?
    |'
  prefs: []
  type: TYPE_TB
- en: Connected to the previous question, I opted for simply tracking the amount of
    time that had elapsed and not specifically when it had occurred, meaning that
    the displaying of information is relatively straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| How will the system handle potential conflicts if multiple users try to update
    the same project’s timesheet simultaneously? |'
  prefs: []
  type: TYPE_TB
- en: At this time, there is no multiple-user feature, although it might be introduced
    later, so I make a note to consider this for future iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Many more questions could be asked and addressed, but what this process demonstrates
    is that, by generating different questions, I am required to think about various
    aspects of this feature’s design. Some I can choose to rule out, or shelve for
    later conversation, and some help me to improve the quality of my feature by raising
    my awareness to validation and date–time formatting.
  prefs: []
  type: TYPE_NORMAL
- en: '4.4.2 Loop 1: Save a timesheet entry'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the questions from the previous prompt in my mind, I start my first loop
    by creating an empty maven project within my IDE before heading to the `pom.xml`
    to add my required dependencies. I require JUnit 5 to build my unit checks, so
    I entered the comment
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Copilot returns the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, the proposed version of `junit-jupiter-engine` is an
    older version, so I update to `5.9.2` as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: This required update is a good example of how data provenance can affect what
    is being suggested. While constantly being trained on code that is being pushed
    to GitHub, Copilot runs the risk of lagging behind the latest versions of libraries
    and APIs. So, we need to be vigilant when adding in dependencies via AI code assistants
    to ensure they are suggesting the version we want.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, what is demonstrated here is how the Copilot IDE plugin will take
    code (or in this case a code comment) I have written, add it to a prompt, and
    then send it to the Copilot LLM to process. The LLM then sends back to the IDE
    plugin a suggestion of what to enter next, and the plugin then presents me with
    the suggestion. I now have the option to either accept the suggestion by hitting
    Tab or continue to write my own code.
  prefs: []
  type: TYPE_NORMAL
- en: Much like other LLMs, Copilot is triggered by prompts. But unlike a chat-based
    LLM (such as ChatGPT), Copilot is tuned to process code-based rather than text-based
    prompts. This distinction is important because it highlights both the benefits
    of different types of fine-tuned LLMs for different activities, as well as the
    need to write prompts that work for different prompt types. This is why TDD and
    AI assistants are an interesting combination to work with. The unit checks we
    create not only frame the design of our work but also serve as prompts to inform
    how we want to implement features.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for our first check, I prompt Copilot to create my first unit
    check for me by adding the following comment into a new class titled `TimesheetTest`
    in `src/test/java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'This triggers Copilot to suggest the following unit check:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this suggested check, we see that it has the necessary details we
    require. It’s given the class I intend to build a sensible name of `Timesheet`
    and suggested the method `submitTimesheet` with the correct parameters of project
    name and hours. This check is enough of a prompt that when I create a new class
    `Timesheet` in `src/main/java`, Copilot suggests
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The unit check served as a prompt to Copilot to create my production code, and
    although what has been produced is not exactly exciting, the check has been fulfilled.
    We can also see that Copilot works by building prompts on prompts to get to what
    we want. First, I created a prompt in the form of a code comment to help Copilot
    suggest the code for the unit check, and then that unit check served as a prompt
    for the suggested production code. As the unit checks grow, so will the prompts
    and directions to Copilot on what to build. Now, let’s look at how I used this
    process to build out the `submitTimesheet` method.
  prefs: []
  type: TYPE_NORMAL
- en: '4.4.3 Loop 2: Retrieve a timesheet entry'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the first loop, I created the skeleton of my project, but at the moment,
    the actual production code bears little fruit. So, with the second loop, I looked
    to add more features to the `Timesheet` class to help flesh out the `submitTimesheet`
    method. Again, I start by adding a new comment prompt to `TimesheetTest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'And upon creating a new line, Copilot returned the following check as a suggestion:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: This time, the generated check is using the `submitTimesheet` method to create
    some data that I can then verify using the newly created `getTimesheets` method.
    So, heading over to the `Timesheet` class, I use Copilot to generate the necessary
    `getTimesheets` method, which returns
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it’s helpful that it created the method signature for me, the internals
    of the method are not so useful. However, I add the following to the top of the
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Then I return to the `submitTimesheet` method and remove `return true` from
    inside the method. When I enter the keyword `if` into the statement, Copilot returns
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: This is a useful demonstration of how a project requires a degree of warmup
    before the LLM can return more accurate suggestions when working with Copilot.
    By returning to the `submitTimesheet` method, removing its contents, and then
    waiting for Copilot to respond, I get a refreshed response from Copilot based
    on the new details I’ve provided. Just as a prompt for a chat LLM requires clear
    and explicit details, so does Copilot. The initial stock methods that were returned
    from Copilot for `Timesheet` are likely because there is little context to work
    with. Essentially, the prompts sent to Copilot lack information. But as I create
    new unit checks and additional code to show our intent in our code, the suggestions
    become more useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also demonstrated when I delete the `getTimesheets` method entirely.
    With the new `HashMap` added to the class, my updated code prompts Copilot to
    return an updated version of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Running both of my unit checks after this loop, I can see both checks are passing
    and now have working code that is a bit more feature rich than when I started
    the loop.
  prefs: []
  type: TYPE_NORMAL
- en: '4.4.4 Loop 3: Calculating times for a project'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the final loop, I went back to the questions returned from ChatGPT and
    thought about the question:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| How will the system handle potential conflicts if multiple users try to update
    the same project’s timesheet simultaneously? |'
  prefs: []
  type: TYPE_TB
- en: 'Although there is no functionality for multiple users at this point in the
    project, the question from ChatGPT did make me think about how to handle multiple
    timesheet submissions to the same project. So, I created my third unit check to
    handle this scenario by once again creating a code comment to prompt Copilot for
    another unit check:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'It returned the following unit check:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, the suggested check is close, but I want the `Timesheet` class to
    return a total, not have a total processed within the unit check itself. So, I
    opted to update the check to explicitly state there is a method for getting total
    timesheet hours:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Heading back over to `Timesheet` and waiting for Copilot, the LLM returns the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows the code to compile, but my new unit check fails because the project
    count isn’t being updated. Instead, because I am using a `HashMap`, the count
    is being overwritten. I need to update `submitTimesheet` to handle duplicate projects,
    which I do by dropping the following comment into `submitTimesheet`, just after
    the initial `if` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dropping in the code comment returns the following suggested code:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'This expands the `submitTimesheet` function to allow it to increase the duration
    of existing projects in a timesheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-plugin.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: A final run of my three-unit checks confirms that they are all passing and that
    I have all the features I want in my `Timesheet` class. However, the code is starting
    to become more complex, so I turn my attention to refactoring my code with the
    use of ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.5 Refactoring code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One benefit of pairing with others is the ability for a pair partner to see
    potential problems and risks that we might not see. However, in the absence of
    a pair partner, I chose to use the second of the prompts we explored earlier in
    this chapter—specifically, the prompt that analyzes code and returns suggestions.
    Taking the prompt and adding my code and the quality characteristics’ accuracy
    and consistency, I sent the following prompt to ChatGPT. I slightly updated the
    instructions to make it clear I am providing Java code:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a software testing expert. Analyze the Java code delimited by triple
    hashes and identify risks that might impact the code. If no code is provided,
    respond with “No risks could be found” |'
  prefs: []
  type: TYPE_TB
- en: 'I update the additional instructions with my quality characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    *   Risks must be related to the quality characteristics: Accuracy and
    Consistency   *   Multiple risks can be identified for each quality characteristic  
    *   Output identified risks in the format of Risk - Quality Characteristic - Explanation
    |'
  prefs: []
  type: TYPE_TB
- en: 'An example is provided to demonstrate the format of the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Example:Overflow - Security - Adding in large integers as parameters might
    cause the method to trigger a buffer overflow. |'
  prefs: []
  type: TYPE_TB
- en: 'The code I created from my earlier loops is added in the delimited section:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending the prompt returned the following risks that I might want to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Risk 1 - Accuracy - `submitTimesheet` method can overwrite existing duration:The
    `submitTimesheet` method allows adding a new duration to an existing project’s
    total if the project already exists in the timesheets. However, this approach
    may not always ensure accuracy. If the user intends to submit a correction or
    update the duration for a specific project, this method will not provide accurate
    results. It merely adds the new duration to the existing one without considering
    the possibility of editing or deleting a previous entry.Risk 2 - Consistency -
    Null key in timesheets HashMap:The timesheets HashMap allows null keys. This might
    lead to inconsistency issues when querying for timesheet information by project
    name. If a project with a null name is mistakenly added to the timesheets, it
    may cause issues in subsequent operations, leading to unexpected behavior.Risk
    3 - Consistency - Lack of validation for project names:The `submitTimesheet` method
    does not validate the project name before adding it to the timesheets HashMap.
    This lack of validation might result in inconsistent data, such as duplicate or
    improperly formatted project names. For example, if two different variations of
    the same project name (e.g., `"ProjectA"` and `"projecta"`) are added, the system
    might treat them as separate projects, leading to inconsistencies in reporting
    or querying.Risk 4 - Accuracy - `getTotalTimesheetHours` method may throw `NullPointer-Exception`:The
    `getTotalTimesheetHours` method directly calls `timesheets.get(projectName)`,
    which may throw a `NullPointerException` if the given `projectName` is not present
    in the timesheets HashMap. This can lead to inaccurate results if the method is
    used without checking for the existence of the project name in the timesheets
    before calling this method.Risk 5 - Accuracy - Return type of `getTotalTimesheetHours`
    method:The `getTotalTimesheetHours` method returns an int data type, which might
    lead to accuracy issues if the actual total timesheet hours for a project exceed
    the maximum value that an int can hold. This can result in data loss due to overflow
    if the sum of durations for a project exceeds `Integer.MAX_VALUE`.Risk 6 - Consistency
    - Lack of synchronization in a multi-threaded environment:If this `Timesheet`
    class is used in a multi-threaded environment, there is a risk of inconsistent
    behavior since the methods of this class are not synchronized. This can lead to
    data corruption or incorrect calculations if multiple threads concurrently access
    and modify the timesheets HashMap. |'
  prefs: []
  type: TYPE_TB
- en: 'Unlike the questions prompt, this response appears to call out specific items
    of my code and gives reasons for how things might go wrong. With this response,
    I have the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: Review each risk on a case-by-case basis and then mitigate the ones I feel are
    important and ignore the others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ignore the proposed risks entirely, or perhaps send another prompt to ChatGPT
    to see whether there are more risks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the response from ChatGPT as a prompt in and of itself to help me refactor
    my code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the purposes of this demonstration, I chose to go with the third option
    and send an additional prompt to ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Refactor the code to mitigate the risks suggested. Check to see that each
    risk has been mitigated before outputting the solution. |'
  prefs: []
  type: TYPE_TB
- en: 'Notice how I used the time-to-think principle to get the LLM to check whether
    each risk has actually been mitigated by the refactored code it suggested. This
    helps increase the likelihood that the code that’s returned will be useful. Sending
    this prompt to ChatGPT returned the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Changes HashMap to ConcurrentHashMap❷ Adds in additional code to lowercase
    project names❸ Converts the int data type to long |
  prefs: []
  type: TYPE_NORMAL
- en: To check whether these modifications hold up, I copied the suggested code into
    `Timesheet`, modified `testCalculateTotalHoursWorked` to take a `long` instead
    of an `int` in `TimesheetTest`, and discovered that my checks still pass and I
    refactored my code.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, has this improved my code? To confirm, I run the original code analysis
    prompt with my updated code again and receive new risks:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Risk - Accuracy - Case Sensitivity Issue:Explanation: The method `submitTimesheet`
    converts the `projectName` to lowercase before using it as a key in the timesheets
    map. This conversion might lead to inaccurate data retrieval if there are projects
    with the same name but different letter cases (e.g., `"ProjectA"` and `"projecta"`
    will be considered the same project).Risk - Consistency - Inconsistent Data Entry:Explanation:
    The `submitTimesheet` method does not enforce consistent data entry for `projectName`
    values. If different users enter project names in various formats (e.g., `"Project
    A"`, `"project-A"`, `"project_A"`), it could lead to multiple entries for what
    should be the same project in the timesheets map.Risk - Accuracy - Missing Validation
    for Duration:Explanation: Although the method `submitTimesheet` checks for a positive
    duration, it does not handle cases where the duration is 0 or negative. This could
    lead to incorrect data being recorded in the timesheets map.Risk - Consistency
    - Lack of Input Sanitization:Explanation: The `submitTimesheet` method does not
    sanitize the `projectName` input, which could lead to inconsistent data due to
    invalid characters or unexpected inputs.Risk - Accuracy - Potential Data Loss:Explanation:
    The method `submitTimesheet` returns a boolean value, but it does not provide
    any feedback or error handling mechanism if the submission fails. This lack of
    feedback could lead to data loss if the submission fails silently, and the user
    is not aware of the issue. |'
  prefs: []
  type: TYPE_TB
- en: What is interesting in this response is I can confirm that some risks have been
    mitigated such as
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Risk 5 - Accuracy - Return type of `getTotalTimesheetHours` method:The `getTotalTimesheetHours`
    method returns an int data type, which might lead to accuracy issues if the actual
    total timesheet hours for a project exceed the maximum value that an int can hold.
    This can result in data loss due to overflow if the sum of durations for a project
    exceeds `Integer.MAX_VALUE`. |'
  prefs: []
  type: TYPE_TB
- en: 'However, it appears that there are still some risks that haven’t mitigated.
    For example, in the first list of risks I received the following risk:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Risk 3 - Consistency - Lack of validation for project names:The `submitTimesheet`
    method does not validate the project name before adding it to the timesheets HashMap.
    This lack of validation might result in inconsistent data, such as duplicate or
    improperly formatted project names. For example, if two different variations of
    the same project name (e.g., `"ProjectA"` and `"projecta"`) are added, the system
    might treat them as separate projects, leading to inconsistencies in reporting
    or querying. |'
  prefs: []
  type: TYPE_TB
- en: 'It was handled by ChatGPT implementing a `lowerCase` method to help sanitize
    the project name. However, on the second analysis, I was returned the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Risk - Consistency - Inconsistent Data Entry:Explanation: The `submitTimesheet`
    method does not enforce consistent data entry for `projectName` values. If different
    users enter project names in various formats (e.g., `"Project A"`, `"project-A"`,
    `"project_A"`), it could lead to multiple entries for what should be the same
    project in the timesheets map. |'
  prefs: []
  type: TYPE_TB
- en: This risk is very similar to the original, allegedly mitigated, risk. It feels
    like this additional risk around inconsistent data entry should have been handled
    properly when my code was refactored. I could once again ask the LLM to refactor
    my code for me, but instead given the potential for going around in circles with
    the LLM, it would be sensible for me to take the lead and fix the problems myself.
    This is an important skill to develop—knowing when to rely on an LLM and when
    to take charge.
  prefs: []
  type: TYPE_NORMAL
- en: The reason this choice is important can be highlighted by one of the other suggested
    risks that came from the second round of analysis. Specifically,
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Risk - Accuracy - Missing Validation for Duration:Explanation: Although the
    method `submitTimesheet` checks for a positive duration, it does not handle cases
    where the duration is 0 or negative. This could lead to incorrect data being recorded
    in the timesheets map. |'
  prefs: []
  type: TYPE_TB
- en: 'This sounds like a convincing risk, but it’s a demonstration of a hallucination.
    As the code stands, if the duration is ≥0, the method simply returns false and
    bails out of the timesheet storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can be biased sometimes to prioritize giving an answer regardless of its
    quality, meaning the more times we ask an LLM to analyze our code, the more likely
    it will start to generate hallucinations to give the impression that it’s producing
    useful results rather than returning with a response that no useful information
    can be shared. This is why we must keep careful tabs on when it’s beneficial to
    use an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, I chose to stop the use case as what we’ve covered demonstrates
    the way in which different types of LLMs can help me in different ways. Copilot
    offers the ability to rapidly generate code, but it requires code-based prompts
    to help it with its suggestions. What this means is that if we’re working on a
    fresh project that has very little production code for our LLM to analyze, we’re
    more likely to get results from Copilot that aren’t useful to us. So, to improve
    the quality of the output from Copilot, we give it more context using unit checks.
    This helps not only guide Copilot in the building of our code, but also provides
    us with the benefits of TDD including well-designed, testable code.
  prefs: []
  type: TYPE_NORMAL
- en: With ChatGPT, we’ve demonstrated that it can be a useful tool for analysis when
    prompted correctly. Building prompts able to analyze ideas and code and suggest
    risks and improvements can rapidly offer us alternative perspectives to consider,
    which we can then either act upon or reject. Utilizing the LLM as a simulation
    of a role that advocates quality can help us improve our work.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Improving documentation and communication with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It might not seem so, but communicating the work we’ve done through code comments
    and release notes can contribute significantly to product quality. By sharing
    new developments and changes to code bases, we can help fellow developers understand
    how our work affects theirs, guide testers in what to focus on when testing our
    work, and even help how users view our products (for example, Slack’s early release
    notes helped market their tooling with their clear communication and humor).
  prefs: []
  type: TYPE_NORMAL
- en: Despite these benefits, documentation and release notes are sometimes left to
    languish at the end of a development cycle or are ignored entirely. This makes
    sense, given the time required to write and maintain code comments and release
    notes that are useful and relevant, especially when there is time pressure to
    be constantly delivering new features. However, with the use of LLMs, we can reduce
    that time overhead while ensuring we generate useful documentation that creates
    value for future readers. So, let’s take a look at some useful prompts that can
    rapidly generate documentation for us.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1 Generating code comments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although we should always strive to create code arranged in a way that is fluent
    and easy to parse, regardless of experience with a code base, code comments can
    provide the extra detail that prevents code misuse and speeds up development.
    This is even more important if we’re releasing APIs that will be used by others.
    (I’ve on many occasions wasted time trying to understand how a library works from
    poorly documented API docs.) The challenge is to get the right balance in code
    comments: too few, and the person reading your code is left to fend for themselves;
    too much, and we create more work for ourselves in maintaining code and comments
    to ensure they align.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, instead, let’s take a look at a prompt that can take care of the process
    of annotating our work with comments for us. I use the check-the-solution tactics
    to ensure the code is not altered:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a Java Developer. Update the code delimited by three hashes and add
    in code comments to explain how the code works. Check that no parts of the code
    have been changed before outputting the completed code commented code. |'
  prefs: []
  type: TYPE_TB
- en: 'Next, I use delimiter tactics to state where the code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this prompt, we use the usual delimiter tactics to provide our code that
    we would like to see commented, but we also want to ensure that the LLM doesn’t
    change our code to make it align with the comments it creates. So, we also make
    it explicit within the prompt to check that no parts of the code have been changed
    before outputting the completed commented code. Sending this prompt with our example
    code from earlier to ChatGPT returned the following commented code:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the methods and sections of code within them have now been annotated
    with comments to give us more detail. What stands out most are comments such as
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'The comment provides not only references to what `ConcurrentHashMap` is used
    for, but also highlights that all project names will be stored in lowercase, a
    reference to part of the code inside `submitTimesheet`, which does stand in contrast
    to comments like these that feel a little unnecessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we choose what to use. Although the prompt we’ve used has provided
    us with comments, we’ve observed that some are probably more useful than others.
    Thus, our role changes from being the author of comments to an editor, selecting
    and tweaking the comments that seem the most useful.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining code comments with prompts
  prefs: []
  type: TYPE_NORMAL
- en: What makes this process so useful is that when we inevitably come to a point
    in which our code changes and our comments need updating, we simply can run the
    prompt again with our updated code to regenerate our comments to incorporate our
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A final observation is that although the comments can be useful for someone
    reading our code directly, what if we want to provide more structured documentation
    in the form of, say, Javadoc? In that case, we can modify the prompt and use structured
    data tactics to create a prompt. We’ll use an updated prompt with structured data
    tactics for Javadoc:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a Java Developer. Update the code delimited by three hashes and add
    in code comments in a Javadoc format to explain how the code works. Check that
    no parts of the code have been changed before outputting the completed code commented
    code. |'
  prefs: []
  type: TYPE_TB
- en: 'We then provide the code using delimiter tactics:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Sending this prompt with our example code to ChatGPT produces
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Using this prompt, we’ve improved the quality of some of the comments and also
    created Javadoc-friendly comments that can be used to document our code for external
    users. Now we only have to tweak and edit at the points we see fit.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Generating release notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Just as we can use a LLM to modify our code to add comments, we can also use
    LLMs to transform our code base into release notes that can be consumed by others
    in the team and beyond. To do this, we create a new prompt with the following
    details; we check the solution tactics to ensure the release notes are legible:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a Java Developer. Convert the code delimited by three hashes into
    a bullet-pointed set of release notes that describes how the code works. Check
    that the release notes can be read by someone who has no experience with the code
    created before outputting it. |'
  prefs: []
  type: TYPE_TB
- en: 'We add an additional instruction to reduce the wordiness of the release notes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    *   Only output the release notes. |'
  prefs: []
  type: TYPE_TB
- en: 'Then we use delimiter tactics to state where the code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The prompt follows a similar pattern to the previous code comment-focused prompt,
    but this time, instead of asking it to inject comments into our code, we instruct
    that the code be transformed into natural language release notes. For example,
    sending the prompt with the sample code to ChatGPT returned
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Release Notes:   *   The code defines a Java class named `Timesheet` that
    is used to manage timesheet data.   *   It uses a `ConcurrentHashMap` to store
    timesheet entries, where project names (in lowercase) are associated with their
    respective durations (in hours).   *   The `submitTimesheet` method allows users
    to submit timesheet entries. It requires a project name and a positive duration
    as parameters. If these conditions are met, it stores the duration associated
    with the project name in the map. If not, it returns false.   *   The `getTimesheets`
    method returns a collection of all recorded timesheet durations.   *   The `getTotalTimesheetHours`
    method allows users to retrieve the total duration for a specific project name
    (case-insensitive). If the project name is not found, it returns 0.Note: Ensure
    that project names are provided in lowercase when using the `submitTimesheet`
    method to avoid case sensitivity issues when retrieving total timesheet hours.
    |'
  prefs: []
  type: TYPE_TB
- en: 'The result we get is in a completely different format compared to our original
    code that attempts to detail what our code base does. Reviewing the output, it
    does feel quite technical, which makes sense considering we’ve asked for detailed
    notes on a small amount of code. However, even these release notes can be of use
    because we can adapt an earlier prompt for suggesting risks to analyze the notes.
    We use delimiters to signal where the release notes will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a software testing expert. Analyze the release notes delimited by
    triple hashes and identify risks that might impact the code. If no code is provided,
    respond with “No risks could be found.” |'
  prefs: []
  type: TYPE_TB
- en: 'We then provide instructions around quality characteristics and output structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    *   Risks must be related to the quality characteristics: Performance,
    security and interoperability   *   Multiple risks can be identified for each
    quality characteristic   *   Output identified risks in the format of Risk - Quality
    Characteristic - Explanation |'
  prefs: []
  type: TYPE_TB
- en: Few-shot prompting allows us to give an example of output structure
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Example:Overflow - Security - Adding in large integers as parameters might
    cause the method to trigger a buffer overflow. |'
  prefs: []
  type: TYPE_TB
- en: 'Release notes are provided within the instructed delimiters:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| ###Release Notes:   *   The code defines a Java class named `Timesheet` that
    is used to manage timesheet data.   *   It uses a `ConcurrentHashMap` to store
    timesheet entries, where project names (in lowercase) are associated with their
    respective durations (in hours).   *   The `submitTimesheet` method allows users
    to submit timesheet entries. It requires a project name and a positive duration
    as parameters. If these conditions are met, it stores the duration associated
    with the project name in the map. If not, it returns false.   *   The `getTimesheets`
    method returns a collection of all recorded timesheet durations.   *   The `getTotalTimesheetHours`
    method allows users to retrieve the total duration for a specific project name
    (case-insensitive). If the project name is not found, it returns 0.Note: Ensure
    that project names are provided in lowercase when using the `submitTimesheet`
    method to avoid case sensitivity issues when retrieving total timesheet hours.###
    |'
  prefs: []
  type: TYPE_TB
- en: 'Using this prompt in ChatGPT returned the following suggested risks:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Concurrent Modification - Performance - The use of `ConcurrentHashMap` may
    not be suitable for high concurrent access, leading to performance bottlenecks
    if there are frequent modifications to the timesheet entries by multiple users
    simultaneously.Lack of Input Validation - Security - The `submitTimesheet` method
    lacks proper input validation. It only checks for positive durations but does
    not validate the project name, making it vulnerable to injection attacks or other
    malicious input.Case Sensitivity - Interoperability - The requirement to provide
    project names in lowercase may lead to interoperability issues when integrating
    with systems that expect case-insensitive input or have different naming conventions.No
    Authentication/Authorization - Security - The release notes do not mention any
    authentication or authorization mechanisms, which can pose security risks if unauthorized
    users can submit or access timesheet data.Data Exposure - Security - The `getTimesheets`
    method provides access to all recorded timesheet durations, which might expose
    sensitive information if not properly restricted or authenticated. |'
  prefs: []
  type: TYPE_TB
- en: Of course, this might be an unnecessary step if we can directly ask an LLM to
    suggest risks based on the code directly, but it does demonstrate the power of
    LLM’s ability to transform data from one format to another.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Maintaining a balance with code assistants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The perspective on the value that tools such as Copilot and ChatGPT offer depends
    on individuals. For some, it means the automation of development and the loss
    of many roles, and for others, it’s nothing but an advanced tool that randomly
    picks suggested code. What this chapter demonstrates is the ongoing theme that
    their worth and usefulness lie somewhere between those two extremes.
  prefs: []
  type: TYPE_NORMAL
- en: These models, trained on masses of data from Stack Overflow and GitHub repositories,
    are very sophisticated in what they suggest for both production and test code.
    But they still require direction from us humans—direction that is guided by our
    abilities to communicate with stakeholders, analyze requirements, and design implementation.
    How well we can use AI tools in development depends on honing our complementary
    skills, which can be summarized by using the area-of-effect model shown in figure
    4.2.
  prefs: []
  type: TYPE_NORMAL
- en: A balanced approach can help us deliver features faster but still ensure that
    we build in quality. So, our goal is to maintain that balance in situations in
    which we may need to rely on our own abilities or the features of our tools. Sometimes,
    code assistants won’t be able to suggest the correct implementation, and we need
    to take charge. This gives us more control but does sacrifice speed. At other
    times, we can rely on code assistant tools to reference vast amounts of data to
    suggest new design ideas through unit checks or conversations. However, we want
    to ensure that we keep our TDD loops focused on design and not on test coverage.
    Too many unit checks, and we lose sight of our design and end up in a box-checking
    activity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F02_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 An area-of-effect model, updated to demonstrate the skills of a human
    and the abilities of code assistant tools
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The majority of generative AI tools currently on the market rely on LLMs trained
    with vast amounts of data scraped from the internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are sophisticated algorithms that apply statistical analysis to our requests
    to determine what output they should respond with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copilot is a coding assistant tool that uses the OpenAI GPT-4 and is trained
    on code stored on GitHub.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copilot works within an IDE and reads your code as a prompt to suggest what
    to add next into test code and production code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools such as Copilot can work well with the TDD red/green/refactor loop to
    help us rapidly create unit checks and production code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To help Copilot return code that is valuable, we need to guide it with prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Success with AI code assistants depends on our understanding of our abilities
    and the features of code assistant tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is push–pull relationship between how much we are leading versus the tooling
    leading design.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must be aware of the tradeoffs when the balance shifts from humans to tools
    leading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
