<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">3</span> </span> <span class="chapter-title-text">Ranking and content-based relevance</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Executing queries and returning search results</li>
<li class="readable-text" id="p3">Ranking search results based on how relevant they are to an incoming query</li>
<li class="readable-text" id="p4">Keyword match and filtering versus vector-based ranking</li>
<li class="readable-text" id="p5">Controlling and specifying custom ranking functions with function queries</li>
<li class="readable-text" id="p6">Catering ranking functions to a specific domain</li>
</ul>
</div>
<div class="readable-text" id="p7">
<p>Search engines fundamentally do three things: ingest content (<em>indexing</em>), return content matching incoming queries (<em>matching</em>), and sort the returned content based on some measure of how well it matches the query (<em>ranking</em>). Additional layers can be added, allowing users to provide better queries (autosuggest, chatbot dialogs, etc.) and to extract better answers from the results or summarize the results by using large language models (see chapters 14‚Äì15), but the core functions of the search engine are matching and ranking on indexed data.</p>
</div>
<div class="readable-text intended-text" id="p8">
<p><em>Relevance</em> is the notion of how well the returned content matches the query. Normally, the content being matched is documents, and the returned and ranked content is the matched documents along with corresponding metadata. In most search engines, the default relevance sorting is based upon a score indicating how well each keyword in a query matches the same keyword in each matched document. Alternatively, queries can be mapped into numerical vector representations, with the score then representing how similar the query vector is to each matched document. The best matches yield the highest relevance score, and they are returned at the top of the search results. The relevance calculation is highly configurable and can be adjusted on a per-query basis, allowing sophisticated ranking behavior.</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>In this chapter, we‚Äôll provide an overview of how relevance is calculated, how the relevance function can be easily controlled and adjusted through function queries, and how we can implement popular domain-specific and user-specific relevance ranking features.</p>
</div>
<div class="readable-text" id="p10">
<h2 class="readable-text-h2" id="sigil_toc_id_37"><span class="num-string">3.1</span> Scoring query and document vectors with cosine similarity</h2>
</div>
<div class="readable-text" id="p11">
<p>In section 2.3, we demonstrated the idea of measuring the similarity of two vectors by calculating the cosine between them. We created vectors (lists of numbers, where each number represents the strength of some feature) that represented different food items, and we then calculated the cosine (the size of the angle between the vectors) to determine their similarity. We‚Äôll expand upon that technique in this section, discussing how text queries and documents can map into vectors for ranking purposes. We‚Äôll then explore some popular text-based feature-weighting techniques and how they can be integrated to create an improved relevance-ranking formula. </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p12">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Running the code examples</h5>
</div>
<div class="readable-text" id="p13">
<p>All the code listings in the book are available in Jupyter notebooks running in preconfigured Docker containers. This enables you to run interactive versions of the code with a single command (<code>docker compose up</code>) without needing to spend time on complicated system configuration and dependency management. The code examples can also run with multiple search engines and vector databases. See appendix A for instructions on how to configure and launch the Jupyter notebooks and follow along in your web browser. </p>
</div>
<div class="readable-text" id="p14">
<p>For brevity, the listings in this book may leave out certain lines of code, such as imports or ancillary code, but the notebooks contain all implementation details.</p>
</div>
</div>
<div class="readable-text" id="p15">
<p>In this section, we‚Äôll dive into our first code listings for the book. It will be helpful to start up the Docker containers needed to run the accompanying Jupyter notebooks so you can follow along with the interactive code examples. Instructions for doing this are provided in appendix A.</p>
</div>
<div class="readable-text" id="p16">
<h3 class="readable-text-h3" id="sigil_toc_id_38"><span class="num-string">3.1.1</span> Mapping text to vectors</h3>
</div>
<div class="readable-text" id="p17">
<p>In a typical search application, we start with a collection of documents, and we then try to rank documents based on how well they match some user‚Äôs query. In this section, we‚Äôll walk through the process of mapping the text of queries and documents into vectors. </p>
</div>
<div class="readable-text intended-text" id="p18">
<p>In the last chapter, we used the example of a search for food and beverage items, like <code>apple juice</code>, so let‚Äôs reuse that example here. Let‚Äôs assume we have two different documents we would like to sort based on how well they match this query.</p>
</div>
<div class="readable-text intended-text" id="p19">
<p><em>Query: </em><code>apple</code> <code>juice</code></p>
</div>
<div class="browsable-container listing-container" id="p20">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Document 1:</em>
  Lynn: ham and cheese sandwich, chocolate cookie, ice water
  Brian: turkey avocado sandwich, plain potato chips, apple juice
  Mohammed: grilled chicken salad, fruit cup, lemonade
<em>Document 2:</em>
  Orchard Farms apple juice is premium, organic apple juice made from the
  freshest apples, never from concentrate. Its juice has received the
  regional award for best apple juice three years in a row.</pre>
</div>
</div>
<div class="readable-text" id="p21">
<p>If we mapped both documents (containing a combined 48 words) to vectors, they would map to a 48-word vector space with the following dimensions:</p>
</div>
<div class="browsable-container listing-container" id="p22">
<div class="code-area-container">
<pre class="code-area">[a, and, apple, apples, avocado, award, best, brian, cheese, chicken, chips,
 chocolate, concentrate, cookie, cup, farms, for, freshest, from, fruit,
 grilled, ham, has, ice, in, is, its, juice, lemonade, lynn, made,
 mohammed, never, orchard, organic, plain, potato, premium, received,
 regional, row, salad, sandwich, the, three, turkey, water, years]</pre>
</div>
</div>
<div class="readable-text" id="p23">
<p>If you recall in section 2.3, we proposed thinking of a query for the phrase ‚Äúapple juice‚Äù as a vector containing a feature for every word in any of our documents, with a value of <code>1</code> for the terms ‚Äúapple‚Äù and ‚Äújuice‚Äù, and a value of <code>0</code> for all other terms.</p>
</div>
<div class="readable-text intended-text" id="p24">
<p>Since the term ‚Äúapple‚Äù is in the 3rd position, and ‚Äújuice‚Äù is in the 28th position of our 48-word vector space, a query vector for the phrase ‚Äúapple juice‚Äù would look as shown in figure 3.1.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p25">
<img alt="figure" height="77" src="../Images/CH03_F01_Grainger.png" width="792"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.1</span> Query vector. The query for <code>apple</code> <code>juice</code> is mapped to a vector containing one dimension for every known term, with a value of <code>1</code> for the terms ‚Äúapple‚Äù and ‚Äújuice‚Äù and a value of <code>0</code> for all other terms.</h5>
</div>
<div class="readable-text" id="p26">
<p>Even though the query vector only contains a nonzero value for two dimensions (representing the positions of ‚Äúapple‚Äù and ‚Äújuice‚Äù), it still contains values of <code>0</code> for all other possible dimensions. Representing a vector like this, including every possible value, is known as a <em>dense vector representation</em>. </p>
</div>
<div class="readable-text intended-text" id="p27">
<p>Each of the documents also maps to the same vector space based upon each of the terms it contains:</p>
</div>
<div class="browsable-container listing-container" id="p28">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Document 1:</em>
 [0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1
  0 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0]

<em>Document 2:</em>
 [1 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0
  1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 1]</pre>
</div>
</div>
<div class="readable-text" id="p29">
<p>With these dense vector representations of our query and documents, we can now use linear algebra to measure the similarity between our query vector and each of the document vectors. </p>
</div>
<div class="readable-text" id="p30">
<h3 class="readable-text-h3" id="sigil_toc_id_39"><span class="num-string">3.1.2</span> Calculating similarity between dense vector representations</h3>
</div>
<div class="readable-text" id="p31">
<p>To rank our documents, we just need to follow the same process we used in chapter 2 to calculate the cosine between each document and the query. This cosine value will then become the relevance score by which we‚Äôll be able to sort each document. </p>
</div>
<div class="readable-text intended-text" id="p32">
<p>The following listing shows how we would represent the query and document vectors in code, and how we would calculate the cosine similarity between the query and each document.</p>
</div>
<div class="browsable-container listing-container" id="p33">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.1</span> Calculating cosine similarity between vectors</h5>
<div class="code-area-container">
<pre class="code-area">query_vector = numpy.array(
  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
   0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

doc1_vector = numpy.array(
  [0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
   0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0])

doc2_vector = numpy.array(
  [1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
   1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1])

def cosine_similarity(vector1, vector2):
  return dot(vector1, vector2) / (norm(vector1) * norm(vector2))

doc1_score = cosine_similarity(query_vector, doc1_vector)
doc2_score = cosine_similarity(query_vector, doc2_vector)

print_scores([doc1_score, doc2_score])</pre>
</div>
</div>
<div class="readable-text" id="p34">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p35">
<div class="code-area-container">
<pre class="code-area">Relevance Scores:
  doc1: 0.2828
  doc2: 0.2828</pre>
</div>
</div>
<div class="readable-text" id="p36">
<p>Interesting . . . both documents received the same relevance score, even though the documents contain lengthy vectors with very different content. It might not be immediately obvious what‚Äôs going on, so let‚Äôs simplify the calculation by focusing only on the features that matter. </p>
</div>
<div class="readable-text" id="p37">
<h3 class="readable-text-h3" id="sigil_toc_id_40"><span class="num-string">3.1.3</span> Calculating similarity between sparse vector representations</h3>
</div>
<div class="readable-text" id="p38">
<p>The key to understanding the calculation in the last section is realizing that the only relevant features are the ones shared between the query and the document. All other features (words appearing in documents that don‚Äôt match the query) have zero effect on whether one document is ranked higher than another. As a result, we can remove all the other insignificant terms from our vector to simplify the example, converting from a dense vector representation to what is known as a <em>sparse vector representation</em>, as shown in figure 3.2. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p39">
<img alt="figure" height="94" src="../Images/CH03_F02_Grainger.png" width="223"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.2</span> Sparse vector representations only contain the ‚Äúpresent‚Äù features, unlike dense vector representations, which also contain the 0-valued entries for every feature.</h5>
</div>
<div class="readable-text" id="p40">
<p>In most search engine scoring operations, we tend to deal with sparse vector representations because they are more efficient to work with when scoring based on the small number of features.</p>
</div>
<div class="readable-text intended-text" id="p41">
<p>In addition, we can further simplify our calculations by creating vectors that only include ‚Äúmeaningful entries‚Äù‚Äîthe terms that are present in the query‚Äîas shown in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p42">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.2</span> Cosine similarity of sparse vector representations</h5>
<div class="code-area-container">
<pre class="code-area">query_vector = [1, 1] #[apple, juice]
doc1_vector  = [1, 1]
doc2_vector  = [1, 1]

doc1_score = cosine_similarity(query_vector, doc1_vector)
doc2_score = cosine_similarity(query_vector, doc2_vector)

print_scores([doc1_score, doc2_score])</pre>
</div>
</div>
<div class="readable-text" id="p43">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p44">
<div class="code-area-container">
<pre class="code-area">Relevance Scores:
  doc1: 1.0
  doc2: 1.0</pre>
</div>
</div>
<div class="readable-text" id="p45">
<p>Notice that doc1 and doc2 still yield equal relevance scores, but now the score for each is <code>1.0</code>. If you remember, a <code>1.0</code> score from a cosine calculation means the vectors are perfect matches, which is sensible considering that both vectors are identical (<code>[1,</code> <code>1]</code>).</p>
</div>
<div class="readable-text intended-text" id="p46">
<p>In fact, you‚Äôll notice several very interesting things:</p>
</div>
<ul>
<li class="readable-text" id="p47"> This simplified sparse vector representation calculation still shows both doc1 and doc2 returning equivalent relevance scores, since they both match all the words in the query. </li>
<li class="readable-text" id="p48"> Even though the absolute score between the dense vector representation similarity (<code>0.2828</code>) and the sparse vector representation similarity (<code>1.0</code>) are different, the scores are still the same relative to each other within each vector type. </li>
<li class="readable-text" id="p49"> The feature weights for the two query terms (‚Äúapple‚Äù, ‚Äújuice‚Äù) are the same between the query and each of the documents, resulting in a cosine score of <code>1.0</code>. </li>
</ul>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p50">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Vectors vs. vector representations</h5>
</div>
<div class="readable-text" id="p51">
<p>We‚Äôve been careful to refer to ‚Äúdense vector representations‚Äù and ‚Äúsparse vector representations‚Äù instead of ‚Äúdense vectors‚Äù and ‚Äúsparse vectors‚Äù. This is because there is a conceptual distinction between the idea of a vector and its representation, and this distinction often causes confusion.</p>
</div>
<div class="readable-text" id="p52">
<p>The sparsity of a <em>vector</em> refers to the proportion of the vector‚Äôs features that have meaningful values. Specifically, a dense vector is any vector whose features have mostly nonzero values, whereas a sparse vector is any vector whose features have mostly zeros, regardless of how they are stored or represented. Vector <em>representations</em>, on the other hand, deal with the data structures used to work with the vectors. For sparse vectors, it can be wasteful to allocate memory and storage space for all the zeros, so we will often use a sparse data structure (such as an inverted index) to only store the nonzero values. Here is an example:</p>
</div>
<div class="readable-text" id="p53">
<p><em>dense vector</em>: </p>
</div>
<div class="readable-text" id="p54">
<p>feature_1: <strong>1.1</strong>, feature_2: <strong>2.3</strong>, feature_3: <strong>7.1</strong>, feature_4: <strong>5.2</strong>, feature_5: <strong>8.1</strong></p>
</div>
<div class="readable-text" id="p55">
<p><em>dense vector representation</em>: <code>[</code><strong>1.1</strong><code>,</code><strong>2.3</strong><code>,</code><strong>7.1</strong><code>,</code><strong>5.2</strong><code>,</code><strong>8.1</strong><code>]</code></p>
</div>
<div class="readable-text" id="p56">
<p><em>sparse vector representation</em>: N/A (the vector is not sparse, so it can‚Äôt be represented sparsely)</p>
</div>
<div class="readable-text" id="p57">
<p><em>sparse vector</em>: feature_1: <strong>1.1</strong>, feature_2: <strong>0</strong>, feature_3: <strong>0</strong>, feature_4: <strong>5.2</strong>, feature_5: <strong>0</strong></p>
</div>
<div class="readable-text" id="p58">
<p><em>dense vector representation</em>: <code>[</code><strong>1.1</strong><code>, 0.0, 0.0,</code><strong>5.2</strong><code>, 0.0 ]</code></p>
</div>
<div class="readable-text" id="p59">
<p><em>sparse vector representation</em>: <code>{ 1:</code><strong>1.1</strong><code>, 4:</code><strong>5.2</strong><code>}</code>, or just <code>[</code><strong>1.1</strong><code>,</code><strong>5.2</strong><code>]</code> if feature positions aren‚Äôt needed</p>
</div>
<div class="readable-text" id="p60">
<p>Because a sparse vector contains predominantly zeros, and its corresponding sparse vector representation contains nearly the opposite (only the nonzero values), it is unfortunately common for people to confuse these concepts and incorrectly refer to dense vector representations (of sparse vectors) as ‚Äúdense vectors‚Äù, or even refer to any vector with many dimensions as a ‚Äúdense vector‚Äù and with few dimensions as a ‚Äúsparse vector‚Äù. You may find this confusion in other literature, so it‚Äôs important to be aware of the distinction.</p>
</div>
<div class="readable-text" id="p61">
<p>Since our query and document vectors are all sparse vectors (most values are zero, since the number of features is the number of keywords in the search index), it makes sense to use a sparse vector representation when doing keyword search.</p>
</div>
</div>
<div class="readable-text" id="p62">
<p>Search engines adjust for these problems by not just considering each feature in the vector as a <code>1</code> (exists) or a <code>0</code> (does not exist), but instead by providing a score for each feature based upon how <em>well</em> the feature matches. </p>
</div>
<div class="readable-text" id="p63">
<h3 class="readable-text-h3" id="sigil_toc_id_41"><span class="num-string">3.1.4</span> Term frequency: Measuring how well documents match a term</h3>
</div>
<div class="readable-text" id="p64">
<p>The problem we encountered in the last section is that the features in our term vectors only signify <em>if</em> the word ‚Äúapple‚Äù or ‚Äújuice‚Äù exists in a document, not how well each document represents either of the terms. A side effect of representing each term from the query with a value of <code>1</code> if it exists is that both doc1 and doc2 will always have the same cosine similarity score for the query, even though, qualitatively, doc2 is a much better match, since it talks about apple juice much more. </p>
</div>
<div class="readable-text intended-text" id="p65">
<p>Instead of using a value of <code>1</code> for each existing term, we can emulate ‚Äúhow well‚Äù a document matches by using the <em>term frequency</em> (TF), which is a measure of the number of times a term occurs within each document. The idea here is that the more frequently a term occurs within a specific document, the greater the likelihood that the document is more related to the query. </p>
</div>
<div class="readable-text intended-text" id="p66">
<p>The following listing shows vectors with a count of the number of times each term occurs within the document or query as the feature weights.</p>
</div>
<div class="browsable-container listing-container" id="p67">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.3</span> Cosine similarity of raw TF vectors</h5>
<div class="code-area-container">
<pre class="code-area">query_vector   = [1, 1] #[apple:1, juice:1]
doc1_tf_vector = [1, 1] #[apple:1, juice:1]
doc2_tf_vector = [3, 4] #[apple:3, juice:4]

doc1_score = cosine_similarity(query_vector, doc1_tf_vector)
doc2_score = cosine_similarity(query_vector, doc2_tf_vector)

print_scores([doc1_score, doc2_score])</pre>
</div>
</div>
<div class="readable-text" id="p68">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p69">
<div class="code-area-container">
<pre class="code-area">Relevance Scores:
  doc1: 1.0
  doc2: 0.9899</pre>
</div>
</div>
<div class="readable-text" id="p70">
<p>Contrary to what you might expect, doc1 is considered a better cosine similarity match than doc2. This is because the terms ‚Äúapple‚Äù and ‚Äújuice‚Äù both occur ‚Äúthe same proportion of times‚Äù (one occurrence of each term for every occurrence of the other term) in both the query and in doc1, making them the most textually similar. In other words, even though doc2 is intuitively more about the query, since it contains the terms in the query significantly more, cosine similarity returns doc1, since it‚Äôs an exact match for the query, unlike doc2.</p>
</div>
<div class="readable-text intended-text" id="p71">
<p>Since our goal is for documents like doc2 with higher TF to score higher, we can accomplish this by switching from cosine similarity to another scoring function, such as <em>dot product</em> or <em>Euclidean distance</em>, that increases as feature weights continue to increase. Let‚Äôs use the dot product (<code>a</code> <code>.</code> <code>b</code>), which is equal to the cosine similarity multiplied by the length of the query vector times the length of the document vector: <code>a</code> <code>.</code> <code>b</code> <code>=</code> <code>|a|</code> <code>√ó</code> <code>|b|</code> <code>√ó</code> <code>cos(Œ∏)</code>. The dot product will result in documents that contain more matching terms scoring higher, as opposed to cosine similarity, which scores documents higher when they contain a more similar proportion of matching terms between the query and documents. </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p72">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Phrase matching and other relevance tricks</h5>
</div>
<div class="readable-text" id="p73">
<p>By now, you may be wondering why we keep treating ‚Äúapple‚Äù and ‚Äújuice‚Äù as independent terms and why we don‚Äôt just treat ‚Äúapple juice‚Äù as a phrase to boost documents higher that match the exact phrase. Phrase matching is one of many easy relevance-tuning tricks we‚Äôll discuss later in the chapter. For now, we‚Äôll keep our query-processing simple and just deal with individual keywords to stay focused on our main goal‚Äîexplaining vector-based relevance scoring and text-based keyword-scoring features.</p>
</div>
</div>
<div class="readable-text" id="p74">
<p>In the next listing, we replace cosine similarity with a dot product calculation to consider the magnitude of the document vectors (which increases with more matches for each query term) in the relevance calculation.</p>
</div>
<div class="browsable-container listing-container" id="p75">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.4</span> Dot product of TF vectors</h5>
<div class="code-area-container">
<pre class="code-area">query_vector   = [1, 1] #[apple:1, juice:1]
doc1_tf_vector = [1, 1] #[apple:1, juice:1]
doc2_tf_vector = [3, 4] #[apple:3, juice:4]

doc1_score = dot(query_vector, doc1_tf_vector)
doc2_score = dot(query_vector, doc2_tf_vector)

print_scores([doc1_score, doc2_score])</pre>
</div>
</div>
<div class="readable-text" id="p76">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p77">
<div class="code-area-container">
<pre class="code-area">Relevance Scores:
  doc1: 2
  doc2: 7</pre>
</div>
</div>
<div class="readable-text" id="p78">
<p>As you can see, doc2 now yields a higher relevance score for the query than doc1, an improvement that aligns better with our intuition. Note that the relevance scores are no longer bounded between <code>0</code> and <code>1</code>, as they were with cosine similarity. This is because the dot product considers the magnitude of the document vectors, which can increase unbounded with additional matching keyword occurrences.</p>
</div>
<div class="readable-text intended-text" id="p79">
<p>While using the TF as the feature weight in our vectors certainly helps, textual queries exhibit additional challenges that need to be considered. Thus far, our documents have all contained every term from our queries, which does not match with most real-world scenarios. The following example will better demonstrate some of the limitations still present when using only term-frequency-based weighting for our text-based sparse vector similarity scoring. Let‚Äôs start with the following three text documents:</p>
</div>
<div class="browsable-container listing-container" id="p80">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Document 1:</em>
  In light of the big reveal in her interview, the interesting
  thing is that the person in the wrong probably made a good
  decision in the end.

<em>Document 2:</em>
  My favorite book is the cat in the hat, which is about a crazy
  cat in a hat who breaks into a house and creates the craziest
  afternoon for two kids.

<em>Document 3:</em>
  My careless neighbors apparently let a stray cat stay in their
  garage unsupervised which resulted in my favorite hat that I
  let them borrow being ruined.</pre>
</div>
</div>
<div class="readable-text" id="p81">
<p>Let‚Äôs now map these documents into their corresponding (sparse) vector representations and calculate a similarity score. The following listing ranks text similarity based on raw TFs (term counts).</p>
</div>
<div class="browsable-container listing-container" id="p82">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.5</span> Ranking text similarity based on term counts</h5>
<div class="code-area-container">
<pre class="code-area">def term_count(content, term):
  tokenized_content = tokenize(content)
  term_count = tokenized_content.count(term.lower())
  return float(round(term_count, 4))

query = "the cat in the hat"
terms = tokenize(query)

query_vector = list(numpy.repeat(1, len(terms)))
doc_vectors = [[term_count(doc, term) for term in terms] for doc in docs]
doc_scores = [dot(v, query_vector) for v in doc_vectors]

print_term_count_scores(terms, doc_vectors, doc_scores)</pre>
</div>
</div>
<div class="readable-text" id="p83">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p84">
<div class="code-area-container">
<pre class="code-area">labels:  ['the', 'cat', 'in', 'the', 'hat']

query vector: [1, 1, 1, 1, 1]

Document Vectors:
  doc1: [5.0, 0.0, 4.0, 5.0, 0.0]
  doc2: [3.0, 2.0, 2.0, 3.0, 2.0]
  doc3: [0.0, 1.0, 2.0, 0.0, 1.0]

Relevance Scores:
  doc1: 14.0
  doc2: 12.0
  doc3: 4.0</pre>
</div>
</div>
<div class="readable-text" id="p85">
<p>While we receive different relevance scores now for each document, based on the number of times each term matches, the ordering of the results doesn‚Äôt necessarily match our expectations about which documents are the best matches. Intuitively, we would instead expect the following ordering:</p>
</div>
<ol>
<li class="readable-text" id="p86"> <em>doc2:</em> because it is about the book <em>The Cat in the Hat</em> </li>
<li class="readable-text" id="p87"> <em>doc3:</em> because it matches all the words ‚Äúthe‚Äù, ‚Äúcat‚Äù, ‚Äúin‚Äù, and ‚Äúhat‚Äù </li>
<li class="readable-text" id="p88"> <em>doc1:</em> because it only matches the words ‚Äúthe‚Äù and ‚Äúin‚Äù, even though it contains them many times </li>
</ol>
<div class="readable-text" id="p89">
<p>The problem here is that since a term is considered just as important every time it appears, the relevance score is indiscriminately increasing with every additional occurrence of that term. In this case, doc1 is getting the highest score, because it contains 14 total term matches (the first ‚Äúthe‚Äù five times, ‚Äúin‚Äù four times, and the second ‚Äúthe‚Äù five times), yielding more total term matches than any other document.</p>
</div>
<div class="readable-text intended-text" id="p90">
<p>It doesn‚Äôt really make sense that a document containing these words 14 times should be considered 14 times as relevant as a document with a single match, though. Instead, a document should be considered more relevant if it matches many different terms from a query versus the same terms over and over. Often, real-world TF calculations dampen the effect of each additional occurrence of a word by calculating TF as a log or square root of the number of occurrences of each term (as we do in figure 3.3). Additionally, TF is often also normalized relative to document length by dividing the TF by the total number of terms in each document. Since longer documents are naturally more likely to contain any given term more often, this helps normalize the score to account for those document length variabilities (per the denominator for figure 3.3). Our final, normalized TF calculation can be seen in figure 3.3.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p91">
<img alt="figure" height="75" src="../Images/grainger-ch3-eqs-0x.png" width="263"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.3</span> Normalized TF calculation. <em>t</em> represents a term and <code>d</code> represents a document. <em>TF</em> equals the square root of the number of times the term appears in the current document (<em>f</em><em><sub>t,d</sub></em>) divided by the number of terms in the document (<code>‚àë</code><em><sub>t'</sub></em><code><sub>‚àà</sub></code><em>d f</em><em><sub>t',d</sub></em>). The square root dampens the additional relevance contribution of each additional occurrence of a term, while the denominator normalizes that dampened frequency to the document length so that longer documents with more terms are comparable to shorter documents with fewer terms.</h5>
</div>
<div class="readable-text" id="p92">
<p>Many variations of the TF calculation exist, only some of which perform document-length normalization (the denominator) or dampen the effect of additional term occurrences (the square root here, or sometimes using a logarithm). Apache Lucene (the search library powering Solr, OpenSearch, and Elasticsearch), for instance, calculates TF as only the square root of the numerator, but then multiplies it by a separate document-length norm (equivalent to the square root of the denominator in our equation) when performing certain ranking calculations.</p>
</div>
<div class="readable-text intended-text" id="p93">
<p>Going forward, we‚Äôll use this normalized TF calculation to ensure additional occurrences of the same term continue to improve relevance, but at a diminishing rate. The following listing shows the new TF function in effect.</p>
</div>
<div class="browsable-container listing-container" id="p94">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.6</span> Ranking text similarity based on TF</h5>
<div class="code-area-container">
<pre class="code-area">def tf(term, doc):
  tokenized_doc = tokenize(doc)
  term_count = tokenized_doc.count(term.lower())
  doc_length = len(tokenized_doc)
  return numpy.sqrt(term_count / doc_length)

query = "the cat in the hat"
terms = tokenize(query)

query_vector = list(numpy.repeat(1, len(terms)))
doc_vectors = [[tf(term, doc) for term in terms] for doc in docs]
doc_scores = [dot(dv, query_vector) for dv in doc_vectors]

print_term_frequency_scores(terms, doc_vectors, doc_scores)</pre>
</div>
</div>
<div class="readable-text" id="p95">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p96">
<div class="code-area-container">
<pre class="code-area">Document TF Vector Calculations:
  doc1: [tf(doc1, "the"), tf(doc1, "cat"), tf(doc1, "in"),
         tf(doc1, "the"), tf(doc1, "hat")]
  doc2: [tf(doc2, "the"), tf(doc2, "cat"), tf(doc2, "in"),
         tf(doc2, "the"), tf(doc2, "hat")]
  doc3: [tf(doc3, "the"), tf(doc3, "cat"), tf(doc3, "in"),
         tf(doc3, "the"), tf(doc3, "hat")]

Document TF Vector Values:
Labels: ['the', 'cat', 'in', 'the', 'hat']
  doc1: [0.4303, 0.0, 0.3849, 0.4303, 0.0]
  doc2: [0.3111, 0.254, 0.254, 0.3111, 0.254]
  doc3: [0.0, 0.1961, 0.2774, 0.0, 0.1961]

Relevance Scores:
  doc1: 1.2456
  doc2: 1.3842
  doc3: 0.6696</pre>
</div>
</div>
<div class="readable-text" id="p97">
<p>The normalized <code>tf</code> function shows an improvement, as doc2 is now ranked the highest, as expected. This is mostly because of the dampening effect on the number of term occurrences in doc1 (which matched ‚Äúthe‚Äù and ‚Äúin‚Äù so many times), such that each additional occurrence contributes less to the feature weight than prior occurrences. Unfortunately, doc1 is still ranked second-highest, so even the improved TF calculation wasn‚Äôt enough to get the better matching doc3 higher. </p>
</div>
<div class="readable-text intended-text" id="p98">
<p>The next step for improvement will be to factor in the relative importance of terms, as ‚Äúcat‚Äù and ‚Äúhat‚Äù are intuitively more important than common words like ‚Äúthe‚Äù and ‚Äúin‚Äù. Let‚Äôs modify our scoring calculation to fix this oversight by introducing a new variable that incorporates the importance of each term. </p>
</div>
<div class="readable-text" id="p99">
<h3 class="readable-text-h3" id="sigil_toc_id_42"><span class="num-string">3.1.5</span> Inverse document frequency: Measuring the importance of a term in the query</h3>
</div>
<div class="readable-text" id="p100">
<p>While TF has proven useful at measuring how well a document matches each term in a query, it does little to differentiate between the importance of the terms in the query. In this section, we‚Äôll introduce a technique using the significance of specific keywords based on their frequency of occurrence across documents. </p>
</div>
<div class="readable-text intended-text" id="p101">
<p>The <em>document frequency</em> (DF) for a term is defined as the total number of documents in the search engine that contain the term, and it serves as a good measure of a term‚Äôs importance. The idea here is that more specific or rare words (like ‚Äúcat‚Äù and ‚Äúhat‚Äù) tend to be more important than common words (like ‚Äúthe‚Äù and ‚Äúin‚Äù). The function used to calculate document frequency is shown in figure 3.4. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p102">
<img alt="figure" height="74" src="../Images/grainger-ch3-eqs-1x.png" width="464"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.4</span> Document frequency calculation. <code>D</code> is the set of all documents, <code>t</code> is the input term, and <code>D<sub>i</sub></code> is the <em>i</em><sup>th</sup> document in <code>D</code>. The lower the document frequency for a term (<code>DF(t)</code>), the more specific and important the term is when seen in queries.</h5>
</div>
<div class="readable-text" id="p103">
<p>Since we would like more important words to score higher, we take the <em>inverse document frequency</em> (IDF), as defined in figure 3.5. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p104">
<img alt="figure" height="60" src="../Images/grainger-ch3-eqs-2x.png" width="315"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.5</span> Inverse document frequency. <code>|D|</code> is the total count of all documents, <code>t</code> is the term, and <code>DF(t)</code> is the document frequency. The lower the <code>IDF(t)</code>, the more insignificant the term, and the higher, the more the term in a query should count toward the relevance score.</h5>
</div>
<div class="readable-text" id="p105">
<p>Carrying forward our <code>the</code> <code>cat</code> <code>in</code> <code>the</code> <code>hat</code> query example from the last section, a vector of IDFs would thus look like the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p106">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.7</span> Calculating inverse document frequency</h5>
<div class="code-area-container">
<pre class="code-area">def idf(term): <span class="aframe-location"/> #1
  df_map = {"the": 9500, "cat": 100,  <span class="aframe-location"/> #2
            "in": 9000, "hat": 50}    #2
  total_docs = 10000
  return 1 + numpy.log((total_docs+1) / (df_map[term] + 1))

terms = ["the", "cat", "in", "the", "hat"]
idf_vector = [idf(term) for term in terms]<span class="aframe-location"/> #3

print_inverse_document_frequency_scores(terms, idf_vector)</pre>
<div class="code-annotations-overlay-container">
     #1 The IDF function, which dictates the importance of a term in the query
     <br/>#2 Mocked document counts simulating realistic statistics from an inverted index
     <br/>#3 IDF is term-dependent, not document-dependent, so it is the same for both queries and documents.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p107">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p108">
<div class="code-area-container">
<pre class="code-area">IDF Vector Values:
  [idf("the"), idf("cat"), idf("in"), idf("the"), idf("hat")]

IDF Vector:
  [1.0513, 5.5953, 1.1053, 1.0513, 6.2786]</pre>
</div>
</div>
<div class="readable-text" id="p109">
<p>These results look encouraging. The terms can now be weighted based on their relative descriptiveness or significance to the query:</p>
</div>
<ol>
<li class="readable-text" id="p110"> ‚Äúhat‚Äù: <code>6.2786</code> </li>
<li class="readable-text" id="p111"> ‚Äúcat‚Äù: <code>5.5953</code> </li>
<li class="readable-text" id="p112"> ‚Äúin‚Äù: <code>1.1053</code> </li>
<li class="readable-text" id="p113"> ‚Äúthe‚Äù: <code>1.0513</code> </li>
</ol>
<div class="readable-text" id="p114">
<p>We‚Äôll next combine the TF and IDF ranking techniques you‚Äôve learned thus far into a balanced relevance ranking function. </p>
</div>
<div class="readable-text" id="p115">
<h3 class="readable-text-h3" id="sigil_toc_id_43"><span class="num-string">3.1.6</span> TF-IDF: A balanced weighting metric for text-based relevance</h3>
</div>
<div class="readable-text" id="p116">
<p>We now have the two principal components of text-based relevance ranking:</p>
</div>
<ul>
<li class="readable-text" id="p117"> TF measures how well a term describes a document. </li>
<li class="readable-text" id="p118"> IDF measures how important each term is. </li>
</ul>
<div class="readable-text" id="p119">
<p>Most search engines, and many other data science applications, use a combination of these factors as the basis for textual similarity scoring, using a variation of the function in figure 3.6.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p120">
<img alt="figure" height="21" src="../Images/grainger-ch3-eqs-3x.png" width="216"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.6</span> TF-IDF score. Combines both the TF and IDF calculations into a balanced text-ranking similarity score.</h5>
</div>
<div class="readable-text" id="p121">
<p>With this improved feature-weighting function in place, we can finally calculate a balanced relevance score as follows.</p>
</div>
<div class="browsable-container listing-container" id="p122">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.8</span> Calculating TF-IDF for the query <code>the cat in the hat</code></h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">def tf_idf(term, doc):
  return <strong>TF(term, doc) * IDF(term)</strong><em>**2</em>

query = "the cat in the hat"
terms = tokenize(query)

query_vector = list(numpy.repeat(1, len(terms)))
doc_vectors = [[tf_idf(doc, term) for term in terms] for doc in docs]
doc_scores = [[dot(query_vector, dv)] for dv in doc_vectors]

print_tf_idf_scores(terms, doc_vectors, doc_scores)</pre>
</div>
</div>
<div class="readable-text" id="p123">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p124">
<div class="code-area-container">
<pre class="code-area">Document TF-IDF Vector Calculations
  doc1: [tf_idf(doc1, "the"), tf_idf(doc1, "cat"), tf_idf(doc1, "in"),
         tf_idf(doc1, "the"), tf_idf(doc1, "hat")]
  doc2: [tf_idf(doc2, "the"), tf_idf(doc2, "cat"), tf_idf(doc2, "in"),
         tf_idf(doc2, "the"), tf_idf(doc2, "hat")]
  doc3: [tf_idf(doc3, "the"), tf_idf(doc3, "cat"), tf_idf(doc3, "in"),
         tf_idf(doc3, "the"), tf_idf(doc3, "hat")]

Document TF-IDF Vector Scores
Labels: ['the', 'cat', 'in', 'the', 'hat']
  doc1: [0.4756, 0.0, 0.4703, 0.4755, 0.0]
  doc2: [0.3438, 7.9521, 0.3103, 0.3438, 10.0129]
  doc3: [0.0, 6.1399, 0.3389, 0.0, 7.7311]

Relevance Scores:
  doc1: 1.4215
  doc2: 18.9633
  doc3: 14.2099</pre>
</div>
</div>
<div class="readable-text" id="p125">
<p>Finally, our search results make sense! doc2 gets the highest score, since it matches the most important words the most, followed by doc3, which contains all the words, but not as many times, followed by doc1, which only contains an abundance of insignificant words.</p>
</div>
<div class="readable-text intended-text" id="p126">
<p>This TF-IDF calculation is at the heart of many search engine relevance algorithms, including the default similarity algorithm, known as BM25, which is currently used for keyword-based ranking in most search engines. We‚Äôll introduce BM25 in the next section. </p>
</div>
<div class="readable-text" id="p127">
<h2 class="readable-text-h2" id="sigil_toc_id_44"><span class="num-string">3.2</span> Controlling the relevance calculation</h2>
</div>
<div class="readable-text" id="p128">
<p>In the last section, we showed how queries and documents can be represented as vectors and how cosine similarity, or other similarity measurements like the dot product, can be used as a relevance function to compare queries and documents. We introduced TF-IDF ranking, which can be used to create a feature weight that balances both the strength of occurrence (TF) and significance of a term (IDF) for each term in a term-based vector. </p>
</div>
<div class="readable-text intended-text" id="p129">
<p>In this section, we‚Äôll show how a full relevance function can be specified and controlled in a search engine, including common query capabilities, modeling queries as functions, ranking versus filtering, and applying different kinds of boosting techniques.</p>
</div>
<div class="readable-text" id="p130">
<h3 class="readable-text-h3" id="sigil_toc_id_45"><span class="num-string">3.2.1</span> BM25: The industry standard default text-similarity algorithm</h3>
</div>
<div class="readable-text" id="p131">
<p>BM25 is the name of the default similarity algorithm in Apache Lucene, Apache Solr, Elasticsearch, OpenSearch, and many other search engines. BM25 (short for Okapi ‚ÄúBest Matching‚Äù version 25) was first published in 1994, and it demonstrates improvements over standard TF-IDF cosine similarity ranking in many real-world, text-based ranking evaluations. For now, it still beats ranking models using embeddings from most non-fine-tuned LLMs, so it serves as a good baseline for keyword-based ranking. </p>
</div>
<div class="readable-text intended-text" id="p132">
<p>BM25 still uses TF-IDF at its core, but it also includes several other parameters, giving more control over factors like TF saturation point and document length normalization. It also sums the weights for each matched keyword instead of calculating a cosine.</p>
</div>
<div class="readable-text intended-text" id="p133">
<p>The full BM25 calculation is shown in figure 3.7. The variables are defined as follows:</p>
</div>
<ul>
<li class="readable-text" id="p134"> <em>t</em> = term; <em>d</em> = document; <em>q</em> = query. </li>
<li class="readable-text" id="p135"> <em>freq</em>(<em>t</em>, &gt;<em>d</em>) is a simple TF, Œ£<sub>ùë°œµùëë</sub> 1 showing the number of times term <code>t</code> occurs in document <code>d</code>. </li>
</ul>
<ul>
<li class="readable-text" id="p136"> <span><img alt="equation image" height="33" src="../Images/eq-chapter-3-136-1.png" width="187"/></span> </li>
</ul>
<ul>
<li class="readable-text" id="p137"> is a variation of IDF used in BM25, where <em>N</em> is the total number of documents and <em>N</em>(<em>t</em>) is the number of documents containing term <em>t</em>. </li>
<li class="readable-text" id="p138"> |<em>d</em>| is the number of terms in document <em>d</em>. </li>
<li class="readable-text" id="p139"> <em>avgdl</em> is the average number of terms per document in the index. </li>
<li class="readable-text" id="p140"> <em>k</em> is a free parameter that usually ranges from 1.2 to 2.0 and increases the TF saturation point. </li>
<li class="readable-text" id="p141"> <em>b</em> is a free parameter usually set to around 0.75. It increases the effect of document normalization.<span class="aframe-location"/> </li>
</ul>
<div class="browsable-container figure-container" id="p142">
<img alt="figure" height="109" src="../Images/CH03_F07_Grainger.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.7</span> BM25 scoring function. It still predominantly uses a simplified <em>TF</em> and a variation of <em>IDF</em>, but it provides more control over how much each additional occurrence of a term contributes to the score (the <code>k</code> parameter) and how much scores are normalized based on document length (the <code>b</code> parameter).</h5>
</div>
<div class="readable-text" id="p143">
<p>You can see that the numerator contains <em>freq</em> (simplified TF) and <em>IDF</em> parameters, while the denominator adds the new normalization parameters <em>k</em> and <em>b</em>. The TF saturation point is controlled by <em>k</em>, making additional matches on the same term count less as <em>k</em> is increased, and by <em>b</em>, which controls the level of document length normalization more as it increases. The <em>TF</em> for each term is calculated as <em>freq</em>(<em>t</em>,<em> </em><em>d</em>) / (<em>freq</em>(<em>t</em>,<em> </em><em>d</em>) + <em>k</em> ¬∑ (1 ‚Äì <em>b</em> + <em>b</em> ¬∑ |<em>d</em>| / <em>avgd</em><em>l</em>)), which is a more complex calculation than the one we used in figure 3.3. </p>
</div>
<div class="readable-text intended-text" id="p144">
<p>Conceptually, BM25 just provides a heuristically determined better way to normalize the TF than traditional TF-IDF. It‚Äôs also worth noting that several variations of the BM25 algorithm exist (BM25F, BM25+), and, depending on the search engine you use, you might see some slight alterations and optimizations.</p>
</div>
<div class="readable-text intended-text" id="p145">
<p>Instead of reimplementing all this math in Python as we test out BM25, let‚Äôs now switch over to using our search engine and see how it performs the calculation. Let‚Äôs start by creating a collection in the search engine (listing 3.9). A collection contains a specific schema and configuration, and it is the unit upon which we will index documents, search, rank, and retrieve search results. Then we‚Äôll index some documents (using our previous <code>the</code> <code>cat</code> <code>in</code> <code>the</code> <code>hat</code> example), as shown in listing 3.10.</p>
</div>
<div class="browsable-container listing-container" id="p146">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.9</span> Creating the <code>cat_in_the_hat</code> collection</h5>
<div class="code-area-container">
<pre class="code-area">engine = get_engine()<span class="aframe-location"/> #1
collection = engine.create_collection("cat_in_the_hat")</pre>
<div class="code-annotations-overlay-container">
     #1 The engine is set to Apache Solr by default. See appendix B to use other supported search engines and vector databases.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p147">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p148">
<div class="code-area-container">
<pre class="code-area">Wiping "cat_in_the_hat" collection
Creating "cat_in_the_hat" collection
Status: Success</pre>
</div>
</div>
<div class="browsable-container listing-container" id="p149">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.10</span> Adding documents to a collection</h5>
<div class="code-area-container">
<pre class="code-area">docs = [{"id": "doc1",
         "title": "Worst",
         "description": """The interesting thing is that the person in the
                           wrong made the right decision in the end."""},
        {"id": "doc2",
         "title": "Best",
         "description": """My favorite book is the cat in the hat, which is
                           about a crazy cat who breaks into a house and
                           creates a crazy afternoon for two kids."""},
        {"id": "doc3",
         "title": "Okay",
         "description": """My neighbors let the stray cat stay in their
                           garage, which resulted in my favorite hat that
                           I let them borrow being ruined."""}]
collection.add_documents(docs)</pre>
</div>
</div>
<div class="readable-text" id="p150">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p151">
<div class="code-area-container">
<pre class="code-area">Adding Documents to 'cat_in_the_hat' collection
Status: Success</pre>
</div>
</div>
<div class="readable-text" id="p152">
<p>With our documents added to the search engine, we can now issue our query and see the full BM25 scores. The following listing searches with the query <code>the cat in the hat</code> and requests the relevance calculation explanation for each document.</p>
</div>
<div class="browsable-container listing-container" id="p153">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.11</span> Ranking by and inspecting the BM25 similarity score</h5>
<div class="code-area-container">
<pre class="code-area">query = "the cat in the hat"
request = {"query": query,
           "query_fields": ["description"],
           "return_fields": ["id", "title", "description", "score"],
           "explain": True}

response = collection.search(**request)
display_search(query, response["docs"])</pre>
</div>
</div>
<div class="readable-text" id="p154">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p155">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Query</em>: the cat in the hat
<em>Ranked Docs</em>:
[{'id': 'doc2',
'title': ['Best'],
'description': ['My favorite book is the cat in the hat, which is about a
<span class="">‚Ü™</span>crazy cat who breaks into a house and creates a crazy afternoon for
<span class="">‚Ü™</span>two kids.'],
'score': 0.68231964, '[explain]': '
  0.68231964  = sum of:
    0.15655403 = weight(description:the in 1) [SchemaSimilarity], result of:
      0.15655403 = score(freq=2.0), product of:
        2.0 = boost
        0.13353139 = idf, computed as log(1 + (N - n + 0.5) / (
          n + 0.5)) from:
          3 = n, number of documents containing term
          3 = N, total number of documents with field
        0.58620685 = tf, computed as freq / (freq + k1 * (
          1 - b + b * dl / avgdl)) from:
          2.0 = freq, occurrences of term within document
          1.2 = k1, term saturation parameter
          0.75 = b, length normalization parameter
          28.0 = dl, length of field
          22.666666 = avgdl, average length of field
    0.19487953 = weight(description:hat in 1) ...
    0.27551934 = weight(description:cat in 1) ...
    0.05536667 = weight(description:in in 1) ...
'}, {'id': 'doc3',
'title': ['Okay'],
'description': ['My neighbors let the stray cat stay in their garage, which
<span class="">‚Ü™</span>resulted in my favorite hat that I let them borrow being ruined.'],
'score': 0.62850046, '[explain]': '
  0.62850046 = sum of:
    0.21236044  = weight(description:the in 2) ...
    0.08311336 = weight(description:hat in 2) ...
    0.21236044 = weight(description:cat in 2) ...
    0.120666236 = weight(description:in in 2) ...
'}, {'id': 'doc1',
'title': ['Worst'],
'description': ['The interesting thing is that the person in the wrong made
<span class="">‚Ü™</span>the right decision in the end.'],
'score': 0.3132525,
'[explain]': '
  0.3132525 = sum of:
    0.089769006 = weight(description:the in 0) ...
    0.2234835 = weight(description:in in 0) ...
'}]</pre>
</div>
</div>
<div class="readable-text" id="p156">
<p>For the top-ranked document, doc2, you can see a portion of the score calculation using the <code>tf</code> and <code>idf</code> components, and you can see the high-level scores for each matched term in the query for the other two documents. If you would like to dig deeper into the math, you can examine the full calculations in the Jupyter notebook. </p>
</div>
<div class="readable-text intended-text" id="p157">
<p>While the BM25 calculation is more complex than the TF-IDF feature weight calculations, it still uses TF-IDF as a core part of its calculation. As a result, the BM25 ranking is in the same relative order as our TF-IDF calculations from listing 3.8:</p>
</div>
<div class="browsable-container listing-container" id="p158">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Ranked Results (Listing 3.8: TF-IDF Cosine Similarity)</em>:
  doc2: 0.998
  doc3: 0.9907
  doc1: 0.0809

<em>Ranked Results (Listing 3.9: BM25 Similarity)</em>:
  doc2: 0.6878265
  doc3: 0.62850046
  doc1: 0.3132525</pre>
</div>
</div>
<div class="readable-text" id="p159">
<p>Our query for <code>the cat in the hat</code> can still very much be thought of as a vector of the BM25 scores for each of the terms: <code>["the", "cat", "in", "the", "hat"]</code>.</p>
</div>
<div class="readable-text intended-text" id="p160">
<p>What may not be obvious is that the feature weights for each of these terms are actually overridable functions. Instead of thinking of our query as a bunch of keywords, we can think of our query as a mathematical function composed of other functions, where some of those functions take keywords as inputs and return numerical values (scores) to be used in the relevance calculation. For example, our query could alternatively be expressed as this vector:</p>
</div>
<div class="browsable-container listing-container" id="p161">
<div class="code-area-container">
<pre class="code-area">[ query("the"), query("cat"), query("in"), query("the"), query("hat") ]</pre>
</div>
</div>
<div class="readable-text" id="p162">
<p>The <code>query</code> function here simply calculates the BM25 of the term passed in, relative to all the documents scored. So the BM25 of the entire query is the sum of the TF-IDFs of each term. In Solr query syntax, this would be </p>
</div>
<div class="browsable-container listing-container" id="p163">
<div class="code-area-container">
<pre class="code-area">{!func}query("the") {!func}query("cat") {!func}query("in")
{!func}query("the") {!func}query("hat")</pre>
</div>
</div>
<div class="readable-text" id="p164">
<p>If we execute this ‚Äúfunctionized‚Äù version of the query, we get the exact same relevance score as if we had executed the query directly. The following listing shows the code that performs this version of the query.</p>
</div>
<div class="browsable-container listing-container" id="p165">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.12</span> Text similarity using the <code>query</code> function</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">query = '{!func}query("the") {!func}query("cat") {!func}query("in")
       <span class="">‚Ü™</span>{!func}query("the") {!func}query("hat")'
request = {"query": query,
           "query_fields": "description",
           "return_fields": ["id", "title", "score"]}

response = collection.search(**request)
display_search(query, response["docs"])</pre>
</div>
</div>
<div class="readable-text" id="p166">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p167">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Query:</em>
 {!func}query("the") {!func}query("cat") {!func}query("in")
  {!func}query("the") {!func}query("hat")
<em>Results:</em>
 [{'id': 'doc2', 'title': ['Best'], 'score': 0.6823196},
  {'id': 'doc3', 'title': ['Okay'], 'score': 0.62850046},
  {'id': 'doc1', 'title': ['Worst'], 'score': 0.3132525}]</pre>
</div>
</div>
<div class="readable-text" id="p168">
<p>As expected, the scores are the same as before‚Äîwe‚Äôve simply substituted explicit functions where implicit functions were previously. </p>
</div>
<div class="readable-text" id="p169">
<h3 class="readable-text-h3" id="sigil_toc_id_46"><span class="num-string">3.2.2</span> Functions, functions, everywhere!</h3>
</div>
<div class="readable-text" id="p170">
<p>We just encountered the <code>query</code> function (at the end of the previous section), which performs the default (BM25) similarity calculation on keywords. Understanding that every part of the query is actually a configurable scoring function opens tremendous possibilities for manipulating the relevance algorithm. What <em>other</em> kinds of functions can be used in queries? Can we use other features in our scoring calculation‚Äîperhaps some that are not text-based?</p>
</div>
<div class="readable-text intended-text" id="p171">
<p>Here is a partial list of functions and scoring techniques commonly applied to influence relevance scores:</p>
</div>
<ul>
<li class="readable-text" id="p172"> <em>Geospatial boosting</em><em>‚Äâ</em>‚ÄîDocuments near the user running the query should rank higher. </li>
<li class="readable-text" id="p173"> <em>Date boosting</em><em>‚Äâ</em>‚ÄîNewer documents should get a higher relevance boost. </li>
<li class="readable-text" id="p174"> <em>Popularity boosting</em><em>‚Äâ</em>‚ÄîMore popular documents should get a higher relevance boost. </li>
<li class="readable-text" id="p175"> <em>Field boosting</em><em>‚Äâ</em>‚ÄîTerms matching in certain fields should get a higher weight than in other fields. </li>
<li class="readable-text" id="p176"> <em>Category boosting</em><em>‚Äâ</em>‚ÄîDocuments in categories related to query terms should get a higher relevance boost. </li>
<li class="readable-text" id="p177"> <em>Phrase boosting</em><em>‚Äâ</em>‚ÄîDocuments matching multi-term phrases in the query should rank higher than those only matching the words separately. </li>
<li class="readable-text" id="p178"> <em>Semantic expansion</em><em>‚Äâ</em>‚ÄîDocuments containing other words or concepts that are highly related to the query keywords and context should be boosted. </li>
</ul>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p179">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Using the book‚Äôs search-engine-agnostic search API</h5>
</div>
<div class="readable-text" id="p180">
<p>Throughout the book and codebase, we‚Äôve implemented a set of Python libraries providing a generic API for indexing documents (<code>collection.add_documents( documents)</code> or <code>collection.write(dataframe)</code>), querying documents (<code>collection.search(**query_parameters)</code>), and performing other search engine operations. This allows you to execute the same code in the book and corresponding notebooks regardless of which supported search engine or vector database you have selected, delegating the creation of engine-specific syntax to the client library. See appendix B for details about how to switch seamlessly between engines.</p>
</div>
<div class="readable-text" id="p182">
<p>While these generic methods for invoking AI-powered search against your favorite engine are powerful, it‚Äôs also helpful in some cases to see the details of the underlying implementation within the search engine, and for more complicated examples it can even be difficult to express the full power of what‚Äôs going on using the higher-level engine-agnostic API. For this reason, we will occasionally also include the raw search engine syntax for our default search engine (Apache Solr) in the book. If you‚Äôre unfamiliar with Apache Solr and its syntax, please don‚Äôt get too bogged down in the details. The important thing is to understand the concepts well-enough that you can apply them to your search engine of choice.</p>
</div>
</div>
<div class="readable-text" id="p183">
<p>These techniques (and many more) are supported by most major search engines. For example, field boosting can be accomplished in our search client by appending a <code>^BOOST_AMOUNT</code> after any of the fields specified in <code>query_fields</code> for a query:</p>
</div>
<div class="browsable-container listing-container" id="p184">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Generic search request syntax:</em>
 {"query": "the cat in the hat",
  <strong>"query_fields": ["title^10", "description^2.5"]</strong>}</pre>
</div>
</div>
<div class="readable-text" id="p185">
<p>This query request provides a 10X relevancy boost for matches in the <code>title</code> field and a 2.5X relevancy boost for matches in the <code>description</code> field. When mapped into Solr syntax, it looks like this:</p>
</div>
<div class="browsable-container listing-container" id="p186">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Solr request syntax:</em>
 {"query": "the cat in the hat",
  "params": {<strong>"defType": "edismax"</strong>,
             <strong>"qf": "title^10 description^2.5"</strong>}}</pre>
</div>
</div>
<div class="readable-text" id="p187">
<p>Every search engine is different, but many of these techniques are built into specific query parsers in Solr, either through query syntax or through query parser options, as with the <code>edismax</code> query parser just shown. </p>
</div>
<div class="readable-text intended-text" id="p188">
<p>Boosting on full phrase matching, on two-word phrases, and on three-word phrases is also a native feature of Solr‚Äôs <code>edismax</code> query parser:</p>
</div>
<ul>
<li class="readable-text" id="p189"> Boost docs containing the exact phrase <code>"the</code> <code>cat</code> <code>in</code> <code>the</code> <code>hat"</code> in the <code>title</code> field: </li>
</ul>
<div class="browsable-container listing-container" id="p190">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Solr request syntax:</em>
 {"query": "the cat in the hat",
  "params": {"defType": "edismax",
             "qf": "title description",
             <strong>"pf": "title"</strong>}}</pre>
</div>
</div>
<ul>
<li class="readable-text" id="p191"> Boost docs containing the two-word phrases <code>"the cat"</code>, <code>"cat in"</code>, <code>"in the"</code>, or <code>"the hat"</code> in the <code>title</code> or <code>description</code> field: </li>
</ul>
<div class="browsable-container listing-container" id="p192">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Solr request syntax:</em>
 {"query": "the cat in the hat",
  "params": {"defType": "edismax",
             "qf": "title description",
             <strong>"pf2": "title description"</strong>}}</pre>
</div>
</div>
<ul>
<li class="readable-text" id="p193"> Boost docs containing the three-word phrases <code>"the</code> <code>cat</code> <code>in"</code> or <code>"in</code> <code>the</code> <code>hat"</code> in the <code>description</code> field: </li>
</ul>
<div class="browsable-container listing-container" id="p194">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Solr request syntax:</em>
 {"query": "the cat in the hat",
  "params": {"defType": "edismax",
             "qf": "title description",
             <strong>"pf3": "description"</strong>}}</pre>
</div>
</div>
<div class="readable-text" id="p195">
<p>Many of the other relevance-boosting techniques require constructing custom features using function queries. For example, if we want to create a query that only boosts the relevance ranking of documents geographically closest to the user running the search, we can issue the following Solr query:</p>
</div>
<div class="browsable-container listing-container" id="p196">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Solr request syntax:</em>
 {"query": "*",
  "sort": <strong>"geodist(location, $user_latitude, $user_longitude) asc"</strong>,
  "params": {"user_latitude": 33.748,
             "user_longitude": -84.39}}</pre>
</div>
</div>
<div class="readable-text" id="p197">
<p>That last query uses the <code>sort</code> parameter to strictly order documents by the <code>geodist</code> function, which takes the document‚Äôs location field name along with the user‚Äôs latitude and longitude as parameters. This works great when considering a single feature, but what if we want to construct a more complex sort based on many features? To accomplish this, we can update our query to apply several functions when calculating the relevance score, and then sort by the relevance score:</p>
</div>
<div class="browsable-container listing-container" id="p198">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Solr request syntax:</em>
 {"query": <strong>"{!func}scale(query($keywords),0,25)
</strong>    <strong> </strong><span class="">‚Ü™</span><strong>{!func}recip(geodist($lat_long_field,$user_latitude,
 </strong>    <span class="">‚Ü™</span><strong>$user_longitude),1,25,1)
 </strong>    <span class="">‚Ü™</span><strong>{!func}recip(ms(NOW/HOUR,modify_date),3.16e-11,25,1)
</strong>    <strong> </strong><span class="">‚Ü™</span><strong>{!func}scale(popularity,0,25)"</strong>,
  "params": {"keywords": "basketball",
             "lat_long_field": "location",
             "user_latitude": 33.748,
             "user_longitude" -84.391}}</pre>
</div>
</div>
<div class="readable-text" id="p199">
<p>That query has a few interesting characteristics:</p>
</div>
<ul>
<li class="readable-text" id="p200"> It constructs a query vector containing four features: BM25 relevance score for the keywords (higher is better), geographical distance (lower is better), publication date (newer is better), and popularity (higher is better). </li>
<li class="readable-text" id="p201"> Each of the feature values is scaled between <code>0</code> and <code>25</code> so they are all comparable, with the best score for each feature being <code>25</code>, and the worst score near <code>0</code>. </li>
<li class="readable-text" id="p202"> Thus, a ‚Äúperfect score‚Äù will add up to <code>100</code> (all 4 features scoring <code>25</code>), and the worst score will be approximately <code>0</code>. </li>
<li class="readable-text" id="p203"> Since the relative contribution of <code>25</code> is specified as part of the query for each function, we can easily adjust the weights for any features on the fly to influence the final relevance calculation. </li>
</ul>
<div class="readable-text" id="p204">
<p>With the last query, we have fully taken the relevance calculation into our own hands by modeling the relevance features and giving them weights. While this is very powerful, it still requires significant manual effort to determine which features matter most for a given domain and to tune their weights. In chapter 10, we‚Äôll walk through building machine-learned ranking models to automatically make those decisions for us (a process known as <em>learning to rank</em>). For now, our goal is just to understand the mechanics of modeling features in query vectors and how to programmatically control their weights.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p205">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Deeper dives on function queries</h5>
</div>
<div class="readable-text" id="p206">
<p>If you‚Äôd like to learn more about how to utilize Solr‚Äôs function queries, we recommend reading chapter 7 of <em>Solr in Action</em>, one of our previous books, by Trey Grainger and Timothy Potter (Manning, 2014; <a href="https://mng.bz/n0Y5">https://mng.bz/n0Y5</a>). For a full list of available function queries in Solr, you can also check out the documentation in the function query section of the Solr Reference Guide (<a href="https://mng.bz/vJop">https://mng.bz/vJop</a>). If you‚Äôre using a different search engine, check out their documentation for similar guidance.</p>
</div>
</div>
<div class="readable-text" id="p207">
<p>We‚Äôve seen the power of utilizing functions as features in our queries, but so far our examples have all been what are called ‚Äúadditive‚Äù boosts, where the sum of the values of each function calculation comprises the final relevance score. It is also frequently useful to combine functions in a fuzzier, more flexible way through ‚Äúmultiplicative‚Äù boosts, which we‚Äôll cover in the next section. </p>
</div>
<div class="readable-text" id="p208">
<h3 class="readable-text-h3" id="sigil_toc_id_47"><span class="num-string">3.2.3</span> Choosing multiplicative vs. additive boosting for relevance functions</h3>
</div>
<div class="readable-text" id="p209">
<p>One last topic to address, concerning how we control our relevance functions, is multiplicative versus additive boosting of relevance features. </p>
</div>
<div class="readable-text intended-text" id="p210">
<p>In all our examples to this point, we have added multiple features into our query vector to contribute to the score. For example, the following Solr queries will all yield equivalent relevance calculations, assuming they are filtered down to the same result set (i.e., <code>filters=["the cat in the hat"]</code>):</p>
</div>
<div class="browsable-container listing-container" id="p211">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Text query (score + filter):</em>
 {"query": "the cat in the hat"}

<em>Function Query (score only, no filter):</em>
 {"query": '{!func}query("the cat in the hat")'}

<em>Multiple Function Queries (score only, no filter):</em>
 {"query": '{!func}query("the")
          <span class="">‚Ü™</span>{!func}query("cat")
          <span class="">‚Ü™</span>{!func}query("in")
          <span class="">‚Ü™</span>{!func}query("the")
          <span class="">‚Ü™</span>{!func}query("hat")'}

<em>Boost Query (score only, no filter):</em>
 {"query": "*",
  "params": {"bq": "the cat in the hat"}}</pre>
</div>
</div>
<div class="readable-text" id="p212">
<p>The kind of relevance boosting in each of these examples is known as <em>additive boosting</em>, and it maps well to our concept of a query as nothing more than a vector of features that needs to have its similarity compared across documents. In additive boosting, the relative contribution of each feature decreases as more features are added, since the total score is just the sum of all the features‚Äô scores. </p>
</div>
<div class="readable-text intended-text" id="p213">
<p><em>Multiplicative boosting</em>, in contrast, allows a document‚Äôs entire calculated relevance score to be scaled (multiplied) by one or more functions. Multiplicative boosting enables boosts to ‚Äúpile up‚Äù on each other, preventing the need for individually constraining the weights of different sections of the query, as we did in section 3.2.2. There, we had to ensure that the keyword scores, geographical distance, age, and popularity of documents were each scaled to 25% of the relevance score so that they would add up to a maximum score of 100%.</p>
</div>
<div class="readable-text intended-text" id="p214">
<p>To supply a multiplicative boost in Apache Solr, you can either use the <code>boost</code> query parser (syntax: <code>{!boost ‚Ä¶}</code>) in your query vector or, if you are using the <code>edismax</code> query parser, the simplified <code>boost</code> query param. The following two queries will multiply a document‚Äôs relevance score by ten times the value in the <code>popularity</code> field:</p>
</div>
<div class="browsable-container listing-container" id="p215">
<div class="code-area-container code-area-with-html">
<pre class="code-area">{"query": "the cat in the hat",
 "params": {"defType": "edismax",
            <strong>"boost": "mul(popularity,10)"</strong>}}

{"query": "<strong>{!boost b=mul(popularity,10)} </strong>the cat in the hat"}</pre>
</div>
</div>
<div class="readable-text" id="p216">
<p>In this example, the query for <code>the cat in the hat</code> still uses additive boosting (the BM25 value of each keyword is added together), but the final score is multiplied by 10 times the value in the <code>popularity</code> field. This multiplicative boosting allows the popularity to scale the relevance score independently of any other features. </p>
</div>
<div class="readable-text intended-text" id="p217">
<p>In general, multiplicative boosts offer you greater flexibility to combine different relevance features without having to explicitly predefine a relevance formula accounting for every potential contributing factor. On the other hand, this flexibility can lead to unexpected consequences if the multiplicative boost values for particular features get too high and overshadow other features. In contrast, additive boosts can be a pain to manage, because you need to explicitly scale them so that they can be combined while still maintaining a predictable contribution to the overall score. However, with this explicit scaling, you maintain tight control over the relevance scoring calculation and range of scores. Both additive and multiplicative boosts can be useful, so it‚Äôs best to consider the problem at hand and experiment with what gets you the best results.</p>
</div>
<div class="readable-text intended-text" id="p218">
<p>We‚Äôve now covered the major ways of controlling relevance ranking in the search engine, but the matching and filtering of documents can often be just as important, so we‚Äôll cover them in the next section. </p>
</div>
<div class="readable-text" id="p219">
<h3 class="readable-text-h3" id="sigil_toc_id_48"><span class="num-string">3.2.4</span> Differentiating matching (filtering) vs. ranking (scoring) of documents</h3>
</div>
<div class="readable-text" id="p220">
<p>We‚Äôve spoken of queries and documents as feature vectors, but we‚Äôve mainly discussed search thus far as a process of either calculating a vector similarity (such as cosine or dot product) or adding up document scores for each feature (keyword or function) in the query. </p>
</div>
<div class="readable-text intended-text" id="p221">
<p>Once documents are indexed, there are two primary steps involved in executing a query:</p>
</div>
<ul>
<li class="readable-text" id="p222"> <em>Matching</em><em>‚Äâ</em>‚ÄîFiltering results to a known set of possible answers </li>
<li class="readable-text" id="p223"> <em>Ranking</em><em>‚Äâ</em>‚ÄîOrdering all the possible answers by relevance </li>
</ul>
<div class="readable-text" id="p224">
<p>We can often completely skip the first step (matching) and still see the exact same results on page one (and for many pages), since the most relevant results should generally rank the highest and thus show up first. If you think back to chapter 2, we even saw some vector scoring calculations (comparing feature vectors for food items‚Äîi.e., ‚Äúapple juice‚Äù versus ‚Äúdonut‚Äù) where we would have been unable to filter results at all. We instead had to first score every document to determine which ones to return based upon relevance alone. In this scenario (using dense vector embeddings), we didn‚Äôt even have keywords or other attributes that could be used as a filter.</p>
</div>
<div class="readable-text intended-text" id="p225">
<p>So, if the initial matching phase is effectively optional, why do it at all? One obvious answer is that it provides a significant performance optimization. Instead of iterating through every single document and calculating a relevance score, we can greatly speed up both our relevance calculations and the overall response time of our search engine by first filtering the initial results to a smaller set of documents that are logical matches.</p>
</div>
<div class="readable-text intended-text" id="p226">
<p>There are additional benefits to being able to filter result sets, in that we can provide analytics, such as the number of matching documents or counts of specific values found in documents (known as <em>facets</em> or <em>aggregations</em>). Returning facets and similar aggregated metadata from the search results helps the user subsequently filter down by specific values to further explore and refine their result set. Finally, there are plenty of scenarios where ‚Äúhaving logical matches‚Äù should be considered among the most important features in the ranking function, so simply filtering on logical matches upfront can greatly simplify the relevance calculation. We‚Äôll discuss these trade-offs in the next section. </p>
</div>
<div class="readable-text" id="p227">
<h3 class="readable-text-h3" id="sigil_toc_id_49"><span class="num-string">3.2.5</span> Logical matching: Weighting the relationships between terms in a query</h3>
</div>
<div class="readable-text" id="p228">
<p>We just mentioned that filtering results before scoring them is primarily a performance optimization and that the first few pages of search results would likely look the same regardless of whether you filter the results or just do relevance ranking. </p>
</div>
<div class="readable-text intended-text" id="p229">
<p>This only holds true, however, if your relevance function successfully contains features that already appropriately boost better logical matches. For example, consider the difference between expectations for the following queries:</p>
</div>
<ol>
<li class="readable-text" id="p230"> <code>"statue of liberty"</code> </li>
<li class="readable-text" id="p231"> <code>statue AND of AND liberty</code> </li>
<li class="readable-text" id="p232"> <code>statue OR of OR liberty</code> </li>
<li class="readable-text" id="p233"> <code>statue of liberty</code> </li>
</ol>
<div class="readable-text" id="p234">
<p>From a logical matching standpoint, the first query will be very precise, only matching documents containing the <em>exact</em> phrase ‚Äústatue of liberty‚Äù. The second query will only match documents containing all the terms ‚Äústatue‚Äù, ‚Äúof‚Äù, and ‚Äúliberty‚Äù, but not necessarily as a phrase. The third query will match any document containing any of the three terms, which means documents <em>only</em> containing ‚Äúof‚Äù will match, but documents containing ‚Äústatue‚Äù and ‚Äúliberty‚Äù should rank much higher due to the BM25 scoring calculation.</p>
</div>
<div class="readable-text intended-text" id="p235">
<p>In theory, if phrase boosting is turned on as a feature, documents containing the full phrase will likely rank highest, followed by documents containing all terms, followed by documents containing any of the words. Assuming that happens, you should see a similar ordering of results regardless of whether you filter them to logical Boolean matches or whether you only sort based on a relevance function.</p>
</div>
<div class="readable-text intended-text" id="p236">
<p>In practice, though, users often consider the logical structure of their queries to be highly relevant to the documents they expect to see, so respecting this logical structure and filtering <em>before</em> ranking allows you to remove results that users‚Äô queries indicate are safe to remove.</p>
</div>
<div class="readable-text intended-text" id="p237">
<p>Sometimes the logical structure of user queries is ambiguous, however, such as with our fourth example: the query <code>statue</code> <code>of</code> <code>liberty</code>. Does this logically mean <code>statue</code> <code>AND</code> <code>of</code> <code>AND</code> <code>liberty</code>, <code>statue</code> <code>OR</code> <code>of</code> <code>OR</code> <code>liberty</code>, or something more nuanced like <code>(statue</code> <code>AND</code> <code>of)</code> <code>OR</code> <code>(statue</code> <code>AND</code> <code>liberty)</code> <code>OR</code> <code>(of</code> <code>AND</code> <code>liberty)</code>, which essentially means ‚Äúmatch at least two of three terms‚Äù. Using the ‚Äúminimum match‚Äù (<code>min_match</code>) parameter in our search API enables you to control these kinds of matching thresholds easily, even on a per-query basis:</p>
</div>
<ul>
<li class="readable-text" id="p238"> 100% of query terms must match (equivalent to <code>statue</code> <code>AND</code> <code>of</code> <code>AND</code> <code>liberty</code>): </li>
</ul>
<div class="browsable-container listing-container" id="p239">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Generic search request syntax:</em>
 {"query": "statue of liberty",
  <strong>"min_match": "100%"</strong>}

<em>Solr request syntax:</em>
 {"query": "statue of liberty",
  "params": {"defType": "edismax",
             <strong>"mm": "100%"</strong>}}</pre>
</div>
</div>
<ul>
<li class="readable-text" id="p240"> At least one query term must match (equivalent to <code>statue</code> <code>OR</code> <code>of</code> <code>OR</code> <code>liberty</code>): </li>
</ul>
<div class="browsable-container listing-container" id="p241">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Generic search request syntax:</em>
 {"query": "statue of liberty",
  <strong>"min_match": "1"</strong>}

<em>Solr request syntax:</em>
 {"query": "statue of liberty",
  "params": {"defType": "edismax",
             <strong>"mm": "1"</strong>}}</pre>
</div>
</div>
<ul>
<li class="readable-text" id="p242"> At least two query terms must match (equivalent to <code>(statue</code> <code>AND</code> <code>of)</code> <code>OR (statue</code> <code>AND</code> <code>liberty)</code> <code>OR</code> <code>(of</code> <code>AND</code> <code>liberty)</code>): </li>
</ul>
<div class="browsable-container listing-container" id="p243">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Generic search request syntax:</em>
 {"query": "statue of liberty",
  "query_parser": "edismax",
  <strong>"min_match": "2"</strong>}

<em>Solr request syntax:</em>
 {"query": "statue of liberty",
  "params": {"defType": "edismax",
             <strong>"mm": "2"</strong>}}</pre>
</div>
</div>
<div class="readable-text" id="p244">
<p>The <code>min_match</code> param in our Python API supports specifying either a minimum percentage (0% to 100%) of terms or a number of terms (1 to <em>N</em> terms) that must match. This parameter corresponds with Solr‚Äôs <code>mm</code> parameter and OpenSearch‚Äôs and Elasticsearch‚Äôs <code>minimum_should_match</code> parameter. In addition to accepting a percentage or number of terms to match, those engines also support a step function like <code>mm=2&lt;-30% 5&lt;3</code>. This example step function means ‚Äúall terms are required if there are less than 2 terms, up to 30% of terms can be missing if there are less than 5 terms, and at least 3 terms must exist if there are 5 or more terms‚Äù. When using Solr, the <code>mm</code> parameter works with the <code>edismax</code> query parser, which is the primary query parser we will use for text-matching queries in this book if Solr is configured as your engine (per appendix B). You can consult the ‚ÄúExtended DisMax Parameters‚Äù section of the Solr Reference Guide for more details on how to fine-tune your logical matching rules with these minimum match capabilities (<a href="https://mng.bz/mRo8">https://mng.bz/mRo8</a>). </p>
</div>
<div class="readable-text intended-text" id="p245">
<p>When thinking about constructing relevance functions, the ideas of filtering and scoring can often get mixed up, particularly since most search engines perform both for their main query parameter. We‚Äôll attempt to separate these concerns in the next section. </p>
</div>
<div class="readable-text" id="p246">
<h3 class="readable-text-h3" id="sigil_toc_id_50"><span class="num-string">3.2.6</span> Separating concerns: Filtering vs. scoring</h3>
</div>
<div class="readable-text" id="p247">
<p>In section 3.2.4, we differentiated between the ideas of matching and ranking. Matching of results is logical and is implemented by filtering search results down to a subset of documents, whereas ranking of results is qualitative and is implemented by scoring all documents relative to the query and then sorting them by that calculated score. In this section, we‚Äôll cover some techniques to provide maximum flexibility in controlling matching and ranking by cleanly separating out the concerns of filtering and scoring. </p>
</div>
<div class="readable-text intended-text" id="p248">
<p>Our search API has two primary ways to control filtering and scoring: the <code>query</code> and <code>filters</code> parameters. Consider the following request:</p>
</div>
<div class="browsable-container listing-container" id="p249">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Generic search request syntax:</em>
 {<strong>"query": "the cat in the hat"</strong>,
  "query_fields": ["description"],
  <strong>"filters": [("category", "books"), ("audience", "kid")]</strong>,
  "min_match": "100%"}

<em>Solr request syntax:</em>
 {<strong>"query": "the cat in the hat"</strong>,
  <strong>"filters": ["category:books", "audience:kid"]</strong>,
  "params": {"qf": ["description"],
             "mm": "100%",
             "defType": "edismax"}}</pre>
</div>
</div>
<div class="readable-text" id="p250">
<p>In this query, the search engine is being instructed to filter the possible result set down to only documents with both a value of ‚Äúbooks‚Äù in the <code>category</code> field and a value of ‚Äúkid‚Äù in the <code>audience</code> field. In addition to those filters, however, the query also acts as a filter, so the result set gets further filtered down to only documents containing (100%) of the values ‚Äúthe‚Äù, ‚Äúcat‚Äù, ‚Äúin‚Äù, and ‚Äúhat‚Äù in the <code>description</code> field. </p>
</div>
<div class="readable-text intended-text" id="p251">
<p>The logical difference between the <code>query</code> and <code>filters</code> parameters is that <code>filters</code> only acts as a filter, whereas <code>query</code> acts as <em>both</em> a filter and a feature vector for relevance ranking. This dual use of the <code>query</code> parameter is helpful default behavior for queries, but mixing the concerns of filtering and scoring in the same parameter can be suboptimal for more advanced queries, especially if we‚Äôre simply trying to manipulate the relevance calculation and not arbitrarily removing results from our document set. </p>
</div>
<div class="readable-text intended-text" id="p252">
<p>There are a few ways to address this:</p>
</div>
<ul>
<li class="readable-text" id="p253"> Model the <code>query</code> parameter as a function (functions only count toward relevance and do not filter): </li>
</ul>
<div class="browsable-container listing-container" id="p254">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Solr request syntax:</em>
 {<strong>"query": '{!func}query("{!edismax qf=description mm=100%
</strong> <strong>  </strong><span class="">‚Ü™</span><strong>v=$user_query}")'</strong>,
  "filters": "{!cache=false v=$user_query}",
  "params": {"user_query": "the cat in the hat"}}</pre>
</div>
</div>
<ul>
<li class="readable-text" id="p255"> Make your query match all documents (no filtering or scoring) and apply a boost query (<code>bq</code>) parameter to influence relevance without scoring: </li>
</ul>
<div class="browsable-container listing-container" id="p256">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><em>Solr request syntax:</em>
 {<strong>"query": "*"</strong>,
  "filters": "{!cache=false v=$user_query}",
  "params": {<strong>"bq": "{!edismax qf=description mm=100% v=$user_quer}"</strong>,
             "user_query": "the cat in the hat"}}</pre>
</div>
</div>
<div class="readable-text" id="p257">
<p>The <code>query</code> parameter both filters and then boosts based upon relevance, <code>filters</code> only filters, and <code>bq</code> only boosts. The two preceding approaches are logically equivalent, but we recommend the second option, since it‚Äôs a bit cleaner to use the dedicated <code>bq</code> parameter, which was designed to contribute toward the relevance calculation without filtering. </p>
</div>
<div class="readable-text intended-text" id="p258">
<p>You may have noticed that both versions of the query also contain a filter query <code>{!cache=false</code> <code>v=$user_query}</code> that filters on the <code>user_query</code>. Since the <code>query</code> parameter intentionally no longer filters our search results, this <code>filters</code> parameter is now required if we still want to filter to the user-entered query. The special <code>cache=false</code> parameter is used to turn off caching of the filter. Caching of filters is turned on by default in Solr, since filters tend to be reused often across requests. Since the <code>user_query</code> parameter is user-entered and is wildly variable in this case (not frequently reused across requests), it doesn‚Äôt make sense to pollute the search engine‚Äôs caches with these values. If you try to filter on user-entered queries without turning the cache off, it will waste system resources and likely slow down your search engine. </p>
</div>
<div class="readable-text intended-text" id="p259">
<p>The overarching theme here is that it‚Äôs possible to cleanly separate logical filtering from ranking features to maintain full control and flexibility over your search results. While going through this effort may be overkill for simple text-based ranking, separating these concerns becomes critical when attempting to build out more sophisticated ranking functions.</p>
</div>
<div class="readable-text intended-text" id="p260">
<p>Now that you understand the mechanics of how to construct these kinds of purpose-built ranking functions, let‚Äôs wrap up this chapter with a brief discussion of how to apply these techniques to implement user- and domain-specific relevance ranking. </p>
</div>
<div class="readable-text" id="p261">
<h2 class="readable-text-h2" id="sigil_toc_id_51"><span class="num-string">3.3</span> Implementing user and domain-specific relevance ranking</h2>
</div>
<div class="readable-text" id="p262">
<p>In section 3.2, we walked through how to modify the parameters of our query-to-document similarity algorithm dynamically. That included passing in our own functions as features that contribute to the score, in addition to text-based relevance ranking. </p>
</div>
<div class="readable-text intended-text" id="p263">
<p>While text-based relevance ranking using BM25, TF-IDF, vector cosine similarity, or some other kind of statistics-based approach on word occurrences can provide decent generic search relevance out of the box, it can‚Äôt hold its own against good domain-specific relevance factors. Here are some domain-specific factors that often matter the most within various domains:</p>
</div>
<ul>
<li class="readable-text" id="p264"> <em>Restaurant search</em><em>‚Äâ</em>‚ÄîGeographical proximity, user-specific dietary restrictions, user-specific taste preferences, price range </li>
<li class="readable-text" id="p265"> <em>News search</em><em>‚Äâ</em>‚ÄîFreshness (date), popularity, geographical area </li>
<li class="readable-text" id="p266"> <em>E-commerce</em><em>‚Äâ</em>‚ÄîLikelihood of conversion (click-through, add-to-cart, and/or purchase) </li>
<li class="readable-text" id="p267"> <em>Movie search</em><em>‚Äâ</em>‚ÄîName match (title, actor, etc.), popularity of movie, release date, critic review score </li>
<li class="readable-text" id="p268"> <em>Job search</em><em>‚Äâ</em>‚ÄîJob title, job level, compensation range, geographical proximity, job industry </li>
<li class="readable-text" id="p269"> <em>Web search</em><em>‚Äâ</em>‚ÄîKeyword match on page, popularity of page, popularity of website, location of match on page (in title, header, body, etc.), quality of page (duplicate content, spammy content, etc.), topic match between page and query </li>
</ul>
<div class="readable-text" id="p270">
<p>These are just examples, but most search engines and domains have unique features that need to be considered to deliver an optimal search experience. This chapter has barely scratched the surface of the countless ways you can control the matching and ranking functions to return the best content. An entire profession exists‚Äîcalled <em>relevance engineering</em>‚Äîthat is dedicated in many organizations to tuning search relevance. If you‚Äôd like to dive deeper, we highly recommend one of our prior books, <em>Relevant Search</em> by Doug Turnbull and John Berryman (Manning, 2016), which is a guide to this kind of relevance engineering. </p>
</div>
<div class="readable-text intended-text" id="p271">
<p>Every search engine and domain have unique features that need to be considered to deliver an optimal search experience. Instead of having to manually model these relevance features, an AI-powered search engine can utilize machine learning to automatically generate and weight such features.</p>
</div>
<div class="readable-text intended-text" id="p272">
<p>The goal of this chapter was to give you the knowledge and tools you‚Äôll need in the coming chapters to affect relevance ranking as we begin integrating more automated machine learning techniques. We‚Äôll begin applying this in our next chapter on crowdsourced relevance. </p>
</div>
<div class="readable-text" id="p273">
<h2 class="readable-text-h2" id="sigil_toc_id_52">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p274"> We can represent queries and documents as dense or sparse numerical vectors and assign documents a relevance rank based on a vector similarity calculation (such as cosine similarity). </li>
<li class="readable-text" id="p275"> Using TF-IDF or the BM25 similarity calculations (also based upon TF-IDF) for our text similarity scores provides a more meaningful measure of feature (keyword) importance in our queries and documents, enabling improved text ranking over just looking at term matches alone.  </li>
<li class="readable-text" id="p276"> Text similarity scoring is one of many kinds of functions we can invoke as a feature within our queries for relevance ranking. We can inject functions within our queries, along with keyword matching and scoring, as each keyword phrase is effectively just a ranking function. </li>
<li class="readable-text" id="p277"> Treating ‚Äúfiltering‚Äù and ‚Äúscoring‚Äù as separate concerns provides better control when specifying our own ranking functions. </li>
<li class="readable-text" id="p278"> To optimize relevance, we need to both create domain-specific relevance functions and use user-specific features instead of relying just on keyword matching and ranking.  </li>
</ul>
</div></body></html>