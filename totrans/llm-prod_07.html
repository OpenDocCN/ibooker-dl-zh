<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">8</span></span> <span class="chapter-title-text">Large language model applications: Building an interactive experience</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Building an interactive application that uses an LLM service</li>
<li class="readable-text" id="p3">Running LLMs on edge devices without a GPU</li>
<li class="readable-text" id="p4">Building LLM agents that can solve multistep problems</li>
</ul>
</div>
<div class="readable-text" id="p5">
<blockquote>
<div>
     No one cares how much you know until they know how much you care. 
     <div class="quote-cite">
       —President Theodore Roosevelt 
     </div>
</div>
</blockquote>
</div>
<div class="readable-text" id="p6">
<p>Throughout this book, we’ve taught you the ins and outs of LLMs—how to train them, how to deploy them, and, in the last chapter, how to build a prompt to guide a model to behave how you want it to. In this chapter, we will put it all together. We will show you how to build an application that can use your deployed LLM service and create a delightful experience for an actual user. The key word there is delightful. Creating a simple application is easy, as we will show, but creating one that delights? Well, that’s a bit more difficult. We’ll discuss multiple features you’ll want to add to your application and why. Then, we’ll discuss different places your application may live, including building such applications for edge devices. Lastly, we’ll dive into the world of LLM agents, building applications that can fulfill a role, not just a request.</p>
</div>
<div class="readable-text" id="p7">
<h2 class="readable-text-h2" id="sigil_toc_id_142"><span class="num-string">8.1</span> Building an application</h2>
</div>
<div class="readable-text" id="p8">
<p>It’s probably best that we start by explaining what we mean by LLM application. Afterall, <em>application</em> is a ubiquitous term that could mean lots of different things. For us, in this book, when we say <em>LLM application</em>, we mean the frontend—the Web App, Phone App, CLI, SDK, VSCode Extension (check out chapter 10!), or any other application that will act as the user interface and client for calling our LLM Service. Figure 8.1 shows both the frontend and backend separately to help focus on the piece of the puzzle we are discussing: the frontend. It’s a pretty important piece to the puzzle but also varies quite a bit! While every environment will come with its own challenges, we hope we can trust you to know the details for your particular use case. For example, if you are building an Android app, it’s up to you to learn Java or Kotlin. In this book, however, we will give you the building blocks you will need and introduce the important features to add.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p9">
<img alt="figure" height="289" src="../Images/8-1.png" width="902"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.1</span> The LLM Application is the web app, phone app, command line interface, or another tool that acts as the client our users will use to interact with our LLM service. </h5>
</div>
<div class="readable-text" id="p10">
<p>The first step to building a successful LLM application is composing and experimenting with your prompt. Of course, having just discussed this in the last chapter, there are many additional features you should consider to offer a better user experience. The most basic LLM application is just a chatbox, which essentially consists of only three objects: an input field, a send button, and a text field to hold the conversation. It’s rather easy to build in almost every context. In addition, since one of our participants in the chat is a bot, most of the complexity of building a chat interface is also stripped away. For example, we don’t need to worry about eventual consistency, mixing up the order of our conversation, or whether both users are sending a message at the same time. If our user has a bad internet connection, we can throw a timeout error and let them resubmit. </p>
</div>
<div class="readable-text intended-text" id="p11">
<p>However, while the interface is easy, not all the finishing touches are. In this section, we are going to share with you some tools of the trade to make your LLM application shine. We focus on best practices, like streaming responses, utilizing the chat history, and methods to handle and utilize prompt engineering. These allow us to craft, format, and clean our users’ prompts and the LLM’s responses under the hood, improving results and overall customer satisfaction. All this to say, building a basic application that utilizes an LLM is actually rather easy, but building a great application is a different story, and we want to build great applications.</p>
</div>
<div class="readable-text" id="p12">
<h3 class="readable-text-h3" id="sigil_toc_id_143"><span class="num-string">8.1.1</span> Streaming on the frontend</h3>
</div>
<div class="readable-text" id="p13">
<p>In chapter 6, we showed you how to stream your LLM’s response on the server side, but that is meaningless if the response isn’t streamed on the client side as well. Streaming on the client side is where it all comes together. It’s where we show the text to the users as it is being generated. This provides an attractive user experience, as it makes it appear like the text is being typed right before our eyes and gives the users a sense that the model is actually thinking about what it will write next. Not only that, but it also provides a more springy and responsive experience, as we can give a feeling of instant feedback, which encourages our users to stick around until the model finishes generating. This also helps the user to be able to see where the output is going before it gets too far so they can stop generation and reprompt.</p>
</div>
<div class="readable-text intended-text" id="p14">
<p>In listing 8.1, we show you how to do this with just HTML, CSS, and vanilla JavaScript. This application is meant to be dead simple. Many of our readers likely aren’t frontend savvy, as that isn’t the focus of this book. Those who are most likely will be using some tooling for their framework of choice anyway. But a basic application with no frills allows us to get to the core of what’s happening.</p>
</div>
<div class="readable-text intended-text" id="p15">
<p>Since the application is so simple, we opted to put all the CSS and JavaScript together into the HTML, although it would be cleaner and a best practice to separate them. The CSS defines sizing to ensure our boxes are big enough to read; we won’t bother with colors or making it look pretty. Our HTML is as simple as it gets: a form containing a text input and a Send button that returns false on submit so the page doesn’t refresh. There’s also a <code>div</code> container to contain our chat messages. Most of the JavaScript is also not that interesting; it just handles adding our conversation to the chat. However, pay attention to the <code>sendToServer</code> function, which does most of the heavy lifting: sending our prompt, receiving a readable stream, and iterating over the results.</p>
</div>
<div class="readable-text print-book-callout" id="p16">
<p><span class="print-book-callout-head">NOTE</span>  On the server side, we set up a <code>StreamingResponse</code> object, which gets converted to a <code>ReadableStream</code> on the JavaScript side. You can learn more about readable streams here: <a href="https://mng.bz/75Dg">https://mng.bz/75Dg</a>.</p>
</div>
<div class="browsable-container listing-container" id="p17">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.1</span> Streaming responses to end users </h5>
<div class="code-area-container">
<pre class="code-area">&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
    &lt;head&gt;
        &lt;meta charset="UTF-8"&gt;
        &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
        &lt;title&gt;Simple Chat App&lt;/title&gt;

        &lt;style&gt;        <span class="aframe-location"/> #1
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                box-sizing: border-box;
            }

            #message-input {
                width: 95%;
                padding: 8px;
            }

            #chat-container {
                width: 95%;
                margin: 20px auto;
                border: 1px solid #ccc;
                padding: 10px;
                overflow-y: scroll;
                max-height: 300px;
            }
        &lt;/style&gt;
    &lt;/head&gt;

    &lt;body&gt;                                <span class="aframe-location"/> #2
        &lt;form onsubmit="return false;""&gt;
            &lt;input type="text" id="message-input" placeholder="Type your message..."&gt;
            &lt;button onclick="sendMessage()" type="submit"&gt;Send&lt;/button&gt;
        &lt;/form&gt;
        &lt;div id="chat-container"&gt;&lt;/div&gt;
    &lt;/body&gt;

    &lt;script&gt;                          <span class="aframe-location"/> #3
        function sendMessage() {                     <span class="aframe-location"/> #4
            var messageInput = document.getElementById('message-input');
            var message = messageInput.value.trim();

            if (message !== '') {
                appendMessage('You: ' + message);
                messageInput.value = '';
                sendToServer(message);
            }
        }


        function appendMessage(message) {           <span class="aframe-location"/> #5
            var chatContainer = document.getElementById('chat-container');
            var messageElement = document.createElement('div');
            messageElement.textContent = message;
            chatContainer.appendChild(messageElement);
            chatContainer.scrollTop = chatContainer.scrollHeight;
            return messageElement
        }

        async function sendToServer(message) {      <span class="aframe-location"/> #6
            var payload = {
                prompt: message
            }

            const response = await fetch('http://localhost:8000/generate', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify(payload),
            });

            var responseText = 'LLM: ';
            messageElement = appendMessage(responseText);

            for await (const chunk of streamAsyncIterator(response.body)) {
                var strChunk = String.fromCharCode.apply(null, chunk);
                responseText += strChunk;
                messageElement.textContent = responseText;
            }
        }

        async function* streamAsyncIterator(stream) {     <span class="aframe-location"/> #7
            const reader = stream.getReader();
            try {
                while (true) {
                    const {done, value} = await reader.read();
                    if (done) return;
                    yield value;
                }
            }
            finally {
                reader.releaseLock();
            }
        }
    &lt;/script&gt;
&lt;/html&gt;</pre>
<div class="code-annotations-overlay-container">
     #1 Some very simple styling
     <br/>#2 Our body is simple with only three fields: a text input, send button, and container for chat.
     <br/>#3 JavaScript to handle communication with LLM and streaming response
     <br/>#4 When the Send button is pushed, moves text from input to chat box and sends the message to the LLM server
     <br/>#5 Adds new messages to the chat box
     <br/>#6 Sends prompt to the server and streams the response back as tokens are received
     <br/>#7 Simple polyfill since StreamResponse still can’t be used as an iterator by most browsers
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p18">
<p>Figure 8.2 shows screenshots of our simple application from listing 8.1. Showing words being streamed to the application would have been better in a movie or GIF, but since books don’t play GIFs, we’ll have to make do with several side-by-side screenshots instead. Regardless, the figure shows the results being streamed to the user token by token, providing a positive user experience.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p19">
<img alt="figure" height="379" src="../Images/8-2.png" width="1012"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.2</span> Screenshots of our simple application showing the response being streamed</h5>
</div>
<div class="readable-text intended-text" id="p20">
<p>There’s nothing glamorous about our little application here, and that’s partly the point. This code is easy to copy and paste and can be used anywhere a web browser can run since it’s just an HTML file. It doesn’t take much to build a quick demo app once you have an LLM service running.</p>
</div>
<div class="readable-text" id="p21">
<h3 class="readable-text-h3" id="sigil_toc_id_144"><span class="num-string">8.1.2</span> Keeping a history</h3>
</div>
<div class="readable-text" id="p22">
<p>One big problem with our simple application so far is that each message sent to our LLM is independent of other messages sent. This is a big problem because most applications that utilize an LLM do so in an interactive environment. Users will ask a question and then, based on the response, make additional questions or adjustments and clarifications to get better results. However, if you simply send the latest query as a prompt, the LLM will not have any context behind the new query. Independence is nice for coin flips, but it will make our LLM look like a birdbrain.</p>
</div>
<div class="readable-text intended-text" id="p23">
<p>What we need to do is keep a history of the conversation, both the user’s prompts and the LLM’s responses. If we do that, we can append that history to the new prompts as context. The LLM will be able to utilize this background information to make better responses. Figure 8.3 shows the overall flow of what we are trying to achieve.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p24">
<img alt="figure" height="299" src="../Images/8-3.png" width="544"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.3</span> Process flow for storing prompts and responses to a chat history, giving our model a memory of the conversation to improve outcomes</h5>
</div>
<div class="readable-text" id="p25">
<p>Now that we know what we are building, let’s take a look at listing 8.2. This time, we will be using Streamlit, a Python framework for building applications. It is simple and easy to use while still creating attractive frontends. From Streamlit, we will be utilizing a <code>chat_input</code> field so users can write and send their input, a <code>chat_message</code> field that will hold the conversation, and <code>session_state</code>, where we will create and store the <code>chat_history</code>. We will use that chat history to craft a better prompt. You’ll also notice that we continue to stream the responses, as demonstrated in the last section, but this time using Python.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p26">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">What is Streamlit? </h5>
</div>
<div class="readable-text" id="p27">
<p>Streamlit is an open-source Python library that makes it easy to create web applications for machine learning, data science, and other fields. It allows you to quickly build interactive web apps using simple Python scripts. With Streamlit, you can create dashboards, data visualizations, and other interactive tools without needing to know web development languages like HTML, CSS, or JavaScript. Streamlit automatically handles the conversion of your Python code into a web app.</p>
</div>
</div>
<div class="browsable-container listing-container" id="p28">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.2</span> An example application using chat history to improve results </h5>
<div class="code-area-container">
<pre class="code-area">import streamlit as st
import requests
import json

url = "http://localhost:8000/generate"   <span class="aframe-location"/> #1

st.title("Chatbot with History")

if "chat_history" not in st.session_state:     <span class="aframe-location"/> #2
    st.session_state.chat_history = []

for chat in st.session_state.chat_history:     <span class="aframe-location"/> #3
    with st.chat_message(chat["role"]):
        st.markdown(chat["content"])

if user_input := st.chat_input("Your question here"):    <span class="aframe-location"/> #4
    with st.chat_message("user"):                  <span class="aframe-location"/> #5
        st.markdown(user_input)

    st.session_state.chat_history.append(         <span class="aframe-location"/> #6
        {"role": "user", "content": user_input}
    )

    with st.chat_message("assistant"):    <span class="aframe-location"/> #7
        placeholder = st.empty()
        full_response = ""

        prompt = "You are an assistant who helps the user. "    <span class="aframe-location"/> #8
        "Answer their questions as accurately as possible. Be concise. "
        history = [
            f'{ch["role"]}: {ch["content"]}'
            for ch in st.session_state.chat_history
        ]
        prompt += " ".join(history)
        prompt += " assistant: "
        data = json.dumps({"prompt": prompt})

        with requests.post(url, data=data, stream=True) as r:    <span class="aframe-location"/> #9
            for line in r.iter_lines(decode_unicode=True):
                full_response += line.decode("utf-8")
                placeholder.markdown(full_response + "▌")    <span class="aframe-location"/> #10
        placeholder.markdown(full_response)

    st.session_state.chat_history.append(     <span class="aframe-location"/> #11
        {"role": "assistant", "content": full_response}
    )</pre>
<div class="code-annotations-overlay-container">
     #1 Points to your model’s API
     <br/>#2 Creates a chat history in the session state
     <br/>#3 Δisplays chat from history
     <br/>#4 Responds to user. Note: we use the walrus operator (:=) to assign the user’s input while also ensuring it is not None at the same time.
     <br/>#5 Δisplays user’s input
     <br/>#6 Adds user input to chat history
     <br/>#7 Streams response
     <br/>#8 Formats prompt adding chat history for additional context
     <br/>#9 Sends request
     <br/>#10 Adds a blinking cursor to simulate typing
     <br/>#11 Adds LLM response to chat history
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p29">
<p>Figure 8.4 is a screenshot capturing the LLM app that we just built. While our first example was quite ugly, you can see that Streamlit automatically creates a nice user interface, complete with finishing touches, like a picture of a human face for the user and a robot face for our LLM assistant. You’ll also notice that the model is taking in and comprehending the conversation history—albeit giving terrible responses. If we want to get better responses, one thing to be sure of is that your LLM has been trained on conversation data.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p30">
<img alt="figure" height="720" src="../Images/8-4.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.4</span> Screenshot of our Streamlit app utilizing a chat history</h5>
</div>
<div class="readable-text" id="p31">
<p>Of course, utilizing the history leads to some problems. The first is that users can have relatively long conversations with our bot, but we are still limited in the token length we can feed to the model, and the longer the input, the longer the generation takes. At some point, the history will begin to be too long. The simplest approach to solving this problem is to drop older messages in favor of newer ones. Sure, our model may forget important details or instructions at the start of our conversation, but humans also tend to have a recency bias in conversations, so this tends to be OK—except, of course, for the fact that humans tend to expect computers never to forget anything.</p>
</div>
<div class="readable-text intended-text" id="p32">
<p>A more robust solution is to use the LLM to summarize the chat history and use the summary as context to our users’ queries instead of the full chat history. LLMs are often quite good at highlighting important pieces of information from a body of text, so this can be an effective way to compress a conversation. Compression can be done on demand or run as a background process. Figure 8.5 illustrates the summarization workflow for chat history compression.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p33">
<img alt="figure" height="514" src="../Images/8-5.png" width="517"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.5</span> A process flow for an app with chat history utilizing summarization for chat history compression</h5>
</div>
<div class="readable-text" id="p34">
<p>There are other strategies you can explore, as well as mixing and matching multiple methods. Another idea is to embed each chat and perform a search for relevant previous chat messages to add to the prompt context. But no matter how you choose to shorten the chat history, details are bound to be lost or forgotten the longer a conversation goes on or the larger the prompts and responses are.</p>
</div>
<div class="readable-text" id="p35">
<h3 class="readable-text-h3" id="sigil_toc_id_145"><span class="num-string">8.1.3</span> Chatbot interaction features</h3>
</div>
<div class="readable-text" id="p36">
<p>Chatting with an LLM bot isn’t like chatting with your friend. For one, the chatbot is always available and waiting for us to talk to it, so we can expect a response right away. There shouldn’t be opportunities for users to spam multiple messages to our bot before receiving feedback. But let’s face it, in the real world, there are connection problems or bad internet, the server could be overwhelmed, and there are a myriad of other reasons a request might fail. These differences encourage us to interact with a chatbot differently, and we should ensure we add several features for our users to improve their experience. Let’s consider several of them now:</p>
</div>
<ul>
<li class="readable-text" id="p37"> <em>Fallback response</em> —A response to give when an error occurs. To keep things clean, you’ll want to ensure a 1:1 ratio of LLM responses for every user query in your chat history. A fallback response ensures our chat history is clean and gives the user instructions on the best course of action, like trying again in a few minutes. Speaking of which, you should also consider disabling the Submit button when receiving a response to prevent weird problems from asynchronous conversations and out-of-order chat history. </li>
<li class="readable-text" id="p38"> <em>Stop button</em> —Interrupts a response midstream. An LLM can often be long-winded, continuing to respond long after answering the user’s questions. Often, it misunderstands a question and starts to answer it incorrectly. In these cases, it’s best to give the user a Stop button so they can interrupt the model and move on. This button is a simple cost-saving feature since we usually pay for output per token one way or another. </li>
<li class="readable-text" id="p39"> <em>Retry button</em> —Resends the last query and replaces the response. LLMs have a bit of randomness to them, which can be great for creative writing, but it means they may respond unfavorably even to prompts they have responded correctly to multiple times before. Since we add the LLM chat history to new prompts to give context, a retry button allows users to attempt to get a better result and keep the conversation moving in the right direction. While retrying, it can make sense to adjust our prompting hyperparameters, for example, reducing temperature each time a user retries. This can help push the responses in the direction the user is likely expecting. Of course, this likely isn’t the best move if they are retrying because of a bad internet connection, so you’ll need to consider the adjustments carefully. </li>
<li class="readable-text" id="p40"> <em>Delete button</em> —Removes portions of the chat history. As mentioned, the chat history is used as context in future responses, but not every response is immediately identifiable as bad. We often see red herrings. For example, a chat assistant used while coding might hallucinate functions or methods that don’t exist, which can lead the conversation down a path that is hard to recover from. Of course, depending on your needs, the solution could be a soft delete, where we only remove it from the frontend and prompting space but not the backend. </li>
<li class="readable-text" id="p41"> <em>Feedback form</em> —A way to collect feedback on users’ experience. If you are training or finetuning your own LLMs, this data is highly valuable, as it can help your team improve results on the next training iteration. This data can often easily be applied when using RLHF. Of course, you won’t want to apply it directly, but first clean and filter out troll responses. Also, even if you aren’t training, it can help your team make decisions to switch models, improve prompting, and identify edge cases. </li>
</ul>
<div class="readable-text" id="p42">
<p>In listing 8.3, we show how to use Gradio to set up an easy chatbot app. Gradio is an open source library for quickly creating customizable user interfaces for data science demos and web applications. It’s highly popular for its ease of integration within Juptyer notebooks, making it easy to create interfaces and edit your web app in a familiar environment. To create a chatbot with Gradio, we’ll use the <code>ChatInterface</code> and give it a function to make our API request. You’ll notice that Gradio expects the history to be part of the <code>generate</code> function, and streaming is just a matter of ensuring the function is a generator.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p43">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">What is Gradio? </h5>
</div>
<div class="readable-text" id="p44">
<p>Gradio is an open-source Python library that allows you to quickly create customizable UI components around your machine-learning models. It provides a simple interface for building interactive web-based applications for your models without requiring you to write any HTML, CSS, or JavaScript code. With Gradio, you can create input forms for your models, display the results, and even share your models with others through a web interface.</p>
</div>
</div>
<div class="browsable-container listing-container" id="p45">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.3</span> Local LLM Gradio chat app with Stop, Retry, and Undo </h5>
<div class="code-area-container">
<pre class="code-area">import gradio as gr
import requests
import json

url = "http://localhost:8000/generate"     <span class="aframe-location"/> #1


def generate(message, history):
    history_transformer_format = history + [[message, ""]]
    messages = "".join(
        [
            "".join(["\n&lt;human&gt;:" + h, "\n&lt;bot&gt;:" + b])
            for h, b in history_transformer_format
        ]
    )
    data = json.dumps({"prompt": messages})

    full_response = ""
    with requests.post(url, data=data, stream=True) as r:  <span class="aframe-location"/> #2
        for line in r.iter_lines(decode_unicode=True):
            full_response += line.decode("utf-8")
            yield full_response + "▌"             <span class="aframe-location"/> #3
        yield full_response


gr.ChatInterface(generate, theme="soft").queue().launch()</pre>
<div class="code-annotations-overlay-container">
     #1 Points to your model’s API
     <br/>#2 Sends request
     <br/>#3 Adds a blinking cursor to simulate typing
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p46">
<p>You can see how simple this code is, with very few lines needed. Gradio does all the heavy lifting for us. You might also be wondering where all our interaction features are. Well, the good news is that Gradio automatically adds most of these features for us. Don’t believe me? Check out the app we just created in figure 8.6.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p47">
<img alt="figure" height="474" src="../Images/8-6.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><strong><span class="num-string">Figure 8.6</span> Screenshot of our Gradio app, including interaction features Stop, Retry, and Undo for better ease of use</strong></h5>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p48">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Chainlit: An application builder just for LLMs</h5>
</div>
<div class="readable-text" id="p49">
<p>We have shown you how to build LLM applications with several different tools: Streamlit, Gradio, and even vanilla HTML and JavaScript. There are many great tools out there, and we can’t give personal attention to each one. But one more tool we think many of our readers will be interested in is Chainlit. Chainlit is a tool specifically built for building LLM applications and comes with most features out of the box, including ones not discussed here, like themes, CSS customization, authentication, and cloud hosting. It is likely one of the fastest ways to get up and running.</p>
</div>
</div>
<div class="readable-text" id="p50">
<p>Each quality-of-life improvement you can add to your application will help it stand out above the competition and potentially save you money. For the same reason, you should consider using a token counter, which we cover next.</p>
</div>
<div class="readable-text" id="p51">
<h3 class="readable-text-h3" id="sigil_toc_id_146"><span class="num-string">8.1.4</span> Token counting</h3>
</div>
<div class="readable-text" id="p52">
<p>One of the most basic but valuable pieces of information you can gather to offer a great user experience is the number of submitted tokens. Since LLMs have token limits, we’ll need to ensure the users’ prompts don’t exceed those limits. Giving feedback early and often will provide a better user experience. No one wants to type a long query only to find that it’s too much upon submitting.</p>
</div>
<div class="readable-text intended-text" id="p53">
<p>Counting tokens also allows us to better prompt-engineer and improve results. For example, in a Q&amp;A bot, if the user’s question is particularly short, we can add more context by extending how many search results our retrieval-augmented generation (RAG) system will return. If their question is long, we’ll want to limit it and ensure we have enough space to append our own context.</p>
</div>
<div class="readable-text intended-text" id="p54">
<p>Tiktoken is just such a library to help with this task. It’s an extremely fast BPE tokenizer built specifically for OpenAI’s models. The package has been ported to multiple languages, including tiktoken-go for Golang, tiktoken-rs for Rust, and several others. In the next listing, we show a basic example of how to use it. It’s been optimized for speed, which allows us to encode and count tokens quickly, which is all we need to do.</p>
</div>
<div class="browsable-container listing-container" id="p55">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.4</span> Using tiktoken to count tokens </h5>
<div class="code-area-container">
<pre class="code-area">import tiktoken

encoding = tiktoken.get_encoding("cl100k_base")
print(encoding.encode("You're users chat message goes here."))     
# [2675, 2351, 3932, 6369, 1984, 5900, 1618, 13]
def count_tokens(string: str) -&gt; int:
    encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(string))

num_tokens = count_tokens("You're users chat message goes here.")
print(num_tokens)       
# 8</pre>
</div>
</div>
<div class="readable-text" id="p56">
<p>Of course, the reader who hasn’t skipped ahead will recognize a few problems with using tiktoken, mainly because it’s built with OpenAI’s encoders in mind. If you are using your own tokenizer (which we recommend), it’s not going to be very accurate. We have seen several developers—out of laziness or not knowing a better solution—still use it for other models. Generally, they saw counts within ±5–10 tokens per 1,000 tokens when using tiktoken results for other models using similar BPE tokenizers. To them, the speed and latency gains justified the inaccuracy, but this was all word of mouth, so take it with a grain of salt.</p>
</div>
<div class="readable-text intended-text" id="p57">
<p>If you are using a different type of tokenizer, like SentencePiece, it’s often better to create your own token counter. For example, we do just that in our project in chapter 10. As you can guess, the code follows the same pattern of encoding the string and counting the tokens. The hard part comes when porting it to the language that needs to run the counter. To do so, compile the tokenizer like you would any other ML model, as we discussed in section 6.1.1.</p>
</div>
<div class="readable-text" id="p58">
<h3 class="readable-text-h3" id="sigil_toc_id_147"><span class="num-string">8.1.5</span> RAG applied</h3>
</div>
<div class="readable-text" id="p59">
<p>RAG is an excellent way to add context and outside knowledge to your LLM to improve the accuracy of your results. In the last chapter, we discussed it in the context of a backend system. Here, we will be discussing it from the frontend perspective. Your RAG system can be set up on either side, each with its own pros and cons.</p>
</div>
<div class="readable-text intended-text" id="p60">
<p>Setting up RAG on the backend ensures a consistent experience for all users and gives us greater control as developers of how exactly the context data will be used. It also provides a bit more security to the data stored in the vector database, as it’s only accessible to the end users through the LLM. Of course, through clever prompt injection, it could potentially still be scraped, but it is still much more secure than simply allowing users to query your data directly.</p>
</div>
<div class="readable-text intended-text" id="p61">
<p>RAG is more often set up on the frontend because doing so allows developers to take whatever generic LLM is available and insert business context. You don’t need to finetune a model on your dataset if you give the model your dataset at run time. Thus, RAG becomes a system to add personality and functionality to our LLM application versus simply being a tool to ensure the accuracy of results and reduce hallucinations.</p>
</div>
<div class="readable-text intended-text" id="p62">
<p>In section 6.1.8, we showed you how to set up a RAG system; now we will show you how to utilize it for efficient query augmentation. In listing 8.5, we show you how to access and use the vector store we set up previously. We will continue to use OpenAI and Pinecone from our last example. We will also use LangChain, a Python framework which we discovered in the last chapter, to help create LLM applications.</p>
</div>
<div class="browsable-container listing-container" id="p63">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.5</span> RAG on the frontend</h5>
<div class="code-area-container">
<pre class="code-area">import os
import pinecone
from langchain.chains import RetrievalQA
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.chat_models import ChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Pinecone

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")          <span class="aframe-location"/> #1
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")     <span class="aframe-location"/> #2


index_name = "pincecone-llm-example"     <span class="aframe-location"/> #3
index = pinecone.Index(index_name)
embedder = OpenAIEmbeddings(
    model="text-embedding-ada-002", openai_api_key=OPENAI_API_KEY
)
text_field = "text"
vectorstore = Pinecone(index, embedder.embed_query, text_field)

query = "Who was Johannes Gutenberg?"     <span class="aframe-location"/> #4
vectorstore.similarity_search(
    query, k=3                 <span class="aframe-location"/> #5
)

llm = ChatOpenAI(              <span class="aframe-location"/> #6
    openai_api_key=OPENAI_API_KEY,
    model_name="gpt-3.5-turbo",
    temperature=0.0,
)

qa = RetrievalQA.from_chain_type(        <span class="aframe-location"/> #7
    llm=llm, chain_type="stuff", retriever=vectorstore.as_retriever()
)
qa.run(query)
qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(    <span class="aframe-location"/> #8
    llm=llm, chain_type="stuff", retriever=vectorstore.as_retriever()
)
qa_with_sources(query)</pre>
<div class="code-annotations-overlay-container">
     #1 Gets OpenAI API key from 
     <a href="https://platform.openai.com">platform.openai.com</a>
<br/>#2 Finds API key in console at app.pinecone.io
     <br/>#3 Sets up vectorstore
     <br/>#4 Makes a query
     <br/>#5 Our search query; returns the three most relevant docs
     <br/>#6 Now let’s use these results to enrich our LLM prompt; sets up the LLM
     <br/>#7 Runs query with vectorstore
     <br/>#8 Includes Wikipedia sources
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p64">
<p>We think the most impressive part of this code is the fact that LangChain has a chain simply named “stuff” because, presumably, they couldn’t think of anything better. (If you want to learn more about the cryptically named module “stuff,” you can find the docs at <a href="https://mng.bz/OBER">https://mng.bz/OBER</a>.) But in actuality, the most impressive thing about this code is that we just have to define our LLM and vector store connections, and we are good to go to start making queries. Simple.</p>
</div>
<div class="readable-text" id="p65">
<h2 class="readable-text-h2" id="sigil_toc_id_148"><span class="num-string">8.2</span> Edge applications</h2>
</div>
<div class="readable-text" id="p66">
<p>So far, we have discussed building LLM applications, assuming we will simply be using an API—one we deployed, but an API nonetheless. However, there are lots of situations where you might want to run the model on the local device inside the application itself. Doing so brings several challenges: mainly, we need to get a model small enough to transfer and run it on the edge device. We also need to be able to run it in the local environment, which likely doesn’t have an accelerator or GPU and may not even support Python—for example, running an app in a user’s web browser with JavaScript, in an Android app on a mobile phone with Java, or on limited hardware like a Raspberry Pi.</p>
</div>
<div class="readable-text intended-text" id="p67">
<p>In chapter 6, we started discussing the building blocks you need to work with edge devices. We showed you how to compile a model, giving examples using TensorRT or ONNX Runtime. TensorRT, coming from NVIDIA, is going to serve you better on a server with expensive NVIDIA hardware to go with it, so it is less useful for edge development. ONNX Runtime is a bit more flexible, but when working with edge devices, llama.cpp is often a better solution for LLMs, and it follows the same flow: compile the model to the correct format, move that model to the edge device, download and install the SDK for your language, and run the model. Let’s take a closer look at these steps for llama.cpp.</p>
</div>
<div class="readable-text intended-text" id="p68">
<p>The llama.cpp project started with the goal of converting an LLM to something that could be run on a MacBook without a GPU (Apple silicon chips are notorious for poor compatibility for many projects). Initially working to quantize the LLaMA model and store it in a binary format that could be used by the C++ language, the project has grown to support a couple of dozen LLM architectures and all major OS platforms, has bindings for a dozen languages, and even CUDA, metal, and OpenCL GPU backend support. Llama.cpp has created two different formats to store the quantized LLMs: the first GPT-Generated Model Language (GGML), which was later abandoned for the better GPT-Generated Unified Format (GGUF).</p>
</div>
<div class="readable-text intended-text" id="p69">
<p>To use llama.cpp, the first thing we’ll need is a model stored in the GGUF format. To convert your own, you’d need to clone the llama.cpp project, install the dependencies, and then run the convert script that comes with the project. The steps have changed frequently enough that you’ll want to consult the latest information in the repo, but currently, it would look like</p>
</div>
<div class="browsable-container listing-container" id="p70">
<div class="code-area-container">
<pre class="code-area">$ git clone https://github.com/ggerganov/llama.cpp.git
$ cd llama.cpp
$ pip install -r requirments/requirements-convert.txt
$ python convert.py -h</pre>
</div>
</div>
<div class="readable-text" id="p71">
<p>Of course, that last command simply displays the convert script’s Help menu for you to investigate the options and does not actually convert a model. For our purposes, we’ll download an already converted model. We briefly mentioned Tom Jobbins (TheBloke) in chapter 6, the man who has converted thousands of models, quantizing and finetuning them so they are in a state ready for use. All you have to do is download them from the Hugging Face Hub. So we’ll do that now. First, we’ll need the <code>huggingface-cli</code>, which comes as a dependency with most of Hugging Face’s Python packages, so you probably already have it, but you can install it directly as well. Then we’ll use it to download the model:</p>
</div>
<div class="browsable-container listing-container" id="p72">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$ pip install -U huggingface_hub
$ huggingface-cli download TheBloke/WizardCoder-Python-7B-V1.0-GGUF --
<span class="">↪</span> local-dir ./models --local-dir-use-symlinks False --include='*Q2_K*gguf'</pre>
</div>
</div>
<div class="readable-text" id="p73">
<p>Here, we are downloading the WizardCoder-7B model that has already been converted to a GGUF format by TheBloke. We are going to save it locally in the models directory. We won’t use symbolic links (symlinks), meaning the model will actually exist in the folder we choose. Normally, <code>huggingface-cli</code> would download it to a cache directory and create a symlink to save space and avoid downloading models multiple times across projects. Lastly, the Hugging Face repo contains multiple versions of the model in different quantized states; here, we’ll select the 2-bit quantized version with the <code>include</code> flag. This extreme quantization will degrade the performance of the quality of our output for the model, but it’s the smallest model available in the repo (only 2.82 GB), which makes it great for demonstration purposes.</p>
</div>
<div class="readable-text intended-text" id="p74">
<p>Now that we have our model, we need to download and install the bindings for our language of choice and run it. For Python, that would mean installing <code>llama-cpp-python</code> via <code>pip</code>. In listing 8.6, we show you how to use the library to run a GGUF model. It’s pretty straightforward, with just two steps: load the model and run it. On one author’s CPU, it ran a little slower than about a token per second, which isn’t fast but impressive enough for a 7B parameter model without an accelerator.</p>
</div>
<div class="browsable-container listing-container" id="p75">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.6</span> Using llama.cpp to run a quantized model on a CPU </h5>
<div class="code-area-container">
<pre class="code-area">import time
from llama_cpp import Llama

llm = Llama(model_path="./models/wizardcoder-python-7b-v1.0.Q2_K.gguf")

start_time = time.time()
output = llm(
    "Q: Write python code to reverse a linked list. A: ",
    max_tokens=200,
    stop=["Q:"],
    echo=True,
)
end_time = time.time()

print(output["choices"])</pre>
</div>
</div>
<div class="readable-text" id="p76">
<p>The results are</p>
</div>
<div class="browsable-container listing-container" id="p77">
<div class="code-area-container">
<pre class="code-area"># [
#     {'text': "Q: Write python code to reverse a linked list. A: 
#         class Node(object):
#             def __init__(self, data=None):
#                 self.data = data
#                 self.next = None

#         def reverse_list(head):
#             prev = None
#             current = head
#             while current is not None:
#                 next = current.next
#                 current.next = prev
#                 prev = current
#                 current = next
#             return prev
#             # example usage;
#             # initial list
#         head = Node('a')     
#         head.next=Node('b')
#         head.next.next=Node('c')
#         head.next.next.next=Node('d')
#         print(head)
#          reverse_list(head) # call the function
#         print(head)
# Expected output: d-&gt;c-&gt;b-&gt;a",
#     'index': 0,         
#     'logprobs': None,
#     'finish_reason': 'stop'
#     }
# ]

print(f"Elapsed time: {end_time - start_time:.3f} seconds")
# Elapsed time: 239.457 seconds</pre>
</div>
</div>
<div class="readable-text" id="p78">
<p>While this example was in Python, there are bindings for Go, Rust, Node.js, Java, React Native, and more. Llama.cpp gives us all the tools we need to run LLMs in otherwise impossible environments.</p>
</div>
<div class="readable-text" id="p79">
<h2 class="readable-text-h2" id="sigil_toc_id_149"><span class="num-string">8.3</span> LLM agents</h2>
</div>
<div class="readable-text" id="p80">
<p>At this point in the book, we can finally discuss LLM agents. Agents are what most people are talking about when they start worrying about AI taking their jobs. If you think back to the last chapter, we showed how, with some clever prompt engineering and tooling, we could train models to answer multistep questions requiring searching for information and running calculations. Agents do the same thing on steroids. Full LLM applications are designed not just to answer multistep questions but to accomplish multistep tasks. For example, a coding agent could not only answer complicated questions about your code base but also edit it, submit PRs, review PRs, and write full projects from scratch.</p>
</div>
<div class="readable-text intended-text" id="p81">
<p>Agents do not differ from other language models in any meaningful way. The big differences all go into the system surrounding and supporting the LLM. LLMs are, fundamentally, closed search systems. They can’t access anything they weren’t trained on explicitly. So for example, if we were to ask Llama 2, “How old was Justin Bieber the last time the Patriots won the Superbowl?” we would be dependent on Meta having trained that model on incredibly up-to-date information. There are three components that make up an agent:</p>
</div>
<ul>
<li class="readable-text" id="p82"> <em>LLM</em> —No explanation necessary. By now, you know what these are and why they’re needed. </li>
<li class="readable-text" id="p83"> <em>Memory</em> —Some way of reintroducing the LLM to what has happened at each step up to that point. Memory goes a long way toward agents performing well. This is the same idea as feeding in the chat history, but the model needs something more than just the literal history of events. There are several ways of completing this: 
    <ul>
<li> <em>Memory buffer</em> —Passes in all of the text that’s come before. Not recommended, as you’ll hit context limits quickly, and the “lost in the middle” problem will exacerbate this. </li>
<li> <em>Memory summarization</em> —Has the LLM take another pass at the text to summarize it for its own memory. Works pretty well; however, at a minimum, it doubles latency, and summarization will delete finer details faster than anyone would like. </li>
<li> <em>Structured memory storage</em> —Thinks ahead and creates a system you can draw from to get the actual best info for the model. It can be related to chunking articles and searching for an article title and then retrieving the most relevant chunk, or perhaps chaining retrievals to find the most relevant keywords or to make sure that the query is contained in the retrieval output. We recommend structured memory storage the most because even though it’s the hardest to set up, it achieves the best results in every scenario. </li>
</ul></li>
<li class="readable-text" id="p84"> <em>External data retrieval tools</em> —The core of agent behavior. These tools give your LLM the ability to take actions, which allows it to perform agent-like tasks. </li>
</ul>
<div class="readable-text" id="p85">
<p>We’ve covered a lot in this book, and agents are a bit of a culmination of much of what we’ve covered. They can be quite tricky to build, so to help you, we’ll break down the steps and give several examples. First, we’ll make some tools, then initialize some agents, and finally create a custom agent, all on our own. Throughout the process, you’ll see why it’s particularly difficult to get agents to work effectively and especially why LangChain and Guidance are great for getting started and difficult to get up and running.</p>
</div>
<div class="readable-text intended-text" id="p86">
<p>In listing 8.7, we start off easy by demonstrating using some tools via LangChain. This example uses the Duck Duck Go search tool and the YouTube search tool. Notice that the LLM will simply give them a prompt, and the tools will handle the search and summary of results. </p>
</div>
<div class="browsable-container listing-container" id="p87">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.7</span> LangChain search tools example </h5>
<div class="code-area-container">
<pre class="code-area">from langchain.tools import DuckDuckGoSearchRun, YouTubeSearchTool

search = DuckDuckGoSearchRun()    <span class="aframe-location"/> #1
hot_topic = search.run(
    "Tiktoker finds proof of Fruit of the Loom cornucopia in the logo"
)

youtube_tool = YouTubeSearchTool()
fun_channel = youtube_tool.run("jaubrey", 3)

print(hot_topic, fun_channel)</pre>
<div class="code-annotations-overlay-container">
     #1 Example of using tools
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p88">
<p>The generated text is </p>
</div>
<div class="browsable-container listing-container" id="p89">
<div class="code-area-container">
<pre class="code-area"># Rating: False About this rating If asked to describe underwear
# manufacturer Fruit of the Loom's logo from memory, some will invariably
# say it includes — or at least included at some point in... A viral claim
# recently surfaced stating that Fruit of the Loom, the American underwear
# and casualwear brand, had a cornucopia in their logo at some point in the
# past. It refers to a goat's... The Fruit of the Loom Mandela Effect is
# really messing with people's memories of the clothing company's iconic
# logo.. A viral TikTok has thousands of people not only thinking about what
# they remember the logo to look like, but also has many searching for proof
# that we're not all losing our minds.. A TikTok Creator Is Trying To Get To
# The Bottom Of The Fruit Of The Loom Mandela Effect What Is 'The Mandela
# Effect?' To understand why people care so much about the Fruit of the Loom
# logo, one must first understand what the Mandela Effect is in the first
# place. It's a slang term for a cultural phenomenon in which a large group
# of people shares false memories of past events. About Fruit of the Loom
# Cornucopia and Fruit of the Loom Mandela Effect refer to the Mandela
# Effect involving a large number of people remembering the clothing company
# Fruit of the Loom having a cornucopia on its logo despite the logo never
# having the item on it.
# ['https://www.youtube.com/watch?v=x81gguSPGcQ&amp;pp=ygUHamF1YnJleQ%3D%3D',
#'https://www.youtube.com/watch?v=bEvxuG6mevQ&amp;pp=ygUHamF1YnJleQ%3D%3D']</pre>
</div>
</div>
<div class="readable-text" id="p90">
<p>Next, we’ll demonstrate running an agent locally. In these examples, we use llama.cpp again; however, this time, we will use an instruction-based model, the 4-bit quantized Mistral 7B Instruct model—a great open source model. You can get the model we are using by running the following command. Notice the similarities to when we pulled the WizardCoder model back in section 8.2:</p>
</div>
<div class="browsable-container listing-container" id="p91">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$ huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF --local-
<span class="">↪</span> dir ./models --local-dir-use-symlinks False --include='*Q4_0*gguf'</pre>
</div>
</div>
<div class="readable-text" id="p92">
<p>In listing 8.8, we demonstrate running two different types of agents you’ll likely find useful. The first agent generates some Python, runs it, and attempts to debug any problems it runs into. The second agent reads and analyzes a CSV file. For this agent, we’ll use the Slack dataset we pulled back in chapter 4. Pay attention to the responses, and make a wager on whether they do a good job.</p>
</div>
<div class="browsable-container listing-container" id="p93">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.8</span> LangChain Python and CSV agents</h5>
<div class="code-area-container">
<pre class="code-area">from langchain.llms import LlamaCpp
from langchain.agents import AgentType
from langchain_experimental.agents import create_csv_agent
from langchain_experimental.agents.agent_toolkits import create_python_agent
from langchain_experimental.tools import PythonREPLTool


llm = LlamaCpp(
    model_path="./models/mistral-7b-instruct-v0.1.Q4_0.gguf",
    n_gpu_layers=0,    <span class="aframe-location"/> #1
    n_batch=512,
    n_ctx=32768,              <span class="aframe-location"/> #2
    verbose=False,
)

agent = create_python_agent(    <span class="aframe-location"/> #3
    llm=llm,
    tool=PythonREPLTool(),
    verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    handle_parsing_errors=True,
)

agent.run(
    """Using Python_REPL please write a neural network in Pytorch.
          Use Python_REPL as the Action and your code as the Action Input.
          Use synthetic data from a normal distribution.
          Train for 1000 epochs and print every 100 epochs.
          Return a prediction for x = 5."""
)</pre>
<div class="code-annotations-overlay-container">
     #1 1 if NEON, any number if CUBLAS else 0
     <br/>#2 Context window for the model
     <br/>#3 An agent that will generate Python code and execute it
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p94">
<p>The output is </p>
</div>
<div class="browsable-container listing-container" id="p95">
<div class="code-area-container">
<pre class="code-area"># &gt; Entering new AgentExecutor chain…
# This is a task about creating and training a neural network. I should
# start by importing Pytorch and defining the synthetic data. Then I will
# define the neural network and train it for 1000 epochs. Finally, I will
# print the prediction for x = 5.
# Action: Python_REPL
# Action Input:
# ```python
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils import data
# from torchvision.datasets import make_classification
# define synthetic data from normal distribution
# data = make_classification(n_samples=1000, n_features=10, n_informative=5,
# random_state=42)
# X = data['features']
# y = data['targets']
# ```Python REPL can execute arbitrary code. Use with caution.
# Observation: ImportError("cannot import name 'make_classification' from
#'torchvision.datasets'
# Thought: It seems like the code is trying to use a function that does not
# exist, I will check the import statement.
# Action: Python_REPL
# Action Input:
# ```python
# fix import
# import torch.utils.data as data     
# from torchvision.datasets import make_classification
# ```
# Observation: ImportError("cannot import name 'make_classification' from
#'torchvision.datasets'
# Thought: It still seems like the code is trying to use a function that
# does not exist, I will make sure that the function name matches the
# correct import statement.
# Action: Python_REPL
# Action Input:
# ```python
# fix import
# from torchvision.datasets.make_classification import make_classification 
# data = make_classification(n_samples=1000, n_features=10, n_informative=5,
# random_state=42)
# X = data['features']
# y = data['targets']
# ```
# Observation: ImportError("cannot import name 'MakeClassification' from
#'torchvision.datasets'
# Thought:
# &gt; Finished chain.</pre>
</div>
</div>
<div class="readable-text" id="p96">
<p>We continue with </p>
</div>
<div class="browsable-container listing-container" id="p97">
<div class="code-area-container">
<pre class="code-area">agent = create_csv_agent(   <span class="aframe-location"/> #1
    llm,
    "./data/Slack_Dataset.csv",
    verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    handle_parsing_errors=True,
)
agent.run(
    """Using python_repl_ast please tell me whether the user polite in their
    messages. Use python_repl_ast as the Action and the command as the
    Action input."""
)</pre>
<div class="code-annotations-overlay-container">
     #1 An agent that will read a CSV and analyze it
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p98">
<p>The generated text is </p>
</div>
<div class="browsable-container listing-container" id="p99">
<div class="code-area-container">
<pre class="code-area"># &gt; Entering new AgentExecutor chain…
# Action: python_repl_ast
# Action Input: df['text'].str.contains('thank you')
# Observation:
# 0    False
# 1    False
# 2    False
# 3    False
# 4    False
#  …
# 286    False
# 287    False
# 288    False
# 289    False
# 290    False
# Name: text, Length: 291, dtype: bool
# Thought: It seems the user was not polite in their messages.
# Final Answer: The user was not polite in their messages.
# &gt; Finished chain.</pre>
</div>
</div>
<div class="readable-text" id="p100">
<p>Well, what do you think? Did either agent do a very good job? You’re likely thinking “No,” which should reassure you that AI isn’t going to take your job anytime soon. The Python agent wrote a PyTorch script that completely depends on a <code>make_ classification()</code> function that doesn’t exist, and the CSV agent decided that being polite is equivalent to saying, “Thank you.” Not a bad guess, but simply not a robust solution. Sure, part of the problem is likely the model we are using; a bigger one like GPT-4 might do better. We’ll leave it as an exercise for the reader to compare.</p>
</div>
<div class="readable-text intended-text" id="p101">
<p>Moving on, in listing 8.9, we build our own agent. We’ll define the tools the agent has access to, set up a memory space for the agent, and then initialize it. Next, we’ll define a system prompt so the agent knows how it should behave, making sure to explain to it what tools it has at its disposal and how to use them. We’ll also utilize few-shot prompting and instruction to give us the best chance of seeing good results. Lastly, we’ll run the agent. Let’s take a look.</p>
</div>
<div class="browsable-container listing-container" id="p102">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.9</span> Agents and agent behavior</h5>
<div class="code-area-container">
<pre class="code-area">from langchain.llms import LlamaCpp
from langchain.chains.conversation.memory import (
    ConversationBufferWindowMemory,
)
from langchain.agents import load_tools, initialize_agent, Tool
from langchain_experimental.tools import PythonREPLTool
from langchain.tools import DuckDuckGoSearchRun, YouTubeSearchTool
llm = LlamaCpp(
    model_path="./models/mistral-7b-instruct-v0.1.Q4_0.gguf",
    n_gpu_layers=0,       <span class="aframe-location"/> #1
    n_batch=512,
    n_ctx=32768,         <span class="aframe-location"/> #2
    verbose=False,
)

search = DuckDuckGoSearchRun()    <span class="aframe-location"/> #3
duckduckgo_tool = Tool(
    name="DuckDuckGo Search",
    func=search.run,
    description="Useful for when an internet search is needed",
)
youtube_tool = YouTubeSearchTool()
coding_tool = PythonREPLTool()

tools = load_tools(["llm-math"], llm=llm)
tools += [duckduckgo_tool, youtube_tool, coding_tool]

memory = ConversationBufferWindowMemory(    <span class="aframe-location"/> #4
    memory_key="chat_history",
    k=5,
    return_messages=True,
    output_key="output",
)

agent = initialize_agent(    <span class="aframe-location"/> #5
    tools=tools,
    llm=llm,
    agent="chat-conversational-react-description",
    verbose=True,
    memory=memory,
    handle_parsing_errors=True,
)

B_INST, E_INST = "[INST]", "[/INST]"       <span class="aframe-location"/> #6
B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"

sys_msg = (    <span class="aframe-location"/> #7
    "&lt;s&gt;"
    + B_SYS
    + """Assistant is a expert JSON builder designed to assist with a wide \
range of tasks.

Assistant is able to respond to the User and use tools using JSON strings \
that contain "action" and "action_input" parameters.

All of Assistant's communication is performed using this JSON format.

Assistant can also use tools by responding to the user with tool use \
instructions in the same "action" and "action_input" JSON format. Tools \
available to Assistant are:

- "Calculator": Useful for when you need to answer questions about math.
  - To use the calculator tool, Assistant should write like so:
    ```json
    {{"action": "Calculator",
      "action_input": "sqrt(4)"}}
    ```
- "DuckDuckGo Search": Useful for when an internet search is needed.
  - To use the duckduckgo search tool, Assistant should write like so:
    ```json
    {{"action": "DuckDuckGo Search",
      "action_input": "When was the Jonas Brothers' first concert"}}

Here are some previous conversations between the Assistant and User:

User: Hey how are you today?
Assistant: ```json
{{"action": "Final Answer",
 "action_input": "I'm good thanks, how are you?"}}
```
User: I'm great, what is the square root of 4?
Assistant: ```json
{{"action": "Calculator",
 "action_input": "sqrt(4)"}}
```
User: 2.0
Assistant: ```json
{{"action": "Final Answer",
 "action_input": "It looks like the answer is 2!"}}
```
User: Thanks, when was the Jonas Brothers' first concert?
Assistant: ```json
{{"action": "DuckDuckGo Search",
 "action_input": "When was the Jonas Brothers' first concert"}}
```
User: 12.0
Assistant: ```json
{{"action": "Final Answer",
 "action_input": "They had their first concert in 2005!"}}
```
User: Thanks could you tell me what 4 to the power of 2 is?
Assistant: ```json
{{"action": "Calculator",
 "action_input": "4**2"}}
```
User: 16.0
Assistant: ```json
{{"action": "Final Answer",
 "action_input": "It looks like the answer is 16!"}}
```

Here is the latest conversation between Assistant and User."""
    + E_SYS
)

new_prompt = agent.agent.create_prompt(system_message=sys_msg, tools=tools)<span class="aframe-location"/> #8
agent.agent.llm_chain.prompt = new_prompt

instruction = (     <span class="aframe-location"/> #9
    B_INST
    + " Respond to the following in JSON with 'action' and 'action_input' "
    "values "
    + E_INST
)
human_msg = instruction + "\nUser: {input}"
agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg

agent.run(            <span class="aframe-location"/> #10
    "Tell me how old Justin Beiber was when the Patriots last won the "
    "Superbowl."
)</pre>
<div class="code-annotations-overlay-container">
     #1 1 if NEON, any number if CUBLAS, else 0
     <br/>#2 Context window for the model
     <br/>#3 Δefines our own agent tools
     <br/>#4 Δefines our agent’s memory
     <br/>#5 Sets up and initializes our custom agent
     <br/>#6 Special tokens used by llama 2 chat
     <br/>#7 Creates the system prompt
     <br/>#8 Adds system prompt to agent
     <br/>#9 Adds instruction to agent
     <br/>#10 Runs with user input
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p103">
<p>Remember that for this, we asked the model to respond in JSON:</p>
</div>
<div class="browsable-container listing-container" id="p104">
<div class="code-area-container">
<pre class="code-area">#&gt; Entering new AgentExecutor chain…
#Assistant: {
# "action": "DuckDuckGo Search",
# "action\_input": "When did the New England Patriots last win the Super 
# Bowl? Justin Bieber birthdate"
#}
#{
# "action": "Final Answer",
# "action\_input": "Justin Bieber was born on March 1, 1994. The Patriots 
# last won the Super Bowl in February 2018."
#}</pre>
</div>
</div>
<div class="readable-text" id="p105">
<p>Not bad! It didn’t answer the question, but it got pretty close; it just needed to do some math. If you ran the example, you might have noticed it was a bit slow compared to using the llama.cpp Python interpreter. Unfortunately, for some reason, LangChain’s wrapper adds some significant time to compute, so be warned: if you need to go really fast, LangChain is not your vehicle. At least not yet. Regardless, LangChain has made some easy-to-use wrappers around popular Python libraries to make them usable as LLM tools. In these listings, we only used a handful, and there’s a lot more to choose from. </p>
</div>
<div class="readable-text intended-text" id="p106">
<p>Overall, you can see that we were able to get the LLM to perform pretty well on some nontrivial tasks (and we were using a 4-bit quantized model, we might add). However, it was nowhere close to perfect. Agents are miraculous in that they work at all, but they are generally underwhelming in the tasks and levels they can perform—including the top-tier paid agents. The more you work with LLMs crafting many different prompts, the more you’ll find that LLMs are quite flaky, just like humans, which can be quite annoying to software engineers who are used to working with machines that are as consistent as anything this world has to offer. Getting LLMs to perform well on just one task is often difficult enough, but chaining several tasks together inside an agent is extremely difficult. We are still very much in the early stages of agent development, and we are excited to see where it goes.</p>
</div>
<div class="readable-text" id="p107">
<h2 class="readable-text-h2" id="sigil_toc_id_150">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p108"> Creating a simple LLM application is straightforward, but creating one that delights your users takes a bit more work. </li>
<li class="readable-text buletless-item" id="p109"> The key features you should include in your app include the following: 
    <ul>
<li> Streaming responses allows a more interactive and responsive experience. </li>
<li> Feeding your model the chat history will prevent your model from having a birdbrain. </li>
<li> Interactive features like Stop, Retry, and Delete buttons give users more control of the conversation. </li>
<li> Token counting is useful for user feedback and allows users to edit responses to fit token limits. </li>
<li> RAG on the frontend allows us to customize an application regardless of the LLM backend. </li>
</ul></li>
<li class="readable-text" id="p110"> Llama.cpp is a powerful open source tool for compiling LLMs and running them on edge devices with constrained resources. </li>
<li class="readable-text" id="p111"> Agents are LLM applications built to solve multistep problems and promise to automate jobs machines currently struggle with. </li>
<li class="readable-text" id="p112"> Agents are extremely difficult to build due to the unpredictability of LLMs and sometimes require advanced prompt engineering to get reasonable results. </li>
</ul>
</div></body></html>