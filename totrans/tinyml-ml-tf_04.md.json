["```py\n# TensorFlow is an open source machine learning library\n!pip install tensorflow==2.0\nimport tensorflow as tf\n# NumPy is a math library\nimport numpy as np\n# Matplotlib is a graphing library\nimport matplotlib.pyplot as plt\n# math is Python's math library\nimport math\n```", "```py\nSuccessfully installed tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0\n```", "```py\n# We'll generate this many sample datapoints\nSAMPLES = 1000\n\n# Set a \"seed\" value, so we get the same random numbers each time we run this\n# notebook. Any number can be used here.\nSEED = 1337\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# Generate a uniformly distributed set of random numbers in the range from\n# 0 to 2\u03c0, which covers a complete sine wave oscillation\nx_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)\n\n# Shuffle the values to guarantee they're not in order\nnp.random.shuffle(x_values)\n\n# Calculate the corresponding sine values\ny_values = np.sin(x_values)\n\n# Plot our data. The 'b.' argument tells the library to print blue dots.\nplt.plot(x_values, y_values, 'b.')\nplt.show()\n```", "```py\n# Plot our data. The 'b.' argument tells the library to print blue dots.\nplt.plot(x_values, y_values, 'b.')\nplt.show()\n```", "```py\n# Add a small random number to each y value\ny_values += 0.1 * np.random.randn(*y_values.shape)\n\n# Plot our data\nplt.plot(x_values, y_values, 'b.')\nplt.show()\n```", "```py\n# We'll use 60% of our data for training and 20% for testing. The remaining 20%\n# will be used for validation. Calculate the indices of each section.\nTRAIN_SPLIT =  int(0.6 * SAMPLES)\nTEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)\n\n# Use np.split to chop our data into three parts.\n# The second argument to np.split is an array of indices where the data will be\n# split. We provide two indices, so the data will be divided into three chunks.\nx_train, x_validate, x_test = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\ny_train, y_validate, y_test = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n\n# Double check that our splits add up correctly\nassert (x_train.size + x_validate.size + x_test.size) ==  SAMPLES\n\n# Plot the data in each partition in different colors:\nplt.plot(x_train, y_train, 'b.', label=\"Train\")\nplt.plot(x_validate, y_validate, 'y.', label=\"Validate\")\nplt.plot(x_test, y_test, 'r.', label=\"Test\")\nplt.legend()\nplt.show()\n```", "```py\n# We'll use Keras to create a simple model architecture\nfrom tf.keras import layers\nmodel_1 = tf.keras.Sequential()\n\n# First layer takes a scalar input and feeds it through 16 \"neurons.\" The\n# neurons decide whether to activate based on the 'relu' activation function.\nmodel_1.add(layers.Dense(16, activation='relu', input_shape=(1,)))\n\n# Final layer is a single neuron, since we want to output a single value\nmodel_1.add(layers.Dense(1))\n\n# Compile the model using a standard optimizer and loss function for regression\nmodel_1.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n\n# Print a summary of the model's architecture\nmodel_1.summary()\n```", "```py\nmodel_1.add(layers.Dense(16, activation='relu', input_shape=(1,)))\n```", "```py\nactivation = activation_function((input * weight) + bias)\n```", "```py\ndef relu(input):\n    return max(0.0, input)\n```", "```py\nmodel_1.add(layers.Dense(1))\n```", "```py\n# Here, `inputs` and `weights` are both NumPy arrays with 16 elements each\noutput = sum((inputs * weights)) + bias\n```", "```py\nmodel_1.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n```", "```py\n# Print a summary of the model's architecture\nmodel_1.summary()\n```", "```py\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ndense (Dense)                (None, 16)                32\n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17\n=================================================================\nTotal params: 49\nTrainable params: 49\nNon-trainable params: 0\n_________________________________________________________________\n```", "```py\nhistory_1 = model_1.fit(x_train, y_train, epochs=1000, batch_size=16,\n                     validation_data=(x_validate, y_validate))\n```", "```py\nTrain on 600 samples, validate on 200 samples\nEpoch 1/1000\n600/600 [==============================] - 1s 1ms/sample - loss: 0.7887 - mae: 0.7848 - val_loss: 0.5824 - val_mae: 0.6867\nEpoch 2/1000\n600/600 [==============================] - 0s 155us/sample - loss: 0.4883 - mae: 0.6194 - val_loss: 0.4742 - val_mae: 0.6056\n```", "```py\nhistory_1 = model_1.fit(x_train, y_train, epochs=1000, batch_size=16,\n                     validation_data=(x_validate, y_validate))\n```", "```py\nEpoch 1/1000\n600/600 [==============================] - 1s 1ms/sample - loss: 0.7887 - mae: 0.7848 - val_loss: 0.5824 - val_mae: 0.6867\n```", "```py\nEpoch 1000/1000\n600/600 [==============================] - 0s 124us/sample - loss: 0.1524 - mae: 0.3039 - val_loss: 0.1737 - val_mae: 0.3249\n```", "```py\nloss = history_1.history['loss']\nval_loss = history_1.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'g.', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n```", "```py\n# Exclude the first few epochs so the graph is easier to read\nSKIP = 100\n\nplt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')\nplt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n```", "```py\n# Draw a graph of mean absolute error, which is another way of\n# measuring the amount of error in the prediction.\nmae = history_1.history['mae']\nval_mae = history_1.history['val_mae']\n\nplt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label='Training MAE')\nplt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')\nplt.title('Training and validation mean absolute error')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.legend()\nplt.show()\n```", "```py\n# Use the model to make predictions from our validation data\npredictions = model_1.predict(x_train)\n\n# Plot the predictions along with the test data\nplt.clf()\nplt.title('Training data predicted vs actual values')\nplt.plot(x_test, y_test, 'b.', label='Actual')\nplt.plot(x_train, predictions, 'r.', label='Predicted')\nplt.legend()\nplt.show()\n```", "```py\nmodel_2 = tf.keras.Sequential()\n\n# First layer takes a scalar input and feeds it through 16 \"neurons.\" The\n# neurons decide whether to activate based on the 'relu' activation function.\nmodel_2.add(layers.Dense(16, activation='relu', input_shape=(1,)))\n\n# The new second layer may help the network learn more complex representations\nmodel_2.add(layers.Dense(16, activation='relu'))\n\n# Final layer is a single neuron, since we want to output a single value\nmodel_2.add(layers.Dense(1))\n\n# Compile the model using a standard optimizer and loss function for regression\nmodel_2.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n\n# Show a summary of the model\nmodel_2.summary()\n```", "```py\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ndense_2 (Dense)              (None, 16)                32\n_________________________________________________________________\ndense_3 (Dense)              (None, 16)                272\n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 17\n=================================================================\nTotal params: 321\nTrainable params: 321\nNon-trainable params: 0\n_________________________________________________________________\n```", "```py\nhistory_2 = model_2.fit(x_train, y_train, epochs=600, batch_size=16,\n                     validation_data=(x_validate, y_validate))\n```", "```py\nEpoch 600/600\n600/600 [==============================] - 0s 150us/sample - loss: 0.0115 - mae: 0.0859 - val_loss: 0.0104 - val_mae: 0.0806\n```", "```py\n# Draw a graph of the loss, which is the distance between\n# the predicted and actual values during training and validation.\nloss = history_2.history['loss']\nval_loss = history_2.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'g.', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n```", "```py\n# Exclude the first few epochs so the graph is easier to read\nSKIP = 100\n\nplt.clf()\n\nplt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')\nplt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n```", "```py\nplt.clf()\n\n# Draw a graph of mean absolute error, which is another way of\n# measuring the amount of error in the prediction.\nmae = history_2.history['mae']\nval_mae = history_2.history['val_mae']\n\nplt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label='Training MAE')\nplt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')\nplt.title('Training and validation mean absolute error')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.legend()\nplt.show()\n```", "```py\n# Calculate and print the loss on our test dataset\nloss = model_2.evaluate(x_test, y_test)\n\n# Make predictions based on our test dataset\npredictions = model_2.predict(x_test)\n\n# Graph the predictions against the actual values\nplt.clf()\nplt.title('Comparison of predictions and actual values')\nplt.plot(x_test, y_test, 'b.', label='Actual')\nplt.plot(x_test, predictions, 'r.', label='Predicted')\nplt.legend()\nplt.show()\n```", "```py\n200/200 [==============================] - 0s 71us/sample - loss: 0.0103 - mae: 0.0718\n```", "```py\n# Convert the model to the TensorFlow Lite format without quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_2)\ntflite_model = converter.convert()\n\n# Save the model to disk\nopen(\"sine_model.tflite,\" \"wb\").write(tflite_model)\n\n# Convert the model to the TensorFlow Lite format with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_2)\n# Indicate that we want to perform the default optimizations,\n# which include quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n# Define a generator function that provides our test data's x values\n# as a representative dataset, and tell the converter to use it\ndef representative_dataset_generator():\n  for value in x_test:\n    # Each scalar value must be inside of a 2D array that is wrapped in a list\n    yield [np.array(value, dtype=np.float32, ndmin=2)]\nconverter.representative_dataset = representative_dataset_generator\n# Convert the model\ntflite_model = converter.convert()\n\n# Save the model to disk\nopen(\"sine_model_quantized.tflite,\" \"wb\").write(tflite_model)\n```", "```py\n# Instantiate an interpreter for each model\nsine_model = tf.lite.Interpreter('sine_model.tflite')\nsine_model_quantized = tf.lite.Interpreter('sine_model_quantized.tflite')\n\n# Allocate memory for each model\nsine_model.allocate_tensors()\nsine_model_quantized.allocate_tensors()\n\n# Get indexes of the input and output tensors\nsine_model_input_index = sine_model.get_input_details()[0][\"index\"]\nsine_model_output_index = sine_model.get_output_details()[0][\"index\"]\nsine_model_quantized_input_index = sine_model_quantized.get_input_details()[0][\"index\"]\nsine_model_quantized_output_index = \\\n  sine_model_quantized.get_output_details()[0][\"index\"]\n\n# Create arrays to store the results\nsine_model_predictions = []\nsine_model_quantized_predictions = []\n\n# Run each model's interpreter for each value and store the results in arrays\nfor x_value in x_test:\n  # Create a 2D tensor wrapping the current x value\n  x_value_tensor = tf.convert_to_tensor([[x_value]], dtype=np.float32)\n  # Write the value to the input tensor\n  sine_model.set_tensor(sine_model_input_index, x_value_tensor)\n  # Run inference\n  sine_model.invoke()\n  # Read the prediction from the output tensor\n  sine_model_predictions.append(\n      sine_model.get_tensor(sine_model_output_index)[0])\n  # Do the same for the quantized model\n  sine_model_quantized.set_tensor\\\n  (sine_model_quantized_input_index, x_value_tensor)\n  sine_model_quantized.invoke()\n  sine_model_quantized_predictions.append(\n      sine_model_quantized.get_tensor(sine_model_quantized_output_index)[0])\n\n# See how they line up with the data\nplt.clf()\nplt.title('Comparison of various models against actual values')\nplt.plot(x_test, y_test, 'bo', label='Actual')\nplt.plot(x_test, predictions, 'ro', label='Original predictions')\nplt.plot(x_test, sine_model_predictions, 'bx', label='Lite predictions')\nplt.plot(x_test, sine_model_quantized_predictions, 'gx', \\\n  label='Lite quantized predictions')\nplt.legend()\nplt.show()\n```", "```py\nimport os\nbasic_model_size = os.path.getsize(\"sine_model.tflite\")\nprint(\"Basic model is %d bytes\" % basic_model_size)\nquantized_model_size = os.path.getsize(\"sine_model_quantized.tflite\")\nprint(\"Quantized model is %d bytes\" % quantized_model_size)\ndifference = basic_model_size - quantized_model_size\nprint(\"Difference is %d bytes\" % difference)\n```", "```py\nBasic model is 2736 bytes\nQuantized model is 2512 bytes\nDifference is 224 bytes\n```", "```py\n# Install xxd if it is not available\n!apt-get -qq install xxd\n# Save the file as a C source file\n!xxd -i sine_model_quantized.tflite > sine_model_quantized.cc\n# Print the source file\n!cat sine_model_quantized.cc\n```", "```py\nunsigned char sine_model_quantized_tflite[] = {\n  0x1c, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x00, 0x00, 0x12, 0x00,\n  0x1c, 0x00, 0x04, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x14, 0x00,\n  // ...\n  0x00, 0x00, 0x08, 0x00, 0x0a, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09,\n  0x04, 0x00, 0x00, 0x00\n};\nunsigned int sine_model_quantized_tflite_len = 2512;\n```"]