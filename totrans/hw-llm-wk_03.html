<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="chp__learning"> <span class="chapter-title-numbering"><span class="num-string">4</span></span> <span class="title-text"> How LLMs learn</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Training algorithms with loss functions and gradient descent</li> 
    <li class="readable-text" id="p3">How LLMs mimic human text</li> 
    <li class="readable-text" id="p4"> How training can lead LLMs to produce errors</li> 
    <li class="readable-text" id="p5">Challenges in scaling LLMs</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>The words <em>learning</em> and <em>training</em> are commonly used in the machine learning community to describe what algorithms do when they observe data and make predictions based on those observations. We use this terminology begrudgingly becausealthough it simplifies the discussion of the operations of these algorithms, we feel that it is not ideal. Fundamentally, this terminology leads to misconceptions about LLMs and artificial intelligence. These words imply that these algorithms have human-like qualities; they seduce you into believing that algorithms display emergent behavior and are capable of more than they are truly capable of. At a fundamental level, this terminology is incorrect. A computer doesn’t learn in any way similar to how humans learn. Models do improve based on data and feedback, but it is incredibly important to keep this mechanistically distinct from anything like human learning. Indeed, you probably do not want an AI to learn like a human: we spend many years of our lives focused on education and still make dumb decisions.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Deep learning algorithms train in a way that is far more formulaic than how humans learn. It is formulaic in the literal sense of using a lot of math and the figurative meaning of following a simple repetitive procedure billions of times until completion. We will spare you the math, but in this chapter, we will help you remove the mystery of how LLMs are trained.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>Many machine learning algorithms use the training algorithm called <em>gradient descent</em>. The name of this algorithm implies some details that we’ll review with a high-level overview of how gradient descent is used for machine learning. Once you understand the general approach used to train many different model types, we will explore how gradient descent is applied to LLMs to create a model that produces convincing textual output. </p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>Understanding these details will help you avoid inaccurate connotations implied by words like <em>learn</em>. More importantly, it will also prepare you to understand better when LLMs succeed and fail in their current design and the often-subtle ways such algorithms can produce misleading outputs.</p> 
  </div> 
  <div class="readable-text" id="p10"> 
   <h2 class=" readable-text-h2" id="gradient-descent"><span class="num-string browsable-reference-id">4.1</span> Gradient descent</h2> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p><em>Gradient descent</em> is the key to all modern deep-learning algorithms. When an industry practitioner mentions gradient descent, they are implicitly referring to two critical elements of the training process. The first is known as a <em>loss function</em>, and the second is calculating <em>gradients</em>, which are measurements that tell you how to adjust the parameters of the neural network so that the loss function produces results in a specific way. You can think of these as two high-level components:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p12"><em>Loss function</em>—You need a single numeric score that calculates how poorly your algorithm works.</li> 
   <li class="readable-text" id="p13"><em>Gradient descent</em>—You need a mechanical process that tweaks the numeric values inside an algorithm to make the loss function score as small as possible.</li> 
  </ul> 
  <div class="readable-text" id="p14"> 
   <p>The loss function and gradient descent are components of the training algorithm used to produce a machine learning model. Many different training algorithms are in use today, but generally, each algorithm sends inputs into a model, observes the model’s output, and tweaks the model to improve its performance. A training algorithm will repeat this process a tremendous number of times. Given enough data, a model will produce the expected outputs repeatedly and reliably when confronted with previously unseen input.</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <h3 class=" readable-text-h3" id="what-is-a-loss-function"><span class="num-string browsable-reference-id">4.1.1</span> What is a loss function?</h3> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>We will use the example of wanting to make money to help develop a mental picture of a suitable loss function. Indeed, an intelligent person can make money, so if you have an intelligent computer, it should be able to help you make money. To pick a suitable loss function for this or any other task (these lessons generalize to any ML problem beyond LLMs), we need to satisfy three criteria: <em>specificity</em>, <em>computability</em>, and <em>smoothness</em>. In other words, the loss function needs to be </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p17">Specific and correlated with the desired behavior of the model</li> 
   <li class="readable-text" id="p18">Computable in a reasonable amount of time with a reasonable amount of resources</li> 
   <li class="readable-text" id="p19">Smooth, in the sense that the function’s output does not fluctuate wildly when given similar inputs</li> 
  </ul> 
  <div class="readable-text" id="p20"> 
   <p>We will use the following examples and counterexamples to help you develop an intuition for each property.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <h4 class=" readable-text-h4" id="loss-function-specificity"> Loss function specificity</h4> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>First, let’s start with a bad example of specificity. If your boss came to you and said, “Build an intelligent computer,” that would be a magnificent goal, but it is not a specific goal. Remember, in chapter 1, we discussed how difficult it is to define intelligence. What exactly does your boss want this computer to be intelligent at? Would a street-smart computer that cannot do your calculus homework suffice? Instead, you could try to optimize for a specific IQ score, but does that correlate with what your boss wants? We have been able to get computers to pass IQ tests for over a decade [1], even before the introduction of LLMs. However, they could not do anything other than pass an IQ test and perform limited tasks. Ultimately, the IQ test does not correlate with what we want computers to do. As a result, it is not worth optimizing IQ as a metric for success in machine learning or for building the intelligent computer your boss asked you to create.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>Another example involves the challenge of managing money. Consider a scenario where you want to minimize the debt you carry. You might even want your debt to go negative, meaning others owe you money! We use the example of debt here because it is intrinsically a value you want to make smaller. This analogy aligns perfectly with the terminology used in practice: you want to minimize your loss just as you want to reduce your debt. The volume of debt is also an objective measure, making it a good way of ensuring our loss function is relevant under changing conditions. Finally, if our overall goal is to maintain a surplus of money, minimizing debt correlates well with that goal. Minimizing debt has all of the characteristics of a good loss function!</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p24"> 
    <h5 class=" callout-container-h5 readable-text-h5">A note on terminology</h5> 
   </div> 
   <div class="readable-text" id="p25"> 
    <p> You may also hear loss functions described as <em>objective functions</em>. We recommend avoiding this term as a newcomer because it is ambiguous. For example, it is unclear whether you want to minimize (debt) or maximize your objective (profit). Both approaches technically work; multiply a maximizing objective by <span><img alt="equation image" src="../Images/eq-chapter-4-25-1.png"/></span>, and you now have a minimizing objective.</p> 
   </div> 
   <div class="readable-text" id="p26"> 
    <p>You may also hear the term <em>reward function</em> used in some contexts, such as reinforcement learning (RL). This is appropriate because RL algorithms seek to maximize reward by performing a desirable behavior.</p> 
   </div> 
   <div class="readable-text" id="p27"> 
    <p>Regardless of the terminology, objective functions, reward functions, and loss functions all address the same fundamental requirement: they provide a way of evaluating the outputs that a machine learning model produces.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p28"> 
   <h4 class=" readable-text-h4" id="loss-function-computability"> Loss function computability</h4> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>The loss function must also be something we can compute quickly with a computer. The debt example is unsuitable for this aspect because all the inputs and outputs you need are not readily available to a computer. Will working harder at your job increase your income and thus lower your debt? Maybe, but how will we encode your hard work into the computer? Here, we have the problem that the most critical factors to minimizing debt are hard to quantify, like job availability, your fit for such jobs, likelihood of promotion, etc. So the loss is specific, but the inputs that connect to that loss are not computable.</p> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>A better, more computable goal would be to predict the loss on an investment. The reasons this goal is better are subtle. The goal is still objective because our algorithms learn from historical data. For example, a historic investment in bonds X and stocks Y had certain returns. The inputs are also now objective: you can quantify the amount of cash you put into each investment. You either put money in, or you took it out. There are no hard-to-encode problems like “hard work” to deal with. With a copy of historical data, a computer can quickly calculate the loss/return on an investment.</p> 
  </div> 
  <div class="readable-text" id="p31"> 
   <h4 class=" readable-text-h4" id="loss-function-smoothness"> Loss function smoothness</h4> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>The third thing we need is smoothness. Many people have good intuition for what smoothness means by thinking about a smooth versus bumpy texture. Instead of texture, we’re talking about the smoothness of a function, which can be depicted by drawing that function as a graph. For example, when trying to predict a loss on an investment, we run into the problem that investment returns are not usually smooth. They may follow a pattern of volatility where price graphs are jagged with sharp, sudden changes. This makes learning difficult. A graph showing the unstable values of real-world investment returns is shown in figure <a href="#fig__stock_returns">4.1</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p33">  
   <img alt="figure" src="../Images/CH04_F01_Boozallen.jpg"/> 
   <h5 class=" figure-container-h5" id="fig__stock_returns"><span class="num-string">Figure <span class="browsable-reference-id">4.1</span></span> Investment returns are not easy to predict, partly because they are not smooth. (Image modified from [2] under the Creative Commons license )</h5>
  </div> 
  <div class="readable-text" id="p34"> 
   <p>Return on investment is an excellent example of a bad (nonsmooth) loss because erratic behavior is problematic for any predictive approach. It would be best if you were always cautious of anyone or any approach that claims to work well in predicting nonsmooth data like this. However, there is a precise technical definition of smooth that, if not satisfied by a loss function, is a hard deal-breaker. Functions that depend on discontinuities, or breaks in the consistency of their values, are the most common functions that are not technically smooth, but we would like to be able to use them in practice. Some examples of nonsmooth functions are shown in figure <a href="#fig__nonSmooth">4.2</a> to help you understand. Smoothness is usually inhibited due to discontinuities, such as that shown in the center graph, or distinct changes in the value of a function, as shown in the graph on the right.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p35">  
   <img alt="figure" src="../Images/CH04_F02_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__nonSmooth"><span class="num-string">Figure <span class="browsable-reference-id">4.2</span></span> Examples of a smooth function on the left and two nonsmooth functions on the right. The center example is mostly smooth, but one region is not smooth because the function has no value. On the right, the function is not smooth anywhere due to the hard change in value. </h5>
  </div> 
  <div class="readable-text" id="p36"> 
   <p>We won’t go deep into the formal mathematical definitions that describe what makes something smooth and what value changes are acceptable or unacceptable in smooth functions. Still, we’ve given you enough background to understand what you need to know. The important thing for you to understand is that your intuition of what smooth means, that the value changes continuously, is a good barometer for how viable a loss function is. This may seem arbitrary, but it is an ubiquitous problem. Say you want to build a model to predict cancer accurately. Accuracy is not a smooth function because you count the number of successful predictions out of the total predictions. For example, if you had 50 patients and predicted 48 of them correctly, a smooth function would have an option for 48.2 cases, 47.921351 cases, or any number you might think of. However, the actual count of cancer cases is constrained to the integers 1, 2, 3, <span><img alt="equation image" src="../Images/eq-chapter-4-36-1.png"/></span>, 48, 49, 50 because there is no such thing as a partial case of cancer.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p37"> 
    <h5 class=" callout-container-h5 readable-text-h5">How do you handle nonsmooth losses?</h5> 
   </div> 
   <div class="readable-text" id="p38"> 
    <p> It may be shocking that accuracy is one of the most common predictive goals, but we cannot use it when training an algorithm. But it is true! So how do we handle this strange phenomenon? The answer is to create a <em>proxy problem</em>. A proxy problem is an alternate way of representing a problem that correlates with what we want to solve but is better behaved. In this case, we use a cross-entropy loss function instead of accuracy. While we won’t go into the details of cross-entropy loss here, its use demonstrates that proxy problems are fundamental tricks used in machine learning and artificial intelligence. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>This discussion leads us to another critical takeaway about how LLMs learn, which is true of most algorithms: the technique we use to train them is not always focused on what we want them to do but on what we can make them learn. This focus can lead to an incentive mismatch, leading to unexpected results or low performance. We will discuss how the nature of an LLM’s loss function creates this incentive mismatch after examining the second major training component: gradient descent.</p> 
  </div> 
  <div class="readable-text" id="p40"> 
   <h3 class=" readable-text-h3" id="what-is-gradient-descent"><span class="num-string browsable-reference-id">4.1.2</span> What is gradient descent?</h3> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>Having a loss function is a prerequisite for performing a gradient descent. The loss function tells you objectively how poorly you are performing the task. Gradient descent is the process we use to figure out how to tweak the parameters of the neural network to reduce the loss incurred. This is done by comparing the input training data and the actual versus expected outputs of the neural network using the loss function. In this case, the gradient is the direction and amount that you need to change the parameters of a neural network to reduce the amount of error measured by the loss function. Gradient descent shows us how to tweak all the parameters of a neural network “just a little bit” to improve its performance and reduce the difference between the expected and actual outputs. A diagram of this process is shown in figure <a href="#fig__nn_gd_tweaks">4.3</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p42">  
   <img alt="figure" src="../Images/CH04_F03_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__nn_gd_tweaks"><span class="num-string">Figure <span class="browsable-reference-id">4.3</span></span> Inputs and labels (the known correct answers for each input) are used to tweak the neural network during gradient descent. A network is made of parameters that are altered a small amount each time gradient descent is applied. We eventually transform the network into something useful by applying gradient descent millions or billions of times. </h5>
  </div> 
  <div class="readable-text" id="p43"> 
   <p>As figure <a href="#fig__nn_gd_tweaks">4.3</a> shows, we create a new, slightly different network every time we apply gradient descent. Because the changes are small, this process has to be performed billions of times. This way, all the small changes add up to a more significant, mean-ingful change in the overall network.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p44"> 
   <p> <span class="print-book-callout-head">Note</span> Modern LLMs perform billions of parameter updates because they are trained on billions of tokens. The more data you have, the more times you run gradient descent. The less data you have, the less often you need to run it. The data used to train an LLM is more than you could read in a lifetime. </p> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>Gradient descent is a mathematical process that is applied repeatedlywithout deviation. There are no guarantees that it will work or find the best or even a good solution. Nevertheless, many researchers have been surprised by how practical this relatively simple approach is.</p> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>To help you understand how gradient descent works, we will use a simple example of rolling a ball down a hill. The ball’s location represents a parameter value for a node in the neural network that the training algorithm can alter. The hill’s height is the amount of loss and describes how poorly the model performs for the training input. We want to roll the ball down the hill into the deepest valley because that is the area with the lowest loss, which indicates that the model is performing its best. An example of this is shown in figure <a href="#fig__gd_start">4.4</a></p> 
  </div> 
  <div class="browsable-container figure-container" id="p47">  
   <img alt="figure" src="../Images/CH04_F04_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__gd_start"><span class="num-string">Figure <span class="browsable-reference-id">4.4</span></span> This shows the global big picture of gradient descent applied to a single parameter problem. The curve illustrates the value of the loss function for a given parameter value. The ball’s location shows the loss for the current parameter value. The goal is to find the parameter values corresponding to a global minimum representing the ideal solution with the least loss.</h5>
  </div> 
  <div class="readable-text" id="p48"> 
   <p>As you can see, the ball could fall into many valleys. The industry jargon would be to call this problem <em>nonconvex</em> because multiple paths lead to reduced loss, but each path does not necessarily progress toward the best possible solution. It is also important to note that this is not an analogy. Gradient descent literally looks at the world this way. These examples show how gradient descent works for a model with one parameter to optimize. The same procedure is applied to billions of parameters when training an LLM.</p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>So from this position, we greedily look at which direction to move the ball downhill. We apply gradient descent two times in figure <a href="#fig__gd_steps">4.5</a>. This shows that the greedy option is to the left. When we move to the left by adjusting our parameter, we slightly move the ball down the slope. From the graph, you can see that a better solution exists by searching to the right, but due to the algorithm’s simplicity, it is unlikely that gradient descent will find it. Finding the optimal result in this case would require a more intelligent strategy involving searching and exploration, which is too costly to do well in practice.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p50">  
   <img alt="figure" src="../Images/CH04_F05_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__gd_steps"><span class="num-string">Figure <span class="browsable-reference-id">4.5</span></span> The gradient descent algorithm takes steps to adjust parameters to find the optimal outcome with the least loss. Unfortunately, the algorithm gets stuck in a local minimum, an area of the graph that is not optimal because other parameter values correspond to areas with a lower loss.</h5>
  </div> 
  <div class="readable-text" id="p51"> 
   <p>Also, notice that in the second step in figure <a href="#fig__gd_steps">4.5</a>, the ball gets stuck. While it is evident that continuing to move to the left will achieve an even lower loss, this result is only obvious because we can see the whole picture. Gradient descent cannot see the entire picture or even what is nearby. It only knows the exact location due to the current parameters and the loss function. Hence, it is a <em>greedy procedure</em>. Greedy procedures such as gradient descent are simplified approaches with the desired property of computability in that they are not prohibitively expensive to run many times to achieve an outcome. Greedy procedures are short-sighted because they choose the next optimal step based only on the current state, although broader, more optimal solutions may exist. They do this because evaluating the current and all possible future states would be impossible due to the number of potential outcomes that need to be considered. It would simply be too much to compute. The hope is that making many simple optimal decisions using limited information will generally lead to the most positive outcome—in this case, minimizing the value of the loss function.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <h4 class=" readable-text-h4" id="important-nuances-in-gradient-descent"> Important nuances in gradient descent</h4> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>In this discussion of gradient descent, we have skipped some important nuances that need to be considered for real-world use. First, as described here, gradient descent would need to use all of the training data simultaneously, which is computationally infeasible. Instead, we use a procedure called <em>stochastic gradient descent</em> (SGD). SGD is precisely the same as we’ve described, except it uses a small random subset of the training data instead of the entire dataset. This dramatically reduces the memory required to train the model, resulting in faster, better solutions. This method works because gradient descent only makes small changes in the current greedy direction. It turns out that a little data is almost as good as using all the data when figuring out which step to take next. If you have a billion tokens, you can take a billion SGD steps in about the same amount of time it takes to do one standard gradient descent step using all the data.</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>Many training approaches use a particular form of SGD called <em>Adaptive MomentEstimation</em> (Adam). Adam includes some extra tricks to help minimize the loss function faster and avoid getting stuck. Adam’s main trick is that it gives the ball some momentum, which builds as updates continually move in one direction. Thismomentum causes the ball to roll down the hill faster and means that if a small local minimum is hit, there might be enough momentum to plow past that point and continue onward, thus reaching the area of the loss function graph with the smallest amount of loss. </p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <p>The downside of Adam is that storing this information about momentum for each parameter increases the memory required for training by a factor of three compared to plain SGD. Memory is the most critical factor when building LLMs because it often determines how many GPUs you need, translating to cash out of your pocket. Although Adam won’t make the final model larger because you can throw away the data related to Adam’s extra momentum calculations once you are done training, you still need a system large enough to perform the training in the first place. The increased accuracy that comes with Adam’s ability to minimize loss more effectively comes with a distinct price.</p> 
  </div> 
  <div class="readable-text" id="p56"> 
   <h2 class=" readable-text-h2" id="llms-learn-to-mimic-human-text"><span class="num-string browsable-reference-id">4.2</span> LLMs learn to mimic human text</h2> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>Now that we understand how deep learning algorithms are trained by specifying a loss function used with gradient descent, we can discuss how this is applied to LLMs. Specifically, we will focus on the data and loss or reward functions used to train LLMs.</p> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>LLMs are generally trained on human-authored text. Specifically, they’re explicitly trained to mimic texts produced by humans. While this sounds a bit obvious (what else would they be trained to do?), this detail is commonly missed or confused with other things, even by experts in the field. In particular, language models are <em>not</em> trained to do any of the following things:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p59">Memorize text</li> 
   <li class="readable-text" id="p60">Generate new ideas</li> 
   <li class="readable-text" id="p61">Build representations of the world</li> 
   <li class="readable-text" id="p62">Produce factually accurate text</li> 
  </ul> 
  <div class="readable-text" id="p63"> 
   <p>It is essential to explain this notion further before we go deeper. When one trains a model to play chess, the model learns to play well because it gets rewarded for winning. A language model, by contrast, only gets rewarded for producing text that</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>looks exactly like the training data. Consequently, all text generated by the LLM that <em>looks like text in the training corpus</em> produces high rewards (or low loss), even when those generations are not truthful or factual. This is an example of misalignment between the loss function and the designer’s higher-level goal, as discussed insection 4.1.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>LLMs are trained on datasets of hundreds of gigabytes of text scraped from the internet. The internet is famous for containing a large amount of incorrect (and weird) information. LLMs that are better at most tasks often end up being worse at tasks that are commonly misrepresented in their training data (see the Inverse Scaling Prize at <a href="https://github.com/inverse-scaling/prize">https://github.com/inverse-scaling/prize</a>). For example, researchers have consistently found that better language models are also better at reproducing common knowledge that is false [3], mimicking stereotypes and social biases [4]. They tend to fall into a downward spiral that reinforces errors. For example, after generating code that contains bugs, they’re more likely to generate code that contains additional bugs [5]. These things are commonly represented in the training text, so LLMs are positively rewarded for predicting them even though it’s wrong. Thus, getting better based on its loss function for an LLM also means getting worse at these tasks that require truth and correctness.</p> 
  </div> 
  <div class="readable-text" id="p66"> 
   <h3 class=" readable-text-h3" id="llm-reward-functions"><span class="num-string browsable-reference-id">4.2.1</span> LLM reward functions</h3> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>Previously, we said that LLMs are rewarded for producing data that “looks like its training data.” In this subsection, we will explore what this means more concretely.</p> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>LLMs are trained by being shown the first couple of tokens of a sentence and having it predict the next token. The loss is based on the accuracy of that prediction compared to the training data. For example, it might be shown “This is a” and be expected to produce “test.” If the model produces “test,” it gets a point, and if it does not, it loses a point. This process is done for all beginning segments of the text, as shown in figure <a href="#fig__autoregression">4.6</a>. Here, it is trained to predict each of the highlighted words independently. This setup is not unique to LLMs. It has been used to train recurrent neural networks (RNNs) for many years. However, an essential part of why LLMs have become so popular is that they can be trained much more efficiently than an RNN. An RNN must be trained on each generation <em>sequentially</em> because each newly generated word depends on the prior words chosen. An LLM can be trained on all generations <em>in parallel</em> due to the transformer architecture discussed in chapter 3. The ability to train a model on related generations in parallel represents a massive speed-up, allowing training at a large scale, and is a prerequisite for building today’s state-of-the-art LLMs using terabytes of data.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p69">  
   <img alt="figure" src="../Images/CH04_F06_Boozallen.jpg"/> 
   <h5 class=" figure-container-h5" id="fig__autoregression"><span class="num-string">Figure <span class="browsable-reference-id">4.6</span></span> An LLM sees this sentence nine times, each time learning from the prediction of a single word at the end of each of the nine sequences.</h5>
  </div> 
  <div class="readable-text" id="p70"> 
   <p>We discussed how predicting the next token can be problematic because the algorithm may be incentivized to produce incorrect or factually errant outputs. We must also discuss the intuition behind why, despite this, this approach can produce such convincing outputs. It is reasonable to ask: How can an algorithm trained to create the next most likely token seemingly perform something we could mistake for reasoning?</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>To develop this intuition, imagine how you might try to predict the next token for a given sentence. A computer has no pressure to respond quickly, so take your time. Consider the sentence “I love to eat &lt;blank&gt;,” and try to guess what word might go into the &lt;blank&gt;. The earlier parts of the sentence give you valuable context. Since we are discussing eating, you can almost immediately narrow the scope to a food item. Keeping a list of all possible food items is not difficult for a computer.</p> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>Now if you consider the background of the authors of this book, you will have even more context. We are Americans in a common geographical area, which makes specific cuisines more likely than others. An LLM will not have this background, but if the sentence was longer and had more context, you could start to narrow down the choices in the same way as shown in figure <a href="#fig__contextHelps">4.7</a>. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p73">  
   <img alt="figure" src="../Images/CH04_F07_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__contextHelps"><span class="num-string">Figure <span class="browsable-reference-id">4.7</span></span> Context can help you make decent predictions about the next word. As you move from left to right, additional text that might occur in a sentence is added. The images in the thought bubble for each sentence show how the added context eliminates predictions.</h5>
  </div> 
  <div class="readable-text" id="p74"> 
   <p>As you identify keywords or phrases in the preceding text, you can gain insight into the best word to predict next. A computer performing these calculations does far more processing than a human requires. This kind of brute-force association mainly narrows the scope to something very reasonable. Again, the model will be updated billions of times to refine these associations and thus acquire a useful capability correlated with our goals of an algorithm able to understand and react to human text.</p> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>However, correlation is not causation, and the next-word prediction strategy can lead to humorous errors. LLMs are susceptible to a “begging the question” error, where the premise of the question implies something untrue. Since the LLM is not trained for accuracy or contradiction, it attempts to produce a sequence of human-like text predictions that might follow your misleading question. An example of ChatGPT struggling with this kind of problem is given in figure <a href="#fig__spaghettiStong">4.8</a>, where we ask about the exceptional strength of dry spaghetti.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p76">  
   <img alt="figure" src="../Images/CH04_F08_Boozallen.jpg"/> 
   <h5 class=" figure-container-h5" id="fig__spaghettiStong"><span class="num-string">Figure <span class="browsable-reference-id">4.8</span></span> While predicting the next token is powerful, it doesn’t imbue the network with reasoning or logic abilities. If we ask ChatGPT something absurd and untrue, it happily explains how it happens.</h5>
  </div> 
  <div class="readable-text" id="p77"> 
   <p>The core of why spaghetti can support hundreds of times its own weight is absurd and untrue. However, the algorithm has been primed to provide an answer about material tensile strength by formatting the question: “Why is it that X is so strong?” The model can extract this key context. Previous training data likely explains such material properties based on a factual question, which informs the model predicting that a similar response is appropriate. The subject of the sentence (spaghetti) and object (10 lb. weight) are used to inform minor details of the response, which is otherwise generic.</p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <h2 class=" readable-text-h2" id="llms-and-novel-tasks"><span class="num-string browsable-reference-id">4.3</span> LLMs and novel tasks</h2> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>The nature of the autoregressive, next-word prediction strategy and its use as a loss or reward during the training process gives us valuable insight into the nature of an LLM’s generated responses and how they can potentially be factually inaccurate. However, it also shows us why LLMs can be effective for looking up information, as a far more powerful keyword search than a standard search engine. There are ways to design around the limitations of nonfactual responses. For example, many LLM approaches add citations to the generated output so that it is possible to quickly verify that factually accurate content was used to produce the generated text. An LLM can also be a valuable sounding board, a pseudo-partner to bounce ideas off of as a source of inspiration and creativity. Critically, this also helps you understand a key case where you should avoid LLMs because they will be more likely to produce errors—novel problems and tasks.</p> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>LLMs are generally not good at performing novel tasks. Figuring out if your task is novel can be pretty challenging, as the internet is weird. Tons of random things exist on the internet, including competitions on how to programmatically draw ducks and unicorns [6]. If the task is sufficiently similar to one already seen before or structurally similar to other things in the training data, you may end up with something that appears reasonable. This result can be extremely useful, but it can degrade as your task becomes more unique compared to what exists in the training data.</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>For example, we asked ChatGPT to write code that calculates the mathematical constant <span><img alt="equation image" src="../Images/eq-chapter-4-81-1.png"/></span> (pi) in Python. This task is not novel; tons of code like this exists online, and ChatGPT faithfully returns the correct code for us.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p82"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="firstcode"><span class="num-string">Listing <span class="browsable-reference-id">4.1</span></span> ChatGPT calculating pi in Python</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def calculate_pi(terms):
    pi = 0
    for i in range(terms):
        term = ((-1) ** i) / (2 * i + 1)
        pi += term
    pi *= 4
    return pi

print(calculate_pi(1000000))  #1</pre> 
    <div class="code-annotations-overlay-container"> #1 Tests the function; the more terms, the more accurate the approximation
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Now let us force ChatGPT to do some not terribly challenging extrapolation. We asked ChatGPT to translate this function to the programming language Modula-3. This task is not too big of an extrapolation; Modula-3 is a programming language with a similar style and a historically significant programming language that influenced the eventual design of almost all the most popular languages today! However, it is excessively esoteric. You can find very few examples of this programming language today, mainly in the context of university compiler classes. The next listing shows Chat-GPT’s reasonable attempt. As you may have been able to predict from the context of this chapter thus far, ChatGPT made some errors, marked in the listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p84"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__pi_modula3"><span class="num-string">Listing <span class="browsable-reference-id">4.2</span></span> ChatGPT calculates pi in Modula-3</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">MODULE CalculatePi;  #1

IMPORT IO, Real;

PROCEDURE CalculatePi(terms: INTEGER): REAL;
VAR
  pi: REAL := 0.0;
  i: INTEGER;
BEGIN
  FOR i := 0 TO terms - 1 DO
       pi := pi + ((-1.0) ** FLOAT(i)) / (2.0 * FLOAT(i) + 1.0);  #2
  END;
  RETURN 4.0 * pi;
END CalculatePi;

BEGIN
  IO.PutReal(CalculatePi(1000000), 0, 15);  #3
END CalculatePi.</pre> 
    <div class="code-annotations-overlay-container"> #1 Missing EXPORTS Main;
     <br/>#2 ** isn't an operator.
     <br/>#3 PutReal can take only one optional second argument, and it's not an integer.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>This short program has three errors that would prevent it from working. It is more interesting that ChatGPT gets these wrong because it confidently extrapolates standard coding practices from other languages. (In this case, <em>confidently</em> means that ChatGPT does not warn us of its potential errors. One of the authors likes to say that ChatGPT sounds like their most overconfident and often incorrect friend.) In this case, <code>**</code> is a commonly used exponentiation function, so ChatGPT decides that Modula-3 supports this operation. As far as we can tell from scouring the internet, Modula-3 has no documented example of how to exponentiate a variable. Because most programming languages support this action with a <code>^</code>, <code>**</code>, or <code>pow()</code> option, Chat-GPT just extrapolates one into existence. The correct answer would be that it must first implement a <span><code>pow</code></span> function and then use it to compute pi. </p> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>The arguments provided to the <code>PutReal</code> function are another mystery. Our best guess is that the <code>15</code> corresponds to an extrapolation of printing out 15 digits of a floating-point value, a typical default when calculating pi. Regardless, it is not how that function works.</p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>The more significant point is that ChatGPT gets some of the nuanced details right but only for the parts that can be found on the internet and are already explained (e.g., <code>FLOAT(i)</code> is required, as is doing <code>4.0 * pi</code> instead of <code>4 * pi</code>). The tasks without examples on the internet are the ones where ChatGPT makes errors.</p> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>This example also highlights the limits of perceived versus actualized “reasoning” within LLMs today. The complete language specification for Modula-3 is available online and has documented all of these details or their lack of existence. ChatGPT has almost surely seen many other coding language specifications, parser specifications, and millions of lines of code in common programming languages. If a person had this background knowledge and resources, performing the logical induction required to avoid all three errors should not be too challenging. However, the LLM does not perform any induction process and, thus, makes errors despite the breadth of available information.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>This is not to say that the result is not massively impressive, and it can be a valuable tool to accelerate your own code development or use of unfamiliar APIs and languages. But it also informs you that such tools will work far better for widely used and documented languages and APIs, especially if they conform to expected standards. For example, most databases use the language SQL, which makes accurate extrapolation of how to use a novel database that also uses SQL more likely.</p> 
  </div> 
  <div class="readable-text" id="p90"> 
   <h3 class=" readable-text-h3" id="failing-to-identify-the-correct-task"><span class="num-string browsable-reference-id">4.3.1</span> Failing to identify the correct task</h3> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>Another notable case in which LLM’s fail is when they cannot correctly identify the task they are supposed to perform and instead will answer a question different from what the user intended. Failure to correctly identify the task used to be a substantial problem for models like the original GPT-3, but subsequent work aimed at increasing the number of task-structured examples in the training data has substantially increased the ability of later ChatGPT models to follow instructions. However, ChatGPT will still fail to identify the correct task in some cases. For example, this behavior can be elicited reliably by asking about an unusual task subtly different from a common task or by modifying a problem it has seen many times in an unfamiliar way.</p> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>One example is a famous logic puzzle about bringing a cabbage, a goat, and a wolf across a river in a boat. The puzzle stipulates that the goat can’t be left alone with the cabbage (as the goat will eat it) or with the wolf (which will devour the goat). ChatGPT can quickly solve this puzzle, but if we change the logical structure of the puzzle slightly, the model continues to use the old reasoning as shown in figure <a href="#fig__cabbage1">4.9</a>.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>While it is often hard to trace errors made by LLMs back to specific causes, in this case, the model happily tells us to “ensure that none of the items (cabbage, goat, wolf) are left together unsupervised.” While this instruction is correct in the original version of the cabbage/goat/wolf problem (and was likely based on the specification of the constraints in the logic problem), the model is unaware that the given version has no problem with the goat and wolf being alone together. Not only is there no need to swap the animals as suggested, but ChatGPT’s advice will fail because it places the wolf and cabbage together, which we explicitly disallowed.</p> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>Another curious example of this phenomenon happens when you remove the need to leave anything behind. Any logical understanding of the puzzle makes it clear that you only need to load everything into the boat and cross. Yet again, the model is too accustomed to answering the version of the problem that it has seen many times before and does so.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p95">  
   <img alt="figure" src="../Images/CH04_F09_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__cabbage1"><span class="num-string">Figure <span class="browsable-reference-id">4.9</span></span> ChatGPT fails to solve two modified versions of a classic logic puzzle due to how LLMs are trained. Content frequently occurring in the same general form (e.g., a famous logic puzzle) leads the model to regurgitate the frequent answer. This can happen even when the content is modified in important ways that are obvious to a person.</h5>
  </div> 
  <div class="readable-text" id="p96"> 
   <p>To understand why this happens, it is important to recall the autoregressive nature of LLM training discussed in chapter 3. The model is explicitly incentivized to generate content based on prior content. The content generated to solve the reframed logic puzzle appears almost exactly like the content that solves the original logic puzzle in terms of words and order. As a result, it is a good fuzzy match in the transformer layer’s query and key pairing that produces the values that make up the original puzzle’s solution. The fuzzy match is made, and the previous solution is faithfully returned via the attention mechanism used by the transformers. While this strategy is excellent for the model to correctly predict the tokens for the famous puzzle, it does not involve reasoning through the puzzle’s logic.</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <h3 class=" readable-text-h3" id="llms-cannot-plan"><span class="num-string browsable-reference-id">4.3.2</span> LLMs cannot plan</h3> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>Another subtle limitation of the autoregressive nature of LLMs is that they can only work with the information they see in context. LLMs are trained to take an input and produce a plausible continuation. However, they cannot plan, make commitments, or track internal states. A great example occurs when you attempt to play the game 20 questions with ChatGPT. When a human plays 20 questions, they precommit to a piece of hidden information, the object they’ve chosen to use the answers to identify. When ChatGPT plays this game, it answers questions individually and then, after the fact, finds an output consistent with the provided answers. This example is illustrated in figure <a href="#fig__20qs">4.10</a>, which shows possible dialog trees for playing 20 questions. When someone plays a game with an LLM, one of these dialog trees is chosen randomly instead of coming up with a target object that stays consistent throughout the game.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p99">  
   <img alt="figure" src="../Images/CH04_F10_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__20qs"><span class="num-string">Figure <span class="browsable-reference-id">4.10</span></span> The dialogue agent doesn’t commit to a specific object at the start of the game.</h5>
  </div> 
  <div class="readable-text" id="p100"> 
   <h2 class=" readable-text-h2" id="if-llms-cannot-extrapolate-well-can-i-use-them"><span class="num-string browsable-reference-id">4.4</span> If LLMs cannot extrapolate well, can I use them?</h2> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>Most work that needs to be done is not novel or new. At least, it’s not novel or new enough to a degree that would make an LLM fail. However, understanding that an LLM’s abilities degrade quickly as more logic or nuance is required can help you narrow the scope of how you use it.</p> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>When we design production-grade computer systems, an essential factor to consider is the scope of when and how the tool will be used. When you make an LLM product like ChatGPT available to a general audience without a specific scope, people will ask it to do all sorts of random, crazy things you do not expect. While this might be great for research, it is often not practical for production applications. Although your users and customers will try to do unpredictable things with your LLM application, suppose you limit who has access to the system and design around your users having a specific goal, limited use cases, or even restrict how their inputs get to your LLM. In that case, you can build something with a much more reliable user experience.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p103"> 
    <h5 class=" callout-container-h5 readable-text-h5">How can I use an LLM without user input?</h5> 
   </div> 
   <div class="readable-text" id="p104"> 
    <p> LLMs are excellent at providing low-effort coding or data processing, especially when you are doing everyday tasks on data that is not so cleanly formatted or curated. However, you can get utility without as much risk by giving users a finite set of choices. Having a limited set of prompts as code that a user can choose from or letting a user decide what data source (e.g., some internal database) a prompt is run over allows you to keep (most) people from giving an LLM arbitrary text.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>Instead, you may ask, “Can we detect novel requests and give the user some error instead?” Hypothetically, yes, you could try to do this. First, we discourage it because it is not great from a user experience perspective. Second, it becomes a task known as <em>novelty detection</em> or <em>outlier detection</em>. This problem is challenging and is likely impossible to solve in a way that is guaranteed to be error-free. As a result, we encourage prevention over detection by choosing use cases that do not require highly accurate prediction of failures through the analysis of LLM input or output.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p106"> 
    <h5 class=" callout-container-h5 readable-text-h5">Applications for prompting</h5> 
   </div> 
   <div class="readable-text" id="p107"> 
    <p> Prompting is the art of crafting an input to a large language model that induces desirable behavior. Language models can be very sensitive to the exact framing of their inputs, making the ability to design inputs that are responded to appropriately highly valuable. A recurring theme in using LLMs is that people typically don’t think about how to interact with them correctly. The best way to prompt an LLM is to think about how the kind of output you’re interested in would look like in the training data and then write the first quarter of it. Instead, people often describe the task they want a language model to perform, assuming that this clarification will keep an LLM focused on the problem. Unfortunately, the approach yields inconsistent results and has inspired research in tuning LLMs by feeding them a large number of instructions and responses as training data.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p108"> 
   <h2 class=" readable-text-h2" id="is-bigger-better"><span class="num-string browsable-reference-id">4.5</span> Is bigger better?</h2> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>In 2019, Rich Sutton coined the term “the bitter lesson” to describe his experience with machine learning. “The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin” [7].</p> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>There is a genuine sense that transformers are the ultimate example of this principle. You can keep making them bigger, training them with more parallelism, and adding more GPUs. This differs notably from RNNs, which cannot be parallelized nearly as efficiently as a transformer. We also see this in the image domain with Generative Adversarial Network (GAN) methods, which struggle to reach the billion-parameters scale. The transformer-based methods used in LLMs easily scale to the tens of billions, allowing the construction of bigger and better models.</p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <p>From a solutions design perspective, your prototype today may encounter significant constraints due to model size. Larger models require more resources and take longer to make predictions. What is the maximum response time your users will accept? How expensive is the hardware needed to run your model at this speed? The growth rate in model size exceeds the growth rate of consumer hardware. As a result, you may not be able to deploy your model to embedded devices, or you may require internet connectivity to offload the costs. Consequently, you need to consider networking infrastructure in your design to handle the need for continuous connection. This requirement increases battery usage, which is a consideration when continually running a Wi-Fi radio instead of local computing. So although larger models are more accurate, design constraints may prevent their deployment in a practical manner. Combining these constraints with the facts about how LLMs make their predictions and the use cases of when and where LLMs fail that you learned in this chapter positions you well for understanding how to use LLMs to solve the problems you care about most effectively.</p> 
  </div> 
  <div class="readable-text" id="p112"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p113">Deep learning needs a loss/reward function that specifically quantifies how badly an algorithm is at making predictions</li> 
   <li class="readable-text" id="p114">This loss/reward function should be designed to correlate with the overarching goal of what we want the algorithm to achieve in real life.</li> 
   <li class="readable-text" id="p115">Gradient descent involves incrementally using a loss/reward function to alter the network’s parameters.</li> 
   <li class="readable-text" id="p116">LLMs are trained to mimic human text by predicting the next token. This task is sufficiently specific to train a model to perform it, but it does not perfectly correlate with high-level objectives like reasoning.</li> 
   <li class="readable-text" id="p117">LLMs will perform best on tasks similar to common and repetitive tasks observed in its training data but will fail when the task is sufficiently novel.</li> 
  </ul>
 </body></html>