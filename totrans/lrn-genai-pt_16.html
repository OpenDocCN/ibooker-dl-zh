<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">13 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/>Music generation with MuseGAN</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-291"/><a id="idIndexMarker002"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">Music representation using musical instrument digital interface <a class="calibre" id="idIndexMarker003"/></li>
<li class="co-summary-bullet">Treating music generation as an object creation problem similar to image generation</li>
<li class="co-summary-bullet">Building and training a generative adversarial network to generate music</li>
<li class="co-summary-bullet">Generating music using the trained MuseGAN model</li>
</ul>
<p class="body">Up to now, we have successfully generated shapes, numbers, images, and text. In this chapter and the next, we will explore two different ways of generating lifelike music. This chapter will apply the techniques from image GANs, treating a piece of music as a multidimensional object akin to an image. The generator will produce a complete piece of music and submit it to the critic (serving as the discriminator because we use the Wasserstein distance with gradient penalty, as discussed in chapter 5) for evaluation. The generator will then modify the music based on the critic’s feedback until it closely resembles real music from the training dataset. In the next chapter, we will treat music as a sequence of musical events, employing natural language processing (NLP) techniques. We will use a GPT-style Transformer to predict the most probable musical event in a sequence based on previous events. This Transformer will generate a long sequence of musical events that can be converted into realistic-sounding music.<a id="idIndexMarker004"/></p>
<p class="body">The field of music generation using AI has gained significant attention; MuseGAN is a prominent model, which was introduced by Dong, Hsiao, Yang, and Yang in 2017.<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">1</a></sup> MuseGAN is a deep neural network that utilizes generative adversarial networks (GANs) to create multitrack music, with the word Muse signifying the creative inspiration behind music. The model is adept at understanding the complex interactions between different tracks that represent different musical instruments or different voices (which is the case in our training data). As a result, MuseGAN can generate compositions that are harmonious and cohesive.<a id="idIndexMarker005"/><a id="marker-292"/><a id="idIndexMarker006"/></p>
<p class="body">MuseGAN, similar to other GAN models, consists of two primary components: the generator and the critic (who provides a continuous measure of how real the sample is rather than classifying a sample into real or fake). The generator’s task is to generate music, whereas the critic assesses the music’s quality and offers feedback to the generator. This adversarial interaction enables the generator to gradually improve, leading to the creation of more realistic and appealing music.</p>
<p class="body">Suppose you’re an avid fan of Johann Sebastian Bach and have listened to all his compositions. You might wonder if it’s possible to use MuseGAN to create synthetic music that mimics his style. The answer is yes, and you’ll learn how to do that in this chapter.</p>
<p class="body">Specifically, you’ll first explore how to represent a piece of multitrack music as a multidimensional object. A track is essentially an individual line of music or sound, which can be a different instrument such as piano, bass, or drums or a different voice such as soprano, alto, tenor, or bass. When composing a track in electronic music, you typically organize it into bars (segments of time), subdivide each bar into steps for finer control over rhythm, and then assign a specific note to each step to craft your melodies and rhythms. As a result, each piece of music in our training set is structured with a (4, 2, 16, 84) shape: this means there are four music tracks, with each track consisting of 2 bars, each bar containing 16 steps, and each step capable of playing one of the 84 different notes.</p>
<p class="body">The style of the music generated by our MuseGAN will be influenced by the training data. Since you are interested in Bach’s work, you’ll be training MuseGAN with The JSB Chorales dataset, which is a collection of chorales composed by Bach, arranged for four tracks. These chorales have been converted into a piano roll representation, a method used for visualizing and encoding music, especially for digital processing purposes. You’ll learn how to transform a piece of music represented in the shape of (4, 2, 16, 84) into a musical instrument digital interface (MIDI) file, which can then be played on your computer.<a id="idIndexMarker007"/></p>
<p class="body">While the generator uses just one single noise vector from the latent space to generate different formats of content such as shapes, numbers, and images in earlier chapters, the generator in MuseGAN will use four noise vectors when producing a piece of music. The use of four separate noise vectors (chords, style, melody, and groove, which I’ll explain in detail later in this chapter) is a design choice that allows for greater control and diversity in the music generation process. Each of these noise vectors represents a different aspect of music, and by manipulating them individually, the model can generate more complex and nuanced compositions.</p>
<p class="body">Once the model is trained, we’ll discard the critic network, a common practice in GAN models. We’ll then utilize the trained generator to produce music pieces by inputting four noise vectors from the latent space. The music generated in this way closely mirrors the style of Bach.<a id="idIndexMarker008"/><a id="idIndexMarker009"/><a id="idIndexMarker010"/><a id="marker-293"/></p>
<h2 class="fm-head" id="heading_id_3">13.1 Digital music representation</h2>
<p class="body">Our goal is to master the art of building and training a GAN model from scratch for music generation. To achieve this, we need to start with the fundamentals of music theory, including understanding musical notes, octaves, and pitch numbers. Following that, we’ll dive into the inner workings of digital music, specifically focusing on MIDI files.<a id="idIndexMarker011"/></p>
<p class="body">Depending on the type of machine learning model we use for music generation, the representation of a piece of music in digital form will vary. For instance, in this chapter, we’ll represent music as a multidimensional object, while in the next chapter, we’ll use a different format: a sequence of indexes.</p>
<p class="body">In this section, we’ll cover basic music theory and then move on to represent music digitally using piano rolls. You’ll learn to load and play an example MIDI file on your computer. We’ll also introduce the music21 Python library, which you’ll install and use to visualize the staff notes associated with the music piece. Finally, you’ll learn to represent a piece of music as a multidimensional object with the shape of (4, 2, 16, 84).</p>
<h3 class="fm-head1" id="heading_id_4">13.1.1 Musical notes, octave, and pitch</h3>
<p class="body">In this chapter, we’ll be working with a training dataset that represents music pieces as 4D objects. To grasp the meaning of the music pieces in the training data, it’s essential to first familiarize ourselves with some fundamental concepts in music theory, such as musical notes, octaves, and pitch. These concepts are interrelated and crucial for understanding the dataset. <a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/></p>
<p class="body">Figure 13.1 illustrates the relationships among these concepts.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="315" src="../../OEBPS/Images/CH13_F01_Liu.png" width="842"/></p>
<p class="figurecaption">Figure 13.1 The relationship between musical notes, octaves, and pitches (also known as note numbers). The first column displays the 11 octa<a id="idTextAnchor003"/>ves (ranging from –1 to 9), representing different levels of musical sound. Each octave is subdivided into 12 semitones, which are listed in the top row: C, C#, D, D#, ..., B. Within each octave, each note is assigned a specific pitch number, ranging from 0 to 127, as indicated in the figure.</p>
</div>
<p class="body">A musical note is a symbol representing a specific sound in music. These notes are the foundational elements of music, used to craft melodies, chords, and rhythms. Each note is assigned a name (such as A, B, C, D, E, F, G) and corresponds to a specific frequency, which determines its pitch: whether the note sounds high or low. For instance, a middle C (C4) typically has a frequency of about 262 hertz, meaning its sound waves vibrate 262 times per second.</p>
<p class="body">You might be wondering about the meaning of the term “middle C (C4).” The number 4 in C4 refers to the octave, which is the distance between one level of musical pitch and the next. In figure 13.1, the far-left column displays 11 octave levels, ranging from –1 to 9. The frequency of a sound doubles as you move from one octave level to the next. For example, note A4 is usually tuned to 440 hertz, while A5, one octave above A4, is tuned to 880 hertz..</p>
<p class="body">In Western music, an octave is divided into 12 semitones, each corresponding to a specific note. The top row of figure 13.1 lists these 12 semitones: C, C#, D, D#, ..., B. Moving up or down by 12 semitones takes you to the same note name but in a higher or lower octave. As mentioned earlier, A5 is one octave above A4.</p>
<p class="body">Each note within a specific octave is assigned a pitch number, ranging from 0 to 127, as depicted in figure 13.1. For example, the note C4 has a pitch number of 60, while F3 has a pitch number of 53. The pitch number is a more efficient way to represent musical notes since it specifies both the octave level and the semitone. The training data you’ll be using in this chapter is encoded using pitch numbers for this very reason.<a id="marker-294"/></p>
<h3 class="fm-head1" id="heading_id_5">13.1.2 An introduction to multitrack music</h3>
<p class="body">Let’s first talk about how multitrack music works and how it is represented digitally. In electronic music production, a “track” typically refers to an individual layer or component of the music, such as a drum track, a bass track, or a melody track. In classical music, tracks might represent different vocal parts, like soprano, alto, tenor, and bass. For instance, the training dataset we’re using in this chapter, the JSB Chorales dataset, consists of four tracks corresponding to four vocal parts. In music production, each track can be individually edited and processed within a digital audio workstation (DAW). These tracks are composed of various musical elements, including bars, steps, and notes.<a id="idIndexMarker019"/><a id="idIndexMarker020"/><a id="idIndexMarker021"/></p>
<p class="body">A bar (or measure) is a segment of time defined by a specified number of beats, with each beat having a certain note duration. In many popular music genres, a bar typically contains four beats, although this can vary based on the time signature of the piece. The total number of bars in a track is determined by the track’s length and structure. For example, in our training dataset, each track comprises two bars.</p>
<p class="body">In the context of step sequencing, a technique commonly used for programming rhythms and melodies in electronic music, a “step” represents a subdivision of a bar. In a standard 4/4 time signature (four beats in a bar and four steps in a beat), you might find 16 steps per bar, with each step corresponding to a sixteenth of a bar.</p>
<p class="body">Lastly, each step contains a musical note. In our dataset, we limit the range to the 84 most frequently used notes (with pitch numbers from 0 to 83). Therefore, the musical note in a step is encoded as a one-hot vector with 84 values.</p>
<p class="body">To illustrate these concepts with a practical example, download the file example.midi from the book’s GitHub repository at <a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a> and save it in the /files/ directory on your computer. A file with the .midi extension is a MIDI file. MIDI is a technical standard that outlines a protocol, digital interface, and connectors for enabling electronic musical instruments, computers, and other related devices to connect and communicate with each other.</p>
<p class="body">MIDI files can be played on most music players on your computer. To get a sense of the type of music in our training data, open the file example.midi you just downloaded with a music player on your computer. It should sound like this music file I placed on my website: <a class="url" href="https://mng.bz/lrJB">https://mng.bz/lrJB</a>. The file example.midi is converted from one of the music pieces in the training dataset in this chapter. Later you’ll learn how to convert a piece of music in the training dataset with a shape of (4, 2, 16, 84) into a MIDI file that can be played on your computer.</p>
<p class="body"><a id="marker-295"/>We’ll use the music21 Python library, a powerful and comprehensive toolkit designed for music analysis, composition, and manipulation, to visualize how various music concepts work. Therefore, run the following line of code in a new cell in the Jupyter Notebook app on your computer:</p>
<pre class="programlisting">!pip install music21</pre>
<p class="body">The music21 library enables you to visualize music as staff notation to have a better understanding of tracks, bars, steps, and notes. To achieve this, you must first install the MuseScore application on your computer. Visit <a class="url" href="https://musescore.org/en/download">https://musescore.org/en/download</a> and download the most recent version of the MuseScore app for your operating system. As of this writing, the latest version is MuseScore 4, which we’ll use as our example. Ensure you know the file path of the MuseScore app on your computer. For instance, in Windows, the path is C:\Program Files\MuseScore 4\bin\MuseScore4.exe. Run the code cell in the following listing to visualize the staff notation for the file example.midi.<a id="idIndexMarker022"/></p>
<p class="fm-code-listing-caption"><a id="marker-296"/>Listing 13.1 Visualizing the staff notation using the music21 library</p>
<pre class="programlisting">%matplotlib inline                                          <span class="fm-combinumeral">①</span>
from music21 import midi, environment
  
mf = midi.MidiFile()    
mf.open("files/example.midi")                               <span class="fm-combinumeral">②</span>
mf.read()
mf.close()
stream = midi.translate.midiFileToStream(mf)
us = environment.Environment() 
path = r'C:\Program Files\MuseScore 4\bin\MuseScore4.exe'
us['musescoreDirectPNGPath'] = path                         <span class="fm-combinumeral">③</span>
stream.show()                                               <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Shows the image in Jupyter notebook instead of in the original app</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Opens the MIDI file</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Defines the path of the MuseScore app</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Shows the staff notation</p>
<p class="body">For users of the macOS operating system, change the path in the preceding code cell to /Applications/MuseScore 4.app/Contents/MacOS/mscore. For Linux users, modify the path to /home/[user name]/.local/bin/mscore4portable, substituting [user name] with your actual username. For instance, my username is <code class="fm-code-in-text">mark</code>, so the path is /home/mark/.local/bin/mscore4portable.</p>
<p class="body">Executing the previous code cell will display a staff notation similar to what is illustrated in figure 13.2. Please note that the annotations in the figure are added by me, so you will only see the staff notation without any annotations.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="335" src="../../OEBPS/Images/CH13_F02_Liu.png" width="635"/></p>
<p class="figurecaption">Figure 13.2 Staff notation for a piece of music in JSB Chorales dataset. The music has four tracks, representing the four voices in a chorale: soprano, alto, tenor, and bass. The notation is structured into two bars for each track, with the left and right halves representing the first and second bars, respectively. Each bar consists of 16 steps, aligning with the 4/4 time signature where a bar contains four beats, each subdivided into four sixteenth notes. A total of 84 different pitches are possible, and each note is represented as a one-hot vector with 84 values.</p>
</div>
<p class="body">The JSB Chorales dataset, which consists of chorale music pieces by Johann Sebastian Bach, is often used for training machine learning models in music generation tasks. The shape (4, 2, 16, 84) of each music piece in the dataset can be explained as follows. Four represents the four voices in a chorale: soprano, alto, tenor, and bass. Each voice is treated as a separate track in the dataset. Each piece is divided into two bars (also called measures). The dataset is formatted this way to standardize the length of the music pieces for training purposes. The number 16 represents the number of steps (or subdivisions) in each bar. Finally, the note is one-hot encoded with 84 values, denoting the number of possible pitches (or notes) that can be played in each step. <a id="idIndexMarker023"/><a id="idIndexMarker024"/></p>
<h3 class="fm-head1" id="heading_id_6">13.1.3 Digitally represent music: Piano rolls</h3>
<p class="body">A piano roll is a visual representation of music often used in MIDI sequencing software and DAWs. It is named after the traditional piano rolls used in player pianos, which contained a physical roll of paper with holes punched in it to represent musical notes. In a digital context, the piano roll serves a similar function but in a virtual format.<a id="idIndexMarker025"/><a id="idIndexMarker026"/><a id="marker-297"/></p>
<p class="body">The piano roll is displayed as a grid, with time represented horizontally (from left to right) and pitch represented vertically (from bottom to top). Each row corresponds to a specific musical note, with higher notes at the top and lower notes at the bottom, similar to the layout of a piano keyboard.</p>
<p class="body">Notes are represented as bars or blocks on the grid. The position of a note block along the vertical axis indicates its pitch, while its position along the horizontal axis indicates its timing in the music. The length of the note block represents the duration of the note.</p>
<p class="body">Let’s use the music21 library to illustrate what a piano roll looks like. Run this line of code in a new cell in your Jupyter Notebook app:</p>
<pre class="programlisting">stream.plot()</pre>
<p class="body">The output is shown in figure 13.3.</p>
<p class="body">The music21 library also allows you to see the quantized notes corresponding to the preceding piano roll:</p>
<pre class="programlisting">for n in stream.recurse().notes:
    print(n.offset, n.pitches)</pre>
<p class="body">The output is</p>
<pre class="programlisting">0.0 (&lt;music21.pitch.Pitch E4&gt;,)
0.25 (&lt;music21.pitch.Pitch A4&gt;,)
0.5 (&lt;music21.pitch.Pitch G4&gt;,)
0.75 (&lt;music21.pitch.Pitch F4&gt;,)
1.0 (&lt;music21.pitch.Pitch E4&gt;,)
1.25 (&lt;music21.pitch.Pitch D4&gt;,)
1.75 (&lt;music21.pitch.Pitch E4&gt;,)
2.0 (&lt;music21.pitch.Pitch E4&gt;,)
2.5 (&lt;music21.pitch.Pitch D4&gt;,)
3.0 (&lt;music21.pitch.Pitch C4&gt;,)
3.25 (&lt;music21.pitch.Pitch A3&gt;,)
3.75 (&lt;music21.pitch.Pitch B3&gt;,)
0.0 (&lt;music21.pitch.Pitch G3&gt;,)
0.25 (&lt;music21.pitch.Pitch A3&gt;,)
0.5 (&lt;music21.pitch.Pitch B3&gt;,)
…
3.25 (&lt;music21.pitch.Pitch F2&gt;,)
3.75 (&lt;music21.pitch.Pitch E2&gt;,)</pre>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="560" src="../../OEBPS/Images/CH13_F03_Liu.png" width="842"/></p>
<p class="figurecaption">Figure 13.3 The piano roll for a piece of music. The piano roll is a graphical representation of a musical piece, depicted as a grid with time progressing horizontally from left to right and pitch represented vertically from bottom to top. Each row on the grid corresponds to a distinct musical note, arranged in a manner akin to the keyboard of a piano, with higher notes positioned at the top and lower notes at the bottom. This specific piece of music comprises two bars, resulting in two distinct sections visible in the graph. The vertical placement of a note block signifies its pitch, while its horizontal location indicates when the note is played in the piece. Additionally, the length of the note block reflects the duration for which the note is sustained.</p>
</div>
<p class="body"><a id="marker-298"/>I omitted most of the output. The first value in each line in the previous output represents time. It increases by 0.25 seconds after each line in most cases. If the time increase in the next line is more than 0.25 seconds, it means a note lasts more than 0.25 seconds. As you can see, the starting note is E4. After 0.25 seconds, the note changes to A4, and then G4, and so on. This explains the first three blocks (far left) in figure 13.3, which have values E, A, and G, respectively.</p>
<p class="body">You might be curious about how to convert the sequence of musical notes into an object with the shape (4, 2, 16, 84). To understand this, let’s examine the pitch number at each time step in the musical notes:</p>
<pre class="programlisting">for n in stream.recurse().notes:
    print(n.offset,n.pitches[0].midi)</pre>
<p class="body">The output is</p>
<pre class="programlisting">0.0 64
0.25 69
0.5 67
0.75 65
1.0 64
1.25 62
1.75 64
2.0 64
2.5 62
3.0 60
3.25 57
3.75 59
0.0 55
0.25 57
0.5 59
…
3.25 41
3.75 40</pre>
<p class="body">The preceding code block has converted the musical note in each time step into a pitch number, in the range of 0 to 83 based on the mapping used in figure 13.1. Each of the pitch numbers is then converted to a one-hot variable with 84 values, with value –1 everywhere, ex<a id="idTextAnchor004"/>cept 1 in one position. We use –1 and 1 in one-hot encoding instead of 0 and 1 because placing values between –1 and 1 centers the data around 0, which can make training more stable and faster. Many activation functions and weight initialization methods assume input data is centered around 0. Figure 13.4 illustrates how a piece of MIDI music is encoded into an object in the shape of (4, 2, 16, 84).<a id="marker-299"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="305" src="../../OEBPS/Images/CH13_F04_Liu.png" width="614"/></p>
<p class="figurecaption">Figure 13.4 How to represent a piece of music using a 4D object. In our training data, each piece of music is represented by a 4D object in the shape of (4, 2, 16, 84). The first dimension represents the four music tracks, which are the four voices in the music (soprano, alto, tenor, and bass). Each music track is divided into two bars. There are four beats in each bar, and each beat has four notes; we therefore have 16 notes in a bar. Finally, each note is represented by a one-hot variable with 84 values, with –1 everywhere and 1 in one place.</p>
</div>
<p class="body">Figure 13.4 explains the dimensions of the music object shaped (4, 2, 16, 84). In essence, each musical piece comprises four tracks, with each track containing two bars. Each bar is subdivided into 16 notes. Given that the pitch numbers range from 0 to 83 in our training set, each note is represented by a one-hot vector with 84 values.</p>
<p class="body">In subsequent discussions on preparing training data, we will explore how to transform an object with the shape (4, 2, 16, 84) back into a music piece in MIDI format, enabling playback on a computer. <a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="idIndexMarker029"/></p>
<h2 class="fm-head" id="heading_id_7">13.2 A blueprint for music generation</h2>
<p class="body">When creating music, we need to incorporate more detailed inputs for enhanced control and variety. Unlike the approach of utilizing a single noise vector from the latent space for generating shapes, numbers, and images, we will employ four distinct noise vectors in the music generation process. Since each music piece comprises four tracks and two bars, we’ll utilize four vectors to manage this structure. We’ll use one vector to govern all tracks and bars collectively, another vector to control each bar across all tracks, a third vector to oversee all tracks across bars, and a fourth one to manage each individual bar in each track. This section will introduce you to the concepts of chords, style, melody, and groove and explain how they influence various aspects of the music generation. After that, we’ll discuss the steps involved in building and training the MuseGAN model. <a id="idIndexMarker030"/><a id="marker-300"/></p>
<h3 class="fm-head1" id="heading_id_8">13.2.1 Constructing music with chords, style, melody, and groove</h3>
<p class="body">Later, in the music generation stage, we obtain four noise vectors (chords, style, melody, and groove) from the latent space and feed them to the generator to create a piece of music. You may be wondering the meaning of these four pieces of information. In music, chords, style, melody, and groove are key elements that contribute to a piece’s overall sound and feel. Next I provide a brief explanation of each element. <a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/></p>
<p class="body">Style refers to the characteristic way in which music is composed, performed, and experienced. It includes the genre (such as jazz, classical, rock, and so on), the era in which the music was created, and the unique approach of the composer or performer. Style is influenced by cultural, historical, and personal factors, and it helps to define the music’s identity.</p>
<p class="body">Groove is the rhythmic feel or swing in music, especially in styles like funk, jazz, and soul. It’s what makes you want to tap your foot or dance. A groove is created by the pattern of accents, the interplay between the rhythm section (drums, bass, etc.), and the tempo. It’s the element that gives music its sense of motion and flow.</p>
<p class="body">Chords are combinations of two or more notes played simultaneously. They provide the harmonic foundation for music. Chords are built on scales and are used to create progressions that give music its structure and emotional depth. Different chord types (major, minor, diminished, augmented, etc.) and their arrangements can evoke various moods and feelings in the listener.</p>
<p class="body">Finally, melody is the sequence of notes that is most easily recognizable in a piece of music. It’s the part that you might hum or sing along to. Melodies are often built from scales and are characterized by their pitch, rhythm, and contour (the pattern of rises and falls in pitch). A good melody is memorable and expressive, conveying the main musical and emotional themes of the piece.</p>
<p class="body">Together, these elements work in harmony to create the overall sound and experience of a musical piece. Each element has its role, but they all interact and influence each other to produce the final music piece. Specifically, a music piece consists of four tracks, each with two bars, resulting in eight bar/track combinations. We’ll use one noise vector for style, applied to all eight bars. We’ll use eight different noise vectors for melody, each used in a unique bar. There are four noise vectors for groove, each applied to a different track, remaining the same across both bars. Two noise vectors will be used for chords, one for each bar. Figure 13.5 provides a diagram of how these four elements contribute to the creation of a complete piece of music.<a id="marker-301"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="432" src="../../OEBPS/Images/CH13_F05_Liu.png" width="466"/></p>
<p class="figurecaption">Figure 13.5 Music generation using chords, style, melody, and groove. Each music composition consists of four tracks and spans two bars. We will extract four noise vectors from the latent space for this purpose. The first vector, representing chords, has a dimension of (1, 32). This vector will be processed through a temporal network to expand the chords into two (1, 32) vectors, corresponding to the two bars, with identical values across all tracks. The second vector, denoting style, also has a dimension of (1, 32) and remains constant across all tracks and bars. The third vector, melody, is shaped as (4, 32). It will be stretched through a temporal network into two (4, 32) vectors, resulting in eight (1, 32) vectors, each representing a unique track and bar combination. Lastly, the fourth vector, groove, with a dimension of (4, 32), will be applied to the four tracks, maintaining the same values for both bars.</p>
</div>
<p class="body">The generator creates a piece of music by generating one bar in one track at a time. For this, it requires four noise vectors, each with a shape of (1, 32), as input. These vectors represent chords, style, melody, and groove, and each controls a distinct aspect of the music, as previously explained. Since the music piece consists of four tracks, each with two bars, there are a total of eight bar/track combinations. Consequently, we need eight sets of chords, style, melody, and groove to generate all parts of the music piece.</p>
<p class="body">We’ll obtain four noise vectors from the latent space corresponding to chords, style, melody, and groove. We’ll also introduce a temporal network later, whose role is to expand the input along the bar dimension. With two bars, this means doubling the size of the input. Music is inherently temporal, with patterns and structures that unfold over time. The temporal network in MuseGAN is designed to capture these temporal dependencies, ensuring that the generated music has a coherent and logical progression.</p>
<p class="body">The noise vector for chords has a shape of (1, 32). After processing it through the temporal network, we obtain two (1, 32) sized vectors. The first vector is used across all four tracks in the first bar, while the second vector is used across all four tracks in the second bar.</p>
<p class="body">The noise vector for style, also with a shape of (1, 32), is applied uniformly across all eight track/bar combinations. Note that we’ll not pass the style vector through the temporal network since the style vector is designed to be the same across bars.</p>
<p class="body">The noise vector for melody has a shape of (4, 32). When passed through the temporal network, it yields two (4, 32) sized vectors, which further break down into eight (1, 32) sized vectors. Each of these is used in a unique track/bar combination.</p>
<p class="body">Lastly, the noise vector for groove, shaped as (4, 32), is used such that each (1, 32) sized vector is applied to a different track, remaining the same across both bars. We won’t pass the groove vector through the temporal network since the groove vector is designed to be the same across bars.</p>
<p class="body">After generating a bar of music for each of the eight bar/track combinations, we’ll merge them to create a full piece of music, consisting of four distinct tracks, each comprising two unique bars.<a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="idIndexMarker039"/><a id="marker-302"/><a id="idIndexMarker040"/></p>
<h3 class="fm-head1" id="heading_id_9">13.2.2 A blueprint to train a MuseGAN</h3>
<p class="body">Chapter 1 provided an overview of the foundational concepts behind GANs. In chapters 3 to 5, you explored the creation and training of GANs for generating shapes, numbers, and images. This subsection will summarize the steps for building and training MuseGAN, highlighting the differences from the previous chapters.<a id="idIndexMarker041"/></p>
<p class="body">The style of music generated by MuseGAN is influenced by the training data’s style. Therefore, you should first collect a dataset of Bach’s compositions in a format suitable for training. Next, you’ll create a MuseGAN model, which consists of a generator and a critic. The generator network takes four random noise vectors as input (chords, style, melody, and groove) and outputs a piece of music. The critic network evaluates a piece of music and assigns a rating, with higher scores for real music (from the training set) and lower scores for fake music (produced by the generator). Both the generator and critic networks utilize deep convolutional layers to capture the spatial features of the inputs.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="405" src="../../OEBPS/Images/CH13_F06_Liu.png" width="773"/></p>
<p class="figurecaption">Figure 13.6 A diagram of the steps involved in training MuseGAN to generate music. The generator produces a fake music piece by drawing four random noise vectors from the latent space (top left) and presents it to the critic (middle). The critic evaluates the piece and assigns a rating. A high rating suggests that the piece is likely from the training dataset, while a lower rating indicates that the piece is likely fake (generated by the generator). Additionally, an interpolated music piece created from a mix of real and fake samples (top left) is presented to the critic. The training process incorporates a gradient penalty based on the critic’s rating of this interpolated piece, which is added to the total loss. The ratings are then compared to the ground truth, allowing both the critic and the generator to learn from these evaluations. After numerous training iterations, the generator becomes proficient at producing music pieces that are virtually indistinguishable from real samples.</p>
</div>
<p class="body"><a id="marker-303"/>Figure 13.6 illustrates the training process of MuseGAN. The generator (the bottom left of the figure) receives four random noise vectors (chords, style, melody, and groove) as input and produces fake music pieces (step 1 in figure 13.6). These noise vectors are drawn from the latent space, which represents the range of potential outputs the GAN can generate, enabling the creation of diverse data samples. These fake music pieces, along with real ones from the training set (top right), are then evaluated by the critic (step 3). The critic (bottom center) assigns scores to all music pieces, aiming to give high scores to real music and low scores to fake music (step 4).</p>
<p class="body">To guide the adjustment of model parameters, appropriate loss functions must be chosen for both the generator and the critic. The generator’s loss function is designed to encourage the production of data points that closely resemble those from the training dataset. Specifically, the loss function for the generator is the negative of the critic’s rating. By minimizing this loss function, the generator strives to create music pieces that receive high ratings from the critic. On the other hand, the critic’s loss function is formulated to encourage accurate assessment of real and generated data points. Thus, the loss function for the critic is the rating itself if the music piece is from the training set and the negative of the rating if it is generated by the generator. In essence, the critic aims to assign high ratings to real music pieces and low ratings to fake ones.</p>
<p class="body">Additionally, we incorporate the Wasserstein distance with gradient penalty into the loss function, as we did in chapter 5, to enhance the training stability and performance of GAN models. To achieve this, an interpolated music piece, blending real and fake music (top left in figure 13.6), is evaluated by the critic. The gradient penalty, based on the critic’s rating of this interpolated piece, is then added to the total loss during the training process.</p>
<p class="body">Throughout the training loop, we alternate between training the critic and the generator. In each training iteration, we sample a batch of real music pieces from the training set and a batch of fake music pieces generated by the generator. We calculate the total loss by comparing the critic’s ratings (i.e., scores) with the ground truth (whether a music piece is real or fake). We then slightly adjust the weights in both the generator and critic networks so that, in subsequent iterations, the generator produces more realistic music pieces, and the critic assigns higher scores to real music and lower scores to fake music.</p>
<p class="body">Once MuseGAN is fully trained, music can be created by inputting four random noise vectors into the trained generator.<a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="marker-304"/></p>
<h2 class="fm-head" id="heading_id_10">13.3 Preparing the training data for MuseGAN</h2>
<p class="body">We’ll use chorale compositions by Johann Sebastian Bach as our training dataset, expecting the generated music to resemble Bach’s style. If you prefer the style of a different musician, you can use their work as the training data instead. In this section, we’ll start by downloading the training data and organizing it into batches for later training.<a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="idIndexMarker046"/></p>
<p class="body">Additionally, we’ve learned that the music pieces in the training set will be represented as 4D objects. In this section, you’ll also learn how to convert these multidimensional objects into playable music pieces on a computer. This conversion is essential because MuseGAN generates multidimensional objects similar to those in the training set. Later in the chapter, we’ll transform the multidimensional objects produced by MuseGAN into MIDI files, enabling you to listen to the generated music on your computer.</p>
<h3 class="fm-head1" id="heading_id_11">13.3.1 Downloading the training data</h3>
<p class="body">We’ll use the JSB Chorales piano rolls dataset as our training set. Go to Cheng-Zhi Anna Huang’s GitHub repository (<a class="url" href="https://github.com/czhuang/JSB-Chorales-dataset">https://github.com/czhuang/JSB-Chorales-dataset</a>) and download the music file Jsb16thSeparated.npz. Save the file in the /files/ directory on your computer.<a id="idIndexMarker047"/><a id="idIndexMarker048"/></p>
<p class="body">Then, download the two utility modules midi_util.py and MuseGAN_util.py from the book’s GitHub repository (<a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>) and save them in the /utils/ directory on your computer. The code in this chapter is adapted from the excellent GitHub repository by Azamat Kanametov (<a class="url" href="https://github.com/akanametov/musegan">https://github.com/akanametov/musegan</a>). With these files in place, we can now load the music files and organize them into batches for processing:</p>
<pre class="programlisting">from torch.utils.data import DataLoader
from utils.midi_util import MidiDataset
  
dataset = MidiDataset('files/Jsb16thSeparated.npz')
first_song=dataset[0]
print(first_song.shape)
loader = DataLoader(dataset, batch_size=64, 
                        shuffle=True, drop_last=True)</pre>
<p class="body">We load the dataset you just downloaded into Python, then extract the first song and name it <code class="fm-code-in-text">first_song</code>. Since songs are represented as multidimensional objects, we print out the shape of the first song. Finally, we place the training data in batches of 64, to be used later in the chapter.</p>
<p class="body">The output from the preceding code block is</p>
<pre class="programlisting">torch.Size([4, 2, 16, 84])</pre>
<p class="body">Each song in the dataset has a shape of (4, 2, 16, 84), as shown in the previous output. This indicates that each song consists of four tracks, each with two bars. Each bar contains 16 time steps, and at each time step, the musical note is represented by a one-hot vector with 84 values. In each one-hot vector, all values are set to –1, except for one position where the value is set to 1, indicating the presence of a note. You can verify the range of values in the dataset as follows:</p>
<pre class="programlisting">flat=first_song.reshape(-1,)
print(set(flat.tolist()))</pre>
<p class="body">The output is</p>
<pre class="programlisting">{1.0, -1.0}</pre>
<p class="body">The previous output shows that the values in each music piece are either –1 or 1.</p>
<h3 class="fm-head1" id="heading_id_12">13.3.2 Converting multidimensional objects to music pieces</h3>
<p class="body"><a id="marker-305"/>Currently, the songs are formatted as PyTorch tensors and are ready to be inputted into the MuseGAN model. However, before we proceed, it’s important to gain a better understanding of how to convert these multidimensional objects into playable music pieces on your computer. This will help us later to convert generated music pieces into playable files.<a id="idIndexMarker049"/><a id="idIndexMarker050"/></p>
<p class="body">To begin, we’ll convert all the 84-value one-hot variables into pitch numbers ranging from 0 to 83:</p>
<pre class="programlisting">import numpy as np
from music21 import note, stream, duration, tempo
  
parts = stream.Score()
parts.append(tempo.MetronomeMark(number=66))
max_pitches = np.argmax(first_song, axis=-1)                <span class="fm-combinumeral">①</span>
midi_note_score = max_pitches.reshape([2 * 16, 4])          <span class="fm-combinumeral">②</span>
print(midi_note_score)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Converts 84-value one-hot vectors to numbers between 0 and 83</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Reshapes the result to (32, 4)</p>
<p class="body">The output is</p>
<pre class="programlisting">tensor([[74, 74, 74, 74],
…
        [70, 70, 69, 69],
        [67, 67, 69, 69],
        [70, 70, 70, 70],
        [69, 69, 69, 69],
        [69, 69, 69, 69],
        [65, 65, 65, 65],
        [58, 58, 60, 60],
…
        [53, 53, 53, 53]])</pre>
<p class="body">In the output displayed here, each column represents a music track, with numbers ranging from 0 to 83. These numbers correspond to pitch numbers, as you have seen earlier in figure 13.1.</p>
<p class="body">Now, we’ll proceed to convert the tensor <code class="fm-code-in-text">midi_note_score</code> in the previous code block into an actual MIDI file, allowing you to play it on your computer.<a id="marker-306"/></p>
<p class="fm-code-listing-caption">Listing 13.2 Converting pitch numbers to a MIDI file</p>
<pre class="programlisting">for i in range(4):                                         <span class="fm-combinumeral">①</span>
    last_x = int(midi_note_score[:, i][0])
    s = stream.Part()
    dur = 0
    for idx, x in enumerate(midi_note_score[:, i]):        <span class="fm-combinumeral">②</span>
        x = int(x)
        if (x != last_x or idx % 4 == 0) and idx &gt; 0:
            n = note.Note(last_x)
            n.duration = duration.Duration(dur)
            s.append(n)
            dur = 0
        last_x = x
        dur = dur + 0.25                                   <span class="fm-combinumeral">③</span>
    n = note.Note(last_x)
    n.duration = duration.Duration(dur)
    s.append(n)                                            <span class="fm-combinumeral">④</span>
    parts.append(s)  
parts.write("midi","files/first_song.midi")</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Iterates through four music tracks</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Iterates through all notes in each track</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Adds 0.25 seconds to each time step</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Adds the note to the music stream</p>
<p class="body">After running the preceding code cell, you’ll see a MIDI file, <code class="fm-code-in-text">first_song.midi</code>, on your computer. Play it with a music player on your computer to get a sense of what type of music we are using to train the MuseGAN.<a id="idIndexMarker051"/><a id="idIndexMarker052"/><a id="idIndexMarker053"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 13.1</p>
<p class="fm-sidebar-text">Convert the second song in the training dataset into a MIDI file. Save it as <code class="fm-code-in-text1">second_song.midi</code> and play it using a music player on your computer. <a id="idIndexMarker054"/><a id="idIndexMarker055"/><a id="idIndexMarker056"/></p>
</div>
<h2 class="fm-head" id="heading_id_13">13.4 Building a MuseGAN</h2>
<p class="body">In essence, we will treat a music piece as an object with multiple dimensions. Using techniques from chapters 4 to 6, we will tackle this task using deep convolutional neural networks for their ability to effectively extract spatial features from multidimensional objects. In MuseGAN, we’ll construct a generator and a critic, similar to how a generator in image creation refines an image based on a critic’s feedback. The generator will produce a music piece as a 4D object.<a id="idIndexMarker057"/><a id="marker-307"/></p>
<p class="body">Both real music from our training set and fake music from the generator will be presented to the critic. The critic will score each piece from negative infinity to positive infinity, with higher scores indicating a higher likelihood of the music being real. The critic aims to give high scores to real music and low scores to fake music. Conversely, the generator aims to produce music that is indistinguishable from real music, thereby receiving high scores from the critic.</p>
<p class="body">In this section, we will build a MuseGAN model, comprising a generator network and a critic network. The critic network employs deep convolutional layers to extract distinct features from multidimensional objects, enhancing its ability to evaluate music pieces. On the other hand, the generator network utilizes deep transposed convolutional layers to produce feature maps aimed at generating realistic music pieces. Later, we will train the MuseGAN model using music pieces from the training set.</p>
<h3 class="fm-head1" id="heading_id_14">13.4.1 A critic in MuseGAN</h3>
<p class="body">As explained in chapter 5, incorporating the Wasserstein distance into the loss function can help stabilize training. Therefore, in MuseGAN, we adopt a similar approach and use a critic instead of a discriminator. The critic is not a binary classifier; rather, it evaluates the output of the generator (in this case, a music piece) and assigns a score ranging from –∞ to ∞. A higher score indicates a greater likelihood that the music is real (i.e., from the training set).<a id="idIndexMarker058"/><a id="idIndexMarker059"/></p>
<p class="body">We construct a music critic neural network as shown in the following listing, and its definition can be found in the file MuseGAN_util.py that you downloaded earlier.</p>
<p class="fm-code-listing-caption">Listing 13.3 The critic network in MuseGAN</p>
<pre class="programlisting">class MuseCritic(nn.Module):
    def __init__(self,hid_channels=128,hid_features=1024,
        out_features=1,n_tracks=4,n_bars=2,n_steps_per_bar=16,
        n_pitches=84):
        super().__init__()
        self.n_tracks = n_tracks
        self.n_bars = n_bars
        self.n_steps_per_bar = n_steps_per_bar
        self.n_pitches = n_pitches
        in_features = 4 * hid_channels if n_bars == 2\
            else 12 * hid_channels
        self.seq = nn.Sequential(
            nn.Conv3d(self.n_tracks, hid_channels, 
                      (2, 1, 1), (1, 1, 1), padding=0),      <span class="fm-combinumeral">①</span>
            nn.LeakyReLU(0.3, inplace=True),
            nn.Conv3d(hid_channels, hid_channels, 
              (self.n_bars - 1, 1, 1), (1, 1, 1), padding=0),
            nn.LeakyReLU(0.3, inplace=True),
            nn.Conv3d(hid_channels, hid_channels, 
                      (1, 1, 12), (1, 1, 12), padding=0),
            nn.LeakyReLU(0.3, inplace=True),
            nn.Conv3d(hid_channels, hid_channels, 
                      (1, 1, 7), (1, 1, 7), padding=0),
            nn.LeakyReLU(0.3, inplace=True),
            nn.Conv3d(hid_channels, hid_channels, 
                      (1, 2, 1), (1, 2, 1), padding=0),
            nn.LeakyReLU(0.3, inplace=True),
            nn.Conv3d(hid_channels, hid_channels, 
                      (1, 2, 1), (1, 2, 1), padding=0),
            nn.LeakyReLU(0.3, inplace=True),
            nn.Conv3d(hid_channels, 2 * hid_channels, 
                      (1, 4, 1), (1, 2, 1), padding=(0, 1, 0)),
            nn.LeakyReLU(0.3, inplace=True),
            nn.Conv3d(2 * hid_channels, 4 * hid_channels,     
                      (1, 3, 1), (1, 2, 1), padding=(0, 1, 0)),
            nn.LeakyReLU(0.3, inplace=True),
            nn.Flatten(),                                     <span class="fm-combinumeral">②</span>
            nn.Linear(in_features, hid_features),
            nn.LeakyReLU(0.3, inplace=True),
            nn.Linear(hid_features, out_features))            <span class="fm-combinumeral">③</span>
    def forward(self, x):  
        return self.seq(x)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Passes the input through several Conv3d layers</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Flattens the output</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Passes the output through two linear layers</p>
<p class="body">The input to the critic network is a music piece with dimensions (4, 2, 16, 84). The network primarily consists of several Conv3d layers. These layers treat each track of the music piece as a 3D object and apply filters to extract spatial features. The operation of the Conv3d layers is similar to the Conv2d layers used in image generation, as discussed in earlier chapters.<a id="idIndexMarker060"/></p>
<p class="body">It’s important to note that the final layer of the critic model is linear, and we do not apply any activation function to its output. As a result, the output from the critic model is a value ranging from –∞ to ∞, which can be interpreted as the critic’s rating of a music piece.<a id="marker-308"/></p>
<h3 class="fm-head1" id="heading_id_15">13.4.2 A generator in MuseGAN</h3>
<p class="body">As discussed earlier in this chapter, the generator will produce one bar of music at a time, and we will then combine these eight bars to form a complete piece of music.<a id="idIndexMarker061"/><a id="idIndexMarker062"/></p>
<p class="body">Instead of using just a single noise vector, the generator in MuseGAN takes four independent noise vectors as input to control various aspects of the music being generated. Two of these vectors will be processed through a temporal network to extend them along the bar dimension. While the style and groove vectors are designed to remain constant across both bars, the chords and melody vectors are designed to vary between bars. Therefore, we will first establish a temporal network to stretch the chords and melody vectors across the two bars, ensuring that the generated music has a coherent and logical progression over time.</p>
<p class="body">In the local module <code class="fm-code-in-text">MuseGAN_util</code> you downloaded earlier, we define the <code class="fm-code-in-text">TemporalNetwork()</code> class as follows:<a id="idIndexMarker063"/><a id="idIndexMarker064"/></p>
<pre class="programlisting">class TemporalNetwork(nn.Module):
    def __init__(self,z_dimension=32,hid_channels=1024,n_bars=2):
        super().__init__()
        self.n_bars = n_bars
        self.net = nn.Sequential(
            Reshape(shape=[z_dimension, 1, 1]),                <span class="fm-combinumeral">①</span>
            nn.ConvTranspose2d(z_dimension,hid_channels,
                kernel_size=(2, 1),stride=(1, 1),padding=0,),
            nn.BatchNorm2d(hid_channels),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(hid_channels,z_dimension,
                kernel_size=(self.n_bars - 1, 1),stride=(1, 1),
                padding=0,),
            nn.BatchNorm2d(z_dimension),
            nn.ReLU(inplace=True),
            Reshape(shape=[z_dimension, self.n_bars]),)        <span class="fm-combinumeral">②</span>
    def forward(self, x):
        return self.net(x)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The input dimension to the TemporalNetwork() class is (1, 32).</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The output dimension is (2, 32).</p>
<p class="body">The <code class="fm-code-in-text">TemporalNetwork()</code> class described here employs two ConvTranspose2d layers to expand a single noise vector into two distinct noise vectors, each corresponding to one of the two bars. As we covered in chapter 4, transposed convolutional layers serve the purpose of upsampling and generating feature maps. In this context, they are utilized to extend noise vectors across different bars.<a id="marker-309"/><a id="idIndexMarker065"/></p>
<p class="body">Instead of generating all bars in all tracks at once, we’ll generate the music one bar at a time. Doing so allows MuseGAN to balance computational efficiency, flexibility, and musical coherence, resulting in more structured and appealing musical compositions. Therefore, we proceed to construct a bar generator that is responsible for generating a segment of the music piece: one bar within a track. We introduce the <code class="fm-code-in-text">BarGenerator()</code> class within the local <code class="fm-code-in-text">MuseGAN_util</code> module:<a id="idIndexMarker066"/><a id="idIndexMarker067"/></p>
<pre class="programlisting">class BarGenerator(nn.Module):
    def __init__(self,z_dimension=32,hid_features=1024,hid_channels=512,
        out_channels=1,n_steps_per_bar=16,n_pitches=84):
        super().__init__()
        self.n_steps_per_bar = n_steps_per_bar
        self.n_pitches = n_pitches
        self.net = nn.Sequential(
            nn.Linear(4 * z_dimension, hid_features),              <span class="fm-combinumeral">①</span>
            nn.BatchNorm1d(hid_features),
            nn.ReLU(inplace=True),
            Reshape(shape=[hid_channels,hid_features//hid_channels,1]),    
            nn.ConvTranspose2d(hid_channels,hid_channels,
               kernel_size=(2, 1),stride=(2, 1),padding=0),        <span class="fm-combinumeral">②</span>
            nn.BatchNorm2d(hid_channels),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(hid_channels,hid_channels // 2,
                kernel_size=(2, 1),stride=(2, 1),padding=0),
            nn.BatchNorm2d(hid_channels // 2),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(hid_channels // 2,hid_channels // 2,
                kernel_size=(2, 1),stride=(2, 1),padding=0),
            nn.BatchNorm2d(hid_channels // 2),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(hid_channels // 2,hid_channels // 2,
                kernel_size=(1, 7),stride=(1, 7),padding=0),
            nn.BatchNorm2d(hid_channels // 2),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(hid_channels // 2,out_channels,
                kernel_size=(1, 12),stride=(1, 12),padding=0),
            Reshape([1, 1, self.n_steps_per_bar, self.n_pitches])) <span class="fm-combinumeral">③</span>
    def forward(self, x):
        return self.net(x)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> We concatenate chords, style, melody, and groove into one vector, with a size of 4 * 32.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The input is then reshaped into 2D, and we use several ConvTranspose2d layers for upsampling and music feature generation.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The output has a shape of (1, 1, 16, 84): 1 track, 1 bar, and 16 notes, and each note is represented by a 84-value vector.</p>
<p class="body"><a id="marker-310"/>The <code class="fm-code-in-text">BarGenerator()</code> class accepts four noise vectors as input, each representing chords, style, melody, and groove for a specific bar in a different track, all with a shape of (1, 32). These vectors are concatenated into a single 128-value vector before being fed into the <code class="fm-code-in-text">BarGenerator()</code> class. The output from the <code class="fm-code-in-text">BarGenerator()</code> class is a bar of music, with dimensions (1, 1, 16, 84), indicating 1 track, 1 bar, and 16 notes, with each note represented by an 84-value vector.<a id="idIndexMarker068"/><a id="idIndexMarker069"/><a id="idIndexMarker070"/></p>
<p class="body">Finally, we will employ the <code class="fm-code-in-text">MuseGenerator()</code> class to generate a complete piece of music, consisting of four tracks with two bars per track. Each bar is constructed using the <code class="fm-code-in-text">BarGenerator()</code> class defined earlier. To achieve this, we define the <code class="fm-code-in-text">MuseGenerator()</code> class in the local MuseGAN_util module.<a id="idIndexMarker071"/><a id="idIndexMarker072"/><a id="idIndexMarker073"/><a id="idIndexMarker074"/></p>
<p class="fm-code-listing-caption">Listing 13.4 The music generator in MuseGAN</p>
<pre class="programlisting">class MuseGenerator(nn.Module):
    def __init__(self,z_dimension=32,hid_channels=1024,
        hid_features=1024,out_channels=1,n_tracks=4,
        n_bars=2,n_steps_per_bar=16,n_pitches=84):
        super().__init__()
        self.n_tracks = n_tracks
        self.n_bars = n_bars
        self.n_steps_per_bar = n_steps_per_bar
        self.n_pitches = n_pitches
        self.chords_network=TemporalNetwork(z_dimension, 
                            hid_channels, n_bars=n_bars)
        self.melody_networks = nn.ModuleDict({})
        for n in range(self.n_tracks):
            self.melody_networks.add_module(
                "melodygen_" + str(n),
                TemporalNetwork(z_dimension, 
                 hid_channels, n_bars=n_bars))
        self.bar_generators = nn.ModuleDict({})
        for n in range(self.n_tracks):
            self.bar_generators.add_module(
                „bargen_" + str(n),BarGenerator(z_dimension,
            hid_features,hid_channels // 2,out_channels,
            n_steps_per_bar=n_steps_per_bar,n_pitches=n_pitches))
    def forward(self,chords,style,melody,groove):
        chord_outs = self.chords_network(chords)
        bar_outs = []
        for bar in range(self.n_bars):                           <span class="fm-combinumeral">①</span>
            track_outs = []
            chord_out = chord_outs[:, :, bar]
            style_out = style
            for track in range(self.n_tracks):                   <span class="fm-combinumeral">②</span>
                melody_in = melody[:, track, :]
                melody_out = self.melody_networks["melodygen_"\
                          + str(track)](melody_in)[:, :, bar]
                groove_out = groove[:, track, :]
                z = torch.cat([chord_out, style_out, melody_out,\
                               groove_out], dim=1)               <span class="fm-combinumeral">③</span>
                track_outs.append(self.bar_generators["bargen_"\
                                          + str(track)](z))      <span class="fm-combinumeral">④</span>
            track_out = torch.cat(track_outs, dim=1)
            bar_outs.append(track_out)
        out = torch.cat(bar_outs, dim=2)                         <span class="fm-combinumeral">⑤</span>
        return out</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Iterates through two bars</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Iterates through four tracks</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Concatenates chords, style, melody, and groove into one input</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Generates one bar using the bar generator</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Concatenates eight bars into one complete piece of music</p>
<p class="body">The generator takes four noise vectors as inputs. It iterates through four tracks and two bars. In each iteration, it utilizes the bar generator to create a single bar of music. Upon completing all iterations, the MuseGenerator() class merges the eight bars into one cohesive music piece, which has dimensions of (4, 2, 16, 84). <a id="idIndexMarker075"/><a id="idIndexMarker076"/></p>
<h3 class="fm-head1" id="heading_id_16">13.4.3 Optimizers and the loss function</h3>
<p class="body"><a id="marker-311"/>We create a generator and a critic based on the <code class="fm-code-in-text">MuseGenerator()</code> and <code class="fm-code-in-text">MuseCritic()</code> classes in the local module:<a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="idIndexMarker079"/><a id="idIndexMarker080"/><a id="idIndexMarker081"/></p>
<pre class="programlisting">import torch
from utils.MuseGAN_util import (init_weights, MuseGenerator, MuseCritic)
  
device = "cuda" if torch.cuda.is_available() else "cpu"
generator = MuseGenerator(z_dimension=32, hid_channels=1024, 
              hid_features=1024, out_channels=1).to(device)
critic = MuseCritic(hid_channels=128,
                    hid_features=1024,
                    out_features=1).to(device)
generator = generator.apply(init_weights)
critic = critic.apply(init_weights) </pre>
<p class="body">As we discussed in chapter 5, the critic generates a rating instead of a classification, so the loss function is defined as the negative average of the product between the prediction and the target. As a result, we define the following <code class="fm-code-in-text">loss_fn()</code> function in the local module <code class="fm-code-in-text">MuseGAN_util</code>:<a id="idIndexMarker082"/><a id="idIndexMarker083"/></p>
<pre class="programlisting">def loss_fn(pred, target):
    return -torch.mean(pred*target)</pre>
<p class="body">During training, for the generator, we’ll assign a value of 1 to the target argument in the <code class="fm-code-in-text">loss_fn()</code> function. This setting aims to guide the generator in producing music that can achieve the highest possible rating (i.e., the variable pred in the <code class="fm-code-in-text">loss_fn()</code> function). For the critic, we’ll set the target to 1 for real music and –1 for fake music in the loss function. This setting guides the critic to assign a high rating to real music and a low rating to fake music.<a id="idIndexMarker084"/><a id="idIndexMarker085"/></p>
<p class="body">Similar to the approach in chapter 5, we incorporate the Wasserstein distance with a gradient penalty into the critic’s loss function to ensure training stability. The gradient penalty is defined in the <code class="fm-code-in-text">MuseGAN_util.py</code> file as follows:<a id="idIndexMarker086"/><a id="marker-312"/></p>
<pre class="programlisting">class GradientPenalty(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, inputs, outputs):
        grad = torch.autograd.grad(
            inputs=inputs,
            outputs=outputs,
            grad_outputs=torch.ones_like(outputs),
            create_graph=True,
            retain_graph=True,
        )[0]
        grad_=torch.norm(grad.view(grad.size(0),-1),p=2,dim=1)
        penalty = torch.mean((1. - grad_) ** 2)
        return penalty</pre>
<p class="body">The <code class="fm-code-in-text">GradientPenalty()</code> class requires two inputs: interpolated music, which is a blend of real and fake music, and the ratings assigned by the critic network to this interpolated music. The class computes the gradient of the critic’s ratings concerning the interpolated music. The gradient penalty is then calculated as the squared difference between the norms of these gradients and the target value of 1, following a similar approach to what we did in chapter 5.<a id="idIndexMarker087"/></p>
<p class="body">As usual, we’ll use the Adam optimizer for both the critic and the generator:</p>
<pre class="programlisting">lr = 0.001
g_optimizer = torch.optim.Adam(generator.parameters(),
                               lr=lr, betas=(0.5, 0.9))
c_optimizer = torch.optim.Adam(critic.parameters(),
                               lr=lr, betas=(0.5, 0.9))</pre>
<p class="body">With that, we have successfully constructed a MuseGAN, which is now ready to be trained using the data we prepared earlier in the chapter. <a id="idIndexMarker088"/></p>
<h2 class="fm-head" id="heading_id_17">13.5 Training the MuseGAN to generate music</h2>
<p class="body">Now that we have both the MuseGAN model and the training data, we’ll proceed to train the model in this section.<a id="idIndexMarker089"/></p>
<p class="body">Similar to our approach in chapters 3 and 4, when training GANs, we’ll alternate between training the critic and the generator. In each training iteration, we’ll sample a batch of real music from the training dataset and a batch of generated music from the generator and present them to the critic for evaluation. During critic training, we compare the critic’s ratings with the ground truth and adjust the critic network’s weights slightly so that, in the next iteration, the ratings will be as high as possible for real music and as low as possible for generated music. During generator training, we feed generated music to the critic model to obtain a rating and then slightly adjust the generator network’s weights so that, in the next iteration, the rating will be higher (as the generator aims to create music pieces that fool the critic into thinking they are real). We repeat this process for many iterations, gradually enabling the generator network to create more realistic music pieces.</p>
<p class="body">Once the model is trained, we’ll discard the critic network and use the trained generator to create music pieces by feeding it four noise vectors (chords, style, melody, and groove).</p>
<h3 class="fm-head1" id="heading_id_18">13.5.1 Training the MuseGAN</h3>
<p class="body">Before we embark on the training loops for the MuseGAN model, we first define a few hyperparameters and helper functions. The hyperparameter <code class="fm-code-in-text">repeat</code> controls how many times we train the critic in each iteration, <code class="fm-code-in-text">display_step</code> specifies how often we display output, and <code class="fm-code-in-text">epochs</code> is the number of epochs we train the model.<a id="idIndexMarker090"/><a id="idIndexMarker091"/><a id="idIndexMarker092"/><a id="idIndexMarker093"/><a id="marker-313"/></p>
<p class="fm-code-listing-caption">Listing 13.5 Hyperparameters and helper functions</p>
<pre class="programlisting">from utils.MuseGAN_util import loss_fn, GradientPenalty
  
batch_size=64
repeat=5
display_step=10
epochs=500                                                         <span class="fm-combinumeral">①</span>
alpha=torch.rand((batch_size,1,1,1,1)).requires_grad_().to(device) <span class="fm-combinumeral">②</span>
gp=GradientPenalty()                                               <span class="fm-combinumeral">③</span>
  
def noise():                                                       <span class="fm-combinumeral">④</span>
    chords = torch.randn(batch_size, 32).to(device)
    style = torch.randn(batch_size, 32).to(device)
    melody = torch.randn(batch_size, 4, 32).to(device)
    groove = torch.randn(batch_size, 4, 32).to(device)
    return chords,style,melody,groove</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines a few hyperparameters</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines alpha to create interpolated music</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Defines a gp() function to calculate gradient penalty</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines a noise() function to retrieve four random noise vectors</p>
<p class="body">The batch size is set at 64, and this helps us determine how many sets of random noise vectors to retrieve to create a batch of fake music. We’ll train the critic for five iterations and the generator just once in each training loop because an effective critic is essential for training the generator. We’ll display training losses after every 10 epochs. We’ll train the model for 500 epochs.</p>
<p class="body">We instantiate the <code class="fm-code-in-text">GradientPenalty()</code> class in the local module to create a <code class="fm-code-in-text">gp()</code> function to calculate the gradient penalty. We also define a <code class="fm-code-in-text">noise()</code> function to generate four random noise vectors to feed to the generator. <a id="idIndexMarker094"/><a id="idIndexMarker095"/><a id="idIndexMarker096"/></p>
<p class="body">Next, we define the following function, <code class="fm-code-in-text">train_epoch()</code>, to train the model for one epoch.<a id="idIndexMarker097"/></p>
<p class="fm-code-listing-caption">Listing 13.6 Training the MuseGAN model for one epoch</p>
<pre class="programlisting">def train_epoch():
    e_gloss = 0
    e_closs = 0
    for real in loader:                                            <span class="fm-combinumeral">①</span>
        real = real.to(device)
        for _ in range(repeat):                                    <span class="fm-combinumeral">②</span>
            chords,style,melody,groove=noise()
            c_optimizer.zero_grad()
            with torch.no_grad():
                fake = generator(chords, style, melody,groove).detach()
            realfake = alpha * real + (1 - alpha) * fake
            fake_pred = critic(fake)
            real_pred = critic(real)
            realfake_pred = critic(realfake)
            fake_loss =  loss_fn(fake_pred,-torch.ones_like(fake_pred))
            real_loss = loss_fn(real_pred,torch.ones_like(real_pred))
            penalty = gp(realfake, realfake_pred)
            closs = fake_loss + real_loss + 10 * penalty           <span class="fm-combinumeral">③</span>
            closs.backward(retain_graph=True)
            c_optimizer.step()
            e_closs += closs.item() / (repeat*len(loader))
        g_optimizer.zero_grad()
        chords,style,melody,groove=noise()
        fake = generator(chords, style, melody, groove)
        fake_pred = critic(fake)
        gloss = loss_fn(fake_pred, torch.ones_like(fake_pred))     <span class="fm-combinumeral">④</span>
        gloss.backward()
        g_optimizer.step()
        e_gloss += gloss.item() / len(loader)
    return e_gloss, e_closs </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Iterates through all batches</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Trains the critic five times in each iteration</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The total loss for the critic has three components: loss from evaluating real music, loss from evaluating fake music, and the gradient penalty loss.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Trains the generator</p>
<p class="body"><a id="marker-314"/>The training process is very much like that we used in chapter 5 when we train the conditional GAN with gradient penalty.</p>
<p class="body">We now train the model for 500 epochs:</p>
<pre class="programlisting">for epoch in range(1,epochs+1):
    e_gloss, e_closs = train_epoch()
    if epoch % display_step == 0:
        print(f"Epoch {epoch}, G loss {e_gloss} C loss {e_closs}")</pre>
<p class="body">If you use GPU training, it takes about an hour. Otherwise, it may take several hours. Once done, you can save the trained generator to the local folder as follows:</p>
<pre class="programlisting">torch.save(generator.state_dict(),'files/MuseGAN_G.pth')</pre>
<p class="body">Alternatively, you can download the trained generator from my website: <a class="url" href="https://mng.bz/Bglr">https://mng.bz/Bglr</a>.</p>
<p class="body">Next, we’ll discard the critic network and use the trained generator to create music that mimics the style of Bach. <a id="idIndexMarker098"/></p>
<h3 class="fm-head1" id="heading_id_19">13.5.2 Generating music with the trained MuseGAN</h3>
<p class="body">To generate music with the trained generator, we’ll feed four noise vectors from the latent space to the generator. Note that we can generate multiple music objects at the same time and decode them together to form one continuous piece of music. You’ll learn how to do that in this subsection. <a id="idIndexMarker099"/><a id="marker-315"/></p>
<p class="body">We first load the trained weights in the generator:</p>
<pre class="programlisting">generator.load_state_dict(torch.load('files/MuseGAN_G.pth',
    map_location=device))</pre>
<p class="body">Rather than producing a single 4D music object, we can simultaneously generate multiple 4D music objects and convert them into one continuous piece of music later. For instance, if we aim to create five music objects, we begin by sampling five sets of noise vectors from the latent spaces. Each set consists of four vectors: chords, style, melody, and groove, like so:</p>
<pre class="programlisting">num_pieces = 5
chords = torch.rand(num_pieces, 32).to(device)
style = torch.rand(num_pieces, 32).to(device)
melody = torch.rand(num_pieces, 4, 32).to(device)
groove = torch.rand(num_pieces, 4, 32).to(device)</pre>
<p class="body">Each generated music object can be transformed into a music piece that lasts approximately 8 seconds. In this case, we choose to generate five music objects and decode them into a single music piece later, resulting in a duration of about 40 seconds. You can adjust the value of the variable <code class="fm-code-in-text">num_pieces</code> according to your preference, depending on the desired length of the music piece.<a id="idIndexMarker100"/></p>
<p class="body">Next, we supply the generator with the five sets of latent variables to produce a set of music objects:</p>
<pre class="programlisting">preds = generator(chords, style, melody, groove).detach()</pre>
<p class="body">The output, <code class="fm-code-in-text">preds</code>, consists of five music objects. Next, we decode these objects into a single piece of music, represented as a MIDI file:</p>
<pre class="programlisting">from utils.midi_util import convert_to_midi
  
music_data = convert_to_midi(preds.cpu().numpy())
music_data.write('midi', 'files/MuseGAN_song.midi')</pre>
<p class="body">We import the <code class="fm-code-in-text">convert_to_midi()</code> function from the local module <code class="fm-code-in-text">midi_util</code>. Open the file <code class="fm-code-in-text">midi_util.py</code> that you downloaded earlier and review the definition of the <code class="fm-code-in-text">convert_to_midi()</code> function. This process is similar to what we have done earlier in this chapter when we converted the first music object in the training set into the file <code class="fm-code-in-text">first_song.midi</code>. Since MIDI files represent sequences of notes over time, we simply concatenate the five music pieces corresponding to the five music objects into one extended sequence of notes. This combined sequence is then saved as <code class="fm-code-in-text">MuseGAN_song.midi</code> on your computer.<a id="idIndexMarker101"/><a id="idIndexMarker102"/><a id="idIndexMarker103"/></p>
<p class="body">Find the generated music piece, <code class="fm-code-in-text">MuseGAN_song.midi</code>, on your computer. Open it with a music player of your choice and listen to see if it resembles the music pieces from the training set. For comparison, you can listen to a piece of music generated by the trained model on my website at <a class="url" href="https://mng.bz/dZJv">https://mng.bz/dZJv</a>. Note that since the input to the generator, the noise vectors, are randomly drawn from the latent space, the music pieces you generate will sound different.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 13.2</p>
<p class="fm-sidebar-text">Obtain three sets of random noise vectors (each set should contain chords, style, melody, and groove) from the latent space. Feed them to the trained generator to obtain three music objects. Decode them into one single piece of music in the form of a MIDI file. Save it as <code class="fm-code-in-text1">generated_song.midi</code> on your computer, and play it using a music player.</p>
</div>
<p class="body"><a id="marker-316"/>In this chapter, you’ve learned how to build and train a MuseGAN to generate music in the style of Bach. Specifically, you’ve approached a piece of music as a 4D object and applied the techniques from chapter 4 on deep convolutional layers to develop a GAN model. In the next chapter, you’ll explore a different way of generating music: treating a piece of music as a sequence of indexes and utilizing techniques from NLP to generate music pieces by predicting one index at a time.<a id="idIndexMarker104"/></p>
<h2 class="fm-head" id="heading_id_20">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">MuseGAN treats a piece of music as a multidimensional object akin to an image. The generator produces a piece of music and submits it, along with real music pieces from the training set, to the critic for evaluation. The generator then modifies the music based on the critic’s feedback until it closely resembles real music from the training dataset.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Musical notes, octaves, and pitch are fundamental concepts in music theory. Octaves represent different levels of musical sound. Each octave is subdivided into 12 semitones: C, C#, D, D#, E, F, F#, G, G#, A, A#, B. Within an octave, a note is assigned a specific pitch number.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In electronic music production, a track typically refers to an individual layer or component of the music. Each track contains multiple bars (or measures). A bar is further divided into multiple steps.</p>
</li>
<li class="fm-list-bullet">
<p class="list">To represent a piece of music as a multidimensional object, we structure it with a (4, 2, 16, 84) shape: 4 music tracks, with each track consisting of 2 bars, each bar containing 16 steps, and each step capable of playing 1 of the 84 different notes.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In music creation, incorporating more detailed inputs is essential for achieving greater control and variety. Instead of using a single noise vector from the latent space for generating shapes, numbers, and images as in previous chapters, we employ four distinct noise vectors in the music generation process. Given that each music piece consists of four tracks and two bars, we use these four vectors to effectively manage this structure. One vector controls all tracks and bars collectively, another controls each bar across all tracks, a third oversees all tracks across bars, and the fourth manages each individual bar in each track.<a id="marker-317"/></p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">1</a></sup>  Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, Yi-Hsuan Yang, 2017, “MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment.” <a class="url" href="https://arxiv.org/abs/1709.06298">https://arxiv.org/abs/1709.06298</a>.</p>
</div></body></html>