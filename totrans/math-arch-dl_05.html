<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="ch-bayesian-tools-for-ml">6 Bayesian tools for machine learning</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Unsupervised machine learning models</li>
<li class="co-summary-bullet">Bayes’ theorem, conditional probability, entropy, cross-entropy, and conditional entropy</li>
<li class="co-summary-bullet">Maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation of model parameters</li>
<li class="co-summary-bullet">Evidence maximization</li>
<li class="co-summary-bullet">KLD</li>
<li class="co-summary-bullet">Gaussian mixture models (GMM) and MLE estimation of GMM parameters</li>
</ul>
<p class="body"><a id="marker-193"/>The Bayesian approach to statistics tries to model the world by modeling the uncertainties and prevailing beliefs and knowledge about the system. This is in contrast to the frequentist paradigm, where probability is strictly measured by observing a phenomenon repeatedly and measuring the fraction of time an event occurs. Machine learning, in particular <i class="fm-italics">unsupervised</i> machine learning, is a lot closer to the Bayesian paradigm of statistics—the subject of this chapter.</p>
<p class="body">In chapter <a class="url" href="../Text/01.xhtml#chap-overview">1</a>, we primarily discussed <i class="fm-italics">supervised</i> machine learning, where the training data is labeled: each input value is accompanied by a manually created desired output value. Labeling training inputs is a manual, labor-intensive process and often the worst pain point in building a machine learning–based system. This has led to considerable recent interest in <i class="fm-italics">unsupervised</i> machine learning, where we build a model from <i class="fm-italics">unlabeled</i> training data. How is this done?</p>
<p class="body">The general approach is best visualized geometrically. Each input data instance is a point in a high-dimensional space. These points form an overall pattern in the space of all possible inputs. If the inputs all have a common property, the points are not distributed randomly over the input space. Rather, they occupy a region in the input space with a definite shape. If the inputs have multiple classes, each class occupies a separate cluster in the space. Sometimes we apply a transformation to the input first—the transform is chosen or learned so that the transformed points exhibit a pattern more clearly than raw input points. We then identify a probability distribution whose sample point cloud matches the shape of the (potentially transformed) training data point cloud. We can generate faux input by sampling from this distribution. We can also classify an arbitrary input by observing which cluster it falls into.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The complete PyTorch code for this chapter is available at <a class="url" href="http://mng.bz/WdZa">http://mng.bz</a> <a class="url" href="http://mng.bz/WdZa">/WdZa</a> in the form of fully functional and executable Jupyter notebooks.</p>
<h2 class="fm-head" id="conditional-probability-and-bayes-theorem">6.1 Conditional probability and Bayes’ theorem</h2>
<p class="body"><a id="marker-194"/>As usual, the discussion is accompanied by examples. In this context, we first offer a refresher on the concepts of joint and marginal probability from section <a class="url" href="../Text/05.xhtml#sec-joint-prob">5.4</a> (you may want to revisit the topic of joint probability in sections <a class="url" href="../Text/05.xhtml#sec-joint-prob">5.4</a>, <a class="url" href="../Text/05.xhtml#sec-marginal-prob">5.4.1</a>, and <a class="url" href="../Text/05.xhtml#sec-joint-prob-depend">5.4.2</a>).</p>
<p class="body">Consider two random variables: the height and weight of adult Statsville residents. Weight (denoted <i class="timesitalic">W</i>) can take three quantized values: <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span>. Height (<i class="timesitalic">H</i>) can also take three quantized values: <span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">3</sub></span>. Table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-marginal-prob">6.1</a> shows their joint probability.</p>
<h3 class="fm-head1" id="sec-joint-marginal-prob-recap">6.1.1 Joint and marginal probability revisited</h3>
<p class="body">One glance at table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-marginal-prob">6.1</a> tells us that the probabilities are concentrated along the main diagonal, which indicates dependent events. This can be validated by inspecting one joint probability—say, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>)</span>—and the corresponding marginal probabilities <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>)</span> and <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>)</span>. We can see that <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.2 ≠ <i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) × <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) = 0.26 × 0.26</span>, establishing that the random variables weight <i class="timesitalic">W</i> and height <i class="timesitalic">H</i> are not independent. For contrast, look at table <a class="url" href="../Text/05.xhtml#tab-jmarginal-prob">5.6</a>. In that case, for any valid <span class="math"><i class="fm-italics">i</i>, <i class="fm-italics">j</i></span> pair, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">i</sub></i>, <i class="fm-italics">G<sub class="fm-subscript">j</sub></i>) = <i class="fm-italics">p</i>(<i class="fm-italics">G<sub class="fm-subscript">i</sub></i>) × <i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">j</sub></i>)</span>: the two events (weight and distance of a resident’s home from the city center) are independent. Note the following:</p>
<p class="fm-table-caption">Table 6.1 Example population sizes and joint probability distribution for variables <span class="math"><i class="fm-italics">W</i> = {<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">E</i><sub class="fm-subscript">3</sub>}</span> and <span class="math"><i class="fm-italics">H</i> = {<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>}</span> weights and heights of adult Statsville residents), showing marginal probabilities</p>
<table border="1" class="contenttable-1-table" id="tab-joint-depend-with-marginal-prob" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head"/>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Less than 60 kg (<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Between 60 and 90 kg (<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">More than 90 kg (<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Marginals for Fs</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Less than 160 cm</b> <b class="fm-bold"><span class="math">(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>)</span></b></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 20,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.2</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 4,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.04</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 2,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.02</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 26,000;</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.2 + 0.04 + 0.02 = 0.26</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Between 160 cm and 183 cm</b> (<span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">2</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 4,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.04</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 40,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.4</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 4,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.04</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 48,000;</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.04 + 0.4 + 0.04 = 0.48</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">More than 183 cm</b> (<span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">3</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 2,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.02</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 4,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.04</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 20,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.2</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 26,000;</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.02 + 0.04 + 0.2 = 0.26</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Marginals for</b> <b class="fm-bold"><i class="timesitalic">E</i>s</b></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.2 + 0.04 + 0.02 = 0.26</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.04 + 0.4 + 0.04 = 0.48</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>)</span></p>
<p class="fm-table-body"><span class="math">= 0.02 + 0.04 + 0.2 = 0.26</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Total pop. = 100,000;</p>
<p class="fm-table-body">Total prob = <span class="math">1</span></p>
</td>
</tr>
</tbody>
</table>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Joint probability</i>—This is the probability of a specific combination of values occurring <i class="fm-italics">together</i>. Each cell in table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-marginal-prob">6.1</a> depicts one joint probability: for example, the probability that a resident’s weight is between <span class="math">60</span> and <span class="math">90</span> kg <i class="fm-italics">and</i> that their height is greater than <span class="math">183</span> cm is <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.04</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Sum rule</i>—The joint probabilities of all possible variable combinations sum to 1 (bottom right cell in table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-marginal-prob">6.1</a>):<a id="marker-195"/></p>
</li>
</ul><!--<p class="Body"><span class="times">$$\sum\displaylimits_{i=1}^{3}\sum\displaylimits_{j =1}^{3} p\left(F_{i} , E_{j}\right) = 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_06-00-a.png" width="160"/></p>
</div>
<p class="body-ind">The sum of probabilities is the probability of one or another of the corresponding events occurring. Here we are adding all possible event combinations—one or another of these combinations will certainly occur. Hence the sum is <span class="math">1</span>, which matches our intuition.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Marginal probability for a variable</i>—This is obtained by “summing away” the other variables (right-most column and bottom-most row in table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-marginal-prob">6.1</a>):</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\begin{aligned} p\left(E_{j}\right) &amp;= \sum\displaylimits_{i=1}^{3} p\left(F_{i} , E_{j}\right)\\ p\left(F_{i}\right) &amp;= \sum\displaylimits_{j=1}^{3} p\left(F_{i} , E_{j}\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="124" src="../../OEBPS/Images/eq_06-00-b.png" width="169"/></p>
</div>
<p class="body-ind">We have added all possible combinations of other variables, so the sum represents the probability of this one variable.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Marginal probabilities</i>—These sum to <span class="math">1</span>:</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\sum\displaylimits_{j=1}^{3} p\left(E_{j}\right)
= \sum\displaylimits_{i=1}^{3} p\left(F_{i}\right) = 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="69" src="../../OEBPS/Images/eq_06-00-c.png" width="191"/></p>
</div>
<p class="body-ind">The sum of the marginal probabilities is the sum of all possible joint probabilities.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Dependent vs. independent variables</i>—If and only if the variables are independent, the product of the marginal probabilities is the same as the joint probability:</p>
</li>
</ul>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F<sub class="fm-subscript">i</sub> , E<sub class="fm-subscript">j</sub></i>) ≠ <i class="fm-italics">p</i>(<i class="fm-italics">F<sub class="fm-subscript">i</sub></i>) × <i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">j</sub></i>) <span class="cambria">⟺</span> for dependent variables in table <a class="url" href="../Text/05.xhtml#tab-jmarginal-prob">5.6</a></span></p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">G<sub class="fm-subscript">i</sub> , E<sub class="fm-subscript">j</sub></i>) = <i class="fm-italics">p</i>(<i class="fm-italics">G<sub class="fm-subscript">i</sub></i>) × <i class="fm-italics">p</i>(<i class="fm-italics">E<sub class="fm-subscript">j</sub></i>) <span class="cambria">⟺</span> for independent variables in table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-marginal-prob">6.1</a></span></p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left( F_{i} , E_{j} \right)  \neq  p\left( F_{i}  \right)  \times p\left( E_{j} \right)  &amp;\iff \text{for dependent variables in table
\ref{tab-jmarginal-prob} }\\ p\left( G_{i} , E_{j} \right)  = p\left( G_{i}  \right)  \times p\left( E_{j} \right)  &amp;\iff \text{for independent variables in table \ref{tab-joint-depend-with-marginal-prob} }\end{aligned}$$</span></p>
<div class="figure">
<p class="figure2"><img src="imgs/equations/eq_06-00-d.png" alt="" /></p>
</div>-->
<p class="body">You should verify that this condition is <i class="fm-italics">not satisfied</i> in table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-marginal-prob">6.1</a> for the weight and height variables. It <i class="fm-italics">is satisfied</i> in table <a class="url" href="../Text/05.xhtml#tab-jmarginal-prob">5.6</a> for the weight and distance-of-home-from-city-center variables.</p>
<h3 class="fm-head1" id="sec-cond-prob-bayes">6.1.2 Conditional probability</h3>
<p class="body"><a id="marker-196"/>Suppose we know that the height of a subject is between <span class="math">160</span> and <span class="math">183</span> cm (<span class="math"><i class="fm-italics">H</i> = <i class="fm-italics">F</i><sub class="fm-subscript">2</sub></span>). What is the probability of the subject’s weight being more than <span class="math">90</span> kg (<span class="math"><i class="fm-italics">W</i> = <i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span>)? In statistical parlance, this probability is denoted <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">W</i> = <i class="fm-italics">E</i><sub class="fm-subscript">3</sub>|<i class="fm-italics">H</i> = <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>)</span>. It is read “probability of <span class="math"><i class="fm-italics">W</i> = <i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span> <i class="fm-italics">given</i> <span class="math"><i class="fm-italics">H</i> = <i class="fm-italics">F</i><sub class="fm-subscript">2</sub></span>,” aka “probability of <span class="math"><i class="fm-italics">W</i> = <i class="fm-italics">E</i><sub class="fm-subscript">3</sub></span> <i class="fm-italics">subject to the condition</i> <span class="math"><i class="fm-italics">H</i> = <i class="fm-italics">F</i><sub class="fm-subscript">2</sub></span>.”</p>
<p class="body">This is an example of <i class="fm-italics">conditional probability</i>. Note that if we are given that the height is between <span class="math">160</span> and <span class="math">183</span> cm (<span class="math"><i class="fm-italics">H</i> = <i class="fm-italics">F</i><sub class="fm-subscript">2</sub></span>), our universe is restricted to the second row of table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-marginal-prob">6.1</a>. In particular, our population size is not 100,000 (that is, the entire population of Statsville). Rather, it is 48,000: the size of the population satisfying the given condition <span class="math"><i class="fm-italics">H</i> = <i class="fm-italics">F</i><sub class="fm-subscript">2</sub></span>. Using the frequentist definition,</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned} p\left( W=E_{3}  \middle\vert  H = F_{2} \right) &amp;=
\frac{\text{population satisfying } W=E_{3} \text{ and } H = F_{2}
}{\text{ population satisfying } H = F_{2} } = \frac{4K}{48K} = 0.083\\
\text{or} &amp;\\ p\left( W=E_{3}  \middle\vert  H = F_{2} \right) &amp;= \frac{p\left( W=E_{3},  H = F_{2} \right) }{p\left( H = F_{2} \right)
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="127" src="../../OEBPS/Images/eq_06-00-e.png" width="609"/></p>
</div>
<p class="body">Table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob">6.2</a> shows table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-marginal-prob">6.1</a> with conditional probabilities added.</p>
<p class="fm-table-caption">Table 6.2 Example population sizes and joint, marginal, and conditional probabilities for variables <span class="math"><i class="fm-italics">W</i> = {<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">E</i><sub class="fm-subscript">3</sub>}</span> and <span class="math"><i class="fm-italics">H</i> = {<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>}</span> weights and heights of adult Statsville residents). (This is table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-marginal-prob">6.1</a> with conditional probabilities added.)</p>
<table border="1" class="contenttable-1-table" id="tab-joint-depend-with-conditional-marginal-prob" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head"/>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Less than 60 kg (<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Between 60 and 90 kg (<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">More than 90 kg (<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Marginals for <i class="fm-italics">F</i>s</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Less than 160 cm</b> <span class="math">(<b class="fm-bold"><i class="fm-italics">F</i><sub class="fm-subscript">1</sub></b>)</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 20,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.2</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>| <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.77</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>| <i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) = 0.77</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 4,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.04</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>| <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.154</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>| <i class="fm-italics">E</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>) = 0.083</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 2,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.02</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>| <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.077</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>| <i class="fm-italics">E</i><sub class="fm-subscript">3</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>) = 0.077</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 26,000;</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">1</sub>) = 0.2</span></p>
<p class="fm-table-body"><span class="math">+ 0.04 + 0.02 = 0.26</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Between 160 cm and 183 cm</b> (<span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">2</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 4,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.04</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>| <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.083</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">2</sub>)| <i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) = 0.154</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 40,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.4</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>| <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.83</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">2</sub>)| <i class="fm-italics">E</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>) = 0.83</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 4,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.04</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>| <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.083</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">2</sub>)| <i class="fm-italics">E</i><sub class="fm-subscript">3</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>) = 0.154</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 48,000;</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">2</sub>) = 0.04 + 0.4 + 0.04 = 0.48</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">More than 183 cm</b> (<span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">3</sub></span>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 2,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.02</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>|<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.077</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>|<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">33</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) = 0.077</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 4,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.04</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>|<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.154</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>|<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">33</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>) = 0.083</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 20,000</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.2</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>|<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.77</span></p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>|<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>) = <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>, <i class="fm-italics">F</i><sub class="fm-subscript">33</sub>) / <i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>) = 0.77</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">pop. = 26,000;</p>
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">F</i><sub class="fm-subscript">3</sub>) = 0.02 + 0.04 + 0.2 = 0.26</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><b class="fm-bold">Marginals for</b> <b class="fm-bold"><i class="timesitalic">E</i>s</b></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">1</sub>) = 0.2 + 0.04 + 0.02 = 0.26</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub>) = 0.04 + 0.4 + 0.04 = 0.48</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">E</i><sub class="fm-subscript">3</sub>) = 0.02 + 0.04 + 0.2 = 0.26</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Total pop.= 100,000;</p>
<p class="fm-table-body">Total prob = <span class="math">1</span></p>
</td>
</tr>
</tbody>
</table>
<h3 class="fm-head1" id="bayes-theorem">6.1.3 Bayes’ theorem</h3>
<p class="body">As demonstrated in table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob">6.2</a>, in general,</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left( W=E_{j}  \middle\vert  H = F_{i} \right) &amp;= \frac{p\left( W=E_{j},  H = F_{i} \right) }{p\left( H = F_{i} \right) }\\ p\left( H = F_{i} \middle\vert W=E_{j}  \right) &amp;= \frac{p\left( W=E_{j},  H = F_{i} \right) }{p\left( W = E_{j} \right)
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="111" src="../../OEBPS/Images/eq_06-00-f.png" width="304"/></p>
</div>
<p class="body">This is the essence of Bayes’ theorem. We can generalize and say the following: given two random variables <i class="timesitalic">X</i> and <i class="timesitalic">Y</i>, the conditional probability of <i class="timesitalic">X</i> taking the value <i class="timesitalic">x</i> given the condition that <i class="timesitalic">Y</i> has value <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> is given by the ratio of the joint probability of the two and the marginal probability of the condition</p><!--<p class="Body"><span class="times">$$p\left( X = x \middle\vert Y = y \right) =
\frac{ p\left( X = x, Y = y \right) }{p\left( Y = y \right)}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_06-01.png" width="251"/></p>
</div>
<p class="fm-equation-caption">Equation 6.1 <span class="calibre" id="eq-bayes-theorem-2var"/></p>
<p class="body"><a id="marker-197"/>Sometimes we drop the names of the random variable and just use the values. Using such notation, Bayes’ theorem can be stated as</p><!--<p class="Body"><span class="times">$$p\left( x \middle\vert y \right) = \frac{ p\left(
x, y \right) }{p\left( y \right)}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_06-01-a.png" width="132"/></p>
</div>
<p class="body">Note that the denominator is the marginal probability, which can be obtained by summing over the joint probabilities. For instance, for continuous variables, Bayes’ theorem can be written as</p><!--<p class="Body"><span class="times">$$p\left( x \middle\vert y \right) = \frac{ p\left(
x, y \right) }{ \int\displaylimits_{-\infty}^{\infty} p\left( x, y
\right) dx }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_06-01-b.png" width="186"/></p>
</div>
<p class="body">Bayes’ theorem can be generalized further to more than two variables and multiple dimensions:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;p\left( X_{1} = \vec{x}_{1} \middle\vert X_{2} = \vec{x}_{2}, X_{3}
= \vec{x}_{3}, \cdots, X_{n} = \vec{x}_{n}  \right) \nonumber\\
&amp;= \frac{ p\left( X_{1} = \vec{x}_{1}, X_{2}  = \vec{x}_{2}, X_{3} =
\vec{x}_{3} \cdots, X_{n} = \vec{x}_{n} \right) }{p\left( X_{2}  =
\vec{x}_{2}, \cdots, X_{n} = \vec{x}_{n} \right)} \\[8pt]
&amp;p\left( X_{1} = \vec{x}_{1}, X_{2} =
\vec{x}_{2}  \middle\vert  X_{3} = \vec{x}_{3}  \cdots, X_{n} =
\vec{x}_{n}  \right) \nonumber\\
&amp;= \frac{ p\left( X_{1} = \vec{x}_{1}, X_{2}  = \vec{x}_{2}, X_{3} =
\vec{x}_{3} \cdots, X_{n} = \vec{x}_{n} \right) }{p\left( X_{3}  =
\vec{x}_{3}, \cdots, X_{n} = \vec{x}_{n} \right)}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_06-02.png" width="634"/></p>
</div>
<p class="fm-equation-caption">Equation 6.2 <span class="calibre" id="eq-gen-bayes"/></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="58" src="../../OEBPS/Images/eq_06-03.png" width="627"/></p>
</div>
<p class="fm-equation-caption">Equation 6.3</p>
<p class="body">It is common practice to drop the name of the random variable uppercase), retain only the value (lowercase), and state these equations informally as</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left( \vec{x}_{1} \middle\vert \vec{x}_{2}, \vec{x}_{3}, \cdots,
\vec{x}_{n}  \right) &amp;= \frac{ p\left( \vec{x}_{1},  \vec{x}_{2},
\vec{x}_{3} \cdots, \vec{x}_{n} \right) }{p\left(  \vec{x}_{2},
\cdots,  \vec{x}_{n} \right)} \\ p\left( \vec{x}_{1},
\vec{x}_{2}  \middle\vert  \vec{x}_{3}  \cdots,  \vec{x}_{n}  \right)
&amp;= \frac{ p\left( \vec{x}_{1}, \vec{x}_{2}, \vec{x}_{3} \cdots,
\vec{x}_{n} \right) }{p\left(  \vec{x}_{3}, \cdots, \vec{x}_{n}
\right)}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="112" src="../../OEBPS/Images/eq_06-03-a.png" width="335"/></p>
</div>
<p class="body">What happens if the random variables are independent? Well, let’s check out equation <a class="url" href="../Text/06.xhtml#eq-bayes-theorem-2var">6.1</a>. If <i class="timesitalic">X</i> and <i class="timesitalic">Y</i> are independent,</p><!--<p class="Body"><span class="times"><i class="fm-italics">p</em>(<i class="fm-italics">x</em>, <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span>) = <i class="fm-italics">p</em>(<i class="fm-italics">x</em>)<i class="fm-italics">p</em>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span>)</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="25" src="../../OEBPS/Images/eq_06-03-b.png" width="154"/></p>
</div>
<p class="body">and hence</p><!--<p class="Body"><span class="times">$$p\left(x \middle\vert y
\right) = \frac{ p\left(x, y \right) }{p\left(y \right)} = p\left(x
\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="49" src="../../OEBPS/Images/eq_06-03-c.png" width="187"/></p>
</div>
<p class="body">This makes intuitive sense: if <i class="timesitalic">X</i> and <i class="timesitalic">Y</i> are independent, knowing <i class="timesitalic">Y</i> does not make any difference to <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = <i class="fm-italics">x</i>)</span>, so the probability of <i class="timesitalic">X</i> given <i class="timesitalic">Y</i> is the same as the probability of <i class="timesitalic">X</i>.</p>
<h2 class="fm-head" id="sec-entropy">6.2 Entropy</h2>
<p class="body"><a id="marker-198"/>Suppose a daily meteorological bulletin informs the folks in the United States whether it rained in the Sahara desert yesterday. Is there much overall information in that bulletin? Not really—it almost always reports the obvious. The probability of “no rain” is overwhelmingly high it is almost certain that there will be no rain), and the uncertainty associated with the outcome is very low. Even without the bulletin, if we guess the outcome “no rain,” we will be right almost every time. Similarly, a daily news bulletin telling us whether it rained yesterday in Cherapunji, India—a place where it pretty much rains all the time—has little informational content because we can guess the results with high certainty even without the bulletin. Stated another way, the uncertainty associated with the probability distributions of “rain vs. no rain in the Sahara” and or “rain vs. no rain in Cherapunji” is low. This is a direct consequence of the fact that the probability of one of the events is close to 1 and the probabilities of the other events are near 0: the probability density function (PDF) has a very tall peak at one location and very low heights elsewhere.</p>
<p class="body">On the other hand, a daily bulletin reporting whether it rained in San Francisco is of considerable interest because the probability of “rain” and “no rain” are comparable. Without the bulletin, we cannot guess the result with much certainty.</p>
<p class="body">The concept of <i class="fm-italics">entropy</i> attempts to quantify the uncertainty associated with a chancy event. If the probability for any one event is overwhelmingly high (meaning the probabilities of other events are very low since the sum is <span class="math">1</span>), the uncertainty is low—we pretty much know that the high-probability event will occur. On the other hand, if there are multiple events with comparable high probabilities, uncertainty is high—we cannot predict which event will occur. Entropy captures this notion of uncertainty in a system. Let’s look at another example.</p>
<p class="body">Suppose we have tiny images, four pixels wide by four pixels high, and each pixel is one of four possible colors: G(reen), R(ed), B(lue), or Y(ellow). Two such images are shown in figure <a class="url" href="../Text/06.xhtml#fig-entropy-img">6.1</a>. We want to encode such images. The simplest thing to do is to use a two-bit representation for each color:<a id="marker-199"/></p>
<p class="fm-equation"><span class="math">G(<i class="fm-italics">reen</i>) = 00</span><br class="calibre20"/>
<span class="math">R(<i class="fm-italics">ed</i>) = 01</span><br class="calibre20"/>
<span class="math">B(<i class="fm-italics">lue</i>) = 10</span><br class="calibre20"/>
<span class="math">Y(<i class="fm-italics">ellow</i>) = 11</span></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="343" id="fig-entropy-img" src="../../OEBPS/Images/CH06_F01_Chaudhury.png" width="832"/></p>
<p class="figurecaption">Figure 6.1 Two <span class="math">4 × 4</span> images with different pixel color distributions. In the left image, the four colors R, G, B, and Y are equally probable. In the right image, one color (green) is much likelier than the others. The left image has higher entropy (uncertainty): we cannot predict any color with much certainty. In the right image, we can predict green with relative certainty.</p>
</div>
<p class="body">The entire <span class="math">16</span>-pixel image on the left can be represented by the string 00 00 00 00 01 01 01 01 10 10 10 10 11 11 11 11. Here, we have iterated over the pixels in <i class="fm-italics">raster scan order</i>, left to right and top to bottom. The total number of bits needed to store the <span class="math">16</span>-pixel image is <span class="math">16 × 2 = 32</span> bits. The right image can be represented as 00 00 00 00 00 00 00 00 00 00 00 00 01 01 10 11. The total number of bits needed is <span class="math">16 × 2 = 32</span> bits. Both images need the same amount of storage. But is this optimal?</p>
<p class="body">Consider the right-hand image. The color G appears much more frequently than the others. We can use this fact to reduce the total number of bits required to store the image. It is not mandatory to use the same number of bits to represent each color. How about using shorter representations for the more frequently occurring (higher-probability) colors and longer representations for the infrequent (lower-probability) colors? This is the core principle behind the technique of <i class="fm-italics">variable bit-rate coding</i>. For instance, we can use the following representation:</p>
<p class="fm-equation"><span class="math">G(<i class="fm-italics">reen</i>) = 0</span><br class="calibre20"/>
<span class="math">R(<i class="fm-italics">ed</i>) = 10</span><br class="calibre20"/>
<span class="math">B(<i class="fm-italics">lue</i>) = 110</span><br class="calibre20"/>
<span class="math">Y(<i class="fm-italics">ellow</i>) = 111</span></p>
<p class="body"><a id="marker-200"/>The right-hand image can thus be represented as <span class="math">0 0 0 0 0 0 0 0 0 0 0 0 10 10 110 111</span>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> This is an example of what is known as <i class="fm-italics">prefix coding</i>: no two colors share the same prefix. It enables us to identify the color as soon as we see its code. For instance, if we see a <span class="math">0</span> bit at the beginning, we immediately know the color is green since no other color code starts with <span class="math">0</span>. If we see <span class="math">10</span>, we immediately know the color is red since no other color code starts with <span class="math">10</span>, and so on.</p>
<p class="body">With this new color code, we need <span class="math">12 × 1 = 12</span> bits to store the <span class="math">12</span> green pixels, <span class="math">2 × 2 = 4</span> bits to store the <span class="math">2</span> red pixels, <span class="math">1 × 3 = 3</span> bits to store the single blue pixel, and <span class="math">1 × 3 = 3</span> bits to store the single yellow pixel—a total of <span class="math">22</span> pixels. Equivalently, we need <span class="math">22/16 = 1.375</span> bits per pixel. This is less than the <span class="math">32</span> pixels at <span class="math">2</span> bits per pixel we needed with the simple fixed bit-rate coding.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> You have just learned about Huffman encoding, an important technique in image compression.</p>
<p class="body">Does the new representation result in smaller storage for the left-hand image? There, we need <span class="math">4 × 1 = 4</span> bits to store the four green pixels, <span class="math">4 × 2 = 8</span> pixels to store the four red pixels, <span class="math">4 × 3 = 12</span> bits to store the four blue pixels, and <span class="math">4 × 3 = 12</span> bits to store the single yellow pixel: a total of <span class="math">36</span> pixels at <span class="math">36/16 = 2.25</span> bits per pixel. Here, variable bit-rate coding does worse than fixed bit-rate coding.</p>
<p class="body">So, the probability distribution of the various pixel colors in the image affects how much compression can be achieved. If the distribution of pixel colors is such that a few colors are much more probable than others, we can assign shorter codes to them to reduce storage for the whole image. Viewed another way, if low uncertainty is associated with the system—certain colors are more or less certain to occur—we can achieve high compression. We assign shorter codes to nearly certain colors, resulting in compression. On the other hand, if high uncertainty is associated with the system—all colors are more or less equally probable, and no color occurs with high certainty—variable bit-rate coding will not be very effective. How do we quantify this notion? In other words, can we examine the pixel color distribution in an image and estimate whether variable bit-rate coding will be effective? The answer again is entropy. Formally,</p>
<p class="fm-quote">Entropy measures the overall uncertainty associated with a probability distribution.</p>
<p class="body">Entropy is a measure that is <i class="fm-italics">high</i> if everything is more or less equally probable and <i class="fm-italics">low</i> if a few items have a much higher probability than the others. It measures the uncertainty in the system. If everything is equally probable, we cannot predict any one item with any extra certainty. Such a system has high entropy. On the other hand, if some items are much more probable than others, we can predict them with relative certainty. Such a system has low entropy.</p>
<p class="body">In the discrete univariate case, for a random variable <i class="timesitalic">X</i> that can take any one of the discrete values <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">3</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">x<sub class="fm-subscript">n</sub></i> with probabilities <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">1</sub>)</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">2</sub>)</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">3</sub>)</span>, <span class="math">⋯</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x<sub class="fm-subscript">n</sub></i>)</span>, entropy is defined as</p><!--<p class="Body"><span class="times">$$\mathbb{H}\left(X\right) = -\sum_{i=1}^{n} p\left(x_{i}\right) log \; p\left(x_{i}\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_06-04.png" width="221"/></p>
</div>
<p class="fm-equation-caption">Equation 6.4 <span class="calibre" id="eq-entropy-discr-univar4"/></p>
<p class="body">The logarithm is taken with respect to the natural base <i class="timesitalic">e</i>.</p>
<p class="body">Let’s apply equation <a class="url" href="../Text/06.xhtml#eq-entropy-discr-univar4">6.4</a> to the images in figure <a class="url" href="../Text/06.xhtml#fig-entropy-img">6.1</a> to see if the results agree with our intuition. The computations are shown in table <a class="url" href="../Text/06.xhtml#tab-entropies-img">6.3</a>. The notion of entropy applies to continuous and multidimensional random variables equally well.<a id="marker-201"/></p>
<p class="fm-table-caption">Table 6.3 Entropy computation for the pair of images in figure <a class="url" href="../Text/06.xhtml#fig-entropy-img">6.1</a>. The right-hand image has lower entropy and can be compressed more.</p>
<table border="1" class="contenttable-1-table" id="tab-entropies-img" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="50%"/>
<col class="contenttable-0-col" span="1" width="50%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">Left image</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Right image</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> = <i class="fm-italics">G</i></span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">1</sub>) = 4/16 = 0.25</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> = <i class="fm-italics">G</i></span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">1</sub>) = 12/16 = 0.75</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">2</sub> = <i class="fm-italics">R</i></span> , <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">2</sub>) = 4/16 = 0.25</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">2</sub> = <i class="fm-italics">R</i></span> , <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">2</sub>) = 2/16 = 0.125</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">3</sub> = <i class="fm-italics">B</i></span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">3</sub>) = 4/16 = 0.25</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">3</sub> = <i class="fm-italics">B</i></span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">3</sub>) = 1/16 = 0.0625</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">4</sub> = <i class="fm-italics">Y</i></span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">4</sub>) = 4/16 = 0.25</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">4</sub> = <i class="fm-italics">Y</i></span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i><sub class="fm-subscript">4</sub>) = 1/16 = 0.0625</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">ℍ = −(0.25 <i class="fm-italics">log</i>(0.25)+0.25 <i class="fm-italics">log</i>(0.25) +<br class="calibre20"/>
          + 0.25 <i class="fm-italics">log</i>(0.25) + 0.25 <i class="fm-italics">log</i>(0.25)) = 1.386294</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">ℍ = −(0.75 <i class="fm-italics">log</i>(0.75)+0.125 <i class="fm-italics">log</i>(0.125) +<br class="calibre20"/>
          + 0.0625 <i class="fm-italics">log</i>(0.0625) + 0.0625 <i class="fm-italics">log</i>(0.0625)) = 0.822265</span></p>
</td>
</tr>
</tbody>
</table>
<p class="body">For a univariate continuous random variable <i class="timesitalic">X</i> that takes values <span class="math"><i class="fm-italics">x</i> <span class="cambria">∈</span> {−∞,∞}</span> with probabilities <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span>,</p><!--<p class="Body"><span class="times">$$\mathbb{H}\left(X\right) =
-\int\displaylimits_{x=-\infty}^{\infty} p\left(x\right) log\;p\left(x\right)\;dx$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="67" src="../../OEBPS/Images/eq_06-05.png" width="249"/></p>
</div>
<p class="fm-equation-caption">Equation 6.5 <span class="calibre" id="eq-entropy-discr-univar"/></p>
<p class="body">For a continuous multidimensional random variable <i class="timesitalic">X</i> that takes values <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> in the domain <i class="timesitalic">D</i>, (<span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <span class="cambria">∈</span> <i class="fm-italics">D</i></span>) with probabilities <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>,</p><!--<p class="Body"><span class="times">$$\mathbb{H}\left(X\right) =
-\int\displaylimits_{\vec{x} \in D} p\left(\vec{x}\right) log\;p\left(\vec{x}\right)\;d\vec{x}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_06-06.png" width="248"/></p>
</div>
<p class="fm-equation-caption">Equation 6.6 <span class="calibre" id="eq-entropy-cont-univar"/></p>
<h3 class="fm-head1" id="geometrical-intuition-for-entropy">6.2.1 Geometrical intuition for entropy</h3>
<p class="body">Geometrically speaking, entropy is a function of how lopsided the PDF is (see figure <a class="url" href="../Text/06.xhtml#fig-entropy-density-cloud">6.2</a>). If all inputs are more or less equally probable, the density function is more or less flat and uniform in height everywhere (see figure <a class="url" href="../Text/06.xhtml#fig-entropy-high-density">6.2a</a>). The corresponding sample point cloud has a diffused mass: there are no regions with a high concentration of points. Such a system has high uncertainty or high entropy (see figure <a class="url" href="../Text/06.xhtml#fig-entropy-high-cloud">6.2b</a>).</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="411" id="fig-entropy-high-density" src="../../OEBPS/Images/CH06_F02a_Chaudhury.png" width="534"/></p>
<p class="figurecaption">(a) Flatter, wider PDFs correspond to higher entropy. Entropy <span class="math">= 12.04</span>.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="537" id="fig-entropy-high-cloud" src="../../OEBPS/Images/CH06_F02b_Chaudhury.png" width="581"/></p>
<p class="figurecaption">(b) Diffused sample point clouds correspond to higher entropy.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="383" id="fig-entropy-low-density" src="../../OEBPS/Images/CH06_F02c_Chaudhury.png" width="535"/></p>
<p class="figurecaption">(c) Taller, narrower peaks in probability density functions correspond to lower entropy. Entropy <span class="math">= 7.44</span>.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="527" id="fig-entropy-low-cloud" src="../../OEBPS/Images/CH06_F02d_Chaudhury.png" width="537"/></p>
<p class="figurecaption">(d) Concentrated sample point clouds correspond to lower entropy.</p>
</div>
<p class="fm-table-caption" id="fig-entropy-density-cloud">Figure 6.2 Entropies of peaked and flat distributions<a id="marker-202"/></p>
<p class="body">On the other hand, if a few of all the possible inputs have disproportionately high probabilities, the PDF has tall peaks in some regions and low heights elsewhere (see figure <a class="url" href="../Text/06.xhtml#fig-entropy-low-density">6.2c</a>). The corresponding sample point cloud has regions of high concentration matching the peaks in the density function and low concentration elsewhere (see figure <a class="url" href="../Text/06.xhtml#fig-entropy-low-cloud">6.2d</a>). Such a system has low uncertainty and low entropy.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Since the sum of all the probabilities is <span class="math">1</span>, if a few are high, the others have to be low. We cannot have all high or all low probabilities.</p>
<h3 class="fm-head1" id="entropy-of-gaussians">6.2.2 Entropy of Gaussians</h3>
<p class="body"><a id="marker-203"/>The wider a Gaussian is, the less peaked it is, and the closer it is to being a uniform distribution. A univariate Gaussian’s variance, <i class="timesitalic">σ</i>, determines its fatness (see figure <a class="url" href="../Text/05.xhtml#fig-multi-univar-gauss">5.10b</a>). Consequently, we expect a Gaussian’s entropy to be an increasing function of <i class="timesitalic">σ</i>. Indeed, that is the case. In this section, we derive the entropy of a Gaussian in the univariate case and simply state the result for the multivariate case.</p>
<p class="body">For a random variable <i class="timesitalic">x</i> whose PDF is given by equation <a class="url" href="../Text/05.xhtml#eq-univar-normal">5.22</a> (repeated here for convenience),</p><!--<p class="Body"><span class="times">$$p\left(x\right)
=   \frac{1}{{\sqrt {2\pi } \sigma}}e^{{\frac{ - \left( {x - \mu }
\right)^2 } {2\sigma ^2 }}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="52" src="../../OEBPS/Images/eq_06-06-a.png" width="167"/></p>
</div>
<p class="body">From that, we get</p><!--<p class="Body"><span class="times">$$log\;p\left( x \right) = -\frac{1}{2} log\;\left(2\pi\right) - log\;\sigma - \frac{\left(x -
\mu\right)^{2}}{2\sigma^{2}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_06-06-b.png" width="321"/></p>
</div>
<p class="body">Using equation <a class="url" href="../Text/06.xhtml#eq-entropy-cont-univar">6.6</a>, the entropy is</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\mathbb{H}\left(X\right) &amp;= -\int\displaylimits_{x=-\infty}^{\infty} p\left(x\right)  \left(\; -\frac{1}{2} log\;\left(2\pi\right) - log\;\sigma - \frac{\left(x - \mu\right)^{2}}{2\sigma^{2}}\;\right) dx\\
&amp;= \frac{1}{2} log\;\left(2\pi\right)
\int\displaylimits_{x=-\infty}^{\infty} p\left(x\right) dx + log\;\sigma
\int\displaylimits_{x=-\infty}^{\infty} p\left(x\right) dx +
\frac{1}{2\sigma^{2}} \int\displaylimits_{x=-\infty}^{\infty} p\left(x\right)  \left(x - \mu\right)^{2} dx\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="141" src="../../OEBPS/Images/eq_06-06-c.png" width="626"/></p>
</div>
<p class="body">Remembering the probability sum rule from equation <a class="url" href="../Text/05.xhtml#eq-continuous-prob-sum">5.6</a>, <span class="math">∫<sub class="fm-subscript"><i class="fm-italics1">x</i> = −∞</sub><sup class="fm-superscript">∞</sup> <i class="fm-italics">p</i>(<i class="fm-italics">x</i>) <i class="fm-italics">dx</i> = 1</span>, we get</p><!--<p class="Body"><span class="times">$$\mathbb{H}\left(X\right) = \frac{1}{2} log\;\left(2\pi\right) + log\;\sigma +  \frac{1}{2\sigma^{2}}
\int\displaylimits_{x=-\infty}^{\infty} p\left(x\right)  \left(x -
\mu\right)^{2} dx$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_06-06-d.png" width="408"/></p>
</div>
<p class="body">Now, by definition (see section <a class="url" href="../Text/05.xhtml#sec-var-covar-std">5.7.2</a>),</p><!--<p class="Body"><span class="times">$$\int\displaylimits_{x=-\infty}^{\infty} p\left(x\right)  \left(x - \mu\right)^{2} dx =\mathbb{E}\left(\left(x -
\mu\right)^{2}\right) = \sigma^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="71" src="../../OEBPS/Images/eq_06-06-e.png" width="323"/></p>
</div>
<p class="body">Hence,</p><!--<p class="Body"><span class="times">$$\mathbb{H}\left(X\right) = \frac{1}{2} log\;\left(2\pi\right) + log\;\sigma +   \frac{\sigma^{2}}{2\sigma^{2}}
= \frac{1}{2} log\;\left(2\pi\right) + log\;\sigma +   \frac{1}{2} =
\frac{1}{2} log\left(  2\pi e \sigma^{2} \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="56" src="../../OEBPS/Images/eq_06-07.png" width="569"/></p>
</div>
<p class="fm-equation-caption">Equation 6.7 <span class="calibre" id="eq-entropy-gauss-univar"/></p>
<p class="body">Entropy for multivariate Gaussians is as follows:</p><!--<p class="Body"><span class="times">$$\mathbb{H}\left(X\right) = \frac{1}{2} log\;\left(2\pi\right) + log\left(det\left(  \boldsymbol{\Sigma}
\right)\right) +  \frac{1}{2} =  \frac{1}{2} log\left(  2\pi\;e\;det
\left(  \boldsymbol{\Sigma} \right) \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="50" src="../../OEBPS/Images/eq_06-08.png" width="451"/></p>
</div>
<p class="fm-equation-caption">Equation 6.8 <span class="calibre" id="eq-entropy-gauss-multivar"/></p>
<p class="body">Listing 6.1 shows the Python PyTorch code to compute the entropy of a Gaussian.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code to compute the entropy of a Gaussian distribution, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/zx7B">http://mng.bz/zx7B</a>.</p>
<p class="fm-code-listing-caption" id="listing-6.1-computing-the-entropy-of-a-gaussian-distribution">Listing 6.1 Computing the entropy of a Gaussian distribution</p>
<pre class="programlisting">def entropy_gaussian_formula(sigma):
    return 0.5 * torch.log(2 * math.pi * math.e * sigma * sigma) <span class="fm-combinumeral">①</span>

p = Normal(0, 10)                                                <span class="fm-combinumeral">②</span>

H_formula = entropy_gaussian_formula(p.stddev)                   <span class="fm-combinumeral">③</span>

H = p.entropy()                                                  <span class="fm-combinumeral">④</span>

assert torch.isclose(H_formula, H)                               <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Equation <a class="url" href="../Text/06.xhtml#eq-entropy-gauss-univar">6.7</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates a Gaussian distribution</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Computes the entropy using the direct formulaComputes the entropy using the direct formula</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Computes the entropy using the PyTorch interface</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Asserts that the entropies computed two different ways match</p>
<h2 class="fm-head" id="sec-cross-entropy">6.3 Cross-entropy</h2>
<p class="body"><a id="marker-204"/>Consider a <i class="fm-italics">supervised</i> classification problem where we have to analyze an image and identify which of the following objects is present: <i class="fm-italics">cat</i>, <i class="fm-italics">dog</i>, <i class="fm-italics">airplane</i>, or <i class="fm-italics">automobile</i>. We assume that one of these will always be present in our universe of images. Given an input image, our machine emits four probabilities: <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">cat</i>)</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">dog</i>)</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">airplane</i>)</span>, and <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">automobile</i>)</span>. During training, for each training data instance, we have a ground truth (GT): a known class to which that training data instance belongs. We have to estimate how different the network output is from the GT—this is the loss for that data instance. We adjust the machine parameters to minimize the loss and continue doing so until the loss stops decreasing.</p>
<p class="body">How do we quantitatively estimate the loss—the difference between the known GT and the probabilities of various classes emitted by the network? One principled approach is to use the cross-entropy loss. Here is how it works.</p>
<p class="body">Consider a random variable <i class="timesitalic">X</i> that can take four possible values: <span class="math"><i class="fm-italics">X</i> = 1</span> signifying <i class="fm-italics">cat</i>, <span class="math"><i class="fm-italics">X</i> = 2</span> signifying <i class="fm-italics">dog</i>, <span class="math"><i class="fm-italics">X</i> = 3</span> signifying <i class="fm-italics">airplane</i>, and <span class="math"><i class="fm-italics">X</i> = 4</span> signifying <i class="fm-italics">automobile</i>. The random variable has the PDF <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 1) ≡ <i class="fm-italics">p</i>(<i class="fm-italics">cat</i>)</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 2) ≡ <i class="fm-italics">p</i>(<i class="fm-italics">dog</i>)</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 3) ≡ <i class="fm-italics">p</i>(<i class="fm-italics">airplane</i>)</span>, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = 4) ≡ <i class="fm-italics">p</i>(<i class="fm-italics">automobile</i>)</span>. The PDF for a GT, which selects one from the set of four possible classes, is a one-hot vector (one of the elements is <span class="math">1</span>, and the others are <span class="math">0</span>). Such random variables and corresponding PDFs can be associated with every GT and machine output. Here are some examples, which are also shown graphically in figure <a class="url" href="../Text/06.xhtml#fig-cross-entropy">6.3</a>. A PDF for GT <i class="fm-italics">cat</i> (one-hot vector) is shown figure <a class="url" href="../Text/06.xhtml#fig-cross-entropy-gt">6.3a</a>:</p>
<p class="body">How do we quantitatively estimate the loss—the difference between the known GT and the probabilities of various classes emitted by the network? One principled approach is to use the cross-entropy loss. Here is how it works.</p><!--<p class="Body"><span class="times">$$p_{gt\_cat} = \left[
\overbrace{1}^{p\left(cat\right)}, \overbrace{0}^{p\left(dog\right)},
\overbrace{0}^{p\left(airplane\right)},
\overbrace{0}^{\left(automobile\right)} \right]$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="84" src="../../OEBPS/Images/eq_06-08-a.png" width="332"/></p>
</div>
<p class="body">A PDF for a good prediction is shown figure <a class="url" href="../Text/06.xhtml#fig-cross-entropy-low-pred">6.3b</a>:</p><!--<p class="Body"><span class="times">$$p_{good\_pred} = \left[
\overbrace{0.8}^{p\left(cat\right)},
\overbrace{0.15}^{p\left(dog\right)},
\overbrace{0.04}^{p\left(airplane\right)},
\overbrace{0.01}^{\left(automobile\right)} \right]$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="85" src="../../OEBPS/Images/eq_06-08-b.png" width="352"/></p>
</div>
<p class="body">A PDF for a bad prediction is shown figure <a class="url" href="../Text/06.xhtml#fig-cross-entropy-high-pred">6.3c</a>:</p><!--<p class="Body"><span class="times">$$p_{bad\_pred} = \left[
\overbrace{0.25}^{p\left(cat\right)},
\overbrace{0.25}^{p\left(dog\right)},
\overbrace{0.25}^{p\left(airplane\right)},
\overbrace{0.25}^{\left(automobile\right)} \right]$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="89" src="../../OEBPS/Images/eq_06-08-c.png" width="347"/></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="379" id="fig-cross-entropy-gt" src="../../OEBPS/Images/CH06_F03a_Chaudhury.png" width="535"/></p>
<p class="figurecaption">(a) Ground truth probability</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="378" id="fig-cross-entropy-low-pred" src="../../OEBPS/Images/CH06_F03b_Chaudhury.png" width="535"/></p>
<p class="figurecaption">(b) Good prediction: probabilities similar to ground truth. Cross-entropy loss = <span class="math">0.22</span>.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="380" id="fig-cross-entropy-high-pred" src="../../OEBPS/Images/CH06_F03c_Chaudhury.png" width="541"/></p>
<p class="figurecaption">(c) Bad prediction: probabilities dissimilar to ground truth. Cross-entropy loss = <span class="math">1.38</span>.</p>
</div>
<p class="fm-table-caption" id="fig-cross-entropy">Figure 6.3 Cross-entropy loss</p>
<p class="body"><a id="marker-205"/>Let <i class="timesitalic">X<sub class="fm-subscript">gt</sub></i> denote such a random variable for a specific GT and <i class="timesitalic">p<sub class="fm-subscript">gt</sub></i> denote the corresponding PDF. Similarly, let <i class="timesitalic">X<sub class="fm-subscript">pred</sub></i> and <i class="timesitalic">p<sub class="fm-subscript">pred</sub></i> denote the random variable and PDF for the machine prediction. Consider the following expression:</p><!--<p class="Body"><span class="times">$$\mathbb{H}_{c}  \left( X _{gt} , X
_{pred}  \right) = -\sum_{i=1}^{4} p_{gt} \left( i \right) \;\; \log
\left( p _{pred} \left( i \right) \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_06-09.png" width="326"/></p>
</div>
<p class="fm-equation-caption">Equation 6.9 <span class="calibre" id="eq-cross-entropy-discrete"/></p>
<p class="body">This is the expression for <i class="fm-italics">cross-entropy</i>. It is a quantitative measure for how dissimilar the two PDFs <i class="timesitalic">p<sub class="fm-subscript">gt</sub></i> and <i class="timesitalic">p<sub class="fm-subscript">pred</sub></i> are: that is, how much error will be caused by approximating the PDF <i class="timesitalic">p<sub class="fm-subscript">gt</sub></i> with <i class="timesitalic">p<sub class="fm-subscript">pred</sub></i>. Equivalently, cross-entropy measures how well the machine is doing that output the prediction <i class="timesitalic">p<sub class="fm-subscript">pred</sub></i> when the correct PDF is <i class="timesitalic">p<sub class="fm-subscript">gt</sub></i>.</p>
<p class="body">To gain insight into how <span class="math">ℍ<i class="fm-italics"><sub class="fm-subscript">c</sub></i>(<i class="fm-italics">X<sub class="fm-subscript">gt</sub></i>, <i class="fm-italics">X<sub class="fm-subscript">pred</sub></i>)</span> measures dissimilarity between PDFs, examine the expression carefully. Remember that <span class="math">Σ<sub class="subscript-italic">i</sub><sup class="fm-superscript">4</sup><sub class="fm-subscript">= 1</sub> <i class="fm-italics">p<sub class="fm-subscript">gt</sub></i> (<i class="fm-italics">i</i>) = Σ<sub class="subscript-italic">i</sub><sup class="fm-superscript">4</sup><sub class="fm-subscript">= 1</sub> <i class="fm-italics">p<sub class="fm-subscript">pred</sub></i> (<i class="fm-italics">i</i>) = 1</span> (using the probability sum rule from equation <a class="url" href="../Text/05.xhtml#eq-discrete-prob-sum">5.3</a>):</p>
<p class="body-case-a"><b class="fm-bold">Case 1</b>: The <i class="timesitalic">i</i> values where <span class="math"><i class="fm-italics">p<sub class="fm-subscript">gt</sub></i>(<i class="fm-italics">i</i>)</span> is high (close to <span class="math">1</span>).</p>
<p class="body-case-b"><b class="fm-bold">Case 1a</b>: If <span class="math"><i class="fm-italics">p<sub class="fm-subscript">pred</sub></i>(<i class="fm-italics">i</i>)</span> is also close to <span class="math">1</span>, then <span class="math">log (<i class="fm-italics">p<sub class="fm-subscript">pred</sub></i>(<i class="fm-italics">i</i>))</span> will be close to zero (since <span class="math">log 1 = 0</span>). Hence the term <span class="math"><i class="fm-italics">p<sub class="fm-subscript">gt</sub></i>(<i class="fm-italics">i</i>)log (<i class="fm-italics">p<sub class="fm-subscript">pred</sub></i>(<i class="fm-italics">i</i>))</span> will be close to zero since the product of anything with a near-zero number is near zero. These terms will contribute little to <span class="math">ℍ<i class="fm-italics"><sub class="fm-subscript">c</sub></i>(<i class="fm-italics">X<sub class="fm-subscript">gt</sub></i>, <i class="fm-italics">X<sub class="fm-subscript">pred</sub></i>)</span>.</p>
<p class="body-case-b"><b class="fm-bold">Case 1b</b>: On the other hand, at the <i class="timesitalic">i</i> values where <span class="math"><i class="fm-italics">p<sub class="fm-subscript">gt</sub></i>(<i class="fm-italics">i</i>)</span> is high, if <span class="math"><i class="fm-italics">p<sub class="fm-subscript">pred</sub></i>(<i class="fm-italics">i</i>)</span> is low (close to zero), then <span class="math">−log (<i class="fm-italics">p<sub class="fm-subscript">pred</sub></i>(<i class="fm-italics">i</i>))</span> will be very high (since <span class="math">log 0 → − ∞</span>).</p>
<p class="body-case-a"><b class="fm-bold">Case 2</b>: The <i class="timesitalic">i</i> values where <span class="math"><i class="fm-italics">p<sub class="fm-subscript">gt</sub></i>(<i class="fm-italics">i</i>)</span> is low (close to <span class="math">0</span>). These will have low values and will contribute little to <span class="math">ℍ<i class="fm-italics"><sub class="fm-subscript">c</sub></i>(<i class="fm-italics">X<sub class="fm-subscript">gt</sub></i>, <i class="fm-italics">X<sub class="fm-subscript">pred</sub></i>)</span> since the product of anything with a near zero number is near zero.</p>
<p class="body"><a id="marker-206"/>Thus, overall, large contributions can happen only in case 1b, where <span class="math"><i class="fm-italics">p<sub class="fm-subscript">gt</sub></i>(<i class="fm-italics">i</i>)</span> is high and <span class="math"><i class="fm-italics">p<sub class="fm-subscript">pred</sub></i>(<i class="fm-italics">i</i>)</span> is low—that is, <i class="timesitalic">p<sub class="fm-subscript">gt</sub></i> and <i class="timesitalic">p<sub class="fm-subscript">pred</sub></i> are very dissimilar. What if <span class="math"><i class="fm-italics">p<sub class="fm-subscript">gt</sub></i>(<i class="fm-italics">i</i>)</span> is low and <span class="math"><i class="fm-italics">p<sub class="fm-subscript">pred</sub></i>(<i class="fm-italics">i</i>)</span> is high? They are also dissimilar, so those terms will not contribute much! True, but if such terms exist, there must be other terms where <span class="math"><i class="fm-italics">p<sub class="fm-subscript">gt</sub></i>(<i class="fm-italics">i</i>)</span> is high and <span class="math"><i class="fm-italics">p<sub class="fm-subscript">pred</sub></i>(<i class="fm-italics">i</i>)</span> is low. This is because the sums of all <span class="math"><i class="fm-italics">p<sub class="fm-subscript">gt</sub></i>(<i class="fm-italics">i</i>)</span> and <span class="math"><i class="fm-italics">p<sub class="fm-subscript">pred</sub></i>(<i class="fm-italics">i</i>)</span> must be both <span class="math">1</span>. Either way, if there is dissimilarity, the cross-entropy is high.</p>
<p class="body">For instance, consider the case where <span class="math"><i class="fm-italics">X<sub class="fm-subscript">gt</sub></i> = <i class="fm-italics">X<sub class="fm-subscript">gt_cat</sub></i></span> and <span class="math"><i class="fm-italics">X<sub class="fm-subscript">pred</sub></i> = <i class="fm-italics">X<sub class="fm-subscript">good_pred</sub></i></span> or <span class="math"><i class="fm-italics">X<sub class="fm-subscript">pred</sub></i> = <i class="fm-italics">X<sub class="fm-subscript">bad_pred</sub></i></span>. We know <i class="timesitalic">p<sub class="fm-subscript">gt_cat</sub></i> is a one-hot selector vector, meaning it has <span class="math">1</span> as one element and <span class="math">0</span>s elsewhere. Only a single term survives, corresponding to <span class="math"><i class="fm-italics">i</i> = 0</span>, and</p><!--<p class="Body"><span class="times">$$\mathbb{H}_{c} \left( X _{gt\_cat} , X
_{pred} \right) =
\begin{cases}
&amp; -\sum\displaylimits_{i=1}^{4} p_{gt\_cat} \left( i
\right) \;\; \log \left( p _{good\_pred} \left( i \right) \right)
  = -\log\left( 0.8 \right) = 0.22 \\[6pt]
&amp;  -\sum\displaylimits_{i=1}^{4} p_{gt\_cat} \left( i \right) \;\; \log \left( p _{bad\_pred} \left( i \right) \right)
  = - \log\left( 0.25 \right) = 1.38
\end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="72" src="../../OEBPS/Images/eq_06-09-a.png" width="606"/></p>
</div>
<p class="body">We see that cross-entropy is higher where similarity is lower (the prediction is bad).</p>
<p class="body">Finally, we are ready to formally define the cross-entropy of two arbitrary random variables. Let <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> be a pair of random variables that take values <i class="timesitalic">x</i> from the same input domain <i class="timesitalic">D</i> that is, <span class="math"><i class="fm-italics">x</i> <span class="cambria">∈</span> <i class="fm-italics">D</i></span>), with probabilities <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">1</sub>(<i class="fm-italics">x</i>)</span>, <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">2</sub>(<i class="fm-italics">x</i>)</span>, respectively:</p><!--<p class="Body"><span class="times">$$\mathbb{H}_{c} \left( X_{1},
X_{2} \right) =
\begin{cases}
&amp;-\sum\displaylimits_{x \in D} p_{1}\left(  x
\right) \log \left( p_{2}\left( x \right)\right) \;\;\;
\text{discrete}\\[6pt]
&amp;-\int\displaylimits_{x \in D} p_{1}\left( x \right)
\log \left( p_{2}\left( x \right)\right) dx \;\;\; \text{  continuous univariate}\\[6pt]
&amp;-\int\displaylimits_{\vec{x} \in D} p_{1}\left( \vec{x} \right)
\log \left( p_{2}\left( \vec{x} \right)\right)  d\vec{x}  \;\;\; \text{ continuous multivariate}
\end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="107" src="../../OEBPS/Images/eq_06-10.png" width="548"/></p>
</div>
<p class="fm-equation-caption">Equation 6.10 <span class="calibre" id="eq-cross-entropy-loss"/></p>
<p class="body">Note that cross-entropy in equation <a class="url" href="../Text/06.xhtml#eq-cross-entropy-loss">6.10</a> reduces to entropy (equations <a class="url" href="../Text/06.xhtml#eq-entropy-discr-univar">6.5</a>, <a class="url" href="../Text/06.xhtml#eq-entropy-cont-univar">6.6</a>) if <span class="math"><i class="fm-italics">Y</i> = <i class="fm-italics">X</i></span>. Listing 6.2 shows the Python PyTorch code to compute the entropy of a Gaussian.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code to compute cross-entropy, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/0mjN">http://mng.bz/0mjN</a>.</p>
<p class="fm-code-listing-caption" id="listing-6.2-computing-cross-entropy">Listing 6.2 Computing cross-entropy</p>
<pre class="programlisting">def cross_entropy(X_gt, X_pred):
    H_c = 0
    for x_gt, x_pred in zip(X_gt, X_pred):
        H_c += -1 * (x_gt * torch.log (x_pred))     <span class="fm-combinumeral">①</span>
    return H_c

X_gt = torch.Tensor([1., 0., 0., 0.])               <span class="fm-combinumeral">②</span>

X_good_pred = torch.Tensor([0.8, 0.15, 0.04, 0.01]) <span class="fm-combinumeral">③</span>

X_bad_pred = torch.Tensor([0.25, 0.25, 0.25, 0.25]) <span class="fm-combinumeral">④</span>

H_c_good = cross_entropy(X_gt, X_good_pred)         <span class="fm-combinumeral">⑤</span>

H_c_bad = cross_entropy(X_gt, X_bad_pred)           <span class="fm-combinumeral">⑥</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Direct computation<br class="calibre20"/>
  of cross-entropy from equation <a class="url" href="../Text/06.xhtml#eq-cross-entropy-discrete">6.9</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Probability density function for the ground truth (one-hot vector)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Probability density function for a good prediction</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Probability density function for a bad prediction</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Cross-entropy between <i class="timesitalic">X<sub class="fm-subscript">gt</sub></i> and <i class="timesitalic">X<sub class="fm-subscript">good_pred</sub></i> a low value)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Cross-entropy between <i class="timesitalic">X<sub class="fm-subscript">gt</sub></i> and <i class="timesitalic">X<sub class="fm-subscript">bad_pred</sub></i> a high value)</p>
<h2 class="fm-head" id="sec-kld">6.4 KL divergence</h2>
<p class="body"><a id="marker-207"/>In section <a class="url" href="../Text/06.xhtml#sec-cross-entropy">6.3</a>, we saw that cross-entropy, <span class="math">ℍ<i class="fm-italics"><sub class="fm-subscript">c</sub></i>(<i class="fm-italics">X</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">X</i><sub class="fm-subscript">2</sub>)</span>, measures the dissimilarity between the distributions of two random variables <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span> and <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> with probabilities <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">1</sub>(<i class="fm-italics">x</i>)</span> and <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">2</sub>(<i class="fm-italics">x</i>)</span>. But cross-entropy has a curious property for a dissimilarity measure. If <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub> = <i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span>, the cross-entropy <span class="math">ℍ<i class="fm-italics"><sub class="fm-subscript">c</sub></i>(<i class="fm-italics">X</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">X</i><sub class="fm-subscript">2</sub>)</span> reduces to the entropy <span class="math">ℍ(<i class="fm-italics">X</i><sub class="fm-subscript">1</sub>)</span>. This is somewhat counterintuitive: we expect the dissimilarity between two copies of the same thing to be zero.</p>
<p class="body">We should look at cross-entropy as a dissimilarity with an offset. Let’s denote the pure dissimilarity measure as <span class="math"><i class="fm-italics">D</i>(<i class="fm-italics">X</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">X</i><sub class="fm-subscript">2</sub>)</span>. Then</p><!--<p class="Body"><span class="times">$$\mathbb{H}_{c} \left( X_{1}, X_{2}
\right) = \overbrace{ \mathbb{H}\left( X_{1} \right) }^{offset} +
\overbrace{ D\left( X_{1}, X_{2} \right) }^{\text{pure dissimilarity}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_06-10-a.png" width="281"/></p>
</div>
<p class="body">This means the pure dissimilarity measure</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;D\left( X_{1}, X_{2} \right) = \mathbb{H}_{c} \left( X_{1},
X_{2}\right) -  \mathbb{H}\left( X_{1} \right)
= -\sum\displaylimits_{x \in D} p_{1}\left(  x \right)\; \log \left( p_{2}\left( x \right) \right) + \sum\displaylimits_{x \in D} p_{1}\left(  x \right)\; \log \left( p_{1}\left( x \right)\right)\\
&amp;\quad= \sum\displaylimits_{x \in D} p_{1}\left(  x \right)\; \log
\left( p_{1}\left( x \right) - p_{2}\left( x \right) \right) =
\sum\displaylimits_{x \in D} p_{1}\left(  x \right)\; \log \left(
\frac{p_{1}\left( x \right)}{p_{2}\left( x
\right)}  \right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="108" src="../../OEBPS/Images/eq_06-10-b.png" width="641"/></p>
</div>
<p class="body">This pure dissimilarity measure, <span class="math"><i class="fm-italics">D</i>(<i class="fm-italics">X</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">X</i><sub class="fm-subscript">2</sub>)</span>, is called <i class="fm-italics">Kullback–Leibler divergence</i> (KL divergence or KLD). As expected, it is <span class="math">0</span> when the two random variables are identical.</p>
<p class="body">Formally, KLD is as follows:</p><!--<p class="Body"><span class="times">$$D\left( X_{1}, X_{2} \right) =
\sum\displaylimits_{x \in D} p_{1}\left(  x \right)\; \log \left(
\frac{p_{1}\left( x \right)}{p_{2}\left( x \right)}  \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_06-11.png" width="276"/></p>
</div>
<p class="fm-equation-caption">Equation 6.11 <span class="calibre" id="eq-kld-discr-univar"/></p>
<p class="body">For continuous univariate randoms,</p><!--<p class="Body"><span class="times">$$D\left( X_{1}, X_{2} \right) =
\int\displaylimits_{x \in D} p_{1}\left(  x \right)\; \log \left(
\frac{p_{1}\left( x \right)}{p_{2}\left( x \right)}  \right) dx$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_06-12.png" width="301"/></p>
</div>
<p class="fm-equation-caption">Equation 6.12 <span class="calibre" id="eq-kld-cont-univar"/></p>
<p class="body">For continuous multivariate randoms,</p><!--<p class="Body"><span class="times">$$D\left( X_{1}, X_{2} \right) =
\int\displaylimits_{ \vec{x} \in D} p_{1}\left( \vec{x} \right)\; \log
\left( \frac{p_{1}\left( \vec{x} \right)}{p_{2}\left( \vec{x}
\right)}  \right) d\vec{x}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="65" src="../../OEBPS/Images/eq_06-13.png" width="301"/></p>
</div>
<p class="fm-equation-caption">Equation 6.13 <span class="calibre" id="eq-kld-cont-multivar"/></p>
<p class="body">Let’s examine some properties of KLD:<a id="marker-208"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The KLD between identical random variables is zero. If <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub> = <i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">1</sub>(<i class="fm-italics">x</i>) = <i class="fm-italics">p</i><sub class="fm-subscript">2</sub>(<i class="fm-italics">x</i>)∀<i class="fm-italics">x</i> <span class="cambria">∈</span> <i class="fm-italics">D</i></span>. Then the <span class="math">log</span> term vanishes at every <i class="timesitalic">x</i>, and KLD is zero.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The KLD between non-identical probability distributions is always positive. We can see this by examining equation <a class="url" href="../Text/06.xhtml#eq-kld-discr-univar">6.11</a>. At all values of <i class="timesitalic">x</i> where <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">1</sub>(<i class="fm-italics">x</i>) &gt; <i class="fm-italics">p</i><sub class="fm-subscript">2</sub>(<i class="fm-italics">x</i>)</span>, the <span class="math">log</span> term is positive (since the logarithm of a number greater than <span class="math">1</span> is positive). On the other hand, at all values of <i class="timesitalic">x</i> where <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">1</sub>(<i class="fm-italics">x</i>) &lt; <i class="fm-italics">p</i><sub class="fm-subscript">2</sub>(<i class="fm-italics">x</i>)</span>, the <span class="math">log</span> term is negative (since the logarithm of a number less than <span class="math">1</span> is negative). But the positive terms get higher weights because <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">1</sub>(<i class="fm-italics">x</i>)</span> are higher at these points. In this context, it is worth noting that given any pair of PDFs, <i class="fm-italics">one cannot be uniformly higher than the other at all points</i>. This is because both of them must sum to <span class="math">1</span>. If one PDF is higher somewhere, it must be lower somewhere else to compensate.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Given a GT PDF <i class="timesitalic">p<sub class="fm-subscript">gt</sub></i> for a classification problem and a machine prediction <i class="timesitalic">p<sub class="fm-subscript">pred</sub></i>, minimizing the cross-entropy <span class="math">ℍ(<i class="fm-italics">gt</i>, <i class="fm-italics">pred</i>)</span> is logically equivalent to minimizing the KLD <span class="math"><i class="fm-italics">D</i>(<i class="fm-italics">gt</i>, <i class="fm-italics">pred</i>)</span>. This is because the entropy <span class="math">ℍ(<i class="fm-italics">gt</i>)</span> is a constant, independent of the machine parameters.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The KLD is <i class="fm-italics">not</i> symmetric: <span class="math"><i class="fm-italics">D</i>(<i class="fm-italics">X</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">X</i><sub class="fm-subscript">2</sub>) ≠ <i class="fm-italics">D</i>(<i class="fm-italics">X</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">X</i><sub class="fm-subscript">1</sub>)</span>.</p>
</li>
</ul>
<h3 class="fm-head1" id="sec-kld-gaussians">6.4.1 KLD between Gaussians</h3>
<p class="body">Since the Gaussian probability distribution is so important, in this subsection we look at the KLD between two Gaussian random variables <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span> and <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> having PDFs <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">1</sub>(<i class="fm-italics">x</i>) = <span class="cambria">𝒩</span>(<i class="fm-italics">x</i>; <i class="fm-italics">μ</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">σ</i><sub class="fm-subscript">1</sub>)</span> and <span class="math"><i class="fm-italics">p</i><sub class="fm-subscript">2</sub>(<i class="fm-italics">x</i>) = <span class="cambria">𝒩</span>(<i class="fm-italics">x</i>; <i class="fm-italics">μ</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">σ</i><sub class="fm-subscript">2</sub>)</span>. We derive the expression for the univariate case and simply state the expression for the multivariate case:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;D\left( X_{1}, X_{2} \right) =
\int\displaylimits_{-\infty}^{\infty} \overbrace{ \frac{1}{{\sqrt {2\pi
} \sigma_{1}}}e^{{\frac{ - \left( {x - \mu_{1} } \right)^2 }
{2\sigma_{1} ^2 }}} }^{ \mathcal{N}\left(x; \, \mu_{1}, \sigma_{1}
\right) }
\;
\log \left( \frac{\frac{1}{{\sqrt {2\pi } \sigma_{1}}}e^{{\frac{ -
\left( {x - \mu_{1} } \right)^2 } {2\sigma_{1} ^2 }}} }{\frac{1}{{\sqrt
{2\pi } \sigma_{2}}}e^{{\frac{ - \left( {x - \mu_{2} } \right)^2 }
{2\sigma_{2} ^2 }}} }  \right) dx \\
&amp;\quad = \int\displaylimits_{-\infty}^{\infty}  \mathcal{N}\left(x;
\, \mu_{1}, \sigma_{1} \right)
\left(
\log \frac{\sigma_{2}}{\sigma_{1}}
+ \frac{ \left(  x - \mu_{2} \right)^2 } {2\sigma_{2}^2 }
-  \frac{ \left( x - \mu_{1}  \right)^2 } {2\sigma_{1}^2 }
  \right) dx\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="124" src="../../OEBPS/Images/eq_06-13-a.png" width="868"/></p>
</div>
<p class="body">Opening the parentheses, we get</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\log \frac{\sigma_{2}}{\sigma_{1}}  \overbrace{
\int\displaylimits_{-\infty}^{\infty}  \mathcal{N}\left(x; \, \mu_{1},
\sigma_{1} \right) dx }^{= 1\text{, by equation~\ref{eq-continuous-prob-sum}}}
+  \frac{ 1 } {2\sigma_{2}^2 }
\int\displaylimits_{-\infty}^{\infty}  \left(  x - \mu_{2} \right)^2
\mathcal{N}\left(x; \, \mu_{1}, \sigma_{1} \right) dx\\
&amp;\qquad -\frac{ 1 } {2\sigma_{1}^2 } \overbrace{
\int\displaylimits_{-\infty}^{\infty}  \left(  x - \mu_{1} \right)^2
\mathcal{N}\left(x; \, \mu_{1}, \sigma_{1} \right) dx
}^{=\sigma_{1}^{2}\text{, by equation~\ref{eq-variance}}}  \\
&amp;\quad=  \log \frac{\sigma_{2}}{\sigma_{1}}
+  \frac{ 1 } {2\sigma_{2}^2 }
\int\displaylimits_{-\infty}^{\infty}  \left(  x - \mu_{1} + \mu_{1} -
\mu_{2} \right)^2 \mathcal{N}\left(x; \, \mu_{1}, \sigma_{1} \right) dx
-\frac{1}{2} %\\
% &amp;\quad =  \log \frac{\sigma_{2}}{\sigma_{1}}  +
%+  \frac{ 1 } {2\sigma_{2}^2 }
\int\displaylimits_{-\infty}^{\infty}  \left(  x - \mu_{1} + \mu_{1} -
\mu_{2} \right)^2 \mathcal{N}\left(x; \, \mu_{1}, \sigma_{1} \right) dx
% -\frac{1}{2}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="107" src="../../OEBPS/Images/eq_06-13-b.png" width="1251"/></p>
</div>
<p class="body"><a id="marker-209"/>Expanding the square term, we get</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;D\left( X_{1}, X_{2} \right) =  \log \frac{\sigma_{2}}{\sigma_{1}}
\\
&amp;\quad + \frac{ 1 } {2\sigma_{2}^2 }
\int\displaylimits_{-\infty}^{\infty}  \left(  \left( x - \mu_{1}
\right)^{2} + \left( \mu_{1} - \mu_{2} \right)^2 + 2 \left( x - \mu_{1}
\right)\left( \mu_{1} - \mu_{2} \right)  \right) \mathcal{N}\left(x; \,
\mu_{1}, \sigma_{1} \right) dx -\frac{1}{2}
\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="68" src="../../OEBPS/Images/eq_06-13-c.png" width="756"/></p>
</div>
<p class="body">Since</p><!--<p class="Body"><span class="times">$$\int\displaylimits_{-\infty}^{\infty}  \left(  x
- \mu_{1} \right) \mathcal{N}\left(x; \, \mu_{1}, \sigma_{1} \right) dx
= \mu_{1} - \mu_{1} = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_06-13-d.png" width="331"/></p>
</div>
<p class="body">the final equation for the KLD between two univariate Gaussian random variables <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> with PDFs <span class="math"><span class="cambria">𝒩</span>(<i class="fm-italics">x</i>; <i class="fm-italics">μ</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">σ</i><sub class="fm-subscript">1</sub>)</span> and <span class="math"><span class="cambria">𝒩</span>(<i class="fm-italics">x</i>; <i class="fm-italics">μ</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">σ</i><sub class="fm-subscript">2</sub>)</span> becomes</p><!--<p class="Body"><span class="times">$$D\left( X_{1}, X_{2} \right)
=  \log \frac{\sigma_{2}}{\sigma_{1}}  +
\frac{ \sigma_{1}^{2} + \left( \mu_{1} - \mu_{2} \right)^2  }
{2\sigma_{2}^2 }
-\frac{1}{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_06-14.png" width="329"/></p>
</div>
<p class="fm-equation-caption">Equation 6.14 <span class="calibre" id="eq-kld-univar-gauss"/></p>
<p class="body">The KLD between two <i class="timesitalic">d</i>-dimensional Gaussian random variables <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">2</sub></span> with PDFs <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <i class="fm-italics">μ</i><sub class="fm-subscript">1</sub>, <b class="fm-bold">Σ</b><sub class="fm-subscript">1</sub>)</span> and <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <i class="fm-italics">μ</i><sub class="fm-subscript">2</sub>, <b class="fm-bold">Σ</b><sub class="fm-subscript">2</sub>)</span> is</p><!--<div class="flalign">
<p class="Body">D( X_1</span>, X_2</span> ) = ( tr(
_2</span>^-1</span> _1</span> ) + ( _2</span> - _1</span> )^T</span> _2</span>^-1</span> (
_2</span> - _1</span> ) -d + ( ) )</p>
</div>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_06-15.png" width="575"/></p>
</div>
<p class="fm-equation-caption">Equation 6.15 <span class="calibre" id="eq-kld-multivar-gauss"/></p>
<p class="body">where the operator <i class="timesitalic">tr</i> denotes the <i class="fm-italics">trace</i> of a matrix (sum of diagonal elements) and the operator <i class="timesitalic">det</i> denotes the determinant.</p>
<p class="body">Listing 6.3 shows the Python PyTorch code to compute the KLD.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code to compute the KLD, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/KMyj">http://mng.bz/KMyj</a>.</p>
<p class="fm-code-listing-caption" id="listing-6.3-computing-the-kld">Listing 6.3 Computing the KLD</p>
<pre class="programlisting">from torch.distributions import kl_divergence

p = Normal(0, 5)
q = Normal(0, 10)             <span class="fm-combinumeral">①</span>
r = Normal(0, 20)

kld_p_p = kl_divergence(p, p)
kld_p_q = kl_divergence(p, q)
kld_q_p = kl_divergence(q, p) <span class="fm-combinumeral">②</span>
kld_p_r = kl_divergence(p, r) <span class="fm-combinumeral">③</span>

assert kld_p_p == 0           <span class="fm-combinumeral">④</span>

assert kld_p_q != kld_q_p     <span class="fm-combinumeral">⑤</span>

assert kld_p_q &lt; kld_p_r \5</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates three Gaussian distributions<br class="calibre20"/>
  with the same means but different<br class="calibre20"/>
  standard deviations</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Computes the KLD between various pairs of distributions</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The KLD between a distribution and itself is 0.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The KLD is not symmetric.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> See figure <a class="url" href="../Text/06.xhtml#fig-kld-gaussian">6.4</a>.</p>
<p class="body"><a id="marker-210"/>In figure <a class="url" href="../Text/06.xhtml#fig-kld-gaussian">6.4</a>, we compare three Gaussian distributions <i class="timesitalic">p</i>, <i class="timesitalic">q</i>, and <i class="timesitalic">r</i> with the same <i class="timesitalic">μ</i>s but different <i class="timesitalic">σ</i>s. <span class="math"><i class="fm-italics">KLD</i>(<i class="fm-italics">p</i>, <i class="fm-italics">q</i>) &lt; <i class="fm-italics">KLD</i>(<i class="fm-italics">p</i>, <i class="fm-italics">r</i>)</span> because <i class="timesitalic">σ<sub class="fm-subscript">p</sub></i> is closer to <i class="timesitalic">σ<sub class="fm-subscript">q</sub></i> than <i class="timesitalic">σ<sub class="fm-subscript">r</sub></i>.<br class="calibre20"/>
  In figure <a class="url" href="../Text/06.xhtml#fig-kld-uniform">6.4</a>, we compare a uniform distribution <i class="timesitalic">p</i> with two Gaussian distributions <i class="timesitalic">q</i> and <i class="timesitalic">r</i> that have different <i class="timesitalic">μ</i>s but the same <i class="timesitalic">σ</i>s. <span class="math"><i class="fm-italics">KLD</i>(<i class="fm-italics">p</i>, <i class="fm-italics">q</i>) &lt; <i class="fm-italics">KLD</i>(<i class="fm-italics">p</i>, <i class="fm-italics">r</i>)</span> because <i class="timesitalic">μ<sub class="fm-subscript">p</sub></i> is closer to <i class="timesitalic">μ<sub class="fm-subscript">q</sub></i> than <i class="timesitalic">μ<sub class="fm-subscript">r</sub></i>.</p>
<h2 class="fm-head" id="conditional-entropy">6.5 Conditional entropy</h2>
<p class="body">In section <a class="url" href="../Text/06.xhtml#sec-entropy">6.2</a>, we learned that entropy measures the uncertainty in a system. Earlier, in section <a class="url" href="../Text/06.xhtml#sec-cond-prob-bayes">6.1.2</a>, we studied conditional probability, which measures the probability of occurrence of one set of random variables under the condition that another set has known fixed values. In this section, we combine the two concepts into a new concept called <i class="fm-italics">conditional entropy</i>.</p>
<p class="body">Consider the following question from table <a class="url" href="../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob">6.2</a>. What is the entropy of the weight variable <i class="timesitalic">W</i> under the condition that the value of the height variable <i class="timesitalic">H</i> is <span class="math"><i class="fm-italics">F</i><sub class="fm-subscript">1</sub></span>? As observed in section <a class="url" href="../Text/06.xhtml#sec-joint-marginal-prob-recap">6.1.1</a>, the condition effectively restricts our universe to a single row (in this case, the top row) of the table. We can compute the entropy of the elements of that row mathematically, using equation <a class="url" href="../Text/06.xhtml#eq-entropy-discr-univar">6.5</a>, as</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\overbrace{ \mathbb{H}\left( W \middle\vert H = F_{1}  \right) }^{
\text{conditional entropy of W given } H = F_{1} } = -
\sum\displaylimits_{j=1}^{3} p\left( E_{j} \middle\vert F_{1} \right) log \left( p\left( E_{j} \middle\vert F_{1} \right) \right)\\
&amp;= -(0.77 \times log\left(0.77\right) + 0.154
\times  log\left(0.154\right) + 0.077  \times log\left(0.077\right))
= 0.6868\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="80" src="../../OEBPS/Images/eq_06-15-a.png" width="988"/></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="478" id="fig-kld-gaussian" src="../../OEBPS/Images/CH06_F04a_Chaudhury.png" width="693"/></p>
<p class="figurecaption">(a) <span class="math"><i class="fm-italics">p</i> ≡ <span class="cambria">𝒩</span>(<i class="fm-italics">μ</i> = 0, <i class="fm-italics">σ</i> = 5)</span>, <span class="math"><i class="fm-italics">q</i> ≡ <span class="cambria">𝒩</span>(<i class="fm-italics">μ</i> = 0, <i class="fm-italics">σ</i> = 10)</span>, <span class="math"><i class="fm-italics">r</i> ≡ <span class="cambria">𝒩</span>(<i class="fm-italics">μ</i> = 0, <i class="fm-italics">σ</i> = 20)</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="470" id="fig-kld-uniform" src="../../OEBPS/Images/CH06_F04b_Chaudhury.png" width="693"/></p>
<p class="figurecaption">(b) <span class="math"><i class="fm-italics">p</i> ≡ <i class="fm-italics">U</i>(<i class="fm-italics">a</i> = −20, <i class="fm-italics">b</i> = 20)</span>, <span class="math"><i class="fm-italics">q</i> ≡ <span class="cambria">𝒩</span>(<i class="fm-italics">μ</i> = 0, <i class="fm-italics">σ</i> = 20)</span>, <span class="math"><i class="fm-italics">r</i> ≡ <span class="cambria">𝒩</span>(<i class="fm-italics">μ</i> = −50, <i class="fm-italics">σ</i> = 20)</span></p>
</div>
<p class="fm-table-caption" id="fig-kld-gaussian-uniform">Figure 6.4 KLD between example distributions<a id="marker-211"/></p>
<p class="body">Similarly,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\mathbb{H}\left( W \middle\vert H = F_{2}  \right)  &amp;=
-\sum\displaylimits_{j=1}^{3} p\left( E_{j} \middle\vert F_{2} \right) log \left( p\left( E_{j} \middle\vert F_{2} \right) \right)\\
&amp;= -(0.083 \times log\left(0.083\right) + 0.83
\times  log\left(0.83\right) + 0.083  \times log\left(0.083\right)) \\
&amp;= 0.5678\\
  \mathbb{H}\left( W \middle\vert H = F_{3}  \right)  &amp;=  -\sum\displaylimits_{j=1}^{3} p\left( E_{j}
\middle\vert F_{3} \right) log \left( p\left( E_{j} \middle\vert F_{3}
\right) \right)\\
&amp;= -(0.077 \times log\left(0.077\right) + 0.154
\times  log\left(0.154\right) + 0.77 \times log\left(0.77\right))  \\
&amp;= 0.6868
\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="135" src="../../OEBPS/Images/eq_06-15-b.png" width="925"/></p>
</div>
<p class="body"><span class="math">ℍ(<i class="fm-italics">W</i>|<i class="fm-italics">H</i> = <i class="fm-italics">F<sub class="fm-subscript">i</sub></i>)</span> is the entropy of <i class="timesitalic">W</i> given <span class="math"><i class="fm-italics">H</i> = <i class="fm-italics">F<sub class="fm-subscript">i</sub></i></span> for <span class="math"><i class="fm-italics">i</i> = 1</span> or <span class="math">2</span> or <span class="math">3</span>. What is the overall conditional entropy of <i class="timesitalic">W</i> given <i class="timesitalic">H</i>: that is, <span class="math">ℍ(<i class="fm-italics">W</i>|<i class="fm-italics">H</i>)</span>? To compute this, we take the expected value (that is, the probability-weighted average; see equation <a class="url" href="../Text/05.xhtml#eq-discrete-univar-expected-val">5.8</a>) of the conditional entropy <span class="math">ℍ(<i class="fm-italics">W</i>|<i class="fm-italics">H</i> = <i class="fm-italics">F<sub class="fm-subscript">i</sub></i>)</span> over all possible values of <i class="timesitalic">i</i>:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\overbrace{ \mathbb{H}\left( W \middle\vert H \right) }^{
\text{conditional entropy of W given } H } &amp;=
\sum\displaylimits_{i=1}^{3} p\left( F_{i}
\right)      \left(-\sum\displaylimits_{j=1}^{3} p\left( E_{j}
\middle\vert F_{i} \right) log \left( p\left( E_{j} \middle\vert F_{i}
\right) \right)\right)\\
&amp;= (0.6868 * 0.26  + 0.5678 * 0.48 + 0.6868 * 0.26) = 0.6297\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="84" src="../../OEBPS/Images/eq_06-15-c.png" width="930"/></p>
</div>
<p class="body">This idea can be generalized. Formally, given two random variables <i class="timesitalic">X</i> and <i class="timesitalic">Y</i> that can take values <span class="math"><i class="fm-italics">x</i> <span class="cambria">∈</span> <i class="fm-italics">D<sub class="fm-subscript">x</sub></i></span>, <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> <span class="cambria">∈</span> <i class="fm-italics">D<sub class="fm-subscript">y</sub></i></span>, respectively,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\mathbb{H}\left( X \middle\vert Y \right) = \overbrace{
\sum\displaylimits_{y \in D_{y}} p\left( y \right) \overbrace{
\left(-\sum\displaylimits_{x \in D_{x}} p\left( x \middle\vert y \right) log \left( p\left( x \middle\vert y \right) \right) \right) }^{
\mathbb{H} \left( X  \middle\vert Y=y \right) } }^{ \mathbb{E}_{y}
\mathbb{H} \left( X \middle\vert Y=y \right) } \Leftrightarrow
\text{discrete}\\
&amp;\mathbb{H}\left( X \middle\vert Y \right) = \int\displaylimits_{y
\in D_{y}} p\left( y \right) \left( -\int\displaylimits_{x \in D_{x}} p\left( x \middle\vert y \right) log \left( p\left( x \middle\vert y
\right) \right)\; dx \right) \; dy  \Leftrightarrow
\text{continuous}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="142" src="../../OEBPS/Images/eq_06-16.png" width="465"/></p>
</div>
<p class="fm-equation-caption">Equation 6.16 <span class="calibre" id="eq-cond-entropy-discr"/></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="83" src="../../OEBPS/Images/eq_06-17.png" width="529"/></p>
</div>
<p class="fm-equation-caption">Equation 6.17 <span class="calibre" id="eq-cond-entropy-cont"/></p>
<h3 class="fm-head1" id="chain-rule-of-conditional-entropy">6.5.1 Chain rule of conditional entropy</h3>
<p class="body"><a id="marker-212"/>This rule states:</p>
<p class="fm-equation"><span class="math">ℍ(<i class="fm-italics">X</i>|<i class="fm-italics">Y</i>) = ℍ(<i class="fm-italics">X</i>, <i class="fm-italics">Y</i>) − ℍ(<i class="fm-italics">Y</i>)</span></p>
<p class="fm-equation-caption">Equation 6.18 <span class="calibre" id="eq-chain-rule-cond-prob"/></p>
<p class="body">This can be derived from equation <a class="url" href="../Text/06.xhtml#eq-cond-entropy-cont">6.17</a>.</p><!--<p class="Body"><span class="times">$$\mathbb{H}\left( X \middle\vert Y
\right) = \int\displaylimits_{y \in D_{y}} p\left( y \right) \left(
-\int\displaylimits_{x \in D_{x}} p\left( x \middle\vert y \right) log
\left( p\left( x \middle\vert y \right) \right)\; dx \right) \; dy$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="82" src="../../OEBPS/Images/eq_06-18-a.png" width="419"/></p>
</div>
<p class="body">Applying Bayes’ theorem (equation <a class="url" href="../Text/06.xhtml#eq-bayes-theorem-2var">6.1</a>),</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\mathbb{H}\left( X \middle\vert Y \right) = -\int\displaylimits_{y
\in D_{y}} \int\displaylimits_{x \in D_{x}} \overbrace{ p\left( y
\right) p\left( x \middle\vert y \right) }^{ p\left( x, y \right) } log
\overbrace{ \left( p\left( x \middle\vert y \right) \right)}^{log \left(
\frac{ p\left( x, y \right) }{ p\left( y \right) } \right)} \;  dx \; dy
\nonumber\\[3pt]
&amp;= \overbrace{-\int\displaylimits_{y \in D_{y}}
\int\displaylimits_{x \in D_{x}}  p\left( x, y \right) log \left( p\left( x, y \right)  \right)  \;  dx \; dy}^{\mathbb{H}\left( X, Y
\right)}
+ \int\displaylimits_{y \in D_{y}} log \left( p\left( y \right) \right)
\overbrace{ \int\displaylimits_{x \in D_{x}} p\left( x, y \right)  dx
}^{ \text{marginal probability } p\left( y \right) }\; dy  \nonumber\\[3pt]
&amp;= \mathbb{H}\left( X, Y \right) - \mathbb{H}\left( Y
\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="103" src="../../OEBPS/Images/eq_06-19.png" width="1168"/></p>
</div>
<p class="fm-equation-caption">Equation 6.19</p>
<h2 class="fm-head" id="sec-model_param_estimation">6.6 Model parameter estimation</h2>
<p class="body">Suppose we have a set of sampled input data points <span class="math"><i class="fm-italics">X</i> = {<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(2)</sup>,⋯, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span> from a distribution. We refer to the set collectively as <i class="fm-italics">training data</i>. Note that we are <i class="fm-italics">not</i> assuming it is <i class="fm-italics">labeled</i> training data—we do not know the outputs corresponding to the inputs <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>. Also, suppose that based on our knowledge of the problem, we have decided which model family to use. Of course, simply knowing the family is not enough; we need to know (or estimate) the model parameters before we can use the model. For instance, our model family might be Gaussian, <span class="math"><span class="cambria">𝒩</span>(<i class="fm-italics">x</i>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>, <b class="fm-bold">Σ</b>)</span>. Until we know the actual value of the parameters <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> and <span class="timesbold">Σ</span>, we do not fully know the model and cannot use it.</p>
<p class="body">How do we estimate the model parameters from the unlabeled training data? This is what we cover in this section. At the moment, we are discussing it without referring to any specific model architecture, so let’s denote model parameters with a generic symbol <i class="timesitalic">θ</i>. For instance, when dealing with Gaussian models, <span class="math"><i class="fm-italics">θ</i> = {<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>, <b class="fm-bold">Σ</b>}</span>.</p>
<h3 class="fm-head1" id="likelihood-evidence-and-posterior-and-prior-probabilities">6.6.1 Likelihood, evidence, and posterior and prior probabilities</h3>
<p class="body"><a id="marker-213"/>Before tackling the problem of parameter estimation, it is important to have a clear understanding of the terms <i class="fm-italics">likelihood</i>, <i class="fm-italics">evidence</i>, <i class="fm-italics">posterior probability</i>, and <i class="fm-italics">prior probability</i> in the current context. Equation <a class="url" href="../Text/06.xhtml#eq-posterior-prior-likelihood-evidence">6.20</a> illustrates them. Using Bayes’ theorem,</p><!--<p class="Body"><span class="times">$$\overbrace{p\left(  \theta  \middle\vert  X
\right)}^{\text{posterior  probability}}  = \frac{ p\left( X, \theta
\right) }{ p\left(X \right) }
= \frac{ \overbrace{p\left( X \middle\vert \theta
\right)}^{\text{likelihood}}  \;  \overbrace{p\left( \theta
\right)}^{\text{prior  probability}} }{ \underbrace{p\left(X
\right)}_{evidence} }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="113" src="../../OEBPS/Images/eq_06-20.png" width="390"/></p>
</div>
<p class="fm-equation-caption">Equation 6.20 <span class="calibre" id="eq-posterior-prior-likelihood-evidence"/></p>
<p class="body">Let’s first examine the likelihood term. Using the fact that data instances are independent of each other,</p><!--<p class="Body"><span class="times">$$p\left(X \middle\vert \theta\right) = p\left(\vec{x}^{ \left( 1 \right) }, \vec{x}^{ \left( 2 \right) },
\cdots, \vec{x}^{ \left( n \right) } \middle\vert \theta\right) =
\prod_{i=1}^{n} p\left(\vec{x}^{ \left( i \right) } \middle\vert
\theta\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_06-20-a.png" width="389"/></p>
</div>
<p class="body">Now, <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>|<i class="fm-italics">θ</i>)</span> is essentially the probability density of the distribution family we have chosen. For instance, if the model in question in Gaussian, then given <span class="math"><i class="fm-italics">θ</i> = {<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>, <b class="fm-bold">Σ</b>}</span>, this will be</p><!--<p class="Body"><span class="times">$$p\left(\vec{x}^{ \left( i
\right) } \middle\vert \theta\right) =
\mathcal{N}\left(\vec{x}^{ \left( i \right) }; \, \vec{\mu},
\boldsymbol{\Sigma}\right) =
\frac{1}{\left(2\pi \det \boldsymbol{\Sigma} \right)^{\frac{1}{2}}} e^{-\frac{1}{2}\left(\vec{x}^{ \left( i \right) } - \vec{\mu}\right)^{T}
\boldsymbol{\Sigma}^{-1} \left(\vec{x}^{ \left( i \right) } -
\vec{\mu}\right)}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="52" src="../../OEBPS/Images/eq_06-20-b.png" width="479"/></p>
</div>
<p class="body">which is basically an expression for the Gaussian PDF: a restatement of equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a> (but in equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a>, we dropped the “given <i class="timesitalic">θ</i>,” part in the notation and expressed <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">θ</i>)</span> simply as <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>). Thus, we can always express the likelihood from the PDF of the chosen model family using the independence of individual training data instances.</p>
<p class="body">Now let’s examine the prior probability, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>)</span>. It typically comes from some physical constraint—without referring to the input. A very popular approach is to say that, all other things being equal, we prefer parameters with smaller magnitudes. By this token, the larger the total magnitude <span class="math">||<i class="fm-italics">θ</i>||<sup class="fm-superscript">2</sup></span>, the lower the prior probability. For instance, we may use</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>) <span class="cambria">∝</span> <i class="fm-italics">e</i><sup class="fm-superscript">−||<i class="fm-italics1">θ</i>||<sup class="fm-superscript">2</sup></sup></span></p>
<p class="fm-equation-caption">Equation 6.21 <span class="calibre" id="eq-prior-mag"/></p>
<p class="body">An indirect justification for favoring parameter vectors with the smallest length (magnitude) can be found in the principle of Occam’s razor. It states, <i class="fm-italics">Entia non sunt multiplicanda praeter necessitatem</i>, which roughly translates to “One should not multiply unnecessarily.” This is often interpreted in machine learning and other disciplines as “favor the briefest representation.”</p>
<p class="body">As shown previously, we can always express the likelihood and prior terms. Using them, we can formulate different paradigms, each with a different quantity, to optimize in order to estimate the unknown probability distribution parameters from training data. These techniques can be broadly classified into the following categories:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Maximum likelihood parameter estimation MLE)</p>
</li>
<li class="fm-list-bullet">
<p class="list">Maximum a posteriori (MAP) parameter estimation</p>
</li>
</ul>
<p class="body">We provide an overview of them next. You will notice that, in all the methods, we typically preselect a distribution family as a model and then estimate the parameter values by maximizing one probability or another.</p>
<p class="body">Later in the chapter, we look at MLE in the special case of the Gaussian family of distributions. Further down the line, we look at MLE with respect to Gaussian mixture models. Another technique outlined later is evidence maximization: we will visit it in the context of variational autoencoders.<a id="marker-214"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">The log-likelihood trick</p>
<p class="fm-sidebar-text">If we choose a distribution family whose PDF is exponential (the most obvious example is Gaussian), instead of maximizing the likelihood, we usually maximize its logarithm, aka the <i class="fm-italics">log-likelihood</i>. We can do this because whatever maximizes a quantity also maximizes its logarithm and vice versa. But the logarithm simplifies expressions in the case of exponential probability functions. This becomes obvious if we note that</p>
<p class="fm-sidebar-text"><span class="math">          <i class="fm-italics">log</i>(<i class="fm-italics">e</i><sup class="superscript-italic">x</sup>) = <i class="fm-italics">x</i></span><br class="calibre20"/>
<span class="math">          <i class="fm-italics">log</i><span class="large">(Π</span> <i class="fm-italics">e</i><sup class="fm-superscript"><i class="fm-italics1">x</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></sup><span class="large">)</span> = <span class="large">Σ</span> <i class="fm-italics">x</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span></p>
</div>
<h3 class="fm-head1" id="sec-max_likelihood_estimation">6.6.2 Maximum likelihood parameter estimation (MLE)</h3>
<p class="body">In MLE of parameters, we ask, “What parameter values will maximize the joint likelihood of the training data instances?” In this context, remember that likelihood is the of a data instance occurring given specific parameter values (equation <a class="url" href="../Text/06.xhtml#eq-posterior-prior-likelihood-evidence">6.20</a>). Expressed mathematically,<a id="marker-215"/></p>
<p class="fm-quote">MLE estimates what value of <i class="timesitalic">θ</i> maximizes <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">θ</i>)</span>. The geometric mental picture is as follows: we want to estimate the unknown parameters for our model probability distribution such that if we draw many samples from that distribution, the sample point cloud will largely overlap the training data.</p>
<p class="body">Often we employ the log-likelihood trick and maximize the log-likelihood instead of the actual likelihood.</p>
<p class="body">For some models, such as Gaussians, this maximization problem can be solved analytically, and a closed-form solution can be obtained (as shown in section <a class="url" href="../Text/06.xhtml#sec-gauss_max_likelihood_estimation">6.8</a>). For others, such as Gaussian mixture models (GMMs), the maximization problem ields no closed-form solution, and we go for an iterative solution (as shown in section <a class="url" href="../Text/06.xhtml#sec-gmm_fit">6.9.4</a>).</p>
<h3 class="fm-head1" id="sec-MAP_estimation">6.6.3 Maximum a posteriori (MAP) parameter estimation and regularization</h3>
<p class="body">Instead of asking what parameter value maximizes the probability of occurrence of the training data instances, we can ask, “What are the most probable parameter values, given the training data?” Expressed mathematically, in MAP, we directly estimate the <i class="timesitalic">θ</i> that maximizes <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>|<i class="fm-italics">X</i>)</span>. Using equation <a class="url" href="../Text/06.xhtml#eq-posterior-prior-likelihood-evidence">6.20</a>,</p><!--<p class="FM-Equation-Caption"><span class="times">$$p\left(  \theta  \middle\vert  X \right) =
\frac{ p\left( X \middle\vert \theta \right)  \;  p\left( \theta\right)
}{ p\left( X \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_06-22.png" width="192"/></p>
</div>
<p class="fm-equation-caption">Equation 6.22 <span class="calibre" id="eq-MAP"/></p>
<p class="body">Since the denominator is independent of <i class="timesitalic">θ</i>, maximizing the numerator with respect to <i class="timesitalic">θ</i> maximizes the fraction. Thus</p>
<p class="fm-equation">In MAP parameter estimation, we look for parameters <i class="timesitalic">θ</i> that maximize <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">θ</i>)<i class="fm-italics">p</i>(<i class="fm-italics">θ</i>)</span>.</p>
<div class="sgc">
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="listb">The first factor, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">θ</i>)</span>, is what we optimized in MLE and comes from the model definition (such as equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a> for multivariate Gaussian models).</p>
</li>
<li class="fm-list-bullet">
<p class="listb">The second factor, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>)</span>, is the prior term, which usually incentivizes the optimization system to choose a solution with predefined properties like smaller parameter magnitudes equation <a class="url" href="../Text/06.xhtml#eq-prior-mag">6.21</a>).</p>
</li>
</ul>
</div>
<p class="body">Viewed this way, MAP estimation is equivalent to <i class="fm-italics">MLE parameter estimation with regularization</i>. Regularization is a technique often used in optimization. In regularized optimization, we add a term to the expression being maximized or minimized. This term effectively incentivizes the system to choose the solution with the smallest magnitudes of the unknown from the set of possible solutions. It is easy to see that MAP estimation essentially imposes the prior probability term on top of MLE. This extra term acts as a regularizer, incentivizing the system to choose the lowest magnitude parameters while still trying to maximize the likelihood of the training data.</p>
<p class="body">Equation <a class="url" href="../Text/06.xhtml#eq-MAP">6.22</a> can be interpreted another way. When we have no training data, all we can do is estimate the parameters from our prior beliefs about the system: the prior term <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>)</span>. When the training data set <i class="timesitalic">X</i> arrives, it influences the system through the likelihood term <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">θ</i>)</span>. As more and more training data arrives, the prior term (whose magnitude does not change with training data) dominates less and less, and the posterior probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>|<i class="fm-italics">X</i>)</span> is dominated more by the likelihood.</p>
<h2 class="fm-head" id="sec-evidence_maximization">6.7 Latent variables and evidence maximization</h2>
<p class="body">Suppose we have the height and weight data for a population (say, for the adult residents of our favorite town, Statsville). A single data instance looks like this:</p><!--<p class="Body"><span class="times">$$\vec{x} =
\begin{bmatrix}
\text{height }\\[-3pt]
\text{weight}
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_06-22-a.png" width="101"/></p>
</div>
<p class="body">Although the data is not explicitly labeled or classified, we know the data points can be clustered into two distinct classes, <i class="fm-italics">male</i> and <i class="fm-italics">female</i>. It is reasonable to expect that the distribution of each class is much simpler than the overall distribution. For instance, here, the distributions for males and females may be Gaussians individually (presumably, the means for females will occur at smaller height and weight values). The combined distribution does not fit any of the distributions we have discussed so far (later, we see it is a Gaussian mixture).</p>
<p class="body">We look at such situations in more detail in connection to Gaussian mixture modeling and variational autoencoders. Here we only note that in these cases, it is often beneficial to introduce a variable for the class, say <i class="timesitalic">Z</i>. In this example, <i class="timesitalic">Z</i> is discrete: it can take one of two values, <i class="fm-italics">male</i> or <i class="fm-italics">female</i>. Then we can model the overall distribution as a combination of simple distributions, each corresponding to a specific value of <i class="timesitalic">Z</i>.</p>
<p class="body">Such variables <i class="timesitalic">Z</i> that are <i class="fm-italics">not</i> part of the observed data <i class="timesitalic">X</i> but are introduced to facilitate modeling are called <i class="fm-italics">latent</i> or <i class="fm-italics">hidden</i> variables/parameters. Latent variables are connected to observed variables through the usual Bayesian expression:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;p(\vec{x}, \vec{z}) = p\left( \vec{x} \middle\vert \vec{z} \right) p\left(\vec{z}\right)\\
&amp;p\left( \vec{z} \middle\vert \vec{x} \right) = \frac{p\left(\vec{x}
\middle\vert \vec{z} \right) p\left(\vec{z}\right) }{p\left( \vec{x}
\right)}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="84" src="../../OEBPS/Images/eq_06-22-b.png" width="164"/></p>
</div>
<p class="body"><a id="marker-216"/>How do we estimate the distribution of <i class="timesitalic">Z</i>? One way is to ask, “What distribution of the hidden variables would maximize the probability of exactly these training data points being returned if we drew random samples from the distribution?” The philosophy behind this is as follows: we assume that the training data points are fairly typical and have a high probability of occurrence in the unknown data distribution. Hence, we try to find a distribution under which the training data points will have the highest probabilities.</p>
<p class="body">Geometrically speaking, each data point (vector) can be viewed as a point in some <i class="timesitalic">d</i>-dimensional space, where <i class="timesitalic">d</i> is the number of elements in the vector <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i></span>. The training data points typically occupy a region within that space. We are looking for a distribution whose mass is largely aligned with the training data region. In other words, the probability associated with the training data points is as high as possible—the sample distribution cloud largely overlaps the training data cloud.</p>
<p class="body">Expressed mathematically, we want to identify <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> and <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> that maximize the quantity</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left( X \right) &amp;= \int p\left( X, z  \right) d z  =
\int  \prod_{i=1}^{N}  p\left( \vec{x}^{ \left( i \right)
},   \vec{z}  \right) d \vec{z}
= \int  \prod_{i=1}^{N}  p\left( \vec{x}^{ \left( i \right)
}  \middle\vert  \vec{z}  \right) p\left(  \vec{z}
\right)  d  \vec{z}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_06-23.png" width="540"/></p>
</div>
<p class="fm-equation-caption">Equation 6.23 <span class="calibre" id="eq-evidence_maximizn"/></p>
<p class="body">As usual, we get <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> from the PDF of our chosen model family and <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span>)</span> through some physical constraint.</p>
<h2 class="fm-head" id="sec-gauss_max_likelihood_estimation">6.8 Maximum likelihood parameter estimation for Gaussians</h2>
<p class="body">We look at this with a one-dimensional example, but the results derived apply to higher dimensions. Suppose we are trying to predict whether an adult Statsville resident is female, given that the resident’s height lies in a specified range <span class="math">[<i class="fm-italics">a</i>, <i class="fm-italics">b</i>]</span>. For this purpose, we have collected a set of height samples of adult <i class="fm-italics">female</i> Statsville residents. These height samples constitute our training data. Let’s denote them as <span class="math"><i class="fm-italics">x</i><sup class="fm-superscript">(1)</sup>, <i class="fm-italics">x</i><sup class="fm-superscript">(2)</sup>, ⋯, <i class="fm-italics">x</i><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup></span>. Based on physical considerations, we expect the distribution of heights of adult Statsville females to be a Gaussian distribution with unknown mean and variance. Our goal is to determine them from the training data via MLE, which effectively estimates a distribution whose sample cloud maximally matches the distribution of the training data points.</p>
<p class="body"><a id="marker-217"/>Let’s denote the (as yet unknown) mean and variance of the distribution as <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i>. Then, from equation <a class="url" href="../Text/05.xhtml#eq-univar-normal">5.22</a>, we get</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left(x^{ \left( i \right) } \middle\vert \mu, \sigma \right)
&amp;=  \frac{1}{{\sqrt {2\pi } \sigma}}e^{{\frac{ - \left( {x^{ \left( i \right) } - \mu } \right)^2 } {2\sigma ^2 }}}\\
\prod_{i=1}^{n} p\left(x^{ \left( 1 \right) }, x^{ \left( 2 \right) },
\cdots, x^{ \left( n \right) } \middle\vert \mu, \sigma \right) &amp;=
\frac{1}{\left(\sqrt {2\pi } \sigma\right)^{n}} e^{{\frac{
-\sum_{i=1}^{n} \left( {x^{ \left( i \right) } - \mu } \right)^2 }
{2\sigma ^2 }}}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="136" src="../../OEBPS/Images/eq_06-23-a.png" width="444"/></p>
</div>
<p class="body">Employing the log-likelihood trick,</p><!--<p class="Body"><span class="times">$$\begin{aligned} log \prod_{i=1}^{n} p\left(x^{ \left( 1  \right) }, x^{ \left( 2 \right)
}, \cdots, x^{ \left( n \right) } \middle\vert \mu, \sigma \right)
&amp;= log \left(\frac{1}{\left(\sqrt {2\pi } \sigma\right)^{n}} e^{{\frac{
-\sum_{i=1}^{n} \left( {x^{ \left( i \right) } - \mu } \right)^2 }
{2\sigma ^2 }}} \right)\\[-2pt]
&amp;= -n log\left(\sqrt{2 \pi}\right) - n log \sigma - \frac{
\sum_{i=1}^{n} \left( {x^{ \left( i  \right) } - \mu } \right)^2 }
{2\sigma^2 }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="90" src="../../OEBPS/Images/eq_06-23-b.png" width="837"/></p>
</div>
<p class="body">To maximize with respect to <i class="timesitalic">μ</i>, we solve</p><!--<p class="Body"><span class="times">$$\frac{\partial}{\partial \mu} log
\prod_{i=1}^{n} p\left(x^{ \left( 1  \right) }, x^{ \left( 2 \right) },
\cdots, x^{ \left( n \right) } \middle\vert \mu, \sigma \right) = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_06-23-c.png" width="332"/></p>
</div>
<p class="body">or</p><!--<p class="Body"><span class="times">$$\frac{ 2\sum_{i=1}^{n} \left( {x^{
\left( i \right) } - \mu } \right) } {2 \sigma^2 } = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_06-23-d.png" width="162"/></p>
</div>
<p class="body">or</p><!--<p class="Body"><span class="times">$$\sum_{i=1}^{n} \left( {x^{ \left( i \right) } -
\mu } \right) = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_06-23-e.png" width="134"/></p>
</div>
<p class="body">Finally, we get a closed-form expression for the unknown <i class="timesitalic">μ</i> in terms of the training data:</p><!--<p class="Body"><span class="times">$$\mu = \frac{1}{n} \sum_{i=1}^{n} x^{ \left( i \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="56" src="../../OEBPS/Images/eq_06-23-f.png" width="103"/></p>
</div>
<p class="body">Similarly, to maximize with respect to <i class="timesitalic">σ</i>, we solve</p><!--<p class="Body"><span class="times">$$\frac{\partial}{\partial \sigma} log
\prod_{i=1}^{n} p\left( x^{ \left( 1 \right) }, x^{ \left( 2 \right) },
\cdots, x^{ \left( n \right) } \middle\vert \mu, \sigma \right) = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_06-23-g.png" width="332"/></p>
</div>
<p class="body">or</p><!--<p class="Body"><span class="times">$$\frac{n}{\sigma} - \frac{2
\sum_{i=1}^{n} \left( {x^{ \left( i \right) } - \mu } \right)^2 }
{2\sigma^3 }  = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="72" src="../../OEBPS/Images/eq_06-23-h.png" width="220"/></p>
</div>
<p class="body">or</p><!--<p class="Body"><span class="times">$$n \sigma^{2}
= \sum_{i=1}^{n} \left( {x^{ \left( i \right) } - \mu }
\right)^2$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="58" src="../../OEBPS/Images/eq_06-23-i.png" width="168"/></p>
</div>
<p class="body">Finally, we get a closed-form expression for the unknown <i class="timesitalic">σ</i> in terms of the training data:</p><!--<p class="Body"><span class="times">$$\sigma^{2} = \frac{1}{n}\sum_{i=1}^{n}
\left( {x^{ \left( i \right) } - \mu } \right)^2$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="63" src="../../OEBPS/Images/eq_06-23-j.png" width="173"/></p>
</div>
<p class="body">Thus we see that for a Gaussian, the maximum-likelihood solutions coincide with the sample mean and variance of the training data. Once we have the mean and standard deviation, we can calculate the probability that a female resident’s height belongs to a specified range <span class="math">[<i class="fm-italics">a</i>, <i class="fm-italics">b</i>]</span> by using the following equation:</p><!--<p class="FM-Equation"><span class="times"><i class="fm-italics">prob</em>(<i class="fm-italics">a</em> &lt; <i class="fm-italics">X</em>&lt;=<i class="fm-italics">b</em>) = ∫<i class="fm-italics"><sub class="FM-Subscript">a</sub><sub class="FM-Subscript">b</sub>p</em>(<i class="fm-italics">X</em>)<i class="fm-italics">dX</em></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="52" src="../../OEBPS/Images/eq_06-24.png" width="249"/></p>
</div>
<p class="fm-equation-caption">Equation 6.24 <span class="calibre" id="eq-prob-gauss"/></p>
<p class="body"><a id="marker-218"/>In the multidimensional case:</p>
<p class="fm-equation"><!--[](</span>)</span>-->Given a training dataset, <span class="math">{<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(2)</sup>,⋯, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>, the best fit Gaussian has the mean</p><!--<p class="Body"><span class="times">$$\vec{\mu} =  \frac{1}{n} \sum_{i=1}^{n} \vec{x}^{
\left( i \right) } \implies \text{mean of the training data samples.}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_06-25.png" width="380"/></p>
</div>
<p class="fm-equation-caption">Equation 6.25 <span class="calibre" id="eq-gauss-MLE-mean"/></p>
<p class="fm-equation">and the covariance matrix</p><!--<p class="Body"><span class="times">$$\boldsymbol{\Sigma}
=  \frac{1}{n} \sum_{i=1}^{n} \left( \vec{x}^{ \left( i \right) } -
\vec{\mu} \right) \left( \vec{x}^{ \left( i \right) } - \vec{\mu}
\right) ^{T} \implies \text{covariance of the training data samples.}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_06-26.png" width="543"/></p>
</div>
<p class="fm-equation-caption">Equation 6.26 <span class="calibre" id="eq-gauss-MLE-covar"/></p>
<p class="body">We began this section by stating the problem of estimating the probability of an adult Statsville resident being female, given that their height lies in a specified range <span class="math">[<i class="fm-italics">a</i>, <i class="fm-italics">b</i>]</span>, when we are provided a training dataset of <i class="timesitalic">n</i> height values of adult Statsville female residents. Let’s now revisit that problem. Using (scalar versions of) equations <a class="url" href="../Text/06.xhtml#eq-gauss-MLE-mean">6.25</a> and <a class="url" href="../Text/06.xhtml#eq-gauss-MLE-covar">6.26</a>, we can estimate <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i> and thereby define a Gaussian probability distribution</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>) = <span class="cambria">𝒩</span>(<i class="fm-italics">x</i>; <i class="fm-italics">μ</i>, <i class="fm-italics">σ</i>)</span></p>
<p class="body">Using this, given any height <i class="timesitalic">x</i>, we can compute the probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span> that the resident is female. Let’s see this using PyTorch.</p>
<h3 class="fm-head1" id="python-pytorch-code-for-maximum-likelihood-estimation">6.8.1 Python PyTorch code for maximum likelihood estimation</h3>
<p class="body">Suppose we assume that the height values of adult female residents of Statsville follow a Gaussian distribution. If we know the parameters of this Gaussian (<i class="timesitalic">μ</i> and <i class="timesitalic">σ</i>), we know the Gaussian distribution fully. That allows us to estimate many interesting things: for instance, the expected height of an adult female resident of Statsville, or the probability that the height of an adult female Statsville resident lies in a certain range such as between <span class="math">160</span> and <span class="math">170</span> cm. The problem is, in a typical real-life situation, we do not know the parameters <i class="timesitalic">μ</i> cm and <i class="timesitalic">σ</i>. All we have is a large dataset <i class="timesitalic">X</i> of height values of adult Statsville female residents—training data. We have to use this data to estimate the unknown parameters <i class="timesitalic">μ</i> cm and <i class="timesitalic">σ</i>. Once we have these, we have an estimated distribution (aka model) from which we can predict the probabilities of events of interest.</p>
<p class="body">As we saw in section <a class="url" href="../Text/06.xhtml#sec-max_likelihood_estimation">6.6.2</a>, MLE is a technique to estimate the parameters from given training data when the family to which the distribution belongs is known but the exact values of the parameters are not known. Listing 6.4 shows the PyTorch implementation of MLE for the Gaussian family.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for model parameter estimation using MLE and MAP, executable via Jupyter Notebook, can be found at <a href="http://mng.bz/9Mv7" style="text-decoration: none; color: #4080C0;">http://mng.bz/9Mv7</a>.</p>
<p class="fm-code-listing-caption" id="listing-6.4-maximum-likelihood-estimate-for-a-gaussian">Listing 6.4 Maximum likelihood estimate for a Gaussian</p>
<pre class="programlisting">sample_mean = X.mean()                          <span class="fm-combinumeral">①</span>

sample_std = X.std()

gaussian_mle = Normal(sample_mean, sample_std)  <span class="fm-combinumeral">②</span>

a, b = torch.Tensor([160]), torch.Tensor([170]) <span class="fm-combinumeral">③</span>

prob = gaussian_mle.cdf(b) - gaussian_mle.cdf(a)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Estimates Gaussian MLE parameters <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> and <span class="timesbold">Σ</span>. They equal the sample mean and sample covariance of the training data. See equations <a class="url" href="../Text/06.xhtml#eq-gauss-MLE-mean">6.25</a> and <a class="url" href="../Text/06.xhtml#eq-gauss-MLE-covar">6.26</a>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines a Gaussian with the estimated parameters</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Once the Gaussian is estimated, we can use it to predict probabilities.</p>
<h3 class="fm-head1" id="python-pytorch-code-for-maximum-likelihood-estimation-using-gradient-descent">6.8.2 Python PyTorch code for maximum likelihood estimation using gradient descent</h3>
<p class="body"><a id="marker-219"/>In listing 6.4, we computed the MLE using the closed-form solution. Now, let’s try to compute the MLE using a different method: gradient descent. In real-life scenarios, we do not use gradient descent to compute the MLE because the closed-form solution is available. However, we discuss this method here to highlight some of the challenges of using gradient descent and how MAP estimation addresses these challenges.</p>
<p class="body">Our goal is to maximize the likelihood function using gradient descent. This can alternatively be viewed as minimizing the negative log-likelihood function. We choose to use the logarithm of the likelihood function since that leads to simpler computation without any loss of generalization. (If you want a quick refresher on gradient descent, see section <a class="url" href="../Text/03.xhtml#sec-gradient-descent">3.5</a>.) Following is the equation for negative log-likelihood:</p><!--<p class="FM-Equation"><span class="times">$$-\log p(X | \theta) =  \frac{n}{2} \log 2 \pi \sigma^2 + \frac{\sum_{i=1}^{n} x^{ \left( i \right) } - \mu) ^ 2}{2\sigma^2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="56" src="../../OEBPS/Images/eq_06-27.png" width="344"/></p>
</div>
<p class="fm-equation-caption">Equation 6.27 <span class="calibre" id="eq-neg-log-likelihood"/></p>
<p class="body">Listings 6.5 and 6.6 show the PyTorch code for the minimization process.</p>
<p class="fm-code-listing-caption" id="code-NLL">Listing 6.5 Gaussian negative log-likelihood for training data</p>
<pre class="programlisting">def neg_log_likelihood(X, mu, sigma):                      <span class="fm-combinumeral">①</span>
    N = X.shape[0]
    X_minus_mu = torch.sub(X, mu)
    t1 = torch.mul(0.5 * N,
               torch.log(2 * np.pi * torch.pow(sigma, 2))) <span class="fm-combinumeral">②</span>

    t2 = torch.div(torch.matmul(X_minus_mu.T, X_minus_mu),
                 2 * torch.pow(sigma, 2))                  <span class="fm-combinumeral">③</span>

    return t1 + t2                                         <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Equation <a class="url" href="../Text/06.xhtml#eq-neg-log-likelihood">6.27</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math"><i class="fm-italics">n</i>/2 <i class="fm-italics">log</i> 2 <i class="fm-italics">πσ</i><sup class="fm-superscript">2</sup></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> <span class="math"><span class="large">(Σ</span><sub class="fm-subscript"><i class="fm-italics1">i</i> = 1</sub><i class="fm-italics"><sup class="fm-superscript">n</sup></i> (<i class="fm-italics">x</i><sub class="fm-subscript">i</sub> – <i class="fm-italics">μ</i>)<sup class="fm-superscript">2</sup><span class="large">)/(</span>2<i class="fm-italics">σ</i><sup class="fm-superscript">2</sup><span class="large">)</span></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Note how all the training data <i class="timesitalic">X</i> is crunched in a single operation. Such vector operations are parallel and very efficient in PyTorch.</p>
<p class="fm-code-listing-caption" id="listing-6.6-minimizing-mle-loss-via-gradient-descent">Listing 6.6 Minimizing MLE loss via gradient descent<a id="marker-220"/></p>
<pre class="programlisting">def minimize(X, mu, sigma, loss_fn, num_iters=100, lr = 0.001): <span class="fm-combinumeral">①</span>

    <span class="fm-combinumeral">②</span>
   for i in range(num_iters):

        loss = loss_fn(X, mu, sigma)                            <span class="fm-combinumeral">③</span>

        loss.backward()                                         <span class="fm-combinumeral">④</span>

        mu.data -= lr * mu.grad
        sigma.data -= lr * sigma.grad                           <span class="fm-combinumeral">⑤</span>

        mu.grad.data.zero_()
        sigma.grad.data.zero_()                                 <span class="fm-combinumeral">⑥</span>

mu = Variable(torch.Tensor([5]).type(dtype), requires_grad=True)
sigma = Variable(torch.Tensor([5]).type(dtype), requires_grad=True)

minimize(X, mu, sigma, neg_log_likelihood)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Negative log-likelihood (listing <a class="url" href="../Text/06.xhtml#code-NLL">6.5</a>)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Iterates to train</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Computes the loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Computes the gradients of the loss with regard to <span class="timesbold">μ</span> and <span class="timesbold">σ</span>. PyTorch stores the gradients in <span class="timesbold">μ</span>.grad and <span class="timesbold">σ</span>.grad.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Scales the gradients by learning the rate and update parameters</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Resets the gradients to zero post-update</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="465" id="fig-gaussian-mle-explode" src="../../OEBPS/Images/CH06_F05a_Chaudhury.png" width="534"/></p>
<p class="figurecaption">(a) MLE explodes: <span class="math"><i class="fm-italics">μ<sub class="fm-subscript">init</sub></i> = 1</span>, <span class="math"><i class="fm-italics">σ<sub class="fm-subscript">init</sub></i> = 1</span>.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="499" id="fig-gaussian-mle-fit" src="../../OEBPS/Images/CH06_F05b_Chaudhury.png" width="534"/></p>
<p class="figurecaption">(b) MLE converges: <span class="math"><i class="fm-italics">μ<sub class="fm-subscript">init</sub></i> = 100</span>, <span class="math"><i class="fm-italics">σ<sub class="fm-subscript">init</sub></i> = 10</span>.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="476" id="fig-gaussian-map-fit" src="../../OEBPS/Images/CH06_F05c_Chaudhury.png" width="534"/></p>
<p class="figurecaption">(c) MAP converges: <span class="math"><i class="fm-italics">μ<sub class="fm-subscript">init</sub></i> = 1</span>, <span class="math"><i class="fm-italics">σ<sub class="fm-subscript">init</sub></i> = 1</span>.</p>
</div>
<p class="fm-table-caption" id="fig-mle-map">Figure 6.5 Gaussian parameter estimation using maximum likelihood estimate and maximum a posteriori estimation. In figure <a class="url" href="../Text/06.xhtml#fig-gaussian-mle-explode">6.5a</a>, the MLE explodes because <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i> are initialized far from <i class="timesitalic">μ<sub class="fm-subscript">expected</sub></i> and <i class="timesitalic">σ<sub class="fm-subscript">expected</sub></i>. However, the MLE converges in figure <a class="url" href="../Text/06.xhtml#fig-gaussian-mle-fit">6.5b</a> because <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i> are initialized closed to <i class="timesitalic">μ<sub class="fm-subscript">expected</sub></i> and <i class="timesitalic">σ<sub class="fm-subscript">expected</sub></i>. Figure <a class="url" href="../Text/06.xhtml#fig-gaussian-map-fit">6.5c</a> shows how, for MAP, <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i> are able to converge to <span class="math"><i class="fm-italics">μ</i>.<i class="fm-italics"><sub class="fm-subscript">expected</sub></i></span> and <i class="timesitalic">σ<sub class="fm-subscript">expected</sub></i> even though they are initialized far away.<a id="marker-221"/></p>
<p class="body">Figure <a class="url" href="../Text/06.xhtml#fig-mle-map">6.5</a> shows how <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i> change with each iteration of gradient descent. We expect <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i> to end up close to <i class="timesitalic">μ<sub class="fm-subscript">expected</sub></i> and <i class="timesitalic">σ<sub class="fm-subscript">expected</sub></i>, respectively. However, when <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i> start off far from <i class="timesitalic">μ<sub class="fm-subscript">expected</sub></i> and <i class="timesitalic">σ<sub class="fm-subscript">expected</sub></i> as in figure <a class="url" href="../Text/06.xhtml#fig-gaussian-mle-explode">6.5a</a>), they do not converge to the expected values and instead become very large numbers. On the other hand, when they are instantiated with values closer to <i class="timesitalic">μ<sub class="fm-subscript">expected</sub></i> and <i class="timesitalic">σ<sub class="fm-subscript">expected</sub></i> as in figure <a class="url" href="../Text/06.xhtml#fig-gaussian-mle-fit">6.5b</a>), they converge to the expected values. MLE is very sensitive to the initial values and has no mechanism to prevent the parameters from exploding. This is why MAP estimation is preferred. The prior <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>)</span> acts as a regularizer and prevents the parameters from becoming too large. Figure <a class="url" href="../Text/06.xhtml#fig-gaussian-map-fit">6.5c</a> shows how <i class="timesitalic">μ</i> and <i class="timesitalic">σ</i> converge to the expected values using MAP even though they started far away.</p>
<p class="body">The MAP loss function is as follows. Note that it is the same equation as the negative log-likelihood, but with two additional terms—<span class="math"><i class="fm-italics">μ</i><sup class="fm-superscript">2</sup></span> and <span class="math"><i class="fm-italics">σ</i><sup class="fm-superscript">2</sup></span>—that act as regularizers:</p><!--<p class="Body"><span class="times">$$-\log p(\theta | X)
=  \frac{N}{2} \log 2 \pi \sigma^2 + \frac{1}{2\sigma^2} \sum_{i=1}^{n} x^{ \left( i \right)} - \mu) ^ 2 + \underbrace{\mu^2 +
\sigma^2}_{Regularizer}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_06-28.png" width="464"/></p>
</div>
<p class="fm-equation-caption">Equation 6.28 <span class="calibre" id="eq-neg-log-likelihood-reg"/></p>
<p class="fm-code-listing-caption" id="listing-6.7-gaussian-negative-log-likelihood-with-regularization">Listing 6.7 Gaussian negative log-likelihood with regularization</p>
<pre class="programlisting">def neg_log_likelihood_reg(X, mu, sigma, k=0.2):               <span class="fm-combinumeral">①</span>
    N = X.shape[0]
    X_minus_mu = torch.sub(X, mu)
    t1 = torch.mul(0.5 * N,
                   torch.log(2 * np.pi * torch.pow(sigma, 2))) <span class="fm-combinumeral">②</span>

    t2 = torch.div(torch.matmul(X_minus_mu.T, X_minus_mu),
                   2 * torch.pow(sigma, 2))                     <span class="fm-combinumeral">③</span>

    loss_likelihood = t1 + t2                                   <span class="fm-combinumeral">④</span>

    loss_reg = k * (torch.pow(mu, 2) + torch.pow(sigma, 2))     <span class="fm-combinumeral">⑤</span>

    return loss_likelihood + loss_reg                           <span class="fm-combinumeral">⑥</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Equation <a class="url" href="../Text/06.xhtml#eq-neg-log-likelihood-reg">6.28</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math"><i class="fm-italics">n</i>/2 <i class="fm-italics">log</i> 2 <i class="fm-italics">πσ</i><sup class="fm-superscript">2</sup></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> <span class="math"><span class="large">(Σ</span><sub class="fm-subscript"><i class="fm-italics1">i</i> = 1</sub><i class="fm-italics"><sup class="fm-superscript">n</sup></i> (<i class="fm-italics">x</i><sub class="fm-subscript">i</sub> – <i class="fm-italics">μ</i>)<sup class="fm-superscript">2</sup><span class="large">)/(</span>2<i class="fm-italics">σ</i><sup class="fm-superscript">2</sup><span class="large">)</span></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Negative log-likelihood</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Regularization</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Note how all the training data <i class="timesitalic">X</i> is crunched in a single operation. Such vector operations are parallel and very efficient in PyTorch.</p>
<h2 class="fm-head" id="sec-GMM">6.9 Gaussian mixture models</h2>
<p class="body"><a id="marker-222"/>In many real-life problems, the simple unimodal (single-peak) probability distributions we learned about in chapter <a class="url" href="../Text/05.xhtml#chap-prob">5</a> fail to model the true underlying distribution of the data. For instance, consider a situation where we are given the heights of many adult Statsville residents. Say there are two classes of adults in Statsville: male and female. The height data we have is <i class="fm-italics">unlabeled</i>, meaning we do not know whether a given instance of height data is associated with a male or a female. Thus the data is one-dimensional, and there are two classes. Figure <a class="url" href="../Text/06.xhtml#fig-gmm_1d_2cls">6.6</a> depicts the situation. None of the simple probability distributions we discussed in chapter <a class="url" href="../Text/05.xhtml#chap-prob">5</a> can be fitted to figure <a class="url" href="../Text/06.xhtml#fig-gmm_1d_2cls">6.6</a>. But the two partial bells in figure <a class="url" href="../Text/06.xhtml#fig-gmm_1d_2cls_PDF">6.6a</a> suggest that we should be able to mix a pair of Gaussians (each of which looks like a bell) to mimic this distribution. This is also consistent with our knowledge that the distribution represents not one but two classes, each of which can be reasonably represented individually by Gaussians. The point cloud also indicates two separate clusters of points. While a single Gaussian will not work, a mixture of two separate <span class="math">1</span>D Gaussians can (and, as we shall shortly see, will) work.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="428" id="fig-gmm_1d_2cls_PDF" src="../../OEBPS/Images/CH06_F06a_Chaudhury.png" width="534"/></p>
<p class="figurecaption">(a) PDF</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="486" id="fig-gmm_1d_2cls-sampledistr" src="../../OEBPS/Images/CH06_F06b_Chaudhury.png" width="525"/></p>
<p class="figurecaption">(b) Sample point distribution</p>
</div>
<p class="fm-table-caption">Figure 6.6 Probability density functions (PDFs) and sample point distributions for 1D height data of adult male and female residents of Statsville <span id="fig-gmm_1d_2cls"/></p>
<p class="body">Let’s now discuss a slightly more complex problem in which the data is two-dimensional and has three classes. Here we are given the weights and heights of three classes of Statsville residents: adult females, adult males, and children. Again, the data is <i class="fm-italics">unlabeled</i>, meaning we do not know whether a given instance of (height, weight) data is associated with a man, woman, or child. This is depicted in figure <a class="url" href="../Text/06.xhtml#fig-gmm_2d_3cls">6.7</a>. Once again, none of the simple probability distributions we studied in chapter <a class="url" href="../Text/05.xhtml#chap-prob">5</a> can be fitted to this situation. But the PDF shows three bell-shaped peaks, the point cloud shows three clusters, and the physical nature of the problem indicates three separate classes, each of which can be reasonably represented by Gaussian. While a single Gaussian will not work, a mixture of three separate <span class="math">2</span>D Gaussians can (and, as we shall shortly see, will) work.<a id="marker-223"/></p>
<p class="fm-quote"><i class="fm-italics">A Gaussian mixture model (GMM) is a weighted combination of a specific number of Gaussian components</i>.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="352" id="fig-gmm_2d_3cls_PDF" src="../../OEBPS/Images/CH06_F07a_Chaudhury.png" width="535"/></p>
<p class="figurecaption">(a) PDF</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="501" id="fig-gmm_2d_3cls-sampledistr" src="../../OEBPS/Images/CH06_F07b_Chaudhury.png" width="545"/></p>
<p class="figurecaption">(b) Sample point distributions</p>
</div>
<p class="fm-table-caption" id="fig-gmm_2d_3cls">Figure 6.7 Probability density functions (PDFs) and sample point distributions for <span class="math">2</span>D (height, weight) data of children, adult males, and adult females of Statsville</p>
<p class="body">For instance, in our first problem with one dimension and two classes, we choose a mixture of two <span class="math">1</span>D Gaussians. For the second problem, we take a mixture of three <span class="math">2</span>D Gaussians. Each individual Gaussian component corresponds to a specific class.</p>
<h3 class="fm-head1" id="probability-density-function-of-the-gmm">6.9.1 Probability density function of the GMM</h3>
<p class="body">Formally,</p>
<p class="fm-equation"><!--[](</span>)</span> -->The PDF for a GMM is</p><!--<p class="Body"><span class="times">$$p\left(\vec{x}\right) =\sum_{k=1}^{K} \pi_{k} \;
\mathcal{N}\left(\vec{x}; \, \vec{\mu_{k}},
\boldsymbol{\Sigma}_{k}\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_06-29.png" width="193"/></p>
</div>
<p class="fm-equation-caption">Equation 6.29 <span class="calibre" id="eq-gmm"/></p>
<p class="fm-equation">where <i class="timesitalic">π<sub class="fm-subscript">k</sub></i> is the weight of the <i class="timesitalic">k</i>th Gaussian component, satisfying</p><!--<p class="Body"><span class="times">$$\sum_{k=1}^{k=K} \pi_{k} = 1\label{eq-gmm_pisum}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_06-29-a.png" width="72"/></p>
</div>
<p class="fm-equation"><i class="timesitalic">K</i> is the number of classes or Gaussian components, and <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>, <b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i>)</span> defined in equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a>) is the PDF for the <i class="timesitalic">k</i>th Gaussian component. Such a GMM models a <i class="timesitalic">K</i>-peaked PDF or, equivalently, a <i class="timesitalic">K</i>-clustered sample point cloud.</p>
<p class="body"><a id="marker-224"/>For instance, the PDF and sample point clouds shown in figure <a class="url" href="../Text/06.xhtml#fig-gmm_1d_2cls">6.6</a> correspond to the following Gaussian mixture:</p><!--<p class="Body"><span class="times">$$p\left( x
\right) =
\overbrace{ \pi_{1}}^{0.7}\mathcal{N}\left( x; \,
\underbrace{\mu_{1}}_{152.0}, \overbrace{\sigma_{1}}^{4.0} \right)
+
\overbrace{ \pi_{2} }^{0.3} \mathcal{N}\left(x; \,
\underbrace{\mu_{2}}_{175.0}, \overbrace{\sigma_{2}}^{7.0}
\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="104" src="../../OEBPS/Images/eq_06-29-b.png" width="466"/></p>
</div>
<p class="body">The <span class="math">2</span>D three-class problem, PDF, and sample point clouds shown in figure <a class="url" href="../Text/06.xhtml#fig-gmm_2d_3cls">6.7</a> correspond to the following Gaussian mixture:</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left( x \right)
&amp;= \overbrace{ \pi_{1}}^{0.33}\mathcal{N}\left(
\begin{matrix}
&amp;{\begin{bmatrix} 152\\ 55
\end{bmatrix}}
&amp;
\begin{bmatrix} 20 &amp;0\\ 0 &amp;28
\end{bmatrix} \\
\vec{x}; &amp;\overbrace{ \vec{\mu}_{1} } &amp;\overbrace{
\boldsymbol{\Sigma}_{1}}
\end{matrix}
\right)\;
+ \overbrace{ \pi_{2} }^{0.33} \mathcal{N}\left(
\begin{matrix}
&amp;\begin{bmatrix} 175\\ 70
\end{bmatrix} &amp; \begin{bmatrix} 35 &amp; 39\\ 39 &amp; 51
\end{bmatrix} \\
\vec{x}; &amp;\overbrace{ \vec{\mu}_{2}  }, &amp;\overbrace{
\boldsymbol{\Sigma}_{2}}
\end{matrix}
\right)\\[14pt]
&amp;+ \overbrace{ \pi_{3} }^{0.33} \mathcal{N}\left(
\begin{matrix}
&amp;\begin{bmatrix} 135\\ 40
\end{bmatrix} &amp;\begin{bmatrix} 10 &amp; 0\\ 0 &amp; 10
\end{bmatrix}\\
\vec{x}; &amp;\overbrace{ \vec{\mu}_{3} }, &amp;\overbrace{
\boldsymbol{\Sigma}_{3}}
\end{matrix}
\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="126" src="../../OEBPS/Images/eq_06-29-c.png" width="859"/></p>
</div>
<p class="body">The PDF and sample point distribution of the GMM depend on the values of <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s, <i class="timesitalic">μ<sub class="fm-subscript">k</sub></i>s, <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span>s, and <i class="timesitalic">K</i>. In particular, <i class="timesitalic">K</i> influences the number of peaks in the PDF (although if two peaks are very close, sometimes they merge). It also influences the number of clusters in the sample point cloud (again, if two clusters are too close, they may not be visually distinct). The <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s regulate the relative heights of the hills. The <i class="timesitalic">μ<sub class="fm-subscript">k</sub></i>s and <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span>s influence the individual hills in the PDF as well as the individual clusters in the sample point cloud. Specifically, <i class="timesitalic">μ<sub class="fm-subscript">k</sub></i> regulates the locations of the <i class="timesitalic">k</i>th peak in the PDF and the centroid of the <i class="timesitalic">k</i>th cluster in the sample point cloud. The <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span>s regulate the shape of the <i class="timesitalic">k</i>th individual hill and the <i class="timesitalic">k</i>th cluster in the sample point cloud. Figures <a class="url" href="../Text/06.xhtml#fig-gmm-1d">6.8</a>, <a class="url" href="../Text/06.xhtml#fig-gmm-2d-pis">6.9</a>, <a class="url" href="../Text/06.xhtml#fig-gmm-2d-mussigmas">6.10</a>, and <a class="url" href="../Text/06.xhtml#fig-gmm-2d-pt-distr">6.11</a> show some example GMMs with various values of these parameters.</p>
<p class="body">Figure <a class="url" href="../Text/06.xhtml#fig-gmm-1d">6.8</a> shows a pair of Gaussian distributions and various GMMs with those as components, with different values for the parameters. Figure <a class="url" href="../Text/06.xhtml#fig-gmm-2d-pis">6.9</a> depicts <span class="math">2</span>D GMMs with various <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s. Figure <a class="url" href="../Text/06.xhtml#fig-gmm-2d-mussigmas">6.10</a> shows GMMs with non-circular bases (non-symmetric <span class="math">Σ</span>s) and various <i class="timesitalic">μ</i>s).<a id="marker-225"/></p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="421" id="fig-male-female-heights-gaussians" src="../../OEBPS/Images/CH06_F08a_Chaudhury.png" width="545"/></p>
<p class="figurecaption">(a) Gaussian components <span class="math"><i class="fm-italics">μ</i><sub class="fm-subscript">1</sub> = 152</span>, <span class="math"><i class="fm-italics">μ</i><sub class="fm-subscript">2</sub> = 175</span>, <span class="math"><i class="fm-italics">σ</i><sub class="fm-subscript">1</sub> = <i class="fm-italics">σ</i><sub class="fm-subscript">2</sub> = 9</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="426" id="fig-gmm-1d-55" src="../../OEBPS/Images/CH06_F08b_Chaudhury.png" width="542"/></p>
<p class="figurecaption">(b) GMM with <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = 0.5</span>, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.5</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="426" id="fig-gmm-1d-73" src="../../OEBPS/Images/CH06_F08c_Chaudhury.png" width="543"/></p>
<p class="figurecaption">(c) GMM with <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = 0.7</span>, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.3</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="424" id="fig-gmm-1d-37" src="../../OEBPS/Images/CH06_F08d_Chaudhury.png" width="542"/></p>
<p class="figurecaption">(d) GMM with <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = 0.3</span>, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.7</span></p>
</div>
<p class="figurecaption" id="fig-gmm-1d">Figure 6.8 Various GMMs (solid curves) with the same Gaussian components (dotted and dashed curves, respectively) but different <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub></span> and <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub></span> values</p>
<p class="body"><a id="marker-224a"/>Another way to visualize GMMs is via sample point distributions. Figure <a class="url" href="../Text/06.xhtml#fig-gmm-2d-pt-distr">6.11</a> shows the sample points from a pair of <span class="math">2</span>D Gaussians and the points sampled from a GMM having those Gaussians as components and various mixture-selections probabilities.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="518" id="fig-gmm-2d-55" src="../../OEBPS/Images/CH06_F09a_Chaudhury.jpg" width="619"/></p>
<p class="figurecaption">(a) <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = 0.5</span>, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.5</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="492" id="fig-gmm-2d-46" src="../../OEBPS/Images/CH06_F09b_Chaudhury.jpg" width="608"/></p>
<p class="figurecaption">(b) <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = 0.4</span>, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.6</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="438" id="fig-gmm-2d-73" src="../../OEBPS/Images/CH06_F09c_Chaudhury.jpg" width="594"/></p>
<p class="figurecaption">(c) <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = 0.7</span>, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.3</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="435" id="fig-gmm-2d-37" src="../../OEBPS/Images/CH06_F09d_Chaudhury.jpg" width="592"/></p>
<p class="figurecaption">(d) <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = 0.3</span>, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.7</span></p>
</div>
<p class="figurecaption" id="fig-gmm-2d-pis">Figure 6.9 Two-dimensional GMMs with circular bases, <!--(<span class="times">$\Sigma_{1} = \Sigma_{2} =  \protect\begin{bmatrix}  5 &amp; 0\protect\\0 &amp; 5\protect \end{bmatrix}$</span>), <span class="times">$\vec{\mu_{1}} = \protect\begin{bmatrix}  -3 \protect \\ -3\protect \end{bmatrix}$</span>, <span class="times">$\vec{\mu_{2}}= \protect\begin{bmatrix}  3 \protect \\3\protect \end{bmatrix}$</span>--><br class="calibre20"/>
<span class="infigure"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_06-29-d.png" width="249"/></span>.<br class="calibre20"/>
  Note how the relative heights of the hills depend on <i class="timesitalic">π</i>s.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="486" id="fig-gmm-2d-33-ellip45135-37" src="../../OEBPS/Images/CH06_F10a_Chaudhury.jpg" width="608"/></p>
<p class="figurecaption">(a) <!--<span class="times">$\vec{\mu_{1}} = \protect\begin{bmatrix}  -3 \protect \\ -3\protect \end{bmatrix}$</span>, <span class="times">$\Sigma_{1} =\protect\begin{bmatrix}  2.75 &amp; 2.25\protect\\2.25 &amp; 2.75\protect \end{bmatrix}$</span>,<br /> <span class="times">$\vec{\mu_{2}} = \protect\begin{bmatrix}  3 \protect \\ 3\protect \end{bmatrix}$</span>, <span class="times">$\Sigma_{2}=\protect\begin{bmatrix}  2.75 &amp; -2.25\protect\\-2.25 &amp; 2.75\protect \end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_06-29-e.png" width="391"/></span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="484" id="fig-gmm-2d-44-ellip45135-37" src="../../OEBPS/Images/CH06_F10b_Chaudhury.jpg" width="608"/></p>
<p class="figurecaption">(b) <!--<span class="times">$\vec{\mu_{1}} = \protect\begin{bmatrix}  -4 \protect \\ -4\protect \end{bmatrix}$</span>, <span class="times">$\Sigma_{1} =\protect\begin{bmatrix}  2.75 &amp; 2.25\protect\\2.25 &amp; 2.75\protect \end{bmatrix}$</span>,<br /> <span class="times">$\vec{\mu_{2}} = \protect\begin{bmatrix}  4 \protect \\ 4\protect \end{bmatrix}$</span>, <span class="times">$\Sigma_{2}=\protect\begin{bmatrix}  2.75 &amp; -2.25\protect\\-2.25 &amp; 2.75\protect \end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="45" src="../../OEBPS/Images/eq_06-29-f.png" width="389"/></span></p>
</div>
<p class="figurecaption" id="fig-gmm-2d-mussigmas">Figure 6.10 Two-dimensional GMMs with elliptical bases, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = 0.3</span>, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.7</span>. Note how the shape of the hill base depends on <span class="timesbold">Σ</span> and how the hill positions depend on the <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>s.<a id="marker-226"/></p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre24" height="372" id="fig-orig_3_gaussians-pt_distr" src="../../OEBPS/Images/CH06_F11a_Chaudhury.png" width="540"/></p>
<p class="figurecaption">(a)</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre25" height="442" id="fig-orig_3_gaussians_superimposed_gmm-pt_distr" src="../../OEBPS/Images/CH06_F11b_Chaudhury.png" width="539"/></p>
<p class="figurecaption">(b)</p>
</div>
<p class="figurecaption" id="fig-gmm-2d-pt-distr">Figure 6.11 (a) <!--<span class="times">1, 000</span> random samples from three Gaussians with <span class="times">$\vec{\mu}_{woman}=\protect\begin{bmatrix}  152\protect\\55  \protect\end{bmatrix}$</span>, <span class="times">$\boldsymbol{\Sigma}_{woman} =\protect\begin{bmatrix}  7 &amp; 0\protect\\0 &amp; 15\protect \end{bmatrix}$</span>, <span class="times">$\vec{\mu}_{man}=\protect\begin{bmatrix}  175\protect\\70  \protect\end{bmatrix}$</span>, <span class="times">$\boldsymbol{\Sigma}_{man} = \protect\begin{bmatrix}  9 &amp; 10\protect\\10 &amp; 25\protect \end{bmatrix}$</span>, <span class="times">$\vec{\mu}_{child}=\protect\begin{bmatrix}  135\protect\\40  \protect\end{bmatrix}$</span>, <span class="times">$\boldsymbol{\Sigma}_{child} =\protect\begin{bmatrix}  5 &amp; 0\protect\\0 &amp; 5\protect \end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_06-29-g.png" width="651"/></span>. (b) <span class="math">1, 000</span> random samples from a GMM with the same three component Gaussians as in (a) and <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub> = <i class="fm-italics">π</i><sub class="fm-subscript">2</sub> = 0.4</span>, <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">3</sub> = 0.2</span>. Note how the GMM sample distribution shape mimics the combined sample distribution shape of the component Gaussians.</p>
<p class="body">It can be proved that equation <a class="url" href="../Text/06.xhtml#eq-gmm">6.29</a> is a proper probability: that is, it sums to <span class="math">1</span> over the space of all possible inputs (all possible values of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> in the <i class="timesitalic">d</i>-dimensional space). Here is the proof outline:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\int\displaylimits_{\vec{x} \in
\Re^{d}}  p\left(\vec{x}\right)  d\vec{x} =
\int\displaylimits_{\vec{x} \in \Re^{d}} \left( \sum_{k=1}^{K} \pi_{k}
\mathcal{N}\left(\vec{x}; \, \vec{\mu_{k}},
\boldsymbol{\Sigma}_{k}\right) \right) d\vec{x} \\[8pt]
&amp;= \sum_{k=1}^{K} \pi_{k} \left( \overbrace{
\int\displaylimits_{\vec{x} \in \Re^{d}}  \mathcal{N}\left( \vec{x}; \,
\vec{\mu_{k}}, \boldsymbol{\Sigma}_{k} \right) d\vec{x} }^{\text{equals
$1$, $\mathcal{N}$ being a PDF}} \right) = \sum_{i=1}^{K} \pi_{k}  = 1\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="118" src="../../OEBPS/Images/eq_06-29-h.png" width="685"/></p>
</div>
<h3 class="fm-head1" id="latent-variables-for-class-selection">6.9.2 Latent variables for class selection</h3>
<p class="body"><a id="marker-227"/>Let’s discuss GMMs in more detail. In particular, we look at the physical meaning of the various terms in equation <a class="url" href="../Text/06.xhtml#eq-gmm">6.29</a>.</p>
<p class="body">Before diving in, let’s introduce an auxiliary random variable <i class="timesitalic">Z</i>, which effectively is a <i class="fm-italics">class selector</i>. In the context of equation <a class="url" href="../Text/06.xhtml#eq-gmm">6.29</a>, <i class="timesitalic">Z</i> can take discrete values in the range <span class="math">[1⋯<i class="fm-italics">K</i>]</span>. It thus follows a categorical distribution (see section <a class="url" href="../Text/05.xhtml#sec-categorical-distr">5.9.6</a>). Physically, <span class="math"><i class="fm-italics">Z</i> = <i class="fm-italics">k</i></span> means the <i class="timesitalic">k</i>th class—that is, the <i class="timesitalic">k</i>th component of the Gaussian mixture—has been selected.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> As usual, we are denoting the random variable with uppercase and the specific value it takes in a given instance with lowercase.</p>
<p class="body">For instance, in the two-class problem shown in figure <a class="url" href="../Text/06.xhtml#fig-gmm_1d_2cls">6.6</a>, <i class="timesitalic">Z</i> can take one of two values: <span class="math">1</span> (implying adult female) or <span class="math">2</span> (implying adult male). For the three-class problem shown in figure <a class="url" href="../Text/06.xhtml#fig-gmm_2d_3cls">6.7</a>, <i class="timesitalic">Z</i> can take one of three values: <span class="math">1</span> (adult female), <span class="math">2</span> (adult male), or <span class="math">3</span> (child). <i class="timesitalic">Z</i> is called a <i class="fm-italics">latent (hidden) random variable</i> because its values are not directly observed. Contrast this with the input random variable <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> whose values are explicitly observed. You may recognize <i class="timesitalic">Z</i> as a latent variable in the GMM (latent variables were introduced in section <a class="url" href="../Text/06.xhtml#sec-evidence_maximization">6.7</a>).<a id="marker-228"/></p>
<p class="body">Consider the joint probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">Z</i> = <i class="fm-italics">k</i>)</span>, which we sometimes informally denote as <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">k</i>)</span>. This is the probability of the input variable <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> occurring together with the class <i class="timesitalic">k</i>. Using Bayes’ theorem,</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">k</i>) = <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">k</i>)<i class="fm-italics">p</i>(<i class="fm-italics">k</i>)</span></p>
<p class="body">The conditional probability term <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">k</i>)</span> is the probability of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> when the <i class="timesitalic">k</i>th class has been selected. This means it is the PDF for the <i class="timesitalic">k</i>th Gaussian component, which is a Gaussian distribution by assumption. As such, using equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a>,</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">k</i>) = <span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>, <b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i>) <i class="fm-italics">k</i> <span class="cambria">∈</span> [1, <i class="fm-italics">K</i>]</span></p>
<p class="body">On the other hand, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Z</i> = <i class="fm-italics">k</i>)</span>, which we sometimes informally refer to as <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">k</i>)</span>, is the <i class="fm-italics">prior probability (that is, without reference to the input) of the input belonging to one of the classes</i>. Let’s denote it as follows:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">k</i>) = <i class="fm-italics">π<sup class="fm-superscript">k</sup></i>, <span class="cambria">∀</span><i class="fm-italics">k</i> <span class="cambria">∈</span> {1, <i class="fm-italics">K</i>}</span></p>
<p class="body">This is often modeled as the <i class="fm-italics">fraction of training data points belonging to class k</i>:</p><!--<p class="FM-Equation"><span class="times">$$\pi_{k} \approx  \frac{N_{k}}{N}
\;\;\;\;\;\;  k \in \left\{1, K\right\}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_06-29-i.png" width="160"/></p>
</div>
<p class="body">where <i class="timesitalic">N<sub class="fm-subscript">k</sub></i> is the number of training data instances belonging to class <i class="timesitalic">k</i>, and <i class="timesitalic">N</i> is the total number of training data instances.</p>
<p class="body">From this, we get</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">k</i>) = <i class="fm-italics">p</i>(<i class="fm-italics">k</i>)<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">k</i>) = <i class="fm-italics">π<sup class="fm-superscript">k</sup></i> <span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>, <b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i>) <i class="fm-italics">k</i> <span class="cambria">∈</span> [1, <i class="fm-italics">K</i>]</span></p>
<p class="body">From equation <a class="url" href="../Text/05.xhtml#eq-marginal-prob">5.5</a>, we get the marginal probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span></p><!--<p class="Body"><span class="times">$$p\left(x\right) = \sum_{k \in \left\{1, K\right\}} p\left(x, k\right) = \sum_{k=1}^{K} \pi_{k} \;
\mathcal{N}\left(\vec{x}; \, \vec{\mu_{k}},
\boldsymbol{\Sigma}_{k}\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="77" src="../../OEBPS/Images/eq_06-29-j.png" width="351"/></p>
</div>
<p class="body">which is the same as equation <a class="url" href="../Text/06.xhtml#eq-gmm">6.29</a>.</p>
<p class="body">This leads to the following physical interpretations:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">A GMM can be viewed as a weighted sum of <i class="timesitalic">K</i> Gaussian components. Equation <a class="url" href="../Text/06.xhtml#eq-gmm">6.29</a> depicts the PDF of the overall GMM.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The <i class="fm-italics">weights</i> <i class="timesitalic">π<sub class="fm-subscript">k</sub></i> <i class="fm-italics">are component selection probabilities</i>. Specifically, <i class="timesitalic">π<sub class="fm-subscript">k</sub></i> can be interpreted as the prior probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Z</i> = <i class="fm-italics">k</i>)</span>, aka <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">k</i>)</span>, of selecting the <i class="timesitalic">k</i>th subclass—modeled as the fraction of the population belonging to the <i class="timesitalic">k</i>th subclass. The <i class="timesitalic">π<sub class="fm-subscript">k</sub></i> are probabilities in a categorical distribution with <i class="timesitalic">K</i> classes. The <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s sum up to <span class="math">1</span>. Sampling from the GMM can be viewed as a two-step process:</p>
<ol class="calibre26">
<li class="fm-list-bullet">
<p class="list">Randomly select a component. The probability of the <i class="timesitalic">k</i>th component being selected is <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>. The sum of all <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s is <span class="math">1</span>, which signifies that one or another component must be selected.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Random sample from the selected Gaussian component. The probability of generating vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>, <b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i>)</span>.</p>
</li>
</ol>
</li>
<li class="fm-list-bullet">
<p class="list">Each of the <i class="timesitalic">K</i> Gaussian components models an individual class. Geometrically speaking, the components correspond to the clusters in the sample point cloud or the peaks in the PDF of the GMM.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The <i class="timesitalic">k</i>th Gaussian component, <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>, <b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i>)</span>, can be interpreted as the conditional probability, <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">k</i>)</span>. This is the likelihood—the probability of data value <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> occurring, <i class="fm-italics">given</i> that the <i class="timesitalic">k</i>th subclass has been selected.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The product <span class="math"><i class="timesitalic">π<sub class="fm-subscript">k</sub></i> <span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>, <b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i>)</span> then represents the joint probability <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">k</i>) = <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">k</i>) <i class="fm-italics">p</i>(<i class="fm-italics">k</i>)</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The sum of all the joint subclass probabilities is the marginal probability <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> of the data value <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>.</p>
</li>
</ul>
<p class="fm-code-listing-caption" id="listing-6.8-gaussian-mixture-model-distribution">Listing 6.8 Gaussian mixture model distribution<a id="marker-229"/></p>
<pre class="programlisting">from torch.distributions.mixture_same_family import MixtureSameFamily <span class="fm-combinumeral">①</span>

pi = Categorical(torch.tensor([0.4, 0.4, 0.2]))                       <span class="fm-combinumeral">②</span>

mu = torch.tensor([[175.0, 70.0], [152.0, 55.0], [135.0, 40.0]])      <span class="fm-combinumeral">③</span>

sigma = torch.tensor([[[30.0, 20.0], [20.0, 30.0]],                   <span class="fm-combinumeral">④</span>
                     [[50.0, 0.0], [0.0, 10.0]],
                     [[20.0, 0.0], [0.0, 20.0]]])

gaussian_components = MultivariateNormal(mu, sigma)                   <span class="fm-combinumeral">⑤</span>

gmm = MixtureSameFamily(pi, gaussian_components)                      <span class="fm-combinumeral">⑥</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Pytorch supports distributions that are mixtures of the same family (here, Gaussian)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Prior probabilities over the three classes (male, female, child): categorical distribution</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Mean height, weight for the three classes (male, female, child)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Covariance matrices for the three classes male, female, child)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates the component Gaussians</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Creates the GMM</p>
<h3 class="fm-head1" id="classification-via-gmm">6.9.3 Classification via GMM</h3>
<p class="body">A typical practical problem involving GMMs goes as follows. A set of unlabeled input data <i class="timesitalic">X</i> training data) is provided. It is important to note that this is unsupervised machine learning—the training data does not come with known output classes. The physical nature of the problem indicates the subclasses in the data (denoted by indices <span class="math">[1⋯<i class="fm-italics">K</i>]</span>). The goal is to classify any arbitrary input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>: that is, map it to one of the <i class="timesitalic">K</i> classes. To do this, we have to fit a GMM (that is, derive the values of <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>, <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>, <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span> for all <span class="math"><i class="fm-italics">k</i> <span class="cambria">∈</span> [1 ⋯ <i class="fm-italics">K</i>]</span>). Given an arbitrary <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, we compute <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">k</i>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> for all the classes (all values of <i class="timesitalic">k</i>). The value of <i class="timesitalic">k</i> yielding the max value for <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">k</i>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> is the class corresponding to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. How do we compute <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">k</i>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>?</p>
<p class="body">Using Bayes’ theorem,</p><!--<p class="Body"><span class="times">$$p\left( k
\middle\vert \vec{x} \right) = \frac{p\left(\vec{x},  k\right) }{ p\left(\vec{x} \right) } = \frac{p\left(\vec{x} \middle\vert k\right) p\left(k\right)}{ \sum_{i = 1}^{K} p\left(\vec{x}, k \right) } =
\frac{\pi_{k} \; \mathcal{N}\left(\vec{x}; \, \vec{\mu_{k}},
\boldsymbol{\Sigma}_{k}\right)}{\sum_{i=1}^{K} \pi_{i} \;
\mathcal{N}\left(\vec{x}; \, \vec{\mu_{i}}, \boldsymbol{\Sigma}_{i}
\right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_06-30.png" width="432"/></p>
</div>
<p class="fm-equation-caption">Equation 6.30 <span class="calibre" id="eq-gmm-posterior-for-classification"/></p>
<p class="body"><a id="marker-230"/>If we know all the GMM parameters, evaluating equation <a class="url" href="../Text/06.xhtml#eq-gmm-posterior-for-classification">6.30</a> is straightforward. We classify the input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> by assigning it to the cluster <i class="timesitalic">k</i> that yields the highest value of <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Z</i> = <i class="fm-italics">k</i>|<i class="fm-italics">X</i> = <i class="fm-italics">x</i>)</span>. Geometrically, this assigns the input to the cluster with the “closest” mean—with distance normalized by the variance of the respective distribution. Basically, we are measuring the distance from the mean, but in clusters of high variance, we are more tolerant of distance from the mean. This makes intuitive sense: if the cluster is widely spread has high variance), a point relatively far from the cluster mean can be said to belong to the cluster. On the other hand, a point the same distance from the mean of a tightly packed cluster may be deemed to be outside the cluster.</p>
<h3 class="fm-head1" id="sec-gmm_fit">6.9.4 Maximum likelihood estimation of GMM parameters (GMM fit)</h3>
<p class="body">A GMM is fully described in terms of its parameter set <span class="math"><i class="fm-italics">θ</i> = {<i class="fm-italics">π<sup class="fm-superscript">k</sup></i>, <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>, <b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i> <span class="cambria">∀</span><i class="fm-italics">k </i><span class="cambria">∈ </span>[1 ⋯ <i class="fm-italics">K</i>]}</span>. But how do we estimate these parameter values? In typical real-life situations, they are not given to us. We only have a set of observed unlabeled training data points <span class="math"><i class="fm-italics">X</i> = {<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>}</span>, such as (weight, height) values for Statsville residents.</p>
<p class="body">Geometrically speaking, each data instance in the training dataset corresponds to a single point in the multidimensional feature space. The training dataset is a point cloud that naturally clusters into Gaussian subclouds (otherwise, we should not be trying GMMs). Our GMM mimicking this dataset should have as many components as there are natural clusters in the data. The parameter values <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>, <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>, <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span> for <span class="math"><i class="fm-italics">k </i> <span class="cambria">∈ </span> [1 ⋯ <i class="fm-italics">K</i>]</span> should be estimated such that the GMM’s sample point cloud overlaps the training data point cloud as much as possible. That is the basic problem we try to solve in this section.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> We do not estimate <i class="timesitalic">K</i>, the number of classes; rather, we use a fixed value of <i class="timesitalic">K</i>, usually estimated from the physical conditions of the problem. For example, in the problem with men, women, and children, it is pretty obvious that <span class="math"><i class="fm-italics">K</i> = 3</span>.</p>
<p class="body">In section <a class="url" href="../Text/06.xhtml#sec-gauss_max_likelihood_estimation">6.8</a>, we did MLE for a simple Gaussian. We computed an expression for the joint log-likelihood of all the training data given a Gaussian probability distribution. Then we took the gradient of that expression with respect to the parameters and equated it to zero. We were able to solve that equation to derive a <i class="fm-italics">closed-form</i> solution for the parameters, <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> and <span class="timesbold">Σ</span> (equations <a class="url" href="../Text/06.xhtml#eq-gauss-MLE-mean">6.25</a> and <a class="url" href="../Text/06.xhtml#eq-gauss-MLE-covar">6.26</a> ). This means we simplified the equation into a form where the unknown (to be solved) appeared alone on the left-hand side and there were only known entities on the right-hand side.</p>
<p class="body">Unfortunately, with GMMs, equating the gradient of the log-likelihood to zero leads to an equation that has no closed-form solution. So, we cannot reduce the equation to a form where the unknowns <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s, <i class="timesitalic">μ<sub class="fm-subscript">k</sub></i>s, and <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span> appear alone on the left-hand sides and only known entities (<span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i></span>s) appear on the right-hand side. Consequently, we have to go for an iterative approximation. We rewrite the equation we get by equating the gradient of the log-likelihood to zero such that the unknowns <i class="timesitalic">μ</i>s and <i class="timesitalic">σ</i>s appear alone on the right-hand side. It looks something like<a id="marker-231"/></p>
<p class="fm-equation"><span class="math"><i class="fm-italics">π<sup class="fm-superscript">k</sup></i> = <i class="fm-italics">f</i><sub class="fm-subscript">1</sub>(<i class="fm-italics">X, Θ</i>)</span><br class="calibre20"/>
<span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i> = <i class="fm-italics">f</i><sub class="fm-subscript">2</sub>(<i class="fm-italics">X, Θ</i>)</span><br class="calibre20"/>
<span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i> = <i class="fm-italics">f</i><sub class="fm-subscript">3</sub>(<i class="fm-italics">X, Θ</i>)</span></p>
<p class="body">where <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">3</sub></span> are some functions whose exact nature is unimportant at the moment. Note that the right-hand side also contains the unknowns: <i class="timesitalic">θ</i> contains <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s, <i class="timesitalic">μ<sub class="fm-subscript">k</sub></i>s, and <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span>. We cannot directly solve such equations, but we can use <i class="fm-italics">iterative relaxation</i>, which works roughly as follows:</p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list">Start with random values of <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s, <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>s, and <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span>s.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Evaluate the right-hand side by plugging current values of <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s, <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>s, and <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span>s into functions <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">2</sub></span>, and <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">3</sub></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Use the values estimated in step 2 to set new values of <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s, <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>s, and <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span>s.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Repeat steps 1–3 until the parameter values stop changing appreciably.</p>
</li>
</ol>
<p class="body">The actual functions <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">3</sub></span> are worked out in equations <a class="url" href="../Text/06.xhtml#eq-gmmfit-pi">6.36</a>, <a class="url" href="../Text/06.xhtml#eq-gmmfit-mu">6.37</a>, and <a class="url" href="../Text/06.xhtml#eq-gmmfit-sigma">6.38</a>). As iteration progresses, the values of <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s, <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>s, and <span class="math"><b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i></span>s start to converge to their true values. This is not a lucky coincidence. If we follow algorithm <a class="url" href="../Text/06.xhtml#alg-gmm_fit">6.3</a>, it can be proved that every iteration improves the approximation, even if by a minuscule amount. Eventually, we reach a point when the approximation is no longer improving appreciably. This is called the <i class="fm-italics">fixed point</i>, and we should stop iterating and declare the current values final.</p>
<p class="body">Figure <a class="url" href="../Text/06.xhtml#fig-GMM-fit-iters">6.12</a> shows the progression of an iterative GMM fit algorithm. Figure <a class="url" href="../Text/06.xhtml#fig-GMM-fit-gaussians">6.12a</a> shows the sampled training data distribution. Figure <a class="url" href="../Text/06.xhtml#fig-GMM-fit-step0">6.12b</a> shows the fitted GMM at the beginning: the parameters are essentially random, and the GMM looks nothing like the target training data distribution. It improves slowly until at iteration <span class="math">15</span>, it matches the target distribution snugly figure <a class="url" href="../Text/06.xhtml#fig-GMM-fit-step15">6.12d</a>).</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="494" id="fig-GMM-fit-gaussians" src="../../OEBPS/Images/CH06_F12a_Chaudhury.png" width="537"/></p>
<p class="figurecaption">(a) Training data point cloud (target for fitting)</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="438" id="fig-GMM-fit-step0" src="../../OEBPS/Images/CH06_F12b_Chaudhury.png" width="539"/></p>
<p class="figurecaption">(b) Fitted GMM’s sample point cloud at step 0</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="437" id="fig-GMM-fit-step5" src="../../OEBPS/Images/CH06_F12c_Chaudhury.png" width="543"/></p>
<p class="figurecaption">(c) Fitted GMM’s sample point cloud at step 5</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="442" id="fig-GMM-fit-step15" src="../../OEBPS/Images/CH06_F12d_Chaudhury.png" width="543"/></p>
<p class="figurecaption">(d) Fitted GMM’s sample point cloud at step 15. It almost matches the target.</p>
</div>
<p class="figurecaption" id="fig-GMM-fit-iters">Figure 6.12 Progression of maximum likelihood estimation for GMM parameters<a id="marker-232"/></p>
<p class="body">Now let’s discuss the details. We <i class="fm-italics">already know</i> the dataset <i class="timesitalic">X</i> that has been observed. What parameter set <i class="timesitalic">θ</i> will maximize the conditional probability, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">θ</i>)</span>, of exactly these data points, given the parameter set? In other words, what model parameters will maximize the overall likelihood of the training data? Those will be our best guesses for the unknown model parameters. This is MLE, which we encountered in section <a class="url" href="../Text/06.xhtml#sec-max_likelihood_estimation">6.6.2</a>.</p>
<p class="body">Let <span class="math">{<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>,⋯<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span> be the set of observed data points, aka training data. From equation <a class="url" href="../Text/06.xhtml#eq-gmm">6.29</a>,</p><!--<p class="Body"><span class="times">$$\overbrace{p\left(\vec{x}^{ \left( i \right) }
\middle\vert \theta \right)}^{likelihood} = \sum_{k=1}^{K} \pi_{k} \;
\mathcal{N}\left(\vec{x}^{ \left( i \right) }; \, \vec{\mu_{k}},
\boldsymbol{\Sigma}_{k}\right) \;\;\;\;\; \forall i \in \left[1, n
\right]$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="84" src="../../OEBPS/Images/eq_06-30-a.png" width="359"/></p>
</div>
<p class="body">Henceforth, for simplicity, we drop the “given <i class="timesitalic">θ</i>” part and refer to <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>|<i class="fm-italics">θ</i>)</span> simply as <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)</span>. As usual, instead of maximizing the likelihood directly, we maximize its logarithm, the <i class="fm-italics">log-likelihood</i>. This will yield the same parameters as maximizing the likelihood directly.</p>
<p class="body">Since the <span class="math"><i class="fm-italics">x</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>s are independent, their joint probability, as per equation <a class="url" href="../Text/05.xhtml#eq-joint-prob-indep">5.4</a>, is</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>)<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(2)</sup>)⋯<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>)</span></p>
<p class="body">The corresponding log joint probability is</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\overbrace{log \left( p\left(\vec{x}^{ \left( 1 \right) }\right) p\left(\vec{x}^{ \left( 2 \right) }\right) \cdots p\left(\vec{x}^{
\left( n \right) }\right)  \right)}^{\text{joint log-likelihood}}
&amp;= \sum_{i=1}^{n} log \left( p\left(\vec{x}^{ \left( i \right)
}\right) \right) \nonumber\\
&amp; = \sum_{i=1}^{n} log \left( \sum_{k=1}^{K} \pi_{k} \;
\mathcal{N}\left(\vec{x}^{ \left( i \right) }; \, \vec{\mu_{k}},
\boldsymbol{\Sigma}_{k}\right)  \right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="158" src="../../OEBPS/Images/eq_06-31.png" width="526"/></p>
</div>
<p class="fm-equation-caption">Equation 6.31 <span class="calibre" id="eq-gmm-loglikelihood"/></p>
<p class="body"><a id="marker-233"/>At this point, we begin to see a difficulty peculiar to GMMs. We have a logarithm of a sum, which is not a very friendly expression to handle; the logarithm of products is much nicer to deal with. But let’s soldier on.</p>
<p class="body">To identify the parameters <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sup class="fm-superscript">1</sup></span>, <span class="math">Σ<sub class="fm-subscript">1</sub></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sup class="fm-superscript">2</sup></span>, <span class="math">Σ<sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span> that will maximize the log joint probability, we take the gradient of the log joint probability with respect to these parameters, equate them to zero, and solve for the parameter value (as discussed in section <a class="url" href="../Text/03.xhtml#sec-gradient">3.3.1</a>). Here we demonstrate the process with respect to <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">1</sub></span>:</p>
<p class="fm-equation"><span class="math">∇<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">1</sub></sub><i class="fm-italics">log</i>(<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>)<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(2)</sup>)⋯<i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>)) = 0</span></p>
<p class="body">Since the log of products is the sum of logs, we get</p><!--<p class="Body"><span class="times">$$\nabla_{\vec{\mu}_{1}} \sum_{i=1}^{n} log  \left( p\left(\vec{x}^{ \left( i \right) }\right)  \right)   = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_06-31-a.png" width="192"/></p>
</div>
<p class="body">Applying equation <a class="url" href="../Text/06.xhtml#eq-gmm">6.29</a>, we get</p><!--<p class="Body"><span class="times">$$\nabla_{\vec{\mu}_{1}}   \sum_{i=1}^{n} log  \left( \sum_{k=1}^{K}  \pi_{k} \; \mathcal{N}\left(\vec{x}^{ \left( i \right) }; \, \vec{\mu_{k}},
\boldsymbol{\Sigma}_{k}\right)   \right)   = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_06-31-b.png" width="307"/></p>
</div>
<p class="body">Since the gradient is a linear operator, we can move it inside the summation:</p><!--<p class="Body"><span class="times">$$\sum_{i=1}^{n}
\nabla_{\vec{\mu}_{1}}  log  \left( \sum_{k=1}^{K}  \pi_{k} \;
\mathcal{N}\left(\vec{x}^{ \left( i \right) }; \, \vec{\mu_{k}},
\boldsymbol{\Sigma}_{k}\right)   \right)   = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_06-31-c.png" width="304"/></p>
</div>
<p class="body">Since <span class="math"><i class="fm-italics">d</i>/<i class="fm-italics">dx</i> log (<i class="fm-italics">f</i>(<i class="fm-italics">x</i>)) = 1/<i class="fm-italics">f</i>(<i class="fm-italics">x</i>) <i class="fm-italics">df</i>/<i class="fm-italics">dx</i></span>, we get</p><!--<p class="Body"><span class="times">$$\sum_{i=1}^{n} \frac{
\nabla_{\vec{\mu}_{1}}  \sum_{k=1}^{K}
\pi_{k}  \mathcal{N}\left(\vec{x}^{ \left( i \right) }; \,
\vec{\mu_{k}}, \boldsymbol{\Sigma}_{k}\right) } { \left( \sum_{k=1}^{K}
\pi_{k} \; \mathcal{N}\left(\vec{x}^{ \left( i \right) }; \,
\vec{\mu_{k}}, \boldsymbol{\Sigma}_{k}\right)  \right)} = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="82" src="../../OEBPS/Images/eq_06-31-d.png" width="274"/></p>
</div>
<p class="body">Now, if <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">1</sub></span> and <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">2</sub></span> are independent variables, <span class="math"><i class="fm-italics">dx</i><sub class="fm-subscript">2</sub>/<i class="fm-italics">dx</i><sub class="fm-subscript">1</sub> = 0</span>. Consequently,</p><!--<p class="Body"><span class="times">$$\nabla_{\vec{\mu}_{1}}  \mathcal{N}\left(\vec{x}^{
\left( i \right) }; \, \vec{\mu_{k}}, \boldsymbol{\Sigma}_{k}\right) = 0\;\;\;\;\text{for }k \neq 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="45" src="../../OEBPS/Images/eq_06-31-e.png" width="266"/></p>
</div>
<p class="body">Only a single term corresponding to <span class="math"><i class="fm-italics">k</i> = 1</span> survives the differentiation (gradient) in the numerator. So,</p><!--<p class="Body"><span class="times">$$\sum_{i=1}^{n} \frac{\pi_{1}
\nabla_{\vec{\mu}_{1}}  \mathcal{N}\left(\vec{x}^{ \left( i \right) };
\, \vec{\mu_{1}}, \boldsymbol{\Sigma}_{1}\right) } { \left(
\sum_{k=1}^{K} \pi_{k} \; \mathcal{N}\left(\vec{x}^{ \left( i \right) };
\, \vec{\mu_{k}}, \boldsymbol{\Sigma}_{k}\right)  \right)} = 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="76" src="../../OEBPS/Images/eq_06-31-f.png" width="262"/></p>
</div>
<p class="body">Now <span class="math"><i class="fm-italics">d</i>/<i class="fm-italics">dx</i> <i class="fm-italics">e</i><sup class="fm-superscript">–(<i class="fm-italics1">x – μ</i>)<sup class="fm-superscript">2</sup></sup> = –2(<i class="fm-italics">x – μ</i>) <i class="fm-italics">e</i><sup class="fm-superscript">–(<i class="fm-italics1">x – μ</i>)<sup class="fm-superscript">2</sup></sup></span>, and in multiple dimensions,</p><!--<p class="Body"><span class="times">$$\nabla_{\vec{\mu}} e^{-\frac{1}{2} \left(\vec{x}
- \vec{ \mu} \right)^{T} \Sigma^{-1} \left(\vec{x} - \vec{\mu} \right)}
= -\nabla_{\vec{\mu}}  \left( \left(\vec{x} - \vec{\mu} \right)^{T}
\Sigma^{-1} \left(\vec{x} - \vec{\mu} \right)  \right) e^{-\frac{1}{2}
\left(\vec{x} - \mu \right)^{T} \Sigma^{-1} \left(\vec{x} - \mu
\right)}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_06-31-g.png" width="525"/></p>
</div>
<p class="body"><a id="marker-234"/>Plugging equation <a class="url" href="../Text/05.xhtml#eq-multivar-normal">5.23</a> into our maximization problem, we get</p><!--<p class="Body"><span class="times">$$\sum_{i=1}^{n}
\frac{\nabla_{\vec{\mu_{1}}} \left( \left( \vec{x}^{ \left( i \right) }
- \vec{\mu}_{1} \right)^{T}  \boldsymbol{\Sigma}_{1}^{-1}\left(
\vec{x}^{ \left( i \right) } - \vec{\mu}_{1} \right) \right)  \pi_{1} \;
\mathcal{N}\left(\vec{x}^{ \left( i \right) }; \, \vec{\mu_{1}},
\boldsymbol{\Sigma}_{1}\right)}{ \left( \sum_{k=1}^{K} \pi_{k} \;
\mathcal{N}\left(\vec{x}^{ \left( i \right) }; \, \vec{\mu_{k}},
\boldsymbol{\Sigma}_{k}\right)  \right)}
= 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="93" src="../../OEBPS/Images/eq_06-31-h.png" width="478"/></p>
</div>
<p class="body">Furthermore, with a little effort, you can prove the following about the gradient of a quadratic form:</p>
<p class="fm-equation"><span class="math">∇<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></sub>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = <i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span></p>
<p class="fm-equation-caption">Equation 6.32 <span class="calibre" id="eq-grad-quad_form"/></p>
<p class="body">Applying equation <a class="url" href="../Text/06.xhtml#eq-grad-quad_form">6.32</a> to our problem, we get</p><!--<p class="Body"><span class="times">$$\sum_{i=1}^{n}
\frac{\boldsymbol{\Sigma}_{1}^{-1}\left( \vec{x}^{ \left( i \right) } -
\vec{\mu}_{1} \right)  \; \pi_{1} \; \mathcal{N}\left(\vec{x}^{ \left( i
\right) }; \, \vec{\mu_{1}}, \boldsymbol{\Sigma}_{1}\right)}{ \left(
\sum_{k=1}^{K} \pi_{k} \; \mathcal{N}\left(\vec{x}^{ \left( i \right) };
\, \vec{\mu_{k}}, \boldsymbol{\Sigma}_{k}\right)  \right)}
= 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="81" src="../../OEBPS/Images/eq_06-32-a.png" width="335"/></p>
</div>
<p class="body">Multiplying both sides by the constant <span class="math"><b class="fm-bold">Σ</b><sub class="fm-subscript">1</sub></span>, we get</p><!--<p class="Body"><span class="times">$$\sum_{i=1}^{n} \frac{\left( \vec{x}^{ \left( i
\right) } - \vec{\mu}_{1} \right)  \; \pi_{1} \;
\mathcal{N}\left(\vec{x}^{ \left( i \right) }; \, \vec{\mu_{1}},
\boldsymbol{\Sigma}_{1}\right)}{ \left( \sum_{k=1}^{K} \pi_{k} \;
\mathcal{N}\left(\vec{x}^{ \left( i \right) }; \, \vec{\mu_{k}},
\boldsymbol{\Sigma}_{k}\right)  \right)}
= 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="79" src="../../OEBPS/Images/eq_06-32-b.png" width="302"/></p>
</div>
<p class="body">Substituting</p><!--<p class="Body"><span class="times">$$\gamma_{i1} =
\frac{\pi_{1} \; \mathcal{N}\left(\vec{x}^{ \left( i \right) }; \,
\vec{\mu_{1}}, \boldsymbol{\Sigma}_{1}\right)}{ \left( \sum_{k=1}^{K}
\pi_{k} \; \mathcal{N}\left(\vec{x}^{ \left( i \right) }; \,
\vec{\mu_{k}}, \boldsymbol{\Sigma}_{k}\right)  \right)}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="84" src="../../OEBPS/Images/eq_06-33.png" width="249"/></p>
</div>
<p class="fm-equation-caption">Equation 6.33 <span class="calibre" id="eq-gmmfit-gamma"/></p>
<p class="body">we get</p><!--<p class="Body"><span class="times">$$\sum_{i=1}^{n} \left( \vec{x}^{
\left( i \right) } - \vec{\mu}_{1} \right)  \; \gamma_{i1}
= 0$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_06-33-a.png" width="175"/></p>
</div>
<p class="body">This expression has <span class="math"><i class="fm-italics">μ</i><sub class="fm-subscript">1</sub></span> inside <span class="math"><i class="fm-italics">γ</i><sub class="fm-subscript"><i class="fm-italics1">i</i>1</sub></span> as well. It is impossible to extract <span class="math"><i class="fm-italics">μ</i><sub class="fm-subscript">1</sub></span> alone on the left side of the equation. In other words, we cannot create a <i class="fm-italics">closed-form</i> solution for <span class="math"><i class="fm-italics">μ</i><sub class="fm-subscript">1</sub></span>. Hence, we have to solve it iteratively.</p>
<p class="body">We can rewrite the previous equation as</p><!--<p class="Body"><span class="times">$$\vec{\mu}_{1} = \frac{1}{N_{1}}
\sum_{i=1}^{n}  \gamma_{i1} \vec{x}^{ \left( i \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_06-33-b.png" width="150"/></p>
</div>
<p class="body">where</p><!--<p class="Body"><span class="times">$$N_{1} =
\sum_{i=1}^{n}  \gamma_{i1}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_06-34.png" width="91"/></p>
</div>
<p class="fm-equation-caption">Equation 6.34</p>
<p class="body">Proceeding similarly, we can derive the corresponding expressions for <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub></span> and <span class="math"><b class="fm-bold">Σ</b><sub class="fm-subscript">1</sub></span>. Let’s collect all the equations for updating the GMM parameters:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;N_{1} = \sum_{i=1}^{n}  \gamma_{i1}\\
&amp;\pi_{1} = \frac{N_{1}}{n}\\
&amp;\vec{\mu}_{1}  = \frac{1}{N_{1}} \sum_{i=1}^{n}  \gamma_{i1}
\vec{x}^{ \left( i \right) }\\
&amp;\boldsymbol{\Sigma}_{1} =  \frac{1}{N_{1}}
\sum_{i=1}^{n}  \gamma_{i1} \left( \vec{x}^{ \left( i \right) } -
\vec{\mu}_{1} \right)  \left( \vec{x}^{ \left( i \right) } -
\vec{\mu}_{1} \right)^{T}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="58" src="../../OEBPS/Images/eq_06-35.png" width="93"/></p>
</div>
<p class="fm-equation-caption">Equation 6.35 <span class="calibre" id="eq-gmmfit-Nk"/></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="50" src="../../OEBPS/Images/eq_06-36.png" width="67"/></p>
</div>
<p class="fm-equation-caption">Equation 6.36 <span class="calibre" id="eq-gmmfit-pi"/></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_06-37.png" width="150"/></p>
</div>
<p class="fm-equation-caption">Equation 6.37 <span class="calibre" id="eq-gmmfit-mu"/></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_06-38.png" width="303"/></p>
</div>
<p class="fm-equation-caption">Equation 6.38 <span class="calibre" id="eq-gmmfit-sigma"/></p>
<p class="body">Equations <a class="url" href="../Text/06.xhtml#eq-gmmfit-pi">6.36</a>, <a class="url" href="../Text/06.xhtml#eq-gmmfit-mu">6.37</a>, and <a class="url" href="../Text/06.xhtml#eq-gmmfit-sigma">6.38</a> provide the definitions for functions <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">2</sub></span>, and <span class="math"><i class="fm-italics">f</i><sub class="fm-subscript">3</sub></span> that we saw at the beginning of this section in the context of iterative relaxation. We can deal similarly with <span class="math"><i class="fm-italics">k</i> = 2⋯<i class="fm-italics">K</i></span>.<a id="marker-235"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Physical significance of <i class="timesitalic1">γ<sub class="fm-subscript">ik</sub></i></p>
<p class="fm-sidebar-text">We encountered the entity <i class="timesitalic">γ<sub class="fm-subscript">ik</sub></i> while computing the gradient of the log-likelihood. It appeared as a multiplicative weight in the final iterative expression for computing <i class="timesitalic">μ<sub class="fm-subscript">k</sub></i> and <i class="timesitalic">Σ<sub class="fm-subscript">k</sub></i> in equations <a class="url" href="../Text/06.xhtml#eq-gmmfit-mu">6.37</a> and <a class="url" href="../Text/06.xhtml#eq-gmmfit-sigma">6.38</a>. It is not an arbitrary entity. By comparing equations <a class="url" href="../Text/06.xhtml#eq-gmmfit-gamma">6.33</a> and <a class="url" href="../Text/06.xhtml#eq-gmm-posterior-for-classification">6.30</a>, we can see that</p>
<p class="fm-sidebar-text">  </p>
<p class="fm-sidebar-text">      <span class="math"><i class="fm-italics">γ<sub class="fm-subscript">ik</sub></i> = <i class="fm-italics">p</i>(<i class="fm-italics">k</i>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)</span></p>
<p class="fm-sidebar-text">  </p>
<p class="fm-sidebar-text">In other words, the quantity <i class="timesitalic">γ<sub class="fm-subscript">ik</sub></i> is really the posterior probability: the conditional probability of the class <i class="timesitalic">k</i> given the <i class="timesitalic">i</i>th data point.</p>
</div>
<p class="body">This gives us a new way to look at equations <a class="url" href="../Text/06.xhtml#eq-gmmfit-Nk">6.35</a>, <a class="url" href="../Text/06.xhtml#eq-gmmfit-pi">6.36</a>, <a class="url" href="../Text/06.xhtml#eq-gmmfit-mu">6.37</a>, and <a class="url" href="../Text/06.xhtml#eq-gmmfit-sigma">6.38</a>:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-Nk">6.35</a> essentially assigns to <span class="math"><i class="fm-italics">N</i><sub class="fm-subscript">1</sub></span> the probability mass concentrated in class <span class="math">1</span> as per the current parameter values.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-pi">6.36</a> assigns to <span class="math"><i class="fm-italics">π</i><sub class="fm-subscript">1</sub></span> the fractional mass in class <span class="math">1</span> as per the current parameter values.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-mu">6.37</a> assigns to <span class="math"><i class="fm-italics">μ</i><sub class="fm-subscript">1</sub></span> the centroid of all the training data points. Each data point’s contribution is weighted by the posterior probability, as per the current parameter values, of that data point belonging to class <span class="math">1</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-sigma">6.38</a> assigns to <span class="math"><b class="fm-bold">Σ</b><sub class="fm-subscript">1</sub></span> the covariance of the training data points. Each data point’s contribution is weighted by the posterior probability, as per the current parameter values, of that data point belonging to class <span class="math">1</span>.</p>
</li>
</ul>
<p class="body">Algorithm <a class="url" href="../Text/06.xhtml#alg-gmm_fit">6.3</a> ties together equations <a class="url" href="../Text/06.xhtml#eq-gmmfit-gamma">6.33</a>, <a class="url" href="../Text/06.xhtml#eq-gmmfit-pi">6.36</a>, <a class="url" href="../Text/06.xhtml#eq-gmmfit-mu">6.37</a>, and <a class="url" href="../Text/06.xhtml#eq-gmmfit-sigma">6.38</a> into a complete approach for iterative MLE of GMM parameters. It is an example of a general class of algorithms called <i class="fm-italics">expectation maximization</i>.<a id="marker-236"/></p>
<div class="calibre3">
<p class="fm-algorithm-caption" id="alg-gmm_fit">Algorithm 6.3 GMM fit (MLE of GMM parameters from unlabeled training data)</p>
<p class="algorithm-body">Input: <span class="math"><i class="fm-italics">X</i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(2)</sup>, … , <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup></span></p>
<p class="algorithm-body">Initialize parameters <span class="math"><i class="fm-italics">Θ</i> = {<i class="fm-italics">π<sup class="fm-superscript">k</sup></i> , <i class="fm-italics">μ<sub class="fm-subscript">k</sub></i> , <b class="fm-bold">Σ</b><i class="fm-italics"><sub class="fm-subscript">k</sub></i> <i class="fm-italics">k</i> <span class="cambria">∈</span> [1, <i class="fm-italics">K</i>]}</span> with random values</p>
<p class="algorithm-body"><span class="segoe">⊳</span> repeat E-step and M-step until likelihood stops increasing</p>
<p class="algorithm-body"><b class="fm-bold">while</b> (likelihood is increasing ) <b class="fm-bold">do</b></p>
<p class="algorithm-body">    <span class="segoe">⊳</span>E-step</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="72" src="../../OEBPS/Images/eq_06-38-a.png" width="407"/></p>
</div>
<p class="algorithm-body">    <span class="segoe">⊳</span>M-step</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="148" src="../../OEBPS/Images/eq_06-38-b.png" width="407"/></p>
</div>
<p class="algorithm-body"><b class="fm-bold">end</b> <b class="fm-bold">while</b></p>
<p class="algorithm-body">return <span class="math">{<i class="fm-italics">x</i><sub class="fm-subscript">1</sub> , <i class="fm-italics">μ</i><sub class="fm-subscript">1</sub>, <b class="fm-bold">Σ<sub class="fm-subscript">1</sub></b>, <i class="fm-italics">x</i><sub class="fm-subscript">2</sub>, <i class="fm-italics">μ</i><sub class="fm-subscript">2</sub>, <b class="fm-bold">Σ<sub class="fm-subscript">2</sub></b> , … , <i class="fm-italics">x<sub class="fm-subscript">K</sub></i>, <i class="fm-italics">μ<sub class="fm-subscript">K</sub></i>, <b class="fm-bold">Σ<i class="fm-italics"><sub class="fm-subscript">K</sub></i></b>}</span></p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for Gaussian mixture modeling, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/j4er">http://mng.bz/j4er</a>.</p>
<p class="fm-code-listing-caption" id="listing-6.9-gmm-fit">Listing 6.9 GMM fit</p>
<pre class="programlisting">while (curr_likelihood - prev_likelihood) &lt; 1e-4:     <span class="fm-combinumeral">①</span>

  # E Step                                            <span class="fm-combinumeral">②</span>

  pi = gmm.mixture_distribution.probs                 <span class="fm-combinumeral">③</span>

  components = gmm.component_distribution             <span class="fm-combinumeral">④</span>
  
   <span class="fm-combinumeral">⑤</span>
   log_gamma_numerators = components.log_prob(
       X.unsqueeze(1)) + torch.log(pi).repeat(n, 1)   <span class="fm-combinumeral">⑥</span>
 
   <span class="fm-combinumeral">⑦</span>
   log_gamma_denominators = torch.logsumexp(
       log_gamma_numerators, dim=1, keepdim=True).
 
   <span class="fm-combinumeral">⑧</span>
   log_gamma = log_gamma_numerators - log_gamma_denominators
   self.gamma = torch.exp(log_gamma)

   # M Step                                           <span class="fm-combinumeral">⑨</span>

   n = X.shape[0]                                     <span class="fm-combinumeral">⑩</span>

   N = torch.sum(gamma, 0)

   pi = N / n                                         <span class="fm-combinumeral">⑪</span>

   mu = ((X.T @ gamma)/N).T                           <span class="fm-combinumeral">⑫</span>

   x_minus_mu = (X.repeat(K, 1, 1) - gmm.component_distribution.unsqueeze(1).
                 repeat(1, n, 1))
 
   <span class="fm-combinumeral">⑬</span>
   x_minus_mu_squared = x_minus_mu.unsqueeze(3)  @ x_minus_mu.unsqueeze(2)

   <span class="fm-combinumeral">⑭</span>
   sigma = torch.sum(gamma.T.unsqueeze(2).unsqueeze(3) * x_minus_mu_squared,
                     axis=1) / N.unsqueeze(1).unsqueeze(1).repeat(1, d, d)

   prev_likelihood = curr_likelihood

   curr_likelihood = torch.sum(gmm.log_prob(X))       <span class="fm-combinumeral">⑮</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Repeats until the likelihood increase is negligible</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Computes the posterior probabilities <span class="math"><i class="fm-italics">γ<sub class="fm-subscript">i,k</sub></i> = <i class="fm-italics">p</i>(<i class="fm-italics">Z</i> = <i class="fm-italics">k</i>|<i class="fm-italics">X</i> = <i class="fm-italics">x<sub class="fm-subscript">i</sub></i>)</span> using current <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>s and <span class="math">Σ<i class="fm-italics"><sub class="fm-subscript">k</sub></i></span>s, equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-gamma">6.33</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Tensor of shape [K] holding <i class="timesitalic">π<sub class="fm-subscript">k</sub></i>s for all <span class="math"><sub class="fm-subscript">k</sub></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Gaussian objects <span class="math"><span class="cambria">𝒩</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>, Σ<i class="fm-italics"><sub class="fm-subscript">k</sub></i>)</span> for all <i class="timesitalic">k</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Vector computation of log of <i class="timesitalic">γ<sub class="fm-subscript">i, k</sub></i> numerators for all <span class="math">i, k</span>, equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-gamma">6.33</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> In practice, the probability involving an exponential goes to 0. So we use the log probability.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Vector computation of the log of <span class="math"><i class="fm-italics">γ</i><sub class="fm-subscript"><i class="fm-italics1">i,</i> <i class="fm-italics1">k</i></sub></span> denominators for all i, k, equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-gamma">6.33</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Vector computation of the [<span class="math"><i class="fm-italics">n</i> <i class="fm-italics">×</i> <i class="fm-italics">K</i></span>] tensor <span class="math"><i class="fm-italics">γ</i><sub class="fm-subscript"><i class="fm-italics1">i,</i> <i class="fm-italics1">k</i></sub></span>, equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-gamma">6.33</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Updates <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i> and <span class="math">Σ<sub class="fm-subscript">k</sub></span> for all <i class="timesitalic">k</i> using <span class="math"><i class="fm-italics">γ</i><sub class="fm-subscript"><i class="fm-italics1">i,</i> <i class="fm-italics1">k</i></sub> = <i class="fm-italics">p</i>(<i class="fm-italics">Z</i> = <i class="fm-italics">k</i>|<i class="fm-italics">X</i>)</span> from the E-step via equations <a class="url" href="../Text/06.xhtml#eq-gmmfit-pi">6.36</a>, <a class="url" href="../Text/06.xhtml#eq-gmmfit-mu">6.37</a>, and <a class="url" href="../Text/06.xhtml#eq-gmmfit-sigma">6.38</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Number of data points</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑪</span> Vector update of <i class="timesitalic">π<sub class="fm-subscript">k</sub></i> for all <span class="math">k</span>, equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-pi">6.36</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑫</span> Vector update of <span class="math">[<i class="fm-italics">K</i> × <i class="fm-italics">d</i>]</span> tensor, <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i> for all <span class="math">k</span>, equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-mu">6.37</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑬</span> Vector computation of <span class="math">(<i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">i</sub></i> – <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>) (<i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">i</sub></i> – <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">k</sub></i>)<sup class="superscript-italic">T</sup></span> for all <i class="timesitalic">l, k</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑭</span> Vector update of <span class="math"><i class="fm-italics">K</i> × <i class="fm-italics">d</i> × <i class="fm-italics">d</i>]</span> tensor <span class="math">Σ<sub class="fm-subscript">k</sub></span> for all <span class="math">k</span>, equation <a class="url" href="../Text/06.xhtml#eq-gmmfit-sigma">6.38</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑮</span> log likelihood, equation <a class="url" href="../Text/06.xhtml#eq-gmm-loglikelihood">6.31</a></p>
<h2 class="fm-head" id="summary-5">Summary</h2>
<p class="body"><a id="marker-237"/>In this chapter, we looked at the Bayesian tools for decision-making in uncertain systems. We discussed conditional probability and Bayes’ theorem, which connects conditional probabilities to joint and marginal probabilities.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Conditional probability is the probability of an event occurring subject to the condition that another event has already occurred. In machine learning, we are often interested in the conditional probability <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">θ</i>)</span> of an input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> given that the parameters of the model predicting the input are <i class="timesitalic">θ</i>. This conditional probability is known as the likelihood of the input. We are also interested in the conditional probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>, known as the posterior probability.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Joint probability is the probability of a set of events occurring together. If the events are independent, the joint probability is the product of their individual probabilities. Whether events are independent or not, Bayes’ theorem connects joint and conditional probabilities. Of particular interest in machine learning is the Bayes’ theorem expression connecting the likelihood and joint and posterior probabilities of inputs and parameters: <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">θ</i>) = <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">θ</i>)<i class="fm-italics">p</i>(<i class="fm-italics">θ</i>)</span> and <!--<span class="times">$p\left( \theta \middle\vert  \vec{x} \right) = \frac{ p\left( \vec{x} \middle\vert \theta \right) p\left(\theta \right) }{ p\left( \vec{x} \right) }$</span>--><span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">θ</i>)<i class="fm-italics">p</i>(<i class="fm-italics">θ</i>) / <i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">θ</i>)</span> is the probability distribution function of the chosen distribution family. <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>)</span> is the prior probability that codifies our belief, sans data, about the system. A popular choice is <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>) <span class="cambria">∝</span> <i class="fm-italics">e</i><sup class="fm-superscript">−||<i class="fm-italics1">θ</i>||<sup class="fm-superscript">2</sup></sup></span>, implying smaller probabilities for higher-magnitude parameters and vice versa.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Entropy models the uncertainty in a system. Systems where all events have more or less similar probabilities tend to be high-entropy. Systems where a particular subset of possible events have significantly high probabilities and others have significantly low probabilities tend to be low-entropy. Equivalently, the probability density functions of low-entropy systems tend to have tall peaks, and their sample point clouds have a high concentration of points in some regions. High-entropy systems tend to have flat probability density functions and diffused sample point clouds.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Cross-entropy allows us to quantify how good our modeling is against a known ground truth.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Kullback–Leibler divergence gives us a measure of the dissimilarity between two probability distributions.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation are two paradigms for estimating model parameters. MLE maximizes <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">θ</i>)</span>, and MAP maximizes <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">θ</i>)<i class="fm-italics">p</i>(<i class="fm-italics">θ</i>)</span>. MLE essentially tries to estimate probability distribution parameters that maximize the overlap between the sample point cloud of the probability distribution and the training data point cloud. MAP is MLE with a regularization condition. The regularization condition is injected via the prior probability term <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">θ</i>)</span>, which favors solutions with a certain property (such as small parameter magnitudes) that we believe to be true from empirical knowledge without data. MLE for Gaussian distributions has a closed-form solution. The mean and variance (covariance in the multidimensional case) of the optimal probability distribution that best fits the training data are the sample mean and sample variance or covariance on the training dataset.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Latent variables in a machine learning system are auxiliary variables that are not directly observed but can be derived from the input. They facilitate the expression of the goal of optimization or the loss to be minimized.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Gaussian mixture models (GMM) are unsupervised probability models that fit multiclass data distributions having multiple clusters in the training dataset, each corresponding to a different class. Here, MLE does not yield a closed-form solution but instead yields an iterative solution to estimate the mixture weights, means, and variances of the individual Gaussians in the mixture.<a id="marker-238"/></p>
</li>
</ul>
</div></body></html>