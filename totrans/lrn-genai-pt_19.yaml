- en: 16 Pretrained large
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: language models and the LangChain library
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Using pretrained large language models for text, image, speech, and code generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-shot, one-shot, and zero-shot prompting techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a zero-shot personal assistant with LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations and ethical concerns of generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rise of pretrained large language models (LLMs) has transformed the field
    of natural language processing (NLP) and generative tasks. OpenAI’s GPT series,
    a notable example, showcases the extensive capabilities of these models in producing
    life-like text, images, speech, and even code. The effective utilization of these
    pretrained LLMs is essential for several reasons. It enables us to deploy advanced
    AI functionalities without the need for vast resources to develop and train these
    models. Moreover, understanding these LLMs paves the way for innovative applications
    that leverage NLP and generative AI, fostering progress across various industries.
  prefs: []
  type: TYPE_NORMAL
- en: In a world increasingly influenced by AI, mastering the integration and customization
    of pretrained LLMs offers a crucial competitive advantage. As AI evolves, leveraging
    these sophisticated models becomes vital for innovation and success in the digital
    landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, these models are operated through browser-based interfaces, which
    vary across different LLMs that function independently of each other. Each model
    has unique strengths and specialties. Interfacing through a browser limits our
    ability to fully take advantage of the potential of each specific LLM. Utilizing
    programming languages like Python, particularly through tools such as the LangChain
    library, provides substantial benefits for the following reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Python’s role in interacting with LLMs enhances the automation of workflows
    and processes. Python scripts, capable of running autonomously, facilitate uninterrupted
    operations without the need for manual input. This is especially beneficial for
    businesses that regularly handle large amounts of data. For instance, a Python
    script could autonomously generate monthly reports by querying an LLM, synthesizing
    the data insights, and disseminating these findings via email or into a database.
    Python offers a greater level of customization and control in managing interactions
    with LLMs than browser-based interfaces do, enabling us to craft custom code to
    meet specific operational needs such as implementing conditional logic, processing
    multiple requests in loops, or managing exceptions. This adaptability is essential
    for customizing outputs to meet particular business objectives or research inquiries.
  prefs: []
  type: TYPE_NORMAL
- en: Python’s extensive collection of libraries makes it ideally suited for integrating
    LLMs with existing software and systems. A prime example of this is the LangChain
    library, which extends Python’s functionality with LLMs. LangChain enables the
    combination of multiple LLMs or the integration of LLM capabilities with other
    services, such as the Wikipedia API or the Wolfram Alpha API, which will be covered
    later in this chapter. This capability of “chaining” different services allows
    for the construction of sophisticated, multistep AI systems where tasks are segmented
    and handled by the best-suited models or services, enhancing both performance
    and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To that end, in this chapter, you’ll first learn how to use the OpenAI API
    to create various content using Python programming: text, images, speech, and
    Python code. You’ll also learn the difference between few-shot, one-shot, and
    zero-shot content generation. Few-shot prompting means you give the model multiple
    examples to help it understand the task, while one-shot or zero-shot prompting
    means one example or no example is provided.'
  prefs: []
  type: TYPE_NORMAL
- en: Modern LLMs such as ChatGPT are trained on preexisting knowledge a few months
    ago so they cannot provide recent or real-time information such as weather conditions,
    flight status, or stock prices. You’ll learn to combine LLMs with Wolfram Alpha
    and Wikipedia APIs using the LangChain library to create a zero-shot know-it-all
    personal assistant.
  prefs: []
  type: TYPE_NORMAL
- en: Despite LLMs’ impressive capabilities, they do not possess an intrinsic understanding
    of the content. This can lead to errors in logic, factual inaccuracies, and a
    failure to grasp complex concepts or nuances. The rapid advancement and widespread
    application of these models also lead to various ethical concerns such as bias,
    misinformation, privacy, and copyright. These issues demand careful consideration
    and proactive measures to ensure that the development and deployment of LLMs align
    with ethical standards and societal values.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1 Content generation with the OpenAI API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While there are other LLMs such as Meta’s LLAMA and Google’s Gemini, OpenAI’s
    GPT series is the most prominent one. We therefore use OpenAI GPTs as our examples
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI allows you to use LLMs to generate various content such as text, images,
    audio, and code. You can access their service either through a web browser or
    an API. We’ll focus on content generation with Python programs via an API in this
    chapter due to the advantages of interacting with LLMs using Python mentioned
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: You do need your OpenAI API key for the programs in this chapter to work. I
    assume you have already obtained your API key in chapter 15\. If not, go back
    to chapter 15 for detailed instructions on how to get one.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll focus mainly on text generation in this section but will provide an example
    for each of the cases of code, image, and speech generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter involves the use of several new Python libraries. To install them,
    run the following lines of code in a new cell in your Jupypter Notebook app on
    your computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Follow the on-screen instructions to finish the installation.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.1 Text generation tasks with OpenAI API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can generate text for many different purposes, such as question-answering,
    text summarization, and creative writing.
  prefs: []
  type: TYPE_NORMAL
- en: When you ask OpenAI GPT a question, keep in mind that all LLMs, including OpenAI
    GPTs, are trained on historical data gathered through automated web crawling.
    As of this writing, GPT-4 was trained using data up to December 2023, with a three-month
    lag. GPT-3.5 was trained on data up to September 2021.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first ask GPT a question about historical facts. Enter the lines of code
    in the following listing in a new cell.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 16.1 Checking historical facts with OpenAI API
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Provides your OpenAI API key
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates an OpenAI() class instance and names it client
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines the role of the system
  prefs: []
  type: TYPE_NORMAL
- en: ④ Asks the question
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you provide your OpenAI API key in the listing 16.1\. We first instantiate
    the `OpenAI()` class and call it `client`. In the `chat.completions.create()`
    method, we specify the model as `gpt-3.5-turbo`. The site [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)
    provides various models available. You can use either gpt-4 or gpt-3.5-turbo for
    text generation. The former provides better results but also incurs higher expenses.
    We’ll use the latter for most cases since our examples are simple enough, so it
    provides equally good results.
  prefs: []
  type: TYPE_NORMAL
- en: The `messages` parameter in the preceding code block consists of several message
    objects, with each object containing a role (which can be “system,” “user,” or
    “assistant”) and content. A system message determines the assistant’s behavior;
    absent a system message, the default setting characterizes the assistant as “a
    helpful assistant.” User messages include inquiries or remarks for the assistant
    to address. For instance, in the previous example, the user message is “Who won
    the Nobel Prize in Economics in 2000?” The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: OpenAI has provided the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also ask the LLM to write an essay on a certain topic. Next, we ask
    it to write a short essay on the importance of self-motivation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `n=1` argument here tells the assistant to generate one response. If you
    want multiple responses, you can set n to a different number. The default value
    for n is 1\. The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The output is six paragraphs long, and I have included only the first few sentences.
    You can go to the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    to see the whole essay. As you can see, the writing is coherent, to the point,
    and without grammatical errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can even ask OpenAI’s GPT to write a joke for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We asked it to tell a math joke, and the result is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can carry out back-and-forth conversations with the assistant. The messages
    parameter automatically includes conversation history. For example, after running
    the previous code block, if you run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: you’ll get a response similar to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The user’s query “Haha, that’s funny! Tell me another one.” only makes sense
    in the context of the prior messages where you ask the assistant to tell a math
    joke. Other text generation capabilities include text summarization and classification,
    and you’ll see such examples later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.2 Code generation with OpenAI API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Codex is specifically designed to understand and generate code. It can work
    with multiple programming languages and can translate natural language descriptions
    into code. Codex is now an integrated part of OpenAI GPTs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we ask OpenAI GPT to produce a Python program to plot a sine curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that ChatGPT uses GPT-3.5-Turbo to manage both the conversational aspect
    and the code generation tasks effectively. It does not provide a separate Codex
    model for code generation only. The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]python'
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: Generate x values from 0 to 2*pi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: x = np.linspace(0, 2*np.pi, 100)
  prefs: []
  type: TYPE_NORMAL
- en: Calculate y values using the sine function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: y = np.sin(x)
  prefs: []
  type: TYPE_NORMAL
- en: Plot the sine graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: plt.figure()
  prefs: []
  type: TYPE_NORMAL
- en: plt.plot(x, y)
  prefs: []
  type: TYPE_NORMAL
- en: plt.title('Sine Graph')
  prefs: []
  type: TYPE_NORMAL
- en: plt.xlabel('x')
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel('sin(x)')
  prefs: []
  type: TYPE_NORMAL
- en: plt.grid(True)
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If you copy and paste the generated Python program into a cell in Jupyter Notebook
    and run it, you’ll see an image similar to figure 16.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH16_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.1 Use OpenAI GPT-3.5 to generate Python code to plot a sine curve.
    We use the text description “Write a Python program to plot a sine graph” to ask
    it to generate a Python program. We then run the program to create the graph.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM not only provides the Python code, but it also lets you know that you
    need to run the code in a Python environment with the matplotlib library installed.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.3 Image generation with OpenAI DALL-E 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DALL-E 2 is an AI model developed by OpenAI, designed to generate images from
    textual descriptions. It is a successor to the original DALL-E model and represents
    advancements in the field of generative AI for visual content.
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E 2 uses a diffusion model similar to what we discussed in chapter 15,
    which starts with a random pattern of pixels and gradually refines it into a coherent
    image that matches the input text. It has improved upon the original DALL-E by
    producing higher-quality images with more accurate and detailed representations
    of the textual descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Incorporating DALL-E 2 into OpenAI’s GPT series allows us to not only generate
    text but also create images based on text prompts. Next, we ask DALL-E 2 to create
    an image of someone fishing at the riverbank:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The code block generates a URL. If you click on the URL, you’ll see an image
    similar to figure 16.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH16_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.2 An image generated by DALL-E 2 with the text prompt “someone fishing
    at the riverbank”
  prefs: []
  type: TYPE_NORMAL
- en: The URL expires in an hour, so make sure you access it promptly. Furthermore,
    the image generated by DALL-E 2 is slightly different even if you use the same
    text prompt because the output is randomly generated.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.4 Speech generation with OpenAI API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Text-to-speech (TTS) is a technology that converts written text into spoken
    words. TTS is trained through multimodal Transformers in which the input is text
    and the output is in audio format. In the context of ChatGPT, integrating TTS
    capabilities means that the LLM can not only generate textual responses but can
    also speak them out loud. Next, we ask OpenAI API to convert a short text into
    speech:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: After running the previous code cell, a file, speech.mp3, is saved on your computer,
    and you can listen to it. The documentation site ([https://platform.openai.com/docs/guides/text-to-speech](https://platform.openai.com/docs/guides/text-to-speech))
    provides voice options. Here we have chosen the `shimmer` option. Other options
    include `alloy`, `echo`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2 Introduction to LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain is a Python library designed to facilitate the use of LLMs in various
    applications. It provides a suite of tools and abstractions that make it easier
    to build, deploy, and manage applications powered by LLMs like GPT-3, GPT-4, and
    other similar models.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain abstracts away the complexities of interacting with different LLMs
    and applications, allowing developers to focus on building their application logic
    without worrying about the underlying model specifics. It is particularly well
    suited for building a “know-it-all” agent by chaining together an LLM with applications
    like Wolfram Alpha and Wikipedia that can provide real-time information or recent
    facts. LangChain’s modular architecture allows for easy integration of different
    components, enabling the agent to leverage the strengths of various LLMs and applications.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2.1 The need for the LangChain library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine that your goal is to build a zero-shot know-it-all agent so that it
    can produce various content, retrieve real-time information, and answer factual
    questions for us. You want the agent to automatically go to the right source to
    retrieve the relevant information based on the task at hand without explicitly
    telling it what to do. The LangChain library is the right tool for this.
  prefs: []
  type: TYPE_NORMAL
- en: In this project, you’ll learn to use the LangChain library to combine LLMs with
    the Wolfram Alpha and Wikipedia APIs to create a zero-shot know-it-all agent.
    We use Wolfram Alpha API to retrieve real-time information and the Wikipedia API
    to answer questions about recent facts. LangChain allows us to create an agent
    to utilize multiple tools to answer a question. The agent first understands the
    query and then decides which tool in the toolbox to use to answer the question.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show you that even the most advanced LLMs lack these abilities, let’s ask
    who won the Best Actor Award in the 2024 Academy Awards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: I made this query on March 17, 2024, and GPT-4 was not able to answer the question.
    It’s possible that when you make the same query, you’ll get the correct answer
    because the model has been updated using more recent data. If that’s the case,
    change the question to an event a few days ago, and you should get a similar response.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we’ll use LangChain to chain together an LLM with the Wolfram Alpha
    and Wikipedia APIs. Wolfram Alpha is good at scientific computations and retrieving
    real-time information, while Wikipedia is famous for providing information on
    both historical and recent events and facts.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2.2 Using the OpenAI API in LangChain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The langchain-openai library you installed earlier in this chapter allows you
    to use OpenAI GPTs with minimal prompt engineering. You only need to explain what
    you want the LLM to do in plain English.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how we ask it to correct grammar errors in text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that we didn’t use any prompt engineering. We didn’t specify which model
    to use either. LangChain found the best model for the job based on the task requirements
    and other factors such as cost, latency, and performance. It also automatically
    formats and structures the queries to be suitable for the model it uses. The preceding
    prompt simply asks the agent, in plain English, to correct the grammar errors
    in the text. It returns text with the correct grammar, as shown in the previous
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another example. We asked the agent to name the capital city of Kentucky:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It tells us the correct answer, which is Frankfort, Kentucky.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2.3 Zero-shot, one-shot, and few-shot prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Few-shot, one-shot, and zero-shot prompting refer to different ways of providing
    examples or instructions to LLMs to guide their responses. These techniques are
    used to help the model understand the task at hand and generate more accurate
    or relevant outputs.
  prefs: []
  type: TYPE_NORMAL
- en: In zero-shot prompting, the model is given a task or a question without any
    examples. The prompt typically includes a clear description of what is expected,
    but the model must generate a response based solely on its preexisting knowledge
    and understanding. In one-shot prompting, the model is provided with a single
    example to illustrate the task. In few-shot prompting, the model is given multiple
    examples to help it understand the task. Few-shot prompting is based on the idea
    that providing more examples can help the model better grasp the pattern or the
    rules of the task, leading to more accurate responses.
  prefs: []
  type: TYPE_NORMAL
- en: All your interactions so far with OpenAI GPTs are zero-shot prompting since
    you haven’t provided them with any examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try an example of few-shot prompting. Suppose you want the LLM to conduct
    sentiment analysis: you want it to classify a sentence as positive or negative.
    You can provide several examples in the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the prompt, we provided three examples. Two reviews are classified as positive,
    while one is classified as negative. We then provided the sentence, “How horrible
    the movie is!” The LLM classified it correctly as negative.
  prefs: []
  type: TYPE_NORMAL
- en: We used `//` to separate the sentence and the corresponding sentiment in the
    previous example. You can use other separators such as `->`, so long as you are
    consistent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of one-shot prompting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: By providing one single example, we are effectively asking the LLM, “What is
    to a plane as a driver is to a car?” The LLM correctly answered `Pilot`.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 16.1
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you want to ask the LLM, “What is to a garden as a chef is to a kitchen?”
    Use one-shot prompting to get the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, here is an example of zero-shot prompting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We didn’t provide any examples in the prompt. However, we provided instruction
    in plain English to ask the LLM to classify the tone in the sentence as positive,
    negative, or neutral.
  prefs: []
  type: TYPE_NORMAL
- en: 16.3 A zero-shot know-it-all agent in LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ll learn to create a zero-shot know-it-all agent in LangChain in this section.
    You’ll use OpenAI GPTs to generate various content such as text, images, and code.
    To compensate for LLM’s inability to provide real-time information, you’ll learn
    to add Wolfram Alpha and Wikipedia APIs to the toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: Wolfram Alpha is a computational knowledge engine designed to handle factual
    queries online, specializing in numerical and computational tasks, particularly
    in the science and technology fields. By integrating the Wolfram Alpha API, the
    agent gains the ability to answer virtually any question across various subjects.
    Should Wolfram Alpha be unable to provide a response, we will use Wikipedia as
    a secondary source for fact-based questions on specific topics.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.3 is a diagram of the steps we’ll take to create the zero-shot know-it-all
    agent in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH16_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.3 Steps to create a zero-shot know-it-all agent with the LangChain
    library
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we’ll first create an agent in LangChain with just one tool—the
    Wolfram Alpha API—to answer questions related to real-time information and recent
    facts. We’ll then add the Wikipedia API to the toolbox as a backup on questions
    related to recent facts. We’ll add various tools utilizing the OpenAI API such
    as text summarizer, joke teller, and sentiment classifier. Finally, we’ll add
    image and code generation functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: 16.3.1 Applying for a Wolfram Alpha API Key
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Wolfram Alpha gives you up to 2,000 noncommercial API calls per month for free.
    To obtain an API key, first go to [https://account.wolfram.com/login/create/](https://account.wolfram.com/login/create/)
    and complete the steps to create an account.
  prefs: []
  type: TYPE_NORMAL
- en: The Wolfram account itself gives you only browser access; you need to apply
    for an API key at [https://products.wolframalpha.com/api/](https://products.wolframalpha.com/api/).
    Once there, click Get API Access in the bottom left corner. A small dialog should
    pop up, fill in the fields Name and Description, select *Simple API* from the
    dropdown menu, and then click Submit, as shown in figure 16.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH16_F04_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.4 Applying for a Wolfram Alpha AppID
  prefs: []
  type: TYPE_NORMAL
- en: After that, your AppID should appear in a new window. Copy the API key and save
    it in a file for later use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how you can use the Wolfram Alpha API to conduct math operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The Wolfram Alpha API provides the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll also include the Wikipedia API to provide answers to various topics.
    You don’t need to apply for an API key if you have installed the Wikipedia library
    on your computer. Here is an example of using the Wikipedia API in the LangChain
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We have omitted most of the output for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 16.3.2 Creating an agent in LangChain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we’ll create an agent in LangChain, with only the Wolfram Alpha API in
    the toolbox. An agent in this context refers to an individual entity designed
    to handle specific tasks or processes through natural language interactions. We’ll
    then gradually add more tools to the chain so that the agent becomes capable of
    handling more tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 16.2 Creating an agent in LangChain
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines which LLM to use
  prefs: []
  type: TYPE_NORMAL
- en: ② Adds Wolfram Alpha to the toolbox
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines an agent
  prefs: []
  type: TYPE_NORMAL
- en: ④ Asks the agent a question
  prefs: []
  type: TYPE_NORMAL
- en: The `hwchase17/react` in LangChain refers to a specific type of ReAct agent
    configuration. ReAct stands for Reactive Action, which is a framework within LangChain
    designed to optimize the use of language model capabilities in combination with
    other tools to solve complex tasks effectively. See [https://python.langchain.com/docs/how_to/migrate_agent/](https://python.langchain.com/docs/how_to/migrate_agent/)
    for more details. When you create an agent in LangChain, you need to specify the
    tools to be used by the agent. In the previous example, we use only one tool,
    the Wolfram Alpha API.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we ask the current temperature in Lexington, Kentucky, and here
    is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The output not only shows the final answer, which says the current temperature
    in Lexington, Kentucky, is 44 degrees Fahrenheit, but it also shows the chain
    of thoughts. It uses Wolfram Alpha as the source to obtain the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also add Wikipedia to the toolbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'I ask who won the Best Actor Award in the 2024 Academy Awards, and the agent
    uses Wikipedia to get the correct answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding output, the agent first decides to use Wikipedia as the tool
    to solve the problem. After searching through various Wikipedia sources, the agent
    provides the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you’ll learn to add various OpenAI GPT tools to the agent’s toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: 16.3.3 Adding tools by using OpenAI GPTs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first add a text summarizer so that the agent can summarize text.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 16.3 Adding a text summarizer to the agent’s tool box
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a template
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines a summarizer function
  prefs: []
  type: TYPE_NORMAL
- en: ③ Adds summarizer as a tool
  prefs: []
  type: TYPE_NORMAL
- en: ④ Redefines the agent with the updated toolbox
  prefs: []
  type: TYPE_NORMAL
- en: We first provide a template to summarize text. We then define a summarizer function
    and add it to the toolbox. Finally, we redefine the agent by using the updated
    toolbox and ask it to summarize the example text with one sentence. Make sure
    your prompt has the same format as those described in the template so that the
    agent knows which tool to use.
  prefs: []
  type: TYPE_NORMAL
- en: The output from listing 16.3 is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The agent chooses the summarizer as the tool for the task since the input matches
    the template described in the summarizer function. We use two long sentences as
    the text input and the preceding output is a one-sentence summary.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add as many tools as you like. For example, you can add a tool to tell
    a joke on a certain subject:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We ask the agent to tell a joke on the subject of coding. The agent identifies
    Joke Teller as the tool. The joke is indeed related to coding.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 16.2
  prefs: []
  type: TYPE_NORMAL
- en: Add a tool to the agent’s toolbox to conduct sentiment analysis. Name the tool
    Sentiment Classifier. Then ask the agent to classify the text “this movie is so-so”
    as positive, negative, or neutral.
  prefs: []
  type: TYPE_NORMAL
- en: 16.3.4 Adding tools to generate code and images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can add various tools to the toolbox in LangChain. Interested readers can
    find more details at [https://python.langchain.com/docs/how_to/#tools](https://python.langchain.com/docs/how_to/#tools).
    Next, we add tools to generate other content forms such as code and images.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add a tool to generate code, you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: If you run the generated code in a cell, you’ll see an image as in figure 16.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH16_F05_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.5 Adding a tool in LangChain to generate Python code. The tool then
    generates code to plot sine and cosine curves in the same graph, with a legend
    and line styles.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add an image generator, you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The output is a URL for you to visualize and download an image. We asked the
    agent to create an image of a horse grazing on the grassland. The image is shown
    in figure 16.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH16_F06_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.6 An image generated by a know-it-all agent in LangChain
  prefs: []
  type: TYPE_NORMAL
- en: With that, you have learned how to create a zero-shot know-it-all agent in LangChain.
    You can add more tools to the toolbox depending on what you want the agent to
    accomplish.
  prefs: []
  type: TYPE_NORMAL
- en: 16.4 Limitations and ethical concerns of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs such as OpenAI’s GPT series have made significant strides in the field
    of NLP and generative AI. Despite their impressive capabilities, these models
    are not without limitations. Understanding these constraints is crucial for both
    leveraging their strengths and mitigating their weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, the rapid advancement and widespread application of these
    models have also given rise to a host of ethical concerns such as bias, inaccuracies,
    breach of privacy, and copyright infringements. These issues demand careful consideration
    and proactive measures to ensure that the development and deployment of LLMs align
    with ethical standards and societal values.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll explore the limitations of LLMs, provide insights into
    why these issues persist, and present examples of notable failures to underscore
    the importance of addressing these challenges. We’ll also examine the key ethical
    concerns associated with LLMs and propose pathways for mitigating these concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 16.4.1 Limitations of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the fundamental limitations of LLMs is their lack of true understanding
    and reasoning. While they can generate coherent and contextually relevant responses,
    they do not possess an intrinsic understanding of the content. This can lead to
    errors in logic, factual inaccuracies, and a failure to grasp complex concepts
    or nuances.
  prefs: []
  type: TYPE_NORMAL
- en: 'This manifests in many epic mistakes made by LLMs. The book Smart Until It’s
    Dumb provides many entertaining instances of such mistakes made by GPT-3 and ChatGPT.^([1](#footnote-001))
    For example, consider this question: Mrs. March gave the mother tea and gruel,
    while she dressed the little baby as tenderly as if it had been her own. Who’s
    the baby’s mother? The answer provided by GPT-3 is Mrs. March.'
  prefs: []
  type: TYPE_NORMAL
- en: To be fair, with the rapid advancement of LLMs, many of these mistakes are corrected
    over time. However, LLMs still make low-level mistakes. A LinkedIn article in
    June 2023 by David Johnston ([https://www.linkedin.com/pulse/intelligence-tests-llms-fail-why
  prefs: []
  type: TYPE_NORMAL
- en: '-david-johnston/](https://www.linkedin.com/pulse/intelligence-tests-llms-fail-why-david-johnston/))
    tests the intelligence of LLMs on a dozen problems that humans can easily solve.
    LLMs, including GPT-4, struggle with these problems. One of the problems is as
    follows: name an animal such that the length of the word is equal to the number
    of legs they have minus the number of tails they have.'
  prefs: []
  type: TYPE_NORMAL
- en: This mistake has not been corrected as of this writing. Figure 16.7 is a screenshot
    of the answer by GPT-4 when I used a browser interface.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH16_F07_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.7 How GPT-4 still makes low-level mistakes
  prefs: []
  type: TYPE_NORMAL
- en: The output in figure 16.7 shows that, according to GPT-4, five is equal to the
    number of letters in the word “bee.”
  prefs: []
  type: TYPE_NORMAL
- en: 16.4.2 Ethical concerns for LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most pressing ethical concerns is the potential for LLMs to perpetuate
    and amplify biases in their training data. Since these models learn from vast
    datasets often derived from human-generated content, they can inherit biases related
    to gender, race, ethnicity, and other social factors. This can result in biased
    outputs that reinforce stereotypes and discrimination.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate bias, it is essential to adopt diverse and inclusive training datasets,
    implement bias detection and correction algorithms, and ensure transparency in
    model development and evaluation. It’s particularly important to establish industry-wide
    collaboration to set standards for bias mitigation practices and promote responsible
    AI development.
  prefs: []
  type: TYPE_NORMAL
- en: However, we must keep in mind not to overcorrect. A counterexample is that Google’s
    Gemini overcorrected the stereotypes in image generation by including people of
    color in groups like Nazi-era German soldiers.^([2](#footnote-000))
  prefs: []
  type: TYPE_NORMAL
- en: Another concern for LLMs is their potential for misinformation and manipulation.
    LLMs have the ability to generate realistic and persuasive text, which can be
    exploited for creating and spreading misinformation, propaganda, or manipulative
    content. This poses significant risks to public discourse, democracy, and trust
    in information.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this concern lies in developing robust content moderation systems.
    Establishing guidelines for responsible use and fostering collaborations between
    AI developers, policymakers, and media organizations are crucial steps in combating
    misinformation.
  prefs: []
  type: TYPE_NORMAL
- en: The third concern is related to privacy. The vast amount of data used to train
    LLMs raises privacy concerns, as sensitive information can be inadvertently revealed
    in the model’s outputs. Additionally, the potential for LLMs to be used in cyberattacks
    or to bypass security measures poses significant security risks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the data used to train LLMs is mostly gathered without authorization.
    Supporters argue that the way data is used to train LLMs is transformative: the
    model doesn’t merely regurgitate the data but uses it to generate new, original
    content. This transformation could qualify under the “fair use” doctrine, which
    allows limited use of copyrighted material without permission if the use adds
    new expression or meaning. Critics argue that LLMs are trained on vast amounts
    of copyrighted texts without permission, which goes beyond what might be considered
    fair use. The scale of data used and the direct ingestion of copyrighted material
    without transformation during training could be seen as infringing. The debate
    is ongoing. The current copyright laws were not designed with generative AI in
    mind, leading to ambiguities about how they apply to technologies like LLMs. It’s
    a debate that likely needs to be resolved by legislative and judicial bodies to
    provide clear guidelines and ensure that the interests of all parties are fairly
    represented.'
  prefs: []
  type: TYPE_NORMAL
- en: The ethical concerns surrounding LLMs are multifaceted and require a holistic
    approach. Collaborative efforts among AI researchers, developers, and policymakers
    are crucial in developing ethical guidelines and frameworks that guide the responsible
    development and deployment of these powerful models. As we continue to harness
    the potential of LLMs, ethical considerations must remain at the forefront of
    our endeavors to ensure that AI advances in harmony with societal values and human
    well-being.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Few-shot prompting means you give LLMs multiple examples to help them understand
    the task, while one-shot or zero-shot prompting means one example or no example
    is provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain is a Python library designed to facilitate the use of LLMs in various
    applications. It abstracts away the complexities of interacting with different
    LLMs and applications. It allows the agent to automatically go to the right tool
    in the toolbox based on the task at hand without explicitly telling it what to
    do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern pretrained LLMs such as OpenAI’s GPT series can create various formats
    of content such as text, images, audio, and code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite their impressive achievements, LLMs lack a true understanding of the
    content or the ability to reason. These limitations can lead to errors in logic,
    factual inaccuracies, and a failure to grasp complex concepts or nuances. Furthermore,
    the rapid advancement and widespread application of these models have given rise
    to a host of ethical concerns such as bias, misinformation, breach of privacy,
    and copyright infringements. These issues demand careful consideration and proactive
    measures to ensure that the development and deployment of LLMs align with ethical
    standards and societal values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](#footnote-001-backlink))  Maggiori, Emmanuel, 2023, *Smart Until It’s
    Dumb: Why Artificial Intelligence Keeps Making Epic Mistakes (and Why the AI Bubble
    Will Burst)*, Applied Maths Ltd. Kindle Edition.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](#footnote-000-backlink))  Adi Robertson, February 21, 2024, “Google Apologizes
    for 'Missing the Mark’ after Gemini Generated Racially Diverse Nazis.” The Verge,
    [https://mng.bz/2ga9](https://mng.bz/2ga9).
  prefs: []
  type: TYPE_NORMAL
