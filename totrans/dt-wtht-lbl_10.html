<html><head></head><body>
  <div class="readable-text" id="p1"> &#13;
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">8</span> </span> <span class="chapter-title-text">Deep learning: The foundational concepts</span></h1> &#13;
  </div> &#13;
  <div class="introduction-summary"> &#13;
   <h3 class="introduction-header">This chapter covers</h3> &#13;
   <ul> &#13;
    <li class="readable-text" id="p2">Core building blocks of deep learning </li> &#13;
    <li class="readable-text" id="p3">Supervised and unsupervised learning approaches</li> &#13;
    <li class="readable-text" id="p4">Convolutional and recurrent neural networks</li> &#13;
    <li class="readable-text" id="p5">The Boltzmann learning rule and deep belief networks </li> &#13;
    <li class="readable-text" id="p6">Python coding with TensorFlow and Keras </li> &#13;
    <li class="readable-text" id="p7">Overview of deep learning libraries </li> &#13;
   </ul> &#13;
  </div> &#13;
  <div class="readable-text" id="p8"> &#13;
   <blockquote>&#13;
    <div>&#13;
     The art of simplicity is a puzzle of complexity. &#13;
     <div class=" quote-cite">&#13;
       —Douglas Horton &#13;
     </div>&#13;
    </div>&#13;
   </blockquote> &#13;
  </div> &#13;
  <div class="readable-text" id="p9"> &#13;
   <p>Welcome to the third part of the book. So far, you have covered a lot of concepts and case studies and Python code. From this chapter onward, the level of complexity will be even higher. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p10"> &#13;
   <p>In the first two parts of the book, we covered various unsupervised learning algorithms like clustering, dimensionality reduction, etc. We discussed both simpler and advanced algorithms. We also covered working on text data in the second part of the book. Starting from this third part of the book, we will start our journey on deep learning. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p11"> &#13;
   <p>Deep learning and neural networks have changed the world and the business domains. You have probably heard about deep learning and neural networks. Their implementations and sophistication result in better cancer detection, autonomous driving cars, improved disaster management systems, better pollution control systems, reduced fraud in transactions, and so on. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p12"> &#13;
   <p>In the third part of the book, we will explore unsupervised learning using deep learning. We will study what deep learning is and the basics of neural networks, as well as the layers in a neural network, activation functions, the process of deep learning, and various libraries. Then we will move to autoencoders and generative adversarial networks (GANs) and generative AI (GenAI). The topics are indeed complex and sometimes quite mathematically heavy. We will use different kinds of datasets for working on the problems, but primarily the datasets will be unstructured in nature. As always, Python will be used to generate the solutions. We also share a lot of external resources to complement the concepts. Please note that these are advanced topics, and a lot of research is still ongoing for these topics. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p13"> &#13;
   <p>We have divided the third part of the book into four chapters. This chapter covers the foundational concepts of deep learning and neural networks. The next two chapters focus on autoencoders, GAN and GenAI. The final chapter of the book talks about the deployment of these models.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p14"> &#13;
   <p>In this chapter, we discuss the concepts of neural networks and deep learning. We discuss what a neural network is, its activation functions, different optimization functions, the neural network training process, etc. The concepts covered in this chapter form the base of neural networks and deep learning and subsequent learning in the next two chapters. Hence, it is vital that you are clear about these concepts. The best external resources to learn these concepts in more detail are given at the end of the chapter.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p15"> &#13;
   <p>Welcome to the eighth chapter, and all the very best!</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p16"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.1</span> Technical toolkit</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p17"> &#13;
   <p>We will continue to use the same version of Python and Jupyter Notebook as we have used so far. The codes and datasets used in this chapter have been checked in at the same GitHub location. You will need to install a couple of Python libraries in this chapter: <code>tensorflow</code> and <code>keras</code>. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p18"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.1.1</span> Deep learning: What is it? What does it do?</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p19"> &#13;
   <p>Deep learning has gathered a lot of momentum in the past few years. Neural networks are pushing the boundaries of machine learning solutions. Deep learning is machine learning only. Deep learning is based on neural networks. It utilizes a similar concept—that is, using historical data and understanding the attributes and the intelligence gathered to find patterns or predict the future, albeit deep learning is more complex than the algorithms we have covered so far.  </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p20"> &#13;
   <p>Recall chapter 1, where we covered the concepts of structured and unstructured datasets. Unstructured datasets include text, images, audio, video, etc. Figure 8.1 describes the major sources of text, images, audio, and video datasets.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p21">  &#13;
   <img alt="figure" src="../Images/CH08_F01_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.1</span> Unstructured datasets like text, audio, images, and video can be analyzed using deep learning. There are multiple sources of such datasets.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p22"> &#13;
   <p>While deep learning can be implemented for structured datasets too, it is mostly working wonders on unstructured datasets. One of the prime reasons is that the classical machine learning algorithms are sometimes not that effective on unstructured datasets like that of images, text, audio, and video. A few of the path-breaking solutions delivered by deep learning across various domains are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p23"> <em>The medical field and pharmaceuticals</em><em> </em>—Deep learning sees application in areas such as the identification of bones and joint problems or in determining if there are any clots in arteries or veins. In the pharmaceutical field, it expedites clinical trials and helps to reach the target drug faster. </li> &#13;
   <li class="readable-text" id="p24"> <em>The banking and financial sector</em><em> </em>—Deep learning-based algorithms are used to detect potential fraud in transactions. Using image recognition-based algorithms, we can also distinguish fake signatures on checks. </li> &#13;
   <li class="readable-text" id="p25"> <em>The automobile sector</em><em> </em>—You have probably heard about autonomous driving (aka self-driving) cars. Using deep learning, the algorithms can detect traffic signals, pedestrians, other vehicles on the road, their respective distances, and so on. </li> &#13;
   <li class="readable-text" id="p26"> <em>Retail</em><em> </em>—In the retail sector, using deep learning-based algorithms, humans can improve customer targeting and develop advanced and customized marketing tactics. The recommended models to provide next-best products to the customers have been improved using deep learning. We can get better returns on investments and improve cross-sell and upsell strategies. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p27"> &#13;
   <p>In addition, automatic speech recognition is possible with deep learning. Using sophisticated neural networks, humans can create speech recognition algorithms. These solutions are being used across Siri, Alexa, Translator, Baidu, etc.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p28"> &#13;
   <p>Image recognition is also advancing. Neural networks are improving image recognition techniques. This can be done using convolutional neural networks, which are improving computer vision. Use cases include the following:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p29"> Deep learning is quite effective for differentiation between cancerous cells and benign cells. Differentiation can be achieved by using the images of cancerous cells and benign cells. </li> &#13;
   <li class="readable-text" id="p30"> An automated number plate reading system has been developed using neural networks. </li> &#13;
   <li class="readable-text" id="p31"> Object detection methods and monitor sensing and tracking systems can be developed using deep learning. </li> &#13;
   <li class="readable-text" id="p32"> In disaster management systems, deep learning can detect the presence of humans in affected areas. Just imagine how, during rescue operations, human lives can be saved using better detection. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p33"> &#13;
   <p>GenAI is changing the world rapidly. Use cases include automating content creation, such as writing articles, essays, and social media posts and generating images and videos. It improves customer service and customer experience by providing chatbots that provide instant, personalized responses to the queries of the customers. It can be implemented in any industry. In data-heavy industries, it creates ripples by summarizing complex and long documents and generating insights from dashboards and reports. These reports can be Power BI/Tableau dashboards, PowerPoints, or pdf files, for example. It has also helped software developers in code generation and debugging and has improved software development efficiency. The use cases are many, ranging from retail; telecommunications; healthcare; R&amp;D; banking, finance, and insurance, etc., in improving sales, reducing costs, saving time, and improving accuracy.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p34"> &#13;
   <p>The use cases listed are certainly not exhaustive. Using deep learning, we can improve natural language processing solutions used to measure customers’ sentiments, language translation, text classification, named-entity recognition, etc. Across use cases in bioinformatics, the military, mobile advertising, technology, the supply chain, and so on, deep learning is paving the path for the future.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p35"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.2</span> Building blocks of a neural network</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p36"> &#13;
   <p>Artificial neural networks (ANNs) are said to be inspired by the way the human brain works. The human brain is the best machine we currently have access to. When we see a picture or a face or hear a tune, we associate a label or a name with it. That allows us to train our brain and senses to recognize a picture or a face or a tune when we see/hear it again. ANNs learn to perform similar tasks by learning or getting trained. </p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p37"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 8.1</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p38"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p39"> What is the meaning of deep learning? </li> &#13;
    <li class="readable-text" id="p40"> Neural networks cannot be used for unsupervised learning. True or False? </li> &#13;
    <li class="readable-text" id="p41"> Explore more use cases for deep learning in nonconventional business domains. </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p42"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.2.1</span> Neural networks for solutions</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p43"> &#13;
   <p>In deep learning, too, the concepts of supervised and unsupervised learning are applicable. We cover both types of training of the network: supervised and unsupervised. This will give you a complete picture. At the same time, to fully appreciate unsupervised deep learning, you should be clear on the supervised deep learning process. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p44"> &#13;
   <p>Let’s understand the deep learning process by using an example. Consider this: we wish to create a solution that can identify faces—a solution that can distinguish faces and identify the person by allocating a name to the face. For training the model, we will use a dataset that will have images of people’s faces and corresponding names. The ANN will start with no prior understanding of the image’s dataset or the attributes. During the process of training, it will learn the attributes and the identification characteristics from the training data. These learned attributes are then used to distinguish between faces. At this moment, we are only covering the process at a high level; we will cover this process in much more detail in subsequent sections. Figure 8.2 shows a representation of a neural network.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p45">  &#13;
   <img alt="figure" src="../Images/CH08_F02_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.2</span> A typical neural network with neurons and various layers</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p46"> &#13;
   <p>The process in a neural network is quite complex. We will first cover all the building blocks of a neural network, like neurons, activation functions, weights, bias terms, etc., and then move on to the process followed in a neural network. Let’s start with the protagonist: a neuron.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p47"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.2.2</span> Artificial neurons and perceptrons</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p48"> &#13;
   <p>The human brain contains billions of neurons. The neurons are interconnected cells in our brains. These neurons receive signals, process them, and generate results. Artificial neurons are based on biological neurons only and can be considered simplified computational models of biological neurons. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p49"> &#13;
   <p>In 1943, researchers Warren McCullock and Walter Pitts proposed the concept of a simplified brain cell called the McCullock-Pitts neuron. It can be thought of as a simple logic gate with binary outputs. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p50"> &#13;
   <p>The working methodology for artificial neurons is similar to that of biological neurons, albeit artificial neurons are far simpler than biological neurons. A perceptron is a mathematical model of a biological neuron. In the actual biological neurons, dendrites receive electrical signals from the axons of other neurons. In a perceptron, these electrical signals are represented as numerical values.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p51"> &#13;
   <p>The artificial neuron receives inputs from the previous neurons or can receive the input data. It then processes that input information and shares an output. The input can be the raw data or processed information from a preceding neuron. The neuron then combines the input with its own internal state, weighs them separately, and passes the output received through a nonlinear function to generate output. These nonlinear functions are also called activation functions (we will cover them later). You can think of an activation function as a mathematical function. A neuron can be represented as shown in figure 8.3.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p52">  &#13;
   <img alt="figure" src="../Images/CH08_F03_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.3</span> A neuron gets the inputs, processes them using mathematical functions, and then generates the output.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p53"> &#13;
   <p>In simpler terms, a neuron can be termed as a mathematical function that computes the weighted average of its input datasets; then this sum is passed through activation functions. The output of the neuron can then be the input to the next neuron, which will again process the input received. Let’s go a bit deeper.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p54"> &#13;
   <p>In a perceptron, each input value is multiplied by a factor called the <em>weight</em>. Biological neurons fire once the total strength of the input signals exceeds a certain threshold. A similar format is followed in a perceptron. In a perceptron, a weighted sum of the inputs is calculated to get the total strength of the input data, and then an activation function is applied to each of the outputs. Each output can then be fed to the next layer of perceptron. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p55"> &#13;
   <p>Let’s assume that there are two input values, <em>a</em> and <em>b</em>, for a perceptron <em>X</em>, which for the sake of simplicity has only one output. Let the respective weights for <em>a</em> and <em>b</em> be <em>P</em> and <em>Q</em>. So the weighted sum can be calculated as <em>P</em> * <em>X</em> + <em>Q</em> * <em>b</em>. The perceptron will fire or will have a nonzero output only if the weighted sum exceeds a certain threshold. Let’s call the threshold <em>C</em>. So, we can say the following:</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p56"> &#13;
   <p>The output of <em>X</em> will be 0 if <em>P</em> * <em>X</em> + <em>Q</em> * <em>y</em> &lt;= <em>C</em>.</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p57"> &#13;
   <p>The output of <em>X</em> will be 1 if <em>P</em> * <em>S</em> + <em>Q</em> * <em>y</em> &gt; <em>C</em>.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p58"> &#13;
   <p>If we generalize this understanding, we can represent it as follows. Representing a perceptron as a function maps input <em>x</em> as the function:<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p59"> &#13;
   <img alt="figure" src="../Images/verdhan-ch8-eqs-0x.png"/> &#13;
  </div> &#13;
  <div class="readable-text" id="p60"> &#13;
   <p>where <em>x</em> is the vector of input values, <em>w</em> represents the vector of weights, and <em>b</em> is the bias term. We explain the bias and the weight terms next.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p61"> &#13;
   <p>Recall the linear equation: <em>y</em> = <em>mx</em> + <em>c</em> where <em>m</em> is the slope of the straight line and <em>c</em> is the constant term. Both bias and weight can be defined using the same linear equation. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p62"> &#13;
   <p>The role of weight is similar to the slope of the line in a linear equation. It defines the change in the value of <em>f</em>(<em>x</em>) by a unit change in the value of <em>x</em>.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p63"> &#13;
   <p>The role of the bias is similar to the role of a constant in a linear function. In case there is no bias, the input to the activation function is <em>x</em> multiplied by the weight.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p64"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Weights and bias terms are the parameters that get trained in a network.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p65"> &#13;
   <p>The output of the function will depend on the activation function used. We will cover various types of activation functions in the next section after we have covered different layers in a network. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p66"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.2.3</span> Different layers in a network</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p67"> &#13;
   <p>A simple and effective way of organizing neurons is the following. Rather than allowing arbitrary neurons connected with arbitrary others, neurons are organized in layers. A neuron in a layer has all its inputs coming only from the previous layer and all its output going only to the next. There are no other connections, for example, between neurons of the same layer or between neurons in neurons belonging to distant layers (with a small exception for specialized cases, which is beyond the scope of this book).</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p68"> &#13;
   <p>We know that information flows through a neural network. That information is processed and passed on from one layer to another layer in a network. There are three layers in a neural network, as shown in figure 8.4. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p69">  &#13;
   <img alt="figure" src="../Images/CH08_F04_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.4</span> A typical neural network with neurons and input, hidden, and output layers</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p70"> &#13;
   <p>The neural network shown in figure 8.4 has three input units and two hidden layers with four neurons each and one final output layer: </p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p71"> <em>Input layer</em><em> </em>—As the name signifies, this receives the input data and shares it with the hidden layers.  </li> &#13;
   <li class="readable-text" id="p72"> <em>Hidden layer</em><em> </em>—This is the heart and soul of the network. The number of hidden layers depends on the problem at hand; the number of layers can range from a few to hundreds. All the processing, feature extraction, and learning of the attributes is done in these layers. In the hidden layers, all the input raw data is broken into attributes and features. This learning is useful for decision-making at a later stage.  </li> &#13;
   <li class="readable-text" id="p73"> <em>Output layer</em><em> </em>—This is the decision layer and final piece in a network. It accepts the outputs from the preceding hidden layers and then makes a prediction. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p74"> &#13;
   <p>For example, the input training data may have raw images or processed images. These images will be fed to the input layer. The data then travels to the hidden layers where all the calculations are done. These calculations are done by neurons in each layer. The output is the task that needs to be accomplished—for example, identification of an object or classification of an image, etc. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p75"> &#13;
   <p>The ANN consists of various connections. Each of the connections aims to receive the input and provide the output to the next neuron. This output to the next neuron will serve as an input to it. Also, as discussed earlier, each connection is assigned a weight, which is representative of its respective importance. It is important to note that a neuron can have multiple input and output connections, which means it can receive inputs and deliver multiple outputs.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p76"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 8.2</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p77"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p78"> The input data is fed to the hidden layers in a neural network. True or False? </li> &#13;
    <li class="readable-text" id="p79"> A bias term is similar to the slope of a linear equation. True or False? </li> &#13;
    <li class="readable-text" id="p80"> Find and explore the deepest neural network ever trained. </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p81"> &#13;
   <p>So what is the role of a layer? A layer receives inputs, processes them, and passes the output to the next layer. Technically, it is imperative that the transformation implemented by a layer is parameterized by its weights, which are also referred to as parameters of a layer. In simple terms, to ensure a neural network is “trained” to a specific task, something must be changed in the network. It turns out that changing the architecture of the network (i.e., how neurons are connected) has only a small effect. On the other hand, as we will see later in this chapter, changing the weights is the key to the “learning” process.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p82"> &#13;
   <p>We now move to the very important topic of activation functions. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p83"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.2.4</span> Activation functions</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p84"> &#13;
   <p>We have already mentioned activation functions. The primary role of an activation function is to decide whether a neuron/perceptron should fire or not. These functions play a central role in the training of the network at a later stage. They are sometimes referred to as <em>transfer functions</em>. It is also important to know why we need nonlinear activation functions. If we use only linear activation functions, the output will also be linear. At the same time, the derivative of a linear function will be constant. Hence, there will not be much learning possible. Thus, we prefer to have nonlinear activation functions. We study the most common activation functions next.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p85"> &#13;
   <h4 class=" readable-text-h4">Sigmoid function</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p86"> &#13;
   <p>A sigmoid is a bounded monotonic mathematical function. It always increases its output value when the input values increase. Its output value is always between –1 and 1.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p87"> &#13;
   <p>A sigmoid is a differentiable function with an S-shaped curve, and its first derivative function is bell-shaped. It has a nonnegative derivative function and is defined for all real input values. The sigmoid function is used if the output value of a neuron is between 0 and 1. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p88"> &#13;
   <p>Mathematically, a sigmoid function can be represented by equation 8.1:</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p89">  &#13;
   <img alt="figure" src="../Images/verdhan-ch8-eqs-1x.png"/> &#13;
   <h5 class=" figure-container-h5">(8.1)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p90"> &#13;
   <p>Figure 8.5 shows a graph of a sigmoid function. The sigmoid function finds its applications in complex learning systems. It is usually used for binary classification and in the final output layer of the network.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p91">  &#13;
   <img alt="figure" src="../Images/CH08_F05_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.5</span> A sigmoid function. Note the shape of the function and the min/max values.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p92"> &#13;
   <h4 class=" readable-text-h4">TANH function</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p93"> &#13;
   <p>In mathematics, the tangent hyperbolic (TANH) function is a differentiable hyperbolic function. It is a smooth function, and its input values are in the range of –1 to +1.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p94"> &#13;
   <p>A TANH function is written as equation 8.2:</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p95">  &#13;
   <img alt="figure" src="../Images/verdhan-ch8-eqs-2x.png"/> &#13;
   <h5 class=" figure-container-h5">(8.2)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p96"> &#13;
   <p>A graphical representation of TANH is shown in figure 8.6. It is a scaled version of the sigmoid function, and hence a TANH function can be derived from a sigmoid function and vice versa. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p97">  &#13;
   <img alt="figure" src="../Images/CH08_F06_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.6</span> A TAHN function, which is a scaled version of a sigmoid function</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p98"> &#13;
   <p>A TANH function is generally used in the hidden layers. It makes the mean closer to zero, which makes the training easier for the next layer in the network. This is also referred to as centering the data.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p99"> &#13;
   <h4 class=" readable-text-h4">Rectified linear unit </h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p100"> &#13;
   <p>A rectified linear unit (ReLU) is an activation function that defines the positives of an argument. Equation 8.3 shows the ReLU function. Note that the value is 0 even for the negative values, and from 0 the value starts to incline. </p> &#13;
  </div> &#13;
  <div class="browsable-container equation-container" id="p101"> &#13;
   <h5 class=" browsable-container-h5">(8.3)</h5> &#13;
   <p><em>F</em>(<em>x</em>) = max (0, <em>x</em>)</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p102"> &#13;
   <p>It will give the output as <em>x</em> if positive, else 0. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p103"> &#13;
   <p>The ReLU is a simple function and hence less expensive to compute and much faster. It is unbounded and not centered at zero. It can be differentiated at all places except zero. Since the ReLU function is less complex, it is computationally less expensive and, hence, is widely used in the hidden layers to train the networks faster. Figure 8.7 is a graphical representation of a ReLU function.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p104">  &#13;
   <img alt="figure" src="../Images/CH08_F07_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.7</span> A ReLU function. It is one of the favored activation functions in the hidden layers of a neural network. A ReLU is simple to use and less expensive to train.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p105"> &#13;
   <h4 class=" readable-text-h4">Softmax function</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p106"> &#13;
   <p>The softmax function is used in the final layer of the neural network to generate the output from the network. It is an activation function that is useful for multiclass classification problems and forces the neural network to output the sum of 1. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p107"> &#13;
   <p>As an example, say the distinct classes for an image are cars, bikes, or trucks. The softmax function will generate three probabilities for each category. The category that has received the highest probability will be the predicted category.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p108"> &#13;
   <p>There are other activation functions too, like ELU, PeLU, etc., which are beyond the scope of this book. We provide a summary of various activation functions at the end of this chapter.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p109"> &#13;
   <p>We next cover hyperparameters, which are the control levers we have while the network is trained. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p110"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.2.5</span> Hyperparameters</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p111"> &#13;
   <p>During training a network, the algorithm is constantly learning the attributes of the raw input data. At the same time, the network cannot learn everything itself; there are a few parameters for which initial settings must be provided. These are the variables that determine the structure of the neural network and the respective variables that are useful to train the network. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p112"> &#13;
   <p>A few examples of hyperparameters are the number of hidden layers in a network, the number of neurons in each layer, the activation functions used in layers, weight initialization, etc. We have to pick the best values of the hyperparameters. To do so, we select some reasonable values for the hyperparameters, train the network, measure the performance of the network, tweak the hyperparameters and retrain the network, reevaluate and retweak, and so on. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p113"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Hyperparameters are controlled by us, as we input hyperparameters to improve the performance.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p114"> &#13;
   <p>We now move to the next important component in a neural network: optimization functions.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p115"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.2.6</span> Optimization functions</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p116"> &#13;
   <p>In deep learning, optimizers play a critical role. They minimize the loss function by adjusting the model parameters, which are weights and biases. The optimizers facilitate faster convergence and improve the overall performance of the network. Some of the most commonly used optimization functions are discussed next.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p117"> &#13;
   <h4 class=" readable-text-h4">Batch gradient descent, stochastic gradient descent, and mini-batch stochastic gradient descent </h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p118"> &#13;
   <p>In any prediction-based solution, we want to predict as best as we can; or, in other words, we want to reduce the error as much as possible. Error is the difference between the actual values and the predicted values. The purpose of a machine learning solution is to find the optimum value for our functions. We want to decrease the error or maximize the accuracy. Gradient descent can help to achieve this purpose.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p119"> &#13;
   <p>The batch gradient descent technique is an optimization technique used to find the global minima of a function. We proceed in the direction of the steepest descent iteratively, which is defined by the negative of the gradient. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p120"> &#13;
   <p>But batch gradient descent can be slow to run on very large datasets or datasets with a very high number of dimensions. This is due to the fact that one iteration of the gradient descent algorithm predicts for every instance in the training dataset. Hence, it is obvious that it will take a lot of time if we have thousands of records. For such a situation, we have stochastic gradient descent (SGD). </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p121"> &#13;
   <p>In SGD, rather than at the end of the batch of the data, the coefficients are updated for each training instance, and hence it takes less time.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p122"> &#13;
   <p>Figure 8.8 shows the way gradient descent works. Notice how we can progress downward toward the global minimum.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p123">  &#13;
   <img alt="figure" src="../Images/CH08_F08_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.8</span> The concept of gradient descent. It is the mechanism to minimize the loss function.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p124"> &#13;
   <p>Mini-batch gradient descent batches gradient descent and SGD by using small subsets of data. They are called mini-batches. In this fashion, it can balance both speed and accuracy. At the same time, it adds a hyperparameter, and we have to carefully tune the batch size. Generally, it is kept in the power of 2 (32, 64, 128, 256, etc.).</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p125"> &#13;
   <h4 class=" readable-text-h4">Adaptive optimization algorithms</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p126"> &#13;
   <p>Researchers have observed that there is a need for optimization algorithms for more complex tasks like image, text, video, or audio analysis. Hence, adaptive optimization solutions like momentum, Nesterov accelerated gradient (NAG), Adagrad, etc., have been developed. We provide a brief summary of these solutions:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p127"> <em>Momentum</em><em> </em>—This optimizer adds a fraction of the previous gradient to the current gradient. The idea is to give more weight to the most recent update as compared to the previous updates. It accelerates the convergence and achieves better accuracy<span class="aframe-location"/> </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p128"> &#13;
   <img alt="figure" src="../Images/verdhan-ch8-eqs-4x.png"/> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p129"> &#13;
   <p>and hence the weights are updated by <em class="obliqued">θ</em> = <em class="obliqued">θ</em> – <em>V</em>(<em>t</em>).</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p130"> &#13;
   <p>Generally, the value of the momentum term (<em class="obliqued">γ</em>) is set to 0.9. With momentum, the convergence is faster, but at the same time, we must compute one more variable for each update.</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p131"> <em>NAG</em><em> </em>—This is an improvement over momentum. In momentum, if the value becomes too large, the optimizer might miss the local minima. Hence, NAG was developed. It is a look-ahead method wherein the weights are modified to determine the future location. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p132"> &#13;
   <p>Next, we discuss the most widely used optimization algorithms in the industry.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p133"> &#13;
   <h4 class=" readable-text-h4">Learning and learning rate</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p134"> &#13;
   <p>For a network, we take various steps to improve the performance of the solution: learning rate is one of them. The learning rate will define the size of the corrective steps that a model takes to reduce the errors. Learning rate defines the amount by which we should adjust the values of weights of the network with respect to the loss gradients (more on this process later). If we have a higher learning rate, the accuracy will be lower. If we have a very low learning rate, the training time will increase.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p135"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 8.3</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p136"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p137"> Compare and contrast the sigmoid and TANH functions. </li> &#13;
    <li class="readable-text" id="p138"> ReLU is generally used in the output layer of the network. True or False? </li> &#13;
    <li class="readable-text" id="p139"> Gradient descent is an optimization technique. True or False? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p140"> &#13;
   <p>We have examined the main concepts of deep learning. Now let us study how a neural network works. You will learn how the various layers interact with each other and how information is passed from one layer to another. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p141"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.3</span> How does deep learning work in a supervised manner?</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p142"> &#13;
   <p>We have covered the major components of a neural network. It is the time for all the pieces to come together and orchestrate the entire learning process. The training of a neural network is quite a complex process and can be examined in a step-by-step fashion.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p143"> &#13;
   <p>You might be wondering what is meant by “learning” of a neural network. Learning is a process to find the best and most optimized values for weights and bias for all the layers of the network so that we can achieve the best accuracy. As deep neural networks can have practically infinite possibilities for weights and bias terms, we have to find the optimum value for all the parameters. This seems like a herculean task considering that changing one value affects the other values, and indeed, it is a process where the various parameters of the networks are changing. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p144"> &#13;
   <p>Recall in the first chapter we covered the basics of supervised learning. We will refresh that understanding here. The reason is to ensure that you are fully able to appreciate the process of training the neural network. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p145"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.3.1</span> Supervised learning algorithms</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p146"> &#13;
   <p>Supervised learning algorithms have a “guidance” or “supervision” to direct toward the business goal of making predictions for the future. Formally put, supervised models are statistical models that use both the input data and the desired output to predict the future. The output is the value we wish to predict and is referred to as the <em>target variable,</em> and the data used to make that prediction is called the <em>training data</em>. The target variable is sometimes referred to as the <em>label</em>. The various attributes or variables present in the data are called <em>independent variables</em>. Each of the historical data points or <em>training examples</em> contain these independent variables and corresponding target variables. Supervised learning algorithms make a prediction for the unseen future data. The accuracy of the solution depends on the training done and patterns learned from the labeled historical data. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p147"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Most deep learning solutions are based on supervised learning. Unsupervised deep learning is rapidly gaining traction, however, as unlabeled datasets are far more abundant than labeled ones.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p148"> &#13;
   <p>Supervised learning problems are used in demand prediction, credit card fraud detection, customer churn prediction, premium estimation, etc. They are heavily used across retail, telecom, banking and finance, aviation, insurance, and other fields. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p149"> &#13;
   <p>We have now refreshed the concepts of supervised learning. We now move on to the first step in the training of the neural network: feed-forward propagation.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p150"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.3.2</span> Step 1: Feed-forward propagation</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p151"> &#13;
   <p>Let us start the process that occurs in a neural network (see figure 8.9). This is the basic skeleton of a network we have created to explain the process. Let’s say we have some input data points and the input data layer, which will consume the input data. The information flows from the input layer to the data transformation layers (hidden layers). In the hidden layers, the data is processed using the activation functions and based on the weights and bias terms. Then a prediction is made on the dataset. This is called <em>feed-forward propagation,</em> as during this process, the input variables are calculated in a sequence from the input layer to the output layer.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p152">  &#13;
   <img alt="figure" src="../Images/CH08_F09_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.9</span> The basic skeleton of a neural network training process. We have the input layers and data transformation layers. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p153"> &#13;
   <p>For example, say we wish to create a solution that can identify the faces of people. In this case, we will have the training data, which is different images of people’s faces from various angles, and a target variable, which is the name of the person.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p154"> &#13;
   <p>This training dataset can be fed to the algorithm. The algorithm will then understand the attributes of various faces or, in other words, <em>learn </em>the various attributes. Based on the training done, the algorithm can then make a prediction on the faces. The prediction will be a probability score if the face belongs to Mr. X. If the probability is high enough, we can safely say that the face belongs to Mr. X. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p155"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.3.3</span> Step 2: Adding the loss function</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p156"> &#13;
   <p>The output is generated in step 1. Now we have to gauge the accuracy of this network. We want our network to have the best possible accuracy in identifying the faces. Using the prediction made by the algorithm, we will control and improve the accuracy of the network.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p157"> &#13;
   <p>Accuracy measurement in the network can be achieved by the loss function, also called the <em>objective function</em>. The loss function compares the actual values and the predicted values. The loss function computes the difference score and hence is able to measure how well the network has done and what the error rates are. Let’s update the diagram we created in step 1 by adding a loss function and corresponding loss score, used to measure the accuracy of the network, as shown in figure 8.10.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p158">  &#13;
   <img alt="figure" src="../Images/CH08_F10_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.10</span> A loss function has been added to measure the accuracy. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p159"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.3.4</span> Step 3: Calculating the error</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p160"> &#13;
   <p>We generated the predictions in step 1 of the network. In step 2, we compared the output with the actual values to get the error in prediction. The objective of our solution is to minimize this error, which is the same as maximizing the accuracy.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p161"> &#13;
   <p>To constantly lower the error, the loss score (Predictions – Actual) is then used as feedback to adjust the value of the weights. This task is done by the backpropagation algorithm. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p162"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.4</span> Backpropagation </h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p163"> &#13;
   <p>In step 3 of the last section, we said we use an optimizer to constantly update the weights to reduce the error. While the learning rate defines the size of the corrective steps to reduce the error, backpropagation is used to adjust the connection weights. These weights are updated backward based on the error. Following this, the errors are recalculated, the gradient descent is calculated, and the respective weights are adjusted. Hence, backpropagation is sometimes called the central algorithm in deep learning. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p164"> &#13;
   <p>Backpropagation was originally suggested in the 1970s. Then, in 1986, David Rumelhartm, Geoffrey Hinton, and Ronald Williams’s paper received a lot of appreciation. Nowadays, backpropagation is the backbone of deep learning solutions. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p165"> &#13;
   <p>Figure 8.11 shows the process for backpropagation, where the information flows from the output layer back to the hidden layers. Note that the flow of information is backward as compared to forward propagation, where the information flows from left to right. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p166">  &#13;
   <img alt="figure" src="../Images/CH08_F11_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.11</span> Backpropagation as a process: the information flows from the final layers to the initial layers</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p167"> &#13;
   <p>First, we describe the process at a very high level. Remember that in step 1, at the start of the training process, some random values were assigned to the weights. Using these random values, an initial output is generated. Since this is the first attempt, the output received can be quite different from the real values and the loss score is accordingly very high. But this is going to improve. While training the neural network, the weights (and biases) are adjusted a little in the correct direction, and subsequently, the loss score decreases. We iterate this training loop many times, and it results in the optimum weight values that minimize the loss function. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p168"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Backpropagation allows us to iteratively reduce the error during the network training process.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p169"> &#13;
   <p>The following section is mathematically heavy. If you are not keen to understand the mathematics behind the process, you can skip it.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p170"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.4.1</span> The mathematics behind backpropagation</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p171"> &#13;
   <p>When we train a neural network, we calculate a loss function. The loss function tells us how different the predictions from the actual values are. Backpropagation calculates the gradient of the loss function with respect to each of the weights. With this information, each weight can be updated individually over iterations, which reduces the loss gradually. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p172"> &#13;
   <p>In backpropagation, the gradient is calculated backward—that is, from the last layer of the network through the hidden layers to the very first layer. The gradients of all the layers are combined using the calculus chain rule to get the gradient of any particular layer.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p173"> &#13;
   <p>We go into more details of the process next. First, let’s denote a few mathematical symbols:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p174"> <em>h</em><sup>(</sup><sup><em>i</em></sup><sup>)</sup>—output of the hidden layer <em>i</em> </li> &#13;
   <li class="readable-text" id="p175"> <em>g</em><sup>(</sup><sup><em>i</em></sup><sup>)</sup>—activation function of hidden layer <em>i</em> </li> &#13;
   <li class="readable-text" id="p176"> <em>w</em><sup>(</sup><sup><em>i</em></sup><sup>)</sup>—hidden weights matrix in the layer <em>i</em> </li> &#13;
   <li class="readable-text" id="p177"> <em>b</em><sup>(</sup><sup><em>i</em></sup><sup>)</sup>—bias in layer <em>i</em> </li> &#13;
   <li class="readable-text" id="p178"> <em>x</em>—input vector </li> &#13;
   <li class="readable-text" id="p179"> <em>N</em>—total number of layers in the network </li> &#13;
   <li class="readable-text" id="p180"> <em>W</em><sup>(</sup><sup><em>i</em></sup><sup>)</sup><sub><em>jk</em></sub>—weight of the network from node <em>j</em> in layer (<em>i</em>–1) to node <em>k</em> in layer <em>i</em> </li> &#13;
   <li class="readable-text" id="p181"> <em>δ</em><em>A</em>/<em>δ</em><em>B</em>—partial derivative of <em>A</em> with respect to <em>B</em> </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p182"> &#13;
   <p>During the training of the network, the input <em>x</em> is fed to the network, and it passes through the layers to generate an output <em>y</em>̂. The expected output is <em>y</em>. Hence, the cost function or the loss function to compare <em>y</em> and <em>y</em>̂ is <em>C</em>(<em>y</em>, <em>y</em>̂). Also, the output for any hidden layer of the network can be represented as equation 8.4</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p183">  &#13;
   <img alt="figure" src="../Images/verdhan-ch8-eqs-6x.png"/> &#13;
   <h5 class=" figure-container-h5">(8.4)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p184"> &#13;
   <p>where <em>i</em> (index) can be any layer in the network.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p185"> &#13;
   <p>The final layer’s output is</p> &#13;
  </div> &#13;
  <div class="browsable-container equation-container" id="p186"> &#13;
   <h5 class=" browsable-container-h5">(8.5)</h5> &#13;
   <p><em> y</em>(<em>x</em>) = <em>W</em><sup>(</sup><sup><em>N</em></sup><sup>)</sup><sup><em>T</em></sup> <em>h</em><sup>(</sup><sup><em>N</em></sup><sup>–</sup><sup>1)</sup> + <em>b</em><sup>(</sup><sup><em>N</em></sup><sup>)</sup></p> &#13;
  </div> &#13;
  <div class="readable-text" id="p187"> &#13;
   <p>During the training of the network, we adjust the network’s weights so that <em>C</em> is reduced. Hence, we calculate the derivative of <em>C</em> with respect to every weight in the network. The following is the derivative of <em>C</em> with respect to every weight in the network:<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p188"> &#13;
   <img alt="figure" src="../Images/verdhan-ch8-eqs-8x.png"/> &#13;
  </div> &#13;
  <div class="readable-text" id="p189"> &#13;
   <p>Now we know that a neural network has many layers. The backpropagation algorithm starts at calculating the derivatives at the last layer of the network, which is the N<sup>th</sup> layer. Then these derivatives are fed backward. So the derivatives at the <em>N</em><sup>th</sup> layers will be fed to the (<em>N</em> – 1) layer of the network and so on. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p190"> &#13;
   <p>Each component of the derivatives of <em>C</em> is calculated individually using the calculus chain rule. As per the chain rule, for a function <em>c</em> depending on <em>b</em>, where <em>b</em> depends on <em>a</em>, the derivative of <em>c</em> with respect to <em>a</em> can be written as equation 8.6:</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p191">  &#13;
   <img alt="figure" src="../Images/verdhan-ch8-eqs-9x.png"/> &#13;
   <h5 class=" figure-container-h5">(8.6)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p192"> &#13;
   <p>Hence, in backpropagation the derivatives of the layer <em>N</em> are used in the layer (<em>N</em> – 1) so that they are saved and again used in the (<em>N</em> – 2) layer. We start with the last layer of the network, through all the layers to the first layer, and each time, we use the derivatives of the last calculations made to get the derivatives of the current layers. Hence, backpropagation turns out to be extremely efficient compared to a normal approach where we would have calculated each weight in the network individually.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p193"> &#13;
   <p>Once we have calculated the gradients, we update all the weights in the network. The objective is to minimize the cost function. We have already studied methods like gradient descent in the last section. We now continue to the next step in the neural network training process.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p194"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.4.2</span> Step 4: Optimization</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p195"> &#13;
   <p>Backpropagation allows us to optimize our network and achieve the best accuracy (see figure 8.12). Notice the optimizer, which provides regular and continuous feedback to reach the best solution.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p196">  &#13;
   <img alt="figure" src="../Images/CH08_F12_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.12</span> Optimization is the process to minimize the loss function.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p197"> &#13;
   <p>Once we have achieved the best values of the weights and biases for our network, we say that our network is trained. We can now use it to make predictions on an unseen dataset that has not been used for training the network.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p198"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.5</span> How deep learning works in an unsupervised manner</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p199"> &#13;
   <p>We know that unsupervised learning solutions work on unlabeled datasets; thus, for deep learning in unsupervised settings, the training dataset is unlabeled.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p200"> &#13;
   <p>As compared to supervised datasets where we have tags, unsupervised methods have to self-organize themselves to get densities, probabilities’ distributions, preferences, and groupings. We can solve a similar problem using supervised and unsupervised methods. For example, a supervised deep learning method can be used to identify dogs versus cats while an unsupervised deep learning method might be used to cluster the pictures of dogs and cats into different groups. In machine learning, a lot of solutions that were initially conceived as supervised learning ones, over a period of time, employed unsupervised learning methods to enrich the data and hence improve the supervised learning solution.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p201"> &#13;
   <p>During the learning phase in unsupervised deep learning, it is expected that the network will mimic the data and then improve itself based on the errors. In the supervised learning algorithm, other methods play the same part as the backpropagation algorithm. These include, among others, </p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p202"> Boltzmann learning rule </li> &#13;
   <li class="readable-text" id="p203"> Contrastive divergence </li> &#13;
   <li class="readable-text" id="p204"> Maximum likelihood </li> &#13;
   <li class="readable-text" id="p205"> Hopfield learning rule </li> &#13;
   <li class="readable-text" id="p206"> GAN </li> &#13;
   <li class="readable-text" id="p207"> Deep belief network (DBN) </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p208"> &#13;
   <p>In this book, we cover autoencoders and GAN in depth in separate chapters. The rest of the methods are covered in this chapter. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p209"> &#13;
   <p>Next, we study the two most widely used types of neural networks in supervised learning settings: the convolutional neural network (CNN) and the recurrent neural network (RNN). </p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p210"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 8.4</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p211"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p212"> Write in a simple form the major steps in a backpropagation technique. </li> &#13;
    <li class="readable-text" id="p213"> Backpropagation is preferred in unsupervised learning. True or False? </li> &#13;
    <li class="readable-text" id="p214"> The objective of deep learning is to maximize the loss function. True or False? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p215"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.6</span> Convolutional neural networks</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p216"> &#13;
   <p>CNNs are a class of deep learning models that are primarily used for image and video processing tasks. They have become a powerful tool in the field of computer vision due to their ability to automatically detect and learn the pattern from raw images and, hence, are used for several use cases across multiple domains and functions. We provide only a brief overview, as there can be an entire book on different types of CNN solutions.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p217"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.6.1</span> Key concepts of CNN</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p218"> &#13;
   <p>The following are the key concepts of CNN:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p219"> <em>Input layer</em><em> </em>—The input to the CNN is generally a tensor representing an image. As we know, an image is made up of pixels, and each pixel is made up of RGB channels. An image is represented by a 3D matrix, which is a width × height channel.  </li> &#13;
   <li class="readable-text" id="p220"> <em>Convolution layer</em><em> </em>—This is the core building layer of a CNN. It applies a filter to the input data, which scans over the image to detect patterns like lines, curves, texture, edges, etc. The filter size is generally small and usually 3 × 3 or 5 × 5. As the kernel slides over the input, it performs an element-wise multiplication and sum, creating a feature map. Multiple filters can be applied to learn different features, generating multiple feature maps. The entire process is illustrated in figure 8.13.<span class="aframe-location"/> </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p221">  &#13;
   <img alt="figure" src="../Images/CH08_F13_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.13</span> CNN process. The original data is 6 × 6, and the filter applied is 3 × 3, which results in a 4 × 4 output.</h5>&#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p222"> <em>ReLU activation function</em><em> </em>—This is applied to add nonlinearity. It helps the network to understand and model more complex and difficult patterns that are present in the data. </li> &#13;
   <li class="readable-text" id="p223"> <em>Polling layer</em><em> </em>—This is used to reduce the spatial dimensions of images while preserving the most significant details. The most common type of pulling is called max pulling. It takes the maximum value from a region of input. The major function of the pooling layer is to reduce the computation load and also reduce overfitting by providing a form of translation in variance. </li> &#13;
   <li class="readable-text" id="p224"> <em>Output</em><em> </em>—After we have created several convolutional and pooling layers, we receive the output. It is generally flattened into a 1D vector and the output is then passed to the fully connected layer. The main task of the fully connected layer is to perform high-level classification of the image based on the features extracted by the previous layers. </li> &#13;
   <li class="readable-text" id="p225"> <em>Output layer</em><em> </em>—If the solution is for classification of data points, the output layer would contain a function like softmax. The softmax function gives respective probabilities for different classes. For example, if you are trying to predict that a given picture is a cat or a dog, the softmax function will give the probability of the picture being a dog or a cat. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p226"> &#13;
   <p>In CNNs, the same filter is applied across different regions of the image. Thus the number of parameters is reduced as compared to a traditional fully connected network. Each neuron in the convolutional layer is connected only to a small region of the input, and so the complexity of the network is also reduced. The network also automatically trains and learns to detect low-level patterns. An example of a low-level pattern is edges. The network subsequently progresses to learn more complex patterns like shapes in the deeper layers.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p227"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.6.2</span> Use of CNN</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p228"> &#13;
   <p>Call networks are fundamental and foundational to the modern-day competition solutions. They are heavily used for image classification, image processing, speech recognition, developing computer board games, and various other video processing solutions. Many solutions are developed using CNN—for example, automatic detection of vehicle license plates, detection of cancerous cells from scans, detection of broken bones from x-rays, facial recognition solutions, automatic entry handwriting, recognition solutions, and many other solutions that are having an amazing affect across our lives.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p229"> &#13;
   <p>There are quite a few CNN architectures available, like Inception, ResNet, LeNet, VGG-16, etc., that are useful for creating computer vision solutions. We now move on to the second common type of neural network: RNN.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p230"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.7</span> Recurrent neural networks</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p231"> &#13;
   <p>RNNs are quite a popular class of networks that are designed to recognize patterns in a sequence of data—for example, time service data or videos, natural languages, or any other kind of data with this sequence of information. Here RNNs are very useful. The most significant feature of RNNs is their ability to maintain a memory about the previous input, which they capture using temporal dependencies and the order in the dataset. This augments their capability to recognize patterns in the sequential datasets, and hence RNN has been found to be a parting solution in multiple domains.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p232"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.7.1</span> Key concepts of RNN</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p233"> &#13;
   <p>RNNs are especially designed for sequential datasets, and here the order of the input display plays a pivotal role. Hence, RNNs are the go-to solution for sequential data handling. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p234"> &#13;
   <p>Unlike a regular neural network, which is also known as a feed-forward neural network, RNNs have recurrent connections. This means that the output from one time step is fed back as the input to the next time step. This information is persistent across the sequence. At the same time, the same weight is used across different time steps. This makes them very efficient in terms of the number of parameters, as the same network can be applied to every time step of the input sequence.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p235"> &#13;
   <p>RNNs work in the following fashion:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p236"> The input data is processed sequentially. At each time step <em>t</em>, the network receives an input <em>x</em><sub><em>t</em></sub>, which is then combined with a hidden state <em>h</em><sub><em>t</em></sub><sub>–1</sub>. This hidden state is the output from the previous time step and serves as a memory that carries information from one time step to the next time step. </li> &#13;
   <li class="readable-text" id="p237"> The hidden state <em>h</em><sub><em>t</em></sub> is then updated using a nonlinear function:<span class="aframe-location"/> </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p238"> &#13;
   <img alt="figure" src="../Images/verdhan-ch8-eqs-10x.png"/> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p239"> The final output at each of the time steps can be calculated and used either for each individual time step or only at the final time step. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p240"> &#13;
   <p>Figure 8.14 illustrates the RNN process.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p241">  &#13;
   <img alt="figure" src="../Images/CH08_F14_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.14</span> The RNN process. RNNs have internal memory, which allows them to use information from the previous inputs to influence the current input and outputs.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p242"> &#13;
   <p>The most basic version of an RNN is a simple recurrent network, but it struggles with a long-term dependency because the gradient can either vanish or explode, making it hard for the network to remember information from far back in the sequence; hence, it cannot be used for a solution like a chatbot. Long short-term memory (LSTM) is much more useful here. LSTM is a special type of network designed to mitigate the vanishing gradient problem and handle long-term dependencies better than plain vanilla RNNs. They achieve this feat by introducing gates. There are three types of gates: input, forget, and output gates. These gates regulate the flow of information through the network and allow it to maintain important information over longer periods of time. Gated recurrent units are another type of RNN, but LSTM and gated recurrent units are beyond the scope of this book.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p243"> &#13;
   <p>RNNs are very powerful for processing sequences, and their ability to model time dependencies makes them indispensable in the fields of natural language processing and time-series analysis. Their use has been pathbreaking for many innovative solutions—for example, predicting the next word in a sentence; translating text from one language to another; processing sequences of video frames to understand behaviors over time; modeling temporal dependencies like audio signals, which can be used to recognize speech patterns over time; and many more. RNNs are the power engines behind GenAI solutions.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p244"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.8</span> Boltzmann learning rule</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p245"> &#13;
   <p>The Boltzmann learning rule is an unsupervised learning rule used in neural networks. It is based on the principle of statistical mechanics of physical systems. It is seldom used in the context of Boltzmann machines. It adjusts the weights of a neural network with an objective to minimize the energy of the system, thereby ensuring the network reaches a stable state.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p246"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.8.1</span> Concepts of the Boltzmann learning rule</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p247"> &#13;
   <p>The following are the key concepts of the Boltzmann learning rule:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p248"> It is a type of probabilistic RNN where neurons are connected in a fully connected graph. </li> &#13;
   <li class="readable-text" id="p249"> The neurons in the Boltzmann machine are stochastic units that fire as per a probability distribution. Thus we can use the Boltzmann learning rule for dimensionality reduction, pattern recognition, feature extraction, and optimization tasks. </li> &#13;
   <li class="readable-text" id="p250"> A Boltzmann machine has an energy function <em>E</em>(<em>v</em>,<em>h</em>) where <em>v</em> is the input visible unit while <em>h</em> is the hidden unit. The energy function determines the cost of a given state of the network. During the training of the network, we aim to adjust the weights in such a manner so that the energy of the system is minimized. </li> &#13;
   <li class="readable-text" id="p251"> The network models the probability of a particular state (<em>v</em>,<em>h</em>) using a Boltzmann distribution. It depends on the energy of the state, which is given by equation 8.7: </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p252">  &#13;
   <img alt="figure" src="../Images/verdhan-ch8-eqs-11x.png"/> &#13;
   <h5 class=" figure-container-h5">(8.7)</h5>&#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p253"> &#13;
   <p>Here, <em>Z</em> is the partition function, which ensures that the sum of probabilities = 1.</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p254"> The rule seeks to adjust the weights to keep on decreasing the energy of the system during the training of the network, and it happens over time. The weights are updated by a rule derived from gradient of the energy function with respect to the weights. The weight update rule is given in equation 8.8: </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p255">  &#13;
   <img alt="figure" src="../Images/verdhan-ch8-eqs-12x.png"/> &#13;
   <h5 class=" figure-container-h5">(8.8)</h5>&#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p256"> &#13;
   <p>Here, <em class="obliqued">h</em> is the learning rate, and (<em>v</em><sub><em>i</em></sub><em>h</em><sub><em>j</em></sub>)<sub>data</sub> is the correction between the visible unit <em>v</em><sub><em>i</em></sub> and hidden unit <em>h</em><sub><em>j</em></sub>. It is computed from the data distribution. It represents how often they are active together in the hidden unit. (<em>v</em><sub><em>i</em></sub><em>h</em><sub><em>j</em></sub>)<sub>model</sub> is the correction computed from the model distribution. It represents how often the visible unit <em>v</em><sub><em>i</em></sub> and hidden unit <em>h</em><sub><em>j</em></sub> are active together in the state generated by the network.</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p257"> &#13;
   <p>During the training of the model, a learning rule is followed, which is to make the data distribution match the model distribution. Hence, it reduces the energy of the system and thereby increases the overall performance. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p258"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.8.2</span> Key points</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p259"> &#13;
   <p>There are certain key points we should bear in mind. Energy-based models like the Boltzmann machine use the Boltzmann learning rule to minimize an energy function by adjusting the network’s weights:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p260"> The network strives to model the probability distribution over its inputs. The core objective here is to associate the higher energy with less likely configurations. Similarly, the lower energy is associated with more like configurations. </li> &#13;
   <li class="readable-text" id="p261"> Boltzmann learning is an unsupervised and probabilistic method. It works on the concept of contrasting the model distribution and data distribution. </li> &#13;
   <li class="readable-text" id="p262"> The rule is computationally expensive in its basic form; hence, to increase the training speed, sometimes we utilize methods like contrastive divergence. We cover contrastive divergence in the next section. </li> &#13;
   <li class="readable-text" id="p263"> The Boltzmann learning rule is primarily used for unsupervised learning tasks such as dimensionality reduction, feature extraction, and generative modeling. </li> &#13;
   <li class="readable-text" id="p264"> The model training is sometimes slower than expected. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p265"> &#13;
   <p>In summary, the Boltzmann learning rule is a probabilistic approach to training neural networks by adjusting weights based on minimizing an energy function, and it provides a foundation for generative models like Boltzmann machines. However, due to computational challenges, approximations such as contrastive divergence are often used to make it practical for real-world applications.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p266"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.9</span> Deep belief networks</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p267"> &#13;
   <p>A DBN is a type of GAN made up of multiple layers of stochastic, binary latent variables (hidden units), where each layer is a restricted Boltzmann machine (RBM) or a variant of it. DBNs were popularized by Geoffrey Hinton (who was awarded the Nobel Prize in Physics in 2024, shared with John Hopfield) and his collaborators in the mid-2000s for pretraining deep networks in an unsupervised way. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p268"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.9.1</span> Key points of DBN</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p269"> &#13;
   <p>The key points of a DBN are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text buletless-item" id="p270"> RBM &#13;
    <ul> &#13;
     <li> A DBN consists of several layers of RBMs. A RBM contains a visible layer and a hidden layer. The visible layer represents the observed data while the hidden layer captures the hidden features.  </li> &#13;
     <li> Each DBN is trained independently with an objective to model the underlying structure of the data. </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text" id="p271"> The objective of the training in DBN is to optimize the log-likelihood of the data under the network’s generative model. For each layer, the contrastive divergence algorithm is used to approximate the gradient of the log-likelihood with respect to the weights. This allows the network to learn a good set of weights for each layer. </li> &#13;
   <li class="readable-text" id="p272"> The contrastive divergence algorithm is a stochastic approximation method used to estimate the gradient of the log-likelihood of the model. The algorithm starts with a sample from the visible layer and then performs Gibbs sampling to update the hidden layer and visible layer iteratively. Contrastive divergence ensures that the network learns to model the input data distribution efficiently. </li> &#13;
   <li class="readable-text buletless-item" id="p273"> Layer-based pretraining: &#13;
    <ul> &#13;
     <li> DBNs are typically trained in a layer-wise manner, where each layer is pretrained as an RBM. The first RBM has an objective to learn to capture low-level features from the data.  </li> &#13;
     <li> Based on this knowledge, each subsequent RBM then learns increasingly complex, abstract features from the representations learned by the previous layers. In this manner, the cycle continues. </li> &#13;
     <li> This phase involves training each RBM individually using contrastive divergence. </li> &#13;
     <li> This process tunes the weights to capture relevant patterns and features in the input data, without the need for labeled data. </li> &#13;
     <li> Since each layer learns features at increasing levels of abstraction and complexity, it makes the overall solution good enough for complex tasks like image or speech recognition. </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text buletless-item" id="p274"> Supervised fine-tuning: &#13;
    <ul> &#13;
     <li> Once the pretraining is done, the entire network is fine-tuned. It is done in a supervised fashion using methods like backpropagation or a labeled dataset with an objective to optimize the network. </li> &#13;
     <li> The supervised system adjusts the network weights to minimize the prediction error such as what is done in classification or regression tasks. </li> &#13;
     <li> The unsupervised pretraining phase helps initialize the weights in such a way that the network is less likely to overfit during supervised fine-tuning, as it starts with a better understanding of the data. </li> &#13;
     <li> They are computationally expensive and time-consuming, particularly when dealing with large datasets or deep architectures. </li> &#13;
     <li> Pretraining using RBMs is useful, but fine-tuning the entire DBN can sometimes be difficult, especially if we are dealing with a very deep neural network. It may necessitate meticulous hyperparameter training and lots of labeled datasets. </li> &#13;
     <li> Similar to other deep learning architectures, DBNs are also prone to the vanishing gradient problem, where gradients diminish as they are propagated backward through many layers. This further complicates the entire training process. </li> &#13;
    </ul> </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p275"> &#13;
   <p>DBNs are typically used for unsupervised learning, dimensionality reduction, and feature learning, but they can also be fine-tuned for supervised tasks such as classification. DBNs are used to improve the performance of speech recognition systems by learning representations of sound features that are invariant to noise and other distortions. As generative models, DBNs can be used to create new data instances that resemble the training data. For example, DBNs have been used in generative art, where new images are created that resemble a set of input images.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p276"> &#13;
   <p>DBNs are a significant milestone in the development of deep learning techniques. They combine the strengths of generative models like RBMs with deep learning principles to create a powerful method for learning complex representations of data. While newer architectures have emerged and gained prominence, DBNs remain a key historical and theoretical component of modern AI, influencing the development of many advanced models. By utilizing unsupervised learning, DBNs can be highly effective for tasks like dimensionality reduction, generative modeling, and classification. However, challenges related to training complexity and fine-tuning remain significant hurdles for widespread adoption.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p277"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.10</span> Popular deep learning libraries</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p278"> &#13;
   <p>Over the last few chapters, we have used a lot of libraries and packages for implementing solutions. There are quite a few libraries available in the industry for deep learning. These packages expedite the solution building and reduce the efforts as most of the heavy lifting is done by these libraries.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p279"> &#13;
   <p>The most popular deep learning libraries are</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p280"> <em>TensorFlow (TF)</em><em> </em><em>—</em>Developed by Google, this is arguably one of the most popular and widely used deep learning frameworks. It was launched in 2015 and since has been used by a number of businesses and brands across the globe. </li> &#13;
  </ul> &#13;
  <div class="readable-text list-body-item" id="p281"> &#13;
   <p>Python is mostly used for TF but C++, Java, C#, Javascript, and Julia can also be used. You have to install the TF library on your system and import the library.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p282"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Go to <a href="http://www.tensorflow.org/install">www.tensorflow.org/install</a> and follow the instructions to install TF. </p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p283"> &#13;
   <p>TF is one of the most popular libraries and can work on mobile devices like iOS and Android. </p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p284"> <em>Keras</em>—Keras is a mature API-driven solution and quite easy to use. It is one of the best choices for starters and is among the best for prototyping simple concepts in an easy and fast manner. Keras was initially released in 2015 and is one of the most recommended libraries. </li> &#13;
  </ul> &#13;
  <div class="readable-text print-book-callout" id="p285"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Go to <a href="https://keras.io">https://keras.io</a> and follow the instructions to install Keras. Tf.keras can be used as an API. </p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p286"> &#13;
   <p>Serialization/deserialization APIs, call-backs, and data streaming using Python generators are very mature. Massive models in Keras are reduced to single-line functions, which makes it a less configurable environment and hence very convenient and easy to use.</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p287"> <em>PyTorch</em><em> </em>—Facebook’s brainchild PyTorch was released in 2016 and is another popular framework. PyTorch operates with dynamically updated graphs and allows data parallelism and distributed learning models. There are debuggers like pdb or PyCharm available in PyTorch. For small projects and prototyping, PyTorch can be a good choice. </li> &#13;
   <li class="readable-text" id="p288"> <em>Sonnet</em><em> </em>—DeepMind’s Sonnet is developed using and on top of TF. Sonnet is designed for complex neural network applications and architectures. It works by creating primary Python objects corresponding to a particular part of the neural network. Then these Python objects are independently connected to the computational TF graph. Because of this separation (creating Python objects and associating them to a graph), the design is simplified.  </li> &#13;
  </ul> &#13;
  <div class="readable-text print-book-callout" id="p289"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Having high-level object-oriented libraries is very helpful, as the abstraction is allowed when we develop machine learning solutions. </p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p290"> <em>MXNet</em><em> </em>—Apache’s MXNet is a highly scalable deep learning tool that is easy to use and has detailed documentation. A large number of languages like C ++, Python, R, Julia, JavaScript, Scala, Go, and Perl are supported by MXNet.  </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p291"> &#13;
   <p>There are other frameworks too, like Swift, Gluon, Chainer, DL4J, etc.; however, we only discuss the popular ones here. We now examine a short code in TF and Keras. It is just to test that you have installed these libraries correctly. You can learn more about TF at <a href="https://www.tensorflow.org">https://www.tensorflow.org</a> and Keras at <a href="https://keras.io">https://keras.io</a>.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p292"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">8.10.1</span> Python code for Keras and TF</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p293"> &#13;
   <p>We implement a very simple code in TF. We simply import the TF library and print “hello”. We also check the version of TF: </p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p294"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import tensorflow as tf&#13;
hello = tf.constant('Hello, TensorFlow!')&#13;
sess = tf.Session()&#13;
print(sess.run(hello))&#13;
print("TensorFlow version:", tf.__version__)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p295"> &#13;
   <p>If this code runs for you and prints the version of TF, it means that you have installed <code>tensorflow</code> correctly:</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p296"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from tensorflow import keras&#13;
from keras import models</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p297"> &#13;
   <p>If this code runs for you and prints the version of Keras, it means that you have installed <code>keras</code> correctly.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p298"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.11</span> Concluding thoughts</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p299"> &#13;
   <p>Deep learning is changing the world we live in. It is enabling us to train and create really complex solutions that were a mere thought earlier. The effect of deep learning can be witnessed across multiple domains and industries. Perhaps there are no industries that have been left unaffected by the marvels of deep learning. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p300"> &#13;
   <p>Deep learning is one of the most-sought-after fields for research and development. Every year, many journals and papers are published on deep learning. Researchers across prominent institutions and universities (like Oxford, Stanford, etc.) of the world are engrossed in finding improved neural network architectures. At the same time, professionals and engineers in reputed organizations (like Google, Facebook, etc.) are working hard to create sophisticated architectures to improve performance. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p301"> &#13;
   <p>Deep learning is making our systems and machines able to solve problems typically assumed to be in the realm of humans only. We have improved the clinical trials process for the pharma sector, fraud detection software, automatic speech detection systems, and various image recognition solutions; and created more robust natural language processing solutions, targeted marketing solutions that improve customer relationship management and recommendation systems, better safety processes, and so on. The list is quite long and growing day by day.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p302"> &#13;
   <p>At the same time, there are still a few challenges. The expectations from deep learning continue to increase. Deep learning is not a silver bullet or a magic wand to resolve all problems. It is surely one of the more sophisticated solutions, but it is certainly not the 100% solution to all business problems. The dataset we need to feed the algorithms is not always available. There is a dearth of good-quality datasets that are representative of business problems. Often, big organizations like Google, Meta, or Amazon can afford to collect such massive datasets. But many times we do find a lot of quality problems in the data. Having the processing power to train these complex algorithms is also a challenge. With the advent of cloud computing, though, this problem has been resolved to a certain extent. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p303"> &#13;
   <p>In this chapter, we explored the basics of neural networks and deep learning. We covered the details around neurons, activation function, different layers of a network, and loss function. We also covered in detail the backpropagation algorithm—the central algorithm used to train a supervised deep learning solution. Then we briefly went through unsupervised deep learning algorithms. We will cover these unsupervised deep learning solutions in greater detail in the later chapters. Figure 8.15 shows the major activation functions.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p304">  &#13;
   <img alt="figure" src="../Images/CH08_F15_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 8.15</span> Major activation functions at a glance (Source: towardsdatascience) </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p305"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">8.12</span> Practical next steps and suggested readings</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p306"> &#13;
   <p>The following provides suggestions for what to do next and offers some helpful reading:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p307"> The book <em>Deep Learning with Python</em> by François Chollet is one of the best resources to clarify the concepts of deep learning. It covers all the concepts of deep learning and neural networks and is written by the creator of Keras. </li> &#13;
   <li class="readable-text buletless-item" id="p308"> Read the following research papers: &#13;
    <ul> &#13;
     <li> Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the Knowledge in a Neural Network. <a href="https://arxiv.org/pdf/1503.02531.pdf">https://arxiv.org/pdf/1503.02531.pdf</a> </li> &#13;
     <li> Srivastava, R., Greff, K., and Schmidhuber, J. (2015). Training Very Deep Networks. <a href="https://arxiv.org/pdf/1507.06228">https://arxiv.org/pdf/1507.06228</a> </li> &#13;
     <li> Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. <a href="https://arxiv.org/abs/1310.4546">https://arxiv.org/abs/1310.4546</a> </li> &#13;
     <li> Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. (2014). Generative Adversarial Networks. <a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a> </li> &#13;
     <li> He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. <a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a> </li> &#13;
    </ul> </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p309"> &#13;
   <h2 class=" readable-text-h2">Summary</h2> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p310"> Deep learning is an advanced form of machine learning based on neural networks, and it’s particularly effective with unstructured data like text, images, audio, and video. </li> &#13;
   <li class="readable-text buletless-item" id="p311"> Deep learning finds applications across various sectors, such as &#13;
    <ul> &#13;
     <li> <em>The medical field and pharmaceuticals</em><em> </em>—Used for diagnosing medical conditions and expediting drug development </li> &#13;
     <li> <em>Banking and finance</em><em> </em>—Detects fraud and distinguishes fake signatures </li> &#13;
     <li> <em>The automobile sector</em><em> </em>—Powers autonomous driving by recognizing traffic elements </li> &#13;
     <li> <em>Speech and image recognition</em><em> </em>—Enables technologies like Siri and image-based systems for medical diagnostics and security </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text buletless-item" id="p312"> Key concepts for neural networks include &#13;
    <ul> &#13;
     <li> <em>Artificial neurons (perceptrons)</em><em> </em>—Simplified models of biological neurons. Weights and biases play crucial roles in the function of a perceptron. </li> &#13;
     <li> <em>Layers</em><em> </em>—Networks are structured with input, hidden, and output layers. Hidden layers extract and learn features critical for decision-making. </li> &#13;
     <li> <em>Activation functions</em><em> </em>—Critical for neural network performance and include sigmoid, TANH, LeLU, and softmax. </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text" id="p313"> Training neural networks involves processes like feed-forward propagation, calculating loss, and employing backpropagation for weight adjustments to maximize prediction accuracy. </li> &#13;
   <li class="readable-text" id="p314"> While unsupervised learning relies on unlabeled data, techniques like Boltzmann learning and DBNs are central to improving data organization in such settings. </li> &#13;
   <li class="readable-text" id="p315"> CNNs are primarily used in image and video processing. CNNs excel in recognizing patterns due to their architecture, featuring layers like convolutional and polling layers for feature extraction. </li> &#13;
   <li class="readable-text" id="p316"> RNNs are suitable for sequential data. RNNs maintain information across inputs and are enhanced by LSTMs for long-term dependency challenges. They are key in natural language processing and time-series analysis. </li> &#13;
   <li class="readable-text" id="p317"> The Boltzmann learning rule is an unsupervised, probabilistic method used in neural networks to adjust weights by minimizing an energy function, often aiding in tasks like dimensionality reduction and feature extraction, but computational challenges require approximations like contrastive divergence. </li> &#13;
   <li class="readable-text" id="p318"> DBNs are GANs consisting of layers of RBMs, utilizing unsupervised pretraining to learn complex data representations and supervised fine-tuning for tasks like classification, yet they face challenges, including computational expense and potential overfitting. </li> &#13;
   <li class="readable-text" id="p319"> DBNs use layer-wise pretraining to capture abstract features, making them suitable for complex applications like image or speech recognition; however, problems like the vanishing gradient problem and intricate fine-tuning processes can impede performance. </li> &#13;
   <li class="readable-text" id="p320"> Despite newer deep learning architectures gaining popularity, DBNs remain integral to the evolution of AI, playing a critical role in the development of models for tasks including dimensionality reduction, generative modeling, and classification, although training complexity continues to be a barrier. </li> &#13;
  </ul>&#13;
 </body></html>