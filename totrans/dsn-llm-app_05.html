<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Architectures and Learning Objectives"><div class="chapter" id="chapter_transformer-architecture">
<h1><span class="label">Chapter 4. </span>Architectures and Learning Objectives</h1>


<p>In Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch02.html#ch02">2</a> and
<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.html#chapter-LLM-tokenization">3</a>, we discussed some of the key ingredients that go into making a language model: the training datasets, and the vocabulary and tokenizer. Next, let’s complete the puzzle by learning about the models themselves, the architectures underpinning them, and their learning objectives.</p>

<p>In this chapter, we will learn the composition of language models and their structure<a data-type="indexterm" data-primary="architectures" id="xi_architectures4886"/><a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="architectures" id="xi_LLMsLargeLanguageModelsarchitectures4886"/>. Modern-day language models are predominantly based on the Transformer architecture<a data-type="indexterm" data-primary="Transformer architecture" id="id715"/>, and hence we will devote most of our focus to understanding it, by going through each component of the architecture in detail. Over the last few years, several variants and alternatives to the original Transformer architecture have been proposed. We will go through the promising ones, including Mixture of Experts (MoE) models. We will also examine commonly used learning objectives the language models are trained over, including next-token prediction. Finally, we will bring together the concepts of the last three chapters in practice by learning how to pre-train a language model from scratch.</p>






<section data-type="sect1" data-pdf-bookmark="Preliminaries"><div class="sect1" id="id46">
<h1>Preliminaries</h1>

<p>Just about every contemporary language model is based on neural networks<a data-type="indexterm" data-primary="neural networks" id="id716"/>, composed of processing units<a data-type="indexterm" data-primary="neurons (processing units)" id="id717"/> called <em>neurons</em>.  While modern neural networks do not resemble the workings of the human brain at all, many of the ideas behind neural networks and the terminology used is inspired by the field of neuroscience.</p>

<p>The neurons in a neural network are connected to each other according to some configuration. Each connection between a pair of neurons is associated with a weight<a data-type="indexterm" data-primary="weights" data-seealso="parameters" id="id718"/> (also called <em>parameter</em>), indicating the strength of the connection. The role these neurons play and the way they are connected to each other constitutes the <em>architecture</em> of the model.</p>

<p>The early 2010s saw the proliferation of multi-layer architectures<a data-type="indexterm" data-primary="multi-layer architectures" id="id719"/>, with layers of neurons stacked on top of each other, each layer extracting progressively more complex features of the input. This paradigm<a data-type="indexterm" data-primary="deep learning" id="id720"/> is called <em>deep learning.</em></p>

<p><a data-type="xref" href="#MLP-00">Figure 4-1</a> depicts a simple multi-layer neural network, also called the multi-layer perceptron.</p>

<figure><div id="MLP-00" class="figure">
<img src="assets/dllm_0401.png" alt="Transformer" width="600" height="454"/>
<h6><span class="label">Figure 4-1. </span>Multi-layer perceptron</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>For a more comprehensive treatment of neural networks, refer to <a href="https://oreil.ly/oDc6x">Goldberg’s book</a> on neural network–based natural language processing.</p>
</div>

<p>As discussed in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, language models are primarily pre-trained using self-supervised learning. Input text from the training dataset is tokenized  and converted to vector form. The input is then propagated through the neural network, affected by its weights<a data-type="indexterm" data-primary="activation functions" id="id721"/> and <em>activation functions</em>, the latter introducing nonlinearity to the model. The output of the model is compared to the expected output, called the gold truth<a data-type="indexterm" data-primary="gold truth" id="id722"/>. The weights of the output are adapted such that next time for the same input, the output can be closer to the gold truth.</p>

<p class="pagebreak-before">In practice, this adaptation process is implemented<a data-type="indexterm" data-primary="loss functions" id="id723"/> through a <em>loss function</em>. The goal of the model is to minimize the loss, which is the difference between the model 
<span class="keep-together">output</span> and the gold truth. To minimize the loss, the weights are updated using a gradient-descent based method, called backpropagation. I strongly recommend developing an intuitive understanding of this algorithm before diving into model training.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id724">
<h1>Self-Supervised Versus Supervised Learning</h1>
<p>The distinction between self-supervised learning<a data-type="indexterm" data-primary="self-supervised learning" id="id725"/><a data-type="indexterm" data-primary="supervised learning" id="id726"/> and supervised learning is artificial. The term “supervised learning” is used to describe learning by example using input-output pairs. To generate the training dataset, the output is typically annotated by a human or a computer. In the self-supervised variant, the output label does not need to be annotated because it already exists in nature, as part of the input. An example is web text on the internet with a learning objective like next-token prediction. The ground truth for the next-token objective exists within the input itself.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Representing Meaning"><div class="sect1" id="representing-meaning">
<h1>Representing Meaning</h1>

<p>While describing neural network–based architectures<a data-type="indexterm" data-primary="architectures" data-secondary="representing meaning" id="xi_architecturesrepresentingmeaning44052"/><a data-type="indexterm" data-primary="representation learning" id="xi_representationlearning44052"/> in the previous section, we glossed over the fact that the input text is converted into vectors and then propagated through the network. What are these vectors composed of and what do they represent? Ideally, after the model is trained, these vectors should accurately represent some aspect of the meaning of the underlying text, including its social connotations. Developing the right representations for modalities like text or images is a very active field of research, called <em>representation learning</em>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>When training a language model from scratch, these vectors initially mean nothing, as they are randomly generated. In practice, there are initialization algorithms used like Glorot, He, etc. Refer to <a href="https://oreil.ly/A8Iro">this report</a> for a primer on neural network initialization.</p>
</div>

<p>How can a list of numbers represent meaning? It is hard for humans to describe the meaning of a word or sentence, let alone represent it in numerical form that can be processed by a computer. The <em>form</em> of a word, i.e., the letters that comprise it, usually do not give any information whatsoever about the meaning it represents. For example, the sequence of letters in the word <em>umbrella</em> contains no hints about its meaning, even if you are already exposed to thousands of other words in the English language.</p>

<p class="pagebreak-before">The prominent way of representing meaning in numerical form is through<a data-type="indexterm" data-primary="distributional hypothesis framework" id="id727"/> the <em>distributional hypothesis</em> framework. The distributional hypothesis states that words that have similar meaning occur in similar contexts. The implication of this hypothesis is best represented by the adage:</p>
<blockquote>
<p>You shall know a word by the company it keeps.</p>
<p data-type="attribution">John Rupert Firth, <cite>1957</cite></p>
</blockquote>

<p>This is one of the primary ways in which we pick up the meaning of words we haven’t encountered previously, without needing to look them up in a dictionary. A large number of words we know weren’t learned from the dictionary or by explicitly learning the meaning of a word but by estimating meaning based on the contexts words appear in.</p>

<p>Let’s investigate how the distributional hypothesis works in practice. The Natural Language Toolkit (NLTK) library<a data-type="indexterm" data-primary="Natural Language Toolkit (NLTK) library" id="id728"/><a data-type="indexterm" data-primary="NLTK (Natural Language Toolkit) library" id="id729"/> provides a feature<a data-type="indexterm" data-primary="concordance view (NLTK)" id="id730"/> called <em>concordance view</em>, which presents you with the surrounding contexts that a given word appears in a corpus.</p>

<p>For example, let’s see the contexts in which the word “nervous” occurs in the Jane Austen classic <em>Emma</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">nltk.corpus</code> <code class="kn">import</code> <code class="n">gutenberg</code>
<code class="kn">from</code> <code class="nn">nltk.text</code> <code class="kn">import</code> <code class="n">Text</code>
<code class="n">corpus</code> <code class="o">=</code> <code class="n">gutenberg</code><code class="o">.</code><code class="n">words</code><code class="p">(</code><code class="s1">'austen-emma.txt'</code><code class="p">)</code>
<code class="n">text</code> <code class="o">=</code> <code class="n">Text</code><code class="p">(</code><code class="n">corpus</code><code class="p">)</code>
<code class="n">text</code><code class="o">.</code><code class="n">concordance</code><code class="p">(</code><code class="s2">"nervous"</code><code class="p">)</code></pre>

<p>The output looks like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Displaying</code> <code class="mi">11</code> <code class="n">of</code> <code class="mi">11</code> <code class="n">matches</code><code class="p">:</code>
<code class="o">...</code><code class="n">spirits</code> <code class="n">required</code> <code class="n">support</code> <code class="o">.</code> <code class="n">He</code> <code class="n">was</code> <code class="n">a</code> <code class="n">nervous</code> <code class="n">man</code> <code class="p">,</code> <code class="n">easily</code> <code class="n">depressed</code><code class="o">...</code>
<code class="o">...</code><code class="n">sitting</code> <code class="k">for</code> <code class="n">his</code> <code class="n">picture</code> <code class="n">made</code> <code class="n">him</code> <code class="n">so</code> <code class="n">nervous</code> <code class="p">,</code> <code class="n">that</code> <code class="n">I</code> <code class="n">could</code> <code class="n">only</code> <code class="n">take</code><code class="o">...</code>
<code class="o">...</code><code class="n">assure</code> <code class="n">you</code> <code class="p">,</code> <code class="n">excepting</code> <code class="n">those</code> <code class="n">little</code> <code class="n">nervous</code> <code class="n">headaches</code> <code class="ow">and</code> <code class="n">palpitations</code><code class="o">...</code>
<code class="o">...</code><code class="n">My</code> <code class="n">visit</code> <code class="n">was</code> <code class="n">of</code> <code class="n">use</code> <code class="n">to</code> <code class="n">the</code> <code class="n">nervous</code> <code class="n">part</code> <code class="n">of</code> <code class="n">her</code> <code class="n">complaint</code> <code class="p">,</code> <code class="n">I</code> <code class="n">hope</code><code class="o">...</code>
<code class="o">...</code><code class="n">much</code> <code class="n">at</code> <code class="n">ease</code> <code class="n">on</code> <code class="n">the</code> <code class="n">subject</code> <code class="k">as</code> <code class="n">his</code> <code class="n">nervous</code> <code class="n">constitution</code> <code class="n">allowed</code><code class="o">...</code>
<code class="o">...</code><code class="n">Her</code> <code class="n">father</code> <code class="n">was</code> <code class="n">growing</code> <code class="n">nervous</code> <code class="p">,</code> <code class="ow">and</code> <code class="n">could</code> <code class="ow">not</code> <code class="n">understand</code> <code class="n">her</code><code class="o">....</code>
<code class="o">...</code></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id731">
<h1>Exercise</h1>
<p>Imagine you have never heard of the word “nervous” before. Would you be able to guess the meaning of the word “nervous” just by reviewing the various contexts it appears in?</p>

<p>Check the contexts of words that are synonyms of the word “nervous.” How similar are they to the contexts of the word “nervous”<a data-type="indexterm" data-startref="xi_architecturesrepresentingmeaning44052" id="id732"/><a data-type="indexterm" data-startref="xi_representationlearning44052" id="id733"/>?</p>
</div></aside>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="The Transformer Architecture"><div class="sect1" id="id48">
<h1>The Transformer Architecture</h1>

<p>Now that we have developed an intuition on how text is represented in vector form, let’s further explore the canonical architecture used for training language models today, the Transformer.</p>

<p>In the mid 2010s, the predominant architectures used for NLP tasks were recurrent neural networks<a data-type="indexterm" data-primary="recurrent neural networks" id="id734"/>, specifically a variant called long short-term memory (LSTM)<a data-type="indexterm" data-primary="long short-term memory (LSTM)" id="id735"/><a data-type="indexterm" data-primary="LSTM (long short-term memory)" id="id736"/>. While knowledge of recurrent neural networks is not a prerequisite for this book, I recommend <a href="https://oreil.ly/CHCTd"><em>Neural Network Methods for Natural Language Processing</em></a> for more details<a data-type="indexterm" data-primary="Goldberg, Yoav" id="id737"/>.</p>

<p>Recurrent neural networks were sequence models<a data-type="indexterm" data-primary="sequence models" id="id738"/>, which means they processed text one token at a time, sequentially. A single vector was used to represent the state of the entire sequence, so as the sequence grew longer, more and more information needed to be captured in the single state vector. Because of the sequential nature of processing, long-range dependencies were harder to capture, as the content from the beginning of the sequence would be harder to retain.</p>

<p>This issue was candidly articulated by Ray Mooney<a data-type="indexterm" data-primary="Mooney, Ray" id="id739"/>, a senior computer scientist who remarked at the Association for Computational Linguistics (ACL) 2014 conference:</p>
<blockquote>
<p>You can’t cram the meaning of a whole %&amp;!$# sentence into a single $&amp;!#* vector!</p>
<p data-type="attribution">Ray Mooney, <cite>2014</cite></p>
</blockquote>

<p>Thus, there was a need for an architecture that solved for the deficiencies of LSTM: the limitations in representing long-range dependencies, the dependence on a single vector for representing the state of the entire sequence, and more. The Transformer architecture was designed to address these issues.</p>

<p><a data-type="xref" href="#Transformer0">Figure 4-2</a> depicts the original Transformer architecture, developed in 2017 by <a href="https://oreil.ly/tIvGZ">Vaswani et al.</a> As we can see in the figure, a Transformer model is typically composed of Transformer blocks<a data-type="indexterm" data-primary="layers" data-secondary="Transformer" id="id740"/> stacked on top of each other, called <em>layers</em>. The key components of each block are:</p>

<ul>
<li>
<p>Self-attention</p>
</li>
<li>
<p>Positional encoding</p>
</li>
<li>
<p>Feedforward networks</p>
</li>
<li>
<p>Normalization blocks</p>
</li>
</ul>

<figure><div id="Transformer0" class="figure">
<img src="assets/dllm_0402.png" alt="Transformer" width="352" height="800"/>
<h6><span class="label">Figure 4-2. </span>The Transformer architecture</h6>
</div></figure>

<p>At the beginning of the first block is a special layer called the <em>embedding</em> layer<a data-type="indexterm" data-primary="embedding layer" id="id741"/><a data-type="indexterm" data-primary="layers" data-secondary="embedding layer" id="id742"/>. This is where the tokens in the input text are mapped to their corresponding vector. The embedding layer is a matrix whose size is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Number</code> <code class="n">of</code> <code class="n">tokens</code> <code class="ow">in</code> <code class="n">the</code> <code class="n">vocabulary</code> <code class="o">*</code> <code class="n">The</code> <code class="n">vector</code> <code class="n">dimension</code> <code class="n">size</code></pre>

<p>On Hugging Face, we can inspect the embedding layer as such, using the 
<span class="keep-together"><code>transformers</code></span> library:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">LlamaTokenizer</code><code class="p">,</code> <code class="n">LlamaModel</code>

<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">LlamaTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s1">'llama3-base'</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">LlamaModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s1">'llama3-base'</code><code class="p">)</code>

<code class="n">sentence</code> <code class="o">=</code> <code class="s2">"He ate it all"</code>

<code class="n">inputs</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="n">sentence</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code>
<code class="n">input_ids</code> <code class="o">=</code> <code class="n">inputs</code><code class="p">[</code><code class="s1">'input_ids'</code><code class="p">]</code>
<code class="n">tokens</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">convert_ids_to_tokens</code><code class="p">(</code><code class="n">input_ids</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>

<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">embeddings</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">embeddings</code><code class="p">(</code><code class="n">input_ids</code><code class="p">)</code>

<code class="k">for</code> <code class="n">token</code><code class="p">,</code> <code class="n">embedding</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="n">embeddings</code><code class="p">[</code><code class="mi">0</code><code class="p">]):</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Token: </code><code class="si">{</code><code class="n">token</code><code class="si">}</code><code class="se">\n</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Embedding: </code><code class="si">{</code><code class="n">embedding</code><code class="si">}</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code></pre>

<p>The embedding vectors are the inputs that are then propagated through the rest of the network.</p>

<p>Next, let’s go through each of the components in a Transformer block in detail and explore their role in the modeling process.</p>








<section data-type="sect2" data-pdf-bookmark="Self-Attention"><div class="sect2" id="id49">
<h2>Self-Attention</h2>

<p>The self-attention mechanism<a data-type="indexterm" data-primary="Transformer architecture" data-secondary="self-attention mechanism" id="xi_Transformerarchitectureselfattentionmechanism416529"/><a data-type="indexterm" data-primary="self-attention mechanism" id="xi_selfattentionmechanism416529"/> draws on the same principle as the distributional hypothesis introduced in <a data-type="xref" href="#representing-meaning">“Representing Meaning”</a>, emphasizing the role of context in shaping the meaning of a token. This operation generates representations for each token in a text sequence, capturing various aspects of language like syntax, semantics, and even pragmatics.</p>

<p>In the standard self-attention implementation, the representation of each token is a function of the representation of all other tokens in the sequence. Given a token for which we are calculating its representation, tokens in the sequence that contribute more to the meaning of the token are given more weight.</p>

<p>For example, consider the sequence:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s1">'Mark told Sam that he was planning to resign.'</code></pre>

<p><a data-type="xref" href="#Attention-map">Figure 4-3</a> depicts how the representation for the token <em>he</em> is heavily weighted by the representation of the token <em>Mark</em>. In this case, the token <em>he</em> is a pronoun used to describe Mark in shorthand. In NLP, mapping a pronoun to its referent is called <em>co-reference resolution</em>.</p>

<figure><div id="Attention-map" class="figure">
<img src="assets/dllm_0403.png" alt="Attention-map" width="600" height="628"/>
<h6><span class="label">Figure 4-3. </span>Attention map</h6>
</div></figure>

<p>In practice, self-attention in the Transformer is calculated using three sets of weight matrices called queries<a data-type="indexterm" data-primary="queries, Transformer" id="xi_QueriesTransformer4182112"/><a data-type="indexterm" data-primary="keys, Transformer" id="xi_KeysTransformer4182112"/><a data-type="indexterm" data-primary="values, Transformer" id="xi_ValuesTransformer4182112"/>, keys, and values. Let’s go through them in detail. <a data-type="xref" href="#kqv">Figure 4-4</a> shows how the query, key, and value matrices are used in the self-attention 
<span class="keep-together">calculation</span>.</p>

<p>Each token is represented by its embedding vector. This vector is multiplied with the query, key, and value weight matrices to generate three input vectors. Self-attention for each token is then calculated like this:</p>
<ol>
<li>
<p>For each token, the dot products of its query vector with the key vectors of all the tokens (including itself) are taken. The resulting values are called attention scores.</p>
</li>
<li>
<p>The scores are scaled down by dividing them by the square root of the dimension of the key vectors.</p>
</li>
<li>
<p>The scores are then passed<a data-type="indexterm" data-primary="softmax function" id="id743"/> through a <a href="https://oreil.ly/b6gHV"> <em>softmax function</em></a> to turn them into a probability distribution that sums to 1. The softmax activation function tends to amplify larger values, hence the reason for scaling down the attention scores in the previous step.</p>
</li>
<li>
<p>The normalized attention<a data-type="indexterm" data-primary="attention normalization" id="id744"/><a data-type="indexterm" data-primary="normalized attention" id="id745"/> scores are then multiplied by the value vector for the corresponding token. The normalized attention score can be interpreted as the proportion that each token contributes to the representation of a given token.</p>
</li>
<li>
<p>In practice, there are multiple sets of query, key, and value vectors, calculating parallel representations. This is called multi-headed attention. The idea behind using multiple heads is that the model gets sufficient capacity to model various aspects of the input. The more the number of heads, the more chances that the <em>right</em> aspects of the input are being represented.</p>
</li>

</ol>

<figure><div id="kqv" class="figure">
<img src="assets/dllm_0404.png" alt="kqv" width="600" height="399"/>
<h6><span class="label">Figure 4-4. </span>Self-attention calculation</h6>
</div></figure>

<p>This is how we implement self-attention in code<a data-type="indexterm" data-startref="xi_QueriesTransformer4182112" id="id746"/><a data-type="indexterm" data-startref="xi_KeysTransformer4182112" id="id747"/><a data-type="indexterm" data-startref="xi_ValuesTransformer4182112" id="id748"/>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">torch.nn.functional</code> <code class="k">as</code> <code class="nn">F</code>

<code class="n">q</code> <code class="o">=</code> <code class="n">wQ</code><code class="p">(</code><code class="n">input_embeddings</code><code class="p">)</code>
<code class="n">k</code> <code class="o">=</code> <code class="n">WK</code><code class="p">(</code><code class="n">input_embeddings</code><code class="p">)</code>
<code class="n">v</code> <code class="o">=</code> <code class="n">WV</code><code class="p">(</code><code class="n">input_embeddings</code><code class="p">)</code>
<code class="n">dim_k</code> <code class="o">=</code> <code class="n">k</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">)</code>

<code class="n">attn_scores</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">q</code><code class="p">,</code> <code class="n">k</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="o">-</code><code class="mi">2</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">))</code>
<code class="n">scaled_attn_scores</code> <code class="o">=</code> <code class="n">attn_scores</code><code class="o">/</code><code class="n">torch</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">dim_k</code><code class="p">,</code>
  <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">))</code>

<code class="n">normalized_attn_scores</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">scaled_attn_scores</code><code class="p">,</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="n">output</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">normalized_attn_scores</code><code class="p">,</code> <code class="n">v</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In some Transformer variants, self-attention is calculated only on a subset of tokens in the sequence; thus the vector representation of a token is a function of the representations of only some and not all the tokens in the sequence<a data-type="indexterm" data-startref="xi_Transformerarchitectureselfattentionmechanism416529" id="id749"/><a data-type="indexterm" data-startref="xi_selfattentionmechanism416529" id="id750"/>.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Positional Encoding"><div class="sect2" id="id50">
<h2>Positional Encoding</h2>

<p>As discussed earlier, pre-Transformer architectures<a data-type="indexterm" data-primary="Transformer architecture" data-secondary="positional encoding" id="id751"/><a data-type="indexterm" data-primary="positional encoding" id="id752"/> like LSTM were sequence models, with tokens being processed one after the other. Thus the positional information about the tokens, i.e., the relative positions of the tokens in a sequence, was implicitly baked into the model. However, for Transformers all calculations are done in parallel, and positional information should be fed to the model explicitly. Several methods have been proposed to add positional information, and this is still a very active field of research. Some of the common methods used in LLMs today include:</p>
<dl>
<dt>Absolute positional embeddings</dt>
<dd>
<p>These were used in the original Transformer<a data-type="indexterm" data-primary="absolute positional embeddings" id="id753"/><a data-type="indexterm" data-primary="embeddings" data-secondary="absolute positional" id="id754"/> implementation by <a href="https://oreil.ly/CDq60">Vaswani et al.</a>; examples of models using absolute positional embeddings include earlier models like BERT and RoBERTa.</p>
</dd>
<dt>Attention with Linear Biases (ALiBi)</dt>
<dd>
<p>In this technique<a data-type="indexterm" data-primary="Attention with Linear Biases (ALiBi)" id="id755"/><a data-type="indexterm" data-primary="ALiBi (Attention with Linear Biases)" id="id756"/>, the attention scores are <a href="https://arxiv.org/abs/2108.12409">penalized</a> with a bias term proportional to the distance between the query token and the key token. This technique also enables modeling sequences of longer length during inference than what was encountered in the training 
<span class="keep-together">process</span>.</p>
</dd>
<dt>Rotary Position Embedding (RoPE)</dt>
<dd>
<p>Just like ALiBi<a data-type="indexterm" data-primary="Rotary Position Embedding (RoPE)" id="id757"/><a data-type="indexterm" data-primary="embeddings" data-secondary="RoPE" id="id758"/>, this <a href="https://arxiv.org/abs/2104.09864">technique</a> has the property of relative decay; there is a decay in the attention scores as the distance between the query token and the key token increases.</p>
</dd>
<dt>No Positional Encoding (NoPE)</dt>
<dd>
<p>A contrarian <a href="https://oreil.ly/QM9dW">technique</a> argues that positional embeddings<a data-type="indexterm" data-primary="No Positional Encoding (NoPE)" id="id759"/> in fact are not required and that Transformers implicitly capture positional information.</p>
</dd>
</dl>

<p>Models these days are mostly using ALiBi or RoPE, although this is one aspect of the Transformer architecture that is still actively improving.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Feedforward Networks"><div class="sect2" id="id51">
<h2>Feedforward Networks</h2>

<p>The output from a self-attention block<a data-type="indexterm" data-primary="Transformer architecture" data-secondary="feedforward networks" id="id760"/><a data-type="indexterm" data-primary="feedforward networks" id="id761"/> is fed through a  <a href="https://oreil.ly/Bdphg"><em>feedforward network</em></a>. Each token representation is independently fed through the network. The feedforward network incorporates a nonlinear activation function<a data-type="indexterm" data-primary="Rectified Linear Unit (ReLU)" id="id762"/><a data-type="indexterm" data-primary="ReLU (Rectified Linear Unit)" id="id763"/><a data-type="indexterm" data-primary="Gaussian Error Linear Units (GeLU)" id="id764"/><a data-type="indexterm" data-primary="activation functions" id="id765"/> like <a href="https://oreil.ly/KUqtP">Rectified Linear Unit (ReLU)</a> or <a href="https://oreil.ly/MSDKE">Gaussian Error Linear Units (GELU)</a>, thus enabling the model to learn more complex features from the data. For more details on these activation functions, refer to this <a href="https://oreil.ly/NfOb0">blog post from v7</a>.</p>

<p>The feedforward layers are implemented in code in this way:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>

<code class="k">class</code> <code class="nc">FeedForward</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">FeedForward</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">l1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">l2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="n">input_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">selu</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">SeLU</code><code class="p">()</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">selu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">l1</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">l2</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>


<code class="n">feed_forward</code> <code class="o">=</code> <code class="n">FeedForward</code><code class="p">(</code><code class="n">input_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">)</code>
<code class="n">outputs</code> <code class="o">=</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code></pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Layer Normalization"><div class="sect2" id="id241">
<h2>Layer Normalization</h2>

<p>Layer normalization<a data-type="indexterm" data-primary="Transformer architecture" data-secondary="layer normalization" id="id766"/><a data-type="indexterm" data-primary="layers" data-secondary="normalization" id="id767"/> is performed to ensure training stability and faster training convergence. While the original Transformer architecture performed normalization at the beginning of the block, modern implementations do it at the end of the block. The normalization is performed as follows:</p>
<ol>
<li>
<p>Given an input of batch size <code>b</code>, sequence length <code>n</code>, and vector dimension <code>d</code>, calculate the mean and variance across each vector dimension.</p>
</li>
<li>
<p>Normalize the input by subtracting the mean and dividing it by the square root of the variance. A small epsilon value is added to the denominator for numerical stability.</p>
</li>
<li>
<p>Multiply by a scale parameter and add a shift parameter to the resulting values. These parameters are learned during the training process.</p>
</li>

</ol>

<p class="pagebreak-before">This is how it is represented in code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>

<code class="k">class</code> <code class="nc">LayerNorm</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">dimension</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">beta</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">epsilon</code><code class="o">=</code><code class="mf">1e-5</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">LayerNorm</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code> <code class="o">=</code> <code class="n">epsilon</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">gamma</code> <code class="o">=</code> <code class="n">gamma</code> <code class="k">if</code> <code class="n">gamma</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code> <code class="k">else</code>
        <code class="n">nn</code><code class="o">.</code><code class="n">Parameter</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">dimension</code><code class="p">))</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">beta</code> <code class="o">=</code> <code class="n">beta</code> <code class="k">if</code> <code class="n">beta</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code> <code class="k">else</code>
        <code class="n">nn</code><code class="o">.</code><code class="n">Parameter</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">dimension</code><code class="p">))</code>


    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">mean</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">keepdim</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="n">variance</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">var</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">keepdim</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">unbiased</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
        <code class="n">x_normalized</code> <code class="o">=</code> <code class="p">(</code><code class="n">x</code> <code class="o">-</code> <code class="n">mean</code><code class="p">)</code> <code class="o">/</code> <code class="n">torch</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">variance</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">gamma</code> <code class="o">*</code> <code class="n">x_normalized</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">beta</code>

<code class="n">layer_norm</code> <code class="o">=</code> <code class="n">LayerNorm</code><code class="p">(</code><code class="n">embedding_dim</code><code class="p">)</code>
<code class="n">outputs</code> <code class="o">=</code> <code class="n">layer_norm</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code></pre>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Loss Functions"><div class="sect1" id="id52">
<h1>Loss Functions</h1>

<p>So far, we have discussed all the components of each Transformer block<a data-type="indexterm" data-primary="architectures" data-secondary="loss functions" id="id768"/><a data-type="indexterm" data-primary="Transformer architecture" data-secondary="loss functions" id="id769"/><a data-type="indexterm" data-primary="loss functions" id="id770"/>. For the next token-prediction learning objective, the input is propagated through the Transformer layers to generate the final output, which is a probability distribution across all tokens. During training, the loss is calculated by comparing the output distribution with the gold truth<a data-type="indexterm" data-primary="gold truth" id="id771"/>. The gold truth distribution assigns a 1 to the gold truth token and 0 to all other tokens.</p>

<p>There are many possible ways to quantify the difference between the output and the gold truth. The most popular one is cross-entropy, which is calculated by the formula:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Cross</code><code class="o">-</code><code class="n">Entropy</code><code class="o">=</code> <code class="err">−∑</code><code class="p">(</code><code class="n">gold</code> <code class="n">truth</code> <code class="n">probability</code><code class="p">)</code><code class="err">×</code><code class="n">log</code><code class="p">(</code><code class="n">output</code> <code class="n">probability</code><code class="p">)</code></pre>

<p>For example, consider the sequence:</p>

<pre data-type="programlisting">'His pizza tasted ______'</pre>

<p>Let’s say the gold truth token is <em>good</em>, and the output probability
distribution is (<em>terrible</em>: 0.65, <em>bad</em>:0.12, <em>good</em>:011,…​)</p>

<p>The cross-entropy is calculated as:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">−</code><code class="p">(</code><code class="mi">0</code><code class="err">×</code><code class="n">log</code><code class="p">(</code><code class="mf">0.65</code><code class="p">)</code><code class="o">+</code><code class="mi">0</code><code class="err">×</code><code class="n">log</code><code class="p">(</code><code class="mf">0.12</code><code class="p">)</code><code class="o">+</code><code class="mi">1</code><code class="err">×</code><code class="n">log</code><code class="p">(</code><code class="mf">0.11</code><code class="p">)</code><code class="o">+...</code><code class="p">)</code><code class="o">=</code> <code class="err">−</code><code class="n">log</code><code class="p">(</code><code class="mf">0.11</code><code class="p">)</code></pre>

<p class="pagebreak-before">Since the gold truth distribution values are 0 for all but the correct token, the equation can be simplified to:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Cross</code><code class="o">-</code><code class="n">Entropy</code> <code class="o">=</code> <code class="o">-</code><code class="n">log</code><code class="p">(</code><code class="n">output</code> <code class="n">probability</code> <code class="n">of</code> <code class="n">gold</code> <code class="n">truth</code> <code class="n">token</code><code class="p">)</code></pre>

<p>Once the loss is calculated, the gradient of the loss with respect to the parameters of the model is calculated and the weights are updated, using the backpropagation 
<span class="keep-together">algorithm</span>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Intrinsic Model Evaluation"><div class="sect1" id="id53">
<h1>Intrinsic Model Evaluation</h1>

<p>How do we know if the backpropagation algorithm<a data-type="indexterm" data-primary="architectures" data-secondary="intrinsic model evaluation" id="id772"/><a data-type="indexterm" data-primary="Transformer architecture" data-secondary="intrinsic model evaluation" id="id773"/><a data-type="indexterm" data-primary="intrinsic model evaluation" id="id774"/><a data-type="indexterm" data-primary="backpropagation algorithm" data-secondary="intrinsic model evaluation" id="id775"/> is actually working and that the model is getting better over time? We can use either intrinsic model evaluation or extrinsic model evaluation.</p>

<p>Extrinsic model evaluation<a data-type="indexterm" data-primary="extrinsic model evaluation" id="id776"/> involves testing the model’s performance on real-world downstream tasks. These tasks directly test the performance of the model but only on a narrow range of the model’s capabilities. In contrast, intrinsic model evaluation involves a more general evaluation of the model’s ability to model language, but with no guarantee that its performance in the intrinsic evaluation metric is directly proportional to its performance across all possible downstream tasks.</p>

<p>The most common intrinsic evaluation metric<a data-type="indexterm" data-primary="perplexity metric" id="id777"/> is <em>perplexity</em>. Perplexity measures the ability of a language model to accurately predict the next token in a sequence. A model that can always correctly predict the next token has a perplexity of 1. The higher the perplexity, the worse the language model. In the worst case, if the model is predicting at random, with probability of predicting each token in a vocabulary of size V being 1/V, then the perplexity is V.</p>

<p>Perplexity is related to cross-entropy<a data-type="indexterm" data-primary="cross-entropy, and perplexity" id="id778"/> by this formula:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Perplexity</code> <code class="o">=</code> <code class="mi">2</code><code class="o">^</code><code class="n">Cross</code><code class="o">-</code><code class="n">Entropy</code></pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Transformer Backbones"><div class="sect1" id="id54">
<h1>Transformer Backbones</h1>

<p>So far, we described the components of the canonical version<a data-type="indexterm" data-primary="Transformer architecture" data-secondary="backbones" id="xi_Transformerarchitecturebackbones436261"/><a data-type="indexterm" data-primary="backbones, Transformer" id="xi_backbonesTransformer436261"/> of the Transformer. In practice, three major types of architecture backbones are used to implement the Transformer:</p>

<ul>
<li>
<p>Encoder-only</p>
</li>
<li>
<p>Encoder-decoder</p>
</li>
<li>
<p>Decoder-only</p>
</li>
</ul>

<p>Let’s look at each of these in detail.</p>

<p><a data-type="xref" href="#enc-dec">Figure 4-5</a> depicts encoder-only, encoder-decoder, and decoder-only architectures.</p>

<figure><div id="enc-dec" class="figure">
<img src="assets/dllm_0405.png" alt="enc-dec" width="553" height="800"/>
<h6><span class="label">Figure 4-5. </span>Visualization of various Transformer backbones</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Encoder-Only Architectures"><div class="sect2" id="id55">
<h2>Encoder-Only Architectures</h2>

<p>Encoder-only architectures<a data-type="indexterm" data-primary="encoder-only models" id="id779"/><a data-type="indexterm" data-primary="models" data-secondary="encoder-only" id="id780"/> were all the rage when Transformer-based language models first burst on the scene. Iconic language models from yesteryear (circa 2018) that use encoder-only architectures include BERT, RoBERTa, etc.</p>

<p>There haven’t been many encoder-only LLMs trained since 2021 for a few reasons, including:</p>

<ul>
<li>
<p>They are relatively harder to train.</p>
</li>
<li>
<p>The masked language modeling objective typically used to train them provides a learning signal in only a small percentage of tokens (the masking rate), thus needing a lot more data to reach the same level of performance as decoder-only models.</p>
</li>
<li>
<p>For every downstream task, you need to train a separate task-specific head, making usage inefficient.</p>
</li>
</ul>

<p>However, the release of ModernBERT<a data-type="indexterm" data-primary="ModernBERT" id="id781"/> seems to have reinvigorated this space.</p>

<p>The creators of the UL2 language model<a data-type="indexterm" data-primary="UL2 language model" id="id782"/> claim that encoder-only models should be considered obsolete. I personally wouldn’t go that far; encoder-only models are still great choices for classification tasks. Moreover, if you already have a satisfactory pipeline for your use case built around encoder-only models, I would say if it ain’t broke, why fix it?</p>

<p>Here are some guidelines for adopting encoder-only models:</p>

<ul>
<li>
<p>RoBERTa performs<a data-type="indexterm" data-primary="RoBERTa" id="id783"/> better than BERT most of the time, since it is trained a lot longer on more data, and it has adopted best practices learned after the release of BERT.</p>
</li>
<li>
<p>DeBERTa and ModernBERT are currently regarded<a data-type="indexterm" data-primary="DeBERTa" id="id784"/> as the best-performing encoder-only models.</p>
</li>
<li>
<p>The distilled versions of encoder-only models<a data-type="indexterm" data-primary="distillation" data-secondary="encoder-only models" id="id785"/><a data-type="indexterm" data-primary="DistillBERT" id="id786"/> like DistilBERT, etc., are not too far off from the original models in terms of performance, and they should be considered if you are operating under resource constraints.</p>
</li>
</ul>

<p>Several embedding models are built from encoder-only models. For example, one of the most important libraries in the field of NLP, the Swiss Army knife of NLP tools<a data-type="indexterm" data-primary="Sentence Transformers library" id="id787"/>, <em>sentence transformers</em>, provides encoder-only embedding models that are very widely used. all-mpnet-base-v2,  based on an encoder-only model called MPNet<a data-type="indexterm" data-primary="MPNet" id="id788"/>, and fine-tuned on several task datasets, is still competitive with much larger embedding 
<span class="keep-together">models</span>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Encoder-Decoder Architectures"><div class="sect2" id="id56">
<h2>Encoder-Decoder Architectures</h2>

<p>This is the original architecture<a data-type="indexterm" data-primary="encoder-decoder architectures" id="id789"/> of the Transformer, as it was first proposed. The T5 series of models<a data-type="indexterm" data-primary="T5 series of models" id="id790"/> uses this architecture type.</p>

<p>In encoder-decoder models, the input is text and the output is also text. A standardized interface ensures that the same model and training procedure can be used for multiple tasks. The inputs are handled by an encoder, and the outputs by the decoder.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Decoder-Only Architectures"><div class="sect2" id="id57">
<h2>Decoder-Only Architectures</h2>

<p>A majority of LLMs trained today use decoder-only<a data-type="indexterm" data-primary="decoder-only architectures" id="id791"/> models. Decoder-only models came into fashion starting from the original GPT model from OpenAI. Decoder-only models excel at zero-shot and few-shot learning.</p>

<p>Decoder models can be causal and noncausal<a data-type="indexterm" data-primary="causal versus noncausal decoder models" id="id792"/><a data-type="indexterm" data-primary="noncausal versus causal decoder models" id="id793"/><a data-type="indexterm" data-primary="decoder models" data-secondary="causal versus noncausal" id="id794"/><a data-type="indexterm" data-primary="models" data-secondary="decoder" id="id795"/>. Noncausal models have bidirectionality over the input sequence, while the output is still autoregressive (you cannot look ahead).</p>
<div data-type="tip"><h6>Tip</h6>
<p>While the field is still evolving, there has been some <a href="https://oreil.ly/Sb7JS">compelling evidence</a> for the following results:</p>

<ul>
<li>
<p>Decoder-only models are the best choice for zero-shot and few-shot generalization.</p>
</li>
<li>
<p>Encoder-decoder models are the best choice for multi-task fine tuning.</p>
</li>
</ul>

<p>The best of both worlds is to combine the two: start with auto-regressive training<a data-type="indexterm" data-primary="auto-regressive training" id="id796"/>, and then in an adaptation step, pre-train further with a noncausal setup using a span corruption objective.</p>
</div>

<p>In this section, we discussed how architectural backbones can be classified according to how they use the architecture’s encoder and decoder. Another architectural backbone type that is making inroads in the past year is the Mixture of Experts (MoE) paradigm. Let’s explore that in detail.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Mixture of Experts"><div class="sect2" id="id58">
<h2>Mixture of Experts</h2>

<p>Remarkably, in the seven years<a data-type="indexterm" data-primary="mixture of experts (MoE) models" id="xi_mixtureofexpertsMoEmodels443131"/><a data-type="indexterm" data-primary="MoE (mixture of experts) models" id="xi_MoEmixtureofexpertsmodels443131"/> since the invention of the Transformer architecture, the Transformer implementation used in current language models isn’t too different from the original version, despite hundreds of papers proposing modifications to it. The original architecture has proven to be surprisingly robust, with most proposed variants barely moving the needle in terms of performance. However, some components of the Transformer have seen changes, like positional encodings as discussed earlier in the chapter.</p>

<p>MoE models have been seeing a lot of success in the past couple of years. Examples include OpenAI’s GPT-4<a data-type="indexterm" data-primary="OpenAI" data-secondary="GPT-4" id="id797"/><a data-type="indexterm" data-primary="Google Switch Transformer" id="id798"/><a data-type="indexterm" data-primary="Mistral Mixtral" id="id799"/><a data-type="indexterm" data-primary="GPT-4" id="id800"/> (unconfirmed), Google’s Switch, DeepSeek’s DeepSeek V3, and Mistral’s Mixtral. In this section, we will learn the motivations behind developing this architecture and how it works in practice.</p>

<p>As shown in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, the scaling laws<a data-type="indexterm" data-primary="scaling laws" id="id801"/> dictate that performance of the language model increases as you increase the size of the model and its training data. However, increasing the model capacity implies more compute is needed for both training and inference. This is undesirable, especially at inference time, when latency requirements can be stringent. Can we increase the capacity of a model without increasing the required compute?</p>

<p>One way to achieve this is using conditional computation<a data-type="indexterm" data-primary="conditional computation" id="id802"/>; each input (either a token or the entire sequence) sees a different subset of the model, interacting with only the parameters that are best suited to process it. This is achieved by composing the architecture to be made up of several components called experts, with only a subset of experts being activated for each input.</p>

<p><a data-type="xref" href="#MoE-0">Figure 4-6</a> depicts a canonical MoE model.</p>

<figure><div id="MoE-0" class="figure">
<img src="assets/dllm_0406.png" alt="MoE" width="600" height="572"/>
<h6><span class="label">Figure 4-6. </span>Mixture of Experts</h6>
</div></figure>

<p>A key component of the MoE architecture<a data-type="indexterm" data-primary="gating function (MoE)" id="id803"/> is the <em>gating function</em>. The gating function helps decide which expert is more suited to process a given input.
The gating function is implemented as a weight applied to each expert.</p>

<p>The experts are typically added to the feedforward<a data-type="indexterm" data-primary="feedforward networks" id="id804"/> component of the Transformer. Therefore, if there are eight experts, then there will be eight feedforward networks instead of one. Based on the routing strategy used, only a small subset of these networks will be activated for a given input.</p>

<p>The routing strategy determines the number and type of experts activated. Two types of popular routing strategies<a data-type="indexterm" data-primary="routing strategies (MoE)" id="id805"/> exist:</p>

<ul>
<li>
<p>Tokens choose</p>
</li>
<li>
<p>Experts choose</p>
</li>
</ul>

<p>In the tokens choose strategy<a data-type="indexterm" data-primary="tokens choose strategy" id="id806"/>, each token chooses k experts. k is typically a small number (~2). The disadvantage of using this strategy is the need for load balancing<a data-type="indexterm" data-primary="load balancing, MoE routing strategies" id="id807"/>. If in a given input batch, most of the tokens end up using the same experts, then additional time is needed to finish the computation as we cannot benefit from the parallelization afforded by multiple experts.</p>

<p>In the experts choose strategy<a data-type="indexterm" data-primary="experts choose strategy (MoE)" id="id808"/>, each expert picks the tokens that it is most equipped to handle. This solves the load balancing problem as we can specify that each expert choose the same number of tokens. However, this also leads to inefficient token-expert matching, as each expert is limited to picking only a finite number of tokens in a batch<a data-type="indexterm" data-startref="xi_mixtureofexpertsMoEmodels443131" id="id809"/><a data-type="indexterm" data-primary="learning objectives" data-secondary="FLM" id="xi_learningobjectivesFLM4457347"/><a data-type="indexterm" data-primary="full language modeling (FLM)" id="xi_fulllanguagemodelingFLM4457347"/><a data-type="indexterm" data-startref="xi_MoEmixtureofexpertsmodels443131" id="id810"/>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id811">
<h1>Upcycling Models</h1>
<p>Very few MoE models are publicly available as of the book’s writing. Can we turn an existing general Transformer-based model into an MoE model? <a href="https://oreil.ly/jJEot">Komatsuzaki et al.</a> have devised an upcycling method that can be used to add an MoE component to an already pre-trained model. This is done by making N copies of the feedforward layers, one for each expert, and using the original parameters of these layers as the initialization parameters for the MoE model<a data-type="indexterm" data-startref="xi_architectures4886" id="id812"/><a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelsarchitectures4886" id="id813"/><a data-type="indexterm" data-startref="xi_Transformerarchitecturebackbones436261" id="id814"/><a data-type="indexterm" data-startref="xi_backbonesTransformer436261" id="id815"/>.</p>
</div></aside>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Learning Objectives"><div class="sect1" id="id59">
<h1>Learning Objectives</h1>

<p>Now that we have discussed the architecture of language models<a data-type="indexterm" data-primary="learning objectives" id="xi_learningobjectives446663"/>, let’s turn our focus to understanding the tasks they are trained on during the pre-training process.</p>

<p>As mentioned earlier in the chapter, language models are pre-trained in a self-supervised<a data-type="indexterm" data-primary="self-supervised learning" id="id816"/> manner. The scale of data we need to train them makes it prohibitively expensive to perform supervised learning, where (input, output) examples need to come from humans. Instead, we use a form of training called self-supervision, where the data itself contains the target labels. The goal of self-supervised learning is to learn a task which acts as a proxy for learning the syntax and semantics of a language, as well as skills like reasoning, arithmetic and logical manipulation, and other cognitive tasks, and (hopefully) eventually leading up to general human intelligence. How does this work?</p>

<p>For example, let’s take the canonical language modeling task: predicting the next word that comes in a sequence. Consider the sequence:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s1">'Tammy jumped over the'</code></pre>

<p>and the language model is asked to predict the next token<a data-type="indexterm" data-primary="next-token prediction objective" id="id817"/>. The total number of possible answers is the size of the vocabulary. There are many valid continuations to this sequence, like (hedge, fence, barbecue, sandcastle, etc.), but many continuations to this sequence would violate English grammar rules like (is, of, the). During the training process, after seeing billions of sequences, the model will know that it is highly improbable for the word “the” to be followed by the word “is” or “of,” regardless of the surrounding context. Thus, you can see how just predicting the next token is such a powerful tool: in order to correctly predict the next token you can eventually learn more and more complex functions that you can encode in your model connections. However, whether this paradigm is all we need to develop general intelligence is an open question.</p>

<p>Self-supervised learning objectives used for pre-training LLMs can be broadly classified (nonexhaustively) into three types:</p>

<ul>
<li>
<p>Full language modeling (FLM)</p>
</li>
<li>
<p>Masked language modeling (MLM)</p>
</li>
<li>
<p>Prefix language modeling (PrefixLM)</p>
</li>
</ul>

<p>Let’s explore these in detail.</p>








<section data-type="sect2" data-pdf-bookmark="Full Language Modeling"><div class="sect2" id="id60">
<h2>Full Language Modeling</h2>

<p><a data-type="xref" href="#full-language-modeling">Figure 4-7</a> shows the canonical FLM objective at work.</p>

<figure><div id="full-language-modeling" class="figure">
<img src="assets/dllm_0407.png" alt="Full Language Modeling" width="600" height="76"/>
<h6><span class="label">Figure 4-7. </span>Full language modeling</h6>
</div></figure>

<p>This is the canonical language modeling objective of learning to predict the next token in a sequence and currently the simplest and most common training objective, used by GPT-4<a data-type="indexterm" data-primary="OpenAI" data-secondary="GPT-4" id="id818"/><a data-type="indexterm" data-primary="GPT-4" id="id819"/> and a vast number of open source models. The loss is computed for every token the model sees, i.e., every single token in the training set that is being asked to be predicted by the language model provides a learning signal for the model, making it very efficient.</p>

<p>Let’s explore an example, using the GPT Neo model<a data-type="indexterm" data-primary="GPT Neo model" id="xi_GPTNeomodel449750"/><a data-type="indexterm" data-primary="Eleuther AI" data-secondary="GPT Neo model" id="xi_EleutherAIGPTNeomodel449750"/>.</p>

<p>Suppose we continue pre-training the GPT Neo model from its publicly available checkpoint, using the full language modeling objective. Let’s say the current training sequence is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s1">'Language models are ubiquitous'</code></pre>

<p>You can run this code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoTokenizer</code><code class="p">,</code> <code class="n">GPTNeoForCausalLM</code>


<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"EleutherAI/gpt-neo-1.3B"</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">GPTNeoForCausalLM</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"EleutherAI/gpt-neo-1.3B"</code><code class="p">)</code>


<code class="n">input_ids</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="s2">"Language models are"</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code>
<code class="n">gen_tokens</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="o">**</code><code class="n">input_ids</code><code class="p">,</code> <code class="n">max_new_tokens</code> <code class="o">=</code><code class="mi">1</code><code class="p">,</code>

<code class="n">output_scores</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">return_dict_in_generate</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">output_scores</code> <code class="o">=</code> <code class="n">gen_tokens</code><code class="p">[</code><code class="s2">"scores"</code><code class="p">]</code>
<code class="n">scores_tensor</code> <code class="o">=</code> <code class="n">output_scores</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="n">sorted_indices</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">argsort</code><code class="p">(</code><code class="n">scores_tensor</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">descending</code><code class="o">=</code><code class="kc">True</code><code class="p">)[:</code><code class="mi">20</code><code class="p">]</code>


<code class="k">for</code> <code class="n">index</code> <code class="ow">in</code> <code class="n">sorted_indices</code><code class="p">:</code>
    <code class="n">token_id</code> <code class="o">=</code> <code class="n">index</code>
    <code class="n">token_name</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">decode</code><code class="p">([</code><code class="n">token_id</code><code class="o">.</code><code class="n">item</code><code class="p">()])</code>
    <code class="n">token_score</code> <code class="o">=</code> <code class="n">scores_tensor</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="n">index</code><code class="p">]</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Token: </code><code class="si">{</code><code class="n">token_name</code><code class="si">}</code><code class="s2">, Score: </code><code class="si">{</code><code class="n">token_score</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>

<p>This code tokenizes the input text <code>Language models are</code> and feeds it to the model by invoking the <code>generate()</code> function. The function predicts the continuation, given the sequence “Language models are.” It outputs only one token and stops generating because <code>max_new_tokens</code> is set to 1. The rest of the code enables it to output the top 20 list of tokens with the highest score, prior to applying the softmax at the last layer.</p>

<p>The top 20 tokens with the highest prediction score are:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Output</code><code class="p">:</code> <code class="n">Token</code><code class="p">:</code>  <code class="n">a</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">1.102203369140625</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">used</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">1.4315788745880127</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">the</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">1.7675716876983643</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">often</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">1.8415470123291016</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">an</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.4652323722839355</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">widely</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.657834053039551</code>
<code class="n">Token</code><code class="p">:</code>  <code class="ow">not</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.6726579666137695</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">increasingly</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.7568516731262207</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">ubiquitous</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.8688106536865234</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">important</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.902832508087158</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">one</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.9083480834960938</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">defined</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.0815649032592773</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">being</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.2117576599121094</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">commonly</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.3110013008117676</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">very</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.317342758178711</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">typically</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.4478530883789062</code>
<code class="n">Token</code><code class="p">:</code>  <code class="nb">complex</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.521362781524658</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">powerful</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.5338563919067383</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">language</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.550961971282959</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">pervasive</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.563507080078125</code></pre>

<p>Every word in the top 20 seems to be a valid continuation of the sequence. The ground truth<a data-type="indexterm" data-primary="ground truth" data-secondary="full language modeling" id="id820"/> is the token <code>ubiquitous</code>, which we can use to calculate the loss and initiate the backpropagation process for learning.</p>

<p>As another example, consider the text sequence:</p>

<pre data-type="programlisting">'I had 25 eggs. I gave away 12. I now have 13'</pre>

<p>Run the same code as previously, except for this change:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">input_ids</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="s2">"'I had 25 eggs. I gave away 12. I now have"</code><code class="p">,</code>
  <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code></pre>

<p>The top 20 output tokens are:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Token</code><code class="p">:</code>  <code class="mi">12</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.3242850303649902</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">25</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.5023117065429688</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">only</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.5456185340881348</code>
<code class="n">Token</code><code class="p">:</code>  <code class="n">a</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.5726099014282227</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">2</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.6731367111206055</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">15</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.6967623233795166</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">4</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.8040688037872314</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">3</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.839219570159912</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">14</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.847306728363037</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">11</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.8585362434387207</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">1</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.877161979675293</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">10</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.9321107864379883</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">6</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.982785224914551</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">18</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.0570476055145264</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">20</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.079172134399414</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">5</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.111320972442627</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">13</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.117424726486206</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">9</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.125835657119751</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">16</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.1476120948791504</code>
<code class="n">Token</code><code class="p">:</code>  <code class="mi">7</code><code class="p">,</code> <code class="n">Score</code><code class="p">:</code> <code class="o">-</code><code class="mf">3.1622045040130615</code></pre>

<p>The correct answer has the 17th highest score. A lot of numbers appear in the top 10, showing that the model is more or less randomly guessing the answer, which is not surprising for a smaller model like GPT Neo<a data-type="indexterm" data-startref="xi_GPTNeomodel449750" id="id821"/><a data-type="indexterm" data-startref="xi_EleutherAIGPTNeomodel449750" id="id822"/>.</p>

<p>The OpenAI API provides the <code>logprobs</code> parameter that allows you to specify the number of tokens along with their log probabilities that need to be returned. As of the book’s writing, only the <code>logprobs</code> of the 20 most probable tokens are available. The tokens returned are in order of their log probabilities:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">openai</code>
<code class="n">openai</code><code class="o">.</code><code class="n">api_key</code> <code class="o">=</code> <code class="o">&lt;</code><code class="n">Insert</code> <code class="n">your</code> <code class="n">OpenAI</code> <code class="n">key</code><code class="o">&gt;</code>


<code class="n">openai</code><code class="o">.</code><code class="n">Completion</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
  <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4o"</code><code class="p">,</code>
  <code class="n">prompt</code><code class="o">=</code><code class="s2">"I had 25 eggs. I gave away 12. I now have "</code><code class="p">,</code>
  <code class="n">max_tokens</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
  <code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
  <code class="n">logprobs</code> <code class="o">=</code> <code class="mi">10</code>
<code class="p">)</code></pre>

<p>This code calls the older gpt-4o model<a data-type="indexterm" data-primary="OpenAI" data-secondary="GPT-3" id="id823"/><a data-type="indexterm" data-primary="GPT-3" id="id824"/>, asking it to generate a maximum of one token. The output is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s2">"top_logprobs"</code><code class="p">:</code> <code class="p">[</code>
          <code class="p">{</code>
            <code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">:</code> <code class="o">-</code><code class="mf">0.08367541</code><code class="p">,</code>
            <code class="s2">" 13"</code><code class="p">:</code> <code class="o">-</code><code class="mf">2.8566456</code><code class="p">,</code>
            <code class="s2">"____"</code><code class="p">:</code> <code class="o">-</code><code class="mf">4.579212</code><code class="p">,</code>
            <code class="s2">"_____"</code><code class="p">:</code> <code class="o">-</code><code class="mf">4.978668</code><code class="p">,</code>
            <code class="s2">"________"</code><code class="p">:</code> <code class="o">-</code><code class="mf">6.220278</code>
            <code class="err">…</code>
          <code class="p">}</code></pre>

<p>gpt-4o<a data-type="indexterm" data-primary="OpenAI" data-secondary="gpt-4o" id="id825"/><a data-type="indexterm" data-primary="gpt-4o" id="id826"/> is pretty confident that the answer is 13, and rightfully so. The rest of the top probability tokens are all related to output formatting.</p>
<div data-type="tip"><h6>Tip</h6>
<p>During inference, we don’t necessarily need to generate the token with the highest score. Several <em>decoding strategies</em> allow you to generate more diverse text<a data-type="indexterm" data-startref="xi_learningobjectivesFLM4457347" id="id827"/><a data-type="indexterm" data-startref="xi_fulllanguagemodelingFLM4457347" id="id828"/>. We will discuss these strategies in 
<span class="keep-together"><a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a></span>.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar" class="less_space pagebreak-before"><div class="sidebar" id="id829">
<h1>Exercise</h1>
<p>Ask the gpt-4o model to solve individual crossword clues in the  <a href="https://oreil.ly/HvK_O">Boatload Puzzles Crossword</a>. You may have to iterate with the prompt. A good start would be “Solve this crossword and answer in one word. The clue is &lt;X&gt; and it is a &lt;Y&gt; letter word. The answer is: Set <code>max_tokens=3</code> to account for formatting tokens. Analyze the <code>logprob</code>’s output. Is it dangerously close to getting it right/wrong? How many clues does it answer correctly?</p>
</div></aside>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Prefix Language Modeling"><div class="sect2" id="id61">
<h2>Prefix Language Modeling</h2>

<p>Prefix LM<a data-type="indexterm" data-primary="learning objectives" data-secondary="prefix LM" id="id830"/><a data-type="indexterm" data-primary="prefix language modeling (prefix LM)" id="id831"/> is similar to the FLM setting. The difference is that FLM is fully causal<a data-type="indexterm" data-primary="noncausal versus causal decoder models" id="id832"/><a data-type="indexterm" data-primary="causal versus noncausal decoder models" id="id833"/><a data-type="indexterm" data-primary="decoder models" data-secondary="causal versus noncausal" id="id834"/><a data-type="indexterm" data-primary="models" data-secondary="decoder" id="id835"/>, i.e., in a left-to-right writing system like English, tokens do not attend to tokens to the right (future). In the prefix LM setting, a part of the text sequence, called the prefix, is allowed to attend to future tokens in the prefix. The prefix part is thus noncausal. For training prefix LMs, a random prefix length is sampled, and the loss is calculated over only the tokens in the suffix.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Masked Language Modeling"><div class="sect2" id="id62">
<h2>Masked Language Modeling</h2>

<p><a data-type="xref" href="#masked-language-modeling-bert">Figure 4-8</a> shows the canonical MLM<a data-type="indexterm" data-primary="learning objectives" data-secondary="MLM" id="xi_learningobjectivesMLM465758"/><a data-type="indexterm" data-primary="masked language modeling (MLM)" id="xi_maskedlanguagemodelingMLM465758"/><a data-type="indexterm" data-primary="MLM (masked language modeling)" id="xi_MLMmaskedlanguagemodeling465758"/> objective at work.</p>

<figure><div id="masked-language-modeling-bert" class="figure">
<img src="assets/dllm_0408.png" alt="Masked Language Modeling in BERT" width="600" height="97"/>
<h6><span class="label">Figure 4-8. </span>Masked Language Modeling in BERT</h6>
</div></figure>

<p>In the MLM setting, rather than predict the next token in a sequence, we ask the model to predict masked tokens within the sequence. In the most basic form of MLM implemented in the BERT model<a data-type="indexterm" data-primary="BERT model" id="id836"/>, 15% of tokens are randomly chosen to be masked and are replaced with a special mask token, and the language model is asked to predict the original tokens.</p>

<p>The T5 model<a data-type="indexterm" data-primary="T5 series of models" id="xi_T5seriesofmodels466713"/> creators used a modification of the original MLM objective. In this variant, 15% of tokens are randomly chosen to be removed from a sequence. Consecutive dropped-out tokens are replaced by a single unique special token<a data-type="indexterm" data-primary="sentinel token" id="id837"/> called the <em>sentinel token</em>. The model is then asked to predict and generate the dropped tokens, delineated by the sentinel tokens.</p>

<p class="pagebreak-before">As an example, consider this sequence:</p>
<blockquote>
<p>Tempura has always been a source of conflict in the family due to unexplained reasons</p></blockquote>

<p>Let’s say we drop the tokens “has,” “always,” “of,” and “conflict.” The sequence is now:</p>
<blockquote>
<p>Tempura &lt;S1&gt; been a source &lt;S2&gt; in the family due to unexplained reasons</p></blockquote>

<p>with S1, S2 being the sentinel tokens. The model is expected to output:</p>
<blockquote>
<p>&lt;S1&gt; has always &lt;S2&gt; of conflict &lt;E&gt;</p></blockquote>

<p>The output sequence is terminated by a special token indicating the end of the sequence.</p>

<p>Generating only the dropped tokens and not the entire sequence is computationally more efficient and saves training time. Note that unlike in Full Language Modeling, the loss is calculated over only a small proportion of tokens (the masked tokens) in the input sequence.</p>

<p>Let’s explore this on Hugging Face:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">T5Tokenizer</code><code class="p">,</code> <code class="n">T5ForConditionalGeneration</code>

<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">T5Tokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"t5-3b"</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">T5ForConditionalGeneration</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"t5-3b"</code><code class="p">)</code>

<code class="n">input_ids</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="s2">"Tempura &lt;extra_id_0&gt;  been a source &lt;extra_id_1&gt; in the</code>
<code class="n">family</code> <code class="n">due</code> <code class="n">to</code> <code class="n">unexplained</code> <code class="n">reasons</code><code class="s2">", return_tensors="</code><code class="n">pt</code><code class="s2">").input_ids</code>
<code class="n">targets</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="s2">"&lt;extra_id_0&gt; has always &lt;extra_id_1&gt; of conflict</code>

<code class="o">&lt;</code><code class="n">extra_id_2</code><code class="o">&gt;</code><code class="s2">", return_tensors="</code><code class="n">pt</code><code class="s2">").input_ids</code>
<code class="n">loss</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">input_ids</code><code class="o">=</code><code class="n">input_ids</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="n">labels</code><code class="p">)</code><code class="o">.</code><code class="n">loss</code></pre>

<p>The targets can be prepared using a simple templating function.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id838">
<h1>Exercise</h1>
<p>Play around with different masking strategies. Specifically:</p>

<ul>
<li>
<p>Change the masking rate. What happens if you mask 30% or 50% of tokens?</p>
</li>
<li>
<p>Change the masking strategy. Can you do better than random masking? What heuristics allow you to mask tokens that would contribute more to learning?</p>
</li>
</ul>
</div></aside>

<p>More generally, MLM can be interpreted<a data-type="indexterm" data-primary="denoisers" id="id839"/> as a <em>denoising autoencoder</em>. You corrupt your input by adding noise (masking, dropping tokens), and then you train a model to regenerate the original input. BART<a data-type="indexterm" data-primary="BART" id="id840"/> takes this to the next level by using five different types of span corruptions:</p>
<dl>
<dt>Random token masking</dt>
<dd>
<p><a data-type="xref" href="#bart-enoiser-objectives1">Figure 4-9</a> depicts the corruption and denoising steps<a data-type="indexterm" data-startref="xi_T5seriesofmodels466713" id="id841"/>.</p>

<figure><div id="bart-enoiser-objectives1" class="figure">
<img src="assets/dllm_0409.png" alt="BART Denoiser Objectives1" width="600" height="133"/>
<h6><span class="label">Figure 4-9. </span>Random token masking in BART</h6>
</div></figure>
</dd>
<dt>Random token deletion</dt>
<dd>
<p>The model needs to predict the positions in the text where tokens have been deleted. <a data-type="xref" href="#bart-enoiser-objectives2">Figure 4-10</a> depicts the corruption and denoising steps.</p>

<figure><div id="bart-enoiser-objectives2" class="figure">
<img src="assets/dllm_0410.png" alt="BART Denoiser Objectives2" width="600" height="178"/>
<h6><span class="label">Figure 4-10. </span>Random token deletion in BART</h6>
</div></figure>
</dd>
<dt>Span masking</dt>
<dd>
<p>Text spans are sampled from text, with span lengths coming from a Poisson distribution. This means zero-length spans are possible. The spans are deleted from the text and replaced with a single mask token. Therefore, the model now has to also predict the number of tokens deleted. <a data-type="xref" href="#bart-enoiser-objectives3">Figure 4-11</a> depicts the corruption and denoising steps.</p>

<figure><div id="bart-enoiser-objectives3" class="figure">
<img src="assets/dllm_0411.png" alt="BART Denoiser Objectives3" width="600" height="146"/>
<h6><span class="label">Figure 4-11. </span>Span masking in BART</h6>
</div></figure>
</dd>
<dt>Document shuffling</dt>
<dd>
<p>Sentences in the input document are shuffled. The model is taught to arrange them in the right order. <a data-type="xref" href="#bart-enoiser-objectives4">Figure 4-12</a> depicts the corruption and denoising steps.</p>

<figure><div id="bart-enoiser-objectives4" class="figure">
<img src="assets/dllm_0412.png" alt="BART Denoiser Objectives4" width="600" height="28"/>
<h6><span class="label">Figure 4-12. </span>Document shuffling objective in BART</h6>
</div></figure>
</dd>
<dt>Document rotation</dt>
<dd>
<p>The document is rotated so that it starts from an arbitrary token<a data-type="indexterm" data-startref="xi_learningobjectivesMLM465758" id="id842"/><a data-type="indexterm" data-startref="xi_maskedlanguagemodelingMLM465758" id="id843"/><a data-type="indexterm" data-startref="xi_MLMmaskedlanguagemodeling465758" id="id844"/>. The model is trained to detect the correct start of the document. <a data-type="xref" href="#bart-enoiser-objectives5">Figure 4-13</a> depicts the corruption and denoising steps.</p>

<figure><div id="bart-enoiser-objectives5" class="figure">
<img src="assets/dllm_0413.png" alt="BART Denoiser Objectives5" width="600" height="28"/>
<h6><span class="label">Figure 4-13. </span>Document rotation objective in BART</h6>
</div></figure>
</dd>
</dl>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Which Learning Objectives Are Better?"><div class="sect2" id="id63">
<h2>Which Learning Objectives Are Better?</h2>

<p>It has been shown that models trained<a data-type="indexterm" data-primary="full language modeling (FLM)" id="id845"/><a data-type="indexterm" data-primary="masked language modeling (MLM)" id="id846"/><a data-type="indexterm" data-primary="MLM (masked language modeling)" id="id847"/> with FLM are better at generation, and models trained with MLM are better at classification tasks. However, it is inefficient to use different language models for different use cases. The consolidation effect continues to take hold, with the introduction of <a href="https://oreil.ly/xJc3U">UL2</a>, a paradigm that combines the best of different learning objective types in a single model.</p>

<p>UL2 mimics<a data-type="indexterm" data-primary="UL2 language model" id="id848"/> the effect of PLMs, MLMs, and PrefixLMs in a single paradigm<a data-type="indexterm" data-primary="Mixture of Denoisers paradigm" id="id849"/><a data-type="indexterm" data-primary="denoisers" id="id850"/> called <em>Mixture of Denoisers</em>.</p>

<p>The denoisers used are as follows<a data-type="indexterm" data-startref="xi_learningobjectives446663" id="id851"/>:</p>
<dl>
<dt>R-Denoiser</dt>
<dd>
<p>This is similar to the T5 span corruption task<a data-type="indexterm" data-primary="R-Denoiser" id="id852"/>. Spans between length 2–5 tokens are replaced by a single mask token. <a data-type="xref" href="#ul2-mixture-denoisers1">Figure 4-14</a> depicts the workings of the R-denoiser.</p>

<figure><div id="ul2-mixture-denoisers1" class="figure">
<img src="assets/dllm_0414.png" alt="UL2's Mixture of Denoisers1" width="600" height="222"/>
<h6><span class="label">Figure 4-14. </span>UL2’s R-Denoiser</h6>
</div></figure>
</dd>
<dt>S-Denoiser</dt>
<dd>
<p>Similar to prefix LM, the text is divided into a prefix<a data-type="indexterm" data-primary="S-Denoiser" id="id853"/> and a suffix. The suffix is masked, while the prefix has access to bidirectional context. <a data-type="xref" href="#ul2-mixture-denoisers2">Figure 4-15</a> depicts the workings of the S-Denoiser.</p>

<figure><div id="ul2-mixture-denoisers2" class="figure">
<img src="assets/dllm_0415.png" alt="UL2's Mixture of Denoisers2" width="600" height="204"/>
<h6><span class="label">Figure 4-15. </span>UL2’s S-Denoiser</h6>
</div></figure>
</dd>
<dt>X-Denoiser</dt>
<dd>
<p>This stands for extreme denoising<a data-type="indexterm" data-primary="X-Denoiser" id="id854"/>, where a large proportion of text is masked (often over 50%). <a data-type="xref" href="#ul2-mixture-denoisers3">Figure 4-16</a> depicts the workings of the X-Denoiser.</p>

<figure><div id="ul2-mixture-denoisers3" class="figure">
<img src="assets/dllm_0416.png" alt="UL2's Mixture of Denoisers3" width="600" height="132"/>
<h6><span class="label">Figure 4-16. </span>UL2’s X-Denoiser</h6>
</div></figure>
</dd>
</dl>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Pre-Training Models"><div class="sect1" id="id64">
<h1>Pre-Training Models</h1>

<p>Now that we have learned about the ingredients that go into a language model<a data-type="indexterm" data-primary="pre-training models" id="xi_pretrainingmodels479977"/><a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="pre-training models" id="xi_LLMsLargeLanguageModelspretrainingmodels479977"/> in detail, let’s learn how to pre-train one from scratch.</p>

<p>The language models of today are learning to model two types of concepts with one model:</p>

<ul>
<li>
<p>Language, the vehicle used to communicate facts, opinions, and feelings.</p>
</li>
<li>
<p>The underlying phenomena that led to the construction of text in the language.</p>
</li>
</ul>

<p>For many application areas, we are far more interested in learning to model the latter than the former. While a language model that is fluent in the language is welcome, we would prefer to see it get better at domains like science or law and skills like reasoning and arithmetic.</p>

<p>These concepts and skills are expressed in languages like English, which primarily serve a social function. Human languages are inherently ambiguous, contain lots of redundancies, and in general are inefficient vehicles to transmit underlying concepts.</p>

<p>This brings us to the question: are human languages even the best vehicle for language models to learn underlying skills and concepts? Can we separate the process of modeling the language from modeling the underlying concepts expressed through language?</p>

<p>Let’s put this theory to the test using an example. Consider training an LLM from scratch to learn to play the game of chess.</p>

<p>Recall the ingredients of a language model from <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>. We need:</p>

<ul>
<li>
<p>A pre-training dataset</p>
</li>
<li>
<p>A vocabulary and tokenization scheme</p>
</li>
<li>
<p>A model architecture</p>
</li>
<li>
<p>A learning objective</p>
</li>
</ul>

<p>For training the chess language model, we can choose the Transformer architecture with the next-token prediction learning objective, which is the de facto paradigm used today.</p>

<p>For the pre-training dataset, we can use the chess games dataset<a data-type="indexterm" data-primary="Lichess dataset" id="id855"/> from <a href="https://oreil.ly/XmWvv">Lichess</a>, containing billions of games. We select a subset of 20 million chess games for our training.</p>

<p>This dataset is in the Portable Game Notation (PGN) format<a data-type="indexterm" data-primary="Portable Game Notation (PGN) format" id="xi_PortableGameNotationPGNformat482659"/><a data-type="indexterm" data-primary="PGN (Portable Game Notation) format" id="xi_PGNPortableGameNotationformat482659"/>, which is used to represent the sequence of chess moves in a concise notation.</p>

<p>Finally, we have to choose the vocabulary of the model. Since the only purpose of this model is to learn chess, we don’t need to support an extensive English vocabulary. In fact, we can take advantage of the PGN notation to assign tokens to specific chess concepts.</p>

<p>Here is an example of a chess game in PGN format, taken from <a href="https://oreil.ly/H3yOs">pgnmentor.com</a>:</p>

<pre data-type="programlisting">1. e4 c5 2. Nf3 a6 3. d3 g6 4. g3 Bg7 5. Bg2 b5 6. O-O Bb7 7. c3 e5 8. a3 Ne7
9. b4 d6 10. Nbd2 O-O 11. Nb3 Nd7 12. Be3 Rc8 13. Rc1 h6 14. Nfd2 f5 15. f4
Kh7 16. Qe2 cxb4 17. axb4 exf4 18. Bxf4 Rxc3 19. Rxc3 Bxc3 20. Bxd6 Qb6+ 21.
Bc5 Nxc5 22. bxc5 Qe6 23. d4 Rd8 24. Qd3 Bxd2 25. Nxd2 fxe4 26. Nxe4 Nf5 27.
d5 Qe5 28. g4 Ne7 29. Rf7+ Kg8 30. Qf1 Nxd5 31. Rxb7 Qd4+ 32. Kh1 Rf8 33. Qg1
Ne3 34. Re7 a5 35. c6 a4 36. Qxe3 Qxe3 37. Nf6+ Rxf6 38. Rxe3 Rd6 39. h4 Rd1+
40. Kh2 b4 41. c7 1-0</pre>

<p>The rows of the board are assigned letters a–h and the columns are assigned numbers 1–8. Except for pawns, each piece type is assigned a capital letter, with N for knight, R for rook, B for bishop, Q for queen, and K for king. A + appended to a move indicates a check, a % appended to the move indicates a checkmate, and 0-0 is used to indicate castling. If you are unfamiliar with the rules of chess, refer to <a href="https://oreil.ly/EbcfQ">this piece for a primer</a>.</p>

<p>Based on this notation, the vocabulary can consist of:</p>

<ul>
<li>
<p>A separate token for each square on the board, with 64 total (a1, a2, a3…​h6, h7, h8)</p>
</li>
<li>
<p>A separate token for each piece type (N, B, R, K, Q)</p>
</li>
<li>
<p>Tokens for move numbers (1., 2., 3., etc.)</p>
</li>
<li>
<p>Tokens for special moves (+ for check, x for capture, etc.)</p>
</li>
</ul>

<p>Now, let’s train a language model from scratch on this chess dataset using our special domain-specific vocabulary. The model is directly learning from the PGN notation with no human language text present in the dataset. The book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a> contains the code and setup for training this model.</p>

<p>After training the model for three epochs, let’s test the model’s ability to play chess. We can see that the model seems to have learned the rules of the game without having to be provided the rules explicitly in natural language. In fact, the model can even beat human players some of the time and can execute moves like castling.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id856">
<h1>Exercise</h1>
<p>While the model trained using the reference implementation is impressive enough to complete chess games and occasionally beat players, we can do a lot better. This can be done by increasing the size of the model, increasing the size of the dataset, and increasing the quality of the dataset. Try improving the model along each of these axes and track the improvement in its chess-playing abilities.</p>
</div></aside>

<p>Note that this model was able to learn the concepts (chess) using a domain-specific language (PGN). How will we fare if the concepts were taught in natural language?</p>

<p>Let’s explore this in another experiment. Take the same dataset used to pre-train the chess language model and run it through an LLM to convert each move in PGN to a sentence in English. An example game would look like:</p>

<p><em>White moves pawn to e4</em></p>

<p><em>Black moves bishop to g7</em></p>

<p>and so on. Train a new language model on the same number of games as the previous one, but this time with the English-language dataset. Let the vocabulary of this model be the standard English vocabulary generated by training the tokenizer over the training set.</p>

<p>How does this compare to the chess LM trained on the PGN dataset? The model trained on English descriptions of chess moves performs worse and doesn’t seem to have understood the rules of the game yet, despite being trained on the same number of games as the other model.</p>

<p>This shows that natural language is not necessarily the most efficient vehicle for a model to learn skills and concepts, and domain-specific languages and notations perform better.</p>

<p>Thus, language design is an important skill to acquire, enabling you to create domain-specific languages for learning concepts and skills. For your application areas, you could use existing domain-specific languages or create a new one yourself<a data-type="indexterm" data-startref="xi_pretrainingmodels479977" id="id857"/><a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelspretrainingmodels479977" id="id858"/><a data-type="indexterm" data-startref="xi_PortableGameNotationPGNformat482659" id="id859"/><a data-type="indexterm" data-startref="xi_PGNPortableGameNotationformat482659" id="id860"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id362">
<h1>Summary</h1>

<p>In this chapter, we discussed the various components of the Transformer architecture in detail, including self-attention, feedforward networks, position encodings, and layer normalization. We also discussed several variants and configurations such as encoder-only, encoder-decoder, decoder-only, and MoE models. Finally, we learned how to put our knowledge of language models together to train our own model from scratch and how to design domain-specific languages for more efficient learning.</p>
</div></section>
</div></section></div>
</div>
</body></html>