["```py\ndef entropy_gaussian_formula(sigma):\n    return 0.5 * torch.log(2 * math.pi * math.e * sigma * sigma) ①\n\np = Normal(0, 10)                                                ②\n\nH_formula = entropy_gaussian_formula(p.stddev)                   ③\n\nH = p.entropy()                                                  ④\n\nassert torch.isclose(H_formula, H)                               ⑤\n```", "```py\ndef cross_entropy(X_gt, X_pred):\n    H_c = 0\n    for x_gt, x_pred in zip(X_gt, X_pred):\n        H_c += -1 * (x_gt * torch.log (x_pred))     ①\n    return H_c\n\nX_gt = torch.Tensor([1., 0., 0., 0.])               ②\n\nX_good_pred = torch.Tensor([0.8, 0.15, 0.04, 0.01]) ③\n\nX_bad_pred = torch.Tensor([0.25, 0.25, 0.25, 0.25]) ④\n\nH_c_good = cross_entropy(X_gt, X_good_pred)         ⑤\n\nH_c_bad = cross_entropy(X_gt, X_bad_pred)           ⑥\n```", "```py\nfrom torch.distributions import kl_divergence\n\np = Normal(0, 5)\nq = Normal(0, 10)             ①\nr = Normal(0, 20)\n\nkld_p_p = kl_divergence(p, p)\nkld_p_q = kl_divergence(p, q)\nkld_q_p = kl_divergence(q, p) ②\nkld_p_r = kl_divergence(p, r) ③\n\nassert kld_p_p == 0           ④\n\nassert kld_p_q != kld_q_p     ⑤\n\nassert kld_p_q < kld_p_r \\5\n```", "```py\nsample_mean = X.mean()                          ①\n\nsample_std = X.std()\n\ngaussian_mle = Normal(sample_mean, sample_std)  ②\n\na, b = torch.Tensor([160]), torch.Tensor([170]) ③\n\nprob = gaussian_mle.cdf(b) - gaussian_mle.cdf(a)\n```", "```py\ndef neg_log_likelihood(X, mu, sigma):                      ①\n    N = X.shape[0]\n    X_minus_mu = torch.sub(X, mu)\n    t1 = torch.mul(0.5 * N,\n               torch.log(2 * np.pi * torch.pow(sigma, 2))) ②\n\n    t2 = torch.div(torch.matmul(X_minus_mu.T, X_minus_mu),\n                 2 * torch.pow(sigma, 2))                  ③\n\n    return t1 + t2                                         ④\n```", "```py\ndef minimize(X, mu, sigma, loss_fn, num_iters=100, lr = 0.001): ①\n\n    ②\n   for i in range(num_iters):\n\n        loss = loss_fn(X, mu, sigma)                            ③\n\n        loss.backward()                                         ④\n\n        mu.data -= lr * mu.grad\n        sigma.data -= lr * sigma.grad                           ⑤\n\n        mu.grad.data.zero_()\n        sigma.grad.data.zero_()                                 ⑥\n\nmu = Variable(torch.Tensor([5]).type(dtype), requires_grad=True)\nsigma = Variable(torch.Tensor([5]).type(dtype), requires_grad=True)\n\nminimize(X, mu, sigma, neg_log_likelihood)\n```", "```py\ndef neg_log_likelihood_reg(X, mu, sigma, k=0.2):               ①\n    N = X.shape[0]\n    X_minus_mu = torch.sub(X, mu)\n    t1 = torch.mul(0.5 * N,\n                   torch.log(2 * np.pi * torch.pow(sigma, 2))) ②\n\n    t2 = torch.div(torch.matmul(X_minus_mu.T, X_minus_mu),\n                   2 * torch.pow(sigma, 2))                     ③\n\n    loss_likelihood = t1 + t2                                   ④\n\n    loss_reg = k * (torch.pow(mu, 2) + torch.pow(sigma, 2))     ⑤\n\n    return loss_likelihood + loss_reg                           ⑥\n```", "```py\nfrom torch.distributions.mixture_same_family import MixtureSameFamily ①\n\npi = Categorical(torch.tensor([0.4, 0.4, 0.2]))                       ②\n\nmu = torch.tensor([[175.0, 70.0], [152.0, 55.0], [135.0, 40.0]])      ③\n\nsigma = torch.tensor([[[30.0, 20.0], [20.0, 30.0]],                   ④\n                     [[50.0, 0.0], [0.0, 10.0]],\n                     [[20.0, 0.0], [0.0, 20.0]]])\n\ngaussian_components = MultivariateNormal(mu, sigma)                   ⑤\n\ngmm = MixtureSameFamily(pi, gaussian_components)                      ⑥\n```", "```py\nwhile (curr_likelihood - prev_likelihood) < 1e-4:     ①\n\n  # E Step                                            ②\n\n  pi = gmm.mixture_distribution.probs                 ③\n\n  components = gmm.component_distribution             ④\n\n   ⑤\n   log_gamma_numerators = components.log_prob(\n       X.unsqueeze(1)) + torch.log(pi).repeat(n, 1)   ⑥\n\n   ⑦\n   log_gamma_denominators = torch.logsumexp(\n       log_gamma_numerators, dim=1, keepdim=True).\n\n   ⑧\n   log_gamma = log_gamma_numerators - log_gamma_denominators\n   self.gamma = torch.exp(log_gamma)\n\n   # M Step                                           ⑨\n\n   n = X.shape[0]                                     ⑩\n\n   N = torch.sum(gamma, 0)\n\n   pi = N / n                                         ⑪\n\n   mu = ((X.T @ gamma)/N).T                           ⑫\n\n   x_minus_mu = (X.repeat(K, 1, 1) - gmm.component_distribution.unsqueeze(1).\n                 repeat(1, n, 1))\n\n   ⑬\n   x_minus_mu_squared = x_minus_mu.unsqueeze(3)  @ x_minus_mu.unsqueeze(2)\n\n   ⑭\n   sigma = torch.sum(gamma.T.unsqueeze(2).unsqueeze(3) * x_minus_mu_squared,\n                     axis=1) / N.unsqueeze(1).unsqueeze(1).repeat(1, d, d)\n\n   prev_likelihood = curr_likelihood\n\n   curr_likelihood = torch.sum(gmm.log_prob(X))       ⑮\n```"]