- en: Chapter 16\. Simulating a Self-Driving Car Using End-to-End Deep Learning with
    Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Contributed by guest authors: Aditya Sharma and Mitchell Spryn'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Batmobile to Knightrider, from robotaxis to autonomous pizza delivery,
    self-driving cars have captured the imagination of both modern pop culture and
    the mainstream media. And why wouldn’t they? How often does it happen that a science-fiction
    concept is brought to life? More important, autonomous cars promise to address
    some key challenges urban societies are facing today: road accidents, pollution
    levels, cities becoming increasingly more congested, hours of productivity lost
    in traffic, and the list goes on.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'It should come as no surprise that a full self-driving system is quite complex
    to build, and cannot be tackled in one book chapter. Much like an onion, any complex
    problem contains layers that can be peeled. In our case, we intend to tackle one
    fundamental problem here: how to steer. Even better, we don’t need a real car
    to do this. We will be training a neural network to drive our car autonomously
    within a simulated environment, using realistic physics and visuals.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: But first, a brief bit of history.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![The SAE Levels of Driving Automation (image source)](../images/00220.jpeg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: Figure 16-1\. The SAE Levels of Driving Automation ([image source](https://oreil.ly/RHKUt))
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A Brief History of Autonomous Driving
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though it might seem like autonomous driving is a very recent technological
    development, research in this field originated more than 30 years ago. Scientists
    at Carnegie Mellon University first tried their hands at this seemingly impossible
    feat back in 1984 when they started working on what would be known as the Navlab
    1\. “Navlab” was a shorthand for Carnegie Mellon’s Navigation Laboratory, a not-so-fancy
    name for what would turn out to be the first step into a very impressive future
    for humanity. The Navlab 1 was a Chevrolet panel van ([Figure 16-2](part0019.html#the_navlab_1_in_all_its_glory_left_paren))
    with racks of hardware onboard that included, among other things, workstations
    for the research team and a Sun Microsystems supercomputer. It was considered
    cutting edge at the time, a self-contained computing system on wheels. Long-distance
    wireless communication was pretty much nonexistent back then, and cloud computing
    was not even a concept. Putting the entire research team in the back of a highly
    experimental self-driving van might seem like a dangerous idea, but the Navlab
    had a maximum speed of 20 MPH and navigated on only empty roads. This ensured
    the safety of the scientists working onboard. What is very impressive to note
    though, is that the technology used in this car laid the foundation for technology
    used in the self-driving cars of today. For example, Navlab 1 used video cameras
    and a rangefinder (an early version of lidar) to observe its surroundings. For
    navigation, they used a single-layer neural network to predict steering angles
    given sensor data. Pretty neat isn’t it?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![The NavLab 1 in all its glory (image source)](../images/00006.jpeg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Figure 16-2\. The NavLab 1 in all its glory ([image source](https://oreil.ly/b2Bnn))
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep Learning, Autonomous Driving, and the Data Problem
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even 35 years ago scientists knew that neural networks were going to play a
    key role in making self-driving cars a reality. Back then, of course, we did not
    have the technology needed (GPUs, cloud computing, FPGAs, etc.) to train and deploy
    deep neural networks at scale for autonomous driving to become a reality. Today’s
    autonomous cars use deep learning to perform all kinds of tasks. [Table 16-1](part0019.html#examples_of_deep_learning_applications_i)
    lists some examples.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Table 16-1\. Examples of deep learning applications in self-driving cars
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task** | **Examples** |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
- en: '| **Perception** | Detect drivable area, lane markings, intersections, crosswalks,
    etc. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
- en: '| Detect other vehicles, pedestrians, animals, objects on the road, etc. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
- en: '| Detect traffic signs and signals |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 检测交通标志和信号 |'
- en: '| **Navigation** | Predict and output steering angle, throttle, etc., based
    on sensor input |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **导航** | 根据传感器输入预测和输出转向角、油门等 |'
- en: '| Perform overtaking, lane changes, U-turns, etc. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 进行超车、变道、掉头等操作 |'
- en: '| Perform merges, exits, navigate roundabouts, etc. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 进行并线、出口、穿越环岛等操作 |'
- en: '| Drive in tunnels, over bridges, etc. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 在隧道、桥梁等地方驾驶 |'
- en: '| Respond to traffic rule breakers, wrong-way drivers, unexpected environment
    changes, etc. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 响应交通违规者、逆行驾驶者、意外环境变化等 |'
- en: '| Navigate stop-and-go traffic |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 穿梭于交通拥堵中 |'
- en: We know that deep learning requires data. In fact, a general rule of thumb is
    that more data ensures better model performance. In previous chapters, we saw
    that training image classifiers can require millions of images. Imagine how much
    data it would take to train a car to drive itself. Things are also very different
    with self-driving cars when it comes to the data needed for validation and testing.
    Not only must a car be able to drive, it must do it safely. Testing and validating
    model performance is therefore fundamentally critical and requires a lot more
    data than what is needed for training, unlike traditional deep learning problems.
    However, it is difficult to accurately predict the exact amount of data needed,
    and estimates vary. [A 2016 study](https://oreil.ly/mOUeJ) shows that it would
    take 11 billion miles of driving for a car to become as good as a human driver.
    For a fleet of 100 self-driving cars collecting data 24 hours per day, 365 days
    per year at an average speed of 25 miles per hour, this would take more than 500
    years!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道深度学习需要数据。事实上，一个经验法则是更多的数据可以确保更好的模型性能。在之前的章节中，我们看到训练图像分类器可能需要数百万张图像。想象一下训练一辆汽车自动驾驶需要多少数据。当涉及到验证和测试所需的数据时，自动驾驶汽车的情况也非常不同。汽车不仅必须能够行驶，还必须安全行驶。因此，测试和验证模型性能是基本关键的，需要比传统深度学习问题所需的数据多得多。然而，准确预测所需数据的确切数量是困难的，估计各不相同。[一项2016年的研究](https://oreil.ly/mOUeJ)显示，一辆汽车要想达到与人类驾驶员一样的水平，需要行驶110亿英里。对于一队100辆自动驾驶汽车，每天24小时收集数据，以25英里每小时的平均速度行驶，这将需要超过500年！
- en: 'Now, of course, it is highly impractical and costly to collect 11 billion miles
    of data by having fleets of cars drive around in the real world. This is why almost
    everyone working in this space—be it a large automaker or a startup—uses simulators
    to collect data and validate trained models. [Waymo](https://waymo.com/tech) (Google’s
    self-driving car team), for example, had driven nearly 10 million miles on the
    road as of late 2018\. Even though this is more than any other company on the
    planet it represents less than one percent of our 11-billion-mile figure. On the
    other hand, Waymo has driven seven billion miles in simulation. Although everyone
    uses simulation for validation, some companies build their own simulation tools,
    whereas others license them from companies like Cognata, Applied Intuition, and
    AVSimulation. There are some great open source simulation tools available as well:
    [AirSim](https://github.com/Microsoft/AirSim) from Microsoft, [Carla](http://carla.org/)
    from Intel, and [Apollo simulation](https://oreil.ly/rag7v) from Baidu. Thanks
    to these tools, we don’t need to connect the CAN bus of our car to learn the science
    behind making it drive itself.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，通过让车队在现实世界中行驶110亿英里来收集数据是非常不切实际和昂贵的。这就是为什么几乎每个在这个领域工作的人——无论是大型汽车制造商还是初创公司——都使用模拟器来收集数据和验证训练模型。例如，[Waymo](https://waymo.com/tech)（谷歌的自动驾驶汽车团队）截至2018年底在道路上行驶了近1000万英里。尽管这比地球上任何其他公司都多，但它仅占我们110亿英里的不到1%。另一方面，Waymo在模拟中行驶了70亿英里。尽管每个人都使用模拟进行验证，但有些公司构建自己的模拟工具，而其他公司则从Cognata、Applied
    Intuition和AVSimulation等公司许可。还有一些很棒的开源模拟工具可用：来自微软的[AirSim](https://github.com/Microsoft/AirSim)，来自英特尔的[Carla](http://carla.org/)，以及来自百度的[Apollo模拟](https://oreil.ly/rag7v)。由于有了这些工具，我们不需要连接我们汽车的CAN总线来学习让它自动驾驶的科学。
- en: In this chapter, we use a custom-built version of AirSim, built specifically
    for Microsoft’s [Autonomous Driving Cookbook](https://oreil.ly/uzOGl).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了一个专为微软的[自动驾驶食谱](https://oreil.ly/uzOGl)定制的AirSim版本。
- en: 'The “Hello, World!” of Autonomous Driving: Steering Through a Simulated Environment'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶的“Hello, World！”：在模拟环境中操纵方向盘
- en: 'In this section, we implement the “Hello, World!” problem of autonomous driving.
    Self-driving cars are complex, with dozens of sensors streaming gigabytes of data
    and multiple decisions being made while driving down a road. Much like programming,
    for the “Hello, World!” of self-driving cars, we strip the requirements down to
    basic fundamentals:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们实现了自动驾驶的“Hello, World！”问题。自动驾驶汽车是复杂的，有数十个传感器传输着大量数据，同时在行驶中做出多个决策。就像编程一样，对于自动驾驶汽车的“Hello,
    World！”，我们将要求简化为基本原理：
- en: The car always stays on the road.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汽车始终保持在道路上。
- en: For external sensor input, the car uses a single image frame from a single camera
    mounted on the front of the hood. No other sensors (lidar, radar, etc.) are used.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于外部传感器输入，汽车使用安装在前引擎盖上的单个摄像头的单个图像帧。不使用其他传感器（激光雷达、雷达等）。
- en: Based on this single-image input, going at a constant speed, the car predicts
    its output steering angle. No other possible outputs (brake, throttle, gear changes,
    etc.) are predicted.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于这个单图像输入，以恒定速度行驶，汽车预测其输出的转向角。不会预测其他可能的输出（刹车、油门、换挡等）。
- en: There are no other vehicles, pedestrians, animals, or anything else on the road
    or relevant surroundings.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道路上没有其他车辆、行人、动物或其他任何东西，也没有相关的环境。
- en: The road being driven on is single lane, with no markings or traffic signs and
    signals and no rules for staying on the left or right side of the road.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所行驶的道路是单车道，没有标线、交通标志和信号，也没有保持在道路左侧或右侧的规则。
- en: The road mostly changes in terms of turns (which will require changing steering
    angle to navigate) and not in terms of elevation (which would require a change
    in throttle, applying brakes, etc.).
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道路主要在转弯方面变化（这将需要改变转向角来导航），而不是在高程方面变化（这将需要改变油门，刹车等）。
- en: Setup and Requirements
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置和要求
- en: We will be implementing the “Hello, World!” problem in AirSim’s Landscape map
    ([Figure 16-4](part0019.html#the_landscape_map_in_airsim)). AirSim is an open
    source photo-realistic simulation platform and is a popular research tool for
    training data collection and validation of deep learning–based autonomous system
    model development.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在AirSim的景观地图中实现“Hello, World!”问题（[图16-4](part0019.html#the_landscape_map_in_airsim)）。AirSim是一个开源的逼真模拟平台，是一个用于训练数据收集和验证基于深度学习的自主系统模型开发的研究工具。
- en: '![The Landscape map in AirSim](../images/00252.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![AirSim中的景观地图](../images/00252.jpeg)'
- en: Figure 16-4\. The Landscape map in AirSim
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-4。AirSim中的景观地图
- en: 'You can find the code for this chapter in the [end-to-end deep learning tutorial](https://oreil.ly/_IJl9)
    from the [*Autonomous Driving Cookbook*](https://oreil.ly/uF5Bz) on GitHub in
    the form of Jupyter Notebooks. In the cookbook repository, go to [*AirSimE2EDeepLearning/*](https://oreil.ly/YVKPp).
    We can do so by running the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上的[*Autonomous Driving Cookbook*](https://oreil.ly/uF5Bz)中找到本章的代码，形式为Jupyter笔记本的[端到端深度学习教程](https://oreil.ly/_IJl9)。在食谱仓库中，转到[*AirSimE2EDeepLearning/*](https://oreil.ly/YVKPp)。我们可以通过运行以下命令来实现：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will use Keras, a library with which we are already familiar, to implement
    our neural network. It’s not necessary for us to learn any new deep learning concepts
    to work on this problem other than what has already been introduced in prior chapters
    of this book.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Keras这个我们已经熟悉的库来实现我们的神经网络。在这个问题上，除了本书之前章节介绍的内容，我们不需要学习任何新的深度学习概念。
- en: For this chapter, we have created a dataset by driving around in AirSim. The
    uncompressed size of the dataset is 3.25 GB. This is a little larger than datasets
    we’ve been using, but remember, we are implementing the “Hello, World!” of a highly
    complex problem. For comparison, it is fairly normal practice for automakers to
    collect multiple petabytes of data on the road every day. You can download the
    dataset by going to [*aka.ms/AirSimTutorialDataset*](https://aka.ms/AirSimTutorialDataset).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们通过在AirSim中驾驶来创建了一个数据集。数据集的未压缩大小为3.25 GB。这比我们之前使用的数据集稍大一些，但请记住，我们正在实现一个非常复杂问题的“Hello,
    World!”。相比之下，汽车制造商每天在道路上收集多个PB的数据是相当正常的做法。您可以通过访问[*aka.ms/AirSimTutorialDataset*](https://aka.ms/AirSimTutorialDataset)来下载数据集。
- en: We can run the provided code and train our model on a Windows or Linux machine.
    We created a standalone build of AirSim for this chapter, which you can download
    from [*aka.ms/ADCookbookAirSimPackage*](http://aka.ms/ADCookbookAirSimPackage).
    After you’ve downloaded it, extract the simulator package to the location of your
    choice. Take note of the path because you will need it later. Please note that
    this build runs only on Windows but is needed only to see our final trained model
    in action. If you’re using Linux, you can take the model files and run them on
    a Windows machine with the provided simulator package.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Windows或Linux机器上运行提供的代码并训练我们的模型。我们为本章创建了AirSim的独立构建，您可以从[*aka.ms/ADCookbookAirSimPackage*](http://aka.ms/ADCookbookAirSimPackage)下载。下载后，请将模拟器包提取到您选择的位置。请注意路径，因为您以后会需要它。请注意，此构建仅在Windows上运行，但仅需要查看我们最终训练好的模型。如果您使用Linux，可以将模型文件拷贝到提供的模拟器包的Windows机器上运行。
- en: AirSim is a highly photo-realistic environment, which means it is capable of
    generating lifelike pictures of the environment for the model to train on. You
    may have encountered similar graphics while playing high-definition video games.
    Given the size of the data and the graphic quality of the provided simulator package,
    a GPU is certainly preferable for running the code as well as the simulator.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: AirSim是一个高度逼真的环境，这意味着它能够生成逼真的环境图片供模型训练。在玩高清视频游戏时，你可能遇到过类似的图形。考虑到提供的模拟器包的数据量和图形质量，GPU肯定是运行代码和模拟器的首选。
- en: Finally, there are some additional tools and Python dependencies needed to run
    the code provided. You can find the details for these at [Environment Setup in
    the README file](https://oreil.ly/RzZe7) in the code repository. At a high level,
    we need Anaconda with Python 3.5 (or higher), TensorFlow (to run as a backend
    to Keras), and h5py (for storing and reading data and model files). After we have
    our Anaconda environment set up, we can install the needed Python dependencies
    by running the *InstallPackages.py* file as root or administrator.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，运行提供的代码还需要一些额外的工具和Python依赖项。您可以在代码仓库的[README文件中的环境设置](https://oreil.ly/RzZe7)中找到这些详细信息。在高层次上，我们需要Anaconda与Python
    3.5（或更高版本）、TensorFlow（作为Keras的后端运行）和h5py（用于存储和读取数据和模型文件）。设置好Anaconda环境后，我们可以通过以root或管理员身份运行*InstallPackages.py*文件来安装所需的Python依赖项。
- en: '[Table 16-2](part0019.html#setup_and_requirements_summary) provides a summary
    of all the requirements that we just defined.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[表16-2](part0019.html#setup_and_requirements_summary)提供了我们刚刚定义的所有要求的摘要。'
- en: Table 16-2\. Setup and requirements summary
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表16-2。设置和要求摘要
- en: '| **Item** | **Requirements/Link** | **Notes/Comments** |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **项目** | **要求/链接** | **备注/评论** |'
- en: '| --- | --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Code repository | [*https://oreil.ly/_IJl9*](https://oreil.ly/_IJl9) | Can
    be run on a Windows or Linux machine |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 代码仓库 | [*https://oreil.ly/_IJl9*](https://oreil.ly/_IJl9) | 可在Windows或Linux机器上运行
    |'
- en: '| Jupyter Notebooks used | [*DataExplorationAndPreparation.ipynb*](https://oreil.ly/6yvCq)[*TrainModel.ipynb*](https://oreil.ly/rcR47)[*TestModel.ipynb*](https://oreil.ly/FE-EP)
    |   |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 使用的Jupyter笔记本 | [*DataExplorationAndPreparation.ipynb*](https://oreil.ly/6yvCq)[*TrainModel.ipynb*](https://oreil.ly/rcR47)[*TestModel.ipynb*](https://oreil.ly/FE-EP)
    |   |'
- en: '| Dataset download | [*aka.ms/AirSimTutorialDataset*](http://aka.ms/AirSimTutorialDataset)
    | 3.25 GB in size; needed to train model |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 数据集下载 | [*aka.ms/AirSimTutorialDataset*](http://aka.ms/AirSimTutorialDataset)
    | 大小为3.25 GB；用于训练模型 |'
- en: '| Simulator download | [*aka.ms/ADCookbookAirSimPackage*](http://aka.ms/ADCookbookAirSimPackage)
    | Not needed to train model, only to deploy and run it; runs only on Windows |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 模拟器下载 | [*aka.ms/ADCookbookAirSimPackage*](http://aka.ms/ADCookbookAirSimPackage)
    | 不需要用于训练模型，只用于部署和运行；仅在Windows上运行 |'
- en: '| GPU | Recommended for training, required for running simulator |   |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| GPU | 推荐用于训练，运行模拟器所需 |   |'
- en: '| Other tools + Python dependencies | [Environment Setup section in README
    file in the repository](https://oreil.ly/RzZe7) |   |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 其他工具+Python依赖项 | [存储库中README文件中的环境设置部分](https://oreil.ly/RzZe7) |   |'
- en: With all that taken care of, let’s get started!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些准备工作，让我们开始吧！
- en: Data Exploration and Preparation
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索和准备
- en: As we journey deeper into the world of neural networks and deep learning, you
    will notice that much of the magic of machine learning doesn’t come from how a
    neural network is built and trained, but from the data scientist’s understanding
    of the problem, the data, and the domain. Autonomous driving is no different.
    As you will see shortly, having a deep understanding of the data and the problem
    we are trying to solve is key when teaching our car how to drive itself.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入探索神经网络和深度学习的世界时，您会注意到，机器学习的许多魔力并不是来自神经网络是如何构建和训练的，而是来自数据科学家对问题、数据和领域的理解。自动驾驶也不例外。正如您很快将看到的，深入了解数据和我们试图解决的问题对于教会我们的汽车如何自动驾驶至关重要。
- en: 'All the steps here are also detailed in the Jupyter Notebook [*DataExplorationAndPreparation.ipynb*](https://oreil.ly/6aE7z).
    We start by importing all the necessary modules for this part of the exercise:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的所有步骤也都在Jupyter Notebook [*DataExplorationAndPreparation.ipynb*](https://oreil.ly/6aE7z)中详细说明。我们首先导入这部分练习所需的所有必要模块：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we make sure our program can find the location of our unzipped dataset.
    We also provide it a location to store our processed (or “cooked”) data:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们确保我们的程序可以找到我们解压缩的数据集的位置。我们还为其提供一个位置来存储我们处理（或“烹饪”）的数据：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Looking at the provided dataset we will see that it contains nine folders:
    *normal_1* through *normal_6* and *swerve_1* through *swerve_3*. We come back
    to the naming of these folders later. Within each of these folders are images
    as well as *.tsv* (or *.txt*) files. Let’s check out one of these images:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 查看提供的数据集，我们会看到它包含九个文件夹：*normal_1*到*normal_6*和*swerve_1*到*swerve_3*。稍后我们会回到这些文件夹的命名。在每个文件夹中都有图像以及*.tsv*（或*.txt*）文件。让我们看看其中一个图像：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We should see the following output. We can see that this image was captured
    by a camera placed centrally on the hood of the car (which is slightly visible
    at the bottom in [Figure 16-5](part0019.html#plot_showing_the_contents_of_the_file_no)).
    We can also see the Landscape environment used for this problem from the simulator.
    Notice that no other vehicles or objects are on the road, keeping in line with
    our requirements. Although the road appears to be snowy, for our experiment, the
    snow is only visual and not physical; that is, it will have no effect on the physics
    and movement of our car. Of course, in the real world, it is critical to accurately
    replicate the physics of snow, rain, etc., so that our simulators can produce
    highly accurate versions of reality. This is a very complicated task to undertake
    and many companies dedicate a lot of resources to it. Thankfully for our purpose,
    we can safely ignore all that and treat the snow as just white pixels in our image.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下输出。我们可以看到这张图像是由放置在汽车引擎盖中央的摄像头拍摄的（在[图16-5](part0019.html#plot_showing_the_contents_of_the_file_no)中底部略有可见）。我们还可以从模拟器中看到用于这个问题的景观环境。请注意，道路上没有其他车辆或物体，符合我们的要求。尽管道路看起来是多雪的，但对于我们的实验来说，雪只是视觉上的，不是实际的；也就是说，它不会对我们汽车的物理和运动产生影响。当然，在现实世界中，准确复制雪、雨等的物理特性非常重要，以便我们的模拟器可以生成高度准确的现实版本。这是一个非常复杂的任务，许多公司都投入了大量资源。幸运的是，对于我们的目的，我们可以安全地忽略所有这些，并将雪视为图像中的白色像素。
- en: '![Plot showing the contents of the file normal_1/images/img_0.png](../images/00211.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![显示文件normal_1/images/img_0.png的内容的图](../images/00211.jpeg)'
- en: Figure 16-5\. Plot showing the contents of the file normal_1/images/img_0.png
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-5\. 显示文件normal_1/images/img_0.png的内容的图
- en: 'Let’s also get a preview of what the rest of the data looks like by displaying
    the contents of one of the *.tsv/.txt* files:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过显示一个*.tsv/.txt*文件的内容来预览其余数据是什么样的：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We should see the following output:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下输出：
- en: '![Image](../images/00177.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/00177.jpeg)'
- en: 'There is a lot going on here, so let’s break it down. First, we can see that
    each row in the table corresponds to an image frame (we are displaying only the
    first five frames here). Because this is at the beginning of our data collection
    drive, we can see that the car hasn’t started moving yet: the speed and throttle
    are 0 and the gear is neutral. Because we are trying to predict only steering
    angles for our problem, we won’t be predicting the speed, throttle, brake, or
    gear values. These values will be used, however, as part of the input data (more
    on this later).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多内容，让我们来分解一下。首先，我们可以看到表中的每一行对应一个图像帧（这里我们只显示了前五帧）。因为这是我们数据收集驱动的开始，我们可以看到汽车还没有开始移动：速度和油门都是0，档位是空档。因为我们只是尝试预测问题中的转向角，我们不会预测速度、油门、刹车或档位的值。然而，这些值将作为输入数据的一部分（稍后会详细介绍）。
- en: Identifying the Region of Interest
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别感兴趣区域
- en: Let’s get back to our sample image now. All the observations we made so far
    were from the eyes of a human being. For a moment, let’s take a look at the image
    again, only this time we step into the shoes (or tires) of a car that is just
    beginning to learn how to drive itself and is trying to understand what it is
    seeing.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在回到我们的示例图像。到目前为止，我们所做的所有观察都是从人类的眼睛看到的。让我们再次看一下这幅图像，只是这一次我们换成了一辆刚开始学习自动驾驶并试图理解所看到的东西的汽车的角度（或轮胎）。
- en: If I’m the car, looking at this image that my camera presented me, I can divide
    it into three distinct parts ([Figure 16-6](part0019.html#the_three_parts_of_the_image_as_seen_by)).
    First, there is the lower third, which more or less looks consistent. It is made
    up of mostly straight lines (the road, lane divider, road fences, etc.) and has
    a uniform coloring of white, black, gray, and brown. There is also a weird black
    arc at the very bottom (this is the hood of the car). The upper third of the image,
    like the lower third, is also consistent. It is mostly gray and white (the sky
    and the clouds). Lastly, there is the middle third and it has a lot going on.
    There are big brown and grey shapes (mountains), which are not uniform at all.
    There are also four tall green figures (trees), their shape and color are different
    from anything else I see, so they must be super important. As I am presented more
    images, the upper third and the lower third don’t change all that much but the
    middle third undergoes a lot of change. Hence, I conclude that this is the part
    that I should be focusing on the most.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![The three parts of the image as seen by the self-driving car](../images/00091.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: Figure 16-6\. The three parts of the image as seen by the self-driving car
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Do you see the challenge in front of our car? It has no way of knowing what
    part of the image being presented to it is important. It might try to associate
    the changing steering angles with the changing scenery instead of focusing on
    turns in the road, which are very subtle compared to everything else that is going
    on. Not only are the changes in the scenery confusing, but they are also not relevant
    to the problem we are trying to solve here at all. The sky above, although mostly
    unchanging, also does not provide us with any information relevant to the steering
    of our car. Finally, the part of the hood being captured in every picture is also
    nonrelevant.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us fix this by focusing only on the relevant portion of the image. We do
    this by creating a region of interest (ROI) in our image. As we can imagine, there
    is no hard-and-fast rule to dictate what our ROI should look like. It really depends
    on our use case. For us, because the camera is in a fixed position and there are
    no elevation changes on the road, the ROI is a simple rectangle focusing on the
    road and road boundaries. We can see the ROI by running the following code snippet:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We should see the output shown in [Figure 16-7](part0019.html#the_roi_for_our_car_to_focus_on_during_t).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![The ROI for our car to focus on during training](../images/00109.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: Figure 16-7\. The ROI for our car to focus on during training
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our model will focus only on the road now and not be confused by anything else
    going on in the environment. This also reduces the size of the image by half,
    which will make training our neural network easier.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, it is always better to have as much data as we can get
    our hands on while training deep learning models. We have seen in previous chapters
    the importance of data augmentation and how it helps to not only get more training
    data but to also avoid overfitting. Most data augmentation techniques discussed
    previously for image classification, however, will not be useful for our current
    autonomous driving problem. Let’s take the example of rotation. Rotating images
    randomly by 20 degrees is very helpful when training a classifier for a smartphone
    camera, but the camera on the hood of our car is fixed in place and will never
    see a rotated image (unless our car is doing flips, at which point we have larger
    concerns). The same goes for random shifts; we will never encounter those while
    driving. There is, however, something else we can do to augment the data for our
    current problem.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Looking closely at our image, we can observe that flipping it on the y-axis
    produces an image that could have easily come from a different test run ([Figure 16-8](part0019.html#flipping_image_on_the_y-axis)).
    We can effectively double the size of our dataset if we used the flipped versions
    of images along with their regular ones. Flipping an image this way will require
    us to modify the steering angle associated with it as well. Because our new image
    is a reflection of our original, we can just change the sign of the corresponding
    steering angle (e.g., from 0.212 to –0.212).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![Flipping image on the y-axis](../images/00048.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: Figure 16-8\. Flipping image on the y-axis
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is another thing we can do to augment our data. Autonomous cars need to
    be ready for not only changes on the roads, but also for changes in external conditions
    like weather, time of day, and available light. Most simulators available today
    allow us to create these conditions synthetically. All of our data was collected
    in bright lighting conditions. Instead of going back to the simulator to collect
    more data under a variety of lighting conditions, we could just introduce random
    lighting changes by adjusting image brightness while training on the data we have.
    [Figure 16-9](part0019.html#reducing_image_brightness_by_40percent) shows an example.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![Reducing image brightness by 40%](../images/00075.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Figure 16-9\. Reducing image brightness by 40%
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dataset Imbalance and Driving Strategies
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning models are only as good as the data on which they are trained.
    Unlike humans, the AI of today cannot think and reason for itself. Hence, when
    presented with a completely new situation it will fall back on what it has seen
    before and make predictions based on the dataset it was trained on. Additionally,
    the statistical nature of deep learning makes it ignore instances that happen
    infrequently as aberrations and outliers, even if they are of significance. This
    is called *dataset imbalance*, and it is a common nuisance for data scientists.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that we are trying to train a classifier that looks at dermatoscopic
    images to detect whether a lesion is benign or malignant. Suppose that our dataset
    has one million images, 10,000 of which contain lesions that are malignant. It
    is very likely that the classifier we train on this data always predicts images
    to be benign and never malignant. This happens because deep learning algorithms
    are designed to minimize error (or maximize accuracy) while training. By classifying
    all images as benign, our model attains 99% accuracy and calls it a day, while
    severely failing at the job it was trained to perform: detecting cancerous lesions.
    This problem is usually solved by training predictors on a more balanced dataset.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous driving is not immune to the problem of dataset imbalance. Let’s
    take our steering-angle prediction model. What do we know about steering angles
    from our day-to-day driving experience? While driving down a road we mostly drive
    straight. If our model was trained on only normal driving data, it would never
    learn how to navigate turns due to their relative scarcity in the training dataset.
    To solve this, it is important that our dataset contains data not only for a normal
    driving strategy but also for a “swerved” driving strategy in a statistically
    significant amount. To illustrate what we mean by this, let’s go back to our *.tsv*/*.txt*
    files. Earlier in the chapter, we pointed out the naming of our data folders.
    It should now be clear that our dataset has data from six collection runs using
    a normal driving strategy and from three collection runs using a swerve driving
    strategy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us aggregate the data from all *.tsv*/*.txt* files into a single DataFrame
    to make the analysis easier:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s plot some steering angles from both driving strategies on a scatter plot:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 16-10](part0019.html#plot_showing_steering_angles_from_the_tw) shows
    the results.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![Plot showing steering angles from the two driving strategies](../images/00299.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: Figure 16-10\. Plot showing steering angles from the two driving strategies
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s also plot the number of data points collected with each strategy:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Figure 16-11](part0019.html#dataset_split_for_the_two_driving_strate) shows
    those results.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset split for the two driving strategies](../images/00114.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Figure 16-11\. Dataset split for the two driving strategies
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking at [Figure 16-10](part0019.html#plot_showing_steering_angles_from_the_tw),
    as we had expected, we see that the normal driving strategy produces steering
    angles that we would observe during everyday driving, mostly straight on the road
    with the occasional turn. By contrast, the swerve driving strategy mostly focuses
    on sharp turns and hence has higher values for steering angles. As shown in [Figure 16-11](part0019.html#dataset_split_for_the_two_driving_strate),
    a combination of these two strategies gives us a nice, albeit unrealistic in real
    life, distribution to train on with a 75/25 split. This also further solidifies
    the importance of simulation for autonomous driving because it is unlikely we’d
    be collecting data using the swerve strategy in real life with an actual car.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Before closing our discussion on data preprocessing and dataset imbalance,
    let’s take a final look at the distribution of our steering angles for the two
    driving strategies by plotting them on a histogram ([Figure 16-12](part0019.html#steering_angle_distribution_for_the_two)):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Steering angle distribution for the two driving strategies](../images/00055.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 16-12\. Steering angle distribution for the two driving strategies
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As noted earlier, the swerve driving strategy gives us a much broader range
    of angles compared to the normal driving strategy, as shown in [Figure 16-12](part0019.html#steering_angle_distribution_for_the_two).
    These angles will help our neural network react appropriately in case it ever
    finds the car going off the road. The problem of imbalance in our dataset is somewhat
    solved, but not entirely. We still have a lot of zeros, across both driving strategies.
    To balance our dataset further, we could just ignore a portion of these during
    training. Although this gives us a very balanced dataset, it greatly reduces the
    overall number of data points available to us. We need to keep this in mind when
    we build and train our neural network.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, let’s make sure our data is suitable for training. Let’s
    take the raw data from all folders, split it into train, test, and validation
    datasets, and compress them into HDF5 files. The HDF5 format allows us to access
    data in chunks without reading the entire dataset into memory at once. This makes
    it ideal for deep learning problems. It also works seamlessly with Keras. The
    following code will take some time to run. When it has finished running, we will
    have three dataset files: *train.h5*, *eval.h5*, and *test.h5*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Each dataset file has four parts:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Image
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: A NumPy array containing the image data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Previous_state
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: A NumPy array containing the last known state of the car. This is a (steering,
    throttle, brake, speed) tuple.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Label
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: A NumPy array containing the steering angles that we wish to predict (normalized
    on the range -1..1).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Metadata
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: A NumPy array containing metadata about the files (which folder they came from,
    etc.).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to take the observations and learnings from our dataset and
    start training our model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Training Our Autonomous Driving Model
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the steps in this section are also detailed in the Jupyter Notebook [*TrainModel.ipynb*](https://oreil.ly/ESHVS).
    As before, we begin by importing some libraries and defining paths:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s also set up our datasets:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Drive Data Generator
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We were introduced to the concept of Keras data generators in previous chapters.
    A data generator iterates through the dataset and reads data in chunks from the
    disk. This allows us to keep both our CPU and GPU busy, increasing throughput.
    To implement ideas discussed in the previous section we created our own Keras
    generator called `DriveDataGenerator`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap some of our observations from the previous section:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Our model should focus only on the ROI within each image.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can augment our dataset by flipping images horizontally and reversing the
    sign of the steering angle.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can further augment the data by introducing random brightness changes in
    the images. This will simulate different lighting conditions and make our model
    more robust.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can randomly drop a percentage of data points where the steering angle is
    zero so that the model sees a balanced dataset when training.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our overall number of data points available will be reduced significantly post-balancing.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see how the `DriveDataGenerator` takes care of the first four of these
    items. We will return to last item when we begin designing our neural network.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: The ROI is a simple image crop. [76,135,0,255] (shown in the code block that
    follows) is the [x1,x2,y1,y2] value of the rectangle representing the ROI. The
    generator extracts this rectangle from each image. We can use the parameter `roi`
    to modify the ROI.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal flip is fairly straightforward. When generating batches, random
    images are flipped along the y-axis and their steering angle values are reversed
    if the parameter `horizontal_flip` is set to `True`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: For random brightness changes, we introduce the parameter `brighten_range`.
    Setting this parameter to `0.4` modifies the brightness of images in any given
    batch randomly up to 40%. We do not recommend increasing this beyond `0.4`. To
    compute brightness, we transform the image from RGB to HSV space, scale the “V”
    coordinate up or down, and transform back to RGB.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: For dataset balancing through dropping zeros, we introduce the parameter `zero_drop_percentage`.
    Setting this to `0.9` will randomly drop 90% of the 0-label data points in any
    given batch.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s initialize our generator with these parameters:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s visualize some sample data points by drawing labels (steering angle)
    over their corresponding images:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We should see an output similar to [Figure 16-13](part0019.html#drawing_steering_angles_on_images).
    Observe that we are now looking at only the ROI when training the model, thereby
    ignoring all the nonrelevant information present in the original images. The line
    indicates the ground truth steering angle. This is the angle the car was driving
    at when that image was taken by the camera during data collection.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![Drawing steering angles on images](../images/00182.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Figure 16-13\. Drawing steering angles on images
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Model Definition
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are now ready to define the architecture of our neural network. It is here
    that we must take into account the problem of our dataset being very limited in
    size after removing zeros. Because of this limitation, we cannot build a network
    that is too deep. Because we are dealing with images, we will need a few convolutional/max-pooling
    pairs to extract features.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Images alone, however, might not be enough to lead our model to convergence.
    Using only images to train also does not align with how driving decisions are
    made in the real world. When driving down the road, we are not only perceiving
    our surrounding environment, we are also aware of how fast we are going, how much
    we are turning and, the state of our gas and brake pedals. Input from sensors
    like cameras, lidars, radars, and so on being fed into a neural network corresponds
    to only one portion of all information a driver has at hand while making a driving
    decision in the real world. An image presented to our neural network could have
    been taken from a stationary car or a car driving at 60 mph; the network would
    have no way of knowing which. Turning the steering wheel by two degrees to the
    right while driving at 5 mph will yield very different results compared to doing
    the same at 50 mph. In short, a model trying to predict steering angles should
    not rely on sensory input alone. It also needs information about the current state
    of the car. Thankfully for us, we do have this information available.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the previous section, we noted that our datasets have four parts.
    For each image, in addition to the steering angle label and metadata, we also
    recorded the last known state of the car corresponding to the image. This was
    stored in the form of a (steering, throttle, brake, speed) tuple, and we will
    use this information, along with our images, as input to the neural network. Note
    that this does not violate our “Hello, World!” requirements, because we are still
    using the single camera as our only external sensor.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Putting all we discussed together, you can see the neural network that we will
    be using for this problem in [Figure 16-14](part0019.html#network_architecture).
    We are using three convolutional layers with 16, 32, 32 filters, respectively,
    and a (3,3) convolutional window. We are merging the image features (output from
    convolutional layers) with an input layer supplying the previous state of the
    car. The combined feature set is then passed through two fully connected layers
    with 64 and 10 hidden neurons, respectively. The activation function used in our
    network is ReLU. Notice that unlike the classification problems we have been working
    on in previous chapters, the last layer of our network is a single neuron with
    no activation. This is because the problem we are trying to solve is a regression
    problem. The output of our network is the steering angle, a floating-point number,
    unlike the discrete classes that we were predicting earlier.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![Network architecture](../images/00141.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: Figure 16-14\. Network architecture
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s now implement our network. We can use `model.summary()`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Callbacks
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the nice features of Keras is the ability to declare *callbacks*. Callbacks
    are functions that are executed after each epoch of training and help us to gain
    an insight into the training process as well as control hyperparameters to an
    extent. They also let us define conditions to perform certain actions while training
    is underway; for example, stopping the training early if the loss stops decreasing.
    We will use a few callbacks for our experiment:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '`ReduceLROnPlateau`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: If the model is near a minimum and the learning rate is too high, the model
    will circle around that minimum without ever reaching it. This callback will allow
    the model to reduce its learning rate when the validation loss hits a plateau
    and stops improving, allowing us to reach the optimal point.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '`CSVLogger`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: This lets us log the output of the model after each epoch into a CSV file, which
    will allow us to track the progress without needing to use the console.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '`ModelCheckpoint`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Generally, we will want to use the model that has the lowest loss on the validation
    set. This callback will save the model each time the validation loss improves.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '`EarlyStopping`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: We will want to stop training when the validation loss stops improving. Otherwise,
    we risk overfitting. This option will detect when the validation loss stops improving
    and will stop the training process when that occurs.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now go ahead and implement these callbacks:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We’re now all set to kick off training. The model will take a while to train,
    so this will make for a nice Netflix break. The training process should terminate
    with a validation loss of approximately .0003:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Our model is now trained and ready to go. Before we see it in action, let’s
    do a quick sanity check and plot some predictions against images:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We should see an output similar to [Figure 16-15](part0019.html#drawing_actual_and_predicted_steering_an).
    In this figure, the thick line is the predicted output, and the thin line is the
    label output. Looks like our predictions are fairly accurate (we can also see
    the actual and predicted values above the images). Time to deploy our model and
    see it in action.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Drawing actual and predicted steering angles on images](../images/00095.jpeg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: Figure 16-15\. Drawing actual and predicted steering angles on images
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deploying Our Autonomous Driving Model
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the steps in this section are also detailed in the Jupyter Notebook [*TestModel.ipynb*](https://oreil.ly/5saDl).
    Now that we have our model trained, it is time to spin up our simulator and use
    our model to drive our car around.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, begin by importing some libraries and defining paths:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, load the model and connect to AirSim in the Landscape environment. To
    start the simulator, on a Windows machine, open a PowerShell command window at
    the location where we unzipped the simulator package and run the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Back in the Jupyter Notebook, run the following to connect the model to the
    AirSim client. Ensure that the simulator is running *before* kicking this off:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With the connection established, let’s now set the initial state of the car
    as well as some buffers used to store the output from the model:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The next step is to set up the model to expect an RGB image from the simulator
    as the input. We need to define a helper function for this:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, let’s set up an infinite loop for our model to read an image from
    the simulator along with the current state of the car, predict the steering angle,
    and send it back to the simulator. Because our model predicts only steering angles,
    we will need to supply a control signal to maintain speed ourselves. Let’s set
    it up so that the car will attempt to run at a constant 5 m/s:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We should see output similar to the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We did it! The car is driving around nicely on the road, keeping to the right
    side, for the most part, carefully navigating all the sharp turns and instances
    where it could potentially go off the road. Kudos on training our first-ever autonomous
    driving model!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![Trained model driving the car](../images/00232.jpeg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: Figure 16-16\. Trained model driving the car
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before we wrap up, there are a couple of things worth noting. First, notice
    that the motion of the car is not perfectly smooth. This is because we are working
    with a regression problem and making a steering angle prediction for every image
    frame seen by the car. One way to fix this would be to average out predictions
    over a buffer of consecutive images. Another idea could be to turn this into a
    classification problem. More specifically, we could define buckets for the steering
    angles (..., –0.1, –0.05, 0, 0.05, 0.1, ...), bucketize the labels, and predict
    the correct bucket for each image.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: If we let the model run for a while (a little more than five minutes), we observe
    that the car eventually veers off the road randomly and crashes. This happens
    on a portion of the track with a steep incline. Remember our last requirement
    during the problem setup? Elevation changes require manipulating throttle and
    brakes. Because our model can control only the steering angle, it doesn’t do too
    well on steep roads.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Further Exploration
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having been trained on the “Hello, World!” scenario, our model is obviously
    not a perfect driver but don’t be disheartened. Keep in mind that we have barely
    scratched the surface of the possibilities at the intersection of deep learning
    and self-driving cars. The fact that we were able to have our car learn to drive
    around almost perfectly using a very small dataset is something to be proud of!
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Here are some new ideas for us to build on top of what we learned in this chapter.
    You can implement all of these ideas using the setup we already have in place
    for this chapter.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Expanding Our Dataset
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a general rule, using more data helps improve model performance. Now that
    we have the simulator up and running, it will be a useful exercise to expand our
    dataset by doing more data collection runs. We can even try combining data from
    various different environments available in AirSim and see how our model trained
    on this data performs in different environments.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: We used only RGB data from a single camera in this chapter. AirSim allows us
    to do a lot more. For example, we can collect images in depth view, segmentation
    view, surface normal view, and more for each of the cameras available. So, we
    can potentially have 20 different images (for five cameras operating in all four
    modes) for each instance. Can using all this extra data help us improve the model
    we just trained?
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Training on Sequential Data
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our model currently uses a single image and a single vehicle state for each
    prediction. This is not really how we drive in real life, though. Our actions
    always take into account the recent series of events leading up to that given
    moment. In our dataset, we have timestamp information available for all our images,
    which we can use to create sequences. We can modify our model to make predictions
    using the previous *N* images and states. For example, given the past 10 images
    and past 10 states, predict the next steering angle. (Hint: This might require
    the use of RNNs.)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After we learn about reinforcement learning in the next chapter, we can come
    back and try the [Distributed Deep Reinforcement Learning for Autonomous Driving](https://oreil.ly/u1hoC)
    tutorial from the [*Autonomous Driving Cookbook*](https://oreil.ly/bTphH). Using
    the Neighborhood environment in AirSim, which is included in the package we downloaded
    for this chapter, we will learn how to scale a deep reinforcement learning training
    job and reduce training time from under a week to less than an hour using transfer
    learning and the cloud.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter gave us a sneak peek into how deep learning enables the autonomous
    driving industry. Taking advantage of the skills acquired in the previous chapters,
    we implemented the “Hello, World!” problem of autonomous driving using Keras.
    Exploring the raw data at hand, we learned how to preprocess it to make it suitable
    for training a high-performing model. And we were able to accomplish this with
    a very small dataset. We were also able to take our trained model and deploy it
    to drive a car in the simulated world. Wouldn’t you agree that there’s just something
    magical about that?
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
