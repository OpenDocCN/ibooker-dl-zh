- en: Chapter 16\. Simulating a Self-Driving Car Using End-to-End Deep Learning with
    Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Contributed by guest authors: Aditya Sharma and Mitchell Spryn'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Batmobile to Knightrider, from robotaxis to autonomous pizza delivery,
    self-driving cars have captured the imagination of both modern pop culture and
    the mainstream media. And why wouldn’t they? How often does it happen that a science-fiction
    concept is brought to life? More important, autonomous cars promise to address
    some key challenges urban societies are facing today: road accidents, pollution
    levels, cities becoming increasingly more congested, hours of productivity lost
    in traffic, and the list goes on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It should come as no surprise that a full self-driving system is quite complex
    to build, and cannot be tackled in one book chapter. Much like an onion, any complex
    problem contains layers that can be peeled. In our case, we intend to tackle one
    fundamental problem here: how to steer. Even better, we don’t need a real car
    to do this. We will be training a neural network to drive our car autonomously
    within a simulated environment, using realistic physics and visuals.'
  prefs: []
  type: TYPE_NORMAL
- en: But first, a brief bit of history.
  prefs: []
  type: TYPE_NORMAL
- en: '![The SAE Levels of Driving Automation (image source)](../images/00220.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-1\. The SAE Levels of Driving Automation ([image source](https://oreil.ly/RHKUt))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A Brief History of Autonomous Driving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though it might seem like autonomous driving is a very recent technological
    development, research in this field originated more than 30 years ago. Scientists
    at Carnegie Mellon University first tried their hands at this seemingly impossible
    feat back in 1984 when they started working on what would be known as the Navlab
    1\. “Navlab” was a shorthand for Carnegie Mellon’s Navigation Laboratory, a not-so-fancy
    name for what would turn out to be the first step into a very impressive future
    for humanity. The Navlab 1 was a Chevrolet panel van ([Figure 16-2](part0019.html#the_navlab_1_in_all_its_glory_left_paren))
    with racks of hardware onboard that included, among other things, workstations
    for the research team and a Sun Microsystems supercomputer. It was considered
    cutting edge at the time, a self-contained computing system on wheels. Long-distance
    wireless communication was pretty much nonexistent back then, and cloud computing
    was not even a concept. Putting the entire research team in the back of a highly
    experimental self-driving van might seem like a dangerous idea, but the Navlab
    had a maximum speed of 20 MPH and navigated on only empty roads. This ensured
    the safety of the scientists working onboard. What is very impressive to note
    though, is that the technology used in this car laid the foundation for technology
    used in the self-driving cars of today. For example, Navlab 1 used video cameras
    and a rangefinder (an early version of lidar) to observe its surroundings. For
    navigation, they used a single-layer neural network to predict steering angles
    given sensor data. Pretty neat isn’t it?
  prefs: []
  type: TYPE_NORMAL
- en: '![The NavLab 1 in all its glory (image source)](../images/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-2\. The NavLab 1 in all its glory ([image source](https://oreil.ly/b2Bnn))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep Learning, Autonomous Driving, and the Data Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even 35 years ago scientists knew that neural networks were going to play a
    key role in making self-driving cars a reality. Back then, of course, we did not
    have the technology needed (GPUs, cloud computing, FPGAs, etc.) to train and deploy
    deep neural networks at scale for autonomous driving to become a reality. Today’s
    autonomous cars use deep learning to perform all kinds of tasks. [Table 16-1](part0019.html#examples_of_deep_learning_applications_i)
    lists some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Table 16-1\. Examples of deep learning applications in self-driving cars
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task** | **Examples** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Perception** | Detect drivable area, lane markings, intersections, crosswalks,
    etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Detect other vehicles, pedestrians, animals, objects on the road, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Detect traffic signs and signals |'
  prefs: []
  type: TYPE_TB
- en: '| **Navigation** | Predict and output steering angle, throttle, etc., based
    on sensor input |'
  prefs: []
  type: TYPE_TB
- en: '| Perform overtaking, lane changes, U-turns, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Perform merges, exits, navigate roundabouts, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Drive in tunnels, over bridges, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Respond to traffic rule breakers, wrong-way drivers, unexpected environment
    changes, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Navigate stop-and-go traffic |'
  prefs: []
  type: TYPE_TB
- en: We know that deep learning requires data. In fact, a general rule of thumb is
    that more data ensures better model performance. In previous chapters, we saw
    that training image classifiers can require millions of images. Imagine how much
    data it would take to train a car to drive itself. Things are also very different
    with self-driving cars when it comes to the data needed for validation and testing.
    Not only must a car be able to drive, it must do it safely. Testing and validating
    model performance is therefore fundamentally critical and requires a lot more
    data than what is needed for training, unlike traditional deep learning problems.
    However, it is difficult to accurately predict the exact amount of data needed,
    and estimates vary. [A 2016 study](https://oreil.ly/mOUeJ) shows that it would
    take 11 billion miles of driving for a car to become as good as a human driver.
    For a fleet of 100 self-driving cars collecting data 24 hours per day, 365 days
    per year at an average speed of 25 miles per hour, this would take more than 500
    years!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, of course, it is highly impractical and costly to collect 11 billion miles
    of data by having fleets of cars drive around in the real world. This is why almost
    everyone working in this space—be it a large automaker or a startup—uses simulators
    to collect data and validate trained models. [Waymo](https://waymo.com/tech) (Google’s
    self-driving car team), for example, had driven nearly 10 million miles on the
    road as of late 2018\. Even though this is more than any other company on the
    planet it represents less than one percent of our 11-billion-mile figure. On the
    other hand, Waymo has driven seven billion miles in simulation. Although everyone
    uses simulation for validation, some companies build their own simulation tools,
    whereas others license them from companies like Cognata, Applied Intuition, and
    AVSimulation. There are some great open source simulation tools available as well:
    [AirSim](https://github.com/Microsoft/AirSim) from Microsoft, [Carla](http://carla.org/)
    from Intel, and [Apollo simulation](https://oreil.ly/rag7v) from Baidu. Thanks
    to these tools, we don’t need to connect the CAN bus of our car to learn the science
    behind making it drive itself.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we use a custom-built version of AirSim, built specifically
    for Microsoft’s [Autonomous Driving Cookbook](https://oreil.ly/uzOGl).
  prefs: []
  type: TYPE_NORMAL
- en: 'The “Hello, World!” of Autonomous Driving: Steering Through a Simulated Environment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we implement the “Hello, World!” problem of autonomous driving.
    Self-driving cars are complex, with dozens of sensors streaming gigabytes of data
    and multiple decisions being made while driving down a road. Much like programming,
    for the “Hello, World!” of self-driving cars, we strip the requirements down to
    basic fundamentals:'
  prefs: []
  type: TYPE_NORMAL
- en: The car always stays on the road.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For external sensor input, the car uses a single image frame from a single camera
    mounted on the front of the hood. No other sensors (lidar, radar, etc.) are used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on this single-image input, going at a constant speed, the car predicts
    its output steering angle. No other possible outputs (brake, throttle, gear changes,
    etc.) are predicted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no other vehicles, pedestrians, animals, or anything else on the road
    or relevant surroundings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The road being driven on is single lane, with no markings or traffic signs and
    signals and no rules for staying on the left or right side of the road.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The road mostly changes in terms of turns (which will require changing steering
    angle to navigate) and not in terms of elevation (which would require a change
    in throttle, applying brakes, etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setup and Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be implementing the “Hello, World!” problem in AirSim’s Landscape map
    ([Figure 16-4](part0019.html#the_landscape_map_in_airsim)). AirSim is an open
    source photo-realistic simulation platform and is a popular research tool for
    training data collection and validation of deep learning–based autonomous system
    model development.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Landscape map in AirSim](../images/00252.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-4\. The Landscape map in AirSim
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can find the code for this chapter in the [end-to-end deep learning tutorial](https://oreil.ly/_IJl9)
    from the [*Autonomous Driving Cookbook*](https://oreil.ly/uF5Bz) on GitHub in
    the form of Jupyter Notebooks. In the cookbook repository, go to [*AirSimE2EDeepLearning/*](https://oreil.ly/YVKPp).
    We can do so by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will use Keras, a library with which we are already familiar, to implement
    our neural network. It’s not necessary for us to learn any new deep learning concepts
    to work on this problem other than what has already been introduced in prior chapters
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, we have created a dataset by driving around in AirSim. The
    uncompressed size of the dataset is 3.25 GB. This is a little larger than datasets
    we’ve been using, but remember, we are implementing the “Hello, World!” of a highly
    complex problem. For comparison, it is fairly normal practice for automakers to
    collect multiple petabytes of data on the road every day. You can download the
    dataset by going to [*aka.ms/AirSimTutorialDataset*](https://aka.ms/AirSimTutorialDataset).
  prefs: []
  type: TYPE_NORMAL
- en: We can run the provided code and train our model on a Windows or Linux machine.
    We created a standalone build of AirSim for this chapter, which you can download
    from [*aka.ms/ADCookbookAirSimPackage*](http://aka.ms/ADCookbookAirSimPackage).
    After you’ve downloaded it, extract the simulator package to the location of your
    choice. Take note of the path because you will need it later. Please note that
    this build runs only on Windows but is needed only to see our final trained model
    in action. If you’re using Linux, you can take the model files and run them on
    a Windows machine with the provided simulator package.
  prefs: []
  type: TYPE_NORMAL
- en: AirSim is a highly photo-realistic environment, which means it is capable of
    generating lifelike pictures of the environment for the model to train on. You
    may have encountered similar graphics while playing high-definition video games.
    Given the size of the data and the graphic quality of the provided simulator package,
    a GPU is certainly preferable for running the code as well as the simulator.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there are some additional tools and Python dependencies needed to run
    the code provided. You can find the details for these at [Environment Setup in
    the README file](https://oreil.ly/RzZe7) in the code repository. At a high level,
    we need Anaconda with Python 3.5 (or higher), TensorFlow (to run as a backend
    to Keras), and h5py (for storing and reading data and model files). After we have
    our Anaconda environment set up, we can install the needed Python dependencies
    by running the *InstallPackages.py* file as root or administrator.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 16-2](part0019.html#setup_and_requirements_summary) provides a summary
    of all the requirements that we just defined.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 16-2\. Setup and requirements summary
  prefs: []
  type: TYPE_NORMAL
- en: '| **Item** | **Requirements/Link** | **Notes/Comments** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Code repository | [*https://oreil.ly/_IJl9*](https://oreil.ly/_IJl9) | Can
    be run on a Windows or Linux machine |'
  prefs: []
  type: TYPE_TB
- en: '| Jupyter Notebooks used | [*DataExplorationAndPreparation.ipynb*](https://oreil.ly/6yvCq)[*TrainModel.ipynb*](https://oreil.ly/rcR47)[*TestModel.ipynb*](https://oreil.ly/FE-EP)
    |   |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset download | [*aka.ms/AirSimTutorialDataset*](http://aka.ms/AirSimTutorialDataset)
    | 3.25 GB in size; needed to train model |'
  prefs: []
  type: TYPE_TB
- en: '| Simulator download | [*aka.ms/ADCookbookAirSimPackage*](http://aka.ms/ADCookbookAirSimPackage)
    | Not needed to train model, only to deploy and run it; runs only on Windows |'
  prefs: []
  type: TYPE_TB
- en: '| GPU | Recommended for training, required for running simulator |   |'
  prefs: []
  type: TYPE_TB
- en: '| Other tools + Python dependencies | [Environment Setup section in README
    file in the repository](https://oreil.ly/RzZe7) |   |'
  prefs: []
  type: TYPE_TB
- en: With all that taken care of, let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Data Exploration and Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we journey deeper into the world of neural networks and deep learning, you
    will notice that much of the magic of machine learning doesn’t come from how a
    neural network is built and trained, but from the data scientist’s understanding
    of the problem, the data, and the domain. Autonomous driving is no different.
    As you will see shortly, having a deep understanding of the data and the problem
    we are trying to solve is key when teaching our car how to drive itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the steps here are also detailed in the Jupyter Notebook [*DataExplorationAndPreparation.ipynb*](https://oreil.ly/6aE7z).
    We start by importing all the necessary modules for this part of the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we make sure our program can find the location of our unzipped dataset.
    We also provide it a location to store our processed (or “cooked”) data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the provided dataset we will see that it contains nine folders:
    *normal_1* through *normal_6* and *swerve_1* through *swerve_3*. We come back
    to the naming of these folders later. Within each of these folders are images
    as well as *.tsv* (or *.txt*) files. Let’s check out one of these images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We should see the following output. We can see that this image was captured
    by a camera placed centrally on the hood of the car (which is slightly visible
    at the bottom in [Figure 16-5](part0019.html#plot_showing_the_contents_of_the_file_no)).
    We can also see the Landscape environment used for this problem from the simulator.
    Notice that no other vehicles or objects are on the road, keeping in line with
    our requirements. Although the road appears to be snowy, for our experiment, the
    snow is only visual and not physical; that is, it will have no effect on the physics
    and movement of our car. Of course, in the real world, it is critical to accurately
    replicate the physics of snow, rain, etc., so that our simulators can produce
    highly accurate versions of reality. This is a very complicated task to undertake
    and many companies dedicate a lot of resources to it. Thankfully for our purpose,
    we can safely ignore all that and treat the snow as just white pixels in our image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Plot showing the contents of the file normal_1/images/img_0.png](../images/00211.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-5\. Plot showing the contents of the file normal_1/images/img_0.png
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s also get a preview of what the rest of the data looks like by displaying
    the contents of one of the *.tsv/.txt* files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'There is a lot going on here, so let’s break it down. First, we can see that
    each row in the table corresponds to an image frame (we are displaying only the
    first five frames here). Because this is at the beginning of our data collection
    drive, we can see that the car hasn’t started moving yet: the speed and throttle
    are 0 and the gear is neutral. Because we are trying to predict only steering
    angles for our problem, we won’t be predicting the speed, throttle, brake, or
    gear values. These values will be used, however, as part of the input data (more
    on this later).'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the Region of Interest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s get back to our sample image now. All the observations we made so far
    were from the eyes of a human being. For a moment, let’s take a look at the image
    again, only this time we step into the shoes (or tires) of a car that is just
    beginning to learn how to drive itself and is trying to understand what it is
    seeing.
  prefs: []
  type: TYPE_NORMAL
- en: If I’m the car, looking at this image that my camera presented me, I can divide
    it into three distinct parts ([Figure 16-6](part0019.html#the_three_parts_of_the_image_as_seen_by)).
    First, there is the lower third, which more or less looks consistent. It is made
    up of mostly straight lines (the road, lane divider, road fences, etc.) and has
    a uniform coloring of white, black, gray, and brown. There is also a weird black
    arc at the very bottom (this is the hood of the car). The upper third of the image,
    like the lower third, is also consistent. It is mostly gray and white (the sky
    and the clouds). Lastly, there is the middle third and it has a lot going on.
    There are big brown and grey shapes (mountains), which are not uniform at all.
    There are also four tall green figures (trees), their shape and color are different
    from anything else I see, so they must be super important. As I am presented more
    images, the upper third and the lower third don’t change all that much but the
    middle third undergoes a lot of change. Hence, I conclude that this is the part
    that I should be focusing on the most.
  prefs: []
  type: TYPE_NORMAL
- en: '![The three parts of the image as seen by the self-driving car](../images/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-6\. The three parts of the image as seen by the self-driving car
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Do you see the challenge in front of our car? It has no way of knowing what
    part of the image being presented to it is important. It might try to associate
    the changing steering angles with the changing scenery instead of focusing on
    turns in the road, which are very subtle compared to everything else that is going
    on. Not only are the changes in the scenery confusing, but they are also not relevant
    to the problem we are trying to solve here at all. The sky above, although mostly
    unchanging, also does not provide us with any information relevant to the steering
    of our car. Finally, the part of the hood being captured in every picture is also
    nonrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us fix this by focusing only on the relevant portion of the image. We do
    this by creating a region of interest (ROI) in our image. As we can imagine, there
    is no hard-and-fast rule to dictate what our ROI should look like. It really depends
    on our use case. For us, because the camera is in a fixed position and there are
    no elevation changes on the road, the ROI is a simple rectangle focusing on the
    road and road boundaries. We can see the ROI by running the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We should see the output shown in [Figure 16-7](part0019.html#the_roi_for_our_car_to_focus_on_during_t).
  prefs: []
  type: TYPE_NORMAL
- en: '![The ROI for our car to focus on during training](../images/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-7\. The ROI for our car to focus on during training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our model will focus only on the road now and not be confused by anything else
    going on in the environment. This also reduces the size of the image by half,
    which will make training our neural network easier.
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, it is always better to have as much data as we can get
    our hands on while training deep learning models. We have seen in previous chapters
    the importance of data augmentation and how it helps to not only get more training
    data but to also avoid overfitting. Most data augmentation techniques discussed
    previously for image classification, however, will not be useful for our current
    autonomous driving problem. Let’s take the example of rotation. Rotating images
    randomly by 20 degrees is very helpful when training a classifier for a smartphone
    camera, but the camera on the hood of our car is fixed in place and will never
    see a rotated image (unless our car is doing flips, at which point we have larger
    concerns). The same goes for random shifts; we will never encounter those while
    driving. There is, however, something else we can do to augment the data for our
    current problem.
  prefs: []
  type: TYPE_NORMAL
- en: Looking closely at our image, we can observe that flipping it on the y-axis
    produces an image that could have easily come from a different test run ([Figure 16-8](part0019.html#flipping_image_on_the_y-axis)).
    We can effectively double the size of our dataset if we used the flipped versions
    of images along with their regular ones. Flipping an image this way will require
    us to modify the steering angle associated with it as well. Because our new image
    is a reflection of our original, we can just change the sign of the corresponding
    steering angle (e.g., from 0.212 to –0.212).
  prefs: []
  type: TYPE_NORMAL
- en: '![Flipping image on the y-axis](../images/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-8\. Flipping image on the y-axis
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is another thing we can do to augment our data. Autonomous cars need to
    be ready for not only changes on the roads, but also for changes in external conditions
    like weather, time of day, and available light. Most simulators available today
    allow us to create these conditions synthetically. All of our data was collected
    in bright lighting conditions. Instead of going back to the simulator to collect
    more data under a variety of lighting conditions, we could just introduce random
    lighting changes by adjusting image brightness while training on the data we have.
    [Figure 16-9](part0019.html#reducing_image_brightness_by_40percent) shows an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reducing image brightness by 40%](../images/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-9\. Reducing image brightness by 40%
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dataset Imbalance and Driving Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning models are only as good as the data on which they are trained.
    Unlike humans, the AI of today cannot think and reason for itself. Hence, when
    presented with a completely new situation it will fall back on what it has seen
    before and make predictions based on the dataset it was trained on. Additionally,
    the statistical nature of deep learning makes it ignore instances that happen
    infrequently as aberrations and outliers, even if they are of significance. This
    is called *dataset imbalance*, and it is a common nuisance for data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that we are trying to train a classifier that looks at dermatoscopic
    images to detect whether a lesion is benign or malignant. Suppose that our dataset
    has one million images, 10,000 of which contain lesions that are malignant. It
    is very likely that the classifier we train on this data always predicts images
    to be benign and never malignant. This happens because deep learning algorithms
    are designed to minimize error (or maximize accuracy) while training. By classifying
    all images as benign, our model attains 99% accuracy and calls it a day, while
    severely failing at the job it was trained to perform: detecting cancerous lesions.
    This problem is usually solved by training predictors on a more balanced dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous driving is not immune to the problem of dataset imbalance. Let’s
    take our steering-angle prediction model. What do we know about steering angles
    from our day-to-day driving experience? While driving down a road we mostly drive
    straight. If our model was trained on only normal driving data, it would never
    learn how to navigate turns due to their relative scarcity in the training dataset.
    To solve this, it is important that our dataset contains data not only for a normal
    driving strategy but also for a “swerved” driving strategy in a statistically
    significant amount. To illustrate what we mean by this, let’s go back to our *.tsv*/*.txt*
    files. Earlier in the chapter, we pointed out the naming of our data folders.
    It should now be clear that our dataset has data from six collection runs using
    a normal driving strategy and from three collection runs using a swerve driving
    strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us aggregate the data from all *.tsv*/*.txt* files into a single DataFrame
    to make the analysis easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot some steering angles from both driving strategies on a scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 16-10](part0019.html#plot_showing_steering_angles_from_the_tw) shows
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Plot showing steering angles from the two driving strategies](../images/00299.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-10\. Plot showing steering angles from the two driving strategies
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s also plot the number of data points collected with each strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 16-11](part0019.html#dataset_split_for_the_two_driving_strate) shows
    those results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset split for the two driving strategies](../images/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-11\. Dataset split for the two driving strategies
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking at [Figure 16-10](part0019.html#plot_showing_steering_angles_from_the_tw),
    as we had expected, we see that the normal driving strategy produces steering
    angles that we would observe during everyday driving, mostly straight on the road
    with the occasional turn. By contrast, the swerve driving strategy mostly focuses
    on sharp turns and hence has higher values for steering angles. As shown in [Figure 16-11](part0019.html#dataset_split_for_the_two_driving_strate),
    a combination of these two strategies gives us a nice, albeit unrealistic in real
    life, distribution to train on with a 75/25 split. This also further solidifies
    the importance of simulation for autonomous driving because it is unlikely we’d
    be collecting data using the swerve strategy in real life with an actual car.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before closing our discussion on data preprocessing and dataset imbalance,
    let’s take a final look at the distribution of our steering angles for the two
    driving strategies by plotting them on a histogram ([Figure 16-12](part0019.html#steering_angle_distribution_for_the_two)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Steering angle distribution for the two driving strategies](../images/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-12\. Steering angle distribution for the two driving strategies
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As noted earlier, the swerve driving strategy gives us a much broader range
    of angles compared to the normal driving strategy, as shown in [Figure 16-12](part0019.html#steering_angle_distribution_for_the_two).
    These angles will help our neural network react appropriately in case it ever
    finds the car going off the road. The problem of imbalance in our dataset is somewhat
    solved, but not entirely. We still have a lot of zeros, across both driving strategies.
    To balance our dataset further, we could just ignore a portion of these during
    training. Although this gives us a very balanced dataset, it greatly reduces the
    overall number of data points available to us. We need to keep this in mind when
    we build and train our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, let’s make sure our data is suitable for training. Let’s
    take the raw data from all folders, split it into train, test, and validation
    datasets, and compress them into HDF5 files. The HDF5 format allows us to access
    data in chunks without reading the entire dataset into memory at once. This makes
    it ideal for deep learning problems. It also works seamlessly with Keras. The
    following code will take some time to run. When it has finished running, we will
    have three dataset files: *train.h5*, *eval.h5*, and *test.h5*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Each dataset file has four parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Image
  prefs: []
  type: TYPE_NORMAL
- en: A NumPy array containing the image data.
  prefs: []
  type: TYPE_NORMAL
- en: Previous_state
  prefs: []
  type: TYPE_NORMAL
- en: A NumPy array containing the last known state of the car. This is a (steering,
    throttle, brake, speed) tuple.
  prefs: []
  type: TYPE_NORMAL
- en: Label
  prefs: []
  type: TYPE_NORMAL
- en: A NumPy array containing the steering angles that we wish to predict (normalized
    on the range -1..1).
  prefs: []
  type: TYPE_NORMAL
- en: Metadata
  prefs: []
  type: TYPE_NORMAL
- en: A NumPy array containing metadata about the files (which folder they came from,
    etc.).
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to take the observations and learnings from our dataset and
    start training our model.
  prefs: []
  type: TYPE_NORMAL
- en: Training Our Autonomous Driving Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the steps in this section are also detailed in the Jupyter Notebook [*TrainModel.ipynb*](https://oreil.ly/ESHVS).
    As before, we begin by importing some libraries and defining paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also set up our datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Drive Data Generator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We were introduced to the concept of Keras data generators in previous chapters.
    A data generator iterates through the dataset and reads data in chunks from the
    disk. This allows us to keep both our CPU and GPU busy, increasing throughput.
    To implement ideas discussed in the previous section we created our own Keras
    generator called `DriveDataGenerator`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap some of our observations from the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: Our model should focus only on the ROI within each image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can augment our dataset by flipping images horizontally and reversing the
    sign of the steering angle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can further augment the data by introducing random brightness changes in
    the images. This will simulate different lighting conditions and make our model
    more robust.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can randomly drop a percentage of data points where the steering angle is
    zero so that the model sees a balanced dataset when training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our overall number of data points available will be reduced significantly post-balancing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see how the `DriveDataGenerator` takes care of the first four of these
    items. We will return to last item when we begin designing our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The ROI is a simple image crop. [76,135,0,255] (shown in the code block that
    follows) is the [x1,x2,y1,y2] value of the rectangle representing the ROI. The
    generator extracts this rectangle from each image. We can use the parameter `roi`
    to modify the ROI.
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal flip is fairly straightforward. When generating batches, random
    images are flipped along the y-axis and their steering angle values are reversed
    if the parameter `horizontal_flip` is set to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: For random brightness changes, we introduce the parameter `brighten_range`.
    Setting this parameter to `0.4` modifies the brightness of images in any given
    batch randomly up to 40%. We do not recommend increasing this beyond `0.4`. To
    compute brightness, we transform the image from RGB to HSV space, scale the “V”
    coordinate up or down, and transform back to RGB.
  prefs: []
  type: TYPE_NORMAL
- en: For dataset balancing through dropping zeros, we introduce the parameter `zero_drop_percentage`.
    Setting this to `0.9` will randomly drop 90% of the 0-label data points in any
    given batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s initialize our generator with these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s visualize some sample data points by drawing labels (steering angle)
    over their corresponding images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We should see an output similar to [Figure 16-13](part0019.html#drawing_steering_angles_on_images).
    Observe that we are now looking at only the ROI when training the model, thereby
    ignoring all the nonrelevant information present in the original images. The line
    indicates the ground truth steering angle. This is the angle the car was driving
    at when that image was taken by the camera during data collection.
  prefs: []
  type: TYPE_NORMAL
- en: '![Drawing steering angles on images](../images/00182.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-13\. Drawing steering angles on images
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Model Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are now ready to define the architecture of our neural network. It is here
    that we must take into account the problem of our dataset being very limited in
    size after removing zeros. Because of this limitation, we cannot build a network
    that is too deep. Because we are dealing with images, we will need a few convolutional/max-pooling
    pairs to extract features.
  prefs: []
  type: TYPE_NORMAL
- en: Images alone, however, might not be enough to lead our model to convergence.
    Using only images to train also does not align with how driving decisions are
    made in the real world. When driving down the road, we are not only perceiving
    our surrounding environment, we are also aware of how fast we are going, how much
    we are turning and, the state of our gas and brake pedals. Input from sensors
    like cameras, lidars, radars, and so on being fed into a neural network corresponds
    to only one portion of all information a driver has at hand while making a driving
    decision in the real world. An image presented to our neural network could have
    been taken from a stationary car or a car driving at 60 mph; the network would
    have no way of knowing which. Turning the steering wheel by two degrees to the
    right while driving at 5 mph will yield very different results compared to doing
    the same at 50 mph. In short, a model trying to predict steering angles should
    not rely on sensory input alone. It also needs information about the current state
    of the car. Thankfully for us, we do have this information available.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the previous section, we noted that our datasets have four parts.
    For each image, in addition to the steering angle label and metadata, we also
    recorded the last known state of the car corresponding to the image. This was
    stored in the form of a (steering, throttle, brake, speed) tuple, and we will
    use this information, along with our images, as input to the neural network. Note
    that this does not violate our “Hello, World!” requirements, because we are still
    using the single camera as our only external sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Putting all we discussed together, you can see the neural network that we will
    be using for this problem in [Figure 16-14](part0019.html#network_architecture).
    We are using three convolutional layers with 16, 32, 32 filters, respectively,
    and a (3,3) convolutional window. We are merging the image features (output from
    convolutional layers) with an input layer supplying the previous state of the
    car. The combined feature set is then passed through two fully connected layers
    with 64 and 10 hidden neurons, respectively. The activation function used in our
    network is ReLU. Notice that unlike the classification problems we have been working
    on in previous chapters, the last layer of our network is a single neuron with
    no activation. This is because the problem we are trying to solve is a regression
    problem. The output of our network is the steering angle, a floating-point number,
    unlike the discrete classes that we were predicting earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '![Network architecture](../images/00141.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-14\. Network architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s now implement our network. We can use `model.summary()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Callbacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the nice features of Keras is the ability to declare *callbacks*. Callbacks
    are functions that are executed after each epoch of training and help us to gain
    an insight into the training process as well as control hyperparameters to an
    extent. They also let us define conditions to perform certain actions while training
    is underway; for example, stopping the training early if the loss stops decreasing.
    We will use a few callbacks for our experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ReduceLROnPlateau`'
  prefs: []
  type: TYPE_NORMAL
- en: If the model is near a minimum and the learning rate is too high, the model
    will circle around that minimum without ever reaching it. This callback will allow
    the model to reduce its learning rate when the validation loss hits a plateau
    and stops improving, allowing us to reach the optimal point.
  prefs: []
  type: TYPE_NORMAL
- en: '`CSVLogger`'
  prefs: []
  type: TYPE_NORMAL
- en: This lets us log the output of the model after each epoch into a CSV file, which
    will allow us to track the progress without needing to use the console.
  prefs: []
  type: TYPE_NORMAL
- en: '`ModelCheckpoint`'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, we will want to use the model that has the lowest loss on the validation
    set. This callback will save the model each time the validation loss improves.
  prefs: []
  type: TYPE_NORMAL
- en: '`EarlyStopping`'
  prefs: []
  type: TYPE_NORMAL
- en: We will want to stop training when the validation loss stops improving. Otherwise,
    we risk overfitting. This option will detect when the validation loss stops improving
    and will stop the training process when that occurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now go ahead and implement these callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re now all set to kick off training. The model will take a while to train,
    so this will make for a nice Netflix break. The training process should terminate
    with a validation loss of approximately .0003:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model is now trained and ready to go. Before we see it in action, let’s
    do a quick sanity check and plot some predictions against images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We should see an output similar to [Figure 16-15](part0019.html#drawing_actual_and_predicted_steering_an).
    In this figure, the thick line is the predicted output, and the thin line is the
    label output. Looks like our predictions are fairly accurate (we can also see
    the actual and predicted values above the images). Time to deploy our model and
    see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: '![Drawing actual and predicted steering angles on images](../images/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-15\. Drawing actual and predicted steering angles on images
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deploying Our Autonomous Driving Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the steps in this section are also detailed in the Jupyter Notebook [*TestModel.ipynb*](https://oreil.ly/5saDl).
    Now that we have our model trained, it is time to spin up our simulator and use
    our model to drive our car around.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, begin by importing some libraries and defining paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, load the model and connect to AirSim in the Landscape environment. To
    start the simulator, on a Windows machine, open a PowerShell command window at
    the location where we unzipped the simulator package and run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Back in the Jupyter Notebook, run the following to connect the model to the
    AirSim client. Ensure that the simulator is running *before* kicking this off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'With the connection established, let’s now set the initial state of the car
    as well as some buffers used to store the output from the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to set up the model to expect an RGB image from the simulator
    as the input. We need to define a helper function for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s set up an infinite loop for our model to read an image from
    the simulator along with the current state of the car, predict the steering angle,
    and send it back to the simulator. Because our model predicts only steering angles,
    we will need to supply a control signal to maintain speed ourselves. Let’s set
    it up so that the car will attempt to run at a constant 5 m/s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We did it! The car is driving around nicely on the road, keeping to the right
    side, for the most part, carefully navigating all the sharp turns and instances
    where it could potentially go off the road. Kudos on training our first-ever autonomous
    driving model!
  prefs: []
  type: TYPE_NORMAL
- en: '![Trained model driving the car](../images/00232.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-16\. Trained model driving the car
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before we wrap up, there are a couple of things worth noting. First, notice
    that the motion of the car is not perfectly smooth. This is because we are working
    with a regression problem and making a steering angle prediction for every image
    frame seen by the car. One way to fix this would be to average out predictions
    over a buffer of consecutive images. Another idea could be to turn this into a
    classification problem. More specifically, we could define buckets for the steering
    angles (..., –0.1, –0.05, 0, 0.05, 0.1, ...), bucketize the labels, and predict
    the correct bucket for each image.
  prefs: []
  type: TYPE_NORMAL
- en: If we let the model run for a while (a little more than five minutes), we observe
    that the car eventually veers off the road randomly and crashes. This happens
    on a portion of the track with a steep incline. Remember our last requirement
    during the problem setup? Elevation changes require manipulating throttle and
    brakes. Because our model can control only the steering angle, it doesn’t do too
    well on steep roads.
  prefs: []
  type: TYPE_NORMAL
- en: Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having been trained on the “Hello, World!” scenario, our model is obviously
    not a perfect driver but don’t be disheartened. Keep in mind that we have barely
    scratched the surface of the possibilities at the intersection of deep learning
    and self-driving cars. The fact that we were able to have our car learn to drive
    around almost perfectly using a very small dataset is something to be proud of!
  prefs: []
  type: TYPE_NORMAL
- en: Here are some new ideas for us to build on top of what we learned in this chapter.
    You can implement all of these ideas using the setup we already have in place
    for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding Our Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a general rule, using more data helps improve model performance. Now that
    we have the simulator up and running, it will be a useful exercise to expand our
    dataset by doing more data collection runs. We can even try combining data from
    various different environments available in AirSim and see how our model trained
    on this data performs in different environments.
  prefs: []
  type: TYPE_NORMAL
- en: We used only RGB data from a single camera in this chapter. AirSim allows us
    to do a lot more. For example, we can collect images in depth view, segmentation
    view, surface normal view, and more for each of the cameras available. So, we
    can potentially have 20 different images (for five cameras operating in all four
    modes) for each instance. Can using all this extra data help us improve the model
    we just trained?
  prefs: []
  type: TYPE_NORMAL
- en: Training on Sequential Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our model currently uses a single image and a single vehicle state for each
    prediction. This is not really how we drive in real life, though. Our actions
    always take into account the recent series of events leading up to that given
    moment. In our dataset, we have timestamp information available for all our images,
    which we can use to create sequences. We can modify our model to make predictions
    using the previous *N* images and states. For example, given the past 10 images
    and past 10 states, predict the next steering angle. (Hint: This might require
    the use of RNNs.)'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After we learn about reinforcement learning in the next chapter, we can come
    back and try the [Distributed Deep Reinforcement Learning for Autonomous Driving](https://oreil.ly/u1hoC)
    tutorial from the [*Autonomous Driving Cookbook*](https://oreil.ly/bTphH). Using
    the Neighborhood environment in AirSim, which is included in the package we downloaded
    for this chapter, we will learn how to scale a deep reinforcement learning training
    job and reduce training time from under a week to less than an hour using transfer
    learning and the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter gave us a sneak peek into how deep learning enables the autonomous
    driving industry. Taking advantage of the skills acquired in the previous chapters,
    we implemented the “Hello, World!” problem of autonomous driving using Keras.
    Exploring the raw data at hand, we learned how to preprocess it to make it suitable
    for training a high-performing model. And we were able to accomplish this with
    a very small dataset. We were also able to take our trained model and deploy it
    to drive a car in the simulated world. Wouldn’t you agree that there’s just something
    magical about that?
  prefs: []
  type: TYPE_NORMAL
