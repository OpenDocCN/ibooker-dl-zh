<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="conclusion" epub:type="conclusion" data-pdf-bookmark="Conclusion"><div class="appendix" id="conclusion_conclusion_1758256568164384">
<h1>Conclusion</h1>

<p>As stated throughout this book, you cannot predict the future. We’re just at the starting line of the AI era. But if there were a prediction to make, the smart money isn’t on tooling integration—it’s on memory.</p>

<p>Tools remain relatively static: once you’ve integrated with them, you generally retain access. Memory, on the other hand, is dynamic. A project that was a high priority one week may become history the next. The best agents will integrate this changing context and proactively expand critical memory and expunge extraneous information.</p>

<p>This dynamic nature of memory is why the current moment presents such a unique opportunity. Unlike traditional software where expertise accumulates over years, there are no true experts in agents yet. The world of AI is simply too fast-paced and fluid for one person to be an expert in everything there is to know. This levels the playing field—the key to expertise for agents is exposure to these systems, to the use of agents, to what they can be used for successfully, and to how they can augment, not replace, your work.</p>

<p>For organizations looking to capitalize on this opportunity, experimentation is key. For some, that will look like giving users licenses and letting them experiment. For other, likely larger organizations, it may be wiser to slowly integrate agents together in seminars and hackathons. It’s an equal playing field, and the only way to truly learn is by doing with agents. Through this collective exploration, teams naturally develop their own TMSs, learning not just what agents can do but also who knows what about using them effectively.</p>

<p>As these experiments unfold, organizations will quickly discover that memory management is the critical challenge. True, memory will always be finite—because memory is data, and data always needs to be stored somewhere. We also know that, unless the attention mechanism that underlies LLMs fundamentally changes, the search space for LLMs will remain quadratic. That means that retrieval will be limited into the known future as well.</p>

<p>But it’s not about having infinite memory—it’s about <em>how</em> we retain and access memory. The algorithms we use, how we route between them based on context—these are the real differentiators. We can be confident that caching will play a growing role in agentic memory management. After all, think about search engines: when a question is asked—such as “What time are the playoffs today?”—it’s very likely not the only instance of that question being asked. Smart search engines cache these queries to make them more readily available—so too will agents of the future.</p>

<p>The same principle applies to queries on knowledge bases within an organization. As we explored in <a data-type="xref" href="ch02.html#ch02_long_term_memory_building_persistent_learning_age_1758256567757093">Chapter 2</a>, semantic caching retains the relative context of an information retrieval system over time by processing the semantics of the content being passed. Information that is retrieved frequently gets prioritized. For systems like internal LLMs or RAG where many users talk to the same corpus of information, it’s both more computationally effective and cost effective to semantically cache that information.</p>

<p>This is where the real innovation is happening. Researchers aren’t just fighting the quadratic-attention problem with bigger context windows; they’re getting smarter about importance scoring—<span class="keep-together">calculating</span> memory importance based on recency, frequency of reference, user-engagement metrics, and keyword relevance. They’re building cascading memory systems that allow the agent itself to choose what to promote to long-term storage. This adaptive approach—knowing what to keep, what to compress, and what to let go—is the key to scalable, dynamic memory systems moving forward.</p>

<p>The organizations that get memory right, that prioritize shared memory systems, that understand these trade-offs of memory retention and retrieval will be the organizations that have an edge in productivity. After all, if memory equals data, then those with the best systems for managing that data will understand not just what is, but what can be.</p>

<p>Yet in all this discussion of systems and algorithms, we must remember a crucial truth: the most important agent will always be the human agent. We are the conductors guiding the machine. Like an orchestra playing Beethoven’s Fifth, the music may be known, the notes memorized, but it’s the conductor who knows exactly what they want from this rendition—the conductor who has done the research, who has an overarching vision. In the same way, even if agents begin to write code for us, do our research, communicate to others for us—it will be us, the humans, who will decide on what we want, who will guide the agent not only on what success is but also what <em>we</em> want success to be.</p>

<p>While we cannot know what the future will bring, we can anticipate that the users of agents will play a bigger role in personalizing how they “conduct”—instructing them in just the right ways to retain important information. The future isn’t about perfect recall or infinite memory. It’s about designing systems that can manage information intelligently, guided by human judgment and values. With guidance from classic software engineering principles and research into how to better leverage our new stochastic systems, we’re only just getting started.</p>
</div></section></div></div>
<div id="book-content"><div id="sbo-rt-content"><section data-type="colophon" epub:type="colophon" class="abouttheauthor" data-pdf-bookmark="About the Authors"><div class="colophon" id="id17">
  <h1>About the Authors</h1>
  <p><strong>Benjamin Labaschin</strong> is currently a principal machine learning engineer and founding hire at Workhelix. Prior to this role, he worked as a senior data scientist at Hopper, XPO Logistics, and Blackstone. He holds a background in economics and environmental science. His areas of expertise include machine learning, MLOps, DevOps, and backend engineering. Ben has a particular passion for scaling engineering systems to support the evolving needs of hyper-growth startups and for advancing sustainable engineering practices.</p>
  <p><strong>Jim Allen Wallace</strong> is group manager of product marketing at Redis, where he bridges technical know-how and market insights to help developers apply Redis in the age of AI. He’s worn hats as an engineer, salesperson, and entrepreneur, giving him a front-row seat to how products get built, sold, and loved (or ignored). When he’s not shaping go-to-market strategy, Jim can be found hiking the mountains outside Denver, Colorado, or maybe pushing his cat Chutney off his keyboard.</p>
  <p><strong>Andrew Brookins</strong> is a principal applied AI engineer at Redis, where he builds open source infrastructure for memory, retrieval, and intelligent agents. He created the Agent Memory Server and Redis OM Python, and contributes to Redis libraries and applied AI tooling. With a background in backend systems and engineering leadership, he focuses on tools that help developers build smarter, more reliable systems.</p>
  <p><strong>Manvinder Singh</strong> is VP of AI products at Redis, where he is responsible for the portfolio of AI offerings, including vector search, semantic caching, and agent memory. Previously, he spent over 10 years in various AI and cloud roles at Google. As a director of product management for AI ecosystem at Google, he was responsible for orchestrating and driving product efforts to engage with and build with the ecosystem across their AI portfolio, including the Gemma family of models.</p>
</div></section></div></div></body></html>