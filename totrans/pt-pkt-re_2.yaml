- en: Chapter 2\. Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive deep into the world of PyTorch development, it’s important to
    familiarize yourself with the fundamental data structure in PyTorch: the `torch.Tensor`.
    By understanding the tensor, you will understand how PyTorch handles and stores
    data, and since deep learning is fundamentally the collection and manipulation
    of floating-point numbers, understanding tensors will help you understand how
    PyTorch implements more advanced functions for deep learning. In addition, you
    may find yourself using tensor operations frequently when preprocessing input
    data or manipulating output data during model development.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter serves as a quick reference to understanding tensors and implementing
    tensor functions within your code. I’ll begin by describing what a tensor is and
    show you some simple examples of how to use functions to create, manipulate, and
    accelerate tensor operations on a GPU. Next, we’ll take a broader look at the
    API for creating tensors and performing math operations so that you can quickly
    reference a comprehensive list of tensor capabilities. In each section, we will
    explore some of the more important functions, identify common pitfalls, and examine
    key points in their usage.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Tensor?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In PyTorch, a tensor is a data structure used to store and manipulate data.
    Like a NumPy array, a tensor is a multidimensional array containing elements of
    a single data type. Tensors can be used to represent scalars, vectors, matrices,
    and *n*-dimensional arrays and are derived from the `torch.Tensor` class. However,
    tensors are more than just arrays of numbers. Creating or instantiating a tensor
    object from the `torch.Tensor` class gives us access to a set of built-in class
    attributes and operations or class methods that provide a robust set of built-in
    capabilities. This chapter describes these attributes and operations in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors also include added benefits that make them more suitable than NumPy
    arrays for deep learning calculations. First, tensor operations can be performed
    significantly faster using GPU acceleration. Second, tensors can be stored and
    manipulated at scale using distributed processing on multiple CPUs and GPUs and
    across multiple servers. And third, tensors keep track of their graph computations,
    which as we will see in [“Automatic Differentiation (Autograd)”](#section_autograd)
    is very important in implementing a deep learning library.
  prefs: []
  type: TYPE_NORMAL
- en: To further explain what a tensor actually is and how to use one, I’ll begin
    by walking through a simple example that creates some tensors and performs a tensor
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: Simple CPU Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s a simple example that creates a tensor, performs a tensor operation,
    and uses a built-in method on the tensor itself. By default, the tensor data type
    will be derived from the input data type and the tensor will be allocated to the
    CPU device. First, we import the PyTorch library, then we create two tensors,
    `x` and `y`, from two-dimensional lists. Next, we add the two tensors and store
    the result in `z`. We can just use the `+` operator here because the `torch.Tensor`
    class supports operator overloading. Finally, we print the new tensor, `z`, which
    we can see is the matrix sum of `x` and `y`, and we print the size of `z`. Notice
    that `z` is a tensor object itself and the `size()` method is used to return its
    matrix dimensions, namely 2 × 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You   may   see   the  `torch.Tensor()`  (capital T)   constructor   used   in 
     legacy   code.   This   is   an   alias   for   the   default   tensor  type
    `torch.FloatTensor`. You should instead use `torch.tensor()` to create your tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Simple GPU Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the ability to accelerate tensor operations on a GPU is a major advantage
    of tensors over NumPy arrays, I’ll show you an easy example of this. This is the
    same example from the last section, but here we move the tensors to the GPU device
    if one is available. Notice that the output tensor is also allocated to the GPU.
    You can use the device attribute (e.g., `z.device`) to double-check where the
    tensor resides.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first line, the `torch.cuda.is_available()` function will return `True`
    if your machine has GPU support. This is a convenient way to write more robust
    code that can be accelerated when a GPU exists but also runs on a CPU when a GPU
    is not present. In the output, `device=''cuda:0''` indicates that the first GPU
    is being used. If your machine contains multiple GPUs, you can also control which
    GPU is being used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Moving Tensors Between CPUs and GPUs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous code uses `torch.tensor()` to create a tensor on a specific device;
    however, it’s more common to move an existing tensor to a device, namely a GPU
    if available. You can do so by using the `torch.to()` method. When new tensors
    are created as a result of tensor operations, PyTorch will create the new tensor
    on the same device. In the following code, `z` resides on the GPU because `x`
    and `y` reside on the GPU. The tensor `z` is moved back to the CPU using `torch.to("cpu")`
    for further processing. Also note that all the tensors within the operation must
    be on the same device. If `x` was on the GPU and `y` was on the CPU, we would
    get an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can use strings directly as device parameters instead of device objects.
    The following are all equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '`device="cuda"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device=torch.device("cuda")`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device="cuda:0"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device=torch.device("cuda:0")`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section showed a simple way to create tensors; however, there are
    many other ways to do it. You can create tensors from preexisting numeric data
    or create random samplings. Tensors can be created from preexisting data stored
    in array-like structures such as lists, tuples, scalars, or serialized data files,
    as well as in NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates some common ways to create tensors. First, it
    shows how to create a tensor from a list using `torch.tensor()`. This method can
    also be used to create tensors from other data structures like tuples, sets, or
    NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_tensors_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: From a list
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_tensors_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: From a tuple
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_tensors_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: From a NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_tensors_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Uninitialized; element values are not predictable
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_tensors_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: All elements initialized with 0.0
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_tensors_CO1-6)'
  prefs: []
  type: TYPE_NORMAL
- en: All elements initialized with 1.0
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the previous code sample, you can also create and initialize tensors
    by using functions like `torch.empty()`, `torch.ones()`, and `torch.zeros()` and
    specifying the desired size.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to initialize a tensor with random values, PyTorch supports a robust
    set of functions that you can use, such as `torch.rand()`, `torch.randn()`, and
    `torch.randint()`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_tensors_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Creates a 100 × 200 tensor with elements from a uniform distribution on the
    interval [0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_tensors_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Elements are random numbers from a normal distribution with a mean of 0 and
    a variance of 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_tensors_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Elements are random integers between 5 and 10.
  prefs: []
  type: TYPE_NORMAL
- en: Upon initialization, you can specify the data type and device (i.e., CPU or
    GPU) as shown in the previous code sample. In addition, the example shows how
    you can use PyTorch to create tensors that have the same properties as other tensors
    but are initialized with different data. Functions with the `_like` postfix such
    as `torch.empty_like()` and `torch.ones_like()` return tensors that have the same
    size, data type, and device as another tensor but are initialized differently
    (see [“Creating Tensors from Random Samples”](#creating-tensors-section)).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are some legacy functions, such as `from_numpy()` and `as_tensor()`, that
    have been replaced in practice by the `torch.tensor()` constructor, which can
    be used to handle all cases.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-1](#table_creation_ops) lists PyTorch functions used to create tensors.
    You should use each one with the `torch` namespace, e.g., `torch.empty()`. You
    can find more details by visiting [the PyTorch tensor documentation](https://pytorch.tips/torch).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Tensor creation functions
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**tensor**(*data, dtype=None, device=None, requires_grad=False, pin_memory=False*)`
    | Creates a tensor from an existing data structure |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**empty**(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | Creates a tensor from uninitialized elements based on
    the random state of values in memory |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**zeros**(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | Creates a tensor with all elements initialized to 0.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**ones**(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | Creates a tensor with all elements initialized to 1.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**arange**(*start=0, end, step=1, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | Creates a 1D tensor of values over a range
    with a common step value |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**linspace**(*start, end, steps=100, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | Creates a 1D tensor of linearly spaced points
    between the `start` and `end` |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**logspace**(*start, end, steps=100, base=10.0, out=None, dtype=None,
    layout=torch.strided, device=None, requires_grad=False*)` | Creates a 1D tensor
    of logarithmically spaced points between the `start` and `end` |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**eye**(*n, m=None, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | Creates a 2D tensor with ones on the diagonal and zeros
    everywhere else |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**full**(*size, fill_value, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | Creates a tensor filled with `fill_value`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**load**(*f*)` | Loads a tensor from a serialized pickle file |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.**save**(*f*)` | Saves a tensor to a serialized pickle file |'
  prefs: []
  type: TYPE_TB
- en: 'The PyTorch documentation contains a complete list of functions for creating
    tensors as well as more detailed explanations of how to use them. Here are some
    common pitfalls and additional insights to keep in mind when creating tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: Most creation functions accept the optional `dtype` and `device` parameters,
    so you can set these at creation time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should use `torch.arange()` in favor of the deprecated `torch.range()` function.
    Use `torch.arange()` when the step size is known. Use `torch.linspace()` when
    the number of elements is known.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use `torch.tensor()` to create tensors from array-like structures such
    as lists, NumPy arrays, tuples, and sets. To convert existing tensors to NumPy
    arrays and lists, use the `torch.numpy()` and `torch.tolist()` functions, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensor Attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One PyTorch quality that has contributed to its popularity is the fact that
    it’s very Pythonic and object oriented in nature. Since a tensor is its own data
    type, you can read attributes of the tensor object itself. Now that you can create
    tensors, it’s useful to be able to quickly find information about them by accessing
    their attributes. Assuming `x` is a tensor, you can access several attributes
    of `x` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x.dtype`'
  prefs: []
  type: TYPE_NORMAL
- en: Indicates the tensor’s data type (see [Table 2-2](#table_tensor_dtype) for a
    list of PyTorch data types)
  prefs: []
  type: TYPE_NORMAL
- en: '`x.device`'
  prefs: []
  type: TYPE_NORMAL
- en: Indicates the tensor’s device location (e.g., CPU or GPU memory)
  prefs: []
  type: TYPE_NORMAL
- en: '`x.shape`'
  prefs: []
  type: TYPE_NORMAL
- en: Shows the tensor’s dimensions
  prefs: []
  type: TYPE_NORMAL
- en: '`x.ndim`'
  prefs: []
  type: TYPE_NORMAL
- en: Identifies the number of a tensor’s dimensions or rank
  prefs: []
  type: TYPE_NORMAL
- en: '`x.requires_grad`'
  prefs: []
  type: TYPE_NORMAL
- en: A Boolean attribute that indicates whether the tensor keeps track of graph computations
    (see [“Automatic Differentiation (Autograd)”](#section_autograd))
  prefs: []
  type: TYPE_NORMAL
- en: '`x.grad`'
  prefs: []
  type: TYPE_NORMAL
- en: Contains the actual gradients if `requires_grad` is `True`
  prefs: []
  type: TYPE_NORMAL
- en: '`x.grad_fn`'
  prefs: []
  type: TYPE_NORMAL
- en: Stores the graph computation function used if `requires_grad` is `True`
  prefs: []
  type: TYPE_NORMAL
- en: '`x.s_cuda`, `x.is_sparse`, `x.is_quantized`, `x.is_leaf`, `x.is_mkldnn`'
  prefs: []
  type: TYPE_NORMAL
- en: Boolean attributes that indicate whether the tensor meets certain conditions
  prefs: []
  type: TYPE_NORMAL
- en: '`x.layout`'
  prefs: []
  type: TYPE_NORMAL
- en: Indicates how a tensor is laid out in memory
  prefs: []
  type: TYPE_NORMAL
- en: Remember that when accessing object attributes, you do not include parentheses
    (`()`) like you would with a class method (e.g., use `x.shape`, not `x.shape()`).
  prefs: []
  type: TYPE_NORMAL
- en: Data Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During deep learning development, it’s important to be aware of the data type
    used by your data and its calculations. So when you create tensors, you should
    control what data types are being used. As mentioned previously, all tensor elements
    have the same data type. You can specify the data type when creating the tensor
    by using the `dtype` parameter, or you can cast a tensor to a new `dtype` using
    the appropriate casting method or the `to()` method, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_tensors_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Pass in the data type.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_tensors_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Define the data type directly with `dtype`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_tensors_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Python automatically converts `x` to `float32` and returns `z` as `float32`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the casting and `to()` methods do not change the tensor’s data type
    unless you reassign the tensor. Also, when performing operations on mixed data
    types, PyTorch will automatically cast tensors to the appropriate type.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the tensor creation functions allow you to specify the data type upon
    creation using the `dtype` parameter. When you set the `dtype` or cast tensors,
    remember to use the `torch` namespace (e.g., `torch.int64`, not just `int64`).
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-2](#table_tensor_dtype) lists all the available data types in PyTorch.
    Each data type results in a different tensor class depending on the tensor’s device.
    The corresponding tensor classes are shown in the two rightmost columns for CPUs
    and GPUs, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2\. Tensor data types
  prefs: []
  type: TYPE_NORMAL
- en: '| Data type | dtype | CPU tensor | GPU tensor |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 32-bit floating point (default) | `torch.float32` or `torch.float` | `torch.​​Float⁠Ten⁠sor`
    | `torch.cuda.​Float⁠Tensor` |'
  prefs: []
  type: TYPE_TB
- en: '| 64-bit floating point | `torch.float64` or `torch.dou⁠ble` | `torch.​​Dou⁠ble⁠Tensor`
    | `torch.cuda.​​Dou⁠bleTensor` |'
  prefs: []
  type: TYPE_TB
- en: '| 16-bit floating point | `torch.float16` or `torch.half` | `torch.​Half⁠Tensor`
    | `torch.cuda.​Half⁠Tensor` |'
  prefs: []
  type: TYPE_TB
- en: '| 8-bit integer (unsigned) | `torch.uint8` | `torch.​Byte⁠Tensor` | `torch.cuda.​Byte⁠Tensor`
    |'
  prefs: []
  type: TYPE_TB
- en: '| 8-bit integer (signed) | `torch.int8` | `torch.​Char⁠Tensor` | `torch.cuda.​Char⁠Tensor`
    |'
  prefs: []
  type: TYPE_TB
- en: '| 16-bit integer (signed) | `torch.int16` or `torch.short` | `torch.​Short⁠Tensor`
    | `torch.cuda.​Short⁠Tensor` |'
  prefs: []
  type: TYPE_TB
- en: '| 32-bit integer (signed) | `torch.int32` or `torch.int` | `torch.​IntTen⁠sor`
    | `torch.cuda.​IntTen⁠sor` |'
  prefs: []
  type: TYPE_TB
- en: '| 64-bit integer (signed) | `torch.int64` or `torch.long` | `torch.​Long⁠Tensor`
    | `torch.cuda.​Long⁠Tensor` |'
  prefs: []
  type: TYPE_TB
- en: '| Boolean | `torch.bool` | `torch.​Bool⁠Tensor` | `torch.cuda.​Bool⁠Tensor`
    |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To reduce space complexity, you may sometimes want to reuse memory and overwrite
    tensor values using *in-place operations*. To perform in-place operations, append
    the underscore (_) postfix to the function name. For example, the function `y.add_(x)`
    adds `x` to `y`, but the results will be stored in `y`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Tensors from Random Samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The need to create random data comes up often during deep learning development.
    Sometimes you will need to initialize weights to random values or create random
    inputs with specified distributions. PyTorch supports a very robust set of functions
    that you can use to create tensors from random data.
  prefs: []
  type: TYPE_NORMAL
- en: As with other creation functions, you can specify the dtype and device when
    creating the tensor. [Table 2-3](#table_random_ops) lists some examples of random
    sampling functions.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-3\. Random sampling functions
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`rand`**`(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | Selects random values from a uniform distribution on
    the interval [0 to 1] |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`randn`**`(**size, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | Selects random values from a standard normal
    distribution with zero mean unit variance |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`normal`**`(*mean, std, *, generator=None, out=None*)` | Selects
    random numbers from a normal distribution with a specified mean and variance |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`randint`**`(*low=0, high, size, *, generator=None, out=None, dtype=None,
    layout=torch.strided, device=None, requires_grad=False*)` | Selects random integers
    generated uniformly between specified low and high values |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`randperm`**`(*n, out=None, dtype=torch.int64, layout=torch.strided,
    device=None, requires_grad=False*)` | Creates a random permutation of integers
    from 0 to *n*–1 |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`bernoulli`**`(*input, *, generator=None, out=None*)` | Draws binary
    random numbers (0 or 1) from a Bernoulli distribution |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`multinomial`**`(*input, num_samples, replacement=False, *, generator=None,
    out=None*)` | Selects a random number from a list according to weights from a
    multinomial distribution |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You  can  also  create  tensors  of  values  sampled  from  more  advanced 
    distributions,  like  Cauchy,  exponential,  geometric,  and log normal. To do
    so, use `torch.empty()` to  create  the  tensor  and  apply  an  in-place  function 
    for  the  distribution  (e.g., Cauchy).  Remember,  in-place  methods use the
    underscore postfix. For example, `x = torch.empty([10,5]).cauchy_()` creates a
    tensor of random numbers drawn from the Cauchy distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Tensors Like Other Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You  may  want  to  create  and  initialize  a  tensor   that  has  similar
    properties to another tensor, including the `dtype`, `device`,  and `layout`  properties
     to  facilitate  calculations.  Many  of the tensor creation operations have a
    similarity function that allows you to easily do this. The similarity functions
    will have the postfix `_like`. For example, `torch.empty_like(tensor_a)` will
    create an empty tensor with the `dtype`, `device`, and `layout` properties of
    `tensor_a`. Some examples of similarity functions include `empty_like()`, `zeros_like()`,
    `ones_like()`, `full_like()`, `rand_like()`, `randn_like()`, and `rand_int_like()`.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you understand how to create tensors, let’s explore what you can do
    with them. PyTorch supports a robust set of tensor operations that allow you to
    access and transform your tensor data.
  prefs: []
  type: TYPE_NORMAL
- en: First I’ll describe how to access portions of your data, manipulate their elements,
    and combine tensors to form new tensors. Then I’ll show you how to perform simple
    calculations as well as advanced mathematical computations, often in constant
    time. PyTorch provides many built-in functions. It’s useful to check what’s available
    before creating your own.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing, Slicing, Combining, and Splitting Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have created tensors, you may want to access portions of the data
    and combine or split tensors to form new tensors. The following code demonstrates
    how to perform these types of operations. You can slice and index tensors in the
    same way you would slice and index NumPy arrays, as shown in the first few lines
    of the following code. Note that indexing and slicing will return tensors even
    if the array is only a single element. You will need to use the `item()` function
    to convert a single-element tensor to a Python value when passing to other functions
    like `print()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we see that we can perform slicing using the same `[*start*:*end*:*step*]`
    format that is used for slicing Python lists and NumPy arrays. We can also use
    Boolean indexing to extract portions of the data that meet certain criteria, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'PyTorch also supports transposing and reshaping arrays, as shown in the next
    few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also combine or split tensors by using functions like `torch.stack()`
    and `torch.unbind()`, respectively, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: PyTorch provides a robust set of built-in functions that can be used to access,
    split, and combine tensors in different ways. [Table 2-4](#table_indexing_ops)
    lists some commonly used functions to manipulate tensor elements.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-4\. Indexing, slicing, combining, and splitting operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`cat`**`()` | Concatenates the given sequence of tensors in the
    given dimension. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`chunk`**`()` | Splits a tensor into a specific number of chunks.
    Each chunk is a view of the input tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`gather`**`()` | Gathers values along an axis specified by the
    dimension. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`index_select`**`()` | Returns a new tensor that indexes the input
    tensor along a dimension using the entries in the index, which is a `LongTensor`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`masked_select`**`()` | Returns a new 1D tensor that indexes the
    input tensor according to the Boolean mask, which is a `BoolTensor`. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`narrow`**`()` | Returns a tensor that is a narrow version of the
    input tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`nonzero`**`()` | Returns the indices of nonzero elements. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`reshape`**`()` | Returns a tensor with the same data and number
    of elements as the input tensor, but a different shape. Use `view()` instead to
    ensure the tensor is not copied. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`split`**`()` | Splits the tensor into chunks. Each chunk is a
    view or subdivision of the original tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`squeeze`**`()` | Returns a tensor with all the dimensions of the
    input tensor of size 1 removed. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`stack`**`()` | Concatenates a sequence of tensors along a new
    dimension. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`t`**`()` | Expects the input to be a 2D tensor and transposes
    dimensions 0 and 1. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`take`**`()` | Returns a tensor at specified indices when slicing
    is not continuous. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`transpose`**`()` | Transposes only the specified dimensions. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`unbind`**`()` | Removes a tensor dimension by returning a tuple
    of the removed dimension. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`unsqueeze`**`()` | Returns a new tensor with a dimension of size
    1 inserted at the specified position. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`where`**`()` | Returns a tensor of selected elements from either
    one of two tensors, depending on the specified condition. |'
  prefs: []
  type: TYPE_TB
- en: 'Some of these functions may seem redundant. However, the following key distinctions
    and best practices are important to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '`item()` is an important and commonly used function to return the Python number
    from a tensor containing a single value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `view()` instead of `reshape()` for reshaping tensors in most cases. Using
    `reshape()` may cause the tensor to be copied, depending on its layout in memory.
    `view()` ensures that it will not be copied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `x.T` or `x.t()` is a simple way to transpose 1D or 2D tensors. Use `transpose()`
    when dealing with multidimensional tensors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `torch.squeeze()` function is used often in deep learning to remove an unused
    dimension. For example, a batch of images with a single image can be reduced from
    4D to 3D using `squeeze()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `torch.unsqueeze()` function is often used in deep learning to add a dimension
    of size 1\. Since most PyTorch models expect a batch of data as an input, you
    could apply `unsqueeze()` when you only have one data sample. For example, you
    can pass a 3D image into `torch.unsqueeze()` to create a batch of one image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch is very Pythonic in nature. Like most Python classes, some PyTorch functions
    can be applied directly on a tensor using a built-in method such as `x.size()`.
  prefs: []
  type: TYPE_NORMAL
- en: Other functions are called directly using the `torch` namespace. These functions
    take a tensor as an input, like the `x` in `torch.save(x, 'tensor.pt')`.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Operations for Mathematics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning development is strongly based on mathematical computations, so
    PyTorch supports a very robust set of built-in math functions. Whether you are
    creating new data transforms, customizing loss functions, or building your own
    optimization algorithms, you can speed up your research and development with the
    math functions provided by PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this section is to provide a quick overview of many of the mathematical
    functions available in PyTorch so that you can quickly build your awareness of
    what currently exists and find the appropriate functions when needed.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch supports many different types of math functions, including pointwise
    operations, reduction functions, comparison calculations, and linear algebra operations,
    as well as spectral and other math computations. The first category of useful
    math operations we’ll look at are *pointwise operations*. Pointwise operations
    perform an operation on each point in the tensor individually and return a new
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: They are useful for rounding and truncation as well as trigonometrical and logical
    operations. By default, the functions will create a new tensor or use one passed
    in by the `out` parameter. If you want to perform an in-place operation, remember
    to append an underscore to the function name.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-5](#table_pointwise_ops) lists some commonly used pointwise operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-5\. Pointwise operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Operation type | Sample functions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Basic math | `add()`, `div()`, `mul()`, `neg()`, `reciprocal()`, `true_divide()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Truncation | `ceil()`, `clamp()`, `floor()`, `floor_divide()`, `fmod()`,
    `frac()`, `lerp()`, `remainder()`, `round()`, `sigmoid()`, `trunc()` |'
  prefs: []
  type: TYPE_TB
- en: '| Complex numbers | `abs()`, `angle()`, `conj()`, `imag()`, `real()` |'
  prefs: []
  type: TYPE_TB
- en: '| Trigonometry | `acos()`, `asin()`, `atan()`, `cos()`, `cosh()`, `deg2rad()`,
    `rad2deg()`, `sin()`, `sinh()`, `tan()`, `tanh()` |'
  prefs: []
  type: TYPE_TB
- en: '| Exponents and logarithms | `exp()`, `expm1()`, `log()`, `log10()`, `log1p()`,
    `log2()`, `logaddexp()`, `pow()`, `rsqrt()`, `sqrt()`, `square()` |'
  prefs: []
  type: TYPE_TB
- en: '| Logical | `logical_and()`, `logical_not()`, `logical_or()`, `logical_xor()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cumulative math | `addcdiv()`, `addcmul()` |'
  prefs: []
  type: TYPE_TB
- en: '| Bitwise operators | `bitwise_not()`, `bitwise_and()`, `bitwise_or()`, `bitwise_xor()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Error functions | `erf()`, `erfc()`, `erfinv()` |'
  prefs: []
  type: TYPE_TB
- en: '| Gamma functions | `digamma()`, `lgamma()`, `mvlgamma()`, `polygamma()` |'
  prefs: []
  type: TYPE_TB
- en: Use Python hints or refer to the PyTorch documentation for details on function
    usage. Note that `true_divide()` converts tensor data to floats first and should
    be used when dividing integers to obtain true division results.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Three different syntaxes can be used for most tensor operations. Tensors support
    operator overloading, so you can use operators directly, as in `z = x + y`. Although
    you can also use PyTorch functions such as `torch.add()` to do the same thing,
    this is less common. Lastly, you can perform in-place operations using the underscore
    (_) postfix. The function `y.add_(x)` achieves the same results, but they’ll’
    be stored in `y`.
  prefs: []
  type: TYPE_NORMAL
- en: The second category of math functions we’ll look at are *reduction operations*.
    Reduction operations reduce a bunch of numbers down to a single number or a smaller
    set of numbers. That is, they reduce the *dimensionality* or *rank* of the tensor.
    Reduction operations include functions for finding maximum or minimum values as
    well as many statistical calculations, like finding the mean or standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: These operations are frequently used in deep learning. For example, deep learning
    classification often uses the `argmax()` function to reduce softmax outputs to
    a dominant class.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-6](#table_reduction_ops) lists some commonly used reduction operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-6\. Reduction operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`argmax`**`(`*`input, dim, keepdim=False, out=None`*`)` | Returns
    the index(es) of the maximum value across all elements, or just a dimension if
    it’s specified |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`argmin`**`(`*`input, dim, keepdim=False, out=None`*`)` | Returns
    the index(es) of the minimum value across all elements, or just a dimension if
    it’s specified |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`dist`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the *p*-norm of two tensors |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`logsumexp`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the log of summed exponentials of each row of the input tensor in the given dimension
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`mean`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the mean or average across all elements, or just a dimension if it’s specified
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`median`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the median or middle value across all elements, or just a dimension if it’s specified
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`mode`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the mode or most frequent value across all elements, or just a dimension if it’s
    specified |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`norm`**`(`*`input, p=''fro'', dim=None,`* *`keepdim=False,`* *`out=None,
    dtype=None`*`)` | Computes the matrix or vector norm across all elements, or just
    a dimension if it’s specified |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`prod`**`(`*`input, dim, keepdim=False, dtype=None`*`)` | Computes
    the product of all elements, or of each row of the input tensor if it’s specified
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`std`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the standard deviation across all elements, or just a dimension if it’s specified
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`std_mean`**`(`*`input, unbiased=True`*`)` | Computes the standard
    deviation and mean across all elements, or just a dimension if it’s specified
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`sum`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the sum of all elements, or just a dimension if it’s specified |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`unique`**`(`*`input, dim, keepdim=False, out=None`*`)` | Removes
    duplicates across the entire tensor, or just a dimension if it’s specified |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`unique_​con⁠⁠secu⁠⁠tive`**`(`*`input, dim, keepdim=False, out=None`*`)`
    | Similar to `torch.unique()` but only removes consecutive duplicates |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`var`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the variance across all elements, or just a dimension if it’s specified |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`var_mean`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the mean and variance across all elements, or just a dimension if it’s specified
    |'
  prefs: []
  type: TYPE_TB
- en: Note that many of these functions accept the `dim` parameter, which specifies
    the dimension of reduction for multidimensional tensors. This is similar to the
    `axis` parameter in NumPy. By default, when `dim` is not specified, the reduction
    occurs across all dimensions. Specifying `dim = 1` will compute the operation
    across each row. For example, `torch.mean(x,1)` will compute the mean for each
    row in tensor `x`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s common to chain methods together. For example, `torch.rand(2,2).max().item()`
    creates a 2 × 2 tensor of random floats, finds the maximum value, and returns
    the value itself from the resulting tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at PyTorch’s *comparison functions*. Comparison functions usually
    compare all the values within a tensor, or compare one tensor’s values to another’s.
    They can return a tensor full of Booleans based on each element’s value such as
    `torch.eq()` or `torch.is_boolean()`. There are also functions to find the maximum
    or minimum value, sort tensor values, return the top subset of tensor elements,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-7](#table_comparison_ops) lists some commonly used comparison functions
    for your reference.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-7\. Comparison operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Operation type | Sample functions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Compare a tensor to other tensors | `eq()`, `ge()`, `gt()`, `le()`, `lt()`,
    `ne()` or `==`, `>`, `>=`, `<`, `<=`, `!=`, respectively |'
  prefs: []
  type: TYPE_TB
- en: '| Test tensor status or conditions | `isclose()`, `isfinite()`, `isinf()`,
    `isnan()` |'
  prefs: []
  type: TYPE_TB
- en: '| Return a single Boolean for the entire tensor | `allclose()`, `equal()` |'
  prefs: []
  type: TYPE_TB
- en: '| Find value(s) over the entire tensor or along a given dimension | `argsort()`,
    `kthvalue()`, `max()`, `min()`, `sort()`, `topk()`, |'
  prefs: []
  type: TYPE_TB
- en: 'Comparison functions seem pretty straightforward; however, there are a few
    key points to keep in mind. Common pitfalls include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `torch.eq()` function or `==` returns a tensor of the same size with a Boolean
    result for each element. The `torch.equal()` function tests if the tensors are
    the same size, and if all elements within the tensor are equal then it returns
    a single Boolean value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function `torch.allclose()` also returns a single Boolean value if all elements
    are close to a specified value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next type of mathematical functions we’ll look at are *linear algebra functions*.
    Linear algebra functions facilitate matrix operations and are important for deep
    learning computations.
  prefs: []
  type: TYPE_NORMAL
- en: Many computations, including gradient descent and optimization algorithms, use
    linear algebra to implement their calculations. PyTorch supports a robust set
    of built-in linear algebra operations, many of which are based on the Basic Linear
    Algebra Subprograms (BLAS) and Linear Algebra Package (LAPACK) standardized libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-8](#table_linalg_ops) lists some commonly used linear algebra operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-8\. Linear algebra operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`matmul`**`()` | Computes a matrix product of two tensors; supports
    broadcasting |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`chain_matmul`**`()` | Computes a matrix product of *N* tensors
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`mm`**`()` | Computes a matrix product of two tensors (if broadcasting
    is required, use `matmul()`) |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`addmm`**`()` | Computes a matrix product of two tensors and adds
    it to the input |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`bmm`**`()` | Computes a batch of matrix products |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`addbmm`**`()` | Computes a batch of matrix products and adds it
    to the input |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`baddbmm`**`()` | Computes a batch of matrix products and adds
    it to the input batch |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`mv`**`()` | Computes the product of the matrix and vector |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`addmv`**`()` | Computes the product of the matrix and vector and
    adds it to the input |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`matrix_power`** | Returns a tensor raised to the power of *n*
    (for square tensors) |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`eig`**`()` | Finds the eigenvalues and eigenvectors of a real
    square tensor |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`inverse`**`()` | Computes the inverse of a square tensor |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`det`**`()` | Computes the determinant of a matrix or batch of
    matrices |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`logdet`**`()` | Computes the log determinant of a matrix or batch
    of matrices |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`dot`**`()` | Computes the inner product of two tensors |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`addr`**`()` | Computes the outer product of two tensors and adds
    it to the input |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`solve`**`()` | Returns the solution to a system of linear equations
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`svd`**`()` | Performs a single-value decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`pca_lowrank`**`()` | Performs a linear principle component analysis
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`cholesky`**`()` | Computes a Cholesky decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`cholesky_inverse`**`()` | Computes the inverse of a symmetric
    positive definite matrix and returns the Cholesky factor |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.`**`cholesky_solve`**`()` | Solves a system of linear equations using
    the Cholesky factor |'
  prefs: []
  type: TYPE_TB
- en: The functions in [Table 2-8](#table_linalg_ops) range from matrix multiplication
    and batch calculations functions to solvers. It’s important to point out that
    matrix multiplication is not the same as pointwise multiplication with `torch.mul()`
    or the * operator.
  prefs: []
  type: TYPE_NORMAL
- en: A complete study of linear algebra is beyond the scope of this book, but you
    may find it useful to access some of the linear algebra functions when performing
    feature reduction or developing custom deep learning algorithms. See the [PyTorch
    linear algebra documentation](https://pytorch.tips/linear-algebra) for a complete
    list of available functions and more details on how to use them.
  prefs: []
  type: TYPE_NORMAL
- en: The final type of mathematical operations we’ll consider are *spectral and other
    math operations*. Depending on the domain of interest, these functions may be
    useful for data transforms or analysis. For example, spectral operations like
    the fast Fourier transform (FFT) can play an important role in computer vision
    or digital signal processing applications.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-9](#table_other_ops) lists some built-in operations for spectrum analysis
    and other mathematical operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-9\. Spectral and other math operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Operation type | Sample functions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Fast, inverse, and short-time Fourier transforms | `fft()`, `ifft()`, `stft()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Real-to-complex FFT and complex-to-real inverse FFT (IFFT) | `rfft()`, `irfft()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Windowing algorithms | `bartlett_window()`, `blackman_window()`, `hamming_window()`,
    `hann_window()` |'
  prefs: []
  type: TYPE_TB
- en: '| Histogram and bin counts | `histc()`, `bincount()` |'
  prefs: []
  type: TYPE_TB
- en: '| Cumulative operations | `cummax()`, `cummin()`, `cumprod()`, `cumsum()`,
    `trace()` (sum of the diagonal), `einsum()` (sum of products using Einstein summation)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Normalization functions | `cdist()`, `renorm()` |'
  prefs: []
  type: TYPE_TB
- en: '| Cross product, dot product, and Cartesian product | `cross()`, `tensordot()`,
    `cartesian_prod()` |'
  prefs: []
  type: TYPE_TB
- en: '| Functions that create a diagonal tensor with elements of the input tensor
    | `diag()`, `diag_embed()`, `diag_flat()`, `diagonal()` |'
  prefs: []
  type: TYPE_TB
- en: '| Einstein summation | `einsum()` |'
  prefs: []
  type: TYPE_TB
- en: '| Matrix reduction and restructuring functions | `flatten()`, `flip()`, `rot90()`,
    `repeat_interleave()`, `meshgrid()`, `roll()`, `combinations()` |'
  prefs: []
  type: TYPE_TB
- en: '| Functions that return the lower or upper triangles and their indices | `tril()`,
    `tril_indices`, `triu()`, `triu_indices()` |'
  prefs: []
  type: TYPE_TB
- en: Automatic Differentiation (Autograd)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One function, `backward()`, is worth calling out in its own subsection because
    it’s what makes PyTorch so powerful for deep learning development. The `backward()`
    function uses PyTorch’s automatic differentiation package, `torch.autograd`, to
    differentiate and compute gradients of tensors based on the chain rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple example of autodifferentiation. We define a function, *f* =
    sum(*x*²), where x is a matrix of variables. If we want to find *df* / *dx* for
    each variable in the matrix, we need to set the `requires_grad = True` flag for
    the tensor *x*, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `f.backward()` function performs the differentiation with respect to *f*
    and stores *df* / *dx* in the `x.grad` attribute. A quick review of calculus differential
    equations will tell us the derivation of *f* with respect to *x*, *df* / *dx*
    = 2*x*. The results of evaluating *df* / *dx* for the values of *x* are shown
    as the output.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Only tensors of floating-point `dtype` can require gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Training NNs requires us to compute the weight gradients on the backward pass.
    As our NNs get deeper and more complex, this feature automates the complex computations.
    For more information on how autograd works, see the [Autograd tutorial](https://pytorch.tips/autograd-explained).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provided a quick reference for creating tensors and performing
    operations. Now that you have a good foundation on tensors, we will focus on how
    to use tensors and PyTorch to perform deep learning research. In the next chapter,
    we will review the deep learning development process before jumping into writing
    code.
  prefs: []
  type: TYPE_NORMAL
