- en: Chapter 2\. Tensors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章. 张量
- en: 'Before we dive deep into the world of PyTorch development, it’s important to
    familiarize yourself with the fundamental data structure in PyTorch: the `torch.Tensor`.
    By understanding the tensor, you will understand how PyTorch handles and stores
    data, and since deep learning is fundamentally the collection and manipulation
    of floating-point numbers, understanding tensors will help you understand how
    PyTorch implements more advanced functions for deep learning. In addition, you
    may find yourself using tensor operations frequently when preprocessing input
    data or manipulating output data during model development.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解PyTorch开发世界之前，熟悉PyTorch中的基本数据结构`torch.Tensor`是很重要的。通过理解张量，您将了解PyTorch如何处理和存储数据，由于深度学习基本上是浮点数的收集和操作，理解张量将帮助您了解PyTorch如何为深度学习实现更高级的功能。此外，在预处理输入数据或在模型开发过程中操作输出数据时，您可能经常使用张量操作。
- en: This chapter serves as a quick reference to understanding tensors and implementing
    tensor functions within your code. I’ll begin by describing what a tensor is and
    show you some simple examples of how to use functions to create, manipulate, and
    accelerate tensor operations on a GPU. Next, we’ll take a broader look at the
    API for creating tensors and performing math operations so that you can quickly
    reference a comprehensive list of tensor capabilities. In each section, we will
    explore some of the more important functions, identify common pitfalls, and examine
    key points in their usage.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章作为理解张量和在代码中实现张量函数的快速参考。我将从描述张量是什么开始，并向您展示如何使用函数来创建、操作和加速GPU上的张量操作的一些简单示例。接下来，我们将更广泛地查看创建张量和执行数学操作的API，以便您可以快速查阅一份全面的张量功能列表。在每个部分中，我们将探讨一些更重要的函数，识别常见的陷阱，并检查它们的使用中的关键点。
- en: What Is a Tensor?
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量是什么？
- en: In PyTorch, a tensor is a data structure used to store and manipulate data.
    Like a NumPy array, a tensor is a multidimensional array containing elements of
    a single data type. Tensors can be used to represent scalars, vectors, matrices,
    and *n*-dimensional arrays and are derived from the `torch.Tensor` class. However,
    tensors are more than just arrays of numbers. Creating or instantiating a tensor
    object from the `torch.Tensor` class gives us access to a set of built-in class
    attributes and operations or class methods that provide a robust set of built-in
    capabilities. This chapter describes these attributes and operations in detail.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，张量是一种用于存储和操作数据的数据结构。与NumPy数组类似，张量是一个包含单一数据类型元素的多维数组。张量可以用来表示标量、向量、矩阵和*n*维数组，并且是从`torch.Tensor`类派生的。然而，张量不仅仅是数字数组。从`torch.Tensor`类创建或实例化张量对象使我们可以访问一组内置的类属性和操作或类方法，提供了一套强大的内置功能。本章详细描述了这些属性和操作。
- en: Tensors also include added benefits that make them more suitable than NumPy
    arrays for deep learning calculations. First, tensor operations can be performed
    significantly faster using GPU acceleration. Second, tensors can be stored and
    manipulated at scale using distributed processing on multiple CPUs and GPUs and
    across multiple servers. And third, tensors keep track of their graph computations,
    which as we will see in [“Automatic Differentiation (Autograd)”](#section_autograd)
    is very important in implementing a deep learning library.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 张量还包括一些附加优势，使它们比NumPy数组更适合用于深度学习计算。首先，使用GPU加速可以显著加快张量操作的速度。其次，可以使用分布式处理在多个CPU和GPU上以及跨多个服务器上存储和操作张量。第三，张量跟踪它们的图计算，正如我们将在[“自动微分（Autograd）”](#section_autograd)中看到的，这在实现深度学习库中非常重要。
- en: To further explain what a tensor actually is and how to use one, I’ll begin
    by walking through a simple example that creates some tensors and performs a tensor
    operation.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步解释张量实际上是什么以及如何使用它，我将从一个简单示例开始，创建一些张量并执行一个张量操作。
- en: Simple CPU Example
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单CPU示例
- en: 'Here’s a simple example that creates a tensor, performs a tensor operation,
    and uses a built-in method on the tensor itself. By default, the tensor data type
    will be derived from the input data type and the tensor will be allocated to the
    CPU device. First, we import the PyTorch library, then we create two tensors,
    `x` and `y`, from two-dimensional lists. Next, we add the two tensors and store
    the result in `z`. We can just use the `+` operator here because the `torch.Tensor`
    class supports operator overloading. Finally, we print the new tensor, `z`, which
    we can see is the matrix sum of `x` and `y`, and we print the size of `z`. Notice
    that `z` is a tensor object itself and the `size()` method is used to return its
    matrix dimensions, namely 2 × 3:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的示例，创建一个张量，执行一个张量操作，并在张量本身上使用一个内置方法。默认情况下，张量数据类型将从输入数据类型派生，并且张量将分配到CPU设备。首先，我们导入PyTorch库，然后我们从二维列表创建两个张量`x`和`y`。接下来，我们将这两个张量相加，并将结果存储在`z`中。我们可以在这里使用`+`运算符，因为`torch.Tensor`类支持运算符重载。最后，我们打印新的张量`z`，我们可以看到它是`x`和`y`的矩阵和，并打印`z`的大小。注意，`z`本身是一个张量对象，`size()`方法用于返回其矩阵维度，即2×3：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You   may   see   the  `torch.Tensor()`  (capital T)   constructor   used   in 
     legacy   code.   This   is   an   alias   for   the   default   tensor  type
    `torch.FloatTensor`. You should instead use `torch.tensor()` to create your tensors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在旧代码中可能会看到使用`torch.Tensor()`（大写T）构造函数。这是`torch.FloatTensor`默认张量类型的别名。您应该改用`torch.tensor()`来创建您的张量。
- en: Simple GPU Example
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单GPU示例
- en: Since the ability to accelerate tensor operations on a GPU is a major advantage
    of tensors over NumPy arrays, I’ll show you an easy example of this. This is the
    same example from the last section, but here we move the tensors to the GPU device
    if one is available. Notice that the output tensor is also allocated to the GPU.
    You can use the device attribute (e.g., `z.device`) to double-check where the
    tensor resides.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在GPU上加速张量操作是张量优于NumPy数组的主要优势，我将向您展示一个简单的示例。这是上一节中的相同示例，但在这里，如果有GPU设备，我们将将张量移动到GPU设备。请注意，输出张量也分配给了GPU。您可以使用设备属性（例如`z.device`）来双重检查张量所在的位置。
- en: 'In the first line, the `torch.cuda.is_available()` function will return `True`
    if your machine has GPU support. This is a convenient way to write more robust
    code that can be accelerated when a GPU exists but also runs on a CPU when a GPU
    is not present. In the output, `device=''cuda:0''` indicates that the first GPU
    is being used. If your machine contains multiple GPUs, you can also control which
    GPU is being used:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行中，`torch.cuda.is_available()`函数将在您的机器支持GPU时返回`True`。这是一种方便的编写更健壮代码的方式，可以在存在GPU时加速运行，但在没有GPU时也可以在CPU上运行。在输出中，`device='cuda:0'`表示正在使用第一个GPU。如果您的机器包含多个GPU，您还可以控制使用哪个GPU：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Moving Tensors Between CPUs and GPUs
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在CPU和GPU之间移动张量
- en: 'The previous code uses `torch.tensor()` to create a tensor on a specific device;
    however, it’s more common to move an existing tensor to a device, namely a GPU
    if available. You can do so by using the `torch.to()` method. When new tensors
    are created as a result of tensor operations, PyTorch will create the new tensor
    on the same device. In the following code, `z` resides on the GPU because `x`
    and `y` reside on the GPU. The tensor `z` is moved back to the CPU using `torch.to("cpu")`
    for further processing. Also note that all the tensors within the operation must
    be on the same device. If `x` was on the GPU and `y` was on the CPU, we would
    get an error:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码使用`torch.tensor()`在特定设备上创建张量；然而，更常见的是将现有张量移动到设备上，即如果有GPU设备的话，通常是GPU。您可以使用`torch.to()`方法来实现。当通过张量操作创建新张量时，PyTorch会在相同设备上创建新张量。在下面的代码中，`z`位于GPU上，因为`x`和`y`位于GPU上。张量`z`通过`torch.to("cpu")`移回CPU进行进一步处理。还请注意，操作中的所有张量必须在同一设备上。如果`x`在GPU上，而`y`在CPU上，我们将会收到错误：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can use strings directly as device parameters instead of device objects.
    The following are all equivalent:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接使用字符串作为设备参数，而不是设备对象。以下都是等效的：
- en: '`device="cuda"`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device="cuda"`'
- en: '`device=torch.device("cuda")`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device=torch.device("cuda")`'
- en: '`device="cuda:0"`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device="cuda:0"`'
- en: '`device=torch.device("cuda:0")`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device=torch.device("cuda:0")`'
- en: Creating Tensors
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建张量
- en: The previous section showed a simple way to create tensors; however, there are
    many other ways to do it. You can create tensors from preexisting numeric data
    or create random samplings. Tensors can be created from preexisting data stored
    in array-like structures such as lists, tuples, scalars, or serialized data files,
    as well as in NumPy arrays.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节展示了创建张量的简单方法；然而，还有许多其他方法可以实现。您可以从现有的数字数据中创建张量，也可以创建随机抽样。张量可以从存储在类似数组结构中的现有数据（如列表、元组、标量或序列化数据文件）以及NumPy数组中创建。
- en: 'The following code illustrates some common ways to create tensors. First, it
    shows how to create a tensor from a list using `torch.tensor()`. This method can
    also be used to create tensors from other data structures like tuples, sets, or
    NumPy arrays:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了创建张量的一些常见方法。首先，它展示了如何使用`torch.tensor()`从列表创建张量。此方法也可用于从其他数据结构（如元组、集合或NumPy数组）创建张量：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_tensors_CO1-1)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_tensors_CO1-1)'
- en: From a list
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从列表中
- en: '[![2](Images/2.png)](#co_tensors_CO1-2)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_tensors_CO1-2)'
- en: From a tuple
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从元组中
- en: '[![3](Images/3.png)](#co_tensors_CO1-3)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_tensors_CO1-3)'
- en: From a NumPy array
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从NumPy数组中
- en: '[![4](Images/4.png)](#co_tensors_CO1-4)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_tensors_CO1-4)'
- en: Uninitialized; element values are not predictable
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 未初始化；元素值不可预测
- en: '[![5](Images/5.png)](#co_tensors_CO1-5)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_tensors_CO1-5)'
- en: All elements initialized with 0.0
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有元素初始化为0.0
- en: '[![6](Images/6.png)](#co_tensors_CO1-6)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_tensors_CO1-6)'
- en: All elements initialized with 1.0
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所有元素初始化为1.0
- en: As shown in the previous code sample, you can also create and initialize tensors
    by using functions like `torch.empty()`, `torch.ones()`, and `torch.zeros()` and
    specifying the desired size.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码示例所示，您还可以使用`torch.empty()`、`torch.ones()`和`torch.zeros()`等函数创建和初始化张量，并指定所需的大小。
- en: 'If you want to initialize a tensor with random values, PyTorch supports a robust
    set of functions that you can use, such as `torch.rand()`, `torch.randn()`, and
    `torch.randint()`, as shown in the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要使用随机值初始化张量，PyTorch支持一组强大的函数，例如`torch.rand()`、`torch.randn()`和`torch.randint()`，如下面的代码所示：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_tensors_CO2-1)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_tensors_CO2-1)'
- en: Creates a 100 × 200 tensor with elements from a uniform distribution on the
    interval [0, 1).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个100×200的张量，元素来自区间[0,1)上的均匀分布。
- en: '[![2](Images/2.png)](#co_tensors_CO2-2)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_tensors_CO2-2)'
- en: Elements are random numbers from a normal distribution with a mean of 0 and
    a variance of 1.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 元素是均值为0、方差为1的正态分布随机数。
- en: '[![3](Images/3.png)](#co_tensors_CO2-3)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_tensors_CO2-3)'
- en: Elements are random integers between 5 and 10.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 元素是介于5和10之间的随机整数。
- en: Upon initialization, you can specify the data type and device (i.e., CPU or
    GPU) as shown in the previous code sample. In addition, the example shows how
    you can use PyTorch to create tensors that have the same properties as other tensors
    but are initialized with different data. Functions with the `_like` postfix such
    as `torch.empty_like()` and `torch.ones_like()` return tensors that have the same
    size, data type, and device as another tensor but are initialized differently
    (see [“Creating Tensors from Random Samples”](#creating-tensors-section)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化时，您可以像前面的代码示例中所示指定数据类型和设备（即CPU或GPU）。此外，示例展示了如何使用PyTorch创建具有与其他张量相同属性但使用不同数据初始化的张量。带有`_like`后缀的函数，如`torch.empty_like()`和`torch.ones_like()`，返回具有与另一个张量相同大小、数据类型和设备的张量，但初始化方式不同（参见[“从随机样本创建张量”](#creating-tensors-section)）。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are some legacy functions, such as `from_numpy()` and `as_tensor()`, that
    have been replaced in practice by the `torch.tensor()` constructor, which can
    be used to handle all cases.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一些旧函数，如`from_numpy()`和`as_tensor()`，在实践中已被`torch.tensor()`构造函数取代，该构造函数可用于处理所有情况。
- en: '[Table 2-1](#table_creation_ops) lists PyTorch functions used to create tensors.
    You should use each one with the `torch` namespace, e.g., `torch.empty()`. You
    can find more details by visiting [the PyTorch tensor documentation](https://pytorch.tips/torch).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-1](#table_creation_ops)列出了用于创建张量的PyTorch函数。您应该使用`torch`命名空间下的每个函数，例如`torch.empty()`。您可以通过访问[PyTorch张量文档](https://pytorch.tips/torch)获取更多详细信息。'
- en: Table 2-1\. Tensor creation functions
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1. 张量创建函数
- en: '| Function | Description |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `torch.**tensor**(*data, dtype=None, device=None, requires_grad=False, pin_memory=False*)`
    | Creates a tensor from an existing data structure |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**tensor**(*data, dtype=None, device=None, requires_grad=False, pin_memory=False*)`
    | 从现有数据结构创建张量 |'
- en: '| `torch.**empty**(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | Creates a tensor from uninitialized elements based on
    the random state of values in memory |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**empty**(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | 根据内存中值的随机状态创建未初始化元素的张量 |'
- en: '| `torch.**zeros**(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | Creates a tensor with all elements initialized to 0.0
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**zeros**(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | 创建一个所有元素初始化为0.0的张量 |'
- en: '| `torch.**ones**(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | Creates a tensor with all elements initialized to 1.0
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**ones**(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | 创建一个所有元素初始化为1.0的张量 |'
- en: '| `torch.**arange**(*start=0, end, step=1, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | Creates a 1D tensor of values over a range
    with a common step value |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**arange**(*start=0, end, step=1, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | 使用公共步长值在范围内创建值的一维张量 |'
- en: '| `torch.**linspace**(*start, end, steps=100, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | Creates a 1D tensor of linearly spaced points
    between the `start` and `end` |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**linspace**(*start, end, steps=100, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | 在`start`和`end`之间创建线性间隔点的一维张量 |'
- en: '| `torch.**logspace**(*start, end, steps=100, base=10.0, out=None, dtype=None,
    layout=torch.strided, device=None, requires_grad=False*)` | Creates a 1D tensor
    of logarithmically spaced points between the `start` and `end` |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**logspace**(*start, end, steps=100, base=10.0, out=None, dtype=None,
    layout=torch.strided, device=None, requires_grad=False*)` | 在`start`和`end`之间创建对数间隔点的一维张量
    |'
- en: '| `torch.**eye**(*n, m=None, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | Creates a 2D tensor with ones on the diagonal and zeros
    everywhere else |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**eye**(*n, m=None, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | 创建一个对角线为1，其他位置为0的二维张量 |'
- en: '| `torch.**full**(*size, fill_value, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | Creates a tensor filled with `fill_value`
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**full**(*size, fill_value, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | 创建一个填充了`fill_value`的张量 |'
- en: '| `torch.**load**(*f*)` | Loads a tensor from a serialized pickle file |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**load**(*f*)` | 从序列化的pickle文件中加载张量 |'
- en: '| `torch.**save**(*f*)` | Saves a tensor to a serialized pickle file |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `torch.**save**(*f*)` | 将张量保存到序列化的pickle文件中 |'
- en: 'The PyTorch documentation contains a complete list of functions for creating
    tensors as well as more detailed explanations of how to use them. Here are some
    common pitfalls and additional insights to keep in mind when creating tensors:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch文档包含了创建张量的完整函数列表，以及如何使用它们的更详细解释。在创建张量时，请记住一些常见的陷阱和额外的见解：
- en: Most creation functions accept the optional `dtype` and `device` parameters,
    so you can set these at creation time.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数创建函数都接受可选的`dtype`和`device`参数，因此您可以在创建时设置这些参数。
- en: You should use `torch.arange()` in favor of the deprecated `torch.range()` function.
    Use `torch.arange()` when the step size is known. Use `torch.linspace()` when
    the number of elements is known.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torch.arange()`而不是已弃用的`torch.range()`函数。当步长已知时，请使用`torch.arange()`。当元素数量已知时，请使用`torch.linspace()`。
- en: You can use `torch.tensor()` to create tensors from array-like structures such
    as lists, NumPy arrays, tuples, and sets. To convert existing tensors to NumPy
    arrays and lists, use the `torch.numpy()` and `torch.tolist()` functions, respectively.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用`torch.tensor()`从类似数组的结构（如列表、NumPy数组、元组和集合）创建张量。要将现有张量转换为NumPy数组和列表，分别使用`torch.numpy()`和`torch.tolist()`函数。
- en: Tensor Attributes
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量属性
- en: 'One PyTorch quality that has contributed to its popularity is the fact that
    it’s very Pythonic and object oriented in nature. Since a tensor is its own data
    type, you can read attributes of the tensor object itself. Now that you can create
    tensors, it’s useful to be able to quickly find information about them by accessing
    their attributes. Assuming `x` is a tensor, you can access several attributes
    of `x` as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch受欢迎的一个特点是它非常符合Python风格且面向对象。由于张量是自己的数据类型，因此可以读取张量对象本身的属性。现在您可以创建张量了，通过访问它们的属性，可以快速查找有关它们的信息是很有用的。假设`x`是一个张量，您可以按如下方式访问`x`的几个属性：
- en: '`x.dtype`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`x.dtype`'
- en: Indicates the tensor’s data type (see [Table 2-2](#table_tensor_dtype) for a
    list of PyTorch data types)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 指示张量的数据类型（请参见[表2-2](#table_tensor_dtype)列出的PyTorch数据类型列表）
- en: '`x.device`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`x.device`'
- en: Indicates the tensor’s device location (e.g., CPU or GPU memory)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 指示张量的设备位置（例如，CPU或GPU内存）
- en: '`x.shape`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`x.shape`'
- en: Shows the tensor’s dimensions
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 显示张量的维度
- en: '`x.ndim`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`x.ndim`'
- en: Identifies the number of a tensor’s dimensions or rank
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 标识张量的维数或秩
- en: '`x.requires_grad`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`x.requires_grad`'
- en: A Boolean attribute that indicates whether the tensor keeps track of graph computations
    (see [“Automatic Differentiation (Autograd)”](#section_autograd))
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个布尔属性，指示张量是否跟踪图计算（参见[“自动微分（Autograd）”](#section_autograd)）
- en: '`x.grad`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`x.grad`'
- en: Contains the actual gradients if `requires_grad` is `True`
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`requires_grad`为`True`，则包含实际的梯度
- en: '`x.grad_fn`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`x.grad_fn`'
- en: Stores the graph computation function used if `requires_grad` is `True`
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`requires_grad`为`True`，则存储使用的图计算函数
- en: '`x.s_cuda`, `x.is_sparse`, `x.is_quantized`, `x.is_leaf`, `x.is_mkldnn`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`x.s_cuda`，`x.is_sparse`，`x.is_quantized`，`x.is_leaf`，`x.is_mkldnn`'
- en: Boolean attributes that indicate whether the tensor meets certain conditions
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 指示张量是否满足某些条件的布尔属性
- en: '`x.layout`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`x.layout`'
- en: Indicates how a tensor is laid out in memory
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 指示张量在内存中的布局方式
- en: Remember that when accessing object attributes, you do not include parentheses
    (`()`) like you would with a class method (e.g., use `x.shape`, not `x.shape()`).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请记但，当访问对象属性时，不要像调用类方法那样包括括号（`()`）（例如，使用`x.shape`，而不是`x.shape()`）。
- en: Data Types
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据类型
- en: 'During deep learning development, it’s important to be aware of the data type
    used by your data and its calculations. So when you create tensors, you should
    control what data types are being used. As mentioned previously, all tensor elements
    have the same data type. You can specify the data type when creating the tensor
    by using the `dtype` parameter, or you can cast a tensor to a new `dtype` using
    the appropriate casting method or the `to()` method, as shown in the following
    code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习开发中，了解数据及其计算所使用的数据类型非常重要。因此，在创建张量时，应该控制所使用的数据类型。如前所述，所有张量元素具有相同的数据类型。您可以在创建张量时使用`dtype`参数指定数据类型，或者可以使用适当的转换方法或`to()`方法将张量转换为新的`dtype`，如下面的代码所示：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_tensors_CO3-1)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_tensors_CO3-1)'
- en: Pass in the data type.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 传入数据类型。
- en: '[![2](Images/2.png)](#co_tensors_CO3-2)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_tensors_CO3-2)'
- en: Define the data type directly with `dtype`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 直接使用`dtype`定义数据类型。
- en: '[![3](Images/3.png)](#co_tensors_CO3-3)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_tensors_CO3-3)'
- en: Python automatically converts `x` to `float32` and returns `z` as `float32`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Python会自动将`x`转换为`float32`，并将`z`返回为`float32`。
- en: Note that the casting and `to()` methods do not change the tensor’s data type
    unless you reassign the tensor. Also, when performing operations on mixed data
    types, PyTorch will automatically cast tensors to the appropriate type.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，转换和`to()`方法不会改变张量的数据类型，除非重新分配张量。此外，在执行混合数据类型的操作时，PyTorch会自动将张量转换为适当的类型。
- en: Most of the tensor creation functions allow you to specify the data type upon
    creation using the `dtype` parameter. When you set the `dtype` or cast tensors,
    remember to use the `torch` namespace (e.g., `torch.int64`, not just `int64`).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数张量创建函数允许您在创建时使用`dtype`参数指定数据类型。在设置`dtype`或转换张量时，请记住使用`torch`命名空间（例如，使用`torch.int64`，而不仅仅是`int64`）。
- en: '[Table 2-2](#table_tensor_dtype) lists all the available data types in PyTorch.
    Each data type results in a different tensor class depending on the tensor’s device.
    The corresponding tensor classes are shown in the two rightmost columns for CPUs
    and GPUs, respectively.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-2](#table_tensor_dtype)列出了PyTorch中所有可用的数据类型。每种数据类型都会导致不同的张量类，具体取决于张量的设备。相应的张量类分别显示在CPU和GPU的最右两列中。'
- en: Table 2-2\. Tensor data types
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-2. 张量数据类型
- en: '| Data type | dtype | CPU tensor | GPU tensor |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | dtype | CPU张量 | GPU张量 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 32-bit floating point (default) | `torch.float32` or `torch.float` | `torch.​​Float⁠Ten⁠sor`
    | `torch.cuda.​Float⁠Tensor` |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 32位浮点数（默认） | `torch.float32`或`torch.float` | `torch.FloatTensor` | `torch.cuda.FloatTensor`
    |'
- en: '| 64-bit floating point | `torch.float64` or `torch.dou⁠ble` | `torch.​​Dou⁠ble⁠Tensor`
    | `torch.cuda.​​Dou⁠bleTensor` |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 64位浮点数 | `torch.float64`或`torch.double` | `torch.DoubleTensor` | `torch.cuda.DoubleTensor`
    |'
- en: '| 16-bit floating point | `torch.float16` or `torch.half` | `torch.​Half⁠Tensor`
    | `torch.cuda.​Half⁠Tensor` |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 16位浮点数 | `torch.float16`或`torch.half` | `torch.HalfTensor` | `torch.cuda.HalfTensor`
    |'
- en: '| 8-bit integer (unsigned) | `torch.uint8` | `torch.​Byte⁠Tensor` | `torch.cuda.​Byte⁠Tensor`
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 8位整数（无符号） | `torch.uint8` | `torch.ByteTensor` | `torch.cuda.ByteTensor`
    |'
- en: '| 8-bit integer (signed) | `torch.int8` | `torch.​Char⁠Tensor` | `torch.cuda.​Char⁠Tensor`
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 8位整数（有符号） | `torch.int8` | `torch.CharTensor` | `torch.cuda.CharTensor` |'
- en: '| 16-bit integer (signed) | `torch.int16` or `torch.short` | `torch.​Short⁠Tensor`
    | `torch.cuda.​Short⁠Tensor` |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 16位整数（有符号） | `torch.int16`或`torch.short` | `torch.ShortTensor` | `torch.cuda.ShortTensor`
    |'
- en: '| 32-bit integer (signed) | `torch.int32` or `torch.int` | `torch.​IntTen⁠sor`
    | `torch.cuda.​IntTen⁠sor` |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 32位整数（有符号） | `torch.int32`或`torch.int` | `torch.IntTensor` | `torch.cuda.IntTensor`
    |'
- en: '| 64-bit integer (signed) | `torch.int64` or `torch.long` | `torch.​Long⁠Tensor`
    | `torch.cuda.​Long⁠Tensor` |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 64位整数（有符号） | `torch.int64`或`torch.long` | `torch.LongTensor` | `torch.cuda.LongTensor`
    |'
- en: '| Boolean | `torch.bool` | `torch.​Bool⁠Tensor` | `torch.cuda.​Bool⁠Tensor`
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 布尔值 | `torch.bool` | `torch.BoolTensor` | `torch.cuda.BoolTensor` |'
- en: Note
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To reduce space complexity, you may sometimes want to reuse memory and overwrite
    tensor values using *in-place operations*. To perform in-place operations, append
    the underscore (_) postfix to the function name. For example, the function `y.add_(x)`
    adds `x` to `y`, but the results will be stored in `y`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少空间复杂度，有时您可能希望重用内存并使用*就地操作*覆盖张量值。要执行就地操作，请在函数名称后附加下划线(_)后缀。例如，函数`y.add_(x)`将`x`添加到`y`，但结果将存储在`y`中。
- en: Creating Tensors from Random Samples
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从随机样本创建张量
- en: The need to create random data comes up often during deep learning development.
    Sometimes you will need to initialize weights to random values or create random
    inputs with specified distributions. PyTorch supports a very robust set of functions
    that you can use to create tensors from random data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习开发过程中经常需要创建随机数据。有时您需要将权重初始化为随机值或创建具有指定分布的随机输入。PyTorch支持一组非常强大的函数，您可以使用这些函数从随机数据创建张量。
- en: As with other creation functions, you can specify the dtype and device when
    creating the tensor. [Table 2-3](#table_random_ops) lists some examples of random
    sampling functions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他创建函数一样，您可以在创建张量时指定dtype和device。[表2-3](#table_random_ops)列出了一些随机抽样函数的示例。
- en: Table 2-3\. Random sampling functions
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-3. 随机抽样函数
- en: '| Function | Description |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `torch.`**`rand`**`(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | Selects random values from a uniform distribution on
    the interval [0 to 1] |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`rand`**`(**size, out=None, dtype=None, layout=torch.strided, device=None,
    requires_grad=False*)` | 从区间[0到1]上的均匀分布中选择随机值 |'
- en: '| `torch.`**`randn`**`(**size, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | Selects random values from a standard normal
    distribution with zero mean unit variance |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`randn`**`(**size, out=None, dtype=None, layout=torch.strided,
    device=None, requires_grad=False*)` | 从均值为零方差为单位的标准正态分布中选择随机值 |'
- en: '| `torch.`**`normal`**`(*mean, std, *, generator=None, out=None*)` | Selects
    random numbers from a normal distribution with a specified mean and variance |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`normal`**`(*mean, std, *, generator=None, out=None*)` | 从具有指定均值和方差的正态分布中选择随机数
    |'
- en: '| `torch.`**`randint`**`(*low=0, high, size, *, generator=None, out=None, dtype=None,
    layout=torch.strided, device=None, requires_grad=False*)` | Selects random integers
    generated uniformly between specified low and high values |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`randint`**`(*low=0, high, size, *, generator=None, out=None, dtype=None,
    layout=torch.strided, device=None, requires_grad=False*)` | 在指定的低值和高值之间生成均匀分布的随机整数
    |'
- en: '| `torch.`**`randperm`**`(*n, out=None, dtype=torch.int64, layout=torch.strided,
    device=None, requires_grad=False*)` | Creates a random permutation of integers
    from 0 to *n*–1 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`randperm`**`(*n, out=None, dtype=torch.int64, layout=torch.strided,
    device=None, requires_grad=False*)` | 创建从0到*n*-1的整数的随机排列 |'
- en: '| `torch.`**`bernoulli`**`(*input, *, generator=None, out=None*)` | Draws binary
    random numbers (0 or 1) from a Bernoulli distribution |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`bernoulli`**`(*input, *, generator=None, out=None*)` | 从伯努利分布中绘制二进制随机数（0或1）
    |'
- en: '| `torch.`**`multinomial`**`(*input, num_samples, replacement=False, *, generator=None,
    out=None*)` | Selects a random number from a list according to weights from a
    multinomial distribution |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`multinomial`**`(*input, num_samples, replacement=False, *, generator=None,
    out=None*)` | 根据多项分布中的权重从列表中选择一个随机数 |'
- en: Tip
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You  can  also  create  tensors  of  values  sampled  from  more  advanced 
    distributions,  like  Cauchy,  exponential,  geometric,  and log normal. To do
    so, use `torch.empty()` to  create  the  tensor  and  apply  an  in-place  function 
    for  the  distribution  (e.g., Cauchy).  Remember,  in-place  methods use the
    underscore postfix. For example, `x = torch.empty([10,5]).cauchy_()` creates a
    tensor of random numbers drawn from the Cauchy distribution.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以创建从更高级分布（如柯西分布、指数分布、几何分布和对数正态分布）中抽样的值张量。为此，使用`torch.empty()`创建张量，并对分布（例如柯西分布）应用就地函数。请记住，就地方法使用下划线后缀。例如，`x
    = torch.empty([10,5]).cauchy_()`创建一个从柯西分布中抽取的随机数张量。
- en: Creating Tensors Like Other Tensors
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 像其他张量一样创建张量
- en: You  may  want  to  create  and  initialize  a  tensor   that  has  similar
    properties to another tensor, including the `dtype`, `device`,  and `layout`  properties
     to  facilitate  calculations.  Many  of the tensor creation operations have a
    similarity function that allows you to easily do this. The similarity functions
    will have the postfix `_like`. For example, `torch.empty_like(tensor_a)` will
    create an empty tensor with the `dtype`, `device`, and `layout` properties of
    `tensor_a`. Some examples of similarity functions include `empty_like()`, `zeros_like()`,
    `ones_like()`, `full_like()`, `rand_like()`, `randn_like()`, and `rand_int_like()`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望创建并初始化一个具有与另一个张量相似属性的张量，包括`dtype`、`device`和`layout`属性，以便进行计算。许多张量创建操作都有一个相似性函数，允许您轻松地执行此操作。相似性函数将具有后缀`_like`。例如，`torch.empty_like(tensor_a)`将创建一个具有`tensor_a`的`dtype`、`device`和`layout`属性的空张量。一些相似性函数的示例包括`empty_like()`、`zeros_like()`、`ones_like()`、`full_like()`、`rand_like()`、`randn_like()`和`rand_int_like()`。
- en: Tensor Operations
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量操作
- en: Now that you understand how to create tensors, let’s explore what you can do
    with them. PyTorch supports a robust set of tensor operations that allow you to
    access and transform your tensor data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您了解如何创建张量，让我们探索您可以对其执行的操作。PyTorch支持一组强大的张量操作，允许您访问和转换张量数据。
- en: First I’ll describe how to access portions of your data, manipulate their elements,
    and combine tensors to form new tensors. Then I’ll show you how to perform simple
    calculations as well as advanced mathematical computations, often in constant
    time. PyTorch provides many built-in functions. It’s useful to check what’s available
    before creating your own.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我将描述如何访问数据的部分，操作它们的元素，并组合张量以形成新的张量。然后我将向您展示如何执行简单的计算以及高级的数学计算，通常在恒定时间内。PyTorch提供了许多内置函数。在创建自己的函数之前检查可用的函数是很有用的。
- en: Indexing, Slicing, Combining, and Splitting Tensors
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引、切片、组合和拆分张量
- en: 'Once you have created tensors, you may want to access portions of the data
    and combine or split tensors to form new tensors. The following code demonstrates
    how to perform these types of operations. You can slice and index tensors in the
    same way you would slice and index NumPy arrays, as shown in the first few lines
    of the following code. Note that indexing and slicing will return tensors even
    if the array is only a single element. You will need to use the `item()` function
    to convert a single-element tensor to a Python value when passing to other functions
    like `print()`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 创建张量后，您可能希望访问数据的部分并组合或拆分张量以形成新张量。以下代码演示了如何执行这些类型的操作。您可以像切片和索引NumPy数组一样切片和索引张量，如以下代码的前几行所示。请注意，即使数组只有一个元素，索引和切片也会返回张量。在传递给`print()`等其他函数时，您需要使用`item()`函数将单个元素张量转换为Python值：
- en: '[PRE6]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the following code, we see that we can perform slicing using the same `[*start*:*end*:*step*]`
    format that is used for slicing Python lists and NumPy arrays. We can also use
    Boolean indexing to extract portions of the data that meet certain criteria, as
    shown here:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们可以看到我们可以使用与用于切片Python列表和NumPy数组相同的`[*start*:*end*:*step*]`格式执行切片。我们还可以使用布尔索引来提取满足某些条件的数据部分，如下所示：
- en: '[PRE7]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'PyTorch also supports transposing and reshaping arrays, as shown in the next
    few lines of code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还支持转置和重塑数组，如下面的代码所示：
- en: '[PRE8]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can also combine or split tensors by using functions like `torch.stack()`
    and `torch.unbind()`, respectively, as shown in the following code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`torch.stack()`和`torch.unbind()`等函数组合或拆分张量，如下面的代码所示：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: PyTorch provides a robust set of built-in functions that can be used to access,
    split, and combine tensors in different ways. [Table 2-4](#table_indexing_ops)
    lists some commonly used functions to manipulate tensor elements.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了一组强大的内置函数，可用于以不同方式访问、拆分和组合张量。[表2-4](#table_indexing_ops)列出了一些常用的用于操作张量元素的函数。
- en: Table 2-4\. Indexing, slicing, combining, and splitting operations
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-4。索引、切片、组合和拆分操作
- en: '| Function | Description |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `torch.`**`cat`**`()` | Concatenates the given sequence of tensors in the
    given dimension. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`cat`**`()` | 在给定维度中连接给定序列的张量。 |'
- en: '| `torch.`**`chunk`**`()` | Splits a tensor into a specific number of chunks.
    Each chunk is a view of the input tensor. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`chunk`**`()` | 将张量分成特定数量的块。每个块都是输入张量的视图。 |'
- en: '| `torch.`**`gather`**`()` | Gathers values along an axis specified by the
    dimension. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`gather`**`()` | 沿着由维度指定的轴收集值。 |'
- en: '| `torch.`**`index_select`**`()` | Returns a new tensor that indexes the input
    tensor along a dimension using the entries in the index, which is a `LongTensor`.
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`index_select`**`()` | 使用索引中的条目沿着维度索引输入张量的新张量，索引是`LongTensor`。
    |'
- en: '| `torch.`**`masked_select`**`()` | Returns a new 1D tensor that indexes the
    input tensor according to the Boolean mask, which is a `BoolTensor`. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`masked_select`**`()` | 根据布尔掩码（`BoolTensor`）索引输入张量的新1D张量。 |'
- en: '| `torch.`**`narrow`**`()` | Returns a tensor that is a narrow version of the
    input tensor. |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`narrow`**`()` | 返回输入张量的窄版本的张量。 |'
- en: '| `torch.`**`nonzero`**`()` | Returns the indices of nonzero elements. |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`nonzero`**`()` | 返回非零元素的索引。 |'
- en: '| `torch.`**`reshape`**`()` | Returns a tensor with the same data and number
    of elements as the input tensor, but a different shape. Use `view()` instead to
    ensure the tensor is not copied. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`reshape`**`()` | 返回一个与输入张量具有相同数据和元素数量但形状不同的张量。使用`view()`而不是确保张量不被复制。
    |'
- en: '| `torch.`**`split`**`()` | Splits the tensor into chunks. Each chunk is a
    view or subdivision of the original tensor. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`split`**`()` | 将张量分成块。每个块都是原始张量的视图或子分区。 |'
- en: '| `torch.`**`squeeze`**`()` | Returns a tensor with all the dimensions of the
    input tensor of size 1 removed. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`squeeze`**`()` | 返回一个去除输入张量所有尺寸为1的维度的张量。 |'
- en: '| `torch.`**`stack`**`()` | Concatenates a sequence of tensors along a new
    dimension. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`stack`**`()` | 沿新维度连接一系列张量。 |'
- en: '| `torch.`**`t`**`()` | Expects the input to be a 2D tensor and transposes
    dimensions 0 and 1. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`t`**`()` | 期望输入为2D张量并转置维度0和1。 |'
- en: '| `torch.`**`take`**`()` | Returns a tensor at specified indices when slicing
    is not continuous. |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`take`**`()` | 在切片不连续时返回指定索引处的张量。 |'
- en: '| `torch.`**`transpose`**`()` | Transposes only the specified dimensions. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`transpose`**`()` | 仅转置指定的维度。 |'
- en: '| `torch.`**`unbind`**`()` | Removes a tensor dimension by returning a tuple
    of the removed dimension. |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`unbind`**`()` | 通过返回已删除维度的元组来移除张量维度。 |'
- en: '| `torch.`**`unsqueeze`**`()` | Returns a new tensor with a dimension of size
    1 inserted at the specified position. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`unsqueeze`**`()` | 返回一个在指定位置插入大小为1的维度的新张量。 |'
- en: '| `torch.`**`where`**`()` | Returns a tensor of selected elements from either
    one of two tensors, depending on the specified condition. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`where`**`()` | 根据指定条件从两个张量中的一个返回所选元素的张量。 |'
- en: 'Some of these functions may seem redundant. However, the following key distinctions
    and best practices are important to keep in mind:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些函数可能看起来多余。然而，重要的是要记住以下关键区别和最佳实践：
- en: '`item()` is an important and commonly used function to return the Python number
    from a tensor containing a single value.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`item()`是一个重要且常用的函数，用于从包含单个值的张量返回Python数字。'
- en: Use `view()` instead of `reshape()` for reshaping tensors in most cases. Using
    `reshape()` may cause the tensor to be copied, depending on its layout in memory.
    `view()` ensures that it will not be copied.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数情况下，用`view()`代替`reshape()`来重新塑造张量。使用`reshape()`可能会导致张量被复制，这取决于其在内存中的布局。`view()`确保不会被复制。
- en: Using `x.T` or `x.t()` is a simple way to transpose 1D or 2D tensors. Use `transpose()`
    when dealing with multidimensional tensors.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`x.T`或`x.t()`是转置1D或2D张量的简单方法。处理多维张量时，请使用`transpose()`。
- en: The `torch.squeeze()` function is used often in deep learning to remove an unused
    dimension. For example, a batch of images with a single image can be reduced from
    4D to 3D using `squeeze()`.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.squeeze()`函数在深度学习中经常用于去除未使用的维度。例如，使用`squeeze()`可以将包含单个图像的图像批次从4D减少到3D。'
- en: The `torch.unsqueeze()` function is often used in deep learning to add a dimension
    of size 1\. Since most PyTorch models expect a batch of data as an input, you
    could apply `unsqueeze()` when you only have one data sample. For example, you
    can pass a 3D image into `torch.unsqueeze()` to create a batch of one image.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.unsqueeze()`函数在深度学习中经常用于添加大小为1的维度。由于大多数PyTorch模型期望批量数据作为输入，当您只有一个数据样本时，可以应用`unsqueeze()`。例如，您可以将一个3D图像传递给`torch.unsqueeze()`以创建一个图像批次。'
- en: Note
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: PyTorch is very Pythonic in nature. Like most Python classes, some PyTorch functions
    can be applied directly on a tensor using a built-in method such as `x.size()`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch在本质上非常符合Python的特性。与大多数Python类一样，一些PyTorch函数可以直接在张量上使用内置方法，例如`x.size()`。
- en: Other functions are called directly using the `torch` namespace. These functions
    take a tensor as an input, like the `x` in `torch.save(x, 'tensor.pt')`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其他函数直接使用`torch`命名空间调用。这些函数以张量作为输入，就像在`torch.save(x, 'tensor.pt')`中的`x`一样。
- en: Tensor Operations for Mathematics
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学张量操作
- en: Deep learning development is strongly based on mathematical computations, so
    PyTorch supports a very robust set of built-in math functions. Whether you are
    creating new data transforms, customizing loss functions, or building your own
    optimization algorithms, you can speed up your research and development with the
    math functions provided by PyTorch.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习开发在很大程度上基于数学计算，因此PyTorch支持非常强大的内置数学函数集。无论您是创建新的数据转换、自定义损失函数还是构建自己的优化算法，您都可以通过PyTorch提供的数学函数加快研究和开发速度。
- en: The purpose of this section is to provide a quick overview of many of the mathematical
    functions available in PyTorch so that you can quickly build your awareness of
    what currently exists and find the appropriate functions when needed.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的是快速概述PyTorch中许多可用的数学函数，以便您可以快速了解当前存在的内容，并在需要时找到适当的函数。
- en: PyTorch supports many different types of math functions, including pointwise
    operations, reduction functions, comparison calculations, and linear algebra operations,
    as well as spectral and other math computations. The first category of useful
    math operations we’ll look at are *pointwise operations*. Pointwise operations
    perform an operation on each point in the tensor individually and return a new
    tensor.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch支持许多不同类型的数学函数，包括逐点操作、缩减函数、比较计算以及线性代数操作，以及频谱和其他数学计算。我们将首先看一下有用的数学操作的第一类是*逐点操作*。逐点操作在张量中的每个点上执行操作，并返回一个新的张量。
- en: They are useful for rounding and truncation as well as trigonometrical and logical
    operations. By default, the functions will create a new tensor or use one passed
    in by the `out` parameter. If you want to perform an in-place operation, remember
    to append an underscore to the function name.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 它们对于舍入和截断以及三角和逻辑操作非常有用。默认情况下，这些函数将创建一个新的张量或使用由`out`参数传递的张量。如果要执行原地操作，请记得在函数名称后附加下划线。
- en: '[Table 2-5](#table_pointwise_ops) lists some commonly used pointwise operations.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-5](#table_pointwise_ops)列出了一些常用的逐点操作。'
- en: Table 2-5\. Pointwise operations
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-5. 逐点操作
- en: '| Operation type | Sample functions |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 操作类型 | 示例函数 |'
- en: '| --- | --- |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Basic math | `add()`, `div()`, `mul()`, `neg()`, `reciprocal()`, `true_divide()`
    |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 基本数学 | `add()`, `div()`, `mul()`, `neg()`, `reciprocal()`, `true_divide()`
    |'
- en: '| Truncation | `ceil()`, `clamp()`, `floor()`, `floor_divide()`, `fmod()`,
    `frac()`, `lerp()`, `remainder()`, `round()`, `sigmoid()`, `trunc()` |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 截断 | `ceil()`, `clamp()`, `floor()`, `floor_divide()`, `fmod()`, `frac()`,
    `lerp()`, `remainder()`, `round()`, `sigmoid()`, `trunc()` |'
- en: '| Complex numbers | `abs()`, `angle()`, `conj()`, `imag()`, `real()` |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 复数 | `abs()`, `angle()`, `conj()`, `imag()`, `real()` |'
- en: '| Trigonometry | `acos()`, `asin()`, `atan()`, `cos()`, `cosh()`, `deg2rad()`,
    `rad2deg()`, `sin()`, `sinh()`, `tan()`, `tanh()` |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 三角函数 | `acos()`, `asin()`, `atan()`, `cos()`, `cosh()`, `deg2rad()`, `rad2deg()`,
    `sin()`, `sinh()`, `tan()`, `tanh()` |'
- en: '| Exponents and logarithms | `exp()`, `expm1()`, `log()`, `log10()`, `log1p()`,
    `log2()`, `logaddexp()`, `pow()`, `rsqrt()`, `sqrt()`, `square()` |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 指数和对数 | `exp()`, `expm1()`, `log()`, `log10()`, `log1p()`, `log2()`, `logaddexp()`,
    `pow()`, `rsqrt()`, `sqrt()`, `square()` |'
- en: '| Logical | `logical_and()`, `logical_not()`, `logical_or()`, `logical_xor()`
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑 | `logical_and()`, `logical_not()`, `logical_or()`, `logical_xor()` |'
- en: '| Cumulative math | `addcdiv()`, `addcmul()` |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 累积数学 | `addcdiv()`, `addcmul()` |'
- en: '| Bitwise operators | `bitwise_not()`, `bitwise_and()`, `bitwise_or()`, `bitwise_xor()`
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 位运算符 | `bitwise_not()`, `bitwise_and()`, `bitwise_or()`, `bitwise_xor()`
    |'
- en: '| Error functions | `erf()`, `erfc()`, `erfinv()` |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 错误函数 | `erf()`, `erfc()`, `erfinv()` |'
- en: '| Gamma functions | `digamma()`, `lgamma()`, `mvlgamma()`, `polygamma()` |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 伽玛函数 | `digamma()`, `lgamma()`, `mvlgamma()`, `polygamma()` |'
- en: Use Python hints or refer to the PyTorch documentation for details on function
    usage. Note that `true_divide()` converts tensor data to floats first and should
    be used when dividing integers to obtain true division results.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python提示或参考PyTorch文档以获取有关函数使用的详细信息。请注意，`true_divide()`首先将张量数据转换为浮点数，应在将整数除以以获得真实除法结果时使用。
- en: Note
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Three different syntaxes can be used for most tensor operations. Tensors support
    operator overloading, so you can use operators directly, as in `z = x + y`. Although
    you can also use PyTorch functions such as `torch.add()` to do the same thing,
    this is less common. Lastly, you can perform in-place operations using the underscore
    (_) postfix. The function `y.add_(x)` achieves the same results, but they’ll’
    be stored in `y`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数张量操作可以使用三种不同的语法。张量支持运算符重载，因此您可以直接使用运算符，例如`z = x + y`。虽然您也可以使用PyTorch函数如`torch.add()`来执行相同的操作，但这较少见。最后，您可以使用下划线(_)后缀执行原地操作。函数`y.add_(x)`可以实现相同的结果，但它们将存储在`y`中。
- en: The second category of math functions we’ll look at are *reduction operations*.
    Reduction operations reduce a bunch of numbers down to a single number or a smaller
    set of numbers. That is, they reduce the *dimensionality* or *rank* of the tensor.
    Reduction operations include functions for finding maximum or minimum values as
    well as many statistical calculations, like finding the mean or standard deviation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类数学函数是 *缩减操作*。 缩减操作将一堆数字减少到一个数字或一组较小的数字。 也就是说，它们减少了张量的 *维度* 或 *秩*。 缩减操作包括查找最大值或最小值以及许多统计计算的函数，例如查找平均值或标准差。
- en: These operations are frequently used in deep learning. For example, deep learning
    classification often uses the `argmax()` function to reduce softmax outputs to
    a dominant class.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作在深度学习中经常使用。 例如，深度学习分类通常使用 `argmax()` 函数将 softmax 输出缩减为主导类。
- en: '[Table 2-6](#table_reduction_ops) lists some commonly used reduction operations.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-6](#table_reduction_ops) 列出了一些常用的缩减操作。'
- en: Table 2-6\. Reduction operations
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-6\. 缩减操作
- en: '| Function | Description |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `torch.`**`argmax`**`(`*`input, dim, keepdim=False, out=None`*`)` | Returns
    the index(es) of the maximum value across all elements, or just a dimension if
    it’s specified |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`argmax`**`(`*`input, dim, keepdim=False, out=None`*`)` | 返回所有元素中最大值的索引，或者如果指定了维度，则只返回一个维度上的索引
    |'
- en: '| `torch.`**`argmin`**`(`*`input, dim, keepdim=False, out=None`*`)` | Returns
    the index(es) of the minimum value across all elements, or just a dimension if
    it’s specified |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`argmin`**`(`*`input, dim, keepdim=False, out=None`*`)` | 返回所有元素中最小值的索引，或者如果指定了维度，则只返回一个维度上的索引
    |'
- en: '| `torch.`**`dist`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the *p*-norm of two tensors |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`dist`**`(`*`input, dim, keepdim=False, out=None`*`)` | 计算两个张量的
    *p*-范数 |'
- en: '| `torch.`**`logsumexp`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the log of summed exponentials of each row of the input tensor in the given dimension
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`logsumexp`**`(`*`input, dim, keepdim=False, out=None`*`)` | 计算给定维度中输入张量的每行的指数和的对数
    |'
- en: '| `torch.`**`mean`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the mean or average across all elements, or just a dimension if it’s specified
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`mean`**`(`*`input, dim, keepdim=False, out=None`*`)` | 计算所有元素的平均值，或者如果指定了维度，则只计算一个维度上的平均值
    |'
- en: '| `torch.`**`median`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the median or middle value across all elements, or just a dimension if it’s specified
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`median`**`(`*`input, dim, keepdim=False, out=None`*`)` | 计算所有元素的中位数或中间值，或者如果指定了维度，则只计算一个维度上的中位数
    |'
- en: '| `torch.`**`mode`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the mode or most frequent value across all elements, or just a dimension if it’s
    specified |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`mode`**`(`*`input, dim, keepdim=False, out=None`*`)` | 计算所有元素的众数或最频繁出现的值，或者如果指定了维度，则只计算一个维度上的值
    |'
- en: '| `torch.`**`norm`**`(`*`input, p=''fro'', dim=None,`* *`keepdim=False,`* *`out=None,
    dtype=None`*`)` | Computes the matrix or vector norm across all elements, or just
    a dimension if it’s specified |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`norm`**`(`*`input, p=''fro'', dim=None,`* *`keepdim=False,`* *`out=None,
    dtype=None`*`)` | 计算所有元素的矩阵或向量范数，或者如果指定了维度，则只计算一个维度上的范数 |'
- en: '| `torch.`**`prod`**`(`*`input, dim, keepdim=False, dtype=None`*`)` | Computes
    the product of all elements, or of each row of the input tensor if it’s specified
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`prod`**`(`*`input, dim, keepdim=False, dtype=None`*`)` | 计算所有元素的乘积，或者如果指定了维度，则只计算输入张量的每行的乘积
    |'
- en: '| `torch.`**`std`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the standard deviation across all elements, or just a dimension if it’s specified
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`std`**`(`*`input, dim, keepdim=False, out=None`*`)` | 计算所有元素的标准差，或者如果指定了维度，则只计算一个维度上的标准差
    |'
- en: '| `torch.`**`std_mean`**`(`*`input, unbiased=True`*`)` | Computes the standard
    deviation and mean across all elements, or just a dimension if it’s specified
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`std_mean`**`(`*`input, unbiased=True`*`)` | 计算所有元素的标准差和平均值，或者如果指定了维度，则只计算一个维度上的标准差和平均值
    |'
- en: '| `torch.`**`sum`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the sum of all elements, or just a dimension if it’s specified |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`sum`**`(`*`input, dim, keepdim=False, out=None`*`)` | 计算所有元素的和，或者如果指定了维度，则只计算一个维度上的和
    |'
- en: '| `torch.`**`unique`**`(`*`input, dim, keepdim=False, out=None`*`)` | Removes
    duplicates across the entire tensor, or just a dimension if it’s specified |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`unique`**`(`*`input, dim, keepdim=False, out=None`*`)` | 在整个张量中删除重复项，或者如果指定了维度，则只删除一个维度上的重复项
    |'
- en: '| `torch.`**`unique_​con⁠⁠secu⁠⁠tive`**`(`*`input, dim, keepdim=False, out=None`*`)`
    | Similar to `torch.unique()` but only removes consecutive duplicates |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`unique_​con⁠⁠secu⁠⁠tive`**`(`*`input, dim, keepdim=False, out=None`*`)`
    | 类似于 `torch.unique()`，但仅删除连续的重复项 |'
- en: '| `torch.`**`var`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the variance across all elements, or just a dimension if it’s specified |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`var`**`(`*`input, dim, keepdim=False, out=None`*`)` | 计算所有元素的方差，或者如果指定了维度，则只计算一个维度上的方差
    |'
- en: '| `torch.`**`var_mean`**`(`*`input, dim, keepdim=False, out=None`*`)` | Computes
    the mean and variance across all elements, or just a dimension if it’s specified
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`var_mean`**`(`*`input, dim, keepdim=False, out=None`*`)` | 计算所有元素的平均值和方差，或者如果指定了维度，则只计算一个维度上的平均值和方差
    |'
- en: Note that many of these functions accept the `dim` parameter, which specifies
    the dimension of reduction for multidimensional tensors. This is similar to the
    `axis` parameter in NumPy. By default, when `dim` is not specified, the reduction
    occurs across all dimensions. Specifying `dim = 1` will compute the operation
    across each row. For example, `torch.mean(x,1)` will compute the mean for each
    row in tensor `x`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，许多这些函数接受 `dim` 参数，该参数指定多维张量的缩减维度。 这类似于 NumPy 中的 `axis` 参数。 默认情况下，当未指定 `dim`
    时，缩减会跨所有维度进行。 指定 `dim = 1` 将在每行上计算操作。 例如，`torch.mean(x,1)` 将计算张量 `x` 中每行的平均值。
- en: Tip
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: It’s common to chain methods together. For example, `torch.rand(2,2).max().item()`
    creates a 2 × 2 tensor of random floats, finds the maximum value, and returns
    the value itself from the resulting tensor.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 将方法链接在一起是常见的。 例如，`torch.rand(2,2).max().item()` 创建一个 2 × 2 的随机浮点数张量，找到最大值，并从结果张量中返回值本身。
- en: Next, we’ll look at PyTorch’s *comparison functions*. Comparison functions usually
    compare all the values within a tensor, or compare one tensor’s values to another’s.
    They can return a tensor full of Booleans based on each element’s value such as
    `torch.eq()` or `torch.is_boolean()`. There are also functions to find the maximum
    or minimum value, sort tensor values, return the top subset of tensor elements,
    and more.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下PyTorch的*比较函数*。比较函数通常比较张量中的所有值，或将一个张量的值与另一个张量的值进行比较。它们可以根据每个元素的值返回一个充满布尔值的张量，例如`torch.eq()`或`torch.is_boolean()`。还有一些函数可以找到最大或最小值，对张量值进行排序，返回张量元素的顶部子集等。
- en: '[Table 2-7](#table_comparison_ops) lists some commonly used comparison functions
    for your reference.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '| `torch.`**`svd`**`()` | 执行奇异值分解 |'
- en: Table 2-7\. Comparison operations
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-7\. 比较操作
- en: '| Operation type | Sample functions |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 操作类型 | 示例函数 |'
- en: '| --- | --- |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Compare a tensor to other tensors | `eq()`, `ge()`, `gt()`, `le()`, `lt()`,
    `ne()` or `==`, `>`, `>=`, `<`, `<=`, `!=`, respectively |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 将张量与其他张量进行比较 | `eq()`, `ge()`, `gt()`, `le()`, `lt()`, `ne()` 或 `==`, `>`,
    `>=`, `<`, `<=`, `!=`, 分别 |'
- en: '| Test tensor status or conditions | `isclose()`, `isfinite()`, `isinf()`,
    `isnan()` |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 测试张量状态或条件 | `isclose()`, `isfinite()`, `isinf()`, `isnan()` |'
- en: '| Return a single Boolean for the entire tensor | `allclose()`, `equal()` |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 返回整个张量的单个布尔值 | `allclose()`, `equal()` |'
- en: '| Find value(s) over the entire tensor or along a given dimension | `argsort()`,
    `kthvalue()`, `max()`, `min()`, `sort()`, `topk()`, |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 查找整个张量或沿给定维度的值 | `argsort()`, `kthvalue()`, `max()`, `min()`, `sort()`, `topk()`
    |'
- en: 'Comparison functions seem pretty straightforward; however, there are a few
    key points to keep in mind. Common pitfalls include the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 比较函数似乎很简单；然而，有一些关键点需要记住。常见的陷阱包括以下内容：
- en: The `torch.eq()` function or `==` returns a tensor of the same size with a Boolean
    result for each element. The `torch.equal()` function tests if the tensors are
    the same size, and if all elements within the tensor are equal then it returns
    a single Boolean value.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.eq()`函数或`==`返回一个相同大小的张量，每个元素都有一个布尔结果。`torch.equal()`函数测试张量是否具有相同的大小，如果张量中的所有元素都相等，则返回一个单个布尔值。'
- en: The function `torch.allclose()` also returns a single Boolean value if all elements
    are close to a specified value.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数`torch.allclose()`也会返回一个单个布尔值，如果所有元素都接近指定值。
- en: The next type of mathematical functions we’ll look at are *linear algebra functions*.
    Linear algebra functions facilitate matrix operations and are important for deep
    learning computations.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看一下*线性代数函数*。线性代数函数促进矩阵运算，对于深度学习计算非常重要。
- en: Many computations, including gradient descent and optimization algorithms, use
    linear algebra to implement their calculations. PyTorch supports a robust set
    of built-in linear algebra operations, many of which are based on the Basic Linear
    Algebra Subprograms (BLAS) and Linear Algebra Package (LAPACK) standardized libraries.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 许多计算，包括梯度下降和优化算法，使用线性代数来实现它们的计算。PyTorch支持一组强大的内置线性代数操作，其中许多基于基本线性代数子程序（BLAS）和线性代数包（LAPACK）标准化库。
- en: '[Table 2-8](#table_linalg_ops) lists some commonly used linear algebra operations.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2-8](#table_linalg_ops) 列出了一些常用的线性代数操作。'
- en: Table 2-8\. Linear algebra operations
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-8\. 线性代数操作
- en: '| Function | Description |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `torch.`**`matmul`**`()` | Computes a matrix product of two tensors; supports
    broadcasting |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`matmul`**`()` | 计算两个张量的矩阵乘积；支持广播 |'
- en: '| `torch.`**`chain_matmul`**`()` | Computes a matrix product of *N* tensors
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`chain_matmul`**`()` | 计算*N*个张量的矩阵乘积 |'
- en: '| `torch.`**`mm`**`()` | Computes a matrix product of two tensors (if broadcasting
    is required, use `matmul()`) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`mm`**`()` | 计算两个张量的矩阵乘积（如果需要广播，请使用`matmul()`） |'
- en: '| `torch.`**`addmm`**`()` | Computes a matrix product of two tensors and adds
    it to the input |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`addmm`**`()` | 计算两个张量的矩阵乘积并将其添加到输入中 |'
- en: '| `torch.`**`bmm`**`()` | Computes a batch of matrix products |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`bmm`**`()` | 计算一批矩阵乘积 |'
- en: '| `torch.`**`addbmm`**`()` | Computes a batch of matrix products and adds it
    to the input |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`addbmm`**`()` | 计算一批矩阵乘积并将其添加到输入中 |'
- en: '| `torch.`**`baddbmm`**`()` | Computes a batch of matrix products and adds
    it to the input batch |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`baddbmm`**`()` | 计算一批矩阵乘积并将其添加到输入批次 |'
- en: '| `torch.`**`mv`**`()` | Computes the product of the matrix and vector |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`mv`**`()` | 计算矩阵和向量的乘积 |'
- en: '| `torch.`**`addmv`**`()` | Computes the product of the matrix and vector and
    adds it to the input |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`addmv`**`()` | 计算矩阵和向量的乘积并将其添加到输入中 |'
- en: '| `torch.`**`matrix_power`** | Returns a tensor raised to the power of *n*
    (for square tensors) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`matrix_power`** | 返回张量的*n*次幂（对于方阵） |'
- en: '| `torch.`**`eig`**`()` | Finds the eigenvalues and eigenvectors of a real
    square tensor |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`eig`**`()` | 找到实方阵的特征值和特征向量 |'
- en: '| `torch.`**`inverse`**`()` | Computes the inverse of a square tensor |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`inverse`**`()` | 计算方阵的逆 |'
- en: '| `torch.`**`det`**`()` | Computes the determinant of a matrix or batch of
    matrices |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`det`**`()` | 计算矩阵或一批矩阵的行列式 |'
- en: '| `torch.`**`logdet`**`()` | Computes the log determinant of a matrix or batch
    of matrices |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`logdet`**`()` | 计算矩阵或一批矩阵的对数行列式 |'
- en: '| `torch.`**`dot`**`()` | Computes the inner product of two tensors |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`dot`**`()` | 计算两个张量的内积 |'
- en: '| `torch.`**`addr`**`()` | Computes the outer product of two tensors and adds
    it to the input |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`addr`**`()` | 计算两个张量的外积并将其添加到输入中 |'
- en: '| `torch.`**`solve`**`()` | Returns the solution to a system of linear equations
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`solve`**`()` | 返回线性方程组的解 |'
- en: '| `torch.`**`svd`**`()` | Performs a single-value decomposition |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '[表 2-7](#table_comparison_ops) 列出了一些常用的比较函数供参考。'
- en: '| `torch.`**`pca_lowrank`**`()` | Performs a linear principle component analysis
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`pca_lowrank`**`()` | 执行线性主成分分析 |'
- en: '| `torch.`**`cholesky`**`()` | Computes a Cholesky decomposition |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`cholesky`**`()` | 计算Cholesky分解 |'
- en: '| `torch.`**`cholesky_inverse`**`()` | Computes the inverse of a symmetric
    positive definite matrix and returns the Cholesky factor |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`cholesky_inverse`**`()` | 计算对称正定矩阵的逆并返回Cholesky因子 |'
- en: '| `torch.`**`cholesky_solve`**`()` | Solves a system of linear equations using
    the Cholesky factor |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| `torch.`**`cholesky_solve`**`()` | 使用Cholesky因子解线性方程组 |'
- en: The functions in [Table 2-8](#table_linalg_ops) range from matrix multiplication
    and batch calculations functions to solvers. It’s important to point out that
    matrix multiplication is not the same as pointwise multiplication with `torch.mul()`
    or the * operator.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-8](#table_linalg_ops)中的函数范围从矩阵乘法和批量计算函数到求解器。重要的是要指出，矩阵乘法与`torch.mul()`或*运算符的逐点乘法不同。'
- en: A complete study of linear algebra is beyond the scope of this book, but you
    may find it useful to access some of the linear algebra functions when performing
    feature reduction or developing custom deep learning algorithms. See the [PyTorch
    linear algebra documentation](https://pytorch.tips/linear-algebra) for a complete
    list of available functions and more details on how to use them.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不涵盖完整的线性代数研究，但在进行特征降维或开发自定义深度学习算法时，您可能会发现访问一些线性代数函数很有用。请参阅[PyTorch线性代数文档](https://pytorch.tips/linear-algebra)以获取可用函数的完整列表以及如何使用它们的更多详细信息。
- en: The final type of mathematical operations we’ll consider are *spectral and other
    math operations*. Depending on the domain of interest, these functions may be
    useful for data transforms or analysis. For example, spectral operations like
    the fast Fourier transform (FFT) can play an important role in computer vision
    or digital signal processing applications.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑的最后一类数学运算是*光谱和其他数学运算*。根据感兴趣的领域，这些函数可能对数据转换或分析有用。例如，光谱运算如快速傅里叶变换（FFT）在计算机视觉或数字信号处理应用中可能起重要作用。
- en: '[Table 2-9](#table_other_ops) lists some built-in operations for spectrum analysis
    and other mathematical operations.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-9](#table_other_ops)列出了一些用于频谱分析和其他数学运算的内置操作。'
- en: Table 2-9\. Spectral and other math operations
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-9. 光谱和其他数学运算
- en: '| Operation type | Sample functions |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 操作类型 | 示例函数 |'
- en: '| --- | --- |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Fast, inverse, and short-time Fourier transforms | `fft()`, `ifft()`, `stft()`
    |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 快速、逆、短时傅里叶变换 | `fft()`, `ifft()`, `stft()` |'
- en: '| Real-to-complex FFT and complex-to-real inverse FFT (IFFT) | `rfft()`, `irfft()`
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 实到复FFT和复到实逆FFT（IFFT） | `rfft()`, `irfft()` |'
- en: '| Windowing algorithms | `bartlett_window()`, `blackman_window()`, `hamming_window()`,
    `hann_window()` |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 窗口算法 | `bartlett_window()`, `blackman_window()`, `hamming_window()`, `hann_window()`
    |'
- en: '| Histogram and bin counts | `histc()`, `bincount()` |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 直方图和箱计数 | `histc()`, `bincount()` |'
- en: '| Cumulative operations | `cummax()`, `cummin()`, `cumprod()`, `cumsum()`,
    `trace()` (sum of the diagonal), `einsum()` (sum of products using Einstein summation)
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 累积操作 | `cummax()`, `cummin()`, `cumprod()`, `cumsum()`, `trace()`（对角线之和），`einsum()`（使用爱因斯坦求和的乘积之和）
    |'
- en: '| Normalization functions | `cdist()`, `renorm()` |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 标准化函数 | `cdist()`, `renorm()` |'
- en: '| Cross product, dot product, and Cartesian product | `cross()`, `tensordot()`,
    `cartesian_prod()` |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 叉积、点积和笛卡尔积 | `cross()`, `tensordot()`, `cartesian_prod()` |'
- en: '| Functions that create a diagonal tensor with elements of the input tensor
    | `diag()`, `diag_embed()`, `diag_flat()`, `diagonal()` |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 创建对角张量的函数，其元素为输入张量的元素 | `diag()`, `diag_embed()`, `diag_flat()`, `diagonal()`
    |'
- en: '| Einstein summation | `einsum()` |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 爱因斯坦求和 | `einsum()` |'
- en: '| Matrix reduction and restructuring functions | `flatten()`, `flip()`, `rot90()`,
    `repeat_interleave()`, `meshgrid()`, `roll()`, `combinations()` |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 矩阵降维和重构函数 | `flatten()`, `flip()`, `rot90()`, `repeat_interleave()`, `meshgrid()`,
    `roll()`, `combinations()` |'
- en: '| Functions that return the lower or upper triangles and their indices | `tril()`,
    `tril_indices`, `triu()`, `triu_indices()` |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 返回下三角形或上三角形及其索引的函数 | `tril()`, `tril_indices`, `triu()`, `triu_indices()`
    |'
- en: Automatic Differentiation (Autograd)
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动微分（Autograd）
- en: One function, `backward()`, is worth calling out in its own subsection because
    it’s what makes PyTorch so powerful for deep learning development. The `backward()`
    function uses PyTorch’s automatic differentiation package, `torch.autograd`, to
    differentiate and compute gradients of tensors based on the chain rule.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一个函数，`backward()`，值得在自己的子节中调用，因为它是PyTorch在深度学习开发中如此强大的原因。`backward()`函数使用PyTorch的自动微分包`torch.autograd`，根据链式法则对张量进行微分和计算梯度。
- en: 'Here’s a simple example of autodifferentiation. We define a function, *f* =
    sum(*x*²), where x is a matrix of variables. If we want to find *df* / *dx* for
    each variable in the matrix, we need to set the `requires_grad = True` flag for
    the tensor *x*, as shown in the following code:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自动微分的一个简单示例。我们定义一个函数，*f* = sum(*x*²)，其中x是一个变量矩阵。如果我们想要找到矩阵中每个变量的 *df* / *dx*，我们需要为张量*x*设置`requires_grad
    = True`标志，如下面的代码所示：
- en: '[PRE10]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `f.backward()` function performs the differentiation with respect to *f*
    and stores *df* / *dx* in the `x.grad` attribute. A quick review of calculus differential
    equations will tell us the derivation of *f* with respect to *x*, *df* / *dx*
    = 2*x*. The results of evaluating *df* / *dx* for the values of *x* are shown
    as the output.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`f.backward()`函数对*f*进行微分，并将*df* / *dx*存储在`x.grad`属性中。对微积分微分方程的快速回顾将告诉我们*f*相对于*x*的导数，*df*
    / *dx* = 2*x*。对*x*的值评估*df* / *dx*的结果显示为输出。'
- en: Note
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Only tensors of floating-point `dtype` can require gradients.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 只有浮点`dtype`的张量可以需要梯度。
- en: Training NNs requires us to compute the weight gradients on the backward pass.
    As our NNs get deeper and more complex, this feature automates the complex computations.
    For more information on how autograd works, see the [Autograd tutorial](https://pytorch.tips/autograd-explained).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络需要我们在反向传播中计算权重梯度。随着我们的神经网络变得更深更复杂，这个功能可以自动化复杂的计算。有关autograd工作原理的更多信息，请参阅[Autograd教程](https://pytorch.tips/autograd-explained)。
- en: This chapter provided a quick reference for creating tensors and performing
    operations. Now that you have a good foundation on tensors, we will focus on how
    to use tensors and PyTorch to perform deep learning research. In the next chapter,
    we will review the deep learning development process before jumping into writing
    code.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了一个快速参考，用于创建张量和执行操作。现在您已经对张量有了良好的基础，我们将重点讨论如何使用张量和PyTorch来进行深度学习研究。在下一章中，我们将回顾深度学习开发过程，然后开始编写代码。
