- en: Chapter 7\. Toward Trustworthy General-Purpose AI and Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The EU AI Act, which came into effect in August 2024, is the world’s first
    comprehensive legal framework for AI. Up to this point, this book has focused
    primarily on how the Act’s risk-based approach applies to predictive AI systems.
    However, the Act also includes provisions for *general-purpose AI* (GPAI)—a relatively
    late addition spurred by the rapid rise of large language models (LLMs) such as
    GPT and BERT, along with the broader emergence of generative AI technologies during
    the period when the legislation was being drafted. This addition was intended
    to address a key regulatory challenge: how to govern AI models that are not tied
    to a specific task and can be adapted to a wide range of downstream uses.'
  prefs: []
  type: TYPE_NORMAL
- en: The EU AI Act aims to balance innovation and risk mitigation by promoting research
    and development while safeguarding fundamental rights. As such, it does not apply
    to GPAI models developed and used solely for prototyping or for research and development
    purposes prior to being placed on the market or put into service. In general,
    the rules for GPAI models and systems are less stringent than those for high-risk
    AI systems—unless they are deemed to present a systemic risk (defined as “a risk
    that is specific to the high-impact capabilities of general-purpose AI models,
    having a significant impact on the Union market due to their reach, or due to
    actual or reasonably foreseeable negative effects on public health, safety, public
    security, fundamental rights, or the society as a whole, that can be propagated
    at scale across the value chain”).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter lays out the requirements for EU AI Act compliance for GPAI and
    generative AI. It also introduces the concept of GenAIOps—the application of MLOps
    principles to the development and deployment of generative AI applications. As
    in the previous chapters, the focus will be on how AI engineering principles and
    practices can be applied to meet the transparency obligations of the EU AI Act
    in this rapidly evolving domain.
  prefs: []
  type: TYPE_NORMAL
- en: The EU AI Act and Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The notion of “general-purpose” AI (also known as “foundation models”) was introduced
    into the EU AI Act relatively late in the negotiations between the European Parliament
    and Council. The rapid emergence and widespread adoption of generative AI systems
    like ChatGPT, DALL·E, and Midjourney during the legislative process revealed that
    the Act’s initial focus on AI systems with specific, identifiable risks stemming
    from clearly defined use cases was too narrow. General-purpose AI was added as
    a distinct category primarily to address concerns around quality control and copyright
    protection related to training data, while also allowing for the assessment of
    systemic risks, as elaborated in Chapter V of the Act. The regulations governing
    these models are relatively limited in scope, focusing mainly on requirements
    for documentation and transparency (Article 53) and obligations for cooperation
    with relevant authorities (Article 91).
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI, or GenAI, offers numerous potential benefits, including enhanced
    decision making, increased productivity, and the ability to generate novel content.
    GenAI technologies can be used in a wide range of domains, such as education,
    research, and customer service, to automate tasks and improve operational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: However, these technologies also pose significant risks. These include the potential
    for bias and discrimination, the generation of inaccurate or misleading information,
    and various form of misuse (e.g., creating deceptive content, or deepfakes). Misuse
    tends to fall into two broad categories:^([1](ch07.html#id616))
  prefs: []
  type: TYPE_NORMAL
- en: Exploitation of GenAI capabilities
  prefs: []
  type: TYPE_NORMAL
- en: Tactics that leverage the features of GenAI models to create harmful outputs
    or support malicious activities. For example, AI robocalls can imitate real people
    and take actions on their behalf, and synthetic content can be generated to create
    fake social media accounts to promote a specific agenda or to fabricate identification
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: Compromise of GenAI systems
  prefs: []
  type: TYPE_NORMAL
- en: Tactics that involve attacking or manipulating the GenAI systems, targeting
    model or data integrity vulnerabilities. Examples include prompt injection (manipulating
    model prompts to enable unintended or unauthorized outputs); compromising the
    privacy of training data to extract personal information; bypassing restrictions
    on the model’s safeguards (known as jailbreaking); and model extraction (reverse
    engineering to obtain details on the model’s architecture, parameters, or training
    data).
  prefs: []
  type: TYPE_NORMAL
- en: The EU AI Act aims to mitigate these risks by imposing certain obligations on
    the providers of generative and general-purpose AI systems to ensure that they
    are developed and deployed responsibly.
  prefs: []
  type: TYPE_NORMAL
- en: GPAI Systems and Transparency Obligations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GPAI systems that interact directly with humans, including those that generate
    synthetic content, are subject to the following transparency obligations under
    Article 50 of the EU AI Act:'
  prefs: []
  type: TYPE_NORMAL
- en: Informing users of AI interaction
  prefs: []
  type: TYPE_NORMAL
- en: Providers must inform users when they are interacting with an AI system, unless
    it’s reasonably obvious or the system is used for law enforcement purposes (e.g.,
    detecting, preventing, investigating, or prosecuting crimes). This applies to
    chatbots and content-generating tools.
  prefs: []
  type: TYPE_NORMAL
- en: Marking synthetic content
  prefs: []
  type: TYPE_NORMAL
- en: Providers of AI systems that generate synthetic content (audio, images, video,
    text) must clearly mark these outputs as artificially generated or manipulated.
    The labels should be machine-readable and easily detectable, signaling the content’s
    non-authentic nature. This requirement does not apply to assistive editing tools
    or systems that do not significantly alter the original input.
  prefs: []
  type: TYPE_NORMAL
- en: Disclosing deepfakes
  prefs: []
  type: TYPE_NORMAL
- en: Providers of AI systems that generate deepfakes (described by Article 3(60)
    as “AI-generated or manipulated image, audio or video content that resembles existing
    persons, objects, places, entities or events and would falsely appear to a person
    to be authentic or truthful”) must clearly label these outputs as artificial.
    The labels should be machine-readable and easily detectable.
  prefs: []
  type: TYPE_NORMAL
- en: Regulating General-Purpose AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The EU AI Act introduces specific regulations for GPAI models and systems, focusing
    on transparency, documentation, and risk management. In this section, I’ll first
    define what constitutes a GPAI model or system under the Act and explain the criteria
    for determining whether a GPAI model presents systemic risk. I will then outline
    the obligations for providers of GPAI models and deployers of GPAI systems, including
    the increased obligations for providers of GPAI models deemed to pose systemic
    risks.
  prefs: []
  type: TYPE_NORMAL
- en: Definition and scope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The EU AI Act defines a *GPAI model* as an AI model with significant generality
    that can perform a wide range of distinct tasks. These models are typically trained
    on large amounts of data, often using self-supervised learning techniques, and
    are designed to be integrated into various downstream systems or applications.
  prefs: []
  type: TYPE_NORMAL
- en: The Act defines a *GPAI system* as an AI system based on a GPAI model that can
    serve a variety of purposes, either as a standalone application or as a component
    integrated into other AI systems. As a reminder, the Act defines an AI system
    as “a machine-based system that is designed to operate with varying levels of
    autonomy and that may exhibit adaptiveness after deployment, and that, for explicit
    or implicit objectives, infers, from the input it receives, how to generate outputs
    such as predictions, content, recommendations, or decisions that can influence
    physical or virtual environments.”
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to distinguish between the model and the system it powers. The
    Act regulates both providers of GPAI models and deployers who use or integrate
    these models into AI systems. The obligations for deployers of GPAI systems are
    generally the same as for other AI systems, with specific oversight powers granted
    to the AI Office in the European Commission.
  prefs: []
  type: TYPE_NORMAL
- en: Systemic risk criteria
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The EU AI Act establishes a two-tiered regulatory approach for GPAI models:'
  prefs: []
  type: TYPE_NORMAL
- en: General obligations
  prefs: []
  type: TYPE_NORMAL
- en: All providers of GPAI models must meet baseline transparency and compatibility
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Systemic risk
  prefs: []
  type: TYPE_NORMAL
- en: If a GPAI model is classified as having systemic risk, it triggers additional
    regulatory oversight and extra obligations. The criteria for this classification
    are outlined in [Table 7-1](#chapter_7_table_1_1748539924522291).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. Criteria for systemic-risk GPAI models
  prefs: []
  type: TYPE_NORMAL
- en: '| Criterion | Description of systemic risk |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| High-impact capabilities | The model has significant potential to cause harm
    due to its broad range of functionalities. Risks include major accidents, disruption
    of critical infrastructure, threats to public health or safety, impacts on democratic
    processes, national or economic security risks, generation of illegal or discriminatory
    content, lowering barriers to development of chemical, biological, radiological,
    and nuclear (CBRN) weapons technology, or the ability of models to make copies
    of themselves or train other models autonomously. |'
  prefs: []
  type: TYPE_TB
- en: '| FLOPs threshold | The model requires more than 10^(25) floating-point operations
    per second (FLOPs) during training. This indicates a potential for high-impact
    capabilities. |'
  prefs: []
  type: TYPE_TB
- en: '| Commission decision based on equivalent capabilities or impact | Even if
    the FLOPs threshold is not met, the European Commission can classify a model as
    posing systemic risk if it demonstrates a potential for high-impact capabilities
    based on the assessment criteria in Annex XIII. |'
  prefs: []
  type: TYPE_TB
- en: '| Additional factors considered by the Commission | Additional factors that
    are taken into account include the number of parameters, quality and size of the
    training dataset (e.g., number of tokens), computational resources used for training
    (FLOPs, cost, time, energy consumption), input and output modalities, and degree
    of autonomy or scalability. |'
  prefs: []
  type: TYPE_TB
- en: '| Contesting the presumption of systemic risk | A provider can contest the
    presumption of systemic risk by presenting sufficiently substantiated arguments
    demonstrating that, even if the FLOPs criterion is met, the model does not actually
    present systemic risks due to its specific characteristics. General arguments
    are not sufficient. |'
  prefs: []
  type: TYPE_TB
- en: Obligations for providers of GPAI models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All providers of GPAI models, regardless of their systemic risk classification,
    are subject to a set of general obligations under the EU AI Act. These obligations
    are intended to promote transparency, accountability, and responsible development
    of powerful AI models. There are simplified requirements for providers offering
    GPAI models under free and open licenses, provided that these models do not present
    systemic risk. The core obligations (as outlined in [Article 53](https://oreil.ly/Xn8iw)) include:'
  prefs: []
  type: TYPE_NORMAL
- en: Technical documentation
  prefs: []
  type: TYPE_NORMAL
- en: Providers must create and maintain comprehensive technical documentation including
    details about the model’s design specifications, training and testing processes,
    and evaluation results. This documentation must enable regulatory authorities
    to assess the model’s compliance with the EU AI Act and must be provided to the
    AI Office and national competent authorities upon request.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation for downstream providers
  prefs: []
  type: TYPE_NORMAL
- en: Given that GPAI models are often integrated into other AI systems, the EU AI
    Act requires providers to furnish downstream providers with documentation that
    enables them to understand the model’s capabilities, limitations, and known risks.
    For example, if the model is known to exhibit bias in certain contexts, this must
    be disclosed so that downstream providers can take appropriate mitigation steps
    in their AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Copyright compliance
  prefs: []
  type: TYPE_NORMAL
- en: GPAI models are trained on vast amounts of data, often including copyrighted
    material. Consequently, providers must implement a policy to comply with EU copyright
    law (in particular, the [EU Copyright Directive](https://oreil.ly/6l3Hq)), including
    measures to identify and respect any copyright restrictions on the data used to
    train the model and documentation of how such restrictions have been addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Training data summary
  prefs: []
  type: TYPE_NORMAL
- en: To promote transparency and accountability, providers must publish a detailed
    summary of the data used to train their GPAI models. This should provide information
    about the types and sources of data used, as well as any known limitations or
    biases in the data.
  prefs: []
  type: TYPE_NORMAL
- en: EU representative
  prefs: []
  type: TYPE_NORMAL
- en: Providers based outside the European Union must appoint an authorized representative
    within the EU. This representative acts as a point of contact for authorities
    and is responsible for ensuring that the provider complies with its obligations
    under the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: Additional obligations for providers of GPAI models with systemic risk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GPAI models with systemic risk are subject to more stringent requirements due
    to their potential for causing significant harm. In addition to the general obligations
    outlined in the previous section, providers of these models must comply with the
    following obligations, specified in [Article 55](https://oreil.ly/2fYcc):'
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Conduct regular rigorous evaluations of the models using standardized protocols
    and tools. This includes adversarial testing to identify vulnerabilities and potential
    risks. The goal is to ensure that the model is robust, reliable, and does not
    pose unacceptable risks.
  prefs: []
  type: TYPE_NORMAL
- en: Risk mitigation
  prefs: []
  type: TYPE_NORMAL
- en: Proactively assess and mitigate potential systemic risks associated with use
    of the models. Identify potential harms related to fundamental rights, health
    and safety, or security, and implement documented measures to minimize those risks.
  prefs: []
  type: TYPE_NORMAL
- en: Incident reporting
  prefs: []
  type: TYPE_NORMAL
- en: Establish a system for tracking, documenting, and reporting serious incidents
    related to use of the models. Incidents must be reported to the AI Office and
    relevant national authorities, and appropriate corrective actions must be taken
    and documented.
  prefs: []
  type: TYPE_NORMAL
- en: Cybersecurity
  prefs: []
  type: TYPE_NORMAL
- en: Implement robust safeguards to protect the models and the infrastructure on
    which they operate. This is crucial to prevent unauthorized access, data breaches,
    and malicious use.
  prefs: []
  type: TYPE_NORMAL
- en: Obligations for deployers of GPAI models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The EU AI Act applies to all deployers who operate an AI system under their
    own authority in a professional capacity. When deployers integrate GPAI models
    into their AI systems, they may incur additional responsibilities under the Act,
    particularly if the resulting systems are classified as high risk. These obligations
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the system as intended
  prefs: []
  type: TYPE_NORMAL
- en: The system must be used in accordance with the provider’s instructions and for
    its intended purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Human oversight
  prefs: []
  type: TYPE_NORMAL
- en: Appropriate human oversight mechanisms must be established—for example, human
    review of the system’s outputs, the ability to intervene in the system’s operation,
    and clear assignment of responsibility for the system’s decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality
  prefs: []
  type: TYPE_NORMAL
- en: The input data provided to the system must be relevant, accurate, and representative.
    This is crucial to prevent biased or inaccurate outputs and to ensure that the
    system operates as intended.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs: []
  type: TYPE_NORMAL
- en: System performance must be monitored and any issues reported to the provider.
    This includes monitoring for unexpected outputs, performance degradation, or other
    indications that the system is not functioning correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Recordkeeping
  prefs: []
  type: TYPE_NORMAL
- en: System logs must be maintained for the purposes of incident investigation and
    demonstration of regulatory compliance.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency
  prefs: []
  type: TYPE_NORMAL
- en: AI systems that generate or manipulate content that is published to inform the
    public on matters of public interest must clearly disclose that the content is
    synthetic.
  prefs: []
  type: TYPE_NORMAL
- en: Predictive ML Versus GPAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we turn our attention to operationalizing EU AI Act compliance for GPAI,
    let’s briefly reflect on the differences between predictive and generative AI
    to better understand where generative AI diverges and why the discipline of GenAIOps
    has emerged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning is a broad field, but at its core, most models can be categorized
    into two distinct paradigms: predictive ML and GPAI (see Figures [7-1](#chapter_7_figure_1_1748539924514309)
    and [7-2](#chapter_7_figure_2_1748539924514347) for concrete examples of each).
    While both rely on statistical learning, they differ in their goals, methodologies,
    and applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. A visualization of predictive ML—a discriminative model trained
    to predict whether a given image is painted by Vincent van Gogh. Image from the
    book [Generative Deep Learning, 2nd edition](https://oreil.ly/L5njI), by David
    Foster (O’Reilly). Used with permission.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/taie_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. A visualization of generative AI—a generative model trained to
    generate realistic photos of horses. Image from the book [Generative Deep Learning,
    2nd edition](https://oreil.ly/L5njI), by David Foster (O’Reilly). Used with permission.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At its core, predictive ML is about answering the question “What’s likely?”
    It focuses on estimation, forecasting, and pattern recognition. Predictive models
    analyze historical data to make informed forecasts or decisions. They learn patterns
    from structured datasets and aim to provide numerical outputs, classifications,
    or recommendations on input features.
  prefs: []
  type: TYPE_NORMAL
- en: GPAI, on the other hand, is an umbrella category that includes many generative
    models—particularly foundation models such as LLMs. Generative AI can therefore
    be thought of as a subset of GPAI. Most state-of-the-art generative models (such
    as GPT-4, DALL·E, and Stable Diffusion) are classified as GPAI under the EU AI
    Act. This includes models that are general-purpose (used across many tasks or
    systems) and models that are made available to others for downstream use or fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, generative AI is about answering the question “What’s possible?”
    It moves beyond predictions to creating new content. Instead of mapping inputs
    to predefined outputs, generative models learn the statistical distribution of
    their training data and generate new, plausible data points that fall within that
    learned distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 7-2](#chapter_7_table_2_1748539924522329) provides a structured comparison
    of predictive ML and GPAI.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-2\. Predictive ML and GPAI—a side-by-side comparison
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | Predictive ML | GPAI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Primary goal | Solve a well-defined, narrow task (e.g., fraud detection,
    loan scoring) | Provide broad capabilities that can be adapted to many tasks and
    use cases |'
  prefs: []
  type: TYPE_TB
- en: '| Data input | Domain-specific structured or unstructured data (e.g., tabular
    data, images) | Massive and diverse cross-domain datasets (e.g., internet text,
    code, audio, images) |'
  prefs: []
  type: TYPE_TB
- en: '| Output | Prediction, classification, regression, or recommendation for a
    specific task | Versatile outputs: text, code, reasoning, vision tasks, language
    understanding, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Downstream use | Purpose-built systems with known users and applications
    | Can be adapted and fine-tuned by third parties for diverse and evolving applications
    |'
  prefs: []
  type: TYPE_TB
- en: '| Examples | Predictive maintenance models, fraud detection systems | GPT-4,
    Gemini, LLaMA, Mistral, DALL·E |'
  prefs: []
  type: TYPE_TB
- en: '| Algorithms | Decision trees, XGBoost, CNNs, RNNs | Large-scale Transformer-based
    architectures, foundation models (e.g., LLMs, vision language models) |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation | Task-specific metrics: accuracy, F1 score, AUC-ROC, MAE/RMSE
    | Evaluated across a wide range of tasks using general benchmarks (e.g., MMLU,
    HELM), robustness, bias, toxicity |'
  prefs: []
  type: TYPE_TB
- en: GenAIOps—Operationalizing EU AI Act Compliance for GPAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GenAIOps is the systematic extension of DevOps and MLOps principles to meet
    the unique challenges of developing, deploying, and maintaining GPAI models and
    systems, including LLMs, image and video generators, and other foundation models.
    While traditional MLOps focuses on managing predictive models, GenAIOps must support
    the adaptation and governance of powerful generative models that may be used for
    a wide range of downstream tasks. This includes practices such as prompt engineering,
    retrieval-augmented generation (RAG), foundation model fine-tuning, and real-time
    monitoring for issues like hallucinations, bias, and safety risks. These needs
    introduce additional complexity in areas ranging from data governance to infrastructure,
    safety monitoring, and responsible use—especially in the context of compliance
    with regulatory frameworks like the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-3](#chapter_7_figure_3_1748539924514373) visualizes the operational
    stages for GPAI models and how they map to the CRISP-ML(Q) development phases.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. CRISP-ML(Q) framework for GPAI models and systems
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Key Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GenAIOps integrates automated pipelines, model versioning, and observability
    with inference optimization, safety guardrails, and compliance workflows to create
    a comprehensive framework for responsible GPAI deployment. This approach ensures
    that GPAI models operate with transparency and auditability in production environments
    while addressing concerns about misinformation, synthetic content attribution,
    and compliance with applicable regulatory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key components of GenAIOps include the following (summarized in [Figure 7-4](#chapter_7_figure_4_1748539924514393)):'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: Tools for managing prompt creation, testing, versioning, and orchestration,
    in particular for complex reasoning chains and RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: Inference optimization
  prefs: []
  type: TYPE_NORMAL
- en: Techniques such as quantization, caching, and model distillation to reduce latency,
    cost, and resource usage in deployment environments.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Real-time monitoring systems to detect hallucinations and performance drift,
    and mechanisms to collect human feedback for ongoing model refinement.
  prefs: []
  type: TYPE_NORMAL
- en: Safety and content controls
  prefs: []
  type: TYPE_NORMAL
- en: Automated content filtering, watermarking, red-teaming protocols, and risk assessment
    workflows to ensure responsible and secure AI outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Governance and compliance
  prefs: []
  type: TYPE_NORMAL
- en: Transparent documentation (e.g., model cards), content provenance systems, and
    comprehensive audit trails to meet evolving regulatory requirements and ethical
    standards.
  prefs: []
  type: TYPE_NORMAL
- en: Foundation model adaptation
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning workflows, reinforcement learning from human feedback (RLHF), and
    parameter-efficient methods (e.g., LoRA) to customize models for specific downstream
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Key components of GenAIOps
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How GenAIOps Extends MLOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core goals of MLOps, which is mainly focused on the lifecycle of predictive
    ML models, are reproducibility, scalability, and continuous delivery. GenAIOps
    retains these goals but adapts them to the specific needs of GPAI. For example,
    generative AI applications must operate on unstructured data at scale, incorporate
    prompt engineering and human-in-the-loop feedback mechanisms, and ensure that
    outputs are not only high quality but also safe, transparent, and appropriately
    labeled. [Table 7-3](#chapter_7_table_3_1748539924522354) outlines key differences
    between MLOps and GenAIOps across several areas, such as data management, model
    training and adaptation, model deployment and inference, monitoring, and ethics
    and regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-3\. Key differences between MLOps and GenAIOps
  prefs: []
  type: TYPE_NORMAL
- en: '| Aspect | MLOps (predictive ML) | GenAIOps (GPAI) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data management | Focuses on curated, labeled datasets, feature engineering,
    and versioning of training data pipelines. Data is often structured and tabular
    or limited to task-specific corpora. | Handles massive, unstructured datasets
    (text, images, audio). Embraces synthetic data generation for augmentation and
    requires robust data filtering to remove toxic or biased content from training
    corpora (to avoid amplifying harms). Uses embedding management (vector representations
    of data) in place of traditional feature stores to enable RAG, typically via vector
    databases. |'
  prefs: []
  type: TYPE_TB
- en: '| Model training and adaptation | Models are trained from scratch or with transfer
    learning, with hyperparameter tuning and model selection based on task-specific
    data. Models are typically smaller and trained to converge on labeled data. |
    Starts from large, pretrained foundation models. Uses parameter-efficient fine-tuning
    (e.g., LoRA, adapter layers), prompt engineering, and few-shot learning instead
    of full retraining. Techniques like direct preference optimization (DPO) and RLHF
    are applied to align model behavior with human preferences. Experiment tracking
    must include prompt versions and chain configurations, not just model parameters.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Model deployment and inference | Models are deployed as microservices or
    batch jobs behind REST APIs, using standard scaling, CI/CD, and version control.
    Inference usually involves a single-model prediction call (e.g., a classification
    API). | Deployment requires specialized infrastructure (GPUs or TPUs, high memory
    usage). Focuses on real-time inference for generative outputs, often with streaming
    responses due to longer outputs (e.g., for chatbots). Frequently involves orchestrating
    multiple models and tools and managing prompt pipelines and session context. Optimization
    techniques such as model quantization, caching of outputs, and load balancing
    are applied to manage heavy inference loads. |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring and continuous improvement | Model performance (accuracy, latency)
    and data drift are monitored in production. User feedback may trigger periodic
    retraining or model updates. Rollback mechanisms minimize downtime in case of
    failures. When using third-party models, custom evaluation sets may be needed
    to detect behavioral changes between versions. | Output quality and safety risks
    (hallucinations, toxicity, bias) are monitored, in addition to standard performance
    metrics. Requires continuous evaluation loops and human feedback to guide improvements.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Regulatory and ethical considerations | Focuses on fairness, bias mitigation,
    explainability, and compliance in decision-making contexts (e.g., loan approvals).
    Models are accompanied by comprehensive documentation (e.g., model cards, bias
    audits). | Faces new ethical and regulatory challenges: GPAI models can produce
    misinformation, harmful content, and deepfakes and can potentially be used for
    impersonation. Must support emerging requirements, such as labeling AI-generated
    content and ensuring transparency and traceability. |'
  prefs: []
  type: TYPE_TB
- en: GenAIOps Tools, Workflows, and Frameworks to Support Transparency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GenAIOps builds on MLOps by adding capabilities like prompt management, chain-of-thought
    orchestration, synthetic data generation, and stringent output controls to address
    the unique demands of GPAI. It’s also essential for ensuring transparency in GPAI
    models and systems, as mandated by regulations like the EU AI Act. The transparency
    obligations outlined in Article 50 of the Act, which we discussed in detail in
    [Chapter 6](ch06.html#chapter_6_ai_engineering_for_limited_risk_ai_systems_1748539923606988),
    apply to both providers and deployers of generative AI systems (as well as predictive
    ML systems). Meeting these obligations and effectively managing GenAI-related
    risks requires cross-disciplinary collaboration among data engineers, model developers,
    user experience designers, and ethics and compliance teams. The following GenAIOps
    tools and workflows can support this effort:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt and chain management
  prefs: []
  type: TYPE_NORMAL
- en: Frameworks like LangChain and NVIDIA NeMo can be used to orchestrate complex
    prompt workflows and ensure that required system prompts (including AI self-identification)
    are consistently included in user interactions. These tools also support prompt
    versioning, testing, and traceability.
  prefs: []
  type: TYPE_NORMAL
- en: Content watermarking and metadata
  prefs: []
  type: TYPE_NORMAL
- en: Libraries and APIs compliant with the C2PA (Content Provenance and Authenticity)
    standard are increasingly being integrated into image and video generation pipelines.
    For example, OpenAI’s DALL·E API [automatically adds C2PA metadata](https://oreil.ly/djcGb)
    to its outputs. There are also third-party services, such as Stability’s Stable
    Diffusion plug-ins or [Steg.AI](https://oreil.ly/PGrpq), that insert invisible
    watermarks into images for later verification. GenAIOps pipelines can call these
    services post-generation. For text-based content, research into statistical watermarking
    is also advancing.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and detection services
  prefs: []
  type: TYPE_NORMAL
- en: To complement watermarks, GenAIOps can integratedeepfake detection models and
    content moderation AI into the monitoring stack. For instance, an enterprise might
    deploy a vision model that scans newly uploaded videos for signs of manipulation.
    If AI-generated media lacks the required disclosure, the detection system can
    flag it for review. Cloud providers are beginning to offer APIs for this purpose
    (e.g., Azure’s Video Authenticator and AWS’s Rekognition).
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails and policy enforcement
  prefs: []
  type: TYPE_NORMAL
- en: Guardrail frameworks such as NVIDIA NeMo Guardrails or the open source Guardrails
    AI library can be used to enforce policy at runtime by intercepting model outputs
    before they reach the user. They can automatically append disclaimers or block
    content that should be labeled as synthetic but isn’t. For example, if a user
    prompts a GPAI model to generate a fake image of a person, a guardrail can attach
    a “Fake Image” label to the output. Similarly, an audible cue can be injected
    into audio content. These enforcement steps are configured directly within the
    inference pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Logging and versioning
  prefs: []
  type: TYPE_NORMAL
- en: Prompts, responses, and content artifacts can be comprehensively logged using
    platforms like MLflow, Weights & Biases, or Arize AI that support GenAIOps features.
    These logs provide a robust audit trail that can verify, for example, whether
    a watermarking function ran and what output it produced.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this chapter, we’ll explore how the SMACTR framework introduced
    in the previous chapter can be integrated into the different phases of the CRISP-ML(Q)
    lifecycle, providing a practical foundation for operationalizing compliance in
    the development and deployment of GPAI models and systems.
  prefs: []
  type: TYPE_NORMAL
- en: Aligning AI Engineering with SMACTR and CRISP-ML(Q) for Transparency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 6](ch06.html#chapter_6_ai_engineering_for_limited_risk_ai_systems_1748539923606988)
    provided a detailed guide for AI engineers on achieving proactive compliance with
    the transparency requirements outlined in Article 50 of the EU AI Act. This section
    applies the same approach to GPAI models and systems, incorporating GenAIOps principles.
    By integrating the SMACTR framework with the CRISP-ML(Q) methodology, teams can
    establish a robust, auditable process for responsible development and deployment
    of general-purpose AI.'
  prefs: []
  type: TYPE_NORMAL
- en: Business and Data Understanding Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GenAIOps is generally broken down into three main operational phases (see [Figure 7-3](#chapter_7_figure_3_1748539924514373)).
    The initial phase encompasses tasks related to use case definition and data requirements
    planning that precede full-scale data engineering. It also includes early data
    engineering activities such as clarifying the application’s purpose and identifying
    data sources. For example, before building an LLM-powered system, teams must define
    the business problem (e.g., “customer support chatbot” versus “code generation
    assistant”) and assess what data and model capabilities are needed to solve it
    efficiently. This aligns closely with the business and data understanding phase
    of the CRISP-ML(Q) methodology.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is translating business goals into ML/LLM objectives—that is,
    determining what you want the model to do, and what data or base models that will
    require. For instance, if the goal is to develop a Q&A chatbot, GenAIOps teams
    must decide whether to use an existing Q&A model or fine-tune a base LLM, and
    identify the relevant domain data. This reflects CRISP-ML(Q)’s emphasis that business
    objectives and data constraints should be considered jointly to avoid building
    “the right answers to the wrong questions.”
  prefs: []
  type: TYPE_NORMAL
- en: Early alignment on objectives, success criteria, and constraints shapes all
    subsequent work. CRISP-ML(Q) calls for success metrics to be defined at the business,
    ML, and system levels. GenAIOps expands this by requiring that user experience
    goals and ethical considerations also be addressed from the start—including expectations
    for model behavior, tone, and safety.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Integrating CRISP-ML(Q) practices into GenAIOps at this stage means rigorously
    documenting requirements, constraints, and success criteria. In practice, the
    engineering team should convene domain experts, data scientists, and LLM engineers
    to define the scope of the GenAI application before development begins. The success
    criteria will go beyond classic metrics like accuracy or ROI to include measures
    of output quality (e.g., fluency, relevance) and safety (e.g., absence of harmful
    or biased content). An early feasibility assessment is crucial; if a required
    dataset is unavailable or the task is ill-suited to LLMs, that should be determined
    up front.
  prefs: []
  type: TYPE_NORMAL
- en: In GenAIOps, this phase often involves selecting an initial base model appropriate
    to the task. For example, the choice between a proprietary LLM like GPT-4 or a
    smaller open source model will depend on business needs and constraints such as
    cost, latency, and data privacy. Similar to algorithm selection in predictive
    ML projects, base model selection in GenAI is a key decision that should be guided
    by the problem scope and data requirements. For example, if the application involves
    sensitive or domain-specific content, a smaller fine-tuned model trained on proprietary
    data might be preferred over a general-purpose LLM. Documenting the decision and
    its rationale will help maintain clarity and transparency as the project progresses.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Scoping and Mapping stages from the SMACTR framework are most relevant in
    this initial phase.
  prefs: []
  type: TYPE_NORMAL
- en: In the Scoping stage, the team conducts a preliminary risk assessment and an
    ethical review of the intended use cases. For a GenAI project, this involves asking
    how the application might affect users or stakeholders (e.g., “Could the chatbot
    give harmful advice or leak private data?”). Defining the ethical AI principles
    the project will follow is also part of this stage. Key outputs, such as a social
    impact assessment, help ensure the application’s design considers and takes steps
    to mitigate potential harms and aligns with the organization’s values and AI ethics
    guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the Mapping stage, the focus shifts to identifying stakeholders and collaborators.
    In a GenAIOps context, this means mapping who needs to be involved—for example,
    data owners for access to data sources, compliance or legal experts if user data
    or intellectual property is involved, and end-user representatives. This stakeholder
    map promotes accountability and traceability by clarifying who is responsible
    for which inputs and decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating Scoping and Mapping into the business and data understanding phase
    helps teams working on GenAI projects mitigate strategic risks before any data
    is collected or model work begins, reducing the chances of building a misaligned
    or unsafe product.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 7-4](#chapter_7_table_4_1748539924522377) provides an outline of key
    artifacts that should be produced during this phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-4\. Summary of artifacts produced during the business and data understanding
    phase
  prefs: []
  type: TYPE_NORMAL
- en: '| SMACTR stage | Key artifacts |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Scoping |'
  prefs: []
  type: TYPE_TB
- en: Preliminary risk assessment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical review of intended use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Definition of ethical AI principles for the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statement of alignment with corporate values and AI ethics guidelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Mapping |'
  prefs: []
  type: TYPE_TB
- en: Identification of stakeholders and collaborators (e.g., data owners, compliance/legal
    experts, end users)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stakeholder map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CRISP-ML(Q)’s data preparation phase, in a GenAI context, includes steps such
    as gathering large-scale text data, cleaning and filtering it, and preparing it
    for model input (for example, performing tokenization and formatting). If the
    GenAI solution involves fine-tuning an LLM or training a domain-specific model,
    this phase will incorporate assembling the fine-tuning dataset and any prompt
    templates. It may also involve setting up a knowledge base for retrieval-augmented
    generation, which is a unique data component in GenAIOps not typically present
    in traditional ML pipelines. The goal of the data preparation phase remains the
    same—to ensure that the data fed into the model is high quality, relevant, and
    appropriately prepared—whether it’s tabular data for classic ML or unstructured
    text and prompt data for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data quality directly affects model success. CRISP-ML(Q) emphasizes robust
    data handling by selecting the right data, cleaning it to remove noise, normalizing
    it, engineering features, and standardizing formats. GenAIOps extends MLOps to
    address the unique challenges of working with GenAI models. One major difference
    is that traditional feature engineering is unnecessary for LLMs. Instead, GenAIOps
    focuses on prompt engineering and data curation—for example, crafting prompt examples
    or structuring input/output pairs to teach the model a task. Another key difference
    is scale and diversity: GenAI applications often require large-scale data collection
    with an emphasis on diversity and representativeness to avoid bias or blind spots.
    If labeling is needed for fine-tuning or RLHF data, GenAIOps might use semi-automated
    techniques such as pre-labeling with another model or active learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data preparation involves implementing systematic data quality checks and creating
    thorough documentation. In practice, the engineering team should treat the text
    corpora or prompt datasets with the same discipline as curated ML datasets. Key
    activities include:'
  prefs: []
  type: TYPE_NORMAL
- en: Data selection and sourcing
  prefs: []
  type: TYPE_NORMAL
- en: Identify and collect data from multiple sources (e.g., internal documents, public
    datasets, web-scraped text) relevant to the use case. GenAIOps emphasizes automating
    this process and continuously ingesting new data if the application requires up-to-date
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning
  prefs: []
  type: TYPE_NORMAL
- en: Filter out problematic content, such as profanity, personally identifiable information,
    or offensive text, to prevent the model from learning undesirable patterns. Both
    MLOps and GenAIOps stress the importance of dataset heterogeneity to reduce bias
    and overfitting. Tools like [Data-Juicer](https://oreil.ly/5wr6H) can help with
    auditing dataset diversity.
  prefs: []
  type: TYPE_NORMAL
- en: Data structuring
  prefs: []
  type: TYPE_NORMAL
- en: In traditional ML, this involves tasks like normalizing values or formatting
    comma-separated values. In GenAI, it includes tokenizing raw text, splitting documents,
    and adding prompt prefixes or suffixes. It can also involve prompt engineering
    at the dataset level—that is, constructing input/output pairs that demonstrate
    the task for fine-tuning or evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs: []
  type: TYPE_NORMAL
- en: GenAI data prep should account for dynamic data, meaning that if the application
    will retrieve documents at runtime (RAG), the team will need to set up an indexed
    vector database and pipelines to update it with new content. This is an extra
    operational consideration in GenAIOps to keep a knowledge base fresh.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  prefs: []
  type: TYPE_NORMAL
- en: For GenAIOps, recording details like data sources, preprocessing steps, and
    known limitations of the dataset is crucial. This supports transparency (e.g.,
    via datasheets for datasets) and helps ensure auditability if issues arise later.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Artifact Collection stage of SMACTR is particularly relevant during data
    preparation. As the team curates the dataset, they should also compile audit artifacts,
    including a comprehensive checklist to verify that all required documentation
    is in place and datasheets that capture data lineage, assumptions, etc. This implements
    quality assurance by design. Before modeling starts, auditors or internal QA reviewers
    should confirm that the dataset meets defined standards and that any potential
    biases or data limitations have been logged. For GenAI projects, this step helps
    surface issues like skewed representation in training data (say, underrepresentation
    of certain user demographics in a chatbot’s training data) so they can be addressed
    (e.g., by augmenting the dataset with more representative samples).
  prefs: []
  type: TYPE_NORMAL
- en: The insights from the Scoping stage also feed into data preparation. For instance,
    if privacy risks were identified, the data pipeline should incorporate appropriate
    mitigation steps such as data minimization or anonymization.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the team should begin planning for how data-related risks will be handled
    in testing. SMACTR’s Testing stage often draws on documented issues and failures,
    so any concerns that are identified during the data preparation phase should be
    noted (e.g., “Our dataset might not cover slang—possible failure mode for the
    chatbot”).
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 7-5](#chapter_7_table_5_1748539924522398) summarizes the key artifacts
    that should be produced during the data preparation phase. By treating data as
    an auditable artifact, supported by checklists, datasheets, and bias analysis,
    GenAIOps can incorporate SMACTR’s accountability early in the pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-5\. Summary of key artifacts produced during the data preparation phase
  prefs: []
  type: TYPE_NORMAL
- en: '| SMACTR stage | Output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Artifact Collection |'
  prefs: []
  type: TYPE_TB
- en: Datasheets (data documentation) detailing data lineage, assumptions, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log of identified biases or limitations in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scoping |'
  prefs: []
  type: TYPE_TB
- en: Documentation of privacy risks that might require additional data privacy controls
    during preparation or training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Testing (planning during data prep) |'
  prefs: []
  type: TYPE_TB
- en: Notes on potential data-related risks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preliminary plans for how these risks will be handled in testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In GenAIOps, during the modeling phase the base model is chosen or built and
    adapted to the task. The base model selection and domain adaptation steps include
    prompt engineering, fine-tuning, or RAG. For a traditional ML project, modeling
    entails selecting algorithms, tuning hyperparameters, and training the model from
    scratch on prepared data. In a GenAI project, you’re usually starting with a preexisting
    model, such as a large pretrained transformer, so “modeling” is more about *adapting*
    that model: choosing which LLM to use as a starting point, deciding whether to
    fine-tune it or use it via prompting, and implementing those adaptations. For
    example, if building a custom chatbot, the team might take a base model like GPT,
    fine-tune it on their domain data, and craft a prompt strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) underscores modeling practices such as ensuring reproducibility
    of experiments, trying multiple modeling techniques, and aligning the model choice
    with business objectives. In GenAI application development, the choice of model
    and the corresponding prompt or fine-tuning strategy must align with the use case
    constraints (latency, accuracy, etc.), and experiments should be tracked. A key
    difference in generative modeling is the iterative experimentation with prompts.
    In structured ML, once you choose an algorithm and features, training is relatively
    straightforward. But with LLMs, achieving the desired output often requires interactive
    prompt tuning and tweaking parameters such as temperature settings. This means
    the modeling phase in GenAIOps can be highly iterative and exploratory, often
    interleaving with evaluation. You might prompt the model, observe its output,
    adjust the prompt or use in-context few-shot examples, and repeat. Still, the
    underlying focus remains on optimizing the model’s behavior to meet predefined
    success criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another difference from traditional ML is the scale and tooling involved: training
    a classical ML model might involve a scikit-learn pipeline or custom code, whereas
    in GenAIOps, you might leverage specialized frameworks such as Transformers and
    parameter-efficient fine-tuning libraries and utilize distributed training or
    serving if fine-tuning a large model. In addition, the complexity of LLMs means
    that experiment tracking and versioning are even more crucial. Every prompt template
    or fine-tuned checkpoint is a variant that should be carefully managed (to avoid
    the “pipeline jungle” problem).'
  prefs: []
  type: TYPE_NORMAL
- en: GenAIOps treats the model as an artifact to configure (select and tune) rather
    than invent from scratch, emphasizing configuration management and performance
    optimization. For example, model compression and GPU optimization are part of
    “modeling” in GenAIOps.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bringing CRISP-ML(Q)’s discipline into GenAI application development means
    adopting systematic experimentation and robust model management practices that
    support compliance with regulations such as the EU AI Act. Practically, this involves
    several key activities:'
  prefs: []
  type: TYPE_NORMAL
- en: Model selection and establishing a baseline
  prefs: []
  type: TYPE_NORMAL
- en: Just as a data scientist might test multiple algorithms, a GenAIOps team should
    evaluate different base LLMs—such as GPT-5, Mistral, or LLaMA-2—on a representative
    sample of tasks. The goal is to select the model that best balances performance
    with business and technical constraints, guided by the success criteria defined
    during the initial phase. The CRISP-ML(Q) approach emphasizes defining baseline
    model performance. In GenAI, that could mean using an off-the-shelf model to generate
    some outputs and evaluating those before customization.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt and fine-tuning experimentation
  prefs: []
  type: TYPE_NORMAL
- en: If fine-tuning is planned, each training run should be treated as an experiment,
    with proper versioning of training data, code, and resulting model checkpoints.
    Similarly, prompt designs—like few-shot or chain-of-thought designs—should be
    versioned and documented. This will ensure that results can be traced to the exact
    models and configurations that produced them. Tools like Weights & Biases or MLflow
    can be integrated into the GenAIOps pipeline to track prompt parameters and model
    versions.
  prefs: []
  type: TYPE_NORMAL
- en: Quality assurance techniques
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) recommends integrating specific quality assurance practices during
    modeling, such as reproducibility checks and model stability assessments. For
    GenAI, this might include unit tests on prompts to verify that a fixed set of
    test inputs consistently produces the expected output formats, or static analysis
    to verify that fine-tuning has not degraded key model capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing risks in model design
  prefs: []
  type: TYPE_NORMAL
- en: If earlier phases flagged certain risks, bias mitigation techniques should be
    applied—for example, embedding alignment, incorporating moderation models into
    the generation pipeline, or choosing a smaller, more controllable model over a
    larger, less interpretable one. In addition, GenAIOps often involves adding guardrails
    at the model interface—such as rules-based filters or rejection sampling—to ensure
    that outputs meet quality and compliance requirements. This aligns with CRISP-ML(Q)’s
    focus on mitigating risks during development.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout these steps, maintaining a “transparency trail” is encouraged by
    SMACTR. This means documenting design decisions such as why a specific model or
    prompt strategy was chosen, how configurations were set, and any known trade-offs.
    For example, you might want to create early drafts of model cards capturing the
    intended use, architecture, and limitations of the model.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the modeling phase, the engineering team should have not only
    a tuned model and pipeline ready for evaluation but also a comprehensive set of
    documentation and artifacts, guaranteeing that the modeling process is reproducible
    and well understood.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During the modeling phase, the key SMACTR stages to focus on are Artifact Collection
    and Testing (planning). The artifacts should include a design history file or
    similar documentation detailing the modeling decisions. This documentation might
    contain architecture diagrams (especially if the application includes multiple
    components, like an LLM and a vector database), records of hyperparameters or
    prompt scripts, and any ethical considerations incorporated into the model design
    (for instance, “we decided not to fine-tune on user chat logs containing sensitive
    data to preserve privacy”).
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR recommends using an audit checklist to verify that essential model documentation,
    such as model cards and associated data documentation, is complete and accessible.
    By the end of the modeling phase, these artifacts should be largely finalized.
  prefs: []
  type: TYPE_NORMAL
- en: Although the Testing stage formally begins during the next phase (evaluation),
    preparation for testing should start during modeling. The team should prioritize
    which risks to test based on earlier risk mapping. For example, if a failure mode
    of concern is the model generating toxic language, the modeling phase might incorporate
    a toxicity classification head or plan for adversarial prompt testing. Essentially,
    SMACTR encourages baking in testability by ensuring the model includes hooks or
    supporting tools to facilitate the intensive testing to come.
  prefs: []
  type: TYPE_NORMAL
- en: To enforce accountability and ownership of quality, individual engineers or
    researchers should be assigned responsibility for various model components or
    experiments, tracing who fine-tuned which version, who designed which prompt set,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: By integrating SMACTR into the modeling phase, the GenAI team treats the model
    not as a black box but as a transparently developed component ready for rigorous
    audit and testing. [Table 7-6](#chapter_7_table_6_1748539924522418) summarizes
    the core artifacts produced during this phase.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-6\. Summary of key artifacts produced during the modeling phase
  prefs: []
  type: TYPE_NORMAL
- en: '| SMACTR stage | Output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Artifact Collection |'
  prefs: []
  type: TYPE_TB
- en: Design history file or documentation of modeling decisions (architecture diagrams,
    hyperparameters, prompt scripts, ethical considerations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model documentation (model cards)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Near-complete data documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Testing (planning during modeling) |'
  prefs: []
  type: TYPE_TB
- en: Prioritized risks to test based on earlier risk mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration of testability features (e.g., toxicity classification head, adversarial
    prompt testing plans)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plans for intensive testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Mapping |'
  prefs: []
  type: TYPE_TB
- en: Identification of individuals responsible for model components or experiments
    (accountability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This phase aligns directly with the model evaluation stage in the GenAIOps lifecycle.
    After the model or prompt is developed, a thorough assessment of its performance
    against the previously defined success criteria should be conducted. In GenAIOps,
    evaluation is an ongoing, multifaceted process. It includes quantitative evaluation
    where applicable (e.g., accuracy on a benchmark or BLEU score for a translation
    task) as well as qualitative evaluation such as human judgment of output quality,
    user feedback loops, etc. It may also extend to assessing system behavior through
    techniques like red teaming or probing the model with adversarial or unusual inputs.
    In practice, the evaluation phase for GenAI often overlaps with deployment, involving
    shadow deployments or A/B testing with real users. Here, we’ll focus on the core
    pre-deployment evaluation steps.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) frames evaluation as verifying the model’s fitness for purpose by
    assessing its ability to meet the defined success criteria. Similarly, GenAIOps
    must determine whether the GenAI application meets business needs, quality standards,
    and thresholds for technical metrics. Both approaches involve testing on holdout
    datasets or scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional ML evaluation typically uses well-defined metrics like accuracy,
    precision, root mean square error (RMSE), and static test datasets. GenAI evaluation
    is more complex, because generative outputs can’t be easily captured by a single
    scalar metric. Evaluation in GenAI remains an unsolved problem. For example, given
    the same inputs, the behavior of the model might change over time, and maintaining
    consistent outputs may require prompt adjustments. GenAI evaluation often involves
    human rating of outputs for correctness or preference, adversarial testing to
    expose failure modes or undesirable behavior, and specialized generative quality
    metrics such as BLEU or ROUGE scores or perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'GenAI evaluation must also consider ethical and safety dimensions: a model
    might have great accuracy but fail due to biased or toxic outputs. Therefore,
    fairness and safety audits must be incorporated. In CRISP-ML(Q), evaluation is
    iterative; if the model fails to meet the defined criteria, you return to modeling
    or even data prep. But in GenAIOps, evaluation is often continuous due to nondeterministic
    outputs and evolving usage. Techniques like “golden sets”—carefully curated test
    prompts with expected answers for regression testing—are becoming standard.'
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) demands rigorous validation against objectives. GenAI expands the
    scope of evaluation to include open-ended output quality and ethical risk testing
    alongside traditional performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the evaluation phase, the engineering team should implement a robust evaluation
    strategy incorporating:'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark testing
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the LLM on a dataset of questions or tasks with known answers. For
    instance, for a code generation model, maintain a suite of programming problems
    where correct outputs are known and measure success rate. This is analogous to
    a test set in traditional ML. The benchmarks should be versioned so that improvements
    and regressions are tracked over time.
  prefs: []
  type: TYPE_NORMAL
- en: Quality metrics
  prefs: []
  type: TYPE_NORMAL
- en: Define quantitative metrics appropriate to the use case. These might include
    BLEU or ROUGE scores for text similarity if a reference output is available, diversity
    metrics, or response latency. Incorporating user satisfaction scores or ratings
    into the evaluation loop is often useful, if the application can gather feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Human and adversarial evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Given the open-ended nature of generative models, humans should assess a sample
    of outputs for factors like correctness, coherence, and safety. Adversarial testing,
    which involves deliberately testing the model with challenging or sensitive prompts
    to observe its behavior, is also strongly encouraged during the Testing stage.
    For example, a tester might ask a chatbot inappropriate or policy-violating questions
    to see whether it responds with disallowed content. The results can be summarized
    in an ethical risk analysis chart, as suggested by SMACTR, rating failure modes
    by severity and likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: Automated monitoring in evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Use automated tools to evaluate outputs at scale. An emerging GenAIOps practice
    is using “watcher models” or classifiers to flag problematic content. For instance,
    toxicity detectors such as Llama Guard or Azure’s Prompt Shields can scan a large
    sample of outputs to estimate the percentage that might be harmful or inappropriate.
    These tools add a layer of quality assurance beyond simple metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison and validation
  prefs: []
  type: TYPE_NORMAL
- en: If multiple model versions or prompt variants were developed, evaluate them
    side by side. A/B testing can be done offline (with evaluators blind to which
    model produced which output) to choose the best version. This resembles ensemble/hyperparameter
    selection in CRISP-ML(Q), but in GenAI it may involve comparing two prompt templates.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  prefs: []
  type: TYPE_NORMAL
- en: Documentation remains critical throughout evaluation. Teams should record what
    tests were run, the outcomes, and the decisions made based on those results. If
    the model fails certain tests, document whether the issues will be addressed or
    accepted with mitigations. For instance, if an LLM occasionally produces outputs
    that are slightly incorrect but harmless, the team might decide that’s acceptable
    for the business context; however, if it sometimes produces a privacy violation,
    that would trigger a model revision.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation phase in GenAIOps is also where “go or no-go” decisions are made
    for deployment. The engineering team should define clear launch criteria, such
    as “fewer than 1% of outputs flagged as offensive” or “at least 85% of test questions
    answered correctly.” If the model fails to meet these thresholds, the process
    loops back to the modeling or even the data preparation phase to address the shortcomings
    through fine-tuning, prompt redesign, or data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Testing stage of SMACTR aligns directly with the evaluation phase. The framework
    recommends techniques such as adversarial testing to assess performance, and the
    production of artifacts such as an ethical risk analysis chart. In a GenAI context,
    this means paying special attention to verifying the model’s compliance with ethical
    and risk-related requirements identified during the Scoping stage. Teams should
    use risk assessment tools like Failure Mode and Effects Analysis (FMEA) to list
    potential failure modes, such as “model gives legal advice” or “model outputs
    biased language,” and implement tests to cover each identified risk.
  prefs: []
  type: TYPE_NORMAL
- en: The Testing stage also focuses on ethical compliance*.* To evaluate high-priority
    ethical risks or known issues arising from the training data or model design,
    teams may create and execute a checklist of ethical tests, including questions
    like “Does the model hallucinate facts?” or “Does it stereotype?”
  prefs: []
  type: TYPE_NORMAL
- en: This phase also lays the groundwork for the Reflection stage. Gather all evaluation
    results and insights to feed into a mitigation plan. For example, if testing reveals
    that the model struggles with a particular category of inputs, make a note of
    that for post-deployment monitoring or retraining.
  prefs: []
  type: TYPE_NORMAL
- en: Artifact Collection continues throughout this phase. Evaluation reports, risk
    charts, and test results should be saved as part of the transparency trail. The
    evaluation phase for GenAI applications effectively becomes an audit of the model’s
    readiness. As well as verifying accuracy and performance, you should also test
    for safety, fairness, and robustness, with documented evidence to show stakeholders
    such as regulators that due diligence was done before deployment. [Table 7-7](#chapter_7_table_7_1748539924522437)
    summarizes the key SMACTR artifacts for this stage.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-7\. Summary of key artifacts produced during the evaluation phase
  prefs: []
  type: TYPE_NORMAL
- en: '| SMACTR stage | Output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Testing |'
  prefs: []
  type: TYPE_TB
- en: Performance assessment using methods like adversarial testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical risk analysis chart
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FMEA or similar risk assessment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tests for identified failure modes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checklist of ethical tests (e.g., hallucination, stereotyping)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reflection |'
  prefs: []
  type: TYPE_TB
- en: Gathered results and insights from evaluation to inform a mitigation plan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Artifact Collection |'
  prefs: []
  type: TYPE_TB
- en: Evaluation reports, risk charts, and test results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CRISP-ML(Q) deployment phase focuses on delivering the model into production
    and making sure it meets business requirements in real-world operation. Other
    key activities include user acceptance testing and producing documentation. In
    GenAIOps, this phase covers multiple operational steps, such as integration and
    orchestration (CI/CD), security and reliability engineering, and the actual model
    deployment. Once a model is considered ready, it must be integrated into the application
    infrastructure, served to end users, and maintained with proper MLOps and GenAIOps
    practices.
  prefs: []
  type: TYPE_NORMAL
- en: As in traditional MLOps, deployment in GenAIOps involves pushing the model or
    pipeline to production by setting up API endpoints or embedding the model into
    an application. It also includes tasks like continuous delivery of prompt or model
    updates, implementing safety controls such as rate limiting, monitoring for misuse,
    and ensuring the system can scale and remain reliable under load.
  prefs: []
  type: TYPE_NORMAL
- en: Given the dynamic nature of GenAI applications, teams may frequently update
    prompts or swap in improved models, making a CI/CD pipeline essential. The security
    and reliability aspects of this stage reflect CRISP-ML(Q)’s quality assurance
    mindset. The main goal is to ensure that the deployed model doesn’t expose vulnerabilities
    (such as prompt injection) and remains robust.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In practice, deploying a GenAI application with CRISP-ML(Q) principles means
    paying attention to engineering best practices and safety as the model goes live.
    Key considerations in the deployment phase include:'
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD for prompts and models
  prefs: []
  type: TYPE_NORMAL
- en: Establish automated pipelines to move updated prompts or model versions through
    testing and into production. For example, similar to unit tests in software deployment,
    if a prompt template is updated to improve performance, a CI pipeline should run
    the evaluation suite and promote the change only if it passes. Likewise, infrastructure-as-code
    should be used to deploy model servers or services. This ensures reproducibility
    by creating a consistent environment and provides traceability regarding which
    version of the model or prompt is in use.
  prefs: []
  type: TYPE_NORMAL
- en: System integration and performance optimization
  prefs: []
  type: TYPE_NORMAL
- en: Unlike simple ML deployments, GenAI applications often have multiple components
    (frontend, backend, model APIs, databases, vector stores, etc.). Integration involves
    connecting these and making sure data flows correctly—for instance, a user query
    routes to the LLM, which might call a knowledge base before returning a response.
    Depending on nonfunctional requirements such as cost, speed, and scalability,
    you may need to optimize for latency and throughput. Because large models can
    be expensive or slow to run, you might want to apply techniques like model quantization,
    caching frequent responses, or using smaller distilled models for specific requests.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring setup
  prefs: []
  type: TYPE_NORMAL
- en: Although monitoring is the next formal phase in CRISP-ML(Q), the groundwork
    is laid during deployment. Implement logging for model inputs and outputs, set
    up dashboards and alerts for key metrics (error rates, response times, etc.),
    and track usage patterns. GenAIOps best practices recommend integrating observability
    from the moment a model is deployed. Tools like Prometheus and Grafana are commonly
    used for infrastructure monitoring, along with custom monitors for model behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Safety mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: 'For GenAI applications, guardrails such as content filters, user authentication
    and authorization, rate limiting, and fallback mechanisms should be put in place
    at deployment time. For instance, if the LLM is part of a user-facing app, you
    might integrate a moderation API or a simple rules engine to check model outputs.
    If an output violates policy (e.g., contains hate speech), the system can block
    or sanitize the response. You should also plan for failover: if the LLM service
    is unavailable, the system should default to a safe fallback such as a simpler
    response or a static message. These safety and reliability measures help ensure
    the live system is resilient and complies with the requirements of the EU AI Act.'
  prefs: []
  type: TYPE_NORMAL
- en: Staged rollouts
  prefs: []
  type: TYPE_NORMAL
- en: Teams may choose to do a shadow deployment, where the model generates outputs
    in response to real user queries without the user seeing them, to collect performance
    data in production-like conditions. This is similar to a pilot or beta test in
    CRISP-ML(Q), providing assurance that the model works as expected before full
    launch.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation and training
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that the operations team (which may be the same as the development team)
    has clear documentation on how to roll back deployments, how to intervene if something
    goes wrong, and what the known issues are. In CRISP-ML(Q), deployment concludes
    with documentation being delivered to maintainers; in GenAIOps, this may take
    the form of runbooks or playbooks. For example, a playbook might say: “If monitoring
    shows a spike in slow responses, consider disabling complex features or scaling
    up compute resources.”'
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SMACTR framework encourages a “deploy responsibly” mentality, treating deployment
    not as the end goal, but as a moment to verify that everything done so far aligns
    with ethical, quality, and safety standards and to set up mechanisms for ongoing
    vigilance once the model is in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this phase, the Reflection stage of SMACTR should be activated. Before going
    live, the team should reflect on whether the development process met the initial
    ethical and risk-related objectives. Concretely, this means reviewing evaluation
    results and determining whether any remaining risks are unacceptable. For example,
    the Reflection stage might lead to arisk mitigation plan: “We observed that the
    model sometimes gives outdated information. Mitigation: Display a disclaimer or
    schedule regular retraining.”'
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR’s Reflection stage also calls for producing an algorithmic audit summary
    report, which is a summary of key findings, risk decisions, and ethical considerations
    throughout the project. For a GenAI deployment, this could be distilled into something
    like a model card plus a responsible AI deployment checklist that is filed for
    governance purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 7-8](#chapter_7_table_8_1748539924522457) provides an overview of the
    artifacts that should be produced during the deployment phase, particularly as
    part of the SMACTR framework’s Reflection stage.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-8\. Summary of key artifacts produced during the deployment phase
  prefs: []
  type: TYPE_NORMAL
- en: '| SMACTR stage | Output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reflection |'
  prefs: []
  type: TYPE_TB
- en: Review of evaluation results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision on acceptability of residual risks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risk mitigation plan (e.g., disclaimers, retraining schedules)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithmic audit summary report (or model card and responsible AI deployment
    checklist)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plan for periodic post-launch audits (internal reviews)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintenance of traceability (model version, deployment date, evaluation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Model Monitoring and Maintenance Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In GenAIOps, once an LLM-powered application is deployed, teams must continuously
    monitor its performance, capture data on real-world use, and maintain or improve
    the model over time. This phase, often called *model operations*, is analogous
    to the CRISP-ML(Q) monitoring and maintenance phase.
  prefs: []
  type: TYPE_NORMAL
- en: Maintenance in a GenAI context can take several forms, such as fine-tuning a
    model on new data, switching to a new model version, updating prompts, refreshing
    the knowledge base in a RAG pipeline, or retraining if the domain has changed
    significantly. The monitoring and maintenance phase is specifically intended to
    address the risk of model degradation in evolving environments, where data drift,
    changing user needs, or concept drift may occur post-deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In traditional ML, monitoring typically focuses on data drift, model drift,
    and core performance metrics—for instance, detecting if the distribution of the
    input data has changed enough to reduce accuracy and trigger retraining. Some
    of those concepts apply to GenAI as well: user queries may evolve over time, making
    the original model or prompt less relevant. However, GenAI introduces additional
    challenges, such as monitoring the content of outputs to detect inappropriate
    or factually incorrect answers and tracking user interaction patterns to detect
    potential misuse.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feedback loops are especially critical in GenAI: models can be improved using
    techniques like RLHF or fine-tuning based on logged errors and failures. CRISP-ML(Q)
    emphasizes continuous quality control but focuses more on technical degradation
    and planned retraining, whereas GenAI maintenance involves active, sometimes real-time,
    management of model behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, integrating monitoring throughout the pipeline is essential. Without
    proper monitoring and maintenance, a model’s performance and utility will likely
    decay over time, and safety can quickly degrade. The dynamic, nondeterministic
    nature of GenAI applications makes this phase even more critical and complex.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a GenAI application, the monitoring and maintenance phase involves several
    ongoing practices, many of which parallel classic MLOps but are adapted for GenAI-specific
    challenges and metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Live performance monitoring
  prefs: []
  type: TYPE_NORMAL
- en: Track the model’s outputs in production, monitoring for quality and reliability.
    This may involve statistical indicators such as the percentage of queries answered
    successfully or user ratings (if available), as well as technical metrics like
    latency and error rates. In a GenAI chatbot, for instance, frequent rephrasing
    of questions by users might signal poor initial responses. Setting up alert thresholds
    is important. For example, if the rate of content flagging by a moderation filter
    doubles overnight, the team should be alerted to investigate potential model drift
    or misuse.
  prefs: []
  type: TYPE_NORMAL
- en: Data and usage drift
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional ML, where input features can be directly compared, GenAI
    may require monitoring embeddings of user queries or clustering them to detect
    topic shifts. In a RAG-based system, for example, monitoring might reveal user
    interest in a new product that isn’t yet in the knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: Logging and audit
  prefs: []
  type: TYPE_NORMAL
- en: Maintain detailed logs of interactions, with appropriate privacy safeguards,
    to support retrospective analysis and auditing. These logs can feed into a feedback
    dataset. In addition, teams might periodically label a random sample of interactions
    to assess satisfaction or accuracy, helping to build an ongoing evaluation dataset
    to use for quality tracking.
  prefs: []
  type: TYPE_NORMAL
- en: Retraining or model updates
  prefs: []
  type: TYPE_NORMAL
- en: Decide on a schedule or criteria for when to retrain or update the model. In
    cases where the base model is provided via an API (like OpenAI’s GPT), retraining
    might not be feasible. However, teams can still fine-tune adapters and switch
    to newer model versions as they become available (e.g., upgrading from GPT-3.5
    to GPT-4). Each update should pass through a condensed version of the evaluation
    and deployment lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt and system maintenance
  prefs: []
  type: TYPE_NORMAL
- en: Maintenance is also important for the surrounding system. Teams may refine prompt
    templates based on observed performance or introduce new safety rules in response
    to emerging failure cases. Because GenAI applications can be highly sensitive
    to prompt changes, treat these with the same caution as code changes—test before
    rollout and monitor performance closely.
  prefs: []
  type: TYPE_NORMAL
- en: User feedback and iteration
  prefs: []
  type: TYPE_NORMAL
- en: Provide channels for user feedback (thumbs up/down for responses, reporting
    issues, etc.), and establish a process for analyzing and acting on this feedback
    regularly.
  prefs: []
  type: TYPE_NORMAL
- en: Risk management
  prefs: []
  type: TYPE_NORMAL
- en: If new misuse patterns emerge—for example, if users find a new way to get the
    model to produce disallowed content—the team should respond quickly, updating
    filters, modifying prompts, or adjusting model behavior to block these vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Monitoring and maintenance corresponds to an ongoing cycle of Reflection and
    renewed Scoping in SMACTR terms. While the Reflection stage is originally framed
    as a pre-deployment audit, its core activity—evaluating outcomes against initial
    goals and risks—should continue periodically post-deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a GenAI application, this could mean scheduling regular internal audits
    or postmortems. For example, after a month of production use, the team might review
    questions such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Are there new ethical concerns arising from how the model is being used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Did our mitigation plan hold up, or do we need new measures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This continuous reflection can lead to updates to the risk mitigation plan and
    may even trigger a fresh round of Scoping if the application’s purpose or usage
    context evolves. For instance, if the system is used in a higher-stakes environment
    than originally intended, the team should revisit and rescope ethical and operational
    risks.
  prefs: []
  type: TYPE_NORMAL
- en: SMACTR also calls for documentation of incidents, meaning teams should maintain
    a history of design changes and keep a log of any significant failures and how
    they were addressed. This is analogous to treating the algorithmic use-related
    risk analysis and audit summary report as living documents, updated with real-world
    findings over time.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the Mapping stage may need to be revisited if new stakeholders
    emerge. For example, customer support might get involved to handle user reports
    about the system, or legal teams might need to review compliance with regulations
    such as the EU AI Act. Maintenance processes should ensure that these stakeholders
    are identified and looped into governance and oversight activities.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, monitoring effectively functions as continuous testing in production.
    The SMACTR framework emphasizes that monitoring and audits help identify emerging
    threats and act quickly. The engineering team should treat monitoring data not
    just as operational information but as audit evidence. Every anomaly should be
    investigated for root causes, every significant output error analyzed for potential
    broader impacts, and oversight responsibilities clearly assigned. This is especially
    important for risk mitigation in GenAI, where issues like model bias or inappropriate
    outputs can lead to reputational or legal consequences.
  prefs: []
  type: TYPE_NORMAL
- en: All notable outputs for the monitoring and maintenance phase for each SMACTR
    stage are outlined in [Table 7-9](#chapter_7_table_9_1748539924522477).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-9\. Summary of key artifacts produced during the monitoring and maintenance
    phase
  prefs: []
  type: TYPE_NORMAL
- en: '| SMACTR stage | Output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reflection (ongoing) |'
  prefs: []
  type: TYPE_TB
- en: Periodic internal audits or postmortems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation of results relative to original goals and risks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates to the risk mitigation plan (living document)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation of incidents (design history, failure logs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintenance of algorithmic use-related risk analysis and audit summary report
    (living documents)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scoping (renewed) |'
  prefs: []
  type: TYPE_TB
- en: Rescoping and reassessment of risks if the application’s scope expands or changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Mapping (revisited) |'
  prefs: []
  type: TYPE_TB
- en: Inclusion of new stakeholders (e.g., customer support, legal teams) and clarification
    of their oversight roles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Testing (continuous) |'
  prefs: []
  type: TYPE_TB
- en: Monitoring as continuous testing in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identification of emerging threats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigation of anomalies and significant output errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clear assignment of oversight responsibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored the core aspects of the EU AI Act as they apply to general-purpose
    AI models and systems, along with the practical role of GenAIOps in supporting
    compliance and fostering trustworthy AI. As outlined here, the Act establishes
    a structured regulatory framework for GPAI, requiring documentation, disclosure,
    and risk mitigation measures to be embedded into providers’ workflows. GenAIOps
    extends traditional MLOps practices to meet the unique challenges of GPAI, enabling
    engineering teams to build systems that are compliant with regulatory requirements,
    resilient, scalable, and aligned with societal expectations. By integrating the
    SMACTR framework, organizations can further enhance CRISP-ML(Q) by embedding accountability
    mechanisms that address GPAI-specific risks such as hallucination and ethical
    misalignment.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving compliance, however, is not a one-time task. It demands a proactive,
    ongoing approach in which transparency, accountability, and ethical reflection
    are integrated throughout the AI development lifecycle. From data preparation
    and model fine-tuning to deployment and continuous monitoring, teams must adopt
    structured, auditable processes to ensure their systems remain reliable and interpretable
    and meet ethical standards. This is particularly crucial for managing risks such
    as bias, misinformation, and unintended model behaviors. GenAIOps provides an
    operational framework that allows organizations to implement these safeguards
    systematically, bridging the gap between regulatory compliance and real-world
    AI development and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Final Words and Future of AI Policymaking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we conclude our journey through AI governance, engineering practices, and
    the EU AI Act, it’s important to reflect on what has been covered and look to
    the future. We began by exploring the foundational landscape of the EU AI Act’s
    risk-based approach and how it introduces crucial obligations, even for systems
    not classified as high risk. A key insight is that the Act requires transparency
    for all AI systems intended to interact directly with natural persons, regardless
    of their risk level (primarily through Article 50).
  prefs: []
  type: TYPE_NORMAL
- en: We then turned to the practical world of AI development and deployment, recognizing
    that compliance is not a post-deployment afterthought but a process that must
    be integrated throughout the AI system lifecycle. The adoption of structured methodologies
    such as CRISP-ML(Q), alongside ethical frameworks like SMACTR, emerges as a proactive
    catalyst for responsible and compliant development. By aligning the phases of
    CRISP-ML(Q), from business and data understanding to monitoring and maintenance,
    with SMACTR principles, organizations can systematically address ethical considerations,
    manage risks, and fulfill transparency requirements at every stage.
  prefs: []
  type: TYPE_NORMAL
- en: A recurring theme throughout this book has been the critical importance of documentation
    and robust metadata management. These are foundational for traceability, internal
    audits, demonstrating compliance, and fostering transparency. Key artifacts generated
    throughout the development process, such as the project scope document, ethical
    review report, stakeholder map, data quality assessment, and initial transparency
    requirements document, serve as evidence of due diligence and ethical consideration.
    Further documentation, such as data quality reports, feature documentation, evaluation
    reports, compliance verification records, deployment configurations, and monitoring
    logs, helps establish a comprehensive compliance trail.
  prefs: []
  type: TYPE_NORMAL
- en: The rapid emergence and widespread adoption of GPAI and generative AI models
    during the legislative process introduced new challenges, prompting the EU AI
    Act to incorporate specific provisions to address them. In this evolving landscape,
    the principles of MLOps—extended into GenAIOps—remain vital for managing these
    complex systems. They support transparency, logging, and version control to create
    auditable trails, enabling teams to operationalize compliance while maintaining
    flexibility and performance.
  prefs: []
  type: TYPE_NORMAL
- en: As you have seen, navigating the landscape of AI regulation requires more than
    just understanding the rules—it demands a fundamental shift in how AI systems
    are engineered and governed. Proactive compliance, embedded within engineering
    workflows and supported by comprehensive documentation, is the path forward.
  prefs: []
  type: TYPE_NORMAL
- en: The landscape of AI technologies is evolving at a remarkable pace. An example
    of this is the emergence of retrieval-augmented generation and agent-based architectures.
    To keep pace, we must continually learn about new technologies and stay informed
    about regulatory updates, best practices, and emerging methodologies for AI governance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Preface, I quoted Fei-Fei Li, the founding codirector of the Stanford
    Institute for Human-Centered AI (HAI) and CEO and cofounder of World Labs, who
    [remarked](https://oreil.ly/_PUIb): “Now more than ever, AI needs a governance
    framework.” She laid out three fundamental principles for the future of AI policymaking:'
  prefs: []
  type: TYPE_NORMAL
- en: AI engineering should *prioritize empirical validation over speculation*, ensuring
    models are rigorously tested with real-world data and transparent benchmarks.
    This means engineers should rely on scientifically grounded methods, avoiding
    hype and focusing on practical applications rather than speculative scenarios.
    Standardized frameworks like ISO/IEC AI guidelines, NIST’s AI Risk Management
    Framework, or SMACTR can be integrated into development workflows to promote reliability,
    fairness, and robustness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Engineers must *adopt risk-aware, pragmatic development practices, balancing
    innovation with responsible deployment* by integrating continuous monitoring and
    adherence to ethical standards. This includes incorporating risk assessment early
    in development, leveraging iterative prototyping to surface and address unintended
    consequences, and continuous post-deployment monitoring to detect bias or misuse.
    AI systems should be designed with built-in safeguards and governance mechanisms,
    especially in high-stakes areas like defense or healthcare.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, *collaboration and open access* should be central, promoting open source
    contributions, knowledge sharing, and cross-sector partnerships to strengthen
    the AI ecosystem while maintaining accountability. Engineers can support this
    by sharing research findings, building reusable frameworks, and creating or contributing
    to tools that lower barriers to entry for startups and academic institutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ultimately, building trustworthy  AI is both a technical challenge and a responsibility
    shared by the whole organization. As engineers, researchers, and practitioners,
    we shape not only what AI can do, but how it affects the world. By embedding ethics,
    transparency, and accountability into every stage of development, we can ensure
    that the systems we create are not only powerful, but worthy of the trust placed
    in them.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch07.html#id616-marker)) For more information, see the paper [“Generative
    AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data”](https://oreil.ly/DDpPc)
    by Nahema Marchal et al.'
  prefs: []
  type: TYPE_NORMAL
