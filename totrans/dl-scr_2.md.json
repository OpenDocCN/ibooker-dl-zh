["```py\ndef forward_linear_regression(X_batch: ndarray,\n                              y_batch: ndarray,\n                              weights: Dict[str, ndarray])\n                              -> Tuple[float, Dict[str, ndarray]]:\n    '''\n Forward pass for the step-by-step linear regression.\n '''\n    # assert batch sizes of X and y are equal\n    assert X_batch.shape[0] == y_batch.shape[0]\n\n    # assert that matrix multiplication can work\n    assert X_batch.shape[1] == weights['W'].shape[0]\n\n    # assert that B is simply a 1x1 ndarray\n    assert weights['B'].shape[0] == weights['B'].shape[1] == 1\n\n    # compute the operations on the forward pass\n    N = np.dot(X_batch, weights['W'])\n\n    P = N + weights['B']\n\n    loss = np.mean(np.power(y_batch - P, 2))\n\n    # save the information computed on the forward pass\n    forward_info: Dict[str, ndarray] = {}\n    forward_info['X'] = X_batch\n    forward_info['N'] = N\n    forward_info['P'] = P\n    forward_info['y'] = y_batch\n\n    return loss, forward_info\n```", "```py\ndLdP = -2 * (Y - P)\n```", "```py\ndPdN = np.ones_like(N)\n```", "```py\ndNdW = np.transpose(X, (1, 0))\n```", "```py\ndPdB = np.ones_like(weights['B'])\n```", "```py\ndef loss_gradients(forward_info: Dict[str, ndarray],\n                   weights: Dict[str, ndarray]) -> Dict[str, ndarray]:\n    '''\n Compute dLdW and dLdB for the step-by-step linear regression model.\n '''\n    batch_size = forward_info['X'].shape[0]\n\n    dLdP = -2 * (forward_info['y'] - forward_info['P'])\n\n    dPdN = np.ones_like(forward_info['N'])\n\n    dPdB = np.ones_like(weights['B'])\n\n    dLdN = dLdP * dPdN\n\n    dNdW = np.transpose(forward_info['X'], (1, 0))\n\n    # need to use matrix multiplication here,\n    # with dNdW on the left (see note at the end of last chapter)\n    dLdW = np.dot(dNdW, dLdN)\n\n    # need to sum along dimension representing the batch size\n    # (see note near the end of this chapter)\n    dLdB = (dLdP * dPdB).sum(axis=0)\n\n    loss_gradients: Dict[str, ndarray] = {}\n    loss_gradients['W'] = dLdW\n    loss_gradients['B'] = dLdB\n\n    return loss_gradients\n```", "```py\nfor key in weights.keys():\n    weights[key] -= learning_rate * loss_grads[key]\n```", "```py\nforward_info, loss = forward_loss(X_batch, y_batch, weights)\n\nloss_grads = loss_gradients(forward_info, weights)\n\nfor key in weights.keys():  # 'weights' and 'loss_grads' have the same keys\n    weights[key] -= learning_rate * loss_grads[key]\n```", "```py\ntrain_info = train(X_train, y_train,\n                   learning_rate = 0.001,\n                   batch_size=23,\n                   return_weights=True,\n                   seed=80718)\n```", "```py\ndef predict(X: ndarray,\n            weights: Dict[str, ndarray]):\n    '''\n Generate predictions from the step-by-step linear regression model.\n '''\n    N = np.dot(X, weights['W'])\n\n    return N + weights['B']\n```", "```py\npreds = predict(X_test, weights)  # weights = train_info[0]\n```", "```py\n    def mae(preds: ndarray, actuals: ndarray):\n        '''\n     Compute mean absolute error.\n     '''\n        return np.mean(np.abs(preds - actuals))\n    ```", "```py\n    def rmse(preds: ndarray, actuals: ndarray):\n        '''\n     Compute root mean squared error.\n     '''\n        return np.sqrt(np.mean(np.power(preds - actuals, 2)))\n    ```", "```py\nMean absolute error: 3.5643\nRoot mean squared error: 5.0508\n```", "```py\nnp.round(weights['W'].reshape(-1), 4)\n```", "```py\narray([-1.0084,  0.7097,  0.2731,  0.7161, -2.2163,  2.3737,  0.7156,\n       -2.6609,  2.629 , -1.8113, -2.3347,  0.8541, -4.2003])\n```", "```py\ndef forward_loss(X: ndarray,\n                 y: ndarray,\n                 weights: Dict[str, ndarray]\n                 ) -> Tuple[Dict[str, ndarray], float]:\n    '''\n Compute the forward pass and the loss for the step-by-step\n neural network model.\n '''\n    M1 = np.dot(X, weights['W1'])\n\n    N1 = M1 + weights['B1']\n\n    O1 = sigmoid(N1)\n\n    M2 = np.dot(O1, weights['W2'])\n\n    P = M2 + weights['B2']\n\n    loss = np.mean(np.power(y - P, 2))\n\n    forward_info: Dict[str, ndarray] = {}\n    forward_info['X'] = X\n    forward_info['M1'] = M1\n    forward_info['N1'] = N1\n    forward_info['O1'] = O1\n    forward_info['M2'] = M2\n    forward_info['P'] = P\n    forward_info['y'] = y\n\n    return forward_info, loss\n```", "```py\nforward_info, loss = forward_loss(X_batch, y_batch, weights)\n\nloss_grads = loss_gradients(forward_info, weights)\n\nfor key in weights.keys():\n    weights[key] -= learning_rate * loss_grads[key]\n```", "```py\npreds = predict(X_test, weights)\n```", "```py\ndef predict(X: ndarray,\n            weights: Dict[str, ndarray]) -> ndarray:\n    '''\n Generate predictions from the step-by-step neural network model.\n '''\n    M1 = np.dot(X, weights['W1'])\n\n    N1 = M1 + weights['B1']\n\n    O1 = sigmoid(N1)\n\n    M2 = np.dot(O1, weights['W2'])\n\n    P = M2 + weights['B2']\n\n    return P\n```", "```py\nMean absolute error: 2.5289\nRoot mean squared error: 3.6775\n```"]