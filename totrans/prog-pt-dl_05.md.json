["```py\nThe cat sat on the mat.\n\nShe got up and impatiently climbed on the chair, meowing for food.\n```", "```py\nThe cat sat on the mat.\n```", "```py\nthe \u2014 [1 0 0 0 0]\ncat \u2014 [0 1 0 0 0]\nsat \u2014 [0 0 1 0 0]\non  \u2014 [0 0 0 1 0]\nmat \u2014 [0 0 0 0 1]\n```", "```py\nembed = nn.Embedding(vocab_size, dimension_size)\n```", "```py\ncat_mat_embed = nn.Embedding(5, 2)\ncat_tensor = Tensor([1])\ncat_mat_embed.forward(cat_tensor)\n\n> tensor([[ 1.7793, -0.3127]], grad_fn=<EmbeddingBackward>)\n```", "```py\npip install torchtext\n```", "```py\nconda install -c derickl torchtext\n```", "```py\nimport pandas as pd\ntweetsDF = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",\n                        header=None)\n```", "```py\nUnicodeDecodeError: 'utf-8' codec can't decode bytes in\nposition 80-81: invalid continuation byte\n```", "```py\ntweetsDF = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",\nengine=\"python\", header=None)\n```", "```py\n>>> tweetDF.head(5)\n0  0  1467810672  ...  NO_QUERY   scotthamilton  is upset that ...\n1  0  1467810917  ...  NO_QUERY        mattycus  @Kenichan I dived many times ...\n2  0  1467811184  ...  NO_QUERY         ElleCTF    my whole body feels itchy\n3  0  1467811193  ...  NO_QUERY          Karoli  @nationwideclass no, it's ...\n4  0  1467811372  ...  NO_QUERY        joy_wolf  @Kwesidei not the whole crew\n```", "```py\n>>> tweetsDF[0].value_counts()\n4    800000\n0    800000\nName: 0, dtype: int64\n```", "```py\ntweetsDF[\"sentiment_cat\"] = tweetsDF[0].astype('category')\n```", "```py\ntweetsDF[\"sentiment\"] = tweetsDF[\"sentiment_cat\"].cat.codes\n```", "```py\ntweetsDF.to_csv(\"train-processed.csv\", header=None, index=None)\n```", "```py\ntweetsDF.sample(10000).to_csv(\"train-processed-sample.csv\", header=None,\n    index=None)\n```", "```py\nfrom torchtext import data\n\nLABEL = data.LabelField()\nTWEET = data.Field(tokenize='spacy', lower=true)\n```", "```py\n fields = [('score',None), ('id',None),('date',None),('query',None),\n      ('name',None),\n      ('tweet', TWEET),('category',None),('label',LABEL)]\n```", "```py\ntwitterDataset = torchtext.data.TabularDataset(\n        path=\"training-processed.csv\",\n        format=\"CSV\",\n        fields=fields,\n        skip_header=False)\n```", "```py\n(train, test, valid) = twitterDataset.split(split_ratio=[0.8,0.1,0.1])\n\n(len(train),len(test),len(valid))\n> (1280000, 160000, 160000)\n```", "```py\n>vars(train.examples[7])\n\n{'label': '6681',\n 'tweet': ['woah',\n  ',',\n  'hell',\n  'in',\n  'chapel',\n  'thrill',\n  'is',\n  'closed',\n  '.',\n  'no',\n  'more',\n  'sweaty',\n  'basement',\n  'dance',\n  'parties',\n  '?',\n  '?']}\n```", "```py\nvocab_size = 20000\nTWEET.build_vocab(train, max_size = vocab_size)\n```", "```py\nlen(TWEET.vocab)\n> 20002\n```", "```py\n>TWEET.vocab.freqs.most_common(10)\n[('!', 44802),\n ('.', 40088),\n ('I', 33133),\n (' ', 29484),\n ('to', 28024),\n ('the', 24389),\n (',', 23951),\n('a', 18366),\n ('i', 17189),\n('and', 14252)]\n```", "```py\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n(train, valid, test),\nbatch_size = 32,\ndevice = device)\n```", "```py\nfrom torchtext import data\n\ndevice = \"cuda\"\nLABEL = data.LabelField()\nTWEET = data.Field(tokenize='spacy', lower=true)\n\nfields = [('score',None), ('id',None),('date',None),('query',None),\n      ('name',None),\n      ('tweet', TWEET),('category',None),('label',LABEL)]\n\ntwitterDataset = torchtext.data.TabularDataset(\n        path=\"training-processed.csv\",\n        format=\"CSV\",\n        fields=fields,\n        skip_header=False)\n\n(train, test, valid) = twitterDataset.split(split_ratio=[0.8,0.1,0.1])\n\nvocab_size = 20002\nTWEET.build_vocab(train, max_size = vocab_size)\n\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n(train, valid, test),\nbatch_size = 32,\ndevice = device)\n```", "```py\nimport torch.nn as nn\n\nclass OurFirstLSTM(nn.Module):\n    def __init__(self, hidden_size, embedding_dim, vocab_size):\n        super(OurFirstLSTM, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.encoder = nn.LSTM(input_size=embedding_dim,\n                hidden_size=hidden_size, num_layers=1)\n        self.predictor = nn.Linear(hidden_size, 2)\n\n    def forward(self, seq):\n        output, (hidden,_) = self.encoder(self.embedding(seq))\n        preds = self.predictor(hidden.squeeze(0))\n        return preds\n\nmodel = OurFirstLSTM(100,300, 20002)\nmodel.to(device)\n```", "```py\noptimizer = optim.Adam(model.parameters(), lr=2e-2)\ncriterion = nn.CrossEntropyLoss()\n\ndef train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n    for epoch in range(1, epochs + 1):\n\n        training_loss = 0.0\n        valid_loss = 0.0\n        model.train()\n        for batch_idx, batch in enumerate(train_iterator):\n            opt.zero_grad()\n            predict = model(batch.tweet)\n            loss = criterion(predict,batch.label)\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.data.item() * batch.tweet.size(0)\n        training_loss /= len(train_iterator)\n\n        model.eval()\n        for batch_idx,batch in enumerate(valid_iterator):\n            predict = model(batch.tweet)\n            loss = criterion(predict,batch.label)\n            valid_loss += loss.data.item() * x.size(0)\n\n        valid_loss /= len(valid_iterator)\n        print('Epoch: {}, Training Loss: {:.2f},\n        Validation Loss: {:.2f}'.format(epoch, training_loss, valid_loss))\n```", "```py\ndef classify_tweet(tweet):\n    categories = {0: \"Negative\", 1:\"Positive\"}\n    processed = TWEET.process([TWEET.preprocess(tweet)])\n    return categories[model(processed).argmax().item()]\n```", "```py\ntensor([[ 0.7828, -0.0024]]\n```", "```py\nThe cat sat on the mat\n```", "```py\nThe cat sat on the rug\n```", "```py\ndef random_insertion(sentence,n):\n    words = remove_stopwords(sentence)\n    for _ in range(n):\n        new_synonym = get_synonyms(random.choice(words))\n        sentence.insert(randrange(len(sentence)+1), new_synonym)\n    return sentence\n```", "```py\nThe cat sat on the mat\nThe cat mat sat on feline the mat\n```", "```py\ndef random_deletion(words, p=0.5):\n    if len(words) == 1:\n        return words\n    remaining = list(filter(lambda x: random.uniform(0,1) > p,words))\n    if len(remaining) == 0:\n        return [random.choice(words)]\n    else\n        return remaining\n```", "```py\ndef random_swap(sentence, n=5):\n    length = range(len(sentence))\n    for _ in range(n):\n        idx1, idx2 = random.sample(length, 2)\n        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1]\n    return sentence\n```", "```py\npip install googletrans\n```", "```py\nimport googletrans\nimport googletrans.Translator\n\ntranslator = Translator()\n\nsentences = ['The cat sat on the mat']\n\ntranslation_fr = translator.translate(sentences, dest='fr')\nfr_text = [t.text for t in translations_fr]\ntranslation_en = translator.translate(fr_text, dest='en')\nen_text = [t.text for t in translation_en]\nprint(en_text)\n\n>> ['The cat sat on the carpet']\n```", "```py\nimport random\n\navailable_langs = list(googletrans.LANGUAGES.keys())\ntr_lang = random.choice(available_langs)\nprint(f\"Translating to {googletrans.LANGUAGES[tr_lang]}\")\n\ntranslations = translator.translate(sentences, dest=tr_lang)\nt_text = [t.text for t in translations]\nprint(t_text)\n\ntranslations_en_random = translator.translate(t_text, src=tr_lang, dest='en')\nen_text = [t.text for t in translations_en_random]\nprint(en_text)\n```"]