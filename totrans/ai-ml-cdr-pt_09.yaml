- en: Chapter 8\. Using ML to Create Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the release of ChatGPT in 2022, the words *generative AI* entered the common
    lexicon. This simple application that allowed you to chat with a cloud-based AI
    seemed almost miraculous in how it could answer your queries with knowledge of
    almost everything in human experience. It worked by using a very advanced evolution
    beyond the recurrent neural networks you saw in the last chapter, by using a technique
    called *transformers*.
  prefs: []
  type: TYPE_NORMAL
- en: A *transformer* learns the patterns that turn one piece of text into another.
    With a large enough transformer architecture and a large enough set of text to
    learn from, the GPT model (GPT stands for generative pretrained transformers)
    could predict the next tokens to follow a piece of text. When GPT was wrapped
    in an application that made it more user friendly, a whole new industry was born.
  prefs: []
  type: TYPE_NORMAL
- en: While creating models with transformers is beyond the scope of this book, we
    will look at their architecture in detail in [Chapter 15](ch15.html#ch15_transformers_and_transformers_1748549808974580).
  prefs: []
  type: TYPE_NORMAL
- en: The principles involved in training models with transformers can be replicated
    with smaller, simpler, architectures like RNNs or LSTM. We’ll explore that in
    this chapter and with a much smaller corpus of text—traditional Irish songs.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, consider this line of text from a famous TV show:'
  prefs: []
  type: TYPE_NORMAL
- en: You know nothing, Jon Snow.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A next-token-predictor model, created with RNNs, came up with these song lyrics
    in response:'
  prefs: []
  type: TYPE_NORMAL
- en: You know nothing, Jon Snow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the place where he’s stationed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: be it Cork or in the blue bird’s son
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sailed out to summer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: old sweet long and gladness rings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: so I’ll wait for the wild Colleen dying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This text was generated by a very simple model that was trained on a small corpus.
    I’ve enhanced it a little by adding line breaks and punctuation, but other than
    the first line, all of the lyrics were generated by the model you’ll learn how
    to build in this chapter. It’s kind of cool that it mentions a *wild Colleen dying*—if
    you’ve watched the show that Jon Snow comes from, you’ll understand why!
  prefs: []
  type: TYPE_NORMAL
- en: In the last few chapters, you saw how you can use PyTorch with text-based data—first
    tokenizing it into numbers and sequences that can be processed by a neural network,
    then using embeddings to simulate sentiment using vectors, and finally using deep
    and recurrent neural networks to classify text. We used the sarcasm dataset, a
    small and simple one, to illustrate how all this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we’re going to shift gears: instead of classifying existing
    text, you’ll create a neural network that can *predict* text and thus *generate*
    text.'
  prefs: []
  type: TYPE_NORMAL
- en: Given a corpus of text, the network will attempt to learn and understand the
    *patterns* of words within the text so that it can, given a new piece of text
    called a *seed*, predict what word should come next. Once the network has that,
    the seed and the predicted word become the new seed, and it can predict the next
    word. Thus, when trained on a corpus of text, a neural network can attempt to
    write new text in a similar style. To create the preceding piece of poetry, I
    collected lyrics from a number of traditional Irish songs, trained a neural network
    with them, and used it to predict words.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start simple, using a small amount of text to illustrate how to build
    up to a predictive model, and we’ll end by creating a full model with a lot more
    text. After that, you can try it out to see what kind of poetry it can create!
  prefs: []
  type: TYPE_NORMAL
- en: To get started, you’ll have to treat the text a little differently from how
    you’ve been treating it thus far. In the previous chapters, you took sentences
    and turned them into sequences that were then classified based on the embeddings
    for the tokens within them. But when it comes to creating data that can be used
    to train a predictive model like this one, there’s an additional step in which
    you need to transform the sequences into *input sequences* and *labels*, where
    the input sequence is a group of words and the label is the next word in the sentence.
    You can then train a model to match the input sequences to their labels, so that
    future predictions can pick a label that’s close to the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Turning Sequences into Input Sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When predicting text, you need to train a neural network with an input sequence
    (feature) that has an associated label. Matching sequences to labels is the key
    to predicting text. In this case, you won’t have explicit labels like you do when
    you are classifying, but instead, you’ll split the sentence, and for a block of
    *n* words, the next word in the sentence will be the label.
  prefs: []
  type: TYPE_NORMAL
- en: So, for example, if in your corpus you had the sentence “Today has a beautiful
    blue sky,” then you could split it into “Today has a beautiful blue” as the feature
    and “sky” as the label. Then, if you were to get a prediction for the text “Today
    has a beautiful blue,” it would likely be “sky.” If, in the training data, you
    also had “Yesterday had a beautiful blue sky,” you would split it in the same
    way, and if you were to get a prediction for the text “Tomorrow will have a beautiful
    blue,” then there’s a high probability that the next word would be “sky.”
  prefs: []
  type: TYPE_NORMAL
- en: If you train a network with lots of sentences, where you remove the last word
    and make it the label, you can quickly build up a predictive model in which the
    most likely next word in the sentence can be predicted from an existing body of
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with a very small corpus of text—an excerpt from a traditional
    Irish song from the 1860s, some of the lyrics of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the town of Athy one Jeremy Lanigan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Battered away ’til he hadn’t a pound.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: His father died and made him a man again
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left him a farm and ten acres of ground.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He gave a grand party for friends and relations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who didn’t forget him when come to the wall,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And if you’ll but listen I’ll make your eyes glisten
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of the rows and the ructions of Lanigan’s Ball.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Myself to be sure got free invitation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all the nice girls and boys I might ask,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And just in a minute both friends and relations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Were dancing round merry as bees round a cask.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Judy O’Daly, that nice little milliner,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: She tipped me a wink for to give her a call,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And I soon arrived with Peggy McGilligan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just in time for Lanigan’s Ball.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You’ll want to create a single string with all the text and set that to be
    your data. Use \n for the line breaks. Then, this corpus can be easily loaded
    and tokenized. First, the tokenize function will split the text into individual
    words, and then the `create_word_dictionary` will create a dictionary with an
    index for each individual word in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is a very simplistic approach for the purpose of learning how
    these work. In production systems, you’d likely either use off-the-shelf components
    that have been built for scale or greatly enhance them for scale and exception
    checking.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these functions, you can then create a `word_index` of the simple corpus,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The result of this process is to replace the words with their token values (see
    [Figure 8-1](#ch08_figure_1_1748549671837881)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Tokenizing a sentence
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To train a predictive model, we should take a further step here: splitting
    the sentence into multiple smaller sequences so, for example, we can have one
    sequence consisting of the first two tokens, another consisting of the first three,
    etc. We would then pad these out to be the same length as the input sequence by
    prepending zeros (see [Figure 8-2](#ch08_figure_2_1748549671837934)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Turning a sequence into a number of input sequences
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To do this, you’ll need to go through each line in the corpus and turn it into
    a list of tokens, using functions to convert the text words into an array of their
    lookup values in the word dictionary, and then to create the padded versions of
    the subsequences. To assist you with this task, I’ve provided these functions:
    `text_to_sequence` and `pad_sequence`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then create the input sequences using these helper functions like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, once you have a set of padded input sequences, you can split them into
    features and labels, where each label is simply the last token in each input sequence
    (see [Figure 8-3](#ch08_figure_3_1748549671837963)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Turning the padded sequences into features (x) and labels (y)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you’re training a neural network, you’re going to match each feature to
    its corresponding label. So, for example, the label for [0 0 0 0 0 0 1] will be
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code you use to separate the labels from the input sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, you need to encode the labels. Right now, they’re just tokens—for example,
    the number 2 at the top of [Figure 8-3](#ch08_figure_3_1748549671837963) is a
    token. But if you want to use a token as a label in a classifier, you’ll have
    to map it to an output neuron. Thus, if you’re going to classify *n* words, with
    each word being a class, you’ll need to have *n* neurons. Here’s where it’s important
    to control the size of the vocabulary, because the more words you have, the more
    classes you’ll need. Remember back in Chapter [2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    and [3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912),
    when you were classifying fashion items with the Fashion MNIST dataset and you
    had 10 types of items of clothing? That required you to have 10 neurons in the
    output layer—but in this case, what if you want to predict up to 10,000 vocabulary
    words? You’ll need an output layer with 10,000 neurons!
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you need to one-hot encode your labels so that they match the
    desired output from a neural network. Consider [Figure 8-3](#ch08_figure_3_1748549671837963)
    again. If a neural network is fed the input *x* consisting of a series of 0s followed
    by a 1, you’ll want the prediction to be 2—but how the network delivers that is
    by having an output layer of `vocabulary_size` neurons, where the second one has
    the highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To encode your labels into a set of *Y*s that you can then use to train, you
    can use this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that there are many libraries and helper functions out there that can do
    this for you, so feel free to use them instead of the simple code in this chapter.
    But I want to put these methodologies in here, so you can see how it works under
    the hood.
  prefs: []
  type: TYPE_NORMAL
- en: You can see this visually in [Figure 8-4](#ch08_figure_4_1748549671837985).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. One-hot encoding labels
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a very sparse representation that, if you have a lot of training data
    and a lot of potential words, will eat memory very quickly! Suppose you had 100,000
    training sentences, with a vocabulary of 10,000 words—you’d need 1,000,000,000
    bytes just to hold the labels! But that’s the way we have to design our network
    if we’re going to classify and predict words.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now create a simple model that can be trained with this input data. It
    will consist of just an embedding layer, followed by an LSTM, followed by a dense
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: For the embedding, you’ll need one vector per word, so the parameters will be
    the total number of words and the number of dimensions you want to embed on. In
    this case, we don’t have many words, so eight dimensions should be enough.
  prefs: []
  type: TYPE_NORMAL
- en: You can make the LSTM bidirectional, and the number of steps can be the length
    of a sequence, which is our max length minus 1 (because we took one token off
    the end to make the label).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the output layer will be a dense layer with the total number of words
    as a parameter, activated by Softmax. Each neuron in this layer will be the probability
    that the next word matches the word for that index value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you compile the model with a categorical loss function such as categorical
    cross entropy and an optimizer like Adam. You can also specify that you want to
    capture metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It’s a very simple model without a lot of data, and it trains quickly. Here
    are the results of training for about 15,000 epochs, which takes maybe 10 minutes.
    In the real world, you’ll likely be training with a lot more data and thus taking
    a lot more time, so you’ll have to consider some of the techniques we saw in [Chapter 7](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648)
    to ensure greater accuracy and potentially less time to train.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see that it has reached very high accuracy, and there may be room for
    more (see [Figure 8-5](#ch08_figure_5_1748549671838006)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Training loss and accuracy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the model at 80%+ accuracy, we can be assured that if we have a string
    of text that it has already seen, it will predict the next word accurately about
    80% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Note, however, that when generating text, it will continually see words that
    it hasn’t previously seen, so despite this good number, you’ll find that the network
    will rapidly end up producing nonsensical text. We’ll explore this in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you’ve trained a network that can predict the next word in a sequence,
    the next step is to give it a sequence of text and have it predict the next word.
    Let’s take a look at how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the Next Word
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ll start by creating a phrase called the *seed text*. This is the initial
    expression on which the network will base all the content it generates, and it
    will do this by predicting the next word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with a phrase that the network has *already* seen, such as “in the town
    of Athy”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you need to tokenize this and turn it into a sequence of tokens of the
    same length as used for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you need to pad that sequence to get it into the same shape as the data
    used for training by converting it to a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can predict the next word for this token list by passing this input
    tensor to the model to get the output. This will be a tensor that contains the
    probabilities for each of the words in the dictionary and the likelihood that
    it will be the next token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return the probabilities for each word in the corpus, so you should
    pass the results to `torch.argmax` to get the most likely one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This should give you the value `6`. If you look at the word index, you’ll see
    that it’s the word “one”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also look it up in code by searching through the word index items until
    you find the predicted word and printing it out. You can do this by creating a
    reverse dictionary (mapping the value to the word, instead of vice-versa):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'So, starting from the text “in the town of Athy,” the network predicted that
    the next word should be “one”—which, if you look at the training data, is correct
    because the song begins with this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In the town of Athy* one Jeremy Lanigan'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Battered away til he hadn’t a pound
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Indeed, if you check the top five predictions based on their values in the
    index, you’ll get something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now that you’ve confirmed that the model is working, you can get creative and
    use different seed text. For example, when I used the seed text “sweet Jeremy
    saw Dublin,” the next word it predicted was “his.”
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This text was chosen because all of those words are in the corpus. In such cases,
    you should expect more accurate results, at least at the beginning, for the predicted
    words.
  prefs: []
  type: TYPE_NORMAL
- en: Compounding Predictions to Generate Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you saw how to use the model to predict the next word
    given a seed text. Now, to have the neural network create new text, you simply
    repeat the prediction, adding new words each time.
  prefs: []
  type: TYPE_NORMAL
- en: For example, earlier, when I used the phrase “sweet Jeremy saw Dublin,” it predicted
    that the next word would be “his.” You can build on this by appending “his” to
    the seed text to get “sweet Jeremy saw Dublin his” and getting another prediction.
    Repeating this process will give you an AI-created string of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the updated code from the previous section that performs this loop a
    number of times, with the number set by the `num_words` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will end up creating a string something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It rapidly descends into gibberish. Why? The first reason is that the body of
    training text is really small, so it has very little context to work with. The
    second is that the prediction of the next word in the sequence depends on the
    previous words in the sequence, and if there is a poor match on the previous ones,
    even the best “next” match will have a low probability of being accurate. When
    you add this to the sequence and predict the next word after that, the likelihood
    of it having a low probability of accuracy is even higher—thus, the predicted
    words will seem semirandom.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, while all of the words in the phrase “sweet Jeremy saw Dublin”
    exist in the corpus, they never exist in that order. When the model made the first
    prediction, it chose the word “his” as the most likely candidate, and it had quite
    a high probability of accuracy (78%):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When the model added that word to the seed to get “sweet Jeremy saw Dublin
    his,” we had another phrase not seen in the training data, so the prediction gave
    the highest probability to the word “right,” at 44%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: While occasionally there will be high certainty of a token following another
    (like “leg” following “right”), over time, you’ll see that continuing to add words
    to the sentence reduces the likelihood of a match in the training data, and as
    such, the prediction accuracy will suffer—leading to there being a more random
    “feel” to the words being predicted.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to the phenomenon of AI-generated content getting increasingly nonsensical
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: For an example, check out the excellent sci-fi short [*Sunspring*](https://oreil.ly/hTBtJ),
    which was written entirely by an LSTM-based network (like the one you’re building
    here) that was trained on science fiction movie scripts. The model was given seed
    content and tasked with generating a new script. The results were hilarious, and
    you’ll see that while the initial content makes sense, as the movie progresses,
    it becomes less and less comprehensible.
  prefs: []
  type: TYPE_NORMAL
- en: This is also the basis of *hallucination* in LLMs, a common phenomenon that
    reduces trust in them.
  prefs: []
  type: TYPE_NORMAL
- en: Extending the Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can easily extend the same pattern that you used for the hardcoded dataset
    to use a text file. I’ve hosted a text file containing about 1,700 lines of text
    gathered from a number of songs that you can use for experimentation. With a little
    modification, you can use this instead of the single hardcoded song.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the data in Colab, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can simply load the text from it into your corpus like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The rest of your code will then work with very little modification!
  prefs: []
  type: TYPE_NORMAL
- en: Training this for 50,000 epochs—which takes about 30 minutes on a T4 Colab instance—brings
    you to about 30% accuracy, with the curve flattening out (see [Figure 8-6](#ch08_figure_6_1748549671838028)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Training on a larger dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When using Google Colab, you can choose various accelerators on the backend,
    which can make your training go much faster. In this case, as noted, I used a
    T4\. To do this for yourself, when in Colab, under the Connect menu, you’ll see
    a Change Runtime Type option. Select that, and you’ll see the accelerators available
    for you to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trying the phrase “in the town of Athy” again yields a prediction of “one”
    but this time with just over 83% probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Running for a few more tokens, we can see output like the following. It’s beginning
    to create songs, though it’s quite quickly descending into gibberish!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'For “sweet Jeremy saw Dublin,” the predicted next word is “she,” with a probability
    of 80%. Predicting the next few words yields this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It’s looking a little better! But can we improve it further?
  prefs: []
  type: TYPE_NORMAL
- en: Improving the Model Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way that you can improve the model is to change its architecture, using
    multiple stacked LSTMs and some other optimization techniques. Given that there’s
    no clear benchmark for accuracy with a dataset like this one—there’s no right
    or wrong classification, nor is there a target regression—it’s difficult to establish
    when a model is good or bad. Therefore, accuracy results can be very subjective.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, they’re still a good yardstick, so in this section, I’m going
    to explore some additions you can make to the architecture to improve the accuracy
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding Dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    we discussed the fact that the optimal dimension for embeddings is the fourth
    root of the number of words. In this scenario, the vocabulary has 3,259 words,
    and the fourth root of this is approximately 8\. Another rule of thumb is the
    log of this number—and log(3259) is a little over 32\. So, if the network is learning
    slowly, you have the option to pick a number between these two values. That gives
    you enough “room” to capture the relationships between words.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, parameters in a neural network are initialized to zero. You can give
    learning a little bit of a kickstart by initializing different layers to different
    types supported by various research findings. We briefly cover these types of
    layers in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Embedding layers can be initialized with a normal distribution, with a standard
    deviation scaled by `1/sqrt(embedding_dim)` for better gradient flow. That’s similar
    to `word2vec`-style initialization.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LSTM has four internal neural types—input, forget, cell, and output—and their
    weight matrix is a bunch of them stacked together. The different types benefit
    from different initializations. Two great papers that discuss this are [“An Empirical
    Exploration of Recurrent Network Architectures” by Rafal Jozefowicz et al.](https://oreil.ly/UuvQO)
    and [“On the Difficulty of Training Recurrent Neural Networks”](https://oreil.ly/Ttvll)
    by Razvan Pascanu et al. The specifics of LSTM are beyond the scope of this chapter,
    but check the associated code for one methodology to initialize them.
  prefs: []
  type: TYPE_NORMAL
- en: Final linear layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In their 2015 paper “[Delving Deep into Rectifiers,” Kaiming He et al.](https://oreil.ly/_MM6A)
    explored initialization of linear layers and proposed “Kaiming” initialization
    (aka “He” initialization). A detailed explanation of this is beyond the scope
    of this book, but the code is available in the notebooks in the [GitHub repository](https://github.com/lmoroney/PyTorch-Book-FIles).
  prefs: []
  type: TYPE_NORMAL
- en: Variable Learning Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In every example we’ve seen so far, we’ve explored different learning rates
    and their impact on the network—but you can actually *vary* the learning rate
    as the network learns. Values that work well in early epochs may not work so well
    in later ones, so putting together a scheduler that adjusts this learning rate
    epoch by epoch can help you create networks that learn more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, PyTorch provides a `torch.optim.lr_scheduler` that you can program
    to change over the course of the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 7](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648),
    we looked at the idea of a *learning rate* (LR), which is a hyperparameter that,
    if set to be too large, will cause the network to overlearn, and if set to be
    too small will prevent the network from learning effectively. The nice thing about
    this is that you can set it as a variable rate, which we do here. In the early
    epochs, we want the network to learn fast, so we have a large LR—and in the later
    epochs, we don’t want it to overfit, so we gradually reduce the LR.
  prefs: []
  type: TYPE_NORMAL
- en: The `pct_start` parameter defines a warm-up period as the first 10% of the training,
    during which the learning rate gradually increases up to the maximum (in this
    case, 0.01) and then decreases to 1/1000 of the initial learning rate (determined
    by `final_div_factor`).
  prefs: []
  type: TYPE_NORMAL
- en: You can see the impact this has on training in [Figure 8-7](#ch08_figure_7_1748549671838042),
    where it reached ~90% accuracy in 6,800 epochs before triggering early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Adding a second LSTM layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This time, when testing with the same phrases as before, I got “little” as the
    next word after “in the town of Athy” with a 26% probability, and I got “one”
    as the next word after “sweet Jeremy saw Dublin” with a 32% probability. Again,
    when predicting more words, the output quickly descended into gibberish.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The word `lámh` shows up in this text, and it’s Gaelic for *hand*. And `do lámh`
    means *your hand*.
  prefs: []
  type: TYPE_NORMAL
- en: If you get different results, don’t worry—you didn’t do anything wrong, but
    the random initialization of the neurons will impact the final scores.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There’s a little trick that you can use to extend the size of this dataset without
    adding any new songs. It’s called *windowing* the data. Right now, every line
    in every song is read as a single line and then turned into input sequences, as
    you saw in [Figure 8-2](#ch08_figure_2_1748549671837934). While humans read songs
    line by line to hear rhyme and meter, the model doesn’t have to, in particular
    when using bidirectional LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: So, instead of taking the line “In the town of Athy, one Jeremy Lanigan,” processing
    that, and then moving to the next line (“Battered away ‘til he hadn’t a pound”)
    and processing that, we could treat all the lines as one long, continuous text.
    We could then create a “window” into that text of *n* words, process that, and
    then move the window forward one word to get the next input sequence (see [Figure 8-8](#ch08_figure_8_1748549671838067)).
  prefs: []
  type: TYPE_NORMAL
- en: In this case, far more training data can be yielded in the form of an increased
    number of input sequences. Moving the window across the entire corpus of text
    would give us ((*number_of_words* – *window_size*) × *window_size*) input sequences
    that we could train with.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. A moving word window
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The code is pretty simple—when loading the data, instead of splitting each
    song line into a “sentence,” we can create them on the fly from the words in the
    corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this case, because we no longer have sentences and we’re creating sequences
    that are the same size as the moving window, `max_sequence_len` is the size of
    the window. The full file is read, converted to lowercase, and split into an array
    of words using string splitting. The code then loops through the words and makes
    sentences of each word from the current index up to the current index plus the
    window size, adding each of those newly constructed sentences to the sentences
    array.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To train this, you’ll likely need a higher-memory GPU. I used the 40Gb A100
    that’s available in Colab.
  prefs: []
  type: TYPE_NORMAL
- en: When training, you’ll notice that the extra data makes training much slower
    per epoch, but the results are greatly improved and the generated text descends
    into gibberish much more slowly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example that caught my eye—particularly the last line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: There are many hyperparameters you can try to tune. Changing the window size
    will change the amount of training data—a smaller window size can yield more data,
    but there will be fewer words to give to a label, so if you set it too small,
    you’ll end up with nonsensical poetry. You can also change the dimensions in the
    embedding, the number of LSTMs, or the size of the vocabulary to use for training.
    Given that percentage accuracy isn’t the best measurement—you’ll want to make
    a more subjective examination of how much “sense” the poetry makes—there’s no
    hard-and-fast rule to follow to determine whether your model is “good” or not.
    Of course, you *will* be limited by the amount of data you have and the compute
    that’s available to you—the bigger the model, the more power you will need.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the important thing is to experiment and have fun!
  prefs: []
  type: TYPE_NORMAL
- en: Character-Based Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the last few chapters, we’ve been looking at NLP using word-based encoding.
    I find that much easier to get started with, but when it comes to generating text,
    you might also want to consider *character-based encoding* because the number
    of unique *characters* in a corpus tends to be a lot less than the number of unique
    *words*. If you use this approach, you can have a lot fewer neurons in your output
    layer, and your output predictions can be spread across fewer probabilities. For
    example, if you look at the dataset of [the complete works of Shakespeare](https://oreil.ly/XW_ab),
    you’ll see that there are only 65 unique characters in the entire set. Basically,
    Shakespeare only really used uppercase and lowercase letters and some punctuation
    to give a unique set of 65 characters!
  prefs: []
  type: TYPE_NORMAL
- en: So, when you are making predictions with this dataset, instead of looking at
    the probabilities for the next word across 2,700 words as in the Irish songs dataset,
    you’re only looking at 65\. This makes your model a bit simpler!
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s also nice about character-based encoding is that punctuation characters
    are also included, so line breaks, etc., can be predicted. As an example, when
    I used an RNN that was trained on the Shakespeare corpus to predict the text following
    on from my favorite *Game of Thrones* line, I got the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'YGRITTE:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You know nothing, Jon Snow.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Good night, we’ll prove those body’s servants to
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The traitor be these mine:*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*So diswarl his body in hope in this resceins,*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: I cannot judg appeal’t.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MENENIUS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why, ’tis pompetsion.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KING RICHARD II:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I think he make her thought on mine;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'She will not: suffer up thy bonds:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How doched it, I pray the gott,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll no fame to this your love, and you were ends
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s kind of cool that she identifies him as a traitor and wants to tie him
    up (“diswarl his body”), but I have no idea what “resceins” means! If you watch
    the show, this is part of the plot, so maybe Shakespeare was on to something without
    realizing it!
  prefs: []
  type: TYPE_NORMAL
- en: Of course, I do think we tend to be a little more forgiving when using something
    like Shakespeare’s texts as our training data, because the language is already
    a little unfamiliar.
  prefs: []
  type: TYPE_NORMAL
- en: As with the Irish songs model, the output from the Shakespeare dataset does
    quickly degenerate into nonsensical text, but it’s still fun to play with. To
    try it for yourself, you can check out the [Colab](https://oreil.ly/cbz9c). This
    Colab is TensorFlow based, not PyTorch based. [See the GitHub repository](https://oreil.ly/kQ7aa)
    for a similar example that gives different results but is PyTorch based.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how to do basic text generation using a trained
    LSTM-based model. You learned how you can split text into training features and
    labels by using words as labels, and you also learned how create a model that,
    when given seed text, can predict the next likely word. Then, you iterated on
    this to improve the model for better results by exploring a dataset of traditional
    Irish songs. Hopefully, this was a fun introduction to how ML models can synthesize
    text and also gave you the knowledge you need to understand the foundational principles
    of generative AI. This approach was massively improved with the transformer architecture
    that underpins how LLM models like GPT and Gemini work!
  prefs: []
  type: TYPE_NORMAL
