- en: Chapter 8\. Using ML to Create Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章\. 使用机器学习创建文本
- en: With the release of ChatGPT in 2022, the words *generative AI* entered the common
    lexicon. This simple application that allowed you to chat with a cloud-based AI
    seemed almost miraculous in how it could answer your queries with knowledge of
    almost everything in human experience. It worked by using a very advanced evolution
    beyond the recurrent neural networks you saw in the last chapter, by using a technique
    called *transformers*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年ChatGPT的发布，*生成式AI*这个词进入了公众的词汇。这个简单的应用程序允许你与云端的AI聊天，它几乎神奇地回答你的查询，几乎涵盖了人类经验的各个方面。它是通过使用比上一章中看到的循环神经网络更高级的进化，通过使用一种称为*转换器*的技术来工作的。
- en: A *transformer* learns the patterns that turn one piece of text into another.
    With a large enough transformer architecture and a large enough set of text to
    learn from, the GPT model (GPT stands for generative pretrained transformers)
    could predict the next tokens to follow a piece of text. When GPT was wrapped
    in an application that made it more user friendly, a whole new industry was born.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*转换器*学习将一段文本转换成另一段文本的模式。有了足够大的转换器架构和足够大的学习文本集，GPT模型（GPT代表生成预训练转换器）可以预测跟随一段文本的下一个标记。当GPT被包裹在一个更用户友好的应用程序中时，一个全新的行业就诞生了。
- en: While creating models with transformers is beyond the scope of this book, we
    will look at their architecture in detail in [Chapter 15](ch15.html#ch15_transformers_and_transformers_1748549808974580).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用转换器创建模型超出了本书的范围，但我们将详细探讨其架构，见[第15章](ch15.html#ch15_transformers_and_transformers_1748549808974580)。
- en: The principles involved in training models with transformers can be replicated
    with smaller, simpler, architectures like RNNs or LSTM. We’ll explore that in
    this chapter and with a much smaller corpus of text—traditional Irish songs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用转换器训练模型涉及的原则可以用较小的、更简单的架构如RNN或LSTM来复制。我们将在本章中探讨这一点，并使用一个更小的文本语料库——传统的爱尔兰歌曲。
- en: 'So, for example, consider this line of text from a famous TV show:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下来自著名电视剧的文本行：
- en: You know nothing, Jon Snow.
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你什么都不知道，琼·斯诺。
- en: 'A next-token-predictor model, created with RNNs, came up with these song lyrics
    in response:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RNN创建的下一个标记预测模型产生了以下歌词作为回应：
- en: You know nothing, Jon Snow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你什么都不知道，琼·斯诺
- en: the place where he’s stationed
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他驻扎的地方
- en: be it Cork or in the blue bird’s son
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不论是在科克还是在蓝鸟之子
- en: sailed out to summer
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驶向夏日
- en: old sweet long and gladness rings
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旧时的甜蜜，长久而快乐的铃声
- en: so I’ll wait for the wild Colleen dying
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我会等待野性的科琳死亡
- en: This text was generated by a very simple model that was trained on a small corpus.
    I’ve enhanced it a little by adding line breaks and punctuation, but other than
    the first line, all of the lyrics were generated by the model you’ll learn how
    to build in this chapter. It’s kind of cool that it mentions a *wild Colleen dying*—if
    you’ve watched the show that Jon Snow comes from, you’ll understand why!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这段文本是由一个训练在小型语料库上的非常简单的模型生成的。我稍微增强了一下，添加了行断和标点符号，但除了第一行之外，所有歌词都是由你将在本章学习的模型生成的。它提到一个*野性的科琳死亡*——如果你看过琼·斯诺来自的节目，你就会明白为什么了！
- en: In the last few chapters, you saw how you can use PyTorch with text-based data—first
    tokenizing it into numbers and sequences that can be processed by a neural network,
    then using embeddings to simulate sentiment using vectors, and finally using deep
    and recurrent neural networks to classify text. We used the sarcasm dataset, a
    small and simple one, to illustrate how all this works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，你看到了如何使用PyTorch处理基于文本的数据——首先将其标记化为数字和序列，这些可以被神经网络处理，然后使用嵌入向量模拟情感，最后使用深度和循环神经网络对文本进行分类。我们使用了讽刺数据集，一个小型且简单的数据集，来展示这一切是如何工作的。
- en: 'In this chapter we’re going to shift gears: instead of classifying existing
    text, you’ll create a neural network that can *predict* text and thus *generate*
    text.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将转换方向：不是对现有文本进行分类，而是创建一个可以*预测*文本的神经网络，从而*生成*文本。
- en: Given a corpus of text, the network will attempt to learn and understand the
    *patterns* of words within the text so that it can, given a new piece of text
    called a *seed*, predict what word should come next. Once the network has that,
    the seed and the predicted word become the new seed, and it can predict the next
    word. Thus, when trained on a corpus of text, a neural network can attempt to
    write new text in a similar style. To create the preceding piece of poetry, I
    collected lyrics from a number of traditional Irish songs, trained a neural network
    with them, and used it to predict words.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文本语料库，网络将尝试学习和理解文本中单词的*模式*，以便在给定一个称为*种子*的新文本时，可以预测下一个单词。一旦网络有了这个，种子和预测的单词就变成了新的种子，它可以预测下一个单词。因此，当在一个文本语料库上训练时，一个神经网络可以尝试以类似风格编写新的文本。为了创建前面的诗歌，我收集了一些传统的爱尔兰歌曲的歌词，用它们训练了一个神经网络，并使用它来预测单词。
- en: We’ll start simple, using a small amount of text to illustrate how to build
    up to a predictive model, and we’ll end by creating a full model with a lot more
    text. After that, you can try it out to see what kind of poetry it can create!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从简单开始，使用少量文本来展示如何构建一个预测模型，最后我们将创建一个包含更多文本的完整模型。之后，你可以尝试一下，看看它能创造出什么样的诗歌！
- en: To get started, you’ll have to treat the text a little differently from how
    you’ve been treating it thus far. In the previous chapters, you took sentences
    and turned them into sequences that were then classified based on the embeddings
    for the tokens within them. But when it comes to creating data that can be used
    to train a predictive model like this one, there’s an additional step in which
    you need to transform the sequences into *input sequences* and *labels*, where
    the input sequence is a group of words and the label is the next word in the sentence.
    You can then train a model to match the input sequences to their labels, so that
    future predictions can pick a label that’s close to the input sequence.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，你将不得不稍微改变一下对待文本的方式，与之前相比。在前面的章节中，你将句子转换为序列，然后根据其中标记的嵌入进行分类。但是，当涉及到创建可以用来训练此类预测模型的训练数据时，你需要一个额外的步骤，在这个步骤中，你需要将序列转换为*输入序列*和*标签*，其中输入序列是一组单词，标签是句子中的下一个单词。然后你可以训练一个模型来匹配输入序列和它们的标签，以便未来的预测可以选择一个接近输入序列的标签。
- en: Turning Sequences into Input Sequences
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将序列转换为输入序列
- en: When predicting text, you need to train a neural network with an input sequence
    (feature) that has an associated label. Matching sequences to labels is the key
    to predicting text. In this case, you won’t have explicit labels like you do when
    you are classifying, but instead, you’ll split the sentence, and for a block of
    *n* words, the next word in the sentence will be the label.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测文本时，你需要用一个具有相关标签的输入序列（特征）来训练一个神经网络。匹配序列到标签是预测文本的关键。在这种情况下，你不会有像分类时那样的明确标签，而是将句子拆分，对于*n*个单词的块，句子中的下一个单词将是标签。
- en: So, for example, if in your corpus you had the sentence “Today has a beautiful
    blue sky,” then you could split it into “Today has a beautiful blue” as the feature
    and “sky” as the label. Then, if you were to get a prediction for the text “Today
    has a beautiful blue,” it would likely be “sky.” If, in the training data, you
    also had “Yesterday had a beautiful blue sky,” you would split it in the same
    way, and if you were to get a prediction for the text “Tomorrow will have a beautiful
    blue,” then there’s a high probability that the next word would be “sky.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你的语料库中有这样的句子“今天有一个美丽的蓝天”，那么你可以将其拆分为“今天有一个美丽的蓝天”作为特征，“sky”作为标签。然后，如果你要对文本“今天有一个美丽的蓝天”进行预测，它很可能是“sky”。如果在训练数据中，你也有“昨天有一个美丽的蓝天”，你将以相同的方式拆分它，如果你要对文本“明天将有一个美丽的蓝天”进行预测，那么下一个词很可能是“sky”。
- en: If you train a network with lots of sentences, where you remove the last word
    and make it the label, you can quickly build up a predictive model in which the
    most likely next word in the sentence can be predicted from an existing body of
    text.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用一个包含许多句子的网络进行训练，其中你移除最后一个单词并将其作为标签，你可以快速构建一个预测模型，其中可以从现有的文本体中预测句子中最可能的下一个单词。
- en: 'We’ll start with a very small corpus of text—an excerpt from a traditional
    Irish song from the 1860s, some of the lyrics of which are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个非常小的文本语料库开始——一首来自19世纪60年代的传统爱尔兰歌曲的摘录，其中的一些歌词如下：
- en: In the town of Athy one Jeremy Lanigan
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在阿斯镇，有一个杰里米·兰尼根
- en: Battered away ’til he hadn’t a pound.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 破碎至一文不值。
- en: His father died and made him a man again
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他的父亲去世，使他再次成为一个男人
- en: Left him a farm and ten acres of ground.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 留给他一个农场和十英亩的土地。
- en: He gave a grand party for friends and relations
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他为朋友和亲戚举办了一场盛大的聚会
- en: Who didn’t forget him when come to the wall,
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 她在墙角时没有忘记他，
- en: And if you’ll but listen I’ll make your eyes glisten
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你愿意听，我会让你的眼睛闪闪发光
- en: Of the rows and the ructions of Lanigan’s Ball.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于兰尼根舞会的行列和骚动。
- en: Myself to be sure got free invitation,
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我自己当然得到了免费的邀请，
- en: For all the nice girls and boys I might ask,
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于我可能邀请的所有好女孩和男孩，
- en: And just in a minute both friends and relations
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就在几分钟内，朋友和亲戚们
- en: Were dancing round merry as bees round a cask.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们像蜜蜂围桶一样快乐地跳舞。
- en: Judy O’Daly, that nice little milliner,
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱迪·奥达利，那位可爱的小帽匠，
- en: She tipped me a wink for to give her a call,
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 她向我使了个眼色，让我给她打电话，
- en: And I soon arrived with Peggy McGilligan
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我很快就和佩吉·麦基根到了那里
- en: Just in time for Lanigan’s Ball.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正好赶上兰尼根的舞会。
- en: 'You’ll want to create a single string with all the text and set that to be
    your data. Use \n for the line breaks. Then, this corpus can be easily loaded
    and tokenized. First, the tokenize function will split the text into individual
    words, and then the `create_word_dictionary` will create a dictionary with an
    index for each individual word in the text:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要创建一个包含所有文本的单个字符串，并将其设置为你的数据。使用 \n 来表示行中断。然后，这个语料库可以轻松地加载和分词。首先，分词函数会将文本分割成单个单词，然后
    `create_word_dictionary` 函数将为文本中的每个单词创建一个索引字典：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that this is a very simplistic approach for the purpose of learning how
    these work. In production systems, you’d likely either use off-the-shelf components
    that have been built for scale or greatly enhance them for scale and exception
    checking.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是一个非常简单的学习方法，用于学习这些方法的工作原理。在生产系统中，你可能会使用现成的、为扩展而构建的组件，或者极大地增强它们以实现扩展和异常检查。
- en: 'With these functions, you can then create a `word_index` of the simple corpus,
    like this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些函数，你可以创建一个简单语料库的 `word_index`，如下所示：
- en: '[PRE1] `tokens` `=` `tokenize``(``data``)` `word_index` `=` `create_word_dictionary``(``tokens``)`  `total_words`
    `=` `len``(``tokenizer``.``word_index``)` `+` `1` [PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1] `tokens` `=` `tokenize``(``data``)` `word_index` `=` `create_word_dictionary``(``tokens``)`  `total_words`
    `=` `len``(``tokenizer``.``word_index``)` `+` `1` [PRE2]'
- en: '[PRE3]` The result of this process is to replace the words with their token
    values (see [Figure 8-1](#ch08_figure_1_1748549671837881)).  ![](assets/aiml_0801.png)  ######
    Figure 8-1\. Tokenizing a sentence    To train a predictive model, we should take
    a further step here: splitting the sentence into multiple smaller sequences so,
    for example, we can have one sequence consisting of the first two tokens, another
    consisting of the first three, etc. We would then pad these out to be the same
    length as the input sequence by prepending zeros (see [Figure 8-2](#ch08_figure_2_1748549671837934)).  ![](assets/aiml_0802.png)  ######
    Figure 8-2\. Turning a sequence into a number of input sequences    To do this,
    you’ll need to go through each line in the corpus and turn it into a list of tokens,
    using functions to convert the text words into an array of their lookup values
    in the word dictionary, and then to create the padded versions of the subsequences.
    To assist you with this task, I’ve provided these functions: `text_to_sequence`
    and `pad_sequence`.    [PRE4]    You can then create the input sequences using
    these helper functions like this:    [PRE5]    Finally, once you have a set of
    padded input sequences, you can split them into features and labels, where each
    label is simply the last token in each input sequence (see [Figure 8-3](#ch08_figure_3_1748549671837963)).  ![](assets/aiml_0803.png)  ######
    Figure 8-3\. Turning the padded sequences into features (x) and labels (y)    When
    you’re training a neural network, you’re going to match each feature to its corresponding
    label. So, for example, the label for [0 0 0 0 0 0 1] will be [2].    Here’s the
    code you use to separate the labels from the input sequences:    [PRE6]    Next,
    you need to encode the labels. Right now, they’re just tokens—for example, the
    number 2 at the top of [Figure 8-3](#ch08_figure_3_1748549671837963) is a token.
    But if you want to use a token as a label in a classifier, you’ll have to map
    it to an output neuron. Thus, if you’re going to classify *n* words, with each
    word being a class, you’ll need to have *n* neurons. Here’s where it’s important
    to control the size of the vocabulary, because the more words you have, the more
    classes you’ll need. Remember back in Chapter [2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    and [3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912),
    when you were classifying fashion items with the Fashion MNIST dataset and you
    had 10 types of items of clothing? That required you to have 10 neurons in the
    output layer—but in this case, what if you want to predict up to 10,000 vocabulary
    words? You’ll need an output layer with 10,000 neurons!    Additionally, you need
    to one-hot encode your labels so that they match the desired output from a neural
    network. Consider [Figure 8-3](#ch08_figure_3_1748549671837963) again. If a neural
    network is fed the input *x* consisting of a series of 0s followed by a 1, you’ll
    want the prediction to be 2—but how the network delivers that is by having an
    output layer of `vocabulary_size` neurons, where the second one has the highest
    probability.    To encode your labels into a set of *Y*s that you can then use
    to train, you can use this code:    [PRE7] `# Create and return one-hot encoded
    list`     `encoded` `=` `[``0``]` `*` `corpus_size`     `encoded``[``value``]`
    `=` `1`     `return` `encoded` [PRE8]   `` `Note that there are many libraries
    and helper functions out there that can do this for you, so feel free to use them
    instead of the simple code in this chapter. But I want to put these methodologies
    in here, so you can see how it works under the hood.    You can see this visually
    in [Figure 8-4](#ch08_figure_4_1748549671837985).  ![](assets/aiml_0804.png)  ######
    Figure 8-4\. One-hot encoding labels    This is a very sparse representation that,
    if you have a lot of training data and a lot of potential words, will eat memory
    very quickly! Suppose you had 100,000 training sentences, with a vocabulary of
    10,000 words—you’d need 1,000,000,000 bytes just to hold the labels! But that’s
    the way we have to design our network if we’re going to classify and predict words.`
    `` [PRE9]``  [PRE10] [PRE11]`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]` 这个过程的结果是将单词替换为它们的标记值（见[图8-1](#ch08_figure_1_1748549671837881)）。  ![](assets/aiml_0801.png)  ######
    图8-1\. 将句子标记化    要训练一个预测模型，我们在这里需要进一步操作：将句子分割成多个更小的序列，例如，我们可以有一个包含前两个标记的序列，另一个包含前三个标记的序列，等等。然后，我们将这些序列填充到与输入序列相同的长度，通过在前面添加零来实现（见[图8-2](#ch08_figure_2_1748549671837934)）。  ![](assets/aiml_0802.png)  ######
    图8-2\. 将序列转换为多个输入序列    要完成这个任务，你需要遍历语料库中的每一行，将其转换为标记列表，使用函数将文本单词转换为单词字典中它们的查找值数组，然后创建子序列的填充版本。为了帮助你完成这项任务，我提供了这些函数：`text_to_sequence`和`pad_sequence`。    [PRE4]    然后，你可以使用这些辅助函数创建输入序列，如下所示：    [PRE5]    最后，一旦你有一组填充的输入序列，你可以将它们分割成特征和标签，其中每个标签只是每个输入序列中的最后一个标记（见[图8-3](#ch08_figure_3_1748549671837963)）。  ![](assets/aiml_0803.png)  ######
    图8-3\. 将填充的序列转换为特征（x）和标签（y）    当你训练一个神经网络时，你需要将每个特征与其对应的标签匹配。例如，对于[0 0 0 0 0 0
    1]，其标签将是[2]。    这是用来从输入序列中分离标签的代码：    [PRE6]    接下来，你需要对标签进行编码。目前，它们只是标记——例如，[图8-3](#ch08_figure_3_1748549671837963)顶部的数字2是一个标记。但如果你想在分类器中使用标记作为标签，你必须将其映射到一个输出神经元。因此，如果你要分类*n*个单词，每个单词都是一个类别，你需要有*n*个神经元。这就是为什么控制词汇表的大小很重要，因为单词越多，你需要的类别就越多。回想一下第[2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)章和[3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)章，当时你使用Fashion
    MNIST数据集对时尚物品进行分类，你有10种类型的服装？这需要你在输出层有10个神经元——但在这个情况下，如果你想要预测多达10,000个词汇表单词怎么办？你需要一个有10,000个神经元的输出层！    此外，你需要对标签进行独热编码，以便它们与神经网络期望的输出相匹配。再次考虑[图8-3](#ch08_figure_3_1748549671837963)。如果一个神经网络被输入*x*，它由一系列0后面跟着一个1组成，你希望预测结果是2——但网络如何实现这一点是通过有一个`vocabulary_size`个神经元的输出层，其中第二个神经元的概率最高。    要将标签编码到一组你可以用来训练的*Y*中，你可以使用以下代码：    [PRE7]
    `# 创建并返回独热编码列表`     `encoded` `=` `[``0``]` `*` `corpus_size`     `encoded``[``value``]`
    `=` `1`     `return` `encoded` [PRE8]   `` `请注意，有许多库和辅助函数可以为你完成这项工作，所以你可以自由使用它们而不是本章中的简单代码。但我想要在这里展示这些方法，这样你就可以看到它们是如何在底层工作的。    你可以在[图8-4](#ch08_figure_4_1748549671837985)中直观地看到这一点。  ![](assets/aiml_0804.png)  ######
    图8-4\. 独热编码标签    这是一个非常稀疏的表示，如果你有很多训练数据和潜在的单词，它会迅速消耗内存！假设你有100,000个训练句子，词汇量为10,000个单词——你只需要1,000,000,000字节来存储标签！但如果我们想要对单词进行分类和预测，我们必须这样设计我们的网络。`
    `` [PRE9]``  [PRE10] [PRE11]`'
