- en: Chapter 8\. Using ML to Create Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the release of ChatGPT in 2022, the words *generative AI* entered the common
    lexicon. This simple application that allowed you to chat with a cloud-based AI
    seemed almost miraculous in how it could answer your queries with knowledge of
    almost everything in human experience. It worked by using a very advanced evolution
    beyond the recurrent neural networks you saw in the last chapter, by using a technique
    called *transformers*.
  prefs: []
  type: TYPE_NORMAL
- en: A *transformer* learns the patterns that turn one piece of text into another.
    With a large enough transformer architecture and a large enough set of text to
    learn from, the GPT model (GPT stands for generative pretrained transformers)
    could predict the next tokens to follow a piece of text. When GPT was wrapped
    in an application that made it more user friendly, a whole new industry was born.
  prefs: []
  type: TYPE_NORMAL
- en: While creating models with transformers is beyond the scope of this book, we
    will look at their architecture in detail in [Chapter 15](ch15.html#ch15_transformers_and_transformers_1748549808974580).
  prefs: []
  type: TYPE_NORMAL
- en: The principles involved in training models with transformers can be replicated
    with smaller, simpler, architectures like RNNs or LSTM. We’ll explore that in
    this chapter and with a much smaller corpus of text—traditional Irish songs.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, consider this line of text from a famous TV show:'
  prefs: []
  type: TYPE_NORMAL
- en: You know nothing, Jon Snow.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A next-token-predictor model, created with RNNs, came up with these song lyrics
    in response:'
  prefs: []
  type: TYPE_NORMAL
- en: You know nothing, Jon Snow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the place where he’s stationed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: be it Cork or in the blue bird’s son
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sailed out to summer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: old sweet long and gladness rings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: so I’ll wait for the wild Colleen dying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This text was generated by a very simple model that was trained on a small corpus.
    I’ve enhanced it a little by adding line breaks and punctuation, but other than
    the first line, all of the lyrics were generated by the model you’ll learn how
    to build in this chapter. It’s kind of cool that it mentions a *wild Colleen dying*—if
    you’ve watched the show that Jon Snow comes from, you’ll understand why!
  prefs: []
  type: TYPE_NORMAL
- en: In the last few chapters, you saw how you can use PyTorch with text-based data—first
    tokenizing it into numbers and sequences that can be processed by a neural network,
    then using embeddings to simulate sentiment using vectors, and finally using deep
    and recurrent neural networks to classify text. We used the sarcasm dataset, a
    small and simple one, to illustrate how all this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we’re going to shift gears: instead of classifying existing
    text, you’ll create a neural network that can *predict* text and thus *generate*
    text.'
  prefs: []
  type: TYPE_NORMAL
- en: Given a corpus of text, the network will attempt to learn and understand the
    *patterns* of words within the text so that it can, given a new piece of text
    called a *seed*, predict what word should come next. Once the network has that,
    the seed and the predicted word become the new seed, and it can predict the next
    word. Thus, when trained on a corpus of text, a neural network can attempt to
    write new text in a similar style. To create the preceding piece of poetry, I
    collected lyrics from a number of traditional Irish songs, trained a neural network
    with them, and used it to predict words.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start simple, using a small amount of text to illustrate how to build
    up to a predictive model, and we’ll end by creating a full model with a lot more
    text. After that, you can try it out to see what kind of poetry it can create!
  prefs: []
  type: TYPE_NORMAL
- en: To get started, you’ll have to treat the text a little differently from how
    you’ve been treating it thus far. In the previous chapters, you took sentences
    and turned them into sequences that were then classified based on the embeddings
    for the tokens within them. But when it comes to creating data that can be used
    to train a predictive model like this one, there’s an additional step in which
    you need to transform the sequences into *input sequences* and *labels*, where
    the input sequence is a group of words and the label is the next word in the sentence.
    You can then train a model to match the input sequences to their labels, so that
    future predictions can pick a label that’s close to the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Turning Sequences into Input Sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When predicting text, you need to train a neural network with an input sequence
    (feature) that has an associated label. Matching sequences to labels is the key
    to predicting text. In this case, you won’t have explicit labels like you do when
    you are classifying, but instead, you’ll split the sentence, and for a block of
    *n* words, the next word in the sentence will be the label.
  prefs: []
  type: TYPE_NORMAL
- en: So, for example, if in your corpus you had the sentence “Today has a beautiful
    blue sky,” then you could split it into “Today has a beautiful blue” as the feature
    and “sky” as the label. Then, if you were to get a prediction for the text “Today
    has a beautiful blue,” it would likely be “sky.” If, in the training data, you
    also had “Yesterday had a beautiful blue sky,” you would split it in the same
    way, and if you were to get a prediction for the text “Tomorrow will have a beautiful
    blue,” then there’s a high probability that the next word would be “sky.”
  prefs: []
  type: TYPE_NORMAL
- en: If you train a network with lots of sentences, where you remove the last word
    and make it the label, you can quickly build up a predictive model in which the
    most likely next word in the sentence can be predicted from an existing body of
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with a very small corpus of text—an excerpt from a traditional
    Irish song from the 1860s, some of the lyrics of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the town of Athy one Jeremy Lanigan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Battered away ’til he hadn’t a pound.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: His father died and made him a man again
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left him a farm and ten acres of ground.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He gave a grand party for friends and relations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who didn’t forget him when come to the wall,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And if you’ll but listen I’ll make your eyes glisten
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of the rows and the ructions of Lanigan’s Ball.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Myself to be sure got free invitation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all the nice girls and boys I might ask,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And just in a minute both friends and relations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Were dancing round merry as bees round a cask.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Judy O’Daly, that nice little milliner,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: She tipped me a wink for to give her a call,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And I soon arrived with Peggy McGilligan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just in time for Lanigan’s Ball.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You’ll want to create a single string with all the text and set that to be
    your data. Use \n for the line breaks. Then, this corpus can be easily loaded
    and tokenized. First, the tokenize function will split the text into individual
    words, and then the `create_word_dictionary` will create a dictionary with an
    index for each individual word in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is a very simplistic approach for the purpose of learning how
    these work. In production systems, you’d likely either use off-the-shelf components
    that have been built for scale or greatly enhance them for scale and exception
    checking.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these functions, you can then create a `word_index` of the simple corpus,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1] `tokens` `=` `tokenize``(``data``)` `word_index` `=` `create_word_dictionary``(``tokens``)`  `total_words`
    `=` `len``(``tokenizer``.``word_index``)` `+` `1` [PRE2]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]` The result of this process is to replace the words with their token
    values (see [Figure 8-1](#ch08_figure_1_1748549671837881)).  ![](assets/aiml_0801.png)  ######
    Figure 8-1\. Tokenizing a sentence    To train a predictive model, we should take
    a further step here: splitting the sentence into multiple smaller sequences so,
    for example, we can have one sequence consisting of the first two tokens, another
    consisting of the first three, etc. We would then pad these out to be the same
    length as the input sequence by prepending zeros (see [Figure 8-2](#ch08_figure_2_1748549671837934)).  ![](assets/aiml_0802.png)  ######
    Figure 8-2\. Turning a sequence into a number of input sequences    To do this,
    you’ll need to go through each line in the corpus and turn it into a list of tokens,
    using functions to convert the text words into an array of their lookup values
    in the word dictionary, and then to create the padded versions of the subsequences.
    To assist you with this task, I’ve provided these functions: `text_to_sequence`
    and `pad_sequence`.    [PRE4]    You can then create the input sequences using
    these helper functions like this:    [PRE5]    Finally, once you have a set of
    padded input sequences, you can split them into features and labels, where each
    label is simply the last token in each input sequence (see [Figure 8-3](#ch08_figure_3_1748549671837963)).  ![](assets/aiml_0803.png)  ######
    Figure 8-3\. Turning the padded sequences into features (x) and labels (y)    When
    you’re training a neural network, you’re going to match each feature to its corresponding
    label. So, for example, the label for [0 0 0 0 0 0 1] will be [2].    Here’s the
    code you use to separate the labels from the input sequences:    [PRE6]    Next,
    you need to encode the labels. Right now, they’re just tokens—for example, the
    number 2 at the top of [Figure 8-3](#ch08_figure_3_1748549671837963) is a token.
    But if you want to use a token as a label in a classifier, you’ll have to map
    it to an output neuron. Thus, if you’re going to classify *n* words, with each
    word being a class, you’ll need to have *n* neurons. Here’s where it’s important
    to control the size of the vocabulary, because the more words you have, the more
    classes you’ll need. Remember back in Chapter [2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    and [3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912),
    when you were classifying fashion items with the Fashion MNIST dataset and you
    had 10 types of items of clothing? That required you to have 10 neurons in the
    output layer—but in this case, what if you want to predict up to 10,000 vocabulary
    words? You’ll need an output layer with 10,000 neurons!    Additionally, you need
    to one-hot encode your labels so that they match the desired output from a neural
    network. Consider [Figure 8-3](#ch08_figure_3_1748549671837963) again. If a neural
    network is fed the input *x* consisting of a series of 0s followed by a 1, you’ll
    want the prediction to be 2—but how the network delivers that is by having an
    output layer of `vocabulary_size` neurons, where the second one has the highest
    probability.    To encode your labels into a set of *Y*s that you can then use
    to train, you can use this code:    [PRE7] `# Create and return one-hot encoded
    list`     `encoded` `=` `[``0``]` `*` `corpus_size`     `encoded``[``value``]`
    `=` `1`     `return` `encoded` [PRE8]   `` `Note that there are many libraries
    and helper functions out there that can do this for you, so feel free to use them
    instead of the simple code in this chapter. But I want to put these methodologies
    in here, so you can see how it works under the hood.    You can see this visually
    in [Figure 8-4](#ch08_figure_4_1748549671837985).  ![](assets/aiml_0804.png)  ######
    Figure 8-4\. One-hot encoding labels    This is a very sparse representation that,
    if you have a lot of training data and a lot of potential words, will eat memory
    very quickly! Suppose you had 100,000 training sentences, with a vocabulary of
    10,000 words—you’d need 1,000,000,000 bytes just to hold the labels! But that’s
    the way we have to design our network if we’re going to classify and predict words.`
    `` [PRE9]``  [PRE10] [PRE11]`'
  prefs: []
  type: TYPE_NORMAL
