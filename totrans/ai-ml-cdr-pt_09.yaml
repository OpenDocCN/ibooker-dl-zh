- en: Chapter 8\. Using ML to Create Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章\. 使用机器学习创建文本
- en: With the release of ChatGPT in 2022, the words *generative AI* entered the common
    lexicon. This simple application that allowed you to chat with a cloud-based AI
    seemed almost miraculous in how it could answer your queries with knowledge of
    almost everything in human experience. It worked by using a very advanced evolution
    beyond the recurrent neural networks you saw in the last chapter, by using a technique
    called *transformers*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着2022年ChatGPT的发布，*生成式AI*这个词进入了公众的词汇表。这个简单的应用允许你与基于云的AI进行聊天，它以几乎神奇的方式回答你的查询，几乎涵盖了人类经验的各个方面。它是通过使用比上一章中看到的循环神经网络更高级的进化，通过使用一种称为*变换器*的技术来工作的。
- en: A *transformer* learns the patterns that turn one piece of text into another.
    With a large enough transformer architecture and a large enough set of text to
    learn from, the GPT model (GPT stands for generative pretrained transformers)
    could predict the next tokens to follow a piece of text. When GPT was wrapped
    in an application that made it more user friendly, a whole new industry was born.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*变换器*学习将一段文本转换成另一段文本的模式。有了足够大的变换器架构和足够大的学习文本集，GPT模型（GPT代表生成式预训练变换器）可以预测跟随一段文本的下一个标记。当GPT被包裹在一个使其更用户友好的应用程序中时，一个全新的行业就诞生了。
- en: While creating models with transformers is beyond the scope of this book, we
    will look at their architecture in detail in [Chapter 15](ch15.html#ch15_transformers_and_transformers_1748549808974580).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用变换器创建模型超出了本书的范围，但我们将详细探讨其架构，见[第15章](ch15.html#ch15_transformers_and_transformers_1748549808974580)。
- en: The principles involved in training models with transformers can be replicated
    with smaller, simpler, architectures like RNNs or LSTM. We’ll explore that in
    this chapter and with a much smaller corpus of text—traditional Irish songs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变换器训练模型涉及的原则可以用较小的、更简单的架构如RNN或LSTM来复制。我们将在本章中探讨这一点，并使用一个更小的文本语料库——传统的爱尔兰歌曲。
- en: 'So, for example, consider this line of text from a famous TV show:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下来自著名电视剧的文本行：
- en: You know nothing, Jon Snow.
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你什么都不知道，琼·斯诺。
- en: 'A next-token-predictor model, created with RNNs, came up with these song lyrics
    in response:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个使用RNN创建的下一个标记预测模型产生了以下歌词作为回应：
- en: You know nothing, Jon Snow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你什么都不知道，琼·斯诺
- en: the place where he’s stationed
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他驻足的地方
- en: be it Cork or in the blue bird’s son
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不论是科克还是在蓝鸟之子
- en: sailed out to summer
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帆船驶向夏日
- en: old sweet long and gladness rings
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旧日的甜蜜、长久和快乐的声音
- en: so I’ll wait for the wild Colleen dying
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我将等待那野性的科琳死去
- en: This text was generated by a very simple model that was trained on a small corpus.
    I’ve enhanced it a little by adding line breaks and punctuation, but other than
    the first line, all of the lyrics were generated by the model you’ll learn how
    to build in this chapter. It’s kind of cool that it mentions a *wild Colleen dying*—if
    you’ve watched the show that Jon Snow comes from, you’ll understand why!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这段文本是由一个非常简单的模型生成的，该模型在一个小型语料库上进行了训练。我稍微增强了一下，添加了行断和标点符号，但除了第一行之外，所有歌词都是由你将在本章中学习如何构建的模型生成的。它提到一个*野性的科琳死去*，如果你看过琼·斯诺来自的电视剧，你就会明白为什么了！
- en: In the last few chapters, you saw how you can use PyTorch with text-based data—first
    tokenizing it into numbers and sequences that can be processed by a neural network,
    then using embeddings to simulate sentiment using vectors, and finally using deep
    and recurrent neural networks to classify text. We used the sarcasm dataset, a
    small and simple one, to illustrate how all this works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，你看到了如何使用PyTorch处理基于文本的数据——首先将其标记化为数字和序列，这些可以被神经网络处理，然后使用嵌入来模拟情感，最后使用深度和循环神经网络来分类文本。我们使用了讽刺数据集，一个小型且简单的数据集，来展示这一切是如何工作的。
- en: 'In this chapter we’re going to shift gears: instead of classifying existing
    text, you’ll create a neural network that can *predict* text and thus *generate*
    text.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将转换方向：不再是分类现有文本，而是创建一个能够*预测*文本并因此*生成*文本的神经网络。
- en: Given a corpus of text, the network will attempt to learn and understand the
    *patterns* of words within the text so that it can, given a new piece of text
    called a *seed*, predict what word should come next. Once the network has that,
    the seed and the predicted word become the new seed, and it can predict the next
    word. Thus, when trained on a corpus of text, a neural network can attempt to
    write new text in a similar style. To create the preceding piece of poetry, I
    collected lyrics from a number of traditional Irish songs, trained a neural network
    with them, and used it to predict words.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文本语料库，网络将尝试学习和理解文本中单词的*模式*，以便在给定一个称为*种子*的新文本时，可以预测下一个单词。一旦网络有了这个，种子和预测的单词就变成了新的种子，它可以预测下一个单词。因此，当在一个文本语料库上训练时，一个神经网络可以尝试以类似风格编写新的文本。为了创建前面的诗歌，我收集了多首传统爱尔兰歌曲的歌词，用它们训练了一个神经网络，并使用它来预测单词。
- en: We’ll start simple, using a small amount of text to illustrate how to build
    up to a predictive model, and we’ll end by creating a full model with a lot more
    text. After that, you can try it out to see what kind of poetry it can create!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的例子开始，使用少量文本来说明如何构建一个预测模型，然后我们将通过创建一个包含更多文本的完整模型来结束。之后，你可以尝试它，看看它能创造出什么样的诗歌！
- en: To get started, you’ll have to treat the text a little differently from how
    you’ve been treating it thus far. In the previous chapters, you took sentences
    and turned them into sequences that were then classified based on the embeddings
    for the tokens within them. But when it comes to creating data that can be used
    to train a predictive model like this one, there’s an additional step in which
    you need to transform the sequences into *input sequences* and *labels*, where
    the input sequence is a group of words and the label is the next word in the sentence.
    You can then train a model to match the input sequences to their labels, so that
    future predictions can pick a label that’s close to the input sequence.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，你将不得不稍微改变一下迄今为止对待文本的方式。在前面的章节中，你将句子转换为序列，然后根据其中标记的嵌入进行分类。但是，当涉及到创建可以用于训练此类预测模型的数据时，你需要一个额外的步骤，在这个步骤中，你需要将序列转换为*输入序列*和*标签*，其中输入序列是一组单词，标签是句子中的下一个单词。然后，你可以训练一个模型来匹配输入序列和它们的标签，以便未来的预测可以选择一个与输入序列相近的标签。
- en: Turning Sequences into Input Sequences
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将序列转换为输入序列
- en: When predicting text, you need to train a neural network with an input sequence
    (feature) that has an associated label. Matching sequences to labels is the key
    to predicting text. In this case, you won’t have explicit labels like you do when
    you are classifying, but instead, you’ll split the sentence, and for a block of
    *n* words, the next word in the sentence will be the label.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测文本时，你需要训练一个神经网络，其输入序列（特征）与一个关联的标签相关联。将序列与标签匹配是预测文本的关键。在这种情况下，你不会像分类时那样有明确的标签，而是将句子分割，对于一组*n*个单词，句子中的下一个单词将是标签。
- en: So, for example, if in your corpus you had the sentence “Today has a beautiful
    blue sky,” then you could split it into “Today has a beautiful blue” as the feature
    and “sky” as the label. Then, if you were to get a prediction for the text “Today
    has a beautiful blue,” it would likely be “sky.” If, in the training data, you
    also had “Yesterday had a beautiful blue sky,” you would split it in the same
    way, and if you were to get a prediction for the text “Tomorrow will have a beautiful
    blue,” then there’s a high probability that the next word would be “sky.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你的语料库中有句子“今天有一个美丽的蓝天”，那么你可以将其拆分为“今天有一个美丽的蓝天”作为特征，“sky”作为标签。然后，如果你要对文本“今天有一个美丽的蓝天”进行预测，它很可能是“sky”。如果在训练数据中，你也有“昨天有一个美丽的蓝天”，你将以相同的方式拆分它，如果你要对文本“明天将会有一个美丽的蓝天”进行预测，那么下一个单词很可能是“sky”。
- en: If you train a network with lots of sentences, where you remove the last word
    and make it the label, you can quickly build up a predictive model in which the
    most likely next word in the sentence can be predicted from an existing body of
    text.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用一个包含许多句子的网络进行训练，去掉最后一个单词并将其作为标签，你可以快速构建一个预测模型，其中可以从现有的文本中预测句子中最可能的下一个单词。
- en: 'We’ll start with a very small corpus of text—an excerpt from a traditional
    Irish song from the 1860s, some of the lyrics of which are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个非常小的文本语料库开始——一首19世纪60年代的传统爱尔兰歌曲的摘录，其中的一些歌词如下：
- en: In the town of Athy one Jeremy Lanigan
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Ath城镇，有一个叫杰里米·兰尼根的人
- en: Battered away ’til he hadn’t a pound.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 破碎到他没有一磅。
- en: His father died and made him a man again
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他的父亲去世，使他再次成为一个男人
- en: Left him a farm and ten acres of ground.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 留给他一个农场和十英亩的土地。
- en: He gave a grand party for friends and relations
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他为朋友和亲戚举办了一场盛大的聚会
- en: Who didn’t forget him when come to the wall,
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当他来到墙边时，谁会忘记他，
- en: And if you’ll but listen I’ll make your eyes glisten
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你愿意听，我会让你的眼睛闪闪发光
- en: Of the rows and the ructions of Lanigan’s Ball.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 兰吉安之球的行进和动荡。
- en: Myself to be sure got free invitation,
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我自己确信得到了免费的邀请，
- en: For all the nice girls and boys I might ask,
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有漂亮的小姑娘和男孩，我可能会问，
- en: And just in a minute both friends and relations
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只需一分钟，朋友和亲戚们
- en: Were dancing round merry as bees round a cask.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们像蜜蜂围绕酒桶一样快乐地跳舞。
- en: Judy O’Daly, that nice little milliner,
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱迪·奥达利，那位可爱的小帽匠，
- en: She tipped me a wink for to give her a call,
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 她向我眨了眨眼，让我给她打电话，
- en: And I soon arrived with Peggy McGilligan
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我很快就和佩吉·麦基根一起到了
- en: Just in time for Lanigan’s Ball.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正好赶上兰吉安之球。
- en: 'You’ll want to create a single string with all the text and set that to be
    your data. Use \n for the line breaks. Then, this corpus can be easily loaded
    and tokenized. First, the tokenize function will split the text into individual
    words, and then the `create_word_dictionary` will create a dictionary with an
    index for each individual word in the text:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望创建一个包含所有文本的单个字符串，并将其设置为您的数据。使用\n作为换行符。然后，这个语料库可以轻松加载和分词。首先，分词函数将文本拆分为单个单词，然后`create_word_dictionary`将为文本中的每个单词创建一个索引的字典：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that this is a very simplistic approach for the purpose of learning how
    these work. In production systems, you’d likely either use off-the-shelf components
    that have been built for scale or greatly enhance them for scale and exception
    checking.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是一个非常简单的学习方法，目的是学习这些方法的工作原理。在生产系统中，您可能会使用现成的、为规模而构建的组件，或者极大地增强它们以实现规模和异常检查。
- en: 'With these functions, you can then create a `word_index` of the simple corpus,
    like this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些函数，您可以为简单的语料库创建一个`word_index`，如下所示：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The result of this process is to replace the words with their token values (see
    [Figure 8-1](#ch08_figure_1_1748549671837881)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的目的是用它们的标记值替换单词（见[图8-1](#ch08_figure_1_1748549671837881)）。
- en: '![](assets/aiml_0801.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0801.png)'
- en: Figure 8-1\. Tokenizing a sentence
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. 分词一个句子
- en: 'To train a predictive model, we should take a further step here: splitting
    the sentence into multiple smaller sequences so, for example, we can have one
    sequence consisting of the first two tokens, another consisting of the first three,
    etc. We would then pad these out to be the same length as the input sequence by
    prepending zeros (see [Figure 8-2](#ch08_figure_2_1748549671837934)).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个预测模型，我们在这里需要采取进一步步骤：将句子拆分为多个更小的序列，例如，我们可以有一个由前两个标记组成的序列，另一个由前三个标记组成，等等。然后，我们将这些序列填充到与输入序列相同的长度，通过在前面添加零（见[图8-2](#ch08_figure_2_1748549671837934)）。
- en: '![](assets/aiml_0802.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0802.png)'
- en: Figure 8-2\. Turning a sequence into a number of input sequences
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 将序列转换为多个输入序列
- en: 'To do this, you’ll need to go through each line in the corpus and turn it into
    a list of tokens, using functions to convert the text words into an array of their
    lookup values in the word dictionary, and then to create the padded versions of
    the subsequences. To assist you with this task, I’ve provided these functions:
    `text_to_sequence` and `pad_sequence`.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，您需要遍历语料库中的每一行，将其转换为标记列表，使用函数将文本单词转换为单词字典中的查找值数组，然后创建子序列的填充版本。为了帮助您完成这项任务，我提供了这些函数：`text_to_sequence`和`pad_sequence`。
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can then create the input sequences using these helper functions like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用这些辅助函数创建输入序列，如下所示：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, once you have a set of padded input sequences, you can split them into
    features and labels, where each label is simply the last token in each input sequence
    (see [Figure 8-3](#ch08_figure_3_1748549671837963)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦你有一组填充输入序列，你可以将它们分成特征和标签，其中每个标签只是每个输入序列中的最后一个标记（见[图8-3](#ch08_figure_3_1748549671837963)）。
- en: '![](assets/aiml_0803.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0803.png)'
- en: Figure 8-3\. Turning the padded sequences into features (x) and labels (y)
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3\. 将填充序列转换为特征（x）和标签（y）
- en: When you’re training a neural network, you’re going to match each feature to
    its corresponding label. So, for example, the label for [0 0 0 0 0 0 1] will be
    [2].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当您训练神经网络时，您将匹配每个特征与其对应的标签。例如，对于[0 0 0 0 0 0 1]，标签将是[2]。
- en: 'Here’s the code you use to separate the labels from the input sequences:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是您用来将标签从输入序列中分离的代码：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, you need to encode the labels. Right now, they’re just tokens—for example,
    the number 2 at the top of [Figure 8-3](#ch08_figure_3_1748549671837963) is a
    token. But if you want to use a token as a label in a classifier, you’ll have
    to map it to an output neuron. Thus, if you’re going to classify *n* words, with
    each word being a class, you’ll need to have *n* neurons. Here’s where it’s important
    to control the size of the vocabulary, because the more words you have, the more
    classes you’ll need. Remember back in Chapter [2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    and [3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912),
    when you were classifying fashion items with the Fashion MNIST dataset and you
    had 10 types of items of clothing? That required you to have 10 neurons in the
    output layer—but in this case, what if you want to predict up to 10,000 vocabulary
    words? You’ll need an output layer with 10,000 neurons!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要对标签进行编码。目前，它们只是标记——例如，[图8-3](#ch08_figure_3_1748549671837963)顶部的数字2就是一个标记。但如果你想在分类器中将标记用作标签，你必须将其映射到一个输出神经元上。因此，如果你要分类*n*个单词，每个单词都是一个类别，你需要有*n*个神经元。这里控制词汇表的大小就变得很重要了，因为单词越多，你需要的类别就越多。回想一下第[2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)章和[3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)章，当你使用Fashion
    MNIST数据集对时尚物品进行分类，你有10种服装类型时？这需要你在输出层有10个神经元——但在这个情况下，如果你想要预测多达10,000个词汇词呢？你需要一个有10,000个神经元的输出层！
- en: Additionally, you need to one-hot encode your labels so that they match the
    desired output from a neural network. Consider [Figure 8-3](#ch08_figure_3_1748549671837963)
    again. If a neural network is fed the input *x* consisting of a series of 0s followed
    by a 1, you’ll want the prediction to be 2—but how the network delivers that is
    by having an output layer of `vocabulary_size` neurons, where the second one has
    the highest probability.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要将你的标签进行独热编码，以便它们与神经网络期望的输出相匹配。再次考虑[图8-3](#ch08_figure_3_1748549671837963)。如果一个神经网络被输入*x*，它由一系列0后面跟着一个1组成，你希望预测结果是2——但网络如何实现这一点是通过有一个`vocabulary_size`个神经元的输出层，其中第二个神经元的概率最高。
- en: 'To encode your labels into a set of *Y*s that you can then use to train, you
    can use this code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的标签编码成一组可以用来训练的*Y*，你可以使用以下代码：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that there are many libraries and helper functions out there that can do
    this for you, so feel free to use them instead of the simple code in this chapter.
    But I want to put these methodologies in here, so you can see how it works under
    the hood.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，有许多库和辅助函数可以为你完成这项工作，所以请随意使用它们而不是本章中的简单代码。但我想要在这里介绍这些方法，这样你可以看到它们是如何在底层工作的。
- en: You can see this visually in [Figure 8-4](#ch08_figure_4_1748549671837985).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图8-4](#ch08_figure_4_1748549671837985)中直观地看到这一点。
- en: '![](assets/aiml_0804.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0804.png)'
- en: Figure 8-4\. One-hot encoding labels
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4\. 独热编码标签
- en: This is a very sparse representation that, if you have a lot of training data
    and a lot of potential words, will eat memory very quickly! Suppose you had 100,000
    training sentences, with a vocabulary of 10,000 words—you’d need 1,000,000,000
    bytes just to hold the labels! But that’s the way we have to design our network
    if we’re going to classify and predict words.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常稀疏的表示，如果你有很多训练数据和潜在的单词，它会迅速消耗内存！假设你有100,000个训练句子，词汇量为10,000个单词——你只需要1,000,000,000字节来存储标签！但如果我们想要对单词进行分类和预测，我们必须这样设计我们的网络。
- en: Creating the Model
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建模型
- en: Let’s now create a simple model that can be trained with this input data. It
    will consist of just an embedding layer, followed by an LSTM, followed by a dense
    layer.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个简单的模型，它可以使用这些输入数据进行训练。它将只包含一个嵌入层，然后是一个LSTM层，最后是一个密集层。
- en: For the embedding, you’ll need one vector per word, so the parameters will be
    the total number of words and the number of dimensions you want to embed on. In
    this case, we don’t have many words, so eight dimensions should be enough.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于嵌入，你需要每个单词一个向量，所以参数将是单词的总数和你要嵌入的维度数。在这种情况下，我们没有很多单词，所以八个维度应该足够了。
- en: You can make the LSTM bidirectional, and the number of steps can be the length
    of a sequence, which is our max length minus 1 (because we took one token off
    the end to make the label).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使LSTM双向的，步数可以是序列的长度，即我们的最大长度减1（因为我们从末尾移除了一个标记来制作标签）。
- en: 'Finally, the output layer will be a dense layer with the total number of words
    as a parameter, activated by Softmax. Each neuron in this layer will be the probability
    that the next word matches the word for that index value:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出层将是一个具有总单词数作为参数的密集层，通过Softmax激活。这个层中的每个神经元都将代表下一个单词匹配该索引值的概率：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, you compile the model with a categorical loss function such as categorical
    cross entropy and an optimizer like Adam. You can also specify that you want to
    capture metrics:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你使用分类损失函数（如分类交叉熵）和优化器（如Adam）编译模型。你也可以指定你想要捕获指标：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It’s a very simple model without a lot of data, and it trains quickly. Here
    are the results of training for about 15,000 epochs, which takes maybe 10 minutes.
    In the real world, you’ll likely be training with a lot more data and thus taking
    a lot more time, so you’ll have to consider some of the techniques we saw in [Chapter 7](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648)
    to ensure greater accuracy and potentially less time to train.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的模型，数据量不大，训练速度快。这里展示了大约15,000个epoch的训练结果，这可能需要大约10分钟。在现实世界中，你可能会使用更多的数据进行训练，因此需要更多的时间，所以你必须考虑我们在[第7章](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648)中看到的某些技术，以确保更高的准确率，并可能减少训练时间。
- en: You’ll see that it has reached very high accuracy, and there may be room for
    more (see [Figure 8-5](#ch08_figure_5_1748549671838006)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到它已经达到了非常高的准确率，并且可能还有提升的空间（见[图8-5](#ch08_figure_5_1748549671838006)）。
- en: '![](assets/aiml_0805.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0805.png)'
- en: Figure 8-5\. Training loss and accuracy
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5\. 训练损失和准确率
- en: With the model at 80%+ accuracy, we can be assured that if we have a string
    of text that it has already seen, it will predict the next word accurately about
    80% of the time.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型达到80%以上的准确率时，我们可以确信，如果我们有一个它已经看到的文本字符串，它将大约80%的时间准确预测下一个单词。
- en: Note, however, that when generating text, it will continually see words that
    it hasn’t previously seen, so despite this good number, you’ll find that the network
    will rapidly end up producing nonsensical text. We’ll explore this in the next
    section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，在生成文本时，它将不断看到它之前没有见过的单词，因此尽管这个数字很好，但你最终会发现网络会迅速生成无意义的文本。我们将在下一节中探讨这个问题。
- en: Generating Text
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成文本
- en: Now that you’ve trained a network that can predict the next word in a sequence,
    the next step is to give it a sequence of text and have it predict the next word.
    Let’s take a look at how to do that.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经训练了一个能够预测序列中下一个单词的网络，下一步就是给它一个文本序列，并让它预测下一个单词。让我们看看如何做到这一点。
- en: Predicting the Next Word
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测下一个单词
- en: You’ll start by creating a phrase called the *seed text*. This is the initial
    expression on which the network will base all the content it generates, and it
    will do this by predicting the next word.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先创建一个名为*种子文本*的短语。这是网络将基于其生成所有内容的初始表达式，它将通过预测下一个单词来实现这一点。
- en: 'Start with a phrase that the network has *already* seen, such as “in the town
    of Athy”:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络已经*看到*的短语开始，例如“在Athy镇”：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, you need to tokenize this and turn it into a sequence of tokens of the
    same length as used for training:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要对这个短语进行分词，并将其转换为与训练中使用的相同长度的标记序列：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, you need to pad that sequence to get it into the same shape as the data
    used for training by converting it to a tensor:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你需要将这个序列填充，使其与用于训练的数据具有相同的形状，通过将其转换为张量：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, you can predict the next word for this token list by passing this input
    tensor to the model to get the output. This will be a tensor that contains the
    probabilities for each of the words in the dictionary and the likelihood that
    it will be the next token.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以通过将这个输入张量传递给模型来预测这个标记列表的下一个单词。这将是一个包含字典中每个单词的概率以及它将是下一个标记的似然性的张量。
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will return the probabilities for each word in the corpus, so you should
    pass the results to `torch.argmax` to get the most likely one:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回语料库中每个单词的概率，因此你应该将结果传递给`torch.argmax`以获取最有可能的一个：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This should give you the value `6`. If you look at the word index, you’ll see
    that it’s the word “one”:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给出值`6`。如果你查看单词索引，你会看到它是单词“one”：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can also look it up in code by searching through the word index items until
    you find the predicted word and printing it out. You can do this by creating a
    reverse dictionary (mapping the value to the word, instead of vice-versa):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过在单词索引项中搜索直到找到预测的单词并打印出来，在代码中查找它。你可以通过创建一个反向字典（将值映射到单词，而不是相反）来完成此操作：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'So, starting from the text “in the town of Athy,” the network predicted that
    the next word should be “one”—which, if you look at the training data, is correct
    because the song begins with this line:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从文本“in the town of Athy”开始，网络预测下一个词应该是“one”——如果你查看训练数据，这是正确的，因为这首歌以这句开始：
- en: '*In the town of Athy* one Jeremy Lanigan'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在Ath镇的城镇*一个Jeremy Lanigan'
- en: Battered away til he hadn’t a pound
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Battered away til he hadn’t a pound
- en: 'Indeed, if you check the top five predictions based on their values in the
    index, you’ll get something like this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果你查看基于索引值的前五个预测，你会得到类似这样的结果：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that you’ve confirmed that the model is working, you can get creative and
    use different seed text. For example, when I used the seed text “sweet Jeremy
    saw Dublin,” the next word it predicted was “his.”
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经确认模型可以正常工作，你可以发挥创意并使用不同的种子文本。例如，当我使用种子文本“sweet Jeremy saw Dublin”时，它预测的下一个词是“his”。
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This text was chosen because all of those words are in the corpus. In such cases,
    you should expect more accurate results, at least at the beginning, for the predicted
    words.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这段文本被选中是因为所有这些单词都在语料库中。在这种情况下，你应该期待预测结果更准确，至少在开始时是这样。
- en: Compounding Predictions to Generate Text
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过复合预测生成文本
- en: In the previous section, you saw how to use the model to predict the next word
    given a seed text. Now, to have the neural network create new text, you simply
    repeat the prediction, adding new words each time.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你看到了如何使用模型根据种子文本预测下一个单词。现在，为了让神经网络创建新的文本，你只需重复预测，每次添加新单词。
- en: For example, earlier, when I used the phrase “sweet Jeremy saw Dublin,” it predicted
    that the next word would be “his.” You can build on this by appending “his” to
    the seed text to get “sweet Jeremy saw Dublin his” and getting another prediction.
    Repeating this process will give you an AI-created string of text.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，早些时候，当我使用短语“sweet Jeremy saw Dublin”时，它预测下一个词将是“his”。你可以通过将“his”附加到种子文本中，得到“sweet
    Jeremy saw Dublin his”，并获取另一个预测。重复这个过程将给出一个由AI创建的文本字符串。
- en: 'Here’s the updated code from the previous section that performs this loop a
    number of times, with the number set by the `num_words` parameter:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是上一节更新的代码，它根据`num_words`参数设置的数量执行这个循环：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will end up creating a string something like this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这将最终创建一个类似这样的字符串：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It rapidly descends into gibberish. Why? The first reason is that the body of
    training text is really small, so it has very little context to work with. The
    second is that the prediction of the next word in the sequence depends on the
    previous words in the sequence, and if there is a poor match on the previous ones,
    even the best “next” match will have a low probability of being accurate. When
    you add this to the sequence and predict the next word after that, the likelihood
    of it having a low probability of accuracy is even higher—thus, the predicted
    words will seem semirandom.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 它迅速陷入胡言乱语。为什么？第一个原因是训练文本的主体真的很小，所以它几乎没有上下文可以工作。第二个原因是序列中下一个单词的预测依赖于序列中的前一个单词，如果前一个匹配不佳，即使是最好的“下一个”匹配也将有很低的准确率。当你将这个添加到序列中并预测下一个单词后，它具有低准确率的可能性甚至更高——因此，预测的单词看起来像是半随机生成的。
- en: 'So, for example, while all of the words in the phrase “sweet Jeremy saw Dublin”
    exist in the corpus, they never exist in that order. When the model made the first
    prediction, it chose the word “his” as the most likely candidate, and it had quite
    a high probability of accuracy (78%):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，虽然短语“sweet Jeremy saw Dublin”中的所有单词都在语料库中，但它们从未以这种顺序存在。当模型进行第一次预测时，它选择了“his”作为最有可能的候选词，并且它有相当高的准确率（78%）：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When the model added that word to the seed to get “sweet Jeremy saw Dublin
    his,” we had another phrase not seen in the training data, so the prediction gave
    the highest probability to the word “right,” at 44%:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型将这个词添加到种子文本中，得到“sweet Jeremy saw Dublin his”时，我们得到了一个在训练数据中未出现的新短语，因此预测给出了最高概率的单词“right”，概率为44%：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: While occasionally there will be high certainty of a token following another
    (like “leg” following “right”), over time, you’ll see that continuing to add words
    to the sentence reduces the likelihood of a match in the training data, and as
    such, the prediction accuracy will suffer—leading to there being a more random
    “feel” to the words being predicted.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然偶尔会有很高的确定性，一个词紧跟着另一个词（比如“leg”跟着“right”），但随着时间的推移，你会发现继续向句子中添加单词会降低在训练数据中匹配的可能性，因此预测准确性会受到影响——导致预测的词语有更多的随机性。
- en: This leads to the phenomenon of AI-generated content getting increasingly nonsensical
    over time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致AI生成的内容随着时间的推移变得越来越不合逻辑。
- en: For an example, check out the excellent sci-fi short [*Sunspring*](https://oreil.ly/hTBtJ),
    which was written entirely by an LSTM-based network (like the one you’re building
    here) that was trained on science fiction movie scripts. The model was given seed
    content and tasked with generating a new script. The results were hilarious, and
    you’ll see that while the initial content makes sense, as the movie progresses,
    it becomes less and less comprehensible.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，查看优秀的科幻短片[*Sunspring*](https://oreil.ly/hTBtJ)，它完全由一个基于LSTM的网络（就像你在这里构建的）编写，该网络在科幻电影剧本上进行了训练。模型被提供了种子内容，并要求生成一个新的剧本。结果非常有趣，你会看到虽然初始内容是有意义的，但随着电影的进行，它变得越来越难以理解。
- en: This is also the basis of *hallucination* in LLMs, a common phenomenon that
    reduces trust in them.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是LLM中*幻觉*的基础，这是一种常见的现象，会降低人们对它们的信任。
- en: Extending the Dataset
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展数据集
- en: You can easily extend the same pattern that you used for the hardcoded dataset
    to use a text file. I’ve hosted a text file containing about 1,700 lines of text
    gathered from a number of songs that you can use for experimentation. With a little
    modification, you can use this instead of the single hardcoded song.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松地将用于硬编码数据集的相同模式扩展到使用文本文件。我已经托管了一个包含约1700行文本的文本文件，这些文本是从多首歌曲中收集的，你可以用它进行实验。稍作修改后，你可以用这个代替单个硬编码的歌曲。
- en: 'To download the data in Colab, use the following code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Colab中下载数据，请使用以下代码：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, you can simply load the text from it into your corpus like this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以简单地像这样将文本加载到你的语料库中：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The rest of your code will then work with very little modification!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你的代码将几乎不需要修改就能运行！
- en: Training this for 50,000 epochs—which takes about 30 minutes on a T4 Colab instance—brings
    you to about 30% accuracy, with the curve flattening out (see [Figure 8-6](#ch08_figure_6_1748549671838028)).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个模型50,000个epoch——在T4 Colab实例上大约需要30分钟——可以将你的准确性提高到大约30%，曲线趋于平坦（见[图8-6](#ch08_figure_6_1748549671838028)）。
- en: '![](assets/aiml_0806.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0806.png)'
- en: Figure 8-6\. Training on a larger dataset
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6\. 在更大的数据集上训练
- en: Note
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When using Google Colab, you can choose various accelerators on the backend,
    which can make your training go much faster. In this case, as noted, I used a
    T4\. To do this for yourself, when in Colab, under the Connect menu, you’ll see
    a Change Runtime Type option. Select that, and you’ll see the accelerators available
    for you to use.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Google Colab时，你可以在后端选择各种加速器，这可以使你的训练速度大大提高。在这种情况下，正如所注，我使用了T4。要自己这样做，当你在Colab中时，在“连接”菜单下，你会看到一个“更改运行时类型”选项。选择它，你将看到可供你使用的加速器。
- en: 'Trying the phrase “in the town of Athy” again yields a prediction of “one”
    but this time with just over 83% probability:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 再次尝试短语“in the town of Athy”，这次预测结果是“one”，但概率略高于83%：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Running for a few more tokens, we can see output like the following. It’s beginning
    to create songs, though it’s quite quickly descending into gibberish!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 运行几个更多的标记，我们可以看到以下输出。它开始创作歌曲，尽管很快就会陷入胡言乱语！
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For “sweet Jeremy saw Dublin,” the predicted next word is “she,” with a probability
    of 80%. Predicting the next few words yields this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“sweet Jeremy saw Dublin”，预测的下一个词是“she”，概率为80%。预测接下来的几个词得到以下结果：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It’s looking a little better! But can we improve it further?
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来好多了！但我们还能进一步改进吗？
- en: Improving the Model Architecture
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进模型架构
- en: One way that you can improve the model is to change its architecture, using
    multiple stacked LSTMs and some other optimization techniques. Given that there’s
    no clear benchmark for accuracy with a dataset like this one—there’s no right
    or wrong classification, nor is there a target regression—it’s difficult to establish
    when a model is good or bad. Therefore, accuracy results can be very subjective.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过改变模型架构，使用多个堆叠的LSTM和一些其他优化技术来提高模型。鉴于对于这样的数据集来说，没有明确的准确度基准——既没有正确的分类，也没有目标回归——很难确定模型是好是坏。因此，准确度结果可能非常主观。
- en: That being said, they’re still a good yardstick, so in this section, I’m going
    to explore some additions you can make to the architecture to improve the accuracy
    metric.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，它们仍然是一个很好的衡量标准，因此在本节中，我将探讨一些可以添加到架构中以改进准确度指标的方法。
- en: Embedding Dimensions
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入维度
- en: In [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    we discussed the fact that the optimal dimension for embeddings is the fourth
    root of the number of words. In this scenario, the vocabulary has 3,259 words,
    and the fourth root of this is approximately 8\. Another rule of thumb is the
    log of this number—and log(3259) is a little over 32\. So, if the network is learning
    slowly, you have the option to pick a number between these two values. That gives
    you enough “room” to capture the relationships between words.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中，我们讨论了嵌入的最佳维度是单词数量的四次方根。在这种情况下，词汇表有3,259个单词，其四次方根约为8。另一个经验法则是这个数字的对数——log(3259)略大于32。因此，如果网络学习缓慢，您可以选择这两个值之间的一个数字。这为您提供了足够的“空间”来捕捉单词之间的关系。
- en: Initializing the LSTMs
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化LSTM
- en: Often, parameters in a neural network are initialized to zero. You can give
    learning a little bit of a kickstart by initializing different layers to different
    types supported by various research findings. We briefly cover these types of
    layers in the following subsections.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络中的参数初始化为零。您可以通过将不同层初始化为各种研究结果支持的不同类型来给学习一点启动。我们将在以下小节中简要介绍这些类型的层。
- en: Embedding layers
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入层
- en: Embedding layers can be initialized with a normal distribution, with a standard
    deviation scaled by `1/sqrt(embedding_dim)` for better gradient flow. That’s similar
    to `word2vec`-style initialization.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层可以用正态分布初始化，标准差按`1/sqrt(embedding_dim)`缩放，以获得更好的梯度流动。这与`word2vec`风格的初始化类似。
- en: LSTM layers
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM层
- en: LSTM has four internal neural types—input, forget, cell, and output—and their
    weight matrix is a bunch of them stacked together. The different types benefit
    from different initializations. Two great papers that discuss this are [“An Empirical
    Exploration of Recurrent Network Architectures” by Rafal Jozefowicz et al.](https://oreil.ly/UuvQO)
    and [“On the Difficulty of Training Recurrent Neural Networks”](https://oreil.ly/Ttvll)
    by Razvan Pascanu et al. The specifics of LSTM are beyond the scope of this chapter,
    but check the associated code for one methodology to initialize them.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM有四种内部神经网络类型——输入、遗忘、细胞和输出——它们的权重矩阵是这些类型的堆叠。不同类型受益于不同的初始化。两篇讨论这一点的优秀论文是Rafal
    Jozefowicz等人撰写的“[对循环网络架构的经验探索]”(https://oreil.ly/UuvQO)和Razvan Pascanu等人撰写的“[训练循环神经网络的困难]”(https://oreil.ly/Ttvll)。LSTM的具体内容超出了本章的范围，但可以查看相关代码以了解初始化它们的一种方法。
- en: Final linear layer
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最终线性层
- en: In their 2015 paper “[Delving Deep into Rectifiers,” Kaiming He et al.](https://oreil.ly/_MM6A)
    explored initialization of linear layers and proposed “Kaiming” initialization
    (aka “He” initialization). A detailed explanation of this is beyond the scope
    of this book, but the code is available in the notebooks in the [GitHub repository](https://github.com/lmoroney/PyTorch-Book-FIles).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们2015年的论文“[深入探究ReLU](https://oreil.ly/_MM6A)”中，Kaiming He等人探讨了线性层的初始化，并提出了“Kaiming”初始化（也称为“He”初始化）。关于这一点的详细解释超出了本书的范围，但代码可以在[GitHub仓库](https://github.com/lmoroney/PyTorch-Book-FIles)中的笔记本中找到。
- en: Variable Learning Rate
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可变学习率
- en: In every example we’ve seen so far, we’ve explored different learning rates
    and their impact on the network—but you can actually *vary* the learning rate
    as the network learns. Values that work well in early epochs may not work so well
    in later ones, so putting together a scheduler that adjusts this learning rate
    epoch by epoch can help you create networks that learn more effectively.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止看到的每个例子中，我们都探讨了不同的学习率及其对网络的影响——但实际上，你可以 *调整* 网络学习过程中的学习率。在早期 epoch 中效果好的值可能在后期
    epoch 中效果不佳，因此创建一个按 epoch 调整这个学习率的调度器可以帮助你创建更有效的学习网络。
- en: 'For this, PyTorch provides a `torch.optim.lr_scheduler` that you can program
    to change over the course of the training:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此，PyTorch 提供了一个 `torch.optim.lr_scheduler`，你可以编程在训练过程中改变它：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In [Chapter 7](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648),
    we looked at the idea of a *learning rate* (LR), which is a hyperparameter that,
    if set to be too large, will cause the network to overlearn, and if set to be
    too small will prevent the network from learning effectively. The nice thing about
    this is that you can set it as a variable rate, which we do here. In the early
    epochs, we want the network to learn fast, so we have a large LR—and in the later
    epochs, we don’t want it to overfit, so we gradually reduce the LR.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 7 章](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648)中，我们探讨了
    *学习率*（LR）的概念，它是一个超参数，如果设置得太大，会导致网络过度学习，如果设置得太小，将阻止网络有效地学习。这个好处是你可以将其设置为可变的学习率，我们在这里就是这样做的。在早期
    epoch 中，我们希望网络快速学习，所以我们有一个大的 LR——而在后期 epoch 中，我们不想让它过度拟合，所以我们逐渐降低 LR。
- en: The `pct_start` parameter defines a warm-up period as the first 10% of the training,
    during which the learning rate gradually increases up to the maximum (in this
    case, 0.01) and then decreases to 1/1000 of the initial learning rate (determined
    by `final_div_factor`).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`pct_start` 参数定义了一个预热期，即训练的前 10%，在此期间，学习率逐渐增加到最大值（在这种情况下，0.01），然后降低到初始学习率的
    1/1000（由 `final_div_factor` 确定）。'
- en: You can see the impact this has on training in [Figure 8-7](#ch08_figure_7_1748549671838042),
    where it reached ~90% accuracy in 6,800 epochs before triggering early stopping.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 8-7](#ch08_figure_7_1748549671838042)中看到这对接训的影响，它在 6,800 个 epoch 之前达到了
    ~90% 的准确率，然后触发了提前停止。
- en: '![](assets/aiml_0807.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0807.png)'
- en: Figure 8-7\. Adding a second LSTM layer
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-7\. 添加第二个 LSTM 层
- en: This time, when testing with the same phrases as before, I got “little” as the
    next word after “in the town of Athy” with a 26% probability, and I got “one”
    as the next word after “sweet Jeremy saw Dublin” with a 32% probability. Again,
    when predicting more words, the output quickly descended into gibberish.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，当使用与之前相同的短语进行测试时，我在“在 Athy 镇”之后以 26% 的概率得到了“little”，在“甜 Jeremy 看到了都柏林”之后以
    32% 的概率得到了“one”。再次，当预测更多单词时，输出迅速变成了乱码。
- en: 'Here are some examples:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些例子：
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The word `lámh` shows up in this text, and it’s Gaelic for *hand*. And `do lámh`
    means *your hand*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 单词 `lámh` 出现在这篇文本中，它是盖尔语中的 *手*。而 `do lámh` 意味着 *你的手*。
- en: If you get different results, don’t worry—you didn’t do anything wrong, but
    the random initialization of the neurons will impact the final scores.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你得到不同的结果，请不要担心——你没有做错任何事情，但神经元的随机初始化会影响最终得分。
- en: Improving the Data
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进数据
- en: There’s a little trick that you can use to extend the size of this dataset without
    adding any new songs. It’s called *windowing* the data. Right now, every line
    in every song is read as a single line and then turned into input sequences, as
    you saw in [Figure 8-2](#ch08_figure_2_1748549671837934). While humans read songs
    line by line to hear rhyme and meter, the model doesn’t have to, in particular
    when using bidirectional LSTMs.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个技巧你可以用来自扩展数据集的大小，而不需要添加任何新的歌曲。这被称为 *窗口化* 数据。目前，每首歌的每一行都被读作单独的一行，然后转换成输入序列，就像你在[图
    8-2](#ch08_figure_2_1748549671837934)中看到的那样。虽然人类一行一行地读歌曲以听押韵和韵律，但模型不必这样做，尤其是在使用双向
    LSTM 时。
- en: So, instead of taking the line “In the town of Athy, one Jeremy Lanigan,” processing
    that, and then moving to the next line (“Battered away ‘til he hadn’t a pound”)
    and processing that, we could treat all the lines as one long, continuous text.
    We could then create a “window” into that text of *n* words, process that, and
    then move the window forward one word to get the next input sequence (see [Figure 8-8](#ch08_figure_8_1748549671838067)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不必处理“在Ath城镇，有一个杰里米·兰尼根”这一行，然后移动到下一行（“被打得体无完肤，直到没有一磅”）并处理它，我们可以将所有行视为一个长而连续的文本。然后我们可以创建一个包含
    *n* 个单词的“窗口”来处理这个文本，然后移动窗口一个单词以获取下一个输入序列（见[图8-8](#ch08_figure_8_1748549671838067)）。
- en: In this case, far more training data can be yielded in the form of an increased
    number of input sequences. Moving the window across the entire corpus of text
    would give us ((*number_of_words* – *window_size*) × *window_size*) input sequences
    that we could train with.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可以通过增加输入序列的数量来产生更多的训练数据。将窗口在整个文本语料库上移动将给我们提供 ((*number_of_words* – *window_size*)
    × *window_size*) 个输入序列，我们可以用这些序列进行训练。
- en: '![](assets/aiml_0808.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0808.png)'
- en: Figure 8-8\. A moving word window
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8\. 移动单词窗口
- en: 'The code is pretty simple—when loading the data, instead of splitting each
    song line into a “sentence,” we can create them on the fly from the words in the
    corpus:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 代码相当简单——在加载数据时，我们不需要将每首歌的行分割成“句子”，而是可以从语料库中的单词动态创建它们：
- en: '[PRE28]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this case, because we no longer have sentences and we’re creating sequences
    that are the same size as the moving window, `max_sequence_len` is the size of
    the window. The full file is read, converted to lowercase, and split into an array
    of words using string splitting. The code then loops through the words and makes
    sentences of each word from the current index up to the current index plus the
    window size, adding each of those newly constructed sentences to the sentences
    array.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，因为我们不再有句子，我们正在创建与移动窗口大小相同的序列，所以 `max_sequence_len` 是窗口的大小。整个文件被读取，转换为小写，并使用字符串分割将其拆分为单词数组。然后代码遍历单词，并从当前索引到当前索引加上窗口大小创建每个单词的句子，将这些新构建的句子添加到句子数组中。
- en: Note
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To train this, you’ll likely need a higher-memory GPU. I used the 40Gb A100
    that’s available in Colab.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练这个，你可能需要一个更高内存的GPU。我在Colab上使用了40Gb的A100。
- en: When training, you’ll notice that the extra data makes training much slower
    per epoch, but the results are greatly improved and the generated text descends
    into gibberish much more slowly.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，你会注意到额外的数据使得每个epoch的训练速度变慢，但结果得到了极大的改善，生成的文本陷入胡言乱语的速度也慢得多。
- en: 'Here’s an example that caught my eye—particularly the last line:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个让我眼前一亮例子——尤其是最后一行：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: There are many hyperparameters you can try to tune. Changing the window size
    will change the amount of training data—a smaller window size can yield more data,
    but there will be fewer words to give to a label, so if you set it too small,
    you’ll end up with nonsensical poetry. You can also change the dimensions in the
    embedding, the number of LSTMs, or the size of the vocabulary to use for training.
    Given that percentage accuracy isn’t the best measurement—you’ll want to make
    a more subjective examination of how much “sense” the poetry makes—there’s no
    hard-and-fast rule to follow to determine whether your model is “good” or not.
    Of course, you *will* be limited by the amount of data you have and the compute
    that’s available to you—the bigger the model, the more power you will need.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多超参数你可以尝试调整。改变窗口大小将改变训练数据量——较小的窗口大小可以产生更多的数据，但提供给标签的单词会更少，所以如果你设置得太小，最终你会得到毫无意义的诗歌。你还可以改变嵌入维度、LSTMs的数量或用于训练的词汇表大小。鉴于百分比准确率并不是最好的衡量标准——你将想要对诗歌的“意义”进行更主观的评估——没有硬性规则可以遵循来确定你的模型是否“好”。当然，你*会*受到你所拥有的数据量和可用计算的限制——模型越大，你需要的力量就越多。
- en: Ultimately, the important thing is to experiment and have fun!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，重要的是要实验并享受乐趣！
- en: Character-Based Encoding
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于字符的编码
- en: For the last few chapters, we’ve been looking at NLP using word-based encoding.
    I find that much easier to get started with, but when it comes to generating text,
    you might also want to consider *character-based encoding* because the number
    of unique *characters* in a corpus tends to be a lot less than the number of unique
    *words*. If you use this approach, you can have a lot fewer neurons in your output
    layer, and your output predictions can be spread across fewer probabilities. For
    example, if you look at the dataset of [the complete works of Shakespeare](https://oreil.ly/XW_ab),
    you’ll see that there are only 65 unique characters in the entire set. Basically,
    Shakespeare only really used uppercase and lowercase letters and some punctuation
    to give a unique set of 65 characters!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: So, when you are making predictions with this dataset, instead of looking at
    the probabilities for the next word across 2,700 words as in the Irish songs dataset,
    you’re only looking at 65\. This makes your model a bit simpler!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s also nice about character-based encoding is that punctuation characters
    are also included, so line breaks, etc., can be predicted. As an example, when
    I used an RNN that was trained on the Shakespeare corpus to predict the text following
    on from my favorite *Game of Thrones* line, I got the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'YGRITTE:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You know nothing, Jon Snow.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Good night, we’ll prove those body’s servants to
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The traitor be these mine:*'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*So diswarl his body in hope in this resceins,*'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: I cannot judg appeal’t.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MENENIUS:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why, ’tis pompetsion.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KING RICHARD II:'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I think he make her thought on mine;
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'She will not: suffer up thy bonds:'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How doched it, I pray the gott,
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll no fame to this your love, and you were ends
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s kind of cool that she identifies him as a traitor and wants to tie him
    up (“diswarl his body”), but I have no idea what “resceins” means! If you watch
    the show, this is part of the plot, so maybe Shakespeare was on to something without
    realizing it!
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Of course, I do think we tend to be a little more forgiving when using something
    like Shakespeare’s texts as our training data, because the language is already
    a little unfamiliar.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: As with the Irish songs model, the output from the Shakespeare dataset does
    quickly degenerate into nonsensical text, but it’s still fun to play with. To
    try it for yourself, you can check out the [Colab](https://oreil.ly/cbz9c). This
    Colab is TensorFlow based, not PyTorch based. [See the GitHub repository](https://oreil.ly/kQ7aa)
    for a similar example that gives different results but is PyTorch based.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how to do basic text generation using a trained
    LSTM-based model. You learned how you can split text into training features and
    labels by using words as labels, and you also learned how create a model that,
    when given seed text, can predict the next likely word. Then, you iterated on
    this to improve the model for better results by exploring a dataset of traditional
    Irish songs. Hopefully, this was a fun introduction to how ML models can synthesize
    text and also gave you the knowledge you need to understand the foundational principles
    of generative AI. This approach was massively improved with the transformer architecture
    that underpins how LLM models like GPT and Gemini work!
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何使用训练好的基于LSTM的模型进行基本的文本生成。你学习了如何通过使用单词作为标签来将文本分割成训练特征和标签，你还学习了如何创建一个模型，当给定种子文本时，可以预测下一个可能的单词。然后，你通过探索传统爱尔兰歌曲的数据集来迭代改进模型，以获得更好的结果。希望这对你来说是一个有趣的入门，了解机器学习模型如何合成文本，同时也为你提供了理解生成式人工智能基础原理所需的知识。这种方法通过Transformer架构得到了极大的改进，正是LLM模型如GPT和Gemini工作的基础！
