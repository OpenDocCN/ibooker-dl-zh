["```py\nfrom ogb.linkproppred import LinkPropPredDataset\n\nfrom dlfb.utils.context import assets\n\n# Quite a large graph, may take a few minutes to load.\ndataset = LinkPropPredDataset(name=\"ogbl-ddi\", root=assets(\"graphs/datasets\"))\n\n```", "```py\ndataset.graph\n\n```", "```py\n{'edge_index': array([[4039, 2424, 4039, ...,  338,  835, 3554],\n        [2424, 4039,  225, ...,  708, 3554,  835]]),\n 'edge_feat': None,\n 'node_feat': None,\n 'num_nodes': 4267}\n\n```", "```py\nprint(\n  f'The graph contains {dataset.graph[\"num_nodes\"]} nodes and '\n  f'{dataset.graph[\"edge_index\"].shape[1]} edges.'\n)\n\n```", "```py\nThe graph contains 4267 nodes and 2135822 edges.\n\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ndegrees = np.bincount(dataset.graph[\"edge_index\"].flatten())\n\nsns.histplot(degrees, kde=True)\nplt.xlabel(\"Degree\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Degree Distribution\");\n\n```", "```py\nnum_nodes = dataset.graph[\"num_nodes\"]\nnum_observed_edges = dataset.graph[\"edge_index\"].shape[1]\n\n# Since each edge in an undirected graph can be represented in two ways, we\n# multiply by 2 to account for the bidirectionality.\nnum_observed_edges = 2 * num_observed_edges\n\n# For any graph with n nodes, the maximum number of edges (assuming no\n# self-loops) is n * (n-1).\nnum_possible_edges = num_nodes * (num_nodes - 1)\n\ndensity = num_observed_edges / num_possible_edges\n\nprint(\n  f\"There are {num_observed_edges} observed edges and {num_possible_edges} \"\n  f\"possible edges,\\ngiving a graph density of {round(density, 2)}\"\n)\n\n```", "```py\nThere are 4271644 observed edges and 18203022 possible edges,\ngiving a graph density of 0.23\n\n```", "```py\ndata_split = dataset.get_edge_split()\ndata_split\n\n```", "```py\n{'train': {'edge': array([[4039, 2424],\n         [4039,  225],\n         [4039, 3901],\n         ...,\n         [ 647,  708],\n         [ 708,  338],\n         [ 835, 3554]])},\n 'valid': {'edge': array([[ 722,  548],\n         [ 874, 3436],\n         [ 838, 1587],\n         ...,\n         [3661, 3125],\n         [3272, 3330],\n         [1330,  776]]),\n  'edge_neg': array([[   0,   58],\n         [   0,   84],\n         [   0,   90],\n         ...,\n         [4162, 4180],\n         [4168, 4260],\n         [4180, 4221]])},\n 'test': {'edge': array([[2198, 1172],\n         [1205,  719],\n         [1818, 2866],\n         ...,\n         [ 326, 1109],\n         [ 911, 1250],\n         [4127, 2480]]),\n  'edge_neg': array([[   0,    2],\n         [   0,   16],\n         [   0,   42],\n         ...,\n         [4168, 4259],\n         [4208, 4245],\n         [4245, 4259]])}}\n\n```", "```py\nprint(\n  f'Number of edges in train set: {data_split[\"train\"][\"edge\"].shape[0]}\\n'\n  f'Number of edges in valid set: {data_split[\"valid\"][\"edge\"].shape[0]}\\n'\n  f'Number of edges in test set: {data_split[\"test\"][\"edge\"].shape[0]}'\n)\n\n```", "```py\nNumber of edges in train set: 1067911\nNumber of edges in valid set: 133489\nNumber of edges in test set: 133489\n\n```", "```py\ntrain_nodes = np.unique(data_split[\"train\"][\"edge\"])\nvalid_nodes = np.unique(data_split[\"valid\"][\"edge\"])\ntest_nodes = np.unique(data_split[\"test\"][\"edge\"])\n\n# Check if all nodes in valid and test sets are present in train set.\nvalid_in_train = np.isin(valid_nodes, train_nodes).all()\ntest_in_train = np.isin(test_nodes, train_nodes).all()\n\nprint(f\"All validation nodes are in training nodes: {valid_in_train}\")\nprint(f\"All test nodes are in training nodes: {test_in_train}\")\n\n```", "```py\nAll validation nodes are in training nodes: True\nAll test nodes are in training nodes: True\n\n```", "```py\nimport pandas as pd\n\nddi_descriptions = pd.read_csv(\n  assets(\"graphs/datasets/ogbl_ddi/mapping/ddi_description.csv.gz\")\n)\nprint(ddi_descriptions)\n\n```", "```py\n        first drug id   first drug name second drug id     second drug name  \\\n0             DB00001         Lepirudin        DB06605             Apixaban   \n1             DB00001         Lepirudin        DB06695  Dabigatran etexi...   \n2             DB00001         Lepirudin        DB01254            Dasatinib   \n...               ...               ...            ...                  ...   \n2669761       DB15657  Ala-geninthiocin        DB14055         (S)-Warfarin   \n2669762       DB15657  Ala-geninthiocin        DB00581            Lactulose   \n2669763       DB15657  Ala-geninthiocin        DB14443  Vibrio cholerae ...   \n\n                 description  \n0        Apixaban may inc...  \n1        Dabigatran etexi...  \n2        The risk or seve...  \n...                      ...  \n2669761  The risk or seve...  \n2669762  The therapeutic ...  \n2669763  The therapeutic ...  \n\n[2669764 rows x 5 columns]\n\n```", "```py\nnode_to_dbid_lookup = pd.read_csv(\n  assets(\"graphs/datasets/ogbl_ddi/mapping/nodeidx2drugid.csv.gz\")\n)\nprint(node_to_dbid_lookup)\n\n```", "```py\n      node idx  drug id\n0            0  DB00001\n1            1  DB00002\n2            2  DB00004\n...        ...      ...\n4264      4264  DB15617\n4265      4265  DB15623\n4266      4266  DB15657\n\n[4267 rows x 2 columns]\n\n```", "```py\nddi_descriptions[\"first drug name\"].value_counts().head(10)\n\n```", "```py\nfirst drug name\nQuinidine         2477\nChlorpromazine    2431\nDesipramine       2345\nAmitriptyline     2338\nClozapine         2324\nDoxepin           2273\nClomipramine      2269\nHaloperidol       2269\nCarbamazepine     2267\nImipramine        2260\nName: count, dtype: int64\n\n```", "```py\nfirst_drug = ddi_descriptions[[\"first drug id\", \"first drug name\"]].rename(\n  columns={\"first drug id\": \"dbid\", \"first drug name\": \"drug_name\"}\n)\nsecond_drug = ddi_descriptions.loc[\n  :, [\"second drug id\", \"second drug name\"]\n].rename(columns={\"second drug id\": \"dbid\", \"second drug name\": \"drug_name\"})\ndbid_to_name_lookup = (\n  pd.concat([first_drug, second_drug]).drop_duplicates().reset_index(drop=True)\n)\n\ndrugs_lookup = pd.merge(\n  node_to_dbid_lookup.rename(\n    columns={\"drug id\": \"dbid\", \"node idx\": \"node_id\"}\n  ),\n  dbid_to_name_lookup,\n  on=\"dbid\",\n  how=\"inner\",\n)\n\ndrugs_lookup.iloc[935]\n\n```", "```py\nnode_id            935\ndbid           DB01043\ndrug_name    Memantine\nName: 935, dtype: object\n\n```", "```py\nimport numpy as np\n\nnp.random.seed(42)\n\ndef get_subgraph(edges: np.ndarray, node_limit: int) -> np.ndarray:\n  \"\"\"Gets a subgraph by sampling nodes and their edges.\"\"\"\n  nodes = np.unique(edges)\n  sampled_nodes = np.random.choice(nodes, size=node_limit, replace=False)\n  filtered_edges = edges[\n    np.isin(edges[:, 0], sampled_nodes) & np.isin(edges[:, 1], sampled_nodes)\n  ]\n  print(f\"Subgraph has {filtered_edges.shape[0]} edges\")\n  return filtered_edges\n\n# Sample 50 nodes from the training set.\nsubgraph = get_subgraph(node_limit=50, edges=data_split[\"train\"][\"edge\"])\n\n```", "```py\nSubgraph has 152 edges\n\n```", "```py\nimport networkx as nx\nfrom adjustText import adjust_text\n\ndef plot_ddi_graph(graph: np.ndarray, drugs_lookup: pd.DataFrame) -> plt.Figure:\n  \"\"\"Plots a drugâ€“drug interaction graph with labeled nodes.\"\"\"\n  fig = plt.figure(figsize=(15, 15))\n  G = nx.Graph()\n  G.add_edges_from(graph)\n  pos = nx.spring_layout(G)\n  nx.draw(\n    G=G,\n    pos=pos,\n    with_labels=False,\n    node_color=\"lightgray\",\n    edge_color=\"gray\",\n    node_size=10,\n    alpha=0.3,\n  )\n  names = (\n    drugs_lookup[drugs_lookup[\"node_id\"].isin(G.nodes)]\n    .set_index(\"node_id\")[\"drug_name\"]\n    .to_dict()\n  )\n  labels = nx.draw_networkx_labels(G=G, pos=pos, labels=names, font_size=20)\n  adjust_text(list(labels.values()))\n  return fig\n\nplot_ddi_graph(subgraph, drugs_lookup);\n\n```", "```py\n  def __init__(self, path):\n    \"\"\"Initializes the dataset builder with a path to the dataset.\"\"\"\n    self.path = path\n\n  def build(\n    self,\n    node_limit: int | None = None,\n    rng: jax.Array | None = None,\n    keep_original_ids: bool = False,\n  ) -> dict[str, Dataset]:\n    \"\"\"Builds and returns a dictionary of dataset splits.\"\"\"\n    dataset_splits = {}\n    n_nodes, split_pairs = self.download()\n    annotation = self.prepare_annotation()\n\n    for name, split in split_pairs.items():\n      pos_pairs, neg_pairs = split[\"edge\"], split[\"edge_neg\"]\n      graph = self.prepare_graph(n_nodes, pos_pairs)\n      pairs = self.prepare_pairs(graph, pos_pairs, neg_pairs)\n      dataset_splits.update({name: Dataset(n_nodes, graph, pairs, annotation)})\n\n    if node_limit and (rng is not None):\n      dataset_splits = self.subset(\n        dataset_splits, rng, node_limit, keep_original_ids\n      )\n\n    return dataset_splits\n\n```", "```py\n  def download(self) -> tuple[int, dict]:\n    \"\"\"Downloads the dataset and returns the number of nodes and edge splits.\"\"\"\n    raw = LinkPropPredDataset(name=\"ogbl-ddi\", root=self.path)\n    n_nodes = raw[0][\"num_nodes\"]\n    split_pairs = raw.get_edge_split()\n    split_pairs[\"train\"][\"edge_neg\"] = None  # Placeholder for negative edges.\n    return n_nodes, split_pairs\n\n```", "```py\n    def prepare_annotation(self) -> pd.DataFrame:\n      \"\"\"Annotates nodes by mapping node IDs to database IDs and drug names.\"\"\"\n      ddi_descriptions = pd.read_csv(\n        f\"{self.path}/ogbl_ddi/mapping/ddi_description.csv.gz\"\n      )\n      node_to_dbid_lookup = pd.read_csv(\n        f\"{self.path}/ogbl_ddi/mapping/nodeidx2drugid.csv.gz\"\n      )\n      # Merge first and second drug descriptions into a single lookup.\n      first_drug = ddi_descriptions.loc[\n        :, [\"first drug id\", \"first drug name\"]\n      ].rename(columns={\"first drug id\": \"dbid\", \"first drug name\": \"drug_name\"})\n\n      second_drug = ddi_descriptions.loc[\n        :, [\"second drug id\", \"second drug name\"]\n      ].rename(\n        columns={\"second drug id\": \"dbid\", \"second drug name\": \"drug_name\"}\n      )\n      dbid_to_name_lookup = (\n        pd.concat([first_drug, second_drug])\n        .drop_duplicates()\n        .reset_index(drop=True)\n      )\n\n      # Merge with node-to-DBID lookup.\n      annotation = pd.merge(\n        node_to_dbid_lookup.rename(\n          columns={\"drug id\": \"dbid\", \"node idx\": \"node_id\"}\n        ),\n        dbid_to_name_lookup,\n        on=\"dbid\",\n        how=\"inner\",\n      )\n      return annotation\n\n```", "```py\n  def prepare_graph(\n    self, n_nodes: int, pos_pairs: jax.Array\n  ) -> jraph.GraphsTuple:\n    \"\"\"Prepares a Jraph graph from positive edge pairs.\"\"\"\n    senders, receivers = self.make_undirected(pos_pairs[:, 0], pos_pairs[:, 1])\n    graph = jraph.GraphsTuple(\n      nodes={\"gid\": jnp.arange(n_nodes)},  # Optional global node ID.\n      edges=None,\n      senders=senders,\n      receivers=receivers,\n      n_node=jnp.array([n_nodes]),\n      n_edge=jnp.array([len(senders)]),\n      globals=None,\n    )\n    return graph\n\n```", "```py\n  @staticmethod\n  def make_undirected(\n    senders: jax.Array, receivers: jax.Array\n  ) -> tuple[jax.Array, jax.Array]:\n    \"\"\"Makes an undirected graph by duplicating edges in both directions.\"\"\"\n    # Jraph requires undirected graphs to have both A->B and B->A edges\n    # explicitly.\n    senders_undir = jnp.concatenate((senders, receivers))\n    receivers_undir = jnp.concatenate((receivers, senders))\n    return senders_undir, receivers_undir\n\n```", "```py\n  def prepare_pairs(\n    self, graph: int, pos_pairs: jax.Array, neg_pairs: jax.Array | None = None\n  ) -> Pairs:\n    \"\"\"Prepares positive and negative edge pairs.\"\"\"\n    if neg_pairs is None:\n      neg_pairs = self.infer_negative_pairs(graph)\n    return Pairs(pos=pos_pairs, neg=neg_pairs)\n\n```", "```py\n  def infer_negative_pairs(self, graph: jraph.GraphsTuple) -> jax.Array:\n    \"\"\"Infers negative edge pairs in a graph.\"\"\"\n    # Initialize a matrix where all possible edges are marked as potential\n    # negative edges (1).\n    neg_adj_mask = np.ones((graph.n_node[0], graph.n_node[0]), dtype=np.uint8)\n\n    # Mask out existing edges in the graph (set to 0).\n    neg_adj_mask[graph.senders, graph.receivers] = 0\n\n    # Use the upper triangular part of the matrix to avoid duplicate pairs and\n    # self-loops.\n    neg_adj_mask = np.triu(neg_adj_mask, k=1)\n    neg_pairs = jnp.array(neg_adj_mask.nonzero()).T  # Extract indices.\n    return neg_pairs\n\n```", "```py\n  def subset(\n    self,\n    dataset_splits: dict[str, Dataset],\n    rng: jax.Array,\n    node_limit: int,\n    keep_original_ids: bool = False,\n  ) -> dict[str, Dataset]:\n    \"\"\"Creates subset of dataset splits by sampling a fixed number of nodes.\"\"\"\n    # Get a random subset of node_ids.\n    node_ids = jax.random.choice(\n      rng, dataset_splits[\"train\"].n_nodes, (node_limit,), replace=False\n    )\n\n    # Subset every dataset split by the same node_ids.\n    dataset_subset_splits = {}\n    for name, dataset in dataset_splits.items():\n      dataset_subset_splits[name] = dataset.subset(node_ids, keep_original_ids)\n\n    return dataset_subset_splits\n\n```", "```py\n@dataclass\nclass Dataset:\n  \"\"\"Graph dataset with nodes, pairs, and optional annotations.\"\"\"\n\n  n_nodes: int\n  graph: jraph.GraphsTuple\n  pairs: Pairs\n  annotation: pd.DataFrame = field(default_factory=pd.DataFrame)\n\n```", "```py\nclass NodeEncoder(nn.Module):\n  \"\"\"Encodes nodes into embeddings using a two-layer GraphSAGE model.\"\"\"\n\n  n_nodes: int\n  embedding_dim: int\n  last_layer_self: bool\n  degree_norm: bool\n  dropout_rate: float\n\n  def setup(self):\n    \"\"\"Initializes node embeddings, which cover the full graph's n_nodes.\"\"\"\n    self.node_embeddings = nn.Embed(\n      num_embeddings=self.n_nodes,\n      features=self.embedding_dim,\n      embedding_init=jax.nn.initializers.glorot_uniform(),\n    )\n\n  @nn.compact\n  def __call__(self, graph: jraph.GraphsTuple, is_training: bool) -> jax.Array:\n    \"\"\"Encodes the nodes of a graph into embeddings.\"\"\"\n    # Graph can be a subgraph and thus we use a subset of embeddings\n    x = self.node_embeddings(graph.nodes[\"gid\"])\n\n    # First convolutional layer.\n    x = SAGEConv(\n      self.embedding_dim, with_self=True, degree_norm=self.degree_norm\n    )(graph, x)\n    x = nn.relu(x)\n    x = nn.Dropout(rate=self.dropout_rate, deterministic=not is_training)(x)\n\n    # Second convolutional layer.\n    x = SAGEConv(\n      self.embedding_dim,\n      with_self=self.last_layer_self,\n      degree_norm=self.degree_norm,\n    )(graph, x)\n\n    return x\n\n```", "```py\nclass SAGEConv(nn.Module):\n  \"\"\"GraphSAGE convolutional layer with optional self-loops.\"\"\"\n\n  embedding_dim: int\n  with_self: bool\n  degree_norm: bool\n\n  @nn.compact\n  def __call__(self, graph: jraph.GraphsTuple, x) -> jax.Array:\n    n_nodes = self.get_n_nodes(graph)\n\n    # Add self-loops if enabled.\n    if self.with_self:\n      senders, receivers = self._add_self_edges(graph, n_nodes)\n    else:\n      senders, receivers = graph.senders, graph.receivers\n\n    # Aggregate node features from neighbors.\n    if not self.degree_norm:\n      x_updated = jraph.segment_mean(\n        x[senders], receivers, num_segments=n_nodes\n      )\n    else:\n\n      def get_degree(n):\n        return jax.ops.segment_sum(jnp.ones_like(senders), n, n_nodes)\n\n      x_updated = self.normalize_by_degree(x, get_degree(senders))\n      x_updated = jraph.segment_mean(\n        x_updated[senders], receivers, num_segments=n_nodes\n      )\n      x_updated = self.normalize_by_degree(x_updated, get_degree(receivers))\n\n    # Combine node and neighbor embeddings by concatenation.\n    combined_embeddings = jnp.concatenate([x, x_updated], axis=-1)\n\n    return nn.Dense(self.embedding_dim)(combined_embeddings)\n\n  @staticmethod\n  def _add_self_edges(\n    graph: jraph.GraphsTuple, n_nodes: int\n  ) -> tuple[jax.Array, jax.Array]:\n    \"\"\"Adds self-loops to the graph.\"\"\"\n    all_nodes = jnp.arange(n_nodes)\n    senders = jnp.concatenate([graph.senders, all_nodes])\n    receivers = jnp.concatenate([graph.receivers, all_nodes])\n    return senders, receivers\n\n  @staticmethod\n  def normalize_by_degree(x: jax.Array, degree: jax.Array) -> jax.Array:\n    \"\"\"Normalizes node features by the square root of the degree.\"\"\"\n    # We set the the degree to a minimum of 1.\n    return x * jax.lax.rsqrt(jnp.maximum(degree, 1.0))[:, None]\n\n  @staticmethod\n  def get_n_nodes(graph):\n    \"\"\"Returns the number of nodes in the graph in a jittable way.\"\"\"\n    return tree.tree_leaves(graph.nodes)[0].shape[0]\n\n```", "```py\nclass LinkPredictor(nn.Module):\n  \"\"\"Predicts interaction scores for pairs of node embeddings.\"\"\"\n\n  embedding_dim: int\n  n_layers: int\n  dropout_rate: float\n\n  @nn.compact\n  def __call__(\n    self,\n    sender_embeddings: jax.Array,\n    receiver_embeddings: jax.Array,\n    is_training: bool,\n  ) -> jax.Array:\n    \"\"\"Computes scores for node pairs.\"\"\"\n    x = sender_embeddings * receiver_embeddings  # Element-wise multiplication.\n\n    # Apply MLP layers with ReLU activation and dropout.\n    for _ in range(self.n_layers)[:-1]:\n      x = nn.Dense(self.embedding_dim)(x)\n      x = nn.relu(x)\n      x = nn.Dropout(self.dropout_rate, deterministic=not is_training)(x)\n\n    # Final output layer is a single neuron. Logit output used for binary link\n    # classification.\n    x = nn.Dense(1)(x)\n\n    return jnp.squeeze(x)\n\n```", "```py\nclass DdiModel(nn.Module):\n  \"\"\"Graph-based model for predicting drug-drug interactions (DDIs).\"\"\"\n\n  n_nodes: int\n  embedding_dim: int\n  dropout_rate: float\n  last_layer_self: bool\n  degree_norm: bool\n  n_mlp_layers: int = 2\n\n  def setup(self):\n    \"\"\"Initializes the node encoder and link predictor modules.\"\"\"\n    self.node_encoder = NodeEncoder(\n      self.n_nodes,\n      self.embedding_dim,\n      self.last_layer_self,\n      self.degree_norm,\n      self.dropout_rate,\n    )\n    self.link_predictor = LinkPredictor(\n      self.embedding_dim, self.n_mlp_layers, self.dropout_rate\n    )\n\n  def __call__(\n    self,\n    graph: jraph.GraphsTuple,\n    pairs: dict,\n    is_training: bool,\n    is_pred: bool = False,\n  ):\n    \"\"\"Generates interaction scores for node pairs.\"\"\"\n    # Compute node embeddings. The 'h' stands for hidden state or embedding.\n    h = self.node_encoder(graph, is_training)\n\n    if is_pred:\n      scores = self.link_predictor(h[pairs[:, 0]], h[pairs[:, 1]], False)\n\n    else:\n      pos_senders, pos_receivers = pairs[\"pos\"][:, 0], pairs[\"pos\"][:, 1]\n      neg_senders, neg_receivers = pairs[\"neg\"][:, 0], pairs[\"neg\"][:, 1]\n      scores = {\n        \"pos\": self.link_predictor(\n          h[pos_senders], h[pos_receivers], is_training\n        ),\n        \"neg\": self.link_predictor(\n          h[neg_senders], h[neg_receivers], is_training\n        ),\n      }\n    return scores\n\n  def create_train_state(self, rng: jax.Array, dummy_input, tx) -> TrainState:\n    \"\"\"Initializes the training state with model parameters.\"\"\"\n    rng, rng_init, rng_dropout = jax.random.split(rng, 3)\n    variables = self.init(rng_init, is_training=False, **dummy_input)\n    return TrainState.create(\n      apply_fn=self.apply, params=variables[\"params\"], tx=tx, key=rng_dropout\n    )\n\n  @staticmethod\n  def add_mean_embedding(embeddings: jax.Array) -> jax.Array:\n    \"\"\"Concatenates a mean embedding to the existing embeddings.\"\"\"\n    mean_embeddings = jnp.mean(embeddings, axis=0, keepdims=True)\n    embeddings = jnp.concatenate([embeddings, mean_embeddings], axis=0)\n    return embeddings\n\n```", "```py\nnode_limit = 500\nrng = jax.random.PRNGKey(42)\nrng, rng_dataset = jax.random.split(rng, 2)\n\ndataset_splits = DatasetBuilder(path=assets(\"graphs/datasets\")).build(\n  node_limit, rng_dataset\n)\n\n```", "```py\nfrom dlfb.graphs.inspect import plot_graph\n\nplot_graph(dataset_splits[\"train\"]);\n\n```", "```py\n@restorable\ndef train(\n  state: TrainState,\n  rng: jax.Array,\n  dataset_splits: dict[str, Dataset],\n  num_epochs: int,\n  loss_fn: Callable,\n  norm_loss: bool = False,\n  eval_every: int = 10,\n) -> tuple[TrainState, dict[str, dict[str, list[dict[str, float]]]]]:\n  \"\"\"Training loop for the drug-drug interaction model.\"\"\"\n  # Initialize metrics and estimate optimal batch sizes.\n  metrics = MetricsLogger()\n  batch_size = optimal_batch_size(dataset_splits)\n\n  # Epochs with progress bar.\n  epochs = tqdm(range(num_epochs))\n  for epoch in epochs:\n    epochs.set_description(f\"Epoch {epoch + 1}\")\n    rng, rng_shuffle, rng_sample = jax.random.split(rng, 3)\n\n    # Training loop.\n    for pairs_batch in dataset_splits[\"train\"].pairs.get_train_batches(\n      batch_size, rng_shuffle, rng_sample\n    ):\n      rng, rng_dropout = jax.random.split(rng, 2)\n      state, batch_metrics = train_step(\n        state,\n        dataset_splits[\"train\"].graph,\n        pairs_batch,\n        rng_dropout,\n        loss_fn,\n        norm_loss,\n      )\n      metrics.log_step(split=\"train\", **batch_metrics)\n\n    # Evaluation loop.\n    if epoch % eval_every == 0:\n      for pairs_batch in dataset_splits[\"valid\"].pairs.get_eval_batches(\n        batch_size\n      ):\n        batch_metrics = eval_step(\n          state, dataset_splits[\"valid\"].graph, pairs_batch, loss_fn, norm_loss\n        )\n        metrics.log_step(split=\"valid\", **batch_metrics)\n\n    metrics.flush(epoch=epoch)\n    epochs.set_postfix_str(metrics.latest([\"hits@20\"]))\n\n  return state, metrics.export()\n\n```", "```py\n@dataclass\nclass Pairs:\n  \"\"\"Represents positive and negative pairs of drug-drug interactions.\"\"\"\n\n  pos: jax.Array\n  neg: jax.Array\n\n  def get_eval_batches(\n    self, batch_size: int\n  ) -> Generator[dict[str, jax.Array], None, None]:\n    \"\"\"Generates evaluation batches of positive and negative pairs.\"\"\"\n    indices = jnp.arange(self._n_pairs())\n    for i in range(self._n_batches(batch_size)):\n      batch_indices = jnp.array(indices[i * batch_size : (i + 1) * batch_size])\n      yield Pairs(\n        pos=self.pos[batch_indices], neg=self.neg[batch_indices]\n      ).to_dict()\n\n  def _n_batches(self, batch_size: int) -> int:\n    \"\"\"Calculates number of batches in the dataset given a batch size.\"\"\"\n    return int(np.floor(self._n_pairs() / batch_size))\n\n  def _n_pairs(self) -> int:\n    \"\"\"Returns the smaller number of positive or negative pairs.\"\"\"\n    return int(min(self.pos.shape[0], self.neg.shape[0]))\n\n  def get_train_batches(\n    self, batch_size: int, rng_shuffle: jax.Array, rng_sample: jax.Array\n  ) -> Generator[dict[str, jax.Array], None, None]:\n    \"\"\"Generates shuffled training batches with sampled negative pairs.\"\"\"\n    # Shuffle indices for positive pairs.\n    indices = jax.random.permutation(rng_shuffle, jnp.arange(self._n_pairs()))\n\n    # Get sample of negative pairs.\n    neg_sample = self._global_negative_sampling(rng_sample)\n\n    for i in range(self._n_batches(batch_size)):\n      batch_indices = jnp.array(indices[i * batch_size : (i + 1) * batch_size])\n      yield Pairs(\n        pos=self.pos[batch_indices], neg=neg_sample[batch_indices]\n      ).to_dict()\n\n  def _global_negative_sampling(self, rng_sample: jax.Array) -> jax.Array:\n    \"\"\"Samples negative pairs from the entire set to match positive set size.\"\"\"\n    return jax.random.choice(\n      rng_sample, self.neg, (self.pos.shape[0],), replace=False\n    )\n\n  def get_dummy_input(self) -> dict[str, jax.Array]:\n    \"\"\"Returns a small dummy subset of positive and negative pairs.\"\"\"\n    return Pairs(pos=self.pos[:2], neg=self.neg[:(2)]).to_dict()\n\n  def to_dict(self) -> dict:\n    \"\"\"Converts the Pairs object back to a dictionary.\"\"\"\n    return {\"pos\": self.pos, \"neg\": self.neg}\n\n```", "```py\ndef optimal_batch_size(\n  dataset_splits: dict[str, Dataset], remainder_tolerance: float = 0.125\n) -> int:\n  \"\"\"Calculates optimal batch size for optimizing JAX compilation.\"\"\"\n  # Calculate the minimum length of positive and negative pairs for each\n  # dataset.\n  lengths = [\n    min(dataset.pairs.pos.shape[0], dataset.pairs.neg.shape[0])\n    for dataset in dataset_splits.values()\n  ]\n\n  # Determine the allowable remainders per split based on the remainder\n  # tolerance.\n  remainder_thresholds = [\n    int(length * remainder_tolerance) for length in lengths\n  ]\n  max_possible_batch_size = min(lengths)\n\n  for batch_size in range(max_possible_batch_size, 0, -1):\n    remainders = [length % batch_size for length in lengths]\n    if all(\n      remainder <= threshold\n      for remainder, threshold in zip(remainders, remainder_thresholds)\n    ):\n      return batch_size\n  return max_possible_batch_size\n\n```", "```py\n  def _global_negative_sampling(self, rng_sample: jax.Array) -> jax.Array:\n    \"\"\"Samples negative pairs from the entire set to match positive set size.\"\"\"\n    return jax.random.choice(\n      rng_sample, self.neg, (self.pos.shape[0],), replace=False\n    )\n\n```", "```py\n@partial(jax.jit, static_argnames=[\"loss_fn\", \"norm_loss\"])\ndef train_step(\n  state: TrainState,\n  graph: jraph.GraphsTuple,\n  pairs: dict[str, jax.Array],\n  rng_dropout: jax.Array,\n  loss_fn: Callable = binary_log_loss,\n  norm_loss: bool = False,\n) -> tuple[TrainState, dict[str, jax.Array]]:\n  \"\"\"Performs a single training step, updating model parameters.\"\"\"\n\n  def calculate_loss(params):\n    \"\"\"Computes loss and hits@20 metric for the given model parameters.\"\"\"\n    scores = state.apply_fn(\n      {\"params\": params},\n      graph,\n      pairs,\n      is_training=True,\n      rngs={\"dropout\": rng_dropout},\n    )\n    loss = loss_fn(scores)\n    metric = evaluate_hits_at_20(scores)\n    return loss, metric\n\n  # to additional variables (e.g., state, graph, pairs) without requiring them\n  # to be explicitly passed, while maintaining compatibility with\n  # jax.value_and_grad.\n  grad_fn = jax.value_and_grad(calculate_loss, has_aux=True)\n  (loss, metric), grads = grad_fn(state.params)\n  state = state.apply_gradients(grads=grads)\n\n  metrics = {\"loss\": loss, \"hits@20\": metric}\n  if norm_loss:\n    metrics[\"loss\"] = metrics[\"loss\"] / (\n      pairs[\"pos\"].shape[0] + pairs[\"neg\"].shape[0]\n    )\n\n  return state, metrics\n\n```", "```py\n@jax.jit\ndef binary_log_loss(scores: dict[str, jax.Array]) -> jax.Array:\n  \"\"\"Computes the binary log loss for positive and negative drug pairs.\"\"\"\n  # Clip probabilities to avoid numerical instability.\n  probs = jax.tree.map(\n    lambda x: jnp.clip(nn.sigmoid(x), 1e-7, 1 - 1e-7), scores\n  )\n\n  # Compute positive and negative losses.\n  pos_loss = -jnp.log(probs[\"pos\"]).mean()\n  neg_loss = -jnp.log(1 - probs[\"neg\"]).mean()\n\n  return pos_loss + neg_loss\n\n```", "```py\n@partial(jax.jit, static_argnames=[\"loss_fn\", \"norm_loss\"])\ndef eval_step(\n  state: TrainState,\n  graph: jraph.GraphsTuple,\n  pairs: dict[str, jax.Array],\n  loss_fn: Callable = binary_log_loss,\n  norm_loss: bool = False,\n) -> dict[str, jax.Array]:\n  \"\"\"Performs an evaluation step, computing loss and hits@20 metric.\"\"\"\n  scores = state.apply_fn(\n    {\"params\": state.params}, graph, pairs, is_training=False\n  )\n  metrics = {\"loss\": loss_fn(scores), \"hits@20\": evaluate_hits_at_20(scores)}\n  if norm_loss:\n    metrics[\"loss\"] = metrics[\"loss\"] / (\n      pairs[\"pos\"].shape[0] + pairs[\"neg\"].shape[0]\n    )\n\n  return metrics\n\n```", "```py\n@jax.jit\ndef evaluate_hits_at_20(scores: dict[str, jax.Array]) -> jax.Array:\n  \"\"\"Computes the hits@20 metric capturing positive pairs ranking.\"\"\"\n  # Implementation inspired by the OGB benchmark: https://oreil.ly/Oej2Y\n  # Find the 20th highest score among negative edges.\n  kth_score_in_negative_edges = jnp.sort(scores[\"neg\"])[-20]\n\n  # Compute the proportion of positive scores greater than the threshold.\n  return (\n    jnp.sum(scores[\"pos\"] > kth_score_in_negative_edges)\n    / scores[\"pos\"].shape[0]\n  )\n\n```", "```py\nimport optax\n\nrng, rng_init, rng_train = jax.random.split(rng, 3)\n\nmodel = DdiModel(\n  n_nodes=dataset_splits[\"train\"].n_nodes,\n  embedding_dim=128,\n  last_layer_self=False,\n  degree_norm=False,\n  dropout_rate=0.3,\n)\n\nstate, metrics = train(\n  state=model.create_train_state(\n    rng=rng_init,\n    dummy_input={\n      \"graph\": dataset_splits[\"train\"].graph,\n      \"pairs\": dataset_splits[\"train\"].pairs.get_dummy_input(),\n    },\n    tx=optax.adam(0.001),\n  ),\n  rng=rng_train,\n  dataset_splits=dataset_splits,\n  num_epochs=500,\n  eval_every=1,\n  loss_fn=binary_log_loss,\n  norm_loss=False,\n  store_path=assets(\"graphs/models/initial_model\"),\n)\n\n```", "```py\nfrom dlfb.graphs.inspect import plot_learning\n\nplot_learning(metrics);\n\n```", "```py\n@jax.jit\ndef auc_loss(scores: dict[str, jax.Array]) -> jax.Array:\n  \"\"\"Computes AUC-based loss for positive and negative drug pairs.\"\"\"\n  return jnp.square(1 - (scores[\"pos\"] - scores[\"neg\"])).sum()\n\n```", "```py\nrng, rng_init, rng_train = jax.random.split(rng, 3)\n\nmodel = DdiModel(\n  n_nodes=dataset_splits[\"train\"].n_nodes,\n  embedding_dim=128,\n  last_layer_self=False,\n  degree_norm=False,\n  dropout_rate=0.3,\n)\n\n_, metrics = train(\n  state=model.create_train_state(\n    rng=rng_init,\n    dummy_input={\n      \"graph\": dataset_splits[\"train\"].graph,\n      \"pairs\": dataset_splits[\"train\"].pairs.get_dummy_input(),\n    },\n    tx=optax.adam(0.001),\n  ),\n  rng=rng_train,\n  dataset_splits=dataset_splits,\n  num_epochs=500,\n  eval_every=1,\n  loss_fn=auc_loss,\n  norm_loss=True,\n  store_path=assets(\"graphs/models/initial_model_auc\"),\n)\n\n```", "```py\nplot_learning(metrics);\n\n```", "```py\nembedding_dims = [64, 128, 256, 512]\nmodel_params = {\n  \"n_nodes\": dataset_splits[\"train\"].n_nodes,\n  \"last_layer_self\": False,\n  \"degree_norm\": False,\n  \"dropout_rate\": 0.3,\n}\ntraining_params = {\n  \"rng\": rng_train,\n  \"dataset_splits\": dataset_splits,\n  \"num_epochs\": 500,\n  \"eval_every\": 25,\n  \"loss_fn\": auc_loss,\n  \"norm_loss\": True,\n}\n\n```", "```py\nfrom dlfb.utils.metric_plots import to_df\n\nall_metrics = []\nfor embedding_dim in embedding_dims:\n  model = DdiModel(**{\"embedding_dim\": embedding_dim, **model_params})\n  _, metrics = train(\n    state=model.create_train_state(\n      rng=rng_init,\n      dummy_input={\n        \"graph\": dataset_splits[\"train\"].graph,\n        \"pairs\": dataset_splits[\"train\"].pairs.get_dummy_input(),\n      },\n      tx=optax.adam(0.001),\n    ),\n    **training_params,\n    store_path=assets(f\"graphs/models/sweep_embedding_dim:{embedding_dim}\"),\n  )\n  df = to_df(metrics).assign(**{\"embedding_dim\": embedding_dim})\n  all_metrics.append(df)\nall_metrics_df = pd.concat(all_metrics, axis=0)\n\n```", "```py\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom dlfb.utils.metric_plots import DEFAULT_SPLIT_COLORS\n\ndata = all_metrics_df\ndata = data[(data[\"metric\"] == \"hits@20\")]\ndata = data.groupby([\"metric\", \"split\", \"embedding_dim\"], as_index=False)[\n  \"mean\"\n].max()\ndata = data.sort_values(by=[\"split\", \"mean\"])\n\nplt.figure(figsize=(7, 3.5))\nsns.barplot(\n  data=data,\n  x=\"embedding_dim\",\n  y=\"mean\",\n  hue=\"split\",\n  palette=DEFAULT_SPLIT_COLORS,\n)\nplt.ylim(0.5, 1)\nplt.xlabel(\"Embedding Dimensions\")\nplt.ylabel(\"Maximum Hits@20\");\n\n```", "```py\nimport itertools\n\nmodel_params = {\n  \"n_nodes\": dataset_splits[\"train\"].n_nodes,\n  \"embedding_dim\": 512,\n}\n\nmodel_params_sweep = {\n  \"dropout_rate\": [0, 0.3, 0.5],\n  \"last_layer_self\": [True, False],\n  \"degree_norm\": [True, False],\n  \"n_mlp_layers\": [1, 2, 3],\n}\nkeys, values = zip(*model_params_sweep.items())\nmodel_param_combn = [\n  dict(zip(keys, combo)) for combo in itertools.product(*values)\n]\nprint(pd.DataFrame(model_param_combn))\n\n```", "```py\n    dropout_rate  last_layer_self  degree_norm  n_mlp_layers\n0   0.0           True             True         1\n1   0.0           True             True         2\n2   0.0           True             True         3\n..  ...           ...              ...          ...\n33  0.5           False            False        1\n34  0.5           False            False        2\n35  0.5           False            False        3\n\n[36 rows x 4 columns]\n\n```", "```py\ndef name_from_params(params: dict) -> str:\n  \"\"\"Generates a string from a parameters dictionary\"\"\"\n  return \"_\".join([f\"{k}:{v}\" for k, v in params.items()])\n\nall_metrics = []\n\nfor combn in model_param_combn:\n  model = DdiModel(**{**combn, **model_params})\n  _, metrics = train(\n    state=model.create_train_state(\n      rng=rng_init,\n      dummy_input={\n        \"graph\": dataset_splits[\"train\"].graph,\n        \"pairs\": dataset_splits[\"train\"].pairs.get_dummy_input(),\n      },\n      tx=optax.adam(0.001),\n    ),\n    **training_params,\n    store_path=assets(f\"graphs/models/sweep_all_{name_from_params(combn)}\"),\n  )\n  df = to_df(metrics).assign(**combn)\n  all_metrics.append(df)\n\nall_metrics_df = pd.concat(all_metrics, axis=0)\n\n```", "```py\ndef conv_layer_annot(row):\n  if row[\"last_layer_self\"] and row[\"degree_norm\"]:\n    return \"with self-edges and norm\"\n  elif row[\"last_layer_self\"]:\n    return \"with self-edges, no norm\"\n  elif row[\"degree_norm\"]:\n    return \"with norm, no self-edges\"\n  else:\n    return \"no self-edges and no norm\"\n\ndata = all_metrics_df\ndata = data[(data[\"metric\"] == \"hits@20\")]\ndata = data.groupby(\n  [\"metric\", \"split\", *list(model_params_sweep.keys())], as_index=False\n)[\"mean\"].max()\ndata = data.sort_values(by=[\"split\", \"mean\"])\ndata[\"conv_layer\"] = data.apply(conv_layer_annot, axis=1)\n\n```", "```py\nfig = sns.relplot(\n  data=data,\n  x=\"conv_layer\",\n  y=\"mean\",\n  row=\"dropout_rate\",\n  col=\"n_mlp_layers\",\n  hue=\"split\",\n  palette=DEFAULT_SPLIT_COLORS,\n  facet_kws=dict(margin_titles=True, despine=False),\n  height=2,\n)\nfig.figure.subplots_adjust(wspace=0.1, hspace=0)\nfor ax in fig.axes.flat:\n  for label in ax.get_xticklabels():\n    label.set_rotation(45)\n    label.set_ha(\"right\")\nfig.set_axis_labels(\"\", \"maximum Hits@20\");\n\n```", "```py\nfrom dlfb.utils.metric_plots import from_df\n\nmetrics = all_metrics_df\nmetrics = from_df(\n  metrics[\n    (metrics[\"dropout_rate\"] == 0.0)\n    & (metrics[\"last_layer_self\"] == False)\n    & (metrics[\"degree_norm\"] == False)\n    & (metrics[\"n_mlp_layers\"] == 1)\n  ]\n)\n\nplot_learning(metrics);\n\n```", "```py\nmetrics = all_metrics_df\nmetrics = from_df(\n  metrics[\n    (metrics[\"dropout_rate\"] == 0.5)\n    & metrics[\"last_layer_self\"]\n    & (metrics[\"degree_norm\"] == False)\n    & (metrics[\"n_mlp_layers\"] == 2)\n  ]\n)\n\nplot_learning(metrics);\n\n```", "```py\nnode_limit = 2134\nrng = jax.random.PRNGKey(42)\nrng, rng_dataset, rng_init, rng_train = jax.random.split(rng, 4)\ndataset_splits = DatasetBuilder(path=assets(\"graphs/datasets\")).build(\n  node_limit, rng_dataset\n)\n\nmodel = DdiModel(\n  n_nodes=dataset_splits[\"train\"].n_nodes,\n  embedding_dim=512,\n  dropout_rate=0.3,\n  last_layer_self=True,\n  degree_norm=True,\n  n_mlp_layers=2,\n)\n\n_, metrics = train(\n  state=model.create_train_state(\n    rng=rng_init,\n    dummy_input={\n      \"graph\": dataset_splits[\"train\"].graph,\n      \"pairs\": dataset_splits[\"train\"].pairs.get_dummy_input(),\n    },\n    tx=optax.adam(0.001),\n  ),\n  rng=rng_train,\n  dataset_splits=dataset_splits,\n  num_epochs=1000,\n  eval_every=25,\n  loss_fn=auc_loss,\n  norm_loss=True,\n  store_path=assets(\"graphs/models/larger_model\"),\n)\n\n```", "```py\nplot_learning(metrics);\n\n```"]