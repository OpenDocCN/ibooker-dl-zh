["```py\nclass Environment(object):\n  \"\"\"An environment in which an actor performs actions to accomplish a task.\n\n An environment has a current state, which is represented as either a single NumPy\n array, or optionally a list of NumPy arrays.  When an action is taken, that causes\n the state to be updated.  Exactly what is meant by an \"action\" is defined by each\n subclass.  As far as this interface is concerned, it is simply an arbitrary object.\n The environment also computes a reward for each action, and reports when the task\n has been terminated (meaning that no more actions may be taken).\n \"\"\"\n\n  def __init__(self, state_shape, n_actions, state_dtype=None):\n    \"\"\"Subclasses should call the superclass constructor in addition to doing their\n own initialization.\"\"\"\n    self.state_shape = state_shape\n    self.n_actions = n_actions\n    if state_dtype is None:\n      # Assume all arrays are float32.\n      if isinstance(state_shape[0], collections.Sequence):\n        self.state_dtype = [np.float32] * len(state_shape)\n      else:\n        self.state_dtype = np.float32\n    else:\n      self.state_dtype = state_dtype\n```", "```py\nclass TicTacToeEnvironment(dc.rl.Environment):\n  \"\"\"\n Play tictactoe against a randomly acting opponent\n \"\"\"\n  X = np.array([1.0, 0.0])\n  O = np.array([0.0, 1.0])\n  EMPTY = np.array([0.0, 0.0])\n\n  ILLEGAL_MOVE_PENALTY = -3.0\n  LOSS_PENALTY = -3.0\n  NOT_LOSS = 0.1\n  DRAW_REWARD = 5.0\n  WIN_REWARD = 10.0\n\n  def __init__(self):\n    super(TicTacToeEnvironment, self).__init__([(3, 3, 2)], 9)\n    self.terminated = None\n    self.reset()\n```", "```py\ndef reset(self):\n  self.terminated = False\n  self.state = [np.zeros(shape=(3, 3, 2), dtype=np.float32)]\n\n  # Randomize who goes first\n  if random.randint(0, 1) == 1:\n    move = self.get_O_move()\n    self.state[0][move[0]][move[1]] = TicTacToeEnvironment.O\n\ndef get_O_move(self):\n  empty_squares = []\n  for row in range(3):\n    for col in range(3):\n      if np.all(self.state[0][row][col] == TicTacToeEnvironment.EMPTY):\n        empty_squares.append((row, col))\n  return random.choice(empty_squares)\n```", "```py\ndef check_winner(self, player):\n  for i in range(3):\n    row = np.sum(self.state[0][i][:], axis=0)\n    if np.all(row == player * 3):\n      return True\n    col = np.sum(self.state[0][:][i], axis=0)\n    if np.all(col == player * 3):\n      return True\n\n  diag1 = self.state[0][0][0] + self.state[0][1][1] + self.state[0][2][2]\n  if np.all(diag1 == player * 3):\n    return True\n  diag2 = self.state[0][0][2] + self.state[0][1][1] + self.state[0][2][0]\n  if np.all(diag2 == player * 3):\n    return True\n  return False\n\ndef game_over(self):\n  for i in range(3):\n    for j in range(3):\n      if np.all(self.state[0][i][j] == TicTacToeEnvironment.EMPTY):\n        return False\n  return True\n```", "```py\ndef step(self, action):\n  self.state = copy.deepcopy(self.state)\n  row = action // 3\n  col = action % 3\n\n  # Illegal move -- the square is not empty\n  if not np.all(self.state[0][row][col] == TicTacToeEnvironment.EMPTY):\n    self.terminated = True\n    return TicTacToeEnvironment.ILLEGAL_MOVE_PENALTY\n\n  # Move X\n  self.state[0][row][col] = TicTacToeEnvironment.X\n\n  # Did X Win\n  if self.check_winner(TicTacToeEnvironment.X):\n    self.terminated = True\n    return TicTacToeEnvironment.WIN_REWARD\n\n  if self.game_over():\n    self.terminated = True\n    return TicTacToeEnvironment.DRAW_REWARD\n\n  move = self.get_O_move()\n  self.state[0][move[0]][move[1]] = TicTacToeEnvironment.O\n\n  # Did O Win\n  if self.check_winner(TicTacToeEnvironment.O):\n    self.terminated = True\n    return TicTacToeEnvironment.LOSS_PENALTY\n\n  if self.game_over():\n    self.terminated = True\n    return TicTacToeEnvironment.DRAW_REWARD\n\n  return TicTacToeEnvironment.NOT_LOSS\n```", "```py\nclass Layer(object):\n\n  def __init__(self, in_layers=None, **kwargs):\n    if \"name\" in kwargs:\n      self.name = kwargs[\"name\"]\n    else:\n      self.name = None\n    if in_layers is None:\n      in_layers = list()\n    if not isinstance(in_layers, Sequence):\n      in_layers = [in_layers]\n    self.in_layers = in_layers\n    self.variable_scope = \"\"\n    self.tb_input = None\n\n  def create_tensor(self, in_layers=None, **kwargs):\n    raise NotImplementedError(\"Subclasses must implement for themselves\")\n\n  def _get_input_tensors(self, in_layers):\n    \"\"\"Get the input tensors to his layer.\n\n Parameters\n ----------\n in_layers: list of Layers or tensors\n the inputs passed to create_tensor().  If None, this layer's inputs will\n be used instead.\n \"\"\"\n    if in_layers is None:\n      in_layers = self.in_layers\n    if not isinstance(in_layers, Sequence):\n      in_layers = [in_layers]\n    tensors = []\n    for input in in_layers:\n      tensors.append(tf.convert_to_tensor(input))\n    return tensors\n\ndef _convert_layer_to_tensor(value, dtype=None, name=None, as_ref=False):\n  return tf.convert_to_tensor(value.out_tensor, dtype=dtype, name=name)\n\ntf.register_tensor_conversion_function(Layer, _convert_layer_to_tensor)\n```", "```py\nclass Squeeze(Layer):\n\n  def __init__(self, in_layers=None, squeeze_dims=None, **kwargs):\n    self.squeeze_dims = squeeze_dims\n    super(Squeeze, self).__init__(in_layers, **kwargs)\n\n  def create_tensor(self, in_layers=None, **kwargs):\n    inputs = self._get_input_tensors(in_layers)\n    parent_tensor = inputs[0]\n    out_tensor = tf.squeeze(parent_tensor, squeeze_dims=self.squeeze_dims)\n    self.out_tensor = out_tensor\n    return out_tensor\n```", "```py\nclass Input(Layer):\n\n  def __init__(self, shape, dtype=tf.float32, **kwargs):\n    self._shape = tuple(shape)\n    self.dtype = dtype\n    super(Input, self).__init__(**kwargs)\n\n  def create_tensor(self, in_layers=None, **kwargs):\n    if in_layers is None:\n      in_layers = self.in_layers\n    out_tensor = tf.placeholder(dtype=self.dtype, shape=self._shape)\n    self.out_tensor = out_tensor\n    return out_tensor\n```", "```py\nclass TensorGraph(object):\n\n  def __init__(self,\n               batch_size=100,\n               random_seed=None,\n               graph=None,\n               learning_rate=0.001,\n               model_dir=None,\n               **kwargs):\n    \"\"\"\n Parameters\n ----------\n batch_size: int\n default batch size for training and evaluating\n graph: tensorflow.Graph\n the Graph in which to create Tensorflow objects.  If None, a new Graph\n is created.\n learning_rate: float or LearningRateSchedule\n the learning rate to use for optimization\n kwargs\n \"\"\"\n\n    # Layer Management\n    self.layers = dict()\n    self.features = list()\n    self.labels = list()\n    self.outputs = list()\n    self.task_weights = list()\n    self.loss = None\n    self.built = False\n    self.optimizer = None\n    self.learning_rate = learning_rate\n\n    # Singular place to hold Tensor objects which don't serialize\n    # See TensorGraph._get_tf() for more details on lazy construction\n    self.tensor_objects = {\n        \"Graph\": graph,\n        #\"train_op\": None,\n    }\n    self.global_step = 0\n    self.batch_size = batch_size\n    self.random_seed = random_seed\n    if model_dir is not None:\n      if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    else:\n      model_dir = tempfile.mkdtemp()\n      self.model_dir_is_temp = True\n    self.model_dir = model_dir\n    self.save_file = \"%s/%s\" % (self.model_dir, \"model\")\n    self.model_class = None\n```", "```py\ndef _add_layer(self, layer):\n  if layer.name is None:\n    layer.name = \"%s_%s\" % (layer.__class__.__name__, len(self.layers) + 1)\n  if layer.name in self.layers:\n    return\n  if isinstance(layer, Input):\n    self.features.append(layer)\n  self.layers[layer.name] = layer\n  for in_layer in layer.in_layers:\n    self._add_layer(in_layer)\n```", "```py\ndef topsort(self):\n\n  def add_layers_to_list(layer, sorted_layers):\n    if layer in sorted_layers:\n      return\n    for in_layer in layer.in_layers:\n      add_layers_to_list(in_layer, sorted_layers)\n    sorted_layers.append(layer)\n\n  sorted_layers = []\n  for l in self.features + self.labels + self.task_weights + self.outputs:\n    add_layers_to_list(l, sorted_layers)\n  add_layers_to_list(self.loss, sorted_layers)\n  return sorted_layers\n```", "```py\ndef build(self):\n  if self.built:\n    return\n  with self._get_tf(\"Graph\").as_default():\n    self._training_placeholder = tf.placeholder(dtype=tf.float32, shape=())\n    if self.random_seed is not None:\n      tf.set_random_seed(self.random_seed)\n    for layer in self.topsort():\n      with tf.name_scope(layer.name):\n        layer.create_tensor(training=self._training_placeholder)\n    self.session = tf.Session()\n\n    self.built = True\n```", "```py\ndef set_loss(self, layer):\n  self._add_layer(layer)\n  self.loss = layer\n\ndef add_output(self, layer):\n  self._add_layer(layer)\n  self.outputs.append(layer)\n\ndef set_optimizer(self, optimizer):\n  \"\"\"Set the optimizer to use for fitting.\"\"\"\n  self.optimizer = optimizer\n```", "```py\ndef get_layer_variables(self, layer):\n  \"\"\"Get the list of trainable variables in a layer of the graph.\"\"\"\n  if not self.built:\n    self.build()\n  with self._get_tf(\"Graph\").as_default():\n    if layer.variable_scope == \"\":\n      return []\n    return tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES, scope=layer.variable_scope)\n\ndef get_global_step(self):\n  return self._get_tf(\"GlobalStep\")\n\ndef _get_tf(self, obj):\n  \"\"\"Fetches underlying TensorFlow primitives.\n\n Parameters\n ----------\n obj: str\n If \"Graph\", returns tf.Graph instance. If \"Optimizer\", returns the\n optimizer. If \"train_op\", returns the train operation. If \"GlobalStep\" returns\n the global step.\n Returns\n -------\n TensorFlow Object\n \"\"\"\n\n  if obj in self.tensor_objects and self.tensor_objects[obj] is not None:\n    return self.tensor_objects[obj]\n  if obj == \"Graph\":\n    self.tensor_objects[\"Graph\"] = tf.Graph()\n  elif obj == \"Optimizer\":\n    self.tensor_objects[\"Optimizer\"] = tf.train.AdamOptimizer(\n        learning_rate=self.learning_rate,\n        beta1=0.9,\n        beta2=0.999,\n        epsilon=1e-7)\n  elif obj == \"GlobalStep\":\n    with self._get_tf(\"Graph\").as_default():\n      self.tensor_objects[\"GlobalStep\"] = tf.Variable(0, trainable=False)\n  return self._get_tf(obj)\n```", "```py\ndef restore(self):\n  \"\"\"Reload the values of all variables from the most recent checkpoint file.\"\"\"\n  if not self.built:\n    self.build()\n  last_checkpoint = tf.train.latest_checkpoint(self.model_dir)\n  if last_checkpoint is None:\n    raise ValueError(\"No checkpoint found\")\n  with self._get_tf(\"Graph\").as_default():\n    saver = tf.train.Saver()\n    saver.restore(self.session, last_checkpoint)\n```", "```py\nclass A3C(object):\n  \"\"\"\n Implements the Asynchronous Advantage Actor-Critic (A3C) algorithm.\n\n The algorithm is described in Mnih et al, \"Asynchronous Methods for Deep\n Reinforcement Learning\" (https://arxiv.org/abs/1602.01783).  This class\n requires the policy to output two quantities: a vector giving the probability\n of taking each action, and an estimate of the value function for the current\n state.  It optimizes both outputs at once using a loss that is the sum of three\n terms:\n\n 1\\. The policy loss, which seeks to maximize the discounted reward for each action.\n 2\\. The value loss, which tries to make the value estimate match the actual\n discounted reward that was attained at each step.\n 3\\. An entropy term to encourage exploration.\n\n This class only supports environments with discrete action spaces, not\n continuous ones.  The \"action\" argument passed to the environment is an\n integer, giving the index of the action to perform.\n\n This class supports Generalized Advantage Estimation as described in Schulman\n et al., \"High-Dimensional Continuous Control Using Generalized Advantage\n Estimation\" (https://arxiv.org/abs/1506.02438).  This is a method of trading\n off bias and variance in the advantage estimate, which can sometimes improve\n the rate of convergence.  Use the advantage_lambda parameter to adjust the\n tradeoff.\n \"\"\"\n  self._env = env\n  self.max_rollout_length = max_rollout_length\n  self.discount_factor = discount_factor\n  self.advantage_lambda = advantage_lambda\n  self.value_weight = value_weight\n  self.entropy_weight = entropy_weight\n  self._optimizer = None\n  (self._graph, self._features, self._rewards, self._actions,\n   self._action_prob, self._value, self._advantages) = self.build_graph(\n       None, \"global\", model_dir)\n  with self._graph._get_tf(\"Graph\").as_default():\n    self._session = tf.Session()\n```", "```py\ndef build_graph(self, tf_graph, scope, model_dir):\n  \"\"\"Construct a TensorGraph containing the policy and loss calculations.\"\"\"\n  state_shape = self._env.state_shape\n  features = []\n  for s in state_shape:\n    features.append(Input(shape=[None] + list(s), dtype=tf.float32))\n  d1 = Flatten(in_layers=features)\n  d2 = Dense(\n      in_layers=[d1],\n      activation_fn=tf.nn.relu,\n      normalizer_fn=tf.nn.l2_normalize,\n      normalizer_params={\"dim\": 1},\n      out_channels=64)\n  d3 = Dense(\n      in_layers=[d2],\n      activation_fn=tf.nn.relu,\n      normalizer_fn=tf.nn.l2_normalize,\n      normalizer_params={\"dim\": 1},\n      out_channels=32)\n  d4 = Dense(\n      in_layers=[d3],\n      activation_fn=tf.nn.relu,\n      normalizer_fn=tf.nn.l2_normalize,\n      normalizer_params={\"dim\": 1},\n      out_channels=16)\n  d4 = BatchNorm(in_layers=[d4])\n  d5 = Dense(in_layers=[d4], activation_fn=None, out_channels=9)\n  value = Dense(in_layers=[d4], activation_fn=None, out_channels=1)\n  value = Squeeze(squeeze_dims=1, in_layers=[value])\n  action_prob = SoftMax(in_layers=[d5])\n\n  rewards = Input(shape=(None,))\n  advantages = Input(shape=(None,))\n  actions = Input(shape=(None, self._env.n_actions))\n  loss = A3CLoss(\n      self.value_weight,\n      self.entropy_weight,\n      in_layers=[rewards, actions, action_prob, value, advantages])\n  graph = TensorGraph(\n      batch_size=self.max_rollout_length,\n      graph=tf_graph,\n      model_dir=model_dir)\n  for f in features:\n    graph._add_layer(f)\n  graph.add_output(action_prob)\n  graph.add_output(value)\n  graph.set_loss(loss)\n  graph.set_optimizer(self._optimizer)\n  with graph._get_tf(\"Graph\").as_default():\n    with tf.variable_scope(scope):\n      graph.build()\n  return graph, features, rewards, actions, action_prob, value, advantages\n```", "```py\nstate_shape = self._env.state_shape\nfeatures = []\nfor s in state_shape:\n  features.append(Input(shape=[None] + list(s), dtype=tf.float32))\n```", "```py\nrewards = Input(shape=(None,))\nadvantages = Input(shape=(None,))\nactions = Input(shape=(None, self._env.n_actions))\n```", "```py\nd1 = Flatten(in_layers=features)\nd2 = Dense(\n    in_layers=[d1],\n    activation_fn=tf.nn.relu,\n    normalizer_fn=tf.nn.l2_normalize,\n    normalizer_params={\"dim\": 1},\n    out_channels=64)\nd3 = Dense(\n    in_layers=[d2],\n    activation_fn=tf.nn.relu,\n    normalizer_fn=tf.nn.l2_normalize,\n    normalizer_params={\"dim\": 1},\n    out_channels=32)\nd4 = Dense(\n    in_layers=[d3],\n    activation_fn=tf.nn.relu,\n    normalizer_fn=tf.nn.l2_normalize,\n    normalizer_params={\"dim\": 1},\n    out_channels=16)\nd4 = BatchNorm(in_layers=[d4])\nd5 = Dense(in_layers=[d4], activation_fn=None, out_channels=9)\nvalue = Dense(in_layers=[d4], activation_fn=None, out_channels=1)\nvalue = Squeeze(squeeze_dims=1, in_layers=[value])\naction_prob = SoftMax(in_layers=[d5])\n```", "```py\nclass A3CLoss(Layer):\n  \"\"\"This layer computes the loss function for A3C.\"\"\"\n\n  def __init__(self, value_weight, entropy_weight, **kwargs):\n    super(A3CLoss, self).__init__(**kwargs)\n    self.value_weight = value_weight\n    self.entropy_weight = entropy_weight\n\n  def create_tensor(self, **kwargs):\n    reward, action, prob, value, advantage = [\n        layer.out_tensor for layer in self.in_layers\n    ]\n    prob = prob + np.finfo(np.float32).eps\n    log_prob = tf.log(prob)\n    policy_loss = -tf.reduce_mean(\n        advantage * tf.reduce_sum(action * log_prob, axis=1))\n    value_loss = tf.reduce_mean(tf.square(reward - value))\n    entropy = -tf.reduce_mean(tf.reduce_sum(prob * log_prob, axis=1))\n    self.out_tensor = policy_loss + self.value_weight * value_loss\n    - self.entropy_weight * entropy\n    return self.out_tensor\n```", "```py\nreward, action, prob, value, advantage = [\n    layer.out_tensor for layer in self.in_layers\n]\nprob = prob + np.finfo(np.float32).eps\nlog_prob = tf.log(prob)\n```", "```py\npolicy_loss = -tf.reduce_mean(\n    advantage * tf.reduce_sum(action * log_prob, axis=1))\n```", "```py\nvalue_loss = tf.reduce_mean(tf.square(reward - value))\n```", "```py\nentropy = -tf.reduce_mean(tf.reduce_sum(prob * log_prob, axis=1))\n```", "```py\nclass Worker(object):\n  \"\"\"A Worker object is created for each training thread.\"\"\"\n\n  def __init__(self, a3c, index):\n    self.a3c = a3c\n    self.index = index\n    self.scope = \"worker%d\" % index\n    self.env = copy.deepcopy(a3c._env)\n    self.env.reset()\n    (self.graph, self.features, self.rewards, self.actions, self.action_prob,\n     self.value, self.advantages) = a3c.build_graph(\n        a3c._graph._get_tf(\"Graph\"), self.scope, None)\n    with a3c._graph._get_tf(\"Graph\").as_default():\n      local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                                     self.scope)\n      global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                                      \"global\")\n      gradients = tf.gradients(self.graph.loss.out_tensor, local_vars)\n      grads_and_vars = list(zip(gradients, global_vars))\n      self.train_op = a3c._graph._get_tf(\"Optimizer\").apply_gradients(\n          grads_and_vars)\n      self.update_local_variables = tf.group(\n          * [tf.assign(v1, v2) for v1, v2 in zip(local_vars, global_vars)])\n      self.global_step = self.graph.get_global_step()\n```", "```py\ndef create_rollout(self):\n  \"\"\"Generate a rollout.\"\"\"\n  n_actions = self.env.n_actions\n  session = self.a3c._session\n  states = []\n  actions = []\n  rewards = []\n  values = []\n\n  # Generate the rollout.\n  for i in range(self.a3c.max_rollout_length):\n    if self.env.terminated:\n      break\n    state = self.env.state\n    states.append(state)\n    feed_dict = self.create_feed_dict(state)\n    results = session.run(\n        [self.action_prob.out_tensor, self.value.out_tensor],\n        feed_dict=feed_dict)\n    probabilities, value = results[:2]\n    action = np.random.choice(np.arange(n_actions), p=probabilities[0])\n    actions.append(action)\n    values.append(float(value))\n    rewards.append(self.env.step(action))\n\n  # Compute an estimate of the reward for the rest of the episode.\n  if not self.env.terminated:\n    feed_dict = self.create_feed_dict(self.env.state)\n    final_value = self.a3c.discount_factor * float(\n        session.run(self.value.out_tensor, feed_dict))\n  else:\n    final_value = 0.0\n  values.append(final_value)\n  if self.env.terminated:\n    self.env.reset()\n  return states, actions, np.array(rewards), np.array(values)\n```", "```py\ndef process_rollout(self, states, actions, rewards, values, step_count):\n  \"\"\"Train the network based on a rollout.\"\"\"\n\n  # Compute the discounted rewards and advantages.\n  if len(states) == 0:\n    # Rollout creation sometimes fails in multithreaded environment.\n    # Don't process if malformed\n    print(\"Rollout creation failed. Skipping\")\n    return\n\n  discounted_rewards = rewards.copy()\n  discounted_rewards[-1] += values[-1]\n  advantages = rewards - values[:-1] + self.a3c.discount_factor * np.array(\n      values[1:])\n  for j in range(len(rewards) - 1, 0, -1):\n    discounted_rewards[j-1] += self.a3c.discount_factor * discounted_rewards[j]\n    advantages[j-1] += (\n        self.a3c.discount_factor * self.a3c.advantage_lambda * advantages[j])\n   # Convert the actions to one-hot.\n  n_actions = self.env.n_actions\n  actions_matrix = []\n  for action in actions:\n    a = np.zeros(n_actions)\n    a[action] = 1.0\n    actions_matrix.append(a)\n\n  # Rearrange the states into the proper set of arrays.\n  state_arrays = [[] for i in range(len(self.features))]\n  for state in states:\n    for j in range(len(state)):\n      state_arrays[j].append(state[j])\n\n  # Build the feed dict and apply gradients.\n  feed_dict = {}\n  for f, s in zip(self.features, state_arrays):\n    feed_dict[f.out_tensor] = s\n  feed_dict[self.rewards.out_tensor] = discounted_rewards\n  feed_dict[self.actions.out_tensor] = actions_matrix\n  feed_dict[self.advantages.out_tensor] = advantages\n  feed_dict[self.global_step] = step_count\n  self.a3c._session.run(self.train_op, feed_dict=feed_dict)\n```", "```py\ndef run(self, step_count, total_steps):\n  with self.graph._get_tf(\"Graph\").as_default():\n    while step_count[0] < total_steps:\n      self.a3c._session.run(self.update_local_variables)\n      states, actions, rewards, values = self.create_rollout()\n      self.process_rollout(states, actions, rewards, values, step_count[0])\n      step_count[0] += len(actions)\n```", "```py\ndef fit(self,\n        total_steps,\n        max_checkpoints_to_keep=5,\n        checkpoint_interval=600,\n        restore=False):\n  \"\"\"Train the policy.\n\n Parameters\n ----------\n total_steps: int\n the total number of time steps to perform on the environment, across all\n rollouts on all threads\n max_checkpoints_to_keep: int\n the maximum number of checkpoint files to keep.  When this number is\n reached, older files are deleted.\n checkpoint_interval: float\n the time interval at which to save checkpoints, measured in seconds\n restore: bool\n if True, restore the model from the most recent checkpoint and continue\n training from there.  If False, retrain the model from scratch.\n \"\"\"\n  with self._graph._get_tf(\"Graph\").as_default():\n    step_count = [0]\n    workers = []\n    threads = []\n    for i in range(multiprocessing.cpu_count()):\n      workers.append(Worker(self, i))\n    self._session.run(tf.global_variables_initializer())\n    if restore:\n      self.restore()\n    for worker in workers:\n      thread = threading.Thread(\n          name=worker.scope,\n          target=lambda: worker.run(step_count, total_steps))\n      threads.append(thread)\n      thread.start()\n    variables = tf.get_collection(\n        tf.GraphKeys.GLOBAL_VARIABLES, scope=\"global\")\n    saver = tf.train.Saver(variables, max_to_keep=max_checkpoints_to_keep)\n    checkpoint_index = 0\n    while True:\n      threads = [t for t in threads if t.isAlive()]\n      if len(threads) > 0:\n        threads[0].join(checkpoint_interval)\n      checkpoint_index += 1\n      saver.save(\n          self._session, self._graph.save_file, global_step=checkpoint_index)\n      if len(threads) == 0:\n        break\n```"]