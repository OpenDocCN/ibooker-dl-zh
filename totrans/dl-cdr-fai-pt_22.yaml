- en: Chapter 17\. A Neural Net from the Foundations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第17章。基础神经网络
- en: This chapter begins a journey where we will dig deep into the internals of the
    models we used in the previous chapters. We will be covering many of the same
    things we’ve seen before, but this time around we’ll be looking much more closely
    at the implementation details, and much less closely at the practical issues of
    how and why things are as they are.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始了一段旅程，我们将深入研究我们在前几章中使用的模型的内部。我们将涵盖许多我们以前见过的相同内容，但这一次我们将更加密切地关注实现细节，而不那么密切地关注事物为什么是这样的实际问题。
- en: We will build everything from scratch, using only basic indexing into a tensor.
    We’ll write a neural net from the ground up, and then implement backpropagation
    manually so we know exactly what’s happening in PyTorch when we call `loss.backward`.
    We’ll also see how to extend PyTorch with custom *autograd* functions that allow
    us to specify our own forward and backward computations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从头开始构建一切，仅使用对张量的基本索引。我们将从头开始编写一个神经网络，然后手动实现反向传播，以便我们在调用`loss.backward`时确切地知道PyTorch中发生了什么。我们还将看到如何使用自定义*autograd*函数扩展PyTorch，允许我们指定自己的前向和后向计算。
- en: Building a Neural Net Layer from Scratch
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始构建神经网络层
- en: Let’s start by refreshing our understanding of how matrix multiplication is
    used in a basic neural network. Since we’re building everything up from scratch,
    we’ll use nothing but plain Python initially (except for indexing into PyTorch
    tensors), and then replace the plain Python with PyTorch functionality after we’ve
    seen how to create it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先刷新一下我们对基本神经网络中如何使用矩阵乘法的理解。由于我们正在从头开始构建一切，所以最初我们将仅使用纯Python（除了对PyTorch张量的索引），然后在看到如何创建后，将纯Python替换为PyTorch功能。
- en: Modeling a Neuron
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模神经元
- en: A neuron receives a given number of inputs and has an internal weight for each
    of them. It sums those weighted inputs to produce an output and adds an inner
    bias. In math, this can be written as
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元接收一定数量的输入，并为每个输入设置内部权重。它对这些加权输入求和以产生输出，并添加内部偏置。在数学上，这可以写成
- en: <math alttext="o u t equals sigma-summation Underscript i equals 1 Overscript
    n Endscripts x Subscript i Baseline w Subscript i Baseline plus b" display="block"><mrow><mi>o</mi>
    <mi>u</mi> <mi>t</mi> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mi>x</mi> <mi>i</mi></msub> <msub><mi>w</mi> <mi>i</mi></msub>
    <mo>+</mo> <mi>b</mi></mrow></math>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="o u t equals sigma-summation Underscript i equals 1 Overscript
    n Endscripts x Subscript i Baseline w Subscript i Baseline plus b" display="block"><mrow><mi>o</mi>
    <mi>u</mi> <mi>t</mi> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mi>x</mi> <mi>i</mi></msub> <msub><mi>w</mi> <mi>i</mi></msub>
    <mo>+</mo> <mi>b</mi></mrow></math>
- en: 'if we name our inputs <math alttext="left-parenthesis x 1 comma ellipsis comma
    x Subscript n Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow></math>
    , our weights <math alttext="left-parenthesis w 1 comma ellipsis comma w Subscript
    n Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>w</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>w</mi> <mi>n</mi></msub> <mo>)</mo></mrow></math>
    , and our bias <math alttext="b"><mi>b</mi></math> . In code this translates into
    the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将输入命名为<math alttext="left-parenthesis x 1 comma ellipsis comma x Subscript
    n Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow></math>，我们的权重为<math
    alttext="left-parenthesis w 1 comma ellipsis comma w Subscript n Baseline right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>w</mi> <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>w</mi>
    <mi>n</mi></msub> <mo>)</mo></mrow></math>，以及我们的偏置<math alttext="b"><mi>b</mi></math>。在代码中，这被翻译为以下内容：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This output is then fed into a nonlinear function called an *activation function*
    before being sent to another neuron. In deep learning, the most common of these
    is the *rectified linear unit*, or *ReLU*, which, as we’ve seen, is a fancy way
    of saying this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将此输出馈送到非线性函数中，称为*激活函数*，然后发送到另一个神经元。在深度学习中，最常见的是*修正线性单元*，或*ReLU*，正如我们所见，这是一种花哨的说法：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A deep learning model is then built by stacking a lot of those neurons in successive
    layers. We create a first layer with a certain number of neurons (known as the
    *hidden size*) and link all the inputs to each of those neurons. Such a layer
    is often called a *fully connected layer* or a *dense layer* (for densely connected),
    or a *linear layer*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过在连续的层中堆叠许多这些神经元来构建深度学习模型。我们创建一个具有一定数量的神经元（称为*隐藏大小*）的第一层，并将所有输入链接到每个神经元。这样的一层通常称为*全连接层*或*密集层*（用于密集连接），或*线性层*。
- en: 'It requires you to compute, for each `input` and each neuron with a given `weight`,
    the dot product:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 它要求您计算每个`input`和具有给定`weight`的每个神经元的点积：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you have done a little bit of linear algebra, you may remember that having
    a lot of those dot products happens when you do a *matrix multiplication*. More
    precisely, if our inputs are in a matrix `x` with a size of `batch_size` by `n_inputs`,
    and if we have grouped the weights of our neurons in a matrix `w` of size `n_neurons`
    by `n_inputs` (each neuron must have the same number of weights as it has inputs)
    as well as all the biases in a vector `b` of size `n_neurons`, then the output
    of this fully connected layer is
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对线性代数有一点了解，您可能会记得当您进行*矩阵乘法*时会发生许多这些点积。更准确地说，如果我们的输入在大小为`batch_size`乘以`n_inputs`的矩阵`x`中，并且如果我们已将神经元的权重分组在大小为`n_neurons`乘以`n_inputs`的矩阵`w`中（每个神经元必须具有与其输入相同数量的权重），以及将所有偏置分组在大小为`n_neurons`的向量`b`中，则此全连接层的输出为
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'where `@` represents the matrix product and `w.t()` is the transpose matrix
    of `w`. The output `y` is then of size `batch_size` by `n_neurons`, and in position
    `(i,j)` we have this (for the mathy folks out there):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`@`表示矩阵乘积，`w.t()`是`w`的转置矩阵。然后输出`y`的大小为`batch_size`乘以`n_neurons`，在位置`(i,j)`上我们有这个（对于数学爱好者）：
- en: <math alttext="y Subscript i comma j Baseline equals sigma-summation Underscript
    k equals 1 Overscript n Endscripts x Subscript i comma k Baseline w Subscript
    k comma j Baseline plus b Subscript j" display="block"><mrow><msub><mi>y</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msub><mi>x</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub> <msub><mi>w</mi> <mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>+</mo> <msub><mi>b</mi> <mi>j</mi></msub></mrow></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Or in code:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The transpose is necessary because in the mathematical definition of the matrix
    product `m @ n`, the coefficient `(i,j)` is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So the very basic operation we need is a matrix multiplication, as it’s what
    is hidden in the core of a neural net.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Multiplication from Scratch
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s write a function that computes the matrix product of two tensors, before
    we allow ourselves to use the PyTorch version of it. We will use only the indexing
    in PyTorch tensors:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We’ll need three nested `for` loops: one for the row indices, one for the column
    indices, and one for the inner sum. `ac` and `ar` stand for number of columns
    of `a` and number of rows of `a`, respectively (the same convention is followed
    for `b`), and we make sure calculating the matrix product is possible by checking
    that `a` has as many columns as `b` has rows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To test this out, we’ll pretend (using random matrices) that we’re working
    with a small batch of 5 MNIST images, flattened into `28*28` vectors, with a linear
    model to turn them into 10 activations:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s time our function, using the Jupyter “magic” command `%time`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: And see how that compares to PyTorch’s built-in `@`?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As we can see, in Python three nested loops is a bad idea! Python is a slow
    language, and this isn’t going to be efficient. We see here that PyTorch is around
    100,000 times faster than Python—and that’s before we even start using the GPU!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Where does this difference come from? PyTorch didn’t write its matrix multiplication
    in Python, but rather in C++ to make it fast. In general, whenever we do computations
    on tensors, we will need to *vectorize* them so that we can take advantage of
    the speed of PyTorch, usually by using two techniques: elementwise arithmetic
    and broadcasting.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Elementwise Arithmetic
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the basic operators (`+`, `-`, `*`, `/`, `>`, `<`, `==`) can be applied
    elementwise. That means if we write `a+b` for two tensors `a` and `b` that have
    the same shape, we will get a tensor composed of the sums of the elements of `a`
    and `b`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The Boolean operators will return an array of Booleans:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If we want to know if every element of `a` is less than the corresponding element
    in `b`, or if two tensors are equal, we need to combine those elementwise operations
    with `torch.all`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Reduction operations like `all`, `sum`, and `mean` return tensors with only
    one element, called *rank-0 tensors*. If you want to convert this to a plain Python
    Boolean or number, you need to call `.item`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The elementwise operations work on tensors of any rank, as long as they have
    the same shape:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'However, you can’t perform elementwise operations on tensors that don’t have
    the same shape (unless they are broadcastable, as discussed in the next section):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'With elementwise arithmetic, we can remove one of our three nested loops: we
    can multiply the tensors that correspond to the `i`-th row of `a` and the `j`-th
    column of `b` before summing all the elements, which will speed things up because
    the inner loop will now be executed by PyTorch at C speed.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: To access one column or row, we can simply write `a[i,:]` or `b[:,j]`. The `:`
    means take everything in that dimension. We could restrict this and take only
    a slice of that dimension by passing a range, like `1:5`, instead of just `:`.
    In that case, we would take the elements in columns 1 to 4 (the second number
    is noninclusive).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'One simplification is that we can always omit a trailing colon, so `a[i,:]`
    can be abbreviated to `a[i]`. With all of that in mind, we can write a new version
    of our matrix multiplication:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简化是我们总是可以省略尾随冒号，因此`a[i,:]`可以缩写为`a[i]`。考虑到所有这些，我们可以编写我们矩阵乘法的新版本：
- en: '[PRE25]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We’re already ~700 times faster, just by removing that inner `for` loop! And
    that’s just the beginning—with broadcasting, we can remove another loop and get
    an even more important speedup.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经快了约700倍，只是通过删除那个内部的`for`循环！这只是开始——通过广播，我们可以删除另一个循环并获得更重要的加速。
- en: Broadcasting
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广播
- en: As we discussed in [Chapter 4](ch04.xhtml#chapter_mnist_basics), *broadcasting*
    is a term introduced by the [Numpy Library](https://oreil.ly/nlV7Q) that describes
    how tensors of different ranks are treated during arithmetic operations. For instance,
    it’s obvious there is no way to add a 3×3 matrix with a 4×5 matrix, but what if
    we want to add one scalar (which can be represented as a 1×1 tensor) with a matrix?
    Or a vector of size 3 with a 3×4 matrix? In both cases, we can find a way to make
    sense of this operation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](ch04.xhtml#chapter_mnist_basics)中讨论的那样，*广播*是由[Numpy库](https://oreil.ly/nlV7Q)引入的一个术语，用于描述在算术操作期间如何处理不同秩的张量。例如，显然无法将3×3矩阵与4×5矩阵相加，但如果我们想将一个标量（可以表示为1×1张量）与矩阵相加呢？或者大小为3的向量与3×4矩阵？在这两种情况下，我们可以找到一种方法来理解这个操作。
- en: Broadcasting gives specific rules to codify when shapes are compatible when
    trying to do an elementwise operation, and how the tensor of the smaller shape
    is expanded to match the tensor of the bigger shape. It’s essential to master
    those rules if you want to be able to write code that executes quickly. In this
    section, we’ll expand our previous treatment of broadcasting to understand these
    rules.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 广播为编码规则提供了特定的规则，用于在尝试进行逐元素操作时确定形状是否兼容，以及如何扩展较小形状的张量以匹配较大形状的张量。如果您想要能够编写快速执行的代码，掌握这些规则是至关重要的。在本节中，我们将扩展我们之前对广播的处理，以了解这些规则。
- en: Broadcasting with a scalar
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用标量进行广播
- en: 'Broadcasting with a scalar is the easiest type of broadcasting. When we have
    a tensor `a` and a scalar, we just imagine a tensor of the same shape as `a` filled
    with that scalar and perform the operation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标量进行广播是最简单的广播类型。当我们有一个张量`a`和一个标量时，我们只需想象一个与`a`形状相同且填充有该标量的张量，并执行操作：
- en: '[PRE28]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How are we able to do this comparison? `0` is being *broadcast* to have the
    same dimensions as `a`. Note that this is done without creating a tensor full
    of zeros in memory (that would be inefficient).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何能够进行这种比较？`0`被*广播*以具有与`a`相同的维度。请注意，这是在不在内存中创建一个充满零的张量的情况下完成的（这将是低效的）。
- en: 'This is useful if you want to normalize your dataset by subtracting the mean
    (a scalar) from the entire dataset (a matrix) and dividing by the standard deviation
    (another scalar):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要通过减去均值（标量）来标准化数据集（矩阵）并除以标准差（另一个标量），这是很有用的：
- en: '[PRE30]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: What if you have different means for each row of the matrix? In that case, you
    will need to broadcast a vector to a matrix.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果矩阵的每行有不同的均值怎么办？在这种情况下，您需要将一个向量广播到一个矩阵。
- en: Broadcasting a vector to a matrix
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将向量广播到矩阵
- en: 'We can broadcast a vector to a matrix as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将一个向量广播到一个矩阵中：
- en: '[PRE32]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here the elements of `c` are expanded to make three rows that match, making
    the operation possible. Again, PyTorch doesn’t actually create three copies of
    `c` in memory. This is done by the `expand_as` method behind the scenes:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`c`的元素被扩展为使三行匹配，从而使操作成为可能。同样，PyTorch实际上并没有在内存中创建三个`c`的副本。这是由幕后的`expand_as`方法完成的：
- en: '[PRE36]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'If we look at the corresponding tensor, we can ask for its `storage` property
    (which shows the actual contents of the memory used for the tensor) to check there
    is no useless data stored:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看相应的张量，我们可以请求其`storage`属性（显示用于张量的内存实际内容）来检查是否存储了无用的数据：
- en: '[PRE38]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Even though the tensor officially has nine elements, only three scalars are
    stored in memory. This is possible thanks to the clever trick of giving that dimension
    a *stride* of 0. on that dimension (which means that when PyTorch looks for the
    next row by adding the stride, it doesn’t move):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管张量在官方上有九个元素，但内存中只存储了三个标量。这是可能的，这要归功于给该维度一个0步幅的巧妙技巧。在该维度上（这意味着当PyTorch通过添加步幅查找下一行时，它不会移动）：
- en: '[PRE40]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Since `m` is of size 3×3, there are two ways to do broadcasting. The fact it
    was done on the last dimension is a convention that comes from the rules of broadcasting
    and has nothing to do with the way we ordered our tensors. If instead we do this,
    we get the same result:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`m`的大小为3×3，有两种广播的方式。在最后一个维度上进行广播的事实是一种约定，这是来自广播规则的规定，与我们对张量排序的方式无关。如果我们这样做，我们会得到相同的结果：
- en: '[PRE42]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In fact, it’s only possible to broadcast a vector of size `n` with a matrix
    of size `m` by `n`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，只有通过`n`，我们才能将大小为`n`的向量广播到大小为`m`的矩阵中：
- en: '[PRE44]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This won’t work:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这不起作用：
- en: '[PRE46]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'If we want to broadcast in the other dimension, we have to change the shape
    of our vector to make it a 3×1 matrix. This is done with the `unsqueeze` method
    in PyTorch:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在另一个维度上进行广播，我们必须改变向量的形状，使其成为一个3×1矩阵。这可以通过PyTorch中的`unsqueeze`方法来实现：
- en: '[PRE48]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This time, `c` is expanded on the column side:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，`c`在列侧进行了扩展：
- en: '[PRE50]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'As before, only three scalars are stored in memory:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，只有三个标量存储在内存中：
- en: '[PRE52]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'And the expanded tensor has the right shape because the column dimension has
    a stride of 0:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展后的张量具有正确的形状，因为列维度的步幅为0：
- en: '[PRE54]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'With broadcasting, if we need to add dimensions, they are added by default
    at the beginning. When we were broadcasting before, PyTorch was executing `c.unsqueeze(0)`
    behind the scenes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用广播，如果需要添加维度，则默认情况下会在开头添加。在之前进行广播时，PyTorch在幕后执行了`c.unsqueeze(0)`：
- en: '[PRE56]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The `unsqueeze` command can be replaced by `None` indexing:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`unsqueeze`命令可以被`None`索引替换：'
- en: '[PRE58]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'You can always omit trailing colons, and `...` means all preceding dimensions:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以始终省略尾随冒号，`...`表示所有前面的维度：
- en: '[PRE60]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'With this, we can remove another `for` loop in our matrix multiplication function.
    Now, instead of multiplying `a[i]` with `b[:,j]`, we can multiply `a[i]` with
    the whole matrix `b` using broadcasting, and then sum the results:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以在我们的矩阵乘法函数中删除另一个`for`循环。现在，我们不再将`a[i]`乘以`b[:,j]`，而是使用广播将`a[i]`乘以整个矩阵`b`，然后对结果求和：
- en: '[PRE62]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: We’re now 3,700 times faster than our first implementation! Before we move on,
    let’s discuss the rules of broadcasting in a little more detail.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们比第一次实现快了3700倍！在继续之前，让我们更详细地讨论一下广播规则。
- en: Broadcasting rules
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广播规则
- en: 'When operating on two tensors, PyTorch compares their shapes elementwise. It
    starts with the *trailing dimensions* and works its way backward, adding 1 when
    it meets empty dimensions. Two dimensions are *compatible* when one of the following
    is true:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作两个张量时，PyTorch会逐个元素地比较它们的形状。它从*尾部维度*开始，逆向工作，在遇到空维度时添加1。当以下情况之一为真时，两个维度是*兼容*的：
- en: They are equal.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是相等的。
- en: One of them is 1, in which case that dimension is broadcast to make it the same
    as the other.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中之一是1，此时该维度会被广播以使其与其他维度相同。
- en: 'Arrays do not need to have the same number of dimensions. For example, if you
    have a 256×256×3 array of RGB values, and you want to scale each color in the
    image by a different value, you can multiply the image by a one-dimensional array
    with three values. Lining up the sizes of the trailing axes of these arrays according
    to the broadcast rules shows that they are compatible:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 数组不需要具有相同数量的维度。例如，如果您有一个256×256×3的RGB值数组，并且想要按不同值缩放图像中的每种颜色，您可以将图像乘以一个具有三个值的一维数组。根据广播规则排列这些数组的尾部轴的大小表明它们是兼容的：
- en: '[PRE65]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'However, a 2D tensor of size 256×256 isn’t compatible with our image:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个大小为256×256的2D张量与我们的图像不兼容：
- en: '[PRE66]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'In our earlier examples with a 3×3 matrix and a vector of size 3, broadcasting
    was done on the rows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们早期的例子中，使用了一个3×3矩阵和一个大小为3的向量，广播是在行上完成的：
- en: '[PRE67]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: As an exercise, try to determine what dimensions to add (and where) when you
    need to normalize a batch of images of size `64 x 3 x 256 x 256` with vectors
    of three elements (one for the mean and one for the standard deviation).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，尝试确定何时需要添加维度（以及在何处），以便将大小为`64 x 3 x 256 x 256`的图像批次与三个元素的向量（一个用于均值，一个用于标准差）进行归一化。
- en: Another useful way of simplifying tensor manipulations is the use of Einstein
    summation convention.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种简化张量操作的有用方法是使用爱因斯坦求和约定。
- en: Einstein Summation
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 爱因斯坦求和
- en: 'Before using the PyTorch operation `@` or `torch.matmul`, there is one last
    way we can implement matrix multiplication: *Einstein summation* (`einsum`). This
    is a compact representation for combining products and sums in a general way.
    We write an equation like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用PyTorch操作`@`或`torch.matmul`之前，我们可以实现矩阵乘法的最后一种方法：*爱因斯坦求和*（`einsum`）。这是一种将乘积和求和以一般方式组合的紧凑表示。我们可以写出这样的方程：
- en: '[PRE68]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The lefthand side represents the operands dimensions, separated by commas. Here
    we have two tensors that each have two dimensions (`i,k` and `k,j`). The righthand
    side represents the result dimensions, so here we have a tensor with two dimensions
    `i,j`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧表示操作数的维度，用逗号分隔。这里我们有两个分别具有两个维度（`i,k`和`k,j`）的张量。右侧表示结果维度，所以这里我们有一个具有两个维度`i,j`的张量。
- en: 'The rules of Einstein summation notation are as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 爱因斯坦求和符号的规则如下：
- en: Repeated indices are implicitly summed over.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复的索引会被隐式求和。
- en: Each index can appear at most twice in any term.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个索引在任何项中最多只能出现两次。
- en: Each term must contain identical nonrepeated indices.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个项必须包含相同的非重复索引。
- en: So in our example, since `k` is repeated, we sum over that index. In the end,
    the formula represents the matrix obtained when we put in (`i,j`) the sum of all
    the coefficients (`i,k`) in the first tensor multiplied by the coefficients (`k,j`)
    in the second tensor… which is the matrix product!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的例子中，由于`k`是重复的，我们对该索引求和。最终，该公式表示当我们在（`i,j`）中放入所有第一个张量中的系数（`i,k`）与第二个张量中的系数（`k,j`）相乘的总和时得到的矩阵……这就是矩阵乘积！
- en: 'Here is how we can code this in PyTorch:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何在PyTorch中编写这段代码：
- en: '[PRE69]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Einstein summation is a very practical way of expressing operations involving
    indexing and sum of products. Note that you can have one member on the lefthand
    side. For instance,
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 爱因斯坦求和是一种非常实用的表达涉及索引和乘积和的操作的方式。请注意，您可以在左侧只有一个成员。例如，
- en: '[PRE70]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'returns the transpose of the matrix `a`. You can also have three or more members:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 返回矩阵`a`的转置。您也可以有三个或更多成员：
- en: '[PRE71]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This will return a vector of size `b`, where the `k`-th coordinate is the sum
    of `a[k,i] b[i,j] c[k,j]`. This notation is particularly convenient when you have
    more dimensions because of batches. For example, if you have two batches of matrices
    and want to compute the matrix product per batch, you could do this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个大小为`b`的向量，其中第`k`个坐标是`a[k,i] b[i,j] c[k,j]`的总和。当您有更多维度时，这种表示特别方便，因为有批次。例如，如果您有两批次的矩阵并且想要计算每批次的矩阵乘积，您可以这样做：
- en: '[PRE72]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Let’s go back to our new `matmul` implementation using `einsum` and look at
    its speed:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到使用`einsum`实现的新`matmul`，看看它的速度：
- en: '[PRE73]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: As you can see, not only is it practical, but it’s *very* fast. `einsum` is
    often the fastest way to do custom operations in PyTorch, without diving into
    C++ and CUDA. (But it’s generally not as fast as carefully optimized CUDA code,
    as you see from the results in [“Matrix Multiplication from Scratch”](#matrix_multiplication).)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，它不仅实用，而且*非常*快。`einsum`通常是在PyTorch中执行自定义操作的最快方式，而无需深入研究C++和CUDA。（但通常不如精心优化的CUDA代码快，正如您从[“从头开始的矩阵乘法”](#matrix_multiplication)的结果中看到的。）
- en: Now that we know how to implement a matrix multiplication from scratch, we are
    ready to build our neural net—specifically, its forward and backward passes—using
    just matrix multiplication.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何从头开始实现矩阵乘法，我们准备构建我们的神经网络——具体来说，是它的前向和后向传递——只使用矩阵乘法。
- en: The Forward and Backward Passes
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向和后向传递
- en: As we saw in [Chapter 4](ch04.xhtml#chapter_mnist_basics), to train a model,
    we will need to compute all the gradients of a given loss with respect to its
    parameters, which is known as the *backward pass*. In a *forward pass*, where
    we compute the output of the model on a given input, based on the matrix products.
    As we define our first neural net, we will also delve into the problem of properly
    initializing the weights, which is crucial for making training start properly.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](ch04.xhtml#chapter_mnist_basics)中看到的，为了训练一个模型，我们需要计算给定损失对其参数的所有梯度，这被称为*反向传播*。在*前向传播*中，我们根据矩阵乘积计算给定输入上模型的输出。当我们定义我们的第一个神经网络时，我们还将深入研究适当初始化权重的问题，这对于使训练正确开始至关重要。
- en: Defining and Initializing a Layer
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义和初始化一个层
- en: 'We will take the example of a two-layer neural net first. As we’ve seen, one
    layer can be expressed as `y = x @ w + b`, with `x` our inputs, `y` our outputs,
    `w` the weights of the layer (which is of size number of inputs by number of neurons
    if we don’t transpose as before), and `b` is the bias vector:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们将以两层神经网络为例。正如我们所看到的，一层可以表示为`y = x @ w + b`，其中`x`是我们的输入，`y`是我们的输出，`w`是该层的权重（如果我们不像之前那样转置，则大小为输入数量乘以神经元数量），`b`是偏置向量：
- en: '[PRE75]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We can stack the second layer on top of the first, but since mathematically
    the composition of two linear operations is another linear operation, this makes
    sense only if we put something nonlinear in the middle, called an activation function.
    As mentioned at the beginning of this chapter, in deep learning applications the
    activation function most commonly used is a ReLU, which returns the maximum of
    `x` and `0`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将第二层叠加在第一层上，但由于数学上两个线性操作的组合是另一个线性操作，只有在中间放入一些非线性的东西才有意义，称为激活函数。正如本章开头提到的，在深度学习应用中，最常用的激活函数是ReLU，它返回`x`和`0`的最大值。
- en: 'We won’t actually train our model in this chapter, so we’ll use random tensors
    for our inputs and targets. Let’s say our inputs are 200 vectors of size 100,
    which we group into one batch, and our targets are 200 random floats:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们实际上不会训练我们的模型，因此我们将为我们的输入和目标使用随机张量。假设我们的输入是大小为100的200个向量，我们将它们分组成一个批次，我们的目标是200个随机浮点数：
- en: '[PRE76]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'For our two-layer model, we will need two weight matrices and two bias vectors.
    Let’s say we have a hidden size of 50 and the output size is 1 (for one of our
    inputs, the corresponding output is one float in this toy example). We initialize
    the weights randomly and the bias at zero:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的两层模型，我们将需要两个权重矩阵和两个偏置向量。假设我们的隐藏大小为50，输出大小为1（对于我们的输入之一，相应的输出在这个玩具示例中是一个浮点数）。我们随机初始化权重，偏置为零：
- en: '[PRE77]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Then the result of our first layer is simply this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们的第一层的结果就是这样的：
- en: '[PRE78]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Note that this formula works with our batch of inputs, and returns a batch
    of hidden state: `l1` is a matrix of size 200 (our batch size) by 50 (our hidden
    size).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个公式适用于我们的输入批次，并返回一个隐藏状态批次：`l1`是一个大小为200（我们的批次大小）乘以50（我们的隐藏大小）的矩阵。
- en: 'There is a problem with the way our model was initialized, however. To understand
    it, we need to look at the mean and standard deviation (std) of `l1`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的模型初始化方式存在问题。要理解这一点，我们需要查看`l1`的均值和标准差（std）：
- en: '[PRE80]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The mean is close to zero, which is understandable since both our input and
    weight matrices have means close to zero. But the standard deviation, which represents
    how far away our activations go from the mean, went from 1 to 10\. This is a really
    big problem because that’s with just one layer. Modern neural nets can have hundreds
    of layers, so if each of them multiplies the scale of our activations by 10, we
    won’t have numbers representable by a computer by the end of the last layer.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 均值接近零，这是可以理解的，因为我们的输入和权重矩阵的均值都接近零。但标准差，表示我们的激活离均值有多远，从1变为10。这是一个真正的大问题，因为这只是一个层。现代神经网络可以有数百层，因此如果每一层将我们的激活的规模乘以10，到了最后一层，我们将无法用计算机表示数字。
- en: 'Indeed, if we make just 50 multiplications between `x` and random matrices
    of size 100×100, we’ll have this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果我们在`x`和大小为100×100的随机矩阵之间进行50次乘法运算，我们将得到这个结果：
- en: '[PRE82]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The result is `nans` everywhere. So maybe the scale of our matrix was too big,
    and we need to have smaller weights? But if we use too small weights, we will
    have the opposite problem—the scale of our activations will go from 1 to 0.1,
    and after 100 layers we’ll be left with zeros everywhere:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是到处都是`nan`。也许我们的矩阵的规模太大了，我们需要更小的权重？但如果我们使用太小的权重，我们将遇到相反的问题-我们的激活的规模将从1变为0.1，在100层之后，我们将到处都是零：
- en: '[PRE84]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: So we have to scale our weight matrices exactly right so that the standard deviation
    of our activations stays at 1\. We can compute the exact value to use mathematically,
    as illustrated by Xavier Glorot and Yoshua Bengio in [“Understanding the Difficulty
    of Training Deep Feedforward Neural Networks”](https://oreil.ly/9tiTC). The right
    scale for a given layer is <math alttext="1 slash StartRoot n Subscript i n Baseline
    EndRoot"><mrow><mn>1</mn> <mo>/</mo> <msqrt><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></msqrt></mrow></math>
    , where <math alttext="n Subscript i n"><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
    represents the number of inputs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须精确地缩放我们的权重矩阵，以使我们的激活的标准差保持在1。我们可以通过数学计算出要使用的确切值，正如Xavier Glorot和Yoshua
    Bengio在[“理解训练深度前馈神经网络的困难”](https://oreil.ly/9tiTC)中所示。给定层的正确比例是<math alttext="1
    slash StartRoot n Subscript i n Baseline EndRoot"><mrow><mn>1</mn> <mo>/</mo>
    <msqrt><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></msqrt></mrow></math>，其中<math
    alttext="n Subscript i n"><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>代表输入的数量。
- en: 'In our case, if we have 100 inputs, we should scale our weight matrices by
    0.1:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，如果有100个输入，我们应该将我们的权重矩阵缩放为0.1：
- en: '[PRE86]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Finally, some numbers that are neither zeros nor `nan`! Notice how stable the
    scale of our activations is, even after those 50 fake layers:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 终于，一些既不是零也不是`nan`的数字！请注意，即使经过了那50个虚假层，我们的激活的规模仍然是稳定的：
- en: '[PRE88]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: If you play a little bit with the value for scale, you’ll notice that even a
    slight variation from 0.1 will get you either to very small or very large numbers,
    so initializing the weights properly is extremely important.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你稍微调整一下scale的值，你会注意到即使从0.1稍微偏离，你会得到非常小或非常大的数字，因此正确初始化权重非常重要。
- en: 'Let’s go back to our neural net. Since we messed a bit with our inputs, we
    need to redefine them:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的神经网络。由于我们稍微改变了我们的输入，我们需要重新定义它们：
- en: '[PRE90]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'And for our weights, we’ll use the right scale, which is known as *Xavier initialization*
    (or *Glorot initialization*):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的权重，我们将使用正确的scale，这被称为*Xavier初始化*（或*Glorot初始化*）：
- en: '[PRE91]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Now if we compute the result of the first layer, we can check that the mean
    and standard deviation are under control:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们计算第一层的结果，我们可以检查均值和标准差是否受控制：
- en: '[PRE92]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Very good. Now we need to go through a ReLU, so let’s define one. A ReLU removes
    the negatives and replaces them with zeros, which is another way of saying it
    clamps our tensor at zero:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 非常好。现在我们需要经过一个ReLU，所以让我们定义一个。ReLU去除负数并用零替换它们，这另一种说法是它将我们的张量夹在零处：
- en: '[PRE94]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'We pass our activations through this:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过这个激活：
- en: '[PRE95]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'And we’re back to square one: the mean of our activations has gone to 0.4 (which
    is understandable since we removed the negatives), and the std went down to 0.58\.
    So like before, after a few layers we will probably wind up with zeros:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们回到原点：我们的激活均值变为0.4（这是可以理解的，因为我们去除了负数），标准差下降到0.58。所以像以前一样，经过几层后我们可能最终会得到零：
- en: '[PRE97]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'This means our initialization wasn’t right. Why? At the time Glorot and Bengio
    wrote their article, the most popular activation in a neural net was the hyperbolic
    tangent (tanh, which is the one they used), and that initialization doesn’t account
    for our ReLU. Fortunately, someone else has done the math for us and computed
    the right scale for us to use. In [“Delving Deep into Rectifiers: Surpassing Human-Level
    Performance”](https://oreil.ly/-_quA) (which we’ve seen before—it’s the article
    that introduced the ResNet), Kaiming He et al. show that we should use the following
    scale instead: <math alttext="StartRoot 2 slash n Subscript i n Baseline EndRoot"><msqrt><mrow><mn>2</mn>
    <mo>/</mo> <msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></msqrt></math>
    , where <math alttext="n Subscript i n"><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
    is the number of inputs of our model. Let’s see what this gives us:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的初始化不正确。为什么？在Glorot和Bengio撰写他们的文章时，神经网络中最流行的激活函数是双曲正切（tanh，他们使用的那个），而该初始化并没有考虑到我们的ReLU。幸运的是，有人已经为我们计算出了正确的scale供我们使用。在[“深入研究整流器：超越人类水平的性能”](https://oreil.ly/-_quA)（我们之前见过的文章，介绍了ResNet），Kaiming
    He等人表明我们应该使用以下scale代替：<math alttext="StartRoot 2 slash n Subscript i n Baseline
    EndRoot"><msqrt><mrow><mn>2</mn> <mo>/</mo> <msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></msqrt></math>，其中<math
    alttext="n Subscript i n"><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>是我们模型的输入数量。让我们看看这给我们带来了什么：
- en: '[PRE99]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'That’s better: our numbers aren’t all zeroed this time. So let’s go back to
    the definition of our neural net and use this initialization (which is named *Kaiming
    initialization* or *He initialization*):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 好了：这次我们的数字不全为零。所以让我们回到我们神经网络的定义，并使用这个初始化（被称为*Kaiming初始化*或*He初始化*）：
- en: '[PRE101]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Let’s look at the scale of our activations after going through the first linear
    layer and ReLU:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看通过第一个线性层和ReLU后激活的规模：
- en: '[PRE103]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Much better! Now that our weights are properly initialized, we can define our
    whole model:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了！现在我们的权重已经正确初始化，我们可以定义我们的整个模型：
- en: '[PRE105]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: This is the forward pass. Now all that’s left to do is to compare our output
    to the labels we have (random numbers, in this example) with a loss function.
    In this case, we will use the mean squared error. (It’s a toy problem, and this
    is the easiest loss function to use for what is next, computing the gradients.)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前向传播。现在剩下的就是将我们的输出与我们拥有的标签（在这个例子中是随机数）进行比较，使用损失函数。在这种情况下，我们将使用均方误差。（这是一个玩具问题，这是下一步计算梯度所使用的最简单的损失函数。）
- en: 'The only subtlety is that our outputs and targets don’t have exactly the same
    shape—after going though the model, we get an output like this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的微妙之处在于我们的输出和目标形状并不完全相同——经过模型后，我们得到这样的输出：
- en: '[PRE106]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'To get rid of this trailing 1 dimension, we use the `squeeze` function:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了去掉这个多余的1维，我们使用`squeeze`函数：
- en: '[PRE108]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'And now we are ready to compute our loss:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备计算我们的损失：
- en: '[PRE109]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: That’s all for the forward pass—let’s now look at the gradients.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播到此结束，现在让我们看一下梯度。
- en: Gradients and the Backward Pass
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度和反向传播
- en: We’ve seen that PyTorch computes all the gradients we need with a magic call
    to `loss.backward`, but let’s explore what’s happening behind the scenes.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到PyTorch通过一个神奇的调用`loss.backward`计算出我们需要的所有梯度，但让我们探究一下背后发生了什么。
- en: 'Now comes the part where we need to compute the gradients of the loss with
    respect to all the weights of our model, so all the floats in `w1`, `b1`, `w2`,
    and `b2`. For this, we will need a bit of math—specifically, the *chain rule*.
    This is the rule of calculus that guides how we can compute the derivative of
    a composed function:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要计算损失相对于模型中所有权重的梯度，即`w1`、`b1`、`w2`和`b2`中的所有浮点数。为此，我们需要一点数学，具体来说是*链式法则*。这是指导我们如何计算复合函数导数的微积分规则：
- en: <math alttext="left-parenthesis g ring f right-parenthesis prime left-parenthesis
    x right-parenthesis equals g prime left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis f prime left-parenthesis x right-parenthesis" display="block"><mrow><msup><mrow><mo>(</mo><mi>g</mi><mo>∘</mo><mi>f</mi><mo>)</mo></mrow>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>g</mi>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="left-parenthesis g ring f right-parenthesis prime left-parenthesis
    x right-parenthesis equals g prime left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis f prime left-parenthesis x right-parenthesis" display="block"><mrow><msup><mrow><mo>(</mo><mi>g</mi><mo>∘</mo><mi>f</mi><mo>)</mo></mrow>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>g</mi>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></mrow></math>
- en: Jeremy Says
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jeremy 说
- en: 'I find this notation hard to wrap my head around, so instead I like to think
    of it as follows: if `y = g(u)` and `u=f(x)`, then `dy/dx = dy/du * du/dx`. The
    two notations mean the same thing, so use whatever works for you.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现这种符号很难理解，所以我喜欢这样想：如果 `y = g(u)` 和 `u=f(x)`，那么 `dy/dx = dy/du * du/dx`。这两种符号意思相同，所以使用任何一种都可以。
- en: 'Our loss is a big composition of different functions: mean squared error (which
    is, in turn, the composition of a mean and a power of two), the second linear
    layer, a ReLU, and the first linear layer. For instance, if we want the gradients
    of the loss with respect to `b2` and our loss is defined by the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的损失是不同函数的大组合：均方误差（实际上是均值和平方的组合），第二个线性层，一个ReLU，和第一个线性层。例如，如果我们想要损失相对于 `b2`
    的梯度，而我们的损失由以下定义：
- en: '[PRE110]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'The chain rule tells us that we have this:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则告诉我们我们有这个：
- en: <math alttext="StartFraction d l o s s Over d b 2 EndFraction equals StartFraction
    d l o s s Over d o u t EndFraction times StartFraction d o u t Over d b 2 EndFraction
    equals StartFraction d Over d o u t EndFraction m s e left-parenthesis o u t comma
    y right-parenthesis times StartFraction d Over d b 2 EndFraction l i n left-parenthesis
    l 2 comma w 2 comma b 2 right-parenthesis" display="block"><mrow><mfrac><mrow><mtext>d</mtext><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow>
    <mrow><mtext>d</mtext><msub><mi>b</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo>
    <mfrac><mrow><mtext>d</mtext><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow> <mrow><mtext>d</mtext><mi>o</mi><mi>u</mi><mi>t</mi></mrow></mfrac>
    <mo>×</mo> <mfrac><mrow><mtext>d</mtext><mi>o</mi><mi>u</mi><mi>t</mi></mrow>
    <mrow><mtext>d</mtext><msub><mi>b</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo>
    <mfrac><mtext>d</mtext> <mrow><mtext>d</mtext><mi>o</mi><mi>u</mi><mi>t</mi></mrow></mfrac>
    <mi>m</mi> <mi>s</mi> <mi>e</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>×</mo> <mfrac><mtext>d</mtext> <mrow><mtext>d</mtext><msub><mi>b</mi>
    <mn>2</mn></msub></mrow></mfrac> <mi>l</mi> <mi>i</mi> <mi>n</mi> <mrow><mo>(</mo>
    <msub><mi>l</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>w</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>b</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction d l o s s Over d b 2 EndFraction equals StartFraction
    d l o s s Over d o u t EndFraction times StartFraction d o u t Over d b 2 EndFraction
    equals StartFraction d Over d o u t EndFraction m s e left-parenthesis o u t comma
    y right-parenthesis times StartFraction d Over d b 2 EndFraction l i n left-parenthesis
    l 2 comma w 2 comma b 2 right-parenthesis" display="block"><mrow><mfrac><mrow><mtext>d</mtext><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow>
    <mrow><mtext>d</mtext><msub><mi>b</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo>
    <mfrac><mrow><mtext>d</mtext><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow> <mrow><mtext>d</mtext><mi>o</mi><mi>u</mi><mi>t</mi></mrow></mfrac>
    <mo>×</mo> <mfrac><mrow><mtext>d</mtext><mi>o</mi><mi>u</mi><mi>t</mi></mrow>
    <mrow><mtext>d</mtext><msub><mi>b</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo>
    <mfrac><mtext>d</mtext> <mrow><mtext>d</mtext><mi>o</mi><mi>u</mi><mi>t</mi></mrow></mfrac>
    <mi>m</mi> <mi>s</mi> <mi>e</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>×</mo> <mfrac><mtext>d</mtext> <mrow><mtext>d</mtext><msub><mi>b</mi>
    <mn>2</mn></msub></mrow></mfrac> <mi>l</mi> <mi>i</mi> <mi>n</mi> <mrow><mo>(</mo>
    <msub><mi>l</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>w</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>b</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
- en: To compute the gradients of the loss with respect to <math alttext="b 2"><msub><mi>b</mi>
    <mn>2</mn></msub></math> , we first need the gradients of the loss with respect
    to our output <math alttext="o u t"><mrow><mi>o</mi> <mi>u</mi> <mi>t</mi></mrow></math>
    . It’s the same if we want the gradients of the loss with respect to <math alttext="w
    2"><msub><mi>w</mi> <mn>2</mn></msub></math> . Then, to get the gradients of the
    loss with respect to <math alttext="b 1"><msub><mi>b</mi> <mn>1</mn></msub></math>
    or <math alttext="w 1"><msub><mi>w</mi> <mn>1</mn></msub></math> , we will need
    the gradients of the loss with respect to <math alttext="l 1"><msub><mi>l</mi>
    <mn>1</mn></msub></math> , which in turn requires the gradients of the loss with
    respect to <math alttext="l 2"><msub><mi>l</mi> <mn>2</mn></msub></math> , which
    will need the gradients of the loss with respect to <math alttext="o u t"><mrow><mi>o</mi>
    <mi>u</mi> <mi>t</mi></mrow></math> .
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算损失相对于 <math alttext="b 2"><msub><mi>b</mi> <mn>2</mn></msub></math> 的梯度，我们首先需要损失相对于我们的输出
    <math alttext="o u t"><mrow><mi>o</mi> <mi>u</mi> <mi>t</mi></mrow></math> 的梯度。如果我们想要损失相对于
    <math alttext="w 2"><msub><mi>w</mi> <mn>2</mn></msub></math> 的梯度也是一样的。然后，要得到损失相对于
    <math alttext="b 1"><msub><mi>b</mi> <mn>1</mn></msub></math> 或 <math alttext="w
    1"><msub><mi>w</mi> <mn>1</mn></msub></math> 的梯度，我们将需要损失相对于 <math alttext="l 1"><msub><mi>l</mi>
    <mn>1</mn></msub></math> 的梯度，这又需要损失相对于 <math alttext="l 2"><msub><mi>l</mi> <mn>2</mn></msub></math>
    的梯度，这将需要损失相对于 <math alttext="o u t"><mrow><mi>o</mi> <mi>u</mi> <mi>t</mi></mrow></math>
    的梯度。
- en: 'So to compute all the gradients we need for the update, we need to begin from
    the output of the model and work our way *backward*, one layer after the other—which
    is why this step is known as *backpropagation*. We can automate it by having each
    function we implemented (`relu`, `mse`, `lin`) provide its backward step: that
    is, how to derive the gradients of the loss with respect to the input(s) from
    the gradients of the loss with respect to the output.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了计算更新所需的所有梯度，我们需要从模型的输出开始，逐层向后工作，一层接一层地——这就是为什么这一步被称为*反向传播*。我们可以通过让我们实现的每个函数（`relu`、`mse`、`lin`）提供其反向步骤来自动化它：也就是说，如何从损失相对于输出的梯度推导出损失相对于输入的梯度。
- en: Here we populate those gradients in an attribute of each tensor, a bit like
    PyTorch does with `.grad`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将这些梯度填充到每个张量的属性中，有点像PyTorch在`.grad`中所做的那样。
- en: 'The first are the gradients of the loss with respect to the output of our model
    (which is the input of the loss function). We undo the `squeeze` we did in `mse`,
    and then we use the formula that gives us the derivative of <math alttext="x squared"><msup><mi>x</mi>
    <mn>2</mn></msup></math> : <math alttext="2 x"><mrow><mn>2</mn> <mi>x</mi></mrow></math>
    . The derivative of the mean is just 1/*n*, where *n* is the number of elements
    in our input:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是我们模型输出（损失函数的输入）相对于损失的梯度。我们撤消了`mse`中的`squeeze`，然后我们使用给出<math alttext="x squared"><msup><mi>x</mi>
    <mn>2</mn></msup></math>的导数的公式：<math alttext="2 x"><mrow><mn>2</mn> <mi>x</mi></mrow></math>。均值的导数只是1/*n*，其中*n*是我们输入中的元素数：
- en: '[PRE111]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'For the gradients of the ReLU and our linear layer, we use the gradients of
    the loss with respect to the output (in `out.g`) and apply the chain rule to compute
    the gradients of the loss with respect to the output (in `inp.g`). The chain rule
    tells us that `inp.g = relu''(inp) * out.g`. The derivative of `relu` is either
    0 (when inputs are negative) or 1 (when inputs are positive), so this gives us
    the following:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ReLU和我们的线性层的梯度，我们使用相对于输出的损失的梯度（在`out.g`中）并应用链式法则来计算相对于输出的损失的梯度（在`inp.g`中）。链式法则告诉我们`inp.g
    = relu'(inp) * out.g`。`relu`的导数要么是0（当输入为负数时），要么是1（当输入为正数时），因此这给出了以下结果：
- en: '[PRE112]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'The scheme is the same to compute the gradients of the loss with respect to
    the inputs, weights, and bias in the linear layer:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失相对于线性层中的输入、权重和偏差的梯度的方案是相同的：
- en: '[PRE113]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: We won’t linger on the mathematical formulas that define them since they’re
    not important for our purposes, but do check out Khan Academy’s excellent calculus
    lessons if you’re interested in this topic.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入讨论定义它们的数学公式，因为对我们的目的来说它们不重要，但如果你对这个主题感兴趣，可以查看可汗学院出色的微积分课程。
- en: 'Once we have defined those functions, we can use them to write the backward
    pass. Since each gradient is automatically populated in the right tensor, we don’t
    need to store the results of those `_grad` functions anywhere—we just need to
    execute them in the reverse order of the forward pass, to make sure that in each
    function `out.g` exists:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了这些函数，我们就可以使用它们来编写后向传递。由于每个梯度都会自动填充到正确的张量中，我们不需要将这些`_grad`函数的结果存储在任何地方——我们只需要按照前向传递的相反顺序执行它们，以确保在每个函数中`out.g`存在：
- en: '[PRE114]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: And now we can access the gradients of our model parameters in `w1.g`, `b1.g`,
    `w2.g`, and `b2.g`. We have sucessfuly defined our model—now let’s make it a bit
    more like a PyTorch module.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在`w1.g`、`b1.g`、`w2.g`和`b2.g`中访问我们模型参数的梯度。我们已经成功定义了我们的模型——现在让我们让它更像一个PyTorch模块。
- en: Refactoring the Model
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重构模型
- en: 'The three functions we used have two associated functions: a forward pass and
    a backward pass. Instead of writing them separately, we can create a class to
    wrap them together. That class can also store the inputs and outputs for the backward
    pass. This way, we will just have to call `backward`:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的三个函数有两个相关的函数：一个前向传递和一个后向传递。我们可以创建一个类将它们包装在一起，而不是分开编写它们。该类还可以存储后向传递的输入和输出。这样，我们只需要调用`backward`：
- en: '[PRE115]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '`__call__` is a magic name in Python that will make our class callable. This
    is what will be executed when we type `y = Relu()(x)`. We can do the same for
    our linear layer and the MSE loss:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`__call__`是Python中的一个魔术名称，它将使我们的类可调用。当我们键入`y = Relu()(x)`时，将执行这个操作。我们也可以对我们的线性层和MSE损失做同样的操作：'
- en: '[PRE116]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Then we can put everything in a model that we initiate with our tensors `w1`,
    `b1`, `w2`, and `b2`:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以把一切都放在一个模型中，我们用我们的张量`w1`、`b1`、`w2`和`b2`来初始化：
- en: '[PRE118]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'What is nice about this refactoring and registering things as layers of our
    model is that the forward and backward passes are now really easy to write. If
    we want to instantiate our model, we just need to write this:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重构和将事物注册为模型的层的好处是，前向和后向传递现在非常容易编写。如果我们想要实例化我们的模型，我们只需要写这个：
- en: '[PRE119]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'The forward pass can then be executed as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然后前向传递可以这样执行：
- en: '[PRE120]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'And the backward pass with this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用这个进行后向传递：
- en: '[PRE121]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Going to PyTorch
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转向PyTorch
- en: 'The `Lin`, `Mse`, and `Relu` classes we wrote have a lot in common, so we could
    make them all inherit from the same base class:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写的`Lin`、`Mse`和`Relu`类有很多共同之处，所以我们可以让它们都继承自同一个基类：
- en: '[PRE122]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Then we just need to implement `forward` and `bwd` in each of our subclasses:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们只需要在每个子类中实现`forward`和`bwd`：
- en: '[PRE123]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: The rest of our model can be the same as before. This is getting closer and
    closer to what PyTorch does. Each basic function we need to differentiate is written
    as a `torch.autograd.Function` object that has a `forward` and a `backward` method.
    PyTorch will then keep track of any computation we do to be able to properly run
    the backward pass, unless we set the `requires_grad` attribute of our tensors
    to `False`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的其余部分可以与以前相同。这越来越接近PyTorch的做法。我们需要区分的每个基本函数都被写成一个`torch.autograd.Function`对象，它有一个`forward`和一个`backward`方法。PyTorch将跟踪我们进行的任何计算，以便能够正确运行反向传播，除非我们将张量的`requires_grad`属性设置为`False`。
- en: 'Writing one of these is (almost) as easy as writing our original classes. The
    difference is that we choose what to save and what to put in a context variable
    (so that we make sure we don’t save anything we don’t need), and we return the
    gradients in the `backward` pass. It’s rare to have to write your own `Function`,
    but if you ever need something exotic or want to mess with the gradients of a
    regular function, here is how to write one:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 编写其中一个（几乎）和编写我们原始类一样容易。不同之处在于我们选择保存什么并将其放入上下文变量中（以确保我们不保存不需要的任何内容），并在`backward`传递中返回梯度。很少需要编写自己的`Function`，但如果您需要某些奇特的东西或想要干扰常规函数的梯度，这里是如何编写的：
- en: '[PRE126]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: The structure used to build a more complex model that takes advantage of those
    `Function`s is a `torch.nn.Module`. This is the base structure for all models,
    and all the neural nets you have seen up until now were from that class. It mostly
    helps to register all the trainable parameters, which as we’ve seen can be used
    in the training loop.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 用于构建利用这些`Function`的更复杂模型的结构是`torch.nn.Module`。这是所有模型的基本结构，到目前为止您看到的所有神经网络都是从该类中继承的。它主要有助于注册所有可训练的参数，正如我们已经看到的可以在训练循环中使用的那样。
- en: 'To implement an `nn.Module` you just need to do the following:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现一个`nn.Module`，你只需要做以下几步：
- en: Make sure the superclass `__init__` is called first when you initialize it.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保在初始化时首先调用超类`__init__`。
- en: Define any parameters of the model as attributes with `nn.Parameter`.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型的任何参数定义为具有`nn.Parameter`属性。
- en: Define a `forward` function that returns the output of your model.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`forward`函数，返回模型的输出。
- en: 'As an example, here is the linear layer from scratch:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个从头开始的线性层的例子：
- en: '[PRE127]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'As you see, this class automatically keeps track of what parameters have been
    defined:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这个类会自动跟踪已定义的参数：
- en: '[PRE128]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: It is thanks to this feature of `nn.Module` that we can just say `opt.step`
    and have an optimizer loop through the parameters and update each one.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 正是由于`nn.Module`的这个特性，我们可以只说`opt.step`，并让优化器循环遍历参数并更新每个参数。
- en: Note that in PyTorch, the weights are stored as an `n_out x n_in` matrix, which
    is why we have the transpose in the forward pass.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在PyTorch中，权重存储为一个`n_out x n_in`矩阵，这就是为什么在前向传递中我们有转置的原因。
- en: 'By using the linear layer from PyTorch (which uses the Kaiming initialization
    as well), the model we have been building up during this chapter can be written
    like this:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用PyTorch中的线性层（也使用Kaiming初始化），我们在本章中一直在构建的模型可以这样编写：
- en: '[PRE130]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'fastai provides its own variant of `Module` that is identical to `nn.Module`,
    but doesn’t require you to call `super().__init__()` (it does that for you automatically):'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: fastai提供了自己的`Module`变体，与`nn.Module`相同，但不需要您调用`super().__init__()`（它会自动为您执行）：
- en: '[PRE131]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: In [Chapter 19](ch19.xhtml#learner_from_scratch), we will start from such a
    model and see how to build a training loop from scratch and refactor it to what
    we’ve been using in previous chapters.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第19章](ch19.xhtml#learner_from_scratch)中，我们将从这样一个模型开始，看看如何从头开始构建一个训练循环，并将其重构为我们在之前章节中使用的内容。
- en: Conclusion
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we explored the foundations of deep learning, beginning with
    matrix multiplication and moving on to implementing the forward and backward passes
    of a neural net from scratch. We then refactored our code to show how PyTorch
    works beneath the hood.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了深度学习的基础，从矩阵乘法开始，然后实现了神经网络的前向和反向传递。然后我们重构了我们的代码，展示了PyTorch在底层的工作原理。
- en: 'Here are a few things to remember:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些需要记住的事项：
- en: A neural net is basically a bunch of matrix multiplications with nonlinearities
    in between.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络基本上是一堆矩阵乘法，中间夹杂着非线性。
- en: Python is slow, so to write fast code, we have to vectorize it and take advantage
    of techniques such as elementwise arithmetic and broadcasting.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python很慢，所以为了编写快速代码，我们必须对其进行向量化，并利用诸如逐元素算术和广播等技术。
- en: Two tensors are broadcastable if the dimensions starting from the end and going
    backward match (if they are the same, or one of them is 1). To make tensors broadcastable,
    we may need to add dimensions of size 1 with `unsqueeze` or a `None` index.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果从末尾开始向后匹配的维度相同（如果它们相同，或者其中一个是1），则两个张量是可广播的。为了使张量可广播，我们可能需要使用`unsqueeze`或`None`索引添加大小为1的维度。
- en: Properly initializing a neural net is crucial to get training started. Kaiming
    initialization should be used when we have ReLU nonlinearities.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确初始化神经网络对于开始训练至关重要。当我们有ReLU非线性时，应使用Kaiming初始化。
- en: The backward pass is the chain rule applied multiple times, computing the gradients
    from the output of our model and going back, one layer at a time.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传递是应用链式法则多次计算，从我们模型的输出开始，逐层向后计算梯度。
- en: When subclassing `nn.Module` (if not using fastai’s `Module`), we have to call
    the superclass `__init__` method in our `__init__` method and we have to define
    a `forward` function that takes an input and returns the desired result.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在子类化`nn.Module`时（如果不使用fastai的`Module`），我们必须在我们的`__init__`方法中调用超类`__init__`方法，并且我们必须定义一个接受输入并返回所需结果的`forward`函数。
- en: Questionnaire
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问卷
- en: Write the Python code to implement a single neuron.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写Python代码来实现一个单个神经元。
- en: Write the Python code to implement ReLU.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写实现ReLU的Python代码。
- en: Write the Python code for a dense layer in terms of matrix multiplication.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用矩阵乘法的术语编写一个密集层的Python代码。
- en: Write the Python code for a dense layer in plain Python (that is, with list
    comprehensions and functionality built into Python).
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用纯Python编写一个密集层的Python代码（即使用列表推导和内置到Python中的功能）。
- en: What is the “hidden size” of a layer?
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个层的“隐藏大小”是什么？
- en: What does the `t` method do in PyTorch?
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在PyTorch中，`t`方法是做什么的？
- en: Why is matrix multiplication written in plain Python very slow?
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在纯Python中编写矩阵乘法非常慢？
- en: In `matmul`, why is `ac==br`?
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`matmul`中，为什么`ac==br`？
- en: In Jupyter Notebook, how do you measure the time taken for a single cell to
    execute?
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中，如何测量执行单个单元格所需的时间？
- en: What is elementwise arithmetic?
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是逐元素算术？
- en: Write the PyTorch code to test whether every element of `a` is greater than
    the corresponding element of `b`.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写 PyTorch 代码来测试 `a` 的每个元素是否大于 `b` 的对应元素。
- en: What is a rank-0 tensor? How do you convert it to a plain Python data type?
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是秩为0的张量？如何将其转换为普通的 Python 数据类型？
- en: What does this return, and why?
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这返回什么，为什么？
- en: '[PRE132]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: What does this return, and why?
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这返回什么，为什么？
- en: '[PRE133]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: How does elementwise arithmetic help us speed up `matmul`?
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐元素算术如何帮助我们加速 `matmul`？
- en: What are the broadcasting rules?
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广播规则是什么？
- en: What is `expand_as`? Show an example of how it can be used to match the results
    of broadcasting.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`expand_as` 是什么？展示一个如何使用它来匹配广播结果的示例。'
- en: How does `unsqueeze` help us to solve certain broadcasting problems?
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`unsqueeze` 如何帮助我们解决某些广播问题？'
- en: How can we use indexing to do the same operation as `unsqueeze`?
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何使用索引来执行与 `unsqueeze` 相同的操作？
- en: How do we show the actual contents of the memory used for a tensor?
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何显示张量使用的内存的实际内容？
- en: When adding a vector of size 3 to a matrix of size 3×3, are the elements of
    the vector added to each row or each column of the matrix? (Be sure to check your
    answer by running this code in a notebook.)
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将大小为3的向量添加到大小为3×3的矩阵时，向量的元素是添加到矩阵的每一行还是每一列？（确保通过在笔记本中运行此代码来检查您的答案。）
- en: Do broadcasting and `expand_as` result in increased memory use? Why or why not?
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广播和 `expand_as` 会导致内存使用增加吗？为什么或为什么不？
- en: Implement `matmul` using Einstein summation.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用爱因斯坦求和实现 `matmul`。
- en: What does a repeated index letter represent on the lefthand side of `einsum`?
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `einsum` 的左侧重复索引字母代表什么？
- en: What are the three rules of Einstein summation notation? Why?
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爱因斯坦求和符号的三条规则是什么？为什么？
- en: What are the forward pass and backward pass of a neural network?
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络的前向传播和反向传播是什么？
- en: Why do we need to store some of the activations calculated for intermediate
    layers in the forward pass?
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要在前向传播中存储一些计算出的中间层的激活？
- en: What is the downside of having activations with a standard deviation too far
    away from 1?
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有标准差远离1的激活的缺点是什么？
- en: How can weight initialization help avoid this problem?
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重初始化如何帮助避免这个问题？
- en: What is the formula to initialize weights such that we get a standard deviation
    of 1 for a plain linear layer, and for a linear layer followed by ReLU?
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化权重的公式是什么，以便在普通线性层和ReLU后跟线性层中获得标准差为1？
- en: Why do we sometimes have to use the `squeeze` method in loss functions?
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么有时我们必须在损失函数中使用 `squeeze` 方法？
- en: What does the argument to the `squeeze` method do? Why might it be important
    to include this argument, even though PyTorch does not require it?
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`squeeze` 方法的参数是做什么的？为什么可能很重要包含这个参数，尽管 PyTorch 不需要它？'
- en: What is the chain rule? Show the equation in either of the two forms presented
    in this chapter.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 链式法则是什么？展示本章中提出的两种形式中的任意一种方程。
- en: Show how to calculate the gradients of `mse(lin(l2, w2, b2), y)` by using the
    chain rule.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展示如何使用链式法则计算 `mse(lin(l2, w2, b2), y)` 的梯度。
- en: What is the gradient of ReLU? Show it in math or code. (You shouldn’t need to
    commit this to memory—try to figure it using your knowledge of the shape of the
    function.)
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ReLU 的梯度是什么？用数学或代码展示它。（您不应该需要记住这个—尝试使用您对函数形状的知识来弄清楚它。）
- en: In what order do we need to call the `*_grad` functions in the backward pass?
    Why?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在反向传播中，我们需要以什么顺序调用 `*_grad` 函数？为什么？
- en: What is `__call__`?
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`__call__` 是什么？'
- en: What methods must we implement when writing a `torch.autograd.Function`?
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写 `torch.autograd.Function` 时我们必须实现哪些方法？
- en: Write `nn.Linear` from scratch and test that it works.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始编写 `nn.Linear` 并测试其是否有效。
- en: What is the difference between `nn.Module` and fastai’s `Module`?
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nn.Module` 和 fastai 的 `Module` 之间有什么区别？'
- en: Further Research
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: Implement ReLU as a `torch.autograd.Function` and train a model with it.
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 ReLU 实现为 `torch.autograd.Function` 并用它训练模型。
- en: If you are mathematically inclined, determine the gradients of a linear layer
    in mathematical notation. Map that to the implementation in this chapter.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您对数学感兴趣，请确定数学符号中线性层的梯度。将其映射到本章中的实现。
- en: Learn about the `unfold` method in PyTorch, and use it along with matrix multiplication
    to implement your own 2D convolution function. Then train a CNN that uses it.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解 PyTorch 中的 `unfold` 方法，并结合矩阵乘法实现自己的二维卷积函数。然后训练一个使用它的 CNN。
- en: Implement everything in this chapter by using NumPy instead of PyTorch.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 NumPy 而不是 PyTorch 在本章中实现所有内容。
