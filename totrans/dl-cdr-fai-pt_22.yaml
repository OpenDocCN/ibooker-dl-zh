- en: Chapter 17\. A Neural Net from the Foundations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter begins a journey where we will dig deep into the internals of the
    models we used in the previous chapters. We will be covering many of the same
    things we’ve seen before, but this time around we’ll be looking much more closely
    at the implementation details, and much less closely at the practical issues of
    how and why things are as they are.
  prefs: []
  type: TYPE_NORMAL
- en: We will build everything from scratch, using only basic indexing into a tensor.
    We’ll write a neural net from the ground up, and then implement backpropagation
    manually so we know exactly what’s happening in PyTorch when we call `loss.backward`.
    We’ll also see how to extend PyTorch with custom *autograd* functions that allow
    us to specify our own forward and backward computations.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Neural Net Layer from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start by refreshing our understanding of how matrix multiplication is
    used in a basic neural network. Since we’re building everything up from scratch,
    we’ll use nothing but plain Python initially (except for indexing into PyTorch
    tensors), and then replace the plain Python with PyTorch functionality after we’ve
    seen how to create it.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling a Neuron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neuron receives a given number of inputs and has an internal weight for each
    of them. It sums those weighted inputs to produce an output and adds an inner
    bias. In math, this can be written as
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="o u t equals sigma-summation Underscript i equals 1 Overscript
    n Endscripts x Subscript i Baseline w Subscript i Baseline plus b" display="block"><mrow><mi>o</mi>
    <mi>u</mi> <mi>t</mi> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mi>x</mi> <mi>i</mi></msub> <msub><mi>w</mi> <mi>i</mi></msub>
    <mo>+</mo> <mi>b</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'if we name our inputs <math alttext="left-parenthesis x 1 comma ellipsis comma
    x Subscript n Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow></math>
    , our weights <math alttext="left-parenthesis w 1 comma ellipsis comma w Subscript
    n Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>w</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>w</mi> <mi>n</mi></msub> <mo>)</mo></mrow></math>
    , and our bias <math alttext="b"><mi>b</mi></math> . In code this translates into
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This output is then fed into a nonlinear function called an *activation function*
    before being sent to another neuron. In deep learning, the most common of these
    is the *rectified linear unit*, or *ReLU*, which, as we’ve seen, is a fancy way
    of saying this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A deep learning model is then built by stacking a lot of those neurons in successive
    layers. We create a first layer with a certain number of neurons (known as the
    *hidden size*) and link all the inputs to each of those neurons. Such a layer
    is often called a *fully connected layer* or a *dense layer* (for densely connected),
    or a *linear layer*.
  prefs: []
  type: TYPE_NORMAL
- en: 'It requires you to compute, for each `input` and each neuron with a given `weight`,
    the dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you have done a little bit of linear algebra, you may remember that having
    a lot of those dot products happens when you do a *matrix multiplication*. More
    precisely, if our inputs are in a matrix `x` with a size of `batch_size` by `n_inputs`,
    and if we have grouped the weights of our neurons in a matrix `w` of size `n_neurons`
    by `n_inputs` (each neuron must have the same number of weights as it has inputs)
    as well as all the biases in a vector `b` of size `n_neurons`, then the output
    of this fully connected layer is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'where `@` represents the matrix product and `w.t()` is the transpose matrix
    of `w`. The output `y` is then of size `batch_size` by `n_neurons`, and in position
    `(i,j)` we have this (for the mathy folks out there):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y Subscript i comma j Baseline equals sigma-summation Underscript
    k equals 1 Overscript n Endscripts x Subscript i comma k Baseline w Subscript
    k comma j Baseline plus b Subscript j" display="block"><mrow><msub><mi>y</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msub><mi>x</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub> <msub><mi>w</mi> <mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>+</mo> <msub><mi>b</mi> <mi>j</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Or in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The transpose is necessary because in the mathematical definition of the matrix
    product `m @ n`, the coefficient `(i,j)` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So the very basic operation we need is a matrix multiplication, as it’s what
    is hidden in the core of a neural net.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Multiplication from Scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s write a function that computes the matrix product of two tensors, before
    we allow ourselves to use the PyTorch version of it. We will use only the indexing
    in PyTorch tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll need three nested `for` loops: one for the row indices, one for the column
    indices, and one for the inner sum. `ac` and `ar` stand for number of columns
    of `a` and number of rows of `a`, respectively (the same convention is followed
    for `b`), and we make sure calculating the matrix product is possible by checking
    that `a` has as many columns as `b` has rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To test this out, we’ll pretend (using random matrices) that we’re working
    with a small batch of 5 MNIST images, flattened into `28*28` vectors, with a linear
    model to turn them into 10 activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s time our function, using the Jupyter “magic” command `%time`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: And see how that compares to PyTorch’s built-in `@`?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, in Python three nested loops is a bad idea! Python is a slow
    language, and this isn’t going to be efficient. We see here that PyTorch is around
    100,000 times faster than Python—and that’s before we even start using the GPU!
  prefs: []
  type: TYPE_NORMAL
- en: 'Where does this difference come from? PyTorch didn’t write its matrix multiplication
    in Python, but rather in C++ to make it fast. In general, whenever we do computations
    on tensors, we will need to *vectorize* them so that we can take advantage of
    the speed of PyTorch, usually by using two techniques: elementwise arithmetic
    and broadcasting.'
  prefs: []
  type: TYPE_NORMAL
- en: Elementwise Arithmetic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the basic operators (`+`, `-`, `*`, `/`, `>`, `<`, `==`) can be applied
    elementwise. That means if we write `a+b` for two tensors `a` and `b` that have
    the same shape, we will get a tensor composed of the sums of the elements of `a`
    and `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The Boolean operators will return an array of Booleans:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to know if every element of `a` is less than the corresponding element
    in `b`, or if two tensors are equal, we need to combine those elementwise operations
    with `torch.all`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Reduction operations like `all`, `sum`, and `mean` return tensors with only
    one element, called *rank-0 tensors*. If you want to convert this to a plain Python
    Boolean or number, you need to call `.item`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The elementwise operations work on tensors of any rank, as long as they have
    the same shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'However, you can’t perform elementwise operations on tensors that don’t have
    the same shape (unless they are broadcastable, as discussed in the next section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'With elementwise arithmetic, we can remove one of our three nested loops: we
    can multiply the tensors that correspond to the `i`-th row of `a` and the `j`-th
    column of `b` before summing all the elements, which will speed things up because
    the inner loop will now be executed by PyTorch at C speed.'
  prefs: []
  type: TYPE_NORMAL
- en: To access one column or row, we can simply write `a[i,:]` or `b[:,j]`. The `:`
    means take everything in that dimension. We could restrict this and take only
    a slice of that dimension by passing a range, like `1:5`, instead of just `:`.
    In that case, we would take the elements in columns 1 to 4 (the second number
    is noninclusive).
  prefs: []
  type: TYPE_NORMAL
- en: 'One simplification is that we can always omit a trailing colon, so `a[i,:]`
    can be abbreviated to `a[i]`. With all of that in mind, we can write a new version
    of our matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We’re already ~700 times faster, just by removing that inner `for` loop! And
    that’s just the beginning—with broadcasting, we can remove another loop and get
    an even more important speedup.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 4](ch04.xhtml#chapter_mnist_basics), *broadcasting*
    is a term introduced by the [Numpy Library](https://oreil.ly/nlV7Q) that describes
    how tensors of different ranks are treated during arithmetic operations. For instance,
    it’s obvious there is no way to add a 3×3 matrix with a 4×5 matrix, but what if
    we want to add one scalar (which can be represented as a 1×1 tensor) with a matrix?
    Or a vector of size 3 with a 3×4 matrix? In both cases, we can find a way to make
    sense of this operation.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting gives specific rules to codify when shapes are compatible when
    trying to do an elementwise operation, and how the tensor of the smaller shape
    is expanded to match the tensor of the bigger shape. It’s essential to master
    those rules if you want to be able to write code that executes quickly. In this
    section, we’ll expand our previous treatment of broadcasting to understand these
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting with a scalar
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Broadcasting with a scalar is the easiest type of broadcasting. When we have
    a tensor `a` and a scalar, we just imagine a tensor of the same shape as `a` filled
    with that scalar and perform the operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How are we able to do this comparison? `0` is being *broadcast* to have the
    same dimensions as `a`. Note that this is done without creating a tensor full
    of zeros in memory (that would be inefficient).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is useful if you want to normalize your dataset by subtracting the mean
    (a scalar) from the entire dataset (a matrix) and dividing by the standard deviation
    (another scalar):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: What if you have different means for each row of the matrix? In that case, you
    will need to broadcast a vector to a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting a vector to a matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can broadcast a vector to a matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the elements of `c` are expanded to make three rows that match, making
    the operation possible. Again, PyTorch doesn’t actually create three copies of
    `c` in memory. This is done by the `expand_as` method behind the scenes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at the corresponding tensor, we can ask for its `storage` property
    (which shows the actual contents of the memory used for the tensor) to check there
    is no useless data stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though the tensor officially has nine elements, only three scalars are
    stored in memory. This is possible thanks to the clever trick of giving that dimension
    a *stride* of 0. on that dimension (which means that when PyTorch looks for the
    next row by adding the stride, it doesn’t move):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `m` is of size 3×3, there are two ways to do broadcasting. The fact it
    was done on the last dimension is a convention that comes from the rules of broadcasting
    and has nothing to do with the way we ordered our tensors. If instead we do this,
    we get the same result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact, it’s only possible to broadcast a vector of size `n` with a matrix
    of size `m` by `n`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This won’t work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to broadcast in the other dimension, we have to change the shape
    of our vector to make it a 3×1 matrix. This is done with the `unsqueeze` method
    in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, `c` is expanded on the column side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, only three scalars are stored in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'And the expanded tensor has the right shape because the column dimension has
    a stride of 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'With broadcasting, if we need to add dimensions, they are added by default
    at the beginning. When we were broadcasting before, PyTorch was executing `c.unsqueeze(0)`
    behind the scenes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The `unsqueeze` command can be replaced by `None` indexing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'You can always omit trailing colons, and `...` means all preceding dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, we can remove another `for` loop in our matrix multiplication function.
    Now, instead of multiplying `a[i]` with `b[:,j]`, we can multiply `a[i]` with
    the whole matrix `b` using broadcasting, and then sum the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: We’re now 3,700 times faster than our first implementation! Before we move on,
    let’s discuss the rules of broadcasting in a little more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When operating on two tensors, PyTorch compares their shapes elementwise. It
    starts with the *trailing dimensions* and works its way backward, adding 1 when
    it meets empty dimensions. Two dimensions are *compatible* when one of the following
    is true:'
  prefs: []
  type: TYPE_NORMAL
- en: They are equal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of them is 1, in which case that dimension is broadcast to make it the same
    as the other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arrays do not need to have the same number of dimensions. For example, if you
    have a 256×256×3 array of RGB values, and you want to scale each color in the
    image by a different value, you can multiply the image by a one-dimensional array
    with three values. Lining up the sizes of the trailing axes of these arrays according
    to the broadcast rules shows that they are compatible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'However, a 2D tensor of size 256×256 isn’t compatible with our image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'In our earlier examples with a 3×3 matrix and a vector of size 3, broadcasting
    was done on the rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: As an exercise, try to determine what dimensions to add (and where) when you
    need to normalize a batch of images of size `64 x 3 x 256 x 256` with vectors
    of three elements (one for the mean and one for the standard deviation).
  prefs: []
  type: TYPE_NORMAL
- en: Another useful way of simplifying tensor manipulations is the use of Einstein
    summation convention.
  prefs: []
  type: TYPE_NORMAL
- en: Einstein Summation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before using the PyTorch operation `@` or `torch.matmul`, there is one last
    way we can implement matrix multiplication: *Einstein summation* (`einsum`). This
    is a compact representation for combining products and sums in a general way.
    We write an equation like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The lefthand side represents the operands dimensions, separated by commas. Here
    we have two tensors that each have two dimensions (`i,k` and `k,j`). The righthand
    side represents the result dimensions, so here we have a tensor with two dimensions
    `i,j`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rules of Einstein summation notation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Repeated indices are implicitly summed over.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each index can appear at most twice in any term.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each term must contain identical nonrepeated indices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So in our example, since `k` is repeated, we sum over that index. In the end,
    the formula represents the matrix obtained when we put in (`i,j`) the sum of all
    the coefficients (`i,k`) in the first tensor multiplied by the coefficients (`k,j`)
    in the second tensor… which is the matrix product!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we can code this in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Einstein summation is a very practical way of expressing operations involving
    indexing and sum of products. Note that you can have one member on the lefthand
    side. For instance,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'returns the transpose of the matrix `a`. You can also have three or more members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return a vector of size `b`, where the `k`-th coordinate is the sum
    of `a[k,i] b[i,j] c[k,j]`. This notation is particularly convenient when you have
    more dimensions because of batches. For example, if you have two batches of matrices
    and want to compute the matrix product per batch, you could do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go back to our new `matmul` implementation using `einsum` and look at
    its speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, not only is it practical, but it’s *very* fast. `einsum` is
    often the fastest way to do custom operations in PyTorch, without diving into
    C++ and CUDA. (But it’s generally not as fast as carefully optimized CUDA code,
    as you see from the results in [“Matrix Multiplication from Scratch”](#matrix_multiplication).)
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to implement a matrix multiplication from scratch, we are
    ready to build our neural net—specifically, its forward and backward passes—using
    just matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: The Forward and Backward Passes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [Chapter 4](ch04.xhtml#chapter_mnist_basics), to train a model,
    we will need to compute all the gradients of a given loss with respect to its
    parameters, which is known as the *backward pass*. In a *forward pass*, where
    we compute the output of the model on a given input, based on the matrix products.
    As we define our first neural net, we will also delve into the problem of properly
    initializing the weights, which is crucial for making training start properly.
  prefs: []
  type: TYPE_NORMAL
- en: Defining and Initializing a Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will take the example of a two-layer neural net first. As we’ve seen, one
    layer can be expressed as `y = x @ w + b`, with `x` our inputs, `y` our outputs,
    `w` the weights of the layer (which is of size number of inputs by number of neurons
    if we don’t transpose as before), and `b` is the bias vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: We can stack the second layer on top of the first, but since mathematically
    the composition of two linear operations is another linear operation, this makes
    sense only if we put something nonlinear in the middle, called an activation function.
    As mentioned at the beginning of this chapter, in deep learning applications the
    activation function most commonly used is a ReLU, which returns the maximum of
    `x` and `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We won’t actually train our model in this chapter, so we’ll use random tensors
    for our inputs and targets. Let’s say our inputs are 200 vectors of size 100,
    which we group into one batch, and our targets are 200 random floats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'For our two-layer model, we will need two weight matrices and two bias vectors.
    Let’s say we have a hidden size of 50 and the output size is 1 (for one of our
    inputs, the corresponding output is one float in this toy example). We initialize
    the weights randomly and the bias at zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the result of our first layer is simply this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this formula works with our batch of inputs, and returns a batch
    of hidden state: `l1` is a matrix of size 200 (our batch size) by 50 (our hidden
    size).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a problem with the way our model was initialized, however. To understand
    it, we need to look at the mean and standard deviation (std) of `l1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The mean is close to zero, which is understandable since both our input and
    weight matrices have means close to zero. But the standard deviation, which represents
    how far away our activations go from the mean, went from 1 to 10\. This is a really
    big problem because that’s with just one layer. Modern neural nets can have hundreds
    of layers, so if each of them multiplies the scale of our activations by 10, we
    won’t have numbers representable by a computer by the end of the last layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, if we make just 50 multiplications between `x` and random matrices
    of size 100×100, we’ll have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is `nans` everywhere. So maybe the scale of our matrix was too big,
    and we need to have smaller weights? But if we use too small weights, we will
    have the opposite problem—the scale of our activations will go from 1 to 0.1,
    and after 100 layers we’ll be left with zeros everywhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: So we have to scale our weight matrices exactly right so that the standard deviation
    of our activations stays at 1\. We can compute the exact value to use mathematically,
    as illustrated by Xavier Glorot and Yoshua Bengio in [“Understanding the Difficulty
    of Training Deep Feedforward Neural Networks”](https://oreil.ly/9tiTC). The right
    scale for a given layer is <math alttext="1 slash StartRoot n Subscript i n Baseline
    EndRoot"><mrow><mn>1</mn> <mo>/</mo> <msqrt><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></msqrt></mrow></math>
    , where <math alttext="n Subscript i n"><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
    represents the number of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, if we have 100 inputs, we should scale our weight matrices by
    0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, some numbers that are neither zeros nor `nan`! Notice how stable the
    scale of our activations is, even after those 50 fake layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: If you play a little bit with the value for scale, you’ll notice that even a
    slight variation from 0.1 will get you either to very small or very large numbers,
    so initializing the weights properly is extremely important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to our neural net. Since we messed a bit with our inputs, we
    need to redefine them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'And for our weights, we’ll use the right scale, which is known as *Xavier initialization*
    (or *Glorot initialization*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we compute the result of the first layer, we can check that the mean
    and standard deviation are under control:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Very good. Now we need to go through a ReLU, so let’s define one. A ReLU removes
    the negatives and replaces them with zeros, which is another way of saying it
    clamps our tensor at zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'We pass our activations through this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'And we’re back to square one: the mean of our activations has gone to 0.4 (which
    is understandable since we removed the negatives), and the std went down to 0.58\.
    So like before, after a few layers we will probably wind up with zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'This means our initialization wasn’t right. Why? At the time Glorot and Bengio
    wrote their article, the most popular activation in a neural net was the hyperbolic
    tangent (tanh, which is the one they used), and that initialization doesn’t account
    for our ReLU. Fortunately, someone else has done the math for us and computed
    the right scale for us to use. In [“Delving Deep into Rectifiers: Surpassing Human-Level
    Performance”](https://oreil.ly/-_quA) (which we’ve seen before—it’s the article
    that introduced the ResNet), Kaiming He et al. show that we should use the following
    scale instead: <math alttext="StartRoot 2 slash n Subscript i n Baseline EndRoot"><msqrt><mrow><mn>2</mn>
    <mo>/</mo> <msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></msqrt></math>
    , where <math alttext="n Subscript i n"><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
    is the number of inputs of our model. Let’s see what this gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s better: our numbers aren’t all zeroed this time. So let’s go back to
    the definition of our neural net and use this initialization (which is named *Kaiming
    initialization* or *He initialization*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the scale of our activations after going through the first linear
    layer and ReLU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Much better! Now that our weights are properly initialized, we can define our
    whole model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: This is the forward pass. Now all that’s left to do is to compare our output
    to the labels we have (random numbers, in this example) with a loss function.
    In this case, we will use the mean squared error. (It’s a toy problem, and this
    is the easiest loss function to use for what is next, computing the gradients.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The only subtlety is that our outputs and targets don’t have exactly the same
    shape—after going though the model, we get an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'To get rid of this trailing 1 dimension, we use the `squeeze` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we are ready to compute our loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: That’s all for the forward pass—let’s now look at the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients and the Backward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen that PyTorch computes all the gradients we need with a magic call
    to `loss.backward`, but let’s explore what’s happening behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the part where we need to compute the gradients of the loss with
    respect to all the weights of our model, so all the floats in `w1`, `b1`, `w2`,
    and `b2`. For this, we will need a bit of math—specifically, the *chain rule*.
    This is the rule of calculus that guides how we can compute the derivative of
    a composed function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="left-parenthesis g ring f right-parenthesis prime left-parenthesis
    x right-parenthesis equals g prime left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis f prime left-parenthesis x right-parenthesis" display="block"><mrow><msup><mrow><mo>(</mo><mi>g</mi><mo>∘</mo><mi>f</mi><mo>)</mo></mrow>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>g</mi>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Jeremy Says
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I find this notation hard to wrap my head around, so instead I like to think
    of it as follows: if `y = g(u)` and `u=f(x)`, then `dy/dx = dy/du * du/dx`. The
    two notations mean the same thing, so use whatever works for you.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our loss is a big composition of different functions: mean squared error (which
    is, in turn, the composition of a mean and a power of two), the second linear
    layer, a ReLU, and the first linear layer. For instance, if we want the gradients
    of the loss with respect to `b2` and our loss is defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'The chain rule tells us that we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction d l o s s Over d b 2 EndFraction equals StartFraction
    d l o s s Over d o u t EndFraction times StartFraction d o u t Over d b 2 EndFraction
    equals StartFraction d Over d o u t EndFraction m s e left-parenthesis o u t comma
    y right-parenthesis times StartFraction d Over d b 2 EndFraction l i n left-parenthesis
    l 2 comma w 2 comma b 2 right-parenthesis" display="block"><mrow><mfrac><mrow><mtext>d</mtext><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow>
    <mrow><mtext>d</mtext><msub><mi>b</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo>
    <mfrac><mrow><mtext>d</mtext><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow> <mrow><mtext>d</mtext><mi>o</mi><mi>u</mi><mi>t</mi></mrow></mfrac>
    <mo>×</mo> <mfrac><mrow><mtext>d</mtext><mi>o</mi><mi>u</mi><mi>t</mi></mrow>
    <mrow><mtext>d</mtext><msub><mi>b</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo>
    <mfrac><mtext>d</mtext> <mrow><mtext>d</mtext><mi>o</mi><mi>u</mi><mi>t</mi></mrow></mfrac>
    <mi>m</mi> <mi>s</mi> <mi>e</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>×</mo> <mfrac><mtext>d</mtext> <mrow><mtext>d</mtext><msub><mi>b</mi>
    <mn>2</mn></msub></mrow></mfrac> <mi>l</mi> <mi>i</mi> <mi>n</mi> <mrow><mo>(</mo>
    <msub><mi>l</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>w</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>b</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: To compute the gradients of the loss with respect to <math alttext="b 2"><msub><mi>b</mi>
    <mn>2</mn></msub></math> , we first need the gradients of the loss with respect
    to our output <math alttext="o u t"><mrow><mi>o</mi> <mi>u</mi> <mi>t</mi></mrow></math>
    . It’s the same if we want the gradients of the loss with respect to <math alttext="w
    2"><msub><mi>w</mi> <mn>2</mn></msub></math> . Then, to get the gradients of the
    loss with respect to <math alttext="b 1"><msub><mi>b</mi> <mn>1</mn></msub></math>
    or <math alttext="w 1"><msub><mi>w</mi> <mn>1</mn></msub></math> , we will need
    the gradients of the loss with respect to <math alttext="l 1"><msub><mi>l</mi>
    <mn>1</mn></msub></math> , which in turn requires the gradients of the loss with
    respect to <math alttext="l 2"><msub><mi>l</mi> <mn>2</mn></msub></math> , which
    will need the gradients of the loss with respect to <math alttext="o u t"><mrow><mi>o</mi>
    <mi>u</mi> <mi>t</mi></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'So to compute all the gradients we need for the update, we need to begin from
    the output of the model and work our way *backward*, one layer after the other—which
    is why this step is known as *backpropagation*. We can automate it by having each
    function we implemented (`relu`, `mse`, `lin`) provide its backward step: that
    is, how to derive the gradients of the loss with respect to the input(s) from
    the gradients of the loss with respect to the output.'
  prefs: []
  type: TYPE_NORMAL
- en: Here we populate those gradients in an attribute of each tensor, a bit like
    PyTorch does with `.grad`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first are the gradients of the loss with respect to the output of our model
    (which is the input of the loss function). We undo the `squeeze` we did in `mse`,
    and then we use the formula that gives us the derivative of <math alttext="x squared"><msup><mi>x</mi>
    <mn>2</mn></msup></math> : <math alttext="2 x"><mrow><mn>2</mn> <mi>x</mi></mrow></math>
    . The derivative of the mean is just 1/*n*, where *n* is the number of elements
    in our input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'For the gradients of the ReLU and our linear layer, we use the gradients of
    the loss with respect to the output (in `out.g`) and apply the chain rule to compute
    the gradients of the loss with respect to the output (in `inp.g`). The chain rule
    tells us that `inp.g = relu''(inp) * out.g`. The derivative of `relu` is either
    0 (when inputs are negative) or 1 (when inputs are positive), so this gives us
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'The scheme is the same to compute the gradients of the loss with respect to
    the inputs, weights, and bias in the linear layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: We won’t linger on the mathematical formulas that define them since they’re
    not important for our purposes, but do check out Khan Academy’s excellent calculus
    lessons if you’re interested in this topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have defined those functions, we can use them to write the backward
    pass. Since each gradient is automatically populated in the right tensor, we don’t
    need to store the results of those `_grad` functions anywhere—we just need to
    execute them in the reverse order of the forward pass, to make sure that in each
    function `out.g` exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: And now we can access the gradients of our model parameters in `w1.g`, `b1.g`,
    `w2.g`, and `b2.g`. We have sucessfuly defined our model—now let’s make it a bit
    more like a PyTorch module.
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The three functions we used have two associated functions: a forward pass and
    a backward pass. Instead of writing them separately, we can create a class to
    wrap them together. That class can also store the inputs and outputs for the backward
    pass. This way, we will just have to call `backward`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '`__call__` is a magic name in Python that will make our class callable. This
    is what will be executed when we type `y = Relu()(x)`. We can do the same for
    our linear layer and the MSE loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can put everything in a model that we initiate with our tensors `w1`,
    `b1`, `w2`, and `b2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'What is nice about this refactoring and registering things as layers of our
    model is that the forward and backward passes are now really easy to write. If
    we want to instantiate our model, we just need to write this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'The forward pass can then be executed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'And the backward pass with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: Going to PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Lin`, `Mse`, and `Relu` classes we wrote have a lot in common, so we could
    make them all inherit from the same base class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we just need to implement `forward` and `bwd` in each of our subclasses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: The rest of our model can be the same as before. This is getting closer and
    closer to what PyTorch does. Each basic function we need to differentiate is written
    as a `torch.autograd.Function` object that has a `forward` and a `backward` method.
    PyTorch will then keep track of any computation we do to be able to properly run
    the backward pass, unless we set the `requires_grad` attribute of our tensors
    to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Writing one of these is (almost) as easy as writing our original classes. The
    difference is that we choose what to save and what to put in a context variable
    (so that we make sure we don’t save anything we don’t need), and we return the
    gradients in the `backward` pass. It’s rare to have to write your own `Function`,
    but if you ever need something exotic or want to mess with the gradients of a
    regular function, here is how to write one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: The structure used to build a more complex model that takes advantage of those
    `Function`s is a `torch.nn.Module`. This is the base structure for all models,
    and all the neural nets you have seen up until now were from that class. It mostly
    helps to register all the trainable parameters, which as we’ve seen can be used
    in the training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement an `nn.Module` you just need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure the superclass `__init__` is called first when you initialize it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define any parameters of the model as attributes with `nn.Parameter`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a `forward` function that returns the output of your model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As an example, here is the linear layer from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'As you see, this class automatically keeps track of what parameters have been
    defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: It is thanks to this feature of `nn.Module` that we can just say `opt.step`
    and have an optimizer loop through the parameters and update each one.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in PyTorch, the weights are stored as an `n_out x n_in` matrix, which
    is why we have the transpose in the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using the linear layer from PyTorch (which uses the Kaiming initialization
    as well), the model we have been building up during this chapter can be written
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'fastai provides its own variant of `Module` that is identical to `nn.Module`,
    but doesn’t require you to call `super().__init__()` (it does that for you automatically):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 19](ch19.xhtml#learner_from_scratch), we will start from such a
    model and see how to build a training loop from scratch and refactor it to what
    we’ve been using in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the foundations of deep learning, beginning with
    matrix multiplication and moving on to implementing the forward and backward passes
    of a neural net from scratch. We then refactored our code to show how PyTorch
    works beneath the hood.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things to remember:'
  prefs: []
  type: TYPE_NORMAL
- en: A neural net is basically a bunch of matrix multiplications with nonlinearities
    in between.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python is slow, so to write fast code, we have to vectorize it and take advantage
    of techniques such as elementwise arithmetic and broadcasting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two tensors are broadcastable if the dimensions starting from the end and going
    backward match (if they are the same, or one of them is 1). To make tensors broadcastable,
    we may need to add dimensions of size 1 with `unsqueeze` or a `None` index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Properly initializing a neural net is crucial to get training started. Kaiming
    initialization should be used when we have ReLU nonlinearities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The backward pass is the chain rule applied multiple times, computing the gradients
    from the output of our model and going back, one layer at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When subclassing `nn.Module` (if not using fastai’s `Module`), we have to call
    the superclass `__init__` method in our `__init__` method and we have to define
    a `forward` function that takes an input and returns the desired result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questionnaire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Write the Python code to implement a single neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the Python code to implement ReLU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the Python code for a dense layer in terms of matrix multiplication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the Python code for a dense layer in plain Python (that is, with list
    comprehensions and functionality built into Python).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the “hidden size” of a layer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the `t` method do in PyTorch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is matrix multiplication written in plain Python very slow?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `matmul`, why is `ac==br`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Jupyter Notebook, how do you measure the time taken for a single cell to
    execute?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is elementwise arithmetic?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the PyTorch code to test whether every element of `a` is greater than
    the corresponding element of `b`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a rank-0 tensor? How do you convert it to a plain Python data type?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does this return, and why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What does this return, and why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How does elementwise arithmetic help us speed up `matmul`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the broadcasting rules?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `expand_as`? Show an example of how it can be used to match the results
    of broadcasting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does `unsqueeze` help us to solve certain broadcasting problems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we use indexing to do the same operation as `unsqueeze`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we show the actual contents of the memory used for a tensor?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When adding a vector of size 3 to a matrix of size 3×3, are the elements of
    the vector added to each row or each column of the matrix? (Be sure to check your
    answer by running this code in a notebook.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do broadcasting and `expand_as` result in increased memory use? Why or why not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement `matmul` using Einstein summation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does a repeated index letter represent on the lefthand side of `einsum`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the three rules of Einstein summation notation? Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the forward pass and backward pass of a neural network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need to store some of the activations calculated for intermediate
    layers in the forward pass?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the downside of having activations with a standard deviation too far
    away from 1?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can weight initialization help avoid this problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the formula to initialize weights such that we get a standard deviation
    of 1 for a plain linear layer, and for a linear layer followed by ReLU?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we sometimes have to use the `squeeze` method in loss functions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the argument to the `squeeze` method do? Why might it be important
    to include this argument, even though PyTorch does not require it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the chain rule? Show the equation in either of the two forms presented
    in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show how to calculate the gradients of `mse(lin(l2, w2, b2), y)` by using the
    chain rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the gradient of ReLU? Show it in math or code. (You shouldn’t need to
    commit this to memory—try to figure it using your knowledge of the shape of the
    function.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In what order do we need to call the `*_grad` functions in the backward pass?
    Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `__call__`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What methods must we implement when writing a `torch.autograd.Function`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write `nn.Linear` from scratch and test that it works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between `nn.Module` and fastai’s `Module`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implement ReLU as a `torch.autograd.Function` and train a model with it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are mathematically inclined, determine the gradients of a linear layer
    in mathematical notation. Map that to the implementation in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn about the `unfold` method in PyTorch, and use it along with matrix multiplication
    to implement your own 2D convolution function. Then train a CNN that uses it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement everything in this chapter by using NumPy instead of PyTorch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
