- en: 'Chapter 9\. Person Detection: Building an Application'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you asked people which of their senses has the biggest impact on their day-to-day
    lives, many would answer vision.^([1](ch09.xhtml#idm46473559664376))
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Vision is a profoundly useful sense. It allows countless natural organisms to
    navigate their environments, find sources of food, and avoid running into danger.
    As humans, vision helps us recognize our friends, interpret symbolic information,
    and understand the world around us—without having to get too close.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Until quite recently, the power of vision was not available to machines. Most
    of our robots merely poked around the world with touch and proximity sensors,
    gleaning knowledge of its structure from a series of collisions. At a glance,
    a person can describe to you the shape, properties, and purpose of an object,
    without having to interact with it at all. A robot would have no such luck. Visual
    information was just too messy, unstructured, and difficult to interpret.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: With the evolution of convolutional neural networks, it’s become easy to build
    programs that can *see*. Inspired by the structure of the mammalian visual cortex,
    CNNs learn to make sense of our visual world, filtering an overwhelmingly complex
    input into a map of known patterns and shapes. The precise combination of these
    pieces can tell us the entities that are present in a given digital image.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Today, vision models are used for many different tasks. Autonomous vehicles
    use vision to spot hazards on the road. Factory robots use cameras to catch defective
    parts. Researchers have trained models that can diagnose disease from medical
    images. And there’s a fair chance your smartphone spots faces in photographs,
    to make sure they’re perfectly in focus.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Machines with sight could help transform our homes and cities, automating chores
    that were previously out of reach. But vision is an intimate sense. Most of us
    don’t like the thought of our actions being recorded, or our lives being streamed
    to the cloud, which is traditionally where ML inference is done.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a household appliance that can “see” with a built-in camera. It could
    be a security system that can spot intruders, a stove that knows it’s been left
    unattended, or a television that shuts off when there’s no one in the room. In
    each of these cases, privacy is critical. Even if no human being ever watches
    the footage, the security implications of internet-connected cameras embedded
    in always-on devices make them unappealing to most consumers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: But all this changes with TinyML. Picture a smart stove that shuts off its burners
    if it’s left unattended for too long. If it can “see” there’s a cook nearby using
    a tiny microcontroller, without any connection to the internet, we get all of
    the benefits of a smart device without any of the privacy trade-offs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Even more, tiny devices with vision can go where no sight-enabled machines have
    dared to go before. With its miniscule power consumption, a microcontroller-based
    vision system could run for months or years on a tiny battery. Planted in the
    jungle, or a coral reef, these devices could keep count of endangered animals
    without the need to be online.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The same technology makes it possible to build a vision sensor as a self-contained
    electronic component. The sensor outputs a 1 if a certain object is in view and
    a 0 if it is not, but it never shares any of the image data collected by its camera.
    This type of sensor could be embedded in all kinds of products—from smart home
    systems to personal vehicles. Your bicycle could flash a light when a car is behind
    you. Your air conditioner could know when someone’s home. And because the image
    data never leaves the self-contained sensor, it’s guaranteed secure, even if the
    product is connected to the internet.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The application we explore in this chapter uses a pretrained person-detection
    model, running on a microcontroller with a camera attached, to know when a human
    being is in view. In [Chapter 10](ch10.xhtml#chapter_person_detection_training),
    you will learn how this model works, and how to train your own models that detect
    whatever you want.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨的应用程序使用一个预训练的人体检测模型，在连接了摄像头的微控制器上运行，以知道何时有人在视野中。在[第10章](ch10.xhtml#chapter_person_detection_training)中，您将了解这个模型是如何工作的，以及如何训练自己的模型来检测您想要的内容。
- en: After reading this chapter, you’ll understand how to work with camera data on
    a microcontroller and how to run inference with a vision model and interpret the
    output. You might be surprised how easy it actually is!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，您将了解如何在微控制器上处理摄像头数据，以及如何使用视觉模型运行推断并解释输出。您可能会惊讶于这实际上是多么容易！
- en: What We’re Building
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们正在构建什么
- en: We’re going to build an embedded application that uses a model to classify images
    captured by a camera. The model is trained to recognize when a person is present
    in the camera input. This means that our application will be able to detect the
    presence or absence of a person and produce an output accordingly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个嵌入式应用程序，该应用程序使用模型对摄像头捕获的图像进行分类。该模型经过训练，能够识别摄像头输入中是否存在人物。这意味着我们的应用程序将能够检测人物的存在或缺席，并相应地产生输出。
- en: This is, essentially, the smart vision sensor we described a little earlier.
    When a person is detected, our example code will light an LED—but you can extend
    it to control all sorts of projects.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这本质上是我们稍早描述的智能视觉传感器。当检测到人物时，我们的示例代码将点亮LED灯—但您可以扩展它以控制各种项目。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As with the application we worked on in [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example),
    you can find the source code for this application in the [TensorFlow GitHub repository](https://oreil.ly/9aLhs).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在[第7章](ch07.xhtml#chapter_speech_wake_word_example)中开发的应用程序一样，您可以在[TensorFlow
    GitHub存储库](https://oreil.ly/9aLhs)中找到此应用程序的源代码。
- en: Like in the previous chapters, we first walk through the tests and the application
    code, followed by the logic that makes the sample work on various devices.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的章节一样，我们首先浏览测试和应用程序代码，然后是使示例在各种设备上运行的逻辑。
- en: 'We provide instructions for deploying the application to the following microcontroller
    platforms:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了将应用程序部署到以下微控制器平台的说明：
- en: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
- en: '[SparkFun Edge](https://oreil.ly/-hoL-)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SparkFun Edge](https://oreil.ly/-hoL-)'
- en: Note
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: TensorFlow Lite regularly adds support for new devices, so if the device you’d
    like to use isn’t listed here, it’s worth checking the example’s [*README.md*](https://oreil.ly/6gRlo).
    You can also check there for updated deployment instructions if you run into trouble
    following these steps.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite定期添加对新设备的支持，因此如果您想要使用的设备未在此处列出，请查看示例的[*README.md*](https://oreil.ly/6gRlo)。如果在按照这些步骤操作时遇到问题，您也可以在那里查找更新的部署说明。
- en: Unlike with the previous chapters, you’ll need some additional hardware to run
    this application. Because neither of these boards have an integrated camera, we
    recommend buying a *camera module*. You’ll find this information in each device’s
    section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的章节不同，您需要一些额外的硬件来运行这个应用程序。因为这两个开发板都没有集成摄像头，我们建议购买一个*摄像头模块*。您将在每个设备的部分中找到这些信息。
- en: Let’s begin by walking through our application’s structure. It’s a lot simpler
    than you might expect.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从了解应用程序的结构开始。它比您想象的要简单得多。
- en: Application Architecture
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序架构
- en: 'By now, we’ve established that embedded machine learning applications do the
    following sequence of things:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经确定了嵌入式机器学习应用程序执行以下一系列操作：
- en: Obtain an input.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取输入。
- en: Preprocess the input to extract features suitable to feed into a model.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输入进行预处理，提取适合输入模型的特征。
- en: Run inference on the processed input.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对处理后的输入运行推断。
- en: Postprocess the model’s output to make sense of it.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对模型的输出进行后处理以理解其含义。
- en: Use the resulting information to make things happen.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用得到的信息来实现所需的功能。
- en: In [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example) we saw this applied
    to wake-word detection, which uses audio as its input. This time around, our input
    will be image data. This might sound more complicated, but it’s actually much
    simpler to work with than audio.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.xhtml#chapter_speech_wake_word_example)中，我们看到这种方法应用于唤醒词检测，其输入是音频。这一次，我们的输入将是图像数据。这听起来可能更复杂，但实际上比音频更容易处理。
- en: Image data is commonly represented as an array of pixel values. We’ll be obtaining
    our image data from embedded camera modules, which all provide data in this format.
    Our model also expects its input to be an array of pixel values. Because of this,
    we won’t have to do much preprocessing before feeding data into our model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据通常表示为像素值数组。我们将从嵌入式摄像头模块获取图像数据，所有这些模块都以这种格式提供数据。我们的模型也期望其输入是像素值数组。因此，在将数据输入模型之前，我们不需要进行太多的预处理。
- en: Given that we don’t have to do much preprocessing, our app will be fairly straightforward.
    It takes a snapshot of data from a camera, feeds it into a model, and determines
    which output class was detected. It then displays the result in some simple manner.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们不需要进行太多的预处理，我们的应用程序将会相当简单。它从摄像头中获取数据快照，将其输入模型，并确定检测到了哪个输出类。然后以一种简单的方式显示结果。
- en: Before we move on, let’s learn a little more about the model we’ll be using.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们更多地了解一下我们将要使用的模型。
- en: Introducing Our Model
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍我们的模型
- en: Back in [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example), we learned
    that convolutional neural networks are neural networks designed to work well with
    multidimensional tensors, for which information is contained in the relationships
    between groups of adjacent values. They’re particularly well suited to working
    with image data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.xhtml#chapter_speech_wake_word_example)中，我们了解到卷积神经网络是专门设计用于处理多维张量的神经网络，其中信息包含在相邻值组之间的关系中。它们特别适合处理图像数据。
- en: Our person-detection model is a convolutional neural network trained on the
    [Visual Wake Words dataset](https://oreil.ly/EC6nd). This dataset consists of
    115,000 images, each one labeled with whether or not it contains a person.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的人体检测模型是一个卷积神经网络，训练于[Visual Wake Words数据集](https://oreil.ly/EC6nd)。该数据集包含115,000张图像，每张图像都标记了是否包含人体。
- en: The model is 250 KB, which is significantly larger than our speech model. As
    well as occupying more memory, this additional size means that it will take a
    lot longer to run a single inference.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型大小为250 KB，比我们的语音模型大得多。除了占用更多内存外，这种额外的大小意味着运行单个推断需要更长的时间。
- en: The model accepts 96 × 96–pixel grayscale images as input. Each image is provided
    as a 3D tensor with shape `(96, 96, 1)`, where the final dimension contains an
    8-bit value that represents a single pixel. The value specifies the shade of the
    pixel, ranging from 0 (fully black) to 255 (fully white).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型接受96×96像素的灰度图像作为输入。每个图像都以形状为`(96, 96, 1)`的3D张量提供，其中最后一个维度包含一个表示单个像素的8位值。该值指定像素的阴影，范围从0（完全黑色）到255（完全白色）。
- en: Our camera modules can return images in a variety of resolutions, so we need
    to ensure they are resized to 96 × 96 pixels. We also need to convert full-color
    images to grayscale so that they work with the model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的摄像头模块可以以各种分辨率返回图像，因此我们需要确保它们被调整为96×96像素。我们还需要将全彩图像转换为灰度图像，以便与模型配合使用。
- en: You might think 96 × 96 pixels sounds like a tiny resolution, but it will be
    more than sufficient to allow us to detect a person in each image. Models that
    work with images often accept surprisingly small resolutions. Increasing a model’s
    input size gives diminishing returns, and the complexity of the network increases
    greatly as the size of the input scales. For this reason, even state-of-the-art
    image classification models commonly work with a maximum of 320 × 320 pixels.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能认为96×96像素听起来像是一个很小的分辨率，但它将足以让我们在每个图像中检测到一个人。处理图像的模型通常接受令人惊讶地小的分辨率。增加模型的输入尺寸会带来递减的回报，而网络的复杂性会随着输入规模的增加而大幅增加。因此，即使是最先进的图像分类模型通常也只能处理最大为320×320像素的图像。
- en: 'The model outputs two probabilities: one indicating the probability that a
    person was present in the input, and another indicating the probability that there
    was nobody there. The probabilities range from 0 to 255.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出两个概率：一个指示输入中是否存在人的概率，另一个指示是否没有人的概率。概率范围从0到255。
- en: Our person detection model uses the *MobileNet* architecture, which is a well-known
    and battle-tested architecture designed for image classification on devices like
    mobile phones. In [Chapter 10](ch10.xhtml#chapter_person_detection_training),
    you will learn how this model was adapted to fit on microcontrollers and how you
    can train your own. For now, let’s continue exploring how our application works.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的人体检测模型使用了*MobileNet*架构，这是一个为移动手机等设备设计的用于图像分类的经过广泛测试的架构。在[第10章](ch10.xhtml#chapter_person_detection_training)中，您将学习如何将该模型适配到微控制器上，并且如何训练您自己的模型。现在，让我们继续探索我们的应用程序是如何工作的。
- en: All the Moving Parts
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所有的组件
- en: '[Figure 9-1](#application_architecture_2) shows the structure of our person
    detection application.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-1](#application_architecture_2)显示了我们人体检测应用程序的结构。'
- en: '![Diagram of the components of our person detection application](Images/timl_0901.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![我们人体检测应用程序的组件图示](Images/timl_0901.png)'
- en: Figure 9-1\. The components of our person detection application
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1。我们人体检测应用程序的组件
- en: As we mentioned previously, this is a lot simpler than the wake-word application,
    because we can pass image data directly into the model—there’s no preprocessing
    required.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，这比唤醒词应用程序要简单得多，因为我们可以直接将图像数据传递到模型中，无需预处理。
- en: Another aspect that keeps things simple is that we don’t average the model’s
    output. Our wake-word model ran multiple times per second, so we had to average
    its output to get a stable result. Our person detection model is much larger,
    and it takes a lot longer to run inference. This means that there’s no need to
    average its output.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个让事情简单的方面是我们不对模型的输出进行平均。我们的唤醒词模型每秒运行多次，因此我们必须对其输出进行平均以获得稳定的结果。我们的人体检测模型更大，推断时间更长。这意味着不需要对其输出进行平均。
- en: 'The code has five main parts:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 代码有五个主要部分：
- en: Main loop
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 主循环
- en: Like the other examples, our application runs in a continuous loop. However,
    because our model is a lot larger and more complex, it will take longer to run
    inference. Depending on the device, we can expect one inference every few seconds
    rather than several inferences per second.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他示例一样，我们的应用程序在一个连续循环中运行。然而，由于我们的模型更大更复杂，因此推断的运行时间会更长。根据设备的不同，我们可以预期每隔几秒进行一次推断，而不是每秒进行多次推断。
- en: Image provider
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图像提供者
- en: This component captures image data from the camera and writes it to the input
    tensor. The methods for capturing images vary from device to device, so this component
    can be overridden and customized.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该组件从摄像头捕获图像数据并将其写入输入张量。捕获图像的方法因设备而异，因此该组件可以被覆盖和自定义。
- en: TensorFlow Lite interpreter
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite解释器
- en: The interpreter runs the TensorFlow Lite model, transforming the input image
    into a set of probabilities.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 解释器运行TensorFlow Lite模型，将输入图像转换为一组概率。
- en: Model
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 模型
- en: The model is included as a data array and run by the interpreter. At 250 KB,
    this model is unreasonably large to commit to the TensorFlow GitHub repository.
    Because of this, it is downloaded by the Makefile when the project is built. If
    you want to take a look, you can download it yourself at [*tf_lite_micro_person_data_grayscale.zip*](https://oreil.ly/Ylq9m).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型作为数据数组包含在内，并由解释器运行。250 KB的模型太大了，无法提交到TensorFlow GitHub存储库。因此，在构建项目时，Makefile会下载它。如果您想查看，可以自行下载[*tf_lite_micro_person_data_grayscale.zip*](https://oreil.ly/Ylq9m)。
- en: Detection responder
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 检测响应器
- en: The detection responder takes the probabilities output by the model and uses
    the device’s output capabilities to display them. We can override it for different
    device types. In our example code it will light an LED, but you can extend it
    to do pretty much anything.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 检测响应器接收模型输出的概率，并使用设备的输出功能来显示它们。我们可以为不同的设备类型进行覆盖。在我们的示例代码中，它将点亮LED，但您可以扩展它以执行几乎任何操作。
- en: To get a sense for how these parts fit together, we’ll take a look at their
    tests.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这些部分如何配合，我们将查看它们的测试。
- en: Walking Through the Tests
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过测试
- en: 'This application is nice and simple, since there are only a few tests to walk
    through. You can find them all in the [GitHub repository](https://oreil.ly/31vB5):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用程序非常简单，因为只有几个测试需要进行。您可以在[GitHub存储库](https://oreil.ly/31vB5)中找到它们：
- en: '[*person_detection_test.cc*](https://oreil.ly/r4ny8)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[*person_detection_test.cc*](https://oreil.ly/r4ny8)'
- en: Shows how to run inference on an array representing a single image
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 展示如何对表示单个图像的数组运行推断
- en: '[*image_provider_test.cc*](https://oreil.ly/Js6M3)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[*image_provider_test.cc*](https://oreil.ly/Js6M3)'
- en: Shows how to use the image provider to capture an image
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 展示如何使用图像提供程序捕获图像
- en: '[*detection_responder_test.cc*](https://oreil.ly/KBVLF)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[*detection_responder_test.cc*](https://oreil.ly/KBVLF)'
- en: Shows how to use the detection responder to output the results of detection
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 展示如何使用检测响应器输出检测结果
- en: Let’s begin by exploring *person_detection_test.cc* to see how inference is
    run on image data. Because this is the third example we’ve walked through, this
    code should feel pretty familiar. You’re well on your way to being an embedded
    ML developer!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从探索*person_detection_test.cc*开始，看看如何对图像数据运行推断。因为这是我们走过的第三个示例，这段代码应该感觉相当熟悉。您已经在成为嵌入式ML开发人员的道路上取得了很大进展！
- en: The Basic Flow
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本流程
- en: 'First up, *person_detection_test.cc*. We begin by pulling in the ops that our
    model is going to need:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是*person_detection_test.cc*。我们首先引入模型需要的操作：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we define a tensor arena that is appropriately sized for the model. As
    usual, this number was determined by trial and error:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个适合模型大小的张量区域。通常情况下，这个数字是通过试错确定的：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then do the typical setup work, to get the interpreter ready to go, which
    includes registering the necessary ops using the `MicroMutableOpResolver`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进行典型的设置工作，准备解释器运行，包括使用`MicroMutableOpResolver`注册必要的操作：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our next step is to inspect the input tensor. We check whether it has the expected
    number of dimensions and whether its dimensions are sized appropriately:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是检查输入张量。我们检查它是否具有预期数量的维度，以及其维度是否适当：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: From this, we can see that the input is technically a 5D tensor. The first dimension
    is just a wrapper containing a single element. The subsequent two dimensions represent
    the rows and columns of the image’s pixels. The final dimension holds the number
    of color channels used to represent each pixel.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从中我们可以看到，输入技术上是一个5D张量。第一个维度只是包含一个元素的包装器。接下来的两个维度表示图像像素的行和列。最后一个维度保存用于表示每个像素的颜色通道的数量。
- en: 'The constants that tell us the expected dimensions, `kNumRows`, `kNumCols`,
    and `kNumChannels`, are defined in [*model_settings.h*](https://oreil.ly/ae2OI).
    They look like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我们预期维度的常量`kNumRows`、`kNumCols`和`kNumChannels`在[*model_settings.h*](https://oreil.ly/ae2OI)中定义。它们看起来像这样：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, the model is expected to accept a 96 × 96–pixel bitmap. The
    image will be grayscale, with one color channel for each pixel.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，模型预计接受一个96×96像素的位图。图像将是灰度的，每个像素有一个颜色通道。
- en: 'Next in the code, we copy a test image into the input tensor using a straightforward
    `for` loop:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来在代码中，我们使用简单的`for`循环将测试图像复制到输入张量中：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The variable that stores image data, `g_person_data`, is defined by *person_image_data.h*.
    To avoid adding more large files to the repository, the data itself is downloaded
    along with the model, as part of *tf_lite_micro_person_data_grayscale.zip*, when
    the tests are first run.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 存储图像数据的变量`g_person_data`由*person_image_data.h*定义。为了避免向存储库添加更多大文件，数据本身会在首次运行测试时作为*tf_lite_micro_person_data_grayscale.zip*的一部分与模型一起下载。
- en: 'After we’ve populated the input tensor, we run inference. It’s just as simple
    as ever:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们填充了输入张量之后，我们运行推断。这和以往一样简单：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now check the output tensor to make sure it’s the expected size and shape:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们检查输出张量，确保它具有预期的大小和形状：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The model’s output has four dimensions. The first three are just wrappers around
    the fourth, which contains one element for each category the model was trained
    on.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输出有四个维度。前三个只是包装器，围绕第四个维度，其中包含模型训练的每个类别的一个元素。
- en: 'The total number of categories is available as a constant, `kCategoryCount`,
    which resides in *model_settings.h* along with some other helpful values:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 类别的总数作为常量`kCategoryCount`可用，它位于*model_settings.h*中，还有一些其他有用的值：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As `kCategoryCount` shows, there are three categories in the output. The first
    happens to be an unused category, which we can ignore. The “person” category comes
    second, as we can see from its index, stored in the constant `kPersonIndex`. The
    “not a person” category comes third, with its index shown by `kNotAPersonIndex`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如`kCategoryCount`所示，输出中有三个类别。第一个恰好是一个未使用的类别，我们可以忽略。“人”类别排在第二位，我们可以从常量`kPersonIndex`中存储的索引中看到。“不是人”类别排在第三位，其索引由`kNotAPersonIndex`显示。
- en: 'There’s also an array of category labels, `kCategoryLabels`, which is implemented
    in [*model_settings.cc*](https://oreil.ly/AB0zS):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个类别标签数组`kCategoryLabels`，在[*model_settings.cc*](https://oreil.ly/AB0zS)中实现：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next chunk of code logs the “person” and “no person” scores, and asserts
    that the “person” score is greater—as it should be given that we passed in an
    image of a person:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的代码块记录“人”和“非人”分数，并断言“人”分数更高——因为我们传入的是一个人的图像：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Since the only data content of the output tensor is the three `uint8` values
    representing class scores, with the first one being unused, we can access the
    scores directly by using `output->data.uint8[kPersonIndex]` and `output->data.uint8[kNotAPersonIndex]`.
    As `uint8` types, they have a minimum value of 0 and a maximum value of 255.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输出张量的唯一数据内容是表示类别分数的三个`uint8`值，第一个值未使用，我们可以通过`output->data.uint8[kPersonIndex]`和`output->data.uint8[kNotAPersonIndex]`直接访问分数。作为`uint8`类型，它们的最小值为0，最大值为255。
- en: Note
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If the “person” and “no person” scores are similar, it can signify that the
    model isn’t very confident of its prediction. In this case, you might choose to
    consider the result inconclusive.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“人”和“非人”分数相似，这可能意味着模型对其预测不太有信心。在这种情况下，您可能选择考虑结果不确定。
- en: 'Next, we test for an image without a person, held by `g_no_person_data`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们测试没有人的图像，由`g_no_person_data`持有：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After inference has run, we then assert that the “not a person” score is higher:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 推理运行后，我们断言“非人”分数更高：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can observe, there’s nothing fancy going on here. We may be feeding in
    images instead of scalars or spectrograms, but the process of inference is similar
    to what we’ve seen before.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这里没有什么花哨的东西。我们可能正在输入图像而不是标量或频谱图，但推理过程与我们以前看到的类似。
- en: 'Running the test is similarly straightforward. Just issue the following command
    from the root of the TensorFlow repository:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 运行测试同样简单。只需从TensorFlow存储库的根目录发出以下命令：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first time the test is run, the model and image data will be downloaded.
    If you want to take a look at the downloaded files, you can find them in *tensorflow/lite/micro/tools/make/downloads/person_model_grayscale*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次运行测试时，将下载模型和图像数据。如果您想查看已下载的文件，可以在*tensorflow/lite/micro/tools/make/downloads/person_model_grayscale*中找到它们。
- en: Next up, we check out the interface for the image provider.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查图像提供程序的接口。
- en: The Image Provider
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像提供程序
- en: 'The image provider is responsible for grabbing data from the camera and returning
    it in a format suitable for writing to the model’s input tensor. The file [*image_provider.h*](https://oreil.ly/5Vjbe)
    defines its interface:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图像提供程序负责从摄像头获取数据，并以适合写入模型输入张量的格式返回数据。文件[*image_provider.h*](https://oreil.ly/5Vjbe)定义了其接口：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Because its actual implementation is platform-specific, there’s a reference
    implementation in [*person_detection/image_provider.cc*](https://oreil.ly/QoQ3O)
    that returns dummy data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其实际实现是特定于平台的，因此在[*person_detection/image_provider.cc*](https://oreil.ly/QoQ3O)中有一个返回虚拟数据的参考实现。
- en: 'The test in [*image_provider_test.cc*](https://oreil.ly/Nbl9x) calls this reference
    implementation to show how it is used. Our first order of business is to create
    an array to hold the image data. This happens in the following line:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[*image_provider_test.cc*](https://oreil.ly/Nbl9x)中的测试调用此参考实现以展示其用法。我们的首要任务是创建一个数组来保存图像数据。这发生在以下行中：'
- en: '[PRE15]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The constant `kMaxImageSize` comes from our old friend, [*model_settings.h*](https://oreil.ly/5naFK).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 常量`kMaxImageSize`来自我们的老朋友[*model_settings.h*](https://oreil.ly/5naFK)。
- en: 'After we’ve set up this array, we can call the `GetImage()` function to capture
    an image from the camera:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 设置了这个数组后，我们可以调用`GetImage()`函数从摄像头捕获图像：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We call it with an `ErrorReporter` instance; the number of columns, rows, and
    channels that we want; and a pointer to our `image_data` array. The function will
    write the image data into this array. We can check the function’s return value
    to determine whether the capture process was successful; it will be set to `kTfLiteError`
    if there is a problem, or `kTfLiteOk` otherwise.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`ErrorReporter`实例、我们想要的列数、行数和通道数以及指向我们的`image_data`数组的指针来调用它。该函数将把图像数据写入此数组。我们可以检查函数的返回值来确定捕获过程是否成功；如果有问题，它将设置为`kTfLiteError`，否则为`kTfLiteOk`。
- en: 'Finally, the test walks through the returned data to show that all of the memory
    locations are readable. Even though the image technically has rows, columns, and
    channels, in practice the data is flattened into a 1D array:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，测试通过返回的数据以显示所有内存位置都是可读的。即使图像在技术上具有行、列和通道，但实际上数据被展平为一维数组：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To run this test, use the following command:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此测试，请使用以下命令：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We’ll examine the device-specific implementations of *image_provider.cc* later
    in the chapter; for now, let’s take a look at the detection responder’s interface.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面查看*image_provider.cc*的特定于设备的实现；现在，让我们看一下检测响应器的接口。
- en: The Detection Responder
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测响应器
- en: Our final test shows how the detection responder is used. This is the code responsible
    for communicating the results of inference. Its interface is defined in [*detection_responder.h*](https://oreil.ly/cTptj),
    and the test is in [*detection_responder_test.cc*](https://oreil.ly/Igx7a).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终测试展示了检测响应器的使用方式。这是负责传达推理结果的代码。其接口在[*detection_responder.h*](https://oreil.ly/cTptj)中定义，测试在[*detection_responder_test.cc*](https://oreil.ly/Igx7a)中。
- en: 'The interface is pretty simple:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接口非常简单：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We just call it with the scores for both the “person” and “not a person” categories,
    and it will decide what to do from there.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需使用“人”和“非人”类别的分数调用它，它将根据情况决定要做什么。
- en: 'The reference implementation in [*detection_responder.cc*](https://oreil.ly/5Wjjt)
    just logs these values. The test in *detection_responder_test.cc* calls the function
    a couple of times:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[*detection_responder.cc*](https://oreil.ly/5Wjjt)中的参考实现只是记录这些值。*detection_responder_test.cc*中的测试调用该函数几次：'
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To run the test and see the output, use the following command:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行测试并查看输出，请使用以下命令：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We’ve explored all of the tests and the interfaces they exercise. Let’s now
    walk through the program itself.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了所有测试和它们所练习的接口。现在让我们走一遍程序本身。
- en: Detecting People
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测人员
- en: The application’s core functions reside in [*main_functions.cc*](https://oreil.ly/64oHW).
    They’re short and sweet, and we’ve seen much of their logic in the tests.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的核心功能位于[*main_functions.cc*](https://oreil.ly/64oHW)中。它们简短而简洁，我们在测试中已经看到了它们的大部分逻辑。
- en: 'First, we pull in all of the ops that our model needs:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们引入模型所需的所有操作：
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we declare a bunch of variables to hold the important moving parts:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们声明一堆变量来保存重要的移动部件：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After that, we allocate some working memory for tensor operations:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们为张量操作分配一些工作内存：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the `setup()` function, which is run before anything else happens, we create
    an error reporter, load our model, set up an interpreter instance, and grab a
    reference to the model’s input tensor:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在`setup()`函数中，在任何其他操作发生之前运行，我们创建一个错误报告器，加载我们的模型，设置一个解释器实例，并获取模型输入张量的引用：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The next part of the code is called continually in the program’s main loop.
    It first grabs an image using the image provider, passing a reference to the input
    tensor so that the image is written directly there:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的下一部分在程序的主循环中被不断调用。它首先使用图像提供程序获取图像，通过传递一个输入张量的引用，使图像直接写入其中：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'It then runs inference, obtains the output tensor, and reads the “person” and
    “no person” scores from it. These scores are passed into the detection responder’s
    `RespondToDetection()` function:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行推理，获取输出张量，并从中读取“人”和“无人”分数。这些分数被传递到检测响应器的`RespondToDetection()`函数中：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: After `RespondToDetection()` has finished outputting the results, the `loop()`
    function will return, ready to be called again by the program’s main loop.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在`RespondToDetection()`完成输出结果后，`loop()`函数将返回，准备好被程序的主循环再次调用。
- en: 'The loop itself is defined within the program’s `main()` function, which is
    located in [*main.cc*](https://oreil.ly/_PR3L). It calls the `setup()` function
    once and then calls the `loop()` function repeatedly and indefinitely:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 循环本身在程序的`main()`函数中定义，该函数位于[*main.cc*](https://oreil.ly/_PR3L)中。它一次调用`setup()`函数，然后重复调用`loop()`函数，直到无限循环：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: And that’s the entire program! This example is great because it shows that working
    with sophisticated machine learning models can be surprisingly simple. The model
    contains all of the complexity, and we just need to feed it data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是整个程序！这个例子很棒，因为它表明与复杂的机器学习模型一起工作可以出奇地简单。模型包含了所有的复杂性，我们只需要提供数据给它。
- en: Before we move along, you can run the program locally to give it a try. The
    reference implementation of the image provider just returns dummy data, so you
    won’t get meaningful recognition results, but you’ll at least see the code at
    work.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，您可以在本地运行程序进行尝试。图像提供程序的参考实现只返回虚拟数据，因此您不会得到有意义的识别结果，但至少可以看到代码在运行。
- en: 'First, use this command to build the program:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用以下命令构建程序：
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Once the build completes, you can run the example with the following command:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 构建完成后，您可以使用以下命令运行示例：
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You’ll see the program’s output scroll past until you press Ctrl-C to terminate
    it:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您会看到程序的输出在屏幕上滚动，直到按下Ctrl-C终止它：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the next section, we walk through the device-specific code that will capture
    camera images and output the results on each platform. We also show how to deploy
    and run this code.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将详细介绍特定设备的代码，该代码将捕获摄像头图像并在每个平台上输出结果。我们还展示了如何部署和运行此代码。
- en: Deploying to Microcontrollers
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署到微控制器
- en: 'In this section, we deploy our code to two familiar devices:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将代码部署到两个熟悉的设备上：
- en: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
- en: '[SparkFun Edge](https://oreil.ly/-hoL-)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SparkFun Edge](https://oreil.ly/-hoL-)'
- en: 'There’s one big difference this time around: because neither of these devices
    has a built-in camera, we recommend that you buy a camera module for whichever
    device you’re using. Each device has its own implementation of *image_provider.cc*,
    which interfaces with the camera module to capture images. There’s also device-specific
    output code in *detection_responder.cc*.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这次有一个很大的不同：因为这两个设备都没有内置摄像头，我们建议您为您使用的任何设备购买摄像头模块。每个设备都有自己的*image_provider.cc*实现，它与摄像头模块进行接口，以捕获图像。*detection_responder.cc*中还有特定于设备的输出代码。
- en: This nice and simple, so it will make an excellent template to start from when
    you’re creating your own vision-based ML applications.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单，所以它将是一个很好的模板，用来创建你自己的基于视觉的ML应用程序。
- en: Let’s begin by exploring the Arduino implementation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始探索Arduino的实现。
- en: Arduino
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Arduino
- en: As an Arduino board, the Arduino Nano 33 BLE Sense has access to a massive ecosystem
    of compatible third-party hardware and libraries. We’re using a third-party camera
    module designed to work with Arduino, along with a couple of Arduino libraries
    that will interface with our camera module and make sense of the data it outputs.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Arduino板，Arduino Nano 33 BLE Sense可以访问大量兼容的第三方硬件和库的生态系统。我们使用了一个专为与Arduino配合使用而设计的第三方摄像头模块，以及一些Arduino库，这些库将与我们的摄像头模块进行接口，并理解其输出的数据。
- en: Which camera module to buy
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要购买哪种摄像头模块
- en: This example uses the [Arducam Mini 2MP Plus](https://oreil.ly/LAwhb) camera
    module. It’s easy to connect to the Arduino Nano 33 BLE Sense, and it can be powered
    by the Arduino board’s power supply. It has a large lens and is capable of capturing
    high-quality 2-megapixel images—though we’ll be using its on-board image rescaling
    feature to obtain a smaller resolution. It’s not particularly power-efficient,
    but its high image quality makes it ideal for building image capture applications,
    like for recording wildlife.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子使用[Arducam Mini 2MP Plus](https://oreil.ly/LAwhb)摄像头模块。它很容易连接到Arduino Nano
    33 BLE Sense，并且可以由Arduino板的电源供应提供电力。它有一个大镜头，能够捕获高质量的200万像素图像 - 尽管我们将使用其内置的图像重缩放功能来获得较小的分辨率。它并不特别节能，但其高质量的图像使其非常适合构建图像捕获应用程序，比如用于记录野生动物。
- en: Capturing images on arduino
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Arduino上捕获图像
- en: We connect the Arducam module to the Arduino board via a number of pins. To
    obtain image data, we send a command from the Arduino board to the Arducam that
    instructs it to capture an image. The Arducam will do that, storing the image
    in its internal data buffer. We then send further commands that allow us to read
    the image data from the Arducam’s internal buffer and store it in the Arduino’s
    memory. To do all of this, we use the official Arducam library.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一些引脚将Arducam模块连接到Arduino板。为了获取图像数据，我们从Arduino板向Arducam发送一个命令，指示它捕获图像。Arducam将执行此操作，将图像存储在其内部数据缓冲区中。然后，我们发送进一步的命令，允许我们从Arducam的内部缓冲区中读取图像数据并将其存储在Arduino的内存中。为了执行所有这些操作，我们使用官方的Arducam库。
- en: The Arducam camera module has a 2-megapixel image sensor, with a resolution
    of 1920 × 1080\. Our person detection model has an input size of only 96 × 96,
    so we don’t need all of that data. In fact, the Arduino itself doesn’t have enough
    memory to hold a 2-megapixel image, which would be several megabytes in size.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Arducam相机模块具有一颗200万像素的图像传感器，分辨率为1920×1080。我们的人体检测模型的输入尺寸仅为96×96，因此我们不需要所有这些数据。事实上，Arduino本身没有足够的内存来容纳一张200万像素的图像，其大小将达到几兆字节。
- en: Fortunately, the Arducam hardware has the ability to resize its output to a
    much smaller resolution, 160 × 120 pixels. We can easily crop this down to 96
    × 96 in our code, by keeping only the central 96 × 96 pixels. However, to complicate
    matters, the Arducam’s resized output is encoded using [JPEG](https://oreil.ly/gwWDh),
    a common compression format for images. Our model requires an array of pixels,
    not a JPEG-encoded image, which means that we need to decode the Arducam’s output
    before we use it. We can do this using an open source library.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Arducam硬件具有将输出调整为更小分辨率的能力，即160×120像素。我们可以通过在代码中仅保留中心的96×96像素来轻松将其裁剪为96×96。然而，为了复杂化问题，Arducam的调整大小输出使用了JPEG，这是一种常见的图像压缩格式。我们的模型需要一个像素数组，而不是一个JPEG编码的图像，这意味着我们需要在使用之前解码Arducam的输出。我们可以使用一个开源库来实现这一点。
- en: Our final task is to convert the Arducam’s color image output into grayscale,
    which is what our person-detection model expects. We’ll write the grayscale data
    into our model’s input tensor.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后任务是将Arducam的彩色图像输出转换为灰度，这是我们的人体检测模型所期望的。我们将灰度数据写入我们模型的输入张量。
- en: The image provider is implemented in [*arduino/image_provider.cc*](https://oreil.ly/kGx0-).
    We won’t explain its every detail, because the code is specific to the Arducam
    camera module. Instead, let’s step through what happens at a high level.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图像提供程序实现在[*arduino/image_provider.cc*](https://oreil.ly/kGx0-)中。我们不会解释其每个细节，因为代码是特定于Arducam相机模块的。相反，让我们以高层次的方式来看一下发生了什么。
- en: 'The `GetImage()` function is the image provider’s interface with the world.
    It’s called in our application’s main loop to obtain a frame of image data. The
    first time it is called, we need to initialize the camera. This happens with a
    call to the `InitCamera()` function, as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`GetImage()`函数是图像提供程序与外部世界的接口。在我们的应用程序主循环中调用它以获取一帧图像数据。第一次调用时，我们需要初始化相机。这通过调用`InitCamera()`函数来实现，如下所示：'
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `InitCamera()` function is defined further up in *image_provider.cc*. We
    won’t walk through it here because it’s very device-specific, and if you want
    to use it in your own code you can just copy and paste it. It configures the Arduino’s
    hardware to communicate with the Arducam and then confirms that communication
    is working. Finally, it instructs the Arducam to output 160 × 120–pixel JPEG images.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`InitCamera()`函数在*image_provider.cc*中进一步定义。我们不会在这里详细介绍它，因为它非常特定于设备，如果您想在自己的代码中使用它，只需复制粘贴即可。它配置Arduino的硬件以与Arducam通信，然后确认通信正常工作。最后，它指示Arducam输出160×120像素的JPEG图像。'
- en: 'The next function called by `GetImage()` is `PerformCapture()`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`GetImage()`函数调用的下一个函数是`PerformCapture()`：'
- en: '[PRE33]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We won’t go into the details of this one, either. All it does is send a command
    to the camera module, instructing it to capture an image and store the image data
    in its internal buffer. It then waits for confirmation that an image was captured.
    At this point, there’s image data waiting in the Arducam’s internal buffer, but
    there isn’t yet any image data on the Arduino itself.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也不会详细介绍这个函数。它只是向相机模块发送一个命令，指示其捕获图像并将图像数据存储在其内部缓冲区中。然后，它等待确认图像已被捕获。此时，Arducam的内部缓冲区中有图像数据，但Arduino本身还没有任何图像数据。
- en: 'The next function we call is `ReadData()`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们调用的函数是`ReadData()`：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `ReadData()` function uses more commands to fetch the image data from the
    Arducam. After the function has run, the global variable `jpeg_buffer` will be
    filled with the JPEG-encoded image data retrieved from the camera.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReadData()`函数使用更多的命令从Arducam获取图像数据。函数运行后，全局变量`jpeg_buffer`将填充从相机检索到的JPEG编码图像数据。'
- en: 'When we have the JPEG-encoded image, our next step is to decode it into raw
    image data. This happens in the `DecodeAndProcessImage()` function:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有JPEG编码的图像时，我们的下一步是将其解码为原始图像数据。这发生在`DecodeAndProcessImage()`函数中：
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The function uses a library named JPEGDecoder to decode the JPEG data and write
    it directly into the model’s input tensor. In the process, it crops the image,
    discarding some of the 160 × 120 data so that all that remains are 96 × 96 pixels,
    roughly at the center of the image. It also reduces the image’s 16-bit color representation
    down to 8-bit grayscale.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数使用一个名为JPEGDecoder的库来解码JPEG数据，并直接将其写入模型的输入张量。在此过程中，它裁剪图像，丢弃一些160×120的数据，使剩下的只有96×96像素，大致位于图像中心。它还将图像的16位颜色表示减少到8位灰度。
- en: After the image has been captured and stored in the input tensor, we’re ready
    to run inference. Next, we show how the model’s output is displayed
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像被捕获并存储在输入张量中后，我们准备运行推理。接下来，我们展示模型的输出是如何显示的。
- en: Responding to detections on Arduino
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Arduino上响应检测
- en: The Arduino Nano 33 BLE Sense has a built-in RGB LED, which is a single component
    that contains distinct red, green, and blue LEDs that you can control separately.
    The detection responder’s implementation flashes the blue LED every time inference
    is run. When a person is detected, it lights the green LED; when a person is not
    detected, it lights the red LED.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Arduino Nano 33 BLE Sense内置了RGB LED，这是一个包含独立红色、绿色和蓝色LED的单一组件，您可以分别控制它们。检测响应器的实现在每次推理运行时闪烁蓝色LED。当检测到人时，点亮绿色LED；当未检测到人时，点亮红色LED。
- en: The implementation is in [*arduino/detection_responder.cc*](https://oreil.ly/-WsSN).
    Let’s take a quick walk through.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 实现在[*arduino/detection_responder.cc*](https://oreil.ly/-WsSN)中。让我们快速浏览一下。
- en: 'The `RespondToDetection()` function accepts two scores, one for the “person”
    category and the other for “not a person.” The first time it is called, it sets
    up the blue, green, and yellow LEDs for output:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`RespondToDetection()`函数接受两个分数，一个用于“人”类别，另一个用于“非人”。第一次调用时，它设置蓝色、绿色和黄色LED为输出：'
- en: '[PRE36]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, to indicate that an inference has just completed, we switch off all the
    LEDs and then flash the blue LED very briefly:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了指示推理刚刚完成，我们关闭所有LED，然后非常简要地闪烁蓝色LED：
- en: '[PRE37]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: You’ll notice that unlike with the Arduino’s built-in LED, these LEDs are switched
    on with `LOW` and off with `HIGH`. This is just a factor of how the LEDs are connected
    to the board.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，与Arduino内置LED不同，这些LED使用`LOW`打开，使用`HIGH`关闭。这只是LED连接到板上的方式的一个因素。
- en: 'Next, we switch on and off the appropriate LEDs depending on which category
    score is higher:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们根据哪个类别的分数更高来打开和关闭适当的LED：
- en: '[PRE38]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, we use the `error_reporter` instance to output the scores to the serial
    port:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`error_reporter`实例将分数输出到串行端口：
- en: '[PRE39]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'And that’s it! The core of the function is a basic `if` statement, and you
    could easily use similar logic to control other types of output. There’s something
    very exciting about such a complex visual input being transformed into a single
    Boolean output: “person” or “no person.”'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！函数的核心是一个基本的`if`语句，您可以轻松使用类似的逻辑来控制其他类型的输出。将如此复杂的视觉输入转换为一个布尔输出“人”或“非人”是非常令人兴奋的事情。
- en: Running the example
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行示例
- en: Running this example is a little more complex than our other Arduino examples,
    because we need to connect the Arducam to the Arduino board. We also need to install
    and configure the libraries that interface with the Arducam and decode its JPEG
    output. But don’t worry, it’s still very easy!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此示例比我们其他Arduino示例更复杂，因为我们需要将Arducam连接到Arduino板。我们还需要安装和配置与Arducam接口并解码其JPEG输出的库。但不用担心，这仍然非常简单！
- en: 'To deploy this example, here’s what we’ll need:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署此示例，我们需要以下内容：
- en: An Arduino Nano 33 BLE Sense board
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Arduino Nano 33 BLE Sense板
- en: An Arducam Mini 2MP Plus
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Arducam Mini 2MP Plus
- en: Jumper cables (and optionally a breadboard)
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳线（和可选的面包板）
- en: A micro-USB cable
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一根Micro-USB电缆
- en: The Arduino IDE
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arduino IDE
- en: Our first task is to connect the Arducam to the Arduino using jumper cables.
    This isn’t an electronics book, so we won’t go into the details of using the cables.
    Instead, [Table 9-1](#arducam_pins) shows how the pins should be connected. The
    pins are labeled on each device.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个任务是使用跳线连接Arducam到Arduino。这不是一本电子书，所以我们不会详细介绍使用电缆的细节。相反，[表9-1](#arducam_pins)显示了引脚应该如何连接。每个设备上都标有引脚标签。
- en: Table 9-1\. Arducam Mini 2MP Plus to Arduino Nano 33 BLE Sense connections
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-1。Arducam Mini 2MP Plus到Arduino Nano 33 BLE Sense的连接
- en: '| Arducam pin | Arduino pin |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Arducam引脚 | Arduino引脚 |'
- en: '| --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| CS | D7 (unlabeled, immediately to the right of D6) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| CS | D7（未标记，紧挨D6右侧） |'
- en: '| MOSI | D11 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| MOSI | D11 |'
- en: '| MISO | D12 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| MISO | D12 |'
- en: '| SCK | D13 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| SCK | D13 |'
- en: '| GND | GND (either pin marked GND is fine) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| GND | GND（任何一个标记为GND的引脚都可以） |'
- en: '| VCC | 3.3 V |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| VCC | 3.3 V |'
- en: '| SDA | A4 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| SDA | A4 |'
- en: '| SCL | A5 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| SCL | A5 |'
- en: After you’ve set up the hardware, you can continue with installing the software.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 设置硬件后，您可以继续安装软件。
- en: Tip
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: There’s always a chance that the build process might have changed since this
    book was written, so check [*README.md*](https://oreil.ly/CR5Pb) for the latest
    instructions.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 建立过程可能会有所变化，所以请查看[*README.md*](https://oreil.ly/CR5Pb)获取最新说明。
- en: The projects in this book are available as example code in the TensorFlow Lite
    Arduino library. If you haven’t already installed the library, open the Arduino
    IDE and select Manage Libraries from the Tools menu. In the window that appears,
    search for and install the library named *Arduino_TensorFlowLite*. You should
    be able to use the latest version, but if you run into issues, the version that
    was tested with this book is 1.14-ALPHA.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的项目作为TensorFlow Lite Arduino库中的示例代码可用。如果您尚未安装该库，请打开Arduino IDE并从“工具”菜单中选择“管理库”。在弹出的窗口中，搜索并安装名为*Arduino_TensorFlowLite*的库。您应该能够使用最新版本，但如果遇到问题，本书测试过的版本是1.14-ALPHA。
- en: Note
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can also install the library from a *.zip* file, which you can either [download](https://oreil.ly/blgB8)
    from the TensorFlow Lite team or generate yourself using the TensorFlow Lite for
    Microcontrollers Makefile. If you’d prefer to do the latter, see [Appendix A](app01.xhtml#appendix_arduino_library_zip).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以从*.zip*文件安装库，您可以从TensorFlow Lite团队[下载](https://oreil.ly/blgB8)或使用TensorFlow
    Lite for Microcontrollers Makefile自动生成。如果您更喜欢后者，请参阅[附录A](app01.xhtml#appendix_arduino_library_zip)。
- en: After you’ve installed the library, the `person_detection` example will show
    up in the File menu under Examples→Arduino_TensorFlowLite, as shown in [Figure 9-2](#arduino_examples_person_detection).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完库后，“person_detection”示例将显示在“文件”菜单下的“示例→Arduino_TensorFlowLite”中，如[图9-2](#arduino_examples_person_detection)所示。
- en: '![Screenshot of the ''Examples'' menu](Images/timl_0604.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![“示例”菜单的屏幕截图](Images/timl_0604.png)'
- en: Figure 9-2\. The Examples menu
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。示例菜单
- en: Click “person_detection” to load the example. It will appear as a new window,
    with a tab for each of the source files. The file in the first tab, *person_detection*,
    is equivalent to the *main_functions.cc* we walked through earlier.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“person_detection”加载示例。它将显示为一个新窗口，每个源文件都有一个选项卡。第一个选项卡中的文件*person_detection*相当于我们之前介绍的*main_functions.cc*。
- en: Note
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[“Running the Example”](ch06.xhtml#hello_world_running_the_example) already
    explained the structure of the Arduino example, so we won’t cover it again here.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[“运行示例”](ch06.xhtml#hello_world_running_the_example)已经解释了Arduino示例的结构，所以我们这里不再重复覆盖。'
- en: 'In addition to the TensorFlow library, we need to install two other libraries:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 除了TensorFlow库，我们还需要安装另外两个库：
- en: The Arducam library, so our code can interface with the hardware
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arducam库，以便我们的代码可以与硬件进行交互
- en: The JPEGDecoder library, so we can decode JPEG-encoded images
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JPEGDecoder库，以便我们可以解码JPEG编码的图像
- en: The Arducam Arduino library is available from [GitHub](https://oreil.ly/93OKK).
    To install it, download or clone the repository. Next, copy its *ArduCAM* subdirectory
    into your *Arduino/libraries* directory. To find the *libraries* directory on
    your machine, check the Sketchbook location in the Arduino IDE’s Preferences window.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Arducam Arduino库可从[GitHub](https://oreil.ly/93OKK)获取。要安装它，请下载或克隆存储库。接下来，将其*ArduCAM*子目录复制到*Arduino/libraries*目录中。要找到您机器上的*libraries*目录，请在Arduino
    IDE的首选项窗口中检查Sketchbook位置。
- en: After downloading the library, you’ll need to edit one of its files to make
    sure it is configured for the Arducam Mini 2MP Plus. To do this, open *Arduino/libraries/ArduCAM/memorysaver.h*.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 下载库后，您需要编辑其中一个文件，以确保为Arducam Mini 2MP Plus进行配置。为此，请打开*Arduino/libraries/ArduCAM/memorysaver.h*。
- en: 'You should see a bunch of `#define` statements listed. Make sure that they
    are all commented out except for `#define OV2640_MINI_2MP_PLUS`, as shown here:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 您会看到一堆`#define`语句。确保它们都被注释掉，除了`#define OV2640_MINI_2MP_PLUS`，如此处所示：
- en: '[PRE40]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: After you save the file, you’re done configuring the Arducam library.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 保存文件后，您已经完成了Arducam库的配置。
- en: Tip
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'The example was developed using commit #e216049 of the Arducam library. If
    you run into problems with the library, you can try downloading this specific
    commit to make sure you’re using the exact same code.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 示例是使用Arducam库的提交#e216049开发的。如果您在使用库时遇到问题，可以尝试下载这个特定的提交，以确保您使用的是完全相同的代码。
- en: The next step is to install the JPEGDecoder library. You can do this from within
    the Arduino IDE. In the Tools menu, select the Manage Libraries option and search
    for JPEGDecoder. You should install version 1.8.0 of the library.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是安装JPEGDecoder库。您可以在Arduino IDE中完成这个操作。在工具菜单中，选择管理库选项并搜索JPEGDecoder。您应该安装库的1.8.0版本。
- en: 'After you’ve installed the library, you’ll need to configure it to disable
    some optional components that are not compatible with the Arduino Nano 33 BLE
    Sense. Open *Arduino/libraries/JPEGDecoder/src/User_Config.h* and make sure that
    both `#define LOAD_SD_LIBRARY` and `#define LOAD_SDFAT_LIBRARY` are commented
    out, as shown in this excerpt from the file:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完库之后，您需要配置它以禁用一些与Arduino Nano 33 BLE Sense不兼容的可选组件。打开*Arduino/libraries/JPEGDecoder/src/User_Config.h*，确保`#define
    LOAD_SD_LIBRARY`和`#define LOAD_SDFAT_LIBRARY`都被注释掉，如文件中的摘录所示：
- en: '[PRE41]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: After you’ve saved the file, you’re done installing libraries. You’re now ready
    to run the person detection application!
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 保存文件后，安装库就完成了。现在您已经准备好运行人员检测应用程序了！
- en: To begin, plug in your Arduino device via USB. Make sure the correct device
    type is selected from the Board drop-down list in the Tools menu, as shown in
    [Figure 9-3](#arduino_board_dropdown_9).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过USB将Arduino设备插入。确保在工具菜单中从板下拉列表中选择正确的设备类型，如[图9-3](#arduino_board_dropdown_9)所示。
- en: '![Screenshot of the ''Board'' dropdown](Images/timl_0605.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![“板”下拉列表的截图](Images/timl_0605.png)'
- en: Figure 9-3\. The Board drop-down list
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3. 板下拉列表
- en: If your device’s name doesn’t appear in the list, you’ll need to install its
    support package. To do this, click Boards Manager. In the window that appears,
    search for your device and install the latest version of the corresponding support
    package.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的设备名称不在列表中显示，您需要安装其支持包。要做到这一点，请点击Boards Manager。在弹出的窗口中搜索您的设备并安装相应支持包的最新版本。
- en: Also in the Tools menu, make sure the device’s port is selected in the Port
    drop-down list, as demonstrated in [Figure 9-4](#arduino_port_dropdown_9).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在工具菜单中，还要确保设备的端口在端口下拉列表中被选中，如[图9-4](#arduino_port_dropdown_9)所示。
- en: '![Screenshot of the ''Port'' dropdown](Images/timl_0606.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![“端口”下拉列表的截图](Images/timl_0606.png)'
- en: Figure 9-4\. The Port drop-down list
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4. 端口下拉列表
- en: Finally, in the Arduino window, click the upload button (highlighted in white
    in [Figure 9-5](#arduino_upload_button_9)) to compile and upload the code to your
    Arduino device.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在Arduino窗口中，点击上传按钮（在[图9-5](#arduino_upload_button_9)中用白色标出）来编译并上传代码到您的Arduino设备。
- en: '![Screenshot of the upload button](Images/timl_0607.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![上传按钮的截图](Images/timl_0607.png)'
- en: Figure 9-5\. The upload button
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5. 上传按钮
- en: As soon as the upload has successfully completed, the program will run.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦上传成功完成，程序将运行。
- en: To test it, start by pointing the device’s camera at something that is definitely
    not a person, or just covering up the lens. The next time the blue LED flashes,
    the device will capture a frame from the camera and begin to run inference. Because
    the vision model we are using for person detection is relatively large, this will
    take a long time inference—around 19 seconds at the time of writing, though it’s
    possible TensorFlow Lite has become faster since then.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试它，首先将设备的摄像头对准明显不是人的东西，或者只是遮住镜头。下次蓝色LED闪烁时，设备将从摄像头捕获一帧并开始运行推理。由于我们用于人员检测的视觉模型相对较大，这将需要很长时间的推理——在撰写本文时大约需要19秒，尽管自那时起TensorFlow
    Lite可能已经变得更快。
- en: When inference is complete, the result will be translated into another LED being
    lit. You pointed the camera at something that isn’t a person, so the red LED should
    illuminate.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 当推断完成时，结果将被翻译为另一个LED被点亮。您将相机对准了一个不是人的东西，所以红色LED应该点亮。
- en: Now, try pointing the device’s camera at yourself! The next time the blue LED
    flashes, the device will capture another image and begin to run inference. After
    roughly 19 seconds, the green LED should turn on.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尝试将设备的相机对准自己！下次蓝色LED闪烁时，设备将捕获另一幅图像并开始运行推断。大约19秒后，绿色LED应该亮起。
- en: Remember, image data is captured as a snapshot before each inference, whenever
    the blue LED flashes. Whatever the camera is pointed at during that moment is
    what will be fed into the model. It doesn’t matter where the camera is pointed
    until the next time an image is captured, when the blue LED will flash again.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在每次推断之前，图像数据都会被捕获为快照，每当蓝色LED闪烁时。在那一刻相机对准的东西将被馈送到模型中。在下一次捕获图像时，相机对准的位置并不重要，当蓝色LED再次闪烁时，图像将被捕获。
- en: If you’re getting seemingly incorrect results, make sure you are in an environment
    with good lighting. You should also make sure that the camera is oriented correctly,
    with the pins pointing downward, so that the images it captures are the right
    way up—the model was not trained to recognize upside-down people. In addition,
    it’s good to remember that this is a tiny model, which trades accuracy for small
    size. It works very well, but it isn’t accurate 100% of the time.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您得到看似不正确的结果，请确保您处于光线良好的环境中。您还应确保相机的方向正确，引脚朝下，以便捕获的图像是正确的方式——该模型没有经过训练以识别颠倒的人。此外，值得记住这是一个微小的模型，它以小尺寸换取准确性。它工作得非常好，但并非100%准确。
- en: 'You can also see the results of inference via the Arduino Serial Monitor. To
    do this, from the Tools menu, open the Serial Monitor. You’ll see a detailed log
    showing what is happening while the application runs. It’s also interesting to
    check the “Show timestamp” box, so you can see how long each part of the process
    takes:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过Arduino串行监视器查看推断的结果。要做到这一点，请从“工具”菜单中打开串行监视器。您将看到一个详细的日志，显示应用程序运行时发生的情况。还有一个有趣的功能是勾选“显示时间戳”框，这样您就可以看到每个过程需要多长时间：
- en: '[PRE42]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: From this log, we can see that it took around 170 ms to capture and read the
    image data from the camera module, 180 ms to decode the JPEG and convert it to
    grayscale, and 18.6 seconds to run inference.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个日志中，我们可以看到从相机模块捕获和读取图像数据大约需要170毫秒，解码JPEG并将其转换为灰度需要180毫秒，运行推断需要18.6秒。
- en: Making your own changes
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进行自己的更改
- en: Now that you’ve deployed the basic application, try playing around and making
    some changes to the code. Just edit the files in the Arduino IDE and save, and
    then repeat the previous instructions to deploy your modified code to the device.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已部署了基本应用程序，请尝试玩耍并对代码进行一些更改。只需在Arduino IDE中编辑文件并保存，然后重复之前的说明以将修改后的代码部署到设备上。
- en: 'Here are a few things you could try:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是您可以尝试的几件事：
- en: Modify the detection responder so that it ignores ambiguous inputs, where there
    isn’t much difference between the “person” and “no person” scores.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改检测响应器，使其忽略模糊的输入，即“人”和“无人”得分之间没有太大差异的情况。
- en: Use the results of person detection to control other components, like additional
    LEDs or servos.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用人员检测的结果来控制其他组件，如额外的LED或伺服。
- en: Build a smart security camera, by storing or transmitting images—but only those
    that contain a person.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个智能安全摄像头，通过存储或传输图像来实现，但仅限于包含人物的图像。
- en: SparkFun Edge
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SparkFun Edge
- en: The SparkFun Edge board is optimized for low power consumption. When paired
    with a similarly efficient camera module, it’s the ideal platform for building
    vision applications that will be running on battery power. It’s easy to plug in
    a camera module via the board’s ribbon cable adapter.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: SparkFun Edge板经过优化，以实现低功耗。当与同样高效的相机模块配对时，它是构建视觉应用程序的理想平台，这些应用程序将在电池供电时运行。通过板上的排线适配器轻松插入相机模块。
- en: Which camera module to buy
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要购买哪种相机模块
- en: 'This example uses SparkFun’s [Himax HM01B0 breakout camera module](https://oreil.ly/H24xS).
    It’s based on a 320 × 320–pixel image sensor that consumes an extremely small
    amount of power: less than 2 mW when capturing at 30 frames per second (FPS).'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例使用SparkFun的[Himax HM01B0分支相机模块](https://oreil.ly/H24xS)。它基于一个320×320像素的图像传感器，当以每秒30帧的速度捕获时，消耗极少的功率：不到2
    mW。
- en: Capturing images on SparkFun Edge
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在SparkFun Edge上捕获图像
- en: To begin capturing images with the Himax HM01B0 camera module, we first must
    initialize the camera. After this is done, we can read a frame from the camera
    every time we need a new image. A frame is an array of bytes representing what
    the camera can currently see.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用Himax HM01B0相机模块捕获图像，我们首先必须初始化相机。完成此操作后，我们可以在需要新图像时从相机读取一帧。一帧是一个表示相机当前所看到的内容的字节数组。
- en: Working with the camera will involve heavy use of both the Ambiq Apollo3 SDK,
    which is downloaded as part of the build process, and the HM01B0 driver, which
    is located in [*sparkfun_edge/himax_driver*](https://oreil.ly/OhBj0).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相机将涉及大量使用Ambiq Apollo3 SDK和HM01B0驱动程序，后者作为构建过程的一部分下载，位于[*sparkfun_edge/himax_driver*](https://oreil.ly/OhBj0)中。
- en: The image provider is implemented in [*sparkfun_edge/image_provider.cc*](https://oreil.ly/ZdU9N).
    We won’t explain its every detail, because the code is specific to the SparkFun
    board and the Himax camera module. Instead, let’s step through what happens at
    a high level.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图像提供程序实现在[*sparkfun_edge/image_provider.cc*](https://oreil.ly/ZdU9N)中。我们不会解释其每个细节，因为代码是针对SparkFun板和Himax相机模块的。相反，让我们以高层次的方式来看看发生了什么。
- en: 'The `GetImage()` function is the image provider’s interface with the world.
    It’s called in our application’s main loop to obtain a frame of image data. The
    first time it is called, we’ll need to initialize the camera. This happens with
    a call to the `InitCamera()` function, as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`GetImage()` 函数是图像提供程序与世界的接口。它在我们的应用程序的主循环中被调用以获取一帧图像数据。第一次调用时，我们需要初始化摄像头。这通过调用
    `InitCamera()` 函数来实现，如下所示：'
- en: '[PRE43]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: If `InitCamera()` returns anything other than a `kTfLiteOk` status, we switch
    on the board’s red LED (using `am_hal_gpio_output_set(AM_BSP_GPIO_LED_RED)`) to
    indicate a problem. This is helpful for debugging.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `InitCamera()` 返回除了 `kTfLiteOk` 状态之外的任何内容，我们会打开板上的红色 LED（使用 `am_hal_gpio_output_set(AM_BSP_GPIO_LED_RED)`）来指示问题。这对于调试很有帮助。
- en: The `InitCamera()` function is defined further up in *image_provider.cc*. We
    won’t walk through it here because it’s very device-specific, and if you want
    to use it in your own code you can just copy and paste it.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`InitCamera()` 函数在 *image_provider.cc* 中进一步定义。我们不会在这里详细介绍它，因为它非常特定于设备，如果您想在自己的代码中使用它，只需复制粘贴即可。'
- en: It calls a bunch of Apollo3 SDK functions to configure the microcontroller’s
    inputs and outputs so that it can communicate with the camera module. It also
    enables *interrupts*, which are the mechanism used by the camera to send over
    new image data. When this is all set up, it uses the camera driver to switch on
    the camera and configures it to start continually capturing images.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 它调用一堆 Apollo3 SDK 函数来配置微控制器的输入和输出，以便它可以与摄像头模块通信。它还启用了*中断*，这是摄像头用来发送新图像数据的机制。当这一切设置完成后，它使用摄像头驱动程序打开摄像头，并配置它开始持续捕获图像。
- en: 'The camera module has an autoexposure feature, which calibrates its exposure
    setting automatically as frames are captured. To allow it the opportunity to calibrate
    before we attempt to perform inference, the next part of the `GetImage()` function
    uses the camera driver’s `hm01b0_blocking_read_oneframe_scaled()` function to
    capture several frames. We don’t do anything with the captured data; we are only
    doing this to give the camera module’s autoexposure function some material to
    work with:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 摄像头模块具有自动曝光功能，它会在捕获帧时自动校准曝光设置。为了让它有机会在我们尝试执行推理之前校准，`GetImage()` 函数的下一部分使用摄像头驱动程序的
    `hm01b0_blocking_read_oneframe_scaled()` 函数捕获几帧图像。我们不对捕获的数据做任何处理；我们只是为了让摄像头模块的自动曝光功能有一些材料可以使用：
- en: '[PRE44]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'After setup is out of the way, the rest of the `GetImage()` function is very
    simple. All we do is call `hm01b0_blocking_read_oneframe_scaled()` to capture
    an image:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成后，`GetImage()` 函数的其余部分非常简单。我们只需调用 `hm01b0_blocking_read_oneframe_scaled()`
    来捕获一幅图像：
- en: '[PRE45]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: When `GetImage()` is called during the application’s main loop, the `frame`
    variable is a pointer to our input tensor, so the data is written directly by
    the camera driver to the area of memory allocated to the input tensor. We also
    specify the width, height, and number of channels we want.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序的主循环中调用 `GetImage()` 时，`frame` 变量是指向我们输入张量的指针，因此数据直接由摄像头驱动程序写入到为输入张量分配的内存区域。我们还指定了我们想要的宽度、高度和通道数。
- en: With this implementation, we’re able to capture image data from our camera module.
    Next, let’s look at how we respond to the model’s output.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个实现，我们能够从我们的摄像头模块中捕获图像数据。接下来，让我们看看如何响应模型的输出。
- en: Responding to detections on SparkFun Edge
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 SparkFun Edge 上响应检测
- en: The detection responder’s implementation is very similar to our wake-word example’s
    command responder. It toggles the device’s blue LED every time inference is run.
    When a person is detected, it lights the green LED, and when a person is not detected
    it lights the yellow LED.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 检测响应器的实现与我们的唤醒词示例的命令响应器非常相似。每次运行推理时，它会切换设备的蓝色 LED。当检测到一个人时，它会点亮绿色 LED，当没有检测到一个人时，它会点亮黄色
    LED。
- en: The implementation is in [*sparkfun_edge/detection_responder.cc*](https://oreil.ly/OeN1M).
    Let’s take a quick walk through.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 实现在 [*sparkfun_edge/detection_responder.cc*](https://oreil.ly/OeN1M) 中。让我们快速浏览一下。
- en: 'The `RespondToDetection()` function accepts two scores, one for the “person”
    category, and the other for “not a person.” The first time it is called, it sets
    up the blue, green, and yellow LEDs for output:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '`RespondToDetection()` 函数接受两个分数，一个用于“人”类别，另一个用于“非人”。第一次调用时，它会为蓝色、绿色和黄色 LED
    设置输出：'
- en: '[PRE46]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Because the function is called once per inference, the next snippet of code
    causes it to toggle the blue LED on and off each time inference is performed:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 因为该函数每次推理调用一次，所以下面的代码片段会导致它在每次执行推理时切换蓝色 LED 的开关：
- en: '[PRE47]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Finally, it turns on the green LED if a person was detected, or the blue LED
    if not. It also logs the score using the `ErrorReporter` instance:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果检测到一个人，它会点亮绿色 LED，如果没有检测到一个人，它会点亮蓝色 LED。它还使用 `ErrorReporter` 实例记录分数：
- en: '[PRE48]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'And that’s it! The core of the function is a basic `if` statement, and you
    could easily use similar logic could to control other types of output. There’s
    something very exciting about such a complex visual input being transformed into
    a single Boolean output: “person” or “no person.”'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！函数的核心是一个基本的 `if` 语句，你可以很容易地使用类似的逻辑来控制其他类型的输出。将如此复杂的视觉输入转换为一个布尔输出“人”或“非人”是非常令人兴奋的事情。
- en: Running the example
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行示例
- en: Now that we’ve seen how the SparkFun Edge implementation works, let’s get it
    up and running.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了 SparkFun Edge 实现的工作原理，让我们开始运行它。
- en: Tip
  id: totrans-309
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: There’s always a chance that the build process might have changed since this
    book was written, so check [*README.md*](https://oreil.ly/kaSXN) for the latest
    instructions.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书编写时可能已更改构建过程，因此请查看 [*README.md*](https://oreil.ly/kaSXN) 获取最新说明。
- en: 'To build and deploy our code, we’ll need the following:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建和部署我们的代码，我们需要以下内容：
- en: A SparkFun Edge board with the [Himax HM01B0 breakout](https://oreil.ly/jNtyv)
    attached
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有 [Himax HM01B0 breakout](https://oreil.ly/jNtyv) 的 SparkFun Edge 开发板
- en: A USB programmer (we recommend the SparkFun Serial Basic Breakout, which is
    available in both [micro-B USB](https://oreil.ly/wXo-f) and [USB-C](https://oreil.ly/-YvfN)
    variants)
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个USB编程器（我们推荐SparkFun串行基础分支，可在[micro-B USB](https://oreil.ly/wXo-f)和[USB-C](https://oreil.ly/-YvfN)变种中获得）
- en: A matching USB cable
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一根匹配的USB电缆
- en: Python 3 and some dependencies
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3和一些依赖项
- en: Note
  id: totrans-316
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re unsure whether you have the correct version of Python installed, [“Running
    the Example”](ch06.xhtml#running_hello_world_sparkfun_edge) has instructions on
    how to check.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定是否安装了正确版本的Python，请参考[“运行示例”](ch06.xhtml#running_hello_world_sparkfun_edge)中的说明进行检查。
- en: 'In a terminal, clone the TensorFlow repository and change into its directory:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中，克隆TensorFlow存储库并切换到其目录：
- en: '[PRE49]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Next, we’re going to build the binary and run some commands that get it ready
    for downloading to the device. To avoid some typing, you can copy and paste these
    commands from [*README.md*](https://oreil.ly/kaSXN).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建二进制文件并运行一些命令，使其准备好下载到设备上。为了避免一些打字，您可以从[*README.md*](https://oreil.ly/kaSXN)中复制并粘贴这些命令。
- en: Build the binary
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建二进制文件
- en: 'The following command downloads all of the required dependencies and then compiles
    a binary for the SparkFun Edge:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令下载所有必需的依赖项，然后为SparkFun Edge编译一个二进制文件：
- en: '[PRE50]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The binary is created as a *.bin* file, in the following location:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制文件被创建为*.bin*文件，位于以下位置：
- en: '[PRE51]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'To check that the file exists, you can use the following command:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查文件是否存在，可以使用以下命令：
- en: '[PRE52]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: When you run that command, you should see `Binary was successfully created`
    printed to the console.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行该命令时，您应该看到`Binary was successfully created`打印到控制台。
- en: If you see `Binary is missing`, there was a problem with the build process.
    If so, it’s likely that there are some clues to what went wrong in the output
    of the `make` command.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如果看到`Binary is missing`，则构建过程中出现问题。如果是这样，很可能在`make`命令的输出中有一些线索指出出了什么问题。
- en: Sign the binary
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对二进制文件进行签名
- en: The binary must be signed with cryptographic keys to be deployed to the device.
    Let’s now run some commands that will sign the binary so that it can be flashed
    to the SparkFun Edge. The scripts used here come from the Ambiq SDK, which is
    downloaded when the Makefile is run.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 必须使用加密密钥对二进制文件进行签名，才能部署到设备上。现在让我们运行一些命令，对二进制文件进行签名，以便可以刷写到SparkFun Edge上。这里使用的脚本来自Ambiq
    SDK，在运行Makefile时下载。
- en: 'Enter the following command to set up some dummy cryptographic keys that you
    can use for development:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 输入以下命令设置一些虚拟的加密密钥，供开发使用：
- en: '[PRE53]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, run the following command to create a signed binary. Substitute `python3`
    with `python` if necessary:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，运行以下命令创建一个已签名的二进制文件。如果需要，将`python3`替换为`python`：
- en: '[PRE54]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This creates the file *main_nonsecure_ota.bin*. Now run this command to create
    a final version of the file that you can use to flash your device with the script
    you will use in the next step:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建文件*main_nonsecure_ota.bin*。现在运行此命令创建文件的最终版本，您可以使用该文件刷写设备，使用下一步中将使用的脚本：
- en: '[PRE55]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: You should now have a file called *main_nonsecure_wire.bin* in the directory
    where you ran the commands. This is the file you’ll be flashing to the device.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该在运行命令的目录中有一个名为*main_nonsecure_wire.bin*的文件。这是您将要刷写到设备的文件。
- en: Flash the binary
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 刷写二进制文件
- en: The SparkFun Edge stores the program it is currently running in its 1 megabyte
    of flash memory. If you want the board to run a new program, you need to send
    it to the board, which will store it in flash memory, overwriting any program
    that was previously saved.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: SparkFun Edge将当前运行的程序存储在其1兆字节的闪存中。如果您希望板运行新程序，您需要将其发送到板上，该程序将存储在闪存中，覆盖先前保存的任何程序。
- en: As we’ve mentioned earlier in the book, this process is called *flashing*.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中早些时候提到的，这个过程被称为*刷写*。
- en: Attach the programmer to the board
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将编程器连接到板上
- en: To download new programs to the board, you’ll use the SparkFun USB-C Serial
    Basic serial programmer. This device allows your computer to communicate with
    the microcontroller via USB.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载新程序到板上，您将使用SparkFun USB-C串行基础串行编程器。该设备允许您的计算机通过USB与微控制器通信。
- en: 'To attach this device to your board, perform the following steps:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 将此设备连接到您的板上，执行以下步骤：
- en: On the side of the SparkFun Edge, locate the six-pin header.
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在SparkFun Edge的一侧，找到六针排针。
- en: Plug the SparkFun USB-C Serial Basic into these pins, ensuring that the pins
    labeled BLK and GRN on each device are lined up correctly, as demonstrated in
    [Figure 9-6](#sparkfun_edge_serial_basic_3).
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将SparkFun USB-C串行基础插入这些引脚，确保每个设备上标记为BLK和GRN的引脚正确对齐，如[图9-6](#sparkfun_edge_serial_basic_3)所示。
- en: '![A photo showing how the SparkFun Edge and USB-C Serial Basic should be connected](Images/timl_0613.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![显示SparkFun Edge和USB-C串行基础如何连接的照片](Images/timl_0613.png)'
- en: Figure 9-6\. Connecting the SparkFun Edge and USB-C Serial Basic (courtesy of
    SparkFun)
  id: totrans-348
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6\. 连接SparkFun Edge和USB-C串行基础（由SparkFun提供）
- en: Attach the programmer to your computer
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将编程器连接到计算机
- en: You connect the board to your computer via USB. To program the board, you need
    to find out the name that your computer gives the device. The best way of doing
    this is to list all of the computer’s devices before and after attaching it and
    then look to see which device is new.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 通过USB将板连接到计算机。要对板进行编程，您需要找出计算机给设备的名称。最好的方法是在连接设备之前和之后列出所有计算机的设备，然后查看哪个设备是新的。
- en: Warning
  id: totrans-351
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Some people have reported issues with their operating system’s default drivers
    for the programmer, so we strongly recommend installing the [driver](https://oreil.ly/yI-NR)
    before you continue.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人报告了他们操作系统的默认驱动程序与编程器存在问题，因此我们强烈建议在继续之前安装[驱动程序](https://oreil.ly/yI-NR)。
- en: 'Before attaching the device via USB, run the following command:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过USB连接设备之前，运行以下命令：
- en: '[PRE56]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'This should output a list of attached devices that looks something like the
    following:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出一个附加设备列表，看起来像以下内容：
- en: '[PRE57]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, connect the programmer to your computer’s USB port and run the following
    command again:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将编程器连接到计算机的USB端口，并再次运行以下命令：
- en: '[PRE58]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'You should see an extra item in the output, as in the example that follows.
    Your new item might have a different name. This new item is the name of the device:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在输出中看到一个额外的项目，如下例所示。您的新项目可能有不同的名称。这个新项目是设备的名称：
- en: '[PRE59]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This name will be used to refer to the device. However, it can change depending
    on which USB port the programmer is attached to, so if you disconnect the board
    from the computer and then reattach it, you might have to look up its name again.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这个名称将用于引用设备。但是，它可能会根据编程器连接的USB端口而变化，因此如果您将板子从计算机断开然后重新连接，可能需要再次查找其名称。
- en: Tip
  id: totrans-362
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Some users have reported two devices appearing in the list. If you see two devices,
    the correct one to use begins with the letters “wch”; for example, `/dev/wchusbserial-14410.`
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 一些用户报告列表中出现了两个设备。如果看到两个设备，则要使用的正确设备以“wch”开头；例如，`/dev/wchusbserial-14410.`
- en: 'After you’ve identified the device name, put it in a shell variable for later
    use:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定设备名称后，将其放入一个shell变量以备后用：
- en: '[PRE60]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This is a variable that you can use when running commands that require the device
    name, later in the process.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在后续过程中运行需要设备名称的命令时可以使用的变量。
- en: Run the script to flash your board
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行脚本以刷写您的板子
- en: To flash the board, you need to put it into a special “bootloader” state that
    prepares it to receive the new binary. You’ll then run a script to send the binary
    to the board.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 要刷写板子，您需要将其置于特殊的“引导加载程序”状态，以准备接收新的二进制文件。然后，您将运行一个脚本将二进制文件发送到板子。
- en: 'First create an environment variable to specify the baud rate, which is the
    speed at which data will be sent to the device:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个环境变量来指定波特率，即数据发送到设备的速度：
- en: '[PRE61]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now paste the following command into your terminal—but *do not press Enter yet*!
    The `${DEVICENAME}` and `${BAUD_RATE}` in the command will be replaced with the
    values you set in the previous sections. Remember to substitute `python3` with
    `python` if necessary.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将以下命令粘贴到终端中，但*不要立即按Enter*！命令中的`${DEVICENAME}`和`${BAUD_RATE}`将被替换为您在前面部分设置的值。如有必要，请记得将`python3`替换为`python`。
- en: '[PRE62]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Next, you’ll reset the board into its bootloader state and flash the board.
    On the board, locate the buttons marked `RST` and `14`, as shown in [Figure 9-7](#sparkfun_edge_buttons_3).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将重置板子到引导加载程序状态并刷写板子。在板子上，找到标有“RST”和“14”的按钮，如[图9-7](#sparkfun_edge_buttons_3)所示。
- en: 'Perform the following steps:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: Ensure that your board is connected to the programmer, and the entire thing
    is connected to your computer via USB.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您的板子连接到编程器，并且整个设备通过USB连接到计算机。
- en: On the board, press and hold the button marked `14`. *Continue holding it.*
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在板子上，按住标有“14”的按钮。*继续按住它。*
- en: While still holding the button marked `14`, press the button marked `RST` to
    reset the board.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在仍按住标有“14”的按钮的情况下，按下标有“RST”的按钮重置板子。
- en: Press Enter on your computer to run the script. *Continue on holding button
    `14`.*
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算机上按Enter运行脚本。*继续按住按钮`14`。*
- en: '![A photo showing the SparkFun Edge''s buttons](Images/timl_0614.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![显示SparkFun Edge按钮的照片](Images/timl_0614.png)'
- en: Figure 9-7\. The SparkFun Edge’s buttons
  id: totrans-380
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7\. SparkFun Edge的按钮
- en: 'You should now see something like the following appearing on your screen:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该在屏幕上看到类似以下内容：
- en: '[PRE63]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Keep holding button `14` until you see `Sending Data Packet of length 8180`.
    You can release the button after seeing this (but it’s okay if you keep holding
    it).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 继续按住按钮`14`，直到看到`Sending Data Packet of length 8180`。在看到此信息后可以释放按钮（但如果继续按住也没关系）。
- en: 'The program will continue to print lines on the terminal. Eventually, you’ll
    see something like the following:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 程序将继续在终端上打印行。最终，您会看到类似以下内容：
- en: '[PRE64]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: This indicates a successful flashing.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示刷写成功。
- en: Tip
  id: totrans-387
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If the program output ends with an error, check whether `Sending Reset Command.`
    was printed. If so, flashing was likely successful despite the error. Otherwise,
    flashing might have failed. Try running through these steps again (you can skip
    over setting the environment variables).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 如果程序输出以错误结束，请检查是否打印了`Sending Reset Command.`。如果是，则尽管出现错误，刷写可能已成功。否则，刷写可能失败。尝试再次运行这些步骤（您可以跳过设置环境变量）。
- en: Testing the program
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试程序
- en: Start by pressing the `RST` button, to make sure the program is running.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 首先按下`RST`按钮，确保程序正在运行。
- en: When the program is running the blue LED will toggle on and off, once for each
    inference. Because the vision model we are using for person detection is relatively
    large, it takes a long time to run inference—around 6 seconds in total.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 当程序运行时，蓝色LED将交替闪烁，每次推理一次。由于我们用于人员检测的视觉模型相对较大，运行推理需要很长时间——总共约6秒。
- en: Start by pointing the device’s camera at something that is definitely not a
    person, or just covering up the lens. The next time the blue LED toggles, the
    device will capture a frame from the camera and begin to run inference. After
    6 seconds or so, the inference result will be translated into another LED being
    lit. Given that you pointed the camera at something that isn’t a person, the orange
    LED should light up.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 首先将设备的摄像头对准绝对不是人的东西，或者只是遮住镜头。下一次蓝色LED切换时，设备将从摄像头捕获一帧并开始运行推理。大约6秒后，推理结果将被转换为另一个LED点亮。鉴于您将摄像头对准的不是人，橙色LED应该点亮。
- en: Now, try pointing the device’s camera at yourself. The next time the blue LED
    toggles, the device will capture another frame and begin to run inference. This
    time, the green LED should light up.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尝试将设备的摄像头对准自己。下一次蓝色LED切换时，设备将捕获另一帧并开始运行推理。这次，绿色LED应该点亮。
- en: Remember, image data is captured as a snapshot before each inference, whenever
    the blue LED toggles. Whatever the camera is pointed at during that moment is
    what will be fed into the model. It doesn’t matter where the camera is pointed
    until the next time a frame is captured, when the blue LED will toggle again.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在每次推理之前，图像数据都会被捕获为快照，每当蓝色LED切换时。在那一刻摄像头对准的东西将被输入模型。在下一次捕获帧时，摄像头对准的位置并不重要，蓝色LED将再次切换。
- en: If you’re getting seemingly incorrect results, make sure that you are in an
    environment with good lighting. It’s also good to remember that this is a tiny
    model, which trades accuracy for small size. It works very well, but it isn’t
    accurate 100% all of the time.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您得到看似不正确的结果，请确保您处于光线良好的环境中。还要记住，这是一个小模型，它以精度换取了小尺寸。它工作得非常好，但并非始终100%准确。
- en: Viewing debug data
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看调试数据
- en: 'The program will log detection results to the serial port. To view them, we
    can monitor the board’s serial port output using a baud rate of 115200\. On macOS
    and Linux, the following command should work:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序将检测结果记录到串行端口。要查看它们，我们可以使用波特率为115200监视板的串行端口输出。在macOS和Linux上，以下命令应该有效：
- en: '[PRE65]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'You should initially see output that looks something like the following:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该最初看到类似以下内容的输出：
- en: '[PRE66]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'As the board captures frames and runs inference, you should see it printing
    debug information:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 当板捕获帧并运行推断时，您应该看到它打印调试信息：
- en: '[PRE67]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: To stop viewing the debug output with `screen`, press Ctrl-A, immediately followed
    by the K key, and then press the Y key.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止使用`screen`查看调试输出，请按Ctrl-A，紧接着按K键，然后按Y键。
- en: Making your own changes
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进行您自己的更改
- en: Now that you’ve deployed the basic application, try playing around and making
    some changes. You can find the application’s code in the *tensorflow/lite/micro/examples/person_detection*
    folder. Just edit and save, and then repeat the preceding instructions to deploy
    your modified code to the device.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经部署了基本应用程序，请尝试玩耍并进行一些更改。您可以在*tensorflow/lite/micro/examples/person_detection*文件夹中找到应用程序的代码。只需编辑并保存，然后重复前面的说明以将修改后的代码部署到设备上。
- en: 'Here are a few things you could try:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是您可以尝试的一些事项：
- en: Modify the detection responder so that it ignores ambiguous inputs, where there
    isn’t much difference between the “person” and “no person” scores.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改检测响应器，使其忽略模糊的输入，即“人”和“无人”得分之间没有太大差异的情况。
- en: Use the results of person detection to control other components, like additional
    LEDs or servos.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用人员检测结果来控制其他组件，如额外的LED或伺服。
- en: Build a smart security camera, by storing or transmitting images—but only those
    that contain a person.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个智能安全摄像头，通过存储或传输图像来检测只包含人的图像。
- en: Wrapping Up
  id: totrans-410
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'The vision model we’ve used in this chapter is an amazing thing. It accepts
    raw and messy input, no preprocessing required, and gives us a beautifully simple
    output: yes, a person is present, or no, there is no one present. This is the
    magic of machine learning: it can filter information from noise, leaving us with
    only the signals we care about. As developers, it’s easy to use these signals
    to build amazing experiences for our users.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中使用的视觉模型是一件了不起的事情。它接受原始且混乱的输入，无需预处理，并为我们提供一个非常简单的输出：是，有人在场，还是，没有人在场。这就是机器学习的魔力：它可以从噪音中过滤信息，留下我们关心的信号。作为开发者，我们可以轻松使用这些信号为用户构建令人惊叹的体验。
- en: When building machine learning applications, it’s very common to use pretrained
    models like this one, which already contain the knowledge required to perform
    a task. Roughly equivalent to code libraries, models encapsulate specific functionality
    and are easily shared between projects. You’ll often find yourself exploring and
    evaluating models, looking for the proper fit for your task.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建机器学习应用程序时，很常见使用像这样的预训练模型，这些模型已经包含执行任务所需的知识。模型大致相当于代码库，封装了特定功能，并且可以在项目之间轻松共享。您经常会发现自己在探索和评估模型，寻找适合您任务的合适模型。
- en: In [Chapter 10](ch10.xhtml#chapter_person_detection_training), we’ll examine
    how the person detection model works. You’ll also learn how to train your own
    vision models to spot different types of objects.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.xhtml#chapter_person_detection_training)中，我们将探讨人员检测模型的工作原理。您还将学习如何训练自己的视觉模型来识别不同类型的对象。
- en: ^([1](ch09.xhtml#idm46473559664376-marker)) In a [2018 YouGov poll](https://oreil.ly/KvzGk),
    70% of respondents said that they would miss sight the most if they lost it.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在[2018年YouGov民意调查](https://oreil.ly/KvzGk)中，70%的受访者表示，如果失去视力，他们会最怀念视觉。
