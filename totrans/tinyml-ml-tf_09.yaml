- en: 'Chapter 9\. Person Detection: Building an Application'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you asked people which of their senses has the biggest impact on their day-to-day
    lives, many would answer vision.^([1](ch09.xhtml#idm46473559664376))
  prefs: []
  type: TYPE_NORMAL
- en: Vision is a profoundly useful sense. It allows countless natural organisms to
    navigate their environments, find sources of food, and avoid running into danger.
    As humans, vision helps us recognize our friends, interpret symbolic information,
    and understand the world around us—without having to get too close.
  prefs: []
  type: TYPE_NORMAL
- en: Until quite recently, the power of vision was not available to machines. Most
    of our robots merely poked around the world with touch and proximity sensors,
    gleaning knowledge of its structure from a series of collisions. At a glance,
    a person can describe to you the shape, properties, and purpose of an object,
    without having to interact with it at all. A robot would have no such luck. Visual
    information was just too messy, unstructured, and difficult to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: With the evolution of convolutional neural networks, it’s become easy to build
    programs that can *see*. Inspired by the structure of the mammalian visual cortex,
    CNNs learn to make sense of our visual world, filtering an overwhelmingly complex
    input into a map of known patterns and shapes. The precise combination of these
    pieces can tell us the entities that are present in a given digital image.
  prefs: []
  type: TYPE_NORMAL
- en: Today, vision models are used for many different tasks. Autonomous vehicles
    use vision to spot hazards on the road. Factory robots use cameras to catch defective
    parts. Researchers have trained models that can diagnose disease from medical
    images. And there’s a fair chance your smartphone spots faces in photographs,
    to make sure they’re perfectly in focus.
  prefs: []
  type: TYPE_NORMAL
- en: Machines with sight could help transform our homes and cities, automating chores
    that were previously out of reach. But vision is an intimate sense. Most of us
    don’t like the thought of our actions being recorded, or our lives being streamed
    to the cloud, which is traditionally where ML inference is done.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a household appliance that can “see” with a built-in camera. It could
    be a security system that can spot intruders, a stove that knows it’s been left
    unattended, or a television that shuts off when there’s no one in the room. In
    each of these cases, privacy is critical. Even if no human being ever watches
    the footage, the security implications of internet-connected cameras embedded
    in always-on devices make them unappealing to most consumers.
  prefs: []
  type: TYPE_NORMAL
- en: But all this changes with TinyML. Picture a smart stove that shuts off its burners
    if it’s left unattended for too long. If it can “see” there’s a cook nearby using
    a tiny microcontroller, without any connection to the internet, we get all of
    the benefits of a smart device without any of the privacy trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Even more, tiny devices with vision can go where no sight-enabled machines have
    dared to go before. With its miniscule power consumption, a microcontroller-based
    vision system could run for months or years on a tiny battery. Planted in the
    jungle, or a coral reef, these devices could keep count of endangered animals
    without the need to be online.
  prefs: []
  type: TYPE_NORMAL
- en: The same technology makes it possible to build a vision sensor as a self-contained
    electronic component. The sensor outputs a 1 if a certain object is in view and
    a 0 if it is not, but it never shares any of the image data collected by its camera.
    This type of sensor could be embedded in all kinds of products—from smart home
    systems to personal vehicles. Your bicycle could flash a light when a car is behind
    you. Your air conditioner could know when someone’s home. And because the image
    data never leaves the self-contained sensor, it’s guaranteed secure, even if the
    product is connected to the internet.
  prefs: []
  type: TYPE_NORMAL
- en: The application we explore in this chapter uses a pretrained person-detection
    model, running on a microcontroller with a camera attached, to know when a human
    being is in view. In [Chapter 10](ch10.xhtml#chapter_person_detection_training),
    you will learn how this model works, and how to train your own models that detect
    whatever you want.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you’ll understand how to work with camera data on
    a microcontroller and how to run inference with a vision model and interpret the
    output. You might be surprised how easy it actually is!
  prefs: []
  type: TYPE_NORMAL
- en: What We’re Building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re going to build an embedded application that uses a model to classify images
    captured by a camera. The model is trained to recognize when a person is present
    in the camera input. This means that our application will be able to detect the
    presence or absence of a person and produce an output accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: This is, essentially, the smart vision sensor we described a little earlier.
    When a person is detected, our example code will light an LED—but you can extend
    it to control all sorts of projects.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with the application we worked on in [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example),
    you can find the source code for this application in the [TensorFlow GitHub repository](https://oreil.ly/9aLhs).
  prefs: []
  type: TYPE_NORMAL
- en: Like in the previous chapters, we first walk through the tests and the application
    code, followed by the logic that makes the sample work on various devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide instructions for deploying the application to the following microcontroller
    platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SparkFun Edge](https://oreil.ly/-hoL-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TensorFlow Lite regularly adds support for new devices, so if the device you’d
    like to use isn’t listed here, it’s worth checking the example’s [*README.md*](https://oreil.ly/6gRlo).
    You can also check there for updated deployment instructions if you run into trouble
    following these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike with the previous chapters, you’ll need some additional hardware to run
    this application. Because neither of these boards have an integrated camera, we
    recommend buying a *camera module*. You’ll find this information in each device’s
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by walking through our application’s structure. It’s a lot simpler
    than you might expect.
  prefs: []
  type: TYPE_NORMAL
- en: Application Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now, we’ve established that embedded machine learning applications do the
    following sequence of things:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain an input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the input to extract features suitable to feed into a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run inference on the processed input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Postprocess the model’s output to make sense of it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the resulting information to make things happen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example) we saw this applied
    to wake-word detection, which uses audio as its input. This time around, our input
    will be image data. This might sound more complicated, but it’s actually much
    simpler to work with than audio.
  prefs: []
  type: TYPE_NORMAL
- en: Image data is commonly represented as an array of pixel values. We’ll be obtaining
    our image data from embedded camera modules, which all provide data in this format.
    Our model also expects its input to be an array of pixel values. Because of this,
    we won’t have to do much preprocessing before feeding data into our model.
  prefs: []
  type: TYPE_NORMAL
- en: Given that we don’t have to do much preprocessing, our app will be fairly straightforward.
    It takes a snapshot of data from a camera, feeds it into a model, and determines
    which output class was detected. It then displays the result in some simple manner.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on, let’s learn a little more about the model we’ll be using.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Our Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back in [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example), we learned
    that convolutional neural networks are neural networks designed to work well with
    multidimensional tensors, for which information is contained in the relationships
    between groups of adjacent values. They’re particularly well suited to working
    with image data.
  prefs: []
  type: TYPE_NORMAL
- en: Our person-detection model is a convolutional neural network trained on the
    [Visual Wake Words dataset](https://oreil.ly/EC6nd). This dataset consists of
    115,000 images, each one labeled with whether or not it contains a person.
  prefs: []
  type: TYPE_NORMAL
- en: The model is 250 KB, which is significantly larger than our speech model. As
    well as occupying more memory, this additional size means that it will take a
    lot longer to run a single inference.
  prefs: []
  type: TYPE_NORMAL
- en: The model accepts 96 × 96–pixel grayscale images as input. Each image is provided
    as a 3D tensor with shape `(96, 96, 1)`, where the final dimension contains an
    8-bit value that represents a single pixel. The value specifies the shade of the
    pixel, ranging from 0 (fully black) to 255 (fully white).
  prefs: []
  type: TYPE_NORMAL
- en: Our camera modules can return images in a variety of resolutions, so we need
    to ensure they are resized to 96 × 96 pixels. We also need to convert full-color
    images to grayscale so that they work with the model.
  prefs: []
  type: TYPE_NORMAL
- en: You might think 96 × 96 pixels sounds like a tiny resolution, but it will be
    more than sufficient to allow us to detect a person in each image. Models that
    work with images often accept surprisingly small resolutions. Increasing a model’s
    input size gives diminishing returns, and the complexity of the network increases
    greatly as the size of the input scales. For this reason, even state-of-the-art
    image classification models commonly work with a maximum of 320 × 320 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model outputs two probabilities: one indicating the probability that a
    person was present in the input, and another indicating the probability that there
    was nobody there. The probabilities range from 0 to 255.'
  prefs: []
  type: TYPE_NORMAL
- en: Our person detection model uses the *MobileNet* architecture, which is a well-known
    and battle-tested architecture designed for image classification on devices like
    mobile phones. In [Chapter 10](ch10.xhtml#chapter_person_detection_training),
    you will learn how this model was adapted to fit on microcontrollers and how you
    can train your own. For now, let’s continue exploring how our application works.
  prefs: []
  type: TYPE_NORMAL
- en: All the Moving Parts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 9-1](#application_architecture_2) shows the structure of our person
    detection application.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of the components of our person detection application](Images/timl_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. The components of our person detection application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we mentioned previously, this is a lot simpler than the wake-word application,
    because we can pass image data directly into the model—there’s no preprocessing
    required.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect that keeps things simple is that we don’t average the model’s
    output. Our wake-word model ran multiple times per second, so we had to average
    its output to get a stable result. Our person detection model is much larger,
    and it takes a lot longer to run inference. This means that there’s no need to
    average its output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code has five main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Main loop
  prefs: []
  type: TYPE_NORMAL
- en: Like the other examples, our application runs in a continuous loop. However,
    because our model is a lot larger and more complex, it will take longer to run
    inference. Depending on the device, we can expect one inference every few seconds
    rather than several inferences per second.
  prefs: []
  type: TYPE_NORMAL
- en: Image provider
  prefs: []
  type: TYPE_NORMAL
- en: This component captures image data from the camera and writes it to the input
    tensor. The methods for capturing images vary from device to device, so this component
    can be overridden and customized.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Lite interpreter
  prefs: []
  type: TYPE_NORMAL
- en: The interpreter runs the TensorFlow Lite model, transforming the input image
    into a set of probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs: []
  type: TYPE_NORMAL
- en: The model is included as a data array and run by the interpreter. At 250 KB,
    this model is unreasonably large to commit to the TensorFlow GitHub repository.
    Because of this, it is downloaded by the Makefile when the project is built. If
    you want to take a look, you can download it yourself at [*tf_lite_micro_person_data_grayscale.zip*](https://oreil.ly/Ylq9m).
  prefs: []
  type: TYPE_NORMAL
- en: Detection responder
  prefs: []
  type: TYPE_NORMAL
- en: The detection responder takes the probabilities output by the model and uses
    the device’s output capabilities to display them. We can override it for different
    device types. In our example code it will light an LED, but you can extend it
    to do pretty much anything.
  prefs: []
  type: TYPE_NORMAL
- en: To get a sense for how these parts fit together, we’ll take a look at their
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: Walking Through the Tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This application is nice and simple, since there are only a few tests to walk
    through. You can find them all in the [GitHub repository](https://oreil.ly/31vB5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[*person_detection_test.cc*](https://oreil.ly/r4ny8)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to run inference on an array representing a single image
  prefs: []
  type: TYPE_NORMAL
- en: '[*image_provider_test.cc*](https://oreil.ly/Js6M3)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to use the image provider to capture an image
  prefs: []
  type: TYPE_NORMAL
- en: '[*detection_responder_test.cc*](https://oreil.ly/KBVLF)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to use the detection responder to output the results of detection
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by exploring *person_detection_test.cc* to see how inference is
    run on image data. Because this is the third example we’ve walked through, this
    code should feel pretty familiar. You’re well on your way to being an embedded
    ML developer!
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First up, *person_detection_test.cc*. We begin by pulling in the ops that our
    model is going to need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a tensor arena that is appropriately sized for the model. As
    usual, this number was determined by trial and error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We then do the typical setup work, to get the interpreter ready to go, which
    includes registering the necessary ops using the `MicroMutableOpResolver`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Our next step is to inspect the input tensor. We check whether it has the expected
    number of dimensions and whether its dimensions are sized appropriately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: From this, we can see that the input is technically a 5D tensor. The first dimension
    is just a wrapper containing a single element. The subsequent two dimensions represent
    the rows and columns of the image’s pixels. The final dimension holds the number
    of color channels used to represent each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The constants that tell us the expected dimensions, `kNumRows`, `kNumCols`,
    and `kNumChannels`, are defined in [*model_settings.h*](https://oreil.ly/ae2OI).
    They look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model is expected to accept a 96 × 96–pixel bitmap. The
    image will be grayscale, with one color channel for each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next in the code, we copy a test image into the input tensor using a straightforward
    `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The variable that stores image data, `g_person_data`, is defined by *person_image_data.h*.
    To avoid adding more large files to the repository, the data itself is downloaded
    along with the model, as part of *tf_lite_micro_person_data_grayscale.zip*, when
    the tests are first run.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we’ve populated the input tensor, we run inference. It’s just as simple
    as ever:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We now check the output tensor to make sure it’s the expected size and shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The model’s output has four dimensions. The first three are just wrappers around
    the fourth, which contains one element for each category the model was trained
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The total number of categories is available as a constant, `kCategoryCount`,
    which resides in *model_settings.h* along with some other helpful values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As `kCategoryCount` shows, there are three categories in the output. The first
    happens to be an unused category, which we can ignore. The “person” category comes
    second, as we can see from its index, stored in the constant `kPersonIndex`. The
    “not a person” category comes third, with its index shown by `kNotAPersonIndex`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s also an array of category labels, `kCategoryLabels`, which is implemented
    in [*model_settings.cc*](https://oreil.ly/AB0zS):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next chunk of code logs the “person” and “no person” scores, and asserts
    that the “person” score is greater—as it should be given that we passed in an
    image of a person:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Since the only data content of the output tensor is the three `uint8` values
    representing class scores, with the first one being unused, we can access the
    scores directly by using `output->data.uint8[kPersonIndex]` and `output->data.uint8[kNotAPersonIndex]`.
    As `uint8` types, they have a minimum value of 0 and a maximum value of 255.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the “person” and “no person” scores are similar, it can signify that the
    model isn’t very confident of its prediction. In this case, you might choose to
    consider the result inconclusive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we test for an image without a person, held by `g_no_person_data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After inference has run, we then assert that the “not a person” score is higher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can observe, there’s nothing fancy going on here. We may be feeding in
    images instead of scalars or spectrograms, but the process of inference is similar
    to what we’ve seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the test is similarly straightforward. Just issue the following command
    from the root of the TensorFlow repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The first time the test is run, the model and image data will be downloaded.
    If you want to take a look at the downloaded files, you can find them in *tensorflow/lite/micro/tools/make/downloads/person_model_grayscale*.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we check out the interface for the image provider.
  prefs: []
  type: TYPE_NORMAL
- en: The Image Provider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The image provider is responsible for grabbing data from the camera and returning
    it in a format suitable for writing to the model’s input tensor. The file [*image_provider.h*](https://oreil.ly/5Vjbe)
    defines its interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Because its actual implementation is platform-specific, there’s a reference
    implementation in [*person_detection/image_provider.cc*](https://oreil.ly/QoQ3O)
    that returns dummy data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test in [*image_provider_test.cc*](https://oreil.ly/Nbl9x) calls this reference
    implementation to show how it is used. Our first order of business is to create
    an array to hold the image data. This happens in the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The constant `kMaxImageSize` comes from our old friend, [*model_settings.h*](https://oreil.ly/5naFK).
  prefs: []
  type: TYPE_NORMAL
- en: 'After we’ve set up this array, we can call the `GetImage()` function to capture
    an image from the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We call it with an `ErrorReporter` instance; the number of columns, rows, and
    channels that we want; and a pointer to our `image_data` array. The function will
    write the image data into this array. We can check the function’s return value
    to determine whether the capture process was successful; it will be set to `kTfLiteError`
    if there is a problem, or `kTfLiteOk` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the test walks through the returned data to show that all of the memory
    locations are readable. Even though the image technically has rows, columns, and
    channels, in practice the data is flattened into a 1D array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this test, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We’ll examine the device-specific implementations of *image_provider.cc* later
    in the chapter; for now, let’s take a look at the detection responder’s interface.
  prefs: []
  type: TYPE_NORMAL
- en: The Detection Responder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our final test shows how the detection responder is used. This is the code responsible
    for communicating the results of inference. Its interface is defined in [*detection_responder.h*](https://oreil.ly/cTptj),
    and the test is in [*detection_responder_test.cc*](https://oreil.ly/Igx7a).
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface is pretty simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We just call it with the scores for both the “person” and “not a person” categories,
    and it will decide what to do from there.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reference implementation in [*detection_responder.cc*](https://oreil.ly/5Wjjt)
    just logs these values. The test in *detection_responder_test.cc* calls the function
    a couple of times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the test and see the output, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We’ve explored all of the tests and the interfaces they exercise. Let’s now
    walk through the program itself.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting People
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application’s core functions reside in [*main_functions.cc*](https://oreil.ly/64oHW).
    They’re short and sweet, and we’ve seen much of their logic in the tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we pull in all of the ops that our model needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we declare a bunch of variables to hold the important moving parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we allocate some working memory for tensor operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `setup()` function, which is run before anything else happens, we create
    an error reporter, load our model, set up an interpreter instance, and grab a
    reference to the model’s input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part of the code is called continually in the program’s main loop.
    It first grabs an image using the image provider, passing a reference to the input
    tensor so that the image is written directly there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'It then runs inference, obtains the output tensor, and reads the “person” and
    “no person” scores from it. These scores are passed into the detection responder’s
    `RespondToDetection()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: After `RespondToDetection()` has finished outputting the results, the `loop()`
    function will return, ready to be called again by the program’s main loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loop itself is defined within the program’s `main()` function, which is
    located in [*main.cc*](https://oreil.ly/_PR3L). It calls the `setup()` function
    once and then calls the `loop()` function repeatedly and indefinitely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: And that’s the entire program! This example is great because it shows that working
    with sophisticated machine learning models can be surprisingly simple. The model
    contains all of the complexity, and we just need to feed it data.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move along, you can run the program locally to give it a try. The
    reference implementation of the image provider just returns dummy data, so you
    won’t get meaningful recognition results, but you’ll at least see the code at
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, use this command to build the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the build completes, you can run the example with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll see the program’s output scroll past until you press Ctrl-C to terminate
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we walk through the device-specific code that will capture
    camera images and output the results on each platform. We also show how to deploy
    and run this code.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Microcontrollers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we deploy our code to two familiar devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SparkFun Edge](https://oreil.ly/-hoL-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There’s one big difference this time around: because neither of these devices
    has a built-in camera, we recommend that you buy a camera module for whichever
    device you’re using. Each device has its own implementation of *image_provider.cc*,
    which interfaces with the camera module to capture images. There’s also device-specific
    output code in *detection_responder.cc*.'
  prefs: []
  type: TYPE_NORMAL
- en: This nice and simple, so it will make an excellent template to start from when
    you’re creating your own vision-based ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by exploring the Arduino implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Arduino
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an Arduino board, the Arduino Nano 33 BLE Sense has access to a massive ecosystem
    of compatible third-party hardware and libraries. We’re using a third-party camera
    module designed to work with Arduino, along with a couple of Arduino libraries
    that will interface with our camera module and make sense of the data it outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Which camera module to buy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This example uses the [Arducam Mini 2MP Plus](https://oreil.ly/LAwhb) camera
    module. It’s easy to connect to the Arduino Nano 33 BLE Sense, and it can be powered
    by the Arduino board’s power supply. It has a large lens and is capable of capturing
    high-quality 2-megapixel images—though we’ll be using its on-board image rescaling
    feature to obtain a smaller resolution. It’s not particularly power-efficient,
    but its high image quality makes it ideal for building image capture applications,
    like for recording wildlife.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing images on arduino
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We connect the Arducam module to the Arduino board via a number of pins. To
    obtain image data, we send a command from the Arduino board to the Arducam that
    instructs it to capture an image. The Arducam will do that, storing the image
    in its internal data buffer. We then send further commands that allow us to read
    the image data from the Arducam’s internal buffer and store it in the Arduino’s
    memory. To do all of this, we use the official Arducam library.
  prefs: []
  type: TYPE_NORMAL
- en: The Arducam camera module has a 2-megapixel image sensor, with a resolution
    of 1920 × 1080\. Our person detection model has an input size of only 96 × 96,
    so we don’t need all of that data. In fact, the Arduino itself doesn’t have enough
    memory to hold a 2-megapixel image, which would be several megabytes in size.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the Arducam hardware has the ability to resize its output to a
    much smaller resolution, 160 × 120 pixels. We can easily crop this down to 96
    × 96 in our code, by keeping only the central 96 × 96 pixels. However, to complicate
    matters, the Arducam’s resized output is encoded using [JPEG](https://oreil.ly/gwWDh),
    a common compression format for images. Our model requires an array of pixels,
    not a JPEG-encoded image, which means that we need to decode the Arducam’s output
    before we use it. We can do this using an open source library.
  prefs: []
  type: TYPE_NORMAL
- en: Our final task is to convert the Arducam’s color image output into grayscale,
    which is what our person-detection model expects. We’ll write the grayscale data
    into our model’s input tensor.
  prefs: []
  type: TYPE_NORMAL
- en: The image provider is implemented in [*arduino/image_provider.cc*](https://oreil.ly/kGx0-).
    We won’t explain its every detail, because the code is specific to the Arducam
    camera module. Instead, let’s step through what happens at a high level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `GetImage()` function is the image provider’s interface with the world.
    It’s called in our application’s main loop to obtain a frame of image data. The
    first time it is called, we need to initialize the camera. This happens with a
    call to the `InitCamera()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `InitCamera()` function is defined further up in *image_provider.cc*. We
    won’t walk through it here because it’s very device-specific, and if you want
    to use it in your own code you can just copy and paste it. It configures the Arduino’s
    hardware to communicate with the Arducam and then confirms that communication
    is working. Finally, it instructs the Arducam to output 160 × 120–pixel JPEG images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next function called by `GetImage()` is `PerformCapture()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We won’t go into the details of this one, either. All it does is send a command
    to the camera module, instructing it to capture an image and store the image data
    in its internal buffer. It then waits for confirmation that an image was captured.
    At this point, there’s image data waiting in the Arducam’s internal buffer, but
    there isn’t yet any image data on the Arduino itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next function we call is `ReadData()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `ReadData()` function uses more commands to fetch the image data from the
    Arducam. After the function has run, the global variable `jpeg_buffer` will be
    filled with the JPEG-encoded image data retrieved from the camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have the JPEG-encoded image, our next step is to decode it into raw
    image data. This happens in the `DecodeAndProcessImage()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The function uses a library named JPEGDecoder to decode the JPEG data and write
    it directly into the model’s input tensor. In the process, it crops the image,
    discarding some of the 160 × 120 data so that all that remains are 96 × 96 pixels,
    roughly at the center of the image. It also reduces the image’s 16-bit color representation
    down to 8-bit grayscale.
  prefs: []
  type: TYPE_NORMAL
- en: After the image has been captured and stored in the input tensor, we’re ready
    to run inference. Next, we show how the model’s output is displayed
  prefs: []
  type: TYPE_NORMAL
- en: Responding to detections on Arduino
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Arduino Nano 33 BLE Sense has a built-in RGB LED, which is a single component
    that contains distinct red, green, and blue LEDs that you can control separately.
    The detection responder’s implementation flashes the blue LED every time inference
    is run. When a person is detected, it lights the green LED; when a person is not
    detected, it lights the red LED.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation is in [*arduino/detection_responder.cc*](https://oreil.ly/-WsSN).
    Let’s take a quick walk through.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `RespondToDetection()` function accepts two scores, one for the “person”
    category and the other for “not a person.” The first time it is called, it sets
    up the blue, green, and yellow LEDs for output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, to indicate that an inference has just completed, we switch off all the
    LEDs and then flash the blue LED very briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You’ll notice that unlike with the Arduino’s built-in LED, these LEDs are switched
    on with `LOW` and off with `HIGH`. This is just a factor of how the LEDs are connected
    to the board.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we switch on and off the appropriate LEDs depending on which category
    score is higher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we use the `error_reporter` instance to output the scores to the serial
    port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'And that’s it! The core of the function is a basic `if` statement, and you
    could easily use similar logic to control other types of output. There’s something
    very exciting about such a complex visual input being transformed into a single
    Boolean output: “person” or “no person.”'
  prefs: []
  type: TYPE_NORMAL
- en: Running the example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Running this example is a little more complex than our other Arduino examples,
    because we need to connect the Arducam to the Arduino board. We also need to install
    and configure the libraries that interface with the Arducam and decode its JPEG
    output. But don’t worry, it’s still very easy!
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy this example, here’s what we’ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: An Arduino Nano 33 BLE Sense board
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Arducam Mini 2MP Plus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jumper cables (and optionally a breadboard)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A micro-USB cable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Arduino IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first task is to connect the Arducam to the Arduino using jumper cables.
    This isn’t an electronics book, so we won’t go into the details of using the cables.
    Instead, [Table 9-1](#arducam_pins) shows how the pins should be connected. The
    pins are labeled on each device.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-1\. Arducam Mini 2MP Plus to Arduino Nano 33 BLE Sense connections
  prefs: []
  type: TYPE_NORMAL
- en: '| Arducam pin | Arduino pin |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CS | D7 (unlabeled, immediately to the right of D6) |'
  prefs: []
  type: TYPE_TB
- en: '| MOSI | D11 |'
  prefs: []
  type: TYPE_TB
- en: '| MISO | D12 |'
  prefs: []
  type: TYPE_TB
- en: '| SCK | D13 |'
  prefs: []
  type: TYPE_TB
- en: '| GND | GND (either pin marked GND is fine) |'
  prefs: []
  type: TYPE_TB
- en: '| VCC | 3.3 V |'
  prefs: []
  type: TYPE_TB
- en: '| SDA | A4 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL | A5 |'
  prefs: []
  type: TYPE_TB
- en: After you’ve set up the hardware, you can continue with installing the software.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There’s always a chance that the build process might have changed since this
    book was written, so check [*README.md*](https://oreil.ly/CR5Pb) for the latest
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The projects in this book are available as example code in the TensorFlow Lite
    Arduino library. If you haven’t already installed the library, open the Arduino
    IDE and select Manage Libraries from the Tools menu. In the window that appears,
    search for and install the library named *Arduino_TensorFlowLite*. You should
    be able to use the latest version, but if you run into issues, the version that
    was tested with this book is 1.14-ALPHA.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also install the library from a *.zip* file, which you can either [download](https://oreil.ly/blgB8)
    from the TensorFlow Lite team or generate yourself using the TensorFlow Lite for
    Microcontrollers Makefile. If you’d prefer to do the latter, see [Appendix A](app01.xhtml#appendix_arduino_library_zip).
  prefs: []
  type: TYPE_NORMAL
- en: After you’ve installed the library, the `person_detection` example will show
    up in the File menu under Examples→Arduino_TensorFlowLite, as shown in [Figure 9-2](#arduino_examples_person_detection).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the ''Examples'' menu](Images/timl_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. The Examples menu
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Click “person_detection” to load the example. It will appear as a new window,
    with a tab for each of the source files. The file in the first tab, *person_detection*,
    is equivalent to the *main_functions.cc* we walked through earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[“Running the Example”](ch06.xhtml#hello_world_running_the_example) already
    explained the structure of the Arduino example, so we won’t cover it again here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the TensorFlow library, we need to install two other libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: The Arducam library, so our code can interface with the hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The JPEGDecoder library, so we can decode JPEG-encoded images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Arducam Arduino library is available from [GitHub](https://oreil.ly/93OKK).
    To install it, download or clone the repository. Next, copy its *ArduCAM* subdirectory
    into your *Arduino/libraries* directory. To find the *libraries* directory on
    your machine, check the Sketchbook location in the Arduino IDE’s Preferences window.
  prefs: []
  type: TYPE_NORMAL
- en: After downloading the library, you’ll need to edit one of its files to make
    sure it is configured for the Arducam Mini 2MP Plus. To do this, open *Arduino/libraries/ArduCAM/memorysaver.h*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see a bunch of `#define` statements listed. Make sure that they
    are all commented out except for `#define OV2640_MINI_2MP_PLUS`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: After you save the file, you’re done configuring the Arducam library.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The example was developed using commit #e216049 of the Arducam library. If
    you run into problems with the library, you can try downloading this specific
    commit to make sure you’re using the exact same code.'
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to install the JPEGDecoder library. You can do this from within
    the Arduino IDE. In the Tools menu, select the Manage Libraries option and search
    for JPEGDecoder. You should install version 1.8.0 of the library.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you’ve installed the library, you’ll need to configure it to disable
    some optional components that are not compatible with the Arduino Nano 33 BLE
    Sense. Open *Arduino/libraries/JPEGDecoder/src/User_Config.h* and make sure that
    both `#define LOAD_SD_LIBRARY` and `#define LOAD_SDFAT_LIBRARY` are commented
    out, as shown in this excerpt from the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: After you’ve saved the file, you’re done installing libraries. You’re now ready
    to run the person detection application!
  prefs: []
  type: TYPE_NORMAL
- en: To begin, plug in your Arduino device via USB. Make sure the correct device
    type is selected from the Board drop-down list in the Tools menu, as shown in
    [Figure 9-3](#arduino_board_dropdown_9).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the ''Board'' dropdown](Images/timl_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. The Board drop-down list
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your device’s name doesn’t appear in the list, you’ll need to install its
    support package. To do this, click Boards Manager. In the window that appears,
    search for your device and install the latest version of the corresponding support
    package.
  prefs: []
  type: TYPE_NORMAL
- en: Also in the Tools menu, make sure the device’s port is selected in the Port
    drop-down list, as demonstrated in [Figure 9-4](#arduino_port_dropdown_9).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the ''Port'' dropdown](Images/timl_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. The Port drop-down list
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, in the Arduino window, click the upload button (highlighted in white
    in [Figure 9-5](#arduino_upload_button_9)) to compile and upload the code to your
    Arduino device.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the upload button](Images/timl_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. The upload button
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As soon as the upload has successfully completed, the program will run.
  prefs: []
  type: TYPE_NORMAL
- en: To test it, start by pointing the device’s camera at something that is definitely
    not a person, or just covering up the lens. The next time the blue LED flashes,
    the device will capture a frame from the camera and begin to run inference. Because
    the vision model we are using for person detection is relatively large, this will
    take a long time inference—around 19 seconds at the time of writing, though it’s
    possible TensorFlow Lite has become faster since then.
  prefs: []
  type: TYPE_NORMAL
- en: When inference is complete, the result will be translated into another LED being
    lit. You pointed the camera at something that isn’t a person, so the red LED should
    illuminate.
  prefs: []
  type: TYPE_NORMAL
- en: Now, try pointing the device’s camera at yourself! The next time the blue LED
    flashes, the device will capture another image and begin to run inference. After
    roughly 19 seconds, the green LED should turn on.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, image data is captured as a snapshot before each inference, whenever
    the blue LED flashes. Whatever the camera is pointed at during that moment is
    what will be fed into the model. It doesn’t matter where the camera is pointed
    until the next time an image is captured, when the blue LED will flash again.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re getting seemingly incorrect results, make sure you are in an environment
    with good lighting. You should also make sure that the camera is oriented correctly,
    with the pins pointing downward, so that the images it captures are the right
    way up—the model was not trained to recognize upside-down people. In addition,
    it’s good to remember that this is a tiny model, which trades accuracy for small
    size. It works very well, but it isn’t accurate 100% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also see the results of inference via the Arduino Serial Monitor. To
    do this, from the Tools menu, open the Serial Monitor. You’ll see a detailed log
    showing what is happening while the application runs. It’s also interesting to
    check the “Show timestamp” box, so you can see how long each part of the process
    takes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: From this log, we can see that it took around 170 ms to capture and read the
    image data from the camera module, 180 ms to decode the JPEG and convert it to
    grayscale, and 18.6 seconds to run inference.
  prefs: []
  type: TYPE_NORMAL
- en: Making your own changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve deployed the basic application, try playing around and making
    some changes to the code. Just edit the files in the Arduino IDE and save, and
    then repeat the previous instructions to deploy your modified code to the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things you could try:'
  prefs: []
  type: TYPE_NORMAL
- en: Modify the detection responder so that it ignores ambiguous inputs, where there
    isn’t much difference between the “person” and “no person” scores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the results of person detection to control other components, like additional
    LEDs or servos.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a smart security camera, by storing or transmitting images—but only those
    that contain a person.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkFun Edge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SparkFun Edge board is optimized for low power consumption. When paired
    with a similarly efficient camera module, it’s the ideal platform for building
    vision applications that will be running on battery power. It’s easy to plug in
    a camera module via the board’s ribbon cable adapter.
  prefs: []
  type: TYPE_NORMAL
- en: Which camera module to buy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This example uses SparkFun’s [Himax HM01B0 breakout camera module](https://oreil.ly/H24xS).
    It’s based on a 320 × 320–pixel image sensor that consumes an extremely small
    amount of power: less than 2 mW when capturing at 30 frames per second (FPS).'
  prefs: []
  type: TYPE_NORMAL
- en: Capturing images on SparkFun Edge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To begin capturing images with the Himax HM01B0 camera module, we first must
    initialize the camera. After this is done, we can read a frame from the camera
    every time we need a new image. A frame is an array of bytes representing what
    the camera can currently see.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the camera will involve heavy use of both the Ambiq Apollo3 SDK,
    which is downloaded as part of the build process, and the HM01B0 driver, which
    is located in [*sparkfun_edge/himax_driver*](https://oreil.ly/OhBj0).
  prefs: []
  type: TYPE_NORMAL
- en: The image provider is implemented in [*sparkfun_edge/image_provider.cc*](https://oreil.ly/ZdU9N).
    We won’t explain its every detail, because the code is specific to the SparkFun
    board and the Himax camera module. Instead, let’s step through what happens at
    a high level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `GetImage()` function is the image provider’s interface with the world.
    It’s called in our application’s main loop to obtain a frame of image data. The
    first time it is called, we’ll need to initialize the camera. This happens with
    a call to the `InitCamera()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: If `InitCamera()` returns anything other than a `kTfLiteOk` status, we switch
    on the board’s red LED (using `am_hal_gpio_output_set(AM_BSP_GPIO_LED_RED)`) to
    indicate a problem. This is helpful for debugging.
  prefs: []
  type: TYPE_NORMAL
- en: The `InitCamera()` function is defined further up in *image_provider.cc*. We
    won’t walk through it here because it’s very device-specific, and if you want
    to use it in your own code you can just copy and paste it.
  prefs: []
  type: TYPE_NORMAL
- en: It calls a bunch of Apollo3 SDK functions to configure the microcontroller’s
    inputs and outputs so that it can communicate with the camera module. It also
    enables *interrupts*, which are the mechanism used by the camera to send over
    new image data. When this is all set up, it uses the camera driver to switch on
    the camera and configures it to start continually capturing images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The camera module has an autoexposure feature, which calibrates its exposure
    setting automatically as frames are captured. To allow it the opportunity to calibrate
    before we attempt to perform inference, the next part of the `GetImage()` function
    uses the camera driver’s `hm01b0_blocking_read_oneframe_scaled()` function to
    capture several frames. We don’t do anything with the captured data; we are only
    doing this to give the camera module’s autoexposure function some material to
    work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'After setup is out of the way, the rest of the `GetImage()` function is very
    simple. All we do is call `hm01b0_blocking_read_oneframe_scaled()` to capture
    an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: When `GetImage()` is called during the application’s main loop, the `frame`
    variable is a pointer to our input tensor, so the data is written directly by
    the camera driver to the area of memory allocated to the input tensor. We also
    specify the width, height, and number of channels we want.
  prefs: []
  type: TYPE_NORMAL
- en: With this implementation, we’re able to capture image data from our camera module.
    Next, let’s look at how we respond to the model’s output.
  prefs: []
  type: TYPE_NORMAL
- en: Responding to detections on SparkFun Edge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The detection responder’s implementation is very similar to our wake-word example’s
    command responder. It toggles the device’s blue LED every time inference is run.
    When a person is detected, it lights the green LED, and when a person is not detected
    it lights the yellow LED.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation is in [*sparkfun_edge/detection_responder.cc*](https://oreil.ly/OeN1M).
    Let’s take a quick walk through.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `RespondToDetection()` function accepts two scores, one for the “person”
    category, and the other for “not a person.” The first time it is called, it sets
    up the blue, green, and yellow LEDs for output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Because the function is called once per inference, the next snippet of code
    causes it to toggle the blue LED on and off each time inference is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it turns on the green LED if a person was detected, or the blue LED
    if not. It also logs the score using the `ErrorReporter` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'And that’s it! The core of the function is a basic `if` statement, and you
    could easily use similar logic could to control other types of output. There’s
    something very exciting about such a complex visual input being transformed into
    a single Boolean output: “person” or “no person.”'
  prefs: []
  type: TYPE_NORMAL
- en: Running the example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we’ve seen how the SparkFun Edge implementation works, let’s get it
    up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There’s always a chance that the build process might have changed since this
    book was written, so check [*README.md*](https://oreil.ly/kaSXN) for the latest
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build and deploy our code, we’ll need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A SparkFun Edge board with the [Himax HM01B0 breakout](https://oreil.ly/jNtyv)
    attached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A USB programmer (we recommend the SparkFun Serial Basic Breakout, which is
    available in both [micro-B USB](https://oreil.ly/wXo-f) and [USB-C](https://oreil.ly/-YvfN)
    variants)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A matching USB cable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3 and some dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re unsure whether you have the correct version of Python installed, [“Running
    the Example”](ch06.xhtml#running_hello_world_sparkfun_edge) has instructions on
    how to check.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a terminal, clone the TensorFlow repository and change into its directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’re going to build the binary and run some commands that get it ready
    for downloading to the device. To avoid some typing, you can copy and paste these
    commands from [*README.md*](https://oreil.ly/kaSXN).
  prefs: []
  type: TYPE_NORMAL
- en: Build the binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following command downloads all of the required dependencies and then compiles
    a binary for the SparkFun Edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The binary is created as a *.bin* file, in the following location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'To check that the file exists, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: When you run that command, you should see `Binary was successfully created`
    printed to the console.
  prefs: []
  type: TYPE_NORMAL
- en: If you see `Binary is missing`, there was a problem with the build process.
    If so, it’s likely that there are some clues to what went wrong in the output
    of the `make` command.
  prefs: []
  type: TYPE_NORMAL
- en: Sign the binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The binary must be signed with cryptographic keys to be deployed to the device.
    Let’s now run some commands that will sign the binary so that it can be flashed
    to the SparkFun Edge. The scripts used here come from the Ambiq SDK, which is
    downloaded when the Makefile is run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following command to set up some dummy cryptographic keys that you
    can use for development:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run the following command to create a signed binary. Substitute `python3`
    with `python` if necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the file *main_nonsecure_ota.bin*. Now run this command to create
    a final version of the file that you can use to flash your device with the script
    you will use in the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: You should now have a file called *main_nonsecure_wire.bin* in the directory
    where you ran the commands. This is the file you’ll be flashing to the device.
  prefs: []
  type: TYPE_NORMAL
- en: Flash the binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The SparkFun Edge stores the program it is currently running in its 1 megabyte
    of flash memory. If you want the board to run a new program, you need to send
    it to the board, which will store it in flash memory, overwriting any program
    that was previously saved.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve mentioned earlier in the book, this process is called *flashing*.
  prefs: []
  type: TYPE_NORMAL
- en: Attach the programmer to the board
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To download new programs to the board, you’ll use the SparkFun USB-C Serial
    Basic serial programmer. This device allows your computer to communicate with
    the microcontroller via USB.
  prefs: []
  type: TYPE_NORMAL
- en: 'To attach this device to your board, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: On the side of the SparkFun Edge, locate the six-pin header.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plug the SparkFun USB-C Serial Basic into these pins, ensuring that the pins
    labeled BLK and GRN on each device are lined up correctly, as demonstrated in
    [Figure 9-6](#sparkfun_edge_serial_basic_3).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A photo showing how the SparkFun Edge and USB-C Serial Basic should be connected](Images/timl_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. Connecting the SparkFun Edge and USB-C Serial Basic (courtesy of
    SparkFun)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Attach the programmer to your computer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You connect the board to your computer via USB. To program the board, you need
    to find out the name that your computer gives the device. The best way of doing
    this is to list all of the computer’s devices before and after attaching it and
    then look to see which device is new.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some people have reported issues with their operating system’s default drivers
    for the programmer, so we strongly recommend installing the [driver](https://oreil.ly/yI-NR)
    before you continue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before attaching the device via USB, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output a list of attached devices that looks something like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, connect the programmer to your computer’s USB port and run the following
    command again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an extra item in the output, as in the example that follows.
    Your new item might have a different name. This new item is the name of the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This name will be used to refer to the device. However, it can change depending
    on which USB port the programmer is attached to, so if you disconnect the board
    from the computer and then reattach it, you might have to look up its name again.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some users have reported two devices appearing in the list. If you see two devices,
    the correct one to use begins with the letters “wch”; for example, `/dev/wchusbserial-14410.`
  prefs: []
  type: TYPE_NORMAL
- en: 'After you’ve identified the device name, put it in a shell variable for later
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: This is a variable that you can use when running commands that require the device
    name, later in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Run the script to flash your board
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To flash the board, you need to put it into a special “bootloader” state that
    prepares it to receive the new binary. You’ll then run a script to send the binary
    to the board.
  prefs: []
  type: TYPE_NORMAL
- en: 'First create an environment variable to specify the baud rate, which is the
    speed at which data will be sent to the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Now paste the following command into your terminal—but *do not press Enter yet*!
    The `${DEVICENAME}` and `${BAUD_RATE}` in the command will be replaced with the
    values you set in the previous sections. Remember to substitute `python3` with
    `python` if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Next, you’ll reset the board into its bootloader state and flash the board.
    On the board, locate the buttons marked `RST` and `14`, as shown in [Figure 9-7](#sparkfun_edge_buttons_3).
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that your board is connected to the programmer, and the entire thing
    is connected to your computer via USB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the board, press and hold the button marked `14`. *Continue holding it.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While still holding the button marked `14`, press the button marked `RST` to
    reset the board.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press Enter on your computer to run the script. *Continue on holding button
    `14`.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A photo showing the SparkFun Edge''s buttons](Images/timl_0614.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. The SparkFun Edge’s buttons
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You should now see something like the following appearing on your screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Keep holding button `14` until you see `Sending Data Packet of length 8180`.
    You can release the button after seeing this (but it’s okay if you keep holding
    it).
  prefs: []
  type: TYPE_NORMAL
- en: 'The program will continue to print lines on the terminal. Eventually, you’ll
    see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: This indicates a successful flashing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the program output ends with an error, check whether `Sending Reset Command.`
    was printed. If so, flashing was likely successful despite the error. Otherwise,
    flashing might have failed. Try running through these steps again (you can skip
    over setting the environment variables).
  prefs: []
  type: TYPE_NORMAL
- en: Testing the program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Start by pressing the `RST` button, to make sure the program is running.
  prefs: []
  type: TYPE_NORMAL
- en: When the program is running the blue LED will toggle on and off, once for each
    inference. Because the vision model we are using for person detection is relatively
    large, it takes a long time to run inference—around 6 seconds in total.
  prefs: []
  type: TYPE_NORMAL
- en: Start by pointing the device’s camera at something that is definitely not a
    person, or just covering up the lens. The next time the blue LED toggles, the
    device will capture a frame from the camera and begin to run inference. After
    6 seconds or so, the inference result will be translated into another LED being
    lit. Given that you pointed the camera at something that isn’t a person, the orange
    LED should light up.
  prefs: []
  type: TYPE_NORMAL
- en: Now, try pointing the device’s camera at yourself. The next time the blue LED
    toggles, the device will capture another frame and begin to run inference. This
    time, the green LED should light up.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, image data is captured as a snapshot before each inference, whenever
    the blue LED toggles. Whatever the camera is pointed at during that moment is
    what will be fed into the model. It doesn’t matter where the camera is pointed
    until the next time a frame is captured, when the blue LED will toggle again.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re getting seemingly incorrect results, make sure that you are in an
    environment with good lighting. It’s also good to remember that this is a tiny
    model, which trades accuracy for small size. It works very well, but it isn’t
    accurate 100% all of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing debug data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The program will log detection results to the serial port. To view them, we
    can monitor the board’s serial port output using a baud rate of 115200\. On macOS
    and Linux, the following command should work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'You should initially see output that looks something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'As the board captures frames and runs inference, you should see it printing
    debug information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: To stop viewing the debug output with `screen`, press Ctrl-A, immediately followed
    by the K key, and then press the Y key.
  prefs: []
  type: TYPE_NORMAL
- en: Making your own changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve deployed the basic application, try playing around and making
    some changes. You can find the application’s code in the *tensorflow/lite/micro/examples/person_detection*
    folder. Just edit and save, and then repeat the preceding instructions to deploy
    your modified code to the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things you could try:'
  prefs: []
  type: TYPE_NORMAL
- en: Modify the detection responder so that it ignores ambiguous inputs, where there
    isn’t much difference between the “person” and “no person” scores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the results of person detection to control other components, like additional
    LEDs or servos.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a smart security camera, by storing or transmitting images—but only those
    that contain a person.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The vision model we’ve used in this chapter is an amazing thing. It accepts
    raw and messy input, no preprocessing required, and gives us a beautifully simple
    output: yes, a person is present, or no, there is no one present. This is the
    magic of machine learning: it can filter information from noise, leaving us with
    only the signals we care about. As developers, it’s easy to use these signals
    to build amazing experiences for our users.'
  prefs: []
  type: TYPE_NORMAL
- en: When building machine learning applications, it’s very common to use pretrained
    models like this one, which already contain the knowledge required to perform
    a task. Roughly equivalent to code libraries, models encapsulate specific functionality
    and are easily shared between projects. You’ll often find yourself exploring and
    evaluating models, looking for the proper fit for your task.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 10](ch10.xhtml#chapter_person_detection_training), we’ll examine
    how the person detection model works. You’ll also learn how to train your own
    vision models to spot different types of objects.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch09.xhtml#idm46473559664376-marker)) In a [2018 YouGov poll](https://oreil.ly/KvzGk),
    70% of respondents said that they would miss sight the most if they lost it.
  prefs: []
  type: TYPE_NORMAL
