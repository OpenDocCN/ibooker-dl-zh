- en: 7 Coding infrastructure and managing deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Dockerfile with the assistance of Copilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drafting your infrastructure as code using large language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing Docker images with a container registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harnessing the power of Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Releasing your code effortlessly using GitHub Actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is nothing more demoralizing than having an application sit unused. For
    this reason, fast-tracking a well-tested application to production is the stated
    goal of every competent developer. Because we spent the last chapter testing our
    product, it is now ready for launch.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will focus on that pivotal moment of transitioning from development
    to product launch. During this critical phase, understanding deployment strategies
    and best practices becomes essential to ensure a successful product launch.
  prefs: []
  type: TYPE_NORMAL
- en: With our application successfully secured and tested, it’s time to shift our
    attention toward launching the product. To this end, we will use the powerful
    capabilities of large language models (LLMs) to explore various deployment options
    tailored to cloud infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: By harnessing the power of LLMs and embracing their deployment options and methodologies,
    we can confidently navigate the complex landscape of launching our product, delivering
    a robust and scalable solution to our customers while using the benefits of cloud
    computing.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will develop deployment files for Docker. We will explore how to create
    Docker images and define deployment files. Additionally, we will discuss best
    practices for containerizing our application and achieving seamless deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will use Terraform to define our infrastructure as code and automate
    the deployment of Elastic Compute Cloud (EC2) instances on Amazon Web Services
    (AWS). We will demonstrate how to write Terraform scripts to provision and deploy
    our application on EC2 instances, ensuring consistent and reproducible infrastructure
    setups.
  prefs: []
  type: TYPE_NORMAL
- en: Then we will utilize LLMs to deploy our application onto Kubernetes (AWS Elastic
    Kubernetes Service [EKS]/Elastic Container Service [ECS]). We will have GitHub
    Copilot create the appropriate Kubernetes deployment files to streamline our deployment
    process and efficiently manage our application’s lifecycle. Given the relative
    simplicity of our application, we will not need a Kubernetes package manager like
    Helm. However, as the complexities and dependencies of services grow, you may
    want to explore it as one option. Fortunately, Copilot can write Helm charts for
    you as well!
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will briefly showcase migrating from local to automated deployments
    using GitHub actions. We can automate our build and deployment processes by integrating
    LLMs with this widespread continuous integration and deployment (CI/CD) tool,
    ensuring faster and more efficient deployments.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE This chapter uses AWS as our cloud provider, but the principles and practices
    covered in the chapter can be adapted and applied to other cloud platforms and
    even on-premises infrastructure without virtualization (bare metal), allowing
    us to adapt and scale your product deployment strategy as your business needs
    evolve. You will find that by employing LLMs and using infrastructure as code,
    you can (partially) mitigate the vendor lock-in that is very common to cloud platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you choose to deploy this (or any application) to AWS, there will
    be a cost associated with your activity. AWS and most cloud providers give you
    free trials to learn their platforms (Google Cloud Platform and Azure, for example),
    but once those credits have expired, you may get hit with a rather unexpectedly
    large bill. If you decide to follow along in this chapter, you need to set threshold
    alerts for an amount you can comfortably afford. Section 1.9 of Andreas Wittig
    and Michael Wittig’s *Amazon Web Services in Action, Third Edition* (Manning,
    2023; [www.manning.com/books/amazon-web-services-in-action-third-edition](https://www.manning.com/books/amazon-web-services-in-action-third-edition))
    is an excellent resource for setting up such a billing notification alert.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Building a Docker image and “deploying” it locally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you may remember from chapter 6, Docker is a containerization platform that
    allows you to run applications with little or no installation of an application
    (outside of Docker) in the traditional sense. Unlike a virtual machine, which
    simulates an entire operating system, a container shares the host system’s kernel
    (the core part of the operating system) and uses the host system’s operating system’s
    capabilities while isolating the application processes and file systems from the
    host. This lets you run multiple isolated applications on a single host system,
    each with its own environment and resource limits. Figure 7.1 gives you a sense
    of the relationship between the Docker runtime and the host.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F01_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 Docker makes use of the host’s operating system while isolating each
    of the containers. This makes Docker containers lightweight compared to virtual
    machines, as they do not require a full OS to run.
  prefs: []
  type: TYPE_NORMAL
- en: One of the more exciting features, from a production readiness perspective,
    is that Docker makes it easier to run applications that can self-heal in some
    sense. If they fail or fall over at runtime, you can configure them to restart
    without intervention. In this section, we will use Copilot to create the file
    (called a *Dockerfile*) from which we will build our *Docker image*.
  prefs: []
  type: TYPE_NORMAL
- en: Definition *Docker images* are like blueprints for Docker containers. They are
    portable, including all the dependencies (libraries, environment variables, code,
    etc.) required for the application to run.
  prefs: []
  type: TYPE_NORMAL
- en: Running Docker instances are called Docker *containers*. Given their lightweight
    nature, we can run multiple containers on a single host without a problem. We
    can do this because the containerization technology shares the OS kernel, operating
    in an isolated user space.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Originally, I wanted to use AWS CodeWhisperer as the LLM for this chapter.
    It seemed logical, given the intended cloud platform. However, at the time of
    this writing, AWS CodeWhisperer only supports programming in a programming language.
    It does not have facilities for infrastructure as code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin with the following prompt to have Copilot draft the Dockerfile
    for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You may be left with an empty file (other than this comment). Support for infrastructure
    as code is ever-evolving (not unlike the LLM ecosystem in general). According
    to Copilot Chat, GitHub Copilot is capable of creating a Dockerfile for you—but
    you have to goad it with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Dockerfile, type `FROM python:` and wait for Copilot to suggest a version
    of Python to use. Select the version you want to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `WORKDIR /app` to set the working directory for the container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `COPY . /app` to copy the contents of your project into the container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `RUN pip install --trusted-host pypi.python.org -r requirements.txt` to
    install the dependencies for your project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `EXPOSE 8080` to expose port 8080 for the container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `CMD ["python", "main.py"]` to specify the command to run when the container
    starts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alternatively, you may want to copy and paste the same prompt that you previously
    wrote into the Dockerfile into the Copilot Chat prompt window. Copilot Chat will
    give you the desired content for the Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.1 Dockerfile to build a Docker image
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With a Dockerfile, we will build an image for deploying and running our application.
    We can enter the following command to build our application (run from the directory
    where the Dockerfile lives, and do not forget the trailing dot). You will need
    internet access to download the dependencies and create the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Building a Docker image can run for a few seconds to a few minutes, depending
    on which images and packages are installed on your system and your internet connection
    speed. Your patience will be rewarded, as you will shortly have an application
    you can install nearly anywhere from the lowliest commodity hardware to the most
    oversized hardware offered by your favorite cloud provider. Before running it
    anywhere, however, you need to try to get it running locally. If you’ve forgotten
    the command, Copilot Chat will happily and helpfully assist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can confirm that your Docker container is running by issuing this command
    at the command line: `docker ps | grep itam`. You should see the running instance.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Standing up infrastructure by copiloting Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using a Docker image on your computer is useful when creating and testing an
    app. But when it comes time to launch your application, you need a machine with
    a little more heft than local computers. In this section, we’ll use GitHub Copilot
    to help us set up and control our AWS infrastructure by having Copilot write the
    requisite deployment descriptors for an infrastructure-as-code tool called Terraform.
    Terraform is made by HashiCorp and lets us write what we want our infrastructure
    to look like using a domain-specific language (DSL). This DSL saves us from having
    to understand all the complexities and intricacies that each cloud service provider
    uses to provision hardware. Additionally, it allows us to store and version our
    infrastructure using infrastructure as code.
  prefs: []
  type: TYPE_NORMAL
- en: To start, we want to create a file called ec2.tf and add the prompt to inform
    Copilot that we intend this to be a Terraform file and how we want our infrastructure
    stood up. Notice that Copilot needs us to enter the first word of a given line
    before it can be cajoled to continue.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.2 Example Terraform file, including instance size
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You may find that Copilot skips over one small but crucial detail: it does
    not provide code for installing and provisioning Docker. Given that Docker is
    required for running our application, we need to correct this oversight. In fact,
    you may need to update the file manually to include the command to install Docker,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Copilot should produce a complete Terraform file that resembles the following
    listing. Your code probably does not exactly match the listing, but that’s fine
    as long as it contains the key features: the provider, the instance, the script
    to add the Docker daemon, the key pair, and the security group.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.3 Terraform file to create the smallest EC2 instance available
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you are using the default Virtual Private Cloud (VPC), the `vpc_id` entry
    is not strictly necessary. You will find that many of the default configurations
    and conventions chosen by the AWS team make sense; if you have stricter security
    requirements, or if you know everything about your infrastructure and assume nothing,
    you might consider setting up a new VPC from scratch using Terraform. You need
    to change the key pair entry on line 21 to be a key pair to which you have access.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have completed this file satisfactorily, run the `terraform init` command.
    This command initializes a new or existing Terraform working directory. It downloads
    and installs the required provider plugins and modules specified in your configuration
    files and gets everything ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next you will have Terraform explain the changes that it intends to make. You
    do this with the `terraform plan` command. This command creates an execution plan
    for your infrastructure changes: it shows you what changes Terraform will make
    to your infrastructure when you apply your configuration files. The plan will
    show you which resources will be created, modified, or destroyed and any other
    changes that will be made to your infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE You may get an error when running `terraform plan` for the first time:
    “Error: configuring Terraform AWS Provider: no valid credential sources for Terraform
    AWS Provider found.” You get this error when Terraform attempts to connect to
    AWS but cannot supply AWS with proper credentials. To address this problem, you
    will need to create (or edit) the file called ~/.aws/credentials and add your
    ITAM AWS Access Key ID and AWS Secret Access Key credentials. You can find more
    details on how to accomplish this correctly in section 4.2.2, “Configuring the
    CLI,” of *Amazon Web Services in Action, Third Edition*.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to apply the Terraform changes, you use the `terraform apply` command.
    Terraform will then read the configuration files in the current directory and
    apply any changes to your infrastructure. If you have made any changes to your
    configuration files since the last time you ran `terraform apply`—for example,
    if you need to start up a new database instance or change the size of your EC2—Terraform
    will show you a preview of the changes that will be made and prompt you to confirm
    before applying the changes.
  prefs: []
  type: TYPE_NORMAL
- en: If you apply these changes, in a manner of minutes you will have a brand-new
    EC2 instance running in your VPC. However, this is only half of the equation.
    Having computing power at your fingertips is fantastic, but you need something
    to apply this power. In this case, we can use this EC2 instance to run our ISAM
    system. The following section briefly demonstrates transferring a locally built
    image to another machine.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Moving a Docker image around (the hard way)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First we will export a Docker image from our local machines and load it onto
    a remote machine. We will use the commands `docker save` and `load` to accomplish
    this. You can use the `docker save` command on your local machine to save the
    image to a tar archive. The following command will save the image to a tar archive
    named <image-name>.tar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, transfer the tar archive to the remote machine using a file transfer
    protocol such as Secure Copy Protocol (SCP) or Secure File Transfer Protocol (SFTP).
    You can use the `docker load` command on the remote machine to load the image
    from the tar archive: `docker load -i <image-name>.tar.` This will load the image
    into the local Docker image cache on the remote machine. Once the image has been
    loaded, use the `docker run` command to start the image and run the Docker container,
    as you did after you built it. Then add this image to your Docker compose file,
    in which you have the Postgres database and Kafka instances.'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE This discussion of Terraform is heavily abridged. When you are ready to
    get serious with Terraform, your go-to resource should be Scott Winkler’s *Terraform
    in Action* (Manning, 2021; [www.manning.com/books/terraform-in-action](https://www.manning.com/books/terraform-in-action)).
  prefs: []
  type: TYPE_NORMAL
- en: 'This section examined how to package up images and load them on remote hosts.
    This process is scriptable, but with the advent of container registries, it is
    now easier than ever to manage deployments without slinging them all around the
    internet. In the next section, we will explore one such tool: Amazon’s Elastic
    Container Registry (ECR).'
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Moving a Docker image around (the easy way)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker images, the blueprints for our containers, are a fundamental building
    block of containerized applications. Managing them correctly ensures that we maintain
    clean, efficient, and organized development and deployment workflows. Amazon ECR
    serves as a fully managed Docker container registry that makes it easy for developers
    to store, manage, and deploy Docker container images.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s dive into pushing Docker images to ECR. This process is vital to
    making your images accessible for use and deployment. We’ll walk through setting
    up your local environment, authenticating with ECR, and pushing your image. Before
    we can move an image to ECR, we must create a repository to house that image.
    This can be done from the AWS Management Console or, as we will do shortly, using
    the AWS command line interface (CLI). The command to create a new repository for
    an image is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next you need to tag your Docker image with the ECR repository URL and the
    image name. You may want to call it `latest` or use semantic versioning. Tagging
    will allow you to easily roll back or forward versions of your system. Tag your
    application image `latest` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, authenticate Docker to the ECR registry using the `aws ecr get-login-password`
    command. This will generate a Docker `login` command that you can use to authenticate
    Docker to the registry. The command to log in is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, push the Docker image to the ECR registry using the `docker push`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once the image is in your registry, your deployment options have greatly increased.
    You could, for example, write a bash script that will log on to the EC2 instance
    and perform a `docker pull` to download and run the image on that EC2\. Alternatively,
    you may want to adopt a more bulletproof deployment pattern. In the next section,
    we’re going to walk through the process of setting up and launching our application
    on a powerful cloud service called Elastic Kubernetes Service (EKS). EKS is a
    managed Kubernetes service provided by AWS. Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Deploying our application onto AWS Elastic Kubernetes Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes confers many benefits over simply running Docker images on EC2 instances.
    For one, managing and scaling our application becomes considerably more straightforward
    with Kubernetes. Also, with Kubernetes, we do not have to spend a lot of additional
    time thinking about what our infrastructure should look like. Plus, thanks to
    its automatic management of the lifecycles of its images, known as *pods*, our
    application will essentially be self-healing. This means if something goes wrong,
    Kubernetes can automatically fix it, keeping our application running smoothly
    at all times.
  prefs: []
  type: TYPE_NORMAL
- en: First we need a deployment descriptor written in YAML (Yet Another Markup Language
    or YAML Ain’t Markup Language, depending on who you ask), which will describe
    the state we want our ITAM system to be in at all times. This file (typically
    called deployment.yaml) will provide the template against which Kubernetes will
    compare the current running system, making corrections as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.4 Kubernetes deployment file for the ITAM system
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This will not work, however. Kubernetes will not be able to find the image that
    we reference in the deployment descriptor file. To correct this, we need to tell
    Kubernetes to use our newly minted ECR. Fortunately, this is not as challenging
    as it may sound. We just have to update the image entry in our file to point to
    the ECR image, as well as grant EKS permissions to access ECR (okay, maybe it
    is a little trickier, but it is manageable).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, update the deployment YAML to use the ECR image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Then you would need to define a policy for EKS to use and apply the policy using
    either the AWS CLI or the Identity and Access Management (IAM) Management Console.
    Although applying the policy is (slightly) outside of the scope of this book,
    you can use Copilot to define it. The resulting policy will resemble the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.5 IAM policy to allow EKS to pull images from ECR
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Once the EKS can pull down the image from ECR, you will see a pod start to run.
    However, you have no way to access this pod externally. You need to create a service.
    In Kubernetes, a *service* is an abstraction that defines a logical set of pods
    (the smallest and simplest unit in the Kubernetes object model that you create
    or deploy) and a policy to access them.
  prefs: []
  type: TYPE_NORMAL
- en: Services enable communication between different parts of an application and
    between different applications. They help distribute network traffic and load
    balance by exposing the pods to the network and other pods in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.6 Kubernetes services file to enable external access for our application
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes is responsible for routing all requests from this ingress through
    the service to the running pods, regardless of what host they are running on.
    This allows for seamless failover. Kubernetes expects things to fail. It banks
    on it. As a result, many of the best practices in distributed systems are baked
    into Kubernetes. Getting to Kube is a significant first step to having a reliable,
    highly available system. In the next section, we will examine how to ease the
    burden of getting our application onto Kubernetes repeatably and continuously.
    We will look at building out a small deployment pipeline using GitHub actions.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Setting up a continuous integration/continuous deployment pipeline in GitHub
    Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If releasing is hard, it will not be done often. This limits our ability to
    add value to the application and thus to our stakeholders. However, automating
    the deployment process significantly reduces the time to release. This allows
    for more frequent releases, accelerating the pace of development and enabling
    faster delivery of features to users. Continuous integration/continuous deployment
    (CI/CD) pipelines limit the risk associated with deployment. By making smaller,
    more frequent updates, any problems that arise can be isolated and fixed quickly,
    minimizing the potential effect on the end users. These pipelines facilitate seamless
    integration of code changes and expedite deployment, simplifying the software
    release process.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Actions allows us to construct customized CI/CD pipelines directly in
    our GitHub repositories. This makes the development workflow more efficient and
    enables the automation of various steps, freeing us to focus on coding rather
    than the logistics of integration and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: This section provides a concise introduction to setting up a CI/CD pipeline
    using GitHub Actions and GitHub Copilot. Note that this will not be a comprehensive
    guide but rather a survey that introduces the potential benefits and general workflow.
    This should serve as a primer, giving you an insight into how these tools can
    be used to optimize your software development process.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a file in your project in the path .github/workflows. Note the
    leading dot. You can call this file itam.yaml or whatever you desire. On the first
    line of this file, add the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: NOTE Like many of the infrastructure-related tasks that we have put to Copilot
    in this chapter, Copilot needs a lot of assistance in creating this file for us.
    We need to be aware of the structure of this file and how to begin every line.
    It makes sense in cases such as this one to ask ChatGPT or Copilot Chat to build
    the file for us.
  prefs: []
  type: TYPE_NORMAL
- en: The first part of this file outlines when this action should take place. The
    on:push instruction denotes that when a git push occurs to the main branch, this
    action should be executed. There is a single job in this file, one with several
    steps. This job “build” uses an embedded function `login-ecr` to log into our
    ECR.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.7 Beginning of GitHub Actions file to build our application
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The build job will first check out the code from our GitHub repository. It uses
    the code written in the module `actions/checkout` version 2\. Similarly, it will
    next grab the EKS CLI and configure the credentials to connect to EKS. Note that
    the AWS access key and secret are values that are automatically passed into the
    application. GitHub Actions uses a built-in secret management system to store
    sensitive data such as API keys, passwords, and certificates. This system is integrated
    into the GitHub platform and allows you to add, remove, or update secrets (and
    other sensitive data) at both the repository and organization levels. Secrets
    are encrypted before they’re stored and are not shown in logs or available for
    download. They’re only exposed as environment variables to the GitHub Actions
    runner, making it a secure way to handle sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, you can create environmental parameters and use them in your actions.
    For example, look at the variable `ECR_REGISTRY`. This variable is created using
    the output from the `login-ecr` function. In this case, you still need to hardcode
    the ECR in your Actions file. However, you should do this because of consistency
    and the need to manage it in only one place in the file. Most of these steps should
    seem familiar, as we have used them throughout the chapter. That is the magic
    of automation: it does it for you.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8 Build and deploy steps of our GitHub Actions file
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The final part of the file logs in to AWS ECR. The steps in the Actions file
    invoke this action. On completion, it returns the output to the calling function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9 A GitHub Actions file to build and deploy to EKS
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Exploring code-as-infrastructure has enabled us to understand its vital role
    in any project and how it can be better managed through code. Tools like Terraform
    provide streamlined solutions for managing infrastructure, and GitHub’s code-centric
    features aid in maintaining the overall workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing CI/CD pipelines, primarily through platforms like GitHub Actions,
    highlights the importance of automating the software delivery process. Automating
    such processes increases the speed and reliability of the software development
    life cycle and minimizes the chances of human errors.
  prefs: []
  type: TYPE_NORMAL
- en: The journey of managing infrastructure as code is ever-evolving, with new tools
    and practices emerging. It requires a constant learning and adaptation mindset.
    This chapter has given you a glimpse of the benefits and possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You learned about the transition from application development to product launch,
    covering deployment strategies, best practices for cloud infrastructure, and the
    use of Docker and Terraform for managing and containerizing applications efficiently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chapter explained how to manage application deployment via Kubernetes, including
    creating YAML deployment descriptors, forming services for network traffic distribution,
    and deploying on AWS’s Elastic Kubernetes Service (EKS).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You discovered how to adapt deployment methods to different environments, whether
    on various cloud platforms or on premises, and how GitHub Copilot can assist in
    creating Dockerfiles and Terraform files accurately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we explored the process of exporting Docker images to remote machines,
    pushing them to Amazon’s Elastic Container Registry (ECR), and migrating to automated
    deployments using GitHub Actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
