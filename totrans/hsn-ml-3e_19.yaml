- en: Chapter 17\. Autoencoders, GANs, and Diffusion Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第17章。自动编码器、GANs和扩散模型
- en: 'Autoencoders are artificial neural networks capable of learning dense representations
    of the input data, called *latent representations* or *codings*, without any supervision
    (i.e., the training set is unlabeled). These codings typically have a much lower
    dimensionality than the input data, making autoencoders useful for dimensionality
    reduction (see [Chapter 8](ch08.html#dimensionality_chapter)), especially for
    visualization purposes. Autoencoders also act as feature detectors, and they can
    be used for unsupervised pretraining of deep neural networks (as we discussed
    in [Chapter 11](ch11.html#deep_chapter)). Lastly, some autoencoders are *generative
    models*: they are capable of randomly generating new data that looks very similar
    to the training data. For example, you could train an autoencoder on pictures
    of faces, and it would then be able to generate new faces.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是人工神经网络，能够学习输入数据的密集表示，称为*潜在表示*或*编码*，而无需任何监督（即，训练集未标记）。这些编码通常比输入数据的维度低得多，使得自动编码器在降维方面非常有用（参见[第8章](ch08.html#dimensionality_chapter)），特别是用于可视化目的。自动编码器还充当特征检测器，并且可以用于深度神经网络的无监督预训练（正如我们在[第11章](ch11.html#deep_chapter)中讨论的那样）。最后，一些自动编码器是*生成模型*：它们能够随机生成看起来非常类似于训练数据的新数据。例如，您可以在人脸图片上训练一个自动编码器，然后它将能够生成新的人脸。
- en: '*Generative adversarial networks* (GANs) are also neural nets capable of generating
    data. In fact, they can generate pictures of faces so convincing that it is hard
    to believe the people they represent do not exist. You can judge so for yourself
    by visiting [*https://thispersondoesnotexist.com*](https://thispersondoesnotexist.com),
    a website that shows faces generated by a GAN architecture called *StyleGAN*.
    You can also check out [*https://thisrentaldoesnotexist.com*](https://thisrentaldoesnotexist.com)
    to see some generated Airbnb listings. GANs are now widely used for super resolution
    (increasing the resolution of an image), [colorization](https://github.com/jantic/DeOldify),
    powerful image editing (e.g., replacing photo bombers with realistic background),
    turning simple sketches into photorealistic images, predicting the next frames
    in a video, augmenting a dataset (to train other models), generating other types
    of data (such as text, audio, and time series), identifying the weaknesses in
    other models to strengthen them, and more.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成对抗网络*（GANs）也是能够生成数据的神经网络。事实上，它们可以生成如此逼真的人脸图片，以至于很难相信它们所代表的人并不存在。您可以通过访问[*https://thispersondoesnotexist.com*](https://thispersondoesnotexist.com)来亲自判断，这是一个展示由名为*StyleGAN*的GAN架构生成的人脸的网站。您还可以查看[*https://thisrentaldoesnotexist.com*](https://thisrentaldoesnotexist.com)来查看一些生成的Airbnb列表。GANs现在被广泛用于超分辨率（增加图像的分辨率）、[着色](https://github.com/jantic/DeOldify)、强大的图像编辑（例如，用逼真的背景替换照片炸弹客）、将简单的草图转换为逼真的图像、预测视频中的下一帧、增强数据集（用于训练其他模型）、生成其他类型的数据（如文本、音频和时间序列）、识别其他模型的弱点以加强它们等等。'
- en: A more recent addition to the generative learning party is *diffusion models*.
    In 2021, they managed to generate more diverse and higher-quality images than
    GANs, while also being much easier to train. However, diffusion models are much
    slower to run.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 生成学习领域的一个较新的成员是*扩散模型*。在2021年，它们成功生成了比GANs更多样化和高质量的图像，同时训练也更容易。然而，扩散模型运行速度较慢。
- en: 'Autoencoders, GANs, and diffusion models are all unsupervised, they all learn
    latent representations, they can all be used as generative models, and they have
    many similar applications. However, they work very differently:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器、GANs和扩散模型都是无监督的，它们都学习潜在表示，它们都可以用作生成模型，并且有许多类似的应用。然而，它们的工作方式非常不同：
- en: Autoencoders simply learn to copy their inputs to their outputs. This may sound
    like a trivial task, but as you will see, constraining the network in various
    ways can make it rather difficult. For example, you can limit the size of the
    latent representations, or you can add noise to the inputs and train the network
    to recover the original inputs. These constraints prevent the autoencoder from
    trivially copying the inputs directly to the outputs, which forces it to learn
    efficient ways of representing the data. In short, the codings are byproducts
    of the autoencoder learning the identity function under some constraints.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器简单地学习将输入复制到输出。这听起来可能是一个琐碎的任务，但正如你将看到的，以各种方式约束网络可能会使其变得相当困难。例如，您可以限制潜在表示的大小，或者您可以向输入添加噪声并训练网络以恢复原始输入。这些约束阻止了自动编码器直接将输入轻松复制到输出，迫使其学习表示数据的有效方式。简而言之，编码是自动编码器在某些约束下学习身份函数的副产品。
- en: 'GANs are composed of two neural networks: a *generator* that tries to generate
    data that looks similar to the training data, and a *discriminator* that tries
    to tell real data from fake data. This architecture is very original in deep learning
    in that the generator and the discriminator compete against each other during
    training: the generator is often compared to a criminal trying to make realistic
    counterfeit money, while the discriminator is like the police investigator trying
    to tell real money from fake. *Adversarial training* (training competing neural
    networks) is widely considered one of the most important innovations of the 2010s.
    In 2016, Yann LeCun even said that it was “the most interesting idea in the last
    10 years in machine learning”.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs由两个神经网络组成：一个*生成器*试图生成看起来类似于训练数据的数据，另一个*鉴别器*试图区分真实数据和假数据。这种架构在深度学习中非常独特，因为生成器和鉴别器在训练过程中相互竞争：生成器经常被比作试图制造逼真的假币的罪犯，而鉴别器则像是试图区分真假货币的警察调查员。*对抗训练*（训练竞争的神经网络）被广泛认为是2010年代最重要的创新之一。2016年，Yann
    LeCun甚至说这是“过去10年中机器学习中最有趣的想法”。
- en: A *denoising diffusion probabilistic model* (DDPM) is trained to remove a tiny
    bit of noise from an image. If you then take an image entirely full of Gaussian
    noise and repeatedly run the diffusion model on that image, a high-quality image
    will gradually emerge, similar to the training images (but not identical).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*去噪扩散概率模型*（DDPM）被训练用于从图像中去除一点噪音。如果你拿一张完全充满高斯噪音的图像，然后反复在该图像上运行扩散模型，一个高质量的图像将逐渐出现，类似于训练图像（但不完全相同）。'
- en: In this chapter we will start by exploring in more depth how autoencoders work
    and how to use them for dimensionality reduction, feature extraction, unsupervised
    pretraining, or as generative models. This will naturally lead us to GANs. We
    will build a simple GAN to generate fake images, but we will see that training
    is often quite difficult. We will discuss the main difficulties you will encounter
    with adversarial training, as well as some of the main techniques to work around
    these difficulties. And lastly, we will build and train a DDPM and use it to generate
    images. Let’s start with autoencoders!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨自动编码器的工作原理以及如何将其用于降维、特征提取、无监督预训练或生成模型。这将自然地引导我们到GAN。我们将构建一个简单的GAN来生成假图像，但我们会看到训练通常相当困难。我们将讨论对抗训练中遇到的主要困难，以及一些解决这些困难的主要技术。最后，我们将构建和训练一个DDPM，并用它生成图像。让我们从自动编码器开始！
- en: Efficient Data Representations
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效的数据表示
- en: Which of the following number sequences do you find the easiest to memorize?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你觉得以下哪个数字序列最容易记住？
- en: 40, 27, 25, 36, 81, 57, 10, 73, 19, 68
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 40, 27, 25, 36, 81, 57, 10, 73, 19, 68
- en: 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14
- en: At first glance, it would seem that the first sequence should be easier, since
    it is much shorter. However, if you look carefully at the second sequence, you
    will notice that it is just the list of even numbers from 50 down to 14\. Once
    you notice this pattern, the second sequence becomes much easier to memorize than
    the first because you only need to remember the pattern (i.e., decreasing even
    numbers) and the starting and ending numbers (i.e., 50 and 14). Note that if you
    could quickly and easily memorize very long sequences, you would not care much
    about the existence of a pattern in the second sequence. You would just learn
    every number by heart, and that would be that. The fact that it is hard to memorize
    long sequences is what makes it useful to recognize patterns, and hopefully this
    clarifies why constraining an autoencoder during training pushes it to discover
    and exploit patterns in the data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，第一个序列似乎更容易，因为它要短得多。然而，如果你仔细看第二个序列，你会注意到它只是从50到14的偶数列表。一旦你注意到这个模式，第二个序列比第一个容易记忆得多，因为你只需要记住模式（即递减的偶数）和起始和结束数字（即50和14）。请注意，如果你能快速轻松地记住非常长的序列，你就不会太在意第二个序列中的模式。你只需要把每个数字背下来，就这样。难以记忆长序列的事实使得识别模式变得有用，希望这解释清楚了为什么在训练期间对自动编码器进行约束会促使其发现和利用数据中的模式。
- en: The relationship between memory, perception, and pattern matching was famously
    studied by [William Chase and Herbert Simon](https://homl.info/111)⁠^([1](ch17.html#idm45720171598896))
    in the early 1970s. They observed that expert chess players were able to memorize
    the positions of all the pieces in a game by looking at the board for just five
    seconds, a task that most people would find impossible. However, this was only
    the case when the pieces were placed in realistic positions (from actual games),
    not when the pieces were placed randomly. Chess experts don’t have a much better
    memory than you and I; they just see chess patterns more easily, thanks to their
    experience with the game. Noticing patterns helps them store information efficiently.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆、感知和模式匹配之间的关系在20世纪70年代初由[威廉·查斯和赫伯特·西蒙](https://homl.info/111)著名研究。他们观察到，专业的国际象棋选手能够在只看棋盘五秒钟的情况下记住游戏中所有棋子的位置，这是大多数人会觉得不可能的任务。然而，这只有在棋子被放置在现实位置（来自实际游戏）时才是这样，而不是当棋子被随机放置时。国际象棋专家的记忆力并不比你我好多少；他们只是更容易看到国际象棋的模式，这要归功于他们对游戏的经验。注意到模式有助于他们有效地存储信息。
- en: 'Just like the chess players in this memory experiment, an autoencoder looks
    at the inputs, converts them to an efficient latent representation, and then spits
    out something that (hopefully) looks very close to the inputs. An autoencoder
    is always composed of two parts: an *encoder* (or *recognition network*) that
    converts the inputs to a latent representation, followed by a *decoder* (or *generative
    network*) that converts the internal representation to the outputs (see [Figure 17-1](#encoder_decoder_diagram)).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这个记忆实验中的国际象棋选手一样，自动编码器查看输入，将其转换为高效的潜在表示，然后输出与输入非常接近的内容（希望如此）。自动编码器始终由两部分组成：一个*编码器*（或*识别网络*），将输入转换为潜在表示，然后是一个*解码器*（或*生成网络*），将内部表示转换为输出（参见[图17-1](#encoder_decoder_diagram)）。
- en: '![mls3 1701](assets/mls3_1701.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1701](assets/mls3_1701.png)'
- en: Figure 17-1\. The chess memory experiment (left) and a simple autoencoder (right)
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-1。国际象棋记忆实验（左）和简单的自动编码器（右）
- en: As you can see, an autoencoder typically has the same architecture as a multilayer
    perceptron (MLP; see [Chapter 10](ch10.html#ann_chapter)), except that the number
    of neurons in the output layer must be equal to the number of inputs. In this
    example, there is just one hidden layer composed of two neurons (the encoder),
    and one output layer composed of three neurons (the decoder). The outputs are
    often called the *reconstructions* because the autoencoder tries to reconstruct
    the inputs. The cost function contains a *reconstruction loss* that penalizes
    the model when the reconstructions are different from the inputs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，自动编码器通常具有与多层感知器（MLP；参见[第10章](ch10.html#ann_chapter)）相同的架构，只是输出层中的神经元数量必须等于输入数量。在这个例子中，有一个由两个神经元组成的隐藏层（编码器），以及一个由三个神经元组成的输出层（解码器）。输出通常被称为*重构*，因为自动编码器试图重构输入。成本函数包含一个*重构损失*，当重构与输入不同时，惩罚模型。
- en: Because the internal representation has a lower dimensionality than the input
    data (it is 2D instead of 3D), the autoencoder is said to be *undercomplete*.
    An undercomplete autoencoder cannot trivially copy its inputs to the codings,
    yet it must find a way to output a copy of its inputs. It is forced to learn the
    most important features in the input data (and drop the unimportant ones).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因为内部表示的维度比输入数据低（是2D而不是3D），所以自动编码器被称为*欠完备*。欠完备自动编码器不能简单地将其输入复制到编码中，但它必须找到一种输出其输入的方式。它被迫学习输入数据中最重要的特征（并丢弃不重要的特征）。
- en: Let’s see how to implement a very simple undercomplete autoencoder for dimensionality
    reduction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现一个非常简单的欠完备自动编码器进行降维。
- en: Performing PCA with an Undercomplete Linear Autoencoder
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用欠完备线性自动编码器执行PCA
- en: If the autoencoder uses only linear activations and the cost function is the
    mean squared error (MSE), then it ends up performing principal component analysis
    (PCA; see [Chapter 8](ch08.html#dimensionality_chapter)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果自动编码器仅使用线性激活函数，并且成本函数是均方误差（MSE），那么它最终会执行主成分分析（PCA；参见[第8章](ch08.html#dimensionality_chapter)）。
- en: 'The following code builds a simple linear autoencoder to perform PCA on a 3D
    dataset, projecting it to 2D:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码构建了一个简单的线性自动编码器，用于在3D数据集上执行PCA，将其投影到2D：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code is really not very different from all the MLPs we built in past chapters,
    but there are a few things to note:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与我们在过去章节中构建的所有MLP并没有太大的不同，但有几点需要注意：
- en: 'We organized the autoencoder into two subcomponents: the encoder and the decoder.
    Both are regular `Sequential` models with a single `Dense` layer each, and the
    autoencoder is a `Sequential` model containing the encoder followed by the decoder
    (remember that a model can be used as a layer in another model).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将自动编码器组织成两个子组件：编码器和解码器。两者都是常规的`Sequential`模型，每个都有一个`Dense`层，自动编码器是一个包含编码器后面是解码器的`Sequential`模型（请记住，模型可以作为另一个模型中的一层使用）。
- en: The autoencoder’s number of outputs is equal to the number of inputs (i.e.,
    3).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器的输出数量等于输入数量（即3）。
- en: To perform PCA, we do not use any activation function (i.e., all neurons are
    linear), and the cost function is the MSE. That’s because PCA is a linear transformation.
    We will see more complex and nonlinear autoencoders shortly.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了执行PCA，我们不使用任何激活函数（即所有神经元都是线性的），成本函数是MSE。这是因为PCA是一种线性变换。很快我们将看到更复杂和非线性的自动编码器。
- en: 'Now let’s train the model on the same simple generated 3D dataset we used in
    [Chapter 8](ch08.html#dimensionality_chapter) and use it to encode that dataset
    (i.e., project it to 2D):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在与我们在[第8章](ch08.html#dimensionality_chapter)中使用的相同简单生成的3D数据集上训练模型，并使用它对该数据集进行编码（即将其投影到2D）：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that `X_train` is used as both the inputs and the targets. [Figure 17-2](#linear_autoencoder_pca_diagram)
    shows the original 3D dataset (on the left) and the output of the autoencoder’s
    hidden layer (i.e., the coding layer, on the right). As you can see, the autoencoder
    found the best 2D plane to project the data onto, preserving as much variance
    in the data as it could (just like PCA).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`X_train`既用作输入又用作目标。[图17-2](#linear_autoencoder_pca_diagram)显示了原始3D数据集（左侧）和自动编码器的隐藏层的输出（即编码层，右侧）。正如您所看到的，自动编码器找到了最佳的2D平面来投影数据，尽可能保留数据中的方差（就像PCA一样）。
- en: '![mls3 1702](assets/mls3_1702.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1702](assets/mls3_1702.png)'
- en: Figure 17-2\. Approximate PCA performed by an undercomplete linear autoencoder
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-2. 由欠完备线性自动编码器执行的近似PCA
- en: Note
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can think of an autoencoder as performing a form of self-supervised learning,
    since it is based on a supervised learning technique with automatically generated
    labels (in this case simply equal to the inputs).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将自动编码器视为执行一种自监督学习，因为它基于一种带有自动生成标签的监督学习技术（在本例中简单地等于输入）。
- en: Stacked Autoencoders
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠自动编码器
- en: Just like other neural networks we have discussed, autoencoders can have multiple
    hidden layers. In this case they are called *stacked autoencoders* (or *deep autoencoders*).
    Adding more layers helps the autoencoder learn more complex codings. That said,
    one must be careful not to make the autoencoder too powerful. Imagine an encoder
    so powerful that it just learns to map each input to a single arbitrary number
    (and the decoder learns the reverse mapping). Obviously such an autoencoder will
    reconstruct the training data perfectly, but it will not have learned any useful
    data representation in the process, and it is unlikely to generalize well to new
    instances.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们讨论过的其他神经网络一样，自动编码器可以有多个隐藏层。在这种情况下，它们被称为*堆叠自动编码器*（或*深度自动编码器*）。添加更多层有助于自动编码器学习更复杂的编码。也就是说，必须小心不要使自动编码器过于强大。想象一个如此强大的编码器，它只学习将每个输入映射到一个单一的任意数字（解码器学习反向映射）。显然，这样的自动编码器将完美地重构训练数据，但它不会在过程中学习任何有用的数据表示，并且不太可能很好地推广到新实例。
- en: The architecture of a stacked autoencoder is typically symmetrical with regard
    to the central hidden layer (the coding layer). To put it simply, it looks like
    a sandwich. For example, an autoencoder for Fashion MNIST (introduced in [Chapter 10](ch10.html#ann_chapter))
    may have 784 inputs, followed by a hidden layer with 100 neurons, then a central
    hidden layer of 30 neurons, then another hidden layer with 100 neurons, and an
    output layer with 784 neurons. This stacked autoencoder is represented in [Figure 17-3](#stacked_autoencoder_diagram).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠自编码器的架构通常关于中心隐藏层（编码层）是对称的。简单来说，它看起来像三明治。例如，时尚MNIST的自编码器（在[第10章](ch10.html#ann_chapter)介绍）可能有784个输入，然后是具有100个神经元的隐藏层，然后是具有30个神经元的中心隐藏层，然后是具有100个神经元的另一个隐藏层，最后是具有784个神经元的输出层。这个堆叠自编码器在[图17-3](#stacked_autoencoder_diagram)中表示。
- en: '![mls3 1703](assets/mls3_1703.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1703](assets/mls3_1703.png)'
- en: Figure 17-3\. Stacked autoencoder
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-3. 堆叠自编码器
- en: Implementing a Stacked Autoencoder Using Keras
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras实现堆叠自编码器
- en: 'You can implement a stacked autoencoder very much like a regular deep MLP:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以实现一个堆叠自编码器，非常类似于常规的深度MLP：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s go through this code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这段代码：
- en: 'Just like earlier, we split the autoencoder model into two submodels: the encoder
    and the decoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就像之前一样，我们将自编码器模型分成两个子模型：编码器和解码器。
- en: The encoder takes 28 × 28–pixel grayscale images, flattens them so that each
    image is represented as a vector of size 784, then processes these vectors through
    two `Dense` layers of diminishing sizes (100 units then 30 units), both using
    the ReLU activation function. For each input image, the encoder outputs a vector
    of size 30.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器接收28×28像素的灰度图像，将它们展平，使每个图像表示为大小为784的向量，然后通过两个逐渐减小的“密集”层（100个单元，然后是30个单元）处理这些向量，都使用ReLU激活函数。对于每个输入图像，编码器输出大小为30的向量。
- en: The decoder takes codings of size 30 (output by the encoder) and processes them
    through two `Dense` layers of increasing sizes (100 units then 784 units), and
    it reshapes the final vectors into 28 × 28 arrays so the decoder’s outputs have
    the same shape as the encoder’s inputs.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器接收大小为30的编码（由编码器输出），并通过两个逐渐增大的“密集”层（100个单元，然后是784个单元）处理它们，并将最终向量重新整形为28×28的数组，以便解码器的输出具有与编码器输入相同的形状。
- en: When compiling the stacked autoencoder, we use the MSE loss and Nadam optimization.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编译堆叠自编码器时，我们使用MSE损失和Nadam优化。
- en: Finally, we train the model using `X_train` as both the inputs and the targets.
    Similarly, we use `X_valid` as both the validation inputs and targets.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用`X_train`作为输入和目标来训练模型。同样，我们使用`X_valid`作为验证输入和目标。
- en: Visualizing the Reconstructions
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化重构
- en: 'One way to ensure that an autoencoder is properly trained is to compare the
    inputs and the outputs: the differences should not be too significant. Let’s plot
    a few images from the validation set, as well as their reconstructions:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 确保自编码器得到正确训练的一种方法是比较输入和输出：差异不应太大。让我们绘制一些验证集中的图像，以及它们的重构：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Figure 17-4](#reconstruction_plot) shows the resulting images.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-4](#reconstruction_plot)显示了生成的图像。'
- en: '![mls3 1704](assets/mls3_1704.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1704](assets/mls3_1704.png)'
- en: Figure 17-4\. Original images (top) and their reconstructions (bottom)
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-4. 原始图像（顶部）及其重构（底部）
- en: The reconstructions are recognizable, but a bit too lossy. We may need to train
    the model for longer, or make the encoder and decoder deeper, or make the codings
    larger. But if we make the network too powerful, it will manage to make perfect
    reconstructions without having learned any useful patterns in the data. For now,
    let’s go with this model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 重构是可以识别的，但有点丢失。我们可能需要训练模型更长时间，或者使编码器和解码器更深，或者使编码更大。但是，如果我们使网络过于强大，它将能够进行完美的重构，而不必学习数据中的任何有用模式。现在，让我们使用这个模型。
- en: Visualizing the Fashion MNIST Dataset
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化时尚MNIST数据集
- en: 'Now that we have trained a stacked autoencoder, we can use it to reduce the
    dataset’s dimensionality. For visualization, this does not give great results
    compared to other dimensionality reduction algorithms (such as those we discussed
    in [Chapter 8](ch08.html#dimensionality_chapter)), but one big advantage of autoencoders
    is that they can handle large datasets with many instances and many features.
    So, one strategy is to use an autoencoder to reduce the dimensionality down to
    a reasonable level, then use another dimensionality reduction algorithm for visualization.
    Let’s use this strategy to visualize Fashion MNIST. First we’ll use the encoder
    from our stacked autoencoder to reduce the dimensionality down to 30, then we’ll
    use Scikit-Learn’s implementation of the t-SNE algorithm to reduce the dimensionality
    down to 2 for visualization:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一个堆叠自编码器，我们可以使用它来降低数据集的维度。对于可视化，与其他降维算法（如我们在[第8章](ch08.html#dimensionality_chapter)中讨论的算法）相比，这并不会产生很好的结果，但自编码器的一个重要优势是它们可以处理具有许多实例和许多特征的大型数据集。因此，一种策略是使用自编码器将维度降低到合理水平，然后使用另一个降维算法进行可视化。让我们使用这种策略来可视化时尚MNIST。首先，我们将使用堆叠自编码器的编码器将维度降低到30，然后我们将使用Scikit-Learn的t-SNE算法实现将维度降低到2以进行可视化：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we can plot the dataset:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制数据集：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Figure 17-5](#fashion_mnist_visualization_plot) shows the resulting scatterplot,
    beautified a bit by displaying some of the images. The t-SNE algorithm identified
    several clusters that match the classes reasonably well (each class is represented
    by a different color).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-5](#fashion_mnist_visualization_plot)显示了生成的散点图，通过显示一些图像进行美化。t-SNE算法识别出几个与类别相匹配的簇（每个类别由不同的颜色表示）。'
- en: '![mls3 1705](assets/mls3_1705.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1705](assets/mls3_1705.png)'
- en: Figure 17-5\. Fashion MNIST visualization using an autoencoder followed by t-SNE
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-5. 使用自编码器后跟t-SNE的时尚MNIST可视化
- en: So, autoencoders can be used for dimensionality reduction. Another application
    is for unsupervised pretraining.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，自编码器可以用于降维。另一个应用是无监督的预训练。
- en: Unsupervised Pretraining Using Stacked Autoencoders
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用堆叠自编码器进行无监督预训练
- en: As we discussed in [Chapter 11](ch11.html#deep_chapter), if you are tackling
    a complex supervised task but you do not have a lot of labeled training data,
    one solution is to find a neural network that performs a similar task and reuse
    its lower layers. This makes it possible to train a high-performance model using
    little training data because your neural network won’t have to learn all the low-level
    features; it will just reuse the feature detectors learned by the existing network.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第11章](ch11.html#deep_chapter)中讨论的，如果你正在处理一个复杂的监督任务，但没有太多标记的训练数据，一个解决方案是找到一个执行类似任务的神经网络，并重复使用其较低层。这样可以使用少量训练数据训练高性能模型，因为你的神经网络不需要学习所有低级特征；它只需重复使用现有网络学习的特征检测器。
- en: Similarly, if you have a large dataset but most of it is unlabeled, you can
    first train a stacked autoencoder using all the data, then reuse the lower layers
    to create a neural network for your actual task and train it using the labeled
    data. For example, [Figure 17-6](#unsupervised_pretraining_autoencoders_diagram)
    shows how to use a stacked autoencoder to perform unsupervised pretraining for
    a classification neural network. When training the classifier, if you really don’t
    have much labeled training data, you may want to freeze the pretrained layers
    (at least the lower ones).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果你有一个大型数据集，但其中大部分是未标记的，你可以首先使用所有数据训练一个堆叠自动编码器，然后重复使用较低层来创建一个用于实际任务的神经网络，并使用标记数据进行训练。例如，[图17-6](#unsupervised_pretraining_autoencoders_diagram)展示了如何使用堆叠自动编码器为分类神经网络执行无监督预训练。在训练分类器时，如果你确实没有太多标记的训练数据，可能需要冻结预训练层（至少是较低的层）。
- en: '![mls3 1706](assets/mls3_1706.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1706](assets/mls3_1706.png)'
- en: Figure 17-6\. Unsupervised pretraining using autoencoders
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-6。使用自动编码器进行无监督预训练
- en: Note
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Having plenty of unlabeled data and little labeled data is common. Building
    a large unlabeled dataset is often cheap (e.g., a simple script can download millions
    of images off the internet), but labeling those images (e.g., classifying them
    as cute or not) can usually be done reliably only by humans. Labeling instances
    is time-consuming and costly, so it’s normal to have only a few thousand human-labeled
    instances, or even less.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有大量未标记数据和少量标记数据是很常见的。构建一个大型未标记数据集通常很便宜（例如，一个简单的脚本可以从互联网上下载数百万张图片），但标记这些图片（例如，将它们分类为可爱或不可爱）通常只能由人类可靠地完成。标记实例是耗时且昂贵的，因此通常只有少量人类标记的实例，甚至更少。
- en: 'There is nothing special about the implementation: just train an autoencoder
    using all the training data (labeled plus unlabeled), then reuse its encoder layers
    to create a new neural network (see the exercises at the end of this chapter for
    an example).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实现上没有什么特别之处：只需使用所有训练数据（标记加未标记）训练一个自动编码器，然后重复使用其编码器层来创建一个新的神经网络（请参考本章末尾的练习示例）。
- en: Next, let’s look at a few techniques for training stacked autoencoders.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一些训练堆叠自动编码器的技术。
- en: Tying Weights
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绑定权重
- en: When an autoencoder is neatly symmetrical, like the one we just built, a common
    technique is to *tie* the weights of the decoder layers to the weights of the
    encoder layers. This halves the number of weights in the model, speeding up training
    and limiting the risk of overfitting. Specifically, if the autoencoder has a total
    of *N* layers (not counting the input layer), and **W**[*L*] represents the connection
    weights of the *L*^(th) layer (e.g., layer 1 is the first hidden layer, layer
    *N*/2 is the coding layer, and layer *N* is the output layer), then the decoder
    layer weights can be defined as **W**[*L*] = **W**[*N*–*L*+1]^⊺ (with *L* = *N*
    / 2 + 1, …​, *N*).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个自动编码器是整齐对称的，就像我们刚刚构建的那样，一个常见的技术是将解码器层的权重与编码器层的权重*绑定*在一起。这样可以减半模型中的权重数量，加快训练速度并限制过拟合的风险。具体来说，如果自动编码器总共有*N*层（不包括输入层），而**W**[*L*]表示第*L*层的连接权重（例如，第1层是第一个隐藏层，第*N*/2层是编码层，第*N*层是输出层），那么解码器层的权重可以定义为**W**[*L*]
    = **W**[*N*–*L*+1]^⊺（其中*L* = *N* / 2 + 1, …​, *N*）。
- en: 'To tie weights between layers using Keras, let’s define a custom layer:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras在层之间绑定权重，让我们定义一个自定义层：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This custom layer acts like a regular `Dense` layer, but it uses another `Dense`
    layer’s weights, transposed (setting `transpose_b=True` is equivalent to transposing
    the second argument, but it’s more efficient as it performs the transposition
    on the fly within the `matmul()` operation). However, it uses its own bias vector.
    Now we can build a new stacked autoencoder, much like the previous one but with
    the decoder’s `Dense` layers tied to the encoder’s `Dense` layers:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个自定义层就像一个常规的`Dense`层，但它使用另一个`Dense`层的权重，经过转置（设置`transpose_b=True`等同于转置第二个参数，但更高效，因为它在`matmul()`操作中实时执行转置）。然而，它使用自己的偏置向量。现在我们可以构建一个新的堆叠自动编码器，与之前的模型类似，但解码器的`Dense`层与编码器的`Dense`层绑定：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This model achieves roughly the same reconstruction error as the previous model,
    using almost half the number of parameters.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型实现了与之前模型大致相同的重构误差，使用了几乎一半的参数数量。
- en: Training One Autoencoder at a Time
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一次训练一个自动编码器
- en: Rather than training the whole stacked autoencoder in one go like we just did,
    it is possible to train one shallow autoencoder at a time, then stack all of them
    into a single stacked autoencoder (hence the name), as shown in [Figure 17-7](#stacking_autoencoders_diagram).
    This technique is not used so much these days, but you may still run into papers
    that talk about “greedy layerwise training”, so it’s good to know what it means.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们刚刚做的整个堆叠自动编码器一次性训练不同，可以一次训练一个浅层自动编码器，然后将它们堆叠成一个单一的堆叠自动编码器（因此得名），如[图17-7](#stacking_autoencoders_diagram)所示。这种技术现在不太常用，但你可能仍然会遇到一些论文讨论“贪婪逐层训练”，所以了解其含义是很有必要的。
- en: '![mls3 1707](assets/mls3_1707.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1707](assets/mls3_1707.png)'
- en: Figure 17-7\. Training one autoencoder at a time
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-7。一次训练一个自动编码器
- en: During the first phase of training, the first autoencoder learns to reconstruct
    the inputs. Then we encode the whole training set using this first autoencoder,
    and this gives us a new (compressed) training set. We then train a second autoencoder
    on this new dataset. This is the second phase of training. Finally, we build a
    big sandwich using all these autoencoders, as shown in [Figure 17-7](#stacking_autoencoders_diagram)
    (i.e., we first stack the hidden layers of each autoencoder, then the output layers
    in reverse order). This gives us the final stacked autoencoder (see the “Training
    One Autoencoder at a Time” section in the chapter’s notebook for an implementation).
    We could easily train more autoencoders this way, building a very deep stacked
    autoencoder.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的第一阶段，第一个自动编码器学习重建输入。然后我们使用这个第一个自动编码器对整个训练集进行编码，这给我们一个新的（压缩的）训练集。然后我们在这个新数据集上训练第二个自动编码器。这是训练的第二阶段。最后，我们构建一个大的三明治，使用所有这些自动编码器，如[图17-7](#stacking_autoencoders_diagram)所示（即，我们首先堆叠每个自动编码器的隐藏层，然后反向堆叠输出层）。这给我们最终的堆叠自动编码器（请参阅本章笔记本中“逐个训练自动编码器”部分以获取实现）。通过这种方式，我们可以轻松训练更多的自动编码器，构建一个非常深的堆叠自动编码器。
- en: As I mentioned earlier, one of the triggers of the deep learning tsunami was
    the discovery in 2006 by [Geoffrey Hinton et al.](https://homl.info/136) that
    deep neural networks can be pretrained in an unsupervised fashion, using this
    greedy layerwise approach. They used restricted Boltzmann machines (RBMs; see
    [*https://homl.info/extra-anns*](https://homl.info/extra-anns)) for this purpose,
    but in 2007 [Yoshua Bengio et al.](https://homl.info/112)⁠^([2](ch17.html#idm45720170477280))
    showed that autoencoders worked just as well. For several years this was the only
    efficient way to train deep nets, until many of the techniques introduced in [Chapter 11](ch11.html#deep_chapter)
    made it possible to just train a deep net in one shot.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，深度学习浪潮的一个触发因素是[Geoffrey Hinton等人](https://homl.info/136)在2006年发现深度神经网络可以通过无监督的方式进行预训练，使用这种贪婪的逐层方法。他们用受限玻尔兹曼机（RBMs；参见[*https://homl.info/extra-anns*](https://homl.info/extra-anns)）来实现这一目的，但在2007年[Yoshua
    Bengio等人](https://homl.info/112)⁠^([2](ch17.html#idm45720170477280))表明自动编码器同样有效。几年来，这是训练深度网络的唯一有效方式，直到[第11章](ch11.html#deep_chapter)中引入的许多技术使得可以一次性训练深度网络。
- en: 'Autoencoders are not limited to dense networks: you can also build convolutional
    autoencoders. Let’s look at these now.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器不仅限于密集网络：你也可以构建卷积自动编码器。现在让我们来看看这些。
- en: Convolutional Autoencoders
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积自动编码器
- en: 'If you are dealing with images, then the autoencoders we have seen so far will
    not work well (unless the images are very small): as you saw in [Chapter 14](ch14.html#cnn_chapter),
    convolutional neural networks are far better suited than dense networks to working
    with images. So if you want to build an autoencoder for images (e.g., for unsupervised
    pretraining or dimensionality reduction), you will need to build a [*convolutional
    autoencoder*](https://homl.info/convae).⁠^([3](ch17.html#idm45720170465088)) The
    encoder is a regular CNN composed of convolutional layers and pooling layers.
    It typically reduces the spatial dimensionality of the inputs (i.e., height and
    width) while increasing the depth (i.e., the number of feature maps). The decoder
    must do the reverse (upscale the image and reduce its depth back to the original
    dimensions), and for this you can use transpose convolutional layers (alternatively,
    you could combine upsampling layers with convolutional layers). Here is a basic
    convolutional autoencoder for Fashion MNIST:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处理的是图像，那么迄今为止我们看到的自动编码器效果不佳（除非图像非常小）：正如你在[第14章](ch14.html#cnn_chapter)中看到的，卷积神经网络比密集网络更适合处理图像。因此，如果你想为图像构建一个自动编码器（例如用于无监督预训练或降维），你将需要构建一个[*卷积自动编码器*](https://homl.info/convae)。⁠^([3](ch17.html#idm45720170465088))
    编码器是由卷积层和池化层组成的常规CNN。它通常减少输入的空间维度（即高度和宽度），同时增加深度（即特征图的数量）。解码器必须执行相反操作（放大图像并将其深度降至原始维度），为此你可以使用转置卷积层（或者，你可以将上采样层与卷积层结合）。以下是Fashion
    MNIST的基本卷积自动编码器：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It’s also possible to create autoencoders with other architecture types, such
    as RNNs (see the notebook for an example).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用其他架构类型创建自动编码器，例如RNNs（请参阅笔记本中的示例）。
- en: 'OK, let’s step back for a second. So far we have looked at various kinds of
    autoencoders (basic, stacked, and convolutional), and how to train them (either
    in one shot or layer by layer). We also looked at a couple of applications: data
    visualization and unsupervised pretraining.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们退后一步。到目前为止，我们已经看过各种类型的自动编码器（基本、堆叠和卷积），以及如何训练它们（一次性或逐层）。我们还看过一些应用：数据可视化和无监督预训练。
- en: 'Up to now, in order to force the autoencoder to learn interesting features,
    we have limited the size of the coding layer, making it undercomplete. There are
    actually many other kinds of constraints that can be used, including ones that
    allow the coding layer to be just as large as the inputs, or even larger, resulting
    in an *overcomplete autoencoder*. Then, in the following sections we’ll look at
    a few more kinds of autoencoders: denoising autoencoders, sparse autoencoders,
    and variational autoencoders.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，为了强迫自动编码器学习有趣的特征，我们限制了编码层的大小，使其欠完备。实际上还有许多其他类型的约束可以使用，包括允许编码层与输入一样大，甚至更大，从而产生*过完备自动编码器*。接下来，我们将看一些其他类型的自动编码器：去噪自动编码器、稀疏自动编码器和变分自动编码器。
- en: Denoising Autoencoders
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自动编码器
- en: Another way to force the autoencoder to learn useful features is to add noise
    to its inputs, training it to recover the original, noise-free inputs. This idea
    has been around since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s
    thesis). In a [2008 paper](https://homl.info/113),⁠^([4](ch17.html#idm45720170108336))
    Pascal Vincent et al. showed that autoencoders could also be used for feature
    extraction. In a [2010 paper](https://homl.info/114),⁠^([5](ch17.html#idm45720170106320))
    Vincent et al. introduced *stacked denoising autoencoders*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种强制自动编码器学习有用特征的方法是向其输入添加噪声，训练它恢复原始的无噪声输入。这个想法自上世纪80年代就存在了（例如，Yann LeCun在1987年的硕士论文中提到了这一点）。在[2008年的一篇论文](https://homl.info/113)中，Pascal
    Vincent等人表明自动编码器也可以用于特征提取。在[2010年的一篇论文](https://homl.info/114)中，Vincent等人介绍了*堆叠去噪自动编码器*。
- en: The noise can be pure Gaussian noise added to the inputs, or it can be randomly
    switched-off inputs, just like in dropout (introduced in [Chapter 11](ch11.html#deep_chapter)).
    [Figure 17-8](#denoising_autoencoders_diagram) shows both options.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声可以是添加到输入的纯高斯噪声，也可以是随机关闭的输入，就像dropout中一样（在[第11章](ch11.html#deep_chapter)中介绍）。[图17-8](#denoising_autoencoders_diagram)展示了这两种选项。
- en: 'The implementation is straightforward: it is a regular stacked autoencoder
    with an additional `Dropout` layer applied to the encoder’s inputs (or you could
    use a `GaussianNoise` layer instead). Recall that the `Dropout` layer is only
    active during training (and so is the `GaussianNoise` layer):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 实现很简单：这是一个常规的堆叠自动编码器，附加了一个`Dropout`层应用于编码器的输入（或者您可以使用一个`GaussianNoise`层）。请记住，`Dropout`层仅在训练期间激活（`GaussianNoise`层也是如此）：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![mls3 1708](assets/mls3_1708.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1708](assets/mls3_1708.png)'
- en: Figure 17-8\. Denoising autoencoders, with Gaussian noise (left) or dropout
    (right)
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-8。去噪自动编码器，带有高斯噪声（左）或dropout（右）
- en: '[Figure 17-9](#dropout_denoising_plot) shows a few noisy images (with half
    the pixels turned off), and the images reconstructed by the dropout-based denoising
    autoencoder. Notice how the autoencoder guesses details that are actually not
    in the input, such as the top of the white shirt (bottom row, fourth image). As
    you can see, not only can denoising autoencoders be used for data visualization
    or unsupervised pretraining, like the other autoencoders we’ve discussed so far,
    but they can also be used quite simply and efficiently to remove noise from images.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-9](#dropout_denoising_plot)显示了一些嘈杂的图像（一半像素关闭），以及基于dropout的去噪自动编码器重建的图像。请注意，自动编码器猜测了实际输入中不存在的细节，例如白色衬衫的顶部（底部行，第四幅图）。正如您所看到的，去噪自动编码器不仅可以用于数据可视化或无监督预训练，就像我们迄今讨论过的其他自动编码器一样，而且还可以非常简单高效地从图像中去除噪声。'
- en: '![mls3 1709](assets/mls3_1709.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1709](assets/mls3_1709.png)'
- en: Figure 17-9\. Noisy images (top) and their reconstructions (bottom)
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-9。嘈杂的图像（顶部）及其重建（底部）
- en: Sparse Autoencoders
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏自动编码器
- en: 'Another kind of constraint that often leads to good feature extraction is *sparsity*:
    by adding an appropriate term to the cost function, the autoencoder is pushed
    to reduce the number of active neurons in the coding layer. For example, it may
    be pushed to have on average only 5% significantly active neurons in the coding
    layer. This forces the autoencoder to represent each input as a combination of
    a small number of activations. As a result, each neuron in the coding layer typically
    ends up representing a useful feature (if you could speak only a few words per
    month, you would probably try to make them worth listening to).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种通常导致良好特征提取的约束是*稀疏性*：通过向成本函数添加适当的项，自动编码器被推动减少编码层中活跃神经元的数量。例如，它可能被推动使编码层中平均只有5%的显著活跃神经元。这迫使自动编码器将每个输入表示为少量激活的组合。结果，编码层中的每个神经元通常最终代表一个有用的特征（如果您每个月只能说几个词，您可能会尽量使它们值得倾听）。
- en: 'A simple approach is to use the sigmoid activation function in the coding layer
    (to constrain the codings to values between 0 and 1), use a large coding layer
    (e.g., with 300 units), and add some ℓ[1] regularization to the coding layer’s
    activations. The decoder is just a regular decoder:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的方法是在编码层中使用sigmoid激活函数（将编码限制在0到1之间），使用一个大的编码层（例如，具有300个单元），并向编码层的激活添加一些ℓ[1]正则化。解码器只是一个常规的解码器：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This `ActivityRegularization` layer just returns its inputs, but as a side effect
    it adds a training loss equal to the sum of the absolute values of its inputs.
    This only affects training. Equivalently, you could remove the `ActivityRegularization`
    layer and set `activity_regularizer=tf.keras.regularizers.l1(1e-4)` in the previous
    layer. This penalty will encourage the neural network to produce codings close
    to 0, but since it will also be penalized if it does not reconstruct the inputs
    correctly, it will have to output at least a few nonzero values. Using the ℓ[1]
    norm rather than the ℓ[2] norm will push the neural network to preserve the most
    important codings while eliminating the ones that are not needed for the input
    image (rather than just reducing all codings).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`ActivityRegularization`层只是返回其输入，但作为副作用，它会添加一个训练损失，等于其输入的绝对值之和。这只影响训练。同样，您可以删除`ActivityRegularization`层，并在前一层中设置`activity_regularizer=tf.keras.regularizers.l1(1e-4)`。这种惩罚将鼓励神经网络生成接近0的编码，但由于如果不能正确重建输入也会受到惩罚，因此它必须输出至少几个非零值。使用ℓ[1]范数而不是ℓ[2]范数将推动神经网络保留最重要的编码，同时消除不需要的编码（而不仅仅是减少所有编码）。
- en: Another approach, which often yields better results, is to measure the actual
    sparsity of the coding layer at each training iteration, and penalize the model
    when the measured sparsity differs from a target sparsity. We do so by computing
    the average activation of each neuron in the coding layer, over the whole training
    batch. The batch size must not be too small, or else the mean will not be accurate.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法，通常会产生更好的结果，是在每次训练迭代中测量编码层的实际稀疏度，并在测量到的稀疏度与目标稀疏度不同时对模型进行惩罚。我们通过计算编码层中每个神经元的平均激活值来实现这一点，整个训练批次上。批次大小不能太小，否则平均值将不准确。
- en: Once we have the mean activation per neuron, we want to penalize the neurons
    that are too active, or not active enough, by adding a *sparsity loss* to the
    cost function. For example, if we measure that a neuron has an average activation
    of 0.3, but the target sparsity is 0.1, it must be penalized to activate less.
    One approach could be simply adding the squared error (0.3 – 0.1)² to the cost
    function, but in practice a better approach is to use the Kullback–Leibler (KL)
    divergence (briefly discussed in [Chapter 4](ch04.html#linear_models_chapter)),
    which has much stronger gradients than the mean squared error, as you can see
    in [Figure 17-10](#sparsity_loss_plot).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了每个神经元的平均激活，我们希望通过向成本函数添加*稀疏损失*来惩罚那些激活过多或不足的神经元。例如，如果我们测量到一个神经元的平均激活为0.3，但目标稀疏度为0.1，那么它必须受到惩罚以减少激活。一种方法可能是简单地将平方误差(0.3
    - 0.1)²添加到成本函数中，但实际上更好的方法是使用Kullback–Leibler (KL)散度（在[第4章](ch04.html#linear_models_chapter)中简要讨论），它比均方误差具有更强的梯度，如您可以在[图17-10](#sparsity_loss_plot)中看到的那样。
- en: '![mls3 1710](assets/mls3_1710.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1710](assets/mls3_1710.png)'
- en: Figure 17-10\. Sparsity loss
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-10. 稀疏损失
- en: Given two discrete probability distributions *P* and *Q*, the KL divergence
    between these distributions, noted *D*[KL](*P* ∥ *Q*), can be computed using [Equation
    17-1](#kl_divergence_equation).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个离散概率分布*P*和*Q*，这些分布之间的KL散度，记为*D*[KL](*P* ∥ *Q*)，可以使用[方程17-1](#kl_divergence_equation)计算。
- en: Equation 17-1\. Kullback–Leibler divergence
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程17-1. Kullback–Leibler散度
- en: <math display="block"><mrow><msub><mi>D</mi> <mi>KL</mi></msub> <mrow><mo>(</mo>
    <mi>P</mi> <mo>∥</mo> <mi>Q</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo>
    <mi>i</mi></munder> <mi>P</mi> <mrow><mo>(</mo> <mi>i</mi> <mo>)</mo></mrow> <mo
    form="prefix">log</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>P</mi><mo>(</mo><mi>i</mi><mo>)</mo></mrow>
    <mrow><mi>Q</mi><mo>(</mo><mi>i</mi><mo>)</mo></mrow></mfrac></mstyle></mrow></math>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>D</mi> <mi>KL</mi></msub> <mrow><mo>(</mo>
    <mi>P</mi> <mo>∥</mo> <mi>Q</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo>
    <mi>i</mi></munder> <mi>P</mi> <mrow><mo>(</mo> <mi>i</mi> <mo>)</mo></mrow> <mo
    form="prefix">log</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>P</mi><mo>(</mo><mi>i</mi><mo>)</mo></mrow>
    <mrow><mi>Q</mi><mo>(</mo><mi>i</mi><mo>)</mo></mrow></mfrac></mstyle></mrow></math>
- en: In our case, we want to measure the divergence between the target probability
    *p* that a neuron in the coding layer will activate and the actual probability
    *q*, estimated by measuring the mean activation over the training batch. So, the
    KL divergence simplifies to [Equation 17-2](#kl_divergence_equation_simplified).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们想要衡量编码层中神经元激活的目标概率*p*和通过测量整个训练批次上的平均激活来估计的实际概率*q*之间的差异。因此，KL散度简化为[方程17-2](#kl_divergence_equation_simplified)。
- en: Equation 17-2\. KL divergence between the target sparsity *p* and the actual
    sparsity *q*
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程17-2. 目标稀疏度*p*和实际稀疏度*q*之间的KL散度
- en: <math display="block"><mrow><msub><mi>D</mi> <mi>KL</mi></msub> <mrow><mo>(</mo>
    <mi>p</mi> <mo>∥</mo> <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mo form="prefix">log</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>p</mi> <mi>q</mi></mfrac></mstyle>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow>
    <mo form="prefix">log</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow>
    <mrow><mn>1</mn><mo>-</mo><mi>q</mi></mrow></mfrac></mstyle></mrow></math>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>D</mi> <mi>KL</mi></msub> <mrow><mo>(</mo>
    <mi>p</mi> <mo>∥</mo> <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mo form="prefix">log</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>p</mi> <mi>q</mi></mfrac></mstyle>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow>
    <mo form="prefix">log</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow>
    <mrow><mn>1</mn><mo>-</mo><mi>q</mi></mrow></mfrac></mstyle></mrow></math>
- en: Once we have computed the sparsity loss for each neuron in the coding layer,
    we sum up these losses and add the result to the cost function. In order to control
    the relative importance of the sparsity loss and the reconstruction loss, we can
    multiply the sparsity loss by a sparsity weight hyperparameter. If this weight
    is too high, the model will stick closely to the target sparsity, but it may not
    reconstruct the inputs properly, making the model useless. Conversely, if it is
    too low, the model will mostly ignore the sparsity objective and will not learn
    any interesting features.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算了编码层中每个神经元的稀疏损失，我们将这些损失相加并将结果添加到成本函数中。为了控制稀疏损失和重构损失的相对重要性，我们可以将稀疏损失乘以一个稀疏权重超参数。如果这个权重太高，模型将严格遵循目标稀疏度，但可能无法正确重构输入，使模型无用。相反，如果权重太低，模型将主要忽略稀疏目标，并且不会学习任何有趣的特征。
- en: 'We now have all we need to implement a sparse autoencoder based on the KL divergence.
    First, let’s create a custom regularizer to apply KL divergence regularization:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有需要基于KL散度实现稀疏自动编码器的东西。首先，让我们创建一个自定义正则化器来应用KL散度正则化：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we can build the sparse autoencoder, using the `KLDivergenceRegularizer`
    for the coding layer’s activations:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建稀疏自动编码器，使用`KLDivergenceRegularizer`来对编码层的激活进行正则化：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: After training this sparse autoencoder on Fashion MNIST, the coding layer will
    have roughly 10% sparsity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fashion MNIST上训练这个稀疏自动编码器后，编码层的稀疏度大约为10%。
- en: Variational Autoencoders
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自动编码器
- en: 'An important category of autoencoders was introduced in 2013 by [Diederik Kingma
    and Max Welling](https://homl.info/115)⁠^([6](ch17.html#idm45720169388400)) and
    quickly became one of the most popular variants: *variational autoencoders* (VAEs).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 2013年，[Diederik Kingma和Max Welling](https://homl.info/115)⁠^([6](ch17.html#idm45720169388400))引入了一个重要类别的自动编码器，并迅速成为最受欢迎的变体之一：*变分自动编码器*（VAEs）。
- en: 'VAEs are quite different from all the autoencoders we have discussed so far,
    in these particular ways:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs在这些特定方面与我们迄今讨论过的所有自动编码器都有很大不同：
- en: They are *probabilistic autoencoders*, meaning that their outputs are partly
    determined by chance, even after training (as opposed to denoising autoencoders,
    which use randomness only during training).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是*概率自动编码器*，这意味着它们的输出在训练后部分地由机会决定（与去噪自动编码器相反，在训练期间仅使用随机性）。
- en: Most importantly, they are *generative autoencoders*, meaning that they can
    generate new instances that look like they were sampled from the training set.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要的是，它们是*生成自动编码器*，这意味着它们可以生成看起来像是从训练集中采样的新实例。
- en: Both these properties make VAEs rather similar to RBMs, but they are easier
    to train, and the sampling process is much faster (with RBMs you need to wait
    for the network to stabilize into a “thermal equilibrium” before you can sample
    a new instance). As their name suggests, variational autoencoders perform variational
    Bayesian inference, which is an efficient way of carrying out approximate Bayesian
    inference. Recall that Bayesian inference means updating a probability distribution
    based on new data, using equations derived from Bayes’ theorem. The original distribution
    is called the *prior*, while the updated distribution is called the *posterior*.
    In our case, we want to find a good approximation of the data distribution. Once
    we have that, we can sample from it.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个特性使得VAEs与RBM相当相似，但它们更容易训练，采样过程也更快（对于RBM，您需要等待网络稳定到“热平衡”状态，然后才能对新实例进行采样）。正如它们的名字所暗示的，变分自动编码器执行变分贝叶斯推断，这是进行近似贝叶斯推断的有效方法。回想一下，贝叶斯推断意味着根据新数据更新概率分布，使用从贝叶斯定理推导出的方程。原始分布称为*先验*，而更新后的分布称为*后验*。在我们的情况下，我们想要找到数据分布的一个很好的近似。一旦我们有了这个，我们就可以从中进行采样。
- en: 'Let’s take a look at how VAEs work. [Figure 17-11](#variational_autoencoders_diagram)
    (left) shows a variational autoencoder. You can recognize the basic structure
    of all autoencoders, with an encoder followed by a decoder (in this example, they
    both have two hidden layers), but there is a twist: instead of directly producing
    a coding for a given input, the encoder produces a *mean coding* **μ** and a standard
    deviation **σ**. The actual coding is then sampled randomly from a Gaussian distribution
    with mean **μ** and standard deviation **σ**. After that the decoder decodes the
    sampled coding normally. The right part of the diagram shows a training instance
    going through this autoencoder. First, the encoder produces **μ** and **σ**, then
    a coding is sampled randomly (notice that it is not exactly located at **μ**),
    and finally this coding is decoded; the final output resembles the training instance.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看VAEs是如何工作的。[图17-11](#variational_autoencoders_diagram)（左）展示了一个变分自动编码器。您可以认出所有自动编码器的基本结构，具有一个编码器后面跟着一个解码器（在这个例子中，它们都有两个隐藏层），但有一个转折：编码器不是直接为给定输入产生编码，而是产生一个*均值编码*
    **μ** 和一个标准差 **σ**。然后，实际编码随机地从均值 **μ** 和标准差 **σ** 的高斯分布中采样。之后解码器正常解码采样的编码。图的右侧显示了一个训练实例通过这个自动编码器的过程。首先，编码器产生
    **μ** 和 **σ**，然后一个编码被随机采样（请注意它并不完全位于 **μ**），最后这个编码被解码；最终输出类似于训练实例。
- en: '![mls3 1711](assets/mls3_1711.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1711](assets/mls3_1711.png)'
- en: Figure 17-11\. A variational autoencoder (left) and an instance going through
    it (right)
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-11. 变分自动编码器（左）和通过它的实例（右）
- en: 'As you can see in the diagram, although the inputs may have a very convoluted
    distribution, a variational autoencoder tends to produce codings that look as
    though they were sampled from a simple Gaussian distribution:⁠^([7](ch17.html#idm45720169367824))
    during training, the cost function (discussed next) pushes the codings to gradually
    migrate within the coding space (also called the *latent space*) to end up looking
    like a cloud of Gaussian points. One great consequence is that after training
    a variational autoencoder, you can very easily generate a new instance: just sample
    a random coding from the Gaussian distribution, decode it, and voilà!'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在图中所看到的，尽管输入可能具有非常复杂的分布，但变分自动编码器倾向于产生看起来像是从简单的高斯分布中采样的编码：⁠^([7](ch17.html#idm45720169367824))在训练期间，成本函数（下面讨论）推动编码逐渐在编码空间（也称为*潜在空间*）内迁移，最终看起来像一个高斯点云。一个很大的结果是，在训练变分自动编码器之后，您可以非常容易地生成一个新实例：只需从高斯分布中随机采样一个随机编码，解码它，然后就完成了！
- en: 'Now, let’s look at the cost function. It is composed of two parts. The first
    is the usual reconstruction loss that pushes the autoencoder to reproduce its
    inputs. We can use the MSE for this, as we did earlier. The second is the *latent
    loss* that pushes the autoencoder to have codings that look as though they were
    sampled from a simple Gaussian distribution: it is the KL divergence between the
    target distribution (i.e., the Gaussian distribution) and the actual distribution
    of the codings. The math is a bit more complex than with the sparse autoencoder,
    in particular because of the Gaussian noise, which limits the amount of information
    that can be transmitted to the coding layer. This pushes the autoencoder to learn
    useful features. Luckily, the equations simplify, so the latent loss can be computed
    using [Equation 17-3](#var_ae_latent_loss_equation).⁠^([8](ch17.html#idm45720169359232))'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下成本函数。它由两部分组成。第一部分是通常的重构损失，推动自动编码器重现其输入。我们可以使用MSE来实现这一点，就像我们之前做的那样。第二部分是*潜在损失*，推动自动编码器具有看起来像是从简单高斯分布中抽样的编码：这是目标分布（即高斯分布）与编码的实际分布之间的KL散度。数学比稀疏自动编码器更复杂，特别是由于高斯噪声，它限制了可以传输到编码层的信息量。这推动自动编码器学习有用的特征。幸运的是，方程简化了，因此可以使用[Equation
    17-3](#var_ae_latent_loss_equation)计算潜在损失。⁠^([8](ch17.html#idm45720169359232))
- en: Equation 17-3\. Variational autoencoder’s latent loss
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程17-3。变分自动编码器的潜在损失
- en: <math display="block"><mo mathvariant="script">L</mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="[" close="]"><mrow><mn>1</mn><mo>+</mo><mi>log</mi><mo>(</mo><msup><msub><mi>σ</mi><mi>i</mi></msub><mn>2</mn></msup><mo>)</mo><mo>-</mo><msup><msub><mi>σ</mi><mi>i</mi></msub><mn>2</mn></msup><mo>-</mo><msup><msub><mi>μ</mi><mi>i</mi></msub><mn>2</mn></msup></mrow></mfenced></math>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mo mathvariant="script">L</mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="[" close="]"><mrow><mn>1</mn><mo>+</mo><mi>log</mi><mo>(</mo><msup><msub><mi>σ</mi><mi>i</mi></msub><mn>2</mn></msup><mo>)</mo><mo>-</mo><msup><msub><mi>σ</mi><mi>i</mi></msub><mn>2</mn></msup><mo>-</mo><msup><msub><mi>μ</mi><mi>i</mi></msub><mn>2</mn></msup></mrow></mfenced></math>
- en: In this equation, ℒ is the latent loss, *n* is the codings’ dimensionality,
    and *μ*[i] and *σ*[i] are the mean and standard deviation of the *i*^(th) component
    of the codings. The vectors **μ** and **σ** (which contain all the *μ*[i] and
    *σ*[i]) are output by the encoder, as shown in [Figure 17-11](#variational_autoencoders_diagram)
    (left).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，ℒ是潜在损失，*n*是编码的维度，*μ*[i]和*σ*[i]是编码的第*i*个分量的均值和标准差。向量**μ**和**σ**（包含所有*μ*[i]和*σ*[i]）由编码器输出，如[Figure
    17-11](#variational_autoencoders_diagram)（左）所示。
- en: A common tweak to the variational autoencoder’s architecture is to make the
    encoder output **γ** = log(**σ**²) rather than **σ**. The latent loss can then
    be computed as shown in [Equation 17-4](#var_ae_latent_loss_equation_2). This
    approach is more numerically stable and speeds up training.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自动编码器架构的常见调整是使编码器输出**γ** = log(**σ**²)而不是**σ**。然后可以根据[Equation 17-4](#var_ae_latent_loss_equation_2)计算潜在损失。这种方法在数值上更稳定，加快了训练速度。
- en: Equation 17-4\. Variational autoencoder’s latent loss, rewritten using *γ* =
    log(*σ*²)
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程17-4。变分自动编码器的潜在损失，使用*γ* = log(*σ*²)重写
- en: <math display="block"><mo mathvariant="script">L</mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="[" close="]"><mrow><mn>1</mn><mo>+</mo><msub><mi>γ</mi><mi>i</mi></msub><mo>-</mo><mi>exp</mi><mo>(</mo><msub><mi>γ</mi><mi>i</mi></msub><mo>)</mo><mo>-</mo><msup><msub><mi>μ</mi><mi>i</mi></msub><mn>2</mn></msup></mrow></mfenced></math>
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mo mathvariant="script">L</mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="[" close="]"><mrow><mn>1</mn><mo>+</mo><msub><mi>γ</mi><mi>i</mi></msub><mo>-</mo><mi>exp</mi><mo>(</mo><msub><mi>γ</mi><mi>i</mi></msub><mo>)</mo><mo>-</mo><msup><msub><mi>μ</mi><mi>i</mi></msub><mn>2</mn></msup></mrow></mfenced></math>
- en: 'Let’s start building a variational autoencoder for Fashion MNIST (as shown
    in [Figure 17-11](#variational_autoencoders_diagram), but using the **γ** tweak).
    First, we will need a custom layer to sample the codings, given **μ** and **γ**:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始为Fashion MNIST构建一个变分自动编码器（如[Figure 17-11](#variational_autoencoders_diagram)所示，但使用**γ**调整）。首先，我们需要一个自定义层来根据**μ**和**γ**抽样编码：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This `Sampling` layer takes two inputs: `mean` (**μ**) and `log_var` (**γ**).
    It uses the function `tf.random.normal()` to sample a random vector (of the same
    shape as **γ**) from the Gaussian distribution, with mean 0 and standard deviation
    1\. Then it multiplies it by exp(**γ** / 2) (which is equal to **σ**, as you can
    verify mathematically), and finally it adds **μ** and returns the result. This
    samples a codings vector from the Gaussian distribution with mean **μ** and standard
    deviation **σ**.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`Sampling`层接受两个输入：`mean`（**μ**）和`log_var`（**γ**）。它使用函数`tf.random.normal()`从均值为0，标准差为1的高斯分布中抽样一个随机向量（与**γ**形状相同）。然后将其乘以exp(**γ**
    / 2)（数学上等于**σ**，您可以验证），最后加上**μ**并返回结果。这样从均值为**μ**，标准差为**σ**的高斯分布中抽样一个编码向量。
- en: 'Next, we can create the encoder, using the functional API because the model
    is not entirely sequential:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建编码器，使用函数式API，因为模型不是完全顺序的：
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that the `Dense` layers that output `codings_mean` (**μ**) and `codings_log_var`
    (**γ**) have the same inputs (i.e., the outputs of the second `Dense` layer).
    We then pass both `codings_mean` and `codings_log_var` to the `Sampling` layer.
    Finally, the `variational_encoder` model has three outputs. Only the `codings`
    are required, but we add `codings_mean` and `codings_log_var` as well, in case
    we want to inspect their values. Now let’s build the decoder:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输出`codings_mean`（**μ**）和`codings_log_var`（**γ**）的`Dense`层具有相同的输入（即第二个`Dense`层的输出）。然后，我们将`codings_mean`和`codings_log_var`都传递给`Sampling`层。最后，`variational_encoder`模型有三个输出。只需要`codings`，但我们也添加了`codings_mean`和`codings_log_var`，以防我们想要检查它们的值。现在让我们构建解码器：
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For this decoder, we could have used the sequential API instead of the functional
    API, since it is really just a simple stack of layers, virtually identical to
    many of the decoders we have built so far. Finally, let’s build the variational
    autoencoder model:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个解码器，我们可以使用顺序API而不是功能API，因为它实际上只是一个简单的层堆栈，与我们迄今构建的许多解码器几乎相同。最后，让我们构建变分自动编码器模型：
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We ignore the first two outputs of the encoder (we only want to feed the codings
    to the decoder). Lastly, we must add the latent loss and the reconstruction loss:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们忽略编码器的前两个输出（我们只想将编码输入解码器）。最后，我们必须添加潜在损失和重构损失：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We first apply [Equation 17-4](#var_ae_latent_loss_equation_2) to compute the
    latent loss for each instance in the batch, summing over the last axis. Then we
    compute the mean loss over all the instances in the batch, and we divide the result
    by 784 to ensure it has the appropriate scale compared to the reconstruction loss.
    Indeed, the variational autoencoder’s reconstruction loss is supposed to be the
    sum of the pixel reconstruction errors, but when Keras computes the `"mse"` loss
    it computes the mean over all 784 pixels, rather than the sum. So, the reconstruction
    loss is 784 times smaller than we need it to be. We could define a custom loss
    to compute the sum rather than the mean, but it is simpler to divide the latent
    loss by 784 (the final loss will be 784 times smaller than it should be, but this
    just means that we should use a larger learning rate).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应用[方程17-4](#var_ae_latent_loss_equation_2)来计算批处理中每个实例的潜在损失，对最后一个轴求和。然后我们计算批处理中所有实例的平均损失，并将结果除以784，以确保它具有适当的比例，与重构损失相比。实际上，变分自动编码器的重构损失应该是像素重构误差的总和，但是当Keras计算`"mse"`损失时，它计算所有784个像素的平均值，而不是总和。因此，重构损失比我们需要的要小784倍。我们可以定义一个自定义损失来计算总和而不是平均值，但将潜在损失除以784（最终损失将比应该的大784倍，但这只是意味着我们应该使用更大的学习率）更简单。
- en: And finally, we can compile and fit the autoencoder!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以编译和拟合自动编码器！
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Generating Fashion MNIST Images
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成时尚MNIST图像
- en: 'Now let’s use this variational autoencoder to generate images that look like
    fashion items. All we need to do is sample random codings from a Gaussian distribution
    and decode them:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用这个变分自动编码器生成看起来像时尚物品的图像。我们只需要从高斯分布中随机采样编码，并解码它们：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Figure 17-12](#vae_generated_images_plot) shows the 12 generated images.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-12](#vae_generated_images_plot)显示了生成的12张图像。'
- en: '![mls3 1712](assets/mls3_1712.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1712](assets/mls3_1712.png)'
- en: Figure 17-12\. Fashion MNIST images generated by the variational autoencoder
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-12\. 由变分自动编码器生成的时尚MNIST图像
- en: The majority of these images look fairly convincing, if a bit too fuzzy. The
    rest are not great, but don’t be too harsh on the autoencoder—it only had a few
    minutes to learn!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像中的大多数看起来相当令人信服，虽然有点模糊。其余的不太好，但不要对自动编码器太苛刻——它只有几分钟时间学习！
- en: 'Variational autoencoders make it possible to perform *semantic interpolation*:
    instead of interpolating between two images at the pixel level, which would look
    as if the two images were just overlaid, we can interpolate at the codings level.
    For example, let’s take a few codings along an arbitrary line in latent space
    and decode them. We get a sequence of images that gradually go from pants to sweaters
    (see [Figure 17-13](#semantic_interpolation_plot)):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自动编码器使得执行*语义插值*成为可能：不是在像素级别插值两个图像，看起来就像两个图像只是叠加在一起，我们可以在编码级别进行插值。例如，让我们在潜在空间中沿着任意线取几个编码，并解码它们。我们得到一系列图像，逐渐从裤子变成毛衣（见[图17-13](#semantic_interpolation_plot)）：
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![mls3 1713](assets/mls3_1713.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1713](assets/mls3_1713.png)'
- en: Figure 17-13\. Semantic interpolation
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-13\. 语义插值
- en: 'Let’s now turn our attention to GANs: they are harder to train, but when you
    manage to get them to work, they produce pretty amazing images.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向GANs：它们更难训练，但当你设法让它们工作时，它们会产生非常惊人的图像。
- en: Generative Adversarial Networks
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: 'Generative adversarial networks were proposed in a [2014 paper](https://homl.info/gan)⁠^([9](ch17.html#idm45720168607168))
    by Ian Goodfellow et al., and although the idea got researchers excited almost
    instantly, it took a few years to overcome some of the difficulties of training
    GANs. Like many great ideas, it seems simple in hindsight: make neural networks
    compete against each other in the hope that this competition will push them to
    excel. As shown in [Figure 17-14](#gan_diagram), a GAN is composed of two neural
    networks:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络是由Ian Goodfellow等人在[2014年的一篇论文](https://homl.info/gan)中提出的⁠^([9](ch17.html#idm45720168607168))，尽管这个想法几乎立即激发了研究人员的兴趣，但要克服训练GANs的一些困难还需要几年时间。就像许多伟大的想法一样，事后看来似乎很简单：让神经网络相互竞争，希望这种竞争能推动它们取得卓越的成就。如[图17-14](#gan_diagram)所示，GAN由两个神经网络组成：
- en: Generator
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器
- en: 'Takes a random distribution as input (typically Gaussian) and outputs some
    data—typically, an image. You can think of the random inputs as the latent representations
    (i.e., codings) of the image to be generated. So, as you can see, the generator
    offers the same functionality as a decoder in a variational autoencoder, and it
    can be used in the same way to generate new images: just feed it some Gaussian
    noise, and it outputs a brand-new image. However, it is trained very differently,
    as you will soon see.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以随机分布（通常是高斯）作为输入，并输出一些数据——通常是图像。您可以将随机输入视为要生成的图像的潜在表示（即编码）。因此，正如您所看到的，生成器提供了与变分自动编码器中的解码器相同的功能，并且可以以相同的方式用于生成新图像：只需将一些高斯噪声输入，它就会输出一个全新的图像。但是，它的训练方式非常不同，您很快就会看到。
- en: Discriminator
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器
- en: Takes either a fake image from the generator or a real image from the training
    set as input, and must guess whether the input image is fake or real.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以生成器的假图像或训练集中的真实图像作为输入，必须猜测输入图像是假还是真。
- en: '![mls3 1714](assets/mls3_1714.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1714](assets/mls3_1714.png)'
- en: Figure 17-14\. A generative adversarial network
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-14\. 生成对抗网络
- en: 'During training, the generator and the discriminator have opposite goals: the
    discriminator tries to tell fake images from real images, while the generator
    tries to produce images that look real enough to trick the discriminator. Because
    the GAN is composed of two networks with different objectives, it cannot be trained
    like a regular neural network. Each training iteration is divided into two phases:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，生成器和鉴别器有着相反的目标：鉴别器试图区分假图像和真实图像，而生成器试图生成看起来足够真实以欺骗鉴别器的图像。由于GAN由具有不同目标的两个网络组成，因此无法像训练常规神经网络那样进行训练。每个训练迭代被分为两个阶段：
- en: In the first phase, we train the discriminator. A batch of real images is sampled
    from the training set and is completed with an equal number of fake images produced
    by the generator. The labels are set to 0 for fake images and 1 for real images,
    and the discriminator is trained on this labeled batch for one step, using the
    binary cross-entropy loss. Importantly, backpropagation only optimizes the weights
    of the discriminator during this phase.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一阶段，我们训练鉴别器。从训练集中抽取一批真实图像，并通过生成器生成相同数量的假图像来完成。标签设置为0表示假图像，1表示真实图像，并且鉴别器在这个带标签的批次上进行一步训练，使用二元交叉熵损失。重要的是，在这个阶段只有鉴别器的权重被优化。
- en: 'In the second phase, we train the generator. We first use it to produce another
    batch of fake images, and once again the discriminator is used to tell whether
    the images are fake or real. This time we do not add real images in the batch,
    and all the labels are set to 1 (real): in other words, we want the generator
    to produce images that the discriminator will (wrongly) believe to be real! Crucially,
    the weights of the discriminator are frozen during this step, so backpropagation
    only affects the weights of the generator.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二阶段，我们训练生成器。我们首先使用它生成另一批假图像，然后再次使用鉴别器来判断图像是假的还是真实的。这次我们不在批次中添加真实图像，所有标签都设置为1（真实）：换句话说，我们希望生成器生成鉴别器会（错误地）认为是真实的图像！在这一步骤中，鉴别器的权重被冻结，因此反向传播只影响生成器的权重。
- en: Note
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The generator never actually sees any real images, yet it gradually learns to
    produce convincing fake images! All it gets is the gradients flowing back through
    the discriminator. Fortunately, the better the discriminator gets, the more information
    about the real images is contained in these secondhand gradients, so the generator
    can make significant progress.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器实际上从未看到任何真实图像，但它逐渐学会生成令人信服的假图像！它所得到的只是通过鉴别器反向传播的梯度。幸运的是，鉴别器越好，这些二手梯度中包含的关于真实图像的信息就越多，因此生成器可以取得显著进展。
- en: Let’s go ahead and build a simple GAN for Fashion MNIST.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续构建一个简单的Fashion MNIST GAN。
- en: 'First, we need to build the generator and the discriminator. The generator
    is similar to an autoencoder’s decoder, and the discriminator is a regular binary
    classifier: it takes an image as input and ends with a `Dense` layer containing
    a single unit and using the sigmoid activation function. For the second phase
    of each training iteration, we also need the full GAN model containing the generator
    followed by the discriminator:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要构建生成器和鉴别器。生成器类似于自动编码器的解码器，鉴别器是一个常规的二元分类器：它以图像作为输入，最终以包含单个单元并使用sigmoid激活函数的`Dense`层结束。对于每个训练迭代的第二阶段，我们还需要包含生成器后面的鉴别器的完整GAN模型：
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we need to compile these models. As the discriminator is a binary classifier,
    we can naturally use the binary cross-entropy loss. The `gan` model is also a
    binary classifier, so it can use the binary cross-entropy loss as well. However,
    the generator will only be trained through the `gan` model, so we do not need
    to compile it at all. Importantly, the discriminator should not be trained during
    the second phase, so we make it non-trainable before compiling the `gan` model:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要编译这些模型。由于鉴别器是一个二元分类器，我们可以自然地使用二元交叉熵损失。`gan`模型也是一个二元分类器，因此它也可以使用二元交叉熵损失。然而，生成器只会通过`gan`模型进行训练，因此我们根本不需要编译它。重要的是，在第二阶段之前鉴别器不应该被训练，因此在编译`gan`模型之前我们将其设置为不可训练：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `trainable` attribute is taken into account by Keras only when compiling
    a model, so after running this code, the `discriminator` *is* trainable if we
    call its `fit()` method or its `train_on_batch()` method (which we will be using),
    while it is *not* trainable when we call these methods on the `gan` model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`trainable`属性只有在编译模型时才会被Keras考虑，因此在运行此代码后，如果我们调用其`fit()`方法或`train_on_batch()`方法（我们将使用），则`discriminator`是可训练的，而在调用这些方法时`gan`模型是不可训练的。'
- en: 'Since the training loop is unusual, we cannot use the regular `fit()` method.
    Instead, we will write a custom training loop. For this, we first need to create
    a `Dataset` to iterate through the images:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练循环是不寻常的，我们不能使用常规的`fit()`方法。相反，我们将编写一个自定义训练循环。为此，我们首先需要创建一个`Dataset`来迭代图像：
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We are now ready to write the training loop. Let’s wrap it in a `train_gan()`
    function:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备编写训练循环。让我们将其封装在一个`train_gan()`函数中：
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As discussed earlier, you can see the two phases at each iteration:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前讨论的，您可以在每次迭代中看到两个阶段：
- en: In phase one we feed Gaussian noise to the generator to produce fake images,
    and we complete this batch by concatenating an equal number of real images. The
    targets `y1` are set to 0 for fake images and 1 for real images. Then we train
    the discriminator on this batch. Remember that the discriminator is trainable
    in this phase, but we are not touching the generator.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一阶段，我们向生成器提供高斯噪声以生成假图像，并通过连接相同数量的真实图像来完成这一批次。目标`y1`设置为0表示假图像，1表示真实图像。然后我们对这一批次训练鉴别器。请记住，在这个阶段鉴别器是可训练的，但我们不会触及生成器。
- en: 'In phase two, we feed the GAN some Gaussian noise. Its generator will start
    by producing fake images, then the discriminator will try to guess whether these
    images are fake or real. In this phase, we are trying to improve the generator,
    which means that we want the discriminator to fail: this is why the targets `y2`
    are all set to 1, although the images are fake. In this phase, the discriminator
    is *not* trainable, so the only part of the `gan` model that will improve is the
    generator.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二阶段，我们向GAN提供一些高斯噪声。其生成器将开始生成假图像，然后鉴别器将尝试猜测这些图像是假还是真实的。在这个阶段，我们试图改进生成器，这意味着我们希望鉴别器失败：这就是为什么目标`y2`都设置为1，尽管图像是假的。在这个阶段，鉴别器是*不*可训练的，因此`gan`模型中唯一会改进的部分是生成器。
- en: 'That’s it! After training, you can randomly sample some codings from a Gaussian
    distribution, and feed them to the generator to produce new images:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！训练后，您可以随机从高斯分布中抽取一些编码，并将它们馈送给生成器以生成新图像：
- en: '[PRE25]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If you display the generated images (see [Figure 17-15](#gan_generated_images_plot)),
    you will see that at the end of the first epoch, they already start to look like
    (very noisy) Fashion MNIST images.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果显示生成的图像（参见[图17-15](#gan_generated_images_plot)），您会发现在第一个时期结束时，它们已经开始看起来像（非常嘈杂的）时尚MNIST图像。
- en: '![mls3 1715](assets/mls3_1715.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1715](assets/mls3_1715.png)'
- en: Figure 17-15\. Images generated by the GAN after one epoch of training
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-15。在训练一个时期后由GAN生成的图像
- en: Unfortunately, the images never really get much better than that, and you may
    even find epochs where the GAN seems to be forgetting what it learned. Why is
    that? Well, it turns out that training a GAN can be challenging. Let’s see why.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，图像从未真正比那更好，甚至可能会出现GAN似乎忘记了它学到的东西的时期。为什么会这样呢？原来，训练GAN可能是具有挑战性的。让我们看看为什么。
- en: The Difficulties of Training GANs
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练GAN的困难
- en: 'During training, the generator and the discriminator constantly try to outsmart
    each other, in a zero-sum game. As training advances, the game may end up in a
    state that game theorists call a *Nash equilibrium*, named after the mathematician
    John Nash: this is when no player would be better off changing their own strategy,
    assuming the other players do not change theirs. For example, a Nash equilibrium
    is reached when everyone drives on the left side of the road: no driver would
    be better off being the only one to switch sides. Of course, there is a second
    possible Nash equilibrium: when everyone drives on the *right* side of the road.
    Different initial states and dynamics may lead to one equilibrium or the other.
    In this example, there is a single optimal strategy once an equilibrium is reached
    (i.e., driving on the same side as everyone else), but a Nash equilibrium can
    involve multiple competing strategies (e.g., a predator chases its prey, the prey
    tries to escape, and neither would be better off changing their strategy).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，生成器和鉴别器不断试图在零和博弈中互相智胜。随着训练的进行，游戏可能会进入博弈论家称之为*纳什均衡*的状态，以数学家约翰·纳什命名：这是当没有玩家会因为改变自己的策略而变得更好，假设其他玩家不改变自己的策略。例如，当每个人都在道路的左侧行驶时，就达到了纳什均衡：没有司机会因为成为唯一一个换边的人而变得更好。当然，还有第二种可能的纳什均衡：当每个人都在道路的*右侧*行驶时。不同的初始状态和动态可能导致一个或另一个均衡。在这个例子中，一旦达到均衡状态（即，与其他人一样在同一侧行驶），就会有一个单一的最佳策略，但是纳什均衡可能涉及多种竞争策略（例如，捕食者追逐猎物，猎物试图逃跑，双方都不会因为改变策略而变得更好）。
- en: 'So how does this apply to GANs? Well, the authors of the GAN paper demonstrated
    that a GAN can only reach a single Nash equilibrium: that’s when the generator
    produces perfectly realistic images, and the discriminator is forced to guess
    (50% real, 50% fake). This fact is very encouraging: it would seem that you just
    need to train the GAN for long enough, and it will eventually reach this equilibrium,
    giving you a perfect generator. Unfortunately, it’s not that simple: nothing guarantees
    that the equilibrium will ever be reached.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这如何应用于GAN呢？嗯，GAN论文的作者们证明了GAN只能达到单一的纳什均衡：那就是生成器生成完全逼真的图像，鉴别器被迫猜测（50%真实，50%假）。这个事实非常令人鼓舞：似乎只需要训练足够长的时间，它最终会达到这种均衡，为您提供一个完美的生成器。不幸的是，事情并不那么简单：没有任何保证这种均衡会被达到。
- en: 'The biggest difficulty is called *mode collapse*: this is when the generator’s
    outputs gradually become less diverse. How can this happen? Suppose that the generator
    gets better at producing convincing shoes than any other class. It will fool the
    discriminator a bit more with shoes, and this will encourage it to produce even
    more images of shoes. Gradually, it will forget how to produce anything else.
    Meanwhile, the only fake images that the discriminator will see will be shoes,
    so it will also forget how to discriminate fake images of other classes. Eventually,
    when the discriminator manages to discriminate the fake shoes from the real ones,
    the generator will be forced to move to another class. It may then become good
    at shirts, forgetting about shoes, and the discriminator will follow. The GAN
    may gradually cycle across a few classes, never really becoming very good at any
    of them.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的困难被称为*模式坍塌*：这是指生成器的输出逐渐变得不那么多样化。这是如何发生的呢？假设生成器在制作令人信服的鞋子方面比其他任何类别都更擅长。它会用鞋子更多地欺骗鉴别器，这将鼓励它生成更多的鞋子图像。逐渐地，它会忘记如何制作其他任何东西。与此同时，鉴别器将看到的唯一假图像将是鞋子，因此它也会忘记如何鉴别其他类别的假图像。最终，当鉴别器设法将假鞋子与真实鞋子区分开来时，生成器将被迫转向另一个类别。然后它可能擅长衬衫，忘记鞋子，鉴别器也会跟随。GAN可能逐渐在几个类别之间循环，从未真正擅长其中任何一个。
- en: 'Moreover, because the generator and the discriminator are constantly pushing
    against each other, their parameters may end up oscillating and becoming unstable.
    Training may begin properly, then suddenly diverge for no apparent reason, due
    to these instabilities. And since many factors affect these complex dynamics,
    GANs are very sensitive to the hyperparameters: you may have to spend a lot of
    effort fine-tuning them. In fact, that’s why I used RMSProp rather than Nadam
    when compiling the models: when using Nadam, I ran into a severe mode collapse.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于生成器和鉴别器不断相互对抗，它们的参数可能最终会振荡并变得不稳定。训练可能开始正常，然后由于这些不稳定性，突然出现无明显原因的分歧。由于许多因素影响这些复杂的动态，GAN对超参数非常敏感：您可能需要花费大量精力对其进行微调。实际上，这就是为什么在编译模型时我使用RMSProp而不是Nadam：使用Nadam时，我遇到了严重的模式崩溃。
- en: 'These problems have kept researchers very busy since 2014: many papers have
    been published on this topic, some proposing new cost functions⁠^([10](ch17.html#idm45720167972816))
    (though a [2018 paper](https://homl.info/gansequal)⁠^([11](ch17.html#idm45720167970816))
    by Google researchers questions their efficiency) or techniques to stabilize training
    or to avoid the mode collapse issue. For example, a popular technique called *experience
    replay* consists of storing the images produced by the generator at each iteration
    in a replay buffer (gradually dropping older generated images) and training the
    discriminator using real images plus fake images drawn from this buffer (rather
    than just fake images produced by the current generator). This reduces the chances
    that the discriminator will overfit the latest generator’s outputs. Another common
    technique is called *mini-batch discrimination*: it measures how similar images
    are across the batch and provides this statistic to the discriminator, so it can
    easily reject a whole batch of fake images that lack diversity. This encourages
    the generator to produce a greater variety of images, reducing the chance of mode
    collapse. Other papers simply propose specific architectures that happen to perform
    well.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 自2014年以来，这些问题一直让研究人员忙碌不已：许多论文已经发表在这个主题上，一些论文提出了新的成本函数（尽管谷歌研究人员在[2018年的一篇论文](https://homl.info/gansequal)中质疑了它们的效率）或稳定训练或避免模式崩溃问题的技术。例如，一种流行的技术称为*经验重播*，它包括在每次迭代中存储生成器生成的图像在重播缓冲区中（逐渐删除较旧的生成图像），并使用来自该缓冲区的真实图像加上假图像来训练鉴别器（而不仅仅是当前生成器生成的假图像）。这减少了鉴别器过度拟合最新生成器输出的机会。另一种常见的技术称为*小批量鉴别*：它测量批次中图像的相似性，并将此统计信息提供给鉴别器，以便它可以轻松拒绝缺乏多样性的整个批次的假图像。这鼓励生成器产生更多样化的图像，减少模式崩溃的机会。其他论文简单地提出了表现良好的特定架构。
- en: In short, this is still a very active field of research, and the dynamics of
    GANs are still not perfectly understood. But the good news is that great progress
    has been made, and some of the results are truly astounding! So let’s look at
    some of the most successful architectures, starting with deep convolutional GANs,
    which were the state of the art just a few years ago. Then we will look at two
    more recent (and more complex) architectures.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这仍然是一个非常活跃的研究领域，GAN的动态仍然没有完全被理解。但好消息是取得了巨大进展，一些结果真的令人惊叹！因此，让我们看一些最成功的架构，从几年前的深度卷积GAN开始。然后我们将看一下两个更近期（更复杂）的架构。
- en: Deep Convolutional GANs
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度卷积GAN
- en: 'The authors of the original GAN paper experimented with convolutional layers,
    but only tried to generate small images. Soon after, many researchers tried to
    build GANs based on deeper convolutional nets for larger images. This proved to
    be tricky, as training was very unstable, but Alec Radford et al. finally succeeded
    in late 2015, after experimenting with many different architectures and hyperparameters.
    They called their architecture [*deep convolutional GANs*](https://homl.info/dcgan)
    (DCGANs).⁠^([12](ch17.html#idm45720167956496)) Here are the main guidelines they
    proposed for building stable convolutional GANs:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 原始GAN论文的作者尝试了卷积层，但只尝试生成小图像。不久之后，许多研究人员尝试基于更深的卷积网络生成更大的图像的GAN。这被证明是棘手的，因为训练非常不稳定，但Alec
    Radford等人最终在2015年底成功了，经过许多不同架构和超参数的实验。他们将其架构称为[*深度卷积GAN*](https://homl.info/dcgan)（DCGANs）。以下是他们为构建稳定的卷积GAN提出的主要准则：
- en: Replace any pooling layers with strided convolutions (in the discriminator)
    and transposed convolutions (in the generator).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用步进卷积（在鉴别器中）和转置卷积（在生成器中）替换任何池化层。
- en: Use batch normalization in both the generator and the discriminator, except
    in the generator’s output layer and the discriminator’s input layer.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成器和鉴别器中使用批量归一化，除了生成器的输出层和鉴别器的输入层。
- en: Remove fully connected hidden layers for deeper architectures.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除更深层次架构的全连接隐藏层。
- en: Use ReLU activation in the generator for all layers except the output layer,
    which should use tanh.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成器的所有层中使用ReLU激活，除了输出层应使用tanh。
- en: Use leaky ReLU activation in the discriminator for all layers.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在鉴别器的所有层中使用泄漏ReLU激活。
- en: 'These guidelines will work in many cases, but not always, so you may still
    need to experiment with different hyperparameters. In fact, just changing the
    random seed and training the exact same model again will sometimes work. Here
    is a small DCGAN that works reasonably well with Fashion MNIST:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这些准则在许多情况下都适用，但并非总是如此，因此您可能仍需要尝试不同的超参数。实际上，仅仅改变随机种子并再次训练完全相同的模型有时会奏效。以下是一个在时尚MNIST上表现相当不错的小型DCGAN：
- en: '[PRE26]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The generator takes codings of size 100, projects them to 6,272 dimensions
    (7 * 7 * 128), and reshapes the result to get a 7 × 7 × 128 tensor. This tensor
    is batch normalized and fed to a transposed convolutional layer with a stride
    of 2, which upsamples it from 7 × 7 to 14 × 14 and reduces its depth from 128
    to 64\. The result is batch normalized again and fed to another transposed convolutional
    layer with a stride of 2, which upsamples it from 14 × 14 to 28 × 28 and reduces
    the depth from 64 to 1\. This layer uses the tanh activation function, so the
    outputs will range from –1 to 1\. For this reason, before training the GAN, we
    need to rescale the training set to that same range. We also need to reshape it
    to add the channel dimension:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器接受大小为100的编码，将其投影到6,272维度（7 * 7 * 128），并重新整形结果以获得一个7×7×128张量。这个张量被批量归一化并馈送到一个步幅为2的转置卷积层，将其从7×7上采样到14×14，并将其深度从128减少到64。结果再次进行批量归一化，并馈送到另一个步幅为2的转置卷积层，将其从14×14上采样到28×28，并将深度从64减少到1。这一层使用tanh激活函数，因此输出将范围从-1到1。因此，在训练GAN之前，我们需要将训练集重新缩放到相同的范围。我们还需要重新整形以添加通道维度：
- en: '[PRE27]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The discriminator looks much like a regular CNN for binary classification,
    except instead of using max pooling layers to downsample the image, we use strided
    convolutions (`strides=2`). Note that we use the leaky ReLU activation function.
    Overall, we respected the DCGAN guidelines, except we replaced the `BatchNormalization`
    layers in the discriminator with `Dropout` layers; otherwise, training was unstable
    in this case. Feel free to tweak this architecture: you will see how sensitive
    it is to the hyperparameters, especially the relative learning rates of the two
    networks.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器看起来很像用于二元分类的常规CNN，只是不是使用最大池化层来对图像进行下采样，而是使用步幅卷积（`strides=2`）。请注意，我们使用了泄漏的ReLU激活函数。总体而言，我们遵守了DCGAN的指导方针，只是将鉴别器中的`BatchNormalization`层替换为`Dropout`层；否则，在这种情况下训练会不稳定。随意调整这个架构：您将看到它对超参数非常敏感，特别是两个网络的相对学习率。
- en: Lastly, to build the dataset and then compile and train this model, we can use
    the same code as earlier. After 50 epochs of training, the generator produces
    images like those shown in [Figure 17-16](#dcgan_generated_images_plot). It’s
    still not perfect, but many of these images are pretty convincing.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要构建数据集，然后编译和训练这个模型，我们可以使用之前的相同代码。经过50个训练周期后，生成器产生的图像如[图17-16](#dcgan_generated_images_plot)所示。它还不完美，但其中许多图像相当令人信服。
- en: '![mls3 1716](assets/mls3_1716.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1716](assets/mls3_1716.png)'
- en: Figure 17-16\. Images generated by the DCGAN after 50 epochs of training
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-16。DCGAN在训练50个周期后生成的图像
- en: 'If you scale up this architecture and train it on a large dataset of faces,
    you can get fairly realistic images. In fact, DCGANs can learn quite meaningful
    latent representations, as you can see in [Figure 17-17](#faces_arithmetics_diagram):
    many images were generated, and nine of them were picked manually (top left),
    including three representing men with glasses, three men without glasses, and
    three women without glasses. For each of these categories, the codings that were
    used to generate the images were averaged, and an image was generated based on
    the resulting mean codings (lower left). In short, each of the three lower-left
    images represents the mean of the three images located above it. But this is not
    a simple mean computed at the pixel level (this would result in three overlapping
    faces), it is a mean computed in the latent space, so the images still look like
    normal faces. Amazingly, if you compute men with glasses, minus men without glasses,
    plus women without glasses—where each term corresponds to one of the mean codings—and
    you generate the image that corresponds to this coding, you get the image at the
    center of the 3 × 3 grid of faces on the right: a woman with glasses! The eight
    other images around it were generated based on the same vector plus a bit of noise,
    to illustrate the semantic interpolation capabilities of DCGANs. Being able to
    do arithmetic on faces feels like science fiction!'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您扩大这个架构并在大量人脸数据集上进行训练，您可以获得相当逼真的图像。事实上，DCGAN可以学习相当有意义的潜在表示，如[图17-17](#faces_arithmetics_diagram)所示：生成了许多图像，手动选择了其中的九个（左上角），包括三个戴眼镜的男性，三个不戴眼镜的男性和三个不戴眼镜的女性。对于这些类别中的每一个，用于生成图像的编码被平均，然后基于结果的平均编码生成图像（左下角）。简而言之，左下角的三幅图像分别代表位于其上方的三幅图像的平均值。但这不是在像素级别计算的简单平均值（这将导致三个重叠的脸），而是在潜在空间中计算的平均值，因此图像看起来仍然像正常的脸。令人惊讶的是，如果您计算戴眼镜的男性，减去不戴眼镜的男性，再加上不戴眼镜的女性——其中每个术语对应于一个平均编码——并生成对应于此编码的图像，您将得到右侧面孔网格中心的图像：一个戴眼镜的女性！其周围的其他八幅图像是基于相同向量加上一点噪音生成的，以展示DCGAN的语义插值能力。能够在人脸上进行算术运算感觉像是科幻！
- en: DCGANs aren’t perfect, though. For example, when you try to generate very large
    images using DCGANs, you often end up with locally convincing features but overall
    inconsistencies, such as shirts with one sleeve much longer than the other, different
    earrings, or eyes looking in opposite directions. How can you fix this?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，DCGAN并不完美。例如，当您尝试使用DCGAN生成非常大的图像时，通常会出现局部令人信服的特征，但整体上存在不一致，比如一只袖子比另一只长得多的衬衫，不同的耳环，或者眼睛看向相反的方向。您如何解决这个问题？
- en: '![mls3 1717](assets/mls3_1717.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1717](assets/mls3_1717.png)'
- en: Figure 17-17\. Vector arithmetic for visual concepts (part of figure 7 from
    the DCGAN paper)⁠^([13](ch17.html#idm45720167572992))
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-17。视觉概念的向量算术（DCGAN论文中的第7部分图）⁠^([13](ch17.html#idm45720167572992))
- en: Tip
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you add each image’s class as an extra input to both the generator and the
    discriminator, they will both learn what each class looks like, and thus you will
    be able to control the class of each image produced by the generator. This is
    called a [*conditional GAN*](https://homl.info/cgan)(CGAN).⁠^([14](ch17.html#idm45720167569680))
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将每个图像的类别作为额外输入添加到生成器和鉴别器中，它们将学习每个类别的外观，因此您将能够控制生成器生成的每个图像的类别。这被称为[*条件GAN*](https://homl.info/cgan)(CGAN)。⁠^([14](ch17.html#idm45720167569680))
- en: Progressive Growing of GANs
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN的渐进增长
- en: 'In a [2018 paper](https://homl.info/progan),⁠^([15](ch17.html#idm45720167566480))
    Nvidia researchers Tero Kerras et al. proposed an important technique: they suggested
    generating small images at the beginning of training, then gradually adding convolutional
    layers to both the generator and the discriminator to produce larger and larger
    images (4 × 4, 8 × 8, 16 × 16, …​, 512 × 512, 1,024 × 1,024). This approach resembles
    greedy layer-wise training of stacked autoencoders. The extra layers get added
    at the end of the generator and at the beginning of the discriminator, and previously
    trained layers remain trainable.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇[2018年的论文](https://homl.info/progan)，Nvidia的研究人员Tero Kerras等人提出了一项重要技术：他们建议在训练开始时生成小图像，然后逐渐向生成器和鉴别器添加卷积层，以生成越来越大的图像（4×4、8×8、16×16，...，512×512、1,024×1,024）。这种方法类似于贪婪逐层训练堆叠自动编码器。额外的层被添加到生成器的末尾和鉴别器的开头，并且之前训练过的层仍然可训练。
- en: For example, when growing the generator’s outputs from 4 × 4 to 8 × 8 (see [Figure 17-18](#progressively_growing_gan_diagram)),
    an upsampling layer (using nearest neighbor filtering) is added to the existing
    convolutional layer (“Conv 1”) to produce 8 × 8 feature maps. These are fed to
    the new convolutional layer (“Conv 2”), which in turn feeds into a new output
    convolutional layer. To avoid breaking the trained weights of Conv 1, we gradually
    fade in the two new convolutional layers (represented with dashed lines in [Figure 17-18](#progressively_growing_gan_diagram))
    and fade out the original output layer. The final outputs are a weighted sum of
    the new outputs (with weight *α*) and the original outputs (with weight 1 – *α*),
    slowly increasing *α* from 0 to 1\. A similar fade-in/fade-out technique is used
    when a new convolutional layer is added to the discriminator (followed by an average
    pooling layer for downsampling). Note that all convolutional layers use `"same"`
    padding and strides of 1, so they preserve the height and width of their inputs.
    This includes the original convolutional layer, so it now produces 8 × 8 outputs
    (since its inputs are now 8 × 8). Lastly, the output layers use kernel size 1\.
    They just project their inputs down to the desired number of color channels (typically
    3).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当将生成器的输出从4×4增加到8×8时（参见[图17-18](#progressively_growing_gan_diagram)），在现有卷积层（“Conv
    1”）中添加了一个上采样层（使用最近邻过滤）以生成8×8特征图。这些被馈送到新的卷积层（“Conv 2”），然后馈送到新的输出卷积层。为了避免破坏Conv
    1的训练权重，我们逐渐淡入两个新的卷积层（在[图17-18](#progressively_growing_gan_diagram)中用虚线表示），并淡出原始输出层。最终输出是新输出（权重为*α*）和原始输出（权重为1-*α*）的加权和，从0逐渐增加*α*到1。当向鉴别器添加新的卷积层时（后跟一个平均池化层进行下采样），也使用类似的淡入/淡出技术。请注意，所有卷积层都使用“same”填充和步幅为1，因此它们保留其输入的高度和宽度。这包括原始卷积层，因此它现在产生8×8的输出（因为其输入现在是8×8）。最后，输出层使用核大小为1。它们只是将它们的输入投影到所需数量的颜色通道（通常为3）。
- en: '![mls3 1718](assets/mls3_1718.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1718](assets/mls3_1718.png)'
- en: 'Figure 17-18\. A progressively growing GAN: a GAN generator outputs 4 × 4 color
    images (left); we extend it to output 8 × 8 images (right)'
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-18。逐渐增长的GAN：GAN生成器输出4×4彩色图像（左）；我们将其扩展到输出8×8图像（右）
- en: 'The paper also introduced several other techniques aimed at increasing the
    diversity of the outputs (to avoid mode collapse) and making training more stable:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文还介绍了几种旨在增加输出多样性（以避免模式崩溃）并使训练更稳定的技术：
- en: Mini-batch standard deviation layer
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量标准差层
- en: Added near the end of the discriminator. For each position in the inputs, it
    computes the standard deviation across all channels and all instances in the batch
    (`S = tf.math.reduce_std(inputs, axis=[0, -1])`). These standard deviations are
    then averaged across all points to get a single value (`v = tf.reduce_​mean(S)`).
    Finally, an extra feature map is added to each instance in the batch and filled
    with the computed value (`tf.concat([inputs, tf.fill([batch_size, height, width,
    1], v)], axis=-1)`). How does this help? Well, if the generator produces images
    with little variety, then there will be a small standard deviation across feature
    maps in the discriminator. Thanks to this layer, the discriminator will have easy
    access to this statistic, making it less likely to be fooled by a generator that
    produces too little diversity. This will encourage the generator to produce more
    diverse outputs, reducing the risk of mode collapse.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到鉴别器的末尾附近。对于输入中的每个位置，它计算批次中所有通道和所有实例的标准差（`S = tf.math.reduce_std(inputs, axis=[0,
    -1])`）。然后，这些标准差在所有点上进行平均以获得单个值（`v = tf.reduce_mean(S)`）。最后，在批次中的每个实例中添加一个额外的特征图，并填充计算出的值（`tf.concat([inputs,
    tf.fill([batch_size, height, width, 1], v)], axis=-1)`）。这有什么帮助呢？如果生成器生成具有很少变化的图像，那么在鉴别器的特征图中将会有很小的标准差。由于这一层，鉴别器将更容易访问这个统计数据，使得它不太可能被生成器欺骗，生成器产生的多样性太少。这将鼓励生成器产生更多样化的输出，减少模式崩溃的风险。
- en: Equalized learning rate
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 均衡学习率
- en: 'Initializes all weights using a Gaussian distribution with mean 0 and standard
    deviation 1 rather than using He initialization. However, the weights are scaled
    down at runtime (i.e., every time the layer is executed) by the same factor as
    in He initialization: they are divided by <math><msqrt><mfrac bevelled="true"><mn>2</mn><msub><mi>n</mi><mtext>inputs</mtext></msub></mfrac></msqrt></math>,
    where *n*[inputs] is the number of inputs to the layer. The paper demonstrated
    that this technique significantly improved the GAN’s performance when using RMSProp,
    Adam, or other adaptive gradient optimizers. Indeed, these optimizers normalize
    the gradient updates by their estimated standard deviation (see [Chapter 11](ch11.html#deep_chapter)),
    so parameters that have a larger dynamic range⁠^([16](ch17.html#idm45720167545296))
    will take longer to train, while parameters with a small dynamic range may be
    updated too quickly, leading to instabilities. By rescaling the weights as part
    of the model itself rather than just rescaling them upon initialization, this
    approach ensures that the dynamic range is the same for all parameters throughout
    training, so they all learn at the same speed. This both speeds up and stabilizes
    training.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用均值为0，标准差为1的高斯分布初始化所有权重，而不是使用He初始化。然而，在运行时（即每次执行该层时），权重会按照He初始化的相同因子进行缩放：它们会被除以<math><msqrt><mfrac
    bevelled="true"><mn>2</mn><msub><mi>n</mi><mtext>inputs</mtext></msub></mfrac></msqrt></math>，其中*n*[inputs]是该层的输入数量。论文表明，当使用RMSProp、Adam或其他自适应梯度优化器时，这种技术显著提高了GAN的性能。实际上，这些优化器通过其估计的标准偏差对梯度更新进行归一化（参见[第11章](ch11.html#deep_chapter)），因此具有较大动态范围的参数将需要更长时间进行训练，而具有较小动态范围的参数可能会更新得太快，导致不稳定性。通过在模型本身中重新缩放权重，而不仅仅在初始化时重新缩放它们，这种方法确保了在整个训练过程中所有参数的动态范围相同，因此它们都以相同的速度学习。这既加快了训练速度，又稳定了训练过程。
- en: Pixelwise normalization layer
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 像素级归一化层
- en: Added after each convolutional layer in the generator. It normalizes each activation
    based on all the activations in the same image and at the same location, but across
    all channels (dividing by the square root of the mean squared activation). In
    TensorFlow code, this is `inputs / tf.sqrt(tf.reduce_mean(tf.square(X), axis=-1,
    keepdims=True) + 1e-8)` (the smoothing term `1e-8` is needed to avoid division
    by zero). This technique avoids explosions in the activations due to excessive
    competition between the generator and the discriminator.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成器的每个卷积层之后添加。它根据同一图像和位置处的所有激活进行归一化，但跨所有通道（除以均方激活的平方根）。在TensorFlow代码中，这是`inputs
    / tf.sqrt(tf.reduce_mean(tf.square(X), axis=-1, keepdims=True) + 1e-8)`（需要平滑项`1e-8`以避免除以零）。这种技术避免了由于生成器和鉴别器之间的激烈竞争而导致激活爆炸。
- en: 'The combination of all these techniques allowed the authors to generate [extremely
    convincing high-definition images of faces](https://homl.info/progandemo). But
    what exactly do we call “convincing”? Evaluation is one of the big challenges
    when working with GANs: although it is possible to automatically evaluate the
    diversity of the generated images, judging their quality is a much trickier and
    subjective task. One technique is to use human raters, but this is costly and
    time-consuming. So, the authors proposed to measure the similarity between the
    local image structure of the generated images and the training images, considering
    every scale. This idea led them to another groundbreaking innovation: StyleGANs.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些技术的结合使作者能够生成[极具说服力的高清面部图像](https://homl.info/progandemo)。但是，“说服力”到底是什么意思呢？评估是在使用GAN时面临的一大挑战：尽管可以自动评估生成图像的多样性，但评估其质量是一项更加棘手和主观的任务。一种技术是使用人类评分者，但这既昂贵又耗时。因此，作者提出了一种方法，即考虑生成图像与训练图像之间的局部图像结构的相似性，考虑每个尺度。这个想法引领他们走向另一个开创性的创新：StyleGANs。
- en: StyleGANs
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StyleGANs
- en: 'The state of the art in high-resolution image generation was advanced once
    again by the same Nvidia team in a [2018 paper](https://homl.info/stylegan)⁠^([17](ch17.html#idm45720167535792))
    that introduced the popular StyleGAN architecture. The authors used *style transfer*
    techniques in the generator to ensure that the generated images have the same
    local structure as the training images, at every scale, greatly improving the
    quality of the generated images. The discriminator and the loss function were
    not modified, only the generator. A StyleGAN generator is composed of two networks
    (see [Figure 17-19](#stylegan_diagram)):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率图像生成领域的最新技术再次由同一Nvidia团队在[2018年的一篇论文](https://homl.info/stylegan)中推进，引入了流行的StyleGAN架构。作者在生成器中使用*风格转移*技术，以确保生成的图像在每个尺度上具有与训练图像相同的局部结构，极大地提高了生成图像的质量。鉴别器和损失函数没有被修改，只有生成器。StyleGAN生成器由两个网络组成（参见[图17-19](#stylegan_diagram)）：
- en: Mapping network
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 映射网络
- en: An eight-layer MLP that maps the latent representations **z** (i.e., the codings)
    to a vector **w**. This vector is then sent through multiple *affine transformations*
    (i.e., `Dense` layers with no activation functions, represented by the “A” boxes
    in [Figure 17-19](#stylegan_diagram)), which produces multiple vectors. These
    vectors control the style of the generated image at different levels, from fine-grained
    texture (e.g., hair color) to high-level features (e.g., adult or child). In short,
    the mapping network maps the codings to multiple style vectors.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将潜在表示**z**（即编码）映射到向量**w**的八层MLP。然后，将该向量通过多个*仿射变换*（即没有激活函数的`Dense`层，在[图17-19](#stylegan_diagram)中用“A”框表示）发送，从而产生多个向量。这些向量控制生成图像的风格在不同层次上，从细粒度纹理（例如头发颜色）到高级特征（例如成人或儿童）。简而言之，映射网络将编码映射到多个风格向量。
- en: Synthesis network
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 合成网络
- en: 'Responsible for generating the images. It has a constant learned input (to
    be clear, this input will be constant *after* training, but *during* training
    it keeps getting tweaked by backpropagation). It processes this input through
    multiple convolutional and upsampling layers, as earlier, but there are two twists.
    First, some noise is added to the input and to all the outputs of the convolutional
    layers (before the activation function). Second, each noise layer is followed
    by an *adaptive instance normalization* (AdaIN) layer: it standardizes each feature
    map independently (by subtracting the feature map’s mean and dividing by its standard
    deviation), then it uses the style vector to determine the scale and offset of
    each feature map (the style vector contains one scale and one bias term for each
    feature map).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 负责生成图像。它有一个恒定的学习输入（明确地说，这个输入在训练之后将是恒定的，但在训练期间，它会通过反向传播不断调整）。它通过多个卷积和上采样层处理这个输入，就像之前一样，但有两个变化。首先，在输入和所有卷积层的输出（在激活函数之前）中添加了一些噪音。其次，每个噪音层后面都跟着一个*自适应实例归一化*（AdaIN）层：它独立地标准化每个特征图（通过减去特征图的均值并除以其标准差），然后使用风格向量确定每个特征图的比例和偏移（风格向量包含每个特征图的一个比例和一个偏置项）。
- en: '![mls3 1719](assets/mls3_1719.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1719](assets/mls3_1719.png)'
- en: Figure 17-19\. StyleGAN’s generator architecture (part of Figure 1 from the
    StyleGAN paper)⁠^([18](ch17.html#idm45720167519152))
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-19. StyleGAN的生成器架构（来自StyleGAN论文的图1的一部分）⁠^([18](ch17.html#idm45720167519152))
- en: 'The idea of adding noise independently from the codings is very important.
    Some parts of an image are quite random, such as the exact position of each freckle
    or hair. In earlier GANs, this randomness had to either come from the codings
    or be some pseudorandom noise produced by the generator itself. If it came from
    the codings, it meant that the generator had to dedicate a significant portion
    of the codings’ representational power to storing noise, which this is quite wasteful.
    Moreover, the noise had to be able to flow through the network and reach the final
    layers of the generator: this seems like an unnecessary constraint that probably
    slowed down training. And finally, some visual artifacts may appear because the
    same noise was used at different levels. If instead the generator tried to produce
    its own pseudorandom noise, this noise might not look very convincing, leading
    to more visual artifacts. Plus, part of the generator’s weights would be dedicated
    to generating pseudorandom noise, which again seems wasteful. By adding extra
    noise inputs, all these issues are avoided; the GAN is able to use the provided
    noise to add the right amount of stochasticity to each part of the image.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 独立于编码添加噪音的想法非常重要。图像的某些部分是相当随机的，比如每个雀斑或头发的确切位置。在早期的GAN中，这种随机性要么来自编码，要么是生成器本身产生的一些伪随机噪音。如果来自编码，这意味着生成器必须将编码的表征能力的相当一部分用于存储噪音，这是相当浪费的。此外，噪音必须能够流经网络并到达生成器的最终层：这似乎是一个不必要的约束，可能会减慢训练速度。最后，一些视觉伪影可能会出现，因为在不同级别使用相同的噪音。如果生成器尝试生成自己的伪随机噪音，这种噪音可能看起来不太令人信服，导致更多的视觉伪影。此外，生成器的一部分权重将被用于生成伪随机噪音，这再次似乎是浪费的。通过添加额外的噪音输入，所有这些问题都可以避免；GAN能够利用提供的噪音为图像的每个部分添加适量的随机性。
- en: The added noise is different for each level. Each noise input consists of a
    single feature map full of Gaussian noise, which is broadcast to all feature maps
    (of the given level) and scaled using learned per-feature scaling factors (this
    is represented by the “B” boxes in [Figure 17-19](#stylegan_diagram)) before it
    is added.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 每个级别的添加噪音都是不同的。每个噪音输入由一个充满高斯噪音的单个特征图组成，该特征图被广播到所有特征图（给定级别的）并使用学习的每个特征比例因子进行缩放（这由[图17-19](#stylegan_diagram)中的“B”框表示）然后添加。
- en: Finally, StyleGAN uses a technique called *mixing regularization* (or *style
    mixing*), where a percentage of the generated images are produced using two different
    codings. Specifically, the codings **c**[1] and **c**[2] are sent through the
    mapping network, giving two style vectors **w**[1] and **w**[2]. Then the synthesis
    network generates an image based on the styles **w**[1] for the first levels and
    the styles **w**[2] for the remaining levels. The cutoff level is picked randomly.
    This prevents the network from assuming that styles at adjacent levels are correlated,
    which in turn encourages locality in the GAN, meaning that each style vector only
    affects a limited number of traits in the generated image.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，StyleGAN使用一种称为*混合正则化*（或*风格混合*）的技术，其中一定比例的生成图像是使用两种不同的编码生成的。具体来说，编码**c**[1]和**c**[2]被送入映射网络，得到两个风格向量**w**[1]和**w**[2]。然后合成网络根据第一级的风格**w**[1]和剩余级别的风格**w**[2]生成图像。截断级别是随机选择的。这可以防止网络假设相邻级别的风格是相关的，从而鼓励GAN中的局部性，这意味着每个风格向量只影响生成图像中的有限数量的特征。
- en: 'There is such a wide variety of GANs out there that it would require a whole
    book to cover them all. Hopefully this introduction has given you the main ideas,
    and most importantly the desire to learn more. Go ahead and implement your own
    GAN, and do not get discouraged if it has trouble learning at first: unfortunately,
    this is normal, and it will require quite a bit of patience to get it working,
    but the result is worth it. If you’re struggling with an implementation detail,
    there are plenty of Keras or TensorFlow implementations that you can look at.
    In fact, if all you want is to get some amazing results quickly, then you can
    just use a pretrained model (e.g., there are pretrained StyleGAN models available
    for Keras).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有这么多种类的GAN，需要一本整书来覆盖它们。希望这个介绍给您带来了主要思想，最重要的是让您有学习更多的愿望。继续实现您自己的GAN，如果一开始学习有困难，请不要灰心：不幸的是，这是正常的，需要相当多的耐心才能使其正常运行，但结果是值得的。如果您在实现细节上遇到困难，有很多Keras或TensorFlow的实现可以参考。实际上，如果您只是想快速获得一些惊人的结果，那么您可以使用预训练模型（例如，有适用于Keras的预训练StyleGAN模型可用）。
- en: 'Now that we’ve examined autoencoders and GANs, let’s look at one last type
    of architecture: diffusion models.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经研究了自动编码器和GANs，让我们看看最后一种架构：扩散模型。
- en: Diffusion Models
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型
- en: 'The ideas behind diffusion models have been around for many years, but they
    were first formalized in their modern form in a [2015 paper](https://homl.info/diffusion)⁠^([19](ch17.html#idm45720167499824))
    by Jascha Sohl-Dickstein et al. from Stanford University and UC Berkeley. The
    authors applied tools from thermodynamics to model a diffusion process, similar
    to a drop of milk diffusing in a cup of tea. The core idea is to train a model
    to learn the reverse process: start from the completely mixed state, and gradually
    “unmix” the milk from the tea. Using this idea, they obtained promising results
    in image generation, but since GANs produced more convincing images back then,
    diffusion models did not get as much attention.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型的理念已经存在多年，但它们首次以现代形式在斯坦福大学和加州大学伯克利分校的Jascha Sohl-Dickstein等人的[2015年论文](https://homl.info/diffusion)中得到正式形式化。作者们应用了热力学工具来建模扩散过程，类似于一滴牛奶在茶杯中扩散的过程。核心思想是训练一个模型来学习反向过程：从完全混合状态开始，逐渐将牛奶从茶中“分离”。利用这个想法，他们在图像生成方面取得了令人期待的结果，但由于当时GANs生成的图像更具说服力，扩散模型并没有得到太多关注。
- en: 'Then, in 2020, [Jonathan Ho et al.](https://homl.info/ddpm), also from UC Berkeley,
    managed to build a diffusion model capable of generating highly realistic images,
    which they called a *denoising diffusion probabilistic model* (DDPM).⁠^([20](ch17.html#idm45720167493280))
    A few months later, a [2021 paper](https://homl.info/ddpm2)⁠^([21](ch17.html#idm45720167489824))
    by OpenAI researchers Alex Nichol and Prafulla Dhariwal analyzed the DDPM architecture
    and proposed several improvements that allowed DDPMs to finally beat GANs: not
    only are DDPMs much easier to train than GANs, but the generated images are more
    diverse and of even higher quality. The main downside of DDPMs, as you will see,
    is that they take a very long time to generate images, compared to GANs or VAEs.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2020年，也来自加州大学伯克利分校的Jonathan Ho等人成功构建了一个能够生成高度逼真图像的扩散模型，他们称之为*去噪扩散概率模型*（DDPM）。几个月后，OpenAI研究人员Alex
    Nichol和Prafulla Dhariwal的[2021年论文](https://homl.info/ddpm2)分析了DDPM架构，并提出了几项改进，使DDPM最终击败了GANs：DDPM不仅比GANs更容易训练，而且生成的图像更加多样化且质量更高。正如您将看到的那样，DDPM的主要缺点是生成图像需要很长时间，与GANs或VAEs相比。
- en: 'So how exactly does a DDPM work? Well, suppose you start with a picture of
    a cat (like the one you’ll see in [Figure 17-20](#denoising_model_diagram)), noted
    **x**[0], and at each time step *t* you add a little bit of Gaussian noise to
    the image, with mean 0 and variance *β*[*t*]. This noise is independent for each
    pixel: we call it *isotropic*. You first obtain the image **x**[1], then **x**[2],
    and so on, until the cat is completely hidden by the noise, impossible to see.
    The last time step is noted *T*. In the original DDPM paper, the authors used
    *T* = 1,000, and they scheduled the variance *β*[*t*] in such a way that the cat
    signal fades linearly between time steps 0 and *T*. In the improved DDPM paper,
    *T* was bumped up to 4,000, and the variance schedule was tweaked to change more
    slowly at the beginning and at the end. In short, we’re gradually drowning the
    cat in noise: this is called the *forward process*.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 那么DDPM究竟是如何工作的呢？假设您从一张猫的图片开始（就像您将在[图17-20](#denoising_model_diagram)中看到的那样），记为**x**[0]，并且在每个时间步*t*中向图像添加一点均值为0，方差为*β*[*t*]的高斯噪声。这种噪声对于每个像素都是独立的：我们称之为*各向同性*。您首先得到图像**x**[1]，然后**x**[2]，依此类推，直到猫完全被噪声隐藏，无法看到。最后一个时间步记为*T*。在最初的DDPM论文中，作者使用*T*
    = 1,000，并且他们安排了方差*β*[*t*]的方式，使得猫信号在时间步0和*T*之间线性消失。在改进的DDPM论文中，*T*增加到了4,000，并且方差安排被调整为在开始和结束时变化更慢。简而言之，我们正在逐渐将猫淹没在噪声中：这被称为*正向过程*。
- en: As we add more and more Gaussian noise in the forward process, the distribution
    of pixel values becomes more and more Gaussian. One important detail I left out
    is that the pixel values get rescaled slightly at each step, by a factor of <math><msqrt><mn>1</mn><mo>-</mo><msub><mi>β</mi><mi>t</mi></msub></msqrt></math>.
    This ensures that the mean of the pixel values gradually approaches 0, since the
    scaling factor is a bit smaller than 1 (imagine repeatedly multiplying a number
    by 0.99). It also ensures that the variance will gradually converge to 1\. This
    is because the standard deviation of the pixel values also gets scaled by <math><msqrt><mn>1</mn><mo>-</mo><msub><mi>β</mi><mi>t</mi></msub></msqrt></math>,
    so the variance gets scaled by 1 – *β*[*t*] (i.e., the square of the scaling factor).
    But the variance cannot shrink to 0 since we’re adding Gaussian noise with variance
    *β*[*t*] at each step. And since variances add up when you sum Gaussian distributions,
    you can see that the variance can only converge to 1 – *β*[*t*] + *β*[*t*] = 1.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们在正向过程中不断添加更多高斯噪声，像素值的分布变得越来越高斯。我遗漏的一个重要细节是，每一步像素值都会被稍微重新缩放，缩放因子为<math><msqrt><mn>1</mn><mo>-</mo><msub><mi>β</mi><mi>t</mi></msub></msqrt></math>。这确保了像素值的均值逐渐接近0，因为缩放因子比1稍微小一点（想象一下反复将一个数字乘以0.99）。这也确保了方差将逐渐收敛到1。这是因为像素值的标准差也会被<math><msqrt><mn>1</mn><mo>-</mo><msub><mi>β</mi><mi>t</mi></msub></msqrt></math>缩放，因此方差会被1
    - *β*[*t*]（即缩放因子的平方）缩放。但是方差不能收缩到0，因为我们在每一步都添加方差为*β*[*t*]的高斯噪声。而且由于当你对高斯分布求和时方差会相加，您可以看到方差只能收敛到1
    - *β*[*t*] + *β*[*t*] = 1。
- en: The forward diffusion process is summarized in [Equation 17-5](#forward_process_equation).
    This equation won’t teach you anything new about the forward process, but it’s
    useful to understand this type of mathematical notation, as it’s often used in
    ML papers. This equation defines the probability distribution *q* of **x**[*t*]
    given **x**[*t*–1] as a Gaussian distribution with mean **x**[*t*–1] times the
    scaling factor, and with a covariance matrix equal to *β*[*t*]**I**. This is the
    identity matrix **I** multiplied by *β*[*t*], which means that the noise is isotropic
    with variance *β*[*t*].
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 前向扩散过程总结在 [Equation 17-5](#forward_process_equation) 中。这个方程不会教你任何新的关于前向过程的知识，但理解这种数学符号是有用的，因为它经常在机器学习论文中使用。这个方程定义了给定
    **x**[*t*–1] 的概率分布 *q* 中 **x**[*t*] 的概率分布，其均值为 **x**[*t*–1] 乘以缩放因子，并且具有等于 *β*[*t*]**I**
    的协方差矩阵。这是由 *β*[*t*] 乘以单位矩阵 **I** 得到的，这意味着噪音是各向同性的，方差为 *β*[*t*]。
- en: Equation 17-5\. Probability distribution *q* of the forward diffusion process
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 17-5\. 前向扩散过程的概率分布 *q*
- en: <math display="block"><mi>q</mi><mo>(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo>|</mo><msub><mi
    mathvariant="bold">x</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>)</mo><mo>=</mo><mi
    mathvariant="script">N</mi><mo>(</mo><msqrt><mn>1</mn><mo>-</mo><msub><mi>β</mi><mi>t</mi></msub></msqrt><msub><mi
    mathvariant="bold">x</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>β</mi><mi>t</mi></msub><mi
    mathvariant="bold">I</mi><mo>)</mo></math>
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mi>q</mi><mo>(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo>|</mo><msub><mi
    mathvariant="bold">x</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>)</mo><mo>=</mo><mi
    mathvariant="script">N</mi><mo>(</mo><msqrt><mn>1</mn><mo>-</mo><msub><mi>β</mi><mi>t</mi></msub></msqrt><msub><mi
    mathvariant="bold">x</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>β</mi><mi>t</mi></msub><mi
    mathvariant="bold">I</mi><mo>)</mo></math>
- en: 'Interestingly, there’s a shortcut for the forward process: it’s possible to
    sample an image **x**[*t*] given **x**[0] without having to first compute **x**[1],
    **x**[2], …​, **x**[*t*–1]. Indeed, since the sum of multiple Gaussian distributions
    is also a Gaussian distribution, all the noise can be added in just one shot using
    [Equation 17-6](#fast_forward_process_equation). This is the equation we will
    be using, as it is much faster.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，前向过程有一个快捷方式：可以在不必先计算 **x**[1], **x**[2], …, **x**[*t*–1] 的情况下，给定 **x**[0]
    来采样图像 **x**[*t*]。实际上，由于多个高斯分布的和也是一个高斯分布，所有的噪音可以在一次性使用 [Equation 17-6](#fast_forward_process_equation)
    中的公式添加。这是我们将要使用的方程，因为它速度更快。
- en: Equation 17-6\. Shortcut for the forward diffusion process
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 17-6\. 前向扩散过程的快捷方式
- en: <math display="block"><mi>q</mi><mo>(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo>|</mo><msub><mi
    mathvariant="bold">x</mi><mn>0</mn></msub><mo>)</mo><mo>=</mo><mi mathvariant="script">N</mi><mfenced><mrow><msqrt><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub></msqrt><msub><mi
    mathvariant="bold">x</mi><mn>0</mn></msub><mo>,</mo><mo>(</mo><mn>1</mn><mo>-</mo><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub><mo>)</mo><mi
    mathvariant="bold">I</mi></mrow></mfenced></math>
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mi>q</mi><mo>(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo>|</mo><msub><mi
    mathvariant="bold">x</mi><mn>0</mn></msub><mo>)</mo><mo>=</mo><mi mathvariant="script">N</mi><mfenced><mrow><msqrt><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub></msqrt><msub><mi
    mathvariant="bold">x</mi><mn>0</mn></msub><mo>,</mo><mo>(</mo><mn>1</mn><mo>-</mo><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub><mo>)</mo><mi
    mathvariant="bold">I</mi></mrow></mfenced></math>
- en: 'Our goal, of course, is not to drown cats in noise. On the contrary, we want
    to create many new cats! We can do so by training a model that can perform the
    *reverse process*: going from **x**[*t*] to **x**[*t*–1]. We can then use it to
    remove a tiny bit of noise from an image, and repeat the operation many times
    until all the noise is gone. If we train the model on a dataset containing many
    cat images, then we can give it a picture entirely full of Gaussian noise, and
    the model will gradually make a brand new cat appear (see [Figure 17-20](#denoising_model_diagram)).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的目标不是让猫淹没在噪音中。相反，我们想要创造许多新的猫！我们可以通过训练一个能够执行*逆过程*的模型来实现这一点：从 **x**[*t*]
    到 **x**[*t*–1]。然后我们可以使用它从图像中去除一点噪音，并重复这个操作多次，直到所有的噪音都消失。如果我们在包含许多猫图像的数据集上训练模型，那么我们可以给它一张完全充满高斯噪音的图片，模型将逐渐使一个全新的猫出现（见
    [Figure 17-20](#denoising_model_diagram)）。
- en: '![mls3 1720](assets/mls3_1720.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1720](assets/mls3_1720.png)'
- en: Figure 17-20\. The forward process *q* and reverse process *p*
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-20\. 前向过程 *q* 和逆过程 *p*
- en: OK, so let’s start coding! The first thing we need to do is to code the forward
    process. For this, we will first need to implement the variance schedule. How
    can we control how fast the cat disappears? Initially, 100% of the variance comes
    from the original cat image. Then at each time step *t*, the variance gets multiplied
    by 1 – *β*[*t*], as explained earlier, and noise gets added. So, the part of the
    variance that comes from the initial distribution shrinks by a factor of 1 – *β*[*t*]
    at each step. If we define *α*[*t*] = 1 – *β*[*t*], then after *t* time steps,
    the cat signal will have been multiplied by a factor of *α̅*[*t*] = *α*[*1*]×*α*[*2*]×…​×*α*[*t*]
    = <math><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msub><mi>α</mi><mi>t</mi></msub></math>.
    It’s this “cat signal” factor *α̅*[*t*] that we want to schedule so it shrinks
    down from 1 to 0 gradually between time steps 0 and *T*. In the improved DDPM
    paper, the authors schedule *α̅*[*t*] according to [Equation 17-7](#variance_schedule_equation).
    This schedule is represented in [Figure 17-21](#variance_schedule_plot).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们开始编码！我们需要做的第一件事是编写前向过程的代码。为此，我们首先需要实现方差计划。我们如何控制猫消失的速度？最初，100%的方差来自原始猫图像。然后在每个时间步*t*，方差会按照1
    - *β*[*t*]乘以，如前所述，并添加噪声。因此，来自初始分布的方差部分在每一步都会缩小1 - *β*[*t*]倍。如果我们定义*α*[*t*] = 1
    - *β*[*t*]，那么经过*t*个时间步骤后，猫信号将被乘以*α̅*[*t*] = *α*[*1*]×*α*[*2*]×…​×*α*[*t*] = <math><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msub><mi>α</mi><mi>t</mi></msub></math>。这个“猫信号”因子*α̅*[*t*]是我们希望安排的，使其在时间步0和*T*之间逐渐从1缩小到0。在改进的DDPM论文中，作者根据[方程17-7](#variance_schedule_equation)安排*α̅*[*t*]。这个计划在[图17-21](#variance_schedule_plot)中表示。
- en: Equation 17-7\. Variance schedule equations for the forward diffusion process
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程17-7。前向扩散过程的方差计划方程
- en: <math display="block"><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub><msub><mover><mi>α</mi><mo>¯</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mfrac><mtext>,
    with </mtext><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow><mrow><mi>f</mi><mo>(</mo><mn>0</mn><mo>)</mo></mrow></mfrac><mtext> and </mtext><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mi>cos</mi><mo>(</mo><mfrac><mrow><mi>t</mi><mo>/</mo><mi>T</mi><mo>+</mo><mi>s</mi></mrow><mrow><mn>1</mn><mo>+</mo><mi>s</mi></mrow></mfrac><mo>·</mo><mfrac><mi>π</mi><mn>2</mn></mfrac><msup><mo>)</mo><mn>2</mn></msup></math>
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub><msub><mover><mi>α</mi><mo>¯</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mfrac><mtext>，其中 </mtext><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow><mrow><mi>f</mi><mo>(</mo><mn>0</mn><mo>)</mo></mrow></mfrac><mtext> 和 </mtext><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mi>cos</mi><mo>(</mo><mfrac><mrow><mi>t</mi><mo>/</mo><mi>T</mi><mo>+</mo><mi>s</mi></mrow><mrow><mn>1</mn><mo>+</mo><mi>s</mi></mrow></mfrac><mo>·</mo><mfrac><mi>π</mi><mn>2</mn></mfrac><msup><mo>)</mo><mn>2</mn></msup></math>
- en: 'In these equations:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方程中：
- en: '*s* is a tiny value which prevents *β*[*t*] from being too small near *t* =
    0\. In the paper, the authors used *s* = 0.008.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s*是一个微小值，防止*β*[*t*]在*t* = 0附近太小。在论文中，作者使用了*s* = 0.008。'
- en: '*β*[*t*] is clipped to be no larger than 0.999, to avoid instabilities near
    *t* = *T*.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*β*[*t*]被剪切为不大于0.999，以避免在*t* = *T*附近的不稳定性。'
- en: '![mls3 1721](assets/mls3_1721.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1721](assets/mls3_1721.png)'
- en: Figure 17-21\. Noise variance schedule *β*[*t*], and the remaining signal variance
    *α̅*[*t*]
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-21。噪声方差计划*β*[*t*]，以及剩余信号方差*α̅*[*t*]
- en: 'Let’s create a small function to compute *α*[*t*], *β*[*t*], and *α̅*[*t*],
    and call it with *T* = 4,000:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个小函数来计算*α*[*t*]，*β*[*t*]和*α̅*[*t*]，并使用*T* = 4,000调用它：
- en: '[PRE28]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To train our model to reverse the diffusion process, we will need noisy images
    from different time steps of the forward process. For this, let’s create a `prepare_batch()`
    function that will take a batch of clean images from the dataset and prepare them:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型来逆转扩散过程，我们需要来自前向过程不同时间步的嘈杂图像。为此，让我们创建一个`prepare_batch()`函数，它将从数据集中获取一批干净图像并准备它们：
- en: '[PRE29]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s go through this code:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下这段代码：
- en: For simplicity we will use Fashion MNIST, so the function must first add a channel
    axis. It will also help to scale the pixel values from –1 to 1, so it’s closer
    to the final Gaussian distribution with mean 0 and variance 1.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为简单起见，我们将使用Fashion MNIST，因此函数必须首先添加一个通道轴。将像素值从-1缩放到1也会有所帮助，这样它更接近均值为0，方差为1的最终高斯分布。
- en: Next, the function creates `t`, a vector containing a random time step for each
    image in the batch, between 1 and *T*.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，该函数创建`t`，一个包含每个图像批次中随机时间步长的向量，介于1和*T*之间。
- en: Then it uses `tf.gather()` to get the value of `alpha_cumprod` for each of the
    time steps in the vector `t`. This gives us the vector `alpha_cm`, containing
    one value of *α̅*[*t*] for each image.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后它使用`tf.gather()`来获取向量`t`中每个时间步长的`alpha_cumprod`的值。这给我们了向量`alpha_cm`，其中包含每个图像的一个*α̅*[*t*]值。
- en: The next line reshapes the `alpha_cm` from [*batch size*] to [*batch size*,
    1, 1, 1]. This is needed to ensure `alpha_cm` can be broadcasted with the batch
    `X`.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一行将`alpha_cm`从[*批次大小*]重塑为[*批次大小*, 1, 1, 1]。这是为了确保`alpha_cm`可以与批次`X`进行广播。
- en: Then we generate some Gaussian noise with mean 0 and variance 1.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们生成一些均值为0，方差为1的高斯噪声。
- en: Lastly, we use [Equation 17-6](#fast_forward_process_equation) to apply the
    diffusion process to the images. Note that `x ** 0.5` is equal to the square root
    of `x`. The function returns a tuple containing the inputs and the targets. The
    inputs are represented as a Python `dict` containing the noisy images and the
    time steps used to generate them. The targets are the Gaussian noise used to generate
    each image.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用[方程17-6](#fast_forward_process_equation)将扩散过程应用于图像。请注意，`x ** 0.5`等于`x`的平方根。该函数返回一个包含输入和目标的元组。输入表示为一个Python
    `dict`，其中包含嘈杂图像和用于生成每个图像的时间步。目标是用于生成每个图像的高斯噪声。
- en: Note
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'With this setup, the model will predict the noise that should be subtracted
    from the input image to get the original image. Why not predict the original image
    directly? Well, the authors tried: it simply doesn’t work as well.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种设置，模型将预测应从输入图像中减去的噪声，以获得原始图像。为什么不直接预测原始图像呢？嗯，作者尝试过：它简单地效果不如预期。
- en: 'Next, we’ll create a training dataset and a validation set that will apply
    the `prepare_batch()` function to every batch. As earlier, `X_train` and `X_valid`
    contain the Fashion MNIST images with pixel values ranging from 0 to 1:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个训练数据集和一个验证集，将`prepare_batch()`函数应用于每个批次。与之前一样，`X_train`和`X_valid`包含像素值从0到1的时尚MNIST图像：
- en: '[PRE30]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we’re ready to build the actual diffusion model itself. It can be any model
    you want, as long as it takes the noisy images and time steps as inputs, and predicts
    the noise to subtract from the input images:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备构建实际的扩散模型本身。它可以是您想要的任何模型，只要它将嘈杂的图像和时间步骤作为输入，并预测应从输入图像中减去的噪声：
- en: '[PRE31]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The DDPM authors used a modified [U-Net architecture](https://homl.info/unet),⁠^([22](ch17.html#idm45720166835536))
    which has many similarities with the FCN architecture we discussed in [Chapter 14](ch14.html#cnn_chapter)
    for semantic segmentation: it’s a convolutional neural network that gradually
    downsamples the input images, then gradually upsamples them again, with skip connections
    crossing over from each level of the downsampling part to the corresponding level
    in the upsampling part. To take into account the time steps, they encoded them
    using the same technique as the positional encodings in the transformer architecture
    (see [Chapter 16](ch16.html#nlp_chapter)). At every level in the U-Net architecture,
    they passed these time encodings through `Dense` layers and fed them to the U-Net.
    Lastly, they also used multi-head attention layers at various levels. See this
    chapter’s notebook for a basic implementation, or [*https://homl.info/ddpmcode*](https://homl.info/ddpmcode)
    for the official implementation: it’s based on TF 1.x, which is deprecated, but
    it’s quite readable.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM的作者使用了一个修改过的[U-Net架构](https://homl.info/unet)，它与我们在[第14章](ch14.html#cnn_chapter)中讨论的FCN架构有许多相似之处，用于语义分割：它是一个卷积神经网络，逐渐对输入图像进行下采样，然后再逐渐对其进行上采样，跳跃连接从每个下采样部分的每个级别跨越到相应的上采样部分的级别。为了考虑时间步长，他们使用了与变压器架构中的位置编码相同的技术对其进行编码（参见[第16章](ch16.html#nlp_chapter)）。在U-Net架构的每个级别上，他们通过`Dense`层传递这些时间编码，并将它们馈送到U-Net中。最后，他们还在各个级别使用了多头注意力层。查看本章的笔记本以获取基本实现，或者[*https://homl.info/ddpmcode*](https://homl.info/ddpmcode)以获取官方实现：它基于已弃用的TF
    1.x，但非常易读。
- en: 'WE can now train the model normally. The authors noted that using the MAE loss
    worked better than the MSE. You can also use the Huber loss:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以正常训练模型了。作者指出，使用MAE损失比MSE效果更好。您也可以使用Huber损失：
- en: '[PRE32]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Once the model is trained, you can use it to generate new images. Unfortunately,
    there’s no shortcut in the reverse diffusion process, so you have to sample **x**[*T*]
    randomly from a Gaussian distribution with mean 0 and variance 1, then pass it
    to the model to predict the noise; subtract it from the image using [Equation
    17-8](#reverse_diffusion_equation), and you get **x**[*T*–1]. Repeat the process
    3,999 more times until you get **x**[0]: if all went well, it should look like
    a regular Fashion MNIST image!'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，您可以使用它生成新图像。不幸的是，在反向扩散过程中没有捷径，因此您必须从均值为0，方差为1的高斯分布中随机抽样**x**[*T*]，然后将其传递给模型预测噪声；使用[方程17-8](#reverse_diffusion_equation)从图像中减去它，然后您会得到**x**[*T*–1]。重复这个过程3999次，直到得到**x**[0]：如果一切顺利，它应该看起来像一个常规的时尚MNIST图像！
- en: Equation 17-8\. Going one step in reverse in the diffusion process
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程17-8。在扩散过程中向后走一步
- en: <math display="block"><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mn>1</mn><msqrt><msub><mi>α</mi><mi>t</mi></msub></msqrt></mfrac><mfenced><mrow><msub><mi
    mathvariant="bold">x</mi><mi>t</mi></msub><mo>-</mo><mfrac><msub><mi>β</mi><mi>t</mi></msub><msqrt><mn>1</mn><mo>-</mo><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub></msqrt></mfrac><msub><mi
    mathvariant="bold">ϵ</mi><mo mathvariant="bold">θ</mo></msub><mo>(</mo><msub><mi
    mathvariant="bold">x</mi><mi>t</mi></msub><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mfenced><mo>+</mo><msqrt><msub><mi>β</mi><mi>t</mi></msub></msqrt><mi
    mathvariant="bold">z</mi></math>
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mn>1</mn><msqrt><msub><mi>α</mi><mi>t</mi></msub></msqrt></mfrac><mfenced><mrow><msub><mi
    mathvariant="bold">x</mi><mi>t</mi></msub><mo>-</mo><mfrac><msub><mi>β</mi><mi>t</mi></msub><msqrt><mn>1</mn><mo>-</mo><msub><mover><mi>α</mi><mo>¯</mo></mover><mi>t</mi></msub></msqrt></mfrac><msub><mi
    mathvariant="bold">ϵ</mi><mo mathvariant="bold">θ</mo></msub><mo>(</mo><msub><mi
    mathvariant="bold">x</mi><mi>t</mi></msub><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mfenced><mo>+</mo><msqrt><msub><mi>β</mi><mi>t</mi></msub></msqrt><mi
    mathvariant="bold">z</mi></math>
- en: 'In this equation, **ϵ[θ]**(**x**[*t*], *t*) represents the noise predicted
    by the model given the input image **x**[*t*] and the time step *t*. The **θ**
    represents the model parameters. Moreover, **z** is Gaussian noise with mean 0
    and variance 1\. This makes the reverse process stochastic: if you run it multiple
    times, you will get different images.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，**ϵ[θ]**(**x**[*t*], *t*)代表模型给定输入图像**x**[*t*]和时间步长*t*预测的噪声。**θ**代表模型参数。此外，**z**是均值为0，方差为1的高斯噪声。这使得反向过程是随机的：如果您多次运行它，您将得到不同的图像。
- en: 'Let’s write a function that implements this reverse process, and call it to
    generate a few images:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个实现这个反向过程的函数，并调用它生成一些图像：
- en: '[PRE33]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This may take a minute or two. That’s the main drawback of diffusion models:
    generating images is slow since the model needs to be called many times. It’s
    possible to make this faster by using a smaller *T* value, or by using the same
    model prediction for several steps at a time, but the resulting images may not
    look as nice. That said, despite this speed limitation, diffusion models do produce
    high-quality and diverse images, as you can see in [Figure 17-22](#ddpm_generated_images_plot).'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一两分钟。这是扩散模型的主要缺点：生成图像很慢，因为模型需要被多次调用。通过使用较小的*T*值或者同时使用相同模型预测多个步骤，可以加快这一过程，但生成的图像可能不那么漂亮。尽管存在这种速度限制，扩散模型确实生成高质量和多样化的图像，正如您在[图17-22](#ddpm_generated_images_plot)中所看到的。
- en: '![mls3 1722](assets/mls3_1722.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1722](assets/mls3_1722.png)'
- en: Figure 17-22\. Images generated by the DDPM
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-22。DDPM生成的图像
- en: Diffusion models have made tremendous progress recently. In particular, a paper
    published in December 2021 by [Robin Rombach, Andreas Blattmann, et al.](https://homl.info/latentdiff),⁠^([23](ch17.html#idm45720166470944))
    introduced *latent diffusion models*, where the diffusion process takes place
    in latent space, rather than in pixel space. To achieve this, a powerful autoencoder
    is used to compress each training image into a much smaller latent space, where
    the diffusion process takes place, then the autoencoder is used to decompress
    the final latent representation, generating the output image. This considerably
    speeds up image generation, and reduces training time and cost dramatically. Importantly,
    the quality of the generated images is outstanding.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，扩散模型取得了巨大的进展。特别是，2021年12月由[Robin Rombach, Andreas Blattmann等人](https://homl.info/latentdiff)发表的一篇论文⁠^([23](ch17.html#idm45720166470944))介绍了*潜在扩散模型*，其中扩散过程发生在潜在空间，而不是在像素空间中。为了实现这一点，使用强大的自动编码器将每个训练图像压缩到一个更小的潜在空间中，扩散过程发生在这里，然后自动编码器用于解压缩最终的潜在表示，生成输出图像。这极大地加快了图像生成速度，大大减少了训练时间和成本。重要的是，生成的图像质量非常出色。
- en: Moreover, the researchers also adapted various conditioning techniques to guide
    the diffusion process using text prompts, images, or any other inputs. This makes
    it possible to quickly produce a beautiful, high-resolution image of a salamander
    reading a book, or anything else you might fancy. You can also condition the image
    generation process using an input image. This enables many applications, such
    as outpainting—where an input image is extended beyond its borders—or inpainting—where
    holes in an image are filled in.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究人员还采用了各种调节技术来引导扩散过程，使用文本提示、图像或任何其他输入。这使得快速生成一个漂亮的高分辨率图像成为可能，比如一只读书的蝾螈，或者你可能喜欢的其他任何东西。您还可以使用输入图像来调节图像生成过程。这使得许多应用成为可能，比如外部绘制——在输入图像的边界之外扩展——或内部绘制——填充图像中的空洞。
- en: Lastly, a powerful pretrained latent diffusion model named *Stable Diffusion*
    was open sourced in August 2022 by a collaboration between LMU Munich and a few
    companies, including StabilityAI, and Runway, with support from EleutherAI and
    LAION. In September 2022, it was ported to TensorFlow and included in [KerasCV](https://keras.io/keras_cv),
    a computer vision library built by the Keras team. Now anyone can generate mindblowing
    images in seconds, for free, even on a regular laptop (see the last exercise in
    this chapter). The possibilities are endless!
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个名为*稳定扩散*的强大预训练潜在扩散模型于2022年8月由慕尼黑大学LMU与包括StabilityAI和Runway在内的几家公司合作开源，得到了EleutherAI和LAION的支持。2022年9月，它被移植到TensorFlow，并包含在[KerasCV](https://keras.io/keras_cv)中，这是由Keras团队构建的计算机视觉库。现在任何人都可以在几秒钟内免费生成令人惊叹的图像，即使是在普通笔记本电脑上（请参阅本章的最后一个练习）。可能性是无限的！
- en: 'In the next chapter we will move on to an entirely different branch of deep
    learning: deep reinforcement learning.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转向深度强化学习的一个完全不同的分支。
- en: Exercises
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What are the main tasks that autoencoders are used for?
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动编码器主要用于哪些任务？
- en: Suppose you want to train a classifier, and you have plenty of unlabeled training
    data but only a few thousand labeled instances. How can autoencoders help? How
    would you proceed?
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你想要训练一个分类器，你有大量未标记的训练数据，但只有几千个标记实例。自动编码器如何帮助？你会如何继续？
- en: If an autoencoder perfectly reconstructs the inputs, is it necessarily a good
    autoencoder? How can you evaluate the performance of an autoencoder?
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果自动编码器完美地重建输入，它一定是一个好的自动编码器吗？你如何评估自动编码器的性能？
- en: What are undercomplete and overcomplete autoencoders? What is the main risk
    of an excessively undercomplete autoencoder? What about the main risk of an overcomplete
    autoencoder?
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是欠完备和过完备自动编码器？过度欠完备自动编码器的主要风险是什么？过度完备自动编码器的主要风险又是什么？
- en: How do you tie weights in a stacked autoencoder? What is the point of doing
    so?
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在堆叠自动编码器中绑定权重？这样做的目的是什么？
- en: What is a generative model? Can you name a type of generative autoencoder?
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是生成模型？你能说出一种生成自动编码器吗？
- en: What is a GAN? Can you name a few tasks where GANs can shine?
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是GAN？你能说出几个GAN可以发挥作用的任务吗？
- en: What are the main difficulties when training GANs?
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练GAN时的主要困难是什么？
- en: What are diffusion models good at? What is their main limitation?
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩散模型擅长什么？它们的主要限制是什么？
- en: 'Try using a denoising autoencoder to pretrain an image classifier. You can
    use MNIST (the simplest option), or a more complex image dataset such as [CIFAR10](https://homl.info/122)
    if you want a bigger challenge. Regardless of the dataset you’re using, follow
    these steps:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用去噪自动编码器预训练图像分类器。您可以使用MNIST（最简单的选项），或者如果您想要更大的挑战，可以使用更复杂的图像数据集，比如[CIFAR10](https://homl.info/122)。无论您使用的数据集是什么，都要遵循以下步骤：
- en: Split the dataset into a training set and a test set. Train a deep denoising
    autoencoder on the full training set.
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分割成训练集和测试集。在完整的训练集上训练一个深度去噪自动编码器。
- en: Check that the images are fairly well reconstructed. Visualize the images that
    most activate each neuron in the coding layer.
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查图像是否被相当好地重建。可视化激活编码层中每个神经元的图像。
- en: Build a classification DNN, reusing the lower layers of the autoencoder. Train
    it using only 500 images from the training set. Does it perform better with or
    without pretraining?
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个分类DNN，重用自动编码器的较低层。仅使用训练集中的500张图像进行训练。它在有无预训练的情况下表现更好吗？
- en: Train a variational autoencoder on the image dataset of your choice, and use
    it to generate images. Alternatively, you can try to find an unlabeled dataset
    that you are interested in and see if you can generate new samples.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您选择的图像数据集上训练一个变分自动编码器，并使用它生成图像。或者，您可以尝试找到一个您感兴趣的无标签数据集，看看是否可以生成新样本。
- en: Train a DCGAN to tackle the image dataset of your choice, and use it to generate
    images. Add experience replay and see if this helps. Turn it into a conditional
    GAN where you can control the generated class.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个DCGAN来处理您选择的图像数据集，并使用它生成图像。添加经验重放，看看这是否有帮助。将其转换为条件GAN，您可以控制生成的类别。
- en: Go through KerasCV’s excellent [Stable Diffusion tutorial](https://homl.info/sdtuto),
    and generate a beautiful drawing of a salamander reading a book. If you post your
    best drawing on Twitter, please tag me at @aureliengeron. I’d love to see your
    creations!
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浏览KerasCV出色的[稳定扩散教程](https://homl.info/sdtuto)，并生成一幅漂亮的图画，展示一只读书的蝾螈。如果您在Twitter上发布您最好的图画，请在@
    aureliengeron处标记我。我很想看看您的创作！
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可在本章笔记本的末尾找到，网址为[*https://homl.info/colab3*](https://homl.info/colab3)。
- en: '^([1](ch17.html#idm45720171598896-marker)) William G. Chase and Herbert A.
    Simon, “Perception in Chess”, *Cognitive Psychology* 4, no. 1 (1973): 55–81.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch17.html#idm45720171598896-marker)) William G. Chase和Herbert A. Simon，“国际象棋中的感知”，*认知心理学*
    4，第1期（1973年）：55-81。
- en: '^([2](ch17.html#idm45720170477280-marker)) Yoshua Bengio et al., “Greedy Layer-Wise
    Training of Deep Networks”, *Proceedings of the 19th International Conference
    on Neural Information Processing Systems* (2006): 153–160.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch17.html#idm45720170477280-marker)) Yoshua Bengio等，“深度网络的贪婪逐层训练”，*第19届神经信息处理系统国际会议论文集*（2006）：153-160。
- en: '^([3](ch17.html#idm45720170465088-marker)) Jonathan Masci et al., “Stacked
    Convolutional Auto-Encoders for Hierarchical Feature Extraction”, *Proceedings
    of the 21st International Conference on Artificial Neural Networks* 1 (2011):
    52–59.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch17.html#idm45720170465088-marker)) Jonathan Masci等，“用于分层特征提取的堆叠卷积自动编码器”，*第21届国际人工神经网络会议论文集*
    1（2011）：52-59。
- en: '^([4](ch17.html#idm45720170108336-marker)) Pascal Vincent et al., “Extracting
    and Composing Robust Features with Denoising Autoencoders”, *Proceedings of the
    25th International Conference on Machine Learning* (2008): 1096–1103.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch17.html#idm45720170108336-marker)) Pascal Vincent等，“使用去噪自动编码器提取和组合稳健特征”，*第25届国际机器学习会议论文集*（2008）：1096-1103。
- en: '^([5](ch17.html#idm45720170106320-marker)) Pascal Vincent et al., “Stacked
    Denoising Autoencoders: Learning Useful Representations in a Deep Network with
    a Local Denoising Criterion”, *Journal of Machine Learning Research* 11 (2010):
    3371–3408.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch17.html#idm45720170106320-marker)) Pascal Vincent等，“堆叠去噪自动编码器：使用局部去噪标准在深度网络中学习有用的表示”，*机器学习研究杂志*
    11（2010）：3371-3408。
- en: ^([6](ch17.html#idm45720169388400-marker)) Diederik Kingma and Max Welling,
    “Auto-Encoding Variational Bayes”, arXiv preprint arXiv:1312.6114 (2013).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch17.html#idm45720169388400-marker)) Diederik Kingma和Max Welling，“自动编码变分贝叶斯”，arXiv预印本arXiv:1312.6114（2013）。
- en: ^([7](ch17.html#idm45720169367824-marker)) Variational autoencoders are actually
    more general; the codings are not limited to Gaussian distributions.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch17.html#idm45720169367824-marker)) 变分自动编码器实际上更通用；编码不限于高斯分布。
- en: ^([8](ch17.html#idm45720169359232-marker)) For more mathematical details, check
    out the original paper on variational autoencoders, or Carl Doersch’s [great tutorial](https://homl.info/116)
    (2016).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch17.html#idm45720169359232-marker)) 要了解更多数学细节，请查看有关变分自动编码器的原始论文，或查看Carl
    Doersch的[优秀教程](https://homl.info/116)（2016）。
- en: '^([9](ch17.html#idm45720168607168-marker)) Ian Goodfellow et al., “Generative
    Adversarial Nets”, *Proceedings of the 27th International Conference on Neural
    Information Processing Systems* 2 (2014): 2672–2680.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch17.html#idm45720168607168-marker)) Ian Goodfellow等，“生成对抗网络”，*第27届神经信息处理系统国际会议论文集*
    2（2014）：2672-2680。
- en: ^([10](ch17.html#idm45720167972816-marker)) For a nice comparison of the main
    GAN losses, check out this great [GitHub project by Hwalsuk Lee](https://homl.info/ganloss).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch17.html#idm45720167972816-marker)) 要了解主要GAN损失的良好比较，请查看Hwalsuk Lee的这个[GitHub项目](https://homl.info/ganloss)。
- en: '^([11](ch17.html#idm45720167970816-marker)) Mario Lucic et al., “Are GANs Created
    Equal? A Large-Scale Study”, *Proceedings of the 32nd International Conference
    on Neural Information Processing Systems* (2018): 698–707.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch17.html#idm45720167970816-marker)) Mario Lucic等，“GAN是否平等？大规模研究”，*第32届神经信息处理系统国际会议论文集*（2018）：698-707。
- en: ^([12](ch17.html#idm45720167956496-marker)) Alec Radford et al., “Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks”,
    arXiv preprint arXiv:1511.06434 (2015).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch17.html#idm45720167956496-marker)) Alec Radford等，“使用深度卷积生成对抗网络进行无监督表示学习”，arXiv预印本arXiv:1511.06434（2015）。
- en: ^([13](ch17.html#idm45720167572992-marker)) Reproduced with the kind authorization
    of the authors.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch17.html#idm45720167572992-marker)) 在作者的亲切授权下再现。
- en: ^([14](ch17.html#idm45720167569680-marker)) Mehdi Mirza and Simon Osindero,
    “Conditional Generative Adversarial Nets”, arXiv preprint arXiv:1411.1784 (2014).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch17.html#idm45720167569680-marker)) Mehdi Mirza和Simon Osindero，“有条件生成对抗网络”，arXiv预印本arXiv:1411.1784（2014）。
- en: ^([15](ch17.html#idm45720167566480-marker)) Tero Karras et al., “Progressive
    Growing of GANs for Improved Quality, Stability, and Variation”, *Proceedings
    of the International Conference on Learning Representations* (2018).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch17.html#idm45720167566480-marker)) Tero Karras等，“用于改善质量、稳定性和变化的GAN的渐进增长”，*国际学习表示会议论文集*（2018）。
- en: ^([16](ch17.html#idm45720167545296-marker)) The dynamic range of a variable
    is the ratio between the highest and the lowest value it may take.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch17.html#idm45720167545296-marker)) 变量的动态范围是其可能取的最高值和最低值之间的比率。
- en: ^([17](ch17.html#idm45720167535792-marker)) Tero Karras et al., “A Style-Based
    Generator Architecture for Generative Adversarial Networks”, arXiv preprint arXiv:1812.04948
    (2018).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch17.html#idm45720167535792-marker)) Tero Karras等人，“基于风格的生成对抗网络架构”，arXiv预印本arXiv:1812.04948（2018）。
- en: ^([18](ch17.html#idm45720167519152-marker)) Reproduced with the kind authorization
    of the authors.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch17.html#idm45720167519152-marker)) 在作者的亲切授权下复制。
- en: ^([19](ch17.html#idm45720167499824-marker)) Jascha Sohl-Dickstein et al., “Deep
    Unsupervised Learning using Nonequilibrium Thermodynamics”, arXiv preprint arXiv:1503.03585
    (2015).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch17.html#idm45720167499824-marker)) Jascha Sohl-Dickstein等人，“使用非平衡热力学进行深度无监督学习”，arXiv预印本arXiv:1503.03585（2015）。
- en: ^([20](ch17.html#idm45720167493280-marker)) Jonathan Ho et al., “Denoising Diffusion
    Probabilistic Models” (2020).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch17.html#idm45720167493280-marker)) Jonathan Ho等人，“去噪扩散概率模型”（2020）。
- en: ^([21](ch17.html#idm45720167489824-marker)) Alex Nichol and Prafulla Dhariwal,
    “Improved Denoising Diffusion Probabilistic Models” (2021).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch17.html#idm45720167489824-marker)) Alex Nichol和Prafulla Dhariwal，“改进的去噪扩散概率模型”（2021）。
- en: '^([22](ch17.html#idm45720166835536-marker)) Olaf Ronneberger et al., “U-Net:
    Convolutional Networks for Biomedical Image Segmentation”, arXiv preprint arXiv:1505.04597
    (2015).'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch17.html#idm45720166835536-marker)) Olaf Ronneberger等人，“U-Net：用于生物医学图像分割的卷积网络”，arXiv预印本arXiv:1505.04597（2015）。
- en: ^([23](ch17.html#idm45720166470944-marker)) Robin Rombach, Andreas Blattmann,
    et al., “High-Resolution Image Synthesis with Latent Diffusion Models”, arXiv
    preprint arXiv:2112.10752 (2021).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch17.html#idm45720166470944-marker)) Robin Rombach，Andreas Blattmann等人，“使用潜在扩散模型进行高分辨率图像合成”，arXiv预印本arXiv:2112.10752（2021）。
