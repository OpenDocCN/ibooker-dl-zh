["```py\nimport torch\nfrom torch.distributions.multivariate_normal \\\n  import MultivariateNormal\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nimport torch.optim as optim\n\n```", "```py\n# Encoder layers (Gaussian MLP)\nD_in, H, D_out = 784, 200, 20\ninput_layer = nn.Linear(D_in, H)\nhidden_layer_mean = nn.Linear(H, D_out)\nhidden_layer_var = nn.Linear(H, D_out)\n\n```", "```py\n# Decoder layers (Bernoulli MLP for MNIST data)\nrecon_layer = nn.Linear(D_out, H)\nrecon_output = nn.Linear(H, D_in)\n\n```", "```py\nclass VAE(nn.Module):\n  def __init__(self, D_in, H, D_out):\n    super(VAE, self).__init__()\n    self.D_in, self.H, self.D_out = D_in, H, D_out\n\n    # Encoder layers (Gaussian MLP)\n    self.input_layer = nn.Linear(D_in, H)\n    self.hidden_layer_mean = nn.Linear(H, D_out)\n    self.hidden_layer_var = nn.Linear(H, D_out)\n\n    # Decoder layers (Bernoulli MLP for MNIST data)\n    self.recon_layer = nn.Linear(D_out, H)\n    self.recon_output = nn.Linear(H, D_in)\n    self.tanh = nn.Tanh()\n    self.sigmoid = nn.Sigmoid()\n\n  def encode(self, inp):\n    h_vec = self.input_layer(inp)\n    h_vec = self.sigmoid(h_vec)\n    means = self.hidden_layer_mean(h_vec)\n    log_vars = self.hidden_layer_var(h_vec)\n    return means, log_vars\n\n  def decode(self, means, log_vars):\n    # Reparametrization trick\n    std_devs = torch.pow(2,log_vars)**0.5\n    aux = MultivariateNormal(torch.zeros(self.D_out), \\\n    torch.eye(self.D_out)).sample()\n    sample = means + aux * std_devs\n\n    # Reconstruction\n    h_vec = self.recon_layer(sample)\n    h_vec = self.tanh(h_vec)\n    output = self.sigmoid(self.recon_output(h_vec))\n    return output\n\n  def forward(self, inp):\n    means, log_vars = self.encode(inp)\n    output = self.decode(means, log_vars)\n    return output, means, log_vars\n\n  def reconstruct(self, sample):\n    h_vec = self.recon_layer(sample)\n    h_vec = self.tanh(h_vec)\n    output = self.sigmoid(self.recon_output(h_vec))\n    return output\n\n```", "```py\ndef compute_loss(inp, recon_inp, means, log_vars):\n  # Calculate reverse KL divergence\n  # (formula provided in Kingma and Welling)\n  kl_loss = -0.5 * torch.sum(1 + log_vars\n                            - means ** 2 - torch.pow(2,log_vars))\n\n  # Calculate BCE loss\n  loss = nn.BCELoss(reduction=\"sum\")\n  recon_loss = loss(recon_inp, inp)\n  return kl_loss + recon_loss\n\n```", "```py\nD_in, H, D_out = 784, 500, 20\nvae = VAE(D_in, H, D_out)\nvae.to(\"cpu\")\n\ndef train():\n  vae.train()\n  optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n\n  train_loader = torch.utils.data.DataLoader(\n      datasets.MNIST('../data',\n                     train=True,\n                     download=True,\n                     transform=transforms.ToTensor()),\n                     batch_size=100,\n                     shuffle=True)\n\n  epochs = 10\n  for epoch in range(epochs):\n    for batch_idx, (data, _) in enumerate(train_loader):\n      optimizer.zero_grad()\n      data = data.view((100,784))\n      output, means, log_vars = vae(data)\n      loss = compute_loss(data, output, means, log_vars)\n      loss.backward()\n      optimizer.step()\n      if (batch_idx * len(data)) % 10000 == 0:\n        print(\n            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}' \\\n            .format(\n            epoch, batch_idx * len(data), len(train_loader.dataset),\n            100\\. * batch_idx / len(train_loader), loss.item()))\n  torch.save(vae.state_dict(), \"vae.%d\" % epoch)\n\n```", "```py\ndef test():\n  dist = MultivariateNormal(torch.zeros(D_out), torch.eye(D_out))\n  vae = VAE(D_in, H, D_out)\n  vae.load_state_dict(torch.load(\"vae.%d\" % 9))\n  vae.eval()\n  outputs = []\n\n  for i in range(100):\n    sample = dist.sample()\n    outputs.append(vae.reconstruct(sample).view((1,1,28,28)))\n  outputs = torch.stack(outputs).view(100,1,28,28)\n  save_image(outputs, \"prior_reconstruct_100.png\", nrow=10)\n\n```"]