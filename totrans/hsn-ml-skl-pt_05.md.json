["```py\nimport numpy as np\n\nrng = np.random.default_rng(seed=42)\nm = 200  # number of instances\nX = 2 * rng.random((m, 1))  # column vector\ny = 4 + 3 * X + rng.standard_normal((m, 1))  # column vector\n```", "```py\nfrom sklearn.preprocessing import add_dummy_feature\n\nX_b = add_dummy_feature(X)  # add x0 = 1 to each instance\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n```", "```py\n>>> theta_best\narray([[3.69084138],\n [3.32960458]])\n```", "```py\n>>> X_new = np.array([[0], [2]])\n>>> X_new_b = add_dummy_feature(X_new)  # add x0 = 1 to each instance\n>>> y_predict = X_new_b @ theta_best\n>>> y_predict\narray([[ 3.69084138],\n [10.35005055]])\n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\nplt.plot(X, y, \"b.\")\n[...]  # beautify the figure: add labels, axis, grid, and legend\nplt.show()\n```", "```py\n>>> from sklearn.linear_model import LinearRegression\n>>> lin_reg = LinearRegression()\n>>> lin_reg.fit(X, y)\n>>> lin_reg.intercept_, lin_reg.coef_\n(array([3.69084138]), array([[3.32960458]]))\n>>> lin_reg.predict(X_new)\narray([[ 3.69084138],\n [10.35005055]])\n```", "```py\n>>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n>>> theta_best_svd\narray([[3.69084138],\n [3.32960458]])\n```", "```py\n>>> np.linalg.pinv(X_b) @ y\narray([[3.69084138],\n [3.32960458]])\n```", "```py\neta = 0.1  # learning rate\nn_epochs = 1000\nm = len(X_b)  # number of instances\n\nrng = np.random.default_rng(seed=42)\ntheta = rng.standard_normal((2, 1))  # randomly initialized model parameters\n\nfor epoch in range(n_epochs):\n    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n    theta = theta - eta * gradients\n```", "```py\n>>> theta\narray([[3.69084138],\n [3.32960458]])\n```", "```py\nn_epochs = 50\nt0, t1 = 5, 50  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nrng = np.random.default_rng(seed=42)\ntheta = rng.standard_normal((2, 1))  # randomly initialized model parameters\n\nfor epoch in range(n_epochs):\n    for iteration in range(m):\n        random_index = rng.integers(m)\n        xi = X_b[random_index : random_index + 1]\n        yi = y[random_index : random_index + 1]\n        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n        eta = learning_schedule(epoch * m + iteration)\n        theta = theta - eta * gradients\n```", "```py\n>>> theta\narray([[3.69826475],\n [3.30748311]])\n```", "```py\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n                       n_iter_no_change=100, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n```", "```py\n>>> sgd_reg.intercept_, sgd_reg.coef_\n(array([3.68899733]), array([3.33054574]))\n```", "```py\nrng = np.random.default_rng(seed=42)\nm = 200  # number of instances\nX = 6 * rng.random((m, 1)) - 3\ny = 0.5 * X ** 2 + X + 2 + rng.standard_normal((m, 1))\n```", "```py\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> poly_features = PolynomialFeatures(degree=2, include_bias=False)\n>>> X_poly = poly_features.fit_transform(X)\n>>> X[0]\narray([1.64373629])\n>>> X_poly[0]\narray([1.64373629, 2.701869  ])\n```", "```py\n>>> lin_reg = LinearRegression()\n>>> lin_reg.fit(X_poly, y)\n>>> lin_reg.intercept_, lin_reg.coef_\n(array([2.00540719]), array([[1.11022126, 0.50526985]]))\n```", "```py\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, valid_scores = learning_curve(\n    LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n    scoring=\"neg_root_mean_squared_error\")\ntrain_errors = -train_scores.mean(axis=1)\nvalid_errors = -valid_scores.mean(axis=1)\n\nplt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\nplt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n[...]  # beautify the figure: add labels, axis, grid, and legend\nplt.show()\n```", "```py\nfrom sklearn.pipeline import make_pipeline\n\npolynomial_regression = make_pipeline(\n    PolynomialFeatures(degree=10, include_bias=False),\n    LinearRegression())\n\ntrain_sizes, train_scores, valid_scores = learning_curve(\n    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n    scoring=\"neg_root_mean_squared_error\")\n[...]  # same as earlier\n```", "```py\n>>> from sklearn.linear_model import Ridge\n>>> ridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\n>>> ridge_reg.fit(X, y)\n>>> ridge_reg.predict([[1.5]])\narray([1.84414523])\n```", "```py\n>>> sgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n...                        max_iter=1000, eta0=0.01, random_state=42)\n...\n>>> sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n>>> sgd_reg.predict([[1.5]])\narray([1.83659707])\n```", "```py\n>>> from sklearn.linear_model import Lasso\n>>> lasso_reg = Lasso(alpha=0.1)\n>>> lasso_reg.fit(X, y)\n>>> lasso_reg.predict([[1.5]])\narray([1.87550211])\n```", "```py\n>>> from sklearn.linear_model import ElasticNet\n>>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n>>> elastic_net.fit(X, y)\n>>> elastic_net.predict([[1.5]])\narray([1.8645014])\n```", "```py\nfrom copy import deepcopy\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, y_train, X_valid, y_valid = [...]  # split the quadratic dataset\n\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n                              StandardScaler())\nX_train_prep = preprocessing.fit_transform(X_train)\nX_valid_prep = preprocessing.transform(X_valid)\nsgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\nn_epochs = 500\nbest_valid_rmse = float('inf')\n\nfor epoch in range(n_epochs):\n    sgd_reg.partial_fit(X_train_prep, y_train)\n    y_valid_predict = sgd_reg.predict(X_valid_prep)\n    val_error = root_mean_squared_error(y_valid, y_valid_predict)\n    if val_error < best_valid_rmse:\n        best_valid_rmse = val_error\n        best_model = deepcopy(sgd_reg)\n```", "```py\n>>> from sklearn.datasets import load_iris\n>>> iris = load_iris(as_frame=True)\n>>> list(iris)\n['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names',\n 'filename', 'data_module']\n>>> iris.data.head(3)\n sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n>>> iris.target.head(3)  # note that the instances are not shuffled\n0    0\n1    0\n2    0\nName: target, dtype: int64\n>>> iris.target_names\narray(['setosa', 'versicolor', 'virginica'], dtype='<U10')\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)\n```", "```py\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\ny_proba = log_reg.predict_proba(X_new)\ndecision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]\n\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n         label=\"Not Iris virginica proba\")\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\nplt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n         label=\"Decision boundary\")\n[...] # beautify the figure: add grid, labels, axis, legend, arrows, and samples\nplt.show()\n```", "```py\n>>> decision_boundary\nnp.float64(1.6516516516516517)\n>>> log_reg.predict([[1.7], [1.5]])\narray([ True, False])\n```", "```py\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax_reg = LogisticRegression(C=30, random_state=42)\nsoftmax_reg.fit(X_train, y_train)\n```", "```py\n>>> softmax_reg.predict([[5, 2]])\narray([2])\n>>> softmax_reg.predict_proba([[5, 2]]).round(2)\narray([[0\\.  , 0.04, 0.96]])\n```"]