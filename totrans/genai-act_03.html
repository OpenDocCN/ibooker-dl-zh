<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">3</span> </span> <span class="chapter-title-text">Working through an API: Generating text</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Generative AI models and their categorization based on specific applications</li> 
    <li class="readable-text" id="p3">The process of listing available models, understanding their capabilities, and choosing the appropriate ones</li> 
    <li class="readable-text" id="p4">The completion API and chat completion API offered by OpenAI</li> 
    <li class="readable-text" id="p5">Advanced options for completion and chat completion APIs that help us steer the model and hence control the generation</li> 
    <li class="readable-text" id="p6">The importance of managing tokens in a conversation for improved user experience and cost-effectiveness</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>We have seen that large language models (LLMs) provide a powerful suite of machine learning tools specifically designed to enhance natural language understanding and generation. OpenAI features two notable APIs: the completion and the chat completion APIs. These APIs, unique in their dynamic and effective text-generation capabilities, resemble human output. In addition, they offer developers exclusive opportunities to craft various applications, from chatbots to writing assistants. OpenAI was the first to introduce the pattern of completion and chat completion APIs, which now embody almost all implementations, especially when companies want to build generative-AI-powered tools and products.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>The completion API by OpenAI is an advanced tool that generates contextually appropriate and coherent text to complete user prompts. Conversely, the chat completion API was designed to emulate an interaction with a machine learning model, preserving the context of a conversation across multiple exchanges, which makes it suitable for interactive applications.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>Chapter 3 establishes the groundwork for scaling enterprises. These APIs can significantly accelerate the development of intelligent applications, thereby reducing the time to value. We’ll mostly use OpenAI and Azure OpenAI as illustrative examples, often interchangeably. The code models remain consistent, and the APIs are largely similar. Many enterprises may gravitate toward Azure OpenAI because of the control it offers, while others might favor OpenAI. It is important to note that we assume here that an Azure OpenAI instance has already been deployed as part of your Azure subscription, and we will be referencing it in the context of our examples.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>This chapter outlines the basics of the completion and the chat completion APIs, including how they differ and when to use each. We will see how to implement them in an application and how we can steer the model generation and its randomness. We’ll also see how to manage tokens, which are key operation considerations when deploying to production. These are the fundamental aspects required to build on for a mission-critical application. But first, let’s start by understanding the different model categories and their advantages.</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_42"><span class="num-string">3.1</span> Model categories</h2> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Generative AI models can be classified into various categories based on their specific applications, and each category includes different types of models. We start our discussion by understanding the different classifications of models within generative AI. This understanding will help us identify the range of models available and choose the most appropriate one for a given situation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>The availability of different types and models may vary, depending on the API in use. For example, Azure OpenAI and OpenAI provide different versions of LLMs. Some versions might be phased out, some could be limited, and others could be exclusive to a certain organization.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>Different models have unique features and capabilities, directly affecting their cost and computational requirements. Thus, choosing the right model for each use case is critical. In conventional computer science, the idea that bigger is better has often been applied to memory, storage, CPUs, or bandwidth. However, in the case of LLMs, this principle is not always applicable. OpenAI provides a host of models categorized, as shown in table 3.1. Note that these are the same for both OpenAI and Azure OpenAI, as the underlying models are identical.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p15"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 3.1</span> OpenAI model categories</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Model category 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  GPT-4 <br/></td> 
      <td>  The newest and most powerful version is a set of multimodal models. GPT-4 is trained on a larger dataset with more parameters, making it even more capable. It can perform tasks that are out of reach for the previous models. There are various models in the GPT-4 family—GPT-4.0, GPT-4 Turbo, and the latest GPT-4o (omni), a multimodal model and the most powerful in the family at the time of publication. <br/></td> 
     </tr> 
     <tr> 
      <td>  GPT-3.5 <br/></td> 
      <td>  A set of models that improve on GPT-3 and can understand and generate natural language or code. When unsure, these should be the default models for most enterprises. <br/></td> 
     </tr> 
     <tr> 
      <td>  DALL.E <br/></td> 
      <td>  A model that can generate images when given a prompt <br/></td> 
     </tr> 
     <tr> 
      <td>  Whisper <br/></td> 
      <td>  A model that is used for speech-to-text, converting audio into text <br/></td> 
     </tr> 
     <tr> 
      <td>  Embeddings <br/></td> 
      <td>  A set of models to convert text into its numerical form <br/></td> 
     </tr> 
     <tr> 
      <td>  GPT-3 (Legacy) <br/></td> 
      <td>  A set of models that can generate and understand natural language. These were the original set of models that are now considered legacy. In most cases, we would want to start with one of the newer models, 3.5 or 4.0, which derive from GPT-3. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>Each model category contains variations that are further distinguished by certain features such as token size. As discussed in the previous chapter, token size determines a model’s context window, which defines the amount of input and output it can process. For instance, the original GPT-3 models had a maximum token size of 2K. GPT-3.5 Turbo, a subset of models within the GPT-3.5 category, has two versions—one with a token size of 4K and another with a token size of 16K. These are double and quadruple the token size of the original GPT-3 models. Table 3.2 outlines the more popular models and their capabilities.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p17"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 3.2</span> Model descriptions and capabilities</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Model 
       </div></th> 
      <th> 
       <div>
         Capabilities 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Ada (legacy) <br/></td> 
      <td>  Simple classification, parsing, and formatting of text. This model is part of the GPT-3 legacy. <br/></td> 
     </tr> 
     <tr> 
      <td>  Babbage (legacy) <br/></td> 
      <td>  Semantic search ranking, medium complex classification. This model is part of the GPT-3 legacy. <br/></td> 
     </tr> 
     <tr> 
      <td>  Curie (legacy) <br/></td> 
      <td>  Answering questions, highly complex classification. This model is part of the GPT-3 legacy. <br/></td> 
     </tr> 
     <tr> 
      <td>  Davinci (legacy) <br/></td> 
      <td>  Summarization, generating creative content. This model is part of the GPT-3 legacy. <br/></td> 
     </tr> 
     <tr> 
      <td>  Cushman-Codex (legacy) <br/></td> 
      <td>  A descendant of the GPT-3 series, trained in natural language and billions of lines of code. It is the most capable in Python and proficient in over a dozen other programming languages. <br/></td> 
     </tr> 
     <tr> 
      <td>  Davinci-Codex <br/></td> 
      <td>  A more capable model of Cushman-codex <br/></td> 
     </tr> 
     <tr> 
      <td>  GPT3.5-Turbo <br/></td> 
      <td>  The most capable GPT-3.5 model optimized for chat use cases is 90% cheaper and more effective than GPT-3 Davinci. <br/></td> 
     </tr> 
     <tr> 
      <td>  GPT-4, GPT-4 Turbo <br/></td> 
      <td>  More capable than any GPT-3.5 model. It is able to do more complex tasks and is optimized for chat models. <br/></td> 
     </tr> 
     <tr> 
      <td>  GPT-4o <br/></td> 
      <td>  The latest GPT-4o model is more capable than the GPT-4 and GPT-4 Turbo, but it is also twice as fast and 50% cheaper. <br/></td> 
     </tr> 
     <tr> 
      <td>  text-embedding-ada-002, text-embedding-ada-003 <br/></td> 
      <td>  This new embedding model replaces five separate models for text search, similarity, and code search, outperforming them at most tasks; furthermore, it is 99.8% cheaper. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>Note that the mentioned legacy models are still available and work as intended. However, the newer models are better, having more mindshare and longer support. Most should start with GPT-3.5 Turbo as the default model and use GPT-4 on a case-by-case basis. Sometimes, even a smaller, older model, such as the GPT-3 Curie, is good. This provides the right balance between the model’s capability, cost, and overall performance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>In the early days of generative AI, all the models were available only to some. These will vary by company, region, and in the case of Azure, your subscription type, among other things. We have to list the models and their capabilities that are available for us to use. However, before listing models, let us see the dependencies required to get things working.</p> 
  </div> 
  <div class="readable-text" id="p20"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_43"><span class="num-string">3.1.1</span> Dependencies</h3> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>In this section, we call out the run time dependencies and configurations needed at a high level. To get things working, we need at least the following items:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p22"> <em>Development IDE</em><em> </em>—We use Visual Studio Code for our examples, but you can use anything you are comfortable with. </li> 
   <li class="readable-text" id="p23"> <em>Python</em><em> </em>—We use v3.11.3 in this book, but you can use any version as long as it is v3.7.1 or later. The installation instructions are available at <a href="https://www.python.org/">https://www.python.org/</a> if you need to install Python. </li> 
   <li class="readable-text" id="p24"> <em>OpenAI Python libraries</em><em> </em><em>—</em>We use Python libraries for most of the code and the demos. The OpenAI Python library can be a simple installation in conda, using <code>conda</code> <code>install</code> <code>-c</code> <code>conda-forge</code> <code>openai</code>. If you are using pip, use <code>pip</code> <code>install --upgrade openai</code>. There are also software development kits (SDKs) for specific languages if you prefer to use those instead of Python packages. </li> 
   <li class="readable-text" id="p25"> <em>Azure Subscription or OpenAI API access</em><em> </em><em>—</em>We use OpenAI’s endpoint and the Azure OpenAI (AOAI) endpoint interchangeably; in most cases, either option will work. Given the emphasis on enterprises for this book, we tend to lean toward using the Azure OpenAI service: 
    <ul> 
     <li> To use the library with Azure endpoints, we need the <code>api_key</code>. </li> 
     <li> We also need to set the <code>api_type</code>, <code>api_base</code>, and <code>api_version</code> properties. The <code>api_type</code> must be set to <code>azure</code>, the <code>api_base</code> points to the endpoint that we deploy, and the corresponding version of the API is specified via <code>api_version</code>. </li> 
     <li> Azure OpenAI uses '<code>engine'</code> as the parameter to specify the model’s name. When deploying the model in your Azure subscription, this name needs to be set to your chosen name. For example, figure 3.1 is a screenshot of the deployments in one subscription. OpenAI, however, uses the parameter <code>model</code> to specify the model’s name. These model names are standard as they release them. You can find more details on Azure OpenAI and OpenAI at <a href="https://mng.bz/yoYd">https://mng.bz/yoYd</a> and <a href="https://platform.openai.com/docs/">https://platform.openai.com/docs/</a>. </li> 
    </ul></li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p26"> 
   <p><span class="print-book-callout-head">Note </span> The GitHub code repository accompanying the book (<a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>) has the details of the code, including dependencies and instructions.</p> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>Hardcoding the endpoint and key is not an advisable practice. There are multiple methods to accomplish this task, one of which includes using environment variables. We demonstrate this method in the steps that follow. Other alternatives could be fetching them from secret stores or environment files. For the sake of simplicity, we will stick to environment variables in this guide. However, you are encouraged to adhere to your enterprise’s best practices and recommendations. Setting up the environment variables can be achieved through the following commands.</p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>For Windows, these are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p29"> 
   <div class="code-area-container"> 
    <pre class="code-area">setx AOAI_KEY "your-openai-key"
setx AOAI_ENDPOINT "your-openai-endpoint"</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p30"> 
   <p><span class="print-book-callout-head">Note</span>  You may need to restart your terminal to read the new variables.</p> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>On Linux/Mac, we have</p> 
  </div> 
  <div class="browsable-container listing-container" id="p32"> 
   <div class="code-area-container"> 
    <pre class="code-area">export AOAI_ENDPOINT=your-openai-endpoint
export AOAI_KEY=your-openai- key</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>Bash uses</p> 
  </div> 
  <div class="browsable-container listing-container" id="p34"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">echo export AOAI_KEY="YOUR_KEY" &gt;&gt; /etc/environment &amp;&amp; source /etc/environment
echo export AOAI_ENDPOINT="YOUR_ENDPOINT" &gt;&gt; /etc/environment &amp;&amp; 
      <span class="">↪</span>source /etc/environment</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p35"> 
   <p><span class="print-book-callout-head">Note </span> In this book, we will use conda, an open source package manager, to manage our specific runtime versions and dependencies. Technically, using a package manager like conda is not mandatory, but it is extremely beneficial for isolating and troubleshooting problems and is highly recommended. We won’t delve into the specifics of installing conda in this context; for detailed, step-by-step instructions on how to install it, please refer to the official documentation at <a href="https://docs.conda.io/">https://docs.conda.io/</a>.</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>First, let us create a new conda environment and install the required OpenAI Python library:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p37"> 
   <div class="code-area-container"> 
    <pre class="code-area">$ conda create -n openai python=3.11.3
(base) $ conda activate openai
(openai) $ conda install -c conda-forge openai</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>Now that we have our dependencies installed, let’s connect to the Azure OpenAI endpoint and get details of the available models.</p> 
  </div> 
  <div class="readable-text" id="p39"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_44"><span class="num-string">3.1.2</span> Listing models</h3> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>As we outlined earlier, each organization may have different models for use. We’ll start by understanding what models we have access to; we’ll use the APIs to help us set up the basic environment and get it running. Then, I’ll show you how to do this using the Azure OpenAI Python SDK and outline the differences when using the OpenAI API. </p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>As the next listing shows, we connect to the Azure OpenAI endpoint, get a list of all the models available, iterate over those, and print out the details of each model to the console.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p42"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.1</span> Listing Azure OpenAI models available</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import os
import json
from openai import AzureOpenAI                 <span class="aframe-location"/> #1

client = AzureOpenAI(
    azure_endpoint=os.getenv("AOAI_ENDPOINT"), <span class="aframe-location"/> #2
    api_version="2023-05-15",                  <span class="aframe-location"/> #3
    api_key=os.getenv("AOAI_KEY")  <span class="aframe-location"/> #4
    )

# Call the models API to retrieve a list of available models
models = client.models.list()

# save to file
with open('azure-oai-models.json', 'w') as file:
    models_dict = [model.__dict__ for model in models]
    json.dump(models_dict, file)

# Print out the names of all the available models, and their capabilities
for model in models:
    print("ID:", model.id)
    print("Current status:", model.lifecycle_status)
    print("Model capabilities:", model.capabilities)
    print("-------------------")</pre> 
    <div class="code-annotations-overlay-container">
     #1 Required for Azure OpenAI endpoints
     <br/>#2 This is the environment variable pointing to the endpoint published via the Azure portal.
     <br/>#3 Choose the API version we want to use from the multiple options.
     <br/>#4 This is the environment variable with the API key.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>Running this code will present us with a list of available models. The following listing shows an example of the models available; the exact list may be different for you. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p44"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.2</span> Listing Azure OpenAI models’ output</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
    "id": "gpt-4-vision-preview",
    "created": null,
    "object": "model",
    "owned_by": null
},
{
    "id": "dall-e-3",
    "created": null,
    "object": "model",
    "owned_by": null
  },
  {
    "id": "gpt-35-turbo",
    "created": null,
    "object": "model",
    "owned_by": null
},
…</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>Each model is characterized by its distinct capabilities, suggesting the use cases for which it is tailored—specifically for chat completions, completions (which are regular text completions), embeddings, and fine-tuning. For example, a chat completion model would be the ideal selection in a situation where conversational engagement is required, like a chat-based interaction that requires significant dialogue exchange. Conversely, a completion model would be the most suitable for text generation. We can view the OpenAI base models with Azure AI Studio in figure 3.1.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p46">  
   <img alt="figure" src="../Images/CH03_F01_Bahree.png" width="828" height="517"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.1</span> Base model listed</h5>
  </div> 
  <div class="readable-text" id="p47"> 
   <p>This feature is part of Azure AI Studio, which you can access when logging into your Azure subscription and accessing your Azure OpenAI deployment. You can also access it directly via the portal at <a href="https://oai.azure.com/portal">https://oai.azure.com/portal</a>. Now that we know which model to use, let’s generate some text. We’ll use the completion API and a model that supports completions.</p> 
  </div> 
  <div class="readable-text" id="p48"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_45"><span class="num-string">3.2</span> Completion API</h2> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>The completion API is a sophisticated tool that generates text to complete prompts provided by the user. It forms the backbone of the OpenAI API and offers a simple yet robust and flexible API. It is designed to produce text that is coherent and contextually fitting for the given prompt.</p> 
  </div> 
  <div class="readable-text intended-text" id="p50"> 
   <p>Many generation examples that are not chat-type constructs use the completion API. We must use the completion API to generate text that is not a chat-style conversation. Some of the benefits of completion API are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p51"> <em>Contextual understanding</em><em> </em>—The completion API can understand the context of the prompt and generate relevant text. </li> 
   <li class="readable-text" id="p52"> <em>Versatility</em><em> </em>—It can be used in various applications, from creating content to answering questions, which makes it a valuable tool for multiple applications. </li> 
   <li class="readable-text" id="p53"> <em>Multiple language understanding</em><em> </em>—The completion API can understand and generate content in several languages, which makes it a global resource. </li> 
   <li class="readable-text" id="p54"> <em>Easy implementation</em><em> </em>—The completion API is straightforward, which makes it accessible to developers of various skill levels. </li> 
  </ul> 
  <div class="readable-text" id="p55"> 
   <p>The API’s structure is quite simple, as shown in the following snippet. The input (prompt) and the output (completion) are in text format. The API response is a JSON object from which the generated text can be extracted using the text key. This response is called text completion. The completion strives to adhere to the instructions and context provided in the prompt and is one of the potential outputs:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p56"> 
   <div class="code-area-container"> 
    <pre class="code-area">from openai import OpenAI
client = OpenAI()

response = client.completions.create(
  model="gpt-3.5-turbo-instruct",
  prompt="Write a few bullets on why pets are so awesome ",
  max_tokens=100,
  temperature=0.8 
)
print(response.choices[0].text.strip())</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>We start with an instruction, which is the prompt that specifies what we aim to generate. In our example, the instruction asks the model to generate a few bullets outlining why pets are awesome. The completion API has numerous parameters, but the most essential ones are detailed in table 3.3. We discussed many other parameters earlier in this chapter and the book (e.g., the prompt, tokens, and temperatures). The stop sequences, however, are a new concept. We can employ these sequences to make the model cease generating tokens at a certain point, such as at the end of a sentence or a list.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p58"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 3.3</span> Completion API</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Parameter 
       </div></th> 
      <th> 
       <div>
         Type 
       </div></th> 
      <th> 
       <div>
         Default value 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  <code>prompt</code> <br/></td> 
      <td>  String or array <br/></td> 
      <td>  <code>&lt;\|endoftext\|&gt;</code> <br/></td> 
      <td>  A string or an array of strings is the prompt used to generate these completions. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>max_tokens</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  16 <br/></td> 
      <td>  This is the maximum number of tokens to generate in the completion, including the prompt. The <code>max_tokens</code> must not exceed the model’s context length.  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>temperature</code> <br/></td> 
      <td>  Number (float) <br/></td> 
      <td>  1 <br/></td> 
      <td>  This ranges between 0 and 2. Higher values mean the model takes more risks and gets more creative.  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>stop</code> <br/></td> 
      <td>  String or array <br/></td> 
      <td>  Null <br/></td> 
      <td>  This can be up to four sequences where the API stops generating further tokens. The returned text will not contain the stop sequence. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>n</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  1 (optional) <br/></td> 
      <td>  This defines the number of completions to generate for each prompt. This generates many completions and can quickly consume the token limit; we should have a reasonable setting for <code>max_tokens</code> and stop managing cost.  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>stream</code> <br/></td> 
      <td>  Boolean <br/></td> 
      <td>  False (optional) <br/></td> 
      <td>  This is a flag controlling whether to stream back partial progress as tokens are generated. If set, the stream is terminated by a data <code>[DONE]</code>message. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>best_of</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  1 (optional) <br/></td> 
      <td>  This generates <code>best_of</code> completions server-side and returns the best completion. This parameter cannot be used with gpt-35-turbo.  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>top_p</code> <br/></td> 
      <td>  Number (float) <br/></td> 
      <td>  1 (optional) <br/></td> 
      <td>  This controls randomness using a technique called nucleus sampling, an alternative to the <code>temperature</code> setting with a value ranging between 0 and 1.  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>logit_bias</code> <br/></td> 
      <td>  Map <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  This defines the likelihood of specified tokens appearing in the completion. It uses a mapping of tokens to a bias value (–100 of a ban to 100 of exclusive selection).  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>user</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  This parameter is a unique ID representing the end-user; it can help debug, monitor, and detect abuse.  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>logprobs</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  This is an optional array of log probabilities representing the alternate tokens and their likelihood considered for completion. This parameter cannot be used with gpt-35-turbo.  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>suffix</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  This parameter can be a string of up to 40 characters added as a suffix to the generated text.  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>echo</code> <br/></td> 
      <td>  Boolean <br/></td> 
      <td>  False (optional) <br/></td> 
      <td>  This determines whether the prompt is included in the completion. This is useful for use cases that need to capture the prompts and for debugging purposes. It cannot be used with gpt-35-turbo.  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>presence_ penalty</code> <br/></td> 
      <td>  Number (float) <br/></td> 
      <td>  0 (optional) <br/></td> 
      <td>  This parameter steers the model’s tendency and helps outline its behavior to introduce new topics or ideas into the generated text. It ranges from 0.0 to 1.0. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>frequency_ penalty</code> <br/></td> 
      <td>  Number (float) <br/></td> 
      <td>  0 (optional) <br/></td> 
      <td>  This is another parameter that helps steer the model and improve the generation results. It controls the level of common or uncommon words in the generated text and can be set to a value from 0.0 to 1.0. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>function_ call</code> <br/></td> 
      <td/> 
      <td/> 
      <td>  This controls how the model responds to functions when function calling is desired. It only works with 0613 or newer versions of the OpenAI models.  <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>functions</code> <br/></td> 
      <td/> 
      <td/> 
      <td>  This is a list of functions that the model may use. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>Note that the table only lists the most used parameters. It helps us understand some of the flows and concepts. Some parameters, such as functions, have more advanced uses, which will be covered in later chapters on prompt engineering.</p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>We stick with the pets theme and use the model to help us suggest names for a pet salon business. We ask for three names, and the instructions also outline some of the important characteristics to use. These aspects of the instructions help us steer the model toward some desired attributes. Please refer to the API documentation for a full list of parameters. Let’s call the completion API and walk through it.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p61"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.3</span> Calling the completion API</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import os
from openai import AzureOpenAI

client = AzureOpenAI(
    azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    api_version="2024-05-01-preview",
    api_key=os.getenv("AOAI_KEY"))

prompt_startphrase = "Suggest three names for a new pet salon business.
<span class="">↪</span>The generated name ideas should evoke positive emotions and the 
<span class="">↪</span>following key features: Professional, friendly, Personalized Service."

response = client.completions.create(       <span class="aframe-location"/> #1
    model="gpt35",   <span class="aframe-location"/> #2
   <span class="aframe-location"/> prompt=prompt_startphrase,                #3


    temperature=0.7,                         <span class="aframe-location"/> #4
    max_tokens=100,                         
    suffix="\nThats all folks!",            
    stop=None)

responsetext = response.choices[0].text     <span class="aframe-location"/> #5

print("Prompt:" + prompt_startphrase + "\nResponse:" + responsetext)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Completion API call for generating text
     <br/>#2 Specifies the model to use; note that this name will change based on what you set in the deployment
     <br/>#3 Prompt
     <br/>#4 Model configurations
     <br/>#5 Extracts the generated text from the response
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>Congratulations! We used the API for our first text generation. Because of the nondeterministic nature of AI, especially generative AI, the output you will see when running this differs from</p> 
  </div> 
  <div class="browsable-container listing-container" id="p63"> 
   <div class="code-area-container"> 
    <pre class="code-area">$ python .\petsalon.py</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>The output is as follows.</p> 
  </div> 
  <div class="readable-text prompt" id="p65"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Prompt.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Suggest three names for a new pet salon business. The generated name ideas should evoke positive emotions and the following key features: professional, friendly, personalized service.</p> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p66"><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong>1. <strong><span class="aframe-location"/></strong>Pawsitively Professional Pet Salon </li> 
  </ol> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p67"><span class="faux-ol-li-counter">2. </span> Fur &amp; Feathers Friendly Pet Parlor </li> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p68"><span class="faux-ol-li-counter">3. </span> Happy Tails Personalized Pet Pampering </li> 
  </ol> 
  <div class="readable-text print-book-callout" id="p69"> 
   <p><span class="print-book-callout-head">Note </span> LLMs and most other generative AI models are nondeterministic, meaning that identical inputs could give different outputs. Changing the temperature setting to zero can make the outputs more deterministic, but a small amount of variability may remain.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_46"><span class="num-string">3.2.1</span> Expanding completions</h3> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Let’s see what a complete response from the API looks like and walk through that structure. The following listing shows the full response from the API. The <code>choices</code> field is among the most interesting, given that it has the completion text. The choices property is an array, where each item has an <code>index</code>, the reason the generation finished (<code>finish_reason</code>), and the generated text (via the <code>text</code> property).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p72"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.4</span> API response from a completion API</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">{
  "choices": [                     <span class="aframe-location"/> #1
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "text": "\n\n1. Pawfect Professionals \n 
              <span class="">↪</span>2. Purrsonalized Pet Pampering\n 
              <span class="">↪</span>3. Friendly Fur Services",
              "content_filter_results"={...}
    }
  ],
  "created": 1689007663,                      <span class="aframe-location"/> #2
  "id": "cmpl-7aoL1MaUEf2j3ZLfSvsUOR7EFyjqC",   <span class="aframe-location"/> #3
  "model": "gpt-35-turbo",                <span class="aframe-location"/> #4
  "object": "text_completion",
  "usage": {               <span class="aframe-location"/> #5
    "completion_tokens": 26,
    "prompt_tokens": 32,
    "total_tokens": 58
  }
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 Array of completion data
     <br/>#2 Response creation datetime stamp
     <br/>#3 Unique ID of the response
     <br/>#4 Model ID used to generate the response
     <br/>#5 Count of tokens used in this request
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>Table 3.4 shows the remaining properties. The usage property outlines the tokens used (<code>total_tokens</code>), including the prompt and response tokens. Because we pay per token, it is important to structure the prompt for aspects—first, to return only what is needed, minimizing token usage, and second, to limit the number of tokens generated in the first place.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p74"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 3.4</span> Completion response properties</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Property 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  <code>choices</code> <br/></td> 
      <td>  An array that can contain one or more completions data <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>created</code> <br/></td> 
      <td>  UNIX date-time stamp when the response was created <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>id</code> <br/></td> 
      <td>  A unique identifier of the response is useful when we need to track responses <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>model</code> <br/></td> 
      <td>  Represents the model that was used for the generation <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>object</code> <br/></td> 
      <td>  Outlines the data type of the response (e.g., in this case, it is a <code>text_completion</code>, outlining a completion API) <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>usage</code> <br/></td> 
      <td>  Counts the number of tokens used by this request <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>A property called <code>logprobs</code> specifies the number of log probabilities to generate for each token in the response. The log probabilities are useful for generating more diverse and interesting responses. It returns the log probabilities of the top <em>n</em> tokens for each token in the response. The log probabilities are returned as an array of arrays, where each subarray corresponds to a token in the response and contains the log probabilities of the top <em>n</em> tokens for that token.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_47"><span class="num-string">3.2.2</span> Azure content safety filter</h3> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>Sometimes, the API returns a <code>null</code> response, as shown in listing 3.5. When this happens, we should check the value of the <code>finish_reason</code> field. If its value is set to <code>content_filter</code>, the content filtering system that works alongside models has been triggered. The <code>finish_reason</code> field indicates why the API returned the output it did, and every response will include this field. This topic will be covered in more detail later in the chapter.</p> 
  </div> 
  <div class="readable-text intended-text" id="p78"> 
   <p>The filtering system uses specific categories to identify and act on potentially harmful content as part of both the input prompts and generated completions. The application that uses these APIs must handle this situation and retry after the appropriate back-off period. The content safety filter and ethical AI will be covered in more detail in chapter 13.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p79"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.5</span> Output showing <code>null</code> response</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">$ python .\petsalon.py 
Prompt:Suggest three names for a new pet salon business. The generated 
       <span class="">↪</span>name ideas should evoke positive emotions and the following key 
       <span class="">↪</span>features: Professional, friendly, Personalized Service.
Response:                                   <span class="aframe-location"/> #1
{
  "choices": [
    {
      "finish_reason": "content_filter",     <span class="aframe-location"/> #2
      "index": 0,
      "logprobs": null,
      "text": "",
      "content_filter_results"={...}
    }
  ],
  "created": 1689006467,
  "id": "cmpl-7ao1jIACW1v8mYH879EE1trbT9Ua6",
  "model": "gpt35",
  "object": "text_completion",
  "usage": {
    "completion_tokens": 31,
    "prompt_tokens": 32,
    "total_tokens": 63
  }
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 null response
     <br/>#2 Content filter is the reason the response finished.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p80"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_48"><span class="num-string">3.2.3</span> Multiple completions</h3> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>We might want multiple completions for a few reasons. Sometimes, we need to generate multiple message choices for the same prompt. At other times, the API is throttled for capacity reasons, and we might want to get more from the same API call instead of being rate limited. The completions API can return multiple responses; this is done by setting the <code>n</code> parameter to more than the default value of 1. For example, we can add this parameter to the completion call:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p82"> 
   <div class="code-area-container"> 
    <pre class="code-area">response = client.completions.create(
    model="gpt-35-turbo",
    prompt=prompt_startphrase,
    temperature=0.7,
    max_tokens=100,
    n=3,
    stop=None)

# loop through the response choices
for choice in response.choices:
    print(choice.text)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>When we run this updated code, we get the response shown in listing 3.6. The property choices are an array, and we have three items, with the index starting at a base zero. Each has the generated text for us to use. Depending on the use case, this is helpful when picking multiple completions.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p84"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.6</span> Output showing multiple responses</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">1. Pet Pampering Palace
2. Pet Grooming Haven
3. Perfect Pet Parlor


1. Pawsitive Pet Spa
2. Fur-Ever Friends Pet Salon
3. Purrfection Pet Care


1. Pampered Paws Professional Pet Care
2. Personalized Pet Pampering
3. Friendly Furrific Pet Care</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>Another similar but more powerful parameter is the <code>best_of</code> parameter. Like the <code>n</code> parameter, it generates multiple completions, allowing the option to pick the best. The <code>best_of</code> is the completion with the highest log probability per token. We cannot stream results when using this option. However, it can be combined with the <code>n</code> parameters, with <code>best_of</code> needs greater than <code>n</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p86"> 
   <p>As shown in the following listing, if we set <code>n</code> to 5, we get five completions as expected; for brevity, we do not show all five of the completions here, but note that this call uses 184 tokens.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p87"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.7</span> Output showing multiple responses</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
  "choices": [
    {
      …
  ],
  "created": 1689097645,
  "id": "cmpl-7bBkLk60mA8R9crAKXqTmTwzx2IEI",
  "model": "gpt-35-turbo",
  "object": "text_completion",
  "usage": {
    "completion_tokens": 152,
    "prompt_tokens": 32,
    "total_tokens": 184
  }
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>If we run a similar call using the <code>best_of</code> parameter, do not specify the <code>n</code> parameter: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p89"> 
   <div class="code-area-container"> 
    <pre class="code-area">response = client.completions.create(
    model="gpt-35-turbo",
    prompt=prompt_startphrase,
    temperature=0.7,
    max_tokens=100,
    best_of=5,
    stop=None)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>When we run this code, we get only one completion, as shown in listing 3.8; however, we are using a similar number of tokens as earlier (171 versus 184). This is because the service generates five completions on the server side and returns the best one. The API uses the log probability per token to pick the best option. The higher the log probability, the more confident the model is about its prediction.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p91"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.8</span> Output generation with <code>best_of</code> five completions</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,

      "text": "\n\n1. Pawsitively Professional Pet Salon\n 
         <span class="">↪</span>2. Friendly Furr Friends Pet Salon\n 
         <span class="">↪</span>3. Personalized Pampered Pets Salon",
               "content_filter_results"={...}
    }
  ],
  "created": 1689098048,
  "id": "cmpl-7bBqqpfuoV5nrgHrahuWGVAiM50Aj",
  "model": "gpt35",
  "object": "text_completion",
  "usage": {
    "completion_tokens": 139,
    "prompt_tokens": 32,
    "total_tokens": 171
  }
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>The one parameter that influences many of the responses is the temperature setting. Let’s see how this changes the output.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_49"><span class="num-string">3.2.4</span> Controlling randomness</h3> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>As discussed in the previous chapter, the <code>temperature</code> setting influences the randomness of the generated output. A lower temperature produces more repetitive and deterministic responses, while a higher temperature produces more innovative responses. Fundamentally, there isn’t a right setting—it all comes down to the use cases.</p> 
  </div> 
  <div class="readable-text intended-text" id="p95"> 
   <p>For enterprises, a more creative output would be when there is interest in diverse output and creating text for use cases such as content generation for marketing, stories, poems, lyrics, jokes, etc. These are things that usually require creativity. However, enterprises need more reliable and precise answers for use cases, such as document automation for invoice generation, proposals, code generation, etc. These settings are applicable per API call, so combining different temperature levels in the same workflow is possible.</p> 
  </div> 
  <div class="readable-text intended-text" id="p96"> 
   <p>As demonstrated in previous examples, we recommend a temperature setting of 0.8 for creative responses. Conversely, a setting of 0.2 is suggested for more predictable responses. Using an example, let us examine how these settings alter the output and observe the variations between multiple calls.</p> 
  </div> 
  <div class="readable-text intended-text" id="p97"> 
   <p>When the temperature was set to 0.8, we received the following responses from three consecutive calls. The output changes as expected, offering suggestions like those seen throughout this chapter. It is important to note that we do not need to make three separate API calls. We can set the <code>n</code> parameter to 3 in a single API call to generate multiple responses. Here is what our API call looks like:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p98"> 
   <div class="code-area-container"> 
    <pre class="code-area">response = client.completions.create(
    model="gpt-35-turbo",
    prompt=prompt_startphrase,
    temperature=0.8,
    max_tokens=100,
    n=3,
    stop=None)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>The following listing shows the creative generation for the three responses.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p100"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.9</span> Completions output with the temperature at 0.8</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">{
  "choices": [
    {
      "finish_reason": "content_filter",
      "index": 0,           <span class="aframe-location"/> #1
      "logprobs": null,
      "text": "",
      "content_filter_results"={...}
    },
    {
      "finish_reason": "stop",
      "index": 1,           <span class="aframe-location"/> #2
      "logprobs": null,
      "text": "\n\n1. Pawsitively Professional Pet Styling\n 
                 <span class="">↪</span>2. Fur-Ever Friendly Pet Groomers \n 
                 <span class="">↪</span>3. Tailored TLC Pet Care",
      "content_filter_results"={...}
    },
    {
      "finish_reason": "stop",
      "index": 2,           <span class="aframe-location"/> #3
      "logprobs": null,
      "text": "\n\n1. Pawsitively Professional Pet Salon \n 
                 <span class="">↪</span>2. Friendly Fur-ternity Pet Care \n 
                 <span class="">↪</span>3. Personalized Pup Pampering Place",
      "content_filter_results"={...}
    }
  ],
  "created": 1689123394,
  "id": "cmpl-7bIRe6Ponn8y1198flJFfagq64r2E",
  "model": "gpt35",
  "object": "text_completion",
  "usage": {
    "completion_tokens": 96,
    "prompt_tokens": 32,
    "total_tokens": 128
  }
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 First response: get blocked by the content filter
     <br/>#2 Second of three responses
     <br/>#3 Final response with very different generated text
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>Let’s change the setting to make this more deterministic and run it again. Note that the only change in the API call is <code>temperature=0.2</code>. The output is predictable and deterministic, with very similar text generated between the three responses.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p102"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.10</span> Completions output with the temperature at 0.2</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,               <span class="aframe-location"/> #1
      "logprobs": null,
      "text": "\n\n1. Pawsitively Professional Pet Salon\n 
                 <span class="">↪</span>2. Friendly Furr Salon\n 
                 <span class="">↪</span>3. Personalized Pet Pampering",
      "content_filter_results"={...}
    },
    {
      "finish_reason": "stop",
      "index": 1,               <span class="aframe-location"/> #2
      "logprobs": null,
      "text": "\n\n1. Pawsitively Professional Pet Salon\n 
                 <span class="">↪</span>2. Friendly Fur-Ever Pet Salon\n 
                 <span class="">↪</span>3. Personalized Pet Pampering Salon",
      "content_filter_results"={...}
    },
    {
      "finish_reason": "stop",
      "index": 2,              <span class="aframe-location"/> #3
      "logprobs": null,
      "text": "\n\n1. Pampered Paws Pet Salon\n 
                 <span class="">↪</span>2. Friendly Fur Salon\n
                 <span class="">↪</span>3. Professional Pet Pampering"
    }
  ],
  ...
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 One of three responses
     <br/>#2 Two of three responses; very similar generated text
     <br/>#3 The final response with very similar generated text
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>The temperature value goes up to 2, but it is not recommended to go that high, as the model starts hallucinating more and creating nonsensical text. If we want more creativity, we usually want it to be at 0.8 and, at most, 1.2. Let us see an example when the temperature is changed to 1.8. In this example, we did not even get the third generation, as we hit the token limit and stopped the generation. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p104"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.11</span> Completions output with the temperature at 1.8</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,               <span class="aframe-location"/> #1
      "logprobs": null,
      "text": "\n\n1. ComfortGroom Pet Furnishing \n2. Pampered TreaBankant Carers \n3. Toptech Sunny Haven Promotion.",
      "content_filter_results"={...}
    },
    {
      "finish_reason": "stop",
      "index": 1,              <span class="aframe-location"/> #2
      "logprobs": null,
      "text": "\n\n1: Naturalistov ClearlywowGroomingz 
                    <span class="">↪</span>Pet Luxusia \n2: VipalMinderers Pet 
                    <span class="">↪</span>Starencatines grooming \n3: Brisasia 
                    <span class="">↪</span>Crownsnus Take Care Buddsroshesipalising",
      "content_filter_results"={...}
    },
    {
      "finish_reason": "length",
      "index": 2,
      "logprobs": null,
      "text": "\n\n1. TrustowStar Pet Salon\n 
              <span class="">↪</span>2. Hartipad TailTagz Grooming &amp; Styles\n 
              <span class="">↪</span>3. LittleLoft Millonista Cosmania DipSavez 
                 <span class="">↪</span>Hubopolis ShineBright Princessly 
                 <span class="">↪</span>Prosnoiffarianistics Kensoph Cowlosophy 
                 <span class="">↪</span>Expressionala Navixfordti Mundulante Effority 
                 <span class="">↪</span>DivineSponn BordloveDV EnityzBFA Prestageinato 
                 <span class="">↪</span>SuperGold Cloutoilyna Critinarillies 
                 <span class="">↪</span>Prochromomumphance Toud",
                 <span class="">↪</span>"content_filter_results"={...}
    }
  ],
  ...
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 One of three responses with names that aren’t very clear
     <br/>#2 Second and third of three responses, with nonsensical names
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p105"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_50"><span class="num-string">3.2.5</span> Controlling randomness using top_p</h3> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>An alternative to the <code>temperature</code> parameter for managing randomness is the <code>top_p</code> parameter. It has the same affect on the generation as the temperature parameter, but it uses a different technique called <em>nucleus sampling</em>. Essentially, nucleus sampling allows only the tokens with a probability equal to or less than the value of <code>top_p</code> to be considered as part of the generation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p107"> 
   <p>Nucleus sampling creates texts by picking words from a small group of the most likely ones with the highest cumulative probability. The <code>top_p</code> value decides how small this group is based on the total chance for the words to appear in it. The group size can change depending on the next word’s chance. Nucleus sampling can help avoid repetition and generate more varied and clearer texts than other methods.</p> 
  </div> 
  <div class="readable-text intended-text" id="p108"> 
   <p>For example, if we have the <code>top_p</code> value set to 0.9, only the tokens that make up 90% of the probability distribution will be sampled for the generation of text. This allows us to avoid the last 10%, which are often quite random and diverse and end up as nonsensical hallucinations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p109"> 
   <p>A lower value of <code>top_p</code> makes the model more consistent and less creative as it chooses fewer tokens to generate. Conversely, a higher value makes the generation more creative and diverse, as it has a larger set of tokens to operate. The larger value also makes it prone to more errors and randomness. The exact value of <code>top_p</code> depends on the use case; in most cases, the ideal value for <code>top_p</code> ranges between 0.7 and 0.95. We should change either the temperature attribute or <code>top_p</code>, but not both. Table 3.5 outlines the relationship between the two.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p110"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 3.5</span> Relationship between temperature and <code>top_p</code></h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Temperature 
       </div></th> 
      <th> 
       <div>
         top_p 
       </div></th> 
      <th> 
       <div>
         Effect 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Low <br/></td> 
      <td>  Low <br/></td> 
      <td>  Generates predictable text that closely follows common language patterns <br/></td> 
     </tr> 
     <tr> 
      <td>  Low <br/></td> 
      <td>  High <br/></td> 
      <td>  Generates predictable text, but with occasional less common words or phrases <br/></td> 
     </tr> 
     <tr> 
      <td>  High <br/></td> 
      <td>  Low <br/></td> 
      <td>  Generates text that is often coherent but with creative and unexpected word usage <br/></td> 
     </tr> 
     <tr> 
      <td>  High <br/></td> 
      <td>  High <br/></td> 
      <td>  Generates highly diverse and unpredictable text with various word choices and ideas; has very creative and diverse output, but may contain many errors <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p111"> 
   <p>Let us look at some of the advanced API options for specific scenarios.</p> 
  </div> 
  <div class="readable-text" id="p112"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_51"><span class="num-string">3.3</span> Advanced completion API options</h2> 
  </div> 
  <div class="readable-text" id="p113"> 
   <p>Now that we have examined the basic constructs of the completion API and understand how they work, we need to consider more advanced aspects of the completion API. Many of these might not seem as complex, but they add many more responsibilities to the system architecture, complicating overall implementation.</p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_52"><span class="num-string">3.3.1</span> Streaming completions</h3> 
  </div> 
  <div class="readable-text" id="p115"> 
   <p>The completions API allows streaming responses, offering immediate access to information as soon as it is ready rather than waiting for a full response. For enterprises, streaming can be important in some cases where real-time content generation with lower latency is key. This feature can enhance user experiences by processing incoming responses promptly.</p> 
  </div> 
  <div class="readable-text intended-text" id="p116"> 
   <p>To enable streaming from the API’s standpoint, modify the <code>stream</code> parameter to <code>true</code>. By default, this optional parameter is set to <code>false</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p117"> 
   <p>Streaming employs server-sent events (SSE), which require a client-side implementation. SSE is a standard protocol allowing servers to continue transmitting data to clients after establishing the initial connection. It is a long-term, one-way connection from server to client. SSE offers advantages such as low latency, reduced bandwidth consumption, and an uncomplicated configuration setup.</p> 
  </div> 
  <div class="readable-text intended-text" id="p118"> 
   <p>Listing 3.12 demonstrates how our example can be adjusted to utilize streaming. Although the API modification is straightforward, the description and requested multiple generations were adjusted (using the <code>n</code> property). This allows us to generate more text artificially, making it easier to observe the streaming generation.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p119"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.12</span> Streaming completion</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import os
import sys
from openai import AzureOpenAI

client = AzureOpenAI(
    azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    api_version="2024-05-01-preview",
    api_key=os.getenv("AOAI_KEY"))

prompt_startphrase = "Suggest three names and a tagline  
<span class="">↪</span>which is at least 3 sentences for a new pet salon business. 
<span class="">↪</span>The generated name ideas should evoke positive emotions and the 
<span class="">↪</span>followingkey features: Professional, friendly, Personalized Service."  <span class="aframe-location"/> #1

for response in client.completions.create(        <span class="aframe-location"/> #2
    model="gpt-35-turbo",
    prompt=prompt_startphrase,
    temperature=0.8,
    max_tokens=500,
    stream=True,                                  <span class="aframe-location"/> #3
    stop=None):
    for choice in response.choices:      <span class="aframe-location"/> #4
        sys.stdout.write(str(choice.text)+"\n")
        sys.stdout.flush()</pre> 
    <div class="code-annotations-overlay-container">
     #1 Tweaked the prompt slightly to add descriptions
     <br/>#2 We need to handle the streaming response on the client side.
     <br/>#3 Enables streaming
     <br/>#4 We need to loop through the array and handle multiple generations.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>When managing a streaming call, we must pay extra attention to the <code>finish_reason</code> property. As messages are streamed, each appears as a standard completion, with the text representing the newly generated token. In these instances, the <code>finish_reason</code> remains null. However, the final message differs; its <code>finish_reason</code> could be either <code>stop</code> or <code>length</code>, depending on what triggered it.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p121"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.13</span> Streaming finish reason</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">...
{
  "finish_reason": null,
  "index": 0,
  "logprobs": null,
  "text": " Pet"
}
{
  "finish_reason": null,
  "index": 0,
  "logprobs": null,
  "text": " Pam"
}
{
  "finish_reason": null,
  "index": 0,
  "logprobs": null,
  "text": "pering"
}
{
  "finish_reason": "stop",
  "index": 0,
  "logprobs": null,
  "text": ""
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p122"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_53"><span class="num-string">3.3.2</span> Influencing token probabilities: logit_bias</h3> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>The <code>logit_bias</code> parameter is one way we can influence output completion. In the API, this parameter allows us to manipulate the probability of certain tokens, which can be words or phrases, that the model generates in its responses. It is called <code>logit_ bias</code> because it directly affects the log odds, or logits, that the model calculates for each potential token during the generation process. The bias values are added to these log-odds before converting them to probabilities, altering the final distribution of tokens the model can pick from.</p> 
  </div> 
  <div class="readable-text intended-text" id="p124"> 
   <p>The importance of this feature lies in its ability to steer the model’s output. Say we are creating a chatbot and want it to avoid certain words or phrases. We can use <code>logit_bias</code> to decrease the likelihood of those tokens being chosen by the model. In contrast, if there are certain words or phrases we want the model to favor, we could use <code>logit_bias</code> to increase their likelihood. The range of this parameter is from –100 to 100, and it operates on tokens for the word. Setting a token to –100 effectively bans it from the generation, whereas setting it to 100 makes it exclusive. </p> 
  </div> 
  <div class="readable-text intended-text" id="p125"> 
   <p>To use <code>logit_bias</code>, we provide a dictionary where the keys are the tokens, and the val-ues are the biases that need to be applied to those tokens. To get the token, we use the <code>tiktoken</code> library. Once you have the appropriate token, you can assign a positive bias to make it more likely to appear or a negative bias to make it less likely, as shown in figure 3.2. The blocks show the degree of probability that different tokens can be at different probabilities of banning or exclusive generation. Smaller changes to the tokens’ value increase or decrease the probability of these tokens in the generated output.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p126">  
   <img alt="figure" src="../Images/CH03_F02_Bahree.png" width="863" height="256"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.2</span> The <code>logit_bias</code> parameter </h5>
  </div> 
  <div class="readable-text" id="p127"> 
   <p>Let’s use an example to see how we can make this work. For our pet salon name, we do not want to use the words “purr,” “purrs,” or “meow.” The first thing we want to do is create the tokens for these words. We also want to add words with a preceding space and capitalize them as spaces. Capital letters are all different tokens. So “Meow” and “Meow” (with a space) and “meow” (again with a space) might read the same to us, but when it comes to tokens, these words are all different. The output shows us the tokens for the corresponding word:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p128"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">'Purr Purrs Meow Purr purr purrs meow:[30026, 81, 9330, 
<span class="">↪</span>3808, 42114, 9330, 81, 1308, 81, 1308, 3808, 502, 322]'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p129"> 
   <p>Now that we have the tokens, we can add them to the completion call. Note that we assign each token a bias of –100, steering the model away from these words.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p130"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.14</span> <code>logit_bias</code> implementation</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import os
from openai import AzureOpenAI

client = AzureOpenAI(
    azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    api_version="2024-05-01-preview",
    api_key=os.getenv("AOAI_KEY"))

GPT_MODEL = "gpt-35-turbo"

prompt_startphrase = "Suggest three names for a new pet salon 
<span class="">↪</span>business. The generated name ideas should evoke positive 
<span class="">↪</span>emotions and the following key features: Professional, 
<span class="">↪</span>friendly, Personalized Service."

response = client.completions(  
  model=GPT_MODEL,  
  prompt=prompt_startphrase,  
  temperature=0.8,  
  max_tokens=100,  
  logit_bias={
      30026:-100,              <span class="aframe-location"/> #1
      81:-100,                 #1
      9330:-100,               #1
      808:-100,                #1
      42114:-100,              #1
      1308:-100,               #1 
      3808:-100,               #1 
      502:-100,                #1 
      322:-100                 #1
  }  
)  

responsetext =response.choices[0].text

print("Prompt:" + prompt_startphrase + "\nResponse:" + responsetext)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Dictionary containing the tokens and the corresponding bias values to steer the model on these specific tokens
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>We do not have any words we want to avoid when we run this code.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p132"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.15</span> Output of <code>logit_bias</code> generation</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "text": "\n\n1. Paw Prints Pet Pampering\n2. Furry Friends Fussing\n3. Posh Pet Pooches"
    }
  ],
...
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p133"> 
   <p>We can do the opposite and positively bias tokens too. Say we want to overemphasize and steer the model toward the word “Furry.” We can use the <code>tiktoken</code> library we saw earlier and find that the tokens for “Furry” are <code>[37, 16682]</code>. We can update the previous API call with this and, in this case, a positive bias of 5.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p134"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.16</span> <code>logit_bias</code>: Positive implementation</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">GPT_MODEL = "gpt-35-turbo"

response = client.completions.create(
    model=GPT_MODEL,
    prompt=prompt_startphrase,
    temperature=0.8,
    max_tokens=100,
    logit_bias={
        30026:-100,
        81:-100,
        9330:-100,
        808:-100,
        42114:-100,
        1308:-100, 
        3808:-100,
        502:-100,
        322:-100,
        37:5,
        16682:5
    }
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>When we run this code, we get the output shown in the following listing. As we can see, there is a much stronger emphasis on “Furry” in our generation. The completions also take longer, as the model competes with the bias when generating certain tokens.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p136"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.17</span> Output <code>logit_bias</code>: Positive implementation</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "text": "\n\n1.FurryFrendz Pet Salon\n2.FurryFurFection Pet Pampering\n3.FurryFurFam Pet Spa"
    }
  ],
  …
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>The <code>logit_bias</code> feature should be used carefully; it is a powerful tool for guiding the model’s output. However, excessive or inappropriate use can lead to nonsensical, overly repetitive, or biased output in unexpected ways.</p> 
  </div> 
  <div class="readable-text" id="p138"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_54"><span class="num-string">3.3.3</span> Presence and frequency penalties</h3> 
  </div> 
  <div class="readable-text" id="p139"> 
   <p>We have two additional parameters in the API, called <em>presence</em> and <em>frequency</em> penalties, that help steer the language model’s output by controlling the generation’s repetition. These two parameters influence the likelihood of words (technically a sequence of tokens) reappearing in a completion. A higher presence penalty encourages the model to focus on the prompt and avoid using tokens that already appear there. In contrast, a higher frequency penalty discourages the model from repeating itself. Let’s take a look at both in a little more detail.</p> 
  </div> 
  <div class="readable-text" id="p140"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Presence penalty parameter</h4> 
  </div> 
  <div class="readable-text" id="p141"> 
   <p>The presence penalty parameter affects how often the same token appears in the output. This is achievable by using the presence penalty as a value subtracted from the probability of a token each time it is generated. This means that the more a token is used, the less likely it is to be used again. This helps make the model use more varied tokens in the generation and explore new topics. The value of this parameter can range from 0 to 2.</p> 
  </div> 
  <div class="readable-text intended-text" id="p142"> 
   <p>The default value is 0, meaning the model does not care if a token is repeated. A high presence penalty (1.0) makes the model less likely to use the same token again, and a higher value makes the model introduce new topics in the output. A low presence penalty (0) makes the model stick to the existing topics in the text. Each time a token is generated, the parameter value is subtracted from the log probability of that token.</p> 
  </div> 
  <div class="readable-text intended-text" id="p143"> 
   <p>We can improve the quality of the generation by preventing the same text from being repeated multiple times, helping control the flow, and making the output more engaging. Now let’s look at the frequency penalty parameter.</p> 
  </div> 
  <div class="readable-text" id="p144"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Frequency penalty parameter</h4> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>This parameter controls how much the model avoids repeating itself in the output. The higher the frequency penalty (1.0), the more the model tries to use different words and phrases, which results in a more diverse generation. The lower the frequency penalty (0.0), the more the model can repeat the same words and phrases and the more predictable the output. This differs from the presence penalty, which encourages the model to use new words and phrases. The frequency penalty adds to the log probability of a token each time it appears in the output.</p> 
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>The best values for both parameters depend on what you want to achieve with the output. Usually, choosing values between 0.1 and 1.0 would be best, which noticeably affects the output. If you want a stronger effect, you can increase the values up to 2.0, but this might reduce the output quality.</p> 
  </div> 
  <div class="readable-text intended-text" id="p147"> 
   <p>Note that tuning these parameters requires some trial and error to get the desired results, as the model’s output is also influenced by many other factors, including the prompt you provide and other fine-tuning parameters. Figure 3.3. shows the correlation for both the presence and frequency penalty parameters.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p148">  
   <img alt="figure" src="../Images/CH03_F03_Bahree.png" width="909" height="415"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.3</span> Penalty presence parameter</h5>
  </div> 
  <div class="readable-text" id="p149"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_55"><span class="num-string">3.3.4</span> Log probabilities</h3> 
  </div> 
  <div class="readable-text" id="p150"> 
   <p>When an LLM generates a token, it assigns a probability to the next considered token and uses various techniques to pick the token used in the completion from these options. The <code>logprobs</code> property of the completion API exposes the natural logarithm for these probabilities at each step.</p> 
  </div> 
  <div class="readable-text intended-text" id="p151"> 
   <p>This is an integer (max value of 5) that shows the alternate tokens considered for each token included in the completion. If this value is set to 3, the API will return a list of the three most likely tokens for each selected token in the generation. Note that the API always returns the <code>logprobs</code> of the sampled token, so in the response, we might end up with <code>logprobs + 1</code> element in the array.</p> 
  </div> 
  <div class="readable-text intended-text" id="p152"> 
   <p>Fundamentally, we use this approach to help debug and improve the prompts. If the model isn’t generating text we like, we can use this to see what other words (technically tokens) the model considered. This allows us to tune some other settings to steer the model. Conversely, we can use the same thing to control randomness in the model generation and make the output more deterministic. Finally, we can also use this to understand how confident the model is. If the probabilities are the same for several different words, this means that the model is not certain what word comes next. </p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>Say we want to get a name for a white dog; we can call the completion API. In this example, we get the name Cotton, which isn’t bad: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p154"> 
   <div class="code-area-container"> 
    <pre class="code-area">response = client.completions.create(
    model=GPT_MODEL,
    prompt="Suggest a one word name for a white miniature poodle.",
    temperature=0.8,max_tokens=100,
    stop=None)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p155"> 
   <p>If we want to see what other tokens were considered for the name, we can add the <code>logprobs</code> properties:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p156"> 
   <div class="code-area-container"> 
    <pre class="code-area">response = client.completions.create(
    model=GPT_MODEL,
    prompt="Suggest a one word name for a white miniature poodle.",
    temperature=0.8,max_tokens=100,
    logprobs=3,
    stop=None)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p157"> 
   <p>As seen in the completion output in the following listing, the model considered the following tokens: Casper, Coco, and Snow.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p158"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.18</span> Output log probabilities</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
  "id": "cmpl-7giPQGlKc6c7BaWmHgOLyZqabIruw",
  "object": "text_completion",
  "created": 1690414840,
  "model": "gpt-35-turbo",
  "choices": [
    {
      "text": "\n\nCotton",
      "index": 0,
      "finish_reason": "stop",
      "logprobs": {
        "tokens": [
          "\n",
          "\n",
          "C",
          "otton"
        ],
        "token_logprobs": [
          -0.0008873215,
          -4.361666e-06,
          -1.026479,
          -0.56846446
        ],
        "top_logprobs": [
          {
            "\n": -0.0008873215,
            "\n\n": -7.660001,
            " Angel": -10.180796
          },
          {
            "\n": -4.361666e-06,
            "\n\n": -12.970553,
            "&lt;|endoftext|&gt;": -15.136529
          },
          {
            "C": -1.026479,
            "P": -2.255978,
            "Snow": -2.1068947
          },
          {
            "asper": -2.001854,
            "oco": -1.957575,
            "otton": -0.56846446
          }
        ],
        "text_offset": [
          54,
          55,
          56,
          57
        ]
      }
    }
  ],
  "usage": {
    "completion_tokens": 4,
    "prompt_tokens": 12,
    "total_tokens": 16
  }
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p159"> 
   <p>As a reminder, we should use this property judiciously and only when required. Not only does it increase the number of tokens generated and, hence, the cost of the API call, but it also takes time and adds time to the API call, thereby increasing overall latency.</p> 
  </div> 
  <div class="readable-text intended-text" id="p160"> 
   <p>Now that we understand the completion API for text generation, let’s see how we can use the chat completion API.</p> 
  </div> 
  <div class="readable-text" id="p161"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_56"><span class="num-string">3.4</span> Chat completion API</h2> 
  </div> 
  <div class="readable-text" id="p162"> 
   <p>The chat completion API has been designed to facilitate interactive and dynamic conversations. It is an evolution of the completion API, providing users with a more conversational and engaging experience. With this API, developers can create applications that have a dialogue with users, making it ideal for creating chatbots, writing assistants, and more.</p> 
  </div> 
  <div class="readable-text intended-text" id="p163"> 
   <p>The key benefits that the chat completion API provides over the completion API are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p164"> <em>Enhanced interactivity</em><em> </em>—The chat completion API allows for a more dynamic and interactive conversation with the user, making the user experience more engaging and natural. </li> 
   <li class="readable-text" id="p165"> <em>Contextual understanding</em><em> </em>—The API maintains the context of the conversation, ensuring that the responses are relevant and coherent. </li> 
   <li class="readable-text" id="p166"> <em>Multiturn conversation</em><em> </em>—Unlike the completion API, which is more suited for single-turn tasks, the multiturn conversation API allows developers to simulate conversations with multiple exchanges. </li> 
   <li class="readable-text" id="p167"> <em>Cost-effective</em><em> </em>—Completion API uses GPT-3.5 Turbo or GPT-4 models, which perform at a similar capability as text-davinci-003 but at 10% of the price per token, making it a more economical choice for developers. </li> 
  </ul> 
  <div class="readable-text" id="p168"> 
   <p>At a high level, using the chat completion API is similar to the completion API. The API takes a series of messages as input, forming the basis of the interaction with the model. The ordering of the messages is important, as it outlines the turn-by-turn interaction. </p> 
  </div> 
  <div class="readable-text intended-text" id="p169"> 
   <p>Each message has two properties: role and content. The role parameter has the following three options: <code>system</code>, <code>user</code>, or <code>assistant</code>. The content contains the message’s text from the role. Table 3.6 outlines the details of each role and its purpose.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p170"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 3.6</span> Chat completion API role description</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Role parameter 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  <code>system</code> <br/></td> 
      <td>  The <code>system</code> role is typically used to set the assistant’s behavior and provide the model with high-level instructions that guide the behavior throughout the conversation. This is where we can describe the assistant’s personality and tell it what it should and should not answer, as well as how to format responses. While there is no token limit, it is included with every API call and is part of the overall token limit. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>user</code> <br/></td> 
      <td>  This represents the user’s input in the conversation; these messages contain the instructions or queries from the user that the assistant responds to. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>assistant</code> <br/></td> 
      <td>  This represents the assistant’s prior messages in the conversation. Think of this as the ongoing memory that helps the model and provides the conversation context as it proceeds, turn by turn. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p171"> 
   <p>Listing 3.19 shows the chat completion API. As we called out earlier, the order of the messages in the array matters, as it represents the flow of the conversation. Usually, the conversation starts with a <code>system</code> message that sets the assistant’s behavior, followed by alternating <code>user</code> and <code>assistant</code> messages as the conversation proceeds turn by turn. The assistant’s replies are generated based on the conversation history.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p172"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.19</span> Chat completion API</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import os
from openai import AzureOpenAI

client = AzureOpenAI(
    azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    api_version="2024-05-01-preview",
    api_key=os.getenv("AOAI_KEY"))

GPT_MODEL = "gpt-35-turbo"

response = client.chat.completions.create(    <span class="aframe-location"/> #1
    model=GPT_MODEL,                        <span class="aframe-location"/> #2
  <span class="aframe-location"/>  messages = [                             #3
      <span class="">↪</span>{"role":"system","content":"You are an AI assistant 
                 <span class="">↪</span>that helps people find information."},
      <span class="">↪</span>{"role":"user","content":"Hello world"},
      <span class="">↪</span>{"role":"assistant","content":"Hello! How can I assist you today?"},
        {"role":"user","content":"I want to know more 
                 <span class="">↪</span>about pets and why dogs are good for humans?"}],
    temperature=0.8,
    max_tokens=800,                             <span class="aframe-location"/> #4
    user="amit",                                #4
    top_p=0.95,                                 #4 
    frequency_penalty=0,                        #4
    presence_penalty=0,                         #4
    stop=None                                   #4 
)

print(response.choices[0].message.content)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Chat complete API call
     <br/>#2 Different models needed (Turbo) compared to completion API
     <br/>#3 List of messages that form the heart of the API
     <br/>#4 These parameters are the same as for the completion API.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p173"> 
   <p>We need to update the engine parameter to use one of the chat-compatible models. As shown earlier in this chapter, not all models support the chat style, and we need to pick the models with the <code>chat_completion</code> capability (GPT-3.5 Turbo, GPT-4, GPT-4 Turbo). All the other parameters are the same as the completion API that we covered earlier in this chapter, and we will not get into those details again.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p174"> 
   <p><span class="print-book-callout-head">Note </span> The following parameters are unavailable with the new GPT-35 Turbo and GPT-4 models: <code>logprobs</code>, <code>best_of</code>, and <code>echo</code>. Trying to set any of these parameters will throw an exception.</p> 
  </div> 
  <div class="readable-text" id="p175"> 
   <p>The output of the previous example is shown in the next listing. The user started with “Hello, World!”, and the system responded, asking how to help us with the assistant message. The question about dog details is the next dialogue turn. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p176"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.20</span> Chat completion API output</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">{
  "id": "chatcmpl-7c9HOaRHmaut94oZacjdUqxOL96H3",
  "object": "chat.completion",
  "created": 1689326490,
  "model": "gpt-35-turbo",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "Dogs are often considered man's best 
             <span class="">↪</span>friend for good reason. They provide 
             <span class="">↪</span>companionship, protection, and loyalty to 
             <span class="">↪</span>their owners. Here are some reasons why dogs are 
             <span class="">↪</span>good for humans:\n\n1. They reduce stress: Dogs 
             <span class="">↪</span>have been shown to reduce stress and anxiety in 
             <span class="">↪</span>their owners. Petting a dog can release endorphins, 
                      <span class="">↪</span>which are feel-good hormones that can help to 
                      <span class="">↪</span>reduce stress.\n\n2. They provide companionship..."
      }
    }
  ],
  "usage": {
    "completion_tokens": 238,
    "prompt_tokens": 60,
    "total_tokens": 298
  }
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p177"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_57"><span class="num-string">3.4.1</span> System role</h3> 
  </div> 
  <div class="readable-text" id="p178"> 
   <p>The system role (some also call it the system message) is included at the beginning of the message array. This message provides the initial instructions for the model, and we can provide various pieces of information in the system role, including</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p179"> A brief description of the assistant </li> 
   <li class="readable-text" id="p180"> Personality traits of the assistant </li> 
   <li class="readable-text" id="p181"> Rules and instructions you want the assistant to follow </li> 
   <li class="readable-text" id="p182"> Additional information needed for the model (e.g., relevant questions from an FAQ) </li> 
  </ul> 
  <div class="readable-text" id="p183"> 
   <p>We customize the system role and include basic instructions for the use case. From an API perspective, even though the system role is optional, it is highly recommended that you make this intentional to get the best results. For example, if we expand on the previous example of chatting for pets and pet salons, we can instruct the model to only reply in rhyme.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p184"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.21</span> Chat completion system message example</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">GPT_MODEL = "gpt-35-turbo"

response = client.chat.completions.create( 
  engine=GPT_MODEL,
  messages = [
    {"role": "system", "content": "You are a helpful AI 
              <span class="">↪</span>assistant that provides truthful information. 
              <span class="">↪</span>You answer all questions in rhyme."}, <span class="aframe-location"/> #1
    {"role": "user", "content": "Hi"},
    {"role": "assistant", "content": "Hello there, how can 
              <span class="">↪</span>I assist?\nAsk me a question, don't resist!"},
    {"role": "user", "content": "who are the founders of Microsoft?"},
    {"role": "assistant", "content": "Bill Gates and Paul Allen,
              <span class="">↪</span>it's true,\nAre the founders of Microsoft, 
              <span class="">↪</span>through and through."},
    {"role": "user", "content": "what is a good name for a pet salon?"},
    {"role": "assistant","content": "For a pet salon [
              <span class="">↪</span>that's simply divine,\nHere's a name that's [
              <span class="">↪</span>sure to shine:\n\"Paws and Pamper\" is what I 
              <span class="">↪</span>propose,\nA name that's catchy and easy to compose."}],
  temperature=0.7,
  max_tokens=800,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  stop=None)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Instructs to answer in rhyme
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p185"> 
   <p>In the example, we can have a conversation as expected, which can vary topics in turns, but all the answers rhyme.</p> 
  </div> 
  <div class="readable-text intended-text" id="p186"> 
   <p>When we want to give the model additional data as context for the conversation, this is called grounding the data. If there is a small amount of data, this can be part of the <code>system</code> role, as shown in the next listing. However, if there is a large amount of data, we should use embeddings and retrieve the most relevant information using a semantic search (e.g., Azure cognitive search).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p187"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.22</span> Grounding system message example</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{"role": "system", "content": Assistant is an intelligent chatbot designed to help users answer technical questions about Azure OpenAI service. Only answer questions using the context below. Don’t make up the answer. If you are unsure of an answer, say 'I don't know'.

Context:
- Azure OpenAI Service provides REST API access to OpenAI's powerful language models including the GPT-3, Codex and Embeddings model series.
- Azure OpenAI Service gives customers advanced language AI with OpenAI GPT-3, Codex, and DALL-E models with the security and enterprise promise of Azure.
..."
},
{"role": "user", "content": "What is Azure OpenAI Service?"}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p188"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_58"><span class="num-string">3.4.2</span> Finish reason</h3> 
  </div> 
  <div class="readable-text" id="p189"> 
   <p>Every chat completion API response has a finish reason encoded in the <code>finish_ reason</code> field. Tracking is important in this case, as it helps us understand why the API returned the response it did. This can be useful for debugging and improving the application. For example, if you receive an incomplete response due to the <code>length</code> finish reason, you may want to adjust the <code>max_tokens</code> parameter to generate more complete responses. The possible values for <code>finish_reason</code> are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p190"> <code>stop</code>—The API finished generating and either returned a complete message or a message terminated by one of the stop sequences provided using the stop parameter. </li> 
   <li class="readable-text" id="p191"> <code>length—</code>The API stopped the model output due to the <code>max_tokens</code> parameter or token limit. </li> 
   <li class="readable-text" id="p192"> <code>function_call—</code>The model decided to call a function. </li> 
   <li class="readable-text" id="p193"> <code>content_filter—</code>Some of the completion was filtered due to harmful content. </li> 
  </ul> 
  <div class="readable-text" id="p194"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_59"><span class="num-string">3.4.3</span> Chat completion API for nonchat scenarios</h3> 
  </div> 
  <div class="readable-text" id="p195"> 
   <p>OpenAI’s chat completion can be used for nonchat scenarios. The API is quite similar and designed to be a flexible tool that can be adapted to various use cases, not just conversations. In most cases, the recommended path uses the chat completion API as if it were the completion API. The main reason is that the newer models (Chat 3.5-Turbo and GPT-4) are much more efficient, cheaper, and powerful than the earlier models. The completion use cases we have seen, such as analyzing and generating text and answering questions from a knowledge base, would all still work with the chat completion API.</p> 
  </div> 
  <div class="readable-text intended-text" id="p196"> 
   <p>Implementing the chat completion API nonchat scenarios usually involves structuring the conversation with a series of messages and a system message to set the assistant’s behavior. For example, as shown in the following listing, the system message sets the role of the assistant, and the user message provides the task.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p197"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.23</span> Chat completion as a completion API example</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">GPT_MODEL = "gpt-35-turbo"

response = client.chat.completions.create(  
  model=GPT_MODEL,  
  messages=[  
        {"role": "system", "content": "You are a helpful assistant."},  
        {"role": "user", "content": "Translate the following 
                 <span class="">↪</span>English text to Spanish: 'Hello, how are you?'"}  
    ]  
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p198"> 
   <p>We can also use a series of user messages to provide more context or accomplish more complex tasks, as shown in the next listing. In this example, the first user message sets up the task, and the second user message provides more specific details. The assistant generates a response that attempts to complete the task in the user messages.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p199"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.24</span> Chat completion as a completion API example</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">GPT_MODEL = "gpt-35-turbo"

response = client.chat.completions.create(  
  model=GPT_MODEL,  
  messages=[  
        {"role": "system", "content": "You are a helpful assistant."},  
        {"role": "user", "content": "I need to write a Python function."},  
        {"role": "user", "content": "This function should take two 
                          <span class="">↪</span>numbers as input and return their sum."}  
    ]  
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p200"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_60"><span class="num-string">3.4.4</span> Managing conversation</h3> 
  </div> 
  <div class="readable-text" id="p201"> 
   <p>Our examples keep running, but the conversation will hit the model’s token limit as it continues. With each turn of the conversation (i.e., the question asked and the answer received), the list of messages grows. As a reminder, the token limit for GPT-35 Turbo is 4K tokens, and for GPT-4 and GPT-4 32K, it is 8K and 32K, respectively; these include the total count from the message list sent and the model response. We get an exception if the total count exceeds the relevant model limit.</p> 
  </div> 
  <div class="readable-text intended-text" id="p202"> 
   <p>No out-of-the-box option can track this token count for us and ensure it falls within the token limit. As part of the enterprise app design, we need to track the token count and only send a prompt that falls within the limit.</p> 
  </div> 
  <div class="readable-text intended-text" id="p203"> 
   <p>Many enterprises are in the process of implementing an enterprise version of ChatGPT using the chat API. Here are some of the best practices that can help enterprises manage these conversations. Remember, the best way to get your desired output involves iterative testing and refining your instructions:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p204"> <em>Setting the behavior with system message</em><em> </em>—You should use the system message at the start of the conversation to guide the model’s behavior and for enterprises to tune to reflect their brand or IP. </li> 
   <li class="readable-text" id="p205"> <em>Providing explicit instructions</em><em> </em>—If the model is not generating your desired output, make your instructions more explicit. Think about it at the same level as if you were telling a toddler what not to do. </li> 
   <li class="readable-text" id="p206"> <em>Breaking down complex tasks</em><em> </em>—If you have a complex task, break it down into several simpler tasks, and send them as separate user messages. You often need to show, not explain it. This is called Chain of Thought (CoT), and it will be covered in more detail in chapter 6. </li> 
   <li class="readable-text" id="p207"> <em>Experimentation</em><em> </em>—Feel free to experiment with the parameters to get the desired output. A higher temperature value (e.g., 0.8) makes the generation more random, while a lower value (e.g., 0.2) makes it more deterministic. You can also use the maximum token value to limit response length. </li> 
   <li class="readable-text" id="p208"> <em>Managing tokens</em><em> </em>—Be aware of the total number of tokens in a conversation, as input and output tokens count toward the total. You must truncate, omit, or shorten your text if a conversation has too many tokens to fit within the model's maximum limit. </li> 
   <li class="readable-text" id="p209"> <em>Handling sensitive content</em><em> </em>—If you’re dealing with potentially unsafe content, you should look at Azure OpenAI’s Responsible AI guidelines (<a href="https://mng.bz/pxVK">https://mng.bz/pxVK</a>). However, if you are using OpenAI’s API, then OpenAI’s moderation guide is helpful (<a href="https://mng.bz/OmEw">https://mng.bz/OmEw</a>) for adding a moderation layer to the outputs of the chat API. </li> 
  </ul> 
  <div class="readable-text" id="p210"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Tracking tokens</h4> 
  </div> 
  <div class="readable-text" id="p211"> 
   <p>As outlined earlier, keeping track of tokens when using the conversational API is key. Not only will the experience suffer if we go over the total token size, but the total number of tokens in an API also has a direct effect on latency and on how long the call takes. Finally, the more tokens we use, the more we pay. Here are some ways you can manage tokens:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p212"> <em>Count tokens</em><em>. </em>Use the <code>tiktoken</code> library, which allows us to count how many tokens are in a string without making an API call. </li> 
   <li class="readable-text" id="p213"> <em>Limit response length</em><em>. </em>When making an API call, use the <code>max_tokens</code> property to limit the length of the model’s responses. </li> 
   <li class="readable-text" id="p214"> <em>Truncate long conversations</em><em>. </em>If a conversation has too many tokens to fit within the model’s maximum limit, we must truncate, omit, or shorten our text. </li> 
   <li class="readable-text" id="p215"> <em>Limit the number of turns</em><em>. </em>Limiting the number of turns in the conversation is a good way to truncate or shorten the text. This also helps steer the model better when the conversation gets longer and tends to start hallucinating. </li> 
   <li class="readable-text" id="p216"> <em>Check the</em> <code>usage</code><em>field in the API response</em><em>. </em>After making an API call, we can check the usage field in the API response to see the total number of tokens used. This is ongoing and includes both input and output tokens. It is a good way to keep track of tokens and show them to the user via some UX. </li> 
   <li class="readable-text" id="p217"> <em>Reduce temperature</em><em>. </em>Reducing the temperature parameter can make the model's outputs more focused and concise, which can help reduce the number of tokens used in the response. </li> 
  </ul> 
  <div class="readable-text" id="p218"> 
   <p>Say we want to build a chat application for our pet salon and allow customers to ask us questions about pets, grooming, and their needs. We can build a console chat application, as shown in listing 3.25. It also shows us a possible way to track and manage tokens. In this example, we have a function <code>num_tokens_from_messages</code> which, as the name suggests, is used to calculate the number of tokens in a conversation. </p> 
  </div> 
  <div class="readable-text intended-text" id="p219"> 
   <p>As the conversation grows turn by turn, we calculate the number of tokens used, and once it reaches the model limit, the old messages are removed from the conversation. Note that we start at index 1. This ensures we always preserve the system message at index 0 and only remove user/assistant messages.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p220"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.25</span> <code>ConsoleChatApp:</code> Token management</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import os
from openai import AzureOpenAI
import tiktoken

client = AzureOpenAI(                                <span class="aframe-location"/> #1
    azure_endpoint=os.getenv("AOAI_ENDPOINT"),      #1
    api_version=”2024-05-01-preview”,               #1         
    api_key=os.getenv(“AOAI_KEY”))                  #1

GPT_MODEL = "gpt-35-turbo"

system_message = {"role": "system", "content": "You are 
                           <span class="">↪</span>a helpful assistant  <span class="aframe-location"/>      #2
max_response_tokens = 250
token_limit = 4096
conversation = []
conversation.append(system_message)                #2

def num_tokens_from_messages(messages):             <span class="aframe-location"/> #3
   <span class="aframe-location"/> encoding= tiktoken.get_encoding("cl100k_base")    #4
    num_tokens = 0
    for message in messages:                <span class="aframe-location"/> #5
        num_tokens += 4
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":
                num_tokens += -1 
    num_tokens += 2 

print("I am a helpful assistant. I can talk about pets and salons.")

while True:      <span class="aframe-location"/>                                  #6
    user_input = input("")     
    conversation.append({"role": "user", "content": user_input})
    conv_history_tokens = num_tokens_from_messages(conversation)

    while conv_history_tokens + max_response_tokens &gt;= token_limit:
        del conversation[1]                     <span class="aframe-location"/> #7
        conv_history_tokens = num_tokens_from_messages(conversation)

    response = client.chat.completions.create(    <span class="aframe-location"/> #8
        model=GPT_MODEL,
        messages=conversation,
        temperature=0.8,
        max_tokens=max_response_tokens)


    conversation.append({"role": "assistant", "content":
    <span class="">↪</span>response.choices[0].message.content})
    print("\n" + response.choices[0].message.content)
    print("(Tokens used: " + str(response.usage.total_tokens)  + ")")</pre> 
    <div class="code-annotations-overlay-container">
     #1 Sets up the OpenAI environment and configuration details
     <br/>#2 Sets up the system message for the chat
     <br/>#3 Function to count the total tokens from all the messages in the conversation
     <br/>#4 Uses the tiktoken library to count tokens
     <br/>#5 Loops through the messages 
     <br/>#6 Captures the user input
     <br/>#7 When the total tokens exceed the token limit, we remove the second token. The first token is the system token, which we always want.
     <br/>#8 Chat completion API call
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p221"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Chat completion vs. completion API</h4> 
  </div> 
  <div class="readable-text" id="p222"> 
   <p>Both chat completion and completion APIs are designed to generate human-like text and are used in different contexts. The completion API is designed for single-turn tasks, providing completion to a prompt provided by the user. It is most suited for tasks where only a single response is required.</p> 
  </div> 
  <div class="readable-text intended-text" id="p223"> 
   <p>In contrast, the chat completion API is designed for multiturn conversations, maintaining the context of the conversation over multiple exchanges. This makes it more suitable for interactive applications such as chatbots. The chat completion API is a new dedicated API for interacting with the GPT-35-Turbo and GPT-4 models and is the preferred method. The chat completion API is geared more toward chatbots, and using the different roles (<code>system</code>, <code>user</code>, and <code>assistant</code>), we can get the memory of previous messages and organize few-shot examples.</p> 
  </div> 
  <div class="readable-text" id="p224"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_61"><span class="num-string">3.4.5</span> Best practices for managing tokens</h3> 
  </div> 
  <div class="readable-text" id="p225"> 
   <p>For LLMs, tokens are the new currency. As most enterprises go beyond kicking tires to business-critical use cases, managing tokens would become a priority for computations, cost, and overall experience. From an enterprise application perspective, here are some of the considerations for managing tokens:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p226"> <em>Concise prompts</em><em> </em>—Where possible, using concise prompts and limiting the maximum number of tokens will reduce the token’s usage, making it more cost-effective. </li> 
   <li class="readable-text" id="p227"> <em>Stop sequences</em><em> </em>—Use stop sequences to stop the generations to avoid generating unnecessary tokens. </li> 
   <li class="readable-text" id="p228"> <em>Counting tokens</em><em> </em>—We can count tokens using the <code>tiktoken</code> library as outlined earlier and avoid making the API calls do the same. </li> 
   <li class="readable-text" id="p229"> <em>Smaller models</em><em> </em>—Generally speaking, in computing, bigger and newer hardware and software are considered faster, cheaper, and better; however, this isn’t necessarily the case for LLMs. Where possible, consider using smaller models such as GPT-3.5 Turbo first, and when they might not be a good fit, consider going to the next one. Smaller models are less compute intensive and, hence, are more economical. </li> 
   <li class="readable-text" id="p230"> <em>Use caching</em><em> </em>—For prompts that are either quite static or frequently repeated, implementing a caching strategy would help save tokens and avoid making API calls repeatedly. For more complex scenarios, look to cache the embeddings using a vector search and store, such as Azure Cognitive Search, Pinecone, etc. The last chapter covered an introduction to embeddings, and we will get more details on embeddings and searching later in chapters 7 and 8 when we cover RAG and chatting with your data. </li> 
  </ul> 
  <div class="readable-text" id="p231"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_62"><span class="num-string">3.4.6</span> Additional LLM providers</h3> 
  </div> 
  <div class="readable-text" id="p232"> 
   <p>Additional vendors also now have LLMs to use for enterprises. These are either available via APIs or, in some cases, as model weights that enterprises can self-host. Table 3.7 outlines some of the more famous ones available at the time of publication. Please note that some restrictions are in place from a commercial-licensing perspective.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p233"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 3.7</span> Other LLM providers</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Models 
       </div></th> 
      <th> 
       <div>
         Descriptions 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Llama 2  <br/></td> 
      <td>  Meta released Llama 2, an open source LLM, which comes in three sizes (7 billion, 13 billion, and 70 billion parameters) and is free for research and commercial purposes. Companies can access this through cloud options such as Azure AI’s model catalog, Hugging Face, or AWS. Enterprises that want to host it using their own compute and GPUs can request access from Meta via <a href="https://ai.meta.com/llama/">https://ai.meta.com/llama/</a>. <br/></td> 
     </tr> 
     <tr> 
      <td>  PaLM  <br/></td> 
      <td>  PaLM is a 13 billion-parameter model from Google that is part of their generative AI for developer products. The model can perform text summarization, dialogue generation, and natural language inference tasks. At the time of publication, there was a waitlist for an API key; details are available at <a href="https://developers.generativeai.google/">https://developers.generativeai.google/</a>. <br/></td> 
     </tr> 
     <tr> 
      <td>  BLOOM  <br/></td> 
      <td>  Bloom is a 223-billion parameter, open source multilingual model that can understand and generate text in over 100 languages by collaborating with over 1,000 researchers across more than 250 institutions. It is available via Hugging Face for deployment. More details are available at <a href="https://huggingface.co/bigscience/bloom">https://huggingface.co/bigscience/bloom</a>. <br/></td> 
     </tr> 
     <tr> 
      <td>  Claude  <br/></td> 
      <td>  Claude is a 12-billion parameter developed by Anthropic. It is accessible through a playground interface and API in its developer console for development and evaluation purposes only. At publication, for production use, enterprises must contact Claude for commercial discussions. More details can be found at <a href="https://mng.bz/YVqz">https://mng.bz/YVqz</a>. <br/></td> 
     </tr> 
     <tr> 
      <td>  Gemini  <br/></td> 
      <td>  Google recently released a new LLM called Gemini, a successor to PaLM 2 and optimized for different sizes: ultra, pro, and nano. It is designed to be more powerful than its predecessor and can be used to generate new content. Google claims it to be their most capable AI model yet. More details can be found at <a href="https://mng.bz/GNxD">https://mng.bz/GNxD</a>. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p234"> 
   <p>Interestingly, all these vendors follow a similar approach to the concepts and APIs established by OpenAI. For example, as outlined by their documents, the PaLM model from Google’s completion API equivalent is presented in the next listing. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p235"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.26</span> PaLM-generated text API signature</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">google.generativeai.generate_text(*,
    model: Optional[model_types.ModelNameOptions] = 'models/text-bison-001',
    prompt: str,
    temperature: Optional[float] = None,
    max_output_tokens: Optional[int] = None,
    top_p: Optional[float] = None,
    top_k: Optional[float] = None,
    stop_sequences: Union[str, Iterable[str]] = None,
) -&gt; text_types.Completion</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p236"> 
   <p>While these options exist, and some are from reputable and leading technology companies, for most enterprises, Azure OpenAI and OpenAI are the most mature, with the most enterprise controls and support needed. The next chapter will deal with images, and we will learn how to move from text to images and generate in that modality.</p> 
  </div> 
  <div class="readable-text" id="p237"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_63">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p238"> GenAI models are classified into various categories, depending on the type. Each model has additional capabilities and characteristics. Choosing the right model for the use case at hand is important. And unlike computer science, in our case, the biggest model isn’t necessarily better. </li> 
   <li class="readable-text" id="p239"> The completion API is a sophisticated tool that generates text, which can be used to complete prompts provided by the user and forms the backbone of the text generation paradigm. </li> 
   <li class="readable-text" id="p240"> The completion API is relatively easy to use with only a few key parameters, such as the prompt, number of tokens to generate, temperature parameter that helps steer the model, and number of completions to generate. </li> 
   <li class="readable-text" id="p241"> The API exposes many advanced options for steering models and controlling randomness and generated text, such as <code>logit_bias</code>, presence penalty, and frequency penalty. All these work in tandem and help generate better output. </li> 
   <li class="readable-text" id="p242"> When using Azure OpenAI, the content safety filter can help filter specific categories to identify and act on potentially harmful content as part of both the input prompts and generated completions. </li> 
   <li class="readable-text" id="p243"> The chat completion API builds on the completion API, going from one set of instructions and APIs to a dialogue with the user in a turn-by-turn interaction. The chat completion consists of multiple systems, user, and assistance roles. The conversation starts with a <code>system</code> message that sets the assistant's behavior, followed by alternating <code>user</code> and <code>assistant</code> messages as the conversation proceeds turn by turn. </li> 
   <li class="readable-text" id="p244"> The system role is included at the beginning of the message array. It provides the initial instructions for the model, including personality traits, instructions and rules for the assistant to follow, and additional information we want to provide as context for the model; this additional information is called grounding the data. </li> 
   <li class="readable-text" id="p245"> Each completion and chat completion API response has a finish reason, which helps us understand why the API returned the response it did. This can be useful for debugging and improving the application. </li> 
   <li class="readable-text" id="p246"> The language learning models all have a finite context window and are quite expensive. Managing tokens becomes important for us to be able to run things at a reasonable cost and within the API allowance. This also helps us manage tokens in conversations for improved user experience and cost-effectiveness. </li> 
   <li class="readable-text" id="p247"> In addition to Azure OpenAI and OpenAI, there are other LLM providers, such as Meta’s Llama 2, Google’s Gemini and PaLM, Bloom by BigScience, and Anthropic’s Claude. Their offerings are similar and follow the completions and chat completions paradigm, including similar APIs. </li> 
  </ul>
 </div></div></body></html>