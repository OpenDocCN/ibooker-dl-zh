<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">7</span></span> <span class="chapter-title-text">From information to knowledge: Building textual context</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Introducing context</li>
<li class="readable-text" id="p3">Calibrating the story to the audience</li>
<li class="readable-text" id="p4">Using ChatGPT for commentaries and annotations</li>
<li class="readable-text" id="p5">Using large language models for textual context</li>
<li class="readable-text" id="p6">A case study: From information to knowledge (part 1)</li>
</ul>
</div>
<div class="readable-text" id="p7">
<p>Talking about knowledge in a computer science book might seem completely out of place. The word <em>knowledge</em> could inspire philosophical concepts or even intimidate. But in this chapter (and the next), we will not be talking about philosophical knowledge but, rather, the knowledge that helps the reader understand the context of a story. It is, therefore, knowledge applied to the context of our data story, rather than general knowledge. In these chapters, we will review the basic concepts behind context in a data story and how to adapt it based on the audience. First, we will focus on textual context in this chapter, and in the next one, we will cover images. We will introduce large language models (LLMs) and use ChatGPT as an example of LLM implementation for data storytelling. Finally, we will explore a practical example.</p>
</div>
<div class="readable-text" id="p8">
<h2 class="readable-text-h2"><span class="num-string">7.1</span> Introducing context</h2>
</div>
<div class="readable-text" id="p9">
<p>When I was a child, I often heard my parents discussing some topic and did not understand anything. Their words rang in my ears as meaningless until, eager to understand what they were talking about, I entered the conversation and asked for explanations. Then, my father or mother, very patiently, explained to me what they were talking about, adapting their adult reasoning to my child’s mind so that I, too, could understand. Years later, I found myself in the same situation as a mother. My children often ask me to explain more complex speech <em>in words they can understand</em>. And the satisfaction is enormous when I see their faces light up and understand what I’m saying.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>The examples described show us the need to adapt the words we use according to the audience we are addressing. If we ignore who will receive our story, we risk talking in a way that may make perfect sense to ourselves but which excludes our audience from the message we want to communicate.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>In the previous chapter, we looked at how to extract and represent an insight through a chart. The next step is to enrich the chart with context (text and images), making reading easier for the reference audience. <em>Context</em> refers to the surrounding elements allowing the audience to understand the displayed information, such as texts, images, and symbols. Data context should prepare the scene of your data story and raise interest in your audience. In this chapter, we’ll primarily be dealing with textual context, while in the next chapter, we’ll look more at visual context.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>Context depends on the type of audience you are addressing. For example, if you are talking with an adult about how much you paid for a product, you don’t need to explain how money works. On the other hand, if you are talking to your kids about the same topic, you probably need to explain the denominations of the different banknotes and how the monetary system works.</p>
</div>
<div class="readable-text intended-text" id="p13">
<p>You can use generative AI tools, such as ChatGPT for text and DALL-E for images, to ease context building. You have already learned the basic techniques for building context using generative AI tools. This chapter will focus on more advanced techniques to write an impactful context tailored to your audience.</p>
</div>
<div class="readable-text intended-text" id="p14">
<p>We will consider the following types of context: </p>
</div>
<ul>
<li class="readable-text" id="p15"> <em>Commentary</em> —The text that precedes your insight. It includes the background that helps the audience to set the scene and understand the insight. In the example of the product cost explained to your kids, the commentary includes banknotes denominations, and how the monetary system works. </li>
<li class="readable-text" id="p16"> <em>Annotation</em> —A short text that explains a detail of your chart, for example, an anomalous point or a trend line. Consider adding annotations only when necessary. Don’t overload your chart with unnecessary annotations. </li>
<li class="readable-text" id="p17"> <em>Image</em> —A picture enforcing the commentary or the annotation. In the example of the product cost, you could add banknote images to help your kids understand the different denominations. </li>
<li class="readable-text" id="p18"> <em>Symbols</em> —Arrows, circles, lines, and so on, combined with annotations. They help the audience focus on particular points of your chart. </li>
</ul>
<div class="readable-text" id="p19">
<p>In the remainder of this chapter, we will use ChatGPT for commentaries and annotations. In the next chapter, we will focus on DALL-E for images and symbols. In addition, we will introduce LLMs and how to use them for commentaries and annotations. But first, let’s describe how to calibrate the story to our audience.</p>
</div>
<div class="readable-text" id="p20">
<h2 class="readable-text-h2"><span class="num-string">7.2</span> Calibrating the story to the audience</h2>
</div>
<div class="readable-text" id="p21">
<p>A few years ago, I was invited to give a seminar to master’s students. The seminar topic concerned the implementation of web applications for the construction of data journalism projects. Unfortunately, I found myself faced with a somewhat embarrassing situation. My seminar topic was very technical, even commenting on some pieces of code. As I began to speak, I realized that the audience couldn’t follow me because they didn’t have the technical skills required to properly understand. My presentation was technically correct, but having spoken too technically to a non-technical audience, the result of my talk was that the audience learned very little. The experience I gained from that episode taught me to always learn about the audience I will be addressing before communicating any message.</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>The <em>audience</em> is the person or the group of persons reading your data story. Understanding the target audience is crucial to building data stories that convey information effectively. In the previous chapter, we saw that you can use multiple types of charts to convey information (table 6.4). Once you’ve chosen the set of charts that answer your question, you can refine your choice, tailoring the chart to your audience.</p>
</div>
<div class="readable-text intended-text" id="p23">
<p>In chapter 4, you learned that there are different types of audiences. For simplicity, in this chapter, we group them into three common types of audiences:</p>
</div>
<ul>
<li class="readable-text" id="p24"> General public </li>
<li class="readable-text" id="p25"> Executives </li>
<li class="readable-text" id="p26"> Professionals </li>
</ul>
<div class="readable-text" id="p27">
<p>Let’s investigate each type of audience separately. To explain how you can calibrate the chart to the target audience, we will use the case study described in chapter 4. The objective of this case study was to understand which athletic disciplines our hypothetical team needed to continue to train to achieve the best possible results in the upcoming competitions. For convenience, figure 7.1 shows the complete data story we implemented: moving from data to wisdom.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p28">
<img alt="figure" height="851" src="../Images/7-1.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.1</span> The use case described in chapter 4</h5>
</div>
<div class="readable-text" id="p29">
<h3 class="readable-text-h3"><span class="num-string">7.2.1</span> General public</h3>
</div>
<div class="readable-text" id="p30">
<p>This audience includes individuals from various backgrounds and levels of knowledge. They may have little to no previous knowledge of your topic. When crafting data stories for the general public, use precise language, avoid overwhelming them with too much information, and focus on presenting the most relevant insights visually and engagingly. The general public could find the chart shown in figure 7.1 complex, with an unnecessary baseline. As an alternative to the chart in figure 7.1, you could draw the chart shown in figure 7.2, which some audiences may find more appealing.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p31">
<img alt="figure" height="658" src="../Images/7-2.png" width="662"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.2</span> The use case adapted to the general public</h5>
</div>
<div class="readable-text" id="p32">
<p>This chart is called a multi-layer donut chart. We could have placed the images close to the relevant bars, but in this case, there wasn’t enough space, so we placed them in the center of the chart. In other scenarios, you might consider placing images next to the bars. You can find the complete code to generate this chart in the GitHub repository for the book under 07/general-public.</p>
</div>
<div class="readable-text" id="p33">
<h3 class="readable-text-h3"><span class="num-string">7.2.2</span> Executives </h3>
</div>
<div class="readable-text" id="p34">
<p>Executives are typically high-level decision makers in organizations, who rely on data-driven insights to make essential business choices. They often have limited time and need concise and actionable information. When creating data stories for executives, it is essential to present key findings, trends, and recommendations up front. </p>
</div>
<div class="readable-text intended-text" id="p35">
<p>Use visualizations highlighting the most critical data points and providing a straightforward narrative linking the data to strategic goals. It can also be helpful to provide additional context or industry benchmarks to support your analysis. The chart shown in figure 7.1 could be great for executives because it does not contain many details and describes why we chose some sports, thanks to its baseline of 50%.</p>
</div>
<div class="readable-text" id="p36">
<h3 class="readable-text-h3"><span class="num-string">7.2.3</span> Professionals</h3>
</div>
<div class="readable-text" id="p37">
<p>This audience consists of individuals with a specific domain expertise or professional background. They have a deeper understanding of data and require more analytical information. When creating data stories for professionals, explain the methodology, assumptions, and limitations of the data analysis. Consider including additional supporting data and references, allowing professionals to explore the data further. </p>
</div>
<div class="readable-text intended-text" id="p38">
<p>As an alternative to the chart in figure 7.1, you could draw the chart shown in figure 7.3, which some audiences may understand easily. The figure shows only the chart, without any annotation or context. You can find the complete code to generate this chart in the GitHub repository for the book under 07/professionals.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p39">
<img alt="figure" height="633" src="../Images/7-3.png" width="962"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.3</span> The use case adapted to professionals</h5>
</div>
<div class="readable-text" id="p40">
<p>Table 7.1 summarizes what to represent in a chart based on the audience type. Now that you have learned how to adapt your chart based on the audience type, let’s move on to the next step: using ChatGPT for commentaries and annotations.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p41">
<h5 class="browsable-container-h5"><span class="num-string">Table 7.1</span> What to represent in a chart based on the audience type</h5>
<table>
<thead>
<tr>
<th>
<div>
         Audience Type 
       </div></th>
<th>
<div>
         Requirements 
       </div></th>
<th>
<div>
         What to Represent 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  General public <br/></td>
<td>  Understand data <br/></td>
<td>  An appealing overview of insights <br/></td>
</tr>
<tr>
<td>  Executives <br/></td>
<td>  High-level overview of data trends to aid strategic decision making <br/></td>
<td>  Highlight critical metrics and trends influencing business outcomes. <br/></td>
</tr>
<tr>
<td>  Professionals <br/></td>
<td>  Detailed insights to understand the phenomenon behind data <br/></td>
<td>  Add numbers, statistics, and useful information to understand insights deeply. <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p42">
<h2 class="readable-text-h2"><span class="num-string">7.3</span> Using ChatGPT for commentaries and annotations</h2>
</div>
<div class="readable-text" id="p43">
<p>In his novella, <em>Metamorphosis</em>, Franz Kafka tells the story of Gregor Samsa, a traveling salesman who wakes up one morning transformed into a giant insect. Encased in this insect’s guise, Samsa cannot interact with his family or communicate his thoughts. Gregor’s family struggles to accept his transformation, leading to their relationship with Gregor deteriorating and Gregor becoming increasingly isolated. The novella unearths the fundamental isolation that emerges when one’s inner world remains inaccessible to others. Data analysts could find themselves in a situation quite similar to that experienced by Gregor Samsa in Kafka’s novella when they have to add text to a data visualization chart. The data analyst, by nature, is a technician and could encounter some difficulties in writing engaging text.</p>
</div>
<div class="readable-text intended-text" id="p44">
<p>ChatGPT can assist you in adding textual context to your data visualization chart. You have already learned that a prompt’s basic structure for ChatGPT comprises three main elements: role, audience, and task. </p>
</div>
<div class="readable-text intended-text" id="p45">
<p>For example, you can write <em>Act as an entertainer</em> (role)<em>, writing for decision makers </em>(audience)<em>. Write 5 titles about &lt;topic&gt;</em> (task). The topic could be whatever you want. The main problem is structuring the topic so that ChatGPT produces the correct context. To include the topic in the ChatGPT prompt as well, we will generate context following the schema shown in figure 7.4.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p46">
<img alt="figure" height="656" src="../Images/7-4.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.4</span> The schema used to generate context</h5>
</div>
<div class="readable-text intended-text" id="p47">
<p>In a prompt, we specify the following four main elements:</p>
</div>
<ul>
<li class="readable-text" id="p48"> <em>Role</em> —The role you want ChatGPT to take. You learned about many role types in chapter 4, including entertainer, educator, informer, inspirer, inviter to action, and relationship builder. </li>
<li class="readable-text" id="p49"> <em>Audience</em> —The audience of your chart. There are different types of audiences, such as the general public, executives, and professionals. </li>
<li class="readable-text" id="p50"> <em>Topic</em> —The subject of your chart. </li>
<li class="readable-text" id="p51"> <em>Type</em> —The text type to generate, including annotations and commentaries. </li>
</ul>
<div class="readable-text" id="p52">
<p>The process of generating context is iterative, in the sense that you can generate the context multiple times if you are not satisfied with the produced result. For example, you can adjust one or more elements to make ChatGPT converge on the desired output. </p>
</div>
<div class="readable-text" id="p53">
<p>In the remainder of this section, we will focus on how to write the topic and type elements of the schema while keeping the role and the audience simple. However, you can adapt the strategies described for the topic and the audience to the other elements.</p>
</div>
<div class="readable-text intended-text" id="p54">
<p>As an example of how to build the context, we will focus on the case study described in chapter 4 and shown in figure 7.1. The following text summarizes the scenario for convenience: <em>Imagine you work in a sports company. You are training a team of young athletes in various disciplines. For each discipline, you have noted the world record and recorded the best time achieved by your team for comparison. Unfortunately, your company has limited investment funds available. Your boss asks you to understand which disciplines are worth training in, hoping to achieve good results in the upcoming competitions. </em></p>
</div>
<div class="readable-text" id="p55">
<h3 class="readable-text-h3"><span class="num-string">7.3.1</span> Describing the topic</h3>
</div>
<div class="readable-text" id="p56">
<p><em>Describing the topic</em> means composing simple words that precisely depict for ChatGPT what you have discovered and shown in your chart. The more precise you are, the better the output will be. </p>
</div>
<div class="readable-text intended-text" id="p57">
<p>To describe the topic, focus on three aspects: scenario, data, and insight, as shown in figure 7.5. Let’s go through each of those three aspects in a bit more detail.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p58">
<img alt="figure" height="318" src="../Images/7-5.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.5</span> The elements used to describe the topic</h5>
</div>
<div class="readable-text" id="p59">
<h4 class="readable-text-h4">Scenario</h4>
</div>
<div class="readable-text" id="p60">
<p>Describe an overview of your scenario, including the background and objective of the analysis. For the scenario in figure 7.1, we could write the following prompt for ChatGPT: <em>We are training a team of young athletes in various disciplines. For each discipline, we have calculated the percentage improvement of each discipline compared to the world record in that discipline. The objective is to search for the best two disciplines to fund.</em></p>
</div>
<div class="readable-text" id="p61">
<h4 class="readable-text-h4">Data</h4>
</div>
<div class="readable-text" id="p62">
<p>Provide an overview of your data. This may include explaining the type of data, its source, and any manipulations you performed. </p>
</div>
<div class="readable-text intended-text" id="p63">
<p>Describe the data using your own words, providing a more personalized description. By manually describing the data, you can highlight important patterns, trends, or correlations that may not be apparent through automated methods alone. Additionally, through manual descriptions, you can incorporate domain expertise observations.</p>
</div>
<div class="readable-text intended-text" id="p64">
<p>For the scenario in figure 7.1, write the following prompt text: <em>There are five sports disciplines: Rowing (percentage improvement = 62.32%), Cycling (57.64%), Sprinting (42.69%), Long-distance running (18.31%), and Swimming (12.38%). </em>Now that you have learned how to describe the data, let’s move on to the last step: describing insights.</p>
</div>
<div class="readable-text" id="p65">
<h4 class="readable-text-h4">Insights</h4>
</div>
<div class="readable-text" id="p66">
<p>Describe the central insights you have derived from the analysis, such as key patterns, trends, correlations, or relationships you have discovered. For the scenario in figure 7.2, include the following text: <em>Rowing and Cycling percentages are more significant than the baseline of 50%.</em> Before illustrating how we can build the audience description, let’s test the prompt built so far on ChatGPT. </p>
</div>
<div class="readable-text" id="p67">
<h4 class="readable-text-h4">Test</h4>
</div>
<div class="readable-text" id="p68">
<p>We write the following prompt: <em>Act as an inspirer. Write 5 titles for the following topic. There are five sports disciplines: Rowing (percentage improvement = 62.32%), Cycling (57.64%), Sprinting (42.69%), Long-distance running (18.31%), and Swimming (12.38%). Rowing and Cycling percentages are more significant than the baseline of 50%. </em>Figure 7.6 shows a possible output produced by ChatGPT.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p69">
<img alt="figure" height="389" src="../Images/7-6.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.6</span> The five titles generated by ChatGPT</h5>
</div>
<div class="readable-text intended-text" id="p70">
<p>As an alternative, you can ask ChatGPT to produce the context as follows: <em>Act as an inspirer. Write the context of a chart using 30 words for the following topic. There are five sports </em><em>disciplines: Rowing (percentage improvement = 62.32%), Cycling (57.64%), Sprinting (42.69%), </em><em>Long-distance running (18.31%), and Swimming (12.38%). Rowing and Cycling percentages are greater than the baseline of 50%.</em> Figure 7.7 shows a possible output produced by ChatGPT.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p71">
<img alt="figure" height="209" src="../Images/7-7.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.7</span> The context generated by ChatGPT</h5>
</div>
<div class="readable-text intended-text" id="p72">
<p>Now that you have learned how to describe the topic, try to generate the context for the case studies described in chapter 1: the pets scenario. For example, you can act as an informer. For convenience, we summarize the scenario: <em>The organizers of an event dedicated to pets are collecting the type of pets that will participate. For each pet category, the organizers advertise the event on specific websites dedicated to that category. The organizers ask you to build a quick report about the current situation. </em></p>
</div>
<div class="readable-text intended-text" id="p73">
<p>For more details, please refer to chapter 1. You can find a prompt with the generated context here: <a href="https://mng.bz/EZvJ">https://mng.bz/EZvJ</a>. For further practice, write the topic for the other scenarios described in the previous chapters. Now that you have learned how to describe the topic, let’s move on to the next element: describing the type.</p>
</div>
<div class="readable-text" id="p74">
<h3 class="readable-text-h3"><span class="num-string">7.3.2</span> Describing the type</h3>
</div>
<div class="readable-text" id="p75">
<p>We consider the following types: commentary and annotations. In the previous sections, you have seen different ways to instruct ChatGPT to generate context types, such as writing the context of a chart using 30 words. </p>
</div>
<div class="readable-text intended-text" id="p76">
<p>When describing the type, be as precise as possible, specifying the following aspects:</p>
</div>
<ul>
<li class="readable-text" id="p77"> The type (commentary, annotation, or general text) </li>
<li class="readable-text" id="p78"> The maximum number of words to generate </li>
</ul>
<div class="readable-text" id="p79">
<p>My suggestion is to try different types and evaluate the results based on your needs.</p>
</div>
<div class="readable-text" id="p80">
<h3 class="readable-text-h3"><span class="num-string">7.3.3</span> Setting custom instructions</h3>
</div>
<div class="readable-text" id="p81">
<p>ChatGPT enables you to configure custom instructions for all your new chats. For example, if we build our charts for the same audience type and act with the same role, we can use this property as a default configuration for ChatGPT. </p>
</div>
<div class="readable-text intended-text" id="p82">
<p>To enable custom instructions, access the ChatGPT web interface, click the three dots near your profile, and then click Custom Instructions. In the new window, write the custom instructions. For example, you can use the first box to configure your role as well as the target audience and the second box for more details, such as the number of words to generate, the tone, and the style, as shown in figure 7.8.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p83">
<img alt="figure" height="1335" src="../Images/7-8.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.8</span> The Custom Instructions dialog box</h5>
</div>
<div class="readable-text" id="p84">
<p>Click the Save button to enable the custom instructions for new chats. If you want to disable this property, deselect the Enable for New Chats property in the dialog box. Then, click the Save button.</p>
</div>
<div class="readable-text intended-text" id="p85">
<p>Let’s try the custom instructions with the previous text (we have removed the number of words to generate, since we have configured them in the custom instructions): <em>Write the context of a chart for the following topic. There are five sports disciplines: Rowing (percentage improvement = 62.32%), Cycling (57.64%), Sprinting (42.69%), Long-distance running (18.31%), and Swimming (12.38%). Rowing and Cycling percentages are greater than the baseline of 50%.</em></p>
</div>
<div class="readable-text intended-text" id="p86">
<p>Figure 7.9 shows a possible output. Notice, for example, the informal tone we have set in the custom instructions.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p87">
<img alt="figure" height="328" src="../Images/7-9.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.9</span> A possible output produced when configuring custom instructions</h5>
</div>
<div class="readable-text" id="p88">
<p>For optimal usage of custom instructions in data storytelling, I suggest using them to configure the role and the audience. In addition, you can configure other specific details correlated to your job or data, as specified in the ChatGPT documentation (<a href="https://mng.bz/8wAP">https://mng.bz/8wAP</a>).</p>
</div>
<div class="readable-text intended-text" id="p89">
<p>Now that you have learned how to use ChatGPT for commentaries and annotations, let’s move on to the next step: using large language models in a different way. So far, you have used the web interface provided by OpenAI to write your prompt for ChatGPT. Now, we will cover an advanced use of LLMs, based on APIs calls.</p>
</div>
<div class="readable-text" id="p90">
<h2 class="readable-text-h2"><span class="num-string">7.4</span> Using large language models for context</h2>
</div>
<div class="readable-text" id="p91">
<p>So far, you have used three generative AI tools: Copilot for code generation, ChatGPT for text generation, and DALL-E for image generation. All these tools are examples of application usage of a <em>large language model</em> (LLM). An LLM is a machine learning (ML) model aimed at predicting plausible language. LLMs have exploded in popularity since 2017, when Google researchers introduced the concept of transformers, a revolutionary architecture that allowed the training of large language models, such as generative pretrained transformers (GPTs), on which ChatGPT is based, and bidirectional encoder representations from transformers (BERT). Transformers allowed for the training of LLMs on massive datasets, resulting in models with incredible language generation capabilities.</p>
</div>
<div class="readable-text intended-text" id="p92">
<p>In this book, we will not focus on how LLMs work. Instead, we aim to demonstrate how you can use them effectively for data storytelling. However, if you’re interested in delving deeper into the technical aspects, a vast bibliography is available on the topic (Vaswani, 2017; Koenigstein, 2024).</p>
</div>
<div class="readable-text intended-text" id="p93">
<p>Before you embark on using LLM to build your data-driven story, it’s essential to ask yourself whether the model needs to know specific information related to your domain of work, as shown in figure 7.10. If the answer is no, then you can safely continue using ChatGPT. If, however, your answer is yes, then you can apply one of the following techniques:</p>
</div>
<ul>
<li class="readable-text" id="p94"> <em>Fine-tuning</em> —This technique adapts a pretrained LLM to a specific domain by updating its parameters on task-specific data, optimizing its performance for that domain. </li>
<li class="readable-text" id="p95"> <em>Retrieval augmented generation</em> —This technique combines information retrieval and language generation, enabling LLMs to incorporate external knowledge sources during the generation process.<span class="aframe-location"/> </li>
</ul>
<div class="browsable-container figure-container" id="p96">
<img alt="figure" height="484" src="../Images/7-10.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.10</span> A criterion to establish whether to extend an LLM or not</h5>
</div>
<div class="readable-text" id="p97">
<p>In the remainder of this section, we assume that your answer is yes and that you must tailor your LLM to your specific domain. A practical case where fine-tuning is useful could be when you must generate different data stories for the same type of audience or even the same audience. In this case, you could build your database with the same structure of annotations so that all new annotations have the same structure as the previous ones. This may generate some familiarity for your audience when they read your data stories. In other cases, you may need to use RAG, for example, when you have a long document and you want to build a short annotation for your data story based on it. Using RAG could help you to build textual summaries. Now that you have learned the potential benefits of extending the LLM, let’s start by analyzing the first strategy: fine-tuning.</p>
</div>
<div class="readable-text" id="p98">
<h3 class="readable-text-h3"><span class="num-string">7.4.1</span> Fine-tuning</h3>
</div>
<div class="readable-text" id="p99">
<p>GPT-3 was trained on 17 gigabytes of data, and GPT-4, the most recent model of OpenAI, has 45 gigabytes of training data. This means they contain a variety of information you can use in almost all cases. However, in some cases, fine-tuning your model could provide better results. </p>
</div>
<div class="readable-text intended-text" id="p100">
<p>Fine-tuning is the process of further training a pretrained language model on a specific dataset that is more relevant to your specific domain. During fine-tuning, you use a smaller dataset, which typically contains examples and specific input–output pairs relevant to your task. In practice, the dataset is a collection of samples, each containing the prompt and the suggested completion. </p>
</div>
<div class="readable-text intended-text" id="p101">
<p>When you apply fine-tuning to data storytelling, you can build a different dataset for each audience type, leading to better results. For example, you can build a dataset for the general public, one for professionals, and another for decision makers. You can even create a different dataset for each scenario you work with (products, topic, and so on) and for each type of text you want to generate (title, annotation, and commentary). The more specific your dataset is, the better your results will be.</p>
</div>
<div class="readable-text intended-text" id="p102">
<p>Preparing the dataset is the most significant effort during the process of fine-tuning. In the remainder of this section, we will describe two strategies to prepare the dataset: manual building and building from sources. In both cases, we will use the OpenAI API. For more details on the installation, refer to appendix A.</p>
</div>
<div class="readable-text" id="p103">
<h4 class="readable-text-h4">Manual building</h4>
</div>
<div class="readable-text" id="p104">
<p>Manual building involves defining each pair (prompt, completion) manually. This solution enables you to obtain the best results since you specify the exact behavior of your model, given a specific input. Consider, for example, the following pair:</p>
</div>
<ul>
<li class="readable-text" id="p105"> <em>Prompt</em> —Generate a title for the general public about topic X. </li>
<li class="readable-text" id="p106"> <em>Completion</em> —X revealed to you! </li>
</ul>
<div class="readable-text" id="p107">
<p>Now, imagine that you have fine-tuned your model with that pair and want to use your model to generate titles for the general public. If you give <em>the theory of relativity </em>topic as input to your fine-tuned model, it will probably generate a title similar to the following one: <em>The theory of relativity revealed to you! </em>The drawback of this strategy is that it is time consuming because you must write each pair manually. </p>
</div>
<div class="readable-text intended-text" id="p108">
<p>To start, you can define a minimum number of curated samples covering all your possible cases. The OpenAI model requires you to represent at least 10 samples. Next, train the model. After that, proceed with the model evaluation by considering the original model (i.e., without fine-tuning) as a reference. Provide the same prompt to the two models, original and fine-tuned, and compare the produced outputs. Use your new model if your fine-tuned model performs better than the original one. Instead, if it performs worse than or has the same behavior as the original model, try to add new samples or improve the existing ones. Repeat this procedure until you reach a good result.</p>
</div>
<div class="readable-text intended-text" id="p109">
<p>To show how manual building works, we will build a dataset tailored for the general public and generate a commentary as an output. In the previous chapter, you saw that you should use a different chart based on the information you want to convey. Here, we will build a different output based on the information to convey. We will build one or more (prompt–completion) pairs for each type of information to convey. Table 7.2 shows a possible dataset representing the described scenario. You can find the code described in this section in the GitHub repository for the book under 07/manual-chatgpt-fine-tuning.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p110">
<h5 class="browsable-container-h5"><span class="num-string">Table 7.2</span> The samples based on the information to convey</h5>
<table>
<thead>
<tr>
<th>
<div>
         Information to Convey 
       </div></th>
<th>
<div>
         Prompt 
       </div></th>
<th>
<div>
         Completion 
        <sup>a</sup>
</div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Parts of a whole <br/></td>
<td>  Percentage of participants in the conference by type (researchers 5%, students 30%, professors 65%) <br/></td>
<td>  More professors participated in the conference (65%). Researchers were not interested in the event (5%). <br/></td>
</tr>
<tr>
<td>  Comparison among entities <br/></td>
<td>  Comparison between red (80%), yellow, and green performance <br/></td>
<td>  Compared to yellow and green, red experienced an improvement in performance of 80%. <br/></td>
</tr>
<tr>
<td>  Trend <br/></td>
<td>  Sales changed in the last 12 months (–12%), due to fewer subscribers and video views. <br/></td>
<td>  Over the last 12 months, sales decreased by 12% based on various metrics, including subscribers and video views. <br/></td>
</tr>
<tr>
<td>  Outcomes of a survey or a questionnaire <br/></td>
<td>  Questionnaire: 3 positive answers, 7 negative answers <br/></td>
<td>  Three out of 10 people answered the questionnaire with a positive answer. <br/></td>
</tr>
<tr>
<td>  Distribution <br/></td>
<td>  Sales of product A (+30%) and product B over the last 12 months <br/></td>
<td>  Compared to product B, sales of product A have increased by 30% over the last 12 months. <br/></td>
</tr>
<tr>
<td>  Spatial information <br/></td>
<td>  Sales in North Europe (+23%) compared to South Europe <br/></td>
<td>  Compared to South Europe, sales in North Europe increased by 23%. <br/></td>
</tr>
<tr>
<td>  Relationship <br/></td>
<td>  The sales trend line from 1990 to 2020 increased by 120%. <br/></td>
<td>  Between 1990 and 2020, sales increased by 120%. <br/></td>
</tr>
<tr>
<td>  Comparison among entities <br/></td>
<td>  Top ingredients for our recipe: sugar and salt <br/></td>
<td>  The chosen ingredients for our recipe are sugar and salt. <br/></td>
</tr>
<tr>
<td>  Comparison among entities <br/></td>
<td>  Comparison between gold (30), silver (20), and bronze (40) <br/></td>
<td>  Bronze beats silver and gold with 40. <br/></td>
</tr>
<tr>
<td>  Distribution <br/></td>
<td>  Distribution of household chores (cooking 35%, cleaning 30%, laundry 20%, and yard work 15%) <br/></td>
<td>  Cooking takes up the most significant portion at 35%. Cleaning follows at 30%, while laundry and yard work account for 20% and 15%, respectively. <br/></td>
</tr>
<tr>
<td colspan="3">a. The word <i>completion</i> may be confusing, but it is used by the OpenAI API. <i>Completion</i> refers to the output produced by the model.</td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p111">
<p>Once you have built the dataset, you must format it as a JSONL file. This file contains a list of messages. Consider each message as a separate chat where you can specify a general configuration, the user prompt, and the assistant (model) output, as shown in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p112">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.1</span> The structure of a JSONL file</h5>
<div class="code-area-container">
<pre class="code-area">{
  "messages": [
     {
    "role": "system",
  "content": "You are a data analyst showing data to the general public."
     },
     {
         "role": "user",
         "content": "Distribution of household chores 
                                  (Cooking 35% Cleaning 30% Laundry 20%, Yard work 15%)"
     },
     {
          "role": "assistant",
           "content": "Cooking takes up the largest portion at 35%. 
                      Cleaning follows at 30% while laundry 
                      and yard work accounts for 20% and 15% respectively."

     }
  ]
}</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p113">
<p><span class="print-book-callout-head">Note</span>  Use the keyword <code>messages</code> to define the list of samples. Imagine each sample as a separate chat, where you can specify the model role: <code>system</code>, for general configuration; <code>user</code>, for user prompt; and <code>assistant</code>, for model output.</p>
</div>
<div class="readable-text" id="p114">
<p>If your dataset is saved as a CSV file, use the code shown in the following listing, which is also available in prepare-data.py, to convert it into JSONL. </p>
</div>
<div class="browsable-container listing-container" id="p115">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.2</span> How to convert the CSV file into JSONL</h5>
<div class="code-area-container">
<pre class="code-area">import pandas as pd
import json

df = pd.read_csv('general-public.csv')

json_list = []

for index, row in df.iterrows():
    json_object = {
        "messages": [
            {
                "role": "system",
                "content": "You are a data analyst showing data to the general public."
            },
            {
                "role": "user",
                "content": row['prompt']
            },
            {
                "role": "assistant",
                "content": row['completion']
            }
        ]
    }
    json_list.append(json_object)

with open('general-public.jsonl', 'w') as outfile:
    for json_object in json_list:
        json.dump(json_object, outfile)
        outfile.write('\n')</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p116">
<p><span class="print-book-callout-head">Note</span>  First, load the dataset as a pandas DataFrame. Next, format it in the JSONL format, as described in listing 7.1. Finally, save the generated JSONL file.</p>
</div>
<div class="readable-text" id="p117">
<p>Now, we are ready to fine-tune our model. We need an <code>OPENAI_API_KEY</code>, as specified in appendix A. If you are transitioning from a free to a for-fee plan, you might need to generate a new API key because the initial key does not work after the switch to a for-fee plan. Open a terminal, and export your <code>OPENAI_API_KEY</code> as an environment variable (<code>export</code> <code>OPENAI_API_KEY='my</code> <code>key'</code>). Next, upload the produced file to the OpenAI server, and when the uploading process is complete, create a job for fine-tuning. The following listing shows the code to perform these operations. Alternatively, read the tune-model.py script in the GitHub repository for the book. Remember that this option is exclusively available with the for-fee version.</p>
</div>
<div class="browsable-container listing-container" id="p118">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.3</span> How to fine-tune the model</h5>
<div class="code-area-container">
<pre class="code-area">import os
import openai
import time

openai.api_key = os.getenv('OPENAI_API_KEY')    <span class="aframe-location"/> #1

dataset = openai.File.create(file=open('general-public.jsonl', 
[CA]'rb'), purpose='fine-tune')         <span class="aframe-location"/> #2
print('Uploaded file id', dataset.id)

while True:     <span class="aframe-location"/> #3
    print('Waiting while file is processed...')
    file_handle = openai.File.retrieve(id=dataset.id)
    if len(file_handle) and file_handle.status == 'processed':
        print('File processed')
        break
    time.sleep(3)

job = openai.FineTuningJob.create(training_file=dataset.id, model="gpt-3.5-turbo")     <span class="aframe-location"/> #4

while True:
    print('Waiting while fine-tuning is completed...')
    job_handle = openai.FineTuningJob.retrieve(id=job.id)
    if job_handle.status == 'succeeded':
        print('Fine-tuning complete')
        print('Fine-tuned model info', job_handle)
        print('Model id', job_handle.fine_tuned_model)    <span class="aframe-location"/> #5
        break
    time.sleep(3)</pre>
<div class="code-annotations-overlay-container">
     #1 An alternative way to get your key: openai.api_key = ‘MY_KEY’
     <br/>#2 Creates a new dataset and uploads it to the OpenAI server
     <br/>#3 Enters into a loop until the model is fine-tuned
     <br/>#4 Create a new fine-tuning job
     <br/>#5 Prints the model ID
     <br/>
</div>
</div>
</div>
<div class="readable-text print-book-callout" id="p119">
<p><span class="print-book-callout-head">Note</span>  First, use the <code>openai.File.create()</code> method to create a new dataset and upload it to the OpenAI server. Next, use the <code>openai.FineTuningJob.create()</code> method to create a fine-tuning job using GPT-3.5 Turbo. Wait until the job is completed. This could take a long time, depending on the dataset size. Once the model is trained, use the <code>fine_tuned_model</code> variable to print the information associated with the fine-tuned model.</p>
</div>
<div class="readable-text" id="p120">
<p>The following listing shows an example of information printed after the execution of the fine-tune-model.py script. This fine-tuning would cost around $0.05.</p>
</div>
<div class="browsable-container listing-container" id="p121">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.4</span> An example of information associated with a fine-tuned model</h5>
<div class="code-area-container">
<pre class="code-area">fine-tuned model info {
  "object": "fine_tuning.job",
  "id": "your model id",
  "model": "gpt-3.5-turbo-0613",
  "created_at": 1693347869,
  "finished_at": 1693348340,
  "fine_tuned_model": "ft:gpt-3.5-turbo-0613:personal::7t1Xuct5",
  "organization_id": "org-jWkYw8hPpaNwkesXezsWOwK8",
  "result_files": [
    "file-ro0BoeariIjOl7NSGRC80v8r"
  ],
  "status": "succeeded",
  "validation_file": null,
  "training_file": "file-InGnigMTto3YLrsiLuIUr7ty",
  "hyperparameters": {
    "n_epochs": 10
  },
  "trained_tokens": 5930
}</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p122">
<p><span class="print-book-callout-head">Note</span>  Some helpful information is provided, including the model type, the model ID, the hyperparameters used, and more.</p>
</div>
<div class="readable-text" id="p123">
<p>Now, we can use the fine-tuned model to generate new commentaries tailored to the general public. Use the value corresponding to the <code>fine_tuned_model</code> key of the previous listing to refer to your model (<code>"ft:gpt-3.5-turbo-0613:personal::7t1Xuct5"</code> in the example). </p>
</div>
<div class="readable-text intended-text" id="p124">
<p>To generate a new commentary, start a new chat session by using the <code>openai.ChatCompletion.create()</code> method, as shown in the following listing and in the generate-description.py script of the GitHub repository for the book. As a use case, consider again the example of figure 7.1.</p>
</div>
<div class="browsable-container listing-container" id="p125">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.5</span> How to generate a new commentary</h5>
<div class="code-area-container">
<pre class="code-area">import os
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

model_id = "ft:gpt-3.5-turbo-0613:personal::7t1Xuct5"

completion = openai.ChatCompletion.create(
    model=model_id,
    messages=[
        {
            'role': 'system',
            'content': 'You are a data analyst showing data to the general public.',
        },
        {
            'role': 'user', 
            'content': 'Top sports: rowing (62%) and cycling (58%)'
        },
    ],
)

print(completion.choices[0].message)</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p126">
<p><span class="print-book-callout-head">Note</span>  Create a new <code>ChatCompletion</code> instance by specifying the model ID and the list of messages. The example defines only one message with the same system role as the fine-tuning dataset and the user role with a short description of our scenario.</p>
</div>
<div class="readable-text" id="p127">
<p>The following listing shows an example output.</p>
</div>
<div class="browsable-container listing-container" id="p128">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.6</span> An example output</h5>
<div class="code-area-container">
<pre class="code-area">{
  "role": "assistant",
  "content": " \"The most popular sports are rowing and cycling with 62% and 58% of people practicing them respectively.\""
}</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p129">
<p><span class="print-book-callout-head">Note</span>  The output contains the role (assistant) and the content.</p>
</div>
<div class="readable-text" id="p130">
<p>Incorporate the produced content into your chart as a commentary. This example has demonstrated how you can perform model fine-tuning using a manual dataset. In the example, the output is straightforward. If you want your model to produce more complex outputs, you must complicate your fine-tuning dataset—for example, by adding new pairs (prompt–completion) specifically designed for your audience or your topic.</p>
</div>
<div class="readable-text intended-text" id="p131">
<p>For comparison with the fine-tuned model, figure 7.11 shows the output produced by ChatGPT (without fine-tuning) with a similar input. Now that you have learned how to build your dataset manually, let’s move on to the next strategy, building from sources.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p132">
<img alt="figure" height="368" src="../Images/7-11.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.11</span> The output produced by ChatGPT without fine-tuning</h5>
</div>
<div class="readable-text" id="p133">
<h4 class="readable-text-h4">Building from sources</h4>
</div>
<div class="readable-text" id="p134">
<p>This strategy involves building your dataset from external sources, such as your company website or a domain-specific blog. For example, if you work in the health field, you could download the title and abstracts of scientific papers about health. This enables you to build a dataset with a very domain-specific language. Or if you work in the ICT field, you can download the titles and subtitles of blog articles from feeds to build your technical dataset. Anyway, you must pay attention to the data license in all cases. If the license explicitly prohibits their usage, you cannot use those sources, and you must search for other data sources. In some cases, contacting the data author directly could be sufficient—for example, if you want to download their data.</p>
</div>
<div class="readable-text intended-text" id="p135">
<p>In the remainder of this section, we will build a dataset tailored to a technical audience by extracting feeds from my Medium blog. The objective is to build a chart title corresponding to the blog title, providing the blog subheading as an input. You can find the example in the GitHub repository for the book under 07/from-source-chatgpt-fine-tuning.</p>
</div>
<div class="readable-text intended-text" id="p136">
<p>First, download the data. Ask Copilot to generate the code for you. The following listing shows the instructions for Copilot. </p>
</div>
<div class="browsable-container listing-container" id="p137">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.7</span> The instructions for Copilot</h5>
<div class="code-area-container">
<pre class="code-area"># import required libraries
# extract the title and link from the following rss/feed url: https://alod83.medium.com/feed
# for each extracted link, extract the subheading from the article
# create a dataframe with the following columns: 'prompt', ‘completion’
# save the dataframe to a csv file called 'medium-articles.csv'</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p138">
<p><span class="print-book-callout-head">Note</span>  Specify the feed URL and the information to extract for each item. Also, ask Copilot to generate the code to save the extracted items into a CSV file.</p>
</div>
<div class="readable-text" id="p139">
<p>Copilot will generate an output similar to that shown in listing 7.8. Set the prompt to the subheading and the completion to the title. Save the script, and run it. You can find the code generated by Copilot in the GitHub repository for the book in the download-raw-data.py script. You should see the medium-articles.csv file in your working directory.</p>
</div>
<div class="browsable-container listing-container" id="p140">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.8</span> How to extract data from feeds</h5>
<div class="code-area-container">
<pre class="code-area">import feedparser
import pandas as pd
import requests
from bs4 import BeautifulSoup

url = 'https://alod83.medium.com/feed'
feed = feedparser.parse(url)

titles = []
links = []
subheadings = []

for entry in feed.entries:
    titles.append(entry.title)
    links.append(entry.link)
    print(entry.link)
    response = requests.get(entry.link)
    soup = BeautifulSoup(response.content, 'html.parser')
    subheading = soup.find('h2', attrs={'class': 'pw-subtitle-paragraph'}).text
    subheadings.append(subheading)

df = pd.DataFrame({'prompt': subheadings,'completion': titles})
df.to_csv('medium-articles.csv', index=False)</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p141">
<p><span class="print-book-callout-head">Note</span>  Use the <code>feedparser</code>, <code>requests</code>, and <code>bs4</code> libraries. If you don’t have them in your environment, install them using the pip package manager. </p>
</div>
<div class="readable-text" id="p142">
<p>Once you have built the dataset, follow the procedure described in section 7.4.1 to fine-tune the dataset (listings 7.2–7.6). You can find the complete example in the GitHub repository for the book.</p>
</div>
<div class="readable-text intended-text" id="p143">
<p>To test the fine-tuned model, provide the following prompt as input: <em>A chart on selecting the best sport to fund</em>. The model generates an output similar to the following: <em>How to Choose the Best Sport to Fund: A Data-Driven Approach</em>. Try a similar prompt with ChatGPT. Figure 7.12 shows a possible output. Since ChatGPT is not fine-tuned, you must specify more details in your prompt, as previously seen. Instead, for your fine-tuned model, describing the content in your prompt is sufficient. Now that you have learned how to perform fine-tuning for data storytelling, let’s move on to the next strategy for adapting your model to your specific context: retrieval augmented generation.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p144">
<img alt="figure" height="272" src="../Images/7-12.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.12</span> The output produced by ChatGPT</h5>
</div>
<div class="readable-text" id="p145">
<h3 class="readable-text-h3"><span class="num-string">7.4.2</span> Retrieval augmented generation</h3>
</div>
<div class="readable-text" id="p146">
<p>So far, you have seen how to adapt an LLM to a context by building an ad hoc dataset. The effort, in this case, consists in preparing the dataset. Imagine how nice it would be to pass a text directly to the LLM without converting it to a specific format. Well, the good news is that this is possible, thanks to retrieval augmented generation (RAG).</p>
</div>
<div class="readable-text intended-text" id="p147">
<p>RAG is an advanced natural language processing (NLP) technique that combines elements of information retrieval and text generation. First, RAG performs a retrieval step, which queries an external knowledge source, such as a vast text corpus or a structured database. Next, RAG uses this knowledge source to enhance its response generation. RAG integrates the retrieved facts into its generated text.</p>
</div>
<div class="readable-text intended-text" id="p148">
<p>In the data storytelling domain, you can use RAG to adapt your LLM to your topic, such as a product, real-time data, customer reviews, or other relevant information. For instance, by querying the knowledge base for specific product details, you can generate ad hoc commentaries and annotations.</p>
</div>
<div class="readable-text intended-text" id="p149">
<p>Imagine you want to build a RAG-based system that retrieves information about a product from your website company. Figure 7.13 shows the architecture of the RAG system we will implement.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p150">
<img alt="figure" height="403" src="../Images/7-13.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.13</span> A RAG-based system</h5>
</div>
<div class="readable-text intended-text" id="p151">
<p>First, we will download the text from a specified URL, split it, and represent it as vectors we store in a vector database. We will provide the vector database as an input to an LLM application, which can answer queries by querying the vector database. We will implement an example that generates commentaries for a specific smartphone, based on its description contained in an HTML page. In practice, we will load the HTML page into the vector database, and then we will implement an LLM application to query it. We will use LangChain to implement the LLM application, Chroma for the vector database, and OpenAI for the LLM to make everything work. For more details on how to install these tools, refer to appendix A.</p>
</div>
<div class="readable-text intended-text" id="p152">
<p>In the remainder of this section, you will learn how to implement the described system. We will start by introducing LangChain. Next, we will see how to store data in Chroma. Finally, you will learn how to query the built system. </p>
</div>
<div class="readable-text" id="p153">
<h4 class="readable-text-h4">Introducing LangChain</h4>
</div>
<div class="readable-text" id="p154">
<p>LangChain (<a href="https://www.langchain.com/">https://www.langchain.com/</a>) is a framework that enables you to create applications that connect an LLM to other sources. LangChain supports several providers, including OpenAI, Google, Microsoft, Hugging Face, and many more. In this book, we will focus on the models provided by OpenAI.</p>
</div>
<div class="readable-text intended-text" id="p155">
<p>The core idea behind LangChain is the concept of a chain, which consists of several components from different modules. There are three main components:</p>
</div>
<ul>
<li class="readable-text" id="p156"> <em>LLM wrappers</em> —Wrappers for LLMs provided by external providers, such as OpenAI and Hugging Face </li>
<li class="readable-text" id="p157"> <em>Prompt templates</em> —Templates for different prompts, such as chatbot and question answering </li>
<li class="readable-text" id="p158"> <em>Indexes</em> —External structures you can use to provide additional context to an LLM </li>
</ul>
<div class="readable-text" id="p159">
<p>The LangChain-based applications are <em>context aware</em> because they connect LLM to external sources. Additionally, such applications are useful because they can answer questions based on the provided context, what actions to take, and so on.</p>
</div>
<div class="readable-text intended-text" id="p160">
<p>The most straightforward chain consists of just one LLM chained with a prompt that enables you to query the model. In the remainder of this section, we will implement a LangChain composed of the components shown in figure 7.14: the vector database (Chroma), the prompt template, the LLM model (GPT-3.5 Turbo by OpenAI), and the retrieval interface. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p161">
<img alt="figure" height="373" src="../Images/7-14.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.14</span> The implemented architecture</h5>
</div>
<div class="readable-text intended-text" id="p162">
<p>You can find the full code described in this example in the GitHub repository of this book under 07/rag. We need an <code>OPENAI_API_KEY</code>, as specified in appendix A. Open a terminal, and export your <code>OPENAI_API_KEY</code> as an environment variable (<code>export</code> <code>OPENAI_API_KEY='my</code> <code>key'</code>). </p>
</div>
<div class="readable-text intended-text" id="p163">
<p>Chroma (<a href="https://www.trychroma.com/">https://www.trychroma.com/</a>) is an embedding database you can use as an indexer for your LangChain. To install and configure Chroma, refer to appendix A. An <em>embedding</em> is a numerical representation of data that is easy to index and retrieve, often for real-time tasks (Lane and Dyshel, 2024). Before storing a text in Chroma, we must convert it into vector embeddings. For more details about embeddings, refer to the references section of this chapter.</p>
</div>
<div class="readable-text intended-text" id="p164">
<p>Consider the product description available on a hypothetical website, as shown in figure 7.15. The objective of our task is to store the product description shown in figure 7.15 in Chroma. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p165">
<img alt="figure" height="465" src="../Images/7-15.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.15</span> The HTML page with the product description</h5>
</div>
<div class="readable-text" id="p166">
<p>The first step involves loading the data from the URL, as shown in the following listing. Since Chroma is fully integrated with LangChain, we will use it to accomplish our task. LangChain supports multiple formats, including PDFs, URLs, and more. </p>
</div>
<div class="browsable-container listing-container" id="p167">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.9</span> How to load the HTML document in LangChain</h5>
<div class="code-area-container">
<pre class="code-area">from langchain_community.document_loaders.xhtml import UnstructuredHTMLLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

loader = UnstructuredHTMLLoader('product.xhtml')   <span class="aframe-location"/> #1
data = loader.load()</pre>
<div class="code-annotations-overlay-container">
     #1 Loads data
     <br/>
</div>
</div>
</div>
<div class="readable-text print-book-callout" id="p168">
<p><span class="print-book-callout-head">Note</span>  To load an HTML document in LangChain, build an <code>UnstructuredHTMLLoader()</code> object. </p>
</div>
<div class="readable-text" id="p169">
<p>Next, split the data into chunks of 20, as shown in the following listing. We could have chosen any number smaller than the total text size for the chunk size.</p>
</div>
<div class="browsable-container listing-container" id="p170">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.10</span> How to split the text into chunks</h5>
<div class="code-area-container">
<pre class="code-area">text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 100,
    chunk_overlap  = 20,
    length_function = len,
    is_separator_regex = False,
)

splitted_data = text_splitter.split_documents(data)</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p171">
<p><span class="print-book-callout-head">Note</span>  Create a <code>RecursiveCharacterTextSplitter()</code> object to split the text into chunks. </p>
</div>
<div class="readable-text" id="p172">
<p>After that, convert the split text into embeddings and store them in Chroma.</p>
</div>
<div class="browsable-container listing-container" id="p173">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.11</span> How to generate embeddings in Chroma</h5>
<div class="code-area-container">
<pre class="code-area">embeddings = OpenAIEmbeddings()

store = Chroma.from_documents(
    splitted_data, 
    embeddings, 
    ids = [f"{item.metadata['source']}-{index}" for index, item in enumerate(splitted_data)],
    collection_name='Product-Info', 
persist_directory='db',
)
store.persist()</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p174">
<p><span class="print-book-callout-head">Note</span>  First, create a new <code>OpenAIEmbeddings()</code> object. Next, create a Chroma store with the split data and the embeddings and associate it with the collection <code>Product-Info</code>. Finally, store the Chroma store on the filesystem, using the <code>persist()</code> method. </p>
</div>
<div class="readable-text" id="p175">
<p>Now, our vector store is ready, so we can move on to the next step: defining a prompt template.</p>
</div>
<div class="readable-text" id="p176">
<h4 class="readable-text-h4">Defining a prompt template</h4>
</div>
<div class="readable-text" id="p177">
<p>A prompt template is a predefined text used for generating prompts for LLMs. A prompt template may include instructions, examples, context, and questions appropriate for your task. The following listing shows an example of a prompt we can provide as an input to our system.</p>
</div>
<div class="browsable-container listing-container" id="p178">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.12</span> How to structure a prompt template</h5>
<div class="code-area-container">
<pre class="code-area">template = """You are a bot that answers questions about the product New SmartX 2023, using only the context provided.
If you don't know the answer, simply state that you don't know.

{context}

Question: {question}"""

prompt = PromptTemplate(
    template=template, input_variables=['context', 'question']
)</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p179">
<p><span class="print-book-callout-head">Note</span>  First, define the structure of your template. Use brackets to define input variables. In the example, there are two variables: <code>context</code> and <code>question</code>. Next, create a new <code>PromptTemplate()</code> object, and pass it the template and the input variables as parameters.</p>
</div>
<div class="readable-text" id="p180">
<p>Once we have built the prompt template, we are ready to proceed with the last step: retrieval and query.</p>
</div>
<div class="readable-text" id="p181">
<h4 class="readable-text-h4">Retrieval interface</h4>
</div>
<div class="readable-text" id="p182">
<p>A <em>retrieval interface</em> is an interface that enables us to combine the data stored in the Chroma database and the OpenAI LLM. We can use a retrieval to query our system and generate commentaries and annotations to incorporate in our charts. The following listing shows an example of the usage of a retrieval.</p>
</div>
<div class="browsable-container listing-container" id="p183">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.13</span> How to build a retrieval</h5>
<div class="code-area-container">
<pre class="code-area">llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo')

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type='stuff',
    retriever=store.as_retriever(),
    chain_type_kwargs={'prompt': prompt, },
    return_source_documents=True,
)

print(
    qa.invoke({"query": 'Describe the product New SmartX 2023 using 30 words'})
)</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p184">
<p><span class="print-book-callout-head">Note</span>  First, create an LLM instance using <code>ChatOpenAI()</code>. Set the temperature to 0 for conservative output. The temperature spans from 0 (low creativity) to 1 (high creativity). Set the model to <code>GPT-3.5-turbo</code>. Next, create a retrieval interface using <code>RetrievalQA()</code> by specifying the LLM, the vector store (<code>retriever</code>), the prompt, and other parameters. Set the <code>chain_type</code> to <code>stuff</code>, a prepackaged document chain that takes a list of documents and inserts them into the prompt, which is then passed to the LLM. Finally, ask the question.</p>
</div>
<div class="readable-text" id="p185">
<p>The following listing shows the produced output. You can insert the produced text (in bold) in your chart.</p>
</div>
<div class="browsable-container listing-container" id="p186">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.14</span> The produced output</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">{'query': 'Describe the product New SmartX 2023 using 30 words', 
    'result': '<strong>The New SmartX 2023 is a cutting-edge smartphone with a 5.7-inch Super AMOLED display and a high-quality camera that captures breathtaking landscapes and detailed close-ups.</strong>', 
    'source_documents': 
    [Document(page_content='© 2023 SmartX Technologies. All rights reserved.', 
        metadata={'source': 'product.xhtml'}), 
        Document(page_content='Get ready to experience the future with the all-new SmartX 2023. This cutting-edge smartphone', 
        metadata={'source': 'product.xhtml'}), 
        Document(page_content='Introducing the New SmartX 2023\n\\nKey Features:\n\\n5.7-inch Super AMOLED Display', 
            metadata={'source': 'product.xhtml'}), 
        Document(page_content="you're taking breathtaking landscapes or detailed close-ups, the SmartX 2023's camera delivers", 
            metadata={'source': 'product.xhtml'})
    ]
}</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p187">
<p><span class="print-book-callout-head">Note</span>  The output contains the text to insert in the chart (in bold) and other useful information, such as the original query and the source documents.</p>
</div>
<div class="readable-text" id="p188">
<p>Now that you have learned how to apply LLMs to build context in data storytelling, let’s move on to a practical example.</p>
</div>
<div class="readable-text" id="p189">
<h2 class="readable-text-h2"><span class="num-string">7.5</span> Case study: From information to knowledge (part 1)</h2>
</div>
<div class="readable-text" id="p190">
<p>In the previous chapter, we analyzed how to turn data into information in the aquaculture case study. As a quick reminder, the case study involved building a story around the problem of safety in the salmon aquaculture in the United States. We decided to plot the salmon aquaculture sales trend line versus the other types of aquaculture. As an insight, we discovered that since 1998, there had been an increase in sales, following a period of decrease in sales from 1992 to 1998. We discovered that the decreasing period was partially due to some health problems in the salmon aquaculture. Figure 7.16 shows the chart produced at the end of the first step of the DIKW pyramid: from data to information.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p191">
<img alt="figure" height="577" src="../Images/7-16.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.16</span> The chart produced at the end of the data-to-information phase</h5>
</div>
<div class="readable-text" id="p192">
<p>To transform the chart into a data story, the next step involves turning information into knowledge. We will accomplish this by doing the following:</p>
</div>
<ul>
<li class="readable-text" id="p193"> Some design considerations to tailor the chart to the audience </li>
<li class="readable-text" id="p194"> Adding a commentary describing the general situation regarding safety in aquaculture </li>
<li class="readable-text" id="p195"> Adding an annotation and a symbol to highlight the period of decrease in sales </li>
</ul>
<div class="readable-text" id="p196">
<p>Let’s start with the first point: tailoring the chart to the audience.</p>
</div>
<div class="readable-text" id="p197">
<h3 class="readable-text-h3"><span class="num-string">7.5.1</span> Tailoring the chart to the audience</h3>
</div>
<div class="readable-text" id="p198">
<p>The scenario required us to present the data story to an audience of executives, which meant we needed a chart easy enough to understand that they could quickly make decisions based on its information. In general, executives are familiar with trend lines, so we do not need to modify the chart. In addition, the chart is neither too detailed nor too simple. It contains the right level of detail to allow the audience not to be overwhelmed by information. Additionally, the chart doesn’t give the impression of being sparse. Therefore, we can conclude that the chart type is perfect for our audience.</p>
</div>
<div class="readable-text intended-text" id="p199">
<p>We also suppose that our audience is familiar with the $ symbol on the y-axis and the Years label on the x-axis, so we do not need to add any further specifications. We can leave the comparison between the salmon trend line and the others because it is useful for our audience to understand how the salmon sales behave compared to the other categories. It is not necessary to add further details.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p200">
<h5 class="callout-container-h5 readable-text-h5">Challenge: How could you tailor the chart to the general public or to an audience of professionals? </h5>
</div>
<div class="readable-text" id="p201">
<p>For the general public, you could consider simplifying the chart—for example, by reducing the number of years. You may also need to better explain the meaning of the y-axis. For professionals, you could add more details, such as points with values for each year, or you could even show the other aquaculture categories.</p>
</div>
</div>
<div class="readable-text" id="p202">
<p>Now that we have discussed some design considerations to tailor the chart to the audience, let’s move on to the next step: adding a commentary. We will use RAG to generate the commentary.</p>
</div>
<div class="readable-text" id="p203">
<h3 class="readable-text-h3"><span class="num-string">7.5.2</span> Using RAG to add a commentary</h3>
</div>
<div class="readable-text" id="p204">
<p>We will add a commentary to the chart immediately under the title. Our commentary should explain how safety works in US aquaculture. We will base the commentary on “Aquacultured Seafood” (<a href="https://mng.bz/WEW0">https://mng.bz/WEW0</a>), an official FDA fact sheet. This document describes, among other topics, the safety levels of aquaculture seafood.</p>
</div>
<div class="readable-text intended-text" id="p205">
<p>You can implement a RAG-based system that builds required commentary using the code implemented in section 7.4.2. You only need to provide this prompt: <em>Describe </em><em>the safety of aquaculture seafood in the U.S</em>. The code of the implemented RAG system is also available in the GitHub repository for the book under CaseStudies/aquaculture/ from-information-to-knowledge/rag.py. The following listing shows the produced output, containing the required commentary.</p>
</div>
<div class="browsable-container listing-container" id="p206">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.15</span> The produced output</h5>
<div class="code-area-container">
<pre class="code-area">{'query': 'Describe Safety of Aquaculture Seafood in the U.S.', 'result': 'Aquaculture seafood in the U.S. is regulated by the FDA to ensure safety. Strict standards are in place to monitor water quality, feed, and disease control. Regular inspections and testing are conducted to minimize risks and protect consumers.', 'source_documents': [Document(page_content='Safety of Aquaculture Seafood', metadata={'source': 'aquaculture.xhtml'}), Document(page_content='Regulatory Requirements for Aquacultured Seafood', metadata={'source': 'aquaculture.xhtml'}), Document(page_content='Domestic Aquaculture Seafood', metadata={'source': 'aquaculture.xhtml'}), Document(page_content='for additional information on how the FDA ensures the safety of imported seafood products.', metadata={'source': 'aquaculture.xhtml'})]}</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p207">
<p><span class="print-book-callout-head">Note</span>  Use the produced output as a commentary for the chart.</p>
</div>
<div class="readable-text" id="p208">
<p>Now, we can add this text as a commentary for our chart. Listing 7.16 shows only the modifications to our original chart including the commentary. You can find the complete code in the GitHub repository for the book under CaseStudies/aquaculture/ from-information-to-knowledge/chart.py.</p>
</div>
<div class="browsable-container listing-container" id="p209">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.16</span> The commentary</h5>
<div class="code-area-container">
<pre class="code-area">commentary = ['Aquaculture seafood in the U.S. is regulated by the FDA to ensure safety. Strict standards are in place to monitor water quality, feed, and disease control.',
'Regular inspections and testing are conducted to minimize risks and protect consumers. (Source: U.S. Food and Drug Administration)'
]

base = alt.Chart(df).encode(
    x=alt.X('YEAR_ID:O', title=''),
    y=alt.Y('AMOUNT', title='$',axis=alt.Axis(format='.2s')),
    color=alt.Color('CATEGORY', 
        legend=None,
        scale=alt.Scale(range=range, domain=domain)
    )
).properties(
    width=800,
    height=400,
    title=alt.TitleParams(
        text='Aquaculture Exports of Salmon in the U.S.',
        subtitle=commentary,
        fontSize=20,
        subtitleFontSize=14,
        align='left',
        anchor='start',
        offset=20,
        color=color,
        subtitleColor='black'
    )
)</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p210">
<p><span class="print-book-callout-head">Note</span>  Use the <code>title</code> property to add commentary to the chart, immediately before the title. Also, add a provisory title to the chart.</p>
</div>
<div class="readable-text" id="p211">
<p>The next step involves highlighting the period of decrease in sales. So let’s proceed. </p>
</div>
<div class="readable-text" id="p212">
<h3 class="readable-text-h3"><span class="num-string">7.5.3</span> Highlighting the period of decrease in sales</h3>
</div>
<div class="readable-text" id="p213">
<p>The period of decrease in sales ranges from 1992 to 1998. We want to highlight it to let the audience know that during this period, there were health problems in the salmon aquaculture. This will prepare the audience to consider respecting the safety rules to avoid the same problems in the future. We will add two elements to highlight this decreasing period: </p>
</div>
<ul>
<li class="readable-text" id="p214"> A light-gray rectangle covering the decreasing period </li>
<li class="readable-text" id="p215"> A textual annotation describing the health problems </li>
</ul>
<div class="readable-text" id="p216">
<p>The following listing shows the code to build the rectangle. </p>
</div>
<div class="browsable-container listing-container" id="p217">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.17</span> The rectangle</h5>
<div class="code-area-container">
<pre class="code-area">N = 100000000          <span class="aframe-location"/> #1
y = df['AMOUNT'].max() + N

rect_df = pd.DataFrame({'x': [1992], 
            'x2': [1998],
            'y' : [0],
            'y2': [y]
        })

rect = alt.Chart(rect_df).mark_rect(
    color='lightgrey',
    opacity=0.5
).encode(
    x='x:O',
    x2='x2:O',
    y= 'y:Q',
    y2= 'y2:Q'
)</pre>
<div class="code-annotations-overlay-container">
     #1 A magic number to set the upper part of the chart
     <br/>
</div>
</div>
</div>
<div class="readable-text print-book-callout" id="p218">
<p><span class="print-book-callout-head">Note</span>  First, build a DataFrame with the rectangle’s coordinates. Next, draw the rectangle using <code>mark_rect()</code>. </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p219">
<h5 class="callout-container-h5 readable-text-h5">Challenge: Which instructions could you write for Copilot to speed up the coding process? </h5>
</div>
<div class="readable-text" id="p220">
<p>You could try adding the following instruction to generate the rectangle: <code>#</code> <code>Add</code> <code>a</code> <code>rectangle</code> <code>starting</code> <code>from</code> <code>1993</code> <code>to</code> <code>2000</code>. What output would you obtain?</p>
</div>
</div>
<div class="readable-text" id="p221">
<p>The following listing shows the code to add the annotation. The decline in sales was partially due to fish health issues.</p>
</div>
<div class="browsable-container listing-container" id="p222">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.18</span> The annotation</h5>
<div class="code-area-container">
<pre class="code-area">ann_df = pd.DataFrame({'x': [1992, 1992, 1992],
            'y': [y, y-N/3*2, y-N/3*4],
            'text': ['The decline in sales was',
            'partially due to fish',
            'health issues']
            })

annotation = alt.Chart(ann_df
).mark_text(
    align='left',
    baseline='middle',
    fontSize=14,
    dx=5,
    dy=10
).encode(
    x='x:O',
    y='y:Q',
    text='text:N'
)

chart = (chart + text + rect + annotation
).configure_axis(
    labelFontSize=14,
    titleFontSize=16,
    grid=False
).configure_view(
    strokeWidth=0
)
chart.save('chart.xhtml')</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p223">
<p><span class="print-book-callout-head">Note</span>  First, build a DataFrame with the annotation text and its position information. Next, draw the annotation using <code>mark_text()</code>. Finally, plot and save the chart.</p>
</div>
<div class="readable-text" id="p224">
<p>Figure 7.17 shows the final chart, after adding context; we have turned information into knowledge. In the next chapter, we will further enrich context by adding some images, and in chapter 9, we will complete the story by adding the next step: wisdom.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p225">
<img alt="figure" height="625" src="../Images/7-17.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 7.17</span> The chart produced at the end of the information-to-knowledge phase</h5>
</div>
<div class="readable-text intended-text" id="p226">
<p>You have now implemented a practical example of turning information into knowledge. Before moving to the next chapter, let’s further solidify the concept by completing a practical exercise.</p>
</div>
<div class="readable-text" id="p227">
<h3 class="readable-text-h3"><span class="num-string">7.5.4</span> Exercise</h3>
</div>
<div class="readable-text" id="p228">
<p>Modify the previous chart as follows:</p>
</div>
<ol>
<li class="readable-text buletless-item" id="p229"> Tailor the chart to an audience of professionals. 
    <ol style="list-style: lower-alpha">
<li> Add points to the salmon line chart. Suggestion: Use <code>point=True</code> as a parameter of <code>mark_line().</code> </li>
<li> Add values for each point. Suggestion: Use <code>mark_text()</code> to add values for each point. </li>
</ol></li>
<li class="readable-text" id="p230"> Implement a RAG-based system to extract an annotation for the decreasing period from the <em>Governor’s Task Force on the Planning and Development of Marine Aquaculture in Maine Report and Recommendations</em> (pp. 28–32, <a href="https://mng.bz/jXqx">https://mng.bz/jXqx</a>). Suggestion: Use <code>PDFMinerLoader()</code> to extract data from PDF. You may need to install some additional Python packages, including pdf2image, pdfminer, and pdfminer.six. You can find the solution in the GitHub repository for the book under CaseStudies/aquaculture/from-information-to-knowledge/rag-annotation.py. </li>
</ol>
<div class="readable-text" id="p231">
<p>In the first part of this chapter, you learned how to turn information into knowledge by adding context to your data visualization chart. You saw that context depends on the audience reading your chart. For example, if your chart will be read by the general public, avoid technical details and use an appealing visualization. On the other hand, if your chart will be read by technical experts, add as many details as you can, while keeping the chart easy to read. In the second part of the chapter, you saw how to use generative AI tools as assistants to build your context. Finally, you learned where to put the textual context in your chart. In the next chapter, you will see how to add images to your chart to enrich context. </p>
</div>
<div class="readable-text" id="p232">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p233"> Adding context to your data visualization is crucial for turning information into knowledge. Textual context includes all the relevant facts and events useful for the audience to understand data. </li>
<li class="readable-text" id="p234"> When you build a chart, tailor it to the audience. In general, there are three types of audiences: the general public, professionals, and executives. </li>
<li class="readable-text" id="p235"> Use generative AI tools as assistants to help create context for your data. In particular, use ChatGPT to generate commentaries and annotations. </li>
<li class="readable-text" id="p236"> If ChatGPT needs to know custom data or topics, extend your LLM with fine-tuning or RAG. </li>
<li class="readable-text" id="p237"> Fine-tuning enables you to optimize a pretrained LLM based on a dataset of prompt–completion pairs. </li>
<li class="readable-text" id="p238"> Retrieval augmented generation uses an external database, called a <em>vector database</em>, to extend the LLM knowledge with domain-specific topics. </li>
</ul>
<div class="readable-text" id="p239">
<h2 class="readable-text-h2">References</h2>
</div>
<div class="readable-text" id="p240">
<h3 class="readable-text-h3">Embeddings</h3>
</div>
<ul>
<li class="readable-text" id="p241"> Lane, H. and Dyshel, M. (2024). <em>Natural Language Processing in Action</em> (2nd ed.). Manning Publications. </li>
<li class="readable-text" id="p242"> OpenAI. (n.d.). <em>Embeddings</em>. <a href="https://platform.openai.com/docs/guides/embeddings">https://platform.openai.com/docs/guides/embeddings</a>. </li>
</ul>
<div class="readable-text" id="p243">
<h3 class="readable-text-h3">Fine-tuning</h3>
</div>
<ul>
<li class="readable-text" id="p244"> Bantilan, N. (2023). <em>Fine Tuning vs. Prompt Engineering Large Language Models.</em> <a href="https://mlops.community/fine-tuning-vs-prompt-engineering-llms/">https://mlops.community/fine-tuning-vs-prompt-engineering-llms/</a>. </li>
<li class="readable-text" id="p245"> Jolley, E. (2023). <em>Introduction to Retrieval Augmented Generation</em>. <a href="https://arize.com/blog-course/introduction-to-retrieval-augmented-generation/">https://arize.com/blog-course/introduction-to-retrieval-augmented-generation/</a>.<span class="link-like"/> </li>
<li class="readable-text" id="p246"> Marcelo, X. (2023). <em>How to Fine-Tune OpenAI GPT</em>. <a href="https://medium.com/@marceloax.br/how-to-fine-tune-openai-gpt-3-d06741f915f4">https://medium.com/@marceloax.br/how-to-fine-tune-openai-gpt-3-d06741f915f4</a>.<span class="link-like"/> </li>
<li class="readable-text" id="p247"> OpenAI. (n.d.). <em>Fine-Tuning.</em> <a href="https://platform.openai.com/docs/guides/fine-tuning">https://platform.openai.com/docs/guides/fine-tuning</a>.<span class="link-like"/> </li>
</ul>
<div class="readable-text" id="p248">
<h3 class="readable-text-h3">LangChain</h3>
</div>
<ul>
<li class="readable-text" id="p249"> Biswas, A. (2023). <em>How to Work with LangChain Python Modules.</em> <a href="https://www.packtpub.com/article-hub/how-to-work-with-langchain-python-modules">https://www.packtpub.com/article-hub/how-to-work-with-langchain-python-modules</a>. </li>
<li class="readable-text" id="p250"> Geeks for Geeks. (2024). <em>Introduction to LangChain.</em> <a href="https://www.geeksforgeeks.org/introduction-to-langchain/">https://www.geeksforgeeks.org/introduction-to-langchain/</a>.<span class="link-like"/> </li>
<li class="readable-text" id="p251"> Pinecone. (n.d.). <em>LangChain: Introduction and Getting Started.</em> <a href="https://www.pinecone.io/learn/series/langchain/langchain-intro/">https://www.pinecone.io/learn/series/langchain/langchain-intro/</a>. </li>
</ul>
<div class="readable-text" id="p252">
<h3 class="readable-text-h3">LLM</h3>
</div>
<ul>
<li class="readable-text" id="p253"> De Angelis, L., Baglivo, F., Arzilli, G., Privitera, G. P., Ferragina, P., Tozzi, A. E., and Rizzo, C. (2023). <em>ChatGPT and the Rise of Large Language Models: The New AI-Driven Infodemic Threat in Public Health</em>. <em>Frontiers in Public Health</em>, <em>11</em>, 1166120. <a href="https://doi.org/10.3389/fpubh.2023.1166120">https://doi.org/10.3389/fpubh.2023.1166120</a>. </li>
<li class="readable-text" id="p254"> Google Developers. (n.d.). <em>Introduction to Large Language Models.</em> <a href="https://developers.google.com/machine-learning/resources/intro-llms?hl=en">https://developers.google.com/machine-learning/resources/intro-llms?hl=en</a>.<span class="link-like"/> </li>
</ul>
<div class="readable-text" id="p255">
<h3 class="readable-text-h3">RAG</h3>
</div>
<ul>
<li class="readable-text" id="p256"> Jolley, E. (2023). <em>Introduction to Retrieval Augmented Generation.</em> <a href="https://arize.com/blog-course/introduction-to-retrieval-augmented-generation">https://arize.com/blog-course/introduction-to-retrieval-augmented-generation</a>. </li>
<li class="readable-text" id="p257"> Needham, M. (2023). <em>Learn Data with Mark</em>. <a href="https://github.com/mneedham/LearnDataWithMark/tree/main">https://github.com/mneedham/LearnDataWithMark/tree/main</a>. </li>
<li class="readable-text" id="p258"> ———. (2023). <em>Retrieval Augmented Generation with OpenAI/GPT and Chrom</em><em>a.</em> <a href="https://www.youtube.com/watch?v=Cim1lNXvCzY">https://www.youtube.com/watch?v=Cim1lNXvCzY</a>. </li>
<li class="readable-text" id="p259"> Routu, V. (2023). <em>Answering with OpenAI and LangChain: Harnessing the Potential of Retrieval Augmented Generation (RAG).</em> <a href="https://www.linkedin.com/pulse/transforming-question-answering-openai-langchain-harnessing-routu/">https://www.linkedin.com/pulse/transforming-question-answering-openai-langchain-harnessing-routu/</a>.<span class="link-like"/> </li>
<li class="readable-text" id="p260"> Schwaber-Cohen, R. (2023). <em>What Is a Vector Database?</em> <a href="https://www.pinecone.io/learn/vector-database">https://www.pinecone.io/learn/vector-database</a>.<span class="link-like"/> </li>
</ul>
<div class="readable-text" id="p261">
<h3 class="readable-text-h3">Thinking for the audience</h3>
</div>
<ul>
<li class="readable-text" id="p262"> Bettes, S. (2019). <em>Technical and Professional Writing Genres.</em> <a href="https://open.library.okstate.edu/technicalandprofessionalwriting/chapter/chapter-2/">https://open.library.okstate.edu/technicalandprofessionalwriting/chapter/chapter-2/</a>. </li>
<li class="readable-text" id="p263"> Emery, A. K. (2021). <em>Why “Know Your Audience” Is Terrible Dataviz Advice—And What to Do Instead.</em> <a href="https://depictdatastudio.com/why-know-your-audience-is-terrible-dataviz-advice-what-to-do-instead/">https://depictdatastudio.com/why-know-your-audience-is-terrible-dataviz-advice-what-to-do-instead/</a>.<span class="link-like"/> </li>
<li class="readable-text" id="p264"> QuantHub. (2023). <em>How to Identify Your Audience for Impactful Data Storytelling.</em> <a href="https://www.quanthub.com/how-to-identify-your-audience-for-impactful-data-storytelling/">https://www.quanthub.com/how-to-identify-your-audience-for-impactful-data-storytelling/</a>.<span class="link-like"/> </li>
<li class="readable-text" id="p265"> LinkedIn Community with AI. (n.d.). <em>How Do You Engage and Nurture Your Technical Audience and Build Trust and Authority?</em> <a href="https://www.linkedin.com/advice/0/how-do-you-engage-nurture-your-technical-audience">https://www.linkedin.com/advice/0/how-do-you-engage-nurture-your-technical-audience</a>.<span class="link-like"/> </li>
<li class="readable-text" id="p266"> WirelessLAN Professionals. (n.d.). <em>How to Present to a Technical Audience.</em> <a href="https://wlanprofessionals.com/how-to-present-to-a-technical-audience/">https://wlanprofessionals.com/how-to-present-to-a-technical-audience/</a>.<span class="link-like"/> </li>
</ul>
<div class="readable-text" id="p267">
<h3 class="readable-text-h3">Transformers</h3>
</div>
<ul>
<li class="readable-text" id="p268"> Koenigstein, N. (2024). <em>Transformers in Action</em>. Manning Publications. </li>
<li class="readable-text" id="p269"> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). “Attention Is All You Need.” <em>Advances in Neural Information Processing Systems</em>, <em>30</em>. </li>
</ul>
</div></body></html>