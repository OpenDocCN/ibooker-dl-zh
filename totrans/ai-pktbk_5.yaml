- en: 6 The fine print
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Controversial and timely discussions around AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copyright disputes regarding training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The economics of AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exaggeration about AI’s performance and advancements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI regulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consumption of resources, such as electricity and water, to train and use AI
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The philosophical debate around AI, biological brains, and consciousness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter addresses some of the bigger questions around AI. It also reveals
    a less flattering AI side—how the field often suffers from exaggeration, speculation,
    and even deception. I think it’s important to be informed about these topics,
    so you can analyze AI announcements and discussions critically. In addition, if
    you’re building AI-based products or using AI intensively, you may want to be
    aware of the broader effects and potential controversies that could arise from
    your work.
  prefs: []
  type: TYPE_NORMAL
- en: Copyright
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large AI models, such as LLMs and text-to-image models, have been trained using
    data collected from the internet, or *scraped,* most often without authorization
    from its owners. This includes millions of documents, images, and books, which
    has made many people angry, and there have been many lawsuits against AI providers.
    One example is a lawsuit from Getty Images, a website that sells stock images,
    against Stability AI, which creates the Stable Diffusion text-to-image models.
    Getty Images argues that Stability AI used images collected from Getty’s website
    without authorization to train its models. The complaint shows images generated
    by Stability AI’s models, which are similar to those sold on Getty. In some cases,
    the AI model even generates images with a rough imitation of Getty Images’ watermark
    (see figure 6.1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F01_Maggiori.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1  Left: Image sold on Getty Images’ website. Right: Image generated
    by a Stable Diffusion model. Note the watermark in the image. These images are
    reproduced from Getty Images (US), Inc. v. Stability AI, Inc., 1:23-cv-00135,
    (D. Del.)*.*'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A similar lawsuit was filed by *The* *New York Times* against OpenAI on the
    grounds that newspaper articles were scraped without authorization to train OpenAI’s
    models. The complaint contains examples of large portions of text outputted by
    GPT-4, which are verbatim reproductions of text found in *The New York Times*
    (see figure 6.2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F02_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2  Example of GPT-4 output (almost) verbatim text as published by The
    New York Times. Figure reproduced from The New York Times Company v. Microsoft
    Corporation, 1:23-cv-11195, (S.D.N.Y.).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In addition, a group of artists sued Midjourney, Stability AI, and other image-generation
    providers for using images of the plaintiffs’ work to train their models. They
    argue this allows the models to generate images “in the style of” the plaintiffs
    ([https://mng.bz/ZlKa](https://mng.bz/ZlKa)). It is likely that many other copyright
    infringement allegations will be made against AI providers in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copyright infringement is usually alleged on two grounds:'
  prefs: []
  type: TYPE_NORMAL
- en: AI models sometimes reproduce verbatim content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI providers use copyrighted data without authorization to train models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verbatim reproductions happen when a model memorizes training data, which is
    a result of overfitting. It is likely that AI providers will try to minimize this
    by using techniques to prevent overfitting. It’s hard to guarantee that no memorization
    will happen, but it might be mitigated successfully.
  prefs: []
  type: TYPE_NORMAL
- en: The second point—that data is used to train models without authorization—is
    more controversial and seems to be the crux of the problem. Supporters of AI providers
    argue that it isn’t a copyright violation. They think it is legitimate to scrape
    data to train a model because the goal is for the model to learn patterns and
    associations from data, not to reproduce a verbatim copy of the data (even if
    that has happened in some unfortunate cases).
  prefs: []
  type: TYPE_NORMAL
- en: I’ve even heard some people argue that us humans learn from reading publicly
    available data, and we then use that knowledge to create our own work. So, why
    wouldn’t AI providers be able to do the same?
  prefs: []
  type: TYPE_NORMAL
- en: The key to this conundrum hinges on the topic of *fair use.* In copyright law,
    it is considered that copying data without authorization is fair in some circumstances.
    This includes copying the data to help build a product that does not replace or
    compete against the original product. For example, throughout this book, I have
    reproduced quotations from other books. I never reached out to their authors to
    ask for permission. This is considered fair use because my quotations don’t make
    this book compete against the other books, and the original author of the quotation
    is clearly attributed. In chapter 1, for instance, I quoted a paragraph from the
    book *The Elements of Statistical Learning.* However, this book covers a different
    topic, so it doesn’t intend to compete with it, stealing some of its customers.
    In fact, I may actually drive some publicity toward that book by mentioning it.
    Had I copied an entire chapter of that book, however, this wouldn’t be considered
    fair use because my book could become a replacement for it. There are no exact
    guidelines on what constitutes fair use, such as a precise number of words in
    a quotation, so this is usually determined case by case in a dispute resolution.
  prefs: []
  type: TYPE_NORMAL
- en: The lawsuits by Getty Images and *The New York Times* attempted to establish
    that scraping their data by AI providers was not fair use because they used it
    to build competing products. This allegation is particularly easy to visualize
    in the case of image generation—one can imagine that customers of Getty Images
    may use Stable Diffusion instead to create images.
  prefs: []
  type: TYPE_NORMAL
- en: As Getty Images argues ([https://mng.bz/RVgO](https://mng.bz/RVgO)),
  prefs: []
  type: TYPE_NORMAL
- en: Stability AI has copied at least 12 million copyrighted images from Getty Images’
    websites. . . . Stability AI now competes directly with Getty Images.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The New York Times* lawsuit also tries to establish that OpenAI’s models act
    as a replacement to the newspaper’s website ([https://mng.bz/2yvm](https://mng.bz/2yvm)):'
  prefs: []
  type: TYPE_NORMAL
- en: Defendants insist that their conduct is protected as “fair use” because their
    unlicensed use of copyrighted content to train GenAI models serves a new “transformative”
    purpose. But there is nothing “transformative” about using *The Times*’s content
    without payment to create products that substitute for *The Times* and steal audiences
    away from it.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As of this writing, these disputes haven’t been settled. We’ll see what happens
    in court.
  prefs: []
  type: TYPE_NORMAL
- en: I think one likely outcome from successive disputes is that regulators will
    request AI providers to honor opt-out requests—if a data owner doesn’t want their
    data used to train AI models, it shouldn’t be used. The data owner will have to
    indicate their wish to opt out through machine-readable metadata in an agreed
    format. This is how it works if you don’t want search engines to scrape and index
    your content. You must specify so in a text file called robots.txt, returned upon
    request to your root domain (e.g., [example.com/robots.txt](http://example.com/robots.txt)).
    In a special format, the file describes which sections of the website are allowed
    to be scraped and by whom. All major search engines honor the protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'A different controversy is whether AI-generated content is itself protected
    by copyright law. For example, if you generate an image using Midjourney, can
    you prevent others from reproducing the AI-generated image, as it’s a violation
    of *your* copyright? The Copyright Alliance argues that work solely generated
    by AI is not protected by copyright. However, it clarifies ([https://mng.bz/1Xnn](https://mng.bz/1Xnn)):'
  prefs: []
  type: TYPE_NORMAL
- en: If a work contains both AI-generated elements and elements of human authorship
    protectable by copyright law—such as human-authored text or a human’s minimally
    creative arrangement, selection, and coordination of various parts of the work—the
    elements of the work that are protected by copyright would be owned by the human
    author.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I’m not quite sure what this means. If I use Midjourney to generate an image,
    is the work solely generated by AI, or am I the work’s coordinator because I wrote
    and refined the prompt? Perhaps the Copyright Alliance doesn’t know yet, as it
    adds after that paragraph, “AI and copyright issues will continue to develop,”
    and it invites you to sign up for the newsletter on AI copyright to stay up to
    date.
  prefs: []
  type: TYPE_NORMAL
- en: Economics of AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the boom of generative AI, we hear a lot about its potential economic
    rewards. By the sounds of it, a lot of people will make a lot of money thanks
    to AI. But is that so?
  prefs: []
  type: TYPE_NORMAL
- en: Some AI providers are already collecting billions in revenue. In 2024, for example,
    OpenAI generated $3.7 billion. This is quite impressive for such a young company.
  prefs: []
  type: TYPE_NORMAL
- en: But revenue is not enough to build a successful business in the long run. For
    that, a business must become profitable—it must collect more revenue than it spends
    on generating it. Otherwise, it can’t pay the bills unless there’s a continued
    injection of cash from investors to subsidize its losses.
  prefs: []
  type: TYPE_NORMAL
- en: In 2024, OpenAI *lost* $5 billion. While its $3.7-billion revenue was impressive,
    it wasn’t enough to cover its even more impressive expenses ([https://mng.bz/PdMv](https://mng.bz/PdMv)).
    This was likely related to the high costs of training and serving large AI models.
    Some people have estimated that running ChatGPT might cost OpenAI $700,000 a day
    ([https://mng.bz/JYna](https://mng.bz/JYna)). Training GPT-4 is said to have cost
    the company $100 million ([https://mng.bz/wJma](https://mng.bz/wJma)). Note that
    some models are retrained periodically with new data, so model training is not
    always a one-off expense. The other major AI providers, such as Anthropic and
    Mistral, are also still unprofitable.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to becoming profitable, a business is successful if it generates
    *good* profits—investors want good bang for their buck. In a competitive market,
    profits tend be eroded over time because copycats enter the market, pushing costs
    up and prices down, so it’s hard to make consistently good profits. The latter
    requires a *moat,* also known as a *competitive advantage,* which is a feature
    that protects a company’s market share from competitors. When a business benefits
    from a moat, competitors can’t enter the market on equal terms, so it’s hard or
    too expensive for them to eat into your market share.
  prefs: []
  type: TYPE_NORMAL
- en: AI providers don’t seem to have a strong moat protecting their market shares.
    In particular, the methodology behind LLMs (the transformer architecture discussed
    in chapter 1) is publicly known, so others can build their own competing models.
    AI providers are a bit uneasy about this. In May 2023, a leaked Google memo said,
    “We have no moat and neither does OpenAI. . . . The uncomfortable truth is, we
    aren’t positioned to win this arms race and neither is OpenAI. . . .While our
    AI still holds a slight edge in terms of quality, the gap is closing astonishingly
    quickly. Open-source AI is faster, more customizable, more private, and pound-for-pound
    more capable.” The memo also admitted, “We have no secret sauce,” and it suggested,
    “People will not pay for restricted AI when free, unrestricted alternatives are
    comparable in quality. We should consider where our value add really is” (Emmanuel
    Maggiori, 2024, *Siliconned*).
  prefs: []
  type: TYPE_NORMAL
- en: Because there’s no secret sauce, the models created by different providers are
    already converging in terms of performance and capabilities, including open source
    ones. It is conceivable that there will be a market shake-up at some point—some
    companies may go out of business or discontinue their products. The economic case
    for developing large AI models is not as clear as it may seem initially.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the big players, numerous smaller companies are building AI-based
    products, which are built on top of foundation models—some people call them “AI
    wrappers.” For example, there are tens of companies that offer an AI tool to turn
    an ordinary picture of you into a professional-looking headshot. These tools are
    likely a thin layer added on top of a publicly available AI model such as Stable
    Diffusion, or perhaps a fine-tuned version of one of them. This might seem like
    a winning business idea at first because you’re genuinely making it easy to create
    headshots for people. However, there is no moat—the “secret sauce” of these apps
    is the prompt which, unless it’s very special, others will probably be able to
    come up with too. So, competition multiplies, as we can already see from the multiple
    apps offering similar functionality. It will be hard for these thin businesses
    to generate significant profits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, much has been said about a dramatic increase in business productivity
    thanks to the use of AI tools. In 2023, McKin­sey shared the following estimates
    ([https://mng.bz/qxz6](https://mng.bz/qxz6)):'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI’s impact on productivity could add trillions of dollars in value
    to the global economy. Our latest research estimates that generative AI could
    add the equivalent of $2.6 trillion to $4.4 trillion annually across the 63 use
    cases we analyzed—by comparison, the United Kingdom’s entire GDP in 2021 was $3.1
    trillion.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'But productivity increases have been pretty much undetectable so far. A 2024
    *Economist* article explains:'
  prefs: []
  type: TYPE_NORMAL
- en: Macroeconomic data . . . show little evidence of a surge in productivity . .
    . In America, the global centre of AI, output per hour remains below its pre-2020
    trend. Even in global data derived from surveys of purchasing managers, which
    are produced with a shorter lag, there is no sign of a productivity surge. (“What
    happened to the artificial-intelligence revolution?” 2024, July 2, The Economist)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The article also explains that the rate of adoption of AI in the business world
    has been very slow due to “concerns about data security, biased algorithms and
    hallucinations.” It concludes, “So far the technology has had almost no economic
    impact.” Indeed, it seems that implementing AI in business is harder than it may
    initially seem. Someone recently told me that the problem was the “last mile”—while
    AI can help you do the initial 80% of a job just fine, it’s hard to make it complete
    the remaining 20% well because of hallucinations or the need for painstaking customization.
    This makes productivity gains less impressive than promised.
  prefs: []
  type: TYPE_NORMAL
- en: So, I advise you to be cautious when you hear big statements about AI’s economic
    benefits. The jury is still out.
  prefs: []
  type: TYPE_NORMAL
- en: Smoke and mirrors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In November 2023, it was revealed that self-driving cars produced by Cruise
    weren’t quite driving themselves. An army of human operators in a remote-control
    room manually intervened when the cars faced problems. This happened once every
    2.5 to 5 miles of driving. The company had 1.5 employees doing this job for every
    car on the streets ([https://mng.bz/7pM7](https://mng.bz/7pM7)). Business professor
    Thomas W. Malone said, “It may be cheaper just to pay a driver to sit in the car
    and drive it” ([https://mng.bz/mGpW](https://mng.bz/mGpW)).
  prefs: []
  type: TYPE_NORMAL
- en: A few months later, Waymo, which is Cruise’s main competitor, explained in a
    blog article, “Much like phone-a-friend, when the Waymo vehicle encounters a particular
    situation on the road, the autonomous driver can reach out to a human fleet response
    agent for additional information to contextualize its environment” ([https://waymo.com/blog/2024/05/fleet-response/](https://waymo.com/blog/2024/05/fleet-response/)).
  prefs: []
  type: TYPE_NORMAL
- en: Something similar happened with Amazon’s “just walk out” technology, installed
    in Amazon’s supermarkets. This technology allegedly used AI to automatically prepare
    your shopping receipt based on footage from cameras installed on the ceiling.
    In April 2024, a reporter revealed that 1,000 remote workers in India were watching
    the videos and manually preparing or adjusting at least 70% of receipts ([https://mng.bz/5gX8](https://mng.bz/5gX8)).
  prefs: []
  type: TYPE_NORMAL
- en: The use of humans to secretly power AI is often compared to the Mechanical Turk,
    a fraudulent machine constructed in 1770, which seemed to play chess by itself
    when, in reality, a human secretly powered it. The machine was exhibited on tours
    for 84 years.
  prefs: []
  type: TYPE_NORMAL
- en: The AI field is plagued with big promises, hype, and exaggeration. Mechanical
    Turks are just one example of this—exaggeration and deception come in different
    forms. In April 2023, for example, Google executives claimed that one of their
    AI models had learned the Bengali language even though it hadn’t been trained
    on Bengali-language text. One of them explained, “We discovered that with very
    few amounts of prompting in Bengali, it can now translate all of Bengali” ([https://futurism.com/the-byte/google-ai-bengali).They](https://futurism.com/the-byte/google-ai-bengali).They)
    argued that this was an example of AI having “emergent properties.”
  prefs: []
  type: TYPE_NORMAL
- en: The news went viral. An Indian newspaper pondered, “AI learns Bengali on its
    own, should we be worried?” ([https://mng.bz/6eyp](https://mng.bz/6eyp)). Someone
    who heard about this news reached out to me asking if I thought we might soon
    face a “singularity event”—a dramatic explosion of AI’s capabilities—since now
    AI could learn new stuff on its own.
  prefs: []
  type: TYPE_NORMAL
- en: With an understanding of how current AI works (see chapter 1), it’s hard to
    believe it could easily learn a new language that is not part of its training
    data. As it turns out, Bengali was indeed one of the languages the model was trained
    on, contrary to what the Google executives had said ([https://mng.bz/oKYy](https://mng.bz/oKYy)).
  prefs: []
  type: TYPE_NORMAL
- en: More recently, in September 2024, OpenAI launched a new model called OpenAI
    o1\. The company framed it as a model capable of “thinking” and “reasoning.” The
    announcement explained, “We are introducing OpenAI o1, a new large language model
    trained with reinforcement learning to perform complex reasoning. o1 thinks before
    it answers—it can produce a long internal chain of thought before responding to
    the user” ([https://mng.bz/nROV](https://mng.bz/nROV)). The article used the word
    “think” 9 times and the word “reason” 17 times. This framing made it sound like
    a major improvement and perhaps a departure from the usual autocompleting LLMs.
    It also sounded like a step toward more human-like AI—the announcement said the
    model could spend more time thinking before responding, “much like a person would.”
  prefs: []
  type: TYPE_NORMAL
- en: 'But once we look beyond the marketing material, we realize that the o1 system
    isn’t as novel as it seems. It works as follows: first, an LLM is used to generate
    a piece of text with a suggested list of steps to solve the problem. These instructions
    are then added to the end of the original prompt. So, the new prompt contains
    the original task followed by a suggested step-by-step recipe to perform it. Afterward,
    this extended prompt is run through an LLM as usual. This mimics the popular chain-of-thought
    prompting technique, in which the user adds a step-by-step guideline of how to
    solve a problem to the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The announcement emphasized that reinforcement learning was used to train the
    system: “Our large-scale reinforcement learning algorithm teaches the model how
    to think productively using its chain of thought in a highly data-efficient training
    process.” This may sound impressive, but it’s probably nothing new. OpenAI has
    been using reinforcement learning with human feedback (RLHF) to refine all its
    models for quite some time (see chapter 1). It’s likely that by “reinforcement
    learning” they meant that humans manually wrote down a small dataset of examples
    of the step-by-step instructions they wanted the LLM to generate, and the LLM
    was refined to produce such instructions more accurately.'
  prefs: []
  type: TYPE_NORMAL
- en: I advise you to be cautious whenever you hear impressive AI announcements. I
    recommend keeping in mind how current AI works when you analyze announcements,
    which makes it easier to read between the lines and separate the wheat from the
    chaff.
  prefs: []
  type: TYPE_NORMAL
- en: Regulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In August 2024, regulation concerning AI came into force in the European Union,
    known as the AI Act. The AI Act applies to AI that is used, or whose outputs are
    used, inside the EU, even if it’s developed and run elsewhere. The regulation
    has been controversial, with some people deeming it insufficient and others excessive.
    Either way, let’s have a quick discussion about it because you might be affected
    (e.g., you might develop an AI-based product used in the EU) and because it may
    become the blueprint for future AI regulation elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: The AI Act contains four special chapters that are especially relevant to developers
    and users of AI systems. We briefly comment on each of them below. You can read
    the full text online ([https://mng.bz/vKWm](https://mng.bz/vKWm)) or have a look
    at the official high-level summary ([https://mng.bz/4aQ5](https://mng.bz/4aQ5)).
  prefs: []
  type: TYPE_NORMAL
- en: Prohibited AI practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This part of the Act describes a list of AI practices that are outright prohibited
    as they’re considered serious violations. These include AI used to manipulate
    or deceive people, AI that exploits people’s vulnerabilities “due to their age,
    disability or a specific social or economic situation,” and AI for social scoring,
    among other categories.
  prefs: []
  type: TYPE_NORMAL
- en: High-risk systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This part contains stipulations that apply to high-risk products. These are
    products that are already regulated by the EU and require a third-party conformity
    assessment, such as certain vehicles, machinery, and medical devices. It also
    adds a few more categories to the list, such as AI for targeted job ads and AI
    for visa applications. The Act imposes several requirements on these high-risk
    systems, including enabling human oversight “to understand its capabilities and
    limitations, detect and address issues, avoid over-reliance on the system, interpret
    its output, decide not to use it, or stop its operation.”
  prefs: []
  type: TYPE_NORMAL
- en: Transparency obligations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This part requires companies to inform users when they’re interacting with an
    AI system (“unless it's obvious or the AI is used for legal purposes like crime
    detection”), which specifically applies to “an AI system that generates or manipulates
    image, audio or video content constituting a deep fake.” This is the case even
    with systems that are not deemed high risk. Note that if you use AI to generate
    content but then you thoroughly review the content and hold editorial responsibility
    over it, you no longer need to inform others about using AI.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, don’t worry about the AI Act ruining your AI art—you can indicate
    that you’re using AI “in an appropriate manner that does not hamper the display
    or enjoyment of the work.”
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This part imposes requirements on foundation models, which are denoted by “general-purpose
    AI models.” The Act requires the AI provider to write documentation detailing
    the model’s development, including “information on the data used for training,
    testing and validation” and “known or estimated energy consumption of the model.”
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there’s a special category of very large foundation models the
    Act deems to pose “systemic risk.” These are models that exceed a certain threshold
    in terms of the amount of training (the threshold is currently set to 1025 floating-point
    operations during training). The creators of these models must notify the EU of
    their work, and the EU might impose additional requirements to mitigate risk.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the Act approves training models from scraped data without authorization,
    so long as opt-outs are respected. This was approved indirectly by referring the
    reader to a directive that allows web scraping with the goal of data mining for
    analytics purposes. Some people have criticized this directive saying that “data
    mining” is too broad and could cover pretty much anything ([https://mng.bz/QDa1](https://mng.bz/QDa1)).
  prefs: []
  type: TYPE_NORMAL
- en: Resource consumption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training and using AI models consumes electricity and other resources, the scale
    of which has been criticized. For example, a journalist called AI “a disaster
    for the climate” ([https://mng.bz/Xxzl](https://mng.bz/Xxzl)).
  prefs: []
  type: TYPE_NORMAL
- en: It is difficult to gauge AI’s electricity consumption because providers haven’t
    yet reported it consistently. So, we have to rely on studies made by other people.
    These studies aren’t quite standardized, so they’re a bit messy and difficult
    to follow. Some of them even mix different units within the same report in a chaotic
    way, such as kWh, CO2 emissions, and “equivalent number of smartphone charges”
    ([https://arxiv.org/pdf/2311.16863](https://arxiv.org/pdf/2311.16863)). Sometimes
    researchers rely on hearsay and loose logical connections to calculate consumption.
    For example, one researcher deduced LLMs’ energy consumption indirectly from the
    fact that a Google executive said in an interview that LLMs likely consumed 10
    times more power than performing a Google search ([https://mng.bz/yW57](https://mng.bz/yW57)).
  prefs: []
  type: TYPE_NORMAL
- en: In the following, I’ll share some results from a study presented by a group
    of researchers from Hugging Face and Carnegie Mellon University. The researchers
    used multiple open source models with their own GPUs and measured consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 shows electricity consumption reported by the researchers for text
    and image generation ([https://arxiv.org/pdf/2311.16863](https://arxiv.org/pdf/2311.16863)).
    Consumption figures are the average across different models studied by the researchers
    (individual consumption per model was not reported in a consistent manner).
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1  Average electricity consumption across different models compared
    with typical household consumption
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '|  | kWh / 1,000 responses | % of daily household kWh (US) | % of daily household
    kWh (UK) |'
  prefs: []
  type: TYPE_TB
- en: '| Text generation | 0.047 | 0.15% | 0.5% |'
  prefs: []
  type: TYPE_TB
- en: '| Image generation | 2.907 | 9.83% | 30% |'
  prefs: []
  type: TYPE_TB
- en: Note that the figures are per 1,000 uses of the model, such as generating an
    entire response 1,000 times with an LLM or generating 1,000 images with a text-to-image
    model. One thousand uses of AI may seem like a lot, but it might easily be reached
    by intensive users in less than a day. For example, a coder using GitHub Copilot
    might generate hundreds of LLM-based autocompletions every hour. Moreover, many
    of our regular online actions, such as performing a Google search or browsing
    an online store, may trigger LLM queries (Google is already showing AI results
    with searches), which would add more LLM usage even if the user doesn’t use LLMs
    directly. We can also imagine that a small group of graphic designers might generate
    1,000 images in a short time frame by prompting the system repeatedly to create
    images and adjust the result.
  prefs: []
  type: TYPE_NORMAL
- en: In these experiments, image generation was much more power-hungry than text
    generation. However, the researchers didn’t reveal the prompt used for text generation
    or how much text was generated each time. In addition, they only used text-generation
    models on the smaller end of the spectrum, such as GPT-2 models, which are 100
    times smaller than the generation that succeeded them. The authors reported significant
    variability across models. In particular, the largest image-generation model consumed
    6,000 times as much power as the smallest one.
  prefs: []
  type: TYPE_NORMAL
- en: Note that AI models are constantly being optimized, so consumption could be
    reduced in the future—sometimes a model can be made much smaller without significantly
    reducing its capabilities. For reference, I’ve added two columns to table 6.1
    that compare AI consumption with the total daily electricity consumption by the
    typical US ([https://www.eia.gov/tools/faqs/faq.php?id=97&t=3](https://www.eia.gov/tools/faqs/faq.php?id=97&t=3))
    and UK ([https://mng.bz/MDQE](https://mng.bz/MDQE)) households.
  prefs: []
  type: TYPE_NORMAL
- en: The greatest worry is not electricity consumption itself, but the CO2 emitted
    to generate it. *Carbon intensity* measures the grams of CO2 emitted per kWh consumed,
    and it varies depending on how power is generated. Table 6.2 restates the above
    results in terms of CO2 emitted based on typical carbon intensity in the US ([https://mng.bz/av6x](https://mng.bz/av6x))
    and the UK ([https://mng.bz/gaXZ](https://mng.bz/gaXZ); both countries produce
    electricity from different sources, so their carbon intensity differs).
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.2  Comparison of the electricity consumption from table 6.1 with equivalent
    CO2 emissions of petrol cars
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '|  | Grams of CO2 / 1,000 responses (US) | Miles driven for equivalent CO2
    (US) | Grams of CO2 / 1,000 responses (UK) | Miles driven for equivalent CO2 (UK)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Text generation | 20 | 0.05 miles | 7.6 | 0.02 miles |'
  prefs: []
  type: TYPE_TB
- en: '| Image generation | 1,200 | 3.1 miles | 470 | 1.2 miles |'
  prefs: []
  type: TYPE_TB
- en: To put things in perspective, the table includes the number of miles you’d have
    to drive a car to emit the same amount of CO2 ([https://mng.bz/av6x](https://mng.bz/av6x)).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the electricity required to use AI models, many people have stressed
    that *training* them is a power-hungry activity. It’s been estimated that training
    GPT-3 consumed 1,287 MWh ([https://arxiv.org/pdf/2104.10350](https://arxiv.org/pdf/2104.10350)).
    This amounted to the electricity consumed in one day by 43,000 US households or
    134,000 UK households. Note that, while models are only trained sporadically,
    AI providers train or retrain multiple models a year.
  prefs: []
  type: TYPE_NORMAL
- en: Using and training AI models also consumes other resources, such as water for
    cooling down data centers. An article in *Fortune* explained, “Microsoft disclosed
    that its global water consumption spiked 34% from 2021 to 2022 (to nearly 1.7
    billion gallons, or more than 2,500 Olympic-sized swimming pools), a sharp increase
    compared to previous years that outside researchers tie to its AI research” ([https://mng.bz/eyNw](https://mng.bz/eyNw)).
  prefs: []
  type: TYPE_NORMAL
- en: When you use AI, I recommend you keep in mind that “cloud computing” actually
    happens on Earth, inside large refrigerated buildings, and this can be resource-intensive
    and have an influence on the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Brains and consciousness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s finish on a lighter and more philosophical note. It is common to compare
    the structure of AI models with our own biological brains. If you remember from
    chapter 1, LLMs perform lots of projections, which are mathematical operations
    that involve matrix multiplications. Biological neurons have been traditionally
    described as performing a similar calculation, so many ML models, including LLMs,
    are categorized as *artificial neural networks.* In addition, some ML model architectures
    have been compared with the structure of specific parts of our brains. For example,
    convolutional neural networks (CNNs) are often compared with the brain’s visual
    cortex.
  prefs: []
  type: TYPE_NORMAL
- en: 'In reality, we still don’t quite understand how brains work. For example, the
    traditional understanding of the calculations made by neurons is too simple (Penrose,
    R., 1989, *The Emperor''s New Mind: Concerning Computers, Mind, and the Laws in
    Physics*. Oxford University Press, p. 511). Over the years, much more complicated
    models have been developed. However, these models still cannot predict what scientists
    observe when studying the workings of real neurons. For example, in 2020, a group
    of researchers discovered that the dendrites that pass signals from one neuron
    to another may actually carry out complicated computations (Gidon, A. et al.,
    2020, “Dendritic action potentials and computation in human layer 2/3 cortical
    neurons,” *Science*, *367*[6473], pp. 83–87). So, they aren’t just wires that
    carry signals as previously thought. To complicate things even more, the fluid
    that surrounds neurons contains molecules, known as neuromodulators, which affect
    neurons’ behavior in a way that isn’t fully understood. While progress has been
    made, our understanding of neurons and brains is still quite poor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As of today, the brain of only one organism has been fully mapped out, meaning
    that researchers could create a map of all connections between neurons, or *connectome*.
    The organism is a tiny worm called C. Elegans*,* which has around 300 neurons
    and 7,000 connections among them. However, it was impossible to simulate the observed
    worm’s behavior, as the map just tells us which neurons are connected to which
    but not exactly how they work. Neuroscientist Anthony Movshon concluded that the
    “connectome by itself has not explained anything" (Jabr, F., 2012, “The Connectome
    Debate: Is Mapping the Mind of a Worm Worth It?” *Scientific American,* [https://mng.bz/pKaE](https://mng.bz/pKaE)).'
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, AI models are designed without considering brain structures,
    and the brain analogy is forced later on. For example, the initial articles describing
    CNNs did *not* say that these were inspired by the brain. The researchers claimed
    their design decisions were “guided by our prior knowledge about shape recognition”
    (LeCun, Y. et al., 1989, “Handwritten digit recognition with a back-propagation
    network,” *Advances in Neural Information Processing Systems*, 2). Years later,
    when CNNs became popular, the same researchers claimed that they were “directly
    inspired by the classic notions of simple cells and complex cells in visual neuroscience,
    and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in
    the visual cortex ventral pathway” (LeCun, Y., Bengio, Y., & Hinton, G., 2015,
    “Deep learning,” *Nature, 521*[7553], pp. 436–444).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, analogies are often quite loose. For example, the comparison between
    CNNs and the visual cortex only works if we ignore some known things about the
    visual cortex that are not a part of CNNs (see *Smart Until It’s Dumb*, Chapter
    2).
  prefs: []
  type: TYPE_NORMAL
- en: So, be cautious whenever you hear analogies between AI and brains. We still
    don’t understand brains, so the connection is likely to be highly speculative.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to brain-related speculation, the latest AI boom has also reignited
    the consciousness debate. Just to cite an example, in 2022, the news went viral
    that an AI engineer claimed Google’s chatbot had become sentient ([https://mng.bz/OBn2](https://mng.bz/OBn2)).
  prefs: []
  type: TYPE_NORMAL
- en: But, just like with brains, we don’t quite understand consciousness. We do know
    that some parts of the brain are in charge of unconscious actions (like controlling
    heartbeat), while others are related to conscious perceptions (like vision), but
    we don’t understand why some parts contribute to our consciousness, while others
    don’t. We also don’t understand how general anesthesia works; we just know from
    experience that anesthetics turn off consciousness temporarily, but we don’t know
    the mechanism behind it.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are many philosophical questions around consciousness that
    don’t have an easy answer. For example, some people think that any computation
    gives rise to consciousness. Under this view, a thermostat is conscious but in
    a different way. Other people, like physicist Roger Penrose, think consciousness
    doesn’t arise from computation at all and thus cannot be created with digital
    computers. The debate is still ongoing, and I’m not sure we’ll ever be able to
    determine whether a thermostat is conscious.
  prefs: []
  type: TYPE_NORMAL
- en: So, I advise you to be cautious when anyone claims to have a definitive answer
    about the link between AI and consciousness. There is so much we don’t know.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve now reached the end of this book, let’s quickly reflect on the content
    covered. Throughout this book, we’ve discussed the power of AI—how ML innovations
    have pushed the boundaries of what AI can do. We’ve also discussed AI’s limitations—how
    sometimes AI hallucinates or isn’t as useful as it seems at first sight. Because
    AI is not all-powerful, its effects will vary depending on the context—sometimes
    AI may automate away jobs, but other times it may not; sometimes it may be the
    best tool for a task, but other times it may not; and so on. In this book, I tried
    to cover both sides of that debate and share advice accordingly. The last chapter
    completed our analysis by discussing some of the bigger questions surrounding
    AI, many of which are still unanswered and are likely to be hot topics in the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The copyright debate hinges on the interpretation of *fair use.* AI providers
    argue that they scrape data so that their models can learn general patterns and
    that they don’t intend to reproduce the original data, implying it’s a fair use
    of that data. Data owners argue that AI providers use this data to build competing
    products and steal their customers, so this isn’t fair use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The economic case for AI is not that clear. AI providers are still largely unprofitable
    and face fierce competition. Smaller companies that create thin AI wrappers also
    face fierce competition and may struggle to make ends meet. Productivity gains
    in the wider economy due to AI have not yet been observed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AI field has a tendency to exaggerate or even deceive. Many products that
    allegedly used AI have been revealed to rely on remote human operators to do the
    job manually. Big AI announcements are often incorrect (like Google saying a model
    learned a language that wasn’t in its training data) or spruced up (like OpenAI
    saying its model “thinks” and “reasons”).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The amount of electricity consumption (and other resources like water) to train
    and run AI models has received a lot of criticism, with some people arguing it
    will have detrimental environmental effects. Studies and reports about AI resource
    consumption are still scarce and preliminary, but we can see that it isn’t a negligible
    amount.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparisons between AI models and the structure of the brain are highly speculative.
    We don’t quite understand how brains work yet, so comparisons tend to be forced.
    The same goes for AI and consciousness—it’s still an ongoing debate without clear-cut
    answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
