<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">4</span></span> <span class="chapter-title-text">Implementing a GPT model from scratch to generate text</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Coding a GPT-like large language model (LLM) that can be trained to generate human-like text</li> 
    <li class="readable-text" id="p3">Normalizing layer activations to stabilize neural network training</li> 
    <li class="readable-text" id="p4">Adding shortcut connections in deep neural networks </li> 
    <li class="readable-text" id="p5">Implementing transformer blocks to create GPT models of various sizes</li> 
    <li class="readable-text" id="p6">Computing the number of parameters and storage requirements of GPT models</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>You‚Äôve already learned and coded the <em>multi-head attention</em> mechanism, one of the core components of LLMs. Now, we will code the other building blocks of an LLM and assemble them into a GPT-like model that we will train in the next chapter to generate human-like text.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p8">  
   <img alt="figure" src="../Images/4-1.png" width="1012" height="444"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.1</span> The three main stages of coding an LLM. This chapter focuses on step 3 of stage 1: implementing the LLM architecture.</h5>
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>The LLM architecture referenced in figure 4.1, consists of several building blocks. We will begin with a top-down view of the model architecture before covering the individual components in more detail.</p> 
  </div> 
  <div class="readable-text" id="p10"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.1</span> Coding an LLM architecture</h2> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>LLMs, such as GPT (which stands for <em>generative pretrained transformer</em>), are large deep neural network architectures designed to generate new text one word (or token) at a time. However, despite their size, the model architecture is less complicated than you might think, since many of its components are repeated, as we will see later. Figure 4.2 provides a top-down view of a GPT-like LLM, with its main components highlighted.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p12">  
   <img alt="figure" src="../Images/4-2.png" width="1100" height="848"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.2</span> A GPT model. In addition to the embedding layers, it consists of one or more transformer blocks containing the masked multi-head attention module we previously implemented.</h5>
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>We have already covered several aspects of the LLM architecture, such as input tokenization and embedding and the masked multi-head attention module. Now, we will implement the core structure of the GPT model, including its <em>transformer blocks</em>, which we will later train to generate human-like text.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>Previously, we used smaller embedding dimensions for simplicity, ensuring that the concepts and examples could comfortably fit on a single page. Now, we are scaling up to the size of a small GPT-2 model, specifically the smallest version with 124 million parameters, as described in ‚ÄúLanguage Models Are Unsupervised Multitask Learners,‚Äù by Radford et al. (<a href="https://mng.bz/yoBq">https://mng.bz/yoBq</a>). Note that while the original report mentions 117 million parameters, this was later corrected. In chapter 6, we will focus on loading pretrained weights into our implementation and adapting it for larger GPT-2 models with 345, 762, and 1,542 million parameters. </p> 
  </div> 
  <div class="readable-text intended-text" id="p15"> 
   <p>In the context of deep learning and LLMs like GPT, the term ‚Äúparameters‚Äù refers to the trainable weights of the model. These weights are essentially the internal variables of the model that are adjusted and optimized during the training process to minimize a specific loss function. This optimization allows the model to learn from the training data.</p> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>For example, in a neural network layer that is represented by a 2,048 √ó 2,048‚Äìdimensional matrix (or tensor) of weights, each element of this matrix is a parameter. Since there are 2,048 rows and 2,048 columns, the total number of parameters in this layer is 2,048 multiplied by 2,048, which equals 4,194,304 parameters.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p17"> 
    <h5 class=" callout-container-h5 readable-text-h5">GPT-2 vs. GPT-3 </h5> 
   </div> 
   <div class="readable-text" id="p18"> 
    <p>Note that we are focusing on GPT-2 because OpenAI has made the weights of the pretrained model publicly available, which we will load into our implementation in chapter 6. GPT-3 is fundamentally the same in terms of model architecture, except that it is scaled up from 1.5 billion parameters in GPT-2 to 175 billion parameters in GPT-3, and it is trained on more data. As of this writing, the weights for GPT-3 are not publicly available. GPT-2 is also a better choice for learning how to implement LLMs, as it can be run on a single laptop computer, whereas GPT-3 requires a GPU cluster for training and inference. According to Lambda Labs (<a href="https://lambdalabs.com/">https://lambdalabs.com/</a>), it would take 355 years to train GPT-3 on a single V100 datacenter GPU and 665 years on a consumer RTX 8000 GPU.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p19"> 
   <p>We specify the configuration of the small GPT-2 model via the following Python dictionary, which we will use in the code examples later:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p20"> 
   <div class="code-area-container"> 
    <pre class="code-area">GPT_CONFIG_124M = {
    "vocab_size": 50257,     # Vocabulary size
    "context_length": 1024,  # Context length
    "emb_dim": 768,          # Embedding dimension
    "n_heads": 12,           # Number of attention heads
    "n_layers": 12,          # Number of layers
    "drop_rate": 0.1,        # Dropout rate
    "qkv_bias": False        # Query-Key-Value bias
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>In the <code>GPT_CONFIG_124M</code> dictionary, we use concise variable names for clarity and to prevent long lines of code:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p22"> <code>vocab_size</code> refers to a vocabulary of 50,257 words, as used by the BPE tokenizer (see chapter 2). </li> 
   <li class="readable-text" id="p23"> <code>context_length</code> denotes the maximum number of input tokens the model can handle via the positional embeddings (see chapter 2). </li> 
   <li class="readable-text" id="p24"> <code>emb_dim</code> represents the embedding size, transforming each token into a 768-dimensional vector. </li> 
   <li class="readable-text" id="p25"> <code>n_heads</code> indicates the count of attention heads in the multi-head attention mechanism (see chapter 3). </li> 
   <li class="readable-text" id="p26"> <code>n_layers</code> specifies the number of transformer blocks in the model, which we will cover in the upcoming discussion. </li> 
   <li class="readable-text" id="p27"> <code>drop_rate</code> indicates the intensity of the dropout mechanism (0.1 implies a 10% random drop out of hidden units) to prevent overfitting (see chapter 3). </li> 
   <li class="readable-text" id="p28"> <code>qkv_bias</code> determines whether to include a bias vector in the <code>Linear</code> layers of the multi-head attention for query, key, and value computations. We will initially disable this, following the norms of modern LLMs, but we will revisit it in chapter 6 when we load pretrained GPT-2 weights from OpenAI into our model (see chapter 6). </li> 
  </ul> 
  <div class="readable-text" id="p29"> 
   <p>Using this configuration, we will implement a GPT placeholder architecture (<code>DummyGPTModel</code>), as shown in figure 4.3. This will provide us with a big-picture view of how everything fits together and what other components we need to code to assemble the full GPT model architecture.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p30">  
   <img alt="figure" src="../Images/4-3.png" width="804" height="381"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.3</span> The order in which we code the GPT architecture. We start with the GPT backbone, a placeholder architecture, before getting to the individual core pieces and eventually assembling them in a transformer block for the final GPT architecture.</h5>
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>The numbered boxes in figure 4.3 illustrate the order in which we tackle the individual concepts required to code the final GPT architecture. We will start with step 1, a placeholder GPT backbone we will call <code>DummyGPTModel</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p32"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 4.1</span> A placeholder GPT model architecture class</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch
import torch.nn as nn

class DummyGPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])
        self.trf_blocks = nn.Sequential(              <span class="aframe-location"/> #1
            *[DummyTransformerBlock(cfg)               #1
              for _ in range(cfg["n_layers"])]         #1
        )                                              #1
        self.final_norm = DummyLayerNorm(cfg["emb_dim"])    <span class="aframe-location"/> #2
        self.out_head = nn.Linear(
            cfg["emb_dim"], cfg["vocab_size"], bias=False
        )

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(
            torch.arange(seq_len, device=in_idx.device)
        )
        x = tok_embeds + pos_embeds
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits

class DummyTransformerBlock(nn.Module):   <span class="aframe-location"/> #3
    def __init__(self, cfg):
        super().__init__()

    def forward(self, x):    <span class="aframe-location"/> #4
        return x

class DummyLayerNorm(nn.Module):          <span class="aframe-location"/> #5
    def __init__(self, normalized_shape, eps=1e-5):   <span class="aframe-location"/> #6
        super().__init__()

    def forward(self, x):
        return x</pre> 
    <div class="code-annotations-overlay-container">
     #1 Uses a placeholder for TransformerBlock
     <br/>#2 Uses a placeholder for LayerNorm
     <br/>#3 A simple placeholder class that will be replaced by a real TransformerBlock later
     <br/>#4 This block does nothing and just returns its input.
     <br/>#5 A simple placeholder class that will be replaced by a real LayerNorm later
     <br/>#6 The parameters here are just to mimic the LayerNorm interface.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>The <code>DummyGPTModel</code> class in this code defines a simplified version of a GPT-like model using PyTorch‚Äôs neural network module (<code>nn.Module</code>). The model architecture in the <code>DummyGPTModel</code> class consists of token and positional embeddings, dropout, a series of transformer blocks (<code>DummyTransformerBlock</code>), a final layer normalization (<code>DummyLayerNorm</code>), and a linear output layer (<code>out_head</code>). The configuration is passed in via a Python dictionary, for instance, the <code>GPT_CONFIG_124M</code> dictionary we created earlier.</p> 
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>The <code>forward</code> method describes the data flow through the model: it computes token and positional embeddings for the input indices, applies dropout, processes the data through the transformer blocks, applies normalization, and finally produces logits with the linear output layer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p35"> 
   <p>The code in listing 4.1 is already functional. However, for now, note that we use placeholders (<code>DummyLayerNorm</code> and <code>DummyTransformerBlock</code>) for the transformer block and layer normalization, which we will develop later.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>Next, we will prepare the input data and initialize a new GPT model to illustrate its usage. Building on our coding of the tokenizer (see chapter 2), let‚Äôs now consider a high-level overview of how data flows in and out of a GPT model, as shown in figure 4.4.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p37">  
   <img alt="figure" src="../Images/4-4.png" width="1007" height="866"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.4</span> A big-picture overview showing how the input data is tokenized, embedded, and fed to the GPT model. Note that in our <code>DummyGPTClass</code> coded earlier, the token embedding is handled inside the GPT model. In LLMs, the embedded input token dimension typically matches the output dimension. The output embeddings here represent the context vectors (see chapter 3).</h5>
  </div> 
  <div class="readable-text intended-text" id="p38"> 
   <p>To implement these steps, we tokenize a batch consisting of two text inputs for the GPT model using the tiktoken tokenizer from chapter 2:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p39"> 
   <div class="code-area-container"> 
    <pre class="code-area">import tiktoken

tokenizer = tiktoken.get_encoding("gpt2")
batch = []
txt1 = "Every effort moves you"
txt2 = "Every day holds a"

batch.append(torch.tensor(tokenizer.encode(txt1)))
batch.append(torch.tensor(tokenizer.encode(txt2)))
batch = torch.stack(batch, dim=0)
print(batch)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>The resulting token IDs for the two texts are as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p41"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[6109,  3626,  6100,   345],   <span class="aframe-location"/> #1
        [6109,  1110,  6622,   257]])</pre> 
    <div class="code-annotations-overlay-container">
     #1 The first row corresponds to the first text, and the second row corresponds to the second text.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>Next, we initialize a new 124-million-parameter <code>DummyGPTModel</code> instance and feed it the tokenized <code>batch</code>: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p43"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
model = DummyGPTModel(GPT_CONFIG_124M)
logits = model(batch)
print("Output shape:", logits.shape)
print(logits)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>The model outputs, which are commonly referred to as logits, are as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p45"> 
   <div class="code-area-container"> 
    <pre class="code-area">Output shape: torch.Size([2, 4, 50257])
tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],
         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],
         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],
         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],

        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],
         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],
         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],
         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],
       grad_fn=&lt;UnsafeViewBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>The output tensor has two rows corresponding to the two text samples. Each text sample consists of four tokens; each token is a 50,257-dimensional vector, which matches the size of the tokenizer‚Äôs vocabulary.</p> 
  </div> 
  <div class="readable-text intended-text" id="p47"> 
   <p>The embedding has 50,257 dimensions because each of these dimensions refers to a unique token in the vocabulary. When we implement the postprocessing code, we will convert these 50,257-dimensional vectors back into token IDs, which we can then decode into words.</p> 
  </div> 
  <div class="readable-text intended-text" id="p48"> 
   <p>Now that we have taken a top-down look at the GPT architecture and its inputs and outputs, we will code the individual placeholders, starting with the real layer normalization class that will replace the <code>DummyLayerNorm</code> in the previous code.</p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.2</span> Normalizing activations with layer normalization</h2> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Training deep neural networks with many layers can sometimes prove challenging due to problems like vanishing or exploding gradients. These problems lead to unstable training dynamics and make it difficult for the network to effectively adjust its weights, which means the learning process struggles to find a set of parameters (weights) for the neural network that minimizes the loss function. In other words, the network has difficulty learning the underlying patterns in the data to a degree that would allow it to make accurate predictions or decisions. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p51"> 
   <p><span class="print-book-callout-head">Note</span> ‚ÄÉIf you are new to neural network training and the concepts of gradients, a brief introduction to these concepts can be found in section A.4 in appendix A. However, a deep mathematical understanding of gradients is not required to follow the contents of this book.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>Let‚Äôs now implement <em>layer normalization</em> to improve the stability and efficiency of neural network training. The main idea behind layer normalization is to adjust the activations (outputs) of a neural network layer to have a mean of 0 and a variance of 1, also known as unit variance. This adjustment speeds up the convergence to effective weights and ensures consistent, reliable training. In GPT-2 and modern transformer architectures, layer normalization is typically applied before and after the multi-head attention module, and, as we have seen with the <code>DummyLayerNorm</code> placeholder, before the final output layer. Figure 4.5 provides a visual overview of how layer normalization functions.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p53">  
   <img alt="figure" src="../Images/4-5.png" width="767" height="414"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.5</span> An illustration of layer normalization where the six outputs of the layer, also called activations, are normalized such that they have a 0 mean and a variance of 1.</h5>
  </div> 
  <div class="readable-text" id="p54"> 
   <p>We can recreate the example shown in figure 4.5 via the following code, where we implement a neural network layer with five inputs and six outputs that we apply to two input examples:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p55"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
batch_example = torch.randn(2, 5)    <span class="aframe-location"/> #1
layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())
out = layer(batch_example)
print(out)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates two training examples with five dimensions (features) each
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>This prints the following tensor, where the first row lists the layer outputs for the first input and the second row lists the layer outputs for the second row:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p57"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],
        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],
       grad_fn=&lt;ReluBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>The neural network layer we have coded consists of a <code>Linear</code> layer followed by a nonlinear activation function, <code>ReLU</code> (short for rectified linear unit), which is a standard activation function in neural networks. If you are unfamiliar with <code>ReLU</code>, it simply thresholds negative inputs to 0, ensuring that a layer outputs only positive values, which explains why the resulting layer output does not contain any negative values. Later, we will use another, more sophisticated activation function in GPT.</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>Before we apply layer normalization to these outputs, let‚Äôs examine the mean and variance:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p60"> 
   <div class="code-area-container"> 
    <pre class="code-area">mean = out.mean(dim=-1, keepdim=True)
var = out.var(dim=-1, keepdim=True)
print("Mean:\n", mean)
print("Variance:\n", var)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p62"> 
   <div class="code-area-container"> 
    <pre class="code-area">Mean:
  tensor([[0.1324],
          [0.2170]], grad_fn=&lt;MeanBackward1&gt;)
Variance:
  tensor([[0.0231],
          [0.0398]], grad_fn=&lt;VarBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>The first row in the mean tensor here contains the mean value for the first input row, and the second output row contains the mean for the second input row. </p> 
  </div> 
  <div class="readable-text intended-text" id="p64"> 
   <p>Using <code>keepdim=True</code> in operations like mean or variance calculation ensures that the output tensor retains the same number of dimensions as the input tensor, even though the operation reduces the tensor along the dimension specified via <code>dim</code>. For instance, without <code>keepdim=True</code>, the returned mean tensor would be a two-dimensional vector <code>[0.1324,</code> <code>0.2170]</code> instead of a 2 √ó 1‚Äìdimensional matrix <code>[[0.1324],</code> <code>[0.2170]]</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p65"> 
   <p>The <code>dim</code> parameter specifies the dimension along which the calculation of the statistic (here, mean or variance) should be performed in a tensor. As figure 4.6 <span class="aframe-location"/>explains, for a two-dimensional tensor (like a matrix), using <code>dim=-1</code> for operations such as mean or variance calculation is the same as using <code>dim=1</code>. This is because <code>-1</code> refers to the tensor‚Äôs last dimension, which corresponds to the columns in a two-dimensional tensor. Later, when adding layer normalization to the GPT model, which produces three-dimensional tensors with the shape <code>[batch_size,</code> <code>num_tokens,</code> <code>embedding_size]</code>, we can still use <code>dim=-1</code> for normalization across the last dimension, avoiding a change from <code>dim=1</code> to <code>dim=2</code>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p66">  
   <img alt="figure" src="../Images/4-6.png" width="675" height="374"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.6</span> An illustration of the dim parameter when calculating the mean of a tensor. For instance, if we have a two-dimensional tensor (matrix) with dimensions <code>[rows,</code> <code>columns]</code>, using <code>dim=0</code> will perform the operation across rows (vertically, as shown at the bottom), resulting in an output that aggregates the data for each column. Using <code>dim=1</code> or <code>dim=-1</code> will perform the operation across columns (horizontally, as shown at the top), resulting in an output aggregating the data for each row. </h5>
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>Next, let‚Äôs apply layer normalization to the layer outputs we obtained earlier. The operation consists of subtracting the mean and dividing by the square root of the variance (also known as the standard deviation):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p68"> 
   <div class="code-area-container"> 
    <pre class="code-area">out_norm = (out - mean) / torch.sqrt(var)
mean = out_norm.mean(dim=-1, keepdim=True)
var = out_norm.var(dim=-1, keepdim=True)
print("Normalized layer outputs:\n", out_norm)
print("Mean:\n", mean)
print("Variance:\n", var)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>As we can see based on the results, the normalized layer outputs, which now also contain negative values, have 0 mean and a variance of 1:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p70"> 
   <div class="code-area-container"> 
    <pre class="code-area">Normalized layer outputs:
 tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],
        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],
       grad_fn=&lt;DivBackward0&gt;)
Mean:
 tensor([[-5.9605e-08],
        [1.9868e-08]], grad_fn=&lt;MeanBackward1&gt;)
Variance:
 tensor([[1.],
        [1.]], grad_fn=&lt;VarBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Note that the value ‚Äì5.9605e-08 in the output tensor is the scientific notation for ‚Äì5.9605 √ó 10<sup>-8</sup>, which is ‚Äì0.000000059605 in decimal form. This value is very close to 0, but it is not exactly 0 due to small numerical errors that can accumulate because of the finite precision with which computers represent numbers. </p> 
  </div> 
  <div class="readable-text intended-text" id="p72"> 
   <p>To improve readability, we can also turn off the scientific notation when printing tensor values by setting <code>sci_mode</code> to <code>False</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p73"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.set_printoptions(sci_mode=False)
print("Mean:\n", mean)
print("Variance:\n", var)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p75"> 
   <div class="code-area-container"> 
    <pre class="code-area">Mean:
 tensor([[    0.0000],
        [    0.0000]], grad_fn=&lt;MeanBackward1&gt;)
Variance:
 tensor([[1.],
        [1.]], grad_fn=&lt;VarBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>So far, we have coded and applied layer normalization in a step-by-step process. Let‚Äôs now encapsulate this process in a PyTorch module that we can use in the GPT model later.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p77"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 4.2</span> A layer normalization class</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>This specific implementation of layer normalization operates on the last dimension of the input tensor x, which represents the embedding dimension (<code>emb_dim</code>). The variable <code>eps</code> is a small constant (epsilon) added to the variance to prevent division by zero during normalization. The <code>scale</code> and <code>shift</code> are two trainable parameters (of the same dimension as the input) that the LLM automatically adjusts during training if it is determined that doing so would improve the model‚Äôs performance on its training task. This allows the model to learn appropriate scaling and shifting that best suit the data it is processing.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p79"> 
    <h5 class=" callout-container-h5 readable-text-h5">Biased variance </h5> 
   </div> 
   <div class="readable-text" id="p80"> 
    <p>In our variance calculation method, we use an implementation detail by setting <code>unbiased=False</code>. For those curious about what this means, in the variance calculation, we divide by the number of inputs <em>n</em> in the variance formula. This approach does not apply Bessel‚Äôs correction, which typically uses <em>n </em>‚Äì<em> 1</em> instead of <em>n</em> in the denominator to adjust for bias in sample variance estimation. This decision results in a so-called biased estimate of the variance. For LLMs, where the embedding dimension <em>n</em> is significantly large, the difference between using <em>n</em> and <em>n </em>‚Äì <em>1</em> is practically negligible. I chose this approach to ensure compatibility with the GPT-2 model‚Äôs normalization layers and because it reflects TensorFlow‚Äôs default behavior, which was used to implement the original GPT-2 model. Using a similar setting ensures our method is compatible with the pretrained weights we will load in chapter 6. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>Let‚Äôs now try the <code>LayerNorm</code> module in practice and apply it to the batch input:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p82"> 
   <div class="code-area-container"> 
    <pre class="code-area">ln = LayerNorm(emb_dim=5)
out_ln = ln(batch_example)
mean = out_ln.mean(dim=-1, keepdim=True)
var = out_ln.var(dim=-1, unbiased=False, keepdim=True)
print("Mean:\n", mean)
print("Variance:\n", var)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>The results show that the layer normalization code works as expected and normalizes the values of each of the two inputs such that they have a mean of 0 and a variance of 1:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p84"> 
   <div class="code-area-container"> 
    <pre class="code-area">Mean:
 tensor([[    -0.0000],
        [     0.0000]], grad_fn=&lt;MeanBackward1&gt;)
Variance:
 tensor([[1.0000],
        [1.0000]], grad_fn=&lt;VarBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>We have now covered two of the building blocks we will need to implement the GPT architecture, as shown in figure 4.7. Next, we will look at the GELU activation function, which is one of the activation functions used in LLMs, instead of the traditional ReLU function we used previously.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p86">  
   <img alt="figure" src="../Images/4-7.png" width="762" height="392"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.7</span> The building blocks necessary to build the GPT architecture. So far, we have completed the GPT backbone and layer normalization. Next, we will focus on GELU activation and the feed forward network.</h5>
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p87"> 
    <h5 class=" callout-container-h5 readable-text-h5">Layer normalization vs. batch normalization</h5> 
   </div> 
   <div class="readable-text" id="p88"> 
    <p>If you are familiar with batch normalization, a common and traditional normalization method for neural networks, you may wonder how it compares to layer normalization. Unlike batch normalization, which normalizes across the batch dimension, layer normalization normalizes across the feature dimension. LLMs often require significant computational resources, and the available hardware or the specific use case can dictate the batch size during training or inference. Since layer normalization normalizes each input independently of the batch size, it offers more flexibility and stability in these scenarios. This is particularly beneficial for distributed training or when deploying models in environments where resources are constrained.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p90"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.3</span> Implementing a feed forward network with GELU activations</h2> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>Next, we will implement a small neural network submodule used as part of the transformer block in LLMs. We begin by implementing the <em>GELU</em> activation function, which plays a crucial role in this neural network submodule. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p92"> 
   <p><span class="print-book-callout-head">Note</span> For additional information on implementing neural networks in PyTorch, see section A.5 in appendix A.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>Historically, the ReLU activation function has been commonly used in deep learning due to its simplicity and effectiveness across various neural network architectures. However, in LLMs, several other activation functions are employed beyond the traditional ReLU. Two notable examples are GELU (<em>Gaussian error linear unit</em>) and SwiGLU (<em>Swish-gated linear unit</em>).</p> 
  </div> 
  <div class="readable-text intended-text" id="p94"> 
   <p>GELU and SwiGLU are more complex and smooth activation functions incorporating Gaussian and sigmoid-gated linear units, respectively. They offer improved performance for deep learning models, unlike the simpler ReLU.</p> 
  </div> 
  <div class="readable-text intended-text" id="p95"> 
   <p>The GELU activation function can be implemented in several ways; the exact version is defined as GELU(x) = x‚ãÖ<span class="regular-symbol">ùõ∑</span>(x), where <span class="regular-symbol">ùõ∑</span>(x) is the cumulative distribution function of the standard Gaussian distribution. In practice, however, it‚Äôs common to implement a computationally cheaper approximation (the original GPT-2 model was also trained with this approximation, which was found via curve fitting):<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p96"> 
   <img alt="figure" src="../Images/Equation-eqs-4.png" width="692" height="74"/> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>In code, we can implement this function as a PyTorch module.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p98"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 4.3</span> An implementation of the GELU activation function</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) * 
            (x + 0.044715 * torch.pow(x, 3))
        ))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>Next, to get an idea of what this GELU function looks like and how it compares to the ReLU function, let‚Äôs plot these functions side by side:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p100"> 
   <div class="code-area-container"> 
    <pre class="code-area">import matplotlib.pyplot as plt
gelu, relu = GELU(), nn.ReLU()

x = torch.linspace(-3, 3, 100)    <span class="aframe-location"/> #1
y_gelu, y_relu = gelu(x), relu(x)
plt.figure(figsize=(8, 3))
for i, (y, label) in enumerate(zip([y_gelu, y_relu], ["GELU", "ReLU"]), 1):
    plt.subplot(1, 2, i)
    plt.plot(x, y)
    plt.title(f"{label} activation function")
    plt.xlabel("x")
    plt.ylabel(f"{label}(x)")
    plt.grid(True)
plt.tight_layout()
plt.show()</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates 100 sample data points in the range ‚Äì3 to 3
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>As we can see in the resulting plot in figure 4.8, ReLU (right) is a piecewise linear function that outputs the input directly if it is positive; otherwise, it outputs zero. GELU (left) is a smooth, nonlinear function that approximates ReLU but with a non-zero gradient for almost all negative values (except at approximately <em>x</em> = ‚Äì0.75).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p102">  
   <img alt="figure" src="../Images/4-8.png" width="1100" height="388"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.8</span> The output of the GELU and ReLU plots using matplotlib. The x-axis shows the function inputs and the y-axis shows the function outputs.</h5>
  </div> 
  <div class="readable-text" id="p103"> 
   <p>The smoothness of GELU can lead to better optimization properties during training, as it allows for more nuanced adjustments to the model‚Äôs parameters. In contrast, ReLU has a sharp corner at zero (figure 4.18, right), which can sometimes make optimization harder, especially in networks that are very deep or have complex architectures. Moreover, unlike ReLU, which outputs zero for any negative input, GELU allows for a small, non-zero output for negative values. This characteristic means that during the training process, neurons that receive negative input can still contribute to the learning process, albeit to a lesser extent than positive inputs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p104"> 
   <p>Next, let‚Äôs use the GELU function to implement the small neural network module, <code>FeedForward</code>, that we will be using in the LLM‚Äôs transformer block later.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p105"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 4.4</span> A feed forward neural network module</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>As we can see, the <code>FeedForward</code> module is a small neural network consisting of two <code>Linear</code> layers and a <code>GELU</code> activation function. In the 124-million-parameter GPT model, it receives the input batches with tokens that have an embedding size of 768 each via the <code>GPT_CONFIG_124M</code> dictionary where <code>GPT_CONFIG_</code> <code>124M["emb_dim"]</code> <code>=</code> <code>768</code>.  Figure 4.9 shows how the embedding size is manipulated inside this small feed forward neural network when we pass it some inputs.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p107">  
   <img alt="figure" src="../Images/4-9.png" width="632" height="464"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.9</span> An overview of the connections between the layers of the feed forward neural network. This neural network can accommodate variable batch sizes and numbers of tokens in the input. However, the embedding size for each token is determined and fixed when initializing the weights.</h5>
  </div> 
  <div class="readable-text" id="p108"> 
   <p>Following the example in figure 4.9, let‚Äôs initialize a new <code>FeedForward</code> module with a token embedding size of 768 and feed it a batch input with two samples and three tokens each:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p109"> 
   <div class="code-area-container"> 
    <pre class="code-area">ffn = FeedForward(GPT_CONFIG_124M)
x = torch.rand(2, 3, 768)         <span class="aframe-location"/> #1
out = ffn(x)
print(out.shape)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates sample input with batch dimension 2
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>As we can see, the shape of the output tensor is the same as that of the input tensor:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p111"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.Size([2, 3, 768])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>The <code>FeedForward</code> module plays a crucial role in enhancing the model‚Äôs ability to learn from and generalize the data. Although the input and output dimensions of this module are the same, it internally expands the embedding dimension into a higher-dimensional space through the first linear layer, as illustrated in figure 4.10. This expansion is followed by a nonlinear GELU activation and then a contraction back to the original dimension with the second linear transformation. Such a design allows for the exploration of a richer representation space.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p113">  
   <img alt="figure" src="../Images/4-10.png" width="829" height="429"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.10</span> An illustration of the expansion and contraction of the layer outputs in the feed forward neural network. First, the inputs expand by a factor of 4 from 768 to 3,072 values. Then, the second layer compresses the 3,072 values back into a 768-dimensional representation.</h5>
  </div> 
  <div class="readable-text" id="p114"> 
   <p>Moreover, the uniformity in input and output dimensions simplifies the architecture by enabling the stacking of multiple layers, as we will do later, without the need to adjust dimensions between them, thus making the model more scalable.</p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>As figure 4.11 shows, we have now implemented most of the LLM‚Äôs building blocks. Next, we will go over the concept of shortcut connections that we insert between different layers of a neural network, which are important for improving the training performance in deep neural network architectures.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p116">  
   <img alt="figure" src="../Images/4-11.png" width="832" height="371"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.11</span> The building blocks necessary to build the GPT architecture. The black checkmarks indicating those we have already covered.</h5>
  </div> 
  <div class="readable-text" id="p117"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.4</span> Adding shortcut connections</h2> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>Let‚Äôs discuss the concept behind <em>shortcut connections</em>, also known as skip or residual connections. Originally, shortcut connections were proposed for deep networks in computer vision (specifically, in residual networks) to mitigate the challenge of vanishing gradients. The vanishing gradient problem refers to the issue where gradients (which guide weight updates during training) become progressively smaller as they propagate backward through the layers, making it difficult to effectively train earlier layers.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p119">  
   <img alt="figure" src="../Images/4-12.png" width="1009" height="983"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.12</span> A comparison between a deep neural network consisting of five layers without (left) and with shortcut connections (right). Shortcut connections involve adding the inputs of a layer to its outputs, effectively creating an alternate path that bypasses certain layers. The gradients denote the mean absolute gradient at each layer, which we compute in listing 4.5.</h5>
  </div> 
  <div class="readable-text intended-text" id="p120"> 
   <p>Figure 4.12 shows that a shortcut connection creates an alternative, shorter path for the gradient to flow through the network by skipping one or more layers, which is achieved by adding the output of one layer to the output of a later layer. This is why these connections are also known as skip connections. They play a crucial role in preserving the flow of gradients during the backward pass in training. </p> 
  </div> 
  <div class="readable-text intended-text" id="p121"> 
   <p>In the following list, we implement the neural network in figure 4.12 to see how we can add shortcut connections in the <code>forward</code> method.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p122"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 4.5</span> A neural network to illustrate shortcut connections</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class ExampleDeepNeuralNetwork(nn.Module):
    def __init__(self, layer_sizes, use_shortcut):
        super().__init__()
        self.use_shortcut = use_shortcut
        self.layers = nn.ModuleList([      <span class="aframe-location"/> #1
            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), 
                          GELU()),
            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), 
                          GELU()),
            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), 
                          GELU()),
            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), 
                          GELU()),
            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), 
                          GELU())
        ])

    def forward(self, x):
        for layer in self.layers:
            layer_output = layer(x)        <span class="aframe-location"/> #2
            if self.use_shortcut and x.shape == layer_output.shape:   <span class="aframe-location"/> #3
                x = x + layer_output
            else:
                x = layer_output
        return x</pre> 
    <div class="code-annotations-overlay-container">
     #1 Implements five layers
     <br/>#2 Compute the output of the current layer
     <br/>#3 Check if shortcut can be applied
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>The code implements a deep neural network with five layers, each consisting of a <code>Linear</code> layer and a <code>GELU</code> activation function. In the forward pass, we iteratively pass the input through the layers and optionally add the shortcut connections if the <code>self.use_ shortcut</code> attribute is set to <code>True</code>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p124"> 
   <p>Let‚Äôs use this code to initialize a neural network without shortcut connections. Each layer will be initialized such that it accepts an example with three input values and returns three output values. The last layer returns a single output value:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p125"> 
   <div class="code-area-container"> 
    <pre class="code-area">layer_sizes = [3, 3, 3, 3, 3, 1]  
sample_input = torch.tensor([[1., 0., -1.]])
torch.manual_seed(123)                           <span class="aframe-location"/> #1
model_without_shortcut = ExampleDeepNeuralNetwork(
    layer_sizes, use_shortcut=False
)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Specifies random seed for the initial weights for reproducibility
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>Next, we implement a function that computes the gradients in the model‚Äôs backward pass:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p127"> 
   <div class="code-area-container"> 
    <pre class="code-area">def print_gradients(model, x):
    output = model(x)            <span class="aframe-location"/> #1
    target = torch.tensor([[0.]])

    loss = nn.MSELoss()
    loss = loss(output, target)   <span class="aframe-location"/> #2

    loss.backward()         <span class="aframe-location"/> #3

    for name, param in model.named_parameters():
        if 'weight' in name:
            print(f"{name} has gradient mean of {param.grad.abs().mean().item()}")</pre> 
    <div class="code-annotations-overlay-container">
     #1 Forward pass
     <br/>#2 Calculates loss based on how close the target and output are
     <br/>#3 Backward pass to calculate the gradients
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>This code specifies a loss function that computes how close the model output and a user-specified target (here, for simplicity, the value 0) are. Then, when calling <code>loss.backward()</code>, PyTorch computes the loss gradient for each layer in the model. We can iterate through the weight parameters via <code>model.named_parameters()</code>. Suppose we have a 3 √ó 3 weight parameter matrix for a given layer. In that case, this layer will have 3 √ó 3 gradient values, and we print the mean absolute gradient of these 3 √ó 3 gradient values to obtain a single gradient value per layer to compare the gradients between layers more easily.</p> 
  </div> 
  <div class="readable-text intended-text" id="p129"> 
   <p>In short, the <code>.backward()</code> method is a convenient method in PyTorch that computes loss gradients, which are required during model training, without implementing the math for the gradient calculation ourselves, thereby making working with deep neural networks much more accessible. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p130"> 
   <p><span class="print-book-callout-head">Note</span> ‚ÄÉIf you are unfamiliar with the concept of gradients and neural network training, I recommend reading sections A.4 and A.7 in appendix A.</p> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>Let‚Äôs now use the <code>print_gradients</code> function and apply it to the model without skip connections:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p132"> 
   <div class="code-area-container"> 
    <pre class="code-area">print_gradients(model_without_shortcut, sample_input)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p133"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p134"> 
   <div class="code-area-container"> 
    <pre class="code-area">layers.0.0.weight has gradient mean of 0.00020173587836325169
layers.1.0.weight has gradient mean of 0.0001201116101583466
layers.2.0.weight has gradient mean of 0.0007152041653171182
layers.3.0.weight has gradient mean of 0.001398873864673078
layers.4.0.weight has gradient mean of 0.005049646366387606</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>The output of the <code>print_gradients</code> function shows, the gradients become smaller as we progress from the last layer (<code>layers.4</code>) to the first layer (<code>layers.0</code>), which is a phenomenon called the <em>vanishing gradient problem</em>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p136"> 
   <p>Let‚Äôs now instantiate a model with skip connections and see how it compares:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p137"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
model_with_shortcut = ExampleDeepNeuralNetwork(
    layer_sizes, use_shortcut=True
)
print_gradients(model_with_shortcut, sample_input)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p138"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p139"> 
   <div class="code-area-container"> 
    <pre class="code-area">layers.0.0.weight has gradient mean of 0.22169792652130127
layers.1.0.weight has gradient mean of 0.20694105327129364
layers.2.0.weight has gradient mean of 0.32896995544433594
layers.3.0.weight has gradient mean of 0.2665732502937317
layers.4.0.weight has gradient mean of 1.3258541822433472</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p140"> 
   <p>The last layer <code>(layers.4</code>) still has a larger gradient than the other layers. However, the gradient value stabilizes as we progress toward the first layer (<code>layers.0</code>) and doesn‚Äôt shrink to a vanishingly small value. </p> 
  </div> 
  <div class="readable-text intended-text" id="p141"> 
   <p>In conclusion, shortcut connections are important for overcoming the limitations posed by the vanishing gradient problem in deep neural networks. Shortcut connections are a core building block of very large models such as LLMs, and they will help facilitate more effective training by ensuring consistent gradient flow across layers when we train the GPT model in the next chapter. </p> 
  </div> 
  <div class="readable-text intended-text" id="p142"> 
   <p>Next, we‚Äôll connect all of the previously covered concepts (layer normalization, GELU activations, feed forward module, and shortcut connections) in a transformer block, which is the final building block we need to code the GPT architecture.</p> 
  </div> 
  <div class="readable-text" id="p143"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.5</span> Connecting attention and linear layers in a transformer block</h2> 
  </div> 
  <div class="readable-text" id="p144"> 
   <p>Now, let‚Äôs implement the <em>transformer block</em>, a fundamental building block of GPT and other LLM architectures. This block, which is repeated a dozen times in the 124-million-parameter GPT-2 architecture, combines several concepts we have previously covered: multi-head attention, layer normalization, dropout, feed forward layers, and GELU activations. Later, we will connect this transformer block to the remaining parts of the GPT architecture.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p145">  
   <img alt="figure" src="../Images/4-13.png" width="927" height="1029"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.13</span> An illustration of a transformer block. Input tokens have been embedded into 768-dimensional vectors. Each row corresponds to one token‚Äôs vector representation. The outputs of the transformer block are vectors of the same dimension as the input, which can then be fed into subsequent layers in an LLM.</h5>
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>Figure 4.13 shows a transformer block that combines several components, including the masked multi-head attention module (see chapter 3) and the <code>FeedForward</code> module we previously implemented (see section 4.3). When a transformer block processes an input sequence, each element in the sequence (for example, a word or subword token) is represented by a fixed-size vector (in this case, 768 dimensions). The operations within the transformer block, including multi-head attention and feed forward layers, are designed to transform these vectors in a way that preserves their dimensionality.</p> 
  </div> 
  <div class="readable-text intended-text" id="p147"> 
   <p>The idea is that the self-attention mechanism in the multi-head attention block identifies and analyzes relationships between elements in the input sequence. In contrast, the feed forward network modifies the data individually at each position. This combination not only enables a more nuanced understanding and processing of the input but also enhances the model‚Äôs overall capacity for handling complex data patterns.</p> 
  </div> 
  <div class="readable-text" id="p148"> 
   <p>We can create the <code>TransformerBlock</code> in code.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p149"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 4.6</span> The transformer block component of GPT</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from chapter03 import MultiHeadAttention

class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"], 
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
 #1
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)
        x = self.drop_shortcut(x)
        x = x + shortcut     <span class="aframe-location"/> #2

        shortcut = x        <span class="aframe-location"/> #3
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut     <span class="aframe-location"/> #4
        return x</pre> 
    <div class="code-annotations-overlay-container">
     #1 Shortcut connection for attention block
     <br/>#2 Add the original input back
     <br/>#3 Shortcut connection for feed forward block
     <br/>#4 Adds the original input back
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p150"> 
   <p>The given code defines a <code>TransformerBlock</code> class in PyTorch that includes a multi-head attention mechanism (<code>MultiHeadAttention</code>) and a feed forward network (<code>FeedForward</code>), both configured based on a provided configuration dictionary (<code>cfg</code>), such as <code>GPT_CONFIG_124M</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p151"> 
   <p>Layer normalization (<code>LayerNorm</code>) is applied before each of these two components, and dropout is applied after them to regularize the model and prevent overfitting. This is also known as <em>Pre-LayerNorm</em>. Older architectures, such as the original transformer model, applied layer normalization after the self-attention and feed forward networks instead, known as <em>Post-LayerNorm</em>, which often leads to worse training dynamics.</p> 
  </div> 
  <div class="readable-text intended-text" id="p152"> 
   <p>The class also implements the forward pass, where each component is followed by a shortcut connection that adds the input of the block to its output. This critical feature helps gradients flow through the network during training and improves the learning of deep models (see section 4.4).</p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>Using the <code>GPT_CONFIG_124M</code> dictionary we defined earlier, let‚Äôs instantiate a transformer block and feed it some sample data:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p154"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
x = torch.rand(2, 4, 768)                  <span class="aframe-location"/> #1
block = TransformerBlock(GPT_CONFIG_124M)
output = block(x)

print("Input shape:", x.shape)
print("Output shape:", output.shape)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates sample input of shape [batch_size, num_tokens, emb_dim]
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p155"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p156"> 
   <div class="code-area-container"> 
    <pre class="code-area">Input shape: torch.Size([2, 4, 768])
Output shape: torch.Size([2, 4, 768])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p157"> 
   <p>As we can see, the transformer block maintains the input dimensions in its output, indicating that the transformer architecture processes sequences of data without altering their shape throughout the network.</p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>The preservation of shape throughout the transformer block architecture is not incidental but a crucial aspect of its design. This design enables its effective application across a wide range of sequence-to-sequence tasks, where each output vector directly corresponds to an input vector, maintaining a one-to-one relationship. However, the output is a context vector that encapsulates information from the entire input sequence (see chapter 3). This means that while the physical dimensions of the sequence (length and feature size) remain unchanged as it passes through the transformer block, the content of each output vector is re-encoded to integrate contextual information from across the entire input sequence.</p> 
  </div> 
  <div class="readable-text intended-text" id="p159"> 
   <p>With the transformer block implemented, we now have all the building blocks needed to implement the GPT architecture. As illustrated in figure 4.14, the transformer block combines layer normalization, the feed forward network, GELU activations, and shortcut connections. As we will eventually see, this transformer block will make up the main component of the GPT architecture.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p160">  
   <img alt="figure" src="../Images/4-14.png" width="752" height="379"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.14</span> The building blocks necessary to build the GPT architecture. The black checks indicate the blocks we have completed.</h5>
  </div> 
  <div class="readable-text" id="p161"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.6</span> Coding the GPT model</h2> 
  </div> 
  <div class="readable-text" id="p162"> 
   <p>We started this chapter with a big-picture overview of a GPT architecture that we called <code>DummyGPTModel</code>. In this <code>DummyGPTModel</code> code implementation, we showed the input and outputs to the GPT model, but its building blocks remained a black box using a <code>DummyTransformerBlock</code> and <code>DummyLayerNorm</code> class as placeholders.</p> 
  </div> 
  <div class="readable-text intended-text" id="p163"> 
   <p>Let‚Äôs now replace the <code>DummyTransformerBlock</code> and <code>DummyLayerNorm</code> placeholders with the real <code>TransformerBlock</code> and <code>LayerNorm</code> classes we coded previously to assemble a fully working version of the original 124-million-parameter version of GPT-2. In chapter 5, we will pretrain a GPT-2 model, and in chapter 6, we will load in the pretrained weights from OpenAI.</p> 
  </div> 
  <div class="readable-text intended-text" id="p164"> 
   <p>Before we assemble the GPT-2 model in code, let‚Äôs look at its overall structure, as shown in figure 4.15, which includes all the concepts we have covered so far.<span class="aframe-location"/> As we can see, the transformer block is repeated many times throughout a GPT model architecture. In the case of the 124-million-parameter GPT-2 model, it‚Äôs repeated 12 times, which we specify via the <code>n_layers</code> entry in the <code>GPT_CONFIG_124M</code> dictionary. This transform block is repeated 48 times in the largest GPT-2 model with 1,542 million parameters.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p165">  
   <img alt="figure" src="../Images/4-15.png" width="1100" height="1292"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.15</span> An overview of the GPT model architecture showing the flow of data through the GPT model. Starting from the bottom, tokenized text is first converted into token embeddings, which are then augmented with positional embeddings. This combined information forms a tensor that is passed through a series of transformer blocks shown in the center (each containing multi-head attention and feed forward neural network layers with dropout and layer normalization), which are stacked on top of each other and repeated 12 times.</h5>
  </div> 
  <div class="readable-text intended-text" id="p166"> 
   <p>The output from the final transformer block then goes through a final layer normalization step before reaching the linear output layer. This layer maps the transformer‚Äôs output to a high-dimensional space (in this case, 50,257 dimensions, corresponding to the model‚Äôs vocabulary size) to predict the next token in the sequence.</p> 
  </div> 
  <div class="readable-text intended-text" id="p167"> 
   <p>Let‚Äôs now code the architecture in figure 4.15.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p168"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 4.7</span> The GPT model architecture implementation</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(
            cfg["emb_dim"], cfg["vocab_size"], bias=False
        )

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
 #1
        pos_embeds = self.pos_emb(
            torch.arange(seq_len, device=in_idx.device)
        )
        x = tok_embeds + pos_embeds
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits</pre> 
    <div class="code-annotations-overlay-container">
     #1 The device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p169"> 
   <p>Thanks to the <code>TransformerBlock</code> class, the <code>GPTModel</code> class is relatively small and compact. </p> 
  </div> 
  <div class="readable-text intended-text" id="p170"> 
   <p>The <code>__init__</code> constructor of this <code>GPTModel</code> class initializes the token and positional embedding layers using the configurations passed in via a Python dictionary, <code>cfg</code>. These embedding layers are responsible for converting input token indices into dense vectors and adding positional information (see chapter 2). </p> 
  </div> 
  <div class="readable-text intended-text" id="p171"> 
   <p>Next, the <code>__init__</code> method creates a sequential stack of <code>TransformerBlock</code> modules equal to the number of layers specified in <code>cfg</code>. Following the transformer blocks, a <code>LayerNorm</code> layer is applied, standardizing the outputs from the transformer blocks to stabilize the learning process. Finally, a linear output head without bias is defined, which projects the transformer‚Äôs output into the vocabulary space of the tokenizer to generate logits for each token in the vocabulary. </p> 
  </div> 
  <div class="readable-text intended-text" id="p172"> 
   <p>The forward method takes a batch of input token indices, computes their embeddings, applies the positional embeddings, passes the sequence through the transformer blocks, normalizes the final output, and then computes the logits, representing the next token‚Äôs unnormalized probabilities. We will convert these logits into tokens and text outputs in the next section. </p> 
  </div> 
  <div class="readable-text intended-text" id="p173"> 
   <p>Let‚Äôs now initialize the 124-million-parameter GPT model using the <code>GPT_CONFIG_ 124M</code> dictionary we pass into the <code>cfg</code> parameter and feed it with the batch text input we previously created:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p174"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)

out = model(batch)
print("Input batch:\n", batch)
print("\nOutput shape:", out.shape)
print(out)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p175"> 
   <p>This code prints the contents of the input batch followed by the output tensor:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p176"> 
   <div class="code-area-container"> 
    <pre class="code-area">Input batch:
 tensor([[6109,  3626,  6100,   345],     <span class="aframe-location"/> #1
         [6109,  1110,  6622,   257]])    <span class="aframe-location"/> #2

Output shape: torch.Size([2, 4, 50257])
tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],
         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],
         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],
         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],

        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],
         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],
         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],
         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],
       grad_fn=&lt;UnsafeViewBackward0&gt;)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Token IDs of text 1
     <br/>#2 Token IDs of text 2
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p177"> 
   <p>As we can see, the output tensor has the shape <code>[2,</code> <code>4,</code> <code>50257]</code>, since we passed in two input texts with four tokens each. The last dimension, <code>50257</code>, corresponds to the vocabulary size of the tokenizer. Later, we will see how to convert each of these 50,257-dimensional output vectors back into tokens.</p> 
  </div> 
  <div class="readable-text intended-text" id="p178"> 
   <p>Before we move on to coding the function that converts the model outputs into text, let‚Äôs spend a bit more time with the model architecture itself and analyze its size. Using the <code>numel()</code> method, short for ‚Äúnumber of elements,‚Äù we can collect the total number of parameters in the model‚Äôs parameter tensors:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p179"> 
   <div class="code-area-container"> 
    <pre class="code-area">total_params = sum(p.numel() for p in model.parameters())
print(f"Total number of parameters: {total_params:,}")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p180"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p181"> 
   <div class="code-area-container"> 
    <pre class="code-area">Total number of parameters: 163,009,536</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p182"> 
   <p>Now, a curious reader might notice a discrepancy. Earlier, we spoke of initializing a 124-million-parameter GPT model, so why is the actual number of parameters 163 million?</p> 
  </div> 
  <div class="readable-text intended-text" id="p183"> 
   <p>The reason is a concept called <em>weight tying</em>, which was used in the original GPT-2 architecture. It means that the original GPT-2 architecture reuses the weights from the token embedding layer in its output layer. To understand better, let‚Äôs take a look at the shapes of the token embedding layer and linear output layer that we initialized on the <code>model</code> via the <code>GPTModel</code> earlier:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p184"> 
   <div class="code-area-container"> 
    <pre class="code-area">print("Token embedding layer shape:", model.tok_emb.weight.shape)
print("Output layer shape:", model.out_head.weight.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p185"> 
   <p>As we can see from the print outputs, the weight tensors for both these layers have the same shape:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p186"> 
   <div class="code-area-container"> 
    <pre class="code-area">Token embedding layer shape: torch.Size([50257, 768])
Output layer shape: torch.Size([50257, 768])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p187"> 
   <p>The token embedding and output layers are very large due to the number of rows for the 50,257 in the tokenizer‚Äôs vocabulary. Let‚Äôs remove the output layer parameter count from the total GPT-2 model count according to the weight tying:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p188"> 
   <div class="code-area-container"> 
    <pre class="code-area">total_params_gpt2 = (
    total_params - sum(p.numel()
    for p in model.out_head.parameters())
)
print(f"Number of trainable parameters "
      f"considering weight tying: {total_params_gpt2:,}"
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p189"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p190"> 
   <div class="code-area-container"> 
    <pre class="code-area">Number of trainable parameters considering weight tying: 124,412,160</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p191"> 
   <p>As we can see, the model is now only 124 million parameters large, matching the original size of the GPT-2 model. </p> 
  </div> 
  <div class="readable-text intended-text" id="p192"> 
   <p>Weight tying reduces the overall memory footprint and computational complexity of the model. However, in my experience, using separate token embedding and output layers results in better training and model performance; hence, we use separate layers in our <code>GPTModel</code> implementation. The same is true for modern LLMs. However, we will revisit and implement the weight tying concept later in chapter 6 when we load the pretrained weights from OpenAI.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p193"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 4.1 Number of parameters in feed forward and attention modules </h5> 
   </div> 
   <div class="readable-text" id="p194"> 
    <p>Calculate and compare the number of parameters that are contained in the feed forward module and those that are contained in the multi-head attention module.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p195"> 
   <p>Lastly, let‚Äôs compute the memory requirements of the 163 million parameters in our <code>GPTModel</code> object:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p196"> 
   <div class="code-area-container"> 
    <pre class="code-area">total_size_bytes = total_params * 4      <span class="aframe-location"/> #1
total_size_mb = total_size_bytes / (1024 * 1024)    <span class="aframe-location"/> #2
print(f"Total size of the model: {total_size_mb:.2f} MB")</pre> 
    <div class="code-annotations-overlay-container">
     #1 Calculates the total size in bytes (assuming float32, 4 bytes per parameter)
     <br/>#2 Converts to megabytes
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p197"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p198"> 
   <div class="code-area-container"> 
    <pre class="code-area">Total size of the model: 621.83 MB</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p199"> 
   <p>In conclusion, by calculating the memory requirements for the 163 million parameters in our <code>GPTModel</code> object and assuming each parameter is a 32-bit float taking up 4 bytes, we find that the total size of the model amounts to 621.83 MB, illustrating the relatively large storage capacity required to accommodate even relatively small LLMs. </p> 
  </div> 
  <div class="readable-text intended-text" id="p200"> 
   <p>Now that we‚Äôve implemented the <code>GPTModel</code> architecture and saw that it outputs numeric tensors of shape <code>[batch_size,</code> <code>num_tokens,</code> <code>vocab_size]</code>, let‚Äôs write the code to convert these output tensors into text.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p201"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 4.2 Initializing larger GPT models </h5> 
   </div> 
   <div class="readable-text" id="p202"> 
    <p>We initialized a 124-million-parameter GPT model, which is known as ‚ÄúGPT-2 small.‚Äù Without making any code modifications besides updating the configuration file, use the <code>GPTModel</code> class to implement GPT-2 medium (using 1,024-dimensional embeddings, 24 transformer blocks, 16 multi-head attention heads), GPT-2 large (1,280-dimensional embeddings, 36 transformer blocks, 20 multi-head attention heads), and GPT-2 XL (1,600-dimensional embeddings, 48 transformer blocks, 25 multi-head attention heads). As a bonus, calculate the total number of parameters in each GPT model.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p203"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.7</span> Generating text</h2> 
  </div> 
  <div class="readable-text" id="p204"> 
   <p>We will now implement the code that converts the tensor outputs of the GPT model back into text. Before we get started, let‚Äôs briefly review how a generative model like an LLM generates text one word (or token) at a time.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p205">  
   <img alt="figure" src="../Images/4-16.png" width="1063" height="975"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.16</span> The step-by-step process by which an LLM generates text, one token at a time. Starting with an initial input context (‚ÄúHello, I am‚Äù), the model predicts a subsequent token during each iteration, appending it to the input context for the next round of prediction. As shown, the first iteration adds ‚Äúa,‚Äù the second ‚Äúmodel,‚Äù and the third ‚Äúready,‚Äù progressively building the sentence. </h5>
  </div> 
  <div class="readable-text intended-text" id="p206"> 
   <p>Figure 4.16 illustrates the step-by-step process by which a GPT model generates text given an input context, such as ‚ÄúHello, I am.‚Äù With each iteration, the input context grows, allowing the model to generate coherent and contextually appropriate text. By the sixth iteration, the model has constructed a complete sentence: ‚ÄúHello, I am a model ready to help.‚Äù We‚Äôve seen that our current <code>GPTModel</code> implementation outputs tensors with shape <code>[batch_size,</code> <code>num_token,</code> <code>vocab_size]</code>. Now the question is: How does a GPT model go from these output tensors to the generated text? </p> 
  </div> 
  <div class="readable-text intended-text" id="p207"> 
   <p>The process by which a GPT model goes from output tensors to generated text involves several steps, as illustrated in figure 4.17. These steps include decoding the output tensors, selecting tokens based on a probability distribution, and converting these tokens into human-readable text.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p208">  
   <img alt="figure" src="../Images/4-17.png" width="1007" height="649"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.17</span> The mechanics of text generation in a GPT model by showing a single iteration in the token generation process. The process begins by encoding the input text into token IDs, which are then fed into the GPT model. The outputs of the model are then converted back into text and appended to the original input text.</h5>
  </div> 
  <div class="readable-text intended-text" id="p209"> 
   <p>The next-token generation process detailed in figure 4.17 illustrates a single step where the GPT model generates the next token given its input. In each step, the model outputs a matrix with vectors representing potential next tokens. The vector corresponding to the next token is extracted and converted into a probability distribution via the <code>softmax</code> function. Within the vector containing the resulting probability scores, the index of the highest value is located, which translates to the token ID. This token ID is then decoded back into text, producing the next token in the sequence. Finally, this token is appended to the previous inputs, forming a new input sequence for the subsequent iteration. This step-by-step process enables the model to generate text sequentially, building coherent phrases and sentences from the initial input context.</p> 
  </div> 
  <div class="readable-text intended-text" id="p210"> 
   <p>In practice, we repeat this process over many iterations, such as shown in figure 4.16, until we reach a user-specified number of generated tokens. In code, we can implement the token-generation process as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p211"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 4.8</span> A function for the GPT model to generate text</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def generate_text_simple(model, idx,                <span class="aframe-location"/> #1
                         max_new_tokens, context_size): 
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]   <span class="aframe-location"/> #2
        with torch.no_grad():
            logits = model(idx_cond)

        logits = logits[:, -1, :]                   <span class="aframe-location"/> #3
        probas = torch.softmax(logits, dim=-1)          <span class="aframe-location"/> #4
        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   <span class="aframe-location"/> #5
        idx = torch.cat((idx, idx_next), dim=1)    <span class="aframe-location"/> #6

    return idx</pre> 
    <div class="code-annotations-overlay-container">
     #1 idx is a (batch, n_tokens) array of indices in the current context.
     <br/>#2 Crops current context if it exceeds the supported context size, e.g., if LLM supports only 5 tokens, and the context size is 10, then only the last 5 tokens are used as context
     <br/>#3 Focuses only on the last time step, so that (batch, n_token, vocab_size) becomes (batch, vocab_size)
     <br/>#4 probas has shape (batch, vocab_size).
     <br/>#5 idx_next has shape (batch, 1).
     <br/>#6 Appends sampled index to the running sequence, where idx has shape (batch, n_tokens+1)
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p212"> 
   <p>This code demonstrates a simple implementation of a generative loop for a language model using PyTorch. It iterates for a specified number of new tokens to be generated, crops the current context to fit the model‚Äôs maximum context size, computes predictions, and then selects the next token based on the highest probability prediction. </p> 
  </div> 
  <div class="readable-text intended-text" id="p213"> 
   <p>To code the <code>generate_text_simple</code> function, we use a <code>softmax</code> function to convert the logits into a probability distribution from which we identify the position with the highest value via <code>torch.argmax</code>. The <code>softmax</code> function is monotonic, meaning it preserves the order of its inputs when transformed into outputs. So, in practice, the softmax step is redundant since the position with the highest score in the softmax output tensor is the same position in the logit tensor. In other words, we could apply the <code>torch.argmax</code> function to the logits tensor directly and get identical results. However, I provide the code for the conversion to illustrate the full process of transforming logits to probabilities, which can add additional intuition so that the model generates the most likely next token, which is known as <em>greedy decoding</em>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p214"> 
   <p>When we implement the GPT training code in the next chapter, we will use additional sampling techniques to modify the softmax outputs such that the model doesn‚Äôt always select the most likely token. This introduces variability and creativity in the generated text. </p> 
  </div> 
  <div class="readable-text intended-text" id="p215"> 
   <p>This process of generating one token ID at a time and appending it to the context using the <code>generate_text_simple</code> function is further illustrated in figure 4.18. (The token ID generation process for each iteration is detailed in figure 4.17.) We generate the token IDs in an iterative fashion. For instance, in iteration 1, the model is provided with the tokens corresponding to ‚ÄúHello, I am,‚Äù predicts the next token (with ID 257, which is ‚Äúa‚Äù), and appends it to the input. This process is repeated until the model produces the complete sentence ‚ÄúHello, I am a model ready to help‚Äù after six iterations.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p216">  
   <img alt="figure" src="../Images/4-18.png" width="1014" height="689"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.18</span> The six iterations of a token prediction cycle, where the model takes a sequence of initial token IDs as input, predicts the next token, and appends this token to the input sequence for the next iteration. (The token IDs are also translated into their corresponding text for better understanding.) </h5>
  </div> 
  <div class="readable-text intended-text" id="p217"> 
   <p>Let‚Äôs now try out the <code>generate_text_simple</code> function with the <code>"Hello,</code> <code>I</code> <code>am"</code> context as model input. First, we encode the input context into token IDs:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p218"> 
   <div class="code-area-container"> 
    <pre class="code-area">start_context = "Hello, I am"
encoded = tokenizer.encode(start_context)
print("encoded:", encoded)
encoded_tensor = torch.tensor(encoded).unsqueeze(0)   <span class="aframe-location"/> #1
print("encoded_tensor.shape:", encoded_tensor.shape)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Adds batch dimension
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p219"> 
   <p>The encoded IDs are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p220"> 
   <div class="code-area-container"> 
    <pre class="code-area">encoded: [15496, 11, 314, 716]
encoded_tensor.shape: torch.Size([1, 4])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p221"> 
   <p>Next, we put the model into <code>.eval()</code> mode. This disables random components like dropout, which are only used during training, and use the <code>generate_text_simple</code> function on the encoded input tensor:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p222"> 
   <div class="code-area-container"> 
    <pre class="code-area">model.eval()                 <span class="aframe-location"/> #1
out = generate_text_simple(
    model=model,
    idx=encoded_tensor, 
    max_new_tokens=6, 
    context_size=GPT_CONFIG_124M["context_length"]
)
print("Output:", out)
print("Output length:", len(out[0]))</pre> 
    <div class="code-annotations-overlay-container">
     #1 Disables dropout since we are not training the model
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p223"> 
   <p>The resulting output token IDs are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p224"> 
   <div class="code-area-container"> 
    <pre class="code-area">Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843,
30961, 42348,  7267]])
Output length: 10</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p225"> 
   <p>Using the <code>.decode</code> method of the tokenizer, we can convert the IDs back into text:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p226"> 
   <div class="code-area-container"> 
    <pre class="code-area">decoded_text = tokenizer.decode(out.squeeze(0).tolist())
print(decoded_text)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p227"> 
   <p>The model output in text format is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p228"> 
   <div class="code-area-container"> 
    <pre class="code-area">Hello, I am Featureiman Byeswickattribute argue</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p229"> 
   <p>As we can see, the model generated gibberish, which is not at all like the coherent text <code>Hello,</code> <code>I</code> <code>am</code> <code>a</code> <code>model</code> <code>ready</code> <code>to</code> <code>help</code>. What happened? The reason the model is unable to produce coherent text is that we haven‚Äôt trained it yet. So far, we have only implemented the GPT architecture and initialized a GPT model instance with initial random weights. Model training is a large topic in itself, and we will tackle it in the next chapter.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p230"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 4.3 Using separate dropout parameters </h5> 
   </div> 
   <div class="readable-text" id="p231"> 
    <p>At the beginning of this chapter, we defined a global <code>drop_rate</code> setting in the <code>GPT_ CONFIG_124M</code> dictionary to set the dropout rate in various places throughout the <code>GPTModel</code> architecture. Change the code to specify a separate dropout value for the various dropout layers throughout the model architecture. (Hint: there are three distinct places where we used dropout layers: the embedding layer, shortcut layer, and multi-head attention module.)</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p232"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p233"> Layer normalization stabilizes training by ensuring that each layer‚Äôs outputs have a consistent mean and variance. </li> 
   <li class="readable-text" id="p234"> Shortcut connections are connections that skip one or more layers by feeding the output of one layer directly to a deeper layer, which helps mitigate the vanishing gradient problem when training deep neural networks, such as LLMs. </li> 
   <li class="readable-text" id="p235"> Transformer blocks are a core structural component of GPT models, combining masked multi-head attention modules with fully connected feed forward networks that use the GELU activation function. </li> 
   <li class="readable-text" id="p236"> GPT models are LLMs with many repeated transformer blocks that have millions to billions of parameters. </li> 
   <li class="readable-text" id="p237"> GPT models come in various sizes, for example, 124, 345, 762, and 1,542 million parameters, which we can implement with the same <code>GPTModel</code> Python class. </li> 
   <li class="readable-text" id="p238"> The text-generation capability of a GPT-like LLM involves decoding output tensors into human-readable text by sequentially predicting one token at a time based on a given input context. </li> 
   <li class="readable-text" id="p239"> Without training, a GPT model generates incoherent text, which underscores the importance of model training for coherent text generation. </li> 
  </ul>
 </div></div></body></html>