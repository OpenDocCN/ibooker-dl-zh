- en: 6 Testing, assessing, and explaining with large language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Drafting unit tests with ease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating integration tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining code quality and coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing software complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating code and text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This chapter will explore a critical aspect of software engineering: testing.
    The act of testing software serves multiple essential purposes. First and foremost,
    it aids in the identification of bugs, errors, and problems that can potentially
    affect the software’s functionality, usability, or performance. Furthermore, it
    ensures that the software adheres to the required quality standards. By conducting
    thorough tests, we can verify whether the software meets the specified requirements,
    functions as intended, and produces the expected outcomes. Through comprehensive
    testing, developers can evaluate software’s reliability, accuracy, efficiency,
    security, and compatibility across various platforms and environments. Detecting
    and resolving software defects early in the development process can result in
    significant time and cost savings.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have finished formulating our tests, we will evaluate the quality of
    our code. You will be introduced to several metrics that are helpful in assessing
    software quality and complexity. Additionally, if we need clarification on the
    purpose of our code or are reviewing it for the first time, we will seek an explanation
    to ensure thorough understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Testing, testing … one, two, three types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Testing plays a vital role in software engineering; therefore, we will explore
    various types of testing in detail. These include unit tests, integration tests,
    and behavior tests. To start, we will use Copilot Chat to help us create a *unit
    test*.
  prefs: []
  type: TYPE_NORMAL
- en: Definition A *unit test* focuses on testing individual components or units of
    code to ensure that they function correctly in isolation. Developers usually run
    unit tests to help identify bugs and problems in specific software units.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Unit testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will create unit tests to test our software components.
    Several unit-testing frameworks are available for Python. Each has unique features
    and is suitable for different scenarios. We will examine each of them briefly
    before settling on a specific framework based on the recommendation provided by
    our AI tool.
  prefs: []
  type: TYPE_NORMAL
- en: The first framework is `unittest`. This is Python’s standard library for creating
    unit tests. It comes bundled with Python and doesn’t need to be installed separately.
    `unittest` provides a rich set of assertions and is great for writing simple to
    complex test cases, but it can be verbose. It is a good choice for writing basic
    unit tests, especially if you don’t want to introduce additional dependencies
    in your project. It’s useful in any scenario where you need to confirm the functionality
    of individual units of code in isolation from the rest of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s examine `pytest`. It is a popular third-party library that can be
    used for unit testing, although it’s versatile enough to handle more than just
    unit tests. It requires less boilerplate code than `unittest` and has powerful
    features like fixtures for setup and teardown, parameterized testing, and the
    ability to run `unittest` and `nose` test suites. p`ytest` is great for both simple
    and complex unit test cases. It’s also useful for functional and integration tests.
    If you value simplicity and ease of use, and your project is not restricted to
    using only the Python standard library, `pytest` is an excellent choice.
  prefs: []
  type: TYPE_NORMAL
- en: '`nose2` is the successor to the deprecated `nose` testing framework. It extends
    `unittest` and makes testing easier. It’s known for its test discovery feature,
    which automatically finds your project’s tests so you don’t have to manually list
    them. `nose2` is good for larger projects where test discovery can save time.
    Like `pytest`, it can run `unittest` test suites, so it’s also a good choice if
    you’re migrating from `unittest` but want to keep your existing tests.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there is `doctest`. This is another module that’s part of the Python
    standard library. It’s not a full-featured testing framework like the others,
    but it allows you to write tests directly in your docstrings. `doctest` is best
    suited for simple cases where you want to demonstrate how to use a function or
    module and confirm that the example code works as expected. It’s a great way to
    ensure that your documentation stays up to date with your code.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot Chat recommended that we use `unittest`, so we will. You might try to
    ask it yourself and see if it reaches a different conclusion, which it may, depending
    on how you frame the question.
  prefs: []
  type: TYPE_NORMAL
- en: First we will define test cases by creating classes that inherit from the `unittest.TestCase`
    class. In writing our first unit test, we will uncover a rather pernicious bug
    that has thus far eluded us; we will get to that shortly. Finally, we are going
    to test the correctness of the `DepartmentStatisticsVisitor` class.
  prefs: []
  type: TYPE_NORMAL
- en: We start with a prompt asking Copilot Chat to create a unit test for `DepartmentStatisticsVisitor`.
    The response will be a reminder of the importance of context. Copilot Chat, given
    that we began a new chat session, will attempt to create this unit test, but it
    lacks the definition of any of our classes.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 First attempt to define a unit test
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We proceed by inputting definitions into Copilot Chat so that it can refine
    the test case. We can tell it more about the visitor class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With this updated information, Copilot Chat can better estimate how our unit
    test should function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 The result of a unit test prompt
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The unit test is close to what we need; however, the definition of `Asset` is
    incorrect. We can enter the proper definition from our asset.py class, and Copilot
    Chat will continue to refine the unit test.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 Next attempt to define a unit test, with `Asset` definition
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is closer. We also need to remind Copilot Chat that the `Asset` class has
    a class called `FundingDetails`. Let’s provide the definition of `FundingDetails`
    and inform Copilot Chat that it should be part of `Asset`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 Third attempt at a unit test, which comes reasonably close
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: All that remains is to manually update the `import` statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE Let this exercise be a cautionary tale in the usage of large language
    models: coercing them to behave with respect to highly focused tasks can be very
    challenging, to the point of not being worth the effort. In the real world, no
    one will blame you for abandoning the prompt engineering and going back to just
    coding out this test. However, with some persistence, you can build a template
    library that you can use to build a suite of unit tests for similarly shaped classes.
    Also note that Copilot Chat can and will generate a test for the file in the editor
    window if you instruct it to Generate (a) unit test for my code, but it will mock
    all objects/properties that are not directly part of the class under test. Depending
    on what you are attempting to test, the utility of this feature may be questionable.
    Another common problem specific to Python is that the indentation is frequently
    incorrect with code copied from Copilot Chat.'
  prefs: []
  type: TYPE_NORMAL
- en: When we attempt to run this test, we discover that there is a *circular dependency*
    between the visitor, asset, funding details, and depreciation strategy. A circular
    dependency is a situation in which two or more modules or components depend on
    each other directly or indirectly. In our case, when Python tries to instantiate
    `Asset`, it loads the definition of `FundingDetails`. We can fix this by moving
    away from a direct instantiation or reference to the `FundingDetails` class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 Updated `Asset`, no direct reference to `FundingDetails`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We need to do the same for the `FundingDetails` class. It should not directly
    reference the `DepreciationStrategy` class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.6 `FundingDetails`, no direct ref to `DepreciationStrategy`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As we’ve seen, we were able to create a unit test using Copilot Chat. However,
    we would likely have been able to create it more easily if we had written it without
    Copilot. The tool is surprisingly good at providing guidance as to when and how
    to test your code, but the implementation (at least currently) leaves something
    to be desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the real world, we would continue to add unit tests to build up a substantial
    body of tests. How many tests is *substantial*, you ask? We will explore this
    shortly. However, we first turn our attention to the next type of test: the *integration
    test*.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition *Integration testing* involves testing the interaction between different
    components or modules of the software to ensure that they work together seamlessly.
    It verifies that the integrated system functions as expected and detects any inconsistencies
    or communication problems between modules.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Integration testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will develop an integration test that will allow us to test
    the end-to-end system. Fortunately, `fastapi` comes with its own test client,
    which will aid us in creating this test.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by copying in the definition of `AssetController` into the Copilot
    Chat window. We can then ask Copilot Chat how to create an integration test for
    this controller. Given that we included the routes in the definition, Copilot
    Chat should be able to provide us with accurate integration tests. We need to
    specify that we will use the `InMemoryAssetRepository` class or fix this after
    the test has been generated.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 Copilot Chat-generated integration test of `AssetController`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now turn our attention to the final type of testing that we’ll examine:
    *behavior testing*.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition *Behavior testing* focuses on the behavior of a system as a whole
    from the perspective of an end user. It is typically used to test the functionality
    of a system and to ensure that the system meets the requirements and specifications
    defined for it.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Behavior testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a sense, all testing is behavior testing, as tests verify the behavior of
    the system. However, behavior testing is unique in some respects. Let’s summarize
    the different types of testing that we have encountered thus far and contrast
    them against behavior tests.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing focuses on testing individual units or components of a system in
    isolation, typically using automated tests. Unit tests are designed to test the
    functionality of individual functions or methods and to ensure that they behave
    correctly under a variety of conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing, on the other hand, focuses on testing the interactions
    between different components or units of a system. Integration testing is typically
    used to test the interfaces between different components or units and to ensure
    that they work together correctly. Integration testing can be performed manually
    or using automated tests, and it typically involves testing the interactions between
    different components or units of a system rather than the system as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Behavioral testing focuses on defining the behavior of the software in terms
    of user stories or scenarios. These scenarios are written in a specific format
    called *given-when-then* (GWT) and are used to drive the development process.
    The GWT format describes the preconditions (given), the actions (when), and the
    expected outcomes (then) of a particular scenario.
  prefs: []
  type: TYPE_NORMAL
- en: As we progress with our testing, we may find that some behaviors or components
    are difficult to set up in our test. Additionally, it may be tricky to isolate
    the behavior of a particular object or module and test the interactions between
    different objects. To address this limitation, we can use a *mock object*.
  prefs: []
  type: TYPE_NORMAL
- en: Definition A *mock object* is a test double that simulates the behavior of a
    real object in a controlled way. Mock objects can also be used to simulate error
    conditions or edge cases that are difficult to reproduce with real objects. They
    can be created manually using a mocking library such as `unittest.mock` or `pytest-mock`.
    These libraries provide functions and classes for creating and configuring mock
    objects. Mock objects can be configured to return specific values or raise specific
    exceptions when their methods are called. They can also be used to record the
    calls made to their methods so that you can verify that the correct methods were
    called with the correct parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will incorporate mock objects into the prompt to create a behavior test
    for `AssetManager`:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| We have a class called AssetManager. AssetManager''s constructor takes two
    classes BaseRepository and AssetLocationMediator. The BaseRepository is a repository
    in which Assets are stored. We want to use InMemoryAssetRepository, which is a
    subclass of BaseRepository that stores the Assets in memory. Use a mock object
    for AssetLocationMediator. The AssetManager stores Assets using the method create
    and gets all of the Assets using the method get_assets. An asset has 10 required
    positional arguments: ''id'', ''name'', ''status'', ''category'', ''cost'', ''useful_life'',
    ''salvage_value'', ''purchase_date'', ''locations'', and ''funding_details''.
    Use named parameters to make it clear which param is going where. Create a BDD
    script that will add two Assets to the AssetManager, confirming that all Assets
    cost $4000.00\. This should be implemented in Python. Take it step by step, telling
    us the tools and libraries you will use. |'
  prefs: []
  type: TYPE_TB
- en: Copilot Chat tells us that we need to create a new file (a *feature file*) called
    `asset_manager.feature`. In this file, we will define `AssetManager`’`s` attributes
    (and any supporting classes). For example, the feature file defines a scenario
    that adds two assets to `AssetManager`, confirms that the total cost of all assets
    is $4,000.00, and mocks the `AssetLocationMediator` class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.8 `asset_manager.feature` file according to Copilot Chat
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We need to create a directory called features at the root of our project and
    place this file in that directory. Additionally, we need to add a subdirectory
    called steps. Next we create a new Python file called steps.py and place it in
    the steps directory. This Python file defines the step implementations for the
    scenario in the feature file. The `AssetManager`, `InMemoryAssetRepository`, `AssetLocationMediator`,
    and `Asset` classes are imported, and six functions are defined to correspond
    to the steps in the scenario. These functions use the context object to store
    and retrieve data between steps. The `Mock` class from the `unittest.mock` library
    is used to create a mock object for the `AssetLocationMediator` class. The `Asset`
    class is instantiated using named parameters to clarify which parameter belongs
    to which attribute. If we ask Copilot Chat for the contents of the steps.py file,
    we will see output similar to the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.9 Implementation details for behavior test defined in steps.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do not have it installed already, you need to install the `behave` library
    using pip: `pip install behave`. Additionally, add it to the requirements.txt
    file to ensure that it will be pulled in when we later build a deployable version
    of this application. We run the behavior test by issuing the following command
    from the root directory of our project.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.10 Running the behavior test, and the resulting output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this section, we have laid a foundation for good software development by
    using three types of tests: unit, integration, and behavior. Some may quibble
    that it came very late in the development lifecycle of this project, and they
    would not be wrong. In the real world, we develop our tests as we develop our
    code. Some may argue that we need to build our tests *before* our code. You may
    or may not hold this belief, but either way, you need to test early and test often.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we dive into some metrics that can be used to determine
    the overall quality of our software, and we will ask Copilot to help us assess
    the quality of our code thus far.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Assessing quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the performance, reliability, maintainability, and overall quality
    of software applications is a crucial aspect of software engineering. This section
    delves into the fascinating and intricate domain of software quality metrics—the
    quantitative standards and benchmarks that guide our understanding of the quality
    of a software system.
  prefs: []
  type: TYPE_NORMAL
- en: Software quality metrics are essential tools that allow stakeholders—developers,
    testers, managers, and users—to assess a software product’s state, identifying
    its strengths and areas for improvement. They provide an empirical foundation
    for various processes such as product development, testing, debugging, maintenance,
    and improvement initiatives. By quantifying specific characteristics of the software,
    these metrics provide a tangible means to understand the otherwise abstract concept
    of software quality.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explore several important categories of software quality
    metrics, including product metrics, process metrics, and project metrics. We’ll
    analyze their significance, methodologies for their calculation, and how they
    can be effectively utilized to evaluate and enhance software quality. This exploration
    will include both static metrics, which are applied to the static software system,
    and dynamic metrics, which assess the system’s behavior during execution.
  prefs: []
  type: TYPE_NORMAL
- en: Software quality metrics not only contribute to the technical soundness of a
    software system but also help ensure customer satisfaction, profitability, and
    long-term business success. Therefore, developing an understanding of these metrics
    is invaluable to anyone involved in the field of software development, from engineers
    and project managers to executives and software users.
  prefs: []
  type: TYPE_NORMAL
- en: We will examine a few common measures of complexity and maintainability of the
    class or code. Complex software can be difficult to comprehend, which makes it
    challenging for developers, particularly new ones, to grasp how different parts
    of the software interact with each other. This can slow down the onboarding process
    and development time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Complex code often leads to more maintenance: modifications or bug fixes can
    take longer because it’s harder to predict the effects of changing a single piece
    of the system. This can result in higher costs over the software’s lifecycle.'
  prefs: []
  type: TYPE_NORMAL
- en: Complex software also tends to be more error-prone. Because it’s harder to understand,
    developers are more likely to introduce bugs when making changes. Also, complex
    code can have many interdependencies, and a change in one area may have unexpected
    effects elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: The more complex the software, the more test cases are required to achieve thorough
    testing. It may also be harder to write these test cases due to the complexity
    of the logic involved.
  prefs: []
  type: TYPE_NORMAL
- en: Writing simple and maintainable code should be one of our highest priorities.
    Observing the change in the metric that accompanies our code should aid us in
    this endeavor. Toward this objective, the first metric that we can (and should)
    use is *cyclomatic complexity*.
  prefs: []
  type: TYPE_NORMAL
- en: Definition *Cyclomatic complexity* is a metric that quantifies the number of
    independent paths through a software module. It measures the complexity of decision-making
    in the code, including loops, conditionals, and branches. A higher cyclomatic
    complexity value indicates increased complexity and suggests the potential for
    more bugs and challenges in understanding and maintaining the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following prompt anywhere in the file department_visitor.py. Copilot
    will immediately output the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Copilot tells us that the complexity of this class is 1\. You may or may not
    be aware of the meaning of this value. If the latter, you can ask Copilot to elaborate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Copilot informs us that cyclomatic complexity is good if it is low. Intuitively,
    this makes sense. Code with low complexity means it is simpler to understand and
    therefore reason about. It is likely easier to maintain as well. Next we will
    explore the *Halstead complexity measures.*
  prefs: []
  type: TYPE_NORMAL
- en: Definition *Halstead complexity measures* assess the complexity of a software
    program based on the number of unique operators and operands used in the code.
    These measures include metrics such as program length (N1), program vocabulary
    (n1), volume (V), difficulty (D), effort (E), and others. These metrics provide
    insights into the size and cognitive complexity of the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to last time, we will start with a prompt asking Copilot to determine
    the Halstead complexity measure for our visitor class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You may want to continue this Q&A session for a while to see what information
    can be gleaned from Copilot. Once you are ready to continue, there is one more
    metric to explore: the *maintainability index.*'
  prefs: []
  type: TYPE_NORMAL
- en: Definition The *maintainability index* is a composite metric that combines several
    factors, including cyclomatic complexity, lines of code, and Halstead complexity
    measures, to provide an overall measure of software maintainability. A higher
    maintainability index suggests easier maintenance and potentially lower complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a similar discussion for the maintainability index in the visitor file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If we get a low maintainability index, we can refactor to reduce this number.
  prefs: []
  type: TYPE_NORMAL
- en: 'A metric is useful in that it gives us a nail to hang our hat on; that is,
    we can take that measure and perform some action to improve it. Metrics move us
    beyond pure aesthetics or the subjectivity of an individual. A metric is real,
    actionable data. But Copilot has (at least) one more trick up its proverbial sleeve.
    Copilot is capable of doing more than just writing and assessing our code: it
    can also address the code’s flaws. Let’s bug hunt.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Hunting for bugs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will use an elementary (albeit contrived) example to demonstrate
    how we can use Copilot to find and fix problems in our code. This code is supposed
    to loop a the list of integers and calculate the sum. However, there is a “blink
    and you’ll miss it” bug: the sum is assigned the value of `i` rather than adding
    the value of `i` to the running total.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.11 Looping over a list of integers and calculating the sum
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To debug this problem, we will introduce a new tool: Copilot Labs. Prior to
    Copilot Chat, Copilot Labs was the only means by which certain features were available
    in an IDE (specifically, Visual Studio Code). For example, we need to use Copilot
    Labs to find and fix bugs. The main advantage that Copilot Labs still offers today
    is that it can access the highlighted contents of your editor pane. This feature
    allows Copilot Labs to operate directly on the editable code in your IDE.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you install the extension into your IDE, you should see a Copilot Labs
    toolkit on the left side of the IDE, as shown in figure 6.1\. If you need a reminder
    about how to install an extension into your IDE, see appendices A–C for instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F01_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 The Copilot Labs toolkit menu, which includes options for finding
    and fixing bugs. The toolkit also provides facilities to enhance your code as
    well as document it.
  prefs: []
  type: TYPE_NORMAL
- en: We will temporarily change the contents of the main.py file to the code listed
    in listing 6.11\. Once you have made this change, highlight the code and click
    the Fix Bug button in the Copilot Labs toolkit. You should see output like that
    in figure 6.2\. Copilot Labs was able to determine the problem in this code and
    provides a suggestion as to how to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F02_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 Copilot Labs, using the GPT model, has identified the bug and how
    to address it.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you could copy this code into ChatGPT and ask it to find the
    bug. However, it is arguable that doing so is less convenient as you would have
    to know there was a bug in your code before asking ChatGPT to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Covering code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Code coverage* is a measure of how much of your code is being exercised by
    your tests. It is typically expressed as a percentage and represents the proportion
    of your code that your tests execute.'
  prefs: []
  type: TYPE_NORMAL
- en: Code coverage can be used as a metric to evaluate the effectiveness of your
    tests. If your code coverage is low, it may indicate that parts of your code are
    not being tested, which can lead to uncaught bugs and other problems. Alternatively,
    with high code coverage, you can rest assured that your code is well-tested. This
    does not guarantee that your code is bug-free, but it should give you a high degree
    of confidence that if there are bugs, they will be caught in a test.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the code coverage in our Python project, we will use the code coverage
    tool provided in the `coverage` library. The `coverage` library works by instrumenting
    our code to collect coverage data as it runs. It can collect coverage data for
    any Python code, including tests, scripts, and modules. By using a code coverage
    tool like `coverage`, we can better understand how much of our code is being exercised
    by our tests and identify areas of our code that may need more testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s install `coverage` using pip: `pip install coverage`. Next, let’s
    run our tests with coverage: `coverage run -m pytest`. This runs your tests and
    collects coverage data.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we will generate a coverage report (see figure 6.3). The coverage report
    shows the code coverage for each file in our project. We can create a text-based
    coverage report using the `coverage report` command or generate an HTML version
    of the report using the `coverage html` command. The HTML version of the report
    is in the htmlcov directory.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F03_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 The code coverage report showing the coverage for each file in our
    ITAM system project
  prefs: []
  type: TYPE_NORMAL
- en: 'Code coverage of 70% is a good start. In the real world, we would continue
    working with our team and generative AI pals to bring this measure up into the
    high 90s. Now we will transition to a new topic: using generative AI to describe
    a code listing to us.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Transliterating code—from code to descriptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often, we are handed existing source code. Determining the exact entry point
    of this code, the purpose of the code, and the overall structure of a brownfield
    project can be challenging. Fortunately, this is one of the areas in which generative
    AIs truly excel: translating code into textual descriptions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we will copy the (buggy) code from the previous section into the
    Copilot Chat dialog box, prefixed with the following prompt (see figure 6.4):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| What does this code do? |'
  prefs: []
  type: TYPE_TB
- en: '![](../Images/CH06_F04_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 The buggy code from the last section with a prompt asking Copilot
    Chat to explain this code to us
  prefs: []
  type: TYPE_NORMAL
- en: What is striking about the explanation in figure 6.5 is that Copilot Chat detects
    a bug in the code and offers suggestions for how to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F05_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 Copilot Chat explains the code’s purpose, identifies the bug, and
    suggests a fix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we will ask it something more challenging: to explain the behavior test
    from listing 6.9 into the Copilot Chat window, and ask it what this class does.
    You will likely receive a response similar to the following.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.12 Copilot Chat explaining the behavior test we wrote earlier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 6.6 Translating from one language to another
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As remote as this may seem, you may find yourself looking through a code base
    written in a programming language with which you have only a passing familiarity.
    Alternatively, you may pine for a specific function from another language to be
    available in your current language. For example, what would the `Asset` and `FundingDetails`
    classes look like if they were rewritten in JavaScript? Well, let’s ask Copilot
    Chat.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.13 `Asset` and `FundingDetails` translated to JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You may think that translating a class or two is too easy. Well, we will attempt
    something more complex:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| What would the AssetManager behavior test look like if written in JavaScript?
    |'
  prefs: []
  type: TYPE_TB
- en: Listing 6.14 `AssetManager` behavior test translated into JavaScript
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, you may wish to have GPT rewrite the code in idiomatic JavaScript.
    You need only say
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Write AssetManager in idiomatic JavaScript |'
  prefs: []
  type: TYPE_TB
- en: Copilot Chat will inform you that it will define the `AssetManager` class using
    the class syntax introduced in ES6.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.15 `AssetManager` class rewritten in idiomatic JavaScript
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here’s an example of what the `AssetManager` behavior test might look like in
    JavaScript using the Jest testing framework (which Copilot Chat included without
    our suggestion).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.16 `AssetManager` behavior test in idiomatic JavaScript
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we use the Jest testing framework to define a set of behavior
    tests for the `AssetManager` class. We are using the `describe` library to determine
    test suites and individual tests.
  prefs: []
  type: TYPE_NORMAL
- en: In each test, we create an instance of the `AssetManager` class and perform
    some operation on it, such as adding or removing an asset. We then use the `expect`
    function to make assertions about the behavior of the `AssetManager` class, such
    as checking that an asset was added to the asset list or that an asset was removed
    from the asset list. Note that the syntax for defining classes and constructors
    is slightly different in JavaScript than in Python, and we are using the required
    function to import the `AssetManager` and `Asset` classes from their respective
    modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, time for the big finale: we will start a new ChatGPT 4.0 session and
    paste in our Mermaid diagram from chapter 3\. Lead with this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Here is a mermaid diagram of my application. Please implement in Golang.
    |'
  prefs: []
  type: TYPE_TB
- en: Marvel as it returns the code, rendered in Golang.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.17 `Asset` class rewritten in idiomatic Golang
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Next, tell it to switch to Java.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.18 `Asset` class rewritten in idiomatic Java
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unit testing focuses on testing individual components or units of code to identify
    bugs and problems in specific units. Unit tests will be the most numerous in your
    codebase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration testing tests the interaction between different components or modules
    of the software to ensure seamless integration and detect communication problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Behavior testing tests a system’s functionality from an end user’s perspective,
    ensuring that it meets requirements and specifications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mock objects simulate the behavior of natural objects in a controlled way and
    are useful for testing and simulating error conditions. Mock objects are especially
    good at mimicking parts of the system that are needed for the test to run but
    are outside the scope of the test: for example, if your class has a constructor
    argument for a database, but you do not want to test the database directly because
    the data may change, causing your test to be inconclusive, nonrepeatable, or nondeterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cyclomatic complexity measures the number of independent paths through a software
    module, indicating complexity and potential for bugs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Halstead complexity measures assess software complexity based on unique operators
    and operands, providing insights into code size and cognitive complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maintainability index combines factors like cyclomatic complexity, lines
    of code, and Halstead measures to evaluate software maintainability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code coverage is a metric for evaluating test effectiveness, indicating the
    extent to which code is tested and the potential for uncaught bugs. Generally,
    higher is better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large language models allow you to navigate code in an unfamiliar programming
    language or translate features from another language in the current or preferred
    one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
