- en: Chapter 4\. Fully Connected Deep Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will introduce you to fully connected deep networks. Fully connected
    networks are the workhorses of deep learning, used for thousands of applications.
    The major advantage of fully connected networks is that they are “structure agnostic.”
    That is, no special assumptions need to be made about the input (for example,
    that the input consists of images or videos). We will make use of this generality
    to use fully connected deep networks to address a problem in chemical modeling
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We delve briefly into the mathematical theory underpinning fully connected networks.
    In particular, we explore the concept that fully connected architectures are “universal
    approximators” capable of learning any function. This concept provides an explanation
    of the generality of fully connected architectures, but comes with many caveats
    that we discuss at some depth.
  prefs: []
  type: TYPE_NORMAL
- en: While being structure agnostic makes fully connected networks very broadly applicable,
    such networks do tend to have weaker performance than special-purpose networks
    tuned to the structure of a problem space. We will discuss some of the limitations
    of fully connected architectures later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Fully Connected Deep Network?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A fully connected neural network consists of a series of fully connected layers.
    A fully connected layer is a function from <math alttext="double-struck upper
    R Superscript m"><msup><mi>ℝ</mi> <mi>m</mi></msup></math> to <math alttext="double-struck
    upper R Superscript n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math> . Each output
    dimension depends on each input dimension. Pictorially, a fully connected layer
    is represented as follows in [Figure 4-1](#ch4-fclayer).
  prefs: []
  type: TYPE_NORMAL
- en: '![FCLayer.png](assets/tfdl_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. A fully connected layer in a deep network.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s dig a little deeper into what the mathematical form of a fully connected
    network is. Let <math><mrow><mi>x</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mi>m</mi></msup></mrow></math>
    represent the input to a fully connected layer. Let <math alttext="y Subscript
    i Baseline element-of double-struck upper R"><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>∈</mo> <mi>ℝ</mi></mrow></math> be the <math alttext="i"><mi>i</mi></math>
    -th output from the fully connected layer. Then <math><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>∈</mo> <mi>ℝ</mi></mrow></math> is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msub><mi>w</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>w</mi> <mi>m</mi></msub> <msub><mi>x</mi>
    <mi>m</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here, <math alttext="sigma"><mi>σ</mi></math> is a nonlinear function (for now,
    think of <math><mi>σ</mi></math> as the sigmoid function introduced in the previous
    chapter), and the <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
    are learnable parameters in the network. The full output *y* is then
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mi>σ</mi>
    <mo>(</mo> <msub><mi>w</mi> <mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>w</mi>
    <mrow><mn>1</mn><mo>,</mo><mi>m</mi></mrow></msub> <msub><mi>x</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mrow><mi>σ</mi>
    <mo>(</mo> <msub><mi>w</mi> <mrow><mi>n</mi><mo>,</mo><mn>1</mn></mrow></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>w</mi>
    <mrow><mi>n</mi><mo>,</mo><mi>m</mi></mrow></msub> <msub><mi>x</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that it’s directly possible to stack fully connected networks. A network
    with multiple fully connected networks is often called a “deep” network as depicted
    in [Figure 4-2](#ch4-multifcnet).
  prefs: []
  type: TYPE_NORMAL
- en: '![multilayer_fcnet.png](assets/tfdl_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. A multilayer deep fully connected network.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As a quick implementation note, note that the equation for a single neuron
    looks very similar to a dot-product of two vectors (recall the discussion of tensor
    basics). For a layer of neurons, it is often convenient for efficiency purposes
    to compute *y* as a matrix multiply:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mi>σ</mi> <mo>(</mo> <mi>w</mi>
    <mi>x</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where sigma is a matrix in <math alttext="double-struck upper R Superscript
    n times m"><msup><mi>ℝ</mi> <mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msup></math>
    and the nonlinearity <math alttext="sigma"><mi>σ</mi></math> is applied componentwise.
  prefs: []
  type: TYPE_NORMAL
- en: “Neurons” in Fully Connected Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The nodes in fully connected networks are commonly referred to as “neurons.”
    Consequently, elsewhere in the literature, fully connected networks will commonly
    be referred to as “neural networks.” This nomenclature is largely a historical
    accident.
  prefs: []
  type: TYPE_NORMAL
- en: In the 1940s, Warren S. McCulloch and Walter Pitts published a first mathematical
    model of the brain that argued that neurons were capable of computing arbitrary
    functions on Boolean quantities. Successors to this work slightly refined this
    logical model by making mathematical “neurons” continuous functions that varied
    between zero and one. If the inputs of these functions grew large enough, the
    neuron “fired” (took on the value one), else was quiescent. With the addition
    of adjustable weights, this description matches the previous equations.
  prefs: []
  type: TYPE_NORMAL
- en: Is this how a real neuron behaves? Of course not! A real neuron ([Figure 4-3](#ch4-neuron))
    is an exceedingly complex engine, with over 100 trillion atoms, and tens of thousands
    of different signaling proteins capable of responding to varying signals. A microprocessor
    is a better analogy for a neuron than a one-line equation.
  prefs: []
  type: TYPE_NORMAL
- en: '![neuron.png](assets/tfdl_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. A more biologically accurate representation of a neuron.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In many ways, this disconnect between biological neurons and artificial neurons
    is quite unfortunate. Uninitiated experts read breathless press releases claiming
    artificial neural networks with billions of “neurons” have been created (while
    the brain has only 100 billion biological neurons) and reasonably come away believing
    scientists are close to creating human-level intelligences. Needless to say, state
    of the art in deep learning is decades (or centuries) away from such an achievement.
  prefs: []
  type: TYPE_NORMAL
- en: As you read further about deep learning, you may come across overhyped claims
    about artificial intelligence. Don’t be afraid to call out these statements. Deep
    learning in its current form is a set of techniques for solving calculus problems
    on fast hardware. It is not a precursor to *Terminator* ([Figure 4-4](#ch4-terminator)).
  prefs: []
  type: TYPE_NORMAL
- en: '![terminator.png](assets/tfdl_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. Unfortunately (or perhaps fortunately), this book won’t teach you
    to build a Terminator!
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: AI Winters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Artificial intelligence has gone through multiple rounds of boom-and-bust development.
    This cyclical development is characteristic of the field. Each new advance in
    learning spawns a wave of optimism in which prophets claim that human-level (or
    superhuman) intelligences are incipient. After a few years, no such intelligences
    manifest, and disappointed funders pull out. The resulting period is called an
    AI winter.
  prefs: []
  type: TYPE_NORMAL
- en: There have been multiple AI winters so far. As a thought exercise, we encourage
    you to consider when the next AI winter will happen. The current wave of deep
    learning progress has solved many more practical problems than any previous wave
    of advances. Is it possible AI has finally taken off and exited the boom-and-bust
    cycle or do you think we’re in for the “Great Depression” of AI soon?
  prefs: []
  type: TYPE_NORMAL
- en: Learning Fully Connected Networks with Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first version of a fully connected neural network was the Perceptron, ([Figure 4-5](#ch4-perceptron)),
    created by Frank Rosenblatt in the 1950s. These perceptrons are identical to the
    “neurons” we introduced in the previous equations.
  prefs: []
  type: TYPE_NORMAL
- en: '![perceptron.png](assets/tfdl_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. A diagrammatic representation of the perceptron.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Perceptrons were trained by a custom “perceptron” rule. While they were moderately
    useful solving simple problems, perceptrons were fundamentally limited. The book
    *Perceptrons* by Marvin Minsky and Seymour Papert from the end of the 1960s proved
    that simple perceptrons were incapable of learning the XOR function. [Figure 4-6](#ch4-perceptron2)
    illustrates the proof of this statement.
  prefs: []
  type: TYPE_NORMAL
- en: '![xor2.gif](assets/tfdl_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. The perceptron’s linear rule can’t learn the perceptron.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This problem was overcome with the invention of the multilayer perceptron (another
    name for a deep fully connected network). This invention was a formidable achievement,
    since earlier simple learning algorithms couldn’t learn deep networks effectively.
    The “credit assignment” problem stumped them; how does an algorithm decide which
    neuron learns what?
  prefs: []
  type: TYPE_NORMAL
- en: The full solution to this problem requires backpropagation. Backpropagation
    is a generalized rule for learning the weights of neural networks. Unfortunately,
    complicated explanations of backpropagation are epidemic in the literature. This
    situation is unfortunate since backpropagation is simply another word for automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose that <math alttext="f left-parenthesis theta comma x right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>θ</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow></math> is a function
    that represents a deep fully connected network. Here <math alttext="x"><mi>x</mi></math>
    is the inputs to the fully connected network and <math alttext="theta"><mi>θ</mi></math>
    is the learnable weights. Then the backpropagation algorithm simply computes <math
    alttext="StartFraction normal partial-differential f Over normal partial-differential
    theta EndFraction"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><mi>θ</mi></mrow></mfrac></math>
    . The practical complexities arise in implementing backpropagation for all possible
    functions *f* that arise in practice. Luckily for us, TensorFlow takes care of
    this already!
  prefs: []
  type: TYPE_NORMAL
- en: Universal Convergence Theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding discussion has touched on the ideas that deep fully connected
    networks are powerful approximations. McCulloch and Pitts showed that logical
    networks can code (almost) any Boolean function. Rosenblatt’s Perceptron was the
    continuous analog of McCulloch and Pitt’s logical functions, but was shown to
    be fundamentally limited by Minsky and Papert. Multilayer perceptrons looked to
    solve the limitations of simple perceptrons and empirically seemed capable of
    learning complex functions. However, it wasn’t theoretically clear whether this
    empirical ability had undiscovered limitations. In 1989, George Cybenko demonstrated
    that multilayer perceptrons were capable of representing arbitrary functions.
    This demonstration provided a considerable boost to the claims of generality for
    fully connected networks as a learning architecture, partially explaining their
    continued popularity.
  prefs: []
  type: TYPE_NORMAL
- en: However, if both backpropagation and fully connected network theory were understood
    in the late 1980s, why didn’t “deep” learning become more popular earlier? A large
    part of this failure was due to computational limitations; learning fully connected
    networks took an exorbitant amount of computing power. In addition, deep networks
    were very difficult to train due to lack of understanding about good hyperparameters.
    As a result, alternative learning algorithms such as SVMs that had lower computational
    requirements became more popular. The recent surge in popularity in deep learning
    is partly due to the increased availability of better computing hardware that
    enables faster computing, and partly due to increased understanding of good training
    regimens that enable stable learning.
  prefs: []
  type: TYPE_NORMAL
- en: Is Universal Approximation That Surprising?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Universal approximation properties are more common in mathematics than one might
    expect. For example, the Stone-Weierstrass theorem proves that any continuous
    function on a closed interval can be a suitable polynomial function. Loosening
    our criteria further, Taylor series and Fourier series themselves offer some universal
    approximation capabilities (within their domains of convergence). The fact that
    universal convergence is fairly common in mathematics provides partial justification
    for the empirical observation that there are many slight variants of fully connected
    networks that seem to share a universal approximation property.
  prefs: []
  type: TYPE_NORMAL
- en: Universal Approximation Doesn’t Mean Universal Learning!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A critical subtlety exists in the universal approximation theorem. The fact
    that a fully connected network can represent any function doesn’t mean that backpropagation
    can learn any function! One of the major limitations of backpropagation is that
    there is no guarantee the fully connected network “converges”; that is, finds
    the best available solution of a learning problem. This critical theoretical gap
    has left generations of computer scientists queasy with neural networks. Even
    today, many academics will prefer to work with alternative algorithms that have
    stronger theoretical guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: Empirical research has yielded many practical tricks that allow backpropagation
    to find good solutions for problems. We will go into many of these tricks in significant
    depth in the remainder of this chapter. For the practicing data scientist, the
    universal approximation theorem isn’t something to take too seriously. It’s reassuring,
    but the art of deep learning lies in mastering the practical hacks that make learning
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Why Deep Networks?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A subtlety in the universal approximation theorem is that it in fact holds true
    for fully connected networks with only one fully connected layer. What then is
    the use of “deep” learning with multiple fully connected layers? It turns out
    that this question is still quite controversial in academic and practical circles.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it seems that deeper networks can sometimes learn richer models
    on large datasets. (This is only a rule of thumb, however; every practitioner
    has a bevy of examples where deep fully connected networks don’t do well.) This
    observation has led researchers to hypothesize that deeper networks can represent
    complex functions “more efficiently.” That is, a deeper network may be able to
    learn more complex functions than shallower networks with the same number of neurons.
    For example, the ResNet architecture mentioned briefly in the first chapter, with
    130 layers, seems to outperform its shallower competitors such as AlexNet. In
    general, for a fixed neuron budget, stacking deeper leads to better results.
  prefs: []
  type: TYPE_NORMAL
- en: A number of erroneous “proofs” for this “fact” have been given in the literature,
    but all of them have holes. It seems the question of depth versus width touches
    on profound concepts in complexity theory (which studies the minimal amount of
    resources required to solve given computational problems). At present day, it
    looks like theoretically demonstrating (or disproving) the superiority of deep
    networks is far outside the ability of our mathematicians.
  prefs: []
  type: TYPE_NORMAL
- en: Training Fully Connected Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned previously, the theory of fully connected networks falls short
    of practice. In this section, we will introduce you to a number of empirical observations
    about fully connected networks that aid practitioners. We strongly encourage you
    to use our code (introduced later in the chapter) to check our claims for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Learnable Representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way of thinking about fully connected networks is that each fully connected
    layer effects a transformation of the feature space in which the problem resides.
    The idea of transforming the representation of a problem to render it more malleable
    is a very old one in engineering and physics. It follows that deep learning methods
    are sometimes called “representation learning.” (An interesting factoid is that
    one of the major conferences for deep learning is called the “International Conference
    on Learning Representations.”)
  prefs: []
  type: TYPE_NORMAL
- en: Generations of analysts have used Fourier transforms, Legendre transforms, Laplace
    transforms, and so on in order to simplify complicated equations and functions
    to forms more suitable for handwritten analysis. One way of thinking about deep
    learning networks is that they effect a data-driven transform suited to the problem
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ability to perform problem-specific transformations can be immensely powerful.
    Standard transformation techniques couldn’t solve problems of image or speech
    analysis, while deep networks are capable of solving these problems with relative
    ease due to the inherent flexibility of the learned representations. This flexibility
    comes with a price: the transformations learned by deep architectures tend to
    be much less general than mathematical transforms such as the Fourier transform.
    Nonetheless, having deep transforms in an analytic toolkit can be a powerful problem-solving
    tool.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s a reasonable argument that deep learning is simply the first representation
    learning method that works. In the future, there may well be alternative representation
    learning methods that supplant deep learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Activations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We previously introduced the nonlinear function <math alttext="sigma"><mi>σ</mi></math>
    as the sigmoidal function. While the sigmoidal is the classical nonlinearity in
    fully connected networks, in recent years researchers have found that other activations,
    notably the rectified linear activation (commonly abbreviated ReLU or relu) <math
    alttext="sigma left-parenthesis x right-parenthesis equals max left-parenthesis
    x comma 0 right-parenthesis"><mrow><mi>σ</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>=</mo> <mi>max</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math>
    work better than the sigmoidal unit. This empirical observation may be due to
    the *vanishing gradient* problem in deep networks. For the sigmoidal function,
    the slope is zero for almost all values of its input. As a result, for deeper
    networks, the gradient would tend to zero. For the ReLU function, the slope is
    nonzero for a much greater part of input space, allowing nonzero gradients to
    propagate. [Figure 4-7](#ch4-activation) illustrates sigmoidal and ReLU activations
    side by side.
  prefs: []
  type: TYPE_NORMAL
- en: '![activation-functions.png](assets/tfdl_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Sigmoidal and ReLU activation functions.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fully Connected Networks Memorize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the striking aspects about fully connected networks is that they tend
    to memorize training data entirely given enough time. As a result, training a
    fully connected network to “convergence” isn’t really a meaningful metric. The
    network will keep training and learning as long as the user is willing to wait.
  prefs: []
  type: TYPE_NORMAL
- en: For large enough networks, it is quite common for training loss to trend all
    the way to zero. This empirical observation is one the most practical demonstrations
    of the universal approximation capabilities of fully connected networks. Note
    however, that training loss trending to zero does not mean that the network has
    learned a more powerful model. It’s rather likely that the model has started to
    memorize peculiarities of the training set that aren’t applicable to any other
    datapoints.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth digging into what we mean by peculiarities here. One of the interesting
    properties of high-dimensional statistics is that given a large enough dataset,
    there will be plenty of spurious correlations and patterns available for the picking.
    In practice, fully connected networks are entirely capable of finding and utilizing
    these spurious correlations. Controlling networks and preventing them from misbehaving
    in this fashion is critical for modeling success.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regularization is the general statistical term for a mathematical operation
    that limits memorization while promoting generalizable learning. There are many
    different types of regularization available, which we will cover in the next few
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Not Your Statistician’s Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization has a long history in the statistical literature, with entire
    sheaves of papers written on the topic. Unfortunately, only some of this classical
    analysis carries over to deep networks. The linear models used widely in statistics
    can behave very differently from deep networks, and many of the intuitions built
    in that setting can be downright wrong for deep networks.
  prefs: []
  type: TYPE_NORMAL
- en: The first rule for working with deep networks, especially for readers with prior
    statistical modeling experience, is to trust empirical results over past intuition.
    Don’t assume that past knowledge about techniques such as LASSO has much meaning
    for modeling deep architectures. Rather, set up an experiment to methodically
    test your proposed idea. We will return at greater depth to this methodical experimentation
    process in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dropout is a form of regularization that randomly drops some proportion of the
    nodes that feed into a fully connected layer ([Figure 4-8](#ch4-dropout)). Here,
    dropping a node means that its contribution to the corresponding activation function
    is set to 0\. Since there is no activation contribution, the gradients for dropped
    nodes drop to zero as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![dropout.png](assets/tfdl_0408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. Dropout randomly drops neurons from a network while training. Empirically,
    this technique often provides powerful regularization for network training.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The nodes to be dropped are chosen at random during each step of gradient descent.
    The underlying design principle is that the network will be forced to avoid “co-adaptation.”
    Briefly, we will explain what co-adaptation is and how it arises in non-regularized
    deep architectures. Suppose that one neuron in a deep network has learned a useful
    representation. Then other neurons deeper in the network will rapidly learn to
    depend on that particular neuron for information. This process will render the
    network brittle since the network will depend excessively on the features learned
    by that neuron, which might represent a quirk of the dataset, instead of learning
    a general rule.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout prevents this type of co-adaptation because it will no longer be possible
    to depend on the presence of single powerful neurons (since that neuron might
    drop randomly during training). As a result, other neurons will be forced to “pick
    up the slack” and learn useful representations as well. The theoretical argument
    follows that this process should result in stronger learned models.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, dropout has a pair of empirical effects. First, it prevents the
    network from memorizing the training data; with dropout, training loss will no
    longer tend rapidly toward 0, even for very large deep networks. Next, dropout
    tends to slightly boost the predictive power of the model on new data. This effect
    often holds for a wide range of datasets, part of the reason that dropout is recognized
    as a powerful invention, and not just a simple statistical hack.
  prefs: []
  type: TYPE_NORMAL
- en: You should note that dropout should be turned off when making predictions. Forgetting
    to turn off dropout can cause predictions to be much noisier and less useful than
    they would be otherwise. We discuss how to handle dropout for training and predictions
    correctly later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How Can Big Networks Not Overfit?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most jarring points for classically trained statisticians is that
    deep networks may routinely have more internal degrees of freedom than are present
    in the training data. In classical statistics, the presence of these extra degrees
    of freedom would render the model useless, since there will no longer exist a
    guarantee that the model learned is “real” in the classical sense.
  prefs: []
  type: TYPE_NORMAL
- en: How then can a deep network with millions of parameters learn meaningful results
    on datasets with only thousands of exemplars? Dropout can make a big difference
    here and prevent brute memorization. But, there’s also a deeper unexplained mystery
    in that deep networks will tend to learn useful facts even in the absence of dropout.
    This tendency might be due to some quirk of backpropagation or fully connected
    network structure that we don’t yet understand.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned, fully connected networks tend to memorize whatever is put before
    them. As a result, it’s often useful in practice to track the performance of the
    network on a held-out “validation” set and stop the network when performance on
    this validation set starts to go down. This simple technique is known as early
    stopping.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, early stopping can be quite tricky to implement. As you will see,
    loss curves for deep networks can vary quite a bit in the course of normal training.
    Devising a rule that separates healthy variation from a marked downward trend
    can take significant effort. In practice, many practitioners just train models
    with differing (fixed) numbers of epochs, and choose the model that does best
    on the validation set. [Figure 4-9](#ch4-traintest) illustrates how training and
    test set accuracy typically change as training proceeds.
  prefs: []
  type: TYPE_NORMAL
- en: '![earlystopping.png](assets/tfdl_0409.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. Model accuracy on training and test sets as training proceeds.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We will dig more into proper methods for working with validation sets in the
    following chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Weight regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A classical regularization technique drawn from the statistical literature penalizes
    learned weights that grow large. Following notation from the previous chapter,
    let <math alttext="script upper L left-parenthesis x comma y right-parenthesis"><mrow><mi>ℒ</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math> denote the
    loss function for a particular model and let <math><mi>θ</mi></math> denote the
    learnable parameters of this model. Then the regularized loss function is defined
    by
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msup><mi>ℒ</mi> <mo>'</mo></msup> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>ℒ</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>α</mi> <mrow><mo>∥</mo>
    <mi>θ</mi> <mo>∥</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="parallel-to theta parallel-to"><mrow><mo>∥</mo> <mi>θ</mi>
    <mo>∥</mo></mrow></math> is the weight penalty and <math alttext="alpha"><mi>α</mi></math>
    is a tunable parameter. The two common choices for penalty are the *L*¹ and *L*²
    penalties
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msubsup><mi>θ</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></msqrt></mrow></math><math
    display="block"><mrow><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow> <mn>1</mn></msub>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mrow><mo>|</mo> <msub><mi>θ</mi> <mi>i</mi></msub> <mo>|</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="parallel-to theta parallel-to"><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub></math> and <math alttext="parallel-to theta parallel-to"><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow>
    <mn>1</mn></msub></math> denote the *L*¹ and *L*² penalties, respectively. From
    personal experience, these penalties tend to be less useful for deep models than
    dropout and early stopping. Some practitioners still make use of weight regularization,
    so it’s worth understanding how to apply these penalties when tuning deep networks.
  prefs: []
  type: TYPE_NORMAL
- en: Training Fully Connected Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training fully connected networks requires a few tricks beyond those you have
    seen so far in this book. First, unlike in the previous chapters, we will train
    models on larger datasets. For these datasets, we will show you how to use minibatches
    to speed up gradient descent. Second, we will return to the topic of tuning learning
    rates.
  prefs: []
  type: TYPE_NORMAL
- en: Minibatching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For large datasets (which may not even fit in memory), it isn’t feasible to
    compute gradients on the full dataset at each step. Rather, practitioners often
    select a small chunk of data (typically 50–500 datapoints) and compute the gradient
    on these datapoints. This small chunk of data is traditionally called a minibatch.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, minibatching seems to help convergence since more gradient descent
    steps can be taken with the same amount of compute. The correct size for a minibatch
    is an empirical question often set with hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The learning rate dictates the amount of importance to give to each gradient
    descent step. Setting a correct learning rate can be tricky. Many beginning deep-learners
    set learning rates incorrectly and are surprised to find that their models don’t
    learn or start returning NaNs. This situation has improved significantly with
    the development of methods such as ADAM that simplify choice of learning rate
    significantly, but it’s worth tweaking the learning rate if models aren’t learning
    anything.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show you how to implement a fully connected network
    in TensorFlow. We won’t need to introduce many new TensorFlow primitives in this
    section since we have already covered most of the required basics.
  prefs: []
  type: TYPE_NORMAL
- en: Installing DeepChem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will use the DeepChem machine learning toolchain for your
    experiments (full disclosure: one of the authors was the creator of DeepChem).
    [Detailed installation directions](https://deepchem.io) for DeepChem can be found
    online, but briefly the Anaconda installation via the `conda` tool will likely
    be most convenient.'
  prefs: []
  type: TYPE_NORMAL
- en: Tox21 Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our modeling case study, we will use a chemical dataset. Toxicologists are
    very interested in the task of using machine learning to predict whether a given
    compound will be toxic or not. This task is extremely complicated, since today’s
    science has only a limited understanding of the metabolic processes that happen
    in a human body. However, biologists and chemists have worked out a limited set
    of experiments that provide indications of toxicity. If a compound is a “hit”
    in one of these experiments, it will likely be toxic for a human to ingest. However,
    these experiments are often costly to run, so data scientists aim to build machine
    learning models that can predict the outcomes of these experiments on new molecules.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important toxicological dataset collections is called Tox21\.
    It was released by the NIH and EPA as part of a data science initiative and was
    used as the dataset in a model building challenge. The winner of this challenge
    used multitask fully connected networks (a variant of fully connected networks
    where each network predicts multiple quantities for each datapoint). We will analyze
    one of the datasets from the Tox21 collection. This dataset consists of a set
    of 10,000 molecules tested for interaction with the androgen receptor. The data
    science challenge is to predict whether new molecules will interact with the androgen
    receptor.
  prefs: []
  type: TYPE_NORMAL
- en: Processing this dataset can be tricky, so we will make use of the MoleculeNet
    dataset collection curated as part of DeepChem. Each molecule in Tox21 is processed
    into a bit-vector of length 1024 by DeepChem. Loading the dataset is then a few
    simple calls into DeepChem ([Example 4-1](#ch4-tox21load)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-1\. Load the Tox21 dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here the `X` variables hold processed feature vectors, `y` holds labels, and
    `w` holds example weights. The labels are binary 1/0 for compounds that interact
    or don’t interact with the androgen receptor. Tox21 holds *imbalanced* datasets,
    where there are far fewer positive examples than negative examples. `w` holds
    recommended per-example weights that give more emphasis to positive examples (increasing
    the importance of rare examples is a common technique for handling imbalanced
    datasets). We won’t use these weights during training for simplicity. All of these
    variables are NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Tox21 has more datasets than we will analyze here, so we need to remove the
    labels associated with these extra datasets ([Example 4-2](#ch4-tox21trim)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-2\. Remove extra datasets from Tox21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Accepting Minibatches of Placeholders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapters, we created placeholders that accepted arguments of
    fixed size. When dealing with minibatched data, it is often convenient to be able
    to feed batches of variable size. Suppose that a dataset has 947 elements. Then
    with a minibatch size of 50, the last batch will have 47 elements. This would
    cause the code in [Chapter 3](ch03.html#linear_and_logistic_regression_with_tensorflow)
    to crash. Luckily, TensorFlow has a simple fix to the situation: using `None`
    as a dimensional argument to a placeholder allows the placeholder to accept tensors
    with arbitrary size in that dimension ([Example 4-3](#ch4-tox21place)).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-3\. Defining placeholders that accept minibatches of different sizes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note `d` is 1024, the dimensionality of our feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Hidden Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code to implement a hidden layer is very similar to code we’ve seen in the
    last chapter for implementing logistic regression, as shown in [Example 4-4](#ch4-tox21hidden).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-4\. Defining a hidden layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We use a `tf.name_scope` to group together introduced variables. Note that we
    use the matricial form of the fully connected layer. We use the form *xW* instead
    of *Wx* in order to deal more conveniently with a minibatch of input at a time.
    (As an exercise, try working out the dimensions involved to see why this is so.)
    Finally, we apply the ReLU nonlinearity with the built-in `tf.nn.relu` activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of the code for the fully connected layer is quite similar to
    that used for the logistic regression in the previous chapter. For completeness,
    we display the full code used to specify the network in [Example 4-5](#ch4-tox21fcnet).
    As a quick reminder, the full code for all models covered is available in the
    GitHub repo associated with this book. We strongly encourage you to try running
    the code for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-5\. Defining the fully connected architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Adding Dropout to a Hidden Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow takes care of implementing dropout for us in the built-in primitive
    `tf.nn.dropout(x, keep_prob)`, where `keep_prob` is the probability that any given
    node is kept. Recall from our earlier discussion that we want to turn on dropout
    when training and turn off dropout when making predictions. To handle this correctly,
    we will introduce a new placeholder for `keep_prob`, as shown in [Example 4-6](#ch4-tox21keep).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-6\. Add a placeholder for dropout probability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: During training, we pass in the desired value, often 0.5, but at test time we
    set `keep_prob` to 1.0 since we want predictions made with all learned nodes.
    With this setup, adding dropout to the fully connected network specified in the
    previous section is simply a single extra line of code ([Example 4-7](#ch4-tox21drophidden)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-7\. Defining a hidden layer with dropout
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Implementing Minibatching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To implement minibatching, we need to pull out a minibatch’s worth of data each
    time we call `sess.run`. Luckily for us, our features and labels are already in
    NumPy arrays, and we can make use of NumPy’s convenient syntax for slicing portions
    of arrays ([Example 4-8](#ch4-tox21minibatch)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-8\. Training on minibatches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating Model Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate model accuracy, standard practice requires measuring the accuracy
    of the model on data not used for training (namely the validation set). However,
    the fact that the data is imbalanced makes this tricky. The classification accuracy
    metric we used in the previous chapter simply measures the fraction of datapoints
    that were labeled correctly. However, 95% of data in our dataset is labeled 0
    and only 5% are labeled 1\. As a result the all-0 model (which labels everything
    negative) would achieve 95% accuracy! This isn’t what we want.
  prefs: []
  type: TYPE_NORMAL
- en: A better choice would be to increase the weights of positive examples so that
    they count for more. For this purpose, we use the recommended per-example weights
    from MoleculeNet to compute a weighted classification accuracy where positive
    samples are weighted 19 times the weight of negative samples. Under this weighted
    accuracy, the all-0 model would have 50% accuracy, which seems much more reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: ForI computing the weighted accuracy, we use the function `accuracy_score(true,
    pred, sample_weight=given_sample_weight)` from `sklearn.metrics`. This function
    has a keyword argument `sample_weight`, which lets us specify the desired weight
    for each datapoint. We use this function to compute the weighted metric on both
    the training and validation sets ([Example 4-9](#ch4-tox21weightaccuracy)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-9\. Computing a weighted accuracy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'While we could reimplement this function ourselves, sometimes it’s easier (and
    less error prone) to use standard functions from the Python data science infrastructure.
    Learning about this infrastructure and available functions is part of being a
    practicing data scientist. Now, we can train the model (for 10 epochs in the default
    setting) and gauge its accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 5](ch05.html#hyperparameter_optimization), we will show you methods
    to systematically improve this accuracy and tune our fully connected model more
    carefully.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard to Track Model Convergence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have specified our model, let’s use TensorBoard to inspect the model.
    Let’s first check the graph structure in TensorBoard ([Figure 4-10](#ch4-tensorboardfcnet)).
  prefs: []
  type: TYPE_NORMAL
- en: The graph looks similar to that for logistic regression, with the addition of
    a new hidden layer. Let’s expand the hidden layer to see what’s inside ([Figure 4-11](#ch4-tensorboardfcnetexp)).
  prefs: []
  type: TYPE_NORMAL
- en: '![fcgraph.png](assets/tfdl_0410.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. Visualizing the computation graph for a fully connected network.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![hidden_expand.png](assets/tfdl_0411.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-11\. Visualizing the expanded computation graph for a fully connected
    network.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see how the new trainable variables and the dropout operation are represented
    here. Everything looks to be in the right place. Let’s end now by looking at the
    loss curve over time ([Figure 4-12](#ch4-tensorboardfcnetloss)).
  prefs: []
  type: TYPE_NORMAL
- en: '![fcnet_loss_curve.png](assets/tfdl_0412.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-12\. Visualizing the loss curve for a fully connected network.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The loss curve trends down as we saw in the previous section. But, let’s zoom
    in to see what this loss looks like up close ([Figure 4-13](#ch4-tensorboardfcnetlosszoom)).
  prefs: []
  type: TYPE_NORMAL
- en: '![fcnet_zoomed_loss.png](assets/tfdl_0413.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-13\. Zooming in on a section of the loss curve.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that loss looks much bumpier! This is one of the prices of using minibatch
    training. We no longer have the beautiful, smooth loss curves that we saw in the
    previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve introduced you to fully connected deep networks. We delved
    into the mathematical theory of these networks, and explored the concept of “universal
    approximation,” which partially explains the learning power of fully connected
    networks. We ended with a case study, where you trained a deep fully connected
    architecture on the Tox21 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we haven’t yet shown you how to tune the fully connected network
    to achieve good predictive performance. In [Chapter 5](ch05.html#hyperparameter_optimization),
    we will discuss “hyperparameter optimization,” the process of tuning network parameters,
    and have you tune the parameters of the Tox21 network introduced in this chapter.
  prefs: []
  type: TYPE_NORMAL
