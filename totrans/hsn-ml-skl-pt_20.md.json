["```py\nimport torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nencoder = nn.Linear(3, 2)\ndecoder = nn.Linear(2, 3)\nautoencoder = nn.Sequential(encoder, decoder).to(device)\n```", "```py\nfrom torch.utils.data import DataLoader, TensorDataset\n\nX_train = [...]  # generate a 3D dataset, like in Chapter 7\ntrain_set = TensorDataset(X_train, X_train)  # the inputs are also the targets\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n```", "```py\nimport torchmetrics\n\noptimizer = torch.optim.NAdam(autoencoder.parameters(), lr=0.2)\nmse = nn.MSELoss()\nrmse = torchmetrics.MeanSquaredError(squared=False).to(device)\ntrain(autoencoder, optimizer, mse, train_loader, n_epochs=20)\n```", "```py\ncodings = encoder(X_train.to(device))\n```", "```py\nstacked_encoder = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(1 * 28 * 28, 128), nn.ReLU(),\n    nn.Linear(128, 32), nn.ReLU(),\n)\nstacked_decoder = nn.Sequential(\n    nn.Linear(32, 128), nn.ReLU(),\n    nn.Linear(128, 1 * 28 * 28), nn.Sigmoid(),\n    nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))\n)\nstacked_ae = nn.Sequential(stacked_encoder, stacked_decoder).to(device)\n```", "```py\nimport matplotlib.pyplot as plt\n\ndef plot_image(image):\n    plt.imshow(image.permute(1, 2, 0).cpu(), cmap=\"binary\")\n    plt.axis(\"off\")\n\ndef plot_reconstructions(model, images, n_images=5):\n    images = images[:n_images]\n    with torch.no_grad():\n        y_pred = model(images.to(device))\n\n    fig = plt.figure(figsize=(len(images) * 1.5, 3))\n    for idx in range(len(images)):\n        plt.subplot(2, len(images), 1 + idx)\n        plot_image(images[idx])\n        plt.subplot(2, len(images), 1 + len(images) + idx)\n        plot_image(y_pred[idx])\n\nX_valid = torch.stack([x for x, _ in valid_data])\nplot_reconstructions(stacked_ae, X_valid)\nplt.show()\n```", "```py\nfrom sklearn.manifold import TSNE\n\nwith torch.no_grad():\n    X_valid_compressed = stacked_encoder(X_valid.to(device))\n\ntsne = TSNE(init=\"pca\", learning_rate=\"auto\", random_state=42)\nX_valid_2D = tsne.fit_transform(X_valid_compressed.cpu())\n```", "```py\nplt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\nplt.show()\n```", "```py\nimport torch.nn.functional as F\n\nclass TiedAutoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.enc1 = nn.Linear(1 * 28 * 28, 128)\n        self.enc2 = nn.Linear(128, 32)\n        self.dec1_bias = nn.Parameter(torch.zeros(128))\n        self.dec2_bias = nn.Parameter(torch.zeros(1 * 28 * 28))\n\n    def encode(self, X):\n        Z = X.view(-1, 1 * 28 * 28)  # flatten\n        Z = F.relu(self.enc1(Z))\n        return F.relu(self.enc2(Z))\n\n    def decode(self, X):\n        Z = F.relu(F.linear(X, self.enc2.weight.t(), self.dec1_bias))\n        Z = F.sigmoid(F.linear(Z, self.enc1.weight.t(), self.dec2_bias))\n        return Z.view(-1, 1, 28, 28)  # unflatten\n\n    def forward(self, X):\n        return self.decode(self.encode(X))\n```", "```py\nconv_encoder = nn.Sequential(\n    nn.Conv2d(1, 16, kernel_size=3, padding=\"same\"), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),  # output: 16 × 14 × 14\n    nn.Conv2d(16, 32, kernel_size=3, padding=\"same\"), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),  # output: 32 × 7 × 7\n    nn.Conv2d(32, 64, kernel_size=3, padding=\"same\"), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),  # output: 64 × 3 × 3\n    nn.Conv2d(64, 32, kernel_size=3, padding=\"same\"), nn.ReLU(),\n    nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten())  # output: 32\n\nconv_decoder = nn.Sequential(\n    nn.Linear(32, 16 * 3 * 3),\n    nn.Unflatten(dim=1, unflattened_size=(16, 3, 3)),\n    nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2), nn.ReLU(),\n    nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1,\n                       output_padding=1), nn.ReLU(),\n    nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1,\n                       output_padding=1), nn.Sigmoid())\n\nconv_ae = nn.Sequential(conv_encoder, conv_decoder).to(device)\n```", "```py\ndropout_encoder = nn.Sequential(\n    nn.Flatten(),\n    nn.Dropout(0.5),\n    nn.Linear(1 * 28 * 28, 128), nn.ReLU(),\n    nn.Linear(128, 128), nn.ReLU(),\n)\ndropout_decoder = nn.Sequential(\n    nn.Linear(128, 128), nn.ReLU(),\n    nn.Linear(128, 1 * 28 * 28), nn.Sigmoid(),\n    nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))\n)\ndropout_ae = nn.Sequential(dropout_encoder, dropout_decoder).to(device)\n```", "```py\nfrom collections import namedtuple\n\nAEOutput = namedtuple(\"AEOutput\", [\"output\", \"codings\"])\n\nclass SparseAutoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(1 * 28 * 28, 128), nn.ReLU(),\n            nn.Linear(128, 256), nn.Sigmoid())\n        self.decoder = nn.Sequential(\n            nn.Linear(256, 128), nn.ReLU(),\n            nn.Linear(128, 1 * 28 * 28), nn.Sigmoid(),\n            nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)))\n\n    def forward(self, X):\n        codings = self.encoder(X)\n        output = self.decoder(codings)\n        return AEOutput(output, codings)\n```", "```py\ndef mse_plus_sparsity_loss(y_pred, y_target, target_sparsity=0.1,\n                           kl_weight=1e-3, eps=1e-8):\n    p = torch.tensor(target_sparsity, device=y_pred.codings.device)\n    q = torch.clamp(y_pred.codings.mean(dim=0), eps, 1 - eps)  # actual sparsity\n    kl_div = p * torch.log(p / q) + (1 - p) * torch.log((1 - p) / (1 - q))\n    return mse(y_pred.output, y_target) + kl_weight * kl_div.sum()\n```", "```py\ntorch.manual_seed(42)\nsparse_ae = SparseAutoencoder().to(device)\noptimizer = torch.optim.NAdam(sparse_ae.parameters(), lr=0.002)\ntrain(sparse_ae, optimizer, mse_plus_sparsity_loss, train_loader, n_epochs=10)\n```", "```py\nVAEOutput = namedtuple(\"VAEOutput\",\n                       [\"output\", \"codings_mean\", \"codings_logvar\"])\n\nclass VAE(nn.Module):\n    def __init__(self, codings_dim=32):\n        super(VAE, self).__init__()\n        self.codings_dim = codings_dim\n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(1 * 28 * 28, 128), nn.ReLU(),\n            nn.Linear(128, 2 * codings_dim))  # output both the mean and logvar\n        self.decoder = nn.Sequential(\n            nn.Linear(codings_dim, 128), nn.ReLU(),\n            nn.Linear(128, 1 * 28 * 28), nn.Sigmoid(),\n            nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)))\n\n    def encode(self, X):\n        return self.encoder(X).chunk(2, dim=-1)  # returns (mean, logvar)\n\n    def sample_codings(self, codings_mean, codings_logvar):\n        codings_std = torch.exp(0.5 * codings_logvar)\n        noise = torch.randn_like(codings_std)\n        return codings_mean + noise * codings_std\n\n    def decode(self, Z):\n        return self.decoder(Z)\n\n    def forward(self, X):\n        codings_mean, codings_logvar = self.encode(X)\n        codings = self.sample_codings(codings_mean, codings_logvar)\n        output = self.decode(codings)\n        return VAEOutput(output, codings_mean, codings_logvar)\n```", "```py\ndef vae_loss(y_pred, y_target, kl_weight=1.0):\n    output, mean, logvar = y_pred\n    kl_div = -0.5 * torch.sum(1 + logvar - logvar.exp() - mean.square(), dim=-1)\n    return F.mse_loss(output, y_target) + kl_weight * kl_div.mean() / 784\n```", "```py\ntorch.manual_seed(42)\nvae = VAE().to(device)\noptimizer = torch.optim.NAdam(vae.parameters(), lr=1e-3)\ntrain(vae, optimizer, vae_loss, train_loader, n_epochs=20)\n```", "```py\ntorch.manual_seed(42)\nvae.eval()\ncodings = torch.randn(3 * 7, vae.codings_dim, device=device)\nwith torch.no_grad():\n    images = vae.decode(codings)\n```", "```py\ncodings = torch.randn(2, vae.codings_dim)  # start and end codings\nn_images = 7\nweights = torch.linspace(0, 1, n_images).view(n_images, 1)\ncodings = torch.lerp(codings[0], codings[1], weights)  # linear interpolation\nwith torch.no_grad():\n    images = vae.decode(codings.to(device))\n```", "```py\nDiscreteVAEOutput = namedtuple(\"DiscreteVAEOutput\",\n                               [\"output\", \"logits\", \"codings_prob\"])\n\nclass DiscreteVAE(nn.Module):\n    def __init__(self, coding_length=32, n_codes=16, temperature=1.0):\n        super().__init__()\n        self.coding_length = coding_length\n        self.n_codes = n_codes\n        self.temperature = temperature\n        self.encoder = nn.Sequential([...])  # outputs [coding_length, n_codes]\n        self.decoder = nn.Sequential([...])  # outputs [1, 28, 28]\n\n    def forward(self, X):\n        logits = self.encoder(X)\n        codings_prob = F.gumbel_softmax(logits, tau=self.temperature, hard=True)\n        output = self.decoder(codings_prob)\n        return DiscreteVAEOutput(output, logits, codings_prob)\n```", "```py\ndef d_vae_loss(y_pred, y_target, kl_weight=1.0):\n    output, logits, _ = y_pred\n    codings_prob = F.softmax(logits, -1)\n    k = logits.new_tensor(logits.size(-1))  # same device and dtype as logits\n    kl_div = (codings_prob * (codings_prob.log() + k.log())).sum(dim=(1, 2))\n    return F.mse_loss(output, y_target) + kl_weight * kl_div.mean() / 784\n```", "```py\nmodel.temperature = 1 - 0.9 * epoch / n_epochs\n```", "```py\ncodings = torch.randint(0, d_vae.n_codes,  # from 0 to k – 1\n                        (n_images, d_vae.coding_length), device=device)\ncodings_prob = F.one_hot(codings, num_classes=d_vae.n_codes).float()\nwith torch.no_grad():\n    images = d_vae.decoder(codings_prob)\n```", "```py\ncodings_dim = 32\ngenerator = nn.Sequential(\n    nn.Linear(codings_dim, 128), nn.ReLU(),\n    nn.Linear(128, 256), nn.ReLU(),\n    nn.Linear(256, 1 * 28 * 28), nn.Sigmoid(),\n    nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))).to(device)\ndiscriminator = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(1 * 28 * 28, 256), nn.ReLU(),\n    nn.Linear(256, 128), nn.ReLU(),\n    nn.Linear(128, 1), nn.Sigmoid()).to(device)\n```", "```py\ndef train_gan(generator, discriminator, train_loader, codings_dim, n_epochs=20,\n              g_lr=1e-3, d_lr=5e-4):\n    criterion = nn.BCELoss()\n    generator_opt = torch.optim.NAdam(generator.parameters(), lr=g_lr)\n    discriminator_opt = torch.optim.NAdam(discriminator.parameters(), lr=d_lr)\n    for epoch in range(n_epochs):\n        for real_images, _ in train_loader:\n            real_images = real_images.to(device)\n            pred_real = discriminator(real_images)\n            batch_size = real_images.size(0)\n            ones = torch.ones(batch_size, 1, device=device)\n            real_loss = criterion(pred_real, ones)\n            codings = torch.randn(batch_size, codings_dim, device=device)\n            fake_images = generator(codings).detach()\n            pred_fake = discriminator(fake_images)\n            zeros = torch.zeros(batch_size, 1, device=device)\n            fake_loss = criterion(pred_fake, zeros)\n            discriminator_loss = real_loss + fake_loss\n            discriminator_opt.zero_grad()\n            discriminator_loss.backward()\n            discriminator_opt.step()\n\n            codings = torch.randn(batch_size, codings_dim, device=device)\n            fake_images = generator(codings)\n            for p in discriminator.parameters():\n                p.requires_grad = False\n            pred_fake = discriminator(fake_images)\n            generator_loss = criterion(pred_fake, ones)\n            generator_opt.zero_grad()\n            generator_loss.backward()\n            generator_opt.step()\n            for p in discriminator.parameters():\n                p.requires_grad = True\n```", "```py\ngenerator.eval()\ncodings = torch.randn(n_images, codings_dim, device=device)\nwith torch.no_grad():\n    generated_images = generator(codings)\n```", "```py\ndef variance_schedule(T, s=0.008, max_beta=0.999):\n    t = torch.linspace(0, T, T + 1)\n    f = torch.cos((t / T + s) / (1 + s) * torch.pi / 2) ** 2\n    alpha_bars = f / f[0]\n    betas = (1 - (f[1:] / f[:-1])).clamp(max=max_beta)\n    betas = torch.cat([torch.zeros(1), betas])  # for easier indexing\n    alphas = 1 - betas\n    return alphas, betas, alpha_bars\n\nT = 4000\nalphas, betas, alpha_bars = variance_schedule(T)\n```", "```py\ndef forward_diffusion(x0, t):\n    eps = torch.randn_like(x0)  # this unscaled noise will be the target\n    xt = alpha_bars[t].sqrt() * x0 + (1 - alpha_bars[t]).sqrt() * eps\n    return xt, eps\n```", "```py\nclass DiffusionSample(namedtuple(\"DiffusionSampleBase\", [\"xt\", \"t\"])):\n    def to(self, device):\n        return DiffusionSample(self.xt.to(device), self.t.to(device))\n```", "```py\nclass DiffusionDataset:\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __getitem__(self, i):\n        x0, _ = self.dataset[i]\n        x0 = (x0 * 2) - 1  # scale from –1 to +1\n        t = torch.randint(1, T + 1, size=[1])\n        xt, eps = forward_diffusion(x0, t)\n        return DiffusionSample(xt, t), eps\n\n    def __len__(self):\n        return len(self.dataset)\n\ntrain_set = DiffusionDataset(train_data)  # wrap Fashion MNIST\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n```", "```py\nclass DiffusionModel(nn.Module):  # see the notebook for full details\n    def __init__(self, T=T, embed_dim=64):\n        [...]  # create all the required modules to build the U-Net\n\n    def forward(self, sample):\n        [...]  # process the sample and predict the noise for each image\n```", "```py\ndiffusion_model = DiffusionModel().to(device)\nhuber = nn.HuberLoss()\noptimizer = torch.optim.NAdam(diffusion_model.parameters(), lr=3e-3)\ntrain(diffusion_model, optimizer, huber, train_loader, n_epochs=20)\n```", "```py\ndef generate_ddim(model, batch_size=32, num_steps=50, eta=0.85):\n    model.eval()\n    with torch.no_grad():\n        xt = torch.randn([batch_size, 1, 28, 28], device=device)\n        times = torch.linspace(T - 1, 0, steps=num_steps + 1).long().tolist()\n        for t, t_prev in zip(times[:-1], times[1:]):\n            t_batch = torch.full((batch_size, 1), t, device=device)\n            sample = DiffusionSample(xt, t_batch)\n            eps_pred = model(sample)\n            x0 = ((xt - (1 - alpha_bars[t]).sqrt() * eps_pred)\n                  / (alpha_bars[t].sqrt()))\n            abar_t_prev = alpha_bars[t_prev]\n            variance = eta * (1 - abar_t_prev) / (1 - alpha_bars[t]) * betas[t]\n            sigma_t = variance.sqrt()\n            pred_dir = (1 - abar_t_prev - sigma_t**2).sqrt() * eps_pred\n            noise = torch.randn_like(xt)\n            xt = abar_t_prev.sqrt() * x0 + pred_dir + sigma_t * noise\n\n        return torch.clamp((xt + 1) / 2, 0, 1)  # from [–1, 1] range to [0, 1]\n\nX_gen_ddim = generate_ddim(diffusion_model, num_steps=500)\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\n\npipe = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/sd-turbo\", variant=\"fp16\", dtype=torch.float16)\npipe.to(device)\nprompt = \"A closeup photo of an orangutan reading a book\"\ntorch.manual_seed(26)\nimage = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n```"]