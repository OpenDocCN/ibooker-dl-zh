- en: 3 The OpenAI Python library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Installing the OpenAI library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invoking GPT models using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the last chapter, we used GPT models via the OpenAI web interface. This works
    well as long as we’re just trying to have a conversation or classify and summarize
    single reviews. However, imagine trying to classify hundreds of reviews. In that
    case, using the web interface manually for each review becomes very tedious (to
    say the least). Also, perhaps we want to use a language model in combination with
    other tools. For instance, we might want to use GPT models to translate questions
    to formal queries and then seamlessly execute those queries in the corresponding
    tool (without having to manually copy queries back and forth between different
    interfaces). In all these scenarios, we need a different interface.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll discuss a Python library from OpenAI that lets you call
    OpenAI’s language models directly from Python. This enables you to integrate calls
    to language models as a subfunction in your code. We will be using this library
    in most chapters of the book. Therefore, it makes sense to at least skim this
    chapter before proceeding to the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Although the current chapter focuses on OpenAI’s Python library, the libraries
    offered by other providers of language models (including Anthropic, Cohere, and
    Google) are similar.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s make sure we have the right environment for OpenAI’s Python library.
    We will use the Python programming language, so make sure Python is installed.
    To do so, open a terminal, and enter the following command (this command should
    work for Linux, macOS, and Windows terminals):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If this command returns an error message, try replacing `python` with `python3`
    in the command and running it again. If Python is installed on your system, you
    should see a version number in reply (e.g., “Python 3.10.13”). If not, you will
    get an error message. For the following examples, you will need at least Python
    3.9 (or a later version). If Python is not installed on your system, or if your
    version is below the required one, visit [www.python.org](http://www.python.org),
    click Downloads, and follow the instructions to install Python. You may also want
    to install an integrated development environment (IDE). PyDev ([www.pydev.org](http://www.pydev.org))
    and PyCharm ([www.jetbrains.com/pycharm](http://www.jetbrains.com/pycharm)) are
    two of the many IDEs available for Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with Python, you will need pip, a package-management system used to install
    Python packages (the OpenAI library comes in the form of such a package). For
    recent Python versions (which you will need in any case), this program is already
    installed by default. Nevertheless, it can’t hurt to make sure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, you should see a version number if everything is installed properly.
    Let’s make sure pip is up to date. The following command should work on Linux,
    macOS, and Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! Your system is ready to install the OpenAI Python client.
  prefs: []
  type: TYPE_NORMAL
- en: What if it doesn’t work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Don’t panic! If any of the previously mentioned steps fail, you may not be able
    to execute the following code on your local machine. However, as long as you have
    web access, you can use a cloud platform instead. For instance, the Google Colab
    platform, accessible at [https://colab.research.google.com](https://colab.research.google.com),
    enables you to create notebooks that can execute all of the following code samples.
    Figure [3.1](#fig__colab) shows the interface after creating a cell installing
    the OpenAI library (upper cell) and the start of a corresponding Python program
    (lower cell). We will discuss library installation and usage in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F01_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 The Google Colab platform can be used to run the following examples.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 3.2 Installing OpenAI’s Python library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Time to start using GPT like a pro! Although the ChatGPT web interface, discussed
    in chapter 2, is useful for conversations and trying out new prompts, it is unsuitable
    for implementing complex data-processing pipelines. For that, OpenAI’s Python
    library is a much better choice, enabling you to invoke language models directly
    from Python. First, let’s install the corresponding library. Enter the following
    command into a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Can I use a different library version?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You might have noticed the reference to a specific version (version 1.29) of
    the OpenAI library. The code presented in this and the following chapters has
    been tested with this version. As the syntax differs slightly across different
    library versions (unless you are willing to adapt the code), install this precise
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Every time we use the OpenAI library, we need to provide a key giving us access
    to the OpenAI models (this is required for billing purposes). If you have not
    yet created an OpenAI account, go to [https://platform.openai.com](https://platform.openai.com),
    click Sign Up, and follow the instructions. If you have an account but are not
    currently logged in, provide your account credentials instead. Make sure to add
    a payment method in the Billing section, and charge it with a couple of dollars.
    After that, if you haven’t done so yet, it is time to generate your secret key.
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).
    You should see the website shown in figure [3.2](#fig__openaikeys).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F02_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 Managing secret keys for accessing the OpenAI API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Click the Create New Secret Key button. The interface will show a text string
    representing the key. Be sure to copy and store that key! You will not be able
    to retrieve the full key again after closing the corresponding window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever we use the Python library, we need to provide our secret key to link
    our requests to the appropriate account. The easiest way to do that is to store
    the secret key in an environment variable named `OPENAI_API_KEY`. OpenAI will
    automatically extract the key from that variable if it exists. The precise command
    used to set environment variables depends on the operating system. For example,
    the following command works for Linux and macOS (replace the three dots with your
    key):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can set the key on a per-invocation basis by prefixing your
    calls to Python with the corresponding assignments. For example, use the following
    command to call the code listing presented in the next section while setting the
    key at the same time (again, substitute your key for the three dots):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, if none of the other options work, you can specify your access key
    directly in your Python code. More precisely, right after importing OpenAI’s Python
    library, we can pass the API access key as a parameter when creating the `client`
    object (which we discuss in more detail later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As before, replace the three dots with your OpenAI access key. The following
    code samples assume that the access key is specified in an environment variable
    and will therefore omit this parameter. If environment variables don’t work for
    you, change the code listings by passing your access key as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Warning Never share your code if it contains your OpenAI access key. Among other
    things, having your key would enable others to invoke OpenAI’s models while making
    you pay for it.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming you have specified your access key in one way or another, we are now
    ready to start calling GPT models using OpenAI’s Python library.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Listing available models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the Python library to retrieve a list of available OpenAI models.
    Listing [3.1](#code__listmodels) shows the corresponding Python code. (You can
    download this and all of the following code listings from the book’s companion
    website.) First, we import the OpenAI library (**1**). Then we create a client
    object, enabling us to access library functions (**2**). Next, we query for all
    available OpenAI models (**3**) and print out the result (**4**).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 Listing available OpenAI models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Imports the OpenAI Python library'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates an OpenAI client'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Gets available OpenAI models'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Prints out the retrieved models'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see a result similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 GPT-4o'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Fine-tuned model version'
  prefs: []
  type: TYPE_NORMAL
- en: Each model is described by an ID (e.g., `GPT-4o` (**1**)). We will use this
    ID to tell OpenAI which model we want to use to process our requests. Besides
    the ID, each model comes with a creation timestamp and information about model
    ownership (`owned_by` field). In most cases, models are owned by OpenAI (marked,
    for example, `system` or `openai-internal`). In some cases, however, models are
    owned by `trummerlab` (**2**), the name of the account used by this book’s author.
    Those models are not publicly accessible but private to the owning account. You
    will not see those models when executing the code using your account. They are
    created by a process called *fine-tuning* from the publicly available base models.
  prefs: []
  type: TYPE_NORMAL
- en: What is fine-tuning?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: By default, language models such as GPT-4o are trained to be versatile, meaning
    they can, in principle, perform any task. But sometimes we don’t want a model
    that is versatile but rather a model that does very well on one specific task.
    Fine-tuning enables us to specialize a model for a task we care about. We discuss
    fine-tuning in more detail in chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Chat completion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Almost all the code in this book uses the same functionality of the OpenAI
    Python library: *chat completion*. With chat completion, your model generates
    a completion for a chat, provided as input. The input can contain various types
    of data, such as text and images. We will exploit those features in the following
    chapters but restrict ourselves to text for the moment. Chat completion is also
    used in the background of OpenAI’s ChatGPT web interface. Given the chat history
    as input (which includes the latest message as well as prior messages, possibly
    containing relevant context), the model generates the most suitable reply.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use chat completion from Python, we first need a format to describe the
    chat history. This is part of the input we’re providing for chat completion. In
    OpenAI’s Python library, chats are represented as a list of messages. Each message
    in turn is represented as a Python dictionary. This Python dictionary specifies
    values for several important properties of the message. At the very least, we
    need to specify two important attributes for each message:'
  prefs: []
  type: TYPE_NORMAL
- en: The `role` attribute, which specifies the source of a message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `content` attribute, which specifies the content of a message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by discussing the `role` attribute. As you know from the last chapter,
    a chat with GPT models is a back-and-forth series of messages, alternating between
    messages written by the user and messages written by the model. Accordingly, we
    can specify the value `user` for the `role` attribute to identify a message as
    written by the user. Alternatively, we can specify the value `assistant` to mark
    a message as generated by the language model. A third possible value for the `role`
    attribute is `system`. Such messages are typically used at the beginning of a
    chat history. They are meant to convey generic guidelines to the model, independent
    of the specific tasks submitted by users. For instance, a typical system message
    could have the content “You are a helpful assistant,” but more specialized versions
    (e.g., “You are an assistant that translates questions about data sets into SQL
    queries”) are also possible. We will not use `system` messages in this book, but
    feel free to experiment and try adding your own system messages to see if they
    influence the model output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `content` attribute specifies the content of a message. In this chapter,
    we will restrict ourselves to text content. In later chapters, we will see how
    language models can be used to process more diverse types of content. In the following
    code samples, we will only need to specify a single message in our chat history.
    This message contains instructions describing a task that the language model should
    solve, as well as relevant context information. For instance, the following chat
    history encourages the model to generate a story for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Message from user'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Task specification'
  prefs: []
  type: TYPE_NORMAL
- en: The list of messages contains only a single message. This message is marked
    as originating from the user (**1**) and describes the previously mentioned task
    in its content (**2**). As a reply, we would expect the model to generate a story
    following the input instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we invoke a model for chat completion? This can be realized with just
    a few lines of Python code. First, we need to import the OpenAI Python library
    (**1**) and create a `client` object (**2**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Imports the OpenAI Python library'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates the OpenAI client'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `client` object for all of the following invocations of the
    language model. The previous code appears in almost all of our code samples. Remember
    that you may need to pass the OpenAI access key manually as a parameter when creating
    the client (unless you specify your access key in an environment variable, which
    is the recommended approach). After creating the `client`, we can issue chat completion
    requests as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Selects a model'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Specifies input messages'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `client.chat.completions.create` function to create a new request.
    The `model` parameter (**1**) specifies the name of the model we want to use for
    completion. In this case, we’re selecting OpenAI’s GPT-4o model, which can process
    multimodal data. We will use this model for most of the code samples in this book.
    Next, we specify the chat history as input via the `messages` parameter (**2**).
    This is the chat history discussed before, instructing the model to generate a
    story.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s put it all together. The following listing (available as listing 2 in
    the chapter 3 section on the book’s companion website) uses GPT-4o to generate
    a story.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Using GPT-4o for chat completion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Imports the OpenAI Python library'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates an OpenAI client'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Invokes chat completion'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Selects a model/'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Specifies input messages/'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the code should produce a result such as the following (your precise
    story may differ due to randomization):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 List of completions'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Termination condition'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Completion message'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Token usage'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss the different components of that result. First, we have a list
    of completion alternatives (**1**) (objects of type `Choice`). In our case, that
    list contains only a single entry. This is the default behavior, although we can
    ask for multiple alternative completions by setting the right configuration parameters
    (discussed in the next section). The `finish_reason` flag (**2**) indicates for
    each completion the reason to stop generating. For instance, this can be due to
    reaching a length limit on generated text. The `stop` value indicates that the
    language model was able to generate complete output (as opposed to reaching a
    length limit). The actual message (**3**) content is abbreviated, and in all likelihood,
    you will see different stories if you invoke the code repeatedly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the completions themselves, the result contains metadata and usage
    statistics (**4**). More precisely, we find values for the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '`completion_tokens`—The number of generated tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_tokens`—The number of tokens in the input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_tokens`—The number of tokens read and generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why would we care about the number of tokens? Because pricing for most OpenAI
    models is proportional to the number of tokens read and generated. For instance,
    at the time of writing, using GPT-4o costs $5 per million tokens read and $15
    per million tokens generated. Note the difference in pricing between tokens read
    and generated. Typically, as in this case, generating tokens is more expensive
    than reading tokens. The pricing depends not only on the number of tokens but
    also on the model used. For example, replacing GPT-4o with the GPT-3.5 Turbo model
    (a slightly less powerful GPT version) cuts costs by a factor of 10\. Before analyzing
    large amounts of data with language models, choose the appropriate model size
    for your task and wallet.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Customizing model behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use various parameters to influence how the model replies to your input.
    These parameters can be specified in addition to the `model` and `messages` parameters
    when invoking the `chat.completions.create` function. In this section, we discuss
    different categories of parameters, classifying parameters by the aspect of model
    behavior they influence.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 Configuring termination conditions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we invoke a model for chat completion, it generates output until a stopping
    condition is met. The two parameters discussed next enable us to configure when
    text generation stops.
  prefs: []
  type: TYPE_NORMAL
- en: The `max_tokens` parameter specifies the maximum number of tokens (i.e., the
    atomic unit at which language models represent text) generated during completion.
    A token corresponds to approximately four characters, and a typical paragraph
    contains around 100 tokens. The maximum admissible value for this parameter is
    determined by the model used. For instance, ada, one of the smallest GPT versions,
    allows up to 2,049 tokens, whereas GPT-4o supports up to 128,000 tokens. Keep
    in mind that the maximum number of tokens supported by the model includes tokens
    read and tokens generated. As `max_tokens` refers only to the number of tokens
    generated, you should not set it higher than the maximum number of tokens supported
    by the model used *minus the number of tokens in the prompt*.
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule, setting a reasonable value for `max_tokens` is almost always
    a good idea. After all, we’re paying for each generated token, and setting a bound
    on the number of tokens enables you to bound monetary fees per model invocation.
  prefs: []
  type: TYPE_NORMAL
- en: In some scenarios, specific text patterns indicate the end of the desired output.
    For instance, when generating code, it can be a string specific to the corresponding
    programming language indicating the end of the program. On the other hand, when
    generating a fairy tale, it can be the string “and they lived happily ever after!”
    In those scenarios, we might want to use the `stop` parameter to configure the
    OpenAI library to stop generating output whenever a specific token sequence appears.
    In some cases, there is only one token sequence indicating termination. In those
    scenarios, we can directly assign the `stop` parameter to the corresponding string
    value. In other scenarios, there are multiple candidate sequences that indicate
    termination. In those cases, we can assign the `stop` parameter to a list of up
    to four sequences. Text generation terminates whenever any of those sequences
    is generated.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can use both of the previously mentioned parameters together.
    In those cases, output generation stops whenever the length limit is reached or
    one of the stop sequences appears (whichever happens first).
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 Configuring output generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The parameters we just discussed enable you to choose when the output terminates.
    But how can you influence the output generated until that point? Here, OpenAI
    offers a few parameters that enable you to bias the way in which GPT models select
    output text.
  prefs: []
  type: TYPE_NORMAL
- en: Several parameters enable you to influence how “repetitive” the generated output
    should be. More precisely, those parameters allow you to influence whether generating
    the same tokens repeatedly is desirable or not.
  prefs: []
  type: TYPE_NORMAL
- en: The `presence_penalty` parameter enables you to penalize chat completions that
    use the same tokens repeatedly. The presence penalty is a value between –2 and
    +2 (with a default value of 0). A positive penalty encourages the model to avoid
    reusing the same tokens. A negative penalty, on the other hand, encourages the
    model to use the same tokens repeatedly. The higher the absolute value, the stronger
    the corresponding effect.
  prefs: []
  type: TYPE_NORMAL
- en: The `frequency_penalty` relates to the prior parameter but enables a more fine-grained
    penalization scheme. The `presence_penalty` parameter is based on the mere *presence*
    of a token. For example, we do not differentiate between a token that appears
    twice and one that appears hundreds of times. The frequency penalty is used as
    a factor, multiplying the number of prior appearances of a token when aggregating
    its score (which is used to determine whether the token should appear next). Hence,
    the more often a token was used before, the less likely it is to appear again.
    Similar to the presence penalty, the `frequency_penalty` parameter takes values
    between –2 and +2 with a default setting of 0\. A positive penalty factor encourages
    GPT models to avoid repeating the same token, whereas a negative value encourages
    repetitions.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we are only interested in one of a limited set of eligible tokens.
    For instance, when classifying text, the set of classes is typically determined
    a priori. If so, let’s tell the model about it! The `logit_bias` parameter allows
    mapping token IDs to a bias factor. A high bias factor encourages the model to
    consider the corresponding token as output. A sufficiently low bias score essentially
    prevents the model from using the token. A sufficiently high score almost guarantees
    that the corresponding token will appear in the output.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `logit_bias` parameter avoids generating useless output in situations
    where we can narrow the set of reasonable tokens. The value for `logit_bias` is
    a Python dictionary that maps token IDs to values between –100 and +100\. Values
    between –1 and +1 are more typical and still give the model room to consider tokens
    with a low value (or to avoid using tokens that are associated with higher values).
    But how do we find the token IDs associated with relevant words? For that, we
    can use the GPT tokenizer tool, available at [https://platform.openai.com/tokenizer?view=bpe](https://platform.openai.com/tokenizer?view=bpe).
    Simply enter the words you want to encourage (or ban), and the associated token
    IDs will be displayed. Note that multiple tokenizer variants are available, associated
    with different models. Select the right tokenizer for your model (because otherwise,
    the token IDs may be incorrect).
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.3 Configuring randomization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How do GPT models select the next output token? At a high level of abstraction,
    we calculate scores for all possible output tokens and then select a token based
    on those scores. Although tokens with higher scores tend to have better chances
    of being selected, we might not always want to select the token with the maximum
    score. For instance, think back to chapter 2, where we were able to regenerate
    replies for the same input, potentially leading to different results. This can
    be useful if the first output does not quite satisfy our requirements. If always
    selecting the tokens with the highest scores, regenerating an answer would be
    unlikely to change the output. Hence, to enable users to get diverse replies,
    we need to introduce a certain degree of randomization when mapping scores to
    output tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, decoupling the output too much from token scores—that is, using too
    much randomization—may lead to useless output (at the extreme, the output no longer
    connects to the input and does not follow our instructions). On the other hand,
    using too little randomization can lead to outputs that are less diverse than
    desired. Choosing the right degree of randomization for a specific scenario can
    take some experimentation. In each case, OpenAI offers multiple parameters that
    enable you to fine-tune how token scores translate to output tokens. We will discuss
    those parameters next.
  prefs: []
  type: TYPE_NORMAL
- en: One of the parameters most commonly used to tune randomization is the `temperature`
    parameter. A higher temperature means more randomization, whereas a lower temperature
    corresponds to less randomization. A low degree of randomization means the token
    with the highest score is very likely to be selected. A very high degree of randomization
    means tokens are (almost) selected with equal probability, independently of the
    scores assigned by the model. The `temperature` parameter enables you to thread
    the needle between those two extremes. Values for this parameter are chosen between
    0 and 2 with a default of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Temperature is one possibility when choosing the degree of randomization. The
    `top_p` parameter is an alternative approach. (It is not recommended that you
    alter both `temperature` and `top_p` in the same invocation of the language model.)
    Based on their scores, we can associate a probability of being “correct” with
    each possible output token. Now imagine that we are sorting those tokens in decreasing
    order of probability. We can reduce the degree of randomization by focusing only
    on the first few tokens: we neglect tokens with lower probability. How many tokens
    should we consider? Instead of fixing the number of eligible tokens directly,
    the `top_p` parameter fixes the *probability mass* of those tokens. In other words,
    we add tokens to the set of eligible tokens in decreasing order of probability.
    Whenever the sum of probability values of all selected tokens (the probability
    mass) exceeds the value of `top_p`, we stop adding tokens. Finally, we pick the
    next output token among those eligible tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: As the `top_p` parameter describes a probability, its values are taken from
    the interval between 0 and 1\. Similar to temperature, choosing a higher value
    leads to more randomization (because even tokens with lower probability become
    eligible).
  prefs: []
  type: TYPE_NORMAL
- en: As soon as we are using a certain degree of randomization, it becomes useful
    to generate multiple answers for the same input prompt. After that, we can choose
    the preferred answer via postprocessing. For instance, assume that we are generating
    multiple SQL queries for the same input prompt. To select the preferred answer,
    we can try executing them on a target database and discard the queries that result
    in a syntax error message. Of course, we can simply call the language model repeatedly
    with the same prompt. However, it is more efficient to call the language model
    once and configure the number of generated replies. The parameter `n` determines
    the number of generated replies. By default, this parameter is set to 1 (i.e.,
    only a single answer is generated). You may choose a higher value to obtain more
    replies. Note that using a higher value for this parameter also increases per-invocation
    costs (because you pay for each token generated, counting tokens across different
    replies).
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.4 Customization example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s try some of the parameters in our code. The following listing prompts
    GPT-4o to write a story, this time using some of the parameters we’ve discussed
    to customize chat completion.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Using GPT-4o for chat completion with custom parameter settings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Limits the output length'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Sets a stopping condition'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets temperature'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Penalizes repetitions'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Adds bias'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, to avoid getting a lengthy story, we set the maximum number of tokens
    to 512 (**1**). This should suffice for a few paragraphs of text. To avoid generating
    more content than necessary, we define a stop sequence as well: “happily ever
    after” (**2**). We do so, hoping that any reasonable story will, of course, end
    with the popular expression “and they lived happily ever after.” This assumption
    is certainly simplifying.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we set the temperature to 1.5 (**3**), allowing a certain degree of randomization.
    Having an overly repetitive story is boring. Therefore, we set a positive presence
    penalty (`presence_penalty`=0.5) (**4**), discouraging GPT-4o from reusing the
    same tokens. Finally, we don’t want a scary story (perhaps we want to read it
    to our kids), so we penalize the word *dragon* (we obtain the corresponding token
    ID as described in section [3.5.2](#sub__ConfigureGeneration)) (**5**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what we get! For instance, in a sample invocation, GPT-4o generates
    the following story:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![chatGpt](../Images/chatGpt.png)***GPT* ****'
  prefs: []
  type: TYPE_NORMAL
- en: '*Once upon a time, in a quaint village nestled between rolling hills and dense
    forests, there lived a girl named Elara. She was known for her curiosity and adventurous
    spirit, traits that often led her to explore the uncharted parts of her village.
    One crisp autumn morning, Elara decided to venture into the Whispering Woods,
    a place shrouded in mystery and legend. The villagers often spoke of the woods
    as a place where magic lingered, but also one that nobody dared to enter beyond
    the edge. Equipped with her trusty satchel and a heart full of excitement, Elara
    set off with the sunrise. As she walked deeper into the woods, the whispers of
    rustling leaves seemed to greet her, and she felt an odd sense of familiarity.
    She wandered through thickets and climbed over fallen logs until she stumbled
    upon a hidden grove, bathed in shimmering light. In the center of the grove stood
    an ancient tree with silvery leaves, its trunk twisted like spiraling tendrils
    of smoke. Elara was mesmerized; it looked unlike any tree she had ever seen. She
    approached it cautiously and noticed a small, ornate chest nestled among its roots.
    Her heart racing, Elara opened the chest and found inside a delicate, golden locket.
    As soon as she touched it, the forest seemed to come alive. The whispers grew
    louder, forming words that she could understand. “Brave heart, you have found
    the Locket of Eternia,” the voice murmured. “It grants the bearer one wish.” Elara’s
    mind raced with possibilities, but she knew what her heart desired most: to bring
    joy and prosperity to her beloved village. Clutching the locket, she whispered
    her wish. In an instant, a warm glow enveloped her, and she felt a surge of energy.
    The grove shimmered momentarily and then returned to its tranquil stillness. Elara
    knew something extraordinary had happened. As she returned home, she noticed subtle
    changes—the crops seemed more vibrant, the animals healthier and the village air
    filled with a sense of contentment. The villagers welcomed her back, their faces
    glowing with happiness. Elara never revealed the secret of the Whispering Woods
    or the Locket of Eternia to anyone. She simply smiled whenever anyone commented
    on the newfound prosperity of the village. She knew that sometimes, the greatest
    magic lies not in what is seen, but in the courage to follow one’s heart. And
    so, Elara’s village thrived, becoming a beacon of joy and harmony. All because
    one brave girl dared to listen to the whispers of the woods. And she lived*'
  prefs: []
  type: TYPE_NORMAL
- en: Happily ever after! It turns out that our stop sequence, the expression “happily
    ever after,” was indeed used at the end of the story (and is therefore omitted
    in the output returned by GPT-4o). Try a few more parameter settings, and see
    how the result changes as a function of the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.5 Further parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have discussed the most important parameters for data-analysis purposes.
    You can use each of them when requesting a completion from OpenAI’s GPT models.
    Note that there are more parameters beyond the ones mentioned in this chapter.
    OpenAI’s API reference documentation ([https://platform.openai.com/docs/api](https://platform.openai.com/docs/api-reference/completions)[-reference/completions](https://platform.openai.com/docs/api-reference/completions))
    describes all parameters in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use OpenAI’s language models via a Python API. Other providers offer
    similar libraries for accessing their models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use OpenAI’s library, create a client object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use OpenAI’s models to complete chats. Chats to complete are specified
    as a list of messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each chat message is characterized by content and a role. Roles can be one of
    `user`, `assistant`, or `system`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain chat completions via the `chat.completions.create` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can configure models using various parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `max_tokens` parameter limits the number of tokens generated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stop` lets you define phrases that stop text generation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can penalize or encourage specific tokens via `logit_bias`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`presence_penalty` penalizes repetitive output.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`frequency_penalty` penalizes repetitive output.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`temperature` chooses the degree of randomization.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`top_p` determines the number of output tokens considered.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`n` chooses the number of generated completions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
