- en: 8 Getting started with deep learning with tabular data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 使用表格数据开始深度学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: An introduction to deep learning with tabular data stacks—low-level frameworks
    and high-level APIs for deep learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用表格数据堆栈的深度学习简介—深度学习的低级框架和高级API
- en: The PyTorch with fastai stack
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch与fastai堆栈
- en: The PyTorch with TabNet stack
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch与TabNet堆栈
- en: The PyTorch with Lightning Flash stack
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch与Lightning Flash堆栈
- en: The stacks we didn’t exercise and why we didn’t exercise them
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有练习的堆栈以及为什么没有练习它们
- en: A comparison of the pros and cons of deep learning with tabular data stacks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习表格数据堆栈的优缺点比较
- en: Up to this point, we have focused on classical machine learning tools and algorithms
    to analyze tabular data. Ranging from traditional regression algorithms to more
    sophisticated gradient boosting techniques, these approaches offer advantages
    in simplicity, transparency, and efficacy. That said, deep learning tools have
    become much easier to access and use, and they also provide a powerful alternative
    for handling tabular data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于经典机器学习工具和算法来分析表格数据。从传统的回归算法到更复杂的梯度提升技术，这些方法在简单性、透明性和有效性方面具有优势。尽管如此，深度学习工具的获取和使用变得更加容易，它们也为处理表格数据提供了一个强大的替代方案。
- en: In this chapter, we will review a set of deep learning stacks (low-level framework,
    high-level API, and deep learning for tabular data library) and use three of these
    stacks—fastai, PyTorch with TabNet, and Lightning Flash—to solve the Airbnb NYC
    problem. We’ll work the same problem three times, once with each stack. The goal
    is to illustrate both the general form of the deep learning approach and to highlight
    the unique characteristics of the three tools we’ve selected.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾一系列深度学习堆栈（低级框架、高级API和表格数据深度学习库），并使用这三个堆栈——fastai、PyTorch与TabNet和Lightning
    Flash来解决Airbnb NYC问题。我们将用每个堆栈解决同样的问题三次。目标是展示深度学习方法的通用形式，并突出我们选择的三种工具的独特特性。
- en: 8.1 The deep learning with tabular data stack
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 使用表格数据的深度学习堆栈
- en: 'Before we examine the stacks that are available for deep learning with tabular
    data in general, let’s look at a specific example: the Keras-based deep learning
    solution for the Airbnb NYC price prediction problem from chapter 3.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们检查用于深度学习表格数据的通用堆栈之前，让我们看看一个具体的例子：第3章中描述的基于Keras的深度学习解决方案，用于解决Airbnb NYC价格预测问题。
- en: The Keras solution vs. the XGBoost solution
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Keras解决方案与XGBoost解决方案的比较
- en: The code that is distinct for the Keras solution is contained in the training
    notebook. In particular, the key differences between the Keras solution and the
    XGBoost solution described in chapter 3 include
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 专属于Keras解决方案的代码包含在训练笔记本中。特别是，第3章中描述的Keras解决方案与XGBoost解决方案之间的关键区别包括
- en: '*Model definition*—The Keras model has a large function to define the layers
    that make up the model, with each class of column (continuous, categorical, and
    text) getting a specific set of layers.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型定义*—Keras模型有一个大函数来定义构成模型的层，每个列类（连续、分类和文本）都得到一组特定的层。'
- en: '*Model training*—The Keras model includes additional code to define the callbacks
    required to make the training process efficient, including a callback to stop
    the training process early if the training is no longer making the model better
    and a callback to ensure that the optimal model is saved during the training process.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型训练*—Keras模型包含额外的代码来定义使训练过程高效的回调函数，包括一个回调函数，如果训练不再使模型变得更好，则提前停止训练过程，以及一个回调函数以确保在训练过程中保存最佳模型。'
- en: The Keras solution that we examined in chapter 3 gives us a concrete baseline
    with which to compare the other stacks we will examine in this chapter. In this
    chapter, we will exercise a set of other stacks so you can see the pros and cons
    of each choice.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章中我们检查的Keras解决方案为我们提供了一个具体的基线，我们可以用它来比较本章我们将检查的其他堆栈。在本章中，我们将练习一系列其他堆栈，以便您可以看到每个选择的优缺点。
- en: We will also discuss an additional set of stacks that we weren’t able to exercise
    and explain what this experience tells us about these choices. It is important
    to understand the stack choices and the pros and cons of the choices so that you
    can select a deep learning with tabular data stack that works best for your requirements.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将讨论一组我们没有能够练习的堆栈，并解释这一经历告诉我们关于这些选择的信息。了解堆栈选择及其优缺点非常重要，这样您就可以选择最适合您需求的深度学习表格数据堆栈。
- en: Let’s briefly review the Keras solution from chapter 3\. Figure 8.1 shows the
    files that make up the Keras solution, with the training notebook highlighted.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F01_Ryan2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 Files that make up the Keras solution to the Airbnb problem
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The training notebook contains the code that varies between the Keras solution
    and the other solutions that we will explore in this chapter. The other files
    stay consistent across all the deep learning solutions, with the exception of
    some settings in the training config file.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 shows the components that make up the stack for this solution. These
    components are used in the training notebook.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F02_Ryan2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 The stack for the Airbnb NYC solution in chapter 3
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: In this stack the underlying, low-level deep learning framework is TensorFlow.
    Since Keras is delivered as part of the TensorFlow distribution and is the recommended
    high-level API for TensorFlow, it may sound a bit redundant to talk about TensorFlow
    and Keras separately, but keeping them distinct will make the description of the
    general stack choices clearer. In the deep learning solution from chapter 3, we
    used custom-written code to define the model itself. For example, listing 8.1
    shows the custom code that defines layers for categorical columns in the deep
    learning solution from chapter 3\. The listing also shows the statements in the
    `get_model()` function that define the layers for categorical columns.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 Statement in the `get_model()`function for categorical column layers
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Defines an input layer to the model for the current column
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: ② Adds the input layer that was just defined to the list of input layers
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines an embedding layer for the current column
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines a batch normalization layer for the current column
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Adds the set of layers defined to this column to the overall list of layers
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'The `get_model()` function specifies the Keras layers in the model for three
    types of input columns: categorical, continuous, and text. The `get_model()` function
    shown in listing 8.1 also contains statements that define the model layers for
    continuous layers and text layers. Note that this model has multiple inputs (each
    column selected to train the model is an input) and a single output: a prediction
    of whether or not the price of a given Airbnb listing will be above or below the
    median. The details of how the layers for each of the input columns are defined
    is beyond the scope of this chapter, so we won’t go through those now.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed what the stack looks like for the deep learning Airbnb
    NYC solution in chapter 3, let’s generalize to other deep learning approaches
    to tabular data. Figure 8.3 shows a selection of choices for the deep learning
    stack for tabular data problems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F03_Ryan2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Deep learning stacks for tabular data
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine each layer of the stacks in more detail:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '*Low-level framework*—There are two predominant low-level deep learning frameworks.
    TensorFlow is used most frequently in industry. PyTorch is the most popular choice
    for researchers.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*低级框架*—有两个主要的低级深度学习框架。在工业界，TensorFlow被使用得最为频繁。PyTorch是研究人员最流行的选择。'
- en: '*High-level API*—To make it easier for beginners to create deep learning applications
    and to abstract some of the complexity for experienced developers, in the mid-2010s
    the need was identified for a high-level API for deep learning. Initially, you
    could use Keras as a front end for several low-level frameworks. In 2019, Keras
    was integrated into the TensorFlow ecosystem and identified as the recommended
    high-level framework for TensorFlow. There isn’t an exact analogy for Keras in
    the PyTorch world. The overall design of PyTorch is supposed to make it more accessible
    than TensorFlow and reduce the need for a high-level API. Nevertheless, there
    are two high-level APIs that abstract different aspects of PyTorch. fastai is
    intended specifically for people coming from other disciplines who want to use
    deep learning to solve problems in their discipline and has as its central ethic
    being able to define, train, and exercise a deep learning model with just a handful
    of lines of code. Lightning, by contrast, abstracts a single aspect of PyTorch,
    the training loop. Lightning Flash, which is built on top of Lightning, is, according
    to its documentation, “a high-level deep learning framework for fast prototyping,
    baselining, fine-tuning and solving deep learning problems.” While both fastai
    and Lightning have devoted communities of users, neither has attracted the popularity
    in the PyTorch world that Keras has in the TensorFlow world.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高级API*—为了使初学者更容易创建深度学习应用，并抽象化一些经验丰富的开发者的复杂性，在2010年代中期，人们发现了对深度学习高级API的需求。最初，您可以使用Keras作为几个低级框架的前端。到2019年，Keras被集成到TensorFlow生态系统中，并被确定为TensorFlow推荐的高级框架。在PyTorch世界中，没有Keras的精确对应物。PyTorch的整体设计旨在使其比TensorFlow更易于使用，并减少对高级API的需求。尽管如此，有两个高级API抽象了PyTorch的不同方面。fastai专门针对来自其他学科的人士，他们希望使用深度学习来解决他们学科中的问题，其核心伦理是只需几行代码就能定义、训练和练习深度学习模型。相比之下，Lightning抽象了PyTorch的单个方面，即训练循环。建立在Lightning之上的Lightning
    Flash，根据其文档，是“一个用于快速原型设计、基准测试、微调和解决深度学习问题的通用深度学习框架。”虽然fastai和Lightning都有专门的用户社区，但它们在PyTorch世界中的受欢迎程度并没有像Keras在TensorFlow世界中的那样。'
- en: '*Tabular data library*—The low-level framework and high-level API provide an
    environment for deep learning in general. The deep learning libraries provide
    capabilities specifically for dealing with tabular data. As we demonstrated with
    the deep learning solution for the Airbnb NYC price prediction problem in chapter
    3, you don’t need to use a tabular data library to do deep learning with tabular
    data.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*表格数据库*—低级框架和高级API为深度学习提供了一个通用环境。深度学习库提供了专门用于处理表格数据的功能。正如我们在第3章中用深度学习解决方案解决Airbnb纽约市价格预测问题所展示的，您不需要使用表格数据库来对表格数据进行深度学习。'
- en: Two details to note about the tabular data libraries are
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 关于表格数据库的两个需要注意的细节是
- en: Tabular data libraries may be supported for both TensorFlow and PyTorch. TabNet
    is an example of a library that is supported for both low-level deep learning
    frameworks.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格数据库可能同时支持TensorFlow和PyTorch。TabNet是支持这两个低级深度学习框架的库的例子。
- en: fastai is a general-purpose, high-level API as well as a tabular data library.
    fastai fits into both categories because it abstracts some of the complexity of
    PyTorch to make it easier to build and train models on a variety of data types
    (including image and text) and also has facilities aimed specifically at tabular
    data (for example, automatically handling basic operations required for categorical
    features in tabular datasets).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fastai是一个通用型、高级API，同时也是表格数据库。fastai符合这两个类别，因为它将PyTorch的一些复杂性抽象化，使其更容易在多种数据类型（包括图像和文本）上构建和训练模型，同时也提供了专门针对表格数据的设施（例如，自动处理表格数据集中分类特征所需的基本操作）。
- en: 'Now that we have examined the deep learning with tabular data stack, let’s
    look at the stacks that we will examine in this chapter by applying them to solve
    the Airbnb NYC price prediction problem:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检查了表格数据的深度学习堆栈，让我们看看在本章中我们将应用以解决Airbnb纽约市价格预测问题的堆栈：
- en: '*PyTorch with fastai*—This is the most “traditional” approach since fastai
    is an established framework with tens of thousands of developers using it. fastai
    is the most popular framework that explicitly supports tabular data, according
    to repo stars. fastai is particularly popular with people who are learning about
    deep learning and hobbyists.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PyTorch with TabNet*—TabNet is the next most popular tabular data library
    after fastai according to repo stars. TabNet is a library for tabular data highlighted
    by Google in its documentation ([https://mng.bz/av1m](https://mng.bz/av1m)). This
    stack demonstrates how a dedicated tabular data library can be used to create
    a model trained on tabular data.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lightning Flash*—PyTorch Lightning is a popular framework that abstracts some
    of the complexity of PyTorch. Lightning Flash is built on top of PyTorch Lightning
    and offers an easily accessible way to create deep learning applications. It also
    includes explicit support for tabular data and thus is an interesting comparison
    point for the other stacks we review in this chapter.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next three sections in this chapter describe the solution to the Airbnb
    NYC price prediction problem using each of these three stacks. In each section
    we will review the code for a solution and compare the pros and cons of the solution
    with those of our baseline, the Keras solution from chapter 3.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 PyTorch with fastai
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s take a look at one of the toolkits: PyTorch/fastai. Because we’re
    considering the same dataset and problem we just discussed, we won’t rehash it
    here. Much of the solution is quite similar among the different toolkits. Here,
    we’ll concentrate on the distinctive portions of the PyTorch code. You can find
    the complete solution in the code repository for the book: [https://mng.bz/gaBv](https://mng.bz/gaBv).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Reviewing the key code aspects of the fastai solution
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s dive into the fastai solution to our Airbnb NYC listing price prediction
    problem. To start with, fastai has a unique set of imports, as shown in listing
    8.2, that get the libraries required to use fastai in a Jupyter notebook.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.2 Import statements for the fastai solution
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Installs the libraries for using fastai in a Jupyter notebook
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: ② Imports the libraries for using fastai in a Jupyter notebook
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: ③ Imports the libraries for working with tabular datasets in fastai
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: With these libraries imported as shown in listing 8.2, you have the libraries
    required to run a fastai tabular data application in a Jupyter notebook.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Next, fastai needs to have the characteristics of the tabular dataset defined,
    including the column that contains the target for the model (called the *dependent
    variable* in fastai) and the lists for the categorical and continuous columns,
    as shown in the following listing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3 Dataset definition statements for the fastai solution
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Specifies the column in the dataset that contains the target, the value that
    is being predicted by the trained model
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: ② Specifies the columns in the dataset that are categorical
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ② 指定数据集中哪些列是分类的
- en: ③ Specifies the columns in the dataset that are continuous
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 指定数据集中哪些列是连续的
- en: We’ll use the values defined in listing 8.3 when we define the `TabularDataloaders`
    ([https://mng.bz/5gwO](https://mng.bz/5gwO)) object for this model. The `TabularDatalloaders`
    object encapsulates the samples from the dataset, including the labels, to make
    it easy to work with the dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义此模型的 `TabularDataloaders` ([https://mng.bz/5gwO](https://mng.bz/5gwO))
    对象时，我们将使用列表 8.3 中定义的值。`TabularDataloaders` 对象封装了数据集的样本，包括标签，以便于与数据集一起工作。
- en: 'Next, we need to ensure that the target column contains string values:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要确保目标列包含字符串值：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If we don’t do this, we will encounter a subtle problem. To try it for yourself,
    comment out this statement and run the fastai training notebook. You will see
    that the training produces some strange results, as shown in figure 8.4.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不这样做，我们将遇到一个微妙的问题。为了亲自尝试，请注释掉此语句并运行 fastai 训练笔记本。您将看到训练产生了一些奇怪的结果，如图 8.4
    所示。
- en: '![](../Images/CH08_F04_Ryan2.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F04_Ryan2.png)'
- en: Figure 8.4 fastai training results when the target column is not explicitly
    converted to string values
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 当目标列未显式转换为字符串值时的 fastai 训练结果
- en: Figure 8.4 shows the results for each epoch of the training process, including
    the training loss, the validation loss, and the accuracy. The accuracy values
    shown in figure 8.4 are significantly lower than the accuracy we saw in chapter
    3 for the XGBoost and Keras deep learning solutions (between 79% and 81%), and
    accuracy does not improve from one epoch to another. Training for a larger number
    of epochs doesn’t help; the accuracy stays the same. Why does fastai produce such
    disappointing results? There’s a clue in the output of the `learn.loss_func` statement,
    as shown in the following listing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 显示了训练过程的每个时期的训练结果，包括训练损失、验证损失和准确率。图 8.4 中显示的准确率值明显低于第 3 章中 XGBoost 和 Keras
    深度学习解决方案的准确率（在 79% 到 81% 之间），并且准确率从一个时期到另一个时期没有提高。训练更多时期没有帮助；准确率保持不变。为什么 fastai
    会产生如此令人失望的结果？在 `learn.loss_func` 语句的输出中有一个线索，如下所示。
- en: Listing 8.4 Statement to show the loss function used in model training
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 显示模型训练中使用的损失函数的语句
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Statement that returns the loss function used in training the fastai model
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ① 返回用于训练 fastai 模型的损失函数的语句
- en: ② Statement output showing the loss function used in training the fastai model
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ② 显示用于训练 fastai 模型的损失函数的语句输出
- en: The output shown in listing 8.4 shows the loss function being used for the model.
    If you don’t specify a loss function for the fastai model, fastai selects a loss
    function based on the values in the target column. We want to train a classification
    model, so the loss function should be cross-entropy. However, it looks like fastai
    selected a loss function for a regression problem rather than a classification
    problem. That’s why the training results shown in listing 8.4 are bad—fastai is
    trying to solve a classification problem (predicting a continuous value) rather
    than the classification problem we intended (predicting a 0 or a 1 to indicate
    whether the listing has a price above or below the median price).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 中显示的输出显示了用于模型的损失函数。如果您没有为 fastai 模型指定损失函数，fastai 将根据目标列中的值选择一个损失函数。我们想要训练一个分类模型，因此损失函数应该是交叉熵。然而，看起来
    fastai 选择了一个用于回归问题的损失函数而不是分类问题。这就是为什么列表 8.4 中显示的训练结果不佳——fastai 正在尝试解决一个分类问题（预测一个连续值）而不是我们想要的分类问题（预测一个
    0 或 1 来指示列表的价格是否高于或低于中位数价格）。
- en: The output of `dls.valid.show_batch()`, as shown in figure 8.5, gives us another
    clue because the values in the `target` column are floating point when they should
    be “0” or “1.”
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`dls.valid.show_batch()` 的输出，如图 8.5 所示，提供了另一个线索，因为当 `target` 列中的值应该是“0”或“1”时，它们是浮点数。'
- en: '![](../Images/CH08_F05_Ryan2.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F05_Ryan2.png)'
- en: Figure 8.5 Sample batch values when the target column is not explicitly converted
    to string values
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 当目标列未显式转换为字符串值时的样本批次值
- en: If we go back and look at the dataset using `merged_data.head()`, as shown in
    figure 8.6, the values in the `target` column all look like 0 or 1.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 `merged_data.head()` 回到数据集查看，如图 8.6 所示，`target` 列中的值看起来都是 0 或 1。
- en: These values in the `target` column are, in fact, numeric values, which means
    that if we don’t explicitly convert them to strings, then by default fastai will
    assume that if we use this dataset to train a model, the model desired is a regression
    model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 `target` 列中的值实际上是数值，这意味着如果我们没有明确地将它们转换为字符串，那么默认情况下，fastai 将假设如果我们使用此数据集来训练模型，所需的模型是回归模型。
- en: '![](../Images/CH08_F06_Ryan2.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F06_Ryan2.png)'
- en: Figure 8.6 Sample batch values when the target column is explicitly converted
    to string values
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 当目标列明确转换为字符串值时的样本批处理值
- en: Now that we have examined why it’s critical to convert the values in the `target`
    column to string values, let’s review the rest of the code to create a trained
    fastai model. Listing 8.5 shows the block of code that defines the `TabularDataLoaders`
    object. This object is a tabular-data specific wrapper around the PyTorch `DataLoader`
    ([https://mng.bz/6eDe](https://mng.bz/6eDe)) object, which is an iterable encapsulation
    of the samples and labels in a dataset.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经探讨了为什么将 `target` 列中的值转换为字符串值是至关重要的，那么让我们回顾一下创建训练好的 fastai 模型的其余代码。列表 8.5
    显示了定义 `TabularDataLoaders` 对象的代码块。该对象是围绕 PyTorch `DataLoader` ([https://mng.bz/6eDe](https://mng.bz/6eDe))
    对象的表格数据特定包装器，它是对数据集中的样本和标签的可迭代封装。
- en: Listing 8.5 Defining the `TabularDataLoaders` object
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.5 定义 `TabularDataLoaders` 对象
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Sets a placeholder value for the path object
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ① 为路径对象设置占位符值
- en: ② Defines the transformations procedures to be applied to the dataset in the
    implied pipeline
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ② 定义在隐式管道中应用于数据集的转换过程
- en: ③ Specifies that the TabularDataLoaders object is based on the merged_data dataframe
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 指定 TabularDataLoaders 对象基于合并后的数据框 merged_data
- en: ④ Specifies the list of transformations to apply with the TabularDataLoaders
    object
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 指定使用 TabularDataLoaders 对象应用的转换列表
- en: ⑤ Specifies the categorical features
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 指定分类特征
- en: ⑥ Specifies the continuous features
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 指定连续特征
- en: ⑦ Specifies the target feature
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 指定目标特征
- en: ⑧ Specifies the subset of the dataset to use for validation in the training
    process
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 指定在训练过程中用于验证的数据集子集
- en: ⑨ Specifies the batch size
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 指定批处理大小
- en: We will use the `TabularDataLoaders` object defined in listing 8.5 to define
    the fastai model shown in listing 8.7.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用列表 8.5 中定义的 `TabularDataLoaders` 对象来定义列表 8.7 中显示的 fastai 模型。
- en: One of the characteristics of fastai is a set of convenience functions that
    makes it easy to examine the dataset through the stages of training. The `show_batch()`
    statement shown in the following listing is an example of such a convenience function.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: fastai 的一个特点是提供了一套便利函数，这使得通过训练阶段检查数据集变得容易。以下列表中显示的 `show_batch()` 语句是此类便利函数的一个例子。
- en: Listing 8.6 Statement to show a batch of the training data
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.6 显示训练数据批次的语句
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The statement shown in listing 8.6 makes it easy to see what the data that is
    training the model looks like after the transformations specified in the `procs`
    parameter of the `TabularDataLoaders` definition. Figure 8.7 shows the output
    of this statement.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.6 中的语句使得可以轻松地看到在 `TabularDataLoaders` 定义中 `procs` 参数指定的转换之后，训练模型的数据看起来像什么。图
    8.7 显示了此语句的输出。
- en: '![](../Images/CH08_F07_Ryan2.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F07_Ryan2.png)'
- en: Figure 8.7 Output of the `show_batch()` statement
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 `show_batch()` 语句的输出
- en: Now that we have specified the data that will be used to train the model, it’s
    time to define and train the model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经指定了将用于训练模型的数据，是时候定义和训练模型了。
- en: Listing 8.7 Defining and fitting the fastai model
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.7 定义和拟合 fastai 模型
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Defines the model as a tabular_learner object using the TabularDataLoaders
    object dls and using accuracy as the performance measurement for training
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用 TabularDataLoaders 对象 dls 将模型定义为 tabular_learner 对象，并使用准确率作为训练的性能度量
- en: ② Trains the model with three epochs
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用三个周期训练模型
- en: Note that the statements in listing 8.7 that define and fit the model are much
    simpler than the model definition and fit statements that we saw for the Keras
    model in chapter 3\. In this sense, the code for the fastai solution resembles
    the code for the XGBoost solution that we saw in chapter 3.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，列表 8.7 中定义和拟合模型的语句比我们在第 3 章中看到的 Keras 模型的定义和拟合语句要简单得多。从这个意义上说，fastai 解决方案的代码与我们在第
    3 章中看到的 XGBoost 解决方案的代码相似。
- en: Figure 8.8 shows the output of the fit statement. For each epoch, the training
    loss, validation loss, and accuracy are listed. If we compare the training results
    shown in figure 8.4 (when the target column was not explicitly converted to string
    values) with the training results shown in figure 8.8 (when the target column
    was converted to string values), it’s clear that we get better results when fastai
    treats the problem as a classification problem rather than a regression problem.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F08_Ryan2.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 Output of the fit statement
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Before continuing with the rest of the fastai solution, let’s take a moment
    to discuss the relationship between training loss, validation loss, and test loss.
    Figure 8.8 shows the training and validation loss at each epoch. Training loss
    that is lower than validation loss indicates that the model could be underfit
    or that regularization techniques that only apply to training (such as dropout)
    are having an outsize effect. Figure 8.8 shows the validation loss as being lower
    than the training loss in the first epoch. For the subsequent epochs, the training
    loss drops faster than the validation loss until it is lower than the validation
    loss by the final epoch.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The following listing confirms that fastai is treating the problem as a classification
    problem because the loss function is `CrossEntropyLoss()`, a loss function that
    is appropriate for a classification problem.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.8 Statement to show the loss function used in model training
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Statement that returns the loss function used in training the fastai model
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: ② Statement output showing the loss function used in training the fastai model
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The output shown in listing 8.8 establishes that we now get the desired loss
    function after setting the target column to contain string values. Now let’s look
    at what layers fastai defines for the model. The following listing shows the `summary()`
    statement, which lets us see the layers that make up the fastai model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.9 Statement to get a summary of the fastai model
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output of the statement in listing 8.9 is shown in figure 8.9, which shows
    the output of the `summary()` statement, including the layers that make up the
    model along with the number of parameters in the model and callbacks used.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F09_Ryan2.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 Output of the `summary()` statement
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have examined the key code areas in the fastai solution, let’s revisit
    the stack diagram to see where the fastai stack fits. Figure 8.10 shows the deep
    learning with tabular data stack from the example in this section. Note that the
    figure shows fastai as both a high-level API and a tabular data library since
    fastai plays both roles in the stack.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F10_Ryan2.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 The stack for PyTorch with fastai
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed the code in the fastai solution, in the next section
    we’ll compare this solution to the Keras solution that we saw in chapter 3.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Comparing the fastai solution with the Keras solution
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 比较fastai解决方案与Keras解决方案
- en: 'We have now seen two deep learning solutions to the Airbnb NYC listing price
    prediction problem: the Keras solution and the fastai solution. In this section,
    we’ll compare the two solutions and review the pros and cons of each.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到了两个解决 Airbnb 纽约市列表价格预测问题的深度学习解决方案：Keras 解决方案和 fastai 解决方案。在本节中，我们将比较这两个解决方案，并回顾每个解决方案的优缺点。
- en: The fastai and Keras solutions make interesting comparison points because they
    are very different. The Keras solution contains a lot of custom code, and all
    the details are evident. The fastai framework infers details about the model from
    the dataset and makes assumptions about the defaults to use so that there aren’t
    many parameters that we need to specify to get a working model. The benefit of
    this is that the fastai code is much more compact than the Keras code. In particular,
    the Keras solution has multiple lines of code to specify the pipeline and the
    details of the layers that make up the model. In the fastai solution, we get the
    pipeline for free by simply specifying the transformations that we want applied
    to the input data (as shown in listing 8.6), and we don’t need to specify the
    layers that make up the model. The downside of the compactness of the fastai solution
    is that subtle problems can get introduced if we’re not careful. In the previous
    section, we saw that if we don’t explicitly convert the target column to string
    values, then fastai will interpret the values in the target column as continuous
    values and assume we want to train a regression model rather than a classification
    model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: fastai 和 Keras 的解决方案是很有趣的比较点，因为它们非常不同。Keras 的解决方案包含大量的自定义代码，所有细节都一目了然。fastai
    框架从数据集中推断模型的细节，并对默认值做出假设，以便我们不需要指定太多参数就能得到一个可工作的模型。这种做法的好处是 fastai 的代码比 Keras
    的代码更加紧凑。特别是，Keras 的解决方案需要多行代码来指定管道和构成模型的层的细节。在 fastai 的解决方案中，我们只需简单地指定要应用于输入数据的转换（如列表
    8.6 所示），就可以免费获得管道，而且我们不需要指定构成模型的层。fastai 解决方案紧凑性的缺点是，如果我们不小心，可能会引入微妙的问题。在前一节中，我们看到了如果我们没有明确地将目标列转换为字符串值，那么
    fastai 将将目标列中的值解释为连续值，并假设我们想要训练一个回归模型而不是分类模型。
- en: Table 8.1 shows a summary of the pros and cons of the Keras and fastai solutions
    to the Airbnb NYC problem. If we compare the performance of the two solutions,
    the Keras model gets between 70% and 74% accuracy, while the fastai model consistently
    gets around 81% accuracy.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 显示了 Keras 和 fastai 解决方案解决 Airbnb 纽约市问题的优缺点总结。如果我们比较两个解决方案的性能，Keras 模型的准确率在
    70% 到 74% 之间，而 fastai 模型始终保持在约 81% 的准确率。
- en: Table 8.1 Summary of the pros and cons of the Keras and fastai solutions
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 Keras 和 fastai 解决方案的优缺点总结
- en: '|  | Keras | fastai |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | Keras | fastai |'
- en: '| --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Pro | Model details are transparent.Large community using the framework means
    that it’s easy to find solutions to common problems | Framework includes explicit
    support for tabular data models, which means the code is much more compact.Framework
    automatically defines pipeline.Framework includes convenience functions that make
    it easy to examine the dataset. |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 优点 | 模型细节透明。使用该框架的大型社区意味着可以轻松找到常见问题的解决方案 | 框架包括对表格数据模型的显式支持，这意味着代码更加紧凑。框架自动定义管道。框架包括方便的函数，使检查数据集变得容易。
    |'
- en: '| Con | No built-in support for tabular data, which means we need to define
    custom code to define the pipeline and layers for the model. | Assumptions made
    by the framework can lead to tricky problems that are hard to debug.User community
    is smaller and less involved in deploying production applications than the Keras
    community, which means it can be harder to find solutions to problems. |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 缺点 | 没有内置对表格数据的支持，这意味着我们需要定义自定义代码来定义模型的管道和层。 | 框架做出的假设可能导致难以调试的棘手问题。用户社区较小，在部署生产应用程序方面不如
    Keras 社区活跃，这意味着可能更难找到问题的解决方案。 |'
- en: 'Let’s cover one more point of comparison between the Keras solution and the
    fastai solution: the underlying low-level deep learning framework. For Keras,
    the underlying framework is TensorFlow, while fastai is built on top of PyTorch.
    This means we have now reviewed deep learning solutions for tabular data problems
    with both of the major deep learning frameworks.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the similarities between Keras and fastai is that they are both general-purpose
    high-level deep learning APIs. We have seen they can both be used for tabular
    data problems, but they are also designed to deal with a range of data types,
    not just tabular data. In the next section, we look at a deep learning library
    that is specifically designed for tabular data problems: TabNet. We examine a
    solution for the Airbnb NYC problem that uses TabNet and then contrast it with
    the Keras solution.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 PyTorch with TabNet
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two tools we’ve considered so far are designed as general deep learning
    libraries. Now, we will try out a purpose-built tabular data library: TabNet.
    Again, we’ll skip the introduction to the problem and concentrate our discussion
    only on the parts of the solution that differ from the previous examples. You
    can find the code for this solution at [https://mng.bz/oK1Z](https://mng.bz/oK1Z).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Key code aspects of the TabNet solution
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we’ll go through the key parts of the code that make up the
    TabNet solution to the Airbnb NYC listing price prediction problem.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The TabNet solution requires a set of imports, as shown in the following listing.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.10 Import statements for TabNet
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Installs the PyTorch implementation of TabNet
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: ② Imports the torch tensor library
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: ③ Imports the TabNetClassifier library. We will use this library to define the
    model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Note that, unlike the fastai import statements, the import statements for TabNet
    in listing 8.10 include an explicit statement to import the PyTorch library `torch`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the fastai solution, which does not have any explicit code to define
    the pipeline and has unique code to define the dataset, the TabNet solution uses
    the same code as the Keras and XGBoost solutions up to and including the definition
    of the pipelines. After the pipeline definitions, the TabNet solutions use code
    similar to XGBoost to convert the list of NumPy arrays that comes out of the pipeline
    into a NumPy array of lists, as shown in the following listing.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.11 Statements to generate NumPy arrays of lists
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Defines lists of lists for the training, validation, and test datasets (one
    list for each feature)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts the train list of lists into a NumPy array of lists
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: ③ Converts the validation list of lists into a NumPy array of lists
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines variables for the training, validation, and test target sets
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The transformations shown in listing 8.11 are needed because the TabNet solution
    expects the input to the model to be in the form of a NumPy array of lists. Next,
    the TabNet solution includes code to define the model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.12 Statements to define the TabNet model
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Defines a TabNetClassifier object as the model for the solution and specify
    an adam optimizer
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets the learning rate for the model
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: ③ Sets parameters for the learning rate scheduler
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The model definition shown in listing 8.12 specifies a set of hyperparameters,
    including the optimizer and learning rate. Next, the TabNet solution includes
    code to train the model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.13 Statements to train the TabNet model
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Specifies the training dataset
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: ② Specifies the validation dataset
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: ③ Specifies the labels for the training and validation results
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: ④ Specifies the metric used to track training performance
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Specifies the number of epochs in the training run and how many epochs to
    run once the model stops improving
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Specifies the batch size
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The statement in listing 8.13 that specifies the training for the TabNet model
    includes early stopping setting, including the `patience` parameter that indicates
    how many epochs the training will continue once the model stops improving.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: The output of the training statement shows the results of each epoch, including
    loss, training accuracy, and validation accuracy, as well as the effect of early
    stopping, as shown in figure 8.11.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F11_Ryan2.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 Output of the TabNet fit statement
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: In the training run output shown in figure 8.11, the maximum number of epochs
    (10) are run because the validation accuracy does not stop improving for more
    than 2 epochs until the maximum number of epochs is reached. This means that the
    `patience` threshold of 3 set in the `fit` statement is never crossed, so the
    training run goes for the maximum number of epochs. Figure 8.12 shows the deep
    learning with tabular data stack from the example in this section.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F12_Ryan2.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 The stack for PyTorch with TabNet
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed the code in the TabNet solution, in the next section
    we’ll compare this solution to the Keras solution that we saw in chapter 3.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 Comparing the TabNet solution with the Keras solution
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have now seen three deep learning solutions to the Airbnb NYC listing price
    prediction problem: the Keras solution, the fastai solution, and the TabNet solution.
    In this section we’ll compare the Keras solution with the TabNet solution and
    review the pros and cons of each.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The Keras solution and the TabNet solution are interesting to compare because
    they demonstrate some of the strengths and weaknesses of their underlying frameworks,
    TensorFlow and PyTorch. The Keras solution benefits from the simple `summary()`
    statement that gives a compact list of the layers that make up the model. PyTorch
    lacks this elegant feature, so the TabNet solution is also missing that benefit.
    Keras, on the other hand, does not provide built-in control for the training process,
    so you have to define callbacks to ensure that you end up with the optimal model
    from the training run at the end of the run and that you don’t waste resources
    running epochs when the model has stopped improving. PyTorch, on the other hand,
    incorporates early stopping and saving of the optimal model by default, so the
    TabNet solution doesn’t need to include code to explicitly define callbacks to
    optimize the training process. Table 8.2 shows a summary of the pros and cons
    of the Keras and TabNet with PyTorch solutions to the Airbnb NYC problem.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.2 Summary of the pros and cons of the Keras and TabNet solutions
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Keras | TabNet |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| Pro | Large community using the framework means that it’s easy to find solutions
    to common problemsSimple summary statement to show the layers in the model | Simple
    statements to define and train the modelDon’t need an explicitly defined callback
    to get the benefit of early stopping |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| Con | No built-in support for tabular data, which means the model definition
    needs to be hand-coded. | Training process is much slower. The Keras model training
    notebook took ~20 seconds to run. The TabNet training notebook took over 4 minutes.No
    one-stop summary statement to see the structure of the model |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: 'In this section we reviewed the Airbnb NYC pricing prediction solution for
    PyTorch TabNet. In the next section we will review our final approach to the Airbnb
    problem: PyTorch with Lightning Flash.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 PyTorch with Lightning Flash
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we’ve considered Keras, fastai, and TabNet PyTorch solutions for the
    Airbnb NYC price prediction problem. Now let’s turn to our final stack: Lightning
    Flash. As a platform designed for fast prototyping, baselining, and fine-tuning,
    along with a clear API and outstanding documentation, Lightning Flash potentially
    offers advantages over the stacks we have explored so far.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for this solution at [https://mng.bz/vKnp](https://mng.bz/vKnp).
    Figure 8.13 shows the fastai and Tabnet on PyTorch stacks.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F13_Ryan2.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 The fastai and Tabnet on PyTorch stacks
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 The key code aspects of the Lightning Flash solution
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code for the Lightning Flash solution has many aspects that are different
    from the solutions we have seen so far. In this section, we go through the model
    training notebook ([https://mng.bz/4aDR](https://mng.bz/4aDR)) to highlight the
    most interesting points about this solution.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: To work in Colab, the Lightning Flash solution requires a set of installs done
    in a particular order, as shown in listing 8.14\. The source for this list is
    [https://mng.bz/QDP6](https://mng.bz/QDP6).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.14 Installs required to get Lightning Flash to work in Colab
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Series of pip installs to get the required levels of PyTorch Lightning
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: ② To eliminate potential conflicts between fastai and Lightning Flash, fastai
    needs to be uninstalled.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: ③ Manually applies a fix for a bug in the current release of icevision
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.14 includes a long list of installs (and one uninstall) to get Lightning
    Flash to work in Colab. From experience, we know that this very specific list
    of installs is required or there will be conflicts between the levels of libraries
    that Lightning Flash requires and the default library levels for Colab.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Next, the libraries required by Lightning Flash need to be imported.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.15 Library imports required by Lightning Flash
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Imports the torch tensor library
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: ② Imports the flash library
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: ③ Imports the objects needed for a tabular classification model
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Note that the torch import statement in listing 8.15 is the same statement that
    you saw to import torch in listing 8.11 for TabNet.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define the parameters for the dataset that we will use to train the
    model.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.16 Setting dataset parameters
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① Sets the target field as the value that the trained model will predict
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines the list of categorical features
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines the list of continuous features
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: The definitions in listing 8.16 should remind you of a similar block of code
    in the fastai solution (listing 8.3), where we defined the target feature along
    with lists for the categorical and continuous features.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Next we use the values we just defined to define a `TabularClassificationData`
    object. This object specifies the minimum characteristics of the dataset that
    we will use to train the model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.17 Defining a `TabularClassificationData`
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Defines the categorical features
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines the continuous features
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines the target feature
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines the training dataset
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Defines the validation dataset
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Defines the test dataset
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: We should explain why the definition of the `TabularClassificationData` object
    shown in listing 8.17 uses separate CSV files for the train, validation, and test
    datasets. For all the other solutions, the dataset is loaded from a pickle file
    that is the output of the data cleanup notebook ([https://mng.bz/XxN9](https://mng.bz/XxN9)),
    which is then split into train, validation, and test datasets in the model training
    notebook. The Lightning Flash solution is different because it has distinct CSV
    files for each segment of the dataset. The reason for this is that the very specific
    installation requirements shown in listing 8.14 were incompatible with loading
    the pickle file that contains the output dataframe from the data cleanup notebook.
    As a workaround, we loaded that pickle file in a separate notebook into a pandas
    DataFrame and saved the separate CSV files for the training, validation, and test
    that you see in the definition of the `TabularClassificationData` object in listing
    8.17\. As an exercise, you could update the data cleanup notebook for the Lightning
    Flash solution so that it saves the cleaned-up dataset as three separate CSV files
    rather than as a single pickle file.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have specified the details about the dataset, we are ready to define
    and train the model, as shown in the following listing.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.18 Setting dataset parameters
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① Defines the model using the TabularClassifierData object defined in listing
    8.18
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines a Trainer object
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: ③ Fits the model
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The training code in listing 8.18 generates the output shown in figure 8.14.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F14_Ryan2.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 Output of the Lightning Flash training process
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Note the output includes the validation accuracy and training accuracy for the
    model. Figure 8.15 shows the deep learning with tabular data stack from the example
    in this section.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F15_Ryan2.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 The PyTorch with Lightning Flash stack
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: The code for the Lightning Flash solution incorporates some very elegant ideas,
    such as being able to specify the training, validation, and test datasets in the
    same object where you define the overall characteristics of the dataset. Overall,
    the API for Lightning Flash is easy to understand. Unfortunately, these benefits
    are undermined because Lightning Flash requires such specific requirements to
    run in Colab. Otherwise, Lightning Flash could have been a favorite, combining
    the simplicity of fastai with the intuitiveness of Keras.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 Comparing the Lightning Flash solution with the Keras solution
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have now seen four deep learning solutions to the Airbnb NYC listing price
    prediction problem: the Keras solution, the fastai solution, the TabNet solution,
    and, finally, the Lightning Flash solution. In this section we’ll compare the
    Keras solution with the Lightning Flash solution and review the pros and cons
    of each.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen that Lightning Flash has some real advantages for rolling out a fast,
    simple solution. However, the lack of a beaten path to using Lightning Flash in
    Colab is concerning, and one has to wonder how long the elaborate set of installs
    shown in listing 8.15 will continue to make it possible to run Lightning Flash
    experiments in Colab. Table 8.3 shows a summary of the pros and cons of the Keras
    and Lightning Flash solutions to the Airbnb NYC problem.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.3 Summary of the pros and cons of the Keras and TabNet solutions
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Keras | Lightning Flash |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| Pro | Large community using the framework means that it’s easy to find solutions
    to common problems.Simple summary statement to show the layers in the model |
    Simple statements to define and train the modelDon’t need to explicitly define
    a pipeline—just need to identify the categorical and continuous columns. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| Con | No built-in support for tabular data, which means the model definition
    needs to be hand-coded. | To get working in Colab, you need a very specific order
    and level of installs. Does not seem to be widely used, at least not with Colab.Out
    of the box, test accuracy was worse than other solutions. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: So far in this chapter we have applied three deep learning with tabular data
    stacks to solve the Airbnb NYC price prediction problem and compared the pros
    and cons of each solution. In the next section, we review an overall comparison
    of all the stacks we exercised in this chapter.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Overall comparison of the stacks
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have looked at a total of four deep learning for tabular data stacks. Table
    8.4 summarizes the performance of all of these stacks on a default Colab setup,
    with a standard GPU Colab runtime option selected for the deep learning solutions.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.4 Summary of accuracy and running time of the deep learning with tabular
    data stacks (plus XGBoost) for the Airbnb NYC listing price prediction problem
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Test Accuracy | Notebook running time |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow with Keras | 81% | 16 seconds |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| fastai with PyTorch | 83% | 69 seconds |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| TabNet with PyTorch | 81% | 568 seconds |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| Lightning Flash with PyTorch | TBD | 14 seconds |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| XGBoost | 79% | 14 seconds |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: It is important to note here that for the purposes of this comparison we didn’t
    do any tuning of any of the approaches. We wanted to have a genuine “apples-to-apples”
    comparison of how the approaches compare to the Keras baseline without additional
    tuning. In subsequent chapters, we will discuss some of the tuning that can be
    done to get optimal results from a deep learning solution.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: For accuracy, fastai is slightly better than the other stacks. For execution
    time, Lightning Flash is best, and TabNet is the slowest. So what is the best
    stack to use for tabular data? First, if we compare XGBoost with any of the deep
    learning solutions, which is best? We will answer this question in more detail
    in chapter 9, but we can say now that if it’s a straight-up comparison between
    deep learning and gradient boosting, for the sake of simplicity and overall performance
    “out of the box,” gradient boosting approaches like XGBoost are currently better
    than any of the deep learning solutions for most datasets. There are use cases
    where it is worthwhile to explore one of the deep learning solutions, and we will
    review these use cases in chapter 9.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do decide to use a deep learning solution for a tabular data problem,
    of the four we explored so far, which one should you use? Here is our overall
    advice:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to deep learning and are primarily interested in exploring a
    solution without needing to immediately implement the solution in production,
    fastai is the best choice. It is the simplest stack to use, and it has a big enough
    user community that you are not likely to be stuck with a problem that nobody
    has seen before. Fastai includes many convenience features to make it easy to
    work with tabular data, so you can rapidly prototype your solution. However, if
    you need to move your solution rapidly to production, fastai is probably not your
    best choice because it is not commonly used in production.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are already comfortable with deep learning and you need to get an application
    into production, we recommend the Keras stack. First, TensorFlow, the low-level
    component of the stack, is the deep learning framework that is used most commonly
    in industry. Second, Keras has a huge user community. While Keras does not yet
    have native support for tabular data the same way that fastai does, Keras is high-level
    enough to lend itself to tabular data problems.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This advice may change as more people use deep learning with tabular data. One
    of the other stacks, such as TabNet on PyTorch, could mature and become the default
    choice for deep learning with tabular data. However, looking at the current state
    of the art, we recommend fastai for beginners and explorers and Keras for people
    who are more experienced and need to proceed to production rapidly.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: In the next section we will discuss the stacks that we didn’t explore in this
    chapter.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 The stacks we didn’t explore
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have asked why we chose three particular stacks (fastai, PyTorch TabNet,
    and Lightning Flash) to solve the Airbnb NYC problem and didn’t explore the other
    options, such as TabNet on TensorFlow, SAINT, or PyTorch Tabular. In this section,
    we’ll look at this question and see what the answer tells us about the options
    that we have for deep learning with tabular data stacks. Figure 8.16 shows the
    deep learning with tabular data stacks that we didn’t explore.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F16_Ryan2.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 The stacks we didn’t explore
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of characteristics that the unexplored stacks have in common:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: All of the unexplored stacks involve dedicated tabular data libraries.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We were not able to get any of the unexplored stacks to work in Colab, the environment
    that we used to exercise the code examples in this book.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our inability to get the unexplored stacks to work in Colab could be due to
    a number of reasons. There could theoretically be some limitations with Colab.
    However, Colab is a very common environment for exploration, so it’s not a good
    sign if a library cannot be coaxed to life in Colab. All of the stacks featured
    “hello world” examples for exercising the stack, and for all the unexplored stacks
    these examples generated errors, mostly to do with contradictory Python library
    prerequisites. It’s possible that if we had been more patient, or looked a little
    harder, we would have been able to work through the errors to get the basic examples
    to work in Colab. Of the stacks that we did explore, Keras, fastai, and PyTorch
    TabNet all worked “out of the box” in Colab. Lighting Flash, on the other hand,
    did require some tweaking before it worked in Colab.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: The difference between Lightning Flash and the unexplored stacks is that it
    was clear that other people had tried to get Lightning Flash to work in Colab,
    and we could find informal documentation that showed exactly what we needed to
    do to get it to work in Colab. If your goal is to solve a tabular data problem
    with deep learning, you want to focus on the problem, not on fiddling with conda
    and pip installing boutique levels of libraries to avoid incompatibilities. By
    that criteria, Keras, fastai, PyTorch TabNet, and, to a lesser extent, Lightning
    Flash are viable choices for deep learning with tabular data in Colab. SAINT,
    DeepTables, PyTorch Tabular, and TabNet on TensorFlow are not viable choices for
    exploration in Colab because they don’t work right away, and the recipes to get
    them to work either don’t exist or are not easy to find.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: While it is disappointing that we weren’t able to exercise the Airbnb NYC price
    prediction problem with more of the dedicated deep learning with tabular data
    libraries, we were still able to accomplish the goal of this chapter by exploring
    three deep learning with tabular data stacks. Figure 8.17 shows all the stacks
    that we were able to explore.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F17_Ryan2.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 The stacks we explored
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'The stacks we explored in this chapter, plus TensorFlow with Keras, present
    a well-rounded set of options:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow with Keras is a rock-solid stack with a huge community of users.
    For just about any problem that you encounter with this stack, you can bet that
    somebody else has hit the problem and posted a solution. The stack works flawlessly
    in Colab, so doing initial investigation is easy. TensorFlow with Keras is commonly
    used in production for all kinds of applications. On the downside, Keras does
    not have built-in support for tabular data, so you need to be prepared to write
    some custom code to tackle tabular data problems.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch with fastai is designed to be easy to get started with, and it works
    flawlessly in Colab, so you can expect to prototype deep learning with tabular
    data problems with this stack with minimal hassle. fastai treats tabular data
    as a first-class citizen, so you get built-in support for dealing with categorical
    and continuous features and you don’t need to worry about hand-coding a pipeline
    to ensure that when you feed data to the trained model to get a prediction, the
    data goes through the same transformations as the data used to train the model.
    On the downside, you can pay a price for the simplicity of fastai code. The automated
    steps that fastai takes can lead to some hard-to-debug problems (such as the problem
    we described in this chapter where the wrong kind of model gets trained if the
    target column isn’t explicitly converted to a string type), and you need to be
    prepared to dig deep into the fastai API if you want to go off the beaten path
    provided by fastai’s tabular data structures.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch with TabNet distinguishes itself among the tabular-data-specific libraries
    by working in Colab without any fuss. Like fastai, with TabNet you can define
    and train a deep learning model on tabular data with just a handful of lines of
    code. Unlike fastai, TabNet uses conventional APIs that are easy for anybody who
    has used Scikit-learn to understand. Compared to the other stacks, TabNet took
    longer to train the model. Also, as a library specifically designed for tabular
    data, TabNet has a smaller user community than fastai and Keras, which means that
    if you run into problems it will be less likely that somebody else has already
    hit them and documented a fix on Stack Overflow.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch with Lightning Flash trains the model quickly and has a simple API,
    once you get it to work. Lightning has a large community—not as big as Keras but
    bigger than fastai. However, the niche of using Lightning Flash on Colab to do
    tabular data problems is not that big, and we were only just able to get Lightning
    Flash to work on Colab.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter we have explored a set of deep learning with tabular data stacks,
    compared the pros and cons of each approach, and discussed why we left some other
    stacks unexplored. In the next chapter we are going to review the best practices
    for deep learning with tabular data.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two low-level deep learning frameworks: TensorFlow and PyTorch.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow is used more frequently in industry.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch is the predominant deep learning framework for research.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras is a high-level API for TensorFlow.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastai is both a general-purpose, high-level API for PyTorch and a tabular data
    library.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch Lightning is a high-level API that abstracts some of the details of
    PyTorch. Lightning Flash is a tabular data library based on Lightning.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightning Flash and fastai each provide some of the same benefits for PyTorch
    that Keras does for TensorFlow by abstracting aspects of the underlying PyTorch
    framework.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TabNet is a tabular data library that is available for both TensorFlow and PyTorch.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAINT is a tabular data library for TensorFlow.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch Tabular is a tabular data library for PyTorch.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of all the choices available, TensorFlow with Keras, PyTorch with fastai, PyTorch
    with TabNet, and PyTorch with Lightning Flash are all valid options for deep learning
    with tabular data on Colab.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
