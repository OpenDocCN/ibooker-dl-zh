- en: 8 Getting started with deep learning with tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to deep learning with tabular data stacks—low-level frameworks
    and high-level APIs for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyTorch with fastai stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyTorch with TabNet stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyTorch with Lightning Flash stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stacks we didn’t exercise and why we didn’t exercise them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comparison of the pros and cons of deep learning with tabular data stacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to this point, we have focused on classical machine learning tools and algorithms
    to analyze tabular data. Ranging from traditional regression algorithms to more
    sophisticated gradient boosting techniques, these approaches offer advantages
    in simplicity, transparency, and efficacy. That said, deep learning tools have
    become much easier to access and use, and they also provide a powerful alternative
    for handling tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will review a set of deep learning stacks (low-level framework,
    high-level API, and deep learning for tabular data library) and use three of these
    stacks—fastai, PyTorch with TabNet, and Lightning Flash—to solve the Airbnb NYC
    problem. We’ll work the same problem three times, once with each stack. The goal
    is to illustrate both the general form of the deep learning approach and to highlight
    the unique characteristics of the three tools we’ve selected.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 The deep learning with tabular data stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we examine the stacks that are available for deep learning with tabular
    data in general, let’s look at a specific example: the Keras-based deep learning
    solution for the Airbnb NYC price prediction problem from chapter 3.'
  prefs: []
  type: TYPE_NORMAL
- en: The Keras solution vs. the XGBoost solution
  prefs: []
  type: TYPE_NORMAL
- en: The code that is distinct for the Keras solution is contained in the training
    notebook. In particular, the key differences between the Keras solution and the
    XGBoost solution described in chapter 3 include
  prefs: []
  type: TYPE_NORMAL
- en: '*Model definition*—The Keras model has a large function to define the layers
    that make up the model, with each class of column (continuous, categorical, and
    text) getting a specific set of layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model training*—The Keras model includes additional code to define the callbacks
    required to make the training process efficient, including a callback to stop
    the training process early if the training is no longer making the model better
    and a callback to ensure that the optimal model is saved during the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras solution that we examined in chapter 3 gives us a concrete baseline
    with which to compare the other stacks we will examine in this chapter. In this
    chapter, we will exercise a set of other stacks so you can see the pros and cons
    of each choice.
  prefs: []
  type: TYPE_NORMAL
- en: We will also discuss an additional set of stacks that we weren’t able to exercise
    and explain what this experience tells us about these choices. It is important
    to understand the stack choices and the pros and cons of the choices so that you
    can select a deep learning with tabular data stack that works best for your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s briefly review the Keras solution from chapter 3\. Figure 8.1 shows the
    files that make up the Keras solution, with the training notebook highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 Files that make up the Keras solution to the Airbnb problem
  prefs: []
  type: TYPE_NORMAL
- en: The training notebook contains the code that varies between the Keras solution
    and the other solutions that we will explore in this chapter. The other files
    stay consistent across all the deep learning solutions, with the exception of
    some settings in the training config file.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 shows the components that make up the stack for this solution. These
    components are used in the training notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F02_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 The stack for the Airbnb NYC solution in chapter 3
  prefs: []
  type: TYPE_NORMAL
- en: In this stack the underlying, low-level deep learning framework is TensorFlow.
    Since Keras is delivered as part of the TensorFlow distribution and is the recommended
    high-level API for TensorFlow, it may sound a bit redundant to talk about TensorFlow
    and Keras separately, but keeping them distinct will make the description of the
    general stack choices clearer. In the deep learning solution from chapter 3, we
    used custom-written code to define the model itself. For example, listing 8.1
    shows the custom code that defines layers for categorical columns in the deep
    learning solution from chapter 3\. The listing also shows the statements in the
    `get_model()` function that define the layers for categorical columns.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 Statement in the `get_model()`function for categorical column layers
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines an input layer to the model for the current column
  prefs: []
  type: TYPE_NORMAL
- en: ② Adds the input layer that was just defined to the list of input layers
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines an embedding layer for the current column
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines a batch normalization layer for the current column
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Adds the set of layers defined to this column to the overall list of layers
  prefs: []
  type: TYPE_NORMAL
- en: 'The `get_model()` function specifies the Keras layers in the model for three
    types of input columns: categorical, continuous, and text. The `get_model()` function
    shown in listing 8.1 also contains statements that define the model layers for
    continuous layers and text layers. Note that this model has multiple inputs (each
    column selected to train the model is an input) and a single output: a prediction
    of whether or not the price of a given Airbnb listing will be above or below the
    median. The details of how the layers for each of the input columns are defined
    is beyond the scope of this chapter, so we won’t go through those now.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed what the stack looks like for the deep learning Airbnb
    NYC solution in chapter 3, let’s generalize to other deep learning approaches
    to tabular data. Figure 8.3 shows a selection of choices for the deep learning
    stack for tabular data problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F03_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Deep learning stacks for tabular data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine each layer of the stacks in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Low-level framework*—There are two predominant low-level deep learning frameworks.
    TensorFlow is used most frequently in industry. PyTorch is the most popular choice
    for researchers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*High-level API*—To make it easier for beginners to create deep learning applications
    and to abstract some of the complexity for experienced developers, in the mid-2010s
    the need was identified for a high-level API for deep learning. Initially, you
    could use Keras as a front end for several low-level frameworks. In 2019, Keras
    was integrated into the TensorFlow ecosystem and identified as the recommended
    high-level framework for TensorFlow. There isn’t an exact analogy for Keras in
    the PyTorch world. The overall design of PyTorch is supposed to make it more accessible
    than TensorFlow and reduce the need for a high-level API. Nevertheless, there
    are two high-level APIs that abstract different aspects of PyTorch. fastai is
    intended specifically for people coming from other disciplines who want to use
    deep learning to solve problems in their discipline and has as its central ethic
    being able to define, train, and exercise a deep learning model with just a handful
    of lines of code. Lightning, by contrast, abstracts a single aspect of PyTorch,
    the training loop. Lightning Flash, which is built on top of Lightning, is, according
    to its documentation, “a high-level deep learning framework for fast prototyping,
    baselining, fine-tuning and solving deep learning problems.” While both fastai
    and Lightning have devoted communities of users, neither has attracted the popularity
    in the PyTorch world that Keras has in the TensorFlow world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tabular data library*—The low-level framework and high-level API provide an
    environment for deep learning in general. The deep learning libraries provide
    capabilities specifically for dealing with tabular data. As we demonstrated with
    the deep learning solution for the Airbnb NYC price prediction problem in chapter
    3, you don’t need to use a tabular data library to do deep learning with tabular
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two details to note about the tabular data libraries are
  prefs: []
  type: TYPE_NORMAL
- en: Tabular data libraries may be supported for both TensorFlow and PyTorch. TabNet
    is an example of a library that is supported for both low-level deep learning
    frameworks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastai is a general-purpose, high-level API as well as a tabular data library.
    fastai fits into both categories because it abstracts some of the complexity of
    PyTorch to make it easier to build and train models on a variety of data types
    (including image and text) and also has facilities aimed specifically at tabular
    data (for example, automatically handling basic operations required for categorical
    features in tabular datasets).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have examined the deep learning with tabular data stack, let’s
    look at the stacks that we will examine in this chapter by applying them to solve
    the Airbnb NYC price prediction problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '*PyTorch with fastai*—This is the most “traditional” approach since fastai
    is an established framework with tens of thousands of developers using it. fastai
    is the most popular framework that explicitly supports tabular data, according
    to repo stars. fastai is particularly popular with people who are learning about
    deep learning and hobbyists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PyTorch with TabNet*—TabNet is the next most popular tabular data library
    after fastai according to repo stars. TabNet is a library for tabular data highlighted
    by Google in its documentation ([https://mng.bz/av1m](https://mng.bz/av1m)). This
    stack demonstrates how a dedicated tabular data library can be used to create
    a model trained on tabular data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lightning Flash*—PyTorch Lightning is a popular framework that abstracts some
    of the complexity of PyTorch. Lightning Flash is built on top of PyTorch Lightning
    and offers an easily accessible way to create deep learning applications. It also
    includes explicit support for tabular data and thus is an interesting comparison
    point for the other stacks we review in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next three sections in this chapter describe the solution to the Airbnb
    NYC price prediction problem using each of these three stacks. In each section
    we will review the code for a solution and compare the pros and cons of the solution
    with those of our baseline, the Keras solution from chapter 3.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 PyTorch with fastai
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s take a look at one of the toolkits: PyTorch/fastai. Because we’re
    considering the same dataset and problem we just discussed, we won’t rehash it
    here. Much of the solution is quite similar among the different toolkits. Here,
    we’ll concentrate on the distinctive portions of the PyTorch code. You can find
    the complete solution in the code repository for the book: [https://mng.bz/gaBv](https://mng.bz/gaBv).'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Reviewing the key code aspects of the fastai solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s dive into the fastai solution to our Airbnb NYC listing price prediction
    problem. To start with, fastai has a unique set of imports, as shown in listing
    8.2, that get the libraries required to use fastai in a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.2 Import statements for the fastai solution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Installs the libraries for using fastai in a Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: ② Imports the libraries for using fastai in a Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: ③ Imports the libraries for working with tabular datasets in fastai
  prefs: []
  type: TYPE_NORMAL
- en: With these libraries imported as shown in listing 8.2, you have the libraries
    required to run a fastai tabular data application in a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Next, fastai needs to have the characteristics of the tabular dataset defined,
    including the column that contains the target for the model (called the *dependent
    variable* in fastai) and the lists for the categorical and continuous columns,
    as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3 Dataset definition statements for the fastai solution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① Specifies the column in the dataset that contains the target, the value that
    is being predicted by the trained model
  prefs: []
  type: TYPE_NORMAL
- en: ② Specifies the columns in the dataset that are categorical
  prefs: []
  type: TYPE_NORMAL
- en: ③ Specifies the columns in the dataset that are continuous
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the values defined in listing 8.3 when we define the `TabularDataloaders`
    ([https://mng.bz/5gwO](https://mng.bz/5gwO)) object for this model. The `TabularDatalloaders`
    object encapsulates the samples from the dataset, including the labels, to make
    it easy to work with the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to ensure that the target column contains string values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If we don’t do this, we will encounter a subtle problem. To try it for yourself,
    comment out this statement and run the fastai training notebook. You will see
    that the training produces some strange results, as shown in figure 8.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F04_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 fastai training results when the target column is not explicitly
    converted to string values
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 shows the results for each epoch of the training process, including
    the training loss, the validation loss, and the accuracy. The accuracy values
    shown in figure 8.4 are significantly lower than the accuracy we saw in chapter
    3 for the XGBoost and Keras deep learning solutions (between 79% and 81%), and
    accuracy does not improve from one epoch to another. Training for a larger number
    of epochs doesn’t help; the accuracy stays the same. Why does fastai produce such
    disappointing results? There’s a clue in the output of the `learn.loss_func` statement,
    as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.4 Statement to show the loss function used in model training
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① Statement that returns the loss function used in training the fastai model
  prefs: []
  type: TYPE_NORMAL
- en: ② Statement output showing the loss function used in training the fastai model
  prefs: []
  type: TYPE_NORMAL
- en: The output shown in listing 8.4 shows the loss function being used for the model.
    If you don’t specify a loss function for the fastai model, fastai selects a loss
    function based on the values in the target column. We want to train a classification
    model, so the loss function should be cross-entropy. However, it looks like fastai
    selected a loss function for a regression problem rather than a classification
    problem. That’s why the training results shown in listing 8.4 are bad—fastai is
    trying to solve a classification problem (predicting a continuous value) rather
    than the classification problem we intended (predicting a 0 or a 1 to indicate
    whether the listing has a price above or below the median price).
  prefs: []
  type: TYPE_NORMAL
- en: The output of `dls.valid.show_batch()`, as shown in figure 8.5, gives us another
    clue because the values in the `target` column are floating point when they should
    be “0” or “1.”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F05_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 Sample batch values when the target column is not explicitly converted
    to string values
  prefs: []
  type: TYPE_NORMAL
- en: If we go back and look at the dataset using `merged_data.head()`, as shown in
    figure 8.6, the values in the `target` column all look like 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: These values in the `target` column are, in fact, numeric values, which means
    that if we don’t explicitly convert them to strings, then by default fastai will
    assume that if we use this dataset to train a model, the model desired is a regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F06_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 Sample batch values when the target column is explicitly converted
    to string values
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have examined why it’s critical to convert the values in the `target`
    column to string values, let’s review the rest of the code to create a trained
    fastai model. Listing 8.5 shows the block of code that defines the `TabularDataLoaders`
    object. This object is a tabular-data specific wrapper around the PyTorch `DataLoader`
    ([https://mng.bz/6eDe](https://mng.bz/6eDe)) object, which is an iterable encapsulation
    of the samples and labels in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.5 Defining the `TabularDataLoaders` object
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets a placeholder value for the path object
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines the transformations procedures to be applied to the dataset in the
    implied pipeline
  prefs: []
  type: TYPE_NORMAL
- en: ③ Specifies that the TabularDataLoaders object is based on the merged_data dataframe
  prefs: []
  type: TYPE_NORMAL
- en: ④ Specifies the list of transformations to apply with the TabularDataLoaders
    object
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Specifies the categorical features
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Specifies the continuous features
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Specifies the target feature
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Specifies the subset of the dataset to use for validation in the training
    process
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Specifies the batch size
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `TabularDataLoaders` object defined in listing 8.5 to define
    the fastai model shown in listing 8.7.
  prefs: []
  type: TYPE_NORMAL
- en: One of the characteristics of fastai is a set of convenience functions that
    makes it easy to examine the dataset through the stages of training. The `show_batch()`
    statement shown in the following listing is an example of such a convenience function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.6 Statement to show a batch of the training data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The statement shown in listing 8.6 makes it easy to see what the data that is
    training the model looks like after the transformations specified in the `procs`
    parameter of the `TabularDataLoaders` definition. Figure 8.7 shows the output
    of this statement.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F07_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 Output of the `show_batch()` statement
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have specified the data that will be used to train the model, it’s
    time to define and train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.7 Defining and fitting the fastai model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines the model as a tabular_learner object using the TabularDataLoaders
    object dls and using accuracy as the performance measurement for training
  prefs: []
  type: TYPE_NORMAL
- en: ② Trains the model with three epochs
  prefs: []
  type: TYPE_NORMAL
- en: Note that the statements in listing 8.7 that define and fit the model are much
    simpler than the model definition and fit statements that we saw for the Keras
    model in chapter 3\. In this sense, the code for the fastai solution resembles
    the code for the XGBoost solution that we saw in chapter 3.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 shows the output of the fit statement. For each epoch, the training
    loss, validation loss, and accuracy are listed. If we compare the training results
    shown in figure 8.4 (when the target column was not explicitly converted to string
    values) with the training results shown in figure 8.8 (when the target column
    was converted to string values), it’s clear that we get better results when fastai
    treats the problem as a classification problem rather than a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F08_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 Output of the fit statement
  prefs: []
  type: TYPE_NORMAL
- en: Before continuing with the rest of the fastai solution, let’s take a moment
    to discuss the relationship between training loss, validation loss, and test loss.
    Figure 8.8 shows the training and validation loss at each epoch. Training loss
    that is lower than validation loss indicates that the model could be underfit
    or that regularization techniques that only apply to training (such as dropout)
    are having an outsize effect. Figure 8.8 shows the validation loss as being lower
    than the training loss in the first epoch. For the subsequent epochs, the training
    loss drops faster than the validation loss until it is lower than the validation
    loss by the final epoch.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing confirms that fastai is treating the problem as a classification
    problem because the loss function is `CrossEntropyLoss()`, a loss function that
    is appropriate for a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.8 Statement to show the loss function used in model training
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① Statement that returns the loss function used in training the fastai model
  prefs: []
  type: TYPE_NORMAL
- en: ② Statement output showing the loss function used in training the fastai model
  prefs: []
  type: TYPE_NORMAL
- en: The output shown in listing 8.8 establishes that we now get the desired loss
    function after setting the target column to contain string values. Now let’s look
    at what layers fastai defines for the model. The following listing shows the `summary()`
    statement, which lets us see the layers that make up the fastai model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.9 Statement to get a summary of the fastai model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output of the statement in listing 8.9 is shown in figure 8.9, which shows
    the output of the `summary()` statement, including the layers that make up the
    model along with the number of parameters in the model and callbacks used.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F09_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 Output of the `summary()` statement
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have examined the key code areas in the fastai solution, let’s revisit
    the stack diagram to see where the fastai stack fits. Figure 8.10 shows the deep
    learning with tabular data stack from the example in this section. Note that the
    figure shows fastai as both a high-level API and a tabular data library since
    fastai plays both roles in the stack.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F10_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 The stack for PyTorch with fastai
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed the code in the fastai solution, in the next section
    we’ll compare this solution to the Keras solution that we saw in chapter 3.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Comparing the fastai solution with the Keras solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have now seen two deep learning solutions to the Airbnb NYC listing price
    prediction problem: the Keras solution and the fastai solution. In this section,
    we’ll compare the two solutions and review the pros and cons of each.'
  prefs: []
  type: TYPE_NORMAL
- en: The fastai and Keras solutions make interesting comparison points because they
    are very different. The Keras solution contains a lot of custom code, and all
    the details are evident. The fastai framework infers details about the model from
    the dataset and makes assumptions about the defaults to use so that there aren’t
    many parameters that we need to specify to get a working model. The benefit of
    this is that the fastai code is much more compact than the Keras code. In particular,
    the Keras solution has multiple lines of code to specify the pipeline and the
    details of the layers that make up the model. In the fastai solution, we get the
    pipeline for free by simply specifying the transformations that we want applied
    to the input data (as shown in listing 8.6), and we don’t need to specify the
    layers that make up the model. The downside of the compactness of the fastai solution
    is that subtle problems can get introduced if we’re not careful. In the previous
    section, we saw that if we don’t explicitly convert the target column to string
    values, then fastai will interpret the values in the target column as continuous
    values and assume we want to train a regression model rather than a classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 shows a summary of the pros and cons of the Keras and fastai solutions
    to the Airbnb NYC problem. If we compare the performance of the two solutions,
    the Keras model gets between 70% and 74% accuracy, while the fastai model consistently
    gets around 81% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 Summary of the pros and cons of the Keras and fastai solutions
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Keras | fastai |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pro | Model details are transparent.Large community using the framework means
    that it’s easy to find solutions to common problems | Framework includes explicit
    support for tabular data models, which means the code is much more compact.Framework
    automatically defines pipeline.Framework includes convenience functions that make
    it easy to examine the dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| Con | No built-in support for tabular data, which means we need to define
    custom code to define the pipeline and layers for the model. | Assumptions made
    by the framework can lead to tricky problems that are hard to debug.User community
    is smaller and less involved in deploying production applications than the Keras
    community, which means it can be harder to find solutions to problems. |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s cover one more point of comparison between the Keras solution and the
    fastai solution: the underlying low-level deep learning framework. For Keras,
    the underlying framework is TensorFlow, while fastai is built on top of PyTorch.
    This means we have now reviewed deep learning solutions for tabular data problems
    with both of the major deep learning frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the similarities between Keras and fastai is that they are both general-purpose
    high-level deep learning APIs. We have seen they can both be used for tabular
    data problems, but they are also designed to deal with a range of data types,
    not just tabular data. In the next section, we look at a deep learning library
    that is specifically designed for tabular data problems: TabNet. We examine a
    solution for the Airbnb NYC problem that uses TabNet and then contrast it with
    the Keras solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 PyTorch with TabNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two tools we’ve considered so far are designed as general deep learning
    libraries. Now, we will try out a purpose-built tabular data library: TabNet.
    Again, we’ll skip the introduction to the problem and concentrate our discussion
    only on the parts of the solution that differ from the previous examples. You
    can find the code for this solution at [https://mng.bz/oK1Z](https://mng.bz/oK1Z).'
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Key code aspects of the TabNet solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we’ll go through the key parts of the code that make up the
    TabNet solution to the Airbnb NYC listing price prediction problem.
  prefs: []
  type: TYPE_NORMAL
- en: The TabNet solution requires a set of imports, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.10 Import statements for TabNet
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ① Installs the PyTorch implementation of TabNet
  prefs: []
  type: TYPE_NORMAL
- en: ② Imports the torch tensor library
  prefs: []
  type: TYPE_NORMAL
- en: ③ Imports the TabNetClassifier library. We will use this library to define the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, unlike the fastai import statements, the import statements for TabNet
    in listing 8.10 include an explicit statement to import the PyTorch library `torch`.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the fastai solution, which does not have any explicit code to define
    the pipeline and has unique code to define the dataset, the TabNet solution uses
    the same code as the Keras and XGBoost solutions up to and including the definition
    of the pipelines. After the pipeline definitions, the TabNet solutions use code
    similar to XGBoost to convert the list of NumPy arrays that comes out of the pipeline
    into a NumPy array of lists, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.11 Statements to generate NumPy arrays of lists
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines lists of lists for the training, validation, and test datasets (one
    list for each feature)
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts the train list of lists into a NumPy array of lists
  prefs: []
  type: TYPE_NORMAL
- en: ③ Converts the validation list of lists into a NumPy array of lists
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines variables for the training, validation, and test target sets
  prefs: []
  type: TYPE_NORMAL
- en: The transformations shown in listing 8.11 are needed because the TabNet solution
    expects the input to the model to be in the form of a NumPy array of lists. Next,
    the TabNet solution includes code to define the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.12 Statements to define the TabNet model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a TabNetClassifier object as the model for the solution and specify
    an adam optimizer
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets the learning rate for the model
  prefs: []
  type: TYPE_NORMAL
- en: ③ Sets parameters for the learning rate scheduler
  prefs: []
  type: TYPE_NORMAL
- en: The model definition shown in listing 8.12 specifies a set of hyperparameters,
    including the optimizer and learning rate. Next, the TabNet solution includes
    code to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.13 Statements to train the TabNet model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Specifies the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: ② Specifies the validation dataset
  prefs: []
  type: TYPE_NORMAL
- en: ③ Specifies the labels for the training and validation results
  prefs: []
  type: TYPE_NORMAL
- en: ④ Specifies the metric used to track training performance
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Specifies the number of epochs in the training run and how many epochs to
    run once the model stops improving
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Specifies the batch size
  prefs: []
  type: TYPE_NORMAL
- en: The statement in listing 8.13 that specifies the training for the TabNet model
    includes early stopping setting, including the `patience` parameter that indicates
    how many epochs the training will continue once the model stops improving.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the training statement shows the results of each epoch, including
    loss, training accuracy, and validation accuracy, as well as the effect of early
    stopping, as shown in figure 8.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F11_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 Output of the TabNet fit statement
  prefs: []
  type: TYPE_NORMAL
- en: In the training run output shown in figure 8.11, the maximum number of epochs
    (10) are run because the validation accuracy does not stop improving for more
    than 2 epochs until the maximum number of epochs is reached. This means that the
    `patience` threshold of 3 set in the `fit` statement is never crossed, so the
    training run goes for the maximum number of epochs. Figure 8.12 shows the deep
    learning with tabular data stack from the example in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F12_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 The stack for PyTorch with TabNet
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed the code in the TabNet solution, in the next section
    we’ll compare this solution to the Keras solution that we saw in chapter 3.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 Comparing the TabNet solution with the Keras solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have now seen three deep learning solutions to the Airbnb NYC listing price
    prediction problem: the Keras solution, the fastai solution, and the TabNet solution.
    In this section we’ll compare the Keras solution with the TabNet solution and
    review the pros and cons of each.'
  prefs: []
  type: TYPE_NORMAL
- en: The Keras solution and the TabNet solution are interesting to compare because
    they demonstrate some of the strengths and weaknesses of their underlying frameworks,
    TensorFlow and PyTorch. The Keras solution benefits from the simple `summary()`
    statement that gives a compact list of the layers that make up the model. PyTorch
    lacks this elegant feature, so the TabNet solution is also missing that benefit.
    Keras, on the other hand, does not provide built-in control for the training process,
    so you have to define callbacks to ensure that you end up with the optimal model
    from the training run at the end of the run and that you don’t waste resources
    running epochs when the model has stopped improving. PyTorch, on the other hand,
    incorporates early stopping and saving of the optimal model by default, so the
    TabNet solution doesn’t need to include code to explicitly define callbacks to
    optimize the training process. Table 8.2 shows a summary of the pros and cons
    of the Keras and TabNet with PyTorch solutions to the Airbnb NYC problem.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.2 Summary of the pros and cons of the Keras and TabNet solutions
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Keras | TabNet |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pro | Large community using the framework means that it’s easy to find solutions
    to common problemsSimple summary statement to show the layers in the model | Simple
    statements to define and train the modelDon’t need an explicitly defined callback
    to get the benefit of early stopping |'
  prefs: []
  type: TYPE_TB
- en: '| Con | No built-in support for tabular data, which means the model definition
    needs to be hand-coded. | Training process is much slower. The Keras model training
    notebook took ~20 seconds to run. The TabNet training notebook took over 4 minutes.No
    one-stop summary statement to see the structure of the model |'
  prefs: []
  type: TYPE_TB
- en: 'In this section we reviewed the Airbnb NYC pricing prediction solution for
    PyTorch TabNet. In the next section we will review our final approach to the Airbnb
    problem: PyTorch with Lightning Flash.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 PyTorch with Lightning Flash
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we’ve considered Keras, fastai, and TabNet PyTorch solutions for the
    Airbnb NYC price prediction problem. Now let’s turn to our final stack: Lightning
    Flash. As a platform designed for fast prototyping, baselining, and fine-tuning,
    along with a clear API and outstanding documentation, Lightning Flash potentially
    offers advantages over the stacks we have explored so far.'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for this solution at [https://mng.bz/vKnp](https://mng.bz/vKnp).
    Figure 8.13 shows the fastai and Tabnet on PyTorch stacks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F13_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 The fastai and Tabnet on PyTorch stacks
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 The key code aspects of the Lightning Flash solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code for the Lightning Flash solution has many aspects that are different
    from the solutions we have seen so far. In this section, we go through the model
    training notebook ([https://mng.bz/4aDR](https://mng.bz/4aDR)) to highlight the
    most interesting points about this solution.
  prefs: []
  type: TYPE_NORMAL
- en: To work in Colab, the Lightning Flash solution requires a set of installs done
    in a particular order, as shown in listing 8.14\. The source for this list is
    [https://mng.bz/QDP6](https://mng.bz/QDP6).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.14 Installs required to get Lightning Flash to work in Colab
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Series of pip installs to get the required levels of PyTorch Lightning
  prefs: []
  type: TYPE_NORMAL
- en: ② To eliminate potential conflicts between fastai and Lightning Flash, fastai
    needs to be uninstalled.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Manually applies a fix for a bug in the current release of icevision
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.14 includes a long list of installs (and one uninstall) to get Lightning
    Flash to work in Colab. From experience, we know that this very specific list
    of installs is required or there will be conflicts between the levels of libraries
    that Lightning Flash requires and the default library levels for Colab.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the libraries required by Lightning Flash need to be imported.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.15 Library imports required by Lightning Flash
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: ① Imports the torch tensor library
  prefs: []
  type: TYPE_NORMAL
- en: ② Imports the flash library
  prefs: []
  type: TYPE_NORMAL
- en: ③ Imports the objects needed for a tabular classification model
  prefs: []
  type: TYPE_NORMAL
- en: Note that the torch import statement in listing 8.15 is the same statement that
    you saw to import torch in listing 8.11 for TabNet.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define the parameters for the dataset that we will use to train the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.16 Setting dataset parameters
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets the target field as the value that the trained model will predict
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines the list of categorical features
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines the list of continuous features
  prefs: []
  type: TYPE_NORMAL
- en: The definitions in listing 8.16 should remind you of a similar block of code
    in the fastai solution (listing 8.3), where we defined the target feature along
    with lists for the categorical and continuous features.
  prefs: []
  type: TYPE_NORMAL
- en: Next we use the values we just defined to define a `TabularClassificationData`
    object. This object specifies the minimum characteristics of the dataset that
    we will use to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.17 Defining a `TabularClassificationData`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines the categorical features
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines the continuous features
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines the target feature
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Defines the validation dataset
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Defines the test dataset
  prefs: []
  type: TYPE_NORMAL
- en: We should explain why the definition of the `TabularClassificationData` object
    shown in listing 8.17 uses separate CSV files for the train, validation, and test
    datasets. For all the other solutions, the dataset is loaded from a pickle file
    that is the output of the data cleanup notebook ([https://mng.bz/XxN9](https://mng.bz/XxN9)),
    which is then split into train, validation, and test datasets in the model training
    notebook. The Lightning Flash solution is different because it has distinct CSV
    files for each segment of the dataset. The reason for this is that the very specific
    installation requirements shown in listing 8.14 were incompatible with loading
    the pickle file that contains the output dataframe from the data cleanup notebook.
    As a workaround, we loaded that pickle file in a separate notebook into a pandas
    DataFrame and saved the separate CSV files for the training, validation, and test
    that you see in the definition of the `TabularClassificationData` object in listing
    8.17\. As an exercise, you could update the data cleanup notebook for the Lightning
    Flash solution so that it saves the cleaned-up dataset as three separate CSV files
    rather than as a single pickle file.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have specified the details about the dataset, we are ready to define
    and train the model, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.18 Setting dataset parameters
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines the model using the TabularClassifierData object defined in listing
    8.18
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines a Trainer object
  prefs: []
  type: TYPE_NORMAL
- en: ③ Fits the model
  prefs: []
  type: TYPE_NORMAL
- en: The training code in listing 8.18 generates the output shown in figure 8.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F14_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 Output of the Lightning Flash training process
  prefs: []
  type: TYPE_NORMAL
- en: Note the output includes the validation accuracy and training accuracy for the
    model. Figure 8.15 shows the deep learning with tabular data stack from the example
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F15_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 The PyTorch with Lightning Flash stack
  prefs: []
  type: TYPE_NORMAL
- en: The code for the Lightning Flash solution incorporates some very elegant ideas,
    such as being able to specify the training, validation, and test datasets in the
    same object where you define the overall characteristics of the dataset. Overall,
    the API for Lightning Flash is easy to understand. Unfortunately, these benefits
    are undermined because Lightning Flash requires such specific requirements to
    run in Colab. Otherwise, Lightning Flash could have been a favorite, combining
    the simplicity of fastai with the intuitiveness of Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 Comparing the Lightning Flash solution with the Keras solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have now seen four deep learning solutions to the Airbnb NYC listing price
    prediction problem: the Keras solution, the fastai solution, the TabNet solution,
    and, finally, the Lightning Flash solution. In this section we’ll compare the
    Keras solution with the Lightning Flash solution and review the pros and cons
    of each.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen that Lightning Flash has some real advantages for rolling out a fast,
    simple solution. However, the lack of a beaten path to using Lightning Flash in
    Colab is concerning, and one has to wonder how long the elaborate set of installs
    shown in listing 8.15 will continue to make it possible to run Lightning Flash
    experiments in Colab. Table 8.3 shows a summary of the pros and cons of the Keras
    and Lightning Flash solutions to the Airbnb NYC problem.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.3 Summary of the pros and cons of the Keras and TabNet solutions
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Keras | Lightning Flash |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pro | Large community using the framework means that it’s easy to find solutions
    to common problems.Simple summary statement to show the layers in the model |
    Simple statements to define and train the modelDon’t need to explicitly define
    a pipeline—just need to identify the categorical and continuous columns. |'
  prefs: []
  type: TYPE_TB
- en: '| Con | No built-in support for tabular data, which means the model definition
    needs to be hand-coded. | To get working in Colab, you need a very specific order
    and level of installs. Does not seem to be widely used, at least not with Colab.Out
    of the box, test accuracy was worse than other solutions. |'
  prefs: []
  type: TYPE_TB
- en: So far in this chapter we have applied three deep learning with tabular data
    stacks to solve the Airbnb NYC price prediction problem and compared the pros
    and cons of each solution. In the next section, we review an overall comparison
    of all the stacks we exercised in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Overall comparison of the stacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have looked at a total of four deep learning for tabular data stacks. Table
    8.4 summarizes the performance of all of these stacks on a default Colab setup,
    with a standard GPU Colab runtime option selected for the deep learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.4 Summary of accuracy and running time of the deep learning with tabular
    data stacks (plus XGBoost) for the Airbnb NYC listing price prediction problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Test Accuracy | Notebook running time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow with Keras | 81% | 16 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| fastai with PyTorch | 83% | 69 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| TabNet with PyTorch | 81% | 568 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Lightning Flash with PyTorch | TBD | 14 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost | 79% | 14 seconds |'
  prefs: []
  type: TYPE_TB
- en: It is important to note here that for the purposes of this comparison we didn’t
    do any tuning of any of the approaches. We wanted to have a genuine “apples-to-apples”
    comparison of how the approaches compare to the Keras baseline without additional
    tuning. In subsequent chapters, we will discuss some of the tuning that can be
    done to get optimal results from a deep learning solution.
  prefs: []
  type: TYPE_NORMAL
- en: For accuracy, fastai is slightly better than the other stacks. For execution
    time, Lightning Flash is best, and TabNet is the slowest. So what is the best
    stack to use for tabular data? First, if we compare XGBoost with any of the deep
    learning solutions, which is best? We will answer this question in more detail
    in chapter 9, but we can say now that if it’s a straight-up comparison between
    deep learning and gradient boosting, for the sake of simplicity and overall performance
    “out of the box,” gradient boosting approaches like XGBoost are currently better
    than any of the deep learning solutions for most datasets. There are use cases
    where it is worthwhile to explore one of the deep learning solutions, and we will
    review these use cases in chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do decide to use a deep learning solution for a tabular data problem,
    of the four we explored so far, which one should you use? Here is our overall
    advice:'
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to deep learning and are primarily interested in exploring a
    solution without needing to immediately implement the solution in production,
    fastai is the best choice. It is the simplest stack to use, and it has a big enough
    user community that you are not likely to be stuck with a problem that nobody
    has seen before. Fastai includes many convenience features to make it easy to
    work with tabular data, so you can rapidly prototype your solution. However, if
    you need to move your solution rapidly to production, fastai is probably not your
    best choice because it is not commonly used in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are already comfortable with deep learning and you need to get an application
    into production, we recommend the Keras stack. First, TensorFlow, the low-level
    component of the stack, is the deep learning framework that is used most commonly
    in industry. Second, Keras has a huge user community. While Keras does not yet
    have native support for tabular data the same way that fastai does, Keras is high-level
    enough to lend itself to tabular data problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This advice may change as more people use deep learning with tabular data. One
    of the other stacks, such as TabNet on PyTorch, could mature and become the default
    choice for deep learning with tabular data. However, looking at the current state
    of the art, we recommend fastai for beginners and explorers and Keras for people
    who are more experienced and need to proceed to production rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section we will discuss the stacks that we didn’t explore in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 The stacks we didn’t explore
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have asked why we chose three particular stacks (fastai, PyTorch TabNet,
    and Lightning Flash) to solve the Airbnb NYC problem and didn’t explore the other
    options, such as TabNet on TensorFlow, SAINT, or PyTorch Tabular. In this section,
    we’ll look at this question and see what the answer tells us about the options
    that we have for deep learning with tabular data stacks. Figure 8.16 shows the
    deep learning with tabular data stacks that we didn’t explore.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F16_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 The stacks we didn’t explore
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of characteristics that the unexplored stacks have in common:'
  prefs: []
  type: TYPE_NORMAL
- en: All of the unexplored stacks involve dedicated tabular data libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We were not able to get any of the unexplored stacks to work in Colab, the environment
    that we used to exercise the code examples in this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our inability to get the unexplored stacks to work in Colab could be due to
    a number of reasons. There could theoretically be some limitations with Colab.
    However, Colab is a very common environment for exploration, so it’s not a good
    sign if a library cannot be coaxed to life in Colab. All of the stacks featured
    “hello world” examples for exercising the stack, and for all the unexplored stacks
    these examples generated errors, mostly to do with contradictory Python library
    prerequisites. It’s possible that if we had been more patient, or looked a little
    harder, we would have been able to work through the errors to get the basic examples
    to work in Colab. Of the stacks that we did explore, Keras, fastai, and PyTorch
    TabNet all worked “out of the box” in Colab. Lighting Flash, on the other hand,
    did require some tweaking before it worked in Colab.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between Lightning Flash and the unexplored stacks is that it
    was clear that other people had tried to get Lightning Flash to work in Colab,
    and we could find informal documentation that showed exactly what we needed to
    do to get it to work in Colab. If your goal is to solve a tabular data problem
    with deep learning, you want to focus on the problem, not on fiddling with conda
    and pip installing boutique levels of libraries to avoid incompatibilities. By
    that criteria, Keras, fastai, PyTorch TabNet, and, to a lesser extent, Lightning
    Flash are viable choices for deep learning with tabular data in Colab. SAINT,
    DeepTables, PyTorch Tabular, and TabNet on TensorFlow are not viable choices for
    exploration in Colab because they don’t work right away, and the recipes to get
    them to work either don’t exist or are not easy to find.
  prefs: []
  type: TYPE_NORMAL
- en: While it is disappointing that we weren’t able to exercise the Airbnb NYC price
    prediction problem with more of the dedicated deep learning with tabular data
    libraries, we were still able to accomplish the goal of this chapter by exploring
    three deep learning with tabular data stacks. Figure 8.17 shows all the stacks
    that we were able to explore.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F17_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 The stacks we explored
  prefs: []
  type: TYPE_NORMAL
- en: 'The stacks we explored in this chapter, plus TensorFlow with Keras, present
    a well-rounded set of options:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow with Keras is a rock-solid stack with a huge community of users.
    For just about any problem that you encounter with this stack, you can bet that
    somebody else has hit the problem and posted a solution. The stack works flawlessly
    in Colab, so doing initial investigation is easy. TensorFlow with Keras is commonly
    used in production for all kinds of applications. On the downside, Keras does
    not have built-in support for tabular data, so you need to be prepared to write
    some custom code to tackle tabular data problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch with fastai is designed to be easy to get started with, and it works
    flawlessly in Colab, so you can expect to prototype deep learning with tabular
    data problems with this stack with minimal hassle. fastai treats tabular data
    as a first-class citizen, so you get built-in support for dealing with categorical
    and continuous features and you don’t need to worry about hand-coding a pipeline
    to ensure that when you feed data to the trained model to get a prediction, the
    data goes through the same transformations as the data used to train the model.
    On the downside, you can pay a price for the simplicity of fastai code. The automated
    steps that fastai takes can lead to some hard-to-debug problems (such as the problem
    we described in this chapter where the wrong kind of model gets trained if the
    target column isn’t explicitly converted to a string type), and you need to be
    prepared to dig deep into the fastai API if you want to go off the beaten path
    provided by fastai’s tabular data structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch with TabNet distinguishes itself among the tabular-data-specific libraries
    by working in Colab without any fuss. Like fastai, with TabNet you can define
    and train a deep learning model on tabular data with just a handful of lines of
    code. Unlike fastai, TabNet uses conventional APIs that are easy for anybody who
    has used Scikit-learn to understand. Compared to the other stacks, TabNet took
    longer to train the model. Also, as a library specifically designed for tabular
    data, TabNet has a smaller user community than fastai and Keras, which means that
    if you run into problems it will be less likely that somebody else has already
    hit them and documented a fix on Stack Overflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch with Lightning Flash trains the model quickly and has a simple API,
    once you get it to work. Lightning has a large community—not as big as Keras but
    bigger than fastai. However, the niche of using Lightning Flash on Colab to do
    tabular data problems is not that big, and we were only just able to get Lightning
    Flash to work on Colab.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter we have explored a set of deep learning with tabular data stacks,
    compared the pros and cons of each approach, and discussed why we left some other
    stacks unexplored. In the next chapter we are going to review the best practices
    for deep learning with tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two low-level deep learning frameworks: TensorFlow and PyTorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow is used more frequently in industry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch is the predominant deep learning framework for research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras is a high-level API for TensorFlow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastai is both a general-purpose, high-level API for PyTorch and a tabular data
    library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch Lightning is a high-level API that abstracts some of the details of
    PyTorch. Lightning Flash is a tabular data library based on Lightning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightning Flash and fastai each provide some of the same benefits for PyTorch
    that Keras does for TensorFlow by abstracting aspects of the underlying PyTorch
    framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TabNet is a tabular data library that is available for both TensorFlow and PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAINT is a tabular data library for TensorFlow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch Tabular is a tabular data library for PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of all the choices available, TensorFlow with Keras, PyTorch with fastai, PyTorch
    with TabNet, and PyTorch with Lightning Flash are all valid options for deep learning
    with tabular data on Colab.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
