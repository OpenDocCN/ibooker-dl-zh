<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Ensemble Learning and Random Forests"><div class="chapter" id="ensembles_chapter">
<h1><span class="label">Chapter 6. </span>Ensemble Learning and Random Forests</h1>


<p>Suppose<a data-type="indexterm" data-primary="ensemble learning" id="xi_ensemblelearning638_1"/> you pose a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert’s answer. This is called the <em>wisdom of the crowd</em>.<a data-type="indexterm" data-primary="wisdom of the crowd" id="id1729"/> Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an <em>ensemble</em>;<a data-type="indexterm" data-primary="ensemble methods" id="id1730"/> thus, this technique is called <em>ensemble learning</em>, and an ensemble learning algorithm is called an <em>ensemble method</em>.</p>

<p>As an example of an ensemble method, you can train a group of decision tree<a data-type="indexterm" data-primary="decision trees" data-secondary="ensemble method" id="id1731"/> classifiers, each on a different random subset of the training set. You can then obtain the predictions of all the individual trees, and the class that gets the most votes is the ensemble’s prediction (see the last exercise in <a data-type="xref" href="ch05.html#trees_chapter">Chapter 5</a>). Such an ensemble of decision trees is called a <em>random forest</em>, and despite its simplicity, this is one of the most powerful machine learning algorithms available today.</p>

<p>As discussed in <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a>, you will often use ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor. In fact, the winning solutions in machine learning competitions often involve several ensemble methods—most famously in the <a href="https://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize competition</a>. There are some downsides, however: ensemble learning requires much more computing resources than using a single model (both for training and for inference), it can be more complex to deploy and manage, and the predictions are harder to interpret. But the pros often outweigh the cons.</p>

<p>In this chapter we will examine the most popular ensemble methods, including voting classifiers, bagging and pasting ensembles, random forests, boosting, and stacking ensembles.</p>






<section data-type="sect1" data-pdf-bookmark="Voting Classifiers"><div class="sect1" id="id102">
<h1>Voting Classifiers</h1>

<p>Suppose<a data-type="indexterm" data-primary="classification" data-secondary="voting classifiers" id="xi_classificationvotingclassifiers6128_1"/><a data-type="indexterm" data-primary="ensemble learning" data-secondary="voting classifiers" id="xi_ensemblelearningvotingclassifiers6128_1"/><a data-type="indexterm" data-primary="voting classifiers" id="xi_votingclassifiers6128_1"/> you have trained a few classifiers, each one achieving about 80% accuracy. You may have a logistic regression classifier, an SVM classifier, a random forest classifier, a <em>k</em>-nearest neighbors classifier, and perhaps a few more (see <a data-type="xref" href="#voting_classifier_training_diagram">Figure 6-1</a>).</p>

<figure><div id="voting_classifier_training_diagram" class="figure">
<img src="assets/hmls_0601.png" alt="Diagram illustrating the training of diverse classifiers, including logistic regression, SVM, random forest, and others, highlighting their role in ensemble learning." width="1134" height="606"/>
<h6><span class="label">Figure 6-1. </span>Training diverse classifiers</h6>
</div></figure>

<p>A very simple way to create an even better classifier is to aggregate the predictions of each classifier: the class that gets the most votes is the ensemble’s prediction. This majority-vote classifier is called a <em>hard voting</em> classifier<a data-type="indexterm" data-primary="classification" data-secondary="hard voting classifiers" id="id1732"/><a data-type="indexterm" data-primary="hard voting classifiers" id="id1733"/> (see <a data-type="xref" href="#voting_classifier_prediction_diagram">Figure 6-2</a>).</p>

<figure><div id="voting_classifier_prediction_diagram" class="figure">
<img src="assets/hmls_0602.png" alt="Diagram illustrating a hard voting classifier, where diverse predictors contribute their predictions, with the majority vote determining the ensemble's final prediction for a new instance." width="1113" height="697"/>
<h6><span class="label">Figure 6-2. </span>Hard voting classifier predictions</h6>
</div></figure>

<p class="pagebreak-before">Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a <em>weak learner</em><a data-type="indexterm" data-primary="weak learners" id="id1734"/> (meaning it does only slightly better than random guessing), the ensemble can still be a <em>strong learner</em><a data-type="indexterm" data-primary="strong learners" id="id1735"/> (achieving high accuracy), provided there are a sufficient number of weak learners in the ensemble and they are sufficiently diverse (i.e., if they focus on different aspects of the data and make different kinds of errors).</p>

<p>How is this possible? The following analogy can help shed some light on this mystery. Suppose you have a slightly biased coin that has a 51% chance of coming up heads and 49% chance of coming up tails. If you toss it 1,000 times, you will generally get more or less 510 heads and 490 tails, and hence a majority of heads. If you do the math, you will find that the probability of obtaining a majority of heads after 1,000 tosses is close to 75%. The more you toss the coin, the higher the probability (e.g., with 10,000 tosses, the probability climbs over 97%). This is due to the <em>law of large numbers</em>:<a data-type="indexterm" data-primary="law of large numbers" id="id1736"/> as you keep tossing the coin, the ratio of heads gets closer and closer to the probability of heads (51%). <a data-type="xref" href="#law_of_large_numbers_plot">Figure 6-3</a> shows 10 series of biased coin tosses. You can see that as the number of tosses increases, the ratio of heads approaches 51%. Eventually all 10 series end up so close to 51% that they are consistently above 50%.</p>

<figure><div id="law_of_large_numbers_plot" class="figure">
<img src="assets/hmls_0603.png" alt="A line chart illustrating 10 series of biased coin tosses, showing that as the number of tosses increases, the heads ratio approaches 51%, demonstrating the law of large numbers." width="2272" height="910"/>
<h6><span class="label">Figure 6-3. </span>The law of large numbers</h6>
</div></figure>

<p>Similarly, suppose you build an ensemble containing 1,000 classifiers that are individually correct only 51% of the time (barely better than random guessing). If you predict the majority voted class, you can hope for up to 75% accuracy! However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case because they are trained on the same data. They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble’s accuracy.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy. You can also play with the model hyperparameters to get diverse models, or train the models on different subsets of the data, as we will see.</p>
</div>

<p>Scikit-Learn provides a <code translate="no">VotingClassifier</code> class<a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.VotingClassifier" id="id1737"/> that’s quite easy to use: just give it a list of name/predictor pairs, and use it like a normal classifier. Let’s try it on the moons dataset (introduced in <a data-type="xref" href="ch05.html#trees_chapter">Chapter 5</a>). We will load and split the moons dataset into a training set and a test set, then we’ll create and train a voting classifier composed of three diverse classifiers:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code><code class="p">,</code> <code class="n">VotingClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.30</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>

<code class="n">voting_clf</code> <code class="o">=</code> <code class="n">VotingClassifier</code><code class="p">(</code>
    <code class="n">estimators</code><code class="o">=</code><code class="p">[</code>
        <code class="p">(</code><code class="s1">'lr'</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)),</code>
        <code class="p">(</code><code class="s1">'rf'</code><code class="p">,</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)),</code>
        <code class="p">(</code><code class="s1">'svc'</code><code class="p">,</code> <code class="n">SVC</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">))</code>
    <code class="p">]</code>
<code class="p">)</code>
<code class="n">voting_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p>When you fit a <code translate="no">VotingClassifier</code>, it clones every estimator and fits the clones. The original estimators are available via the <code translate="no">estimators</code> attribute, while the fitted clones are available via the <code translate="no">estimators_</code> attribute. If you prefer a dict rather than a list, you can use <code translate="no">named_estimators</code> or <code translate="no">named_estimators_</code> instead. To begin, let’s look at each fitted classifier’s accuracy on the test set:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">clf</code> <code class="ow">in</code> <code class="n">voting_clf</code><code class="o">.</code><code class="n">named_estimators_</code><code class="o">.</code><code class="n">items</code><code class="p">():</code><code class="w"/>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="n">name</code><code class="p">,</code> <code class="s2">"="</code><code class="p">,</code> <code class="n">clf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">))</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="go">lr = 0.864</code>
<code class="go">rf = 0.896</code>
<code class="go">svc = 0.896</code></pre>

<p class="pagebreak-before">When you call the voting classifier’s <code translate="no">predict()</code><a data-type="indexterm" data-primary="predict()" id="id1738"/> method, it performs hard voting. For example, the voting classifier predicts class 1 for the first instance of the test set, because two out of three classifiers predict that class:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">voting_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">[:</code><code class="mi">1</code><code class="p">])</code><code class="w"/>
<code class="go">array([1])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="p">[</code><code class="n">clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">[:</code><code class="mi">1</code><code class="p">])</code> <code class="k">for</code> <code class="n">clf</code> <code class="ow">in</code> <code class="n">voting_clf</code><code class="o">.</code><code class="n">estimators_</code><code class="p">]</code><code class="w"/>
<code class="go">[array([1]), array([1]), array([0])]</code></pre>

<p>Now let’s look at the performance of the voting classifier on the test set:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">voting_clf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code><code class="w"/>
<code class="go">0.912</code></pre>

<p class="pagebeak-before">There you have it! The voting classifier outperforms all the individual <span class="keep-together">classifiers</span>.</p>

<p>If all classifiers are able to estimate class probabilities<a data-type="indexterm" data-primary="probabilities, estimating" id="id1739"/> (i.e., if they all have a 
<span class="keep-together"><code translate="no">predict_proba()</code></span> method), then you should generally tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called <em>soft voting</em>. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is set the voting classifier’s <code translate="no">voting</code> hyperparameter to <code translate="no">"soft"</code>, and ensure that all classifiers can estimate class probabilities. This is not the case for the <code translate="no">SVC</code> class by default, so you need to set its <code translate="no">probability</code> hyperparameter to <code translate="no">True</code> (this will make the <code translate="no">SVC</code> class use cross-validation to estimate class probabilities, slowing down training, and it will add a <code translate="no">predict_proba()</code> method). Let’s try that:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">voting_clf</code><code class="o">.</code><code class="n">voting</code> <code class="o">=</code> <code class="s2">"soft"</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">voting_clf</code><code class="o">.</code><code class="n">named_estimators</code><code class="p">[</code><code class="s2">"svc"</code><code class="p">]</code><code class="o">.</code><code class="n">probability</code> <code class="o">=</code> <code class="kc">True</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">voting_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">voting_clf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code><code class="w"/>
<code class="go">0.92</code></pre>

<p>We reach 92% accuracy simply by using soft voting<a data-type="indexterm" data-primary="soft voting" id="id1740"/>—not bad!<a data-type="indexterm" data-startref="xi_classificationvotingclassifiers6128_1" id="id1741"/><a data-type="indexterm" data-startref="xi_ensemblelearningvotingclassifiers6128_1" id="id1742"/><a data-type="indexterm" data-startref="xi_votingclassifiers6128_1" id="id1743"/></p>
<div data-type="tip"><h6>Tip</h6>
<p>Soft voting works best when the estimated probabilities are well-calibrated. If they are not, you can use <code translate="no">sklearn.calibration.CalibratedClassifierCV</code> to calibrate them (see <a data-type="xref" href="ch03.html#classification_chapter">Chapter 3</a>).</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Bagging and Pasting"><div class="sect1" id="id103">
<h1>Bagging and Pasting</h1>

<p>One<a data-type="indexterm" data-primary="bagging (bootstrap aggregating)" id="xi_baggingbootstrapaggregating61144_1"/><a data-type="indexterm" data-primary="bootstrap aggregating (bagging)" id="xi_bootstrapaggregatingbagging61144_1"/><a data-type="indexterm" data-primary="ensemble learning" data-secondary="bagging and pasting" id="xi_ensemblelearningbaggingandpasting61144_1"/> way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another way is to use the same training algorithm for every predictor but train them on different random subsets of the training set. When sampling is performed <em>with</em> replacement,⁠<sup><a data-type="noteref" id="id1744-marker" href="ch06.html#id1744">1</a></sup> this method is called <a href="https://homl.info/20"><em>bagging</em></a>⁠<sup><a data-type="noteref" id="id1745-marker" href="ch06.html#id1745">2</a></sup> (short for <em>bootstrap aggregating</em>⁠<sup><a data-type="noteref" id="id1746-marker" href="ch06.html#id1746">3</a></sup>). When sampling is performed <em>without</em> replacement, it is called <a href="https://homl.info/21"><em>pasting</em></a>.⁠<sup><a data-type="noteref" id="id1747-marker" href="ch06.html#id1747">4</a></sup></p>

<p>In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor. This sampling and training process is represented in <a data-type="xref" href="#bagging_training_diagram">Figure 6-4</a>.</p>

<figure><div id="bagging_training_diagram" class="figure">
<img src="assets/hmls_0604.png" alt="Diagram illustrating the bagging process, where multiple predictors are trained on random samples with replacement from the training set." width="1242" height="784"/>
<h6><span class="label">Figure 6-4. </span>Bagging and pasting involve training several predictors on different random samples of the training set</h6>
</div></figure>

<p>Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. For classification, the aggregation function is typically the <em>statistical mode</em><a data-type="indexterm" data-primary="statistical mode" id="id1748"/> (i.e., the most frequent prediction, just like with a hard voting classifier), and for regression it’s usually just the average. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance.⁠<sup><a data-type="noteref" id="id1749-marker" href="ch06.html#id1749">5</a></sup></p>

<p>To get an intuition of why this is the case, imagine that you trained two regressors to predict house prices. The first underestimates the prices by $40,000 on average, while the second overestimates them by $50,000 on average. Assuming these regressors are 100% independent and their predictions follow a normal distribution, if you compute the average of the two predictions, the result will overestimate the prices by only (–40,000 + 50,000)/2 = $5,000 on average: that’s a much lower bias! Similarly, if both predictors have a $10,000 standard deviation (i.e., a variance of 100,000,000), then the average prediction will have a variance<a data-type="indexterm" data-primary="variance" id="id1750"/> of (10,000² + 10,000²)/2² = 50,000,000 (i.e., the standard deviation will be $7,071). The variance is halved!</p>

<p>In practice, the ensemble often ends up with a similar bias but a lower variance than a single predictor trained on the original training set. Therefore it works best with high-variance and low-bias models (e.g., ensembles of decision trees, not ensembles of linear regressors).<a data-type="indexterm" data-primary="decision trees" data-secondary="bagging and pasting" id="id1751"/></p>
<div data-type="tip"><h6>Tip</h6>
<p>Prefer bagging when your dataset is noisy or your model is prone to overfitting (e.g., deep decision tree). Otherwise, prefer pasting as it avoids redundancy during training, making it a bit more computationally efficient.</p>
</div>

<p>As you can see in <a data-type="xref" href="#bagging_training_diagram">Figure 6-4</a>, predictors can all be trained in parallel, via different CPU cores or even different servers. Similarly, predictions can be made in parallel. This is one of the reasons bagging and pasting are such popular methods: they scale very well.</p>








<section data-type="sect2" data-pdf-bookmark="Bagging and Pasting in Scikit-Learn"><div class="sect2" id="id104">
<h2>Bagging and Pasting in Scikit-Learn</h2>

<p>Scikit-Learn<a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="bagging and pasting in" id="xi_ScikitLearnbaggingandpastingin613313_1"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.BaggingClassifier" id="xi_ScikitLearnsklearnensembleBaggingClassifier613313_1"/><a data-type="indexterm" data-primary="BaggingClassifier" id="xi_BaggingClassifier613313_1"/> offers a simple API for both bagging and pasting: the <code translate="no">BaggingClassifier</code> class (or <code translate="no">BaggingRegressor</code> for regression). The following code trains an ensemble of 500 decision tree classifiers:⁠<sup><a data-type="noteref" id="id1752-marker" href="ch06.html#id1752">6</a></sup> each is trained on 100 training instances randomly sampled from the training set with replacement (this is an example of bagging, but if you want to use pasting instead, just set <code translate="no">bootstrap=False</code>). The <code translate="no">n_jobs</code> parameter tells Scikit-Learn the number of CPU cores to use for training and predictions, and 
<span class="keep-together"><span class="keep-together"><code>–1</code></span> tells</span> Scikit-Learn to use all available cores:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">BaggingClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>

<code class="n">bag_clf</code> <code class="o">=</code> <code class="n">BaggingClassifier</code><code class="p">(</code><code class="n">DecisionTreeClassifier</code><code class="p">(),</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>
                            <code class="n">max_samples</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">bag_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A <code translate="no">BaggingClassifier</code> automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities (i.e., if it has a <code translate="no">predict_proba()</code> method), which is the case with decision tree classifiers.</p>
</div>

<p><a data-type="xref" href="#decision_tree_without_and_with_bagging_plot">Figure 6-5</a> <a data-type="indexterm" data-primary="soft voting" id="id1753"/>compares the decision boundary of a single decision tree with the decision boundary of a bagging ensemble of 500 trees (from the preceding code), both trained on the moons dataset. As you can see, the ensemble’s predictions will likely generalize much better than the single decision tree’s predictions: the ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the training set, but the decision boundary is less irregular).</p>

<p>Bagging introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity also means that the predictors end up being less correlated, so the ensemble’s variance is reduced. Overall, bagging often results in better models, which explains why it’s generally preferred. But if you have spare time and CPU power, you can use cross-validation to evaluate both bagging and pasting, and select the one that works best.<a data-type="indexterm" data-startref="xi_ScikitLearnbaggingandpastingin613313_1" id="id1754"/><a data-type="indexterm" data-startref="xi_ScikitLearnsklearnensembleBaggingClassifier613313_1" id="id1755"/><a data-type="indexterm" data-startref="xi_BaggingClassifier613313_1" id="id1756"/></p>

<figure><div id="decision_tree_without_and_with_bagging_plot" class="figure">
<img src="assets/hmls_0605.png" alt="Comparison of a single decision tree's decision boundaries versus those of a bagging ensemble with 500 trees, illustrating variance reduction through bagging." width="2875" height="1069"/>
<h6><span class="label">Figure 6-5. </span>A single decision tree (left) versus a bagging ensemble of 500 trees (right)</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Out-of-Bag Evaluation"><div class="sect2" id="id105">
<h2>Out-of-Bag Evaluation</h2>

<p>With<a data-type="indexterm" data-primary="OOB (out-of-bag) evaluation" id="xi_OOBoutofbagevaluation61565_1"/><a data-type="indexterm" data-primary="out-of-bag (OOB) evaluation" id="xi_outofbagOOBevaluation61565_1"/> bagging, some training instances may be sampled several times for any given predictor, while others may not be sampled at all. By default, a <code translate="no">BaggingClassifier</code> samples <em>m</em> training instances with replacement (<code translate="no">bootstrap=True</code>), where <em>m</em> is the size of the training set. With this process, it can be shown mathematically that only about 63% of the training instances are sampled on average for each predictor.⁠<sup><a data-type="noteref" id="id1757-marker" href="ch06.html#id1757">7</a></sup> The remaining 37% of the training instances that are not sampled are called <em>out-of-bag</em> (OOB) instances. Note that they are not the same 37% for all predictors.</p>

<p>A bagging ensemble can be evaluated using OOB instances, without the need for a separate validation set: indeed, if there are enough estimators, then each instance in the training set will likely be an OOB instance of several estimators, so these estimators can be used to make a fair ensemble prediction for that instance. Once you have a prediction for each instance, you can compute the ensemble’s prediction accuracy (or any other metric).</p>

<p>In Scikit-Learn, you can set <code translate="no">oob_score=True</code> when creating a <code translate="no">BaggingClassifier</code><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.BaggingClassifier" id="id1758"/><a data-type="indexterm" data-primary="BaggingClassifier" id="id1759"/> to request an automatic OOB evaluation after training. The following code demonstrates this. The resulting evaluation score is available in the <code translate="no">oob_score_</code> attribute:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bag_clf</code> <code class="o">=</code> <code class="n">BaggingClassifier</code><code class="p">(</code><code class="n">DecisionTreeClassifier</code><code class="p">(),</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>                            <code class="n">oob_score</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bag_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bag_clf</code><code class="o">.</code><code class="n">oob_score_</code><code class="w"/>
<code class="go">0.896</code></pre>

<p>According to this OOB evaluation, this <code translate="no">BaggingClassifier</code> is likely to achieve about 89.6% accuracy on the test set. Let’s verify this:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code><code class="w"> </code><code class="nn">sklearn.metrics</code><code class="w"> </code><code class="kn">import</code> <code class="n">accuracy_score</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">bag_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">)</code><code class="w"/>
<code class="go">0.92</code></pre>

<p>We get 92.0% accuracy on the test. The OOB evaluation was a bit too pessimistic.</p>

<p>The OOB decision function for each training instance is also available through the <code translate="no">oob_decision_function_</code> attribute. Since the base estimator has a <code translate="no">predict_proba()</code> method, the decision function returns the class probabilities for each training instance. For example, the OOB evaluation estimates that the first training instance has a 67.6% probability of belonging to the positive class and a 32.4% probability of belonging to the negative class:<a data-type="indexterm" data-startref="xi_OOBoutofbagevaluation61565_1" id="id1760"/><a data-type="indexterm" data-startref="xi_outofbagOOBevaluation61565_1" id="id1761"/></p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bag_clf</code><code class="o">.</code><code class="n">oob_decision_function_</code><code class="p">[:</code><code class="mi">3</code><code class="p">]</code>  <code class="c1"># probas for the first 3 instances</code><code class="w"/>
<code class="go">array([[0.32352941, 0.67647059],</code>
<code class="go">       [0.3375    , 0.6625    ],</code>
<code class="go">       [1.        , 0.        ]])</code></pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Random Patches and Random Subspaces"><div class="sect2" id="id106">
<h2>Random Patches and Random Subspaces</h2>

<p>The <code translate="no">BaggingClassifier</code> class supports sampling the features as well. Sampling is controlled by two hyperparameters<a data-type="indexterm" data-primary="bootstrap_features hyperparameter" id="id1762"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="max_features and bootstrap_features" id="id1763"/><a data-type="indexterm" data-primary="max_features hyperparameter" id="id1764"/>: <code translate="no">max_features</code> and <code translate="no">bootstrap_features</code>. They work the same way as <code translate="no">max_samples</code> and <code translate="no">bootstrap</code>, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features.</p>

<p>This technique is particularly useful when you are dealing with high-dimensional inputs (such as images), as it can considerably speed up training. Sampling both training instances and features is called the <a href="https://homl.info/22"><em>random patches</em> method</a>.⁠<sup><a data-type="noteref" id="id1765-marker" href="ch06.html#id1765">8</a></sup> Keeping all training instances (by setting <code translate="no">bootstrap=False</code> and <code translate="no">max_samples=1.0</code>) but sampling features (by setting <code translate="no">bootstrap_features</code> to <code translate="no">True</code> and/or <code translate="no">max_features</code> to a value smaller than <code translate="no">1.0</code>) is called the <a href="https://homl.info/23"><em>random subspaces</em> method</a>.<a data-type="indexterm" data-primary="random subspaces" id="id1766"/>⁠<sup><a data-type="noteref" id="id1767-marker" href="ch06.html#id1767">9</a></sup></p>

<p>Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance.<a data-type="indexterm" data-startref="xi_baggingbootstrapaggregating61144_1" id="id1768"/><a data-type="indexterm" data-startref="xi_bootstrapaggregatingbagging61144_1" id="id1769"/><a data-type="indexterm" data-startref="xi_ensemblelearningbaggingandpasting61144_1" id="id1770"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Random Forests"><div class="sect1" id="id107">
<h1>Random Forests</h1>

<p>As<a data-type="indexterm" data-primary="random forests" id="xi_randomforests62023_1"/> we have discussed, a <a href="https://homl.info/24">random forest</a>⁠<sup><a data-type="noteref" id="id1771-marker" href="ch06.html#id1771">10</a></sup> is an ensemble of decision trees, generally trained via the bagging method (or sometimes pasting), typically with <code translate="no">max_samples</code> set to the size of the training set. Instead of building a <code translate="no">BaggingClassifier</code> and passing it a <code translate="no">DecisionTreeClassifier</code>, you can use the <code translate="no">RandomForestClassifier</code><a data-type="indexterm" data-primary="RandomForestClassifier" id="xi_RandomForestClassifier6202508_1"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.RandomForestClassifier" id="xi_ScikitLearnsklearnensembleRandomForestClassifier6202508_1"/> class, which is more convenient and optimized for decision trees⁠<sup><a data-type="noteref" id="id1772-marker" href="ch06.html#id1772">11</a></sup> (similarly, there is a <code translate="no">RandomForestRegressor</code><a data-type="indexterm" data-primary="RandomForestRegressor" id="id1773"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.RandomForestRegressor" id="id1774"/> class for regression tasks). The following code trains a random forest classifier with 500 trees, each limited to maximum 16 leaf nodes, using all available CPU cores:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>

<code class="n">rnd_clf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">max_leaf_nodes</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code>
                                 <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rnd_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="n">y_pred_rf</code> <code class="o">=</code> <code class="n">rnd_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code></pre>

<p>With a few exceptions, a <code translate="no">RandomForestClassifier</code> has all the hyperparameters of a <code translate="no">DecisionTreeClassifier</code> (to control how trees are grown), plus all the hyperparameters of a <code translate="no">BaggingClassifier</code> to control the ensemble itself.</p>

<p>The <code translate="no">RandomForestClassifier</code> class introduces extra randomness when growing trees: instead of searching for the very best feature when splitting a node (see <a data-type="xref" href="ch05.html#trees_chapter">Chapter 5</a>), it searches for the best feature among a random subset of features. By default, it samples <math alttext="StartRoot n EndRoot">
  <msqrt>
    <mi>n</mi>
  </msqrt>
</math> features (where <em>n</em> is the total number of features). The algorithm results in greater tree diversity, which (again) trades a higher bias for a lower variance, generally yielding an overall better model. So, the following <code translate="no">BaggingClassifier</code> is equivalent to the previous <code translate="no">RandomForestClassifier</code><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.BaggingClassifier" id="id1775"/><a data-type="indexterm" data-primary="BaggingClassifier" id="id1776"/>:<a data-type="indexterm" data-startref="xi_RandomForestClassifier6202508_1" id="id1777"/><a data-type="indexterm" data-startref="xi_ScikitLearnsklearnensembleRandomForestClassifier6202508_1" id="id1778"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">bag_clf</code> <code class="o">=</code> <code class="n">BaggingClassifier</code><code class="p">(</code>
    <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">max_features</code><code class="o">=</code><code class="s2">"sqrt"</code><code class="p">,</code> <code class="n">max_leaf_nodes</code><code class="o">=</code><code class="mi">16</code><code class="p">),</code>
    <code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code></pre>








<section data-type="sect2" data-pdf-bookmark="Extra-Trees"><div class="sect2" id="id108">
<h2>Extra-Trees</h2>

<p>When<a data-type="indexterm" data-primary="extra-trees, random forest" id="id1779"/><a data-type="indexterm" data-primary="extremely randomized trees ensemble (extra-trees)" id="id1780"/><a data-type="indexterm" data-primary="random forests" data-secondary="extra-trees" id="id1781"/> you are growing a tree in a random forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular decision trees do). For this, simply set <code translate="no">splitter="random"</code> when creating a <code translate="no">DecisionTreeClassifier</code>.</p>

<p>A forest of such extremely random trees is called an <a href="https://homl.info/25"><em>extremely randomized trees</em></a>⁠<sup><a data-type="noteref" id="id1782-marker" href="ch06.html#id1782">12</a></sup> (or <em>extra-trees</em> for short) ensemble. Once again, this technique trades more bias for a lower variance, so they may perform better than regular random forests if you encounter overfitting, especially with noisy and/or high-dimensional datasets. Extra-trees classifiers are also much faster to train than regular random forests, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree in a random forest.</p>

<p>You can create an extra-trees classifier using Scikit-Learn’s <code translate="no">ExtraTreesClassifier</code> class. Its API is identical to the <code translate="no">RandomForestClassifier</code><a data-type="indexterm" data-primary="ExtraTreesClassifier" id="id1783"/><a data-type="indexterm" data-primary="ExtraTreesRegressor" id="id1784"/><a data-type="indexterm" data-primary="sklearn" data-secondary="tree.ExtraTreesClassifier" id="id1785"/><a data-type="indexterm" data-primary="sklearn" data-secondary="tree.ExtraTreesRegressor" id="id1786"/> class, except <code translate="no">bootstrap</code> defaults to <code translate="no">False</code>. Similarly, the <code translate="no">ExtraTreesRegressor</code> class has the same API as the <code translate="no">RandomForestRegressor</code> class, except <code translate="no">bootstrap</code> defaults to <code translate="no">False</code>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Just like decision tree classes, random forest classes and extra-trees classes in recent Scikit-Learn versions support missing values natively, no need for an imputer.</p>
</div>
</div></section>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Feature Importance"><div class="sect2" id="id109">
<h2>Feature Importance</h2>

<p>Yet<a data-type="indexterm" data-primary="random forests" data-secondary="feature importance measurement" id="id1787"/> another great quality of random forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average, across all trees in the forest. More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it (see <a data-type="xref" href="ch05.html#trees_chapter">Chapter 5</a>).</p>

<p>Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. You can access the result using the <code translate="no">feature_importances_</code> variable. For example, the following code trains a <code translate="no">RandomForestClassifier</code><a data-type="indexterm" data-primary="RandomForestClassifier" id="id1788"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.RandomForestClassifier" id="id1789"/> on the iris dataset (introduced in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>) and outputs each feature’s importance. It seems that the most important features are the petal length (44%) and width (42%), while sepal length and width are rather unimportant in comparison (11% and 2%, respectively):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code><code class="w"> </code><code class="nn">sklearn.datasets</code><code class="w"> </code><code class="kn">import</code> <code class="n">load_iris</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">(</code><code class="n">as_frame</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">rnd_clf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">rnd_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">score</code><code class="p">,</code> <code class="n">name</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">rnd_clf</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">,</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">columns</code><code class="p">):</code><code class="w"/>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="nb">round</code><code class="p">(</code><code class="n">score</code><code class="p">,</code> <code class="mi">2</code><code class="p">),</code> <code class="n">name</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="go">0.11 sepal length (cm)</code>
<code class="go">0.02 sepal width (cm)</code>
<code class="go">0.44 petal length (cm)</code>
<code class="go">0.42 petal width (cm)</code></pre>

<p>Similarly, if you train a random forest classifier on the MNIST dataset (introduced in <a data-type="xref" href="ch03.html#classification_chapter">Chapter 3</a>) and plot each pixel’s importance, you get the image represented in <a data-type="xref" href="#mnist_feature_importance_plot">Figure 6-6</a>.</p>

<figure class="width-55"><div id="mnist_feature_importance_plot" class="figure">
<img src="assets/hmls_0606.png" alt="Heatmap showing MNIST pixel importance with colors indicating feature significance according to a random forest classifier, where yellow denotes high importance and red denotes low importance." width="1799" height="1314"/>
<h6><span class="label">Figure 6-6. </span>MNIST pixel importance (according to a random forest classifier)</h6>
</div></figure>

<p>Random forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection<a data-type="indexterm" data-primary="feature selection" id="id1790"/>.<a data-type="indexterm" data-startref="xi_randomforests62023_1" id="id1791"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Boosting"><div class="sect1" id="id110">
<h1>Boosting</h1>

<p><em>Boosting</em><a data-type="indexterm" data-primary="boosting" id="xi_boosting626711_1"/><a data-type="indexterm" data-primary="ensemble learning" data-secondary="boosting" id="xi_ensemblelearningboosting626711_1"/><a data-type="indexterm" data-primary="weights" data-secondary="boosting" id="xi_weightsboosting626711_1"/> (originally called <em>hypothesis boosting</em>) refers to any ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are <a href="https://homl.info/26"><em>AdaBoost</em></a>⁠<sup><a data-type="noteref" id="id1792-marker" href="ch06.html#id1792">13</a></sup> (short for <em>adaptive boosting</em>) and <em>gradient boosting</em>. Let’s start with AdaBoost.</p>








<section data-type="sect2" data-pdf-bookmark="AdaBoost"><div class="sect2" id="id111">
<h2>AdaBoost</h2>

<p>One<a data-type="indexterm" data-primary="AdaBoost" id="xi_AdaBoost62704_1"/><a data-type="indexterm" data-primary="adaptive boosting (AdaBoost)" id="xi_adaptiveboostingAdaBoost62704_1"/> way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfit. This results in new predictors focusing more and more on the hard cases. This is the technique used by <span class="keep-together">AdaBoost</span>.</p>

<p>For example, when training an AdaBoost classifier, the algorithm first trains a base classifier (such as a decision tree) and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on (see <a data-type="xref" href="#adaboost_training_diagram">Figure 6-7</a>).</p>

<figure class="width-75"><div id="adaboost_training_diagram" class="figure">
<img src="assets/hmls_0607.png" alt="Diagram illustrating the sequential training process of AdaBoost, showing how instance weights and predictions are updated through stages." width="1149" height="764"/>
<h6><span class="label">Figure 6-7. </span>AdaBoost sequential training with instance weight updates</h6>
</div></figure>

<p><a data-type="xref" href="#boosting_plot">Figure 6-8</a> shows the decision boundaries of five consecutive predictors on the moons dataset (in this example, each predictor is a highly regularized SVM classifier with an RBF kernel).⁠<sup><a data-type="noteref" id="id1793-marker" href="ch06.html#id1793">14</a></sup> The first classifier gets many instances wrong, so their weights get boosted. The second classifier therefore does a better job on these instances, and so on. The plot on the right represents the same sequence of predictors, except that the learning rate is halved (i.e., the misclassified instance weights are boosted much less at every iteration). As you can see, this sequential learning technique has some similarities with gradient descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function<a data-type="indexterm" data-primary="cost function" data-secondary="AdaBoost" id="id1794"/>, AdaBoost adds predictors to the ensemble, gradually making it better.</p>

<figure><div id="boosting_plot" class="figure">
<img src="assets/hmls_0608.png" alt="Two diagrams showing decision boundaries of five consecutive SVM classifiers on the moons dataset, with learning rates 1 and 0.5, illustrating the impact of learning rate on AdaBoost’s performance." width="2875" height="1069"/>
<h6><span class="label">Figure 6-8. </span>Decision boundaries of consecutive predictors</h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>There is one important drawback to this sequential learning technique: training cannot be parallelized since each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bagging or pasting.</p>
</div>

<p>Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.</p>

<p>Let’s take a closer look at the AdaBoost algorithm. Each instance weight <em>w</em><sup>(<em>i</em>)</sup> is initially set to 1/<em>m</em>, so their sum is 1. A first predictor is trained, and its weighted error rate <em>r</em><sub>1</sub> is computed on the training set; see <a data-type="xref" href="#weighted_error_rate">Equation 6-1</a>.</p>
<div id="weighted_error_rate" data-type="equation" class="less_space pagebreak-before">
<h5><span class="label">Equation 6-1. </span>Weighted error rate of the j<sup>th</sup> predictor</h5>
<math display="block">
    <mrow>
        <msub><mi>r</mi><mi>j</mi></msub>
        <mo>=</mo>
        <mstyle scriptlevel="0" displaystyle="true"><mrow>
            <munderover>
                <mo>∑</mo>
                <mstyle scriptlevel="0" displaystyle="false"><mrow>
                    <mfrac linethickness="0pt">
                        <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
                        <mrow>
                            <msubsup>
                                <mover accent="true">
                                    <mi>y</mi><mo>^</mo>
                                </mover>
                                <mi>j</mi>
                                <mrow>
                                    <mo>(</mo><mi>i</mi><mo>)</mo>
                                </mrow>
                            </msubsup>
                            <mo>≠</mo>
                            <msup>
                                <mi>y</mi>
                                <mrow>
                                    <mo>(</mo><mi>i</mi><mo>)</mo>
                                </mrow>
                            </msup>
                        </mrow>
                    </mfrac>
                </mrow></mstyle>
                <mi>m</mi>
            </munderover>
            <msup>
                <mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow>
            </msup>
        </mrow></mstyle>
        <mspace width="1.em"/>
        <mtext>where</mtext>
        <mspace width="4.pt"/>
        <msubsup>
            <mover accent="true"><mi>y</mi><mo>^</mo></mover>
            <mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow>
        </msubsup>
        <mspace width="4.pt"/><mtext>is</mtext><mspace width="4.pt"/>
        <mtext>the</mtext><mspace width="4.pt"/>
        <msup><mi>j</mi><mtext>th</mtext></msup>
        <mspace width="4.pt"/><mtext>predictor’s</mtext><mspace width="4.pt"/>
        <mtext>prediction</mtext><mspace width="4.pt"/><mtext>for</mtext>
        <mspace width="4.pt"/><mtext>the</mtext><mspace width="4.pt"/>
        <msup>
            <mi>i</mi>
            <mtext>th</mtext>
        </msup>
        <mspace width="4.pt"/><mtext>instance</mtext>
    </mrow>
</math>
</div>

<p>The predictor’s weight <em>α</em><sub><em>j</em></sub> is then computed using <a data-type="xref" href="#predictor_weight">Equation 6-2</a>, where <em>η</em> is the learning rate hyperparameter (defaults to 1).⁠<sup><a data-type="noteref" id="id1795-marker" href="ch06.html#id1795">15</a></sup> The more accurate the predictor is, the higher its weight will be. If it is just guessing randomly, then its weight will be close to zero. However, if it is most often wrong (i.e., less accurate than random guessing), then its weight will be negative.</p>
<div id="predictor_weight" data-type="equation">
<h5><span class="label">Equation 6-2. </span>Predictor weight</h5>
<math alttext="dollar-sign alpha Subscript j Baseline equals eta log StartFraction 1 minus r Subscript j Baseline Over r Subscript j Baseline EndFraction dollar-sign">
  <mrow>
    <msub><mi>α</mi> <mi>j</mi> </msub>
    <mo>=</mo>
    <mi>η</mi>
    <mspace width="0.222222em"/>
    <mtext>log</mtext>
    <mspace width="0.222222em"/>
    <mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>r</mi> <mi>j</mi> </msub></mrow> <msub><mi>r</mi> <mi>j</mi> </msub></mfrac>
  </mrow>
</math>
</div>

<p>Next, the AdaBoost algorithm updates the instance weights, using <a data-type="xref" href="#instance_weight_update">Equation 6-3</a>, which boosts the weights of the misclassified instances and encourages the next predictor to pay more attention to them.</p>
<div id="instance_weight_update" data-type="equation">
<h5><span class="label">Equation 6-3. </span>Weight update rule</h5>
<math alttext="dollar-sign StartLayout 1st Row 1st Column Blank 2nd Column for i equals 1 comma 2 comma period period period comma m 2nd Row 1st Column Blank 2nd Column w Superscript left-parenthesis i right-parenthesis Baseline left-arrow StartLayout Enlarged left-brace 1st Row  w Superscript left-parenthesis i right-parenthesis Baseline if ModifyingAbove y With caret Superscript left-parenthesis i right-parenthesis Baseline equals y Superscript left-parenthesis i right-parenthesis Baseline 2nd Row  w Superscript left-parenthesis i right-parenthesis Baseline exp left-parenthesis alpha Subscript j Baseline right-parenthesis if ModifyingAbove y With caret Superscript left-parenthesis i right-parenthesis Baseline not-equals y Superscript left-parenthesis i right-parenthesis EndLayout EndLayout dollar-sign">
  <mtable displaystyle="true">
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mtext>for</mtext>
          <mspace width="0.222222em"/>
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
          <mo lspace="0%" rspace="0%">,</mo>
          <mn>2</mn>
          <mo lspace="0%" rspace="0%">,</mo>
          <mo lspace="0%" rspace="0%">.</mo>
          <mo lspace="0%" rspace="0%">.</mo>
          <mo lspace="0%" rspace="0%">.</mo>
          <mo lspace="0%" rspace="0%">,</mo>
          <mi>m</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mo>←</mo>
          <mfenced separators="" open="{" close="">
            <mtable>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mtext>if</mtext>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <msup><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                    <mo>=</mo>
                    <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                    <mspace width="0.222222em"/>
                    <mtext>exp</mtext>
                    <mrow>
                      <mo>(</mo>
                      <msub><mi>α</mi> <mi>j</mi> </msub>
                      <mo>)</mo>
                    </mrow>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <mtext>if</mtext>
                    <mspace width="0.222222em"/>
                    <mspace width="0.222222em"/>
                    <msup><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                    <mo>≠</mo>
                    <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>Then all the instance weights are normalized to ensure that their sum is once again 1 (i.e., they are divided by <math alttext="sigma-summation Underscript i equals 1 Overscript m Endscripts w Superscript left-parenthesis i right-parenthesis">
  <mrow>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </msubsup>
    <msup><mrow><mi>w</mi></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
  </mrow>
</math>).</p>

<p>Finally, a new predictor is trained using the updated weights, and the whole process is repeated: the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained, and so on. The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.</p>

<p>To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights <em>α</em><sub><em>j</em></sub>. The predicted class is the one that receives the majority of weighted votes (see <a data-type="xref" href="#adaboost_prediction">Equation 6-4</a>).</p>
<div id="adaboost_prediction" data-type="equation" class="less_space pagebreak-before"><h5><span class="label">Equation 6-4. </span>AdaBoost predictions</h5>
<math display="block">
  <mrow>
    <mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mrow>
      <mo>(</mo>
      <mi mathvariant="bold">x</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mo form="prefix">argmax</mo> <mi>k</mi></munder>
    <mrow>
      <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle> <mstyle scriptlevel="1" displaystyle="false"><mrow><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>j</mi> </msub><mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow><mo>=</mo><mi>k</mi></mrow></mstyle></mfrac> <mi>N</mi> </munderover>
      <msub><mi>α</mi> <mi>j</mi> </msub>
    </mrow>
    <mspace width="1.em"/>
    <mtext>where</mtext>
    <mspace width="4.pt"/>
    <mi>N</mi>
    <mspace width="4.pt"/>
    <mtext>is</mtext>
    <mspace width="4.pt"/>
    <mtext>the</mtext>
    <mspace width="4.pt"/>
    <mtext>number</mtext>
    <mspace width="4.pt"/>
    <mtext>of</mtext>
    <mspace width="4.pt"/>
    <mtext>predictors</mtext>
  </mrow>
</math>
</div>

<p>Scikit-Learn uses a multiclass version of AdaBoost called<a data-type="indexterm" data-primary="SAMME" id="id1796"/> <a href="https://homl.info/27"><em>SAMME</em></a>⁠<sup><a data-type="noteref" id="id1797-marker" href="ch06.html#id1797">16</a></sup> (which stands for <em>Stagewise Additive Modeling using a Multiclass Exponential loss function</em>). When there are just two classes, SAMME is equivalent to AdaBoost.</p>

<p>The following code trains an AdaBoost classifier based on 30 <em>decision stumps</em><a data-type="indexterm" data-primary="decision stumps" id="id1798"/> using Scikit-Learn’s <code translate="no">AdaBoostClassifier</code> class (as you might expect, there is also an 
<span class="keep-together"><code translate="no">AdaBoostRegressor</code></span> class). A decision stump is a decision tree with <code translate="no">max_depth=1</code>—in other words, a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the <code translate="no">AdaBoostClassifier</code> class:<a data-type="indexterm" data-startref="xi_AdaBoost62704_1" id="id1799"/><a data-type="indexterm" data-primary="AdaBoostClassifier" id="id1800"/><a data-type="indexterm" data-startref="xi_adaptiveboostingAdaBoost62704_1" id="id1801"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.AdaBoostClassifier" id="id1802"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">AdaBoostClassifier</code>

<code class="n">ada_clf</code> <code class="o">=</code> <code class="n">AdaBoostClassifier</code><code class="p">(</code>
    <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code>
    <code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">algorithm</code><code class="o">=</code><code class="s2">"SAMME"</code><code class="p">)</code>
<code class="n">ada_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Gradient Boosting"><div class="sect2" id="id112">
<h2>Gradient Boosting</h2>

<p>Another<a data-type="indexterm" data-primary="gradient boosting" id="xi_gradientboosting64408_1"/> very popular boosting algorithm is <a href="https://homl.info/28"><em>gradient boosting</em></a>.⁠<sup><a data-type="noteref" id="id1803-marker" href="ch06.html#id1803">17</a></sup> Just like AdaBoost, gradient boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the <em>residual errors</em><a data-type="indexterm" data-primary="residual errors" id="xi_residualerrors6440656_1"/> made by the previous predictor.</p>

<p>Let’s go through a simple regression example, using decision trees as the base predictors; this is called <em>gradient tree boosting</em>, or <em>gradient boosted regression trees</em> (GBRT).<a data-type="indexterm" data-primary="GBRT (gradient boosted regression trees)" id="xi_GBRTgradientboostedregressiontrees6442179_1"/><a data-type="indexterm" data-primary="gradient boosted regression trees (GBRT)" id="xi_gradientboostedregressiontreesGBRT6442179_1"/><a data-type="indexterm" data-primary="gradient tree boosting" id="xi_gradienttreeboosting6442179_1"/> First, let’s generate a noisy quadratic dataset and fit a <code translate="no">DecisionTreeRegressor</code><a data-type="indexterm" data-primary="DecisionTreeRegressor" id="xi_DecisionTreeRegressor6442261_1"/> to it:<a data-type="indexterm" data-primary="sklearn" data-secondary="tree.DecisionTreeRegressor" id="xi_ScikitLearnsklearntreeDecisionTreeRegressor6442268_1"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeRegressor</code>

<code class="n">m</code> <code class="o">=</code> <code class="mi">100</code>  <code class="c1"># number of instances</code>
<code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">default_rng</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">random</code><code class="p">((</code><code class="n">m</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code> <code class="o">-</code> <code class="mf">0.5</code>
<code class="n">noise</code> <code class="o">=</code> <code class="mf">0.05</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">standard_normal</code><code class="p">(</code><code class="n">m</code><code class="p">)</code>
<code class="n">y</code> <code class="o">=</code> <code class="mi">3</code> <code class="o">*</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]</code> <code class="o">**</code> <code class="mi">2</code> <code class="o">+</code> <code class="n">noise</code>  <code class="c1"># y = 3x² + Gaussian noise</code>

<code class="n">tree_reg1</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">tree_reg1</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>Next, we’ll train a second <code translate="no">DecisionTreeRegressor</code> on the residual errors made by the first predictor:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">y2</code> <code class="o">=</code> <code class="n">y</code> <code class="o">-</code> <code class="n">tree_reg1</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">tree_reg2</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">43</code><code class="p">)</code>
<code class="n">tree_reg2</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y2</code><code class="p">)</code></pre>

<p>And then we’ll train a third regressor on the residual errors made by the second predictor:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">y3</code> <code class="o">=</code> <code class="n">y2</code> <code class="o">-</code> <code class="n">tree_reg2</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">tree_reg3</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">44</code><code class="p">)</code>
<code class="n">tree_reg3</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y3</code><code class="p">)</code></pre>

<p>Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all the trees:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="o">-</code><code class="mf">0.4</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.5</code><code class="p">]])</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">sum</code><code class="p">(</code><code class="n">tree</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code> <code class="k">for</code> <code class="n">tree</code> <code class="ow">in</code> <code class="p">(</code><code class="n">tree_reg1</code><code class="p">,</code> <code class="n">tree_reg2</code><code class="p">,</code> <code class="n">tree_reg3</code><code class="p">))</code><code class="w"/>
<code class="go">array([0.57356534, 0.0405142 , 0.66914249])</code></pre>

<p><a data-type="xref" href="#gradient_boosting_plot">Figure 6-9</a> <a data-type="indexterm" data-startref="xi_DecisionTreeRegressor6442261_1" id="id1804"/>represents the predictions of these three trees in the left column, and the ensemble’s predictions in the right column. In the first row, the ensemble has just one tree, so its predictions are exactly the same as the first tree’s predictions. In the second row, a new tree is trained on the residual errors of the first tree. On the right you can see that the ensemble’s predictions are equal to the sum of the predictions of the first two trees. Similarly, in the third row another tree is trained on the residual errors of the second tree. You can see that the ensemble’s predictions gradually get better as trees are added to the ensemble.<a data-type="indexterm" data-startref="xi_residualerrors6440656_1" id="id1805"/><a data-type="indexterm" data-startref="xi_ScikitLearnsklearntreeDecisionTreeRegressor6442268_1" id="id1806"/></p>

<p>You can use Scikit-Learn’s <code translate="no">GradientBoostingRegressor</code><a data-type="indexterm" data-primary="GradientBoostingRegressor" id="xi_GradientBoostingRegressor648955_1"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.GradientBoostingRegressor" id="xi_ScikitLearnsklearnensembleGradientBoostingRegressor648955_1"/> class to train GBRT ensembles more easily (there’s also a <code translate="no">GradientBoostingClassifier</code> class for classification). Much like the <code translate="no">RandomForestRegressor</code> class, it has hyperparameters<a data-type="indexterm" data-primary="hyperparameters" data-secondary="decision tree" id="xi_hyperparametersdecisiontree6489237_1"/> to control the growth of decision trees (e.g., <code translate="no">max_depth</code>, <code translate="no">min_samples_leaf</code>), as well as 
<span class="keep-together">hyperparameters</span> to control the ensemble training, such as the number of trees (<code translate="no">n_estimators</code>). The following code creates the same ensemble as the previous one:</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="less_space pagebreak-before"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">GradientBoostingRegressor</code>

<code class="n">gbrt</code> <code class="o">=</code> <code class="n">GradientBoostingRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                                 <code class="n">learning_rate</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">gbrt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<figure><div id="gradient_boosting_plot" class="figure">
<img src="assets/hmls_0609.png" alt="Diagram illustrating gradient boosting; the left column shows individual predictors trained on residuals, and the right column shows the ensemble's progressive predictions." width="3175" height="3169"/>
<h6><span class="label">Figure 6-9. </span>In this depiction of gradient boosting, the first predictor (top left) is trained normally, then each consecutive predictor (middle left and lower left) is trained on the previous predictor’s residuals; the right column shows the resulting ensemble’s predictions</h6>
</div></figure>

<p>The <code translate="no">learning_rate</code> hyperparameter<a data-type="indexterm" data-primary="learning_rate hyperparameter" id="id1807"/> scales the contribution of each tree. If you set it to a low value, such as <code translate="no">0.05</code>, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called <em>shrinkage</em>.<a data-type="indexterm" data-primary="regularization" data-secondary="shrinkage" id="id1808"/><a data-type="indexterm" data-primary="shrinkage" id="id1809"/> <a data-type="xref" href="#gbrt_learning_rate_plot">Figure 6-10</a> shows two GBRT ensembles trained with different hyperparameters: the one on the left does not have enough trees to fit the training set, while the one on the right has about the right amount. If we added more trees, the GBRT would start to overfit the training set. As usual, you can use cross-validation to find the optimal learning rate, using <code translate="no">GridSearchCV</code> or <code translate="no">RandomizedSearchCV</code>.</p>

<figure><div id="gbrt_learning_rate_plot" class="figure">
<img src="assets/hmls_0610.png" alt="Comparison of GBRT ensemble predictions with insufficient predictors (left graph) versus sufficient predictors (right graph)." width="2875" height="1069"/>
<h6><span class="label">Figure 6-10. </span>GBRT ensembles with not enough predictors (left) and just enough (right)</h6>
</div></figure>

<p>To find the optimal number of trees, you could also perform cross-validation, but there’s a simpler way: if you set the <code translate="no">n_iter_no_change</code> hyperparameter<a data-type="indexterm" data-primary="n_iter_no_change hyperparameter" id="id1810"/> to an integer value, say 10, then the <code translate="no">GradientBoostingRegressor</code> will automatically stop adding more trees during training if it sees that the last 10 trees didn’t help. This is simply early stopping<a data-type="indexterm" data-primary="early stopping regularization" id="id1811"/><a data-type="indexterm" data-primary="regularization" data-secondary="early stopping" id="id1812"/> (introduced in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>), but with a little bit of patience: it tolerates having no progress for a few iterations before it stops. Let’s train the ensemble using early stopping:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">gbrt_best</code> <code class="o">=</code> <code class="n">GradientBoostingRegressor</code><code class="p">(</code>
    <code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>
    <code class="n">n_iter_no_change</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">gbrt_best</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>If you set <code translate="no">n_iter_no_change</code> too low, training may stop too early and the model will underfit. But if you set it too high, it will overfit instead. We also set a fairly small learning rate and a high number of estimators, but the actual number of estimators in the trained ensemble is much lower, thanks to early stopping:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gbrt_best</code><code class="o">.</code><code class="n">n_estimators_</code><code class="w"/>
<code class="go">53</code></pre>

<p>When <code translate="no">n_iter_no_change</code> is set, the <code translate="no">fit()</code> method automatically splits the training set into a smaller training set and a validation set: this allows it to evaluate the model’s performance each time it adds a new tree. The size of the validation set is controlled by the <code translate="no">validation_fraction</code> hyperparameter<a data-type="indexterm" data-primary="validation_fraction hyperparameter" id="id1813"/>, which is 10% by default. The <code translate="no">tol</code> hyperparameter<a data-type="indexterm" data-primary="tol hyperparameter" id="id1814"/> determines the maximum performance improvement that still counts as negligible. It defaults to 0.0001.</p>

<p>The <code translate="no">GradientBoostingRegressor</code> class also supports a <code translate="no">subsample</code> hyperparameter<a data-type="indexterm" data-primary="subsample hyperparameter" id="id1815"/>, which specifies the fraction of training instances to be used for training each tree. For example, if <code translate="no">subsample=0.25</code>, then each tree is trained on 25% of the training instances, selected randomly. As you can probably guess by now, this technique trades a higher bias for a lower variance. It also speeds up training considerably. This is called <em>stochastic gradient boosting</em>.<a data-type="indexterm" data-startref="xi_GBRTgradientboostedregressiontrees6442179_1" id="id1816"/><a data-type="indexterm" data-startref="xi_gradientboostedregressiontreesGBRT6442179_1" id="id1817"/><a data-type="indexterm" data-startref="xi_GradientBoostingRegressor648955_1" id="id1818"/><a data-type="indexterm" data-startref="xi_gradienttreeboosting6442179_1" id="id1819"/><a data-type="indexterm" data-startref="xi_hyperparametersdecisiontree6489237_1" id="id1820"/><a data-type="indexterm" data-startref="xi_ScikitLearnsklearnensembleGradientBoostingRegressor648955_1" id="id1821"/><a data-type="indexterm" data-primary="stochastic gradient boosting" id="id1822"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Histogram-Based Gradient Boosting"><div class="sect2" id="id113">
<h2>Histogram-Based Gradient Boosting</h2>

<p>Scikit-Learn<a data-type="indexterm" data-primary="computational complexity" data-secondary="histogram-based gradient boosting" id="xi_computationalcomplexityhistogrambasedgradientboosting653713_1"/><a data-type="indexterm" data-primary="HGB (histogram-based gradient boosting)" id="xi_HGBhistogrambasedgradientboosting653713_1"/><a data-type="indexterm" data-primary="histogram-based gradient boosting (HGB)" id="xi_histogrambasedgradientboostingHGB653713_1"/> also provides another GBRT implementation, optimized for large datasets: <em>histogram-based gradient boosting</em> (HGB). It works by binning the input features, replacing them with integers. The number of bins is controlled by the <code translate="no">max_bins</code> hyperparameter, which defaults to 255 and cannot be set any higher than this. Binning can greatly reduce the number of possible thresholds that the training algorithm needs to evaluate. Moreover, working with integers makes it possible to use faster and more memory-efficient data structures. And the way the bins are built removes the need for sorting the features when training each tree.</p>

<p>As a result, this implementation has a computational complexity of <em>O</em>(<em>b</em>×<em>m</em>) instead of <em>O</em>(<em>n</em>×<em>m</em>×log(<em>m</em>)), where <em>b</em> is the number of bins, <em>m</em> is the number of training instances, and <em>n</em> is the number of features. In practice, this means that HGB can train hundreds of times faster than regular GBRT on large datasets. However, binning causes a precision loss, which acts as a regularizer: depending on the dataset, this may help reduce overfitting, or it may cause underfitting.</p>

<p>Scikit-Learn provides two classes for HGB: <code translate="no">HistGradientBoostingRegressor</code> and <code translate="no">HistGradientBoostingClassifier</code><a data-type="indexterm" data-primary="HistGradientBoostingClassifier" id="id1823"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.HistGradientBoostingClassifier" id="id1824"/>.<a data-type="indexterm" data-primary="HistGradientBoostingRegressor" id="id1825"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.HistGradientBoostingRegressor" id="id1826"/> They’re similar to <code translate="no">GradientBoostingRegressor</code> and <code translate="no">GradientBoostingClassifier</code>, with a few notable differences:</p>

<ul>
<li>
<p>Early stopping is automatically activated if the number of instances is greater than 10,000. You can turn early stopping always on or always off by setting the <code translate="no">early_stopping</code> hyperparameter to <code translate="no">True</code> or <code translate="no">False</code>.</p>
</li>
<li>
<p>Subsampling is not supported.</p>
</li>
<li>
<p><code translate="no">n_estimators</code> is renamed to <code translate="no">max_iter</code>.</p>
</li>
<li>
<p>The only decision tree hyperparameters that can be tweaked are <code translate="no">max_leaf_nodes</code>, <code translate="no">min_samples_leaf</code>, <code translate="no">max_depth</code>, and <code translate="no">max_features</code>.</p>
</li>
</ul>

<p>The HGB classes support missing values natively, as well as categorical features. This simplifies preprocessing quite a bit. However, the categorical features must be represented as integers ranging from 0 to a number lower than <code translate="no">max_bins</code>. You can use an <code translate="no">OrdinalEncoder</code><a data-type="indexterm" data-primary="OrdinalEncoder" id="id1827"/><a data-type="indexterm" data-primary="sklearn" data-secondary="preprocessing.OrdinalEncoder" id="id1828"/> for this. For example, here’s how to build and train a complete pipeline for the California housing dataset introduced in <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>
<code class="kn">from</code> <code class="nn">sklearn.compose</code> <code class="kn">import</code> <code class="n">make_column_transformer</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">HistGradientBoostingRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">OrdinalEncoder</code>

<code class="n">hgb_reg</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code>
    <code class="n">make_column_transformer</code><code class="p">((</code><code class="n">OrdinalEncoder</code><code class="p">(),</code> <code class="p">[</code><code class="s2">"ocean_proximity"</code><code class="p">]),</code>
                            <code class="n">remainder</code><code class="o">=</code><code class="s2">"passthrough"</code><code class="p">,</code>
                            <code class="n">force_int_remainder_cols</code><code class="o">=</code><code class="kc">False</code><code class="p">),</code>
    <code class="n">HistGradientBoostingRegressor</code><code class="p">(</code><code class="n">categorical_features</code><code class="o">=</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="p">)</code>
<code class="n">hgb_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">housing</code><code class="p">,</code> <code class="n">housing_labels</code><code class="p">)</code></pre>

<p>The whole pipeline is almost as short as the imports! No need for an imputer, scaler, or a one-hot encoder, it’s really convenient. Note that <code translate="no">categorical_features</code> must be set to the categorical column indices (or a Boolean array). Without any hyperparameter tuning, this model yields an RMSE of about 47,600, which is not too bad.</p>

<p>In short, HGB is a great choice when you have a fairly large dataset, especially when it contains categorical features and missing values: it performs well, doesn’t require much preprocessing work, and training is fast. However, it can be a bit less accurate than GBRT, due to the binning, so you might want to try both.<a data-type="indexterm" data-startref="xi_boosting626711_1" id="id1829"/><a data-type="indexterm" data-startref="xi_computationalcomplexityhistogrambasedgradientboosting653713_1" id="id1830"/><a data-type="indexterm" data-startref="xi_ensemblelearningboosting626711_1" id="id1831"/><a data-type="indexterm" data-startref="xi_HGBhistogrambasedgradientboosting653713_1" id="id1832"/><a data-type="indexterm" data-startref="xi_histogrambasedgradientboostingHGB653713_1" id="id1833"/><a data-type="indexterm" data-startref="xi_weightsboosting626711_1" id="id1834"/></p>
<div data-type="tip"><h6>Tip</h6>
<p>Several other optimized implementations of gradient boosting are available in the Python ML ecosystem: in particular, <a href="https://github.com/dmlc/xgboost">XGBoost</a>, <a href="https://catboost.ai">CatBoost</a>, and <a href="https://lightgbm.readthedocs.io">LightGBM</a>. These libraries have been around for several years. They are all specialized for gradient boosting, their APIs are very similar to Scikit-Learn’s, and they provide many additional features, including hardware acceleration using GPUs; you should definitely check them out! Moreover, <a href="https://ydf.readthedocs.io">Yggdrasil Decision Forests (YDF)</a> provides optimized implementations of a variety of random forest algorithms.<a data-type="indexterm" data-startref="xi_gradientboosting64408_1" id="id1835"/></p>
</div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Stacking"><div class="sect1" id="id114">
<h1>Stacking</h1>

<p>The<a data-type="indexterm" data-primary="ensemble learning" data-secondary="stacking" id="xi_ensemblelearningstacking65794_1"/><a data-type="indexterm" data-primary="stacking (stacked generalization)" id="xi_stackingstackedgeneralization65794_1"/> last ensemble method we will discuss in this chapter is called <em>stacking</em> (short for <a href="https://homl.info/29"><em>stacked generalization</em></a>).⁠<sup><a data-type="noteref" id="id1836-marker" href="ch06.html#id1836">18</a></sup> It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation? <a data-type="xref" href="#blending_prediction_diagram">Figure 6-11</a> shows such an ensemble performing a regression task on a new instance. Each of the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor (called a <em>blender</em>, or a <em>meta learner</em>)<a data-type="indexterm" data-primary="blender and blending, in stacking" id="id1837"/><a data-type="indexterm" data-primary="meta learner" id="id1838"/> takes these predictions as inputs and makes the final prediction (3.0).</p>

<figure class="width-40"><div id="blending_prediction_diagram" class="figure">
<img src="assets/hmls_0611.png" alt="Diagram illustrating how blending combines predictions from multiple predictors to produce a singular prediction for a new instance." width="628" height="801"/>
<h6><span class="label">Figure 6-11. </span>Aggregating predictions using a blending predictor</h6>
</div></figure>

<p>To train the blender, you first need to build the blending training set. You can use <code translate="no">cross_val_predict()</code><a data-type="indexterm" data-primary="cross_val_predict()" id="id1839"/><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.cross_val_predict()" id="id1840"/> on every predictor in the ensemble to get out-of-sample predictions for each instance in the original training set (<a data-type="xref" href="#blending_layer_training_diagram">Figure 6-12</a>), and use these as the input features to train the blender; the targets can simply be copied from the original training set. Note that regardless of the number of features in the original training set (just one in this example), the blending training set will contain one input feature per predictor (three in this example). Once the blender is trained, the base predictors must be retrained one last time on the full original training set (since <code translate="no">cross_val_predict()</code> does not give access to the trained estimators).</p>

<p>It is actually possible to train several different blenders this way (e.g., one using linear regression, another using random forest regression) to get a whole layer of blenders, and then add another blender on top of that to produce the final prediction, as shown in <a data-type="xref" href="#multi_layer_blending_diagram">Figure 6-13</a>. You may be able to squeeze out a few more drops of performance by doing this, but it will cost you in both training time and system complexity.</p>

<figure class="width-55"><div id="blending_layer_training_diagram" class="figure">
<img src="assets/hmls_0612.png" alt="Diagram illustrating the process of training a blender in a stacking ensemble, showing how predictions from multiple models form a blending training set for the final blending step." width="870" height="968"/>
<h6><span class="label">Figure 6-12. </span>Training the blender in a stacking ensemble</h6>
</div></figure>

<figure class="width-40"><div id="multi_layer_blending_diagram" class="figure">
<img src="assets/hmls_0613.png" alt="Diagram illustrating a multilayer stacking ensemble with three layers, showing how predictions are processed through interconnected blenders from a new instance input to produce a final prediction." width="605" height="989"/>
<h6><span class="label">Figure 6-13. </span>Predictions in a multilayer stacking ensemble</h6>
</div></figure>

<p>Scikit-Learn provides two classes for stacking ensembles: <code translate="no">StackingClassifier</code><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.StackingClassifier" id="id1841"/><a data-type="indexterm" data-primary="StackingClassifier" id="id1842"/> and <code translate="no">StackingRegressor</code>. For example, we can replace the <code translate="no">VotingClassifier</code><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.StackingRegressor" id="id1843"/><a data-type="indexterm" data-primary="StackingRegressor" id="id1844"/> we used at the beginning of this chapter on the moons dataset with a <code translate="no">StackingClassifier</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">StackingClassifier</code>

<code class="n">stacking_clf</code> <code class="o">=</code> <code class="n">StackingClassifier</code><code class="p">(</code>
    <code class="n">estimators</code><code class="o">=</code><code class="p">[</code>
        <code class="p">(</code><code class="s1">'lr'</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)),</code>
        <code class="p">(</code><code class="s1">'rf'</code><code class="p">,</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)),</code>
        <code class="p">(</code><code class="s1">'svc'</code><code class="p">,</code> <code class="n">SVC</code><code class="p">(</code><code class="n">probability</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">))</code>
    <code class="p">],</code>
    <code class="n">final_estimator</code><code class="o">=</code><code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">43</code><code class="p">),</code>
    <code class="n">cv</code><code class="o">=</code><code class="mi">5</code>  <code class="c1"># number of cross-validation folds</code>
<code class="p">)</code>
<code class="n">stacking_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p>For each predictor, the stacking classifier will call <code translate="no">predict_proba()</code> if available; if not it will fall back to <code translate="no">decision_function()</code> or, as a last resort, call <code translate="no">predict()</code>. If you don’t provide a final estimator, <code translate="no">StackingClassifier</code> will use <code translate="no">LogisticRegression</code> and <code translate="no">StackingRegressor</code> will use <code translate="no">RidgeCV</code>.</p>

<p>If you evaluate this stacking model on the test set, you will find 92.8% accuracy, which is a bit better than the voting classifier using soft voting, which got 92%. Depending on your use case, this may or may not be worth the extra complexity and computational cost (since there’s an extra model to run after all the others).<a data-type="indexterm" data-startref="xi_ensemblelearningstacking65794_1" id="id1845"/><a data-type="indexterm" data-startref="xi_stackingstackedgeneralization65794_1" id="id1846"/></p>

<p>In conclusion, ensemble methods are versatile, powerful, and fairly simple to use. They can overfit if you’re not careful, but that’s true of every powerful model. <a data-type="xref" href="#ensemble_summary_table">Table 6-1</a> summarizes all the techniques we discussed in this chapter, and when to use each one.<a data-type="indexterm" data-primary="AdaBoost" id="id1847"/><a data-type="indexterm" data-primary="bagging (bootstrap aggregating)" id="id1848"/><a data-type="indexterm" data-primary="extra-trees, random forest" id="id1849"/><a data-type="indexterm" data-primary="gradient boosting" id="id1850"/><a data-type="indexterm" data-primary="hard voting classifiers" id="id1851"/><a data-type="indexterm" data-primary="HGB (histogram-based gradient boosting)" id="id1852"/><a data-type="indexterm" data-primary="histogram-based gradient boosting (HGB)" id="id1853"/><a data-type="indexterm" data-primary="pasting" id="id1854"/><a data-type="indexterm" data-primary="random forests" id="id1855"/><a data-type="indexterm" data-primary="soft voting" id="id1856"/><a data-type="indexterm" data-primary="adaptive boosting (AdaBoost)" id="id1857"/><a data-type="indexterm" data-primary="bootstrap aggregating (bagging)" id="id1858"/></p>
<table id="ensemble_summary_table">
<caption><span class="label">Table 6-1. </span>When to use each ensemble learning method</caption>
<thead>
<tr>
<th>Ensemble method</th>
<th>When to use it</th>
<th>Example use cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Hard voting</p></td>
<td><p>Balanced classification dataset with multiple strong but diverse classifiers.</p></td>
<td><p>Spam detection, sentiment analysis, disease classification</p></td>
</tr>
<tr>
<td><p>Soft voting</p></td>
<td><p>Classification dataset with probabilistic models, where confidence scores matter.</p></td>
<td><p>Medical diagnosis, credit risk analysis, fake news detection</p></td>
</tr>
<tr>
<td><p>Bagging</p></td>
<td><p>Structured or semi-structured dataset with high variance and overfitting-prone models.</p></td>
<td><p>Financial risk modeling, ecommerce recommendation</p></td>
</tr>
<tr>
<td><p>Pasting</p></td>
<td><p>Structured or semi-structured dataset where more independent models are needed.</p></td>
<td><p>Customer segmentation, protein classification</p></td>
</tr>
<tr>
<td><p>Random forest</p></td>
<td><p>High-dimensional structured datasets with potentially noisy features.</p></td>
<td><p>Customer churn prediction, genetic data analysis, fraud detection</p></td>
</tr>
<tr>
<td><p>Extra-trees</p></td>
<td><p>Large structured datasets with many features, where speed is critical and reducing variance is important.</p></td>
<td><p>Real-time fraud detection, sensor data analysis</p></td>
</tr>
<tr>
<td><p>AdaBoost</p></td>
<td><p>Small to medium-sized, low-noise, structured datasets with weak learners (e.g., decision stumps), where interpretability is helpful.</p></td>
<td><p>Credit scoring, anomaly detection, predictive maintenance</p></td>
</tr>
<tr>
<td><p>Gradient boosting</p></td>
<td><p>Medium to large structured datasets where high predictive power is required, even at the cost of extra tuning.</p></td>
<td><p>Housing price prediction, risk assessment, demand forecasting</p></td>
</tr>
<tr>
<td><p>Histogram-based gradient boosting (HGB)</p></td>
<td><p>Large structured datasets where training speed and scalability are key.</p></td>
<td><p>Click-through rate prediction, ranking algorithms, real-time bidding in advertising</p></td>
</tr>
<tr>
<td><p>Stacking</p></td>
<td><p>Complex, high-dimensional datasets where combining multiple diverse models can maximize accuracy.</p></td>
<td><p>Recommendation engines, autonomous vehicle decision-making, Kaggle competitions</p></td>
</tr>
</tbody>
</table>
<div data-type="tip"><h6>Tip</h6>
<p>Random forests, AdaBoost, GBRT, and HGB are among the first models you should test for most machine learning tasks, particularly with heterogeneous tabular data. Moreover, as they require very little preprocessing, they’re great for getting a prototype up and running quickly.<a data-type="indexterm" data-startref="xi_ensemblelearning638_1" id="id1859"/></p>
</div>

<p>So far, we have looked only at supervised learning techniques. In the next chapter, we will turn to the most common unsupervised learning task: dimensionality reduction.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id730">
<h1>Exercises</h1>
<ol>
<li>
<p>If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?</p>
</li>
<li>
<p>What is the difference between hard and soft voting classifiers?</p>
</li>
<li>
<p>Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?</p>
</li>
<li>
<p>What is the benefit of out-of-bag evaluation?</p>
</li>
<li>
<p>What makes extra-trees ensembles more random than regular random forests? How can this extra randomness help? Are extra-trees classifiers slower or faster than regular random forests?</p>
</li>
<li>
<p>If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak, and how?</p>
</li>
<li>
<p>If your gradient boosting ensemble overfits the training set, should you increase or decrease the learning rate?</p>
</li>
<li>
<p>Load the MNIST dataset (introduced in <a data-type="xref" href="ch03.html#classification_chapter">Chapter 3</a>), and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a random forest classifier, an extra-trees classifier, and an SVM classifier. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?</p>
</li>
<li>
<p>Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. Train a classifier on this new training set. Congratulations—you have just trained a blender, and together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classifier you trained earlier? Now try again using a <code translate="no">StackingClassifier</code> instead. Do you get better performance? If so, why?</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1744"><sup><a href="ch06.html#id1744-marker">1</a></sup> Imagine picking a card randomly from a deck of cards, writing it down, then placing it back in the deck before picking the next card: the same card could be sampled multiple times.</p><p data-type="footnote" id="id1745"><sup><a href="ch06.html#id1745-marker">2</a></sup> Leo Breiman, “Bagging Predictors”, <em>Machine Learning</em> 24, no. 2 (1996): 123–140.</p><p data-type="footnote" id="id1746"><sup><a href="ch06.html#id1746-marker">3</a></sup> In statistics, resampling with replacement is<a data-type="indexterm" data-primary="bootstrapping" id="id1860"/> called <em>bootstrapping</em>.</p><p data-type="footnote" id="id1747"><sup><a href="ch06.html#id1747-marker">4</a></sup> Leo Breiman, “Pasting Small Votes for Classification in Large Databases and On-Line”, <em>Machine Learning</em> 36, no. 1–2 (1999): 85–103.</p><p data-type="footnote" id="id1749"><sup><a href="ch06.html#id1749-marker">5</a></sup> Bias and variance were introduced in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>. Recall that a high bias means that the average prediction is far off target, while a high variance means that the predictions are very scattered. We want both to be low.</p><p data-type="footnote" id="id1752"><sup><a href="ch06.html#id1752-marker">6</a></sup> <code translate="no">max_samples</code> can alternatively be set to a float between 0.0 and 1.0, in which case the max number of sampled instances is equal to the size of the training set times <code translate="no">max_samples</code>.</p><p data-type="footnote" id="id1757"><sup><a href="ch06.html#id1757-marker">7</a></sup> As <em>m</em> grows, this ratio approaches 1 – exp(–1) ≈ 63%.</p><p data-type="footnote" id="id1765"><sup><a href="ch06.html#id1765-marker">8</a></sup> Gilles Louppe and Pierre Geurts, “Ensembles on Random Patches”,<a data-type="indexterm" data-primary="random patches" id="id1861"/> <em>Lecture Notes in Computer Science</em> 7523 (2012): 346–361.</p><p data-type="footnote" id="id1767"><sup><a href="ch06.html#id1767-marker">9</a></sup> Tin Kam Ho, “The Random Subspace Method for Constructing Decision Forests”, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 20, no. 8 (1998): 832–844.</p><p data-type="footnote" id="id1771"><sup><a href="ch06.html#id1771-marker">10</a></sup> Tin Kam Ho, “Random Decision Forests”, <em>Proceedings of the Third International Conference on Document Analysis and Recognition</em> 1 (1995): 278.</p><p data-type="footnote" id="id1772"><sup><a href="ch06.html#id1772-marker">11</a></sup> The <code translate="no">BaggingClassifier</code> class remains useful if you want a bag of something other than decision trees.</p><p data-type="footnote" id="id1782"><sup><a href="ch06.html#id1782-marker">12</a></sup> Pierre Geurts et al., “Extremely Randomized Trees”, <em>Machine Learning</em> 63, no. 1 (2006): 3–42.</p><p data-type="footnote" id="id1792"><sup><a href="ch06.html#id1792-marker">13</a></sup> Yoav Freund and Robert E. Schapire, “A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting”, <em>Journal of Computer and System Sciences</em> 55, no. 1 (1997): 119–139.</p><p data-type="footnote" id="id1793"><sup><a href="ch06.html#id1793-marker">14</a></sup> This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost; they are slow and tend to be unstable with it.</p><p data-type="footnote" id="id1795"><sup><a href="ch06.html#id1795-marker">15</a></sup> The original AdaBoost algorithm does not use a learning rate hyperparameter.</p><p data-type="footnote" id="id1797"><sup><a href="ch06.html#id1797-marker">16</a></sup> For more details, see Ji Zhu et al., “Multi-Class AdaBoost”, <em>Statistics and Its Interface</em> 2, no. 3 (2009): 349–360.</p><p data-type="footnote" id="id1803"><sup><a href="ch06.html#id1803-marker">17</a></sup> Gradient boosting was first introduced in Leo Breiman’s <a href="https://homl.info/arcing">1997 paper</a> “Arcing the Edge” and was further developed in the <a href="https://homl.info/gradboost">1999 paper</a> “Greedy Function Approximation: A Gradient Boosting Machine” by Jerome H. Friedman.</p><p data-type="footnote" id="id1836"><sup><a href="ch06.html#id1836-marker">18</a></sup> David H. Wolpert, “Stacked Generalization”, <em>Neural Networks</em> 5, no. 2 (1992): 241–259.</p></div></div></section></div></div></body></html>