["```py\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs([...])  # make the blobs: y contains the cluster IDs, but we\n                          # will not use them; that's what we want to predict\nk = 5\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(X)\n```", "```py\n>>> y_pred `array([4, 0, 1, ..., 2, 1, 0], dtype=int32)`\n`>>>` `y_pred` `is` `kmeans``.``labels_` `` `True` ``\n```", "```py```", "````py`` ````", "````` We can also take a look at the five centroids that the algorithm found:    ```py >>> kmeans.cluster_centers_ `array([[-2.80389616,  1.80117999],`  `[ 0.20876306,  2.25551336],`  `[-2.79290307,  2.79641063],`  `[-1.46679593,  2.28585348],`  `[-2.80037642,  1.30082566]])` ```   ```py`````", "``` >>> import numpy as np `>>>` `X_new` `=` `np``.``array``([[``0``,` `2``],` `[``3``,` `2``],` `[``-``3``,` `3``],` `[``-``3``,` `2.5``]])` ```", "``` ```", "``````py``` ``````", "``````py` If you plot the cluster’s decision boundaries, you get a Voronoi tessellation: see [Figure 8-3](#voronoi_plot), where each centroid is represented with an ⓧ.  ![Diagram showing Voronoi tessellation with k-means centroids marked, highlighting decision boundaries between clusters.](assets/hmls_0803.png)  ###### Figure 8-3\\. k-means decision boundaries (Voronoi tessellation)    The vast majority of the instances were clearly assigned to the appropriate cluster, but a few instances were probably mislabeled, especially near the boundary between the top-left cluster and the central cluster. Indeed, the *k*-means algorithm does not behave very well when the blobs have very different diameters because all it cares about when assigning an instance to a cluster is the distance to the centroid.    Instead of assigning each instance to a single cluster, which is called *hard clustering*, it can be useful to give each instance a score per cluster, which is called *soft clustering*. The score can be the distance between the instance and the centroid or a similarity score (or affinity), such as the Gaussian radial basis function we used in [Chapter 2](ch02.html#project_chapter). In the `KMeans` class, the `transform()` method measures the distance from each instance to every centroid:    ``` >>> kmeans.transform(X_new).round(2) `array([[2.81, 0.33, 2.9 , 1.49, 2.89],`  `[5.81, 2.8 , 5.85, 4.48, 5.84],`  `[1.21, 3.29, 0.29, 1.69, 1.71],`  `[0.73, 3.22, 0.36, 1.55, 1.22]])` ```py   ``````", "``` good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]]) kmeans = KMeans(n_clusters=5, init=good_init, random_state=42) kmeans.fit(X) ```", "``` >>> kmeans.inertia_ `211.59853725816828` ```", "```` The `score()` method returns the negative inertia (it’s negative because a predictor’s `score()` method must always respect Scikit-Learn’s “greater is better” rule—if a predictor is better than another, its `score()` method should return a greater score):    ```py >>> kmeans.score(X) `-211.59853725816828` ```   ``An important improvement to the *k*-means algorithm, *k-means++*, was proposed in a [2006 paper](https://homl.info/37) by David Arthur and Sergei Vassilvitskii.⁠^([3](ch08.html#id1983)) They introduced a smarter initialization step that tends to select centroids that are distant from one another. This change makes the *k*-means algorithm much more likely to locate all important clusters, and less likely to converge to a suboptimal solution (just like spreading out fishing boats increases the chance of locating more schools of fish). The paper showed that the additional computation required for the smarter initialization step is well worth it because it makes it possible to drastically reduce the number of times the algorithm needs to be run to find the optimal solution. The *k*-means++ initialization algorithm works like this:    1.  Take one centroid **c**^((1)), chosen uniformly at random from the dataset.           2.  Take a new centroid **c**^((*i*)), choosing an instance **x**^((*i*)) with probability $upper D left-parenthesis bold x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis squared$ / $sigma-summation Underscript j equals 1 Overscript m Endscripts upper D left-parenthesis bold x Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis squared$ , where D(**x**^((*i*))) is the distance between the instance **x**^((*i*)) and the closest centroid that was already chosen. This probability distribution ensures that instances farther away from already chosen centroids are much more likely to be selected as centroids.           3.  Repeat the previous step until all *k* centroids have been chosen.              When you set `init=\"k-means++\"` (which is the default), the `KMeans` class actually uses a variant of *k*-means++ called *greedy k-means++*: instead of sampling a single centroid at each iteration, it samples multiple and picks the best one. When using this algorithm, `n_init` defaults to 1.`` ```py`  ````", "```py` ### Accelerated k-means and mini-batch k-means    Another improvement to the *k*-means algorithm was proposed in a [2003 paper](https://homl.info/38) by Charles Elkan.⁠^([4](ch08.html#id1989)) On some large datasets with many clusters, the algorithm can be accelerated by avoiding many unnecessary distance calculations. Elkan achieved this by exploiting the triangle inequality (i.e., that a straight line is always the shortest distance between two points⁠^([5](ch08.html#id1990))) and by keeping track of lower and upper bounds for distances between instances and centroids. However, Elkan’s algorithm does not always accelerate training, and sometimes it can even slow down training significantly; it depends on the dataset. Still, if you want to give it a try, set `algorithm=\"elkan\"`.    Yet another important variant of the *k*-means algorithm was proposed in a [2010 paper](https://homl.info/39) by David Sculley.⁠^([6](ch08.html#id1993)) Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the `MiniBatchKMeans` class, which you can use just like the `KMeans` class:    ```", "```py    If the dataset does not fit in memory, the simplest option is to use the `memmap` class, as we did for incremental PCA in [Chapter 7](ch07.html#dimensionality_chapter). Alternatively, you can pass one mini-batch at a time to the `partial_fit()` method, but this will require much more work, since you will need to perform multiple initializations and select the best one yourself.    ### Finding the optimal number of clusters    So far, we’ve set the number of clusters *k* to 5 because it was obvious by looking at the data that this was the correct number of clusters. But in general, it won’t be so easy to know how to set *k*, and the result might be quite bad if you set it to the wrong value. As you can see in [Figure 8-6](#bad_n_clusters_plot), for this dataset setting *k* to 3 or 8 results in fairly bad models.    You might be thinking that you could just pick the model with the lowest inertia. Unfortunately, it is not that simple. The inertia for *k* = 3 is about 653.2, which is much higher than for *k* = 5 (211.7). But with *k* = 8, the inertia is just 127.1\\. The inertia is not a good performance metric when trying to choose *k* because it keeps getting lower as we increase *k*. Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be. Let’s plot the inertia as a function of *k*. When we do this, the curve often contains an inflexion point called the *elbow* (see [Figure 8-7](#inertia_vs_k_plot)).  ![Diagram showing clustering outcomes; with _k_ = 3, distinct clusters merge, and with _k_ = 8, some clusters split into multiple parts.](assets/hmls_0806.png)  ###### Figure 8-6\\. Bad choices for the number of clusters: when k is too small, separate clusters get merged (left), and when k is too large, some clusters get chopped into multiple pieces (right)  ![Line graph showing inertia decreasing sharply with increasing clusters \\(k\\), with an elbow at \\(k = 4\\).](assets/hmls_0807.png)  ###### Figure 8-7\\. Plotting the inertia as a function of the number of clusters *k*    As you can see, the inertia drops very quickly as we increase *k* up to 4, but then it decreases much more slowly as we keep increasing *k*. This curve has roughly the shape of an arm, and there is an elbow at *k* = 4\\. So, if we did not know better, we might think 4 was a good choice: any lower value would be dramatic, while any higher value would not help much, and we might just be splitting perfectly good clusters in half for no good reason.    This technique for choosing the best value for the number of clusters is rather coarse. A more precise (but also more computationally expensive) approach is to use the *silhouette score*, which is the mean *silhouette coefficient* over all the instances. An instance’s silhouette coefficient is equal to (*b* – *a*) / max(*a*, *b*), where *a* is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance) and *b* is the mean nearest-cluster distance (i.e., the mean distance to the instances of the next closest cluster, defined as the one that minimizes *b*, excluding the instance’s own cluster). The silhouette coefficient can vary between –1 and +1\\. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary; finally, a coefficient close to –1 means that the instance may have been assigned to the wrong cluster.    To compute the silhouette score, you can use Scikit-Learn’s `silhouette_score()` function, giving it all the instances in the dataset and the labels they were assigned:    ```", "```py   `` `Let’s compare the silhouette scores for different numbers of clusters (see [Figure 8-8](#silhouette_score_vs_k_plot)).  ![Line plot showing silhouette scores for various cluster counts, with peaks at _k_ = 4 and 5, indicating optimal clustering choices.](assets/hmls_0808.png)  ###### Figure 8-8\\. Selecting the number of clusters *k* using the silhouette score    As you can see, this visualization is much richer than the previous one: although it confirms that *k* = 4 is a very good choice, it also highlights the fact that *k* = 5 is quite good as well, and much better than *k* = 6 or 7\\. This was not visible when comparing inertias.    An even more informative visualization is obtained when we plot every instance’s silhouette coefficient, sorted by the clusters they are assigned to and by the value of the coefficient. This is called a *silhouette diagram* (see [Figure 8-9](#silhouette_analysis_plot)). Each diagram contains one knife shape per cluster. The shape’s height indicates the number of instances in the cluster, and its width represents the sorted silhouette coefficients of the instances in the cluster (wider is better).    The vertical dashed lines represent the mean silhouette score for each number of clusters. When most of the instances in a cluster have a lower coefficient than this score (i.e., if many of the instances stop short of the dashed line, ending to the left of it), then the cluster is rather bad since this means its instances are much too close to other clusters. Here we can see that when *k* = 3 or 6, we get bad clusters. But when *k* = 4 or 5, the clusters look pretty good: most instances extend beyond the dashed line, to the right and closer to 1.0\\. When *k* = 4, the cluster at index 0 (at the bottom) is rather big. When *k* = 5, all clusters have similar sizes. So, even though the overall silhouette score from *k* = 4 is slightly greater than for *k* = 5, it seems like a good idea to use *k* = 5 to get clusters of similar sizes.  ![Silhouette diagrams comparing cluster quality for different values of _k_, showing that clusters are more balanced when _k_ = 5, despite slightly higher overall scores for _k_ = 4.](assets/hmls_0809.png)  ###### Figure 8-9\\. Analyzing the silhouette diagrams for various values of *k*` `` ```", "```py`` ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "``````py```` ```py``````", "``````py``````", "```py```", "````py````", "```py`` ```", "```py```", "````py````", "```py```", "````py````", "```py ## Limits of k-Means    Despite its many merits, most notably being fast and scalable, *k*-means is not perfect. As we saw, it is necessary to run the algorithm several times to avoid suboptimal solutions, plus you need to specify the number of clusters, which can be quite a hassle. Moreover, *k*-means does not behave very well when the clusters have varying sizes, different densities, or nonspherical shapes. For example, [Figure 8-10](#bad_kmeans_plot) shows how *k*-means clusters a dataset containing three ellipsoidal clusters of different dimensions, densities, and orientations.    As you can see, neither of these solutions is any good. The solution on the left is better, but it still chops off 25% of the middle cluster and assigns it to the cluster on the right. The solution on the right is just terrible, even though its inertia is lower. So, depending on the data, different clustering algorithms may perform better. On these types of elliptical clusters, Gaussian mixture models work great.  ![Diagram comparing two k-means clustering attempts on ellipsoidal data, highlighting poor performance in clustering accuracy.](assets/hmls_0810.png)  ###### Figure 8-10\\. k-means fails to cluster these ellipsoidal blobs properly    ###### Tip    It is important to scale the input features (see [Chapter 2](ch02.html#project_chapter)) before you run *k*-means, or the clusters may be very stretched and *k*-means will perform poorly. Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally helps *k*-means.    Now let’s look at a few ways we can benefit from clustering. We will use *k*-means, but feel free to experiment with other clustering algorithms.    ## Using Clustering for Image Segmentation    *Image segmentation* is the task of partitioning an image into multiple segments. There are several variants:    *   In *color segmentation*, pixels with a similar color get assigned to the same segment. This is sufficient in many applications. For example, if you want to analyze satellite images to measure how much total forest area there is in a region, color segmentation may be just fine.           *   In *semantic segmentation*, all pixels that are part of the same object type get assigned to the same segment. For example, in a self-driving car’s vision system, all pixels that are part of a pedestrian’s image might be assigned to the “pedestrian” segment (there would be one segment containing all the pedestrians).           *   In *instance segmentation*, all pixels that are part of the same individual object are assigned to the same segment. In this case there would be a different segment for each pedestrian.              The state of the art in semantic or instance segmentation today is achieved using complex architectures based on convolutional neural networks (see [Chapter 12](ch12.html#cnn_chapter)) or vision transformers (see [Chapter 16](ch16.html#vit_chapter)). In this chapter we are going to focus on the (much simpler) color segmentation task, using *k*-means.    We’ll start by importing the Pillow package (successor to the Python Imaging Library, PIL), which we’ll then use to load the *ladybug.png* image (see the upper-left image in [Figure 8-11](#image_segmentation_plot)), assuming it’s located at `filepath`:    ```", "```py `>>>` `image``.``shape` `` `(533, 800, 3)` `` ```", "```py   ```", "```py ```", "```py`The image is represented as a 3D array. The first dimension’s size is the height; the second is the width; and the third is the number of color channels, in this case red, green, and blue (RGB). In other words, for each pixel there is a 3D vector containing the intensities of red, green, and blue as unsigned 8-bit integers between 0 and 255\\. Some images may have fewer channels (such as grayscale images, which only have one), and some images may have more channels (such as images with an additional *alpha channel* for transparency, or satellite images, which often contain channels for additional light frequencies, like infrared).    The following code reshapes the array to get a long list of RGB colors, then it clusters these colors using *k*-means with eight clusters. It creates a `segmented_img` array containing the nearest cluster center for each pixel (i.e., the mean color of each pixel’s cluster), and lastly it reshapes this array to the original image shape. The third line uses advanced NumPy indexing; for example, if the first 10 labels in `kmeans_.labels_` are equal to 1, then the first 10 colors in `segmented_img` are equal to `kmeans.cluster_centers_[1]`:    ```", "```py    This outputs the image shown in the upper right of [Figure 8-11](#image_segmentation_plot). You can experiment with various numbers of clusters, as shown in the figure. When you use fewer than eight clusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it gets merged with colors from the environment. This is because *k*-means prefers clusters of similar sizes. The ladybug is small—much smaller than the rest of the image—so even though its color is flashy, *k*-means fails to dedicate a cluster to it.  ![Image showing the effect of _k_-means clustering with 10, 8, 6, 4, and 2 color clusters on an original image of a ladybug on a dandelion, illustrating how fewer clusters merge colors and lose detail.](assets/hmls_0811.png)  ###### Figure 8-11\\. Image segmentation using *k*-means with various numbers of color clusters    That wasn’t too hard, was it? Now let’s look at another application of clustering.```", "```py`` ```", "```py  ```", "```py```", "````py``` ````", "```````py`` ``````py```````", "``` from sklearn.datasets import load_digits  X_digits, y_digits = load_digits(return_X_y=True) X_train, y_train = X_digits[:1400], y_digits[:1400] X_test, y_test = X_digits[1400:], y_digits[1400:] ```", "``` from sklearn.linear_model import LogisticRegression  n_labeled = 50 log_reg = LogisticRegression(max_iter=10_000) log_reg.fit(X_train[:n_labeled], y_train[:n_labeled]) ```", "``` >>> log_reg.score(X_test, y_test) `0.7581863979848866` ```", "``````py``````", "``` k = 50 kmeans = KMeans(n_clusters=k, random_state=42) X_digits_dist = kmeans.fit_transform(X_train) representative_digit_idx = X_digits_dist.argmin(axis=0) X_representative_digits = X_train[representative_digit_idx] ```", "``` y_representative_digits = np.array([8, 0, 1, 3, 6, 7, 5, 4, 2, 8, ..., 6, 4]) ```", "``` >>> log_reg = LogisticRegression(max_iter=10_000) `>>>` `log_reg``.``fit``(``X_representative_digits``,` `y_representative_digits``)` ```", "``` ```", "``````py``````", "```py```", "````py` ````", "```` Wow! We jumped from 75.8% accuracy to 83.4%, although we are still only training the model on 50 instances. Since it is often costly and painful to label instances, especially when it has to be done manually by experts, it is a good idea to label representative instances rather than just random instances.    But perhaps we can go one step further: what if we propagated the labels to all the other instances in the same cluster? This is called *label propagation*:    ```py y_train_propagated = np.empty(len(X_train), dtype=np.int64) for i in range(k):     y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i] ```    Now let’s train the model again and look at its performance:    ```py >>> log_reg = LogisticRegression() `>>>` `log_reg``.``fit``(``X_train``,` `y_train_propagated``)` ``` `>>>` `log_reg``.``score``(``X_test``,` `y_test``)` `` `0.8690176322418136` `` ```py ```   ```py````", "```py```", "```py```", "```py percentile_closest = 50  X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_] for i in range(k):     in_cluster = (kmeans.labels_ == i)     cluster_dist = X_cluster_dist[in_cluster]     cutoff_distance = np.percentile(cluster_dist, percentile_closest)     above_cutoff = (X_cluster_dist > cutoff_distance)     X_cluster_dist[in_cluster & above_cutoff] = -1  partially_propagated = (X_cluster_dist != -1) X_train_partially_propagated = X_train[partially_propagated] y_train_partially_propagated = y_train_propagated[partially_propagated] ```", "```py >>> log_reg = LogisticRegression(max_iter=10_000) `>>>` `log_reg``.``fit``(``X_train_partially_propagated``,` `y_train_partially_propagated``)` ```", "```py ```", "```py```", "````` ```py` Nice! With just 50 labeled instances (only 5 examples per class on average!) we got 88.4% accuracy, pretty close to the performance we got on the fully labeled digits dataset. This is partly thanks to the fact that we dropped some outliers, and partly because the propagated labels are actually pretty good—their accuracy is about 98.9%, as the following code shows:    ``` >>> (y_train_partially_propagated == y_train[partially_propagated]).mean() `np.float64(0.9887798036465638)` ```py   ``###### Tip    Scikit-Learn also offers two classes that can propagate labels automatically: `LabelSpreading` and `LabelPropagation` in the `sklearn.​semi_supervised` package. Both classes construct a similarity matrix between all the instances, and iteratively propagate labels from labeled instances to similar unlabeled instances. There’s also a different class called `SelfTrainingClassifier` in the same package: you give it a base classifier (e.g., `RandomForestClassifier`) and it trains it on the labeled instances, then uses it to predict labels for the unlabeled samples. It then updates the training set with the labels it is most confident about, and repeats this process of training and labeling until it cannot add labels anymore. These techniques are not magic bullets, but they can occasionally give your model a little boost.    Before we move on to Gaussian mixture models, let’s take a look at DBSCAN, another popular clustering algorithm that illustrates a very different approach based on local density estimation. This approach allows the algorithm to identify clusters of arbitrary shapes.`` ```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "```` ```py````", "```py` ```", "```py```", "``` ```", "```````py```  ``````py```````", "``````py``````", "```py```", "````py` ## DBSCAN    The *density-based spatial clustering of applications with noise* (DBSCAN) algorithm defines clusters as continuous regions of high density. Here is how it works:    *   For each instance, the algorithm counts how many instances are located within a small distance ε (epsilon) from it. This region is called the instance’s *ε-neighborhood*.           *   If an instance has at least `min_samples` instances in its ε-neighborhood (including itself), then it is considered a *core instance*. In other words, core instances are those that are located in dense regions.           *   All instances in the neighborhood of a core instance belong to the same cluster. This neighborhood may include other core instances; therefore, a long sequence of neighboring core instances forms a single cluster.           *   Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly.              This algorithm works well if all the clusters are well separated by low-density regions. The `DBSCAN` class in Scikit-Learn is as simple to use as you might expect. Let’s test it on the moons dataset, introduced in [Chapter 5](ch05.html#trees_chapter):    ``` from sklearn.cluster import DBSCAN from sklearn.datasets import make_moons  X, y = make_moons(n_samples=1000, noise=0.05, random_state=42) dbscan = DBSCAN(eps=0.05, min_samples=5) dbscan.fit(X) ```py    The labels of all the instances are now available in the `labels_` instance variable:    ``` >>> dbscan.labels_ `array([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5, [...], 3,  3,  4,  2,  6,  3])` ```py   ````", "```` Notice that some instances have a cluster index equal to –1, which means that they are considered as anomalies by the algorithm. The indices of the core instances are available in the `core_sample_indices_` instance variable, and the core instances themselves are available in the `components_` instance variable:    ```py >>> dbscan.core_sample_indices_ `array([  0,   4,   5,   6,   7,   8,  10,  11, [...], 993, 995, 997, 998, 999])` `>>>` `dbscan``.``components_` `` `array([[-0.02137124,  0.40618608],`  `[-0.84192557,  0.53058695],`  `[...],`  `[ 0.79419406,  0.60777171]])` `` ```   ```py````", "```py```", "```py from sklearn.neighbors import KNeighborsClassifier  knn = KNeighborsClassifier(n_neighbors=50) knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_]) ```", "```py >>> X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]]) `>>>` `knn``.``predict``(``X_new``)` ```", "```py ```", "```py```", "```py```", "````` Note that we only trained the classifier on the core instances, but we could also have chosen to train it on all the instances, or all but the anomalies: this choice depends on the final task.    The decision boundary is represented in [Figure 8-14](#cluster_classification_plot) (the crosses represent the four instances in `X_new`). Notice that since there is no anomaly in the training set, the classifier always chooses a cluster, even when that cluster is far away. It is fairly straightforward to introduce a maximum distance, in which case the two instances that are far away from both clusters are classified as anomalies. To do this, use the `kneighbors()` method of the `KNeighborsClassifier`. Given a set of instances, it returns the distances and the indices of the *k*-nearest neighbors in the training set (two matrices, each with *k* columns):    ```py >>> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1) `>>>` `y_pred` `=` `dbscan``.``labels_``[``dbscan``.``core_sample_indices_``][``y_pred_idx``]` ```` `>>>` `y_pred``[``y_dist` `>` `0.2``]` `=` `-``1` ```py `>>>` `y_pred``.``ravel``()` `` `array([-1,  0,  1, -1])` `` ``` ```py` ```  ```py` ``` ``![Diagram showing a decision boundary between two clusters, illustrating how DBSCAN identifies clusters of varying shapes.](assets/hmls_0814.png)  ###### Figure 8-14\\. Decision boundary between two clusters    In short, DBSCAN is a very simple yet powerful algorithm capable of identifying any number of clusters of any shape. It is robust to outliers, and it has just two hyperparameters (`eps` and `min_samples`). If the density varies significantly across the clusters, however, or if there’s no sufficiently low-density region around some clusters, DBSCAN can struggle to capture all the clusters properly. Moreover, its computational complexity is roughly *O*(*m*²*n*), so it does not scale well to large datasets.    ###### Tip    You may also want to try *hierarchical DBSCAN* (HDBSCAN), using `sklearn.cluster.HDBSCAN`: it is often better than DBSCAN at finding clusters of varying densities.`` ```py ```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "````  ```py` ``` ``## Other Clustering Algorithms    Scikit-Learn implements several more clustering algorithms that you should take a look at. I cannot cover them all in detail here, but here is a brief overview:    Agglomerative clustering      A hierarchy of clusters is built from the bottom up. Think of many tiny bubbles floating on water and gradually attaching to each other until there’s one big group of bubbles. Similarly, at each iteration, agglomerative clustering connects the nearest pair of clusters (starting with individual instances). If you drew a tree with a branch for every pair of clusters that merged, you would get a binary tree of clusters, where the leaves are the individual instances. This approach can capture clusters of various shapes; it also produces a flexible and informative cluster tree instead of forcing you to choose a particular cluster scale, and it can be used with any pairwise distance. It can scale nicely to large numbers of instances if you provide a connectivity matrix, which is a sparse *m* × *m* matrix that indicates which pairs of instances are neighbors (e.g., returned by `sklearn.neighbors.kneighbors_graph()`). Without a connectivity matrix, the algorithm does not scale well to large datasets.      BIRCH      The balanced iterative reducing and clustering using hierarchies (BIRCH) algorithm was designed specifically for very large datasets, and it can be faster than batch *k*-means, with similar results, as long as the number of features is not too large (<20). During training, it builds a tree structure containing just enough information to quickly assign each new instance to a cluster, without having to store all the instances in the tree: this approach allows it to use limited memory while handling huge datasets.      Mean-shift      This algorithm starts by placing a circle centered on each instance; then for each circle it computes the mean of all the instances located within it, and it shifts the circle so that it is centered on the mean. Next, it iterates this mean-shifting step until all the circles stop moving (i.e., until each of them is centered on the mean of the instances it contains). Mean-shift shifts the circles in the direction of higher density, until each of them has found a local density maximum. Finally, all the instances whose circles have settled in the same place (or close enough) are assigned to the same cluster. Mean-shift has some of the same features as DBSCAN, like how it can find any number of clusters of any shape, it has very few hyperparameters (just one—the radius of the circles, called the *bandwidth*), and it relies on local density estimation. But unlike DBSCAN, mean-shift tends to chop clusters into pieces when they have internal density variations. Unfortunately, its computational complexity is *O*(*m*²*n*), so it is not suited for large datasets.      Affinity propagation      In this algorithm, instances repeatedly exchange messages between one another until every instance has elected another instance (or itself) to represent it. These elected instances are called *exemplars*. Each exemplar and all the instances that elected it form one cluster. In real-life politics, you typically want to vote for a candidate whose opinions are similar to yours, but you also want them to win the election, so you might choose a candidate you don’t fully agree with, but who is more popular. You typically evaluate popularity through polls. Affinity propagation works in a similar way, and it tends to choose exemplars located near the center of clusters, similar to *k*-means. But unlike with *k*-means, you don’t have to pick a number of clusters ahead of time: it is determined during training. Moreover, affinity propagation can deal nicely with clusters of different sizes. Sadly, this algorithm has a computational complexity of *O*(*m*²), so it is not suited for large datasets.      Spectral clustering      This algorithm takes a similarity matrix between the instances and creates a low-dimensional embedding from it (i.e., it reduces the matrix’s dimensionality), then it uses another clustering algorithm in this low-dimensional space (Scikit-Learn’s implementation uses *k*-means). Spectral clustering can capture complex cluster structures, and it can also be used to cut graphs (e.g., to identify clusters of friends on a social network). It does not scale well to large numbers of instances, and it does not behave well when the clusters have very different sizes.      Now let’s dive into Gaussian mixture models, which can be used for density estimation, clustering, and anomaly detection.`` ```py ````", "```py```", "````py` ````", "`````` ```py``````", "``` ```", "```py```", "````py` ````", "```````py`` ``````py```````", "``` ```", "```py```", "````py````", "```py```", "````py````", "```py` ```", "```py```", "````py````", "```py```", "````py````", "```py```", "```py```", "````py````", "```py```", "```py```", "````py````", "```py```", "``` from sklearn.mixture import GaussianMixture  gm = GaussianMixture(n_components=3, n_init=10, random_state=42) gm.fit(X) ```", "``` >>> gm.weights_ `array([0.40005972, 0.20961444, 0.39032584])` `>>>` `gm``.``means_` ```", "``` ```", "``````py``````", "`````` ```py``````", "```````py` ``````py```````", "```` Great, it worked fine! Indeed, two of the three clusters were generated with 500 instances each, while the third cluster only contains 250 instances. So the true cluster weights are 0.4, 0.4, and 0.2, respectively, and that’s roughly what the algorithm found (in a different order). Similarly, the true means and covariance matrices are quite close to those found by the algorithm. But how? This class relies on the *expectation-maximization* (EM) algorithm, which has many similarities with the *k*-means algorithm: it also initializes the cluster parameters randomly, then it repeats two steps until convergence, first assigning instances to clusters (this is called the *expectation step*) and then updating the clusters (this is called the *maximization step*). Sounds familiar, right? In the context of clustering, you can think of EM as a generalization of *k*-means that not only finds the cluster centers (**μ**^((1)) to **μ**^((*k*))), but also their size, shape, and orientation (**Σ**^((1)) to **Σ**^((*k*))), as well as their relative weights (*ϕ*^((1)) to *ϕ*^((*k*))). Unlike *k*-means, though, EM uses soft cluster assignments, not hard assignments. For each instance, during the expectation step, the algorithm estimates the probability that it belongs to each cluster (based on the current cluster parameters). Then, during the maximization step, each cluster is updated using *all* the instances in the dataset, with each instance weighted by the estimated probability that it belongs to that cluster. These probabilities are called the *responsibilities* of the clusters for the instances. During the maximization step, each cluster’s update will mostly be impacted by the instances it is most responsible for.    ###### Warning    Unfortunately, just like *k*-means, EM can end up converging to poor solutions, so it needs to be run several times, keeping only the best solution. This is why we set `n_init` to 10\\. Be careful: by default `n_init` is set to 1.    You can check whether the algorithm converged and how many iterations it took:    ```py >>> gm.converged_ `True` `>>>` `gm``.``n_iter_` `` `4` `` ```   ```py````", "```py```", "``` ```", "```````py````` Now that you have an estimate of the location, size, shape, orientation, and relative weight of each cluster, the model can easily assign each instance to the most likely cluster (hard clustering) or estimate the probability that it belongs to a particular cluster (soft clustering). Just use the `predict()` method for hard clustering, or the `predict_proba()` method for soft clustering:    ```py >>> gm.predict(X) `array([2, 2, 0, ..., 1, 1, 1])` `>>>` `gm``.``predict_proba``(``X``)``.``round``(``3``)` `` `array([[0\\.   , 0.023, 0.977],`  `[0.001, 0.016, 0.983],`  `[1\\.   , 0\\.   , 0\\.   ],`  `...,`  `[0\\.   , 1\\.   , 0\\.   ],`  `[0\\.   , 1\\.   , 0\\.   ],`  `[0\\.   , 1\\.   , 0\\.   ]])` `` ```   ```py```````", "```` ```py````", "```py```", "``` >>> X_new, y_new = gm.sample(6) `>>>` `X_new` ```", "``` ```", "``````py``````", "```py```", "````py` ````", "```` It is also possible to estimate the density of the model at any given location. This is achieved using the `score_samples()` method: for each instance it is given, this method estimates the log of the *probability density function* (PDF) at that location. The greater the score, the higher the density:    ```py >>> gm.score_samples(X).round(2) `array([-2.61, -3.57, -3.33, ..., -3.51, -4.4 , -3.81])` ```   ```py````", "```py densities = gm.score_samples(X) density_threshold = np.percentile(densities, 2) anomalies = X[densities < density_threshold] ```", "```py >>> gm.bic(X) `np.float64(8189.733705221636)` `>>>` `gm``.``aic``(``X``)` `` `np.float64(8102.508425106598)` `` ```", "```py```", "```py```", "```py >>> from sklearn.mixture import BayesianGaussianMixture `>>>` `bgm` `=` `BayesianGaussianMixture``(``n_components``=``10``,` `n_init``=``10``,` `max_iter``=``500``,` ```", "```py `... `                              `random_state``=``42``)` ```", "```py` `>>>` `bgm``.``fit``(``X``)` ```", "```py ```", "```py`` ```", "```py ```", "```py `` `Perfect: the algorithm automatically detected that only three clusters are needed, and the resulting clusters are almost identical to the ones in [Figure 8-15](#gaussian_mixtures_plot).    A final note about Gaussian mixture models: although they work great on clusters with ellipsoidal shapes, they don’t do so well with clusters of very different shapes. For example, let’s see what happens if we use a Bayesian Gaussian mixture model to cluster the moons dataset (see [Figure 8-20](#moons_vs_bgm_plot)).    Oops! The algorithm desperately searched for ellipsoids, so it found eight different clusters instead of two. The density estimation is not too bad, so this model could perhaps be used for anomaly detection, but it failed to identify the two moons. To conclude this chapter, let’s take a quick look at a few algorithms capable of dealing with arbitrarily shaped clusters.  ![Diagram showing a Gaussian mixture model's attempt to fit nonellipsoidal clusters, illustrating its limitation in detecting two moon-shaped clusters by identifying eight separate sections instead.](assets/hmls_0820.png)  ###### Figure 8-20\\. Fitting a Gaussian mixture to nonellipsoidal clusters` `` ```", "```py` ```", "```py ```", "```py```", "```py```", "```py```", "``` ```", "```````py ``````py````` ```py```````", "``````py``````", "``````py``````", "``````py``````", "``````py``````", "``` ```", "```py```", "````py````", "```py```", "````py````", "```py` ```", "```py```", "````py````", "```py` ```", "```py ```", "```py```", "````py````", "```py```", "``````py``````", "```````py` ``````py```````", "`````````"]