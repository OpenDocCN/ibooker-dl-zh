- en: Chapter 6\. Fine-Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章\. 微调
- en: In the previous chapter, we discussed the various factors that need to be taken
    into account while choosing the right LLM for your specific needs, including pointers
    on how to evaluate LLMs to be able to make an informed choice. Next, let us utilize
    these LLMs to solve our tasks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了在选择适合你特定需求的LLM时需要考虑的各种因素，包括如何评估LLMs以便做出明智选择的一些提示。接下来，让我们利用这些LLMs来解决我们的任务。
- en: In this chapter, we will explore the process of adapting an LLM to solve your
    task of interest, using fine-tuning. We will go through a full example of fine-tuning,
    covering all the important decisions one needs to make. We will also discuss the
    art and science of creating fine-tuning datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨使用微调使LLM适应解决你感兴趣的任务的过程。我们将通过一个完整的微调示例来展示，涵盖一个人需要做出的所有重要决策。我们还将讨论创建微调数据集的艺术和科学。
- en: The Need for Fine-Tuning
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调的必要性
- en: 'Why do we need to fine-tune LLMs? Why doesn’t a pre-trained LLM with few-shot
    prompts suffice for our needs? Let us look at a couple of examples to drive the
    point home:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么需要微调LLMs？为什么一个经过预训练且带有少量提示的LLM不足以满足我们的需求？让我们通过几个例子来说明这一点：
- en: Use Case 1
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 用例1
- en: Consider you are working on the rather whimsical task of detecting all sentences
    written in the past tense within a body of text and transforming them to future
    tense. To solve this task, you might provide a few examples of past tense sentences
    and input-output pairs representing past tense and their corresponding future
    tense sentences. However, the LLM doesn’t seem to be able to tackle this task
    to your satisfaction, making mistakes in both the identification and transformation
    steps. In response, you elaborate on your instructions, adding grammar rules and
    exceptions in the English language into your prompt. You notice an increase in
    performance. But with each new rule added, your prompt balloons, slowly turning
    into a grammar mini-book.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在处理一个相当古怪的任务，即检测文本中所有用过去时态写的句子并将它们转换为将来时态。为了解决这个问题，你可能会提供一些过去时态句子的例子以及表示过去时态及其对应将来时态句子的输入-输出对。然而，LLM似乎无法满足你的要求，在识别和转换步骤中都犯了错误。作为回应，你详细说明了你的指令，将英语语法规则和例外情况添加到你的提示中。你注意到性能有所提高。但随着每条新规则的添加，你的提示逐渐膨胀，慢慢地变成了一本语法迷你书。
- en: As we saw in [Chapter 5](ch05.html#chapter_utilizing_llms), the LLM can adhere
    to only a finite set of instructions in the prompt, and its effective context
    window is much smaller than the advertised context window. We have hit an impasse.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第5章](ch05.html#chapter_utilizing_llms)中看到的，LLM只能遵循提示中的有限指令集，其有效上下文窗口远小于宣传的上下文窗口。我们已经陷入了僵局。
- en: Use Case 2
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 用例2
- en: Consider a task that deals with answering questions from content in financial
    text. LLMs are not financial experts and have difficulty dealing with financial
    jargon. To address this, you add the definitions of key financial terms in the
    prompt. While you notice a small improvement in performance, it is not long before
    you realize you need to stuff the entire curriculum of the CPA exam into your
    measly context window to achieve the desired gains.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个处理从金融文本中回答问题的任务。大型语言模型（LLMs）并非金融专家，难以处理金融术语。为了解决这个问题，你在提示中添加了关键金融术语的定义。虽然你注意到性能有轻微的提升，但很快你就会意识到，为了实现预期的收益，你需要将整个注册会计师（CPA）考试的整个课程塞进你那微不足道的上下文窗口中。
- en: This is where fine-tuning comes in. By providing a dataset of input-output pairs,
    such that the model learns the input-output mapping by updating its weights, you
    can accomplish tasks that cannot be performed by in-context learning alone. For
    both the tasks mentioned above, fine-tuning the model massively improves performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是微调发挥作用的地方。通过提供一个输入-输出对的数据集，模型通过更新其权重来学习输入-输出映射，你可以完成仅凭上下文学习无法完成的任务。对于上述提到的两个任务，微调模型大大提高了性能。
- en: When should fine-tuning not be used? If your primary goal is to impart new or
    updated facts or knowledge to the language model, this is better served with techniques
    like RAG, which we will explore in Chapters [10](ch10.html#ch10) and [12](ch12.html#ch12).
    Fine-tuning is best suited for situations where you need the model to learn a
    particular input-output mapping, be familiarized to a new textual domain, or exhibit
    more complex capabilities and behavior.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 何时不应使用微调？如果你的主要目标是向语言模型传授新的或更新的事实或知识，那么像RAG这样的技术会更好，我们将在第[10章](ch10.html#ch10)和[12章](ch12.html#ch12)中探讨这些技术。微调最适合需要模型学习特定的输入-输出映射、熟悉新的文本领域或展示更复杂的能力和行为的情况。
- en: Warning
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Recall from [Chapter 5](ch05.html#chapter_utilizing_llms) that updating a language
    model’s parameters can cause the base model capabilities to regress! Fine-tuning
    a model on one task can inadvertently cause the base model to perform worse on
    other tasks. Handle with care.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第5章](ch05.html#chapter_utilizing_llms)，更新语言模型的参数可能会使基础模型的能力下降！在一个特定任务上微调模型可能会无意中导致基础模型在其他任务上的表现变差。请谨慎处理。
- en: 'Fine-Tuning: A Full Example'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调：一个完整示例
- en: Let’s walk through a practical fine-tuning example from start to finish. We
    would like to train a *political promises detector*, which can be used to identify
    promises made by representatives of the ruling party in campaign speeches or parliamentary
    proceedings. We define a political promise as something that is tangible, specific,
    and an action that the government has the agency to make.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头到尾走一遍一个实际的微调示例。我们希望训练一个*政治承诺检测器*，它可以用来识别执政党代表在竞选演讲或议会程序中做出的承诺。我们定义政治承诺为具体、明确，并且政府有权力采取的行动。
- en: 'An example of such a sentence is: “We will build 10,000 kilometres of subway
    lines in the next ten years.”'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的句子例子是：“我们将在未来十年内建设10,000公里的地铁线路。”
- en: 'However, not all future tense or forward-looking statements are promises. The
    following sentences are not promises, per our definition:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有未来时态或前瞻性陈述都是承诺。以下句子根据我们的定义不是承诺：
- en: “We expect the Japanese to increase tariffs next year.” (expectation, and not
    something the government can control)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我们预计日本明年将提高关税。”（预期，而不是政府可以控制的事情）
- en: “We will work toward making Canada a better place.” (no specifics provided)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我们将努力使加拿大成为一个更好的地方。”（没有提供具体信息）
- en: “AI will cause the loss of a million jobs next year.” (prediction, not promise)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “AI将在明年导致一百万个工作岗位的丧失。”（预测，不是承诺）
- en: Our base LLM, Llama2-7B, finds it difficult to accurately identify such promises
    in an in-context learning setup. Therefore, we will fine-tune it for this specific
    task. We can then use the resulting model to detect political promises, and then
    match those promises against structured datasets or budgetary text to track whether
    these promises have been fulfilled over a period of time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基础LLM，Llama2-7B，在上下文学习设置中难以准确识别此类承诺。因此，我们将针对这个特定任务对其进行微调。然后我们可以使用生成的模型来检测政治承诺，并将这些承诺与结构化数据集或预算文本进行匹配，以跟踪这些承诺在一段时间内是否得到履行。
- en: To this end, I have constructed a synthetic fine-tuning dataset containing examples
    of both promises and mere statements. Later in this chapter, we will go through
    the process of creating such a dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，我已经构建了一个包含承诺和陈述示例的合成微调数据集。在本章的后面部分，我们将介绍创建此类数据集的过程。
- en: Fortunately, fine-tuning today is easier due to the existence of several libraries
    that streamline the fine-tuning process. The most important of these libraries
    are [Transformers](https://oreil.ly/BTi76), [Accelerate](https://oreil.ly/W8oLi),
    [PEFT](https://oreil.ly/QbQoq), [TRL](https://oreil.ly/Ya9Xj), and [bitsandbytes](https://oreil.ly/ruVEX).
    The first four are from Hugging Face. You have encountered many of these libraries
    in prior chapters already. Being familiar with the inner workings of these libraries
    is a very useful skill.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，由于存在几个简化微调过程的库，今天的微调变得更加容易。其中最重要的库包括[Transformers](https://oreil.ly/BTi76)、[Accelerate](https://oreil.ly/W8oLi)、[PEFT](https://oreil.ly/QbQoq)、[TRL](https://oreil.ly/Ya9Xj)和[bitsandbytes](https://oreil.ly/ruVEX)。前四个来自Hugging
    Face。你已经在之前的章节中遇到了许多这些库。熟悉这些库的内部工作原理是一项非常有用的技能。
- en: Tip
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Given that these libraries are relatively new and are part of a fast-moving
    field, they frequently undergo substantial updates. I recommend keeping in touch
    with major updates of these libraries, as they continue to introduce enhancements
    that will simplify your workflow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些库相对较新，并且属于快速发展的领域，它们经常经历重大更新。我建议关注这些库的主要更新，因为它们继续引入将简化你工作流程的增强功能。
- en: 'Let’s begin by loading the dataset. The custom dataset can be downloaded from
    this book’s [GitHub repo](https://oreil.ly/llm-playbooks):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载数据集开始。自定义数据集可以从本书的 [GitHub 仓库](https://oreil.ly/llm-playbooks) 下载：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: I highly recommend using the [*datasets* library](https://oreil.ly/3LX5X) for
    loading your training and fine-tuning datasets, as it is an excellent abstraction
    for efficiently loading large datasets, abstracting away memory management details.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐使用 [*datasets* 库](https://oreil.ly/3LX5X) 来加载你的训练和微调数据集，因为它是一个高效加载大型数据集的绝佳抽象，可以抽象出内存管理细节。
- en: 'Next, let us set some relevant hyperparameters in the `Transformers` library
    through the `TrainingArguments` class:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过 `TrainingArguments` 类在 `Transformers` 库中设置一些相关的超参数：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There are more than a hundred arguments available; we will go through the important
    ones. The arguments relate to the learning algorithms used, memory and space optimizations,
    quantization, regularization, and distributed training. Let’s explore these in
    detail.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可用超过一百个参数；我们将讨论其中重要的参数。这些参数与使用的学习算法、内存和空间优化、量化、正则化和分布式训练相关。让我们详细探讨这些内容。
- en: Learning Algorithms Parameters
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习算法参数
- en: Let’s explore optimization algorithms used for training the network and learn
    how to choose the right one for our purposes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索用于训练网络的优化算法，并学习如何为我们的目的选择正确的优化器。
- en: Optimizers
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器
- en: AdamW and Adafactor are currently the most used optimizers. Other popular optimization
    algorithms include stochastic gradient descent (SGD), RMSProp, Adagrad, Lion,
    and their variants. For more background on optimization algorithms, refer to Florian
    June’s [blog post](https://oreil.ly/VTiDa).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: AdamW 和 Adafactor 目前是最常用的优化器。其他流行的优化算法包括随机梯度下降（SGD）、RMSProp、Adagrad、Lion 以及它们的变体。有关优化算法的更多背景信息，请参阅
    Florian June 的 [博客文章](https://oreil.ly/VTiDa)。
- en: Adafactor and SGD use four bytes of memory per parameter, while AdamW uses eight
    bytes per parameter. This means that a 7B model undergoing full fine-tuning with
    the AdamW optimizer requires 7 * 8 = ~56GB of memory to store the optimizer states
    alone. Even more memory is needed to store the parameters, gradients, and the
    forward activations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Adafactor 和 SGD 每个参数使用 4 个字节的内存，而 AdamW 每个参数使用 8 个字节的内存。这意味着使用 AdamW 优化器进行完全微调的
    7B 模型需要 7 * 8 = ~56GB 的内存来存储优化器状态。存储参数、梯度和前向激活还需要更多的内存。
- en: More recently, [8-bit optimizers](https://oreil.ly/4Z14D) have been introduced
    that perform quantization of the optimizer state. A 7B model undergoing full fine-tuning
    with the AdamW 8-bit version requires only ~14GB of memory for the optimizer state.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[8 位优化器](https://oreil.ly/4Z14D) 已被引入，它们可以对优化器状态进行量化。使用 AdamW 8 位版本进行完全微调的
    7B 模型只需要大约 ~14GB 的内存来存储优化器状态。
- en: 'These 8-bit optimizers are available through the bitsnbytes library and are
    also supported by Hugging Face. For using the 8-bit AdamW version, you can set
    in the `TrainingArguments`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 8 位优化器通过 bitsnbytes 库提供，并且也得到 Hugging Face 的支持。要使用 8 位 AdamW 版本，你可以在 `TrainingArguments`
    中设置：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For all the optimizer options directly available through Hugging Face, refer
    to the [OptimizerNames class](https://oreil.ly/7kdSO).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有通过 Hugging Face 直接可用的优化器选项，请参阅 [OptimizerNames 类](https://oreil.ly/7kdSO)。
- en: Tip
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: In his benchmarking experiments, Stas Bekman [shows](https://oreil.ly/0_0lt)
    that surprisingly, the 8-bit AdamW optimizer is actually faster than the standard
    AdamW optimizer. His experiments also show that Adafactor is slightly slower than
    AdamW overall.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的基准测试实验中，Stas Bekman [显示](https://oreil.ly/0_0lt)令人惊讶的是，8 位 AdamW 优化器实际上比标准
    AdamW 优化器更快。他的实验还显示，Adafactor 整体上略慢于 AdamW。
- en: The default optimizer provided in the Hugging Face `TrainingArguments` class
    is AdamW. For most cases, the default optimizer works just fine. However, if it
    doesn’t, you can try Adafactor and Lion. For reinforcement learning, SGD seems
    to work well.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face `TrainingArguments` 类中提供的默认优化器是 AdamW。对于大多数情况，默认优化器工作得很好。然而，如果它不起作用，你可以尝试
    Adafactor 和 Lion。对于强化学习，SGD 似乎效果不错。
- en: If you are especially memory constrained, 8-bit AdamW is a compelling choice.
    If available, the paged version of these optimizers will further mitigate your
    memory requirements.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你特别内存受限，8位AdamW是一个不错的选择。如果可用，这些优化器的分页版本将进一步减轻你的内存需求。
- en: Learning rates
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率
- en: For each optimizer, certain learning rates have been shown to be very effective.
    A recommended learning rate for AdamW is 1e-4 with a weight decay of 0.01\. Weight
    decay is a regularization technique that helps reduce overfitting. Similarly,
    the default values for minor optimizer parameters like *adam_beta1*, *adam_beta2*,
    and *adam_epsilon* are good enough and need not be changed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个优化器，某些学习率已被证明非常有效。AdamW推荐的学习率是1e-4，权重衰减为0.01。权重衰减是一种正则化技术，有助于减少过拟合。同样，对于像*adam_beta1*、*adam_beta2*和*adam_epsilon*这样的次要优化器参数，默认值已经足够好，无需更改。
- en: Learning schedules
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习计划
- en: Toward the end of the training process, it is a good idea to lower the learning
    rate because you do not want to overshoot when you are so close to convergence.
    In a similar vein, you would like to prevent your model from learning too much
    from the first few batches of examples. In either case, we would like to be able
    to automatically adjust the learning rate as training progresses. To facilitate
    this, we can use a learning schedule.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程的后期，降低学习率是一个好主意，因为你不想在接近收敛时 overshoot。在类似的情况下，你希望防止你的模型从第一批示例中学习太多。在任何情况下，我们都希望能够在训练过程中自动调整学习率。为了实现这一点，我们可以使用学习计划。
- en: 'Hugging Face supports several different types of learning schedulers. Here
    are a few important ones:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face支持多种不同类型的调度器。以下是一些重要的调度器：
- en: Constant
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 常数
- en: This is the vanilla training schedule where the learning rate remains constant
    throughout the course of the training.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种纯训练计划，其中学习率在整个训练过程中保持不变。
- en: Constant with warmup
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 带有预热的常数
- en: In this setting, the learning rate starts from zero and is increased linearly
    toward the specified learning rate during a warmup phase. After the warmup phase
    is completed, the learning rate remains constant.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置中，学习率从零开始，在预热阶段线性增加到指定的学习率。预热阶段完成后，学习率保持不变。
- en: '[Figure 6-1](#constant-lr) shows how the learning rate changes over time while
    using the constant with warmup scheduler.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-1](#constant-lr)展示了在使用带有预热常数的调度器时，学习率随时间的变化情况。'
- en: '![constant-lr](assets/dllm_0601.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![constant-lr](assets/dllm_0601.png)'
- en: Figure 6-1\. Learning rate with a constant schedule with warmup
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1。带有预热常数的调度器的学习率
- en: Cosine
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦
- en: In this setting, called *cosine annealing*, the learning rate has a warmup phase
    after which it slowly declines to zero, as per the cosine function.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种称为*余弦退火*的设置中，学习率在预热阶段之后会按照余弦函数缓慢下降到零。
- en: '[Figure 6-2](#cosine-warmup) shows how the learning rate changes over time
    while using the cosine scheduler.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-2](#cosine-warmup)展示了在使用余弦调度器时，学习率随时间的变化情况。'
- en: '![cosine-warmup](assets/dllm_0602.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![cosine-warmup](assets/dllm_0602.png)'
- en: Figure 6-2\. Learning rate with a cosine schedule
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2。带有余弦调度器的学习率
- en: Cosine with restarts
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 带有重启的余弦
- en: In this setting, called *cosine annealing with warm restart*, after a warmup
    phase, the learning rate decreases to zero following the cosine function, but
    undergoes several hard restarts, where the learning rate shoots back to the specified
    learning rate after it reaches zero. For more details on why this is effective,
    check out Loshcilov and Hutter’s [paper](https://oreil.ly/Q4c3o) that introduced
    this concept.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种称为*余弦退火带预热重启*的设置中，在预热阶段之后，学习率按照余弦函数下降到零，但会经历几次硬重启，学习率在达到零后迅速回到指定的学习率。关于为什么这种方法有效的更多细节，请参阅介绍了这一概念的Loshcilov和Hutter的[论文](https://oreil.ly/Q4c3o)。
- en: '[Figure 6-3](#cosine-restart) shows how the learning rate changes across time
    while using the cosine with restarts scheduler.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-3](#cosine-restart)展示了在使用带有重启的余弦调度器时，学习率随时间的变化情况。'
- en: '![cosine-restart](assets/dllm_0603.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![cosine-restart](assets/dllm_0603.png)'
- en: Figure 6-3\. Learning rate with a cosine with restarts schedule
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3。带有带有重启余弦调度器的学习率
- en: Linear
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 线性
- en: This is very similar to the cosine setting, except that the learning rate decreases
    to zero linearly instead of following the cosine function.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这与余弦设置非常相似，不同之处在于学习率是线性下降到零，而不是遵循余弦函数。
- en: '[Figure 6-4](#linear-lr) shows how the learning rate changes over time while
    using the linear scheduler.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-4](#linear-lr)展示了在使用线性调度器时，学习率随时间的变化情况。'
- en: '![linear-lr](assets/dllm_0604.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![线性学习率](assets/dllm_0604.png)'
- en: Figure 6-4\. Learning rate with a linear scheduler
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4. 线性调度器的学习率
- en: If you are using AdamW, schedulers with a warmup phase are even more important
    to prevent getting trapped in a bad minima. Empirically, it has been found that
    cosine annealing outperforms linear decay.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用AdamW，具有预热阶段的调度器甚至更加重要，以防止陷入局部最小值。经验上发现，余弦退火优于线性衰减。
- en: 'For our political promises detector fine-tuning, let’s use the paged variant
    of AdamW, a learning rate of 3e-4, a weight decay of 0.01, and the cosine learning
    schedule:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的政治承诺检测器微调，让我们使用AdamW的分页变体，学习率为3e-4，权重衰减为0.01，并采用余弦学习计划：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Memory Optimization Parameters
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存优化参数
- en: After we have set the parameters related to the optimizers, let’s explore memory
    and compute optimization parameters. Two prevalent techniques in this area include
    gradient checkpointing and gradient accumulation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设置了与优化器相关的参数之后，让我们探索内存和计算优化参数。这个领域中的两种流行技术包括梯度检查点和梯度累积。
- en: Gradient checkpointing
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度检查点
- en: Gradient checkpointing helps save memory at the cost of more compute. During
    the forward pass of the backpropagation algorithm, activations are computed and
    saved in memory so that they can be used in the backward pass. What if we did
    not save all of the activations? The missing activations could be recalculated
    on the fly during the backward pass. This does cost us more compute, but we could
    save a lot of memory. We could even train models where a batch size of only one
    does not fit in our GPU memory. For more technical details on gradient checkpointing,
    check out Yaroslav Bulatov’s [blog](https://oreil.ly/i-R4I).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度检查点技术以增加计算成本为代价来帮助节省内存。在反向传播算法的前向传递过程中，激活值被计算并保存在内存中，以便在反向传递中使用。如果我们没有保存所有的激活值怎么办？缺失的激活值可以在反向传递过程中即时重新计算。这确实会增加我们的计算成本，但我们可以节省大量的内存。我们甚至可以训练那些批次大小仅为一个无法适应我们GPU内存的模型。有关梯度检查点的更多技术细节，请参阅Yaroslav
    Bulatov的[博客](https://oreil.ly/i-R4I)。
- en: Gradient accumulation
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度累积
- en: Let’s say we have a desired batch size but we do not have the required memory
    to support that batch size. We can simulate the desired batch size using a technique
    called gradient accumulation. In this technique, the gradient updates are not
    done at every batch, but are accumulated over several batches and then summed
    or averaged.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个期望的批次大小，但我们没有足够的内存来支持该批次大小。我们可以使用一种称为梯度累积的技术来模拟期望的批次大小。在这种技术中，梯度更新不是在每个批次都进行，而是在几个批次中累积，然后求和或平均。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Gradient accumulation can make training slower, since there are fewer updates
    being made. Gradient accumulation does not reduce the computation required.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度累积可能会使训练变慢，因为更新的次数较少。梯度累积不会减少所需的计算量。
- en: Quantization
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 量化
- en: A very effective form of saving memory is through quantization, as introduced
    in [Chapter 5](ch05.html#chapter_utilizing_llms). We will go through quantization
    techniques in more detail in [Chapter 9](ch09.html#ch09). For our use case, we
    will use bf16 as it represents a sound tradeoff between memory savings and performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过量化来节省内存是一种非常有效的方法，如第5章（ch05.html#chapter_utilizing_llms）中介绍的那样。我们将在第9章（ch09.html#ch09）中更详细地介绍量化技术。对于我们的用例，我们将使用bf16，因为它在内存节省和性能之间提供了一个合理的权衡。
- en: 'For our political promises detector fine-tuning, we’ll set the following parameters
    for memory optimization, given that we are trying to train it on a relatively
    memory constrained 16 GB RAM GPU:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的政治承诺检测器微调，鉴于我们试图在一个相对内存受限的16 GB RAM GPU上训练它，我们将设置以下参数以进行内存优化：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Regularization Parameters
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化参数
- en: Next, let’s look at various techniques available for tackling model overfitting.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看可用于解决模型过拟合的各种技术。
- en: Label smoothing
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签平滑
- en: Label smoothing is a technique that not only helps with combatting overfitting
    but also aids in model calibration.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 标签平滑是一种技术，它不仅有助于对抗过拟合，还有助于模型校准。
- en: Calibration is an underappreciated topic in deep learning. A model is said to
    be well-calibrated if there is a correlation between its output probability values
    and task accuracy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 校准是深度学习中一个被低估的话题。如果一个模型其输出概率值与任务准确性之间存在相关性，则称该模型校准良好。
- en: For example, consider a task that classifies a sentence as being abusive or
    not. If the model is well-calibrated, then among all examples for which the model
    produces an output probability of 0.9, 90% of them would be expected to be correctly
    classified. Similarly, for an output probability of 0.6, there should be a lower
    (~60%) likelihood of the classification being correct. Simply put, the output
    probability should accurately reflect the confidence in the classification decision.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: A model being well-calibrated implies that it is not overconfident. This helps
    us in nuanced handling of examples that have low output probabilities (using a
    bigger model to handle those examples, for instance).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Larger models are less calibrated compared to models like BERT, according to
    a study by [Li et al.](https://oreil.ly/ij7mS) Larger models tend to be more confident
    in general about their predictions. The inability to calculate reasonably accurate
    uncertainty estimates for large language models could be an argument to use smaller
    ones instead!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: One of the techniques for calibrating models is label smoothing. The usual training
    process involves training against hard target labels (0 or 1 for a binary classification
    task). When using cross-entropy as the loss function, this amounts to pushing
    the model logits closer to 0 or 1, thus making the model highly confident. Label
    smoothing involves using a regularization term that is subtracted or divided from
    the hard target label.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Label smoothing is especially useful when the input dataset is noisy, i.e.,
    contains some inaccurate labels. Regularization prevents the model from learning
    too much from incorrect examples.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: For the political promises detector, we will use label smoothing, given that
    some examples could be subjective or open to interpretation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Noise Embeddings
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The datasets we use for fine-tuning typically consist of a small number of examples
    (< 50,000). We would like our model to not overfit to the stylistic characteristics
    of the dataset, like the formatting, wording, and length of the text. One way
    to address this is by adding noise to the input embeddings.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[Jain et al.](https://oreil.ly/ouESL) observe that adding noise embeddings
    reduces the tendency of the model to overfit to wording and formatting of the
    fine-tuning datasets. An interesting side effect of noise embeddings is that the
    models generate longer, verbose texts. By measuring token diversity of the outputs,
    they confirmed that the longer texts actually include more information and are
    not just repetitive.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face supports [Noisy Embedding Instruction Fine-Tuning (NEFTune)](https://oreil.ly/dSaem),
    a noise addition technique. In NEFTune, a noise vector is added to each embedding
    vector. The elements in the noise vector are generated by sampling independent
    and identically distributed (iid) from [-1,1]. The resulting vector is scaled
    using a scaling factor before being added to the embedding vector.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Noise embeddings have been empirically found to be very effective in reducing
    overfitting. Therefore, we will use it for our political promises detector fine-tuning.
    Note that the noise embeddings are added only during training and not during inference.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The impact of noise embeddings is not yet well understood. Improvements in the
    fine-tuning task could come at the cost of other model capabilities. Make sure
    you test the model for regressions!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'For our political promises detector fine-tuning task, let’s activate both label
    smoothing and noise embeddings:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Batch Size
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Along with the learning rate, the batch size is one of the most important hyperparameters
    we need to set. A larger batch size means training will proceed faster. However,
    larger batch sizes also require more memory. Larger batch sizes can also lead
    the model to land in a sharp local minima, which can be a sign of overfitting.
    Therefore, there are trade offs involving memory, compute, and performance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: For the political promises detector, we will use a batch size of 8, given our
    memory limitations. Of course during inference, the maximum possible batch size
    is the ideal one. Note that it is recommended that the batch size be always a
    number that is a power of two, to reduce GPU I/O overhead.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'The `TrainingArguments` class by Hugging Face supports *auto_find_batch_size*,
    which when set, selects the maximum possible batch size supported by the memory.
    To use this feature, you need to install the `accelerate` library:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Tip
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can reduce your maximum sequence length to support a larger batch size.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s set some miscellaneous parameters:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '`max_grad_norm`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: This is used for gradient clipping, which is a solution for the exploding gradients
    issue that is sometimes encountered during training. The `max_grad_norm` value
    is the threshold for gradient clipping. If the L2 gradient norm is above the threshold,
    then it will be rescaled to `max_grad_norm`. For more details on gradient clipping,
    see [“Understanding Gradient Clipping (and How It Can Fix Exploding Gradients
    Problem)”](https://oreil.ly/gH7L7).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '`group_by_length`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: This is used to group examples that have similar lengths in the same batch,
    so that the padding tokens can be optimized.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '`max_train_epochs`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of passes over the training dataset. This is usually set to less than
    five to prevent overfitting:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameter-Efficient Fine-Tuning
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After filling in the `TrainingArguments`, let’s next fill in parameters of the
    PEFT library.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The PEFT library by Hugging Face is an impressive facilitator of parameter-efficient
    fine-tuning. This refers to a set of fine-tuning techniques that update only a
    small proportion of parameters in the model while keeping the performance closer
    to what it would have been if all the parameters were updated.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will use low-rank adaptation (LoRA) as the fine-tuning
    technique. Here are some hyperparameters to consider:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '`r`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The attention dimension of LoRA.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '`lora_alpha`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The alpha parameter in the LoRA technique.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA技术中的alpha参数。
- en: '`lora_dropout`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`lora_dropout`'
- en: The dropout probability used in the layers being tuned. This helps reduce overfitting.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在正在调整的层中使用的丢弃概率。这有助于减少过拟合。
- en: '`layers_to_transform`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`layers_to_transform`'
- en: This specifies the layers for which the LoRA transformation is to be applied.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这指定了要应用LoRA变换的层。
- en: 'Here are some recommended default values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些推荐的默认值：
- en: '[PRE8]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For more background on LoRA, refer to Ogban Ugot’s [blog post](https://oreil.ly/_l91y).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LoRA的更多背景信息，请参阅Ogban Ugot的[博客文章](https://oreil.ly/_l91y)。
- en: Working with Reduced Precision
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用降低精度
- en: The bitsandbytes library, built by Tim Dettmers, facilitates working with reduced
    precision formats, which we introduced in [Chapter 5](ch05.html#chapter_utilizing_llms).
    In this example, we will work with the FP4 format. Note that you need the bitsandbytes
    version to be >= 0.39.0.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由Tim Dettmers构建的bitsandbytes库，简化了使用降低精度格式的操作，我们在[第5章](ch05.html#chapter_utilizing_llms)中介绍了这些格式。在这个例子中，我们将使用FP4格式。请注意，你需要bitsandbytes版本
    >= 0.39.0。
- en: 'Hugging Face has integrated bitsandbytes support into its ecosystem. The `BitsAndBytesConfig`
    class allows us to set the parameters. Here are some relevant ones:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face已将其生态系统中的bitsandbytes支持集成。`BitsAndBytesConfig`类允许我们设置参数。以下是一些相关的参数：
- en: '`load_in_8bit/load_in_4bit`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_in_8bit/load_in_4bit`'
- en: This is used to specify if we want to load the model in 4-bit mode or 8-bit
    mode.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这用于指定我们是否要以4位模式或8位模式加载模型。
- en: '`llm_int8_threshold`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`llm_int8_threshold`'
- en: We need to specify a threshold of values beyond which fp16 will be used. This
    is because int8 quantization works well only for values lesser than 5–6.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要指定一个阈值，超过该阈值将使用fp16。这是因为int8量化仅适用于小于5-6的值。
- en: '`llm_int8_skip_modules`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`llm_int8_skip_modules`'
- en: This is used to specify the exceptions for which we do not want int8 quantization.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这用于指定我们不想进行int8量化的异常。
- en: '`llm_int8_enable_fp32_cpu_offload`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`llm_int8_enable_fp32_cpu_offload`'
- en: If we want parts of the model to be run in int8 on GPU and the rest in FP32
    on CPU, this parameter facilitates it. This is used in cases where the model is
    too large to fit on our GPU.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在GPU上以int8运行模型的一部分，而在CPU上以FP32运行其余部分，此参数可以简化操作。这在模型太大而无法适应我们的GPU时使用。
- en: '`bnb_4bit_compute_dtype`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`bnb_4bit_compute_dtype`'
- en: This sets the computational type, regardless of the input type.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这设置计算类型，而不管输入类型如何。
- en: '`bnb_4bit_quant_type`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`bnb_4bit_quant_type`'
- en: The options here are FP4 or NF4\. This is used to set the quantization type
    in the 4-bit layers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的选项是FP4或NF4。这用于在4位层中设置量化类型。
- en: 'Here are some recommended default values:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些推荐的默认值：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Finally, we use the Transformer Reinforcement Learning (TRL) library that, in
    addition to reinforcement learning, provides support for supervised fine-tuning.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用Transformer Reinforcement Learning (TRL)库，除了强化学习之外，还提供了监督微调的支持。
- en: 'Here are some recommended default values:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些推荐的默认值：
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Putting It All Together
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整合所有内容
- en: 'Now that we have set up all the requisite parameters, here is the full code
    for the fine-tuning process:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了所有必要的参数，以下是微调过程的完整代码：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The relationship between the hyperparameters is very complex, and you might
    find surprising results. It will take several iterations before you hit the sweet
    spot. However, do not spend too much time squeezing out the last bit of performance
    from your fine-tuning, as that time is better spent developing better training
    data. In the next section, we will learn how to create effective training datasets.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数之间的关系非常复杂，你可能会发现令人惊讶的结果。在找到最佳点之前可能需要多次迭代。然而，不要花太多时间从微调中挤出最后一点性能，因为这段时间最好用于开发更好的训练数据。在下一节中，我们将学习如何创建有效的训练数据集。
- en: 'The exact memory you need to fine-tune an LLM depends on several factors: the
    optimizer used, whether gradient accumulation and gradient checkpointing are activated,
    the type of quantization used, etc.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM所需的精确内存取决于多个因素：使用的优化器、是否激活梯度累积和梯度检查点、使用的量化类型等。
- en: Fine-Tuning Datasets
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调数据集
- en: In our fine-tuning example, we directly loaded a preconstructed dataset, focusing
    primarily on the fine-tuning process. Now, let’s shift our attention to the dataset,
    to understand the various techniques for creating datasets.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的微调示例中，我们直接加载了一个预构建的数据集，主要关注微调过程。现在，让我们将注意力转向数据集，了解创建数据集的各种技术。
- en: 'First, let’s look into the dataset we used in our fine-tuning example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看我们在微调示例中使用的数据集：
- en: '[PRE12]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE13]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As we can see, this is not a traditional dataset with just (input, output)
    pairs but one that also contains the task description in natural language. A typical
    example in this type of fine-tuning dataset consists of :'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这不仅仅是一个只包含（输入，输出）对的常规数据集，而是一个还包含自然语言中任务描述的数据集。这类微调数据集的一个典型例子包括：
- en: The instruction, which describes the task and specifies the desired output format.
    Optionally, the instruction contains positive and/or negative examples of the
    task. It can also contain constraints and exceptions to be followed.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令，它描述了任务并指定了所需的输出格式。可选地，指令包含任务的正面和/或负面示例。它还可以包含需要遵循的约束和例外。
- en: An optional input, which in our example is the sentence or paragraph for the
    model to evaluate.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选的输入，在我们的例子中是模型要评估的句子或段落。
- en: The output, which is the correct answer to the task in the format specified
    in the instruction.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出，即任务在指令中指定格式的正确答案。
- en: Note
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Fine-tuning datasets can be either multi-task or single-task. Multi-task datasets
    are used for instruction-tuning. In general, instruction-tuning can be treated
    as an intermediate step before single-task fine-tuning. For example, you can take
    a T5 language model, instruction-tune it with FLAN to create FLAN-T5, and then
    further fine-tune it with your task-specific dataset. This approach is [shown](https://oreil.ly/e-MVh)
    to yield better results than directly fine-tuning on T5 alone.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 微调数据集可以是多任务或单任务的。多任务数据集用于指令微调。通常，指令微调可以被视为单任务微调之前的一个中间步骤。例如，你可以使用T5语言模型，用FLAN对其进行指令微调以创建FLAN-T5，然后进一步使用你的特定任务数据集对其进行微调。这种方法[显示](https://oreil.ly/e-MVh)比仅在T5上直接微调有更好的结果。
- en: Later in this chapter, we will learn how to create task-specific datasets. First,
    let’s look at how we can create instruction-tuning datasets.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将学习如何创建特定任务的数据集。首先，让我们看看我们如何创建指令微调数据集。
- en: There are plenty of instruction-tuned LLMs available, both open source and proprietary.
    Why do we want to instruction-tune the LLM ourselves? Public datasets are too
    general, lack diversity, and are primarily geared to general usage. Leveraging
    your domain expertise and knowledge of intended use cases to construct the dataset
    can be highly effective. In fact, at my company, which specializes in the financial
    domain, this technique delivered the single largest boost in performance.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的指令微调LLM有很多，既有开源的也有专有的。我们为什么想自己指令微调LLM呢？公共数据集过于通用，缺乏多样性，并且主要针对通用用途。利用你的领域专业知识和对预期用例的了解来构建数据集可以非常有效。事实上，在我公司，我们专注于金融领域，这项技术带来了单次最大的性能提升。
- en: 'Approaches to creating instruction-tuning datasets include:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 创建指令微调数据集的方法包括：
- en: Utilizing publicly available instruction-tuning datasets
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用公开可用的指令微调数据集
- en: Transforming traditional fine-tuning datasets into instruction-tuning datasets
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将传统微调数据集转换为指令微调数据集
- en: Starting with manually crafted seed examples, followed by optionally augmenting
    the dataset by utilizing an LLM to generate similar examples
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从手动制作的种子示例开始，然后可选地通过利用LLM生成类似示例来扩充数据集
- en: Next, let’s examine these methods more closely.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更详细地考察这些方法。
- en: Utilizing Publicly Available Instruction-Tuning Datasets
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用公开可用的指令微调数据集
- en: If your use case is sufficiently general or popular, you may be able to use
    publicly available datasets for instruction-tuning. The following table lists
    some popular instruction-tuning datasets, along with information on their creators,
    sizes, and creation process.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的用例足够通用或流行，你可能能够使用公开可用的数据集进行指令微调。以下表格列出了一些流行的指令微调数据集，以及它们创建者、大小和创建过程的信息。
- en: Table 6-1\. Popular instruction-tuning datasets
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-1. 流行的指令微调数据集
- en: '| Name | Size | Created by | Created using |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 大小 | 创建者 | 创建方式 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| OIG | 43M | [Ontocord](https://www.ontocord.ai/) | Rule-based  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| OIG | 43M | [Ontocord](https://www.ontocord.ai/) | 基于规则 |'
- en: '| FLAN | 4.4M | Google | Templates |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| FLAN | 4.4M | Google | 模板 |'
- en: '| P3 (Public Pool of Prompts) | 12M | Big Science | Templates |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| P3（公共提示池） | 12M | 大科学 | 模板 |'
- en: '| Natural Instruction | 193K | Allen AI | Templates |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 自然指令 | 193K | Allen AI | 模板 |'
- en: '| Unnatural Instructions | 240K | [Honovich et al.](https://github.com/orhonovich/unnatural-instructions),
    Meta | LLMs |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 不自然的指令 | 240K | [Honovich等人](https://github.com/orhonovich/unnatural-instructions),
    Meta | LLMs |'
- en: '| LIMA (Less Is More for Alignment) | 1K | [Zhou et al.](https://arxiv.org/abs/2305.11206),
    Meta | Templates |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| LIMA (Less Is More for Alignment) | 1K | [周等人](https://arxiv.org/abs/2305.11206),
    Meta | 模板 (Templates) |'
- en: '| Self-Instruct | 52K | [Wang et al.](https://github.com/yizhongw/self-instruct)
    | LLMs |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Self-Instruct | 52K | [王等人](https://github.com/yizhongw/self-instruct) |
    大型语言模型 (LLMs) |'
- en: '| Evol-Instruct | 52K | [Xu et al.](https://arxiv.org/abs/2304.12244) | LLMs
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Evol-Instruct | 52K | [徐等人](https://arxiv.org/abs/2304.12244) | 大型语言模型 (LLMs)
    |'
- en: '| InstructWild v2 | 110K | [Ni et al.](https://github.com/XueFuzhao/InstructionWild)
    | LLMs |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| InstructWild v2 | 110K | [Ni 等人](https://github.com/XueFuzhao/InstructionWild)
    | 大型语言模型 (LLMs) |'
- en: '| Alpaca | 52K | Stanford | LLMs |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | 52K | 斯坦福 | 大型语言模型 (LLMs) |'
- en: '| Guanaco | 534K | [Dettmers et al.](https://arxiv.org/abs/2305.14314) | LLMs
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco | 534K | [Dettmers 等人](https://arxiv.org/abs/2305.14314) | 大型语言模型
    (LLMs) |'
- en: '| Vicuna | 70K | LMSYS | Human conversations |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna | 70K | LMSYS | 人际对话 |'
- en: '| OpenAssistant | 161K | Open Assistant | Human conversations |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| OpenAssistant | 161K | 开放助手 | 人际对话 |'
- en: Let’s go through fine-tuned language net (FLAN), one of the most popular instruction-tuning
    datasets in detail. Understanding how it was constructed will provide you with
    roadmaps to create your own instruction-tuning datasets. Most publicly available
    instruction-tuning datasets are meant to augment an LLM that will be used for
    open-ended tasks, as opposed to domain-specific use cases.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解一下微调语言网络 (FLAN)，这是最受欢迎的指令微调数据集之一。了解它是如何构建的，将为你提供创建自己指令微调数据集的路线图。大多数公开可用的指令微调数据集旨在增强用于开放性任务的LLM，而不是特定领域的用例。
- en: 'FLAN is actually a collection of several datasets. The [FLAN collection](https://oreil.ly/SrXV-),
    published in 2022, is composed of five components:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN实际上是一组几个数据集的集合。2022年发布的[FLAN集合](https://oreil.ly/SrXV-)由五个组件组成：
- en: FLAN 2021
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FLAN 2021
- en: T0
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T0
- en: Super-natural Instructions
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超自然指令
- en: Chain-of-Thought
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维链
- en: Dialog
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话
- en: The original FLAN 2021 datasets were one of the pioneering instruction-tuning
    datasets, which were used to train FLAN-T5\. The FLAN 2021 datasets were constructed
    by taking existing academic NLP datasets and converting them to the instruction
    format using instruction templates. The templates were manually constructed, with
    ten templates created for each task. The templates are available [here](https://oreil.ly/DNKCv).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 原始FLAN 2021数据集是早期指令微调数据集之一，用于训练FLAN-T5。FLAN 2021数据集是通过将现有的学术NLP数据集转换为使用指令模板的指令格式来构建的。这些模板是手动构建的，每个任务创建了十个模板。模板可在[这里](https://oreil.ly/DNKCv)找到。
- en: 'Here is how a template list for a task looks, as drawn from the [templates.py](https://oreil.ly/DNKCv)
    file in the FLAN GitHub repo. Our example task is text summarization on the CNN/DailyMail
    news dataset:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个任务模板列表的示例，它来自FLAN GitHub仓库中的[templates.py](https://oreil.ly/DNKCv)文件。我们的示例任务是CNN/DailyMail新闻数据集上的文本摘要：
- en: '[PRE14]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that the last three instructions represent an inverted version of the task,
    where given a summary, the model is encouraged to write the entire article. This
    has been done to increase the diversity of the instructions at scale.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最后三个指令代表任务的倒置版本，即给定一个摘要，模型被鼓励写出整篇文章。这样做是为了增加大规模指令的多样性。
- en: 'Rather than painstakingly constructing these templates by hand, can we automate
    their generation using LLMs? Yes, this is possible. We can leverage LLMs to generate
    more diverse templates. When I asked my favorite LLM to generate similar instructions
    to a news summarization task template provided in the prompt, it came up with:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否通过使用LLMs来自动化这些模板的构建过程呢？是的，这是可能的。我们可以利用LLMs生成更多样化的模板。当我要求我最喜欢的LLM生成与提示中提供的新闻摘要任务模板类似的指令时，它提出了以下内容：
- en: '[PRE15]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, the generated templates reflect various ways of expressing the
    summarization task.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，生成的模板反映了表达摘要任务的各种方式。
- en: 'For classification tasks, it is recommended to append the instruction with
    an *Options* clause. This introduces the LLM to the output space and can thus
    concentrate the probability mass over the defined label space. Without this guidance,
    the LLM would distribute its probability across several different tokens that
    express the same concept, for example there are several different ways of expressing
    the *True* label in a binary classification task. An example prompt is: “Identify
    the tone of this text. OPTIONS: happy, sad, neutral.”'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，建议在指令后附加一个 *Options* 子句。这向 LLM 介绍了输出空间，因此可以集中概率质量在定义的标签空间上。没有这种指导，LLM
    将将其概率分布到表示相同概念的几个不同标记上，例如在二元分类任务中，有几种不同的方式来表达 *True* 标签。一个示例提示是：“确定这段文字的语气。选项：快乐、悲伤、中性。”
- en: 'Constructing these prompts manually can be a tedious exercise. The [*promptsource*
    tool](https://oreil.ly/WIyOq) enables you to create, access, and apply prompts
    through a graphical user interface tool or through the promptsource Python library.
    Here is an example from the Public Pool of Prompts (P3) collection for the paraphrasing
    task, constructed by Big Science, which is available through the promptsource
    tool. P3 prompts consist of an Input template, a Target template, and an Answer
    Choices template:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 手动构建这些提示可能是一项繁琐的工作。[*promptsource* 工具](https://oreil.ly/WIyOq) 允许您通过图形用户界面工具或通过
    promptsource Python 库创建、访问和应用提示。以下是从公共提示池（P3）收集的用于释义任务的示例，由 Big Science 构建，可通过
    promptsource 工具获取。P3 提示由输入模板、目标模板和答案选项模板组成：
- en: '[PRE16]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Another key component of the FLAN collection is the [Super-NaturalInstructions
    dataset](https://oreil.ly/D_rv_). This dataset contains very rich descriptions
    of instructions that contain not just task definitions, but also positive and
    negative examples, constraints, and things to watch out for. The answers are enriched
    with explanations on why the answer was chosen. The effectiveness of adding explanations
    to the answer is not yet determined.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN 收集的关键组成部分之一是 [Super-NaturalInstructions 数据集](https://oreil.ly/D_rv_)。这个数据集包含了非常丰富的指令描述，不仅包括任务定义，还包括正例、反例、约束条件和需要注意的事项。答案被丰富了为什么选择该答案的解释。添加解释到答案的有效性尚未确定。
- en: 'Here is an example of such a task from the Super-NaturalInstructions dataset:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从 Super-NaturalInstructions 数据集中这样一个任务的例子：
- en: '[PRE17]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s now look at datasets that are constructed with the help of LLMs.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看在 LLM 的帮助下构建的数据集。
- en: LLM-Generated Instruction-Tuning Datasets
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 生成的指令微调数据集
- en: As seen earlier, hand-constructing these datasets can be painstaking, and paraphrasing/synthetic
    data generation is where LLMs shine. Therefore, we can leverage LLMs to generate
    our instruction-tuning datasets. The [Self-Instruct](https://oreil.ly/HVBfK) and
    [Unnatural Instructions papers](https://oreil.ly/1wV_G) are the first attempts
    in this regard. Both start from a seed set of high-quality hand-generated examples,
    and then in a few-shot setting, ask the LLM to generate similar examples with
    more diverse linguistic expressions.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，手动构建这些数据集可能非常耗时，而释义/合成数据生成正是 LLM 发挥优势的地方。因此，我们可以利用 LLM 来生成我们的指令微调数据集。[Self-Instruct](https://oreil.ly/HVBfK)
    和 [Unnatural Instructions 论文](https://oreil.ly/1wV_G) 是这方面的首次尝试。两者都是从一组高质量的手动生成示例开始的，然后在几样本设置中，要求
    LLM 生成具有更多样化语言表达的类似示例。
- en: Given an instruction, a combination of input-first and output-first is shown
    to be beneficial for generating input-output pairs. Typically, you would generate
    input-output pairs using an input-first approach, where the LLM is asked to generate
    an input instance for the given instruction and subsequently asked to generate
    the output label for that input. However, this approach might lead to label imbalance
    as shown in [Wang et al.](https://oreil.ly/hYFYH), with certain labels being overrepresented.
    Therefore, it is a good approach to mix output-first generation, where you ask
    the LLM to generate the output label first and then ask it to generate an input
    text that satisfies the label.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个指令，输入优先和输出优先的组合被证明对生成输入-输出对有益。通常，你会使用输入优先的方法来生成输入-输出对，即要求 LLM 为给定的指令生成一个输入实例，然后要求它为该输入生成输出标签。然而，这种方法可能会导致标签不平衡，如
    [Wang 等人](https://oreil.ly/hYFYH) 所示，某些标签被过度表示。因此，混合输出优先的生成方法是一个好方法，即先要求 LLM 生成输出标签，然后要求它生成满足该标签的输入文本。
- en: Warning
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It is against OpenAI’s policies to use its outputs to generate data that can
    be used to train a competing model. While there are several public instruction-tuning
    datasets that have been synthetically generated using GPT-4, they are technically
    violating OpenAI’s terms of service. I recommend using open source LLMs for synthetic
    data generation instead.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenAI的输出生成可用于训练竞争模型的训练数据违反了OpenAI的政策。虽然有几个使用GPT-4合成的公共指令微调数据集，但它们在技术上违反了OpenAI的服务条款。我建议使用开源LLM来生成合成数据。
- en: Simply asking an LLM to generate similar examples to your seed set may not give
    you the desired results. You want a diverse but relevant set of examples, and
    it is easy for your LLM to drift into territory that ends up generating spurious
    examples outside of your desired distribution.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地要求LLM生成与你的种子集相似的示例可能不会给你带来期望的结果。你想要一个多样化但相关的示例集，而且你的LLM很容易偏离你的期望分布，最终生成虚假的示例。
- en: Note
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'How large should your instruction-tuning dataset be? The [“LIMA: Less Is More
    for Alignment” paper](https://oreil.ly/z0BWh) shows that you need only a few thousand
    high-quality examples to effectively fine-tune a model.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '你的指令微调数据集应该有多大？[“LIMA: Less Is More for Alignment”论文](https://oreil.ly/z0BWh)显示，你只需要几千个高质量示例就能有效地微调模型。'
- en: 'Xu et al. propose [Evol-Instruct](https://oreil.ly/9nw3G), a structured way
    to generate these synthetic instructions by making controlled edits to the seed
    examples. The process consists of three steps:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Xu等人提出了[Evol-Instruct](https://oreil.ly/9nw3G)，这是一种通过在种子示例上进行有控制的编辑来生成这些合成指令的结构化方法。该过程包括三个步骤：
- en: 'Instruction evolution: The seed examples are evolved using in-depth and in-breadth
    strategies. In-depth evolution increases the complexity and difficulty of the
    original instruction through five types of prompts:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指令进化：通过深度和广度策略进化种子示例。深度进化通过五种类型的提示增加了原始指令的复杂性和难度：
- en: Adding constraints
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加约束
- en: Increasing reasoning steps
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加推理步骤
- en: Asking deeper questions
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出更深层次的问题
- en: Asking more specific questions
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出更具体的问题
- en: Increasing the complexity of the input
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加输入的复杂性
- en: In-breadth evolution increases topic coverage by generating a completely new
    instruction from the same domain as the original instruction.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在广度上进化通过从与原始指令相同领域的完全新的指令生成来增加主题覆盖范围。
- en: 'Response generation: The response for the evolved instruction is generated,
    either using humans or LLMs.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 响应生成：对进化指令的响应是通过使用人类或LLM生成的。
- en: 'Candidate filtering: Candidate instances that do not meet quality criteria
    are filtered out. You could use either heuristics or LLMs for candidate filtering.'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 候选过滤：不符合质量标准的候选实例被过滤掉。你可以使用启发式方法或LLM进行候选过滤。
- en: Note
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why not pre-train on instruction-tuning datasets? If instruction-tuning is a
    necessary step after pre-training a model, why don’t we just pre-train the model
    using an instruction-tuning dataset? It is indeed possible, but these datasets
    are hard to construct at scale without incurring a significant drop in quality.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不在指令微调数据集上预训练？如果指令微调是模型预训练后的一个必要步骤，为什么我们不直接使用指令微调数据集来预训练模型？这确实可能，但这些数据集在规模上难以构建，而不会导致质量显著下降。
- en: We need not wait until someone releases a massive dataset to reap the benefits
    of instruction-tuning during the pre-training phase. It has been [shown](https://oreil.ly/tfO4a)
    that mixing instruction-tuning data during pre-training is beneficial.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要等到有人发布一个大规模数据集，才能在预训练阶段获得指令微调的好处。已经[证明](https://oreil.ly/tfO4a)，在预训练期间混合指令微调数据是有益的。
- en: Summary
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we underscored the inevitability of needing to fine-tune models
    to solve more complex tasks. We performed a deep dive of the fine-tuning process
    and highlighted the tradeoffs involved in selecting hyperparameters. We also showed
    the uncanny effectiveness of instruction-tuning along with pointers on how to
    create your own instruction-tuning datasets.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们强调了需要微调模型以解决更复杂任务的必然性。我们对微调过程进行了深入研究，并突出了在选择超参数时涉及的权衡。我们还展示了指令微调的非凡有效性，并提供了如何创建自己的指令微调数据集的指导。
- en: In the next chapter, we will discuss more advanced techniques for updating an
    LLM’s parameters, including continual pre-training, parameter efficient fine-tuning,
    and model merging.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论更新LLM参数的更高级技术，包括持续预训练、参数高效的微调和模型合并。
