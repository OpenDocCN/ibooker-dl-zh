["```py\n# 1\\. Setup and Dependencies\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\nimport evaluate\nimport numpy as np\n```", "```py\n# 2\\. Load and Examine Data\ndataset = load_dataset(\"imdb\")  # Movie reviews for sentiment analysis\nprint(f\"Train size: {len(dataset['train'])}\")\nprint(f\"Test size: {len(dataset['test'])}\")\n```", "```py\nTrain size: 25000\nTest size: 25000\n```", "```py\n# 3\\. Initialize Model and Tokenizer\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=2\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n```", "```py\n# 4\\. Preprocess Data\ndef preprocess_function(examples):\n   result = tokenizer(\n       examples[\"text\"],\n       truncation=True,\n       max_length=512,\n       padding=True\n   )\n   # Trainer expects a column called labels, so copy over from label\n   result[\"labels\"] = examples[\"label\"]\n   return result\n\ntokenized_dataset = dataset.map(\n   preprocess_function,\n   batched=True,\n   remove_columns=dataset[\"train\"].column_names\n)\n```", "```py\n# 5\\. Create Data Collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n```", "```py\n# 6\\. Define Metrics\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return metric.compute(predictions=predictions, references=labels)\n```", "```py\n# 7\\. Configure Training\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=500,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=False,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    fp16=True\n)\n```", "```py\n# 8\\. Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n```", "```py\n# 9\\. Train and Evaluate\ntrain_results = trainer.train()\nprint(f\"\\nTraining results: {train_results}\")\n\neval_results = trainer.evaluate()\nprint(f\"\\nEvaluation results: {eval_results}\")\n```", "```py\nTraining results:\nTrainOutput(global_step=585,\n            training_loss=0.18643947177463108,\n            metrics={'train_runtime': 597.9931,\n            'train_samples_per_second': 125.42,\n            'train_steps_per_second': 0.978,\n            'total_flos': 1.968912649469952e+16,\n            'train_loss': 0.18643947177463108,\n            'epoch': 2.9923273657289})\nEvaluation results:\n            {'eval_loss': 0.18489666283130646,\n            'eval_accuracy': 0.93596,\n            'eval_runtime': 63.8406,\n            'eval_samples_per_second': 391.601,\n            'eval_steps_per_second': 48.95,\n            'epoch': 2.9923273657289}\n```", "```py\n# 10\\. Save Model\ntrainer.save_model(\"./final_model\")\n\n```", "```py\n# 11\\. Example Usage\ndef predict_sentiment(text):\n    inputs = tokenizer(\n        text,\n        truncation=True,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n```", "```py\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n\n```", "```py\n    positive_prob = predictions[0][1].item()\n    return {\n        'sentiment': 'positive' if positive_prob > 0.5 else 'negative',\n        'confidence': positive_prob if positive_prob > 0.5 else 1 – positive_prob\n    }\n```", "```py\n# Test prediction\ntest_text = \"This movie was absolutely fantastic! The acting was superb.\"\nresult = predict_sentiment(test_text)\nprint(f\"\\nTest prediction for '{test_text}':\")\nprint(f\"Sentiment: {result['sentiment']}\")\nprint(f\"Confidence: {result['confidence']:.2%}\")\n```", "```py\nTest prediction for 'This movie was absolutely fantastic! \n                     The acting was superb.':\nSentiment: positive\nConfidence: 99.16%\n```", "```py\n# Data preparation\ndataset = load_dataset(\"imdb\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmax_length = 512\nnum_virtual_tokens = 20\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length - num_virtual_tokens\n    )\n```", "```py\n# Use only 5000 examples for training\ntrain_size = 5000\nnp.random.seed(42)\ntrain_indices = np.random.choice(len(dataset[\"train\"]), train_size, \n                                                        replace=False)\ntest_indices = np.random.choice(len(dataset[\"test\"]), train_size, replace=False)\n\ntokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\ntokenized_test = dataset[\"test\"].map(tokenize_function, batched=True)\n```", "```py\n# Create subset for training\ntokenized_train = tokenized_train.select(train_indices)\ntokenized_test = tokenized_test.select(test_indices)\n\n```", "```py\ntokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \n                                                  \"attention_mask\", \"label\"])\ntokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \n                                                 \"attention_mask\", \"label\"])\n\n```", "```py\ntrain_dataloader = DataLoader(tokenized_train, batch_size=64, shuffle=True)\neval_dataloader = DataLoader(tokenized_test, batch_size=128)\n```", "```py\n# Define the model\nmodel = PromptTuningBERT(num_virtual_tokens=num_virtual_tokens, \n                         max_length=max_length)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel.to(device)\n```", "```py\nclass PromptTuningBERT(nn.Module):\n    def __init__(self, model_name=\"bert-base-uncased\", \n                       num_virtual_tokens=50, \n                       max_length=512):\n        super().__init__()\n        self.bert = AutoModelForSequenceClassification.from_pretrained(\n                        model_name, \n                        num_labels=2)\n        self.bert.requires_grad_(False)\n\n        self.n_tokens = num_virtual_tokens\n        self.max_length = max_length - num_virtual_tokens\n\n        vocab_size = self.bert.config.vocab_size\n        token_ids = torch.randint(0, vocab_size, (num_virtual_tokens,))\n        word_embeddings = self.bert.bert.embeddings.word_embeddings\n        prompt_embeddings = word_embeddings(token_ids).unsqueeze(0)\n        self.prompt_embeddings = nn.Parameter(prompt_embeddings)\n```", "```py\n        self.bert = AutoModelForSequenceClassification.from_pretrained(\n                        model_name, \n                        num_labels=2)\n```", "```py\nself.bert.requires_grad_(False)\n```", "```py\ntoken_ids = torch.randint(0, vocab_size, (num_virtual_tokens,))\n```", "```py\nword_embeddings = self.bert.bert.embeddings.word_embeddings\nprompt_embeddings = word_embeddings(token_ids).unsqueeze(0)\n```", "```py\nself.prompt_embeddings = nn.Parameter(prompt_embeddings)\n```", "```py\ndef forward(self, input_ids, attention_mask, labels=None):\n    batch_size = input_ids.shape[0]\n    input_ids = input_ids[:, :self.max_length]\n    attention_mask = attention_mask[:, :self.max_length]\n\n    embeddings = self.bert.bert.embeddings.word_embeddings(input_ids)\n    prompt_embeddings = self.prompt_embeddings.expand(batch_size, –1, –1)\n    inputs_embeds = torch.cat([prompt_embeddings, embeddings], dim=1)\n\n    prompt_attention_mask = torch.ones(batch_size, self.n_tokens, \n                                       device=attention_mask.device)\n    attention_mask = torch.cat([prompt_attention_mask, attention_mask], dim=1)\n\n    return self.bert(\n        inputs_embeds=inputs_embeds,\n        attention_mask=attention_mask,\n        labels=labels,\n        return_dict=True\n    )\n```", "```py\n    batch_size = input_ids.shape[0]\n    input_ids = input_ids[:, :self.max_length]\n    attention_mask = attention_mask[:, :self.max_length]\n\n```", "```py\nembeddings = self.bert.bert.embeddings.word_embeddings(input_ids)\n```", "```py\nprompt_embeddings = self.prompt_embeddings.expand(batch_size, –1, –1)\n\n```", "```py\ninputs_embeds = torch.cat([prompt_embeddings, embeddings], dim=1)\n```", "```py\nprompt_attention_mask = torch.ones(batch_size, self.n_tokens, \n                                   device=attention_mask.device)\n\nattention_mask = torch.cat([prompt_attention_mask, attention_mask], \n                            dim=1)\n\n```", "```py\n    return self.bert(\n        inputs_embeds=inputs_embeds,\n        attention_mask=attention_mask,\n        labels=labels,\n        return_dict=True\n    )\n\n```", "```py\noptimizer = AdamW(model.parameters(), lr=1e-2)\n```", "```py\nnum_epochs = 3\n\n# Perform the training\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n```", "```py\nfor batch in tqdm(train_dataloader, \n                  desc=f'Training Epoch {epoch + 1}'):\n    batch = {k: v.to(device) for k, v in batch.items()}\n    labels = batch.pop('label')\n    outputs = model(**batch, labels=labels)\n```", "```py\nbatch = {k: v.to(device) for k, v in batch.items()}\n```", "```py\nlabels = batch.pop('label')\n```", "```py\noutputs = model(**batch, labels=labels)\n```", "```py\nloss = outputs.loss\ntotal_train_loss += loss.item()\n\nloss.backward()\n```", "```py\nclip_grad_norm_(model.parameters(), max_grad_norm)  # Add here\noptimizer.step()\noptimizer.zero_grad()\n```", "```py\nmodel.eval()\nval_accuracy = []\ntotal_val_loss = 0\n```", "```py\nwith torch.no_grad():\n    for batch in tqdm(eval_dataloader, desc='Validating'):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        labels = batch.pop('label')\n\n        outputs = model(**batch, labels=labels)\n        total_val_loss += outputs.loss.item()\n\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        val_accuracy.extend((predictions == labels).cpu().numpy())\n```", "```py\navg_train_loss = total_train_loss / len(train_dataloader)\navg_val_loss = total_val_loss / len(eval_dataloader)\nval_accuracy = np.mean(val_accuracy)\n\nprint(f\"\\nEpoch {epoch + 1}:\")\nprint(f\"Average training loss: {avg_train_loss:.4f}\")\nprint(f\"Average validation loss: {avg_val_loss:.4f}\")\nprint(f\"Validation accuracy: {val_accuracy:.4f}\")\n```", "```py\nTraining Epoch 1: 100%|██████████| 79/79 [01:01<00:00, 1.28it/s]\nValidating: 100%|██████████| 40/40 [00:27<00:00, 1.44it/s]\n\nEpoch 1:\nAverage training loss: 0.6559\nAverage validation loss: 0.6037\nValidation accuracy: 0.8036\nTraining Epoch 2: 100%|██████████| 79/79 [01:01<00:00, 1.28it/s]\nValidating: 100%|██████████| 40/40 [00:27<00:00, 1.44it/s]\n\nEpoch 2:\nAverage training loss: 0.6112\nAverage validation loss: 0.5854\nValidation accuracy: 0.8386\nTraining Epoch 3: 100%|██████████| 79/79 [01:01<00:00, 1.28it/s]\nValidating: 100%|██████████| 40/40 [00:27<00:00, 1.44it/s]\n\nEpoch 3:\nAverage training loss: 0.5799\nAverage validation loss: 0.5270\nValidation accuracy: 0.8736\n\n```", "```py\ntorch.save(model.prompt_embeddings, \"imdb_prompt_embeddings.pt\")\n```", "```py\nclass PromptTunedBERTInference:\n    def __init__(self, model_name=\"bert-base-uncased\", \n                       prompt_path=\"imdb_prompt_embeddings.pt\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model =  \n                  AutoModelForSequenceClassification.from_pretrained(\n                                            model_name, num_labels=2)\n        self.model.eval()\n        self.prompt_embeddings = torch.load(prompt_path)\n        self.device = \n           torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model.to(self.device)\n```", "```py\nself.model.eval()\n```", "```py\nself.prompt_embeddings = torch.load(prompt_path)\n```", "```py\ndef predict(self, text):\n    inputs = self.tokenizer(text, padding=True, \n                        truncation=True,\n                        max_length=512-self.prompt_embeddings.shape[1],\n                        return_tensors=\"pt\")\n    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n```", "```py\nwith torch.no_grad():\n    embeddings = self.model.bert.embeddings.word_embeddings(\n                                        inputs['input_ids'])\n\n    batch_size = embeddings.shape[0]\n\n    prompt_embeds = self.prompt_embeddings.expand(\n                         batch_size, –1, –1).to(self.device)\n\n    inputs_embeds = torch.cat([prompt_embeds, embeddings], dim=1)\n```", "```py\nattention_mask = inputs['attention_mask']\nprompt_attention = torch.ones(batch_size, self.prompt_embeddings.shape[1],\n                            device=self.device)\nattention_mask = torch.cat([prompt_attention, attention_mask], dim=1)\n```", "```py\noutputs = self.model(inputs_embeds=inputs_embeds,\n                   attention_mask=attention_mask)\n```", "```py\nprobs = torch.nn.functional.softmax(outputs.logits, dim=-1)\nreturn {\"prediction\": outputs.logits.argmax(-1).item(),\n       \"confidence\": probs.max(-1).values.item()}\n```", "```py\n# Usage example\nif __name__ == \"__main__\":\n    model = PromptTunedBERTInference()\n    result = model.predict(\"This movie was great!\")\n    print(f\"Prediction: {'Positive' \n                          if result['prediction'] == 1 else 'Negative'}\")\n    print(f\"Confidence: {result['confidence']:.2f}\")\n\n```"]