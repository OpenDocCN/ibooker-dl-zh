["```py\n# 1\\. Setup and Dependencies\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\nimport evaluate\nimport numpy as np\n```", "```py\n# 2\\. Load and Examine Data\ndataset = load_dataset(\"imdb\")  # Movie reviews for sentiment analysis\nprint(f\"Train size: {len(dataset['train'])}\")\nprint(f\"Test size: {len(dataset['test'])}\")\n```", "```py\nTrain size: 25000\nTest size: 25000\n```", "```py\n# 3\\. Initialize Model and Tokenizer\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=2\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n```", "```py\n# 4\\. Preprocess Data\ndef preprocess_function(examples):\n   result = tokenizer(\n       examples[\"text\"],\n       truncation=True,\n       max_length=512,\n       padding=True\n   )\n   # Trainer expects a column called labels, so copy over from label\n   result[\"labels\"] = examples[\"label\"]\n   return result\n\ntokenized_dataset = dataset.map(\n   preprocess_function,\n   batched=True,\n   remove_columns=dataset[\"train\"].column_names\n)\n```", "```py\n# 5\\. Create Data Collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n```", "```py\n# 6\\. Define Metrics\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return metric.compute(predictions=predictions, references=labels)\n```", "```py\n# 7\\. Configure Training\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=500,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=False,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    fp16=True\n)\n```", "```py\n# 8\\. Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n```", "```py\n# 9\\. Train and Evaluate\ntrain_results = trainer.train()\nprint(f\"\\nTraining results: {train_results}\")\n\neval_results = trainer.evaluate()\nprint(f\"\\nEvaluation results: {eval_results}\")\n```", "```py\nTraining results:\nTrainOutput(global_step=585,\n            training_loss=0.18643947177463108,\n            metrics={'train_runtime': 597.9931,\n            'train_samples_per_second': 125.42,\n            'train_steps_per_second': 0.978,\n            'total_flos': 1.968912649469952e+16,\n            'train_loss': 0.18643947177463108,\n            'epoch': 2.9923273657289})\nEvaluation results:\n            {'eval_loss': 0.18489666283130646,\n            'eval_accuracy': 0.93596,\n            'eval_runtime': 63.8406,\n            'eval_samples_per_second': 391.601,\n            'eval_steps_per_second': 48.95,\n            'epoch': 2.9923273657289}\n```", "```py\n# 10\\. Save Model\ntrainer.save_model(\"./final_model\")\n\n```", "```py\n# 11\\. Example Usage\ndef predict_sentiment(text):\n    inputs = tokenizer(\n        text,\n        truncation=True,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n```", "```py\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n\n```", "```py\n    positive_prob = predictions[0][1].item()\n    return {\n        'sentiment': 'positive' if positive_prob > 0.5 else 'negative',\n        'confidence': positive_prob if positive_prob > 0.5 else 1 – positive_prob\n    }\n```", "```py\n# Test prediction\ntest_text = \"This movie was absolutely fantastic! The acting was superb.\"\nresult = predict_sentiment(test_text)\nprint(f\"\\nTest prediction for '{test_text}':\")\nprint(f\"Sentiment: {result['sentiment']}\")\nprint(f\"Confidence: {result['confidence']:.2%}\")\n```", "```py\nTest prediction for 'This movie was absolutely fantastic! \n                     `The` `acting` `was` `superb``.``':` ```", "```py\n```", "```py`` ```", "```py # Data preparation dataset = load_dataset(\"imdb\") tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") max_length = 512 num_virtual_tokens = 20   def tokenize_function(examples):     return tokenizer(         examples[\"text\"],         padding=\"max_length\",         truncation=True,         max_length=max_length - num_virtual_tokens     ) ```", "```py # Use only 5000 examples for training train_size = 5000 np.random.seed(42) train_indices = np.random.choice(len(dataset[\"train\"]), train_size,                                                          replace=False) test_indices = np.random.choice(len(dataset[\"test\"]), train_size, replace=False)   tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True) tokenized_test = dataset[\"test\"].map(tokenize_function, batched=True) ```", "```py # Create subset for training tokenized_train = tokenized_train.select(train_indices) tokenized_test = tokenized_test.select(test_indices)   ```", "```py tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\",                                                    \"attention_mask\", \"label\"]) tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\",                                                   \"attention_mask\", \"label\"])   ```", "```py train_dataloader = DataLoader(tokenized_train, batch_size=64, shuffle=True) eval_dataloader = DataLoader(tokenized_test, batch_size=128) ```", "```py # Define the model model = PromptTuningBERT(num_virtual_tokens=num_virtual_tokens,                           max_length=max_length)   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   model.to(device) ```", "```py class PromptTuningBERT(nn.Module):     def __init__(self, model_name=\"bert-base-uncased\",                         num_virtual_tokens=50,                         max_length=512):         super().__init__()         self.bert = AutoModelForSequenceClassification.from_pretrained(                         model_name,                          num_labels=2)         self.bert.requires_grad_(False)           self.n_tokens = num_virtual_tokens         self.max_length = max_length - num_virtual_tokens           vocab_size = self.bert.config.vocab_size         token_ids = torch.randint(0, vocab_size, (num_virtual_tokens,))         word_embeddings = self.bert.bert.embeddings.word_embeddings         prompt_embeddings = word_embeddings(token_ids).unsqueeze(0)         self.prompt_embeddings = nn.Parameter(prompt_embeddings) ```", "```py         self.bert = AutoModelForSequenceClassification.from_pretrained(                         model_name,                          num_labels=2) ```", "```py self.bert.requires_grad_(False) ```", "```py token_ids = torch.randint(0, vocab_size, (num_virtual_tokens,)) ```", "```py word_embeddings = self.bert.bert.embeddings.word_embeddings prompt_embeddings = word_embeddings(token_ids).unsqueeze(0) ```", "```py self.prompt_embeddings = nn.Parameter(prompt_embeddings) ```", "```py def forward(self, input_ids, attention_mask, labels=None):     batch_size = input_ids.shape[0]     input_ids = input_ids[:, :self.max_length]     attention_mask = attention_mask[:, :self.max_length]       embeddings = self.bert.bert.embeddings.word_embeddings(input_ids)     prompt_embeddings = self.prompt_embeddings.expand(batch_size, –1, –1)     inputs_embeds = torch.cat([prompt_embeddings, embeddings], dim=1)       prompt_attention_mask = torch.ones(batch_size, self.n_tokens,                                         device=attention_mask.device)     attention_mask = torch.cat([prompt_attention_mask, attention_mask], dim=1)       return self.bert(         inputs_embeds=inputs_embeds,         attention_mask=attention_mask,         labels=labels,         return_dict=True     ) ```", "```py     batch_size = input_ids.shape[0]     input_ids = input_ids[:, :self.max_length]     attention_mask = attention_mask[:, :self.max_length]   ```", "```py embeddings = self.bert.bert.embeddings.word_embeddings(input_ids) ```", "```py prompt_embeddings = self.prompt_embeddings.expand(batch_size, –1, –1)   ```", "```py inputs_embeds = torch.cat([prompt_embeddings, embeddings], dim=1) ```", "```py prompt_attention_mask = torch.ones(batch_size, self.n_tokens,                                     device=attention_mask.device)   attention_mask = torch.cat([prompt_attention_mask, attention_mask],                              dim=1)   ```", "```py     return self.bert(         inputs_embeds=inputs_embeds,         attention_mask=attention_mask,         labels=labels,         return_dict=True     )   ```", "```py optimizer = AdamW(model.parameters(), lr=1e-2) ```", "```py num_epochs = 3   # Perform the training for epoch in range(num_epochs):     model.train()     total_train_loss = 0 ```", "```py for batch in tqdm(train_dataloader,                    desc=f'Training Epoch {epoch + 1}'):     batch = {k: v.to(device) for k, v in batch.items()}     labels = batch.pop('label')     outputs = model(**batch, labels=labels) ```", "```py batch = {k: v.to(device) for k, v in batch.items()} ```", "```py labels = batch.pop('label') ```", "```py outputs = model(**batch, labels=labels) ```", "```py loss = outputs.loss total_train_loss += loss.item()   loss.backward() ```", "```py clip_grad_norm_(model.parameters(), max_grad_norm)  # Add here optimizer.step() optimizer.zero_grad() ```", "```py model.eval() val_accuracy = [] total_val_loss = 0 ```", "```py with torch.no_grad():     for batch in tqdm(eval_dataloader, desc='Validating'):         batch = {k: v.to(device) for k, v in batch.items()}         labels = batch.pop('label')           outputs = model(**batch, labels=labels)         total_val_loss += outputs.loss.item()           predictions = torch.argmax(outputs.logits, dim=-1)         val_accuracy.extend((predictions == labels).cpu().numpy()) ```", "```py avg_train_loss = total_train_loss / len(train_dataloader) avg_val_loss = total_val_loss / len(eval_dataloader) val_accuracy = np.mean(val_accuracy)   print(f\"\\nEpoch {epoch + 1}:\") print(f\"Average training loss: {avg_train_loss:.4f}\") print(f\"Average validation loss: {avg_val_loss:.4f}\") print(f\"Validation accuracy: {val_accuracy:.4f}\") ```", "```py Training Epoch 1: 100%|██████████| 79/79 [01:01<00:00, 1.28it/s] Validating: 100%|██████████| 40/40 [00:27<00:00, 1.44it/s]  Epoch 1: Average training loss: 0.6559 Average validation loss: 0.6037 Validation accuracy: 0.8036 Training Epoch 2: 100%|██████████| 79/79 [01:01<00:00, 1.28it/s] Validating: 100%|██████████| 40/40 [00:27<00:00, 1.44it/s]   Epoch 2: Average training loss: 0.6112 Average validation loss: 0.5854 Validation accuracy: 0.8386 Training Epoch 3: 100%|██████████| 79/79 [01:01<00:00, 1.28it/s] Validating: 100%|██████████| 40/40 [00:27<00:00, 1.44it/s]   Epoch 3: Average training loss: 0.5799 Average validation loss: 0.5270 Validation accuracy: 0.8736  ```", "```py torch.save(model.prompt_embeddings, \"imdb_prompt_embeddings.pt\") ```", "```py class PromptTunedBERTInference:     def __init__(self, model_name=\"bert-base-uncased\",                         prompt_path=\"imdb_prompt_embeddings.pt\"):         self.tokenizer = AutoTokenizer.from_pretrained(model_name)         self.model =                     AutoModelForSequenceClassification.from_pretrained(                                             model_name, num_labels=2)         self.model.eval()         self.prompt_embeddings = torch.load(prompt_path)         self.device =             torch.device('cuda' if torch.cuda.is_available() else 'cpu')         self.model.to(self.device) ```", "```py self.model.eval() ```", "```py self.prompt_embeddings = torch.load(prompt_path) ```", "```py def predict(self, text):     inputs = self.tokenizer(text, padding=True,                          truncation=True,                         max_length=512-self.prompt_embeddings.shape[1],                         return_tensors=\"pt\")     inputs = {k: v.to(self.device) for k, v in inputs.items()} ```", "```py with torch.no_grad():     embeddings = self.model.bert.embeddings.word_embeddings(                                         inputs['input_ids'])       batch_size = embeddings.shape[0]       prompt_embeds = self.prompt_embeddings.expand(                          batch_size, –1, –1).to(self.device)       inputs_embeds = torch.cat([prompt_embeds, embeddings], dim=1) ```", "```py attention_mask = inputs['attention_mask'] prompt_attention = torch.ones(batch_size, self.prompt_embeddings.shape[1],                             device=self.device) attention_mask = torch.cat([prompt_attention, attention_mask], dim=1) ```", "```py outputs = self.model(inputs_embeds=inputs_embeds,                    attention_mask=attention_mask) ```", "```py probs = torch.nn.functional.softmax(outputs.logits, dim=-1) return {\"prediction\": outputs.logits.argmax(-1).item(),        \"confidence\": probs.max(-1).values.item()} ```", "```py # Usage example if __name__ == \"__main__\":     model = PromptTunedBERTInference()     result = model.predict(\"This movie was great!\")     print(f\"Prediction: {'Positive'                            if result['prediction'] == 1 else 'Negative'}\")     print(f\"Confidence: {result['confidence']:.2f}\")   ```", "```py` ```"]