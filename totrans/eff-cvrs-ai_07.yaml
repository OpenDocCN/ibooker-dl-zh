- en: 6 Enhancing responses with retrieval-augmented generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 通过检索增强生成来增强响应
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Enhancing chatbot responses without coding intents
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需编码意图来增强聊天机器人响应
- en: Improving weak understanding with RAG
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RAG改进薄弱理解
- en: Evaluating the advantage of using RAG over traditional search models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估使用RAG相对于传统搜索模型的优势
- en: Selecting the proper RAG techniques for your conversational AI
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的对话式AI选择合适的RAG技术
- en: Assessing and improving the performance of RAG in your conversational AI systems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估和改进你在对话式AI系统中RAG的性能
- en: In previous chapters, we saw the “chatbot doesn’t understand” pain point for
    question-answering bots. We first addressed it by helping the chatbot understand
    more intents, but at some point there are diminishing returns to this strategy.
    Uncommon questions from the “long tail” may never make sense to implement as intents.
    This chapter introduces ways to handle that “long tail,” including search and
    retrieval-augmented generation (RAG). These are great methods for improving a
    chatbot’s weak understanding.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了问答机器人“不理解”的痛点。我们首先通过帮助聊天机器人理解更多意图来解决它，但在这个策略上，回报是递减的。来自“长尾”的不常见问题可能永远不会有意义地实现为意图。本章介绍了处理“长尾”的方法，包括搜索和检索增强生成（RAG）。这些是提高聊天机器人薄弱理解能力的好方法。
- en: We concluded chapter 5 with advice on when to avoid adding new intents, especially
    when dealing with diverse, infrequent domain-related problems. In this chapter,
    we’ll add search capabilities to improve weak understanding.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第5章的结尾给出了关于何时避免添加新意图的建议，尤其是在处理多样化、不频繁的领域相关问题时。在本章中，我们将添加搜索功能来提高薄弱理解。
- en: Both search and RAG allow you to improve a chatbot by adding data and documents
    without programming new intents. This allows you to serve thousands of intents
    with the simplicity of training just a few. The answers provided by these methods
    are more straightforward to change—just change the documents rather than changing
    your chatbot.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索和RAG都允许你通过添加数据和文档来改进聊天机器人，而无需编写新的意图。这使得你能够以仅训练少数几个的简单性来服务数千个意图。这些方法提供的答案更容易更改——只需更改文档而不是更改你的聊天机器人。
- en: Search and RAG can be easier for you as a builder and efficient for your users.
    Let’s explore how a chatbot can evolve using search and RAG capabilities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你作为构建者来说，搜索和RAG可能更容易，对于你的用户来说，它们可能更高效。让我们探索聊天机器人如何通过搜索和RAG功能来进化。
- en: '6.1 Beyond intents: The role of search in conversational AI'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 超出意图：搜索在对话式AI中的作用
- en: Traditional conversational AI centers on understanding user intents. Systems
    are trained to recognize predefined categories of user queries and to provide
    pre-scripted responses.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的对话式AI集中在理解用户意图上。系统被训练来识别预定义的用户查询类别，并提供预编写的响应。
- en: Figure 6.1 illustrates a conceptual intent-based chatbot architecture in its
    simplest form. The chatbot’s classifier detects the intent and determines the
    appropriate dialogue flow. When the classifier cannot identify the user’s intent,
    the answer is a generic “I didn’t understand” response style, leading to the “chatbot
    not understanding me” pain point. Intent-based question-answering is a great way
    to start handling frequently asked questions—you can define an exact answer to
    be given for different question types. Initially, this is quite effective, but
    it breaks down. Users often ask questions that deviate from predefined intents.
    When the predefined intents are insufficient to handle the user’s questions, the
    user may receive irrelevant or incorrect responses, leading to frustration. Further,
    maintaining and evolving these intents requires significant effort.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1展示了基于意图的聊天机器人最简单的概念架构。聊天机器人的分类器检测意图并确定适当的对话流程。当分类器无法识别用户的意图时，答案是通用的“我没有理解”响应风格，导致“聊天机器人不理解我”的痛点。基于意图的问答是处理常见问题的好方法——你可以为不同的问题类型定义一个确切的答案。最初，这非常有效，但会崩溃。用户经常提出偏离预定义意图的问题。当预定义的意图不足以处理用户的问题时，用户可能会收到不相关或不正确的响应，导致挫败感。此外，维护和演进这些意图需要大量的努力。
- en: '![figure](../Images/CH06_F01_Freed2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH06_F01_Freed2.png)'
- en: Figure 6.1 An intent-based chatbot first detects an intent and then maps it
    to an answer.
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1 基于意图的聊天机器人首先检测意图，然后将其映射到答案。
- en: There is a tradeoff between the specificity of the answer and the number of
    variations covered. Figure 6.2 demonstrates an example from the PharmaBot we introduced
    in chapter 3\. The answer is accurate but generic—it detected a question about
    side effects but did not answer all the nuances in the user’s question.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在答案的特定性和覆盖的变体数量之间存在权衡。图6.2展示了我们在第三章中引入的PharmaBot的例子。答案是准确的但通用的——它检测到一个关于副作用的问题，但没有回答用户问题中的所有细微差别。
- en: '![figure](../Images/CH06_F02_Freed2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F02_Freed2.png)'
- en: Figure 6.2 Intent-based systems identify the main theme of an utterance and
    often give a static or generic answer.
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2基于意图的系统识别话语的主旨，并经常给出静态或通用的答案。
- en: 'We can handle this nuance by adding search capabilities. There are two primary
    methods:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加搜索功能来处理这种细微差别。有两种主要方法：
- en: '*Traditional search* supplies the user with documents or passages relevant
    to their query. The user uses these documents to find their answer.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*传统搜索*向用户提供与他们的查询相关的文档或段落。用户使用这些文档来找到他们的答案。'
- en: '*RAG* starts with a search process but expands on (augments) it by summarizing
    the passages into an answer.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RAG*从搜索过程开始，但通过总结段落为答案来扩展（增强）它。'
- en: Figure 6.3 illustrates how RAG refines information retrieval by locating relevant
    document passages and synthesizing them into a specific, contextualized answer.
    Unlike traditional intent-based systems that often respond with fixed or general
    answers, RAG dynamically pulls in content to address the user’s unique query,
    demonstrating how specific passages on ibuprofen and blood pressure are distilled
    into a targeted recommendation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3说明了RAG如何通过定位相关文档段落并将它们综合成特定、情境化的答案来细化信息检索。与传统基于意图的系统不同，这些系统通常以固定或一般的答案响应，RAG动态地拉入内容来回答用户的独特查询，展示了关于布洛芬和血压的具体段落是如何提炼成有针对性的建议的。
- en: '![figure](../Images/CH06_F03_Freed2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F03_Freed2.png)'
- en: Figure 6.3 RAG finds relevant passages and summarizes them, giving a targeted
    answer.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3RAG找到相关段落并总结它们，给出有针对性的答案。
- en: Let’s dive deeper into how we can add these capabilities effectively.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨如何有效地添加这些功能。
- en: 6.1.1 Using search in conversational AI
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 在对话式AI中使用搜索
- en: User questions follow a “short head, long tail” distribution, as shown in figure
    6.4\. This distribution has a high frequency of common or popular questions (the
    short head). Most interactions involve less frequent, niche, or specialized queries
    (the long tail).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 用户问题遵循“短头，长尾”分布，如图6.4所示。这种分布具有高频的常见或流行问题（短头）。大多数交互涉及较少见、利基或专业查询（长尾）。
- en: '![figure](../Images/CH06_F04_Freed2.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F04_Freed2.png)'
- en: Figure 6.4 Distribution of user questions. Intents address the most common,
    high-volume questions, while low-volume, unique questions may necessitate search
    integration.
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4用户问题分布。意图解决最常见的、高频率的问题，而低频率、独特的问题可能需要搜索集成。
- en: For PharmaBot, the short head includes general COVID inquiries, such as vaccine
    information and appointments. Each bot will have a different short head, but it
    will cover the most popular questions. When the chatbot is trained well, these
    questions are recognized with high confidence. Intents afford builders complete
    control over short head queries, albeit potentially overlooking nuanced distinctions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PharmaBot来说，简短头部包括一般的COVID咨询，例如疫苗信息和预约。每个机器人将有一个不同的简短头部，但它将涵盖最常见的问题。当聊天机器人训练得很好时，这些问题会以高置信度被识别。意图允许构建者完全控制简短头部查询，尽管可能会忽略细微的区别。
- en: We saw earlier that PharmaBot did not handle a nuanced question well because
    it used a static intent for `#side_effects`. Figure 6.5 shows PharmaBot handling
    the same nuanced question using traditional search capability.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到PharmaBot没有很好地处理细微的问题，因为它使用了静态的`#side_effects`意图。图6.5显示了PharmaBot使用传统的搜索能力处理相同细微问题的方法。
- en: '![figure](../Images/CH06_F05_Freed2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F05_Freed2.png)'
- en: Figure 6.5 Search finds relevant passages and displays those directly to the
    user, often with links to the source documents.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5搜索找到相关段落并直接显示给用户，通常还带有指向源文档的链接。
- en: The response includes all nuances from the user’s question in this example.
    The passages reference vaccine side effects, ibuprofen, blood pressure, and pain.
    However, the chatbot did not provide a single, cohesive answer. Instead, it offered
    document links and snippets. Users need to combine the answers from those documents
    and passages.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，响应包括了用户问题的所有细微差别。段落引用了疫苗副作用、布洛芬、血压和疼痛。然而，聊天机器人并没有提供一个单一、连贯的回答。相反，它提供了文档链接和片段。用户需要将这些文档和段落的答案结合起来。
- en: 6.1.2 Benefits of traditional search
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 传统搜索的优点
- en: 'Traditional search can complement an intent-based chatbot by enabling it to
    retrieve relevant information from a document repository. The approach offers
    several advantages:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 传统搜索可以通过使聊天机器人能够从文档存储库中检索相关信息来补充基于意图的聊天机器人。这种方法提供了几个优点：
- en: '*Breadth*—The bot can access various materials in your document repository,
    giving it answers to different question types.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*广度*—机器人可以访问您的文档存储库中的各种材料，为不同类型的问题提供答案。'
- en: '*Maintenance*—Adding knowledge to your bot can be as easy as adding or editing
    documents in your repository.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*维护*—将知识添加到您的机器人中可以像在您的存储库中添加或编辑文档一样简单。'
- en: '*Technology*—Search is a well-established technology with mature algorithms
    and implementation methods. It can be implemented with relatively low computational
    resources and infrastructure.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*技术*—搜索是一项成熟的技术，具有成熟的算法和实现方法。它可以用相对较低的计算资源和基础设施来实现。'
- en: '*Speed*—While slower than a static intent-based response, traditional search
    executes reasonably quickly.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*速度*—虽然比静态的基于意图的回答慢，但传统搜索执行得相当快。'
- en: Thus, search is an excellent complement to an intent-based system. The most
    common way to combine intents and search is to use a confidence threshold in the
    chatbot’s natural language understanding (NLU) component. The NLU attempts to
    detect an intent from the user’s utterance. If an intent is detected with high
    confidence, an intent-based answer is returned. Otherwise, the user’s utterance
    is passed to a search component (in some conversational AI systems, this is called
    a *fallback action* or *inten**t*). The high-level architecture is illustrated
    in figure 6.6.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，搜索是意图系统的一个很好的补充。将意图和搜索最常见的结合方式是在聊天机器人的自然语言理解（NLU）组件中使用置信度阈值。NLU试图从用户的表述中检测意图。如果检测到意图的置信度较高，则返回基于意图的回答。否则，将用户的表述传递到搜索组件（在某些对话AI系统中，这被称为*回退操作*或*意图*）。高级架构如图6.6所示。
- en: '![figure](../Images/CH06_F06_Freed2.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH06_F06_Freed2.png)'
- en: Figure 6.6 Intents and searches have complementary functions. A search-augmented
    bot uses intent-based answers when it recognizes the utterance with high confidence;
    otherwise, it defers to search. Using intents and search together improves chatbot
    capabilities, but this approach still has some limitations.
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6 意图和搜索具有互补功能。一个搜索增强型机器人当它对识别到的表述有高置信度时，会使用基于意图的回答；否则，它将推迟到搜索。使用意图和搜索一起可以提高聊天机器人的能力，但这种方法仍然存在一些局限性。
- en: 6.1.3 Drawbacks of traditional search
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 传统搜索的缺点
- en: 'Two fundamental problem areas exist when integrating traditional search with
    chatbot applications: the quality of search results and the user experience of
    how the search results are presented.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当将传统搜索与聊天机器人应用集成时，存在两个基本问题领域：搜索结果的质量以及搜索结果呈现的用户体验。
- en: 'One major drawback to search quality is its reliance on keyword matching, which
    may be inaccurate or brittle, depending on the user’s phrasing. Consider the previous
    example question: “Can I take ibuprofen with my blood pressure medication? My
    arms are sore after getting the vaccine.” This may be converted to “ibuprofen
    blood pressure medication arms sore vaccine,” emphasizing the most relevant keywords
    but losing the nuance of the question.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索质量的一个主要缺点是它依赖于关键词匹配，这可能会根据用户的表述方式而变得不准确或脆弱。考虑之前的例子问题：“我可以和我的血压药物一起服用布洛芬吗？接种疫苗后我的手臂很疼。”这可能会被转换为“布洛芬
    血压 药物 手臂 疼痛 疫苗”，强调最相关的关键词，但失去了问题的细微差别。
- en: Not all search engines limit themselves to keyword matching, but it is ubiquitous.
    Newer search engines support searching by meaning rather than keywords. This approach
    is done with vector databases and will be described more fully in the next section.
    Like traditional searches, vector database searches take an input query and return
    a set of relevant documents and passages.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有搜索引擎都局限于关键词匹配，但这是普遍现象。较新的搜索引擎支持通过意义而非关键词进行搜索。这种方法使用向量数据库，将在下一节中更详细地描述。像传统搜索一样，向量数据库搜索接受一个输入查询，并返回一组相关文档和段落。
- en: The other major drawback of search-based options is the user experience of receiving
    documents and passages. Some of the user experience limitations derive from the
    limited space in a chatbot window and the challenges of presenting multiple search
    results well. These are commonly addressed by showing a small number of results
    (possibly asking the user if they want to see more).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于搜索的选项的另一个主要缺点是接收文档和段落的用户体验。一些用户体验限制源于聊天机器人窗口的空间有限以及展示多个搜索结果所面临的挑战。这些问题通常通过显示少量结果（可能询问用户是否想查看更多）来解决。
- en: Screen real estate is sometimes preserved by showing document links (not the
    passages). In this case, users must leave the chat interface, which disrupts the
    conversation flow and may lead to the user abandoning the chatbot and continuing
    where the document links took them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有时通过显示文档链接（而非段落）来保留屏幕空间。在这种情况下，用户必须离开聊天界面，这会打断对话流程，可能导致用户放弃聊天机器人，继续在文档链接所指向的地方。
- en: Search results are also challenging to handle through a voice interface, leading
    to lengthy readouts and a non-optimal user experience.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过语音界面处理搜索结果也具有挑战性，导致长时间的朗读和非最佳的用户体验。
- en: Most critically, the search does not result in a cohesive answer. Some users
    may prefer to construct their answers from relevant documents. Most users, however,
    are frustrated when a cohesive answer is not given, and they must do the piecing
    together—“Why did I use the chatbot in the first place? I could have searched
    on my own.”
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，搜索并没有产生一个连贯的答案。一些用户可能更喜欢从相关文档中构建自己的答案。然而，当没有给出连贯的答案时，大多数用户都会感到沮丧，他们必须自己拼接——“我最初为什么要使用聊天机器人？我自己也能搜索。”
- en: Note  You might consider combining web search with answer synthesis. While a
    chatbot can create synthesized answers from web search results, this approach
    also has limitations. It requires more sophisticated processing, but it addresses
    the user experience limitations by eliminating the need for users to sift through
    multiple documents. This approach depends heavily on how well individual passages
    match the query, and the summarization may miss nuances or context if the retrieved
    data isn’t comprehensive.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可能考虑将网络搜索与答案综合相结合。虽然聊天机器人可以从网络搜索结果中创建综合答案，但这种方法也有局限性。它需要更复杂的处理，但通过消除用户筛选多份文档的需求，解决了用户体验的局限性。这种方法在很大程度上取决于单个段落与查询的匹配程度，如果检索到的数据不全面，总结可能会遗漏细微差别或上下文。
- en: Search with answer synthesis typically relies on rule-based extraction methods,
    ranking algorithms, keyword matching, or predefined heuristics. The responses
    are presented by combining information from retrieved documents. While this can
    efficiently surface relevant information, it may struggle with incomplete or ambiguous
    queries. The system does not truly “understand” the content. Instead, it selects
    and reformulates existing text, which can lead to missing context, fragmented
    responses, or over-reliance on the most prominent retrieved results rather than
    the most accurate ones. It lacks the flexibility of generative approaches.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于答案综合的搜索通常依赖于基于规则的提取方法、排名算法、关键词匹配或预定义的启发式方法。通过结合检索到的文档中的信息来呈现这些响应。虽然这可以有效地呈现相关信息，但它可能难以处理不完整或模糊的查询。系统并不真正“理解”内容。相反，它选择并重新表述现有文本，这可能导致遗漏上下文、响应碎片化或过度依赖最突出的检索结果而不是最准确的结果。它缺乏生成方法的灵活性。
- en: This is whereRAGis a powerful alternative. RAG doesn’t just pull text from documents—it
    combines retrieval with generation, allowing the chatbot to produce a cohesive,
    contextually aware answer using relevant content from various sources. Unlike
    traditional search and summarization approaches, RAG can adapt to a broader range
    of user questions and provide deeper, more accurate responses by using a combination
    of real-time retrieval and language generation capabilities.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是RAG作为一个强大替代方案的地方。RAG不仅仅是从文档中提取文本——它结合了检索与生成，使得聊天机器人能够使用来自各种来源的相关内容，生成一个连贯、情境感知的答案。与传统搜索和摘要方法不同，RAG可以通过结合实时检索和语言生成能力，适应更广泛的用户问题，并提供更深入、更准确的回答。
- en: The next section will explore how RAG enhances chatbot responses by improving
    accuracy and maintaining context, even with complex or nuanced queries.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将探讨RAG如何通过提高准确性和保持上下文，即使在复杂或细微的查询中，也能增强聊天机器人的响应。
- en: '6.2 Beyond search: Generating answers with RAG'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 超越搜索：使用RAG生成答案
- en: The lack of clear answers is a limitation of traditional search methods. To
    overcome these limitations, we’ll look at RAG as an advanced alternative. At its
    core, RAG combines the strengths of search-based information retrieval with the
    flexibility of generative models, offering a more comprehensive approach to understanding
    and responding to user queries. Most importantly, this response includes an answer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏明确的答案是传统搜索方法的局限性。为了克服这些局限性，我们将探讨RAG作为一个高级替代方案。在其核心，RAG结合了基于搜索的信息检索的优势和生成模型的灵活性，提供了一种更全面的方法来理解和回应用户查询。最重要的是，这个响应包括一个答案。
- en: 6.2.1 Using RAG in conversational AI
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 在对话式AI中使用RAG
- en: RAG combines the best of retrieval and generation techniques to enhance the
    user experience. Like traditional search, it retrieves relevant passages to handle
    long-tail questions. RAG then feeds the passages and the user’s request to generative
    AI, which creates the answer. RAG “augments” the retrieved passages by generating
    an answer, creating a seamless conversational flow, even for complex or long-tail
    queries.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: RAG结合了检索和生成技术的最佳之处，以提升用户体验。像传统搜索一样，它检索相关段落来处理长尾问题。然后RAG将段落和用户的请求输入到生成AI中，生成答案。RAG通过生成答案“增强”检索到的段落，创建一个无缝的对话流程，即使是复杂或长尾查询也是如此。
- en: 'Figure 6.7 shows PharmaBot answering our familiar example using RAG:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7展示了PharmaBot使用RAG回答我们熟悉的例子：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![figure](../Images/CH06_F07_Freed2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F07_Freed2.png)'
- en: Figure 6.7 RAG retrieves relevant passages and augments the response by synthesizing
    the information into a grounded answer.
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7 RAG检索相关段落，并通过综合信息生成一个基于事实的答案来增强响应。
- en: PharmaBot may still retrieve the same passages as the traditional search, but
    now it summarizes them to generate an answer. The answer acknowledges the user’s
    specific concerns and provides tailored advice. Most importantly, the answer is
    grounded in PharmaBot’s source documents—not the generative AI’s general knowledge.
    PharmaBot may provide links to supporting documentation, but it has made the answer
    prominent rather than the document passages. This is a more effortless experience
    for users.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: PharmaBot可能仍然会检索到与传统搜索相同的段落，但现在它会总结它们来生成答案。这个答案承认了用户的特定关注点，并提供了定制建议。最重要的是，这个答案基于PharmaBot的源文档——而不是生成AI的一般知识。PharmaBot可能会提供支持文档的链接，但它已经使答案突出，而不是文档段落。这对用户来说是一个更轻松的体验。
- en: RAG empowers chatbots to better understand user questions, and it streamlines
    development efforts by minimizing the need for explicit intent classification.
    This shift in approach enhances user satisfaction and future-proofs conversational
    AI systems against the evolving landscape of human language and user needs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: RAG赋予聊天机器人更好地理解用户问题的能力，并通过最小化显式意图分类的需求来简化开发工作。这种方法的转变提高了用户满意度，并为对话式AI系统在未来人类语言和用户需求不断变化的环境中提供了保障。
- en: The use of RAG introduces dynamism and diminishes user effort, contingent upon
    the avoidance of hallucinations. While RAG inherently reduces hallucinations,
    it does not eliminate them entirely. Attention should be paid to the quality of
    the retrieved documents, the generative model’s behavior, and when the retrieval
    fails to find relevant documents, as we’ll discuss later.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RAG（检索与生成）引入了动态性并减少了用户努力，前提是避免幻觉。虽然RAG本身可以减少幻觉，但并不能完全消除它们。应关注检索到的文档质量、生成模型的行为，以及当检索未能找到相关文档时的情况，我们将在后面讨论。
- en: When the user interacts with the conversational AI, the retrieval system connects
    to the trusted content sources, executes the search (keyword, semantic, or vector),
    and provides a relevancy score for the retrieved results. A large language model
    (LLM) then augments its response using the retrieved information. It generates
    a response from the retrieved content and presents it to the user through the
    chatbot interface. It may also apply translation if needed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户与对话 AI 交互时，检索系统连接到受信任的内容来源，执行搜索（关键词、语义或向量），并为检索到的结果提供相关性评分。大型语言模型（LLM）随后使用检索到的信息增强其响应。它从检索的内容中生成响应，并通过聊天机器人界面将其呈现给用户。如有需要，它还可以应用翻译。
- en: Like traditional search, RAG can complement intent-based systems. The short
    head is handled via intents, and RAG handles the long tail. The answers are as
    current as the document sources. Documents are easier to update than chatbots!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统搜索一样，RAG 可以补充基于意图的系统。短尾通过意图处理，而 RAG 处理长尾。答案与文档来源一样最新。文档比聊天机器人更容易更新！
- en: 6.2.2 Benefits of RAG
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 RAG 的好处
- en: RAG is a great addition to question-answer bots. By retrieving information from
    your knowledge bases (and not generative AI model knowledge), RAG ensures that
    answers are grounded in accurate and up-to-date data, thus enhancing the bot’s
    credibility and reliability. Additionally, RAG expands the versatility of conversational
    AI systems by enabling them to handle a wide range of user queries and tasks effectively.
    It also reduces the “bot doesn’t understand” and “too much complexity out on the
    user” pain points.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 是问答机器人的一个伟大补充。通过从您的知识库（而不是生成式 AI 模型知识）中检索信息，RAG 确保答案基于准确和最新的数据，从而增强机器人的可信度和可靠性。此外，RAG
    通过使对话 AI 系统能够有效地处理广泛的用户查询和任务，扩展了其多功能性。它还减少了“机器人不理解”和“用户面临过多复杂性”的痛点。
- en: Benefits from RAG include enhanced answer relevance, adaptiveness, and improved
    handover to agents.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 的好处包括增强答案的相关性、适应性和改进的手动转接给代理。
- en: Enhanced relevance
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强相关性
- en: RAG’s essential advantage is its ability to access and integrate the latest
    information from your knowledge bases. The knowledge bases provide more varied
    information than pretrained intents do, and LLMs can adapt knowledge from your
    documents into an answer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 的基本优势在于其访问和整合您知识库最新信息的能力。知识库提供的信息比预训练的意图更加多样化，LLM 可以将您的文档中的知识适应到答案中。
- en: Let’s consider another example from PharmaBot. The initial PharmaBot included
    several intents, including guidance for dining in restaurants. It responded based
    on general guidelines and included a reference to “check local regulations.” There
    is no tailoring of responses to the user—it is a one-size-fits-all answer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再考虑 PharmaBot 的另一个例子。最初的 PharmaBot 包含了几个意图，包括餐馆用餐的指导。它根据一般指南进行响应，并包括“检查当地法规”的引用。没有针对用户的响应定制——这是一个一刀切式的答案。
- en: Table 6.1 compares intent-based and RAG-based versions of PharmaBot. Intent-based
    gives a generic answer, while RAG-based localizes to New York per the user’s question.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1 比较了基于意图和基于 RAG 的 PharmaBot 版本。基于意图的提供通用答案，而基于 RAG 的则根据用户的问题本地化到纽约。
- en: Table 6.1 A comparison of answers for intents and RAG
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 6.1 意图和 RAG 的答案比较
- en: '| Interaction with an intent-based bot | Interaction with a RAG-integrated
    bot |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 基于意图的机器人交互 | 基于 RAG 集成的机器人交互 |'
- en: '| --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `User: What are the current COVID-19 guidelines for restaurants in New York?`
    `PharmaBot: Current COVID-19 guidelines for restaurant dining include limited
    indoor capacity, mask requirements for staff and patrons when not seated, and
    enhanced cleaning protocols. Please check local regulations for the most up-to-date
    information.`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '| `用户：纽约餐馆当前的 COVID-19 指南是什么？` `PharmaBot：餐馆餐饮的当前 COVID-19 指南包括限制室内容量、非就座时工作人员和顾客需佩戴口罩，以及加强清洁程序。请查阅当地法规以获取最新信息。`'
- en: '| `User: What are the current COVID-19 guidelines for restaurants in New York?`
    `PharmaBot: COVID-19 remains a significant concern for New Yorkers. Make sure
    you are up to date on the updated COVID-19 vaccines for the 2023-2024 season that
    offer protection against emerging variants and enhance overall immunity. Masks
    continue to be effective tools in mitigating the spread of COVID-19 and other
    respiratory viruses, reducing the transmission of droplets during various activities.
    To minimize the risk of contracting COVID-19, individuals are advised to wear
    masks in crowded indoor public settings.`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '| `用户：纽约餐馆当前的COVID-19指南是什么？` `PharmaBot：COVID-19仍然是纽约人的一个重大关注点。确保您了解2023-2024季节更新的COVID-19疫苗，这些疫苗可以提供对新兴变异株的保护并增强整体免疫力。口罩仍然是减轻COVID-19和其他呼吸道病毒传播的有效工具，在各项活动中减少飞沫传播。为了最大限度地减少感染COVID-19的风险，建议个人在拥挤的室内公共场所佩戴口罩。`'
- en: 'Source: NYC Health'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：纽约市卫生部门
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The RAG-integrated chatbot acknowledges the user’s location in New York City.
    It provides more specific information tailored to the local guidelines and regulations,
    pulled from the current NYC Health Guidelines and summarized for restaurant dining.
    This personalized approach enhances the user experience by delivering more relevant
    and actionable guidance based on the user’s context.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 集成RAG的聊天机器人承认用户位于纽约市。它提供更具体的信息，针对当地指南和法规，从当前的纽约市卫生指南中提取并总结，以适应餐馆就餐。这种个性化的方法通过根据用户的环境提供更相关和可操作的指导来增强用户体验。
- en: Adaptiveness
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 适应性
- en: On the conversational side, another advantage of RAG is in the way the conversational
    AI generates the response—RAG can adjust its generated response to the style of
    the user’s question. The response can be similarly more formal if the user’s tone
    is more fact-seeking. The responses to the user’s unique question are generated
    in real time. Sometimes users expect a concise and direct answer (“Yes or no,
    are there restrictions on dining in restaurants?”*),* and sometimes they expect
    a longer and more complex response (“Can I take my extended family to a restaurant,
    and will we have to wear masks?”). Both questions may use the same source documents
    but will receive very different answers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话方面，RAG的另一个优点在于对话式人工智能生成响应的方式——RAG可以调整其生成的响应以适应用户问题的风格。如果用户的语气更倾向于寻求事实，则响应可以相应地更加正式。对用户独特问题的响应是实时生成的。有时用户期望一个简洁直接的答案（“是或否，餐馆就餐有限制吗？”*），*有时他们期望一个更长且更复杂的回答（“我可以带我的大家庭去餐馆，我们是否需要戴口罩？”）。这两个问题可能使用相同的源文档，但会收到非常不同的答案。
- en: Handover to human agent (or not answering)
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转接到人工代理（或不回答）
- en: There are occasions when the conversational AI cannot find a definite response.
    In these cases, it is better for the bot to answer that it doesn’t know or to
    transfer the user to a human agent. Figure 6.8 depicts a user asking a question
    a chatbot can’t answer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，对话式人工智能无法找到明确的答案。在这种情况下，机器人回答不知道或转接用户到人工代理会更好。图6.8描述了用户向一个无法回答问题的聊天机器人提问的情况。
- en: '![figure](../Images/CH06_F08_Freed2.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F08_Freed2.png)'
- en: Figure 6.8 Supplementing RAG with human agents. If the answer has poor semantic
    overlap with the retrieved documents, send the user to a human agent instead.
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8 补充RAG与人工代理。如果答案与检索到的文档在语义上重叠度较差，则将用户转接到人工代理。
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When a user asks a question, the conversational AI follows a multistep process
    to retrieve information, generate a response, and determine whether the answer
    is sufficiently grounded in retrieved evidence before delivering a final response.
    The following steps illustrate this process, showing how the system retrieves
    relevant passages, generates a candidate answer, evaluates its accuracy, and ultimately
    decides whether to respond or transfer the user to a human agent:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户提出问题时，对话式人工智能会遵循一个多步骤的过程来检索信息、生成响应，并在交付最终响应之前确定答案是否足够基于检索到的证据。以下步骤说明了这一过程，展示了系统如何检索相关段落、生成候选答案、评估其准确性，并最终决定是否响应或将用户转接到人工代理：
- en: '*Passage retrieval*—The system retrieves passages related to achalasia, dysphagia,
    and general information about vaccine side effects:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*段落检索*——系统检索与贲门失弛缓症、吞咽困难以及疫苗副作用的一般信息相关的段落：'
- en: 'Example passage 1: `"Achalasia` `is` `a` `condition` `affecting` `the` `esophagus,`
    `causing` `difficulty` `in` `swallowing."`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '示例段落1: `"贲门失弛缓症` `是` `一种` `影响` `食管` `的` `疾病`，`导致` `吞咽` `困难`。"`'
- en: 'Example passage 2: `"Common` `side` `effects` `of` `vaccines` `include` `soreness,
    fever,` `and` `fatigue."`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 示例段落2：“疫苗的常见副作用包括疼痛、发烧和疲劳。”
- en: 'Example passage 3: `"Dysphagia,` `or` `difficulty` `swallowing,` `can` `be`
    `a symptom` `of` `esophageal` `conditions` `like` `achalasia."`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 示例段落3：“吞咽困难，或吞咽困难，可能是食管疾病如贲门失弛缓症的症状。”
- en: '2\. *Answer generation*—The LLM generates a candidate response based on the
    retrieved passages:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. *答案生成*—LLM基于检索到的段落生成一个候选响应：
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '3\. *Comparison check*—The system evaluates this generated answer against the
    retrieved passages and identifies a potential problem: the generated answer contains
    an element of “no clear evidence linking it to worsening dysphagia” that is not
    directly supported by the retrieved passages.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. *比较检查*—系统将此生成的答案与检索到的段落进行比较，并识别出一个潜在问题：生成的答案包含一个“没有明确证据将其与吞咽困难恶化联系起来”的元素，而这个元素并未直接由检索到的段落支持。
- en: '4\. *Transfer decision*—Given the low match rate between the answer and the
    retrieved passages, the conversational AI determines that the answer may lack
    sufficient grounding and could be misleading. It then offers to transfer the user
    to a human agent for a more reliable answer:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. *转移决策*—鉴于答案与检索到的段落之间的匹配率较低，对话式AI确定该答案可能缺乏足够的依据，可能会误导。然后，它提出将用户转移到人工代理以获得更可靠的答案：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this scenario, the conversational system searched for relevant document passages
    and fed them to an LLM. The LLM generated an answer, and the conversational AI
    then compared the generated answer to the retrieved passages. This comparison
    includes detecting how many words and phrases in the generated answer appear in
    the passages. If the percentage is low, the conversational AI decides that the
    answer is not grounded in the documents. The conversational AI then gracefully
    acknowledges its inability to provide a suitable grounded response and offers
    an alternate resolution path.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，对话式系统搜索相关文档段落，并将它们提供给LLM。LLM生成一个答案，然后对话式AI将生成的答案与检索到的段落进行比较。这种比较包括检测生成的答案中有多少单词和短语出现在段落中。如果百分比低，对话式AI决定该答案没有在文档中得到支持。然后，对话式AI优雅地承认其无法提供合适的基于文档的回答，并提出了另一种解决方案路径。
- en: Alternatively, the search process may not have retrieved any documents. In that
    case, the conversational AI would not have to invoke the LLM to generate an answer,
    and it could directly deflect the question. For both scenarios, the conversational
    AI could instead return an “I don’t know” or other fallback responses. Both options
    reduce the chance of hallucinated and irrelevant answers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，搜索过程可能没有检索到任何文档。在这种情况下，对话式AI就不需要调用LLM来生成答案，它可以直接回避问题。对于这两种情况，对话式AI可以返回“我不知道”或其他后备响应。这两种选择都减少了产生幻觉和不相关答案的可能性。
- en: 6.2.3 Combining RAG with other generative AI use cases
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 将RAG与其他生成式AI用例结合
- en: RAG may also be combined with other generative AI use cases. For instance, RAG
    can handle informational queries, while other generative AI models specialize
    in tasks like sentiment analysis or language translation. By using a combination
    of AI capabilities, conversational AI systems can offer users a comprehensive
    range of services, further enhancing efficiency and satisfaction.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: RAG也可以与其他生成式AI用例结合使用。例如，RAG可以处理信息查询，而其他生成式AI模型则专注于情感分析或语言翻译等任务。通过结合AI能力，对话式AI系统可以为用户提供全面的服务范围，进一步提高效率和满意度。
- en: RAG is only one of several generative AI patterns that enhance conversational
    AI. When users pose common questions or seek detailed information about a product
    or service, RAG draws upon the enterprise knowledge base to provide accurate and
    up-to-date answers. By grounding responses in the organization’s specific domain,
    RAG ensures that users receive relevant information tailored to their needs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: RAG只是几个增强对话式AI的生成式AI模式之一。当用户提出常见问题或寻求有关产品或服务的详细信息时，RAG会利用企业知识库提供准确和最新的答案。通过将回答建立在组织的特定领域，RAG确保用户获得符合他们需求的相关信息。
- en: However, specific user inquiries may require more than informational responses,
    necessitating actionable steps (information-seeking versus transactional questions).
    In such cases, the conversational AI system executes transactions and guides users
    through specific tasks or processes. For instance, users may express an interest
    in purchasing after receiving information about a product or service from RAG.
    In response, the conversational AI can seamlessly transition to a transactional
    action, such as initiating a checkout process or scheduling a vaccination, facilitating
    a smooth and efficient user journey.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，特定的用户查询可能需要比信息性回应更多的内容，需要采取可操作性的步骤（信息寻求与交易性问题）。在这种情况下，对话式人工智能系统执行交易并引导用户完成特定的任务或流程。例如，用户在从RAG获取有关产品或服务的相关信息后可能会表达出购买的兴趣。作为回应，对话式人工智能可以无缝过渡到交易性行动，例如启动结账流程或安排疫苗接种，从而促进用户旅程的顺畅和高效。
- en: While RAG excels at efficiently and accurately responding to user queries, additional
    options, such as handing over to human agents or combining RAG with other generative
    AI use cases, can further optimize the user experience. These options ensure that
    users receive the support and assistance they need in the most efficient manner
    possible.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RAG在高效和准确地响应用户查询方面表现出色，但额外的选项，如转交给人工代理或结合RAG与其他生成式人工智能用例，可以进一步优化用户体验。这些选项确保用户以最有效的方式获得他们需要的支持和帮助。
- en: Note  For scenarios where RAG responses may fall short—such as providing real-time
    data or fulfilling specific customer requests—function calling can be integrated
    to retrieve information from external systems dynamically. This approach allows
    chatbots to identify relevant intents and parameters for real-time responses,
    extending RAG’s utility in complex interactions. While it is not covered in depth
    here, the function call is valuable if you are seeking a more dynamic conversational
    AI system.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：对于RAG回应可能不足的情况——例如提供实时数据或满足特定客户请求——可以集成函数调用以动态地从外部系统检索信息。这种方法允许聊天机器人识别与实时响应相关的意图和参数，扩展RAG在复杂交互中的效用。虽然这里没有深入探讨，但如果您寻求一个更具动态性的对话式人工智能系统，函数调用是有价值的。
- en: 6.2.4 Comparing intents, search, and RAG approaches
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4 比较意图、搜索和RAG方法
- en: 'Table 6.2 summarizes the capabilities and performance of three types of chatbots:
    intent-based chatbots, chatbots integrated with search, and RAG-integrated chatbots.
    Each chatbot type is evaluated based on the requirements and capabilities users
    and chatbot creators expect from conversational AI. You can discern the most suitable
    chatbot solution for your specific needs by comparing these attributes.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2总结了三种类型聊天机器人的功能和性能：基于意图的聊天机器人、与搜索集成的聊天机器人和RAG集成的聊天机器人。每种聊天机器人类型都是根据用户和聊天机器人创建者对对话式人工智能的期望和要求进行评估的。您可以通过比较这些属性来辨别最适合您特定需求的聊天机器人解决方案。
- en: Table 6.2 Comparing the capabilities of intent-based chatbots, chatbots integrated
    with search, and RAG-integrated chatbots
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.2 比较基于意图的聊天机器人、与搜索集成的聊天机器人和RAG集成的聊天机器人的功能
- en: '| Requirements | Intent-based chatbots | Chatbots integrated with search |
    RAG-integrated chatbots |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 要求 | 基于意图的聊天机器人 | 与搜索集成的聊天机器人 | RAG集成的聊天机器人 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Flexibility in handling queries  | Limited to short-head predefined intents.
    May ignore nuance.  | Handle long-tail queries by returning links and snippets  |
    Handle long-tail queries by returning answers  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 处理查询的灵活性 | 限制于短头预定义的意图。可能忽略细微差别。 | 通过返回链接和片段处理长尾查询 | 通过返回答案处理长尾查询 |'
- en: '| Accuracy and relevance of responses  | When an intent is recognized with
    high confidence, the answers are accurate and precrafted.  | Provide contextually
    relevant and accurate documents that help the user find an answer  | Provide contextually
    relevant and accurate answers grounded in your documents  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 回复的准确性和相关性 | 当意图以高置信度被识别时，答案准确且预先制作。 | 提供上下文相关且准确的文档，帮助用户找到答案 | 提供基于您的文档的上下文相关且准确的答案
    |'
- en: '| Adding new know-ledge to the bot  | Add or revise manually curated intent-response
    pairs  | Add or revise documents in your knowledge base  | Add or revise documents
    in your knowledge base  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 向机器人添加新知识 | 手动添加或修改手动精选的意图-响应对 | 添加或修改您的知识库中的文档 | 添加或修改您的知识库中的文档 |'
- en: '| Maintenance and scalability  | Extensive regression testing when intent training
    data is changed  | Document repository needs to be maintained by adding new documents
    and removing stale documents  | Document repository needs to be maintained by
    adding new documents and removing stale documents  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 维护和可扩展性 | 当意图训练数据发生变化时进行广泛的回归测试。 | 文档库需要通过添加新文档和删除过时文档来维护。 | 文档库需要通过添加新文档和删除过时文档来维护。
    |'
- en: '| Response generation quality  | Predefined responses are presented.  | User
    must put together their own answer from retrieved passages and documents  | Answers
    are grounded in source documents but adapted to nuance from the question  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 响应生成质量 | 展示预定义的响应。 | 用户必须从检索到的段落和文档中自行组合答案。 | 答案基于源文档，但根据问题的细微差别进行了调整。 |'
- en: While traditional chatbots help organizations automate simple tasks and provide
    essential customer support, integrating RAG techniques enhances their ability
    to deliver more accurate, context-aware responses, ultimately improving the user
    experience.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然传统聊天机器人帮助组织自动化简单任务并提供基本客户支持，但整合RAG技术增强了它们提供更准确、上下文感知的响应的能力，从而最终改善了用户体验。
- en: Exercises
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习
- en: 'Consider your last chatbot implementation and consider the long-tail concept:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑你上一次的聊天机器人实现，并考虑长尾概念：
- en: List three examples of niche or uncommon user queries that traditional intent-based
    chatbots may not adequately address.
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出三个传统基于意图的聊天机器人可能无法充分解决的细分或罕见用户查询的例子。
- en: Discuss how these queries exemplify the long-tail phenomenon in conversational
    AI.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论这些查询如何体现对话式人工智能中的长尾现象。
- en: For the same chatbot implementation, consider what answers you can provide with
    traditional searches versus RAG.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于相同的聊天机器人实现，考虑使用传统搜索与RAG可以提供哪些答案。
- en: 6.3 How is RAG implemented?
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 RAG是如何实现的？
- en: 'As the “retrieval-augmented generation” name suggests, RAG has two phases:
    retrieval and generation. In the retrieval phase, algorithms search for and retrieve
    snippets of information relevant to the user’s prompt or question. In an open-domain
    consumer setting, those facts can come from indexed documents on the internet;
    in a closed-domain enterprise setting, a narrower set of private sources are typically
    used for added security and reliability.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如“检索增强生成”（RAG）的名称所暗示的，RAG有两个阶段：检索和生成。在检索阶段，算法搜索并检索与用户提示或问题相关的信息片段。在开放域消费者环境中，这些事实可以来自互联网上的索引文档；在封闭域企业环境中，通常使用更窄的私有源以增加安全性和可靠性。
- en: 6.3.1 High-level implementation
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 高级实现
- en: 'With RAG, the system searches a knowledge base for information relevant to
    a question and uses that information to generate a conversational answer. Let’s
    break down the steps:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RAG，系统在知识库中搜索与问题相关的信息，并使用这些信息生成对话式答案。让我们分解步骤：
- en: The user asks the chatbot a question.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户向聊天机器人提问问题。
- en: 'The system uses its NLU capabilities to determine the intent of the user’s
    question:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统使用其NLU能力来确定用户问题的意图：
- en: If it recognizes the question with high confidence—for example, if it is one
    of the intents it was trained on—it will be able to respond, and a search will
    not be needed. This ends the flow.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果它以高置信度识别问题——例如，如果它是它训练的意图之一——它将能够回答，并且不需要搜索。这结束了流程。
- en: If it cannot recognize the query, it will go to search. The system will send
    the user’s query to the search tool to search the document content and produce
    and rank search results.
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果它无法识别查询，它将转到搜索。系统会将用户的查询发送到搜索工具以搜索文档内容，并生成和排名搜索结果。
- en: It passes back the ranked search results to the chatbot for display. (Before
    RAG, the ranked link and snippet list would have been passed back to the chatbot—handling
    the long-tail questions with search results was still more helpful than providing
    a “sorry, I cannot understand” response.)
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将排名搜索结果返回给聊天机器人进行显示。（在RAG之前，排名链接和摘要列表会被返回给聊天机器人——使用搜索结果处理长尾问题仍然比提供“抱歉，我无法理解”的响应更有帮助。）
- en: Instead of simply displaying the results, the original question and the search
    results are sent to an LLM. The LLM may rerank the search results, but most importantly,
    it generates a concise, summarized, linguistically correct answer.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 而不是简单地显示结果，原始问题和搜索结果会被发送到一个LLM。LLM可能会重新排名搜索结果，但最重要的是，它生成一个简洁、总结、语言正确的答案。
- en: The answer is then passed back to the system.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将答案返回给系统。
- en: The answer is presented to the user through the chatbot UI.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案通过聊天机器人用户界面呈现给用户。
- en: Note  Neither the original user question nor the generated answer needs to match
    the documents *exactly*. While verbatim responses can indeed occur and are sometimes
    even preferred for legal reasons, the primary focus is on grounding the content
    in the knowledge base, ensuring that the generated answer is rooted in the curated
    document set.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：既不需要原始用户问题也不需要生成的答案与文档“完全”一致。虽然逐字回复确实可能发生，有时甚至出于法律原因更受欢迎，但主要关注的是将内容建立在知识库的基础上，确保生成的答案根植于精心挑选的文档集中。
- en: By their nature, LLMs do not generate consistent results each time a query is
    processed. These models can produce different responses to the same question depending
    on subtle variations in context or phrasing. This variability is due to the probabilistic
    nature of LLMs, which generate text based on learned patterns rather than retrieving
    fixed responses. While this flexibility allows for more nuanced and contextually
    appropriate answers, it can also lead to expectations of consistent outputs, which
    is not how these models function.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM的本质，每次处理查询时都不会生成一致的结果。这些模型可以根据上下文或措辞的微妙变化对相同的问题产生不同的回答。这种可变性是由于LLM的概率性质，它们基于学习到的模式生成文本，而不是检索固定的回复。虽然这种灵活性允许提供更细腻和上下文相关的答案，但它也可能导致对一致输出的期望，而这并不是这些模型的工作方式。
- en: Emphasizing this point is essential, because team members unfamiliar with how
    LLMs work often expect consistent results. This expectation can hinder projects,
    leading to differing approaches to the problem among team members. Understanding
    that LLMs prioritize relevance and context over the exact replication of document
    content can help align expectations and improve collaboration within the team.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 强调这一点至关重要，因为对LLM（大型语言模型）的工作原理不熟悉的团队成员通常会期望得到一致的结果。这种期望可能会阻碍项目进展，导致团队成员在解决问题的方法上存在差异。理解LLM优先考虑相关性和上下文，而不是精确复制文档内容，有助于调整期望并改善团队内部的协作。
- en: In some cases, the generated response may closely resemble or even match the
    wording in the documents. This can occur when the documents contain relevant and
    informative passages directly addressing the user’s question. In such instances,
    the RAG model may include verbatim excerpts from the documents in the generated
    response to provide the user with the most accurate and relevant information.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，生成的回复可能与文档中的措辞非常相似，甚至完全一致。这通常发生在文档中包含直接针对用户问题的相关和有信息量的段落时。在这种情况下，RAG模型可能会在生成的回复中包含文档的逐字摘录，以向用户提供最准确和相关的信息。
- en: In other cases, the RAG model uses the information within the knowledge base
    to understand the context and relevant concepts related to the user’s question.
    It then uses this understanding to generate a response that aligns with the content
    found in the documents, even if the specific wording of the user’s question or
    the generated answer does not exist verbatim within the documents. This approach
    allows for greater flexibility and adaptability in developing responses that effectively
    address user queries while drawing upon the information available in the curated
    document set.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，RAG模型利用知识库中的信息来理解与用户问题相关的上下文和概念。然后，它利用这种理解来生成与文档中找到的内容一致的回复，即使用户的提问或生成的答案的具体措辞在文档中并未逐字出现。这种方法允许在开发有效解决用户查询的回复时具有更大的灵活性和适应性，同时利用精心挑选的文档集中的信息。
- en: 6.3.2 Preparing your document repository for RAG
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 为RAG准备您的文档存储库
- en: Let’s also consider how the document content is retrieved during RAG searches.
    Figure 6.9 provides more detail about creating appropriate data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再考虑一下在RAG搜索过程中如何检索文档内容。图6.9提供了更多关于创建适当数据的信息。
- en: '![figure](../Images/CH06_F09_Freed2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F09_Freed2.png)'
- en: Figure 6.9 RAG uses a vector database during build time and run time.
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9 RAG在构建时间和运行时间都使用向量数据库。
- en: 'A systematic preprocessing pipeline ensures that both the raw data and the
    user’s question (or the LLM’s rephrasing of it) are optimized for use in RAG-based
    searches. This pipeline is crucial for transforming data into embeddings, enabling
    the model to match the user’s query with relevant information efficiently. Techniques
    such as cosine similarity or other methods are then applied to identify the best
    matches, ensuring accurate and contextually appropriate results. The following
    list outlines the key steps involved in this pipeline, detailing how data is processed
    before being used in retrieval:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一个系统的预处理管道确保了原始数据和用户的问题（或LLM对其的重述）都得到了优化，以便在基于RAG的搜索中使用。这个管道对于将数据转换为嵌入至关重要，使得模型能够高效地将用户的查询与相关信息匹配。然后应用如余弦相似度或其他方法来识别最佳匹配，确保准确和上下文适当的搜索结果。以下列表概述了该管道中涉及的关键步骤，详细说明了数据在用于检索之前是如何被处理的：
- en: '*Preprocessing data*—The system (typically a data pipeline, not the LLM itself)
    processes raw documents to make them searchable. For example, PDF documents are
    converted to text, or table structures are converted to processable statements.
    Metadata may be added to enhance the original content. The text is then divided
    into coherent semantic units, called *chunks*. For instance, a document may be
    chunked at paragraph boundaries. Chunking is a common process for identifying
    and extracting meaningful groups of words (“chunks”) from sentences for further
    analysis or processing. The chunking strategy impacts the overall results. There
    are open source tools that can help with visualizing and understanding different
    chunking or splitting strategies.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*预处理数据*—系统（通常是一个数据管道，而不是LLM本身）处理原始文档，使其可搜索。例如，PDF文档被转换为文本，或表格结构被转换为可处理语句。可能添加元数据以增强原始内容。然后文本被划分为连贯的语义单元，称为*块*。例如，文档可能在段落边界处被分割。分割是一种常见的识别和提取句子中有意义的单词组（“块”）的过程，用于进一步分析或处理。分割策略影响整体结果。有一些开源工具可以帮助可视化和理解不同的分割或拆分策略。'
- en: '*Embedding generation*—An embedding model converts these chunks into embeddings
    or numerical representations of words or phrases in a high-dimensional vector
    space. Embeddings capture semantic relationships between words and documents,
    enabling a more efficient understanding of the connections. Similar meanings or
    contexts are mapped nearby in the vector space, and dissimilar meanings are mapped
    to more distant points. This provides more relevant search results than keyword
    matches.'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*嵌入生成*—嵌入模型将这些块转换为嵌入或高维向量空间中单词或短语的数值表示。嵌入捕捉了单词和文档之间的语义关系，使得对连接的理解更加高效。相似的意义或上下文在向量空间中映射到附近，而不同的意义则映射到更远的点。这比关键字匹配提供了更相关的搜索结果。'
- en: '*Storage in a vector database*—The generated embeddings are stored in a vector
    database, which enables efficient similarity searches. Each document chunk is
    indexed using its vector representation, allowing fast retrieval based on meaning
    rather than exact word matches.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*向量数据库中的存储*—生成的嵌入向量存储在向量数据库中，这使得高效的相似性搜索成为可能。每个文档块都使用其向量表示进行索引，允许基于意义而非精确的单词匹配进行快速检索。'
- en: '*Retrieval and matching at runtime*—At run time, the end user interacts with
    the chatbot. Their question will be converted to a vector using the same embedding
    model, and that vector will be searched in the vector database to find the most
    relevant passages (chunks) based on semantic similarity. These retrieved passages
    are then passed to the LLM, which synthesizes them into a response presented to
    the user.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*运行时的检索和匹配*—在运行时，最终用户与聊天机器人交互。他们的问题将使用相同的嵌入模型转换为向量，并在向量数据库中搜索，根据语义相似性找到最相关的段落（块）。然后检索到的段落被传递给LLM，它将它们综合成对用户展示的响应。'
- en: Each of these steps ensures that the retrieval process is optimized, making
    it possible for the LLM to generate accurate, context-aware responses based on
    the most relevant retrieved data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤中的每一个都确保检索过程得到优化，使得LLM能够基于检索到的最相关数据生成准确、上下文感知的响应。
- en: 'Listing 6.1 shows sample code for an embedding function. You can use any custom
    embedding function or other vector databases, and the performance may differ depending
    on the embedding model used. This is the most common approach to RAG: you create
    a dense vector representation of the knowledge base to calculate the semantic
    similarity to the user queries. For this sample, we used Chroma as the vector
    database.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.1显示了嵌入函数的示例代码。您可以使用任何自定义嵌入函数或其他向量数据库，性能可能取决于所使用的嵌入模型。这是RAG最常见的方法：您创建知识库的密集向量表示，以计算与用户查询的语义相似度。对于这个样本，我们使用了Chroma作为向量数据库。
- en: Listing 6.1 Splitting a file into chunks, embedding it, and storing it in a
    vector database
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.1 将文件分割成块，嵌入并存储在向量数据库中
- en: '[PRE4]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At its core, RAG operates by retrieving relevant documents or passages based
    on a user’s query and then generating a response using natural language generation
    techniques. This process can be achieved without explicit chunking or embedding
    by using other methods for document retrieval and language generation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，RAG通过根据用户的查询检索相关文档或段落，然后使用自然语言生成技术生成响应来运行。这个过程可以通过使用其他文档检索和语言生成方法，而不进行显式的分块或嵌入来实现。
- en: For example, using Lucene as an alternative to chunking and embeddings involves
    using its document indexing and retrieval capabilities. Lucene can handle the
    retrieval part, fetching the most relevant documents based on the query. After
    retrieval, the generator part of the RAG can take over to produce coherent responses
    based on the content of the retrieved documents. Lucene is very efficient at text
    retrieval, which leaves the complex task of generating human-like responses to
    the more specialized generative components of the RAG model. This approach can
    be particularly advantageous in systems emphasizing retrieval accuracy and speed
    over nuanced understanding.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用Lucene作为分块和嵌入的替代方案，涉及使用其文档索引和检索功能。Lucene可以处理检索部分，根据查询检索最相关的文档。检索后，RAG的生成部分可以接管，根据检索文档的内容生成连贯的响应。Lucene在文本检索方面非常高效，这把生成类似人类响应的复杂任务留给了RAG模型中更专业的生成组件。这种方法在强调检索准确性和速度而不仅仅是细微理解的系统中特别有利。
- en: Exercises
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习
- en: 'Text chunking—In this exercise, you will experiment with different chunking
    strategies and embeddings:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本分块—在这个练习中，您将尝试不同的分块策略和嵌入：
- en: Choose your sample text data (small text files of your choice).
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择您的样本文本数据（您选择的任意小文本文件）。
- en: Decide on a chunking strategy (splitting by sentences or words). For your experiments,
    try a chunking tool.
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定分块策略（按句子或单词分割）。对于您的实验，尝试使用分块工具。
- en: Embed the chunks using an open source embedding model, and then load the chunks
    into a vector database (Chroma).
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用开源嵌入模型嵌入块，然后将块加载到向量数据库（Chroma）中。
- en: 'Setting up an ingestion pipeline—This exercise guides you through building
    a simple ingestion pipeline for processing documents in a RAG system:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置摄取管道—本练习将指导您构建一个简单的摄取管道，用于处理RAG系统中的文档：
- en: Choose a document set relevant to your organization’s domain or a specific use
    case for the chatbot. Start with simple, text-only documents, i.e., no tables,
    etc.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择与您组织领域或聊天机器人特定用例相关的文档集。从简单的纯文本文档开始，即没有表格等。
- en: Build the ingestion pipeline, considering factors such as ease of use and compatibility
    with RAG. For querying, use open source models, Hugging Face embedding models,
    and a llama index.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑易用性和与RAG兼容性等因素构建摄取管道。对于查询，使用开源模型、Hugging Face嵌入模型和llama索引。
- en: Implement the ingestion pipeline to preprocess and structure the dataset for
    use with RAG.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现数据集的摄取管道，以便预处理和结构化数据集以用于与RAG一起使用。
- en: Test the ingestion pipeline with sample data to ensure proper functionality
    and data integrity.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用样本数据测试摄取管道，以确保其正常功能和数据完整性。
- en: 6.4 Additional considerations of RAG implementations
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 RAG实现的附加考虑
- en: Traditional search returns links, passages, or the full text of relevant documents,
    and the user needs to sift through this information to find their answer. RAG
    conversely returns the answer directly, and the user can optionally see the documents
    used.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 传统搜索返回相关文档的链接、段落或全文，用户需要筛选这些信息以找到他们的答案。RAG相反，直接返回答案，用户可以选择查看所用的文档。
- en: 6.4.1 Can’t we just use an LLM directly?
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 我们就不能直接使用一个LLM吗？
- en: What if the conversational AI passed the user’s query to an LLM and got the
    answer? After all, LLMs are trained on vast amounts of data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对话式AI将用户的查询传递给LLM并得到答案呢？毕竟，LLM是在大量数据上训练的。
- en: First, LLMs trained on internet-scale data have limitations due to the nature
    of their training data. This data represents a snapshot of the training time from
    publicly available sources—it does not contain business-specific, personal, or
    classified information, and it doesn’t contain public data created after the cutoff
    date. Thus, even the newest LLM’s knowledge can become outdated, leading to inaccurate
    responses over time. RAG addresses this by offering data to LLMs after they are
    trained.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，由于LLM是在互联网规模的数据上训练的，其局限性是由于其训练数据的性质。这些数据代表了从公开来源在训练时间点的快照——它不包含特定业务、个人或机密信息，也不包含截止日期之后创建的公开数据。因此，即使是最新LLM的知识也可能过时，随着时间的推移导致不准确的结果。RAG通过在LLM训练后提供数据来解决这一问题。
- en: Second, as LLMs are trained from extensive datasets, it is challenging to trace
    their responses to sources, undermining the reliability and trustworthiness of
    the model’s output. RAG is inherently grounded because you know exactly what data
    was provided to the LLM for a given question.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，由于LLM是从大量数据集中训练的，很难追踪其响应到来源，这损害了模型输出的可靠性和可信度。RAG本质上是基于事实的，因为您确切知道为给定问题提供给LLM的数据是什么。
- en: The broad domain LLMs cover poses another significant challenge. With access
    to vast information, they may generate responses with high confidence, even when
    lacking concrete evidence or context. This tendency to produce plausible but incorrect
    or unverified information is known as *hallucination*. In contrast, you want your
    conversational AI to provide correct and grounded answers. Advanced prompting
    techniques can help mitigate hallucinations, but providing source data through
    RAG is more reliable.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛领域的LLM带来了另一个重大挑战。由于可以访问大量信息，它们可能会在没有具体证据或上下文的情况下，自信地生成响应。这种产生看似合理但实际上错误或未经证实的信息的倾向被称为*幻觉*。相比之下，您希望您的对话式AI提供正确且基于事实的答案。高级提示技术可以帮助减轻幻觉，但通过RAG提供源数据更为可靠。
- en: You also want to prioritize answers grounded in the specific documents or corpus
    being indexed, not those on which LLM was trained. A RAG system’s primary focus
    is to provide responses based on the content and context of your documents, so
    answers are directly generated from the information within the corpus, promoting
    accuracy, relevance, and trustworthiness in the responses provided to users.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您还希望优先考虑基于特定文档或语料库的答案，而不是LLM所训练的文档。RAG系统的主要重点是提供基于您文档的内容和上下文的响应，因此答案直接从语料库中的信息生成，从而提高了提供给用户的响应的准确性、相关性和可信度。
- en: It is important to consider the training data and domain of a specific LLM before
    selecting it for your use. If the LLM was trained on generic data and you need
    domain-specific results, it may not produce the desired outcomes. In such cases,
    you could explore techniques like model blending, where you combine multiple models
    to use the strengths of each, enhancing performance in specific domains. If you
    have the resources and data available, you may also consider fine-tuning the selected
    model to better suit your needs. However, this can require a significant budget
    for computational resources and data, so consider prompt-tuning first. While fine-tuning
    costs are decreasing and will continue to do so, they still need to be carefully
    considered. Other methods are also emerging for domain-specific training, offering
    further flexibility.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择特定的大型语言模型（LLM）用于您的用途之前，考虑其训练数据和领域是很重要的。如果LLM是在通用数据上训练的，而您需要特定领域的成果，它可能无法产生期望的结果。在这种情况下，您可以探索诸如模型混合等技术，通过结合多个模型来利用每个模型的优点，从而在特定领域提高性能。如果您有资源和可用数据，您还可以考虑微调所选模型以更好地满足您的需求。然而，这可能需要大量的计算资源和数据预算，因此首先考虑提示微调。尽管微调成本正在下降，并将继续下降，但它们仍然需要仔细考虑。其他针对特定领域训练的方法也正在出现，提供了更多的灵活性。
- en: 6.4.2 Keeping answers current and relevant with RAG
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 使用RAG保持答案的时效性和相关性
- en: RAG represents a significant advantage over directly using LLMs for question
    answering. While RAG still uses LLM for natural language generation, LLM is crafting
    accurate responses from the searched documents. Real-time retrieval will find
    up-to-date information when new or updated documents are added during the build
    phase. RAG ensures the answer reflects the latest documents from the searched
    knowledge sources. This real-time integration of enterprise content enhances the
    relevance and accuracy of responses and instills confidence in users, who know
    that they are receiving current and reliable information.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: RAG在问答方面直接使用LLM具有显著优势。虽然RAG仍然使用LLM进行自然语言生成，但LLM是从搜索到的文档中构建准确响应的。实时检索将在构建阶段添加新或更新文档时找到最新信息。RAG确保答案反映了搜索知识源的最新文档。这种企业内容的实时集成增强了响应的相关性和准确性，并增强了用户的信心，因为他们知道他们正在接收最新和可靠的信息。
- en: Furthermore, RAG goes beyond merely accessing enterprise content. It accesses
    specific passages and retrieves information from multiple documents. This granularity
    allows RAG to trace and verify answers to their exact sources, providing users
    with full transparency and trustworthiness. This facilitates accountability and
    the verification process for your development team too, ensuring you know what
    your bot is doing and why.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RAG不仅限于访问企业内容。它能够访问特定的段落，并从多份文档中检索信息。这种粒度允许RAG追踪和验证答案的确切来源，为用户提供全面透明度和可信度。这也为您的开发团队提供了责任和验证过程的便利，确保您知道您的机器人正在做什么以及为什么这么做。
- en: Moreover, RAG defines the domain of the LLM’s understanding, enabling it to
    recognize the limits of its knowledge and expertise. Unlike LLMs that are used
    directly and that may attempt to provide answers outside of their domain, RAG
    can acknowledge when it encounters queries beyond its scope. This ability to say
    “I don’t know” prevents it from giving inaccurate answers and fosters transparency
    in conversational interactions. By establishing clear boundaries for its understanding,
    RAG empowers developers to build AI systems prioritizing accuracy, reliability,
    and integrity, ultimately enhancing the overall user experience.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RAG定义了LLM理解的范围，使其能够识别其知识和专长的限制。与直接使用且可能尝试在其领域之外提供答案的LLM不同，RAG在遇到超出其范围的问题时能够承认。这种说“我不知道”的能力防止了它给出不准确答案，并在对话交互中促进了透明度。通过为其理解设定清晰的边界，RAG使开发者能够构建优先考虑准确性、可靠性和完整性的AI系统，从而最终提升整体用户体验。
- en: There is a difference between “I don’t understand” and “I cannot find an answer
    to your question.” While the primary goal of RAG is to generate informative and
    relevant responses based on the content of the retrieved documents, there are
    scenarios where the system may not find sufficient or appropriate information
    to generate a meaningful response. In such cases, it is common for the RAG model
    to acknowledge its inability to provide a satisfactory answer and communicate
    this to the user.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: “我不懂”和“我找不到你问题的答案”之间有区别。虽然RAG的主要目标是根据检索到的文档内容生成有信息和相关的响应，但在某些情况下，系统可能找不到足够或适当的信息来生成有意义的响应。在这种情况下，RAG模型通常会承认其无法提供令人满意的答案，并将此信息传达给用户。
- en: However, it’s important to note that a RAG system’s specific behavior, such
    as returning an “I don’t know” response, can be influenced by the retrieval component’s
    design, the knowledge base’s quality, and the generation model’s settings or parameters.
    Additionally, developers may choose to implement specific strategies or fallback
    mechanisms to handle cases where the system cannot generate a response, such as
    providing alternative suggestions or prompting the user for more information.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，RAG系统的特定行为，例如返回“我不知道”的响应，可能受到检索组件的设计、知识库的质量、生成模型的设置或参数的影响。此外，开发者可以选择实施特定的策略或回退机制来处理系统无法生成响应的情况，例如提供替代建议或提示用户提供更多信息。
- en: 6.4.3 How easy is it to set up the ingestion pipeline?
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 设置摄取管道有多容易？
- en: Setting up an ingestion pipeline that effectively preserves document structure
    is critical for ensuring accurate search results within a retrieval system for
    RAG. Several key areas must be considered. Essentially, every architectural decision
    you make about the components will have an influence on the overall accuracy of
    the results.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个能够有效保留文档结构的摄取管道对于确保RAG检索系统中的搜索结果准确性至关重要。必须考虑几个关键领域。本质上，您在组件上做出的每一个架构决策都将影响结果的总体准确性。
- en: First, you must establish mechanisms to connect existing content stores to the
    retrieval system or migrate content into a new repository. This will allow the
    retrieval system to access the necessary data and maintain data integrity.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您必须建立机制将现有内容存储连接到检索系统或迁移内容到新的存储库。这将允许检索系统访问必要的数据并保持数据完整性。
- en: The next challenge is correctly extracting structures (such as headings, tables,
    and lists) during ingestion. These formatting elements contribute to the document’s
    organization and clarity. By retaining this structural information during ingestion,
    the retrieval system can use it to enhance search accuracy and relevance.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个挑战是在摄取过程中正确提取结构（如标题、表格和列表）。这些格式元素有助于文档的组织和清晰度。通过在摄取过程中保留这些结构信息，检索系统可以使用它来提高搜索准确性和相关性。
- en: There are also challenges related to chunking. The ability to chunk, split,
    or partition large documents into representative subdocuments for indexing enhances
    the retrieval process’s efficiency. This allows for more granular indexing and
    retrieval of information, facilitating quicker access to specific content within
    lengthy documents. Additionally, selecting appropriate search methodologies, such
    as vector, semantic, federated, keyword, or hybrid, further augments the retrieval
    system’s capabilities.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些与分块相关的问题。将大型文档分割成代表性的子文档以进行索引的能力提高了检索过程的效率。这允许进行更细粒度的索引和检索信息，从而加快对长文档中特定内容的访问。此外，选择适当的搜索方法，如向量、语义、联邦、关键词或混合，进一步增强了检索系统的功能。
- en: Using LangChain simplifies setting up the ingestion pipeline. Recall that you
    will need
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LangChain简化了设置摄取管道的过程。回想一下，您将需要
- en: '*Document loaders*—Load data from various formats.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文档加载器*——从各种格式加载数据。'
- en: '*Document transformers*—Process and structure the data for efficient retrieval.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文档转换器*——处理和结构化数据以实现高效检索。'
- en: '*Retrievers*—Fetch the most relevant document chunks during query time.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索器*——在查询时获取最相关的文档片段。'
- en: '*Document loaders* facilitate the ingestion of diverse document formats. These
    loaders streamline the workflow, ensuring efficient processing and retrieval of
    pertinent context for LLMs to deliver precise responses. They load data from the
    source documents, treating each extracted piece as a document comprising textual
    content and associated metadata. LangChain provides built-in capabilities for
    handling various files: all files in a directory, PDF, CSV, JSON, HTML, markdown,
    txt, and more.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*文档加载器*简化了不同文档格式的摄取。这些加载器简化了工作流程，确保了高效处理和检索与LLM相关的上下文，以提供精确的响应。它们从源文档中加载数据，将每个提取的部分视为包含文本内容和相关元数据的文档。LangChain提供了处理各种文件的内建功能：目录中的所有文件、PDF、CSV、JSON、HTML、markdown、txt等。'
- en: 'For example, you can load text from a web page, transcripts, or corporate documents.
    Document loaders provide a `load` method for loading data as documents from a
    configured source:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以从网页、字幕或企业文档中加载文本。文档加载器提供了一种`load`方法，用于从配置的源加载数据作为文档：
- en: 'Text loader:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本加载器：
- en: '[PRE5]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'CSV loader:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV加载器：
- en: '[PRE6]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Look at LangChain’s documentation on customizing the CSV parsing and loading.
    For example, you may want to specify your delimiters, field names, etc. Similarly,
    LangChain provides a `DirectoryLoader` for all documents in a directory, an `UnstructuredHTMLLoader`
    to load HTML docs, and so on for the common types. It is essential to know the
    `AzureAIDocumentIntelligenceLoader`, which is useful for Microsoft Office-type
    documents.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 查看LangChain关于自定义CSV解析和加载的文档。例如，您可能希望指定您的分隔符、字段名等。同样，LangChain提供了一个`DirectoryLoader`用于目录中的所有文档，一个`UnstructuredHTMLLoader`用于加载HTML文档，等等，用于常见类型。了解`AzureAIDocumentIntelligenceLoader`非常重要，这对于Microsoft
    Office类型的文档非常有用。
- en: 'Microsoft Document Loader:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软文档加载器：
- en: '[PRE7]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once you have loaded the documents, you need to look at the *document transformers*,
    which can split a long document into smaller chunks that the selected LLM can
    process. LLMs have a “context window” property, determining the text length they
    can effectively process in a single pass, so the chunks must fit into the LLM’s
    context window. It is easy to assume that setting a more extensive context (i.e.,
    longer text passages) would inherently lead to better performance across various
    language understanding tasks. However, recent studies have revealed that this
    isn’t always the case. Evidence suggests that language models can achieve improved
    performance when presented with less text overall, but text that is highly relevant
    to the task at hand.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你加载了文档，你需要查看*文档转换器*，它可以将长文档分割成LLM可以处理的更小的块。LLMs有一个“上下文窗口”属性，决定了它们在一次遍历中可以有效地处理的文本长度，因此块必须适合LLM的上下文窗口。很容易假设设置更广泛的上文（即更长的文本段落）会自然而然地导致在各种语言理解任务上的性能提升。然而，最近的研究表明，这并不总是如此。证据表明，当呈现的文本总体较少时，语言模型可以达到更好的性能，但文本必须与当前任务高度相关。
- en: A larger context window allows for including more information in the prompt
    during inference, but this technique, often called *prompt stuffing*, comes with
    trade-offs. Processing more text demands greater computational resources, which
    slows inference and increases costs—particularly for companies paying by the token,
    where summarizing lengthy documents, like annual reports or meeting transcripts,
    can become costly. While larger context windows can improve results to some extent,
    there are diminishing returns. Like humans, LLMs can experience information overload;
    when presented with excessive detail, they may overlook critical points. Studies
    have shown that LLMs are more likely to focus on essential information at the
    beginning or end of a prompt, potentially missing key insights buried in the middle.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的上下文窗口允许在推理过程中包含更多信息，但这种通常称为*提示填充*的技术有其权衡之处。处理更多文本需要更多的计算资源，这会减慢推理并增加成本——尤其是对于按字符付费的公司，总结长文档（如年度报告或会议记录）可能会变得昂贵。虽然较大的上下文窗口可以在一定程度上提高结果，但回报是递减的。像人类一样，LLMs也可能经历信息过载；当面对过多的细节时，它们可能会忽略关键点。研究表明，LLMs更有可能在提示的开始或结束时关注关键信息，可能会错过隐藏在中间的关键见解。
- en: We need document transformers to preprocess the documents, extract relevant
    information, and transform it into a structured representation that the language
    model can efficiently use during generation. LangChain has several built-in transformers
    that make document manipulation easy.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要文档转换器来预处理文档，提取相关信息，并将其转换成语言模型在生成过程中可以高效使用的结构化表示。LangChain有几个内置的转换器，使得文档操作变得简单。
- en: 'The splitting process is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 分割过程如下：
- en: Divide the text into smaller chunks.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本分成更小的块。
- en: Combine these smaller chunks into larger chunks of a certain size, usually measurable
    by some function.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些较小的块组合成一定大小的较大块，通常可以通过某些函数来衡量。
- en: Once it reaches that size, it becomes the new unit of the text. Then, you create
    a new text segment with some overlap to maintain context between the fragments.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦达到那个大小，它就变成了文本的新单元。然后，你创建一个新的文本段，并保留一些重叠以保持片段之间的上下文。
- en: 'You can choose your division rules (characters, words, tokens) and how to measure
    the chunk size. Again, LangChain offers many different types of splitters in the
    langchain-text-splitters package. These are some examples of text splitters:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择你的分割规则（字符、单词、标记）以及如何衡量块的大小。再次强调，LangChain在langchain-text-splitters包中提供了许多不同类型的分割器。以下是一些文本分割器的示例：
- en: '*Recursive*—Splitting text recursively is the recommended way to start. It
    aims to keep related pieces of text next to each other.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*递归*—递归分割文本是推荐的开始方式。它的目的是将相关的文本片段保持在一起。'
- en: '*HTML*—A “structure-aware” chunker splits text based on HTML-specific characters;
    an example is shown in listing 6.2\. It splits at the element level, adding metadata
    to headers for chunk relevance. It preserves semantic grouping and context-rich
    information in document structures:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*HTML*—一个“结构感知”的块分割器基于HTML特定的字符分割文本；示例见列表6.2。它在元素级别进行分割，为块的相关性添加元数据。它保留文档结构中的语义分组和丰富的上下文信息：'
- en: '*Character*—It breaks the document at user-defined characters (e.g., "\n\n").'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*字符*—它在用户定义的字符处（例如，"\n\n"）打断文档。'
- en: '*Code*—It employs code syntax and grammar identifiers for languages like Python
    and JavaScript (and 13 others), organizing code into logical groups.'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Code*——它使用 Python 和 JavaScript（以及其他 13 种语言）的代码语法和语法标识符，将代码组织成逻辑组。'
- en: '*Markdown*—It identifies markdown language and organizes the document into
    a structured format (similar to HTML).'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Markdown*——它识别 markdown 语言并将文档组织成结构化格式（类似于 HTML）。'
- en: '*Tokens*—It uses a tokenizer, like tiktoken, to split text based on model-defined
    token limits in the code.'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Tokens*——它使用分词器，如 tiktoken，根据代码中定义的模型令牌限制来分割文本。'
- en: Listing 6.2 HTML splitter
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.2 HTML 分割器
- en: '[PRE8]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we need to deal with the embeddings. The type of data and the language
    support requirements govern the selection of embedding models. Furthermore, when
    you are dealing with specific domain or industry terms, these models may have
    to be extended.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要处理嵌入。数据类型和语言支持要求决定了嵌入模型的选择。此外，当处理特定领域或行业术语时，这些模型可能需要扩展。
- en: Embedding models in LangChain transform the text into numerical representations,
    or embeddings, that can be processed. LangChain integrates with different model
    providers (OpenAI, Cohere, Hugging Face, and more) to generate embeddings. The
    `OpenAIEmbeddings` class, for instance, uses the OpenAI API to create embeddings,
    and this can be done using either OpenAI’s API key or Azure’s OpenAI API key.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 中的嵌入模型将文本转换为数值表示，或嵌入，以便进行处理。LangChain 与不同的模型提供商（OpenAI、Cohere、Hugging
    Face 等）集成以生成嵌入。例如，`OpenAIEmbeddings` 类使用 OpenAI API 创建嵌入，这可以通过 OpenAI 的 API 密钥或
    Azure 的 OpenAI API 密钥来完成。
- en: '[PRE9]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Other integrations include `CohereEmbeddings`, `TensorFlowEmbeddings`, and `HuggingFaceInferenceEmbeddings`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 其他集成包括 `CohereEmbeddings`、`TensorFlowEmbeddings` 和 `HuggingFaceInferenceEmbeddings`。
- en: After you have the embeddings, you must store them in a vector database, such
    as Chroma, which we used earlier. When selecting the vector database, you’ll want
    to consider run-time performance, how it scales for the size of your data set,
    and overall performance. Another important consideration is integrating tools
    like LangChain, which is continually improving. LangChain enhances the capabilities
    of vector databases by providing streamlined processes for handling embeddings
    and integrating with various machine learning and AI workflows. This combination
    ensures efficient data management and retrieval, making it a robust choice for
    scalable and high-performance applications.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得嵌入后，您必须将它们存储在向量数据库中，例如我们之前使用的 Chroma。在选择向量数据库时，您需要考虑运行时性能、它如何根据您的数据集大小进行扩展以及整体性能。另一个重要考虑因素是集成
    LangChain 等工具，它正在不断改进。LangChain 通过提供处理嵌入和集成各种机器学习和 AI 工作流的简化流程来增强向量数据库的功能。这种组合确保了高效的数据管理和检索，使其成为可扩展和高性能应用的稳健选择。
- en: '*Retrievers* bridge the gap between embeddings and user queries. While embeddings
    store numerical representations of documents in a vector database, retrievers
    identify and fetch the most relevant chunks based on similarity scoring.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*Retrievers* 在嵌入和用户查询之间架起桥梁。虽然嵌入在向量数据库中存储了文档的数值表示，但检索器根据相似度评分识别和检索最相关的片段。'
- en: 'The retriever works as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器的工作方式如下：
- en: The user’s query is embedded using the same embedding model used during ingestion.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在摄取期间使用的相同嵌入模型将用户的查询嵌入。
- en: The vector database searches for the most semantically similar embeddings.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量数据库搜索最语义相似的嵌入。
- en: The retriever fetches the top matches and passes them to the LLM for response
    generation.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索器检索最相关的匹配项并将它们传递给 LLM 以生成响应。
- en: 'LangChain includes multiple retrieval methods. For example, there is a similarity-based
    retriever in LangChain:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 包含多种检索方法。例如，LangChain 中有一个基于相似度的检索器：
- en: '[PRE10]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Retrievers play a crucial role in returning only the most relevant document
    chunks, ensuring the LLM works with focused, high-quality context rather than
    raw, unprocessed data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器在仅返回最相关的文档片段中扮演着关键角色，确保 LLM 在专注、高质量上下文中工作，而不是原始、未处理的数据。
- en: 6.4.4 Handling latency
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 处理延迟
- en: 'A universal best practice for handling latency has yet to be developed. Long
    response times are frustrating for users, but these techniques can enhance their
    experience:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 处理延迟的通用最佳实践尚未开发。对于用户来说，长响应时间令人沮丧，但这些技术可以增强他们的体验：
- en: '*Use a quality vector store with efficient search.* The Facebook AI Similarity
    Search (FAISS) library allows you to search for similar embeddings quickly. There
    are many purpose-built vector databases, like Chroma, Milvus, Pinecone, and Weaviate,
    with many more emerging. Traditional databases and search systems like Elasticsearch
    provide vector search plugins. Each has unique strengths and can be selected based
    on your needs, including scalability, functionality, performance, and cost.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用高效的搜索质量向量存储。* Facebook AI相似性搜索（FAISS）库允许您快速搜索相似嵌入。有许多专门构建的向量数据库，如Chroma、Milvus、Pinecone和Weaviate，还有更多正在出现。传统的数据库和搜索系统，如Elasticsearch，提供向量搜索插件。每个都有其独特的优势，可以根据您的需求进行选择，包括可扩展性、功能、性能和成本。'
- en: '*Preprocess and curate your dataset.* Having multiple similar versions of the
    same document increases search time and lowers search result quality.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预处理和整理您的数据集。* 相同文档的多个相似版本会增加搜索时间并降低搜索结果质量。'
- en: '*Inform the user before executing a slow action.* An appropriate-toned message,
    such as “Just need a moment,” may placate the user and bridge the delay.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在执行慢动作之前通知用户。* 一个适当的语气信息，例如“只需稍等片刻”，可以安抚用户并填补延迟。'
- en: '*Stream responses to show the user the answer as each token is generated.*
    LLMs may take 1.5 to 5 seconds or more to generate an answer, and searches may
    take 5 to 10 seconds. The user may think the chatbot is broken if the conversational
    AI waits for the LLM to finish.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*流式响应以在生成每个标记时向用户展示答案。* LLM可能需要1.5到5秒或更长时间来生成答案，搜索可能需要5到10秒。如果对话式AI等待LLM完成，用户可能会认为聊天机器人已损坏。'
- en: '*Consider caching.* By caching each user’s vector database and chat history,
    commonly accessed information relevant to that user’s interactions can be stored
    locally. This reduces the need to generate responses from scratch every time,
    saving the computational resources required. While caching may consume additional
    tokens, the trade-off is improved efficiency.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*考虑缓存。* 通过缓存每个用户的向量数据库和聊天历史，可以存储与用户交互相关的常用信息。这减少了每次都需要从头生成响应的需求，从而节省了所需的计算资源。虽然缓存可能会消耗额外的标记，但权衡的结果是提高了效率。'
- en: 6.4.5 When to use a fallback mechanism and when to search
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.5 何时使用回退机制和何时搜索
- en: Determining whether to use RAG’s response or to deflect to a human agent involves
    several key considerations. For instance, the generation part of RAG should not
    be invoked if the retrieval does not yield appropriate results. In this case,
    the conversational AI can gracefully exit the query and respond with the offer
    to pass the user to a human agent. By bypassing the generation of an answer based
    on potentially subpar search results, you effectively reduce latency for end users,
    ensuring they receive prompt responses while saving computational resources.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 确定是否使用RAG的响应或转向人工代理涉及几个关键考虑因素。例如，如果检索没有产生适当的结果，则不应调用RAG的生成部分。在这种情况下，对话式AI可以优雅地退出查询，并响应提出将用户转交给人工代理的提议。通过绕过基于可能较差的搜索结果生成答案，您有效地减少了最终用户的延迟，确保他们能够收到及时的响应，同时节省了计算资源。
- en: 'Figure 6.10 illustrates the decision-making process for determining whether
    to use the RAG response or to hand over the query to a human agent:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10展示了确定是否使用RAG响应或手动将查询转交给人工代理的决策过程：
- en: The conversational AI processes the user query and passes it to the retrieval
    system, which searches the knowledge base to find relevant information.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对话式AI处理用户查询并将其传递给检索系统，检索系统搜索知识库以找到相关信息。
- en: The retrieval results are evaluated to determine if they are appropriate for
    generating a response.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索结果将被评估以确定它们是否适合生成响应。
- en: If the retrieval results are deemed appropriate, the system generates a response
    using the full-on RAG approach.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果检索结果被认为合适，系统将使用全功能的RAG方法生成响应。
- en: '![figure](../Images/CH06_F10_Freed2.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F10_Freed2.png)'
- en: Figure 6.10 When the answer from RAG does not match the retrieved passages,
    it can be better to offer a human agent instead.
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.10 当RAG的答案与检索到的段落不匹配时，提供人工代理可能更好。
- en: 4\. If the retrieval results are inappropriate (e.g., insufficient or no results),
    the system gracefully offers to pass the query to a human agent.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 如果检索结果不合适（例如，结果不足或无结果），系统将优雅地提出将查询转交给人工代理。
- en: 5\. The response (either generated by the system or passed to a human agent)
    is returned to the user.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 响应（无论是系统生成还是传递给人工代理）将返回给用户。
- en: 6.5 Evaluating and analyzing RAG performance
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 评估和分析RAG性能
- en: 'Evaluating the capabilities of a RAG model within a conversational AI system
    is multifaceted. Each capability must be evaluated for an overall result. Most
    evaluations consider three aspects:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话式人工智能系统中评估RAG模型的能力是多方面的。必须对每个能力进行评估以得出总体结果。大多数评估考虑了三个方面：
- en: '*Was it the right response?* Did the answer directly address the user’s question?
    For example, if the user asked about resetting a password, the response should
    clearly explain the steps rather than discussing broader account security topics.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*这是否是正确的响应？* 答案是否直接回答了用户的问题？例如，如果用户询问重置密码，响应应清楚地解释步骤，而不是讨论更广泛的账户安全问题。'
- en: '*Was it in the right context for this user?* Did the response consider the
    user’s specific situation or history? For example, if a user previously reported
    an account problem, the system should provide a tailored follow-up solution instead
    of generic advice.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*这个用户是否处于正确的上下文中？* 响应是否考虑了用户的特定情况或历史？例如，如果用户之前报告了账户问题，系统应提供定制的后续解决方案，而不是通用的建议。'
- en: '*Was it grounded in the documents (or hallucinated or made up by the generation
    process)?* Did the response accurately reflect the information retrieved from
    the source documents without fabricating details? For example, a response should
    correctly reference a company’s policy document when explaining return procedures
    instead of creating nonexistent policies.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*响应是否基于文档（或由生成过程虚构或编造）？* 响应是否准确反映了从源文档中检索的信息，而没有编造细节？例如，在解释退货程序时，应正确引用公司的政策文件，而不是创建不存在的政策。'
- en: 'An LLM can score responses based on the key criteria to enhance the evaluation
    process. This approach works best when combined with human review: the LLM provides
    an initial assessment, and human evaluators then verify the accuracy and contextual
    relevance of the responses.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LLM可以根据关键标准对响应进行评分，以增强评估过程。这种方法与人工审查结合使用时效果最佳：LLM提供初步评估，然后人工评估人员验证响应的准确性和上下文相关性。
- en: These evaluation criteria help determine the truthfulness of the chatbot’s responses.
    The generated answers should also be accurate if the source documents are accurate
    and RAG retrieved the correct documents. The evaluation of the responses can be
    broken down into assessing the different components of RAG, which can be individually
    evaluated for overall performance, including the quality of the document index,
    the effectiveness of the retrieval process, and the accuracy of the answer generation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这些评估标准有助于确定聊天机器人的回答的真实性。如果源文档准确且RAG检索了正确的文档，则生成的答案也应准确。响应的评估可以分解为评估RAG的不同组件，可以单独评估其整体性能，包括文档索引的质量、检索过程的有效性和答案生成的准确性。
- en: 6.5.1 Indexing metrics
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 索引指标
- en: Indexing metrics provide insights into how efficiently a system can organize,
    store, and retrieve vast amounts of data. Key considerations include indexing
    speed, storage requirements, scalability, and how well the system handles high-dimensional
    data like vectors. Table 6.3 summarizes these important aspects, offering a quick
    overview and relevant examples.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 索引指标提供了关于系统如何高效组织、存储和检索大量数据的见解。关键考虑因素包括索引速度、存储需求、可扩展性以及系统处理高维数据（如向量）的能力。表6.3总结了这些重要方面，提供了快速概述和相关示例。
- en: Table 6.3 Critical metrics that influence the efficiency and accuracy of a RAG
    system’s document index
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.3 影响RAG系统文档索引效率和准确性的关键指标
- en: '| Aspect | Summary | Example |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 摘要 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Indexing metrics  | Evaluates speed, storage needs, and scalability. Critical
    for large-scale data systems.  | Indexing for a news aggregator where speed and
    scale are crucial  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 索引指标 | 评估速度、存储需求和可扩展性。对大规模数据系统至关重要 | 对于新闻聚合器，速度和规模至关重要 |'
- en: '| Vector database performance  | Measures performance in handling high-dimensional
    data  | For technical support, accurate troubleshooting steps must be assembled
    for multiple documents. E.g., “Why is my device overheating?”  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 向量数据库性能 | 衡量处理高维数据的性能 | 对于技术支持，必须为多个文档组装准确的故障排除步骤。例如，“为什么我的设备过热？” |'
- en: '| Recall rate  | Indicates accuracy in retrieving relevant data. High recall
    is vital for complete retrieval.  | In legal document retrieval, high recall ensures
    all relevant cases are found.  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 表示检索相关数据的准确性。高召回率对于完整检索至关重要。 | 在法律文件检索中，高召回率确保找到所有相关案例。 |'
- en: '| Query complexity  | Affects performance based on query specifics, dimensionality,
    and dataset diversity  | Financial databases handling complex queries across multiple
    data points  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 查询复杂性 | 根据查询的具体情况、维度和数据集多样性影响性能 | 处理多个数据点的金融数据库中的复杂查询 |'
- en: '| Benchmarking tools  | Tools like ANN-Benchmark compare algorithms on metrics
    like recall versus QPS.  | Evaluating which algorithm best balances speed and
    accuracy for a video search engine  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 基准测试工具 | 像ANN-Benchmark这样的工具会在诸如召回率与QPS（每秒查询数）之类的指标上比较算法。 | 评估哪种算法在视频搜索引擎中最好地平衡了速度和准确性
    |'
- en: The first critical component to assess is the indexing metrics, which involve
    evaluating the efficiency and effectiveness of organizing and accessing data in
    a system’s knowledge base. This includes examining factors such as indexing speed,
    storage requirements, and the scalability of the indexing process. Efficient indexing
    is crucial for a RAG system, as it impacts the speed and accuracy of information
    retrieval. Ineffective indexing can result in slow response times and inaccurate
    data retrieval, compromising response quality.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 首要的关键组成部分是评估索引指标，这涉及到评估在一个系统知识库中组织和访问数据的效率和效果。这包括检查诸如索引速度、存储需求和索引过程的可扩展性等因素。高效的索引对于RAG系统至关重要，因为它影响信息检索的速度和准确性。无效的索引可能导致响应时间慢和数据检索不准确，从而损害响应质量。
- en: Vector database performance is another vital metric specializing in storing
    and retrieving high-dimensional vectors representing complex data, such as text,
    images, or embeddings. These databases perform approximate rather than exact match
    searches, necessitating performance evaluation beyond traditional database performance
    measures like queries per second (QPS) and latency. While these metrics are important
    for evaluating system speed and responsiveness, they do not directly capture the
    accuracy of retrieval results. Therefore, besides QPS and latency, the *recall
    rate* is another essential performance metric for vector databases. If the vector
    database performs well, the RAG model can access high-quality, relevant information,
    leading to more accurate and contextually appropriate generated content. Conversely,
    poor performance can result in slow retrieval times and irrelevant or less useful
    data being used for generation.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库的性能是另一个重要的指标，专注于存储和检索表示复杂数据（如文本、图像或嵌入）的高维向量。这些数据库执行近似匹配搜索而不是精确匹配搜索，因此需要超越传统数据库性能指标（如每秒查询数QPS和延迟）的性能评估。虽然这些指标对于评估系统速度和响应性很重要，但它们并不能直接捕捉检索结果的准确性。因此，除了QPS和延迟之外，*召回率*也是向量数据库的另一个重要性能指标。如果向量数据库表现良好，RAG模型可以访问高质量、相关的信息，从而生成更准确和上下文适当的生成内容。相反，性能不佳可能导致检索时间慢，以及使用不相关或不那么有用的数据生成。
- en: Consider a customer support scenario where a chatbot powered by a RAG system
    is utilized to handle inquiries. With a high recall rate, the chatbot accesses
    a broad range of information from the knowledge base, effectively resolving customer
    queries and enhancing satisfaction. However, a low recall rate can lead to missed
    crucial information, resulting in inadequate responses and increased customer
    frustration. Therefore, the chatbot’s effectiveness significantly depends on its
    ability to comprehensively retrieve relevant information, underscoring the importance
    of a high recall rate in such automated support systems.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个客户支持场景，其中由RAG系统驱动的聊天机器人被用来处理询问。在高召回率的情况下，聊天机器人能够从知识库中访问广泛的信息，有效地解决客户查询并提高满意度。然而，低召回率可能导致关键信息的遗漏，导致响应不足并增加客户的不满。因此，聊天机器人的有效性在很大程度上取决于其全面检索相关信息的能力，这突出了在自动化支持系统中高召回率的重要性。
- en: Query complexity, influenced by factors like the dimensionality and specificity
    of the query, as well as the data diversity, also affects vector database performance.
    Higher-dimensional queries require more computational resources because distance
    calculations between vectors become more complex. This can lead to increased time
    and memory usage for retrieval tasks. More specific queries might target very
    narrow segments of the vector space, which can challenge the indexing system to
    efficiently isolate and retrieve the relevant vectors, especially in large datasets.
    High query complexity can strain the system, potentially leading to slower retrieval
    times and less relevant data being returned. Also, complex queries make distinguishing
    between relevant and irrelevant results difficult.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 查询复杂度，受查询的维度和特定性、数据多样性等因素影响，也会影响向量数据库的性能。高维查询需要更多的计算资源，因为向量之间的距离计算变得更加复杂。这可能导致检索任务的时间和内存使用增加。更具体的查询可能针对向量空间中的非常狭窄的片段，这可能会挑战索引系统有效地隔离和检索相关向量，尤其是在大型数据集中。高查询复杂度可能会使系统压力增大，可能导致检索时间变慢和返回的数据相关性降低。此外，复杂的查询使得区分相关和不相关结果变得困难。
- en: Imagine a chatbot on an e-commerce platform designed to help customers find
    products using complex queries involving multiple attributes like brand, color,
    size, and user ratings. For example, a customer might ask the chatbot for “6.5-sized
    blue Adidas running shoes with a minimum of a 4-star rating.” This query presents
    a multifaceted challenge due to its specificity across several dimensions. Each
    of these attributes represents a different vector in the database.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个电子商务平台上的聊天机器人，它旨在帮助客户使用涉及多个属性（如品牌、颜色、尺码和用户评分）的复杂查询来查找产品。例如，客户可能会要求聊天机器人提供“6.5号蓝色阿迪达斯运动鞋，最低4星评分。”这个查询由于其跨多个维度的特定性而提出了多方面的挑战。这些属性中的每一个在数据库中代表一个不同的向量。
- en: Benchmarking tools like ANN-Benchmarks and VectorDBBench help evaluate these
    aspects by comparing different algorithms and configurations, ensuring the RAG
    system is built on a robust retrieval foundation for consistently high-quality
    content generation. ANN-Benchmarks plots the recall rate on the *x*-axis against
    QPS on the *y*-axis, illustrating each algorithm’s performance at different retrieval
    accuracy levels. VectorDBBench displays QPS and recall rates separately.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 像ANN-Benchmarks和VectorDBBench这样的基准测试工具通过比较不同的算法和配置来帮助评估这些方面，确保RAG系统建立在稳健的检索基础上，以实现持续的高质量内容生成。ANN-Benchmarks在*x*轴上绘制召回率，在*y*轴上绘制每秒查询数（QPS），说明了每种算法在不同检索准确度水平上的性能。VectorDBBench分别显示QPS和召回率。
- en: 6.5.2 Retrieval metrics
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 检索指标
- en: The next capability is retrieval metrics, which gauge the system’s ability to
    fetch relevant information from indexed data. Key aspects include retrieval accuracy,
    precision, recall, and response time. Effective retrieval metrics ensure users
    receive accurate and relevant responses, boosting satisfaction and trust in the
    conversational AI system. Table 6.4 summarizes these important aspects, followed
    by more detailed explanations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个能力是检索指标，它衡量系统从索引数据中检索相关信息的能力。关键方面包括检索准确度、精确度、召回率和响应时间。有效的检索指标确保用户收到准确和相关的响应，从而提高对对话式人工智能系统的满意度和信任度。表6.4总结了这些重要方面，随后是更详细的解释。
- en: Table 6.4 Critical aspects influencing retrieval metrics
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.4 影响检索指标的关键方面
- en: '| Aspect | Summary | Example |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 概述 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Retrieval accuracy  | Evaluates the system’s ability to retrieve relevant
    information from indexed data  | Ensuring a chatbot retrieves accurate troubleshooting
    guides from a large dataset  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 检索准确度 | 评估系统从索引数据中检索相关信息的能力 | 确保聊天机器人从大型数据集中检索准确的故障排除指南 |'
- en: '| Precision and recall  | Precision measures relevance of retrieved docs; recall
    measures how many relevant docs are retrieved.  | Balancing precision and recall
    when retrieving product recommendations in an e-commerce chatbot  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 精确度和召回率 | 精确度衡量检索文档的相关性；召回率衡量检索到的相关文档数量。 | 在电子商务聊天机器人中检索产品推荐时平衡精确度和召回率 |'
- en: '| Context precision and context recall  | Specific to RAG: context precision
    checks relevance; context recall checks coverage of relevant info  | Evaluating
    how well a generated response in a support chatbot matches the query’s context  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 上下文精确度和上下文召回率 | 特定于RAG：上下文精确度检查相关性；上下文召回率检查相关信息覆盖范围 | 评估生成的响应在支持聊天机器人中与查询上下文匹配的程度
    |'
- en: '| Parameter optimization  | Tuning search parameters and algorithms to improve
    speed, accuracy, and relevance of results  | Adjusting FAISS clusters or Elasticsearch
    settings to improve document retrieval for legal databases  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 参数优化 | 调整搜索参数和算法以提高结果的速率、准确性和相关性 | 调整FAISS集群或Elasticsearch设置以改善法律数据库的文档检索
    |'
- en: '| Embedding models  | The use of different embeddings impacts the retrieval
    quality by enhancing precision or recall.  | Using BERT for precise context understanding
    in a legal advice chatbot  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入模型 | 使用不同的嵌入通过提高精确度或召回率来影响检索质量 | 在法律建议聊天机器人中使用BERT进行精确的上下文理解 |'
- en: '| Filtering and reranking  | Strategies to remove noise and rerank results
    to improve both relevance and accuracy  | Filtering out irrelevant articles in
    a news aggregation chatbot and then reranking top results  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 过滤和重新排序 | 策略用于去除噪声并重新排序结果以提高相关性和准确性 | 在新闻聚合聊天机器人中过滤掉不相关的文章，然后重新排序顶级结果 |'
- en: '| Normalized Discounted Cumulative Gain (NDCG)  | Assesses the ranking quality
    by considering relevance and position of retrieved documents  | Ensuring the most
    relevant help articles appear at the top in a technical support chatbot  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 标准化折现累积增益 (NDCG) | 通过考虑检索文档的相关性和位置来评估排名质量 | 确保在技术支持聊天机器人中，最相关的帮助文章出现在顶部 |'
- en: These metrics evaluate search quality, document relevance, and how well user
    queries align with responses. *Retrieval accuracy* measures the system’s ability
    to fetch the most relevant information from its indexed data. If retrieval accuracy
    is low, the chatbot may return responses that are only loosely related to the
    user’s query or fail to retrieve critical details.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标评估搜索质量、文档相关性和用户查询与响应的匹配程度。*检索准确性*衡量系统从其索引数据中检索最相关信息的能力。如果检索准确性低，聊天机器人可能返回仅与用户查询松散相关的响应或无法检索关键细节。
- en: Precision measures the proportion of relevant retrieved documents. Recall measures
    how many relevant documents are retrieved by the system. A high recall value indicates
    the system retrieves many relevant documents from the database. In contrast, a
    high precision value indicates that the retrieved documents are mostly relevant
    to the user’s query. Balancing recall and precision is crucial to ensure comprehensive
    coverage of relevant information and minimize irrelevant results.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度衡量检索到的相关文档的比例。召回率衡量系统检索到的相关文档的数量。高召回值表示系统从数据库中检索到许多相关文档。相比之下，高精确度值表示检索到的文档大多与用户的查询相关。平衡召回率和精确度对于确保全面覆盖相关信息并最小化无关结果至关重要。
- en: Specific to RAG models, context precision and context recall evaluate the alignment
    and coverage of generated responses relative to the user’s query. Context precision
    measures how precisely the retrieved context matches the user’s query, indicating
    the relevance and accuracy of the generated response. Context recall measures
    how comprehensively the generated response covers relevant information from the
    retrieved context.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RAG模型，上下文精确度和上下文召回率评估生成的响应与用户查询的对应关系和覆盖范围。上下文精确度衡量检索到的上下文与用户查询的匹配程度，表明生成响应的相关性和准确性。上下文召回率衡量生成的响应如何全面地覆盖检索上下文中的相关信息。
- en: 'You should implement various strategies to enhance the retrieval process:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该实施各种策略来增强检索过程：
- en: Optimizing search parameters
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化搜索参数
- en: Using different embedding models
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的嵌入模型
- en: Implementing filtering and reranking
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施过滤和重新排序
- en: The first strategy is to adjust retrieval parameters to improve both speed and
    accuracy. Adjusting the search parameters, such as the number of clusters in FAISS
    or the search query complexity in Elasticsearch, can significantly enhance the
    precision and recall of retrieved documents. This ensures that the system returns
    the most relevant documents, increasing precision and retrieving all pertinent
    documents, boosting recall.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种策略是调整检索参数以改善速度和准确性。调整搜索参数，如FAISS中的集群数量或Elasticsearch中的搜索查询复杂性，可以显著提高检索文档的精确度和召回率。这确保了系统返回最相关的文档，提高了精确度，并检索所有相关文档，提高了召回率。
- en: Parameter optimization can also reduce response time, making the system more
    responsive. Tuning the indexing and query algorithms can lead to faster retrieval
    times without compromising accuracy.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 参数优化还可以减少响应时间，使系统更加响应迅速。调整索引和查询算法可以导致检索时间更快，而不会影响准确性。
- en: The second strategy is to utilize different embedding models to find the most
    effective combination for retrieving relevant documents. Embeddings can drastically
    affect retrieval quality. For example, BERT and Sentence-BERT can improve the
    system’s understanding of the context and semantics of user queries, enhancing
    precision. GPT-3 embeddings can offer a broader contextual grasp, improving recall
    by retrieving more relevant documents from diverse contexts.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个策略是利用不同的嵌入模型来找到检索相关文档的最有效组合。嵌入可以极大地影响检索质量。例如，BERT和Sentence-BERT可以提高系统对用户查询的上下文和语义的理解，从而提高精确度。GPT-3嵌入可以提供更广泛的上下文理解，通过从不同的上下文中检索更多相关文档来提高召回率。
- en: The third strategy is to implement filtering and reranking. Filtering and reranking
    strategies further enhance precision by removing irrelevant documents and boost
    recall by prioritizing relevant documents. Techniques like domain-specific filters
    or context-aware reranking can significantly refine the relevance of top results.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个策略是实现过滤和重新排序。过滤和重新排序策略通过移除无关文档进一步提高了精确度，并通过优先考虑相关文档来提高召回率。如特定领域过滤器或上下文感知重新排序等技术可以显著提高顶部结果的相关性。
- en: Normalized Discounted Cumulative Gain (NDCG) is useful when the order of retrieved
    documents matters. It measures the effectiveness of ranking algorithms by evaluating
    the relevance and position of documents in the search results, providing a comprehensive
    ranking quality assessment.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化折现累积增益（NDCG）在检索文档顺序很重要时很有用。它通过评估搜索结果中文档的相关性和位置来衡量排名算法的有效性，提供全面的排名质量评估。
- en: 6.5.3 Generation metrics
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.3 生成指标
- en: 'The last area for evaluation is the generation itself: whether the generated
    output provides a relevant and complete answer. Table 6.5 offers a concise overview
    of these key metrics, strategies for enhancing generation quality, and practical
    examples of their application, followed by further details.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 评估的最后一个领域是生成本身：生成的输出是否提供了相关且完整的答案。表6.5提供了一个关于这些关键指标、提高生成质量策略及其应用实例的简要概述，随后是更详细的说明。
- en: Table 6.5 Critical aspects influencing generation metrics
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.5 影响生成指标的关键方面
- en: '| Aspect | Summary | Example |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 摘要 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Faithfulness  | Evaluates the factual accuracy of generated output based
    on retrieved context  | Ensuring chatbot answers are factually correct when responding
    to questions. E.g., “What are the consequences of breaching a contract?”  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 忠实度 | 根据检索到的上下文评估生成输出的事实准确性 | 确保聊天机器人回答在回答问题时是事实正确的。例如，“违反合同会有什么后果？” |'
- en: '| Answer relevancy  | Assesses how relevant the generated answer is to the
    specific user question  | “What is the status of my order?” The chatbot, after
    retrieving the relevant data, responds with, “Your order #12345 is currently in
    transit and expected to be delivered by August 15th.” This response is directly
    relevant to the user’s question, providing specific information about the order
    status without unnecessary details.  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 答案相关性 | 评估生成的答案与特定用户问题的相关性 | “我的订单状态如何？”聊天机器人检索到相关数据后，回答：“您的订单编号12345目前正在运输中，预计将于8月15日交付。”此响应直接与用户的问题相关，提供了关于订单状态的特定信息，而没有不必要的细节。
    |'
- en: '| Fine-tuning  | Improves generation by aligning the LLM with domain- specific
    data, enhancing accuracy and relevance  | Fine-tuning an LLM for legal advice,
    ensuring generated responses are accurate and legally relevant  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 通过将LLM与特定领域的数据对齐，提高生成内容的准确性和相关性 | 微调LLM以提供法律建议，确保生成的响应准确且具有法律相关性 |'
- en: '| Prompt engineering  | Crafting prompts to guide the LLM in generating more
    contextually appropriate and relevant responses  | Using prompt engineering to
    ensure a healthcare chatbot provides clear, relevant medical suggestions  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 提示工程 | 通过构建提示来引导LLM生成更符合语境和相关的响应 | 使用提示工程确保医疗聊天机器人提供清晰、相关的医疗建议 |'
- en: '| Model blending  | Combining specialized models to enhance the quality of
    generation, balancing accuracy and fluency  | Blending a retrieval-focused model
    with a language-focused model to generate accurate, fluent responses  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 模型融合 | 结合专业模型以提升生成质量，平衡准确性和流畅性 | 将以检索为重点的模型与以语言为重点的模型融合，以生成准确、流畅的响应 |'
- en: '| Sensibleness and Specificity Average (SSA)  | Measures response quality in
    open-domain chatbots, ensuring responses are sensible and specific  | Assessing
    open-domain chatbot responses to ensure they make sense and are not overly vague  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| Faithfulness- Evaluator  | Assesses whether the generated response avoids
    hallucinations by aligning with retrieved context  | Using FaithfulnessEvaluator
    to ensure that a finance chatbot’s responses are grounded in retrieved documents  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: Two primary metrics for assessing LLM performance are faithfulness and answer
    relevancy. Faithfulness evaluates the factual accuracy of the answer based on
    the retrieved context, while answer relevancy assesses how pertinent the answer
    is to the given question. An answer may be factually accurate (faithful) but not
    well-matched to the question (less relevant), or it may be accurate but not based
    on sourced documents.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Several strategies can be employed to enhance generation metrics. Fine-tuning
    the LLM on domain-specific data can improve faithfulness and relevancy by aligning
    the model more closely with the context in which it will operate. This ensures
    that generated responses are accurate and pertinent to the specific domain. You
    may also want to incorporate prompt engineering. The generation process becomes
    more focused and aligned with the user’s intent by explicitly including necessary
    context or constraints within the prompt. Model blending may also be used. Combining
    multiple models, each specialized in different aspects of the task, can help enhance
    generation quality. For example, one model may excel at retrieving accurate information,
    while another might be better at generating fluent and contextually appropriate
    language. Blending these models can lead to more balanced and effective output.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Sensibleness and Specificity Average (SSA) metrics evaluate response quality
    in open-domain chatbots. Sensibleness ensures that responses make contextual sense,
    while the specificity average ensures comprehensiveness without vagueness. Historically,
    human interactions were necessary for assigning these ratings.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: While avoiding vague responses is essential, preventing LLMs from hallucinating
    is equally critical. LlamaIndex established a FaithfulnessEvaluator metric to
    measure hallucination by assessing whether the response aligns with the retrieved
    context. LlamaIndex was developed to address the challenge of connecting LLMs
    with various data sources, so LLMs can access, query, and generate insights from
    structured and unstructured data. It is a framework for building context-augmented
    generative AI applications. It offers capabilities for data integration, indexing,
    enrichment query processing, and more. Stay current with the latest updates from
    LlamaIndex.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: RAG-integrated conversational AI efficiently addresses rare or complex queries.
    Nevertheless, realizing these advantages from RAG necessitates ongoing monitoring
    of all components, particularly where failures are common, such as during retrieval
    and generation.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.4 Comparing efficiency of indexing and embedding solutions for RAG
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we introduced the contributing factors to the efficiency
    of RAG systems. The indexing and embedding components are crucial for effectively
    retrieving relevant documents and ensuring the system’s responsiveness. When implementing
    RAG in a conversational AI system, it is important to measure its impact on retrieval
    accuracy and response relevance. Benchmarking is essential to creating an effective
    RAG system. You must have a method to evaluate whether changes in the system prompt
    improve user-query hit rates. Is the improvement 1%, 2%, or more? This is fundamental
    to knowing if your RAG system is truly effective.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, without proper monitoring, validation, and evaluation, it will
    be difficult to prove the effectiveness of your system. RAG is inherently complex,
    and typical implementations have 50% to 60% accuracy at first. You’ll want to
    increase the accuracy to above 80% for a practical solution.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: PharmaBot, initially developed as a COVID-19 chatbot to handle general inquiries,
    such as vaccine information and appointment scheduling, is now set to be enhanced
    with RAG. The goal is to extend PharmaBot’s capabilities to answer more nuanced
    questions, such as “Can I take ibuprofen with my blood pressure medication?” and
    “My arms are sore after getting the vaccine. What should I do?” First, we’ll want
    to select a dataset of medical articles, research papers, and guidelines from
    health organizations, all focusing on various health problems and drug and vaccine
    interactions. Next, we’ll compile a set of representative queries like those previously
    listed, which are what users might ask.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: We’ll select the indexing solutions and the embeddings we want to use. For indexing,
    we’ll consider several options, such as FAISS and Elasticsearch. The most popular
    source for the latest performance benchmarks of text embedding models is the MTEB
    leaderboards hosted by Hugging Face. While MTEB provides a valuable starting point,
    the displayed results are self-reported, and many models may not perform as accurately
    when applied to real-world data. BERT, Sentence-BERT, or GPT-3 embeddings are
    worth considering, as they have been used in many solutions. Then we’ll run our
    selections to generate embeddings for our content and index our embeddings using
    the selected methods. Finally, we’ll run our queries and measure our performance.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating PharmaBot enhanced with RAG, we can use table 6.6 to determine
    relevant metrics, establish baselines and goals for these metrics to assess improvements
    introduced by different combinations of indexing (FAISS, Elasticsearch) and embedding
    (BERT, Sentence-BERT, GPT-3) solutions, and compare solutions. For example, we
    could compare how the combinations of RAG components perform against business
    objectives.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.6 Prioritized metrics for RAG evaluation based on business objectives
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Business objective | Prioritized metrics | Why? | Example |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| Customer satisfaction  | Response accuracy Relevance'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '| Directly impacts user experience and satisfaction  | PharmaBot providing
    accurate answers to users’ queries  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| Operational efficiency  | Latency Throughput'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '| Ensures the system can handle high query volumes quickly  | A customer support
    chatbot for a large e-commerce platform should prioritize low latency to provide
    quick responses during peak shopping times.  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| Scalability  | Queries processed per second Resource utilization'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluates system performance under increasing loads  | Chatbot for a healthcare
    provider managing seasonal spikes in appointments and queries  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| Cost-effectiveness  | CPU usage Memory usage'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '| Ensures high performance without excessive resource consumption  | Chatbot
    for a non-profit organization providing 24/7 mental health support on limited
    funding  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: You could create a comparison table like the one in table 6.7\. These sample
    numbers are hypothetical and should be adjusted based on actual selected components
    and benchmarking results. For example, one configuration may be selecting Elasticsearch
    and then using three different embedding models to arrive at your numbers.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.7 Evaluation of PharmaBot with various configurations
  id: totrans-328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Metrics |  | Configuration 1 | Configuration 2 | Configuration 3 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| Response accuracy  | Recall  | 0.85  | 0.87  | 0.88  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '|  | Precision  | 0.75  | 0.77  | 0.78  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '|  | F1 score  | 0.8  | 0.82  | 0.83  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| Relevance  | Mean reciprocal rank (MMR)  | 0.70  | 0.72  | 0.74  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '|  | Average precision  | 0.65  | 0.68  | 0.70  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| Latency  | Average latency (ms)  | 50  | 55  | 60  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| Throughput  | Queries/second  | 20  | 18  | 16  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| Resource utilization  | CPU usage (%)  | 70  | 65  | 90  |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '|  | Memory usage  | 8  | 7  | 12  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: When trying to satisfy the overall business requirements for PharmaBot using
    RAG, various trade-offs must be considered based on the evaluation metrics. An
    indexing component combined with selected embedding components offers high response
    accuracy and relevance with lower latency and higher throughput, making them suitable
    for systems requiring quick and accurate responses. However, these combinations
    exhibit moderate to high resource utilization, which may increase operational
    costs. Conversely, integrating another embedding component with various indexing
    strategies provides superior response accuracy and relevance but at the cost of
    significantly higher latency and lower throughput due to the computational demands
    of the selected embeddings. This can impact the system’s ability to handle high
    query volumes efficiently. The high CPU and memory usage may also strain resources,
    increasing operational costs. Ultimately, selecting the optimal combination requires
    balancing the need for high accuracy and relevance with the system’s capacity
    to handle queries efficiently while managing resource utilization to control costs.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the ongoing re-evaluation and refinement of the RAG system should
    be considered. Table 6.7 is not comprehensive, but the key point is to decide
    on key evaluation metrics and then use a structured evaluation approach. Systematic
    testing and focusing on the RAG component provide a robust RAG evaluation pipeline.
    The overall goal is to see an upward trend at the end, as illustrated in figure
    6.11\. Systematically applying the strategies discussed throughout this chapter
    and then analyzing the results reveal the impact of different configurations on
    RAG performance. Some tweaks show significant improvements, emphasizing the importance
    of experimentation and tuning. There is no best approach; exploring multiple directions
    when tuning your RAG systems is crucial.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F11_Freed2.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 Different configurations and enhancements improve the accuracy of
    the RAG system.
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Additionally, you may want to use RAG evaluation frameworks, from proprietary
    paid solutions to open source tools. Selecting the right solution requires balancing
    considerations around ease of maintenance and operational burden, plus how well
    the metrics observed by the tool map to your RAG pipeline and your business objectives.
    The following solutions are current examples, but more are being developed, providing
    even more options:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '*Arize*—A model monitoring platform focusing on precision, recall, and F1 score.
    It is beneficial in scenarios requiring ongoing performance tracking, ensuring
    RAG systems consistently meet accuracy thresholds in real-time applications. Arize
    is a proprietary paid offering that provides robust support and continuous updates
    for enterprise deployments.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*RAGAS*—An open-source tool that offers streamlined, reference-free evaluation
    focusing on average precision (AP) and custom metrics like faithfulness. It assesses
    how well the generated content aligns with provided contexts, and it is suitable
    for initial assessments or when reference data is scarce.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Assess the relevance of responses generated by a RAG model within a conversational
    AI system:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define evaluation criteria to measure the relevance of responses generated by
    the RAG model.
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish a scoring system to quantify the relevance of responses based on factors
    such as semantic similarity and informativeness.
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Devise a set of user queries you will evaluate.
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a set of expected responses (manually).
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare the generated responses with the previously created responses to determine
    the level of relevance.
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate evaluation metrics such as precision, recall, and F1 score to quantitatively
    assess the performance of the RAG model in generating relevant responses.
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the evaluation results to identify patterns or areas where the RAG model
    excels or fails to generate relevant responses.
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss potential factors influencing response relevance and strategies for
    improving the RAG model’s performance in this aspect.
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evaluate document grounding with RAG:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate responses to user queries using the RAG model, and identify the source
    documents or passages from which the responses are derived.
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess the degree of grounding by comparing the relevance of the source documents
    or passages to the corresponding user queries.
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop a scoring mechanism to quantify the RAG model’s grounding effectiveness
    based on factors such as document relevance and coverage.
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional intent-based chatbots can be greatly enhanced by integrating search
    functionality.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intents are great for answering common short-head questions, and search is great
    for long-tail questions.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional search returns links or document passages instead of an answer.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG extends search capability by generating an answer from the documents retrieved
    by the search.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using RAG, chatbots can provide contextually appropriate responses in real
    time, reducing user frustration and enhancing the conversational experience. Grounding
    answers in the organization’s domain also solves intent maintenance and enhancement
    for developers.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG implementations must consider several problems, from handling latency to
    providing fallback mechanisms or handover to human agents to prevent hallucinations.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation of RAG must consider the different components of indexing, retrieval,
    and generation.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
