- en: 6 Enhancing responses with retrieval-augmented generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enhancing chatbot responses without coding intents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving weak understanding with RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the advantage of using RAG over traditional search models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the proper RAG techniques for your conversational AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing and improving the performance of RAG in your conversational AI systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In previous chapters, we saw the “chatbot doesn’t understand” pain point for
    question-answering bots. We first addressed it by helping the chatbot understand
    more intents, but at some point there are diminishing returns to this strategy.
    Uncommon questions from the “long tail” may never make sense to implement as intents.
    This chapter introduces ways to handle that “long tail,” including search and
    retrieval-augmented generation (RAG). These are great methods for improving a
    chatbot’s weak understanding.
  prefs: []
  type: TYPE_NORMAL
- en: We concluded chapter 5 with advice on when to avoid adding new intents, especially
    when dealing with diverse, infrequent domain-related problems. In this chapter,
    we’ll add search capabilities to improve weak understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Both search and RAG allow you to improve a chatbot by adding data and documents
    without programming new intents. This allows you to serve thousands of intents
    with the simplicity of training just a few. The answers provided by these methods
    are more straightforward to change—just change the documents rather than changing
    your chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Search and RAG can be easier for you as a builder and efficient for your users.
    Let’s explore how a chatbot can evolve using search and RAG capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '6.1 Beyond intents: The role of search in conversational AI'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional conversational AI centers on understanding user intents. Systems
    are trained to recognize predefined categories of user queries and to provide
    pre-scripted responses.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 illustrates a conceptual intent-based chatbot architecture in its
    simplest form. The chatbot’s classifier detects the intent and determines the
    appropriate dialogue flow. When the classifier cannot identify the user’s intent,
    the answer is a generic “I didn’t understand” response style, leading to the “chatbot
    not understanding me” pain point. Intent-based question-answering is a great way
    to start handling frequently asked questions—you can define an exact answer to
    be given for different question types. Initially, this is quite effective, but
    it breaks down. Users often ask questions that deviate from predefined intents.
    When the predefined intents are insufficient to handle the user’s questions, the
    user may receive irrelevant or incorrect responses, leading to frustration. Further,
    maintaining and evolving these intents requires significant effort.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F01_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 An intent-based chatbot first detects an intent and then maps it
    to an answer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There is a tradeoff between the specificity of the answer and the number of
    variations covered. Figure 6.2 demonstrates an example from the PharmaBot we introduced
    in chapter 3\. The answer is accurate but generic—it detected a question about
    side effects but did not answer all the nuances in the user’s question.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F02_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 Intent-based systems identify the main theme of an utterance and
    often give a static or generic answer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can handle this nuance by adding search capabilities. There are two primary
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Traditional search* supplies the user with documents or passages relevant
    to their query. The user uses these documents to find their answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*RAG* starts with a search process but expands on (augments) it by summarizing
    the passages into an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 6.3 illustrates how RAG refines information retrieval by locating relevant
    document passages and synthesizing them into a specific, contextualized answer.
    Unlike traditional intent-based systems that often respond with fixed or general
    answers, RAG dynamically pulls in content to address the user’s unique query,
    demonstrating how specific passages on ibuprofen and blood pressure are distilled
    into a targeted recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F03_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 RAG finds relevant passages and summarizes them, giving a targeted
    answer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s dive deeper into how we can add these capabilities effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Using search in conversational AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: User questions follow a “short head, long tail” distribution, as shown in figure
    6.4\. This distribution has a high frequency of common or popular questions (the
    short head). Most interactions involve less frequent, niche, or specialized queries
    (the long tail).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F04_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 Distribution of user questions. Intents address the most common,
    high-volume questions, while low-volume, unique questions may necessitate search
    integration.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For PharmaBot, the short head includes general COVID inquiries, such as vaccine
    information and appointments. Each bot will have a different short head, but it
    will cover the most popular questions. When the chatbot is trained well, these
    questions are recognized with high confidence. Intents afford builders complete
    control over short head queries, albeit potentially overlooking nuanced distinctions.
  prefs: []
  type: TYPE_NORMAL
- en: We saw earlier that PharmaBot did not handle a nuanced question well because
    it used a static intent for `#side_effects`. Figure 6.5 shows PharmaBot handling
    the same nuanced question using traditional search capability.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F05_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 Search finds relevant passages and displays those directly to the
    user, often with links to the source documents.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The response includes all nuances from the user’s question in this example.
    The passages reference vaccine side effects, ibuprofen, blood pressure, and pain.
    However, the chatbot did not provide a single, cohesive answer. Instead, it offered
    document links and snippets. Users need to combine the answers from those documents
    and passages.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Benefits of traditional search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traditional search can complement an intent-based chatbot by enabling it to
    retrieve relevant information from a document repository. The approach offers
    several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Breadth*—The bot can access various materials in your document repository,
    giving it answers to different question types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Maintenance*—Adding knowledge to your bot can be as easy as adding or editing
    documents in your repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Technology*—Search is a well-established technology with mature algorithms
    and implementation methods. It can be implemented with relatively low computational
    resources and infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Speed*—While slower than a static intent-based response, traditional search
    executes reasonably quickly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, search is an excellent complement to an intent-based system. The most
    common way to combine intents and search is to use a confidence threshold in the
    chatbot’s natural language understanding (NLU) component. The NLU attempts to
    detect an intent from the user’s utterance. If an intent is detected with high
    confidence, an intent-based answer is returned. Otherwise, the user’s utterance
    is passed to a search component (in some conversational AI systems, this is called
    a *fallback action* or *inten**t*). The high-level architecture is illustrated
    in figure 6.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F06_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 Intents and searches have complementary functions. A search-augmented
    bot uses intent-based answers when it recognizes the utterance with high confidence;
    otherwise, it defers to search. Using intents and search together improves chatbot
    capabilities, but this approach still has some limitations.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.1.3 Drawbacks of traditional search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Two fundamental problem areas exist when integrating traditional search with
    chatbot applications: the quality of search results and the user experience of
    how the search results are presented.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One major drawback to search quality is its reliance on keyword matching, which
    may be inaccurate or brittle, depending on the user’s phrasing. Consider the previous
    example question: “Can I take ibuprofen with my blood pressure medication? My
    arms are sore after getting the vaccine.” This may be converted to “ibuprofen
    blood pressure medication arms sore vaccine,” emphasizing the most relevant keywords
    but losing the nuance of the question.'
  prefs: []
  type: TYPE_NORMAL
- en: Not all search engines limit themselves to keyword matching, but it is ubiquitous.
    Newer search engines support searching by meaning rather than keywords. This approach
    is done with vector databases and will be described more fully in the next section.
    Like traditional searches, vector database searches take an input query and return
    a set of relevant documents and passages.
  prefs: []
  type: TYPE_NORMAL
- en: The other major drawback of search-based options is the user experience of receiving
    documents and passages. Some of the user experience limitations derive from the
    limited space in a chatbot window and the challenges of presenting multiple search
    results well. These are commonly addressed by showing a small number of results
    (possibly asking the user if they want to see more).
  prefs: []
  type: TYPE_NORMAL
- en: Screen real estate is sometimes preserved by showing document links (not the
    passages). In this case, users must leave the chat interface, which disrupts the
    conversation flow and may lead to the user abandoning the chatbot and continuing
    where the document links took them.
  prefs: []
  type: TYPE_NORMAL
- en: Search results are also challenging to handle through a voice interface, leading
    to lengthy readouts and a non-optimal user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Most critically, the search does not result in a cohesive answer. Some users
    may prefer to construct their answers from relevant documents. Most users, however,
    are frustrated when a cohesive answer is not given, and they must do the piecing
    together—“Why did I use the chatbot in the first place? I could have searched
    on my own.”
  prefs: []
  type: TYPE_NORMAL
- en: Note  You might consider combining web search with answer synthesis. While a
    chatbot can create synthesized answers from web search results, this approach
    also has limitations. It requires more sophisticated processing, but it addresses
    the user experience limitations by eliminating the need for users to sift through
    multiple documents. This approach depends heavily on how well individual passages
    match the query, and the summarization may miss nuances or context if the retrieved
    data isn’t comprehensive.
  prefs: []
  type: TYPE_NORMAL
- en: Search with answer synthesis typically relies on rule-based extraction methods,
    ranking algorithms, keyword matching, or predefined heuristics. The responses
    are presented by combining information from retrieved documents. While this can
    efficiently surface relevant information, it may struggle with incomplete or ambiguous
    queries. The system does not truly “understand” the content. Instead, it selects
    and reformulates existing text, which can lead to missing context, fragmented
    responses, or over-reliance on the most prominent retrieved results rather than
    the most accurate ones. It lacks the flexibility of generative approaches.
  prefs: []
  type: TYPE_NORMAL
- en: This is whereRAGis a powerful alternative. RAG doesn’t just pull text from documents—it
    combines retrieval with generation, allowing the chatbot to produce a cohesive,
    contextually aware answer using relevant content from various sources. Unlike
    traditional search and summarization approaches, RAG can adapt to a broader range
    of user questions and provide deeper, more accurate responses by using a combination
    of real-time retrieval and language generation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will explore how RAG enhances chatbot responses by improving
    accuracy and maintaining context, even with complex or nuanced queries.
  prefs: []
  type: TYPE_NORMAL
- en: '6.2 Beyond search: Generating answers with RAG'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The lack of clear answers is a limitation of traditional search methods. To
    overcome these limitations, we’ll look at RAG as an advanced alternative. At its
    core, RAG combines the strengths of search-based information retrieval with the
    flexibility of generative models, offering a more comprehensive approach to understanding
    and responding to user queries. Most importantly, this response includes an answer.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Using RAG in conversational AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG combines the best of retrieval and generation techniques to enhance the
    user experience. Like traditional search, it retrieves relevant passages to handle
    long-tail questions. RAG then feeds the passages and the user’s request to generative
    AI, which creates the answer. RAG “augments” the retrieved passages by generating
    an answer, creating a seamless conversational flow, even for complex or long-tail
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.7 shows PharmaBot answering our familiar example using RAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH06_F07_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 RAG retrieves relevant passages and augments the response by synthesizing
    the information into a grounded answer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: PharmaBot may still retrieve the same passages as the traditional search, but
    now it summarizes them to generate an answer. The answer acknowledges the user’s
    specific concerns and provides tailored advice. Most importantly, the answer is
    grounded in PharmaBot’s source documents—not the generative AI’s general knowledge.
    PharmaBot may provide links to supporting documentation, but it has made the answer
    prominent rather than the document passages. This is a more effortless experience
    for users.
  prefs: []
  type: TYPE_NORMAL
- en: RAG empowers chatbots to better understand user questions, and it streamlines
    development efforts by minimizing the need for explicit intent classification.
    This shift in approach enhances user satisfaction and future-proofs conversational
    AI systems against the evolving landscape of human language and user needs.
  prefs: []
  type: TYPE_NORMAL
- en: The use of RAG introduces dynamism and diminishes user effort, contingent upon
    the avoidance of hallucinations. While RAG inherently reduces hallucinations,
    it does not eliminate them entirely. Attention should be paid to the quality of
    the retrieved documents, the generative model’s behavior, and when the retrieval
    fails to find relevant documents, as we’ll discuss later.
  prefs: []
  type: TYPE_NORMAL
- en: When the user interacts with the conversational AI, the retrieval system connects
    to the trusted content sources, executes the search (keyword, semantic, or vector),
    and provides a relevancy score for the retrieved results. A large language model
    (LLM) then augments its response using the retrieved information. It generates
    a response from the retrieved content and presents it to the user through the
    chatbot interface. It may also apply translation if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Like traditional search, RAG can complement intent-based systems. The short
    head is handled via intents, and RAG handles the long tail. The answers are as
    current as the document sources. Documents are easier to update than chatbots!
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Benefits of RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG is a great addition to question-answer bots. By retrieving information from
    your knowledge bases (and not generative AI model knowledge), RAG ensures that
    answers are grounded in accurate and up-to-date data, thus enhancing the bot’s
    credibility and reliability. Additionally, RAG expands the versatility of conversational
    AI systems by enabling them to handle a wide range of user queries and tasks effectively.
    It also reduces the “bot doesn’t understand” and “too much complexity out on the
    user” pain points.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits from RAG include enhanced answer relevance, adaptiveness, and improved
    handover to agents.
  prefs: []
  type: TYPE_NORMAL
- en: Enhanced relevance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RAG’s essential advantage is its ability to access and integrate the latest
    information from your knowledge bases. The knowledge bases provide more varied
    information than pretrained intents do, and LLMs can adapt knowledge from your
    documents into an answer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider another example from PharmaBot. The initial PharmaBot included
    several intents, including guidance for dining in restaurants. It responded based
    on general guidelines and included a reference to “check local regulations.” There
    is no tailoring of responses to the user—it is a one-size-fits-all answer.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 compares intent-based and RAG-based versions of PharmaBot. Intent-based
    gives a generic answer, while RAG-based localizes to New York per the user’s question.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 A comparison of answers for intents and RAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Interaction with an intent-based bot | Interaction with a RAG-integrated
    bot |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `User: What are the current COVID-19 guidelines for restaurants in New York?`
    `PharmaBot: Current COVID-19 guidelines for restaurant dining include limited
    indoor capacity, mask requirements for staff and patrons when not seated, and
    enhanced cleaning protocols. Please check local regulations for the most up-to-date
    information.`'
  prefs: []
  type: TYPE_NORMAL
- en: '| `User: What are the current COVID-19 guidelines for restaurants in New York?`
    `PharmaBot: COVID-19 remains a significant concern for New Yorkers. Make sure
    you are up to date on the updated COVID-19 vaccines for the 2023-2024 season that
    offer protection against emerging variants and enhance overall immunity. Masks
    continue to be effective tools in mitigating the spread of COVID-19 and other
    respiratory viruses, reducing the transmission of droplets during various activities.
    To minimize the risk of contracting COVID-19, individuals are advised to wear
    masks in crowded indoor public settings.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: NYC Health'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The RAG-integrated chatbot acknowledges the user’s location in New York City.
    It provides more specific information tailored to the local guidelines and regulations,
    pulled from the current NYC Health Guidelines and summarized for restaurant dining.
    This personalized approach enhances the user experience by delivering more relevant
    and actionable guidance based on the user’s context.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptiveness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On the conversational side, another advantage of RAG is in the way the conversational
    AI generates the response—RAG can adjust its generated response to the style of
    the user’s question. The response can be similarly more formal if the user’s tone
    is more fact-seeking. The responses to the user’s unique question are generated
    in real time. Sometimes users expect a concise and direct answer (“Yes or no,
    are there restrictions on dining in restaurants?”*),* and sometimes they expect
    a longer and more complex response (“Can I take my extended family to a restaurant,
    and will we have to wear masks?”). Both questions may use the same source documents
    but will receive very different answers.
  prefs: []
  type: TYPE_NORMAL
- en: Handover to human agent (or not answering)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are occasions when the conversational AI cannot find a definite response.
    In these cases, it is better for the bot to answer that it doesn’t know or to
    transfer the user to a human agent. Figure 6.8 depicts a user asking a question
    a chatbot can’t answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F08_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 Supplementing RAG with human agents. If the answer has poor semantic
    overlap with the retrieved documents, send the user to a human agent instead.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When a user asks a question, the conversational AI follows a multistep process
    to retrieve information, generate a response, and determine whether the answer
    is sufficiently grounded in retrieved evidence before delivering a final response.
    The following steps illustrate this process, showing how the system retrieves
    relevant passages, generates a candidate answer, evaluates its accuracy, and ultimately
    decides whether to respond or transfer the user to a human agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Passage retrieval*—The system retrieves passages related to achalasia, dysphagia,
    and general information about vaccine side effects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example passage 1: `"Achalasia` `is` `a` `condition` `affecting` `the` `esophagus,`
    `causing` `difficulty` `in` `swallowing."`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example passage 2: `"Common` `side` `effects` `of` `vaccines` `include` `soreness,
    fever,` `and` `fatigue."`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example passage 3: `"Dysphagia,` `or` `difficulty` `swallowing,` `can` `be`
    `a symptom` `of` `esophageal` `conditions` `like` `achalasia."`'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. *Answer generation*—The LLM generates a candidate response based on the
    retrieved passages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '3\. *Comparison check*—The system evaluates this generated answer against the
    retrieved passages and identifies a potential problem: the generated answer contains
    an element of “no clear evidence linking it to worsening dysphagia” that is not
    directly supported by the retrieved passages.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '4\. *Transfer decision*—Given the low match rate between the answer and the
    retrieved passages, the conversational AI determines that the answer may lack
    sufficient grounding and could be misleading. It then offers to transfer the user
    to a human agent for a more reliable answer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this scenario, the conversational system searched for relevant document passages
    and fed them to an LLM. The LLM generated an answer, and the conversational AI
    then compared the generated answer to the retrieved passages. This comparison
    includes detecting how many words and phrases in the generated answer appear in
    the passages. If the percentage is low, the conversational AI decides that the
    answer is not grounded in the documents. The conversational AI then gracefully
    acknowledges its inability to provide a suitable grounded response and offers
    an alternate resolution path.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, the search process may not have retrieved any documents. In that
    case, the conversational AI would not have to invoke the LLM to generate an answer,
    and it could directly deflect the question. For both scenarios, the conversational
    AI could instead return an “I don’t know” or other fallback responses. Both options
    reduce the chance of hallucinated and irrelevant answers.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Combining RAG with other generative AI use cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG may also be combined with other generative AI use cases. For instance, RAG
    can handle informational queries, while other generative AI models specialize
    in tasks like sentiment analysis or language translation. By using a combination
    of AI capabilities, conversational AI systems can offer users a comprehensive
    range of services, further enhancing efficiency and satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: RAG is only one of several generative AI patterns that enhance conversational
    AI. When users pose common questions or seek detailed information about a product
    or service, RAG draws upon the enterprise knowledge base to provide accurate and
    up-to-date answers. By grounding responses in the organization’s specific domain,
    RAG ensures that users receive relevant information tailored to their needs.
  prefs: []
  type: TYPE_NORMAL
- en: However, specific user inquiries may require more than informational responses,
    necessitating actionable steps (information-seeking versus transactional questions).
    In such cases, the conversational AI system executes transactions and guides users
    through specific tasks or processes. For instance, users may express an interest
    in purchasing after receiving information about a product or service from RAG.
    In response, the conversational AI can seamlessly transition to a transactional
    action, such as initiating a checkout process or scheduling a vaccination, facilitating
    a smooth and efficient user journey.
  prefs: []
  type: TYPE_NORMAL
- en: While RAG excels at efficiently and accurately responding to user queries, additional
    options, such as handing over to human agents or combining RAG with other generative
    AI use cases, can further optimize the user experience. These options ensure that
    users receive the support and assistance they need in the most efficient manner
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Note  For scenarios where RAG responses may fall short—such as providing real-time
    data or fulfilling specific customer requests—function calling can be integrated
    to retrieve information from external systems dynamically. This approach allows
    chatbots to identify relevant intents and parameters for real-time responses,
    extending RAG’s utility in complex interactions. While it is not covered in depth
    here, the function call is valuable if you are seeking a more dynamic conversational
    AI system.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.4 Comparing intents, search, and RAG approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 6.2 summarizes the capabilities and performance of three types of chatbots:
    intent-based chatbots, chatbots integrated with search, and RAG-integrated chatbots.
    Each chatbot type is evaluated based on the requirements and capabilities users
    and chatbot creators expect from conversational AI. You can discern the most suitable
    chatbot solution for your specific needs by comparing these attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.2 Comparing the capabilities of intent-based chatbots, chatbots integrated
    with search, and RAG-integrated chatbots
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Requirements | Intent-based chatbots | Chatbots integrated with search |
    RAG-integrated chatbots |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Flexibility in handling queries  | Limited to short-head predefined intents.
    May ignore nuance.  | Handle long-tail queries by returning links and snippets  |
    Handle long-tail queries by returning answers  |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy and relevance of responses  | When an intent is recognized with
    high confidence, the answers are accurate and precrafted.  | Provide contextually
    relevant and accurate documents that help the user find an answer  | Provide contextually
    relevant and accurate answers grounded in your documents  |'
  prefs: []
  type: TYPE_TB
- en: '| Adding new know-ledge to the bot  | Add or revise manually curated intent-response
    pairs  | Add or revise documents in your knowledge base  | Add or revise documents
    in your knowledge base  |'
  prefs: []
  type: TYPE_TB
- en: '| Maintenance and scalability  | Extensive regression testing when intent training
    data is changed  | Document repository needs to be maintained by adding new documents
    and removing stale documents  | Document repository needs to be maintained by
    adding new documents and removing stale documents  |'
  prefs: []
  type: TYPE_TB
- en: '| Response generation quality  | Predefined responses are presented.  | User
    must put together their own answer from retrieved passages and documents  | Answers
    are grounded in source documents but adapted to nuance from the question  |'
  prefs: []
  type: TYPE_TB
- en: While traditional chatbots help organizations automate simple tasks and provide
    essential customer support, integrating RAG techniques enhances their ability
    to deliver more accurate, context-aware responses, ultimately improving the user
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Consider your last chatbot implementation and consider the long-tail concept:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List three examples of niche or uncommon user queries that traditional intent-based
    chatbots may not adequately address.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss how these queries exemplify the long-tail phenomenon in conversational
    AI.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For the same chatbot implementation, consider what answers you can provide with
    traditional searches versus RAG.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 6.3 How is RAG implemented?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the “retrieval-augmented generation” name suggests, RAG has two phases:
    retrieval and generation. In the retrieval phase, algorithms search for and retrieve
    snippets of information relevant to the user’s prompt or question. In an open-domain
    consumer setting, those facts can come from indexed documents on the internet;
    in a closed-domain enterprise setting, a narrower set of private sources are typically
    used for added security and reliability.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 High-level implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With RAG, the system searches a knowledge base for information relevant to
    a question and uses that information to generate a conversational answer. Let’s
    break down the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The user asks the chatbot a question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The system uses its NLU capabilities to determine the intent of the user’s
    question:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If it recognizes the question with high confidence—for example, if it is one
    of the intents it was trained on—it will be able to respond, and a search will
    not be needed. This ends the flow.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If it cannot recognize the query, it will go to search. The system will send
    the user’s query to the search tool to search the document content and produce
    and rank search results.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It passes back the ranked search results to the chatbot for display. (Before
    RAG, the ranked link and snippet list would have been passed back to the chatbot—handling
    the long-tail questions with search results was still more helpful than providing
    a “sorry, I cannot understand” response.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of simply displaying the results, the original question and the search
    results are sent to an LLM. The LLM may rerank the search results, but most importantly,
    it generates a concise, summarized, linguistically correct answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The answer is then passed back to the system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The answer is presented to the user through the chatbot UI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note  Neither the original user question nor the generated answer needs to match
    the documents *exactly*. While verbatim responses can indeed occur and are sometimes
    even preferred for legal reasons, the primary focus is on grounding the content
    in the knowledge base, ensuring that the generated answer is rooted in the curated
    document set.
  prefs: []
  type: TYPE_NORMAL
- en: By their nature, LLMs do not generate consistent results each time a query is
    processed. These models can produce different responses to the same question depending
    on subtle variations in context or phrasing. This variability is due to the probabilistic
    nature of LLMs, which generate text based on learned patterns rather than retrieving
    fixed responses. While this flexibility allows for more nuanced and contextually
    appropriate answers, it can also lead to expectations of consistent outputs, which
    is not how these models function.
  prefs: []
  type: TYPE_NORMAL
- en: Emphasizing this point is essential, because team members unfamiliar with how
    LLMs work often expect consistent results. This expectation can hinder projects,
    leading to differing approaches to the problem among team members. Understanding
    that LLMs prioritize relevance and context over the exact replication of document
    content can help align expectations and improve collaboration within the team.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the generated response may closely resemble or even match the
    wording in the documents. This can occur when the documents contain relevant and
    informative passages directly addressing the user’s question. In such instances,
    the RAG model may include verbatim excerpts from the documents in the generated
    response to provide the user with the most accurate and relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, the RAG model uses the information within the knowledge base
    to understand the context and relevant concepts related to the user’s question.
    It then uses this understanding to generate a response that aligns with the content
    found in the documents, even if the specific wording of the user’s question or
    the generated answer does not exist verbatim within the documents. This approach
    allows for greater flexibility and adaptability in developing responses that effectively
    address user queries while drawing upon the information available in the curated
    document set.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Preparing your document repository for RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s also consider how the document content is retrieved during RAG searches.
    Figure 6.9 provides more detail about creating appropriate data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F09_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 RAG uses a vector database during build time and run time.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A systematic preprocessing pipeline ensures that both the raw data and the
    user’s question (or the LLM’s rephrasing of it) are optimized for use in RAG-based
    searches. This pipeline is crucial for transforming data into embeddings, enabling
    the model to match the user’s query with relevant information efficiently. Techniques
    such as cosine similarity or other methods are then applied to identify the best
    matches, ensuring accurate and contextually appropriate results. The following
    list outlines the key steps involved in this pipeline, detailing how data is processed
    before being used in retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Preprocessing data*—The system (typically a data pipeline, not the LLM itself)
    processes raw documents to make them searchable. For example, PDF documents are
    converted to text, or table structures are converted to processable statements.
    Metadata may be added to enhance the original content. The text is then divided
    into coherent semantic units, called *chunks*. For instance, a document may be
    chunked at paragraph boundaries. Chunking is a common process for identifying
    and extracting meaningful groups of words (“chunks”) from sentences for further
    analysis or processing. The chunking strategy impacts the overall results. There
    are open source tools that can help with visualizing and understanding different
    chunking or splitting strategies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Embedding generation*—An embedding model converts these chunks into embeddings
    or numerical representations of words or phrases in a high-dimensional vector
    space. Embeddings capture semantic relationships between words and documents,
    enabling a more efficient understanding of the connections. Similar meanings or
    contexts are mapped nearby in the vector space, and dissimilar meanings are mapped
    to more distant points. This provides more relevant search results than keyword
    matches.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Storage in a vector database*—The generated embeddings are stored in a vector
    database, which enables efficient similarity searches. Each document chunk is
    indexed using its vector representation, allowing fast retrieval based on meaning
    rather than exact word matches.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Retrieval and matching at runtime*—At run time, the end user interacts with
    the chatbot. Their question will be converted to a vector using the same embedding
    model, and that vector will be searched in the vector database to find the most
    relevant passages (chunks) based on semantic similarity. These retrieved passages
    are then passed to the LLM, which synthesizes them into a response presented to
    the user.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of these steps ensures that the retrieval process is optimized, making
    it possible for the LLM to generate accurate, context-aware responses based on
    the most relevant retrieved data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 6.1 shows sample code for an embedding function. You can use any custom
    embedding function or other vector databases, and the performance may differ depending
    on the embedding model used. This is the most common approach to RAG: you create
    a dense vector representation of the knowledge base to calculate the semantic
    similarity to the user queries. For this sample, we used Chroma as the vector
    database.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Splitting a file into chunks, embedding it, and storing it in a
    vector database
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: At its core, RAG operates by retrieving relevant documents or passages based
    on a user’s query and then generating a response using natural language generation
    techniques. This process can be achieved without explicit chunking or embedding
    by using other methods for document retrieval and language generation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, using Lucene as an alternative to chunking and embeddings involves
    using its document indexing and retrieval capabilities. Lucene can handle the
    retrieval part, fetching the most relevant documents based on the query. After
    retrieval, the generator part of the RAG can take over to produce coherent responses
    based on the content of the retrieved documents. Lucene is very efficient at text
    retrieval, which leaves the complex task of generating human-like responses to
    the more specialized generative components of the RAG model. This approach can
    be particularly advantageous in systems emphasizing retrieval accuracy and speed
    over nuanced understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Text chunking—In this exercise, you will experiment with different chunking
    strategies and embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose your sample text data (small text files of your choice).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Decide on a chunking strategy (splitting by sentences or words). For your experiments,
    try a chunking tool.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Embed the chunks using an open source embedding model, and then load the chunks
    into a vector database (Chroma).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Setting up an ingestion pipeline—This exercise guides you through building
    a simple ingestion pipeline for processing documents in a RAG system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a document set relevant to your organization’s domain or a specific use
    case for the chatbot. Start with simple, text-only documents, i.e., no tables,
    etc.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Build the ingestion pipeline, considering factors such as ease of use and compatibility
    with RAG. For querying, use open source models, Hugging Face embedding models,
    and a llama index.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the ingestion pipeline to preprocess and structure the dataset for
    use with RAG.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the ingestion pipeline with sample data to ensure proper functionality
    and data integrity.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.4 Additional considerations of RAG implementations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional search returns links, passages, or the full text of relevant documents,
    and the user needs to sift through this information to find their answer. RAG
    conversely returns the answer directly, and the user can optionally see the documents
    used.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1 Can’t we just use an LLM directly?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What if the conversational AI passed the user’s query to an LLM and got the
    answer? After all, LLMs are trained on vast amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: First, LLMs trained on internet-scale data have limitations due to the nature
    of their training data. This data represents a snapshot of the training time from
    publicly available sources—it does not contain business-specific, personal, or
    classified information, and it doesn’t contain public data created after the cutoff
    date. Thus, even the newest LLM’s knowledge can become outdated, leading to inaccurate
    responses over time. RAG addresses this by offering data to LLMs after they are
    trained.
  prefs: []
  type: TYPE_NORMAL
- en: Second, as LLMs are trained from extensive datasets, it is challenging to trace
    their responses to sources, undermining the reliability and trustworthiness of
    the model’s output. RAG is inherently grounded because you know exactly what data
    was provided to the LLM for a given question.
  prefs: []
  type: TYPE_NORMAL
- en: The broad domain LLMs cover poses another significant challenge. With access
    to vast information, they may generate responses with high confidence, even when
    lacking concrete evidence or context. This tendency to produce plausible but incorrect
    or unverified information is known as *hallucination*. In contrast, you want your
    conversational AI to provide correct and grounded answers. Advanced prompting
    techniques can help mitigate hallucinations, but providing source data through
    RAG is more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: You also want to prioritize answers grounded in the specific documents or corpus
    being indexed, not those on which LLM was trained. A RAG system’s primary focus
    is to provide responses based on the content and context of your documents, so
    answers are directly generated from the information within the corpus, promoting
    accuracy, relevance, and trustworthiness in the responses provided to users.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to consider the training data and domain of a specific LLM before
    selecting it for your use. If the LLM was trained on generic data and you need
    domain-specific results, it may not produce the desired outcomes. In such cases,
    you could explore techniques like model blending, where you combine multiple models
    to use the strengths of each, enhancing performance in specific domains. If you
    have the resources and data available, you may also consider fine-tuning the selected
    model to better suit your needs. However, this can require a significant budget
    for computational resources and data, so consider prompt-tuning first. While fine-tuning
    costs are decreasing and will continue to do so, they still need to be carefully
    considered. Other methods are also emerging for domain-specific training, offering
    further flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2 Keeping answers current and relevant with RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG represents a significant advantage over directly using LLMs for question
    answering. While RAG still uses LLM for natural language generation, LLM is crafting
    accurate responses from the searched documents. Real-time retrieval will find
    up-to-date information when new or updated documents are added during the build
    phase. RAG ensures the answer reflects the latest documents from the searched
    knowledge sources. This real-time integration of enterprise content enhances the
    relevance and accuracy of responses and instills confidence in users, who know
    that they are receiving current and reliable information.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, RAG goes beyond merely accessing enterprise content. It accesses
    specific passages and retrieves information from multiple documents. This granularity
    allows RAG to trace and verify answers to their exact sources, providing users
    with full transparency and trustworthiness. This facilitates accountability and
    the verification process for your development team too, ensuring you know what
    your bot is doing and why.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, RAG defines the domain of the LLM’s understanding, enabling it to
    recognize the limits of its knowledge and expertise. Unlike LLMs that are used
    directly and that may attempt to provide answers outside of their domain, RAG
    can acknowledge when it encounters queries beyond its scope. This ability to say
    “I don’t know” prevents it from giving inaccurate answers and fosters transparency
    in conversational interactions. By establishing clear boundaries for its understanding,
    RAG empowers developers to build AI systems prioritizing accuracy, reliability,
    and integrity, ultimately enhancing the overall user experience.
  prefs: []
  type: TYPE_NORMAL
- en: There is a difference between “I don’t understand” and “I cannot find an answer
    to your question.” While the primary goal of RAG is to generate informative and
    relevant responses based on the content of the retrieved documents, there are
    scenarios where the system may not find sufficient or appropriate information
    to generate a meaningful response. In such cases, it is common for the RAG model
    to acknowledge its inability to provide a satisfactory answer and communicate
    this to the user.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to note that a RAG system’s specific behavior, such
    as returning an “I don’t know” response, can be influenced by the retrieval component’s
    design, the knowledge base’s quality, and the generation model’s settings or parameters.
    Additionally, developers may choose to implement specific strategies or fallback
    mechanisms to handle cases where the system cannot generate a response, such as
    providing alternative suggestions or prompting the user for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.3 How easy is it to set up the ingestion pipeline?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Setting up an ingestion pipeline that effectively preserves document structure
    is critical for ensuring accurate search results within a retrieval system for
    RAG. Several key areas must be considered. Essentially, every architectural decision
    you make about the components will have an influence on the overall accuracy of
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: First, you must establish mechanisms to connect existing content stores to the
    retrieval system or migrate content into a new repository. This will allow the
    retrieval system to access the necessary data and maintain data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: The next challenge is correctly extracting structures (such as headings, tables,
    and lists) during ingestion. These formatting elements contribute to the document’s
    organization and clarity. By retaining this structural information during ingestion,
    the retrieval system can use it to enhance search accuracy and relevance.
  prefs: []
  type: TYPE_NORMAL
- en: There are also challenges related to chunking. The ability to chunk, split,
    or partition large documents into representative subdocuments for indexing enhances
    the retrieval process’s efficiency. This allows for more granular indexing and
    retrieval of information, facilitating quicker access to specific content within
    lengthy documents. Additionally, selecting appropriate search methodologies, such
    as vector, semantic, federated, keyword, or hybrid, further augments the retrieval
    system’s capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Using LangChain simplifies setting up the ingestion pipeline. Recall that you
    will need
  prefs: []
  type: TYPE_NORMAL
- en: '*Document loaders*—Load data from various formats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Document transformers*—Process and structure the data for efficient retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retrievers*—Fetch the most relevant document chunks during query time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Document loaders* facilitate the ingestion of diverse document formats. These
    loaders streamline the workflow, ensuring efficient processing and retrieval of
    pertinent context for LLMs to deliver precise responses. They load data from the
    source documents, treating each extracted piece as a document comprising textual
    content and associated metadata. LangChain provides built-in capabilities for
    handling various files: all files in a directory, PDF, CSV, JSON, HTML, markdown,
    txt, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you can load text from a web page, transcripts, or corporate documents.
    Document loaders provide a `load` method for loading data as documents from a
    configured source:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Text loader:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'CSV loader:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Look at LangChain’s documentation on customizing the CSV parsing and loading.
    For example, you may want to specify your delimiters, field names, etc. Similarly,
    LangChain provides a `DirectoryLoader` for all documents in a directory, an `UnstructuredHTMLLoader`
    to load HTML docs, and so on for the common types. It is essential to know the
    `AzureAIDocumentIntelligenceLoader`, which is useful for Microsoft Office-type
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft Document Loader:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Once you have loaded the documents, you need to look at the *document transformers*,
    which can split a long document into smaller chunks that the selected LLM can
    process. LLMs have a “context window” property, determining the text length they
    can effectively process in a single pass, so the chunks must fit into the LLM’s
    context window. It is easy to assume that setting a more extensive context (i.e.,
    longer text passages) would inherently lead to better performance across various
    language understanding tasks. However, recent studies have revealed that this
    isn’t always the case. Evidence suggests that language models can achieve improved
    performance when presented with less text overall, but text that is highly relevant
    to the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: A larger context window allows for including more information in the prompt
    during inference, but this technique, often called *prompt stuffing*, comes with
    trade-offs. Processing more text demands greater computational resources, which
    slows inference and increases costs—particularly for companies paying by the token,
    where summarizing lengthy documents, like annual reports or meeting transcripts,
    can become costly. While larger context windows can improve results to some extent,
    there are diminishing returns. Like humans, LLMs can experience information overload;
    when presented with excessive detail, they may overlook critical points. Studies
    have shown that LLMs are more likely to focus on essential information at the
    beginning or end of a prompt, potentially missing key insights buried in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: We need document transformers to preprocess the documents, extract relevant
    information, and transform it into a structured representation that the language
    model can efficiently use during generation. LangChain has several built-in transformers
    that make document manipulation easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The splitting process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the text into smaller chunks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine these smaller chunks into larger chunks of a certain size, usually measurable
    by some function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once it reaches that size, it becomes the new unit of the text. Then, you create
    a new text segment with some overlap to maintain context between the fragments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can choose your division rules (characters, words, tokens) and how to measure
    the chunk size. Again, LangChain offers many different types of splitters in the
    langchain-text-splitters package. These are some examples of text splitters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Recursive*—Splitting text recursively is the recommended way to start. It
    aims to keep related pieces of text next to each other.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*HTML*—A “structure-aware” chunker splits text based on HTML-specific characters;
    an example is shown in listing 6.2\. It splits at the element level, adding metadata
    to headers for chunk relevance. It preserves semantic grouping and context-rich
    information in document structures:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Character*—It breaks the document at user-defined characters (e.g., "\n\n").'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Code*—It employs code syntax and grammar identifiers for languages like Python
    and JavaScript (and 13 others), organizing code into logical groups.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Markdown*—It identifies markdown language and organizes the document into
    a structured format (similar to HTML).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tokens*—It uses a tokenizer, like tiktoken, to split text based on model-defined
    token limits in the code.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 6.2 HTML splitter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to deal with the embeddings. The type of data and the language
    support requirements govern the selection of embedding models. Furthermore, when
    you are dealing with specific domain or industry terms, these models may have
    to be extended.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding models in LangChain transform the text into numerical representations,
    or embeddings, that can be processed. LangChain integrates with different model
    providers (OpenAI, Cohere, Hugging Face, and more) to generate embeddings. The
    `OpenAIEmbeddings` class, for instance, uses the OpenAI API to create embeddings,
    and this can be done using either OpenAI’s API key or Azure’s OpenAI API key.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Other integrations include `CohereEmbeddings`, `TensorFlowEmbeddings`, and `HuggingFaceInferenceEmbeddings`.
  prefs: []
  type: TYPE_NORMAL
- en: After you have the embeddings, you must store them in a vector database, such
    as Chroma, which we used earlier. When selecting the vector database, you’ll want
    to consider run-time performance, how it scales for the size of your data set,
    and overall performance. Another important consideration is integrating tools
    like LangChain, which is continually improving. LangChain enhances the capabilities
    of vector databases by providing streamlined processes for handling embeddings
    and integrating with various machine learning and AI workflows. This combination
    ensures efficient data management and retrieval, making it a robust choice for
    scalable and high-performance applications.
  prefs: []
  type: TYPE_NORMAL
- en: '*Retrievers* bridge the gap between embeddings and user queries. While embeddings
    store numerical representations of documents in a vector database, retrievers
    identify and fetch the most relevant chunks based on similarity scoring.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The retriever works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The user’s query is embedded using the same embedding model used during ingestion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The vector database searches for the most semantically similar embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The retriever fetches the top matches and passes them to the LLM for response
    generation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LangChain includes multiple retrieval methods. For example, there is a similarity-based
    retriever in LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Retrievers play a crucial role in returning only the most relevant document
    chunks, ensuring the LLM works with focused, high-quality context rather than
    raw, unprocessed data.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.4 Handling latency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A universal best practice for handling latency has yet to be developed. Long
    response times are frustrating for users, but these techniques can enhance their
    experience:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Use a quality vector store with efficient search.* The Facebook AI Similarity
    Search (FAISS) library allows you to search for similar embeddings quickly. There
    are many purpose-built vector databases, like Chroma, Milvus, Pinecone, and Weaviate,
    with many more emerging. Traditional databases and search systems like Elasticsearch
    provide vector search plugins. Each has unique strengths and can be selected based
    on your needs, including scalability, functionality, performance, and cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Preprocess and curate your dataset.* Having multiple similar versions of the
    same document increases search time and lowers search result quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inform the user before executing a slow action.* An appropriate-toned message,
    such as “Just need a moment,” may placate the user and bridge the delay.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stream responses to show the user the answer as each token is generated.*
    LLMs may take 1.5 to 5 seconds or more to generate an answer, and searches may
    take 5 to 10 seconds. The user may think the chatbot is broken if the conversational
    AI waits for the LLM to finish.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Consider caching.* By caching each user’s vector database and chat history,
    commonly accessed information relevant to that user’s interactions can be stored
    locally. This reduces the need to generate responses from scratch every time,
    saving the computational resources required. While caching may consume additional
    tokens, the trade-off is improved efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.4.5 When to use a fallback mechanism and when to search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Determining whether to use RAG’s response or to deflect to a human agent involves
    several key considerations. For instance, the generation part of RAG should not
    be invoked if the retrieval does not yield appropriate results. In this case,
    the conversational AI can gracefully exit the query and respond with the offer
    to pass the user to a human agent. By bypassing the generation of an answer based
    on potentially subpar search results, you effectively reduce latency for end users,
    ensuring they receive prompt responses while saving computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.10 illustrates the decision-making process for determining whether
    to use the RAG response or to hand over the query to a human agent:'
  prefs: []
  type: TYPE_NORMAL
- en: The conversational AI processes the user query and passes it to the retrieval
    system, which searches the knowledge base to find relevant information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The retrieval results are evaluated to determine if they are appropriate for
    generating a response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the retrieval results are deemed appropriate, the system generates a response
    using the full-on RAG approach.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F10_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 When the answer from RAG does not match the retrieved passages,
    it can be better to offer a human agent instead.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4\. If the retrieval results are inappropriate (e.g., insufficient or no results),
    the system gracefully offers to pass the query to a human agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5\. The response (either generated by the system or passed to a human agent)
    is returned to the user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 6.5 Evaluating and analyzing RAG performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluating the capabilities of a RAG model within a conversational AI system
    is multifaceted. Each capability must be evaluated for an overall result. Most
    evaluations consider three aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Was it the right response?* Did the answer directly address the user’s question?
    For example, if the user asked about resetting a password, the response should
    clearly explain the steps rather than discussing broader account security topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Was it in the right context for this user?* Did the response consider the
    user’s specific situation or history? For example, if a user previously reported
    an account problem, the system should provide a tailored follow-up solution instead
    of generic advice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Was it grounded in the documents (or hallucinated or made up by the generation
    process)?* Did the response accurately reflect the information retrieved from
    the source documents without fabricating details? For example, a response should
    correctly reference a company’s policy document when explaining return procedures
    instead of creating nonexistent policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An LLM can score responses based on the key criteria to enhance the evaluation
    process. This approach works best when combined with human review: the LLM provides
    an initial assessment, and human evaluators then verify the accuracy and contextual
    relevance of the responses.'
  prefs: []
  type: TYPE_NORMAL
- en: These evaluation criteria help determine the truthfulness of the chatbot’s responses.
    The generated answers should also be accurate if the source documents are accurate
    and RAG retrieved the correct documents. The evaluation of the responses can be
    broken down into assessing the different components of RAG, which can be individually
    evaluated for overall performance, including the quality of the document index,
    the effectiveness of the retrieval process, and the accuracy of the answer generation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Indexing metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Indexing metrics provide insights into how efficiently a system can organize,
    store, and retrieve vast amounts of data. Key considerations include indexing
    speed, storage requirements, scalability, and how well the system handles high-dimensional
    data like vectors. Table 6.3 summarizes these important aspects, offering a quick
    overview and relevant examples.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.3 Critical metrics that influence the efficiency and accuracy of a RAG
    system’s document index
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Aspect | Summary | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Indexing metrics  | Evaluates speed, storage needs, and scalability. Critical
    for large-scale data systems.  | Indexing for a news aggregator where speed and
    scale are crucial  |'
  prefs: []
  type: TYPE_TB
- en: '| Vector database performance  | Measures performance in handling high-dimensional
    data  | For technical support, accurate troubleshooting steps must be assembled
    for multiple documents. E.g., “Why is my device overheating?”  |'
  prefs: []
  type: TYPE_TB
- en: '| Recall rate  | Indicates accuracy in retrieving relevant data. High recall
    is vital for complete retrieval.  | In legal document retrieval, high recall ensures
    all relevant cases are found.  |'
  prefs: []
  type: TYPE_TB
- en: '| Query complexity  | Affects performance based on query specifics, dimensionality,
    and dataset diversity  | Financial databases handling complex queries across multiple
    data points  |'
  prefs: []
  type: TYPE_TB
- en: '| Benchmarking tools  | Tools like ANN-Benchmark compare algorithms on metrics
    like recall versus QPS.  | Evaluating which algorithm best balances speed and
    accuracy for a video search engine  |'
  prefs: []
  type: TYPE_TB
- en: The first critical component to assess is the indexing metrics, which involve
    evaluating the efficiency and effectiveness of organizing and accessing data in
    a system’s knowledge base. This includes examining factors such as indexing speed,
    storage requirements, and the scalability of the indexing process. Efficient indexing
    is crucial for a RAG system, as it impacts the speed and accuracy of information
    retrieval. Ineffective indexing can result in slow response times and inaccurate
    data retrieval, compromising response quality.
  prefs: []
  type: TYPE_NORMAL
- en: Vector database performance is another vital metric specializing in storing
    and retrieving high-dimensional vectors representing complex data, such as text,
    images, or embeddings. These databases perform approximate rather than exact match
    searches, necessitating performance evaluation beyond traditional database performance
    measures like queries per second (QPS) and latency. While these metrics are important
    for evaluating system speed and responsiveness, they do not directly capture the
    accuracy of retrieval results. Therefore, besides QPS and latency, the *recall
    rate* is another essential performance metric for vector databases. If the vector
    database performs well, the RAG model can access high-quality, relevant information,
    leading to more accurate and contextually appropriate generated content. Conversely,
    poor performance can result in slow retrieval times and irrelevant or less useful
    data being used for generation.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a customer support scenario where a chatbot powered by a RAG system
    is utilized to handle inquiries. With a high recall rate, the chatbot accesses
    a broad range of information from the knowledge base, effectively resolving customer
    queries and enhancing satisfaction. However, a low recall rate can lead to missed
    crucial information, resulting in inadequate responses and increased customer
    frustration. Therefore, the chatbot’s effectiveness significantly depends on its
    ability to comprehensively retrieve relevant information, underscoring the importance
    of a high recall rate in such automated support systems.
  prefs: []
  type: TYPE_NORMAL
- en: Query complexity, influenced by factors like the dimensionality and specificity
    of the query, as well as the data diversity, also affects vector database performance.
    Higher-dimensional queries require more computational resources because distance
    calculations between vectors become more complex. This can lead to increased time
    and memory usage for retrieval tasks. More specific queries might target very
    narrow segments of the vector space, which can challenge the indexing system to
    efficiently isolate and retrieve the relevant vectors, especially in large datasets.
    High query complexity can strain the system, potentially leading to slower retrieval
    times and less relevant data being returned. Also, complex queries make distinguishing
    between relevant and irrelevant results difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a chatbot on an e-commerce platform designed to help customers find
    products using complex queries involving multiple attributes like brand, color,
    size, and user ratings. For example, a customer might ask the chatbot for “6.5-sized
    blue Adidas running shoes with a minimum of a 4-star rating.” This query presents
    a multifaceted challenge due to its specificity across several dimensions. Each
    of these attributes represents a different vector in the database.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking tools like ANN-Benchmarks and VectorDBBench help evaluate these
    aspects by comparing different algorithms and configurations, ensuring the RAG
    system is built on a robust retrieval foundation for consistently high-quality
    content generation. ANN-Benchmarks plots the recall rate on the *x*-axis against
    QPS on the *y*-axis, illustrating each algorithm’s performance at different retrieval
    accuracy levels. VectorDBBench displays QPS and recall rates separately.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2 Retrieval metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next capability is retrieval metrics, which gauge the system’s ability to
    fetch relevant information from indexed data. Key aspects include retrieval accuracy,
    precision, recall, and response time. Effective retrieval metrics ensure users
    receive accurate and relevant responses, boosting satisfaction and trust in the
    conversational AI system. Table 6.4 summarizes these important aspects, followed
    by more detailed explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.4 Critical aspects influencing retrieval metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Aspect | Summary | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval accuracy  | Evaluates the system’s ability to retrieve relevant
    information from indexed data  | Ensuring a chatbot retrieves accurate troubleshooting
    guides from a large dataset  |'
  prefs: []
  type: TYPE_TB
- en: '| Precision and recall  | Precision measures relevance of retrieved docs; recall
    measures how many relevant docs are retrieved.  | Balancing precision and recall
    when retrieving product recommendations in an e-commerce chatbot  |'
  prefs: []
  type: TYPE_TB
- en: '| Context precision and context recall  | Specific to RAG: context precision
    checks relevance; context recall checks coverage of relevant info  | Evaluating
    how well a generated response in a support chatbot matches the query’s context  |'
  prefs: []
  type: TYPE_TB
- en: '| Parameter optimization  | Tuning search parameters and algorithms to improve
    speed, accuracy, and relevance of results  | Adjusting FAISS clusters or Elasticsearch
    settings to improve document retrieval for legal databases  |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding models  | The use of different embeddings impacts the retrieval
    quality by enhancing precision or recall.  | Using BERT for precise context understanding
    in a legal advice chatbot  |'
  prefs: []
  type: TYPE_TB
- en: '| Filtering and reranking  | Strategies to remove noise and rerank results
    to improve both relevance and accuracy  | Filtering out irrelevant articles in
    a news aggregation chatbot and then reranking top results  |'
  prefs: []
  type: TYPE_TB
- en: '| Normalized Discounted Cumulative Gain (NDCG)  | Assesses the ranking quality
    by considering relevance and position of retrieved documents  | Ensuring the most
    relevant help articles appear at the top in a technical support chatbot  |'
  prefs: []
  type: TYPE_TB
- en: These metrics evaluate search quality, document relevance, and how well user
    queries align with responses. *Retrieval accuracy* measures the system’s ability
    to fetch the most relevant information from its indexed data. If retrieval accuracy
    is low, the chatbot may return responses that are only loosely related to the
    user’s query or fail to retrieve critical details.
  prefs: []
  type: TYPE_NORMAL
- en: Precision measures the proportion of relevant retrieved documents. Recall measures
    how many relevant documents are retrieved by the system. A high recall value indicates
    the system retrieves many relevant documents from the database. In contrast, a
    high precision value indicates that the retrieved documents are mostly relevant
    to the user’s query. Balancing recall and precision is crucial to ensure comprehensive
    coverage of relevant information and minimize irrelevant results.
  prefs: []
  type: TYPE_NORMAL
- en: Specific to RAG models, context precision and context recall evaluate the alignment
    and coverage of generated responses relative to the user’s query. Context precision
    measures how precisely the retrieved context matches the user’s query, indicating
    the relevance and accuracy of the generated response. Context recall measures
    how comprehensively the generated response covers relevant information from the
    retrieved context.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should implement various strategies to enhance the retrieval process:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing search parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using different embedding models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing filtering and reranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first strategy is to adjust retrieval parameters to improve both speed and
    accuracy. Adjusting the search parameters, such as the number of clusters in FAISS
    or the search query complexity in Elasticsearch, can significantly enhance the
    precision and recall of retrieved documents. This ensures that the system returns
    the most relevant documents, increasing precision and retrieving all pertinent
    documents, boosting recall.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter optimization can also reduce response time, making the system more
    responsive. Tuning the indexing and query algorithms can lead to faster retrieval
    times without compromising accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The second strategy is to utilize different embedding models to find the most
    effective combination for retrieving relevant documents. Embeddings can drastically
    affect retrieval quality. For example, BERT and Sentence-BERT can improve the
    system’s understanding of the context and semantics of user queries, enhancing
    precision. GPT-3 embeddings can offer a broader contextual grasp, improving recall
    by retrieving more relevant documents from diverse contexts.
  prefs: []
  type: TYPE_NORMAL
- en: The third strategy is to implement filtering and reranking. Filtering and reranking
    strategies further enhance precision by removing irrelevant documents and boost
    recall by prioritizing relevant documents. Techniques like domain-specific filters
    or context-aware reranking can significantly refine the relevance of top results.
  prefs: []
  type: TYPE_NORMAL
- en: Normalized Discounted Cumulative Gain (NDCG) is useful when the order of retrieved
    documents matters. It measures the effectiveness of ranking algorithms by evaluating
    the relevance and position of documents in the search results, providing a comprehensive
    ranking quality assessment.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.3 Generation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last area for evaluation is the generation itself: whether the generated
    output provides a relevant and complete answer. Table 6.5 offers a concise overview
    of these key metrics, strategies for enhancing generation quality, and practical
    examples of their application, followed by further details.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.5 Critical aspects influencing generation metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Aspect | Summary | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Faithfulness  | Evaluates the factual accuracy of generated output based
    on retrieved context  | Ensuring chatbot answers are factually correct when responding
    to questions. E.g., “What are the consequences of breaching a contract?”  |'
  prefs: []
  type: TYPE_TB
- en: '| Answer relevancy  | Assesses how relevant the generated answer is to the
    specific user question  | “What is the status of my order?” The chatbot, after
    retrieving the relevant data, responds with, “Your order #12345 is currently in
    transit and expected to be delivered by August 15th.” This response is directly
    relevant to the user’s question, providing specific information about the order
    status without unnecessary details.  |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuning  | Improves generation by aligning the LLM with domain- specific
    data, enhancing accuracy and relevance  | Fine-tuning an LLM for legal advice,
    ensuring generated responses are accurate and legally relevant  |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt engineering  | Crafting prompts to guide the LLM in generating more
    contextually appropriate and relevant responses  | Using prompt engineering to
    ensure a healthcare chatbot provides clear, relevant medical suggestions  |'
  prefs: []
  type: TYPE_TB
- en: '| Model blending  | Combining specialized models to enhance the quality of
    generation, balancing accuracy and fluency  | Blending a retrieval-focused model
    with a language-focused model to generate accurate, fluent responses  |'
  prefs: []
  type: TYPE_TB
- en: '| Sensibleness and Specificity Average (SSA)  | Measures response quality in
    open-domain chatbots, ensuring responses are sensible and specific  | Assessing
    open-domain chatbot responses to ensure they make sense and are not overly vague  |'
  prefs: []
  type: TYPE_TB
- en: '| Faithfulness- Evaluator  | Assesses whether the generated response avoids
    hallucinations by aligning with retrieved context  | Using FaithfulnessEvaluator
    to ensure that a finance chatbot’s responses are grounded in retrieved documents  |'
  prefs: []
  type: TYPE_TB
- en: Two primary metrics for assessing LLM performance are faithfulness and answer
    relevancy. Faithfulness evaluates the factual accuracy of the answer based on
    the retrieved context, while answer relevancy assesses how pertinent the answer
    is to the given question. An answer may be factually accurate (faithful) but not
    well-matched to the question (less relevant), or it may be accurate but not based
    on sourced documents.
  prefs: []
  type: TYPE_NORMAL
- en: Several strategies can be employed to enhance generation metrics. Fine-tuning
    the LLM on domain-specific data can improve faithfulness and relevancy by aligning
    the model more closely with the context in which it will operate. This ensures
    that generated responses are accurate and pertinent to the specific domain. You
    may also want to incorporate prompt engineering. The generation process becomes
    more focused and aligned with the user’s intent by explicitly including necessary
    context or constraints within the prompt. Model blending may also be used. Combining
    multiple models, each specialized in different aspects of the task, can help enhance
    generation quality. For example, one model may excel at retrieving accurate information,
    while another might be better at generating fluent and contextually appropriate
    language. Blending these models can lead to more balanced and effective output.
  prefs: []
  type: TYPE_NORMAL
- en: Sensibleness and Specificity Average (SSA) metrics evaluate response quality
    in open-domain chatbots. Sensibleness ensures that responses make contextual sense,
    while the specificity average ensures comprehensiveness without vagueness. Historically,
    human interactions were necessary for assigning these ratings.
  prefs: []
  type: TYPE_NORMAL
- en: While avoiding vague responses is essential, preventing LLMs from hallucinating
    is equally critical. LlamaIndex established a FaithfulnessEvaluator metric to
    measure hallucination by assessing whether the response aligns with the retrieved
    context. LlamaIndex was developed to address the challenge of connecting LLMs
    with various data sources, so LLMs can access, query, and generate insights from
    structured and unstructured data. It is a framework for building context-augmented
    generative AI applications. It offers capabilities for data integration, indexing,
    enrichment query processing, and more. Stay current with the latest updates from
    LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: RAG-integrated conversational AI efficiently addresses rare or complex queries.
    Nevertheless, realizing these advantages from RAG necessitates ongoing monitoring
    of all components, particularly where failures are common, such as during retrieval
    and generation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.4 Comparing efficiency of indexing and embedding solutions for RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we introduced the contributing factors to the efficiency
    of RAG systems. The indexing and embedding components are crucial for effectively
    retrieving relevant documents and ensuring the system’s responsiveness. When implementing
    RAG in a conversational AI system, it is important to measure its impact on retrieval
    accuracy and response relevance. Benchmarking is essential to creating an effective
    RAG system. You must have a method to evaluate whether changes in the system prompt
    improve user-query hit rates. Is the improvement 1%, 2%, or more? This is fundamental
    to knowing if your RAG system is truly effective.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, without proper monitoring, validation, and evaluation, it will
    be difficult to prove the effectiveness of your system. RAG is inherently complex,
    and typical implementations have 50% to 60% accuracy at first. You’ll want to
    increase the accuracy to above 80% for a practical solution.
  prefs: []
  type: TYPE_NORMAL
- en: PharmaBot, initially developed as a COVID-19 chatbot to handle general inquiries,
    such as vaccine information and appointment scheduling, is now set to be enhanced
    with RAG. The goal is to extend PharmaBot’s capabilities to answer more nuanced
    questions, such as “Can I take ibuprofen with my blood pressure medication?” and
    “My arms are sore after getting the vaccine. What should I do?” First, we’ll want
    to select a dataset of medical articles, research papers, and guidelines from
    health organizations, all focusing on various health problems and drug and vaccine
    interactions. Next, we’ll compile a set of representative queries like those previously
    listed, which are what users might ask.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll select the indexing solutions and the embeddings we want to use. For indexing,
    we’ll consider several options, such as FAISS and Elasticsearch. The most popular
    source for the latest performance benchmarks of text embedding models is the MTEB
    leaderboards hosted by Hugging Face. While MTEB provides a valuable starting point,
    the displayed results are self-reported, and many models may not perform as accurately
    when applied to real-world data. BERT, Sentence-BERT, or GPT-3 embeddings are
    worth considering, as they have been used in many solutions. Then we’ll run our
    selections to generate embeddings for our content and index our embeddings using
    the selected methods. Finally, we’ll run our queries and measure our performance.
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating PharmaBot enhanced with RAG, we can use table 6.6 to determine
    relevant metrics, establish baselines and goals for these metrics to assess improvements
    introduced by different combinations of indexing (FAISS, Elasticsearch) and embedding
    (BERT, Sentence-BERT, GPT-3) solutions, and compare solutions. For example, we
    could compare how the combinations of RAG components perform against business
    objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.6 Prioritized metrics for RAG evaluation based on business objectives
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Business objective | Prioritized metrics | Why? | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Customer satisfaction  | Response accuracy Relevance'
  prefs: []
  type: TYPE_NORMAL
- en: '| Directly impacts user experience and satisfaction  | PharmaBot providing
    accurate answers to users’ queries  |'
  prefs: []
  type: TYPE_TB
- en: '| Operational efficiency  | Latency Throughput'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ensures the system can handle high query volumes quickly  | A customer support
    chatbot for a large e-commerce platform should prioritize low latency to provide
    quick responses during peak shopping times.  |'
  prefs: []
  type: TYPE_TB
- en: '| Scalability  | Queries processed per second Resource utilization'
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluates system performance under increasing loads  | Chatbot for a healthcare
    provider managing seasonal spikes in appointments and queries  |'
  prefs: []
  type: TYPE_TB
- en: '| Cost-effectiveness  | CPU usage Memory usage'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ensures high performance without excessive resource consumption  | Chatbot
    for a non-profit organization providing 24/7 mental health support on limited
    funding  |'
  prefs: []
  type: TYPE_TB
- en: You could create a comparison table like the one in table 6.7\. These sample
    numbers are hypothetical and should be adjusted based on actual selected components
    and benchmarking results. For example, one configuration may be selecting Elasticsearch
    and then using three different embedding models to arrive at your numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.7 Evaluation of PharmaBot with various configurations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Metrics |  | Configuration 1 | Configuration 2 | Configuration 3 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Response accuracy  | Recall  | 0.85  | 0.87  | 0.88  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Precision  | 0.75  | 0.77  | 0.78  |'
  prefs: []
  type: TYPE_TB
- en: '|  | F1 score  | 0.8  | 0.82  | 0.83  |'
  prefs: []
  type: TYPE_TB
- en: '| Relevance  | Mean reciprocal rank (MMR)  | 0.70  | 0.72  | 0.74  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Average precision  | 0.65  | 0.68  | 0.70  |'
  prefs: []
  type: TYPE_TB
- en: '| Latency  | Average latency (ms)  | 50  | 55  | 60  |'
  prefs: []
  type: TYPE_TB
- en: '| Throughput  | Queries/second  | 20  | 18  | 16  |'
  prefs: []
  type: TYPE_TB
- en: '| Resource utilization  | CPU usage (%)  | 70  | 65  | 90  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Memory usage  | 8  | 7  | 12  |'
  prefs: []
  type: TYPE_TB
- en: When trying to satisfy the overall business requirements for PharmaBot using
    RAG, various trade-offs must be considered based on the evaluation metrics. An
    indexing component combined with selected embedding components offers high response
    accuracy and relevance with lower latency and higher throughput, making them suitable
    for systems requiring quick and accurate responses. However, these combinations
    exhibit moderate to high resource utilization, which may increase operational
    costs. Conversely, integrating another embedding component with various indexing
    strategies provides superior response accuracy and relevance but at the cost of
    significantly higher latency and lower throughput due to the computational demands
    of the selected embeddings. This can impact the system’s ability to handle high
    query volumes efficiently. The high CPU and memory usage may also strain resources,
    increasing operational costs. Ultimately, selecting the optimal combination requires
    balancing the need for high accuracy and relevance with the system’s capacity
    to handle queries efficiently while managing resource utilization to control costs.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the ongoing re-evaluation and refinement of the RAG system should
    be considered. Table 6.7 is not comprehensive, but the key point is to decide
    on key evaluation metrics and then use a structured evaluation approach. Systematic
    testing and focusing on the RAG component provide a robust RAG evaluation pipeline.
    The overall goal is to see an upward trend at the end, as illustrated in figure
    6.11\. Systematically applying the strategies discussed throughout this chapter
    and then analyzing the results reveal the impact of different configurations on
    RAG performance. Some tweaks show significant improvements, emphasizing the importance
    of experimentation and tuning. There is no best approach; exploring multiple directions
    when tuning your RAG systems is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F11_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 Different configurations and enhancements improve the accuracy of
    the RAG system.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Additionally, you may want to use RAG evaluation frameworks, from proprietary
    paid solutions to open source tools. Selecting the right solution requires balancing
    considerations around ease of maintenance and operational burden, plus how well
    the metrics observed by the tool map to your RAG pipeline and your business objectives.
    The following solutions are current examples, but more are being developed, providing
    even more options:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Arize*—A model monitoring platform focusing on precision, recall, and F1 score.
    It is beneficial in scenarios requiring ongoing performance tracking, ensuring
    RAG systems consistently meet accuracy thresholds in real-time applications. Arize
    is a proprietary paid offering that provides robust support and continuous updates
    for enterprise deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*RAGAS*—An open-source tool that offers streamlined, reference-free evaluation
    focusing on average precision (AP) and custom metrics like faithfulness. It assesses
    how well the generated content aligns with provided contexts, and it is suitable
    for initial assessments or when reference data is scarce.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Assess the relevance of responses generated by a RAG model within a conversational
    AI system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define evaluation criteria to measure the relevance of responses generated by
    the RAG model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish a scoring system to quantify the relevance of responses based on factors
    such as semantic similarity and informativeness.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Devise a set of user queries you will evaluate.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a set of expected responses (manually).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare the generated responses with the previously created responses to determine
    the level of relevance.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate evaluation metrics such as precision, recall, and F1 score to quantitatively
    assess the performance of the RAG model in generating relevant responses.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the evaluation results to identify patterns or areas where the RAG model
    excels or fails to generate relevant responses.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss potential factors influencing response relevance and strategies for
    improving the RAG model’s performance in this aspect.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evaluate document grounding with RAG:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate responses to user queries using the RAG model, and identify the source
    documents or passages from which the responses are derived.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess the degree of grounding by comparing the relevance of the source documents
    or passages to the corresponding user queries.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop a scoring mechanism to quantify the RAG model’s grounding effectiveness
    based on factors such as document relevance and coverage.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional intent-based chatbots can be greatly enhanced by integrating search
    functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intents are great for answering common short-head questions, and search is great
    for long-tail questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional search returns links or document passages instead of an answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG extends search capability by generating an answer from the documents retrieved
    by the search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using RAG, chatbots can provide contextually appropriate responses in real
    time, reducing user frustration and enhancing the conversational experience. Grounding
    answers in the organization’s domain also solves intent maintenance and enhancement
    for developers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG implementations must consider several problems, from handling latency to
    providing fallback mechanisms or handover to human agents to prevent hallucinations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation of RAG must consider the different components of indexing, retrieval,
    and generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
