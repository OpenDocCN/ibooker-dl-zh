["```py\ndataset = PygNodePropPredDataset(name='ogbn-products',\\\n root=root)   #1\ndata = dataset[0]   #2\n\nsubset_indices = torch.arange(0, 10000)   #3\n\nsubset_edge_index, edge_attr, edge_mask = \\\nsubgraph(subset_indices, data.edge_index, \\\nNone, relabel_nodes=True, num_nodes=\\\ndata.num_nodes, return_edge_mask=True)   #4\n\nsubset_features = data.x[subset_indices]   #5\nsubset_labels = data.y[subset_indices]   #6\n\nsubset_graph = data.__class__()   #7\nsubset_graph.edge_index = subset_edge_index   #8\nsubset_graph.x = subset_features   #9\nsubset_graph.y = subset_labels   #10\n```", "```py\nclass GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n    super(GCN, self).__init__()\n    self.conv1 = GCNConv(in_channels, hidden_channels)  #1\n    self.conv2 = GCNConv(hidden_channels, out_channels)\n\n    def forward(self, x, edge_index, \\\nreturn_embeds=False):  #2\n        x = self.conv1(x, edge_index)\n        x = torch.relu(x)  #3\n        x = self.conv2(x, edge_index)\n        if return_embeds:  #4\n             return x\n\n    return torch.log_softmax(x, dim=1)  #5\n```", "```py\nclass GraphSAGE(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n    super(GraphSAGE, self).__init__()\n    self.conv1 = SAGEConv(in_channels, \\\n    hidden_channels)  #1\n    self.conv2 = SAGEConv(hidden_channels, out_channels)\n\n    def forward(self, x, edge_index, \\\n    return_embeds=False):  #2\n        x = self.conv1(x, edge_index)\n        x = torch.relu(x)  #3\n        x = self.conv2(x, edge_index)\n        if return_embeds:  #4\n            return x\n\n     return torch.log_softmax(x, dim=1)  #5\n```", "```py\ngcn_model = GCN(in_channels=dataset.num_features,\\\n hidden_channels=64, out_channels=\\\ndataset.num_classes)  #1\ngraphsage_model = GraphSAGE(in_channels=d\\\nataset.num_features, hidden_channels=64, \\\nout_channels=dataset.num_classes)  #2\n\noptimizer_gcn = torch.optim.Adam\\\n(gcn_model.parameters(), lr=0.01)   #3\noptimizer_sage = torch.optim.Adam(\\\ngraphsage_model.parameters(), lr=0.01)   #4\ncriterion = torch.nn.CrossEntropyLoss()   #5\n\ndef train(model, optimizer, data):  #6\n    model.train()  \n    optimizer.zero_grad() \n    out = model(data.x, data.edge_index)  \n    loss = criterion(out[data.train_mask], data.y[data.train_mask].squeeze()) \n    loss.backward()  \n    optimizer.step()  \n    return loss.item()  \n\ndef validate(model, data):  #7\n    model.eval()  \n    with torch.no_grad():  \n        out = model(data.x, data.edge_index)  \n        val_loss = criterion(out[data.val_mask], data.y[data.val_mask].squeeze())  \n    return val_loss.item()  \n\ntrain_loss_gcn = []   #8\nval_loss_gcn = []  \ntrain_loss_sage = []  \nval_loss_sage = []  \n\nfor epoch in range(200):  #9\n    loss_gcn = train(gcn_model, optimizer_gcn, subset_graph)  \n    train_loss_gcn.append(loss_gcn)  \n    val_loss_gcn.append(validate(gcn_model, subset_graph))  \n\n    loss_sage = train(graphsage_model, optimizer_sage, subset_graph)  \n    train_loss_sage.append(loss_sage)  \n    val_loss_sage.append(validate(graphsage_model, subset_graph))  \n\n    if epoch % 10 == 0:  \n        print(f'Epoch {epoch}, GCN Loss: \\\n{loss_gcn:.4f}, GraphSAGE Loss: \\\n{loss_sage:.4f}, GCN Val Loss: \\\n{val_loss_gcn[-1]:.4f}, GraphSAGE \\\nVal Loss: {val_loss_sage[-1]:.4f}')\n```", "```py\ngcn_model.eval()\n\nwith torch.no_grad():\n     gcn_embeddings = gcn_model(subset_graph.x, \\\nsubset_graph.edge_index, return_embeds=True)\n```", "```py\ngcn_similarity_matrix = cosine_similarity(gcn_embeddings.cpu().numpy())\n```", "```py\nproduct_idx = 123 \ntop_k = 6\ntop_k_similar_indices_gcn = np.argsort(-\ngcn_similarity_matrix[product_idx])[:top_k]\n```", "```py\nclass GraphSAGE(torch.nn.Module):\n    def __init__(self, in_channels, \\\nhidden_channels, out_channels, agg_func='mean'):  #1\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(in_channels, \\\nhidden_channels, aggr=agg_func)   #2\n        self.conv2 = SAGEConv(hidden_channels, \\\nout_channels, aggr=agg_func)   #3\n\n    def forward(self, x, edge_index):\n         x = self.conv1(x, edge_index)\n         x = F.relu(x)\n         x = self.conv2(x, edge_index)\n\n    return F.log_softmax(x, dim=1)\n```", "```py\n       self.conv1 = SAGEConv(in_channels,\\\n hidden_channels, aggr=['max', 'sum', 'mean'])\n\n       self.conv1 = SAGEConv(in_channels,\\\n hidden_channels, aggr=[SoftmaxAggregation(),\\\n StdAggregation() ])\n```", "```py\nclass CustomGCN(torch.nn.Module):\n   def __init__(self, in_channels, hidden_channels, out_channels):\n       super(CustomGCN, self).__init__()\n       self.conv1 = GCNConv(in_channels, hidden_channels)\n       self.conv2 = GCNConv(hidden_channels, out_channels)\n\n       self.jk = JumpingKnowledge(mode='cat')  #1\n\n   def forward(self, x, edge_index):\n       layer_outputs = []  #2\n\n       x1 = self.conv1(x, edge_index)\n       x1 = F.relu(x1)\n       layer_outputs.append(x1)  #3\n\n       x2 = self.conv2(x1, edge_index)\n       layer_outputs.append(x2) \n\n       x = self.jk(layer_outputs)  #4\n\n       return x\n```", "```py\nclass GraphSAGEWithCustomDropout(torch.nn.Module):\n   def __init__(self, in_channels, \\\nhidden_channels, out_channels, num_layers, \\\ndropout_rate=0.5, aggr='mean'):  #1\n       super(GraphSAGEWithCustomDropout, self).__init__()\n       self.layers = torch.nn.ModuleList\\\n([SAGEConv(in_channels, hidden_channels, aggr=aggr)])\n       for _ in range(1, num_layers-1):  #2\n           self.layers.append(SAGEConv\\\n(hidden_channels, hidden_channels, aggr=aggr))\n       self.layers.append(SAGEConv\\\n(hidden_channels, out_channels, aggr=aggr))\n       self.dropout_rate = dropout_rate\n\n   def forward(self, x, edge_index):\n       for layer in self.layers[:-1]:\n           x = F.relu(layer(x, edge_index))\n           x = F.dropout(x, p=self.dropout_rate, training=self.training)\n       x = self.layers[-1](x, edge_index)\n       return F.log_softmax(x, dim=1)\n```", "```py\nmodel_1 = GraphSAGEWithCustomDropout\\\n(subset_graph.num_features, 64, \\\ndataset.num_classes, 2, dropout_rate=.5, \\\naggr= ‘max’).to(device)\n\nmodel_2 = GraphSAGEWithCustomDropout\\\n(subset_graph.num_features, 64, \\\ndataset.num_classes, 2, dropout_rate=0.5, \\\naggr=['max', 'sum', 'mean']).to(device)\n\nmodel_3 = GraphSAGEWithCustomDropout\\\n(subset_graph.num_features, 64, \\\ndataset.num_classes, 2, dropout_rate=0.50,\\\n aggr=[SoftmaxAggregation(), \\\nStdAggregation() ] ).to(device)\n```", "```py\ndef gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False,\n             add_self_loops=True, dtype=None):  #1\n\n   fill_value = 2\\. **if** improved **else** 1\\.  #2\n\n   **if** isinstance(edge_index, SparseTensor):  #3\n       adj_t = edge_index\n       **if** not adj_t.has_value():\n           adj_t = adj_t.fill_value(1., dtype=dtype)\n       **if** add_self_loops:\n           adj_t = fill_diag(adj_t, fill_value)\n       deg = sparsesum(adj_t, dim=1)\n       deg_inv_sqrt = deg.pow_(-0.5) \n       deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0.)\n       adj_t = mul(adj_t, deg_inv_sqrt.view(-1, 1))\n       adj_t = mul(adj_t, deg_inv_sqrt.view(1, -1))\n       **return** adj_t\n\n   **else**: \n       num_nodes = maybe_num_nodes(edge_index, num_nodes)\n\n       **if** edge_weight is None:\n           edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n                                    device=edge_index.device)\n\n       **if** add_self_loops:\n           edge_index, tmp_edge_weight = add_remaining_self_loops(\n               edge_index, edge_weight, fill_value, num_nodes)\n           **assert** tmp_edge_weight is not None\n           edge_weight = tmp_edge_weight\n\n       row, col = edge_index[0], edge_index[1]\n       deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n       deg_inv_sqrt = deg.pow_(-0.5)\n       deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n       **return** edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n```", "```py\n    edge_index = data.edge_index\n    num_nodes = edge_index.max().item() + 1  #1\n\n    deg = torch.zeros(num_nodes, \\\n    dtype=torch.float).to(edge_index.device)       #2\n    deg.scatter_add_(0, edge_index[1],            \n                 torch.ones(edge_index.size(1))\\\n.to(edge_index.device))                           \n\n    deg_inv_sqrt = deg.pow(-0.5)  #3\n    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0   #3\n\n    edge_weight = torch.ones(edge_index.size(1))\\\n    .to(edge_index.devic)  #4\n    edge_weight = deg_inv_sqrt[edge_index[0]]*edge_weight*\\\n    deg_inv_sqrt[edge_index[1]]  #5\n\n    num_nodes = edge_index.max().item() + 1   #6\n\n    adj_t = torch.sparse_coo_tensor(indices=edge_index,\\\n    values=edge_weight, size=(num_nodes, num_nodes))      #7\n    data.adj_t = adj_t.coalesce()                         #7\n```", "```py\nclass GCNConv(MessagePassing):    \n\n    def __init__(self, in_channels: int, out_channels: int,\nimproved: bool = False, cached: bool = False,\n        add_self_loops: bool = True, normalize: bool = True,\n        bias: bool = True, **kwargs):    \n\n        self.lin = Linear(in_channels, out_channels, bias=False,\n                         weight_initializer='glorot')\n    def forward(self, x: Tensor, edge_index: Adj,\n                edge_weight: OptTensor = None) -> Tensor:\n\nif self.normalize:  #1\n    if isinstance(edge_index, Tensor):\n        cache = self._cached_edge_index\n            if cache is None:\n                edge_index, edge_weight = gcn_norm( \n                edge_index, edge_weight, x.size(self.node_dim),\n                self.improved, self.add_self_loops)\n                if self.cached:\n                    self._cached_edge_index = (edge_index, edge_weight)\n                else:\n                    edge_index, edge_weight = cache[0], cache[1]\n\n        x = self.lin(x)  #2\n\n        out = self.propagate(edge_index, x=x,\\\n edge_weight=edge_weight, size=None)  #3\n\n        **if** self.bias is not None:  #4\n            out += self.bias\n\n        **return** out\n```", "```py\nclass SAGEConv(MessagePassing):\n…\n   def forward(self, x, edge_index, size):\n\n        if isinstance(x, Tensor):\n            x: OptPairTensor = (x, x)\n\n       if self.project and hasattr(self, 'lin'):  #1\n           x = (self.lin(x[0]).relu(), x[1])\n\n       out = self.propagate(edge_index, x=x, size=size)  #2\n       out = self.lin_l(out)  #2\n       x_r = x[1]  #3\n\n       **if** self.root_weight and x_r is not None:  #4\n           out += self.lin_r(x_r)  #4\n\n       **if** self.normalize:  #5\n           out = F.normalize(out, p=2., dim=-1)  #5\n       **return** out\n\n   **def** message(self, x_j):\n       **return** x_j\n\n   **def** message_and_aggregate(self, adj_t, x):\n       adj_t = adj_t.set_value(None, layout=None)  #6\n       **return** matmul(adj_t, x[0], reduce=self.aggr)  #6\n…\n```"]