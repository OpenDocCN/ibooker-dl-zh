- en: Chapter 9\. Inference Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。推理优化
- en: 'New models come and go, but one thing will always remain relevant: making them
    better, cheaper, and faster. Up until now, the book has discussed various techniques
    for making models better. This chapter focuses on making them faster and cheaper.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 新模型层出不穷，但有一点始终相关：使它们更好、更便宜、更快。到目前为止，本书已经讨论了各种使模型更好的技术。这一章专注于使它们更快、更便宜。
- en: No matter how good your model is, if it’s too slow, your users might lose patience,
    or worse, its predictions might become useless—imagine a next-day stock price
    prediction model that takes two days to compute each outcome. If your model is
    too expensive, its return on investment won’t be worth it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的模型有多好，如果它太慢，用户可能会失去耐心，或者更糟，其预测可能变得毫无价值——想象一下一个需要两天时间来计算每个结果的次日股票价格预测模型。如果你的模型成本太高，其投资回报率可能不值得。
- en: Inference optimization can be done at the model, hardware, and service levels.
    At the model level, you can reduce a trained model’s size or develop more efficient
    architectures, such as one without the computation bottlenecks in the attention
    mechanism often used in transformer models. At the hardware level, you can design
    more powerful hardware.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 推理优化可以在模型、硬件和服务层面进行。在模型层面，你可以减小训练模型的尺寸或开发更高效的架构，例如没有在Transformer模型中常用注意力机制中的计算瓶颈的架构。在硬件层面，你可以设计更强大的硬件。
- en: The inference service runs the model on the given hardware to accommodate user
    requests. It can incorporate techniques that optimize models for specific hardware.
    It also needs to consider usage and traffic patterns to efficiently allocate resources
    to reduce latency and cost.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 推理服务在给定的硬件上运行模型以适应用户请求。它可以采用针对特定硬件优化的模型技术。它还需要考虑使用和流量模式，以有效地分配资源以减少延迟和成本。
- en: Because of this, inference optimization is an interdisciplinary field that often
    sees collaboration among model researchers, application developers, system engineers,
    compiler designers, hardware architects, and even data center operators.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，推理优化是一个跨学科的领域，通常涉及模型研究人员、应用开发者、系统工程师、编译器设计人员、硬件架构师甚至数据中心运营商之间的合作。
- en: This chapter discusses bottlenecks for AI inference and techniques to overcome
    them. It’ll focus mostly on optimization at the model and service levels, with
    an overview of AI accelerators.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了AI推理的瓶颈和克服这些瓶颈的技术。它将主要关注模型和服务层面的优化，并对AI加速器进行概述。
- en: This chapter also covers performance metrics and trade-offs. Sometimes, a technique
    that speeds up a model can also reduce its cost. For example, reducing a model’s
    precision makes it smaller and faster. But often, optimization requires trade-offs.
    For example, the best hardware might make your model run faster but at a higher
    cost.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还涵盖了性能指标和权衡。有时，加快模型的技术也可以降低其成本。例如，降低模型的精度使其更小、更快。但通常，优化需要权衡。例如，最好的硬件可能会使你的模型运行得更快，但成本更高。
- en: Given the growing availability of open source models, more teams are building
    their own inference services. However, even if you don’t implement these inference
    optimization techniques, understanding these techniques will help you evaluate
    inference services and frameworks. If your application’s latency and cost are
    hurting you, read on. This chapter might help you diagnose the causes and potential
    solutions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着开源模型的日益可用，越来越多的团队正在构建自己的推理服务。然而，即使你没有实现这些推理优化技术，了解这些技术也会帮助你评估推理服务和框架。如果你的应用的延迟和成本正在困扰你，请继续阅读。这一章可能有助于你诊断原因和潜在解决方案。
- en: Understanding Inference Optimization
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解推理优化
- en: 'There are two distinct phases in an AI model’s lifecycle: training and inference.
    Training refers to the process of building a model. Inference refers to the process
    of using a model to compute an output for a given input.^([1](ch09.html#id1597))
    Unless you train or finetune a model, you’ll mostly need to care about inference.^([2](ch09.html#id1598))'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: AI模型的生命周期有两个不同的阶段：训练和推理。训练是指构建模型的过程。推理是指使用模型为给定输入计算输出的过程。[1](ch09.html#id1597)除非你训练或微调模型，否则你主要需要关注推理。[2](ch09.html#id1598)
- en: This section starts with an overview of inference that introduces a shared vocabulary
    to discuss the rest of the chapter. If you’re already familiar with these concepts,
    feel free to skip to the section of interest.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节从推理概述开始，引入一个共享词汇来讨论本章的其余部分。如果你已经熟悉这些概念，请随意跳到感兴趣的章节。
- en: Inference Overview
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理概述
- en: In production, the component that runs model inference is called an inference
    server. It hosts the available models and has access to the necessary hardware.
    Based on requests from applications (e.g., user prompts), it allocates resources
    to execute the appropriate models and returns the responses to users. An inference
    server is part of a broader inference service, which is also responsible for receiving,
    routing, and possibly preprocessing requests before they reach the inference server.
    A visualization of a simple inference service is shown in [Figure 9-1](#ch09_figure_1_1730130962952524).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中，运行模型推理的组件被称为推理服务器。它托管可用的模型，并有权访问必要的硬件。基于来自应用程序（例如，用户提示）的请求，它分配资源以执行适当的模型，并将响应返回给用户。推理服务器是更广泛推理服务的一部分，该服务还负责在请求到达推理服务器之前接收、路由和可能预处理请求。一个简单的推理服务的可视化如图
    [图9-1](#ch09_figure_1_1730130962952524) 所示。
- en: .
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 。
- en: '![A diagram of a computer hardware system'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机硬件系统图'
- en: Description automatically generated](assets/aien_0901.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成描述](assets/aien_0901.png)
- en: Figure 9-1\. A simple inference service.
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 一个简单的推理服务。
- en: Model APIs like those provided by OpenAI and Google are inference services.
    If you use one of these services, you won’t be implementing most of the techniques
    discussed in this chapter. However, if you host a model yourself, you’ll be responsible
    for building, optimizing, and maintaining its inference service.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型API，如OpenAI和Google提供的，是推理服务。如果你使用这些服务之一，你将不会实现本章讨论的大多数技术。然而，如果你自己托管模型，你将负责构建、优化和维护其推理服务。
- en: Computational bottlenecks
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算瓶颈
- en: 'Optimization is about identifying bottlenecks and addressing them. For example,
    to optimize traffic, city planners might identify congestion points and take measures
    to alleviate congestion. Similarly, an inference server should be designed to
    address the computational bottlenecks of the inference workloads it serves. There
    are two main computational bottlenecks, *compute-bound* and *memory bandwidth-bound*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 优化是关于识别瓶颈并解决它们。例如，为了优化交通，城市规划者可能会识别拥堵点并采取措施缓解拥堵。同样，推理服务器应该被设计来解决其服务的推理工作负载的计算瓶颈。主要有两个计算瓶颈，*计算密集型*
    和 *内存带宽限制型*：
- en: Compute-bound
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 计算密集型
- en: This refers to tasks whose time-to-complete is determined by the computation
    needed for the tasks. For example, password decryption is typically compute-bound
    due to the intensive mathematical calculations required to break encryption algorithms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这指的是完成时间由任务所需的计算量决定的任务。例如，密码解密通常由于破解加密算法所需的密集数学计算而成为计算密集型。
- en: Memory bandwidth-bound
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 内存带宽限制型
- en: These tasks are constrained by the data transfer rate within the system, such
    as the speed of data movement between memory and processors. For example, if you
    store your data in the CPU memory and train a model on GPUs, you have to move
    data from the CPU to the GPU, which can take a long time. This can be shortened
    as bandwidth-bound. In literature, memory bandwidth-bound is often referred to
    as memory-bound.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务受系统内部数据传输速率的限制，例如数据在内存和处理器之间移动的速度。例如，如果你将数据存储在CPU内存中并在GPU上训练模型，你必须将数据从CPU移动到GPU，这可能需要很长时间。这可以缩短为带宽限制型。在文献中，内存带宽限制型通常被称为内存限制型。
- en: The concepts of compute-bound or memory bandwidth-bound were introduced in the
    paper “Roofline” ([Williams et al., 2009](https://oreil.ly/M_aGR)).^([4](ch09.html#id1606))
    Mathematically, an operation can be classified as compute-bound or memory bandwidth-bound
    based on its [*arithmetic intensity*](https://oreil.ly/K3j6t), which is the number
    of arithmetic operations per byte of memory access. Profiling tools like NVIDIA
    Nsight will show you a roofline chart to tell you whether your workload is compute-bound
    or memory bandwidth-bound, as shown in [Figure 9-2](#ch09_figure_2_1730130962952613).
    This chart is a *roofline* chart because it resembles a roof. Roofline charts
    are common in hardware performance analyses.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 计算密集型或内存带宽限制的概念在论文“Roofline”中提出（[威廉姆斯等人，2009](https://oreil.ly/M_aGR)）。^([4](ch09.html#id1606))
    从数学上讲，一个操作可以根据其[*算术强度*](https://oreil.ly/K3j6t)被分类为计算密集型或内存带宽限制型，这是每字节内存访问的算术操作数。NVIDIA
    Nsight 等分析工具会显示一个屋顶线图，告诉你你的工作负载是计算密集型还是内存带宽限制型，如图 [图9-2](#ch09_figure_2_1730130962952613)
    所示。这个图表被称为 *屋顶线* 图，因为它看起来像屋顶。屋顶线图在硬件性能分析中很常见。
- en: Different optimization techniques aim to mitigate different bottlenecks. For
    example, a compute-bound workload might be sped up by spreading it out to more
    chips or by leveraging chips with more computational power (e.g., a higher FLOP/s
    number). A memory bandwidth-bound workload might be sped up by leveraging chips
    with higher bandwidth.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的优化技术旨在缓解不同的瓶颈。例如，计算密集型工作负载可以通过将工作负载分散到更多芯片或利用计算能力更强的芯片（例如，更高的FLOP/s数值）来加速。内存带宽受限的工作负载可以通过利用带宽更高的芯片来加速。
- en: '![A graph with a line and a point'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![一张包含线和点的图表'
- en: Description automatically generated with medium confidence](assets/aien_0902.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成，中等置信度](assets/aien_0902.png)
- en: Figure 9-2\. The roofline chart can help you visualize whether an operation
    is compute-bound or memory bandwidth-bound. This graph is on a log scale.
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. 屋顶线图可以帮助您可视化操作是计算密集型还是内存带宽受限。此图是按对数刻度绘制的。
- en: Different model architectures and workloads result in different computational
    bottlenecks. For example, inference for image generators like Stable Diffusion
    is typically compute-bound, whereas inference for autoregression language models
    is typically memory bandwidth-bound.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的模型架构和工作负载导致不同的计算瓶颈。例如，Stable Diffusion等图像生成器的推理通常是计算密集型的，而自回归语言模型的推理通常是内存带宽受限的。
- en: 'As an illustration, let’s look into language model inference. Recall from [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359)
    that inference for a transformer-based language model consists of two steps, prefilling
    and decoding:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作为说明，让我们看看语言模型推理。回想一下[第2章](ch02.html#ch02_understanding_foundation_models_1730147895571359)中提到的，基于Transformer的语言模型的推理包括两个步骤：预填充和解码：
- en: Prefill
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 预填充
- en: The model processes the input tokens in parallel.^([5](ch09.html#id1607)) How
    many tokens can be processed at once is limited by the number of operations your
    hardware can execute in a given time. Therefore, prefilling is *compute-bound*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行处理输入标记。^([5](ch09.html#id1607))一次可以处理多少个标记受限于硬件在给定时间内可以执行的操作数量。因此，预填充是*计算密集型*的。
- en: Decode
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 解码
- en: The model generates one output token at a time. At a high level, this step typically
    involves loading large matrices (e.g., model weights) into GPUs, which is limited
    by how quickly your hardware can load data into memory. Decoding is, therefore,
    *memory bandwidth-bound*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模型一次生成一个输出标记。在较高层次上，这一步骤通常涉及将大型矩阵（例如，模型权重）加载到GPU中，这受限于硬件将数据加载到内存中的速度。因此，解码是*内存带宽受限*的。
- en: '[Figure 9-3](#ch09_figure_3_1730130962952638) visualizes prefilling and decoding.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](#ch09_figure_3_1730130962952638)展示了预填充和解码。'
- en: '![A diagram of a computer'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机的示意图'
- en: Description automatically generated](assets/aien_0903.png)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](assets/aien_0903.png)
- en: 'Figure 9-3\. Autoregressive language models follow two steps for inference:
    prefill and decode. `<eos>` denotes the end of the sequence token.'
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. 自回归语言模型在推理时遵循两个步骤：预填充和解码。`<eos>`表示序列结束标记。
- en: Because prefill and decode have different computational profiles, they are often
    decoupled in production with separate machines. This technique will be discussed
    [“Inference Service Optimization”](#ch09_inference_service_optimization_1730130963008735).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预填充和解码具有不同的计算特征，它们在生产中通常使用不同的机器解耦。这种技术将在“推理服务优化”中讨论[“Inference Service Optimization”](#ch09_inference_service_optimization_1730130963008735)。
- en: The factors that affect the amount of prefilling and decoding computation in
    an LLM inference server, and therefore its bottlenecks, include context length,
    output length, and request batching strategies. Long context typically results
    in a memory bandwidth-bound workload, but clever optimization techniques, such
    as those discussed later in this chapter, can remove this bottleneck.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 影响LLM推理服务器中预填充和解码计算量以及其瓶颈的因素包括上下文长度、输出长度和请求批处理策略。长上下文通常会导致内存带宽受限的工作负载，但像本章后面讨论的巧妙优化技术可以消除这个瓶颈。
- en: As of this writing, due to the prevalence of the transformer architecture and
    the limitations of the existing accelerator technologies, many AI and data workloads
    are memory bandwidth-bound. However, future software and hardware advancements
    will be able to make AI and data workloads compute-bound.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，由于Transformer架构的普及和现有加速技术的限制，许多AI和数据工作负载是内存带宽受限的。然而，未来的软件和硬件进步将能够使AI和数据工作负载成为计算密集型。
- en: Online and batch inference APIs
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线批处理推理API
- en: 'Many providers offer two types of inference APIs, online and batch:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 许多提供商提供两种类型的推理 API，在线和批量：
- en: Online APIs optimize for latency. Requests are processed as soon as they arrive.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线 API 优化延迟。请求一到就处理。
- en: Batch APIs optimize for cost. If your application doesn’t have strict latency
    requirements, you can send them to batch APIs for more efficient processing. Higher
    latency allows a broader range of optimization techniques, including batching
    requests together and using cheaper hardware. For example, as of this writing,
    both Google Gemini and OpenAI offer batch APIs at a 50% cost reduction and significantly
    higher turnaround time, i.e., in the order of hours instead of seconds or minutes.^([6](ch09.html#id1612))
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量 API 优化成本。如果你的应用程序没有严格的延迟要求，你可以将它们发送到批量 API 以进行更有效的处理。更高的延迟允许更广泛的最优化技术，包括将请求一起批处理和使用更便宜的硬件。例如，截至本文撰写时，Google
    Gemini 和 OpenAI 都提供批量 API，成本降低 50%，周转时间显著提高，即以小时而不是秒或分钟为单位.^([6](ch09.html#id1612))
- en: Online APIs might still batch requests together as long as it doesn’t significantly
    impact latency, as discussed in [“Batching”](#ch09_batching_1730130963008799).
    The only real difference is that an online API focuses on lower latency, whereas
    a batch API focuses on higher throughput.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在线 API 可能仍然将请求批处理在一起，只要它不会显著影响延迟，正如在“批处理”中讨论的那样。[“批处理”](#ch09_batching_1730130963008799)。唯一的真正区别是，在线
    API 专注于更低的延迟，而批量 API 专注于更高的吞吐量。
- en: 'Customer-facing use cases, such as chatbots and code generation, typically
    require lower latency, and, therefore, tend to use online APIs. Use cases with
    less stringent latency requirements, which are ideal for batch APIs, include the
    following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 面向客户的用例，例如聊天机器人和代码生成，通常需要更低的延迟，因此倾向于使用在线 API。对于对延迟要求不那么严格的用例，包括以下内容，它们是批量 API
    的理想选择：
- en: Synthetic data generation
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成合成数据
- en: Periodic reporting, such as summarizing Slack messages, sentiment analysis of
    brand mentions on social media, and analyzing customer support tickets
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期报告，例如总结 Slack 消息、社交媒体上品牌提及的情感分析以及分析客户支持工单
- en: Onboarding new customers who require processing of all their uploaded documents
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为需要处理所有上传文档的新客户办理入职手续
- en: Migrating to a new model that requires reprocessing of all the data
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移到需要重新处理所有数据的新模型
- en: Generating personalized recommendations or newsletters for a large customer
    base
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为大量客户群生成个性化推荐或新闻通讯
- en: Knowledge base updates by reindexing an organization’s data
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过重新索引组织的数据来更新知识库
- en: APIs usually return complete responses by default. However, with autoregressive
    decoding, it can take a long time for a model to complete a response, and users
    are impatient. Many online APIs offer *streaming mode*, which returns each token
    as it’s generated. This reduces the time the users have to wait until the first
    token. The downside of this approach is that you can’t score a response before
    showing it to users, increasing the risk of users seeing bad responses. However,
    you can still retrospectively update or remove a response as soon as the risk
    is detected.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: API 默认情况下通常会返回完整的响应。然而，对于自回归解码，模型完成响应可能需要很长时间，而用户则缺乏耐心。许多在线 API 提供了 *流式模式*，在生成每个标记时返回。这减少了用户等待第一个标记的时间。这种方法的缺点是，在向用户展示之前，你无法对响应进行评分，这增加了用户看到不良响应的风险。然而，一旦检测到风险，你仍然可以立即回溯更新或删除响应。
- en: Warning
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'A batch API for foundation models differs from batch inference for traditional
    ML. In traditional ML:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的批量 API 与传统机器学习的批量推理不同。在传统机器学习中：
- en: Online inference means that predictions are computed *after* requests have arrived.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在请求到达后进行在线推理，意味着预测是在请求到达后计算的 *之后*。
- en: Batch inference means that predictions are precomputed *before* requests have
    arrived.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量推理意味着预测是在请求到达之前预先计算的 *之前*。
- en: Precompution is possible for use cases with finite and predictable inputs like
    recommendation systems, where recommendations can be generated for all users in
    advance. These precomputed predictions are fetched when requests arrive, e.g.,
    when a user visits the website. However, with foundation model use cases where
    the inputs are open-ended, it’s hard to predict all user prompts.^([7](ch09.html#id1619))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有有限和可预测输入的用例，例如推荐系统，其中可以预先为所有用户生成推荐，预计算是可能的。当请求到达时，例如当用户访问网站时，会检索这些预计算的预测。然而，对于输入开放的底层模型用例，很难预测所有用户提示.^([7](ch09.html#id1619))
- en: Inference Performance Metrics
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理性能指标
- en: Before jumping into optimization, it’s important to understand what metrics
    to optimize for. From the user perspective, the central axis is latency (response
    quality is a property of the model itself, not of the inference service). However,
    application developers must also consider throughput and utilization as they determine
    the cost of their applications.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在着手优化之前，了解要优化的指标非常重要。从用户的角度来看，核心轴是延迟（响应质量是模型本身的属性，而不是推理服务的属性）。然而，应用开发者也必须在确定其应用成本时考虑吞吐量和利用率。
- en: Latency, TTFT, and TPOT
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟、TTFT和TPOT
- en: 'Latency measures the time from when users send a query until they receive the
    complete response. For autoregressive generation, especially in the streaming
    mode, the overall latency can be broken into several metrics:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟衡量用户发送查询到收到完整响应的时间。对于自回归生成，尤其是在流式模式下，整体延迟可以分解为几个指标：
- en: Time to first token
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个标记的时间
- en: TTFT measures how quickly the first token is generated after users send a query.
    It corresponds to the duration of the prefill step and depends on the input’s
    length. Users might have different expectations for TTFT for different applications.
    For example, for conversational chatbots, the TTFT should be instantaneous.^([8](ch09.html#id1620))
    However, users might be willing to wait longer to summarize long documents.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: TTFT衡量用户发送查询后第一个标记生成的速度。它对应于预填充步骤的持续时间，并取决于输入的长度。用户对不同应用对TTFT可能有不同的期望。例如，对于对话聊天机器人，TTFT应该是瞬间的。[8](ch09.html#id1620)
    然而，用户可能愿意等待更长的时间来总结长文档。
- en: Time per output token
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出标记的时间
- en: TPOT measures how quickly each output token is generated after the first token.
    If each token takes 100 ms, a response of 1,000 tokens will take 100 s.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TPOT衡量第一个标记之后每个输出标记生成的速度。如果每个标记需要100 ms，那么1,000个标记的响应将需要100秒。
- en: In the streaming mode, where users read each token as it’s generated, TPOT should
    be faster than human reading speed but doesn’t have to be much faster. A very
    fast reader can read 120 ms/token, so a TPOT of around 120 ms, or 6–8 tokens/second,
    is sufficient for most use cases.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在流式模式下，用户读取每个生成的标记，TPOT应该比人类阅读速度快，但不必快得多。一个阅读速度非常快的人可以以120 ms/标记的速度阅读，因此大约120
    ms，或每秒6-8个标记的TPOT对于大多数用例来说就足够了。
- en: Time between tokens and inter-token latency
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 标记间时间和i标记延迟
- en: Variations of this metric include *time between tokens (TBT)* and i*nter-token
    latency (ITL)*.^([9](ch09.html#id1623)) Both measure the time between output tokens.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标的变体包括*标记间时间（TBT）*和*i标记延迟（ITL）*。[9](ch09.html#id1623) 这两个指标都衡量输出标记之间的时间。
- en: The total latency will equal `TTFT + TPOT` × `(number of output tokens).`
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 总延迟将等于`TTFT + TPOT` × `(输出标记数量)`。
- en: Two applications with the same total latency can offer different user experiences
    with different TTFT and TPOT. Would your users prefer instant first tokens with
    a longer wait between tokens, or would they rather wait slightly longer for the
    first tokens but enjoy faster token generation afterward? User studies will be
    necessary to determine the optimal user experience. Reducing TTFT at the cost
    of higher TPOT is possible by shifting more compute instances from decoding to
    prefilling and vice versa.^([10](ch09.html#id1624))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 两个具有相同总延迟的应用可以通过不同的TTFT和TPOT提供不同的用户体验。用户更愿意在标记之间有更长的等待时间以获得即时的第一个标记，还是更愿意等待稍微长一点的时间以获得第一个标记，然后享受更快的标记生成？需要进行用户研究以确定最佳用户体验。通过将更多的计算实例从解码转移到预填充，并反之亦然，可以在牺牲更高的TPOT的情况下降低TTFT。[10](ch09.html#id1624)
- en: It’s important to note that the TTFT and TPOT values observed by users might
    differ from those observed by models, especially in scenarios involving CoT (chain-of-thought)
    or agentic queries where models generate intermediate steps not shown to users.
    Some teams use the metric *time to publish* to make it explicit that it measures
    time to the first token users see.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，用户观察到的TTFT和TPOT值可能与模型观察到的值不同，尤其是在涉及CoT（思维链）或代理查询的场景中，模型生成不显示给用户的中间步骤。一些团队使用*发布时间*这个指标来明确表示它衡量的是用户看到第一个标记的时间。
- en: 'Consider the scenario where, after a user sends a query, the model performs
    the following steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下场景：在用户发送查询后，模型执行以下步骤：
- en: Generate a plan, which consists of a sequence of actions. This plan isn’t shown
    to the user.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个包含一系列操作的计划。这个计划不会显示给用户。
- en: Take actions and log their outputs. These outputs aren’t shown to the user.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行操作并记录它们的输出。这些输出不会显示给用户。
- en: Based on these outputs, generate a final response to show the user.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这些输出，生成一个最终响应以向用户展示。
- en: From the model’s perspective, the first token is generated in step 1\. This
    is when the model internally begins its token generation process. The user, however,
    only sees the first token of the final output generated in step 3\. Thus, from
    their perspective, TTFT is much longer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型的角度来看，第一个令牌是在步骤1中生成的。这是模型开始其令牌生成过程的内部时刻。然而，用户只能看到在步骤3中生成的最终输出的第一个令牌。因此，从他们的角度来看，TTFT要长得多。
- en: Because latency is a distribution, the average can be misleading. Imagine you
    have 10 requests whose TTFT values are 100 ms, 102 ms, 100 ms, 100 ms, 99 ms,
    104 ms, 110 ms, 90 ms, 3,000 ms, 95 ms. The average TTFT value is 390 ms, which
    makes your inference service seem slower than it is. There might have been a network
    error that slowed down one request or a particularly long prompt that took a much
    longer time to prefill. Either way, you should investigate. With a large volume
    of requests, outliers that skew the average latency are almost inevitable.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因为延迟是一个分布，平均数可能会误导。想象一下，你有10个请求，其TTFT值分别是100毫秒、102毫秒、100毫秒、100毫秒、99毫秒、104毫秒、110毫秒、90毫秒、3,000毫秒、95毫秒。平均TTFT值是390毫秒，这使得你的推理服务看起来比实际要慢。可能有一个网络错误导致一个请求变慢，或者一个特别长的提示需要更长的时间来预填充。无论如何，你应该进行调查。在大量请求的情况下，导致平均延迟偏斜的异常值几乎是不可避免的。
- en: It’s more helpful to look at latency in percentiles, as they tell you something
    about a certain percentage of your requests. The most common percentile is the
    50th percentile, abbreviated as p50 (median). If the median is 100 ms, half of
    the requests take longer than 100 ms to generate the first token, and half take
    less than 100 ms. Percentiles also help you discover outliers, which might be
    symptoms of something wrong. Typically, the percentiles you’ll want to look at
    are p90, p95, and p99\. It’s also helpful to plot TTFT values against inputs’
    lengths.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 观察百分位数中的延迟更有帮助，因为它们能告诉你关于你请求中一定百分比的某些信息。最常见的百分位数是第50百分位数，简称为p50（中位数）。如果中位数是100毫秒，那么有一半的请求生成第一个令牌的时间超过100毫秒，另一半则少于100毫秒。百分位数还有助于你发现异常值，这些异常值可能是某些问题的症状。通常，你想要查看的百分位数是p90、p95和p99。将TTFT值与输入长度绘制成图表也有帮助。
- en: Throughput and goodput
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 吞吐量和有效吞吐量
- en: Throughput measures the number of output tokens per second an inference service
    can generate across all users and requests.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量衡量的是推理服务在所有用户和请求中每秒可以生成的输出令牌数量。
- en: Some teams count both input and output tokens in throughput calculation. However,
    since processing input tokens (prefilling) and generating output tokens (decoding)
    have different computational bottlenecks and are often decoupled in modern inference
    servers, input and output throughput should be counted separately. When throughput
    is used without any modifier, it usually refers to output tokens.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一些团队在吞吐量计算中同时计算输入和输出令牌。然而，由于处理输入令牌（预填充）和生成输出令牌（解码）有不同的计算瓶颈，并且在现代推理服务器中通常解耦，因此输入和输出吞吐量应单独计算。当吞吐量没有使用任何修饰符时，通常指的是输出令牌。
- en: Throughput is typically measured as tokens/s (TPS). If you serve multiple users,
    tokens/s/user is also used to evaluate how the system scales with more users.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量通常以每秒令牌数（TPS）来衡量。如果你为多个用户提供服务，每用户每秒令牌数（TPS/user）也用于评估系统如何随着用户数量的增加而扩展。
- en: Throughput can also be measured as the number of *completed* requests during
    a given time. Many applications use requests per second (RPS). However, for applications
    built on top of foundation models, a request might take seconds to complete, so
    many people use completed requests per minute (RPM) instead. Tracking this metric
    is useful for understanding how an inference service handles concurrent requests.
    Some providers might throttle your service if you send too many concurrent requests
    at the same time.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量也可以衡量为给定时间内完成的请求数量。许多应用程序使用每秒请求数（RPS）。然而，对于建立在基础模型之上的应用程序，一个请求可能需要几秒钟才能完成，因此许多人使用每分钟完成的请求数（RPM）来代替。跟踪这个指标有助于了解推理服务如何处理并发请求。一些提供商可能会在同时发送过多的并发请求时限制你的服务。
- en: Throughput is directly linked to compute cost. A higher throughput typically
    means lower cost. If your system costs $2/h in compute and its throughput is 100
    tokens/s, it costs around $5.556 per 1M output tokens. If each request generates
    200 output tokens on average, the cost for decoding 1K requests would be $1.11.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The prefill cost can be similarly calculated. If your hardware costs $2 per
    hour and it can prefill 100 requests per minute, the cost for prefilling 1K requests
    would be $0.33.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: The total cost per request is the sum of the prefilling and decoding costs.
    In this example, the total cost for 1K requests would be $1.11 + $0.33 = $1.44.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: What’s considered good throughput depends on the model, the hardware, and the
    workload. Smaller models and higher-end chips typically result in higher throughput.
    Workloads with consistent input and output lengths are easier to optimize than
    workloads with variable lengths.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Even for similarly sized models, hardware, and workloads, direct throughput
    comparisons might be only approximate because token count depends on what constitutes
    a token, and different models have different tokenizers. It’s better to compare
    the efficiency of inference servers using metrics such as cost per request.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Just like most other software applications, AI applications have the latency/throughput
    trade-off. Techniques like batching can improve throughput but reduce latency.
    According to the LinkedIn AI team in their reflection after a year of deploying
    generative AI products ([LinkedIn, 2024](https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US)),
    it’s not uncommon to double or triple the throughput if you’re willing to sacrifice
    TTFT and TPOT.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Due to this trade-off, focusing on an inference service based solely on its
    throughput and cost can lead to a bad user experience. Instead, some teams focus
    on [*goodput*](https://en.wikipedia.org/wiki/Goodput), a metric adapted from networking
    for LLM applications. Goodput measures the number of requests per second that
    satisfies the SLO, software-level objective.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that your application has the following objectives: TTFT of at most
    200 ms and TPOT of at most 100 ms. Let’s say that your inference service can complete
    100 requests per minute. However, out of these 100 requests, only 30 satisfy the
    SLO. Then, the goodput of this service is 30 requests per minute. A visualization
    of this is shown in [Figure 9-4](#ch09_figure_4_1730130962952660).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph showing different colored bars'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](assets/aien_0904.png)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9-4\. If an inference service can complete 10 RPS but only 3 satisfy
    the SLO, then its goodput is 3 RPS.
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Utilization, MFU, and MBU
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Utilization metrics measure how efficiently a resource is being used. It typically
    quantifies the proportion of the resource actively being used compared to its
    total available capacity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: A common but often misunderstood metric is *GPU utilization*, and NVIDIA is
    partially to blame for this misunderstanding. The official NVIDIA tool for monitoring
    GPU usage is [`nvidia-smi`](https://oreil.ly/ludJ2)—SMI stands for System Management
    Interface. One metric this tool shows is GPU utilization, which represents the
    percentage of time during which the GPU is actively processing tasks. For example,
    if you run inference on a GPU cluster for 10 hours, and the GPUs are actively
    processing tasks for 5 of those hours, your GPU utilization would be 50%.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见但经常被误解的指标是*GPU利用率*，NVIDIA在这方面部分应承担责任。官方NVIDIA监控GPU使用的工具是[`nvidia-smi`](https://oreil.ly/ludJ2)——SMI代表系统管理接口。该工具显示的一个指标是GPU利用率，它表示GPU在积极处理任务的时间百分比。例如，如果你在一个GPU集群上运行推理10小时，其中5小时GPU正在积极处理任务，那么你的GPU利用率将是50%。
- en: However, actively processing tasks doesn’t mean doing so efficiently. For simplicity,
    consider a tiny GPU capable of doing 100 operations per second. In `nvidia-smi`’s
    definition of utilization, this GPU can report 100% utilization even if it’s only
    doing one operation per second.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，积极处理任务并不意味着这样做是高效的。为了简单起见，考虑一个每秒能执行100个操作的微型GPU。在`nvidia-smi`对利用率的定义中，即使这个GPU每秒只执行一个操作，它也可以报告100%的利用率。
- en: If you pay for a machine that can do 100 operations and use it for only 1 operation,
    you’re wasting money. `nvidia-smi`’s GPU optimization metric is, therefore, not
    very useful. A utilization metric you might care about, out of all the operations
    a machine is capable of computing, is how many it’s doing in a given time. This
    metric is called *MFU (Model FLOP/s Utilization)*, which distinguishes it from
    the NVIDIA GPU utilization metric.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你支付了一台可以执行100个操作的机器，但只使用了1个操作，你就是在浪费钱。因此，`nvidia-smi`的GPU优化指标并不是非常有用。你可能关心的利用率指标之一，是机器在给定时间内能够执行的操作数量。这个指标被称为*MFU（模型FLOP/s利用率）*，它将其与NVIDIA的GPU利用率指标区分开来。
- en: MFU is the ratio of the observed throughput (tokens/s) relative to the theoretical
    maximum throughput of a system operating at peak FLOP/s. If at the peak FLOP/s
    advertised by the chip maker, the chip can generate 100 tokens/s, but when used
    for your inference service, it can generate only 20 tokens/s, your MFU is 20%.^([11](ch09.html#id1637))
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: MFU是观察到的吞吐量（token/s）与系统在峰值FLOP/s下运行的理论最大吞吐量之比。如果芯片制造商宣传的峰值FLOP/s是100 tokens/s，但当你用于推理服务时，只能生成20
    tokens/s，那么你的MFU是20%。^([11](ch09.html#id1637))
- en: Similarly, because memory bandwidth is expensive, you might also want to know
    how efficiently your hardware’s bandwidth is utilized. *MBU (Model Bandwidth Utilization)*
    measures the percentage of achievable memory bandwidth used. If the chip’s peak
    bandwidth is 1 TB/s and your inference uses only 500 GB/s, your MBU is 50%.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，由于内存带宽很昂贵，你也可能想知道你的硬件带宽的利用率有多高。*MBU（模型带宽利用率）*衡量的是可达到的内存带宽使用的百分比。如果芯片的峰值带宽是1
    TB/s，而你的推理只使用了500 GB/s，那么你的MBU是50%。
- en: 'Computing the memory bandwidth being used for LLM inference is straightforward:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 计算用于LLM推理的内存带宽是直接的：
- en: '[PRE0]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'MBU is computed as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: MBU的计算方法如下：
- en: '[PRE1]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For example, if you use a 7B-parameter model in FP16 (two bytes per parameter)
    and achieve 100 tokens/s, the bandwidth used is:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你使用一个7B参数的FP16模型（每个参数两个字节）并且达到100个token/s，使用的带宽是：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This underscores the importance of quantization (discussed in [Chapter 7](ch07.html#ch07)).
    Fewer bytes per parameter mean your model consumes less valuable bandwidth.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这强调了量化（在第7章中讨论）的重要性。每个参数的字节数越少，你的模型消耗的宝贵带宽就越少。
- en: 'If this is done on an A100-80GB GPU with a theoretical 2 TB/s of memory bandwidth,
    the MBU is:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在一个理论内存带宽为2 TB/s的A100-80GB GPU上这样做，MBU是：
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The relationships between throughput (tokens/s) and MBU and between throughput
    and MFU are linear, so some people might use throughput to refer to MBU and MFU.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量（token/s）与MBU和MFU之间的关系是线性的，因此有些人可能会用吞吐量来指代MBU和MFU。
- en: What’s considered a good MFU and MBU depends on the model, hardware, and workload.
    Compute-bound workloads typically have higher MFU and lower MBU, while bandwidth-bound
    workloads often show lower MFU and higher MBU.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的MFU和MBU取决于模型、硬件和工作负载。计算密集型工作负载通常具有更高的MFU和较低的MBU，而带宽密集型工作负载通常显示较低的MFU和较高的MBU。
- en: Because training can benefit from more efficient optimization (e.g., better
    batching), thanks to having more predictable workloads, MFU for training is typically
    higher than MFU for inference. For inference, since prefill is compute-bound and
    decode is memory bandwidth-bound, MFU during prefilling is typically higher than
    MFU during decoding. For model training, as of this writing, an MFU above 50%
    is generally considered good, but it can be hard to achieve on specific hardware.^([12](ch09.html#id1638))
    [Table 9-1](#ch09_table_1_1730130962971021) shows MFU for several models and accelerators.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练可以从更有效的优化（例如，更好的批处理）中受益，这得益于更可预测的工作负载，因此训练的MFU通常高于推理的MFU。对于推理，由于预填充是计算限制的，而解码是内存带宽限制的，因此预填充期间的MFU通常高于解码期间的MFU。对于模型训练，截至本文撰写时，一般认为MFU超过50%是好的，但在特定硬件上可能难以实现。[表9-1](#ch09_table_1_1730130962971021)
    展示了几个模型和加速器的MFU。
- en: 'Table 9-1\. MFU examples from “PaLM: Scaling Language Modeling with Pathways”
    (Chowdhery et al., 2022).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-1\. 来自“PaLM：通过路径扩展语言模型”（Chowdhery等，2022年）的MFU示例。
- en: '| Model | Number of parameters (in billions) | Accelerator chips | Model FLOP/s
    utilization |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数数量（以十亿计） | 加速器芯片 | 模型FLOP/s利用率 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GPT-3 | 175B | V100 | 21.3% |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 | 175B | V100 | 21.3% |'
- en: '| Gopher | 280B | 4096 TPU v3 | 32.5% |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Gopher | 280B | 4096 TPU v3 | 32.5% |'
- en: '| Megatron-Turing NLG | 530B | 2240 A100 | 30.2% |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-Turing NLG | 530B | 2240 A100 | 30.2% |'
- en: '| PaLM | 540B | 6144 TPU v4 | 46.2% |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| PaLM | 540B | 6144 TPU v4 | 46.2% |'
- en: '[Figure 9-5](#ch09_figure_5_1730130962952692) shows the MBU for the inference
    process using Llama 2-70B in FP16 on different hardware. The decline is likely
    due to the higher computational load per second with more users, shifting the
    workload from being bandwidth-bound to compute-bound.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-5](#ch09_figure_5_1730130962952692) 展示了使用Llama 2-70B在FP16模式下在不同硬件上进行的推理过程的MBU。下降可能是由于每秒更高的计算负载，随着用户数量的增加，将工作负载从带宽限制转变为计算限制。'
- en: '![A graph of a number of users'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![用户数量的图表'
- en: Description automatically generated with medium confidence](assets/aien_0905.png)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成，中等置信度](assets/aien_0905.png)
- en: Figure 9-5\. Bandwidth utilization for Llama 2-70B in FP16 across three different
    chips shows a decrease in MBU as the number of concurrent users increases. Image
    from “LLM Training and Inference with Intel Gaudi 2 AI Accelerators” ([Databricks,
    2024](https://oreil.ly/tOOOD)).
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5\. Llama 2-70B在FP16模式下跨三个不同芯片的带宽利用率显示，随着并发用户数量的增加，MBU有所下降。图片来自“使用Intel Gaudi
    2 AI加速器进行LLM训练和推理”（[Databricks，2024](https://oreil.ly/tOOOD)）。
- en: Utilization metrics are helpful to track your system’s efficiency. Higher utilization
    rates for similar workloads on the same hardware generally mean that your services
    are becoming more efficient. However, *the goal isn’t to get the chips with the
    highest utilization*. What you really care about is how to get your jobs done
    faster and cheaper. A higher utilization rate means nothing if the cost and latency
    both increase.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 利用率指标有助于跟踪您系统的效率。在相同硬件上对类似工作负载的更高利用率通常意味着您的服务变得更加高效。然而，*目标不是获得利用率最高的芯片*。您真正关心的是如何更快、更便宜地完成任务。如果成本和延迟都增加，更高的利用率就没有意义了。
- en: AI Accelerators
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能加速器
- en: How fast and cheap software can run depends on the hardware it runs on. While
    there are optimization techniques that work across hardware, understanding hardware
    allows for deeper optimization. This section looks at hardware from an inference
    perspective, but it can be applied to training as well.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 软件运行速度快和便宜取决于它运行的硬件。虽然有一些适用于所有硬件的优化技术，但了解硬件可以允许进行更深入的优化。本节从推理的角度来看硬件，但也适用于训练。
- en: The development of AI models and hardware has always been intertwined. The lack
    of sufficiently powerful computers was one of the contributing factors to the
    first AI winter in the 1970s.^([13](ch09.html#id1649))
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能模型和硬件的发展始终是相互交织的。缺乏足够强大的计算机是20世纪70年代第一次人工智能冬天的一个促成因素。[13](ch09.html#id1649)
- en: The revival of interest in deep learning in 2012 was also closely tied to compute.
    One commonly acknowledged reason for the popularity of AlexNet ([Krizhevsky et
    al., 2012](https://oreil.ly/Yv4V7)) is that it was the first paper to successfully
    use [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit), graphics processing
    units, to train neural networks.^([14](ch09.html#id1650)) Before GPUs, if you
    wanted to train a model at AlexNet’s scale, you’d have to use thousands of CPUs,
    like the one [Google released just a few months before AlexNet](https://oreil.ly/Xpwco).
    Compared to thousands of CPUs, a couple of GPUs were a lot more accessible to
    PhD students and researchers, setting off the deep learning research boom.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年对深度学习的兴趣复苏也与计算紧密相关。公认的一个原因是AlexNet（[Krizhevsky等人，2012](https://oreil.ly/Yv4V7)）的流行，它是第一篇成功使用[GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)，即图形处理单元，来训练神经网络的论文。[14](ch09.html#id1650)
    在GPU出现之前，如果你想要训练与AlexNet规模相当的模型，你将不得不使用数千个CPU，就像谷歌在AlexNet发布前几个月发布的那个一样。[Google
    released just a few months before AlexNet](https://oreil.ly/Xpwco)。与数千个CPU相比，几块GPU对博士研究生和研究人员来说要容易获取得多，从而引发了深度学习研究的繁荣。
- en: What’s an accelerator?
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加速器是什么？
- en: An accelerator is a chip designed to accelerate a specific type of computational
    workload. An AI accelerator is designed for AI workloads. The dominant type of
    AI accelerator is GPUs, and the biggest economic driver during the AI boom in
    the early 2020s is undoubtedly NVIDIA.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 加速器是一种专为加速特定类型的计算工作负载而设计的芯片。人工智能加速器是为人工智能工作负载设计的。在2020年代初人工智能繁荣期间，最大的经济推动力无疑是NVIDIA。
- en: 'The main difference between CPUs and GPUs is that CPUs are designed for general-purpose
    usage, whereas GPUs are designed for parallel processing:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: CPU和GPU之间的主要区别在于，CPU是为通用用途设计的，而GPU是为并行处理设计的：
- en: CPUs have a few powerful cores, typically up to 64 cores for high-end consumer
    machines. While many CPU cores can handle multi-threaded workloads effectively,
    they excel at tasks requiring high single-thread performance, such as running
    an operating system, managing I/O (input/output) operations, or handling complex,
    sequential processes.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU拥有几个强大的核心，通常高端消费级机器可达64个核心。虽然许多CPU核心可以有效地处理多线程工作负载，但它们在需要高单线程性能的任务上表现出色，例如运行操作系统、管理I/O（输入/输出）操作或处理复杂、顺序的过程。
- en: GPUs have thousands of smaller, less powerful cores optimized for tasks that
    can be broken down into many smaller, independent calculations, such as graphics
    rendering and machine learning. The operation that constitutes most ML workloads
    is matrix multiplication, which is highly parallelizable.^([15](ch09.html#id1651))
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU拥有数千个较小、性能较弱的内核，这些内核针对可以将任务分解成许多较小、独立的计算的任务进行了优化，例如图形渲染和机器学习。构成大多数机器学习工作负载的操作是矩阵乘法，这是一种高度可并行化的操作。[15](ch09.html#id1651)
- en: While the pursuit of efficient parallel processing increases computational capabilities,
    it imposes challenges on memory design and power consumption.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然追求高效的并行处理增加了计算能力，但它对内存设计和功耗提出了挑战。
- en: The success of NVIDIA GPUs has inspired many accelerators designed to speed
    up AI workloads, including [Advanced Micro Devices (AMD)’s newer generations of
    GPUs](https://en.wikipedia.org/wiki/List_of_AMD_graphics_processing_units), Google’s
    TPU ([Tensor Processing Unit](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)),
    [Intel’s Habana Gaudi](https://oreil.ly/oDQOk), [Graphcore’s Intelligent Processing
    Unit](https://oreil.ly/6ySTY) (IPU), [Groq’s Language Processing Unit](https://oreil.ly/R7gXn)
    (LPU), [Cerebras’ Wafer-Scale](https://oreil.ly/ACIty) [Quant Processing Unit](https://en.wikipedia.org/wiki/List_of_quantum_processors)
    (QPU), and many more being introduced.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA GPU的成功激励了许多旨在加速人工智能工作负载的加速器，包括[高级微设备公司（AMD）的新一代GPU](https://en.wikipedia.org/wiki/List_of_AMD_graphics_processing_units)、谷歌的TPU（[Tensor
    Processing Unit](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)）、[英特尔的海巴纳·高迪](https://oreil.ly/oDQOk)、[Graphcore的智能处理单元](https://oreil.ly/6ySTY)（IPU）、[Groq的语言处理单元](https://oreil.ly/R7gXn)（LPU）、[Cerebras的晶圆级](https://oreil.ly/ACIty)
    [量子处理单元](https://en.wikipedia.org/wiki/List_of_quantum_processors)（QPU）以及许多其他正在推出的产品。
- en: While many chips can handle both training and inference, one big theme emerging
    is specialized chips for inference. A survey by [Desislavov et al. (2023)](https://oreil.ly/qSpMK)
    shares that inference can exceed the cost of training in commonly used systems,
    and that inference accounts for up to 90% of the machine learning costs for deployed
    AI systems.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多芯片可以处理训练和推理，但一个明显的趋势是针对推理的专用芯片。Desislavov等人（2023）的一项调查表明，在常用系统中，推理的成本可能超过训练成本，并且推理占到了部署的AI系统机器学习成本的90%以上。
- en: As discussed in [Chapter 7](ch07.html#ch07), training demands much more memory
    due to backpropagation and is generally more difficult to perform in lower precision.
    Furthermore, training usually emphasizes throughput, whereas inference aims to
    minimize latency.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第7章](ch07.html#ch07)所述，由于反向传播，训练需要更多的内存，通常在低精度下执行也更为困难。此外，训练通常强调吞吐量，而推理的目标是尽量减少延迟。
- en: Consequently, chips designed for inference are often optimized for lower precision
    and faster memory access, rather than large memory capacity. Examples of such
    chips include the Apple [Neural Engine](https://en.wikipedia.org/wiki/Neural_Engine),
    [AWS Inferentia](https://oreil.ly/42LSB), and [MTIA](https://oreil.ly/XH2bh) (Meta
    Training and Inference Accelerator). Chips designed for edge computing, like [Google’s
    Edge TPU](https://oreil.ly/m8daG) and the [NVIDIA Jetson Xavier](https://oreil.ly/PRZSQ),
    are also typically geared toward inference.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，针对推理设计的芯片通常优化为低精度和更快的内存访问，而不是大容量内存。这类芯片的例子包括苹果的[Neural Engine](https://en.wikipedia.org/wiki/Neural_Engine)、[AWS
    Inferentia](https://oreil.ly/42LSB)和[MTIA](https://oreil.ly/XH2bh)（Meta Training
    and Inference Accelerator）。针对边缘计算的芯片，如[Google的Edge TPU](https://oreil.ly/m8daG)和[NVIDIA
    Jetson Xavier](https://oreil.ly/PRZSQ)，通常也针对推理进行优化。
- en: There are also chips specialized for different model architectures, such as
    chips specialized for the transformer.^([16](ch09.html#id1652)) Many chips are
    designed for data centers, with more and more being designed for consumer devices
    (such as phones and laptops).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 还有针对不同模型架构的专用芯片，例如针对transformer的专用芯片。[16](ch09.html#id1652)。许多芯片是为数据中心设计的，而越来越多的芯片是为消费设备（如手机和笔记本电脑）设计的。
- en: Different hardware architectures have different memory layouts and specialized
    compute units that evolve over time. These units are optimized for specific data
    types, such as scalars, vectors, or tensors, as shown in [Figure 9-6](#ch09_figure_6_1730130962952710).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的硬件架构有不同的内存布局和随时间演变的专用计算单元。这些单元针对特定的数据类型进行了优化，例如标量、向量或tensor，如图[图9-6](#ch09_figure_6_1730130962952710)所示。
- en: '![A diagram of a computer'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机的示意图'
- en: Description automatically generated](assets/aien_0906.png)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0906.png)
- en: Figure 9-6\. Different compute primitives. Image inspired by [Chen et al. (2018)](https://arxiv.org/abs/1802.04799).
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6\. 不同的计算原语。图像灵感来自[Chen等人（2018）](https://arxiv.org/abs/1802.04799)。
- en: A chip might have a mixture of different compute units optimized for various
    data types. For example, GPUs traditionally supported vector operations, but many
    modern GPUs now include tensor cores optimized for matrix and tensor computations.
    TPUs, on the other hand, are designed with tensor operations as their primary
    compute primitive. To efficiently operate a model on a hardware architecture,
    its memory layout and compute primitives need to be taken into account.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一块芯片可能包含多种不同类型的计算单元，这些单元针对不同的数据类型进行了优化。例如，GPU传统上支持向量运算，但许多现代GPU现在包括针对矩阵和tensor计算的tensor核心。另一方面，TPU的设计以tensor运算为其主要的计算原语。为了在硬件架构上高效地运行模型，需要考虑其内存布局和计算原语。
- en: A chip’s specifications contain many details that can be useful when evaluating
    this chip for each specific use case. However, the main characteristics that matter
    across use cases are computational capabilities, memory size and bandwidth, and
    power consumption. I’ll use GPUs as examples to illustrate these characteristics.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 芯片的规格包含许多在评估每个特定用例时可能有用的细节。然而，跨越不同用例的主要特性是计算能力、内存大小和带宽以及功耗。我将使用GPU作为例子来说明这些特性。
- en: Computational capabilities
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算能力
- en: Computational capabilities are typically measured by the number of operations
    a chip can perform in a given time. The most common metric is *FLOP/s*, often
    written as FLOPS, which measures the *peak* number of floating-point operations
    per second. In reality, however, it’s very unlikely that an application can achieve
    this peak FLOP/s. The ratio between the actual FLOP/s and the theoretical FLOP/s
    is one *utilization* metric.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 计算能力通常通过芯片在给定时间内可以执行的操作数量来衡量。最常用的指标是 *FLOP/s*，通常写作FLOPS，它衡量每秒的峰值浮点运算次数。然而，实际上，应用程序达到这个峰值FLOP/s的可能性非常低。实际FLOP/s与理论FLOP/s之间的比率是一个
    *利用率* 指标。
- en: The number of operations a chip can perform in a second depends on the numerical
    precision—the higher the precision, the fewer operations the chip can execute.
    Think about how adding two 32-bit numbers generally requires twice the computation
    of adding two 16-bit numbers. The number of 32-bit operations a chip can perform
    in a given time is not exactly half that of 16-bit operations because of different
    chips’ optimization. For an overview of numerical precision, revisit [“Numerical
    Representations”](ch07.html#ch07b_numerical_representations_1730159634259493).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 芯片每秒可以执行的操作数量取决于数值精度——精度越高，芯片可以执行的操作就越少。想想看，加两个32位数字通常需要是加两个16位数字计算量的两倍。芯片在给定时间内可以执行的32位操作数量并不正好是16位操作数量的一半，因为不同芯片的优化不同。关于数值精度的概述，请回顾“数值表示”部分[“数值表示”](ch07.html#ch07b_numerical_representations_1730159634259493)。
- en: '[Table 9-2](#ch09_table_2_1730130962971057) shows the FLOP/s specs for different
    precision formats for [NVIDIA H100 SXM chips](https://oreil.ly/bNAOG).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[表9-2](#ch09_table_2_1730130962971057)显示了[NVIDIA H100 SXM芯片](https://oreil.ly/bNAOG)不同精度格式的FLOP/s规格。'
- en: Table 9-2\. FLOP/s specs for NVIDIA H100 SXM chips.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-2\. NVIDIA H100 SXM芯片的FLOP/s规格。
- en: '| Numerical precision | teraFLOP/s (trillion FLOP/s) with sparsity |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 数值精度 | 带稀疏性的teraFLOP/s（万亿FLOP/s）|'
- en: '| --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| TF32 Tensor Core^([a](ch09.html#id1658)) | 989 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| TF32 Tensor Core^([a](ch09.html#id1658)) | 989 |'
- en: '| BFLOAT16 Tensor Core | 1,979 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| BFLOAT16 Tensor Core | 1,979 |'
- en: '| FP16 Tensor Core | 1,979 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| FP16 Tensor Core | 1,979 |'
- en: '| FP8 Tensor Core | 3,958 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| FP8 Tensor Core | 3,958 |'
- en: '| ^([a](ch09.html#id1658-marker)) Recall from [Chapter 7](ch07.html#ch07) that
    TF32 is a 19-bit, not 32-bit, format. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch09.html#id1658-marker)) 回想一下[第7章](ch07.html#ch07)，TF32是一种19位格式，而不是32位格式。'
- en: Memory size and bandwidth
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存大小和带宽
- en: Because a GPU has many cores working in parallel, data often needs to be moved
    from the memory to these cores, and, therefore, data transfer speed is important.
    Data transfer is crucial when working with AI models that involve large weight
    matrices and training data. These large amounts of data need to be moved quickly
    to keep the cores efficiently occupied. Therefore, GPU memory needs to have higher
    bandwidth and lower latency than CPU memory, and thus, GPU memory requires more
    advanced memory technologies. This is one of the factors that makes GPU memory
    more expensive than CPU memory.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPU有许多并行工作的核心，数据通常需要从内存移动到这些核心，因此数据传输速度很重要。当处理涉及大型权重矩阵和训练数据的AI模型时，数据传输至关重要。这些大量数据需要快速移动，以保持核心的高效占用。因此，GPU内存需要比CPU内存具有更高的带宽和更低的延迟，因此GPU内存需要更先进的内存技术。这是使GPU内存比CPU内存更昂贵的一个因素。
- en: To be more specific, CPUs typically use [DDR SDRAM](https://en.wikipedia.org/wiki/DDR_SDRAM)
    (Double Data Rate Synchronous Dynamic Random-Access Memory), which has a 2D structure.
    GPUs, particularly high-end ones, often use [HBM](https://en.wikipedia.org/wiki/High_Bandwidth_Memory)
    (high-bandwidth memory), which has a 3D stacked structure.^([17](ch09.html#id1661))
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，CPU通常使用[DDR SDRAM](https://en.wikipedia.org/wiki/DDR_SDRAM)（双数据速率同步动态随机存取存储器），它具有二维结构。特别是高端GPU，通常使用[HBM](https://en.wikipedia.org/wiki/High_Bandwidth_Memory)（高带宽内存），它具有三维堆叠结构.^([17](ch09.html#id1661))
- en: 'An accelerator’s memory is measured by its *size and bandwidth*. These numbers
    need to be evaluated within the system an accelerator is part of. An accelerator,
    such as a GPU, typically interacts with three levels of memory, as visualized
    in [Figure 9-7](#ch09_figure_7_1730130962952731):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 加速器的内存通过其 *大小和带宽* 来衡量。这些数字需要在加速器所属的系统中进行评估。一个加速器，如GPU，通常与三个级别的内存交互，如图9-7所示：
- en: CPU memory (DRAM)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: CPU内存（DRAM）
- en: Accelerators are usually deployed alongside CPUs, giving them access to the
    CPU memory (also known as system memory, host memory, or just CPU DRAM).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 加速器通常与CPU一起部署，从而可以访问CPU内存（也称为系统内存、主机内存，或简称CPU DRAM）。
- en: CPU memory usually has the lowest bandwidth among these memory types, with data
    transfer speeds ranging from 25 GB/s to 50 GB/s. CPU memory size varies. Average
    laptops might have around 16–64 GB, whereas high-end workstations can have one
    TB or more.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: CPU内存通常在这些内存类型中具有最低的带宽，数据传输速度从25 GB/s到50 GB/s不等。CPU内存的大小也各不相同。平均笔记本电脑可能拥有大约16–64
    GB，而高端工作站可能拥有1 TB或更多。
- en: GPU high-bandwidth memory (HBM)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: GPU高带宽内存（HBM）
- en: This is the memory dedicated to the GPU, located close to the GPU for faster
    access than CPU memory.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是专门为GPU分配的内存，它位于GPU附近，以便比CPU内存更快地访问。
- en: HBM provides significantly higher bandwidth, with data transfer speeds typically
    ranging from 256 GB/s to over 1.5 TB/s. This speed is essential for efficiently
    handling large data transfers and high-throughput tasks. A consumer GPU has around
    24–80 GB of HBM.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: HBM提供了显著更高的带宽，数据传输速度通常在256 GB/s到超过1.5 TB/s之间。这种速度对于高效处理大量数据传输和高吞吐量任务至关重要。消费级GPU大约有24–80
    GB的HBM。
- en: GPU on-chip SRAM
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: GPU片上SRAM
- en: Integrated directly into the chip, this memory is used to store frequently accessed
    data and instructions for nearly instant access. It includes L1 and L2 caches
    made of SRAM, and, in some architectures, L3 caches as well. These caches are
    part of the broader on-chip memory, which also includes other components like
    register files and shared memory.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 直接集成到芯片中的这种内存用于存储频繁访问的数据和指令，以便几乎瞬间访问。它包括由SRAM制成的L1和L2缓存，在某些架构中还包括L3缓存。这些缓存是更广泛的片上内存的一部分，还包括其他组件，如寄存器文件和共享内存。
- en: RAM has extremely high data transfer speeds, often exceeding 10 TB/s. The size
    of GPU SRAM is small, typically 40 MB or under.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: RAM具有极高的数据传输速度，通常超过10 TB/s。GPU SRAM的大小较小，通常为40 MB或更少。
- en: '![A colorful pyramid with multiple layers'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个多层多彩的金字塔'
- en: Description automatically generated with medium confidence](assets/aien_0907.png)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成中等置信度的描述](assets/aien_0907.png)
- en: Figure 9-7\. The memory hierarchy of an AI accelerator. The numbers are for
    reference only. The actual numbers vary for each chip.
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7。AI加速器的内存层次结构。数字仅供参考。实际数字因芯片而异。
- en: A lot of GPU optimization is about how to make the most out of this memory hierarchy.
    However, as of this writing, popular frameworks such as PyTorch and TensorFlow
    don’t yet allow fine-grained control of memory access. This has led many AI researchers
    and engineers to become interested in GPU programming languages such as [CUDA](https://en.wikipedia.org/wiki/CUDA)
    (originally Compute Unified Device Architecture), [OpenAI’s Triton](https://github.com/triton-lang/triton),
    and [ROCm](https://github.com/ROCm/ROCm) (Radeon Open Compute). The latter is
    AMD’s open source alternative to NVIDIA’s proprietary CUDA.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 许多GPU优化都是关于如何最大限度地利用这个内存层次结构。然而，截至本文撰写时，流行的框架如PyTorch和TensorFlow还无法实现对内存访问的精细控制。这导致许多AI研究人员和工程师对GPU编程语言产生了兴趣，例如[CUDA](https://en.wikipedia.org/wiki/CUDA)（最初是Compute
    Unified Device Architecture）、[OpenAI的Triton](https://github.com/triton-lang/triton)和[ROCm](https://github.com/ROCm/ROCm)（Radeon
    Open Compute）。后者是AMD对NVIDIA专有CUDA的开源替代品。
- en: Power consumption
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 功耗
- en: Chips rely on transistors to perform computation. Each computation is done by
    transistors switching on and off, which requires energy. A GPU can have billions
    of transistors—an NVIDIA A100 has [54 billion](https://oreil.ly/5vRsP) transistors,
    while an NVIDIA H100 has [80 billion](https://en.wikipedia.org/wiki/Hopper_(microarchitecture)).
    When an accelerator is used efficiently, billions of transistors rapidly switch
    states, consuming a substantial amount of energy and generating a nontrivial amount
    of heat. This heat requires cooling systems, which also consume electricity, adding
    to data centers’ overall energy consumption.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 芯片依赖于晶体管来执行计算。每一次计算都是由晶体管的开和关来完成的，这需要能量。一个GPU可以拥有数十亿个晶体管——NVIDIA A100有[540亿个](https://oreil.ly/5vRsP)晶体管，而NVIDIA
    H100则有[800亿个](https://en.wikipedia.org/wiki/Hopper_(microarchitecture))。当加速器被高效使用时，数十亿个晶体管会快速切换状态，消耗大量的能量并产生相当数量的热量。这些热量需要冷却系统，而冷却系统本身也会消耗电力，从而增加了数据中心的总能耗。
- en: Chip energy consumption threatens to have a staggering impact on the [environment](https://oreil.ly/RqY-3),
    increasing the pressure on companies to invest in technologies for [green data
    centers](https://en.wikipedia.org/wiki/Green_data_center). An NVIDIA H100 running
    at its peak for a year consumes approximately 7,000 kWh. For comparison, the average
    US household’s annual electricity consumption is 10,000 kWh. That’s why electricity
    is a bottleneck to scaling up compute.^([18](ch09.html#id1669))
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 芯片能耗可能会对[环境](https://oreil.ly/RqY-3)产生惊人的影响，增加公司投资绿色数据中心技术的压力。NVIDIA H100以峰值运行一年大约消耗约7,000千瓦时。相比之下，美国普通家庭的年度电力消耗为10,000千瓦时。这就是为什么电力是计算扩展的瓶颈。[^18](ch09.html#id1669)
- en: Accelerators typically specify their power consumption under *maximum power
    draw* or a proxy metric *TDP (thermal design power):*
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 加速器通常会在*最大功耗*或代理指标*热设计功耗（TDP）*下指定其功耗：
- en: Maximum power draw indicates the peak power that the chip could draw under full
    load.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大功耗表示芯片在满载情况下可能消耗的峰值功率。
- en: '*TDP* represents the maximum heat a cooling system needs to dissipate when
    the chip operates under typical workloads. While it’s not an exact measure of
    power consumption, it’s an indication of the expected power draw. For CPUs and
    GPUs, the maximum power draw can be roughly 1.1 to 1.5 times the TDP, though the
    exact relationship varies depending on the specific architecture and workload.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TDP*代表芯片在典型工作负载下运行时，冷却系统需要散发的最大热量。虽然它不是功率消耗的精确度量，但它是对预期功耗的指示。对于CPU和GPU，最大功耗可以大致为TDP的1.1到1.5倍，但具体关系取决于特定的架构和工作负载。'
- en: If you opt for cloud providers, you won’t need to worry about cooling or electricity.
    However, these numbers can still be of interest to understand the impact of accelerators
    on the environment and the overall electricity demand.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择云服务提供商，你就不必担心冷却或电力问题。然而，这些数字仍然可以用来了解加速器对环境以及总体电力需求的影响。
- en: Inference Optimization
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理优化
- en: Inference optimization can be done at the model, hardware, or service level.
    To illustrate their differences, consider archery. Model-level optimization is
    like crafting better arrows. Hardware-level optimization is like training a stronger
    and better archer. Service-level optimization is like refining the entire shooting
    process, including the bow and aiming conditions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 推理优化可以在模型、硬件或服务级别进行。为了说明它们之间的差异，可以考虑射箭。模型级别的优化就像制作更好的箭。硬件级别的优化就像训练一个更强更好的射手。服务级别的优化就像完善整个射击过程，包括弓和瞄准条件。
- en: Ideally, optimizing a model for speed and cost shouldn’t change the model’s
    quality. However, many techniques might cause model degradation. [Figure 9-8](#ch09_figure_8_1730130962952759)
    shows the same Llama models’ performance on different benchmarks, served by different
    inference service providers.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，为了速度和成本优化模型，不应改变模型的质量。然而，许多技术可能会引起模型退化。[图9-8](#ch09_figure_8_1730130962952759)显示了同一Llama模型在不同基准测试中的性能，由不同的推理服务提供商提供。
- en: '![A graph of different types of numbers'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '![不同类型数字的图表'
- en: Description automatically generated with medium confidence](assets/aien_0908.png)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成，置信度中等](assets/aien_0908.png)
- en: Figure 9-8\. An inference service provider might use optimization techniques
    that can alter a model’s behavior, causing different providers to have slight
    model quality variations. The experiment was conducted by [Cerebras (2024)](https://oreil.ly/5hFSF).
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. 推理服务提供商可能会使用可以改变模型行为的优化技术，导致不同提供商的模型质量略有差异。该实验由[Cerebras (2024)](https://oreil.ly/5hFSF)进行。
- en: Since hardware design is outside the scope of this book, I’ll discuss techniques
    at the model and service levels. While the techniques are discussed separately,
    keep in mind that, in production, optimization typically involves techniques at
    more than one level.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于硬件设计超出了本书的范围，我将讨论模型和服务级别的技术。虽然技术是分别讨论的，但请记住，在生产中，优化通常涉及多个级别的技术。
- en: Model Optimization
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型优化
- en: 'Model-level optimization aims to make the model more efficient, often by modifying
    the model itself, which can alter its behavior. As of this writing, many foundation
    models follow the transformer architecture and include an autoregressive language
    model component. These models have three characteristics that make inference resource-intensive:
    model size, autoregressive decoding, and the attention mechanism. Let’s discuss
    approaches to address these challenges.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Model compression
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Model compression involves techniques that reduce a model’s size. Making a
    model smaller can also make it faster. This book has already discussed two model
    compression techniques: quantization and distillation. Quantization, reducing
    the precision of a model to reduce its memory footprint and increase its throughput,
    is discussed in [Chapter 7](ch07.html#ch07). Model distillation, training a small
    model to mimic the behavior of the large model, is discussed in [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Model distillation suggests that it’s possible to capture a large model’s behaviors
    using fewer parameters. Could it be that within the large model, there exists
    a subset of parameters capable of capturing the entire model’s behavior? This
    is the core concept behind pruning.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Pruning, in the context of neural networks, has two meanings. One is to remove
    entire nodes of a neural network, which means changing its architecture and reducing
    its number of parameters. Another is to find parameters least useful to predictions
    and set them to zero. In this case, pruning doesn’t reduce the total number of
    parameters, only the number of non-zero parameters. This makes the model more
    sparse, which both reduces the model’s storage space and speeds up computation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Pruned models can be used as-is or be further finetuned to adjust the remaining
    parameters and restore any performance degradation caused by the pruning process.
    Pruning can help discover promising model architectures ([Liu et al., 2018](https://arxiv.org/abs/1810.05270)).
    These pruned architectures, smaller than the pre-pruned architectures, can also
    be trained from scratch ([Zhu et al., 2017](https://arxiv.org/abs/1710.01878)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In the literature, there have been many encouraging pruning results. For example,
    [Frankle and Carbin (2019)](https://oreil.ly/qwlHE) showed that pruning techniques
    can reduce the non-zero parameter counts of certain trained networks by over 90%,
    decreasing memory footprints and improving speed without compromising accuracy.
    However, in practice, as of this writing, pruning is less common. It’s harder
    to do, as it requires an understanding of the original model’s architecture, and
    the performance boost it can bring is often much less than that of other approaches.
    Pruning also results in sparse models, and not all hardware architectures are
    designed to take advantage of the resulting sparsity.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '*Weight-only quantization is by far the most popular approach since it’s easy
    to use, works out of the box for many models, and is extremely effective.* Reducing
    a model’s precision from 32 bits to 16 bits reduces its memory footprint by half.
    However, we’re close to the limit of quantization—we can’t go lower than 1 bit
    per value. Distillation is also common because it can result in a smaller model
    whose behavior is comparative to that of a much larger one for your needs.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*权重量化是迄今为止最受欢迎的方法，因为它易于使用，对于许多模型来说开箱即用，并且非常有效。*将模型的精度从32位降低到16位可以将内存占用减半。然而，我们接近量化的极限——我们无法低于每个值1位。蒸馏也很常见，因为它可以产生一个更小的模型，其行为与一个更大的模型相比，对于你的需求来说是可以比较的。'
- en: Overcoming the autoregressive decoding bottleneck
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 克服自回归解码瓶颈
- en: As discussed in [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359),
    autoregressive language models generate one token after another. If it takes 100
    ms to generate one token, a response of 100 tokens will take 10 s.^([19](ch09.html#id1683))
    This process is not just slow, it’s also expensive. Across model API providers,
    an output token costs approximately two to four times an input token. In an experiment,
    Anyscale found that a single output token can have the same impact on latency
    as 100 input tokens ([Kadous et al., 2023](https://oreil.ly/QYdG8)). Improving
    the autoregressive generation process by a small percentage can significantly
    improve user experience.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在[第二章](ch02.html#ch02_understanding_foundation_models_1730147895571359)中讨论的那样，自回归语言模型是逐个生成标记的。如果生成一个标记需要100毫秒，那么生成100个标记的响应将需要10秒。[19](ch09.html#id1683)这个过程不仅慢，而且成本高昂。在模型API提供商之间，输出标记的成本大约是输入标记的两到四倍。在一个实验中，Anyscale发现单个输出标记对延迟的影响可以与100个输入标记相当([Kadous等人，2023](https://oreil.ly/QYdG8))。通过提高自回归生成过程的小幅百分比可以显著改善用户体验。
- en: As the space is rapidly evolving, new techniques are being developed to overcome
    this seemingly impossible bottleneck. Perhaps one day, there will be architectures
    that don’t have this bottleneck. The techniques covered here are to illustrate
    what the solution might look like, but the techniques are still evolving.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 由于空间正在迅速发展，新的技术正在被开发以克服这个看似不可能的瓶颈。也许有一天，会出现没有这种瓶颈的架构。这里介绍的技术是为了说明解决方案可能的样子，但这些技术仍在不断发展。
- en: Speculative decoding
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 投机解码
- en: Speculative decoding (also called speculative sampling) uses a faster but less
    powerful model to generate a sequence of tokens, which are then verified by the
    target model. The target model is the model you want to use. The faster model
    is called the draft or proposal model because it proposes the draft output.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 投机解码（也称为投机采样）使用一个更快但功能较弱的模型来生成一系列标记，然后由目标模型进行验证。目标模型是你想要使用的模型。较快的模型被称为草稿或提案模型，因为它提出了草稿输出。
- en: 'Imagine the input tokens are *x*[1], *x*[2], …, *x*[*t*]:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输入标记是*x*[1]，*x*[2]，…，*x*[*t*]：
- en: 'The draft model generates a sequence of *K* tokens: *x*[*t* + 1], *x*[*t* +
    2], …, *x*[*t* + *K*].'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 草稿模型生成*K*个标记的序列：*x*[*t* + 1]，*x*[*t* + 2]，…，*x*[*t* + *K*]。
- en: The target model verifies these *K* generated tokens in parallel.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标模型并行验证这些*K*个生成的标记。
- en: The target model *accepts* the longest subsequence of draft tokens, from left
    to right, which the target model agrees to use.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标模型*接受*从左到右的最长草稿标记子序列，这是目标模型同意使用的。
- en: Let’s say the target model accepts *j* draft tokens, *x*[*t* + 1], *x*[*t* +
    2], …, *x*[*t* + *j*]. The target model then generates one extra token, *x*[*t*
    + *j* + 1].
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设目标模型接受*j*个草稿标记，*x*[*t* + 1]，*x*[*t* + 2]，…，*x*[*t* + *j*]。然后目标模型再生成一个额外的标记，*x*[*t*
    + *j* + 1]。
- en: The process returns to step 1, with the draft model generating *K* tokens conditioned
    on *x*[1], *x*[2], …, *x*[*t*], *x*[*t* + 1], *x*[*t* + 2], …, *x*[*t* + *j*].
    The process is visualized in [Figure 9-9](#ch09_figure_9_1730130962952786).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 过程回到步骤1，草稿模型在*x*[1]，*x*[2]，…，*x*[*t*]，*x*[*t* + 1]，*x*[*t* + 2]，…，*x*[*t* +
    *j*]的条件下生成*K*个标记。这个过程在[图9-9](#ch09_figure_9_1730130962952786)中进行了可视化。
- en: If no draft token is accepted, this loop produces only one token generated by
    the target model. If all draft tokens are accepted, this loop produces *K* + 1
    tokens, with *K* generated by the draft model and one by the target model.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有草稿标记被接受，这个循环只产生目标模型生成的一个标记。如果所有草稿标记都被接受，这个循环将产生*K* + 1个标记，其中*K*个由草稿模型生成，一个由目标模型生成。
- en: '![A diagram of words'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![单词的图示'
- en: Description automatically generated with medium confidence](assets/aien_0909.png)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成中等置信度的描述](assets/aien_0909.png)
- en: Figure 9-9\. A draft model generates a sequence of K tokens, and the main model
    accepts the longest subsequence that it agrees with. The image is from “Blockwise
    Parallel Decoding for Deep Autoregressive Models” ([Stern et al., 2018](https://arxiv.org/abs/1811.03115)).
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9。草稿模型生成K个标记的序列，主模型接受它同意的最长子序列。该图像来自“Blockwise Parallel Decoding for Deep
    Autoregressive Models”（[Stern et al., 2018](https://arxiv.org/abs/1811.03115)）。
- en: 'If all draft sequences are rejected, the target model must generate the entire
    response in addition to verifying it, potentially leading to increased latency.
    However, this can be avoided because of these three insights:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有草稿序列都被拒绝，目标模型必须生成整个响应并进行验证，这可能导致延迟增加。然而，可以通过以下三个见解来避免这种情况：
- en: The time it takes for the target model to verify a sequence of tokens is less
    than the time it takes to generate it, because verification is parallelizable,
    while generation is sequential. Speculative decoding effectively turns the computation
    profile of decoding into that of prefilling.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标模型验证一个标记序列所需的时间少于生成它所需的时间，因为验证是可并行的，而生成是顺序的。推测解码有效地将解码的计算特征转变为预填充的计算特征。
- en: In an output token sequence, some tokens are easier to predict than others.
    It’s possible to find a weaker draft model capable of getting these easier-to-predict
    tokens right, leading to a high acceptance rate of the draft tokens.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在输出标记序列中，有些标记比其他标记更容易预测。可以找到一个能力较弱的草稿模型，能够正确预测这些容易预测的标记，从而提高草稿标记的接受率。
- en: Decoding is memory bandwidth-bound, which means that during the coding process,
    there are typically idle FLOPs that can be used for free verification.^([20](ch09.html#id1684))
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码受内存带宽限制，这意味着在编码过程中，通常有可以免费用于验证的空闲FLOPs。^([20](ch09.html#id1684))
- en: Acceptance rates are domain-dependent. For texts that follow specific structures
    like code, the acceptance rate is typically higher. Larger values of *K* mean
    fewer verifying calls for the target model but a low acceptance rate of the draft
    tokens. The draft model can be of any architecture, though ideally it should share
    the same vocabulary and tokenizer as the target model. You can train a custom
    draft model or use an existing weaker model.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接受率取决于领域。对于遵循特定结构（如代码）的文本，接受率通常更高。较大的*K*值意味着目标模型需要更少的验证调用，但草稿标记的接受率较低。草稿模型可以是任何架构，尽管理想情况下它应该与目标模型共享相同的词汇表和分词器。您可以训练一个定制的草稿模型或使用现有的较弱模型。
- en: For example, to speed up the decoding process of Chinchilla-70B, DeepMind trained
    a 4B-parameter draft model of the same architecture ([Chen et al., 2023](https://arxiv.org/abs/2302.01318)).
    The draft model can generate a token eight times faster than the target model
    (1.8 ms/token compared to 14.1 ms/token). This reduces the overall response latency
    by more than half without compromising response quality. A similar speed-up was
    achieved for T5-XXL ([Laviathan et al., 2022](https://arxiv.org/abs/2211.17192)).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了加快Chinchilla-70B的解码过程，DeepMind训练了一个相同架构的4B参数草稿模型（[Chen et al., 2023](https://arxiv.org/abs/2302.01318)）。草稿模型可以比目标模型快八倍生成标记（1.8
    ms/token与14.1 ms/token相比）。这在不影响响应质量的情况下将整体响应延迟减少了超过一半。T5-XXL也实现了类似的加速效果（[Laviathan
    et al., 2022](https://arxiv.org/abs/2211.17192)）。
- en: This approach has gained traction because it’s relatively easy to implement
    and doesn’t change a model’s quality. For example, it’s possible to do so in [50
    lines of code in PyTorch](https://oreil.ly/IaPOB). It’s been incorporated into
    popular inference frameworks such as [vLLM](https://oreil.ly/uzg1s), [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    and [llama.cpp](https://github.com/ggerganov/llama.cpp/pull/2926).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法之所以受到青睐，是因为它相对容易实现，并且不会改变模型的质量。例如，在PyTorch中，可以在[50行代码](https://oreil.ly/IaPOB)内实现。它已被纳入流行的推理框架，如[vLLM](https://oreil.ly/uzg1s)、[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)和[llama.cpp](https://github.com/ggerganov/llama.cpp/pull/2926)。
- en: Inference with reference
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于参考的推理
- en: Often, a response needs to reference tokens from the input. For example, if
    you ask your model a question about an attached document, the model might repeat
    a chunk of text verbatim from the document. Another example is if you ask the
    model to fix bugs in a piece of code, the model might reuse the majority of the
    original code with minor changes. Instead of making the model generate these repeated
    tokens, what if we copy these tokens from the input to speed up the generation?
    This is the core idea behind inference with reference.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个响应需要引用输入中的标记。例如，如果你向你的模型提出有关附件文档的问题，模型可能会从文档中逐字重复一段文本。另一个例子是，如果你要求模型修复代码中的错误，模型可能会对原始代码进行少量修改后重用大部分代码。而不是让模型生成这些重复的标记，我们为什么不从输入中复制这些标记来加速生成呢？这就是带有参考的推理背后的核心思想。
- en: Inference with reference is similar to speculative decoding, but instead of
    using a model to generate draft tokens, it selects draft tokens from the input.
    The key challenge is to develop an algorithm to identify the most relevant text
    span from the context at each decoding step. The simplest option is to find a
    text span that matches the current tokens.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 带有参考的推理类似于推测性解码，但它不是使用模型生成草稿标记，而是从输入中选择草稿标记。关键挑战是开发一个算法，在每个解码步骤中识别与上下文最相关的文本跨度。最简单的方法是找到一个与当前标记匹配的文本跨度。
- en: 'Unlike speculative decoding, inference with reference doesn’t require an extra
    model. However, it’s useful only in generation scenarios where there’s a significant
    overlap between contexts and outputs, such as in retrieval systems, coding, or
    multi-turn conversations. In “Inference with Reference: Lossless Acceleration
    of Large Language Models” ([Yang et al., 2023](https://arxiv.org/abs/2304.04487)),
    this technique helps achieve two times generation speedup in such use cases.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 与推测性解码不同，带有参考的推理不需要额外的模型。然而，它仅在存在显著重叠的上下文和输出场景中才有用，例如在检索系统、编码或多轮对话中。在“带有参考的推理：大型语言模型的无损加速”（[杨等人，2023](https://arxiv.org/abs/2304.04487)）中，这项技术帮助在这些用例中实现了两倍的生产速度提升。
- en: Examples of how inference with reference works are shown in [Figure 9-10](#ch09_figure_10_1730130962952808).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 带有参考的推理的工作示例显示在[图9-10](#ch09_figure_10_1730130962952808)中。
- en: '![A screenshot of a diagram'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图表的截图'
- en: Description automatically generated](assets/aien_0910.png)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0910.png)
- en: Figure 9-10\. Two examples of inference with reference. The text spans that
    are successfully copied from the input are in red and green. Image from Yang et
    al. (2023). The image is licensed under CC BY 4.0.
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10. 两个带有参考的推理示例。成功从输入复制的文本跨度用红色和绿色表示。图片来自杨等人（2023年）。图片许可协议为CC BY 4.0。
- en: Parallel decoding
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 并行解码
- en: Instead of making autoregressive generation faster with draft tokens, some techniques
    aim to break the sequential dependency. Given an existing sequence of tokens *x*[1],
    *x*[2],…,*x*[*t*], these techniques attempt to generate *x*[*t* + 1], *x*[*t*
    + 2],…,*x*[*t* + *k*] simultaneously. This means that the model generates *x*[*t*
    + 2] before it knows that the token before it is *x*[*t* + 1].
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用草稿标记使自回归生成更快不同，一些技术旨在打破序列依赖性。给定一个标记序列 *x*[1]，*x*[2]，…，*x*[*t*]，这些技术试图同时生成
    *x*[*t* + 1]，*x*[*t* + 2]，…，*x*[*t* + *k*]。这意味着模型在知道它前面的标记是 *x*[*t* + 1] 之前就生成
    *x*[*t* + 2]。
- en: This can work because the knowledge of the existing sequence often is sufficient
    to predict the next few tokens. For example, given “the cat sits”, without knowing
    that the next token is “on”, “under”, or “behind”, you might still predict that
    the word after it is “the”.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这之所以可行，是因为现有序列的知识通常足以预测接下来的几个标记。例如，给定“the cat sits”，即使不知道下一个标记是“on”、“under”还是“behind”，你仍然可以预测它后面的词是“the”。
- en: The parallel tokens can be generated by the same decoder, as in Lookahead decoding
    ([Fu et al., 2024](https://arxiv.org/abs/2402.02057)), or by different decoding
    heads, as in Medusa ([Cai et al., 2024](https://arxiv.org/abs/2401.10774)). In
    Medusa, the original model is extended with multiple decoding heads, and each
    head is a small neural network layer that is then trained to predict a future
    token at a specific position. If the original model is trained to predict the
    next token *x*[*t* + 1], the *k*^(*th*) head will predict the token *x*[*t* +
    *k* + 1]. These heads are trained together with the original model, but the original
    model is frozen. NVIDIA claimed Medusa helped boost Llama 3.1 token generation
    by up to 1.9× on their HGX H200 GPUs ([Eassa et al., 2024](https://oreil.ly/FWYf5)).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 并行标记可以由相同的解码器生成，如Lookahead解码([Fu et al., 2024](https://arxiv.org/abs/2402.02057))，或者由不同的解码头生成，如Medusa([Cai
    et al., 2024](https://arxiv.org/abs/2401.10774))。在Medusa中，原始模型扩展了多个解码头，每个头都是一个小的神经网络层，然后训练以预测特定位置的未来标记。如果原始模型被训练来预测下一个标记*x*[*t*
    + 1]，则*k*^(*th*)个头将预测标记*x*[*t* + *k* + 1]。这些头与原始模型一起训练，但原始模型被冻结。NVIDIA声称Medusa帮助在他们的HGX
    H200 GPU上提高了Llama 3.1标记生成的速度，最高可达1.9倍([Eassa et al., 2024](https://oreil.ly/FWYf5))。
- en: 'However, because these tokens aren’t generated sequentially, they need to be
    verified to make sure that they fit together. An essential part of parallel decoding
    is verification and integration. Lookahead decoding uses the [Jacobi method](https://en.wikipedia.org/wiki/Jacobi_method)^([21](ch09.html#id1694))
    to verify the generated tokens, which works as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，因为这些标记不是按顺序生成的，所以需要验证以确保它们可以组合在一起。并行解码的一个关键部分是验证和整合。Lookahead解码使用[Jacobi方法](https://en.wikipedia.org/wiki/Jacobi_method)^([21](ch09.html#id1694))来验证生成的标记，其工作原理如下：
- en: K future tokens are generated in parallel.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并行生成K个未来标记。
- en: These *K* tokens are verified for coherence and consistency with the context.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些*K*个标记被验证以确保它们与上下文的一致性和连贯性。
- en: If one or more tokens fail verification, instead of aggregating all *K* future
    tokens, the model regenerates or adjusts only these failed tokens.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个或多个标记验证失败，模型将不会聚合所有*K*个未来标记，而是仅重新生成或调整这些失败的标记。
- en: The model keeps refining the generated tokens until they all pass verification
    and are integrated into the final output. This family of parallel decoding algorithms
    is also called Jacobi decoding.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 模型不断优化生成的标记，直到它们全部通过验证并整合到最终输出中。这一系列并行解码算法也被称为Jacobi解码。
- en: On the other hand, Medusa uses a tree-based attention mechanism to verify and
    integrate tokens. Each Medusa head produces several options for each position.
    These options are then organized into a tree-like structure to select the most
    promising combination. The process is visualized in [Figure 9-11](#ch09_figure_11_1730130962952823).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Medusa使用基于树的注意力机制来验证和整合标记。每个Medusa头为每个位置生成几个选项。然后，这些选项被组织成树状结构以选择最有希望的组合。该过程在[图9-11](#ch09_figure_11_1730130962952823)中进行了可视化。
- en: '![A diagram of a model'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '![模型图'
- en: Description automatically generated](assets/aien_0911.png)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0911.png)
- en: Figure 9-11\. In Medusa (Cai et al., 2024), each head predicts several options
    for a token position. The most promising sequence from these options is selected.
    Image adapted from the paper, which is licensed under CC BY 4.0.
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-11\. 在Medusa（Cai et al., 2024）中，每个头为标记位置预测几个选项。从这些选项中选择最有希望的序列。图像改编自论文，该论文受CC
    BY 4.0许可。
- en: While the perspective of being able to circumvent sequential dependency is appealing,
    parallel decoding is not intuitive, and some techniques, like Medusa, can be challenging
    to implement.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然能够绕过序列依赖性的观点很有吸引力，但并行解码并不直观，一些技术，如Medusa，可能难以实现。
- en: Attention mechanism optimization
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力机制优化
- en: 'Recall from [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359)
    that generating the next token requires the key and value vectors for all previous
    tokens. This means that the following applies:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第2章](ch02.html#ch02_understanding_foundation_models_1730147895571359)回顾，生成下一个标记需要所有先前标记的关键和值向量。这意味着以下适用：
- en: Generating token *x*[*t*] requires the key and value vectors for tokens *x*[1],
    *x*[2], …, *x*[*t* – 1].
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成标记*x*[*t*]需要标记*x*[1]、*x*[2]、…、*x*[*t* – 1]的关键和值向量。
- en: Generating token *x*[*t* + 1] requires the key and value vectors for tokens
    *x*[1], *x*[2], …,*x*[*t* – 1], *x*[*t*].
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成标记*x*[*t* + 1]需要标记*x*[1]、*x*[2]、…、*x*[*t* – 1]、*x*[*t*]的关键和值向量。
- en: When generating token *x*[*t* + 1], instead of computing the key and value vectors
    for tokens *x*[1], *x*[2], …, *x*[*t* – 1] again, you reuse these vectors from
    the previous step. This means that you’ll need to compute the key and value vectors
    for only the most recent token, *x*[*t*]. The cache that stores key and value
    vectors for reuse is called the KV cache. The newly computed key and value vectors
    are then added to the KV cache, which is visualized in [Figure 9-12](#ch09_figure_12_1730130962952844).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成标记 *x*[*t* + 1] 时，而不是再次计算标记 *x*[1]、*x*[2]、…、*x*[*t* – 1] 的键和值向量，你可以从上一步重新使用这些向量。这意味着你只需要计算最近标记
    *x*[*t*] 的键和值向量。用于存储可重复使用键和值向量的缓存称为 KV 缓存。然后，新计算出的键和值向量被添加到 KV 缓存中，这在 [图 9-12](#ch09_figure_12_1730130962952844)
    中进行了可视化。
- en: '![A diagram of a graph'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图示'
- en: Description automatically generated](assets/aien_0912.png)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0912.png)
- en: Figure 9-12\. To avoid recomputing the key and value vectors at each decoding
    step, use a KV cache to store these vectors to reuse.
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-12\. 为了避免在每次解码步骤中重新计算键和值向量，使用 KV 缓存来存储这些向量以供重复使用。
- en: Note
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A KV cache is used only during inference, not training. During training, because
    all tokens in a sequence are known in advance, next token generation can be computed
    all at once instead of sequentially, as during inference. Therefore, there’s no
    need for a KV cache.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: KV 缓存仅在推理期间使用，不在训练期间使用。在训练期间，由于序列中的所有标记都是预先已知的，因此可以一次性计算下一个标记的生成，而不是像推理期间那样按顺序计算。因此，不需要
    KV 缓存。
- en: Because generating a token requires computing the attention scores with all
    previous tokens, the number of attention computations grows exponentially with
    sequence length.^([22](ch09.html#id1700)) The KV cache size, on the other hand,
    grows linearly with sequence length.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成一个标记需要计算所有先前标记的注意力分数，因此注意力计算的数量会随着序列长度的增加而呈指数增长.^([22](ch09.html#id1700))
    相反，KV 缓存的大小会随着序列长度的增加而线性增长。
- en: The KV cache size also grows with larger batch sizes. A Google paper calculated
    that for a 500B+ model with multi-head attention, batch size 512, and context
    length 2048, the KV cache totals 3TB [(Pope et al., 2022)](https://arxiv.org/abs/2211.05102).
    This is three times the size of that model’s weights.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: KV 缓存的大小也会随着批量大小的增加而增长。一篇谷歌论文计算得出，对于一个具有多头注意力的 500B+ 模型，批大小为 512，上下文长度为 2048，KV
    缓存总量为 3TB [(Pope 等人，2022)](https://arxiv.org/abs/2211.05102)。这是该模型权重大小的三倍。
- en: The KV cache size is ultimately limited by the available hardware storage, creating
    a bottleneck for running applications with long context. A large cache size also
    takes time to load into memory, which can be an issue for applications with strict
    latency.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: KV 缓存的大小最终受可用硬件存储的限制，这为运行具有长上下文的应用程序创造了瓶颈。大缓存大小也需要时间加载到内存中，这对于具有严格延迟要求的应用程序可能是一个问题。
- en: The computation and memory requirements of the attention mechanism are one of
    the reasons why it’s so hard to have longer context.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的计算和内存需求是难以实现更长上下文的原因之一。
- en: 'Many techniques have been developed to make the attention mechanism more efficient.
    In general, they fall into three buckets: redesigning the attention mechanism,
    optimizing the KV cache, and writing kernels for attention computation.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发了许多技术来使注意力机制更加高效。总的来说，它们可以分为三个类别：重新设计注意力机制、优化 KV 缓存和为注意力计算编写内核。
- en: Redesigning the attention mechanism
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重新设计注意力机制
- en: These techniques involve altering how the attention mechanism works. Even though
    these techniques help optimize inference, because they change a model’s architecture
    directly, they can be applied only during training or finetuning.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术涉及改变注意力机制的工作方式。尽管这些技术有助于优化推理，但由于它们直接改变了模型的架构，因此只能在训练或微调期间应用。
- en: For example, when generating a new token, instead of attending to all previous
    tokens, *local windowed attention* attends only to a fixed size window of nearby
    tokens ([Beltagy et al., 2020](https://arxiv.org/abs/2004.05150v2)). This reduces
    the effective sequence length to a fixed size window, reducing both the KV cache
    and the attention computation. If the average sequence length is 10,000 tokens,
    attending to a window size of 1,000 tokens reduces the KV cache size by 10 times.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在生成新标记时，不是关注所有先前标记，而是 *局部窗口注意力* 只关注固定大小的附近标记 ([Beltagy 等人，2020](https://arxiv.org/abs/2004.05150v2))。这将有效序列长度减少到固定大小的窗口，从而减少了
    KV 缓存和注意力计算。如果平均序列长度为 10,000 个标记，关注 1,000 个标记的窗口大小可以将 KV 缓存大小减少 10 倍。
- en: Local windowed attention can be interleaved with global attention, with local
    attention capturing nearby context; the global attention captures task-specific
    information across the document.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 局部窗口注意力可以与全局注意力交错，局部注意力捕捉附近的上下文；全局注意力捕捉文档中的任务特定信息。
- en: Both *cross-layer attention* ([Brandon et al., 2024](https://arxiv.org/abs/2405.12981?ref=research.character.ai))
    and *multi-query attention* ([Shazeer, 2019](https://arxiv.org/abs/1911.02150?ref=research.character.ai))
    reduce the memory footprint of the KV cache by reducing the number of key-value
    pairs. Cross-layer attention shares key and value vectors across adjacent layers.
    Having three layers sharing the same key-value vectors means reducing the KV cache
    three times. On the other hand, multi-query attention shares key-value vectors
    across query heads.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*跨层注意力* ([Brandon 等人，2024](https://arxiv.org/abs/2405.12981?ref=research.character.ai))
    和 *多查询注意力* ([Shazeer，2019](https://arxiv.org/abs/1911.02150?ref=research.character.ai))
    通过减少键值对的数量来降低 KV 缓存的内存占用。跨层注意力在相邻层之间共享键和值向量。三层共享相同的键值向量意味着将 KV 缓存减少了三次。另一方面，多查询注意力在查询头之间共享键值向量。'
- en: '*Grouped-query attention* ([Ainslie et al., 2023](https://arxiv.org/abs/2305.13245))
    is a generalization of multi-query attention. Instead of using only one set of
    key-value pairs for all query heads, its grouped-query attention puts query heads
    into smaller groups and shares key-value pairs only among query heads in the same
    group. This allows for a more flexible balance between the number of query heads
    and the number of key-value pairs.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '*分组查询注意力* ([Ainslie 等人，2023](https://arxiv.org/abs/2305.13245)) 是多查询注意力的一种推广。它不是为所有查询头使用一组键值对，而是将查询头放入更小的组中，并且只在同一组内的查询头之间共享键值对。这允许在查询头的数量和键值对的数量之间实现更灵活的平衡。'
- en: Character.AI, an AI chatbot application, shares that their average conversation
    has a dialogue history of [180 messages](https://oreil.ly/nLt6A) (2024). Given
    the typically long sequences, the primary bottleneck for inference throughput
    is the KV cache size. Three attention mechanism designs—multi-query attention,
    interleaving local attention and global attention, and cross-layer attention—help
    them *reduce KV cache by over 20 times*. More importantly, this significant KV
    cache reduction means that memory is no longer a bottleneck for them for serving
    large batch sizes.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: AI 聊天机器人应用 Character.AI 表示，他们的平均对话历史有 [180 条消息](https://oreil.ly/nLt6A)（2024）。鉴于通常较长的序列，推理吞吐量的主要瓶颈是
    KV 缓存大小。三种注意力机制设计——多查询注意力、交错局部和全局注意力以及跨层注意力——帮助他们 *将 KV 缓存减少了超过 20 倍*。更重要的是，这种显著的
    KV 缓存减少意味着对于服务大型批量数据，内存不再是他们的瓶颈。
- en: Optimizing the KV cache size
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化 KV 缓存大小
- en: The way the KV cache is managed is critical in mitigating the memory bottleneck
    during inference and enabling a larger batch size, especially for applications
    with long context. Many techniques are actively being developed to reduce and
    manage the KV cache.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 管理KV缓存的方式对于缓解推理过程中的内存瓶颈和实现更大的批量大小至关重要，特别是对于具有长上下文的应用。许多技术正在积极开发中，以减少和管理 KV 缓存。
- en: One of the fastest growing inference frameworks, [vLLM](https://github.com/vllm-project/vllm),
    gained popularity for introducing PagedAttention, which optimizes memory management
    by dividing the KV cache into non-contiguous blocks, reducing fragmentation, and
    enabling flexible memory sharing to improve LLM serving efficiency ([Kwon et al.,
    2023](https://arxiv.org/abs/2309.06180)).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最快增长的推理框架之一 [vLLM](https://github.com/vllm-project/vllm) 因引入 PagedAttention
    而受到欢迎，它通过将 KV 缓存划分为非连续块来优化内存管理，减少碎片，并实现灵活的内存共享，从而提高 LLM 服务效率 ([Kwon 等人，2023](https://arxiv.org/abs/2309.06180))。
- en: Other techniques include KV cache quantization ([Hooper et al., 2024](https://arxiv.org/abs/2401.18079);
    [Kang et al., 2024](https://arxiv.org/abs/2403.05527)), adaptive KV cache compression
    ([Ge et al., 2023](https://arxiv.org/abs/2310.01801)), and selective KV cache
    ([Liu et al., 2024](https://oreil.ly/ixtBl)).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 其他技术包括 KV 缓存量化 ([Hooper 等人，2024](https://arxiv.org/abs/2401.18079)；[Kang 等人，2024](https://arxiv.org/abs/2403.05527))、自适应
    KV 缓存压缩 ([Ge 等人，2023](https://arxiv.org/abs/2310.01801)) 和选择性 KV 缓存 ([Liu 等人，2024](https://oreil.ly/ixtBl))。
- en: Writing kernels for attention computation
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编写注意力计算内核
- en: Instead of changing the mechanism design or optimizing the storage, this approach
    looks into how attention scores are computed and finds ways to make this computation
    more efficient. This approach is the most effective when it takes into account
    the hardware executing the computation. The code optimized for a specific chip
    is called a kernel. Kernel writing will be discussed further in the next section.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是改变机制设计或优化存储，这种方法研究了注意力分数是如何计算的，并找到了使这种计算更有效的方法。当考虑到执行计算的硬件时，这种方法是最有效的。针对特定芯片优化的代码称为内核。内核编写将在下一节中进一步讨论。
- en: One of the most well-known kernels optimized for attention computation is [FlashAttention](https://github.com/Dao-AILab/flash-attention)
    (Dao et al., 2022). This kernel fused together many operations commonly used in
    a transformer-based model to make them run faster, as shown in [Figure 9-13](#ch09_figure_13_1730130962952862).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的针对注意力计算优化的内核之一是[FlashAttention](https://github.com/Dao-AILab/flash-attention)（Dao等人，2022年）。这个内核将许多在基于transformer的模型中常用的操作融合在一起，以使它们运行得更快，如图[图9-13](#ch09_figure_13_1730130962952862)所示。
- en: '![A graph of a graph with text'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个带有文本的图'
- en: Description automatically generated with medium confidence](assets/aien_0913.png)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成，置信度中等](assets/aien_0913.png)
- en: Figure 9-13\. FlashAttention is a kernel that fuses together several common
    operators. Adapted from an original image licensed under BSD 3-Clause.
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-13。FlashAttention是一个将多个常见操作融合在一起的内核。改编自BSD 3-Clause许可的原始图像。
- en: Kernels and compilers
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内核和编译器
- en: Kernels are specialized pieces of code optimized for specific hardware accelerators,
    such as GPUs or TPUs. They are typically written to perform computationally intensive
    routines that need to be executed repeatedly, often in parallel, to maximize the
    performance of these accelerators.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 内核是针对特定硬件加速器（如GPU或TPU）优化的专用代码片段。它们通常被编写来执行需要重复执行的计算密集型例程，通常在并行执行，以最大化这些加速器的性能。
- en: Common AI operations, including matrix multiplication, attention computation,
    and convolution operation, all have specialized kernels to make their computation
    more efficient on different hardware.^([23](ch09.html#id1716))
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的AI操作，包括矩阵乘法、注意力计算和卷积操作，都有专门的内核来使它们在不同硬件上的计算更高效。[23](ch09.html#id1716)]
- en: Writing kernels requires a deep understanding of the underlying hardware architecture.
    This includes knowledge about how the memory hierarchy is structured (such as
    caches, global memory, shared memory, and registers) and how data is accessed
    and moved between these different levels.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 编写内核需要深入了解底层硬件架构。这包括对内存层次结构如何构建（例如缓存、全局内存、共享内存和寄存器）以及数据如何在这些不同级别之间访问和移动的知识。
- en: Moreover, kernels are typically written in lower-level programming languages
    like CUDA (for NVIDIA GPUs), Triton (a language developed by OpenAI for writing
    custom kernels), and ROCm (for AMD GPUs). These languages allow fine-grained control
    over thread management and memory access but are also harder to learn than the
    languages that most AI engineers are familiar with, like Python.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，内核通常用CUDA（用于NVIDIA GPU）、Triton（OpenAI开发用于编写自定义内核的语言）和ROCm（用于AMD GPU）等低级编程语言编写。这些语言允许对线程管理和内存访问进行细粒度控制，但与大多数AI工程师熟悉的语言（如Python）相比，它们也更难学习。
- en: Due to this entry barrier, writing kernels used to be a dark art practiced by
    a few. Chip makers like NVIDIA and AMD employ optimization engineers to write
    kernels to make their hardware efficient for AI workloads, whereas AI frameworks
    like PyTorch and TensorFlow employ kernel engineers to optimize their frameworks
    on different accelerators.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个入门门槛，编写内核曾经是一种只有少数人实践的秘密艺术。芯片制造商如NVIDIA和AMD雇佣优化工程师来编写内核，以使他们的硬件对AI工作负载更高效，而像PyTorch和TensorFlow这样的AI框架则雇佣内核工程师来优化它们在不同加速器上的框架。
- en: 'However, with the rising demand for inference optimization and the ubiquity
    of accelerators, more AI engineers have taken an interest in writing kernels.
    There are many great online tutorials for kernel writing. Here, I’ll cover four
    common techniques often used to speed up computation:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着推理优化需求的增加和加速器的普遍存在，越来越多的AI工程师开始对编写内核感兴趣。有许多关于内核编写的优秀在线教程。在这里，我将介绍四种常用于加速计算的技术：
- en: Vectorization
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化
- en: Given a loop or a nested loop, instead of processing one data element at a time,
    simultaneously execute multiple data elements that are contiguous in memory. This
    reduces latency by minimizing data I/O operations.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个循环或嵌套循环，而不是一次处理一个数据元素，同时执行内存中连续的多个数据元素。通过最小化数据I/O操作来减少延迟。
- en: Parallelization
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化
- en: Divide an input array (or n-dimensional array) into independent chunks that
    can be processed simultaneously on different cores or threads, speeding up the
    computation.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入数组（或n维数组）划分为可以在不同核心或线程上同时处理的独立块，从而加快计算速度。
- en: Loop tiling
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 循环展平
- en: Optimize the data accessing order in a loop for the hardware’s memory layout
    and cache. This optimization is hardware-dependent. An efficient CPU tiling pattern
    may not work well on GPUs.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 优化循环中的数据访问顺序以适应硬件的内存布局和缓存。这种优化是硬件相关的。一个高效的CPU展平模式可能在GPU上效果不佳。
- en: Operator fusion
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 算子融合
- en: Combine multiple operators into a single pass to avoid redundant memory access.
    For example, if two loops operate over the same array, they can be fused into
    one, reducing the number of times data is read and written.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个算子组合成单个遍历，以避免冗余的内存访问。例如，如果两个循环操作相同的数组，它们可以融合成一个，减少数据读取和写入的次数。
- en: While vectorization, parallelization, and loop tiling can be applied broadly
    across different models, operator fusion requires a deeper understanding of a
    model’s specific operators and architecture. As a result, operator fusion demands
    more attention from optimization engineers.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然向量化、并行化和循环展平可以在不同的模型中广泛应用，但算子融合需要更深入地理解模型的具体算子和架构。因此，算子融合需要优化工程师更多的关注。
- en: Kernels are optimized for a hardware architecture. This means that whenever
    a new hardware architecture is introduced, new kernels need to be developed. For
    example, [FlashAttention](https://github.com/Dao-AILab/flash-attention) (Dao et
    al., 2022) was originally developed primarily for NVIDIA A100 GPUs. Later on,
    FlashAttention-3 was introduced for H100 GPUs ([Shah et al., 2024](https://arxiv.org/abs/2407.08608)).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 内核是针对硬件架构进行优化的。这意味着每当引入新的硬件架构时，就需要开发新的内核。例如，[FlashAttention](https://github.com/Dao-AILab/flash-attention)（Dao等人，2022年）最初主要是为NVIDIA
    A100 GPU开发的。后来，为了H100 GPU，引入了FlashAttention-3（[Shah等人，2024](https://arxiv.org/abs/2407.08608)）。
- en: A model script specifies a series of operations that need to be performed to
    execute that model. To run this code on a piece of hardware, such as a GPU, it
    has to be converted into a language compatible with that hardware. This process
    is called *lowering*. A tool that *lowers* code to run a specific hardware is
    called a compiler. Compilers bridge ML models and the hardware they run on. During
    the lowering process, whenever possible, these operations are converted into specialized
    kernels to run faster on the target hardware.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 模型脚本指定了一系列需要执行的操作以运行该模型。要在如GPU这样的硬件上运行此代码，必须将其转换为与该硬件兼容的语言。这个过程称为*降低*。可以将代码降低到特定硬件以运行的工具称为编译器。编译器连接机器学习模型和它们运行的硬件。在降低过程中，尽可能将这些操作转换为针对目标硬件优化的专用内核，以加快运行速度。
- en: Compilers can be standalone tools, such as [Apache TVM](https://github.com/apache/tvm)
    and [MLIR](https://mlir.llvm.org) (Multi-Level Intermediate Representation) or
    integrated into ML and inference frameworks, like [`torch.compile`](https://oreil.ly/6bjVM)
    (a feature in PyTorch), [XLA](https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra)
    (Accelerated Linear Algebra, originally developed by TensorFlow, with an open
    source version called [OpenXLA](https://github.com/openxla/xla)), and the compiler
    built into the [TensorRT](https://github.com/NVIDIA/TensorRT), which is optimized
    for NVIDIA GPUs. AI companies might have their own compilers, with their proprietary
    kernels designed to speed up their own workloads.^([24](ch09.html#id1729))
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器可以是独立的工具，如[Apache TVM](https://github.com/apache/tvm)和[MLIR](https://mlir.llvm.org)（多级中间表示）或集成到机器学习和推理框架中，如PyTorch中的[`torch.compile`](https://oreil.ly/6bjVM)（一个功能），[XLA](https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra)（加速线性代数，最初由TensorFlow开发，开源版本称为[OpenXLA](https://github.com/openxla/xla)），以及内置在[TensorRT](https://github.com/NVIDIA/TensorRT)中的编译器，它针对NVIDIA
    GPU进行了优化。AI公司可能有他们自己的编译器，其专有内核旨在加速他们自己的工作负载。[24](ch09.html#id1729)
- en: Inference Service Optimization
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理服务优化
- en: Most service-level optimization techniques focus on resource management. Given
    a fixed amount of resources (compute and memory) and dynamic workloads (inference
    requests from users that may involve different models), the goal is to efficiently
    allocate resources to these workloads to optimize for latency and cost. Unlike
    many model-level techniques, service-level techniques don’t modify models and
    shouldn’t change the output quality.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数服务级优化技术都集中在资源管理上。在给定固定数量的资源（计算和内存）和动态工作负载（用户的推理请求，可能涉及不同的模型）的情况下，目标是高效地分配资源给这些工作负载，以优化延迟和成本。与许多模型级技术不同，服务级技术不会修改模型，也不应该改变输出质量。
- en: Batching
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批处理
- en: One of the easiest ways to reduce your cost is batching. In production, your
    inference service might receive multiple requests simultaneously. Instead of processing
    each request separately, batching the requests that arrive around the same time
    together can significantly reduce the service’s throughput. If processing each
    request separately is like everyone driving their own car, batching is like putting
    them together on a bus. A bus can move more people, but it can also make each
    person’s journey longer. However, if you do it intelligently, the impact on latency
    can be minimal.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 降低成本的最简单方法之一是批处理。在生产中，你的推理服务可能会同时接收到多个请求。而不是单独处理每个请求，将同时到达的请求一起批处理可以显著降低服务的吞吐量。如果单独处理每个请求就像每个人都开着自己的车，那么批处理就像把他们一起放在公交车上。公交车可以运送更多的人，但也会使每个人的旅程更长。然而，如果你做得明智，对延迟的影响可以最小化。
- en: 'The three main techniques for batching are: static batching, dynamic batching,
    and continuous batching.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理的主要技术有三种：静态批处理、动态批处理和连续批处理。
- en: The simplest batching technique is *static batching*. The service groups a fixed
    number of inputs together in a batch. It’s like a bus that waits until every seat
    is filled before departing. The drawback of static batching is that all requests
    have to wait until the batch is full to be executed. Thus the first request in
    a batch is delayed until the batch’s last request arrives, no matter how late
    the last request is.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的批处理技术是*静态批处理*。服务将固定数量的输入组合在一起形成一个批次。这就像一辆公交车，在所有座位都填满之前不会出发。静态批处理的缺点是所有请求都必须等到批次填满才能执行。因此，批次中的第一个请求会延迟，直到批次的最后一个请求到达，无论最后一个请求到达得多晚。
- en: '*Dynamic batching*, on the other hand, sets a maximum time window for each
    batch. If the batch size is four and the window is 100 ms, the server processes
    the batch either when it has four requests or when 100 ms has passed, whichever
    happens first. It’s like a bus that leaves on a fixed schedule or when it’s full.
    This approach keeps latency under control, so earlier requests aren’t held up
    by later ones. The downside is that batches may not always be full when processed,
    possibly leading to wasted compute. Static batching and dynamic batching are visualized
    in [Figure 9-15](#ch09_figure_15_1730130962952896).'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*动态批处理*为每个批次设定一个最大时间窗口。如果批次大小为四个，窗口为100毫秒，服务器将在有四个请求或100毫秒过去时处理批次，以先到者为准。这就像一辆按照固定时间表出发或满载时出发的公交车。这种方法可以控制延迟，因此早期的请求不会被后来的请求所阻碍。缺点是处理时批次可能不会总是满载，可能导致计算资源的浪费。静态批处理和动态批处理在[图9-15](#ch09_figure_15_1730130962952896)中进行了可视化。
- en: '![A screenshot of a computer'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机屏幕截图'
- en: Description automatically generated](assets/aien_0915.png)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0915.png)
- en: Figure 9-15\. Dynamic batching keeps the latency manageable but might be less
    compute-efficient.
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-15。动态批处理可以保持延迟在可控范围内，但可能计算效率较低。
- en: In naive batching implementations, all batch requests have to be completed before
    their responses are returned. For LLMs, some requests might take much longer than
    others. If one request in a batch generates only 10 response tokens and another
    request generates 1,000 response tokens, the short response has to wait until
    the long response is completed before being returned to the user. This results
    in unnecessary latency for short requests.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的批处理实现中，所有批处理请求都必须完成，才能返回它们的响应。对于LLMs，一些请求可能比其他请求耗时更长。如果一个批次的请求只生成10个响应令牌，而另一个请求生成1,000个响应令牌，那么短响应必须等待长响应完成才能返回给用户。这导致短请求出现不必要的延迟。
- en: '*Continuous batching* allows responses in a batch to be returned to users as
    soon as they are completed. It works by selectively batching operations that don’t
    cause the generation of one response to hold up another, as introduced in the
    paper Orca ([Yu et al., 2022](https://oreil.ly/SJ7Mb)). After a request in a batch
    is completed and its response returned, the service can add another request into
    the batch in its place, making the batching continuous. It’s like a bus that,
    after dropping off one passenger, can immediately pick up another passenger to
    maximize its occupancy rate. Continuous batching, also called [*in-flight batching*](https://oreil.ly/DlIPs),
    is visualized in [Figure 9-16](#ch09_figure_16_1730130962952915).'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '*连续批处理*允许批处理中的响应在完成时立即返回给用户。它通过选择性地批处理不会导致一个响应的生成阻碍另一个响应的操作来实现，如论文Orca ([Yu
    等人，2022](https://oreil.ly/SJ7Mb)) 中所述。在批处理中的请求完成并返回其响应后，服务可以在其位置添加另一个请求，使批处理连续。这就像一辆公交车，在放下一位乘客后，可以立即接载另一位乘客以最大化其载客率。连续批处理，也称为[*飞行批处理*](https://oreil.ly/DlIPs)，在[图9-16](#ch09_figure_16_1730130962952915)中进行了可视化。'
- en: '![A screenshot of a diagram'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表的截图](assets/aien_0916.png)'
- en: Description automatically generated](assets/aien_0916.png)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0916.png)
- en: Figure 9-16\. With continuous batching, completed responses can be returned
    immediately to users, and new requests can be processed in their place.
  id: totrans-311
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-16。在连续批处理中，完成的响应可以立即返回给用户，并可以处理新的请求。
- en: Decoupling prefill and decode
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解耦预填充和解码
- en: 'LLM inference consists of two steps: prefill and decode. Because prefill is
    compute-bound and decode is memory bandwidth-bound, using the same machine to
    perform both can cause them to inefficiently compete for resources and significantly
    slow down both TTFT and TPOT. Imagine a GPU that is already handling prefilling
    and decoding near its peak computational capacity. It might be able to handle
    another low computational job like decoding. However, adding a new query to this
    GPU means introducing a prefilling job along with a decoding job. This one prefilling
    job can drain computational resources from existing decoding jobs, slowing down
    TPOT for these requests.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: LLM推理包括两个步骤：预填充和解码。因为预填充是计算密集型的，而解码是内存带宽密集型的，所以使用同一台机器执行这两个操作可能会导致它们低效地竞争资源，并显著降低TTFT和TPOT。想象一下，一个GPU已经接近其计算能力的峰值来处理预填充和解码。它可能能够处理另一个低计算量的工作，如解码。然而，向这个GPU添加新的查询意味着引入一个预填充工作以及一个解码工作。这个预填充工作可能会从现有的解码工作中消耗计算资源，从而降低这些请求的TPOT。
- en: One common optimization technique for inference servers is to disaggregate prefill
    and decode. “DistServe” ([Zhong et al., 2024](https://arxiv.org/html/2401.09670v1))
    and “Inference Without Interference” ([Hu et al., 2024](https://arxiv.org/abs/2401.11181))
    show that for various popular LLMs and applications, assigning prefill and decode
    operations to different instances (e.g., different GPUs) can significantly improve
    the volume of processed requests while adhering to latency requirements. Even
    though decoupling requires transferring intermediate states from prefill instances
    to decode instances, the paper shows communication overhead is not substantial
    in modern GPU clusters with high-bandwidth connections such as [NVLink](https://en.wikipedia.org/wiki/NVLink)
    within a node.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理服务器的一种常见优化技术是将预填充和解码分解。 “DistServe” ([Zhong 等人，2024](https://arxiv.org/html/2401.09670v1))
    和 “无干扰推理” ([Hu 等人，2024](https://arxiv.org/abs/2401.11181)) 表明，对于各种流行的LLM和应用程序，将预填充和解码操作分配给不同的实例（例如，不同的GPU）可以显著提高处理请求的量，同时满足延迟要求。尽管解耦需要将中间状态从预填充实例传输到解码实例，但论文显示，在现代具有高带宽连接的GPU集群中（例如节点内的[NVLink](https://en.wikipedia.org/wiki/NVLink)），通信开销并不大。
- en: The ratio of prefill instances to decode instances depends on many factors,
    such as the workload characteristics (e.g., longer input lengths require more
    prefill compute) and latency requirements (e.g., whether you want lower TTFT or
    TPOT). For example, if input sequences are usually long and you want to prioritize
    TTFT, this ratio can be between 2:1 and 4:1\. If input sequences are short and
    you want to prioritize TPOT, this ratio can be 1:2 to 1:1.^([25](ch09.html#id1741))
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 预填充实例与解码实例的比例取决于许多因素，例如工作负载特征（例如，较长的输入长度需要更多的预填充计算）和延迟要求（例如，您是否希望降低TTFT或TPOT）。例如，如果输入序列通常较长且您希望优先考虑TTFT，则此比例可以是2:1到4:1。如果输入序列较短且您希望优先考虑TPOT，则此比例可以是1:2到1:1。[25](ch09.html#id1741)
- en: Prompt caching
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示缓存
- en: Many prompts in an application have overlapping text segments. A prompt cache
    stores these overlapping segments for reuse, so you only need to process them
    once. A common overlapping text segment in different prompts is the system prompt.
    Without a prompt cache, your model needs to process the system prompt with every
    query. With a prompt cache, the system prompt needs to be processed just once
    for the first query.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序中的许多提示都有重叠的文本段。提示缓存存储这些重叠段以供重用，因此你只需要处理一次。不同提示中常见的重叠文本段是系统提示。没有提示缓存，你的模型需要每次查询都处理系统提示。有了提示缓存，系统提示只需要在第一次查询时处理一次。
- en: Prompt caching is useful for queries that involve long documents. For example,
    if many of your user queries are related to the same long document (such as a
    book or a codebase), this long document can be cached for reuse across queries.
    It’s also useful for long conversations when the processing of earlier messages
    can be cached and reused when predicting future messages.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 提示缓存对于涉及长文档的查询非常有用。例如，如果你的许多用户查询都与同一长文档（如书籍或代码库）相关，则可以将该长文档缓存以跨查询重用。它对于长对话也很有用，当早期消息的处理可以缓存并在预测未来消息时重用时。
- en: A prompt cache is visualized in [Figure 9-17](#ch09_figure_17_1730130962952933).
    It’s also called a context cache or prefix cache.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 提示缓存在[图9-17](#ch09_figure_17_1730130962952933)中进行了可视化。它也被称为上下文缓存或前缀缓存。
- en: '![A screenshot of a computer'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机屏幕截图'
- en: Description automatically generated](assets/aien_0917.png)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0917.png)
- en: Figure 9-17\. With a prompt cache, overlapping segments in different prompts
    can be cached and reused.
  id: totrans-322
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-17\. 使用提示缓存时，不同提示中的重叠段可以缓存和重用。
- en: For applications with long system prompts, prompt caching can significantly
    reduce both latency and cost. If your system prompt is 1,000 tokens, and your
    application generates one million model API calls daily, a prompt cache will save
    you from processing approximately one billion repetitive input tokens a day! However,
    this isn’t entirely free. Like the KV cache, prompt cache size can be quite large
    and take up memory space. Unless you use a model API with this functionality,
    implementing prompt caching can require significant engineering effort.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要长系统提示的应用，提示缓存可以显著降低延迟和成本。如果你的系统提示是1,000个token，并且你的应用每天生成一百万次模型API调用，提示缓存将为你节省每天处理大约十亿个重复输入token！然而，这并不是完全免费的。就像KV缓存一样，提示缓存的大小可以相当大，并占用内存空间。除非你使用具有此功能的模型API，否则实现提示缓存可能需要大量的工程努力。
- en: Since its introduction in November 2023 by [Gim et al.](https://oreil.ly/Pd6Pk),
    the prompt cache has been rapidly incorporated into model APIs. As of this writing,
    Google Gemini offers this [functionality](https://oreil.ly/pIHkL), with cached
    input tokens given a 75% discount compared to regular input tokens, but you’ll
    have to pay extra for cache storage (as of writing, $1.00/one million tokens per
    hour). Anthropic offers [prompt caching](https://oreil.ly/8rtsF) that promises
    up to 90% cost savings (the longer the cached context, the higher the savings)
    and up to 75% latency reduction. The impact of prompt caching on the cost and
    latency of different scenarios is shown in [Table 9-3](#ch09_table_3_1730130962971081).^([26](ch09.html#id1747))
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 自2023年11月由Gim等人[介绍](https://oreil.ly/Pd6Pk)以来，提示缓存已迅速集成到模型API中。截至本文撰写时，Google
    Gemini提供此[功能](https://oreil.ly/pIHkL)，缓存输入token比常规输入token便宜75%，但你需要额外支付缓存存储费用（截至撰写本文时，每小时1.00美元/一百万个token）。Anthropic提供[提示缓存](https://oreil.ly/8rtsF)，承诺最高90%的成本节省（缓存的上下文越长，节省越多）和最高75%的延迟降低。提示缓存对不同场景的成本和延迟的影响在[表9-3](#ch09_table_3_1730130962971081)中显示。[26](ch09.html#id1747)
- en: Table 9-3\. Cost and latency reduced by prompt caching. Information from Anthropic
    (2024).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-3\. 通过提示缓存降低成本和延迟。信息来自Anthropic（2024）。
- en: '| Use case | Latency w/o caching (time to first token) | Latency with caching
    (time to first token) | Cost reduction |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 用例 | 无缓存延迟（第一个token的时间） | 有缓存延迟（第一个token的时间） | 成本降低 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Chat with a book (100,000-token cached prompt) | 11.5 s | 2.4 s (–79%) |
    –90% |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 与书籍聊天（100,000个token缓存的提示） | 11.5秒 | 2.4秒（–79%） | –90% |'
- en: '| Many-shot prompting (10,000-token prompt) | 1.6 s | 1.1 s (–31%) | –86% |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 多次提示（10,000个token提示） | 1.6秒 | 1.1秒（–31%） | –86% |'
- en: '| Multi-turn conversation (10-turn convo with a long system prompt) | ~10 s
    | ~2.5 s (–75%) | –53% |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 多轮对话（10轮长系统提示对话） | ~10秒 | ~2.5秒（–75%） | –53% |'
- en: Parallelism
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行处理
- en: Accelerators are designed for parallel processing, and parallelism strategies
    are the backbone of high-performance computing. Many new parallelization strategies
    are being developed. This section covers only a few of them for reference. Two
    families of parallelization strategies that can be applied across all models are
    data parallelism and model parallelism. A family of strategies applied specifically
    for LLMs is context and sequence parallelism. An optimization technique might
    involve multiple parallelism strategies.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 加速器是为并行处理而设计的，并行策略是高性能计算的核心。许多新的并行化策略正在被开发。本节仅介绍其中的一些，供参考。适用于所有模型的两种并行化策略是数据并行和模型并行。专门用于LLM的策略是上下文和序列并行。一种优化技术可能涉及多种并行化策略。
- en: '*Replica parallelism* is the most straightforward strategy to implement. It
    simply creates multiple replicas of the model you want to serve.^([27](ch09.html#id1749))
    More replicas allow you to handle more requests at the same time, potentially
    at the cost of using more chips. Trying to fit models of different sizes onto
    different chips is a bin-packing problem, which can get complicated with more
    models, more replicas, and more chips.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*副本并行*是实现策略中最直接的方法。它只是简单地创建了你要服务的模型的多个副本。[27](ch09.html#id1749) 更多副本允许你同时处理更多请求，但可能会以使用更多芯片为代价。试图将不同大小的模型放入不同的芯片上是一个装箱问题，随着模型、副本和芯片数量的增加，这个问题可能会变得更加复杂。'
- en: 'Let’s say you have a mixture of models of different sizes (e.g., 8B, 13B, 34B,
    and 70B parameters) and access to GPUs of different memory capabilities (e.g.,
    24 GB, 40 GB, 48 GB, and 80 GB). For simplicity, assume that all models are in
    the same precision, 8 bits:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有不同大小的模型混合体（例如，8B、13B、34B和70B参数）以及不同内存容量的GPU（例如，24 GB、40 GB、48 GB和80 GB）。为了简化，假设所有模型都是相同的精度，8位：
- en: If you have a fixed number of chips, you need to decide how many replicas to
    create for each model and what GPUs to use for each replica to maximize your metrics.
    For example, should you place three 13B models on a 40 GB GPU, or should you reserve
    this GPU for one 34B model?
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你拥有固定数量的芯片，你需要决定为每个模型创建多少副本，以及为每个副本使用哪种GPU以最大化你的指标。例如，你应该将三个13B模型放置在40 GB
    GPU上，还是应该为单个34B模型保留这个GPU？
- en: If you have a fixed number of model replicas, you need to decide what chips
    to acquire to minimize the cost. This situation, however, rarely occurs.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你拥有固定数量的模型副本，你需要决定购买哪些芯片以最小化成本。然而，这种情况很少发生。
- en: Often, your model is so big that it can’t fit into one machine. *Model parallelism*
    refers to the practice of splitting the same model across multiple machines. Fitting
    models onto chips can become an even more complicated problem with model parallelism.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你的模型太大，无法放入一台机器中。*模型并行*指的是将相同的模型分布在多台机器上的做法。在模型并行的情况下，将模型适配到芯片上可能成为一个更加复杂的问题。
- en: There are several ways to split a model. The most common approach for inference
    is *tensor parallelism*, also known as *intra-operator parallelism*. Inference
    involves a sequence of operators on multidimensional tensors, such as matrix multiplication.
    In this approach, tensors involved in an operator are partitioned across multiple
    devices, effectively breaking up this operator into smaller pieces to be executed
    in parallel, thus speeding up the computation. For example, when multiplying two
    matrices, you can split one of the matrices columnwise, as shown in [Figure 9-18](#ch09_figure_18_1730130962952949).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以分割一个模型。对于推理来说，最常见的方法是*张量并行*，也称为*操作内并行*。推理涉及一系列多维张量上的操作，例如矩阵乘法。在这种方法中，参与操作的张量被分割到多个设备上，有效地将这个操作分解成更小的部分以并行执行，从而加快计算速度。例如，当乘以两个矩阵时，你可以按列分割其中一个矩阵，如图[图9-18](#ch09_figure_18_1730130962952949)所示。
- en: Tensor parallelism provides two benefits. First, it makes it possible to serve
    large models that don’t fit on single machines. Second, it reduces latency. The
    latency benefit, however, might be reduced due to extra communication overhead.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行提供了两个好处。首先，它使得能够服务那些无法在单个机器上运行的的大型模型。其次，它减少了延迟。然而，由于额外的通信开销，延迟的好处可能会降低。
- en: '![A diagram of a grid with squares and a few squares'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个由方格和一些方格组成的网格图](assets/aien_0918.png)'
- en: Description automatically generated with medium confidence](assets/aien_0918.png)
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成，置信度中等](assets/aien_0918.png)
- en: Figure 9-18\. Tensor parallelism for matrix multiplication.
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-18\. 矩阵乘法的张量并行。
- en: Another way to split a model is *pipeline parallelism*, which involves dividing
    a model’s computation into distinct stages and assigning each stage to a different
    device. As data flows through the model, each stage processes one part while others
    process subsequent parts, enabling overlapping computations. [Figure 9-19](#ch09_figure_19_1730130962952966)
    shows what pipeline parallelism looks like on four machines.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种分割模型的方法是 *管道并行化*，它涉及将模型计算划分为不同的阶段，并将每个阶段分配给不同的设备。当数据通过模型流动时，每个阶段处理一部分，而其他阶段处理后续部分，从而实现计算重叠。[图9-19](#ch09_figure_19_1730130962952966)
    展示了在四台机器上管道并行化的样子。
- en: '![A diagram of a layer'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '![层的一个图示'
- en: Description automatically generated](assets/aien_0919.png)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0919.png)
- en: Figure 9-19\. Pipeline parallelism enables model splits to be executed in parallel.
  id: totrans-346
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-19\. 管道并行化使得模型分割可以并行执行。
- en: '[Figure 9-19](#ch09_figure_19_1730130962952966) shows a batch can be split
    into smaller micro-batches. After a micro-batch is processed on one machine, its
    output is passed onto the next part of the model on the next machine.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-19](#ch09_figure_19_1730130962952966) 展示了一个批次可以被分割成更小的微批次。在一个微批次在一台机器上被处理后，其输出会被传递到下一台机器上的模型下一部分。'
- en: While pipeline parallelism enables serving large models on multiple machines,
    it increases the total latency for each request due to extra communication between
    pipeline stages. Therefore, for applications with strict latency requirements,
    pipeline parallelism is typically avoided in favor of replica parallelism. However,
    pipeline parallelism is commonly used in training since it can help increase throughput.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然管道并行化使得在多台机器上服务大型模型成为可能，但由于管道阶段之间的额外通信，它增加了每个请求的总延迟。因此，对于对延迟要求严格的应用，通常避免使用管道并行化，而选择副本并行化。然而，由于它可以帮助提高吞吐量，管道并行化在训练中通常被广泛使用。
- en: Two techniques that are less common but might warrant a quick mention to illustrate
    the diversity of techniques are *context parallelism* and *sequence parallelism*.
    They were both developed to make long input sequence processing more efficient,
    including context parallelism and sequence parallelism.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种不太常见但可能值得简要提及的技术，以展示技术的多样性，它们是 *上下文并行化* 和 *序列并行化*。它们都是为了使长输入序列处理更高效而开发的，包括上下文并行化和序列并行化。
- en: In [*context parallelism*](https://oreil.ly/On2-B), the input sequence itself
    is split across different devices to be processed separately. For example, the
    first half of the input is processed on machine 1 and the second half on machine
    2.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*上下文并行化*](https://oreil.ly/On2-B) 中，输入序列本身被分散到不同的设备上分别处理。例如，输入的前半部分在机器1上处理，后半部分在机器2上处理。
- en: In *sequence parallelism*, operators needed for the entire input are split across
    machines. For example, if the input requires both attention and feedforward computation,
    attention might be processed on machine 1 while feedforward is processed on machine
    2.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *序列并行化* 中，整个输入所需的操作被分散到多台机器上。例如，如果输入需要注意力和前馈计算，注意力可能在机器1上处理，而前馈可能在机器2上处理。
- en: Summary
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: A model’s usability depends heavily on its inference cost and latency. Cheaper
    inference makes AI-powered decisions more affordable, while faster inference enables
    the integration of AI into more applications. Given the massive potential impact
    of inference optimization, it has attracted many talented individuals who continually
    come up with innovative approaches.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '模型的可用性在很大程度上取决于其推理成本和延迟。更便宜的推理使得人工智能驱动的决策更加经济实惠，而更快的推理使得人工智能能够集成到更多应用中。鉴于推理优化的巨大潜在影响，它吸引了众多有才华的个人，他们不断提出创新的方法。 '
- en: Before we start making things more efficient, we need to understand how efficiency
    is measured. This chapter started with common efficiency metrics for latency,
    throughput, and utilization. For language model-based inference, latency can be
    broken into time to first token (TTFT), which is influenced by the prefilling
    phase, and time per output token (TPOT), which is influenced by the decoding phase.
    Throughput metrics are directly related to cost. There’s a trade-off between latency
    and throughput. You can potentially reduce cost if you’re okay with increased
    latency, and reducing latency often involves increasing cost.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始提高效率之前，我们需要了解效率是如何衡量的。本章从延迟、吞吐量和利用率等常见的效率指标开始。对于基于语言模型的推理，延迟可以分解为第一个标记的时间（TTFT），这受预填充阶段的影响，以及每个输出标记的时间（TPOT），这受解码阶段的影响。吞吐量指标与成本直接相关。在延迟和吞吐量之间存在权衡。如果你可以接受增加延迟，那么你可能会降低成本，而降低延迟通常涉及增加成本。
- en: How efficiently a model can run depends on the hardware it is run on. For this
    reason, this chapter also provided a quick overview of AI hardware and what it
    takes to optimize models on different accelerators.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 模型运行效率取决于其运行的硬件。因此，本章还提供了一个关于AI硬件的快速概述，以及在不同加速器上优化模型所需的内容。
- en: The chapter then continued with different techniques for inference optimization.
    Given the availability of model APIs, most application developers will use these
    APIs with their built-in optimization instead of implementing these techniques
    themselves. While these techniques might not be relevant to all application developers,
    I believe that understanding what techniques are possible can be helpful for evaluating
    the efficiency of model APIs.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 本章接着介绍了推理优化的不同技术。鉴于模型API的可用性，大多数应用开发者将使用这些API内置的优化，而不是自己实现这些技术。虽然这些技术可能并不适用于所有应用开发者，但我相信了解可能的技术可以帮助评估模型API的效率。
- en: This chapter also focused on optimization at the model level and the inference
    service level. Model-level optimization often requires changing the model itself,
    which can lead to changes in the model behaviors. Inference service-level optimization,
    on the other hand, typically keeps the model intact and only changes how it’s
    served.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还专注于模型级别和推理服务级别的优化。模型级别的优化通常需要改变模型本身，这可能导致模型行为的改变。另一方面，推理服务级别的优化通常保持模型不变，只改变其服务方式。
- en: Model-level techniques include model-agnostic techniques like quantization and
    distillation. Different model architectures require their own optimization. For
    example, because a key bottleneck of transformer models is in the attention mechanism,
    many optimization techniques involve making attention more efficient, including
    KV cache management and writing attention kernels. A big bottleneck for an autoregressive
    language model is in its autoregressive decoding process, and consequently, many
    techniques have been developed to address it, too.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 模型级别的技术包括模型无关的技术，如量化蒸馏。不同的模型架构需要自己的优化。例如，由于转换器模型的关键瓶颈在于注意力机制，许多优化技术都涉及使注意力更高效，包括KV缓存管理和编写注意力内核。自回归语言模型的一个大瓶颈在于其自回归解码过程，因此也开发了许多针对这一问题的技术。
- en: Inference service-level techniques include various batching and parallelism
    strategies. There are also techniques developed especially for autoregressive
    language models, including prefilling/decoding decoupling and prompt caching.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 推理服务级别的技术包括各种批处理和并行策略。还有专门为自回归语言模型开发的技巧，包括预填充/解码解耦和提示缓存。
- en: The choice of optimization techniques depends on your workloads. For example,
    KV caching is significantly more important for workloads with long contexts than
    those with short contexts. Prompt caching, on the other hand, is crucial for workloads
    involving long, overlapping prompt segments or multi-turn conversations. The choice
    also depends on your performance requirements. For instance, if low latency is
    a higher priority than cost, you might want to scale up replica parallelism. While
    more replicas require additional machines, each machine handles fewer requests,
    allowing it to allocate more resources per request and, thus, improve response
    time.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 优化技术的选择取决于你的工作负载。例如，对于具有长上下文的工作负载，KV 缓存比那些具有短上下文的工作负载重要得多。另一方面，对于涉及长、重叠提示段或多轮对话的工作负载，提示缓存至关重要。选择还取决于你的性能要求。例如，如果低延迟比成本更重要，你可能希望增加副本并行度。虽然更多的副本需要额外的机器，但每台机器处理的请求数量更少，这使得它可以为每个请求分配更多资源，从而提高响应时间。
- en: However, across various use cases, the most impactful techniques are typically
    quantization (which generally works well across models), tensor parallelism (which
    both reduces latency and enables serving larger models), replica parallelism (which
    is relatively straightforward to implement), and attention mechanism optimization
    (which can significantly accelerate transformer models).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在各个用例中，最具影响力的技术通常是量化（通常在所有模型中都表现良好）、张量并行（它既减少了延迟，又能够服务更大的模型）、副本并行（相对容易实现）和注意力机制优化（可以显著加速变压器模型）。
- en: Inference optimization concludes the list of model adaptation techniques covered
    in this book. The next chapter will explore how to integrate these techniques
    into a cohesive system.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 推理优化是本书中涵盖的模型适应技术列表的结尾。下一章将探讨如何将这些技术整合成一个统一的系统。
- en: ^([1](ch09.html#id1597-marker)) As discussed in [Chapter 7](ch07.html#ch07),
    inference involves the forward pass while training involves both the forward and
    backward passes.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.html#id1597-marker)) 如第 7 章所述，推理涉及正向传递，而训练涉及正向和反向传递。
- en: ^([2](ch09.html#id1598-marker)) A friend, Mark Saroufim, pointed me to an interesting
    relationship between a model’s training cost and inference cost. Imagine you’re
    a model provider. Let *T* be the total training cost, *p* be the cost you’re charging
    per inference, and *N* be the number of inference calls you can sell. Developing
    a model only makes sense if the money you can recover from inference for a model
    is more than its training cost, i.e., *T* <= *p* × *N*. The more a model is used
    in production, the more model providers can reduce inference cost. However, this
    doesn’t apply for third-party API providers who sell inference calls on top of
    open source models.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.html#id1598-marker)) 一个朋友，马克·萨拉菲姆（Mark Saroufim），向我指出了一个有趣的模型训练成本与推理成本之间的关系。想象一下，你是一个模型提供商。设
    *T* 为总训练成本，*p* 为每次推理的收费，*N* 为你可以销售的推理调用次数。只有当你可以从推理中收回的模型收入超过其训练成本时，开发模型才有意义，即
    *T* <= *p* × *N*。模型在生产中使用得越多，模型提供商可以降低推理成本。然而，这并不适用于在开源模型之上销售推理调用的第三方 API 提供商。
- en: ^([3](ch09.html#id1605-marker)) Anecdotally, I find that people coming from
    a system background (e.g., optimization engineers and GPU engineers) use *memory-bound*
    to refer to *bandwidth-bound*, and people coming from an AI background (e.g.,
    ML and AI engineers) use to memory-bound to refer to memory capacity-bound.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch09.html#id1605-marker)) 根据个人经验，我发现来自系统背景（例如，优化工程师和 GPU 工程师）的人使用 *memory-bound*
    来指代 *bandwidth-bound*，而来自 AI 背景（例如，ML 和 AI 工程师）的人则使用 memory-bound 来指代 memory capacity-bound。
- en: ^([4](ch09.html#id1606-marker)) The Roofline paper uses the term memory-bound
    to refer to memory-bandwidth bound.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch09.html#id1606-marker)) Roofline 论文使用术语 memory-bound 来指代 memory-bandwidth
    bound。
- en: ^([5](ch09.html#id1607-marker)) Prefilling effectively populates the initial
    KV cache for the transformer model.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch09.html#id1607-marker)) 预填充有效地填充了变压器模型的初始 KV 缓存。
- en: ^([6](ch09.html#id1612-marker)) If you run an inference service, separating
    your inference APIs into online and batch can help you prioritize latency for
    requests where latency matters the most. Let’s say that your inference server
    can serve only a maximum of X requests/second without latency degradation, you
    have to serve Y requests/second, and Y is larger than X. In an ideal world, users
    with less-urgent requests can send their requests to the batch API, so that your
    service can focus on processing the online API requests first.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch09.html#id1612-marker)) 如果你运行推理服务，将你的推理API分为在线和批量可以帮助你在最需要延迟的地方优先处理请求。假设你的推理服务器在没有延迟退化的情况下只能服务X个请求/秒，你必须服务Y个请求/秒，而Y大于X。在理想情况下，对紧急请求较少的用户可以将他们的请求发送到批量API，这样你的服务就可以首先专注于处理在线API请求。
- en: ^([7](ch09.html#id1619-marker)) As discussed in [“Prompt caching”](#ch09_prompt_caching_1730130963008914),
    it’s common to know in advance the system prompt of an application. It’s just
    the exact user queries that are hard to predict.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch09.html#id1619-marker)) 如在[“提示缓存”](#ch09_prompt_caching_1730130963008914)中讨论的那样，通常可以提前知道应用程序的系统提示。只是确切的用户查询很难预测。
- en: ^([8](ch09.html#id1620-marker)) In the early days of chatbots, some people complained
    about chatbots responding too fast, which seemed unnatural. See [“Lufthansa Delays
    Chatbot’s Responses to Make It More ‘Human’”](https://oreil.ly/jD5Pj) (Ry Crozier,
    iTnews, May 2017). However, as people become more familiar with chatbots, this
    is no longer the case.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch09.html#id1620-marker)) 在聊天机器人早期，有些人抱怨聊天机器人响应太快，这看起来不自然。参见[“汉莎航空延迟聊天机器人的响应以使其更具‘人性’”](https://oreil.ly/jD5Pj)（Ry
    Crozier，iTnews，2017年5月）。然而，随着人们对聊天机器人的熟悉，这种情况已经不再存在。
- en: ^([9](ch09.html#id1623-marker)) Time between tokens (TBT) is used by [LinkedIn](https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US)
    and inter-token latency (ITL) is used by [NVIDIA](https://oreil.ly/zHsb8).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch09.html#id1623-marker)) [LinkedIn](https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US)
    使用标记间时间（TBT），而[NVIDIA](https://oreil.ly/zHsb8)使用标记间延迟（ITL）。
- en: ^([10](ch09.html#id1624-marker)) An experiment by Anyscale shows that 100 input
    tokens have approximately the same impact on the overall latency as a single output
    token.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch09.html#id1624-marker)) Anyscale的一项实验表明，100个输入标记对整体延迟的影响与单个输出标记大致相同。
- en: ^([11](ch09.html#id1637-marker)) People have cared about FLOP/s utilization
    for a long time, but the term MFU was introduced in the PaLM paper ([Chowdhery
    et al., 2022](https://arxiv.org/abs/2204.02311)).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch09.html#id1637-marker)) 人们长期以来一直关注FLOP/s利用率，但MFU这个术语是在PaLM论文([Chowdhery等人，2022](https://arxiv.org/abs/2204.02311))中引入的。
- en: ^([12](ch09.html#id1638-marker)) Chip makers might also be doing what I call
    *peak FLOP/s hacking*. This might run experiments in certain conditions, such
    as using sparse matrices with specific shapes, to increase their peak FLOP/s.
    Higher peak FLOP/s numbers make their chips more attractive, but it can be harder
    for users to achieve high MFU.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch09.html#id1638-marker)) 芯片制造商可能也在进行我所说的*峰值FLOP/s黑客攻击*。这可能在某些条件下进行实验，例如使用具有特定形状的稀疏矩阵，以提高其峰值FLOP/s。更高的峰值FLOP/s数值使他们的芯片更具吸引力，但用户实现高MFU可能更困难。
- en: '^([13](ch09.html#id1649-marker)) In the 1960s, computers could run only one-layer
    neural networks, which had very limited capabilities. In their famous 1969 book
    [*Perceptrons: An Introduction to Computational Geometry*](https://en.wikipedia.org/wiki/Perceptrons_(book))
    (MIT Press), two AI pioneers, Marvin Minsky and Seymour Papert, argued that neural
    networks with hidden layers would still be able to do little. Their exact quote
    was: “Virtually nothing is known about the computational capabilities of this
    latter kind of machine. We believe that it can do little more than can a low order
    perceptron*.*” There wasn’t sufficient compute power to dispute their argument,
    which was then cited by many people as a key reason for the drying up of AI funding
    in the 1970s.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch09.html#id1649-marker)) 在20世纪60年代，计算机只能运行单层神经网络，其功能非常有限。在他们的著名1969年书籍[*感知器：计算几何导论*](https://en.wikipedia.org/wiki/Perceptrons_(book))（麻省理工学院出版社）中，两位AI先驱，Marvin
    Minsky和Seymour Papert，认为具有隐藏层的神经网络仍然只能做很少的事情。他们的确切说法是：“关于这种后一种机器的计算能力几乎一无所知。我们相信它所做的不会比低阶感知器多多少*。”当时没有足够的计算能力来反驳他们的论点，这个论点后来被许多人引用为20世纪70年代AI资金枯竭的关键原因。
- en: ^([14](ch09.html#id1650-marker)) There have been discussions on whether to [rename
    the GPU](https://oreil.ly/mRNCP) since it’s used for a lot more than graphics
    (Jon Peddie, “Chasing Pixels,” July 2018). Jensen Huang, NVIDIA’s CEO, said in
    an [interview](https://oreil.ly/iK0tN) (*Stratechery*, March 2022) that once the
    GPU took off and they added more capabilities to it, they considered renaming
    it to something more general like GPGPU (general-purpose GPU) or XGU. They decided
    against renaming because they assumed that people who buy GPUs will be smart enough
    to know what a GPU is good for beyond its name.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch09.html#id1650-marker)) 有关是否要[重命名GPU](https://oreil.ly/mRNCP)的讨论已经展开，因为GPU的应用远不止图形（Jon
    Peddie，“Chasing Pixels”，2018年7月）。NVIDIA首席执行官黄仁勋在一次[采访](https://oreil.ly/iK0tN)（*Stratechery*，2022年3月）中表示，一旦GPU起飞并且他们为其添加了更多功能，他们考虑将其重命名为更通用的名称，如GPGPU（通用型GPU）或XGU。他们决定不重命名，因为他们认为购买GPU的人足够聪明，足以知道GPU除了名称之外还有什么用途。
- en: '^([15](ch09.html#id1651-marker)) Matrix multiplication, affectionately known
    as matmul, is estimated to account for more than 90% of all floating point operations
    in a neural network, according to [“Data Movement Is All You Need: A Case Study
    on Optimizing Transformers”](https://arxiv.org/abs/2007.00072) (Ivanov et al.,
    *arXiv*, v3, November 2021) and [“Scalable MatMul-free Language Modeling”](https://arxiv.org/abs/1802.04799)
    (Zhu et al., *arXiv*, June 2024).'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '^([15](ch09.html#id1651-marker)) 根据Ivanov等人发表的论文“Data Movement Is All You Need:
    A Case Study on Optimizing Transformers”（https://arxiv.org/abs/2007.00072）（*arXiv*，v3，2021年11月）和Zhu等人发表的论文“Scalable
    MatMul-free Language Modeling”（https://arxiv.org/abs/1802.04799）（*arXiv*，2024年6月），矩阵乘法（亲切地称为matmul）据估计占神经网络中所有浮点运算的90%以上。'
- en: ^([16](ch09.html#id1652-marker)) While a chip can be developed to run one model
    architecture, a model architecture can be developed to make the most out of a
    chip, too. For example, the transformer was originally designed by Google to [run
    fast on TPUs](https://oreil.ly/y45q6) and only later optimized on GPUs.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch09.html#id1652-marker)) 虽然可以开发出运行单一模型架构的芯片，但也可以开发出充分利用芯片的模型架构。例如，Transformer最初是由谷歌设计用于在TPU上[快速运行](https://oreil.ly/y45q6)，后来才在GPU上进行了优化。
- en: ^([17](ch09.html#id1661-marker)) Lower-end to mid-range GPUs might use [GDDR](https://en.wikipedia.org/wiki/GDDR_SDRAM)
    (Graphics Double Data Rate) memory.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch09.html#id1661-marker)) 中低端到中端GPU可能会使用[GDDR](https://en.wikipedia.org/wiki/GDDR_SDRAM)（图形双数据速率）内存。
- en: ^([18](ch09.html#id1669-marker)) A main challenge in building data centers with
    tens of thousands of GPUs is finding a location that can guarantee the necessary
    electricity. Building large-scale data centers requires navigating electricity
    supply, speed, and geopolitical constraints. For example, remote regions might
    provide cheaper electricity but can increase network latency, making the data
    centers less appealing for use cases with stringent latency requirements like
    inference.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch09.html#id1669-marker)) 在建设拥有数万个GPU的数据中心时，一个主要挑战是找到一个可以保证所需电力的地点。建设大规模数据中心需要考虑电力供应、速度和地缘政治限制。例如，偏远地区可能提供更便宜的电力，但会增加网络延迟，使得数据中心对于具有严格延迟要求的用例（如推理）不那么有吸引力。
- en: ^([19](ch09.html#id1683-marker)) Each token generation step necessitates the
    transfer of the entire model’s parameters from the accelerator’s high-bandwidth
    memory to its compute units. This makes this operation bandwidth-heavy. Because
    the model can produce only one token at a time, the process consumes only a small
    number of FLOP/s, resulting in computational inefficiency.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch09.html#id1683-marker)) 每个令牌生成步骤都需要将整个模型的参数从加速器的宽带内存传输到其计算单元。这使得该操作带宽密集。由于模型一次只能生成一个令牌，因此该过程仅消耗少量FLOP/s，导致计算效率低下。
- en: ^([20](ch09.html#id1684-marker)) This also means that if your MFU is already
    maxed out, speculative decoding makes less sense.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch09.html#id1684-marker)) 这也意味着，如果你的MFU已经达到最大容量，推测性解码就不再有意义。
- en: ^([21](ch09.html#id1694-marker)) The Jacobi method is an iterative algorithm
    where multiple parts of a solution can be updated simultaneously and independently.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch09.html#id1694-marker)) 约翰逊-诺伊曼方法是一种迭代算法，其中多个解决方案的部分可以同时独立更新。
- en: ^([22](ch09.html#id1700-marker)) The number of attention computations for an
    autoregressive model is *O*(*n*²).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch09.html#id1700-marker)) 对于自回归模型，注意力计算的次数是*O*(*n*²)。
- en: ^([23](ch09.html#id1716-marker)) Convolution operations are often used in image
    generation models like Stable Diffusion.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch09.html#id1716-marker)) 卷积操作常用于图像生成模型，如Stable Diffusion。
- en: ^([24](ch09.html#id1729-marker)) Many companies consider their kernels their
    trade secrets. Having kernels that allow them to run models faster and cheaper
    than their competitors is a competitive advantage.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch09.html#id1729-marker)) 许多公司认为他们的内核是他们的商业机密。拥有能够比竞争对手更快、更便宜地运行模型的内核是一种竞争优势。
- en: ^([25](ch09.html#id1741-marker)) Talks mentioning the prefill to decode instance
    ratio include [“Llama Inference at Meta”](https://oreil.ly/eMQ_P) (Meta, 2024).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch09.html#id1741-marker)) 提到预填充到解码实例比率的讨论包括 [“Llama Inference at Meta”](https://oreil.ly/eMQ_P)（Meta，2024）。
- en: ^([26](ch09.html#id1747-marker)) While llama.cpp also has [prompt caching](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md#prompt-caching),
    it seems to cache only whole prompts and work for queries in the same chat session,
    as of this writing. Its documentation is limited, but my guess from reading the
    code is that in a long conversation, it caches the previous messages and processes
    only the newest message.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch09.html#id1747-marker)) 虽然 llama.cpp 也实现了 [prompt caching](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md#prompt-caching)，但根据目前的写作，它似乎只缓存整个提示，并且只适用于同一聊天会话中的查询。它的文档有限，但根据阅读代码的猜测，在长时间对话中，它会缓存之前的消息，并且只处理最新的消息。
- en: ^([27](ch09.html#id1749-marker)) During training, the same technique is called
    data parallelism.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch09.html#id1749-marker)) 在训练过程中，这种相同的技巧被称为数据并行。
