- en: 5 Decision trees and gradient boosting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 决策树和梯度提升
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Decision trees and their ensembles
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树及其集成
- en: Gradient boosting decision trees
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升决策树
- en: Scikit-learn’s gradient boosting decision trees options
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn的梯度提升决策树选项
- en: XGBoost algorithm and its innovations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost算法及其创新
- en: How LightGBM algorithm works
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM算法的工作原理
- en: So far, we have explored machine learning algorithms based on linear models
    because they can handle tabular problems from datasets consisting of a few rows
    and columns and find a way to scale to problems of millions of rows and many columns.
    In addition, linear models are fast to train and get predictions from. Moreover,
    they are relatively easy to understand, explain, and tweak. Linear models are
    also helpful because they present many concepts we will keep building on in the
    book, such as L1 and L2 regularization and gradient descent.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了基于线性模型的机器学习算法，因为它们可以处理来自只有几行几列的数据集的表格问题，并找到一种方法来扩展到有数百万行和许多列的问题。此外，线性模型训练和获取预测的速度快。此外，它们相对容易理解、解释和调整。线性模型还有助于我们理解本书中将要构建的许多概念，例如L1和L2正则化和梯度下降。
- en: 'This chapter will discuss a different classical machine learning algorithm:
    decision trees. Decision trees are the foundations of ensemble models such as
    random forests and boosting. We will especially focus on a machine learning ensemble
    algorithm, gradient boosting, and its implementations eXtreme Gradient Boosting
    (XGBoost) and Light Gradient Boosted Machines (LightGBM), which are considered
    state-of-the-art solutions for tabular data.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论一种不同的经典机器学习算法：决策树。决策树是随机森林和提升等集成模型的基础。我们将特别关注一种机器学习集成算法——梯度提升，以及其实现eXtreme
    Gradient Boosting (XGBoost)和Light Gradient Boosted Machines (LightGBM)，它们被认为是表格数据的最佳解决方案。
- en: 5.1 Introduction to tree-based methods
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 树方法简介
- en: Tree-based models are a family of ensemble algorithms of different kinds and
    the favored methods for handling tabular data because of their performance and
    low requirements in data preprocessing. Ensemble algorithms are sets of machine
    learning models that contribute together toward a single prediction. All tree-based
    ensemble models are based on decision trees, a popular algorithm dating to the
    1960s. The basic idea behind decision trees, no matter whether they are used for
    classification or regression, is that you can split your training set to have
    subsets where your prediction is more favorable because there is a predominant
    output class (in a classification problem) or there is a decreased variability
    of the target values (i.e., they are all very near; this refers to a regression
    problem instead).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 树模型是一系列不同种类的集成算法，由于性能良好和数据预处理要求低，因此是处理表格数据的首选方法。集成算法是一组机器学习模型，它们共同贡献于单个预测。所有基于树的集成模型都是基于决策树，这是一种自20世纪60年代以来流行的算法。决策树背后的基本思想，无论它们是用于分类还是回归，都是你可以将训练集分割成子集，在这些子集中，你的预测更有利，因为有一个占主导地位的目标类（在分类问题中）或者目标值的变异性降低（即它们都非常接近；这指的是回归问题）。
- en: Figure 5.1 shows a scheme of the key elements that make up a decision tree.
    The problem the decision tree is trying to solve is classifying an animal based
    on the number of legs and eyes. You start from the root of the tree, which corresponds
    to the entire dataset you have available, and set a condition for split. The condition
    is usually true/false—a so-called binary split. Still, some variants of decision
    trees allow for multiple conditions applied at the same node, resulting in multiple
    splits, each decided based on a different value or label from the feature. Each
    branch leads to another node, where a new condition may be applied, or to a terminal
    node, which is used for predictions based on the instances that terminate there.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1展示了构成决策树的关键元素方案。决策树试图解决的问题是根据腿和眼睛的数量对动物进行分类。你从树的根开始，这对应于你拥有的整个数据集，并设置一个分割条件。条件通常是真/假——所谓的二分分割。尽管如此，一些决策树的变体允许在同一个节点应用多个条件，从而产生多个分割，每个分割基于特征的不同值或标签来决定。每个分支都导向另一个节点，在那里可以应用新的条件，或者导向一个终端节点，该节点用于基于终止于此的实例进行预测。
- en: '![](../Images/CH05_F01_Ryan2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F01_Ryan2.png)'
- en: Figure 5.1 The key elements, such as roots, branches, and leaves, constituting
    a decision tree classifying animals based on the number of legs and eyes
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 构成决策树的关键元素，如根、分支和叶子，根据腿和眼睛的数量对动物进行分类
- en: 'A split happens based on a deliberate search of the algorithm among features
    and, inside the feature, its observed values. As a splitting criterion in a classification
    problem, the decision trees algorithm searches for the best feature and feature
    value combination that splits the data into subsets with a homogeneous target.
    The homogeneity of a target in a subset is typically measured in a classification
    problem using criteria such as entropy, information gain, or Gini impurity:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 分割是基于算法在特征中进行的故意搜索，以及在特征内部，其观察到的值。在分类问题中，决策树算法寻找最佳特征和特征值组合，将数据分割成具有同质目标的子集。在分类问题中，子集中目标的同质性通常使用熵、信息增益或基尼不纯度等标准来衡量：
- en: '*Entropy* measures the degree of disorder or randomness in the distribution
    of labels in the subset.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*熵*衡量子集中标签分布的无序度或随机度。'
- en: '*Information gain***,** derived from entropy, measures the reduction in uncertainty
    about the class labels of the data, which is achieved by splitting the data based
    on a particular feature.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*信息增益*，从熵中衍生出来，衡量通过基于特定特征的分割数据来减少关于数据类别标签的不确定性。'
- en: '*Gini impurity* measures the probability of misclassifying a randomly chosen
    element in the subset if labeled randomly according to the distribution of labels
    in the subset.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基尼不纯度*衡量在子集中随机选择一个元素被随机标记为子集中标签分布的概率。'
- en: If the decision tree is being used for regression, it resorts to different splitting
    criteria than classification. In regression, the goal is to split the data into
    subsets to minimize the resulting mean squared error, the mean absolute error,
    or simply the variance of the target variable within each subset. In the training
    process, there’s an automatic selection of the best features, and most of the
    computations for a decision tree are to determine the best feature splits. However,
    once the tree is constructed, predicting the class label or target value for new
    data is relatively fast and straightforward, involving traversing the tree starting
    from the root and ending at the leaf based on the values of a limited set of features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果决策树用于回归，它将采用与分类不同的分割标准。在回归中，目标是分割数据以最小化结果均方误差、平均绝对误差或简单地每个子集中目标变量的方差。在训练过程中，会自动选择最佳特征，决策树的大部分计算都是为了确定最佳特征分割。然而，一旦树构建完成，预测新数据的类别标签或目标值相对快速且直接，涉及从根开始遍历树，根据有限特征集的值结束于叶子。
- en: 'Decision trees are simple to compute and are also relatively easy to visualize.
    They don’t require scaling or modeling nonlinearities or otherwise transforming
    your features or output target because they can approximate any nonlinear relationship
    between target and predictors since they can consider separately single parts
    of their distribution. Basically, they are cutting a curve into parts so that
    each part looks like a line. On the other hand, decision trees are prone to overfitting
    and end up with an excessive number of splits that can fit the training data you
    are working on. Over time, different strategies have been devised to avoid overfitting:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树易于计算，也相对容易可视化。它们不需要缩放或建模非线性，或者以其他方式转换你的特征或输出目标，因为它们可以单独考虑其分布的单个部分来近似目标与预测变量之间的任何非线性关系。基本上，它们是将曲线切割成部分，使得每一部分看起来像一条线。另一方面，决策树容易过拟合，最终导致过多的分割来拟合你正在工作的训练数据。随着时间的推移，已经设计了不同的策略来避免过拟合：
- en: Limiting the number of splits in the tree
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制树中的分割数量
- en: Pruning the splitting nodes backward after they have been built to reduce their
    overfitting
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建后，通过剪枝分割节点来减少它们的过拟合
- en: Figure 5.2 shows a different perspective on a decision tree. Figure 5.1 visualizes
    a tree as a graph based on two features, while figure 5.2 visualizes a decision
    tree in terms of partitions of the data itself. Each split of the tree is a line
    in the chart, and there are seven vertical lines (thus a result of binary conditions
    on the feature on the x-axis) and three horizontal ones (thus on the feature on
    the y-axis) for a total of 10 splits. You can consider the decision tree a success
    because each class is well separated into its partitions (each partition is a
    terminal node in the end). However, by observation, it also becomes evident that
    specific partitions have been carved out just to fit examples in a certain position
    in the space. There are a few partitions containing only one single case. Any
    new example risks being incorrectly classified if it doesn’t perfectly fit the
    training distribution (an overfitting situation).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 展示了决策树的不同视角。图 5.1 基于两个特征将树可视化为一个图，而图 5.2 则从数据本身分区的角度来可视化决策树。树的每个分割在图表中都是一条线，对于这个树，有七条垂直线（因此是
    x 轴上特征的二进制条件的结果）和三条水平线（因此是 y 轴上的特征），总共 10 个分割。你可以认为决策树是成功的，因为每个类别都很好地分离到其分区中（每个分区最终都是一个终端节点）。然而，通过观察，也变得明显的是，某些分区只是为了适应空间中某个位置上的示例而被划分出来。有几个分区只包含一个单独的案例。任何新的示例如果与训练分布不完全匹配（一个过拟合的情况），都有可能被错误分类。
- en: '![](../Images/CH05_F02_Ryan2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F02_Ryan2.png)'
- en: Figure 5.2 How a fully grown decision tree branching can also be interpreted
    as a series of dataset splits
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 如何将一个完全成长的决策树分支也解释为一系列数据集分割
- en: Figure 5.3 shows the same problem visualized with fewer splits—two for every
    feature. You can achieve this by pruning the previous tree splits backward, removing
    the ones enclosing too few training examples, or you accomplish that simply by
    limiting in the first place the tree’s growth—for instance, by imposing a maximum
    number of splits to be created. If you use fewer partitions, the tree may not
    fit the training data as perfectly as before. However, a simpler approach provides
    more confidence that new instances will likely be classified correctly, as the
    solution depends explicitly on single points in the training set.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 展示了使用更少的分割来可视化相同的问题——每个特征两个分割。你可以通过向后剪枝之前的树分割，移除包含训练样本过少的那些分割，或者你通过一开始就限制树的成长来实现这一点——例如，通过施加最大分割数。如果你使用更少的分区，树可能不会像以前那样完美地拟合训练数据。然而，一个更简单的方法提供了更多的信心，即新实例很可能会被正确分类，因为解决方案明确依赖于训练集中的单个点。
- en: '![](../Images/CH05_F03_Ryan2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F03_Ryan2.png)'
- en: Figure 5.3 The problem handled by a simpler decision tree, obtained by pruning
    or by limiting its growth
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 通过剪枝或限制其增长获得的一个更简单的决策树处理的问题
- en: Thinking of this algorithm in terms of underfitting and overfitting, it is a
    high-variance algorithm because its complexity always tends to exceed what should
    be given the problem and the data. It is not easy to find its sweet spot by tuning.
    In truth, the best way to use decision trees to achieve more accurate predictions
    is not as single models but as part of an ensemble of models. In subsequent subsections,
    we will explore ensemble methods such as bagging, random forests, and gradient
    boosting, an advanced method based on decision trees.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从欠拟合和过拟合的角度考虑这个算法，它是一个高方差算法，因为其复杂性总是倾向于超过给定的问题和数据应有的复杂度。通过调整很难找到它的最佳点。实际上，使用决策树实现更准确预测的最好方法不是作为单一模型，而是作为模型集合的一部分。在接下来的小节中，我们将探讨集合方法，如
    bagging、随机森林和基于决策树的梯度提升，这是一种高级方法。
- en: In this chapter, we’ll return to the Airbnb NYC dataset to illustrate the core
    gradient-boosted decision tree implementations and how the technique works. The
    code in the following listing reprises the data and some key functions and classes
    we previously used to illustrate other classical machine learning algorithms.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回到 Airbnb 纽约市数据集，以说明核心梯度提升决策树实现以及该技术是如何工作的。以下列表中的代码重新审视了之前用来展示其他经典机器学习算法的数据和一些关键函数和类。
- en: Listing 5.1 Reprising the Airbnb NYC dataset
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 重新审视 Airbnb 纽约市数据集
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① List of features to be excluded from data processing
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ① 需要从数据处理中排除的特征列表
- en: ② List of low-cardinality categorical features to be one-hot encoded
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ② 需要独热编码的低基数分类特征列表
- en: ③ List of high-cardinality categorical features to be ordinally encoded
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 需要按顺序编码的高基数分类特征列表
- en: ④ Creates a binary target indicating whether the price is above the mean (unbalanced
    binary target)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建一个二元目标，表示价格是否高于平均值（不平衡的二元目标）
- en: ⑤ Creates a binary target indicating whether the price is above the median (balanced
    binary target)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个二元目标，表示价格是否高于中位数（平衡的二元目标）
- en: ⑥ Creates a multiclass target by quantile binning the price into five classes
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 通过将价格划分为五个类别来创建一个多类目标
- en: ⑦ Sets the target for regression as the price column
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 将回归目标设置为价格列
- en: ⑧ Creates a column transformer that applies different transformations to different
    groups of features
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 创建一个列转换器，对不同的特征组应用不同的转换
- en: 'The code reads a CSV file containing data related to Airbnb listings in New
    York City in 2019 using the pandas library. It then defines several lists that
    categorize the features of the data into different types:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 代码使用pandas库读取包含2019年纽约市Airbnb列表数据的CSV文件。然后定义了几个列表，将数据的特征分类到不同的类型：
- en: '`excluding_list`—A list of features that should be excluded from the analysis,
    such as unique identifiers and text features'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`excluding_list`—应从分析中排除的特征列表，例如唯一标识符和文本特征'
- en: '`low_card_categorical`—A subset of categorical features that have a low cardinality
    (few unique values) and will be one-hot encoded'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_card_categorical`—具有低基数（少量唯一值）的分类特征子集，将进行独热编码'
- en: '`high_card_categorical`—A subset of categorical features that have a high cardinality
    (many unique values) and will be encoded using an ordinal encoding'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`high_card_categorical`—具有高基数（许多唯一值）的分类特征子集，将使用序数编码进行编码'
- en: '`continuous`—A list of continuous numerical features that will be standardized
    for analysis'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`continuous`—一个连续数值特征的列表，这些特征将被标准化以进行分析'
- en: 'The code then creates several target variables based on the Price feature of
    the data:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，代码根据数据的“价格”特征创建几个目标变量：
- en: '`target_mean`—A binary variable indicating whether the price is higher than
    the mean price of all listings'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_mean`—一个二元变量，表示价格是否高于所有列表的平均价格'
- en: '`target_median`—A binary variable indicating whether the price is higher than
    the median price of all listings'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_median`—一个二元变量，表示价格是否高于所有列表的中位数价格'
- en: '`target_multiclass`—A variable with five classes based on the quantiles of
    the price distribution'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_multiclass`—一个基于价格分布分位数划分的五个类别的变量'
- en: '`target_regression`—The actual price values, which will be used for regression
    analysis'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_regression`—实际的价格值，这些值将被用于回归分析'
- en: All these targets allow us to deal with different regression and classification
    problems and thus test machine learning algorithms. In this chapter, we will always
    use `target_median`, but you can experiment with all the other targets by making
    small changes in the code.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些目标使我们能够处理不同的回归和分类问题，从而测试机器学习算法。在本章中，我们将始终使用`target_median`，但你可以通过在代码中做小的改动来实验所有其他目标。
- en: 'Next, the code sets up several transformers to preprocess the data for the
    analysis in this chapter:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，代码设置了几种转换器，以预处理本章分析中的数据：
- en: '`categorical_onehot_encoding`—A one-hot encoding transformer for the low-cardinality
    categorical features'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categorical_onehot_encoding`—用于低基数分类特征的独热编码转换器'
- en: '`categorical_ord_encoding`—An ordinal encoding transformer for the high-cardinality
    categorical features'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categorical_ord_encoding`—用于高基数分类特征的序数编码转换器'
- en: '`numeric_passthrough`—A transformer that simply passes through the continuous
    numerical features'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numeric_passthrough`—一个简单传递连续数值特征的转换器'
- en: Finally, the code sets up a `ColumnTransformer` object that will apply the appropriate
    transformers to each subset of features based on their type. It applies one-hot
    encoding to the low-cardinality categorical features and passes through the continuous
    numerical features. The transformer is set to drop any features not explicitly
    included in the transformation steps and output concise feature names. The `sparse_threshold`
    parameter is set to zero to ensure that the transformer always returns dense arrays.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，代码设置了一个`ColumnTransformer`对象，该对象将根据特征类型对每个特征子集应用适当的转换器。它对低基数分类特征应用独热编码，并将连续数值特征传递通过。转换器被设置为丢弃未在转换步骤中明确包含的任何特征，并输出简洁的特征名称。`sparse_threshold`参数设置为零，以确保转换器始终返回密集数组。
- en: Listing 5.2 shows how a standard decision trees model is applied to our example
    problem. As in the examples seen in the previous chapter, we import the necessary
    modules from the Scikit-learn library, define a custom scoring metric based on
    accuracy, and set up a five-fold cross-validation strategy. Then we define a ColumnTransformer
    named `column_transform`, which orchestrates data preprocessing. It involves
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 展示了如何将标准决策树模型应用于我们的示例问题。正如在上一章中看到的示例一样，我们从Scikit-learn库中导入必要的模块，定义一个基于准确率的自定义评分指标，并设置五折交叉验证策略。然后我们定义一个名为`column_transform`的列转换器，它负责数据预处理。它包括
- en: Transforming categorical variables using a function `categorical_onehot_encoding`
    for specific low-cardinality categorical columns
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用函数`categorical_onehot_encoding`对特定的低基数分类列进行分类变量转换
- en: Passing through numeric features with a function `numeric_passthrough` for continuous
    variables
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用函数`numeric_passthrough`对连续变量进行数值特征传递
- en: Dropping any remaining unprocessed columns (`remainder='drop'`)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃任何剩余未处理的列（`remainder='drop'`）
- en: Setting some options like suppressing verbose feature names and not applying
    sparse matrix representation
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一些选项，如抑制详细特征名称和未应用稀疏矩阵表示
- en: At this point, a pipeline combining the ColumnTransformer with a decision tree
    classifier model is tested using cross-validation, which returns accuracy scores
    along with the average fit time and score time.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，使用交叉验证测试结合列转换器和决策树分类器模型的管道，返回准确度分数以及平均拟合时间和评分时间。
- en: Under the hood of the cross-validation procedure and the data pipelining, the
    dataset is separated multiple times by the decision tree classifier during the
    training based on a splitting value from a feature. The procedure can be algorithmically
    explained as “greedy” because the decision tree picks the feature with the best
    split at each step without questioning whether alternatives could lead to a better
    result. Despite such a simple approach, decision trees are effective machine learning
    algorithms. The process goes on until there are no more splits that improve the
    training, as shown in the following listing.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证过程和数据管道的底层，数据集在训练过程中被决策树分类器根据特征的一个分割值多次分割。这个过程可以从算法上解释为“贪婪”，因为决策树在每一步都选择具有最佳分割的特征，而不考虑其他替代方案是否可能带来更好的结果。尽管这种方法很简单，但决策树是有效的机器学习算法。这个过程会一直进行，直到没有更多的分割可以改善训练，如下面的列表所示。
- en: Listing 5.2 A decision tree classifier
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 一个决策树分类器
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Creates a column transformer that applies different transformations to categorical
    and numeric features
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个列转换器，对分类和数值特征应用不同的转换
- en: ② An instance of a decision tree classifier
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ② 决策树分类器的一个实例
- en: ③ A pipeline that sequentially applies column transformation and the decision
    tree model
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 依次应用列转换和决策树模型的管道
- en: ④ A five-fold cross-validation using the defined pipeline, calculating accuracy
    scores, and returning additional information
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用定义的管道进行五折交叉验证，计算准确度分数，并返回附加信息
- en: ⑤ Prints the mean and standard deviation of the accuracy scores from cross-validation
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印交叉验证准确度分数的均值和标准差
- en: The result we obtain in terms of accuracy is
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在准确度方面获得的结果是
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The result could be better after comparing the results of our previous experiments
    with other machine learning algorithms. We can determine this because the decision
    tree has overfitted, and it ended up building too many ramifications. We can get
    better performance by limiting its growth by trial and error (you have to state
    the `max_depth` parameter to do so). However, there are even better ways to obtain
    improved results from this algorithm. In the next subsection, we will examine
    the first of such methods, which is based on multiple decision trees based on
    variations of the examples and the employed features.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他机器学习算法的先前实验结果进行比较后，结果可能会更好。我们可以确定这一点，因为决策树已经过拟合，最终构建了太多的分支。我们可以通过尝试和错误来限制其增长以获得更好的性能（你必须声明`max_depth`参数来这样做）。然而，还有更好的方法可以从这个算法中获得改进的结果。在下一小节中，我们将检查这些方法中的第一个，它基于基于示例和使用的特征的变体构建的多个决策树。
- en: 5.1.1 Bagging and sampling
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 折叠和采样
- en: 'We have examined all the single learning algorithms with decision trees. Ensembling
    algorithms of the same type is the next step that can help you achieve more predictive
    power on your problem. The idea is intuitive: if a single algorithm can perform
    at a certain level, using the insights of multiple models or chaining them together
    (so that one can learn from the results and errors of the other) should render
    even better results. There are two core ensemble strategies:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经检查了所有基于决策树的单一学习算法。同类型的集成算法是下一步可以帮助你在你的问题上实现更多预测能力的步骤。这个想法是直观的：如果一个单一算法可以在某个水平上表现，使用多个模型或链式连接它们的见解（这样可以使一个从另一个的结果和错误中学习）应该会产生更好的结果。有两种核心的集成策略：
- en: '*Averaging*—Predictions are obtained by averaging the predictions of multiple
    models. Differences in how the models are built, for instance by pasting, bagging,
    random subspaces, and random patches as we will see in this section, lead to different
    results. The best example of ensemble models of this kind is the random forests
    algorithm, which is built on an approach similar to random patches.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平均法*—通过平均多个模型的预测来获得预测。模型构建方式的不同，例如通过粘贴、袋装、随机子空间和随机补丁（我们将在本节中看到），会导致不同的结果。这种类型集成模型的最好例子是随机森林算法，它是基于类似随机补丁的方法构建的。'
- en: '*Boosting*—Predictions are built as a weighted average of chained models, which
    are models sequentially built on the results of previous ones. The best example
    of a boosting algorithm is gradient boosting machines such as XGBoost and LightGBM.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提升法*—预测是通过链式模型的加权平均来构建的，这些模型是依次建立在先前模型结果之上的。提升算法的最好例子是梯度提升机，如XGBoost和LightGBM。'
- en: In the following subsection, we will look at random forests. Before delving
    into the random forests algorithm, it is necessary to spend some time on the other
    averaging approaches, not only because the random patches approach is built upon
    them but also because they point out solutions that are always worth applying
    to tabular data when you need to reduce the variance of the estimates, hence obtaining
    more reliable predictions, of any machine learning model you may want to use on
    your data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子节中，我们将探讨随机森林。在深入研究随机森林算法之前，有必要花一些时间在其他平均方法上，这不仅因为随机补丁方法建立在它们之上，而且因为它们指出了在需要减少估计的方差时始终值得应用于表格数据的解决方案，从而获得更可靠的预测，无论你希望在你的数据上使用哪种机器学习模型。
- en: '*Pasting* is the first approach to consider. Leo Breiman, the creator of the
    random forests algorithm, suggests that pasting consists of creating a set of
    different models trained on subsamples, obtained by sampling without replacement,
    of your training data. The models’ predictions are pooled together by averaging
    in the case of a regression problem or by majority voting in the case of a classification
    task.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*粘贴*是首先要考虑的方法。随机森林算法的创造者Leo Breiman建议，粘贴包括创建一组不同的模型，这些模型是在通过不重复抽样的子样本上训练的，这些子样本是从你的训练数据中获得的。在回归问题的情况下，通过平均来汇总模型的预测，在分类任务的情况下，通过多数投票来汇总。'
- en: The pros of pasting are
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 粘贴的优点是
- en: Improvement of the results by reducing the variance of the predictions by only
    partially increasing their bias, which is a measure of how far the predictions
    of a model are from the true values
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过仅部分增加偏差来减少预测的方差，从而提高结果，偏差是衡量模型预测值与真实值之间距离的指标。
- en: Predictions that are more robust and less affected by outliers
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测结果更稳健，受异常值影响较小。
- en: Reduction of the amount of data to learn at training time, thus reducing memory
    requirements
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少训练时需要学习的数据量，从而减少内存需求。
- en: The cons are
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点是
- en: Reduction of the amount of data available, which increases the bias because
    there is the chance of excluding important parts of the data distribution by sampling
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少可用的数据量，这会增加偏差，因为有可能通过采样排除数据分布中的重要部分。
- en: Very computationally intensive with complex algorithms
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常计算密集，具有复杂的算法
- en: The last con depends on your time constraints or available resources. Historically,
    averaging methods have been suggested to be applied using weak models (i.e., machine
    learning models that are very fast to be trained because of their simplicity,
    such as a linear regression or a k-nearest neighbors model). Practitioners observed
    that combining multiple weak models could beat the results of a single, more complex
    algorithm. However, weak models usually have a high bias problem, and by subsampling,
    you induce only some variance in their estimates, but their bias problem remains
    mostly untouched. Using an averaging approach has the main advantage in that it
    reduces the variance of the estimates by trading it with a bit more bias. Since
    weak models inherently carry a substantial bias, they might not achieve comparable
    results to the same approach applied to more complex models. In situations where
    more significant improvements are needed by reducing the variance of the estimates,
    employing an averaging strategy can be more effective with more complex models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个缺点取决于你的时间限制或可用资源。从历史上看，建议使用弱模型（即由于它们的简单性而非常快就能训练的机器学习模型，如线性回归或k最近邻模型）应用平均方法。从业者观察到，结合多个弱模型可以击败单个更复杂算法的结果。然而，弱模型通常存在高偏差问题，通过子采样，你只在其估计中引入了一些方差，但它们的偏差问题基本上没有改变。使用平均方法的主要优势在于，它通过以略微增加的偏差为代价来减少估计的方差。由于弱模型本质上携带大量的偏差，它们可能无法实现与应用于更复杂模型相同的方法的可比结果。在需要通过减少估计的方差来获得更显著改进的情况下，使用平均策略可以与更复杂的模型更有效地结合。
- en: '*Bagging*, also suggested by Leo Breimar as a better solution, differs from
    pasting because you switch from subsampling to bootstrapping. Bootstrapping consists
    of sampling with replacement multiple times from a data sample to approximate
    the population distribution of a statistic. Bootstrapping is a frequently employed
    statistical technique that allows us to estimate the variability and uncertainty
    of statistics relative to the underlying data population from which our sample
    has been drawn. By using the information from the available sample through multiple
    resamples that mimic the original population’s behavior, bootstrapping emulates
    the population’s behavior without requiring explicit knowledge about its statistical
    distribution.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bagging*，也被Leo Breimar提出作为一种更好的解决方案，与粘贴不同，因为你从子采样切换到自助采样。自助采样包括多次从数据样本中带替换地采样，以近似统计量的总体分布。自助采样是一种常用的统计技术，它允许我们估计相对于从我们的样本中抽取的潜在数据总体，统计量的变异性与不确定性。通过使用从可用样本中获取的信息，通过多次重采样来模拟原始总体行为，自助采样模拟了总体的行为，而无需显式了解其统计分布。'
- en: The reason for using bootstrapping in machine learning is to estimate the uncertainty
    of a model’s performance or to assess the distribution of a statistic. In addition,
    bootstrapping helps create more diverse variations of the original dataset for
    training and ensembling purposes. This is based on the observation that averaging
    multiple models reduces variance more if the predictions of the models being used
    are less correlated (i.e., more diverse). Subsampling creates diverse datasets
    to train. However, it has limitations because if you subsample aggressively—for
    instance, picking up less than 50% of the original data—you tend to introduce
    bias.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中使用自助采样的原因是为了估计模型性能的不确定性或评估统计量的分布。此外，自助采样有助于创建更多样化的原始数据集变体，用于训练和集成目的。这是基于观察，如果所使用的模型的预测较少相关（即更多样化），则平均多个模型可以减少方差。子采样创建多样化的数据集进行训练。然而，它有局限性，因为如果你进行积极的子采样——例如，选择不到原始数据的50%——你往往会引入偏差。
- en: In contrast, if you subsample in a more limited way, such as using 90% of the
    data, the resulting subsamples will tend to be correlated. Instead, bootstrapping
    is more efficient because, on average, you use about 63.2% of the original data
    at each bootstrap. For a detailed statistical explanation of such calculated proportions,
    see the detailed cross-validated answer at [https://mng.bz/zZ0w](https://mng.bz/zZ0w).
    Moreover, sampling with replacement tends to give results that mimic the original
    distribution of the data. Bootstrapping creates a set of more different datasets
    to learn from and, consequently, a set of more different predictions that can
    ensemble, reducing variance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果你以更有限的方式进行子采样，例如使用90%的数据，得到的子样本将倾向于相关。相反，自助法更有效，因为平均而言，你会在每个自助法中使用大约63.2%的原始数据。有关此类计算比例的详细统计解释，请参阅详细交叉验证答案[https://mng.bz/zZ0w](https://mng.bz/zZ0w)。此外，带替换的采样往往会产生模仿原始数据分布的结果。自助法创建了一组更多样化的数据集来学习，从而产生一组更多样化的预测，可以更有效地进行集成，减少方差。
- en: In fact, since in averaging we are building a distribution of predictions and
    getting the center of the distribution as our prediction, the more the averaged
    predictions resemble a random distribution, the less the center of the distribution
    will be biased by problems in the data picked up by the model (such as overfitting).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，由于在平均过程中我们正在构建一个预测分布，并将分布的中心作为我们的预测，平均预测越接近随机分布，分布中心受模型（如过拟合）收集到的数据问题的影响就越小。
- en: By contrast, with *random subspaces*, introduced by T. Ho [“The Random Subspace
    Method for Constructing Decision Forests,” *Pattern Analysis and Machine Intelligence*,
    20(8), 832-844, 1998], the sampling is limited only to features. This works because
    the model used for the ensemble is the decision tree, a model whose high variance
    of the estimates is greatly reduced in the ensemble by using only a part of the
    features for every model that is a part of it. The improved result is because
    the models trained on a subsample of the features tend to produce uncorrelated
    predictions—all the decision trees overfit the data but in a different way with
    respect to each other.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，通过*随机子空间*，由T. Ho [“The Random Subspace Method for Constructing Decision
    Forests,” *Pattern Analysis and Machine Intelligence*, 20(8), 832-844, 1998]引入，采样仅限于特征。这是因为用于集成的模型是决策树，这种模型通过仅使用每个模型的一部分特征来显著降低估计的高方差。改进的结果是因为在特征子样本上训练的模型往往会产生不相关的预测——所有的决策树都会过拟合数据，但相对于彼此而言，方式不同。
- en: 'Finally, with *random patches* [G. Louppe and P. Geurts, “Ensembles on Random
    Patches,” in *Machine Learning and Knowledge Discovery in Databases* (2012): 346–361],
    sampling of both samples and features are used together to achieve even more uncorrelated
    predictions that can be averaged even more profitably.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，通过*随机补丁* [G. Louppe和P. Geurts, “Ensembles on Random Patches,” in *Machine
    Learning and Knowledge Discovery in Databases* (2012): 346–361]，同时使用样本和特征的采样，以实现更多不相关的预测，从而可以更有效地进行平均。'
- en: 'Pasting, bagging, random subspaces, and random patches can all be implemented
    using Scikit-learn functions for bagging. The behavior of `BaggingClassifier`
    for classification tasks and `BaggingRegressor` for regression tasks can be controlled
    in regards to training data thanks to the following parameters:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 粘贴、Bagging、随机子空间和随机补丁都可以使用Scikit-learn的Bagging函数实现。通过以下参数，可以控制`BaggingClassifier`对分类任务和`BaggingRegressor`对回归任务的训练数据的行为：
- en: '`bootstrap`'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bootstrap`'
- en: '`max_sample`'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_sample`'
- en: '`max_features`'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`'
- en: By combining them according to each averaging method’s specifications, you can
    obtain all four of the averaging strategies we have described (see table 5.1).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过根据每种平均方法的规范组合它们，你可以获得我们描述的所有四种平均策略（见表5.1）。
- en: Table 5.1 Bagging and sampling strategies
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 Bagging和采样策略
- en: '| Averaging strategy | What happens to data | Parameters for BaggingClassifier/BaggingRegressor
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 平均策略 | 数据会发生什么 | BaggingClassifier/BaggingRegressor的参数 |'
- en: '| --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Pasting | Training examples are sampled without replacement | `bootstrap
    = False` `max_samples < 1.0` `max_features = 1.0`  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Pasting | 使用不替换方式对训练示例进行采样 | `bootstrap = False` `max_samples < 1.0` `max_features
    = 1.0`  |'
- en: '| Bagging | Training examples are sampled with replacement (bootstrapping)
    | `bootstrap = True` `max_samples = 1.0` `max_features = 1.0`  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Bagging | 使用替换方式（自助法）对训练示例进行采样 | `bootstrap = True` `max_samples = 1.0` `max_features
    = 1.0`  |'
- en: '| Random subspaces | Features are sampled (without replacement) | `bootstrap
    = False` `max_samples = 1.0` `max_features < 1.0`  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 随机子空间 | 采样特征（不替换） | `bootstrap = False` `max_samples = 1.0` `max_features
    < 1.0`  |'
- en: '| Random patches | Training examples and features are sampled without replacement
    | `bootstrap = False` `max_samples < 1.0` `max_features < 1.0`  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 随机补丁 | 在不替换的情况下采样训练示例和特征 | `bootstrap = False` `max_samples < 1.0` `max_features
    < 1.0`  |'
- en: By inputting the desired Scikit-learn model class in the parameter estimator,
    you can instead decide what algorithm to use for building the ensemble. A decision
    tree is the default, but you can decide what weak or strong models you prefer.
    In the following example, we apply a bagged classifier, setting the number of
    decision tree models to 300\. The following listing shows all the models contributing
    together to improve the low performances that, as we have seen from listing 5.2,
    a decision tree tends to produce in this problem.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在参数估计器中输入所需的Scikit-learn模型类，您可以决定用于构建集成所使用的算法。默认情况下是决策树，但您可以选择您更喜欢弱或强模型。在以下示例中，我们应用了一个Bagged分类器，将决策树模型的数量设置为300。以下列表显示了所有模型共同贡献以改善低性能，正如我们从列表5.2中看到的那样，决策树往往会产生这种问题的低性能。
- en: Listing 5.3 Bagged tree-based classifier
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 基于Bagged树的分类器
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Creates a BaggingClassifier ensemble model based on decision trees
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ① 基于决策树创建BaggingClassifier集成模型
- en: ② Sets bootstrap sampling for the BaggingClassifier
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ② 为BaggingClassifier设置自助采样
- en: ③ Sets no sampling of features for the BaggingClassifier
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 为BaggingClassifier设置不进行特征采样
- en: ④ Sets no sampling of data for the BaggingClassifier
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 为BaggingClassifier设置不进行数据采样
- en: ⑤ A column transformer that applies different transformations to categorical
    and numeric features
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 应用不同转换到分类和数值特征的列转换器
- en: ⑥ A pipeline that sequentially applies column transformation and the bagging
    classifier model
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 依次应用列转换和Bagging分类器模型的管道
- en: ⑦ Five-fold cross-validation using the defined pipeline and calculating accuracy
    scores
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 使用定义的管道进行五折交叉验证并计算准确度分数
- en: ⑧ Prints the mean and standard deviation of the accuracy scores from cross-validation
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 打印交叉验证准确度分数的均值和标准差
- en: 'The results take a little while more and look promising, but they are still
    not enough to compete with our previous solutions based on support vector machines
    and logistic regression:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果需要更多一些时间，但看起来很有希望，但它们仍然不足以与我们的基于支持向量机和逻辑回归的先前解决方案竞争：
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the next subsection, we take the next step in ensembling by revisiting random
    forests, which make use of the random patches in bagging for a good reason.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们通过重新审视随机森林来进一步探讨集成，随机森林利用Bagging中的随机补丁是有充分理由的。
- en: 5.1.2 Predicting with random forests
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 使用随机森林进行预测
- en: 'Random forests work similarly to bagging. Still, it also simultaneously applies
    random patches (training examples and features are sampled without replacement):
    it bootstraps the samples before each model is trained and subsamples the features
    during modeling. Since the basic algorithm used in a random forests ensemble is
    a decision tree built by binary splits, the feature sampling happens at every
    tree split when a set of features is sampled as potential candidates for the split
    itself.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的工作原理与Bagging类似。然而，它同时应用随机补丁（在不替换的情况下采样训练示例和特征）：在训练每个模型之前对样本进行自助采样，并在建模期间对特征进行子采样。由于随机森林集成中使用的基本算法是由二分分割构建的决策树，因此在采样一组特征作为分割本身的潜在候选时，特征采样发生在每个树的分割处。
- en: Allowing each decision tree in the ensemble to grow to its extremities may lead
    to overfitting the data and high variance in the estimates; employing bootstrapping
    and feature sampling might mitigate these problems. Bootstrapping ensures that
    the models are trained on slightly different data samples from the same distribution,
    while feature sampling at each split guarantees diverse tree structures. This
    combination helps generate a set of models that are quite distinct from one another.
    Different models produce very different predictions (hence, we can say that their
    predictions are quite uncorrelated), and that’s a great advantage for the averaging
    technique because, when ensembling to a single prediction vector, the result is
    more reliable and accurate predictions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 允许集成中的每个决策树生长到其极限可能会导致数据过拟合和估计的高方差；采用自助抽样和特征抽样可能有助于缓解这些问题。自助抽样确保模型在来自同一分布的不同数据样本上训练，而在每个分割处的特征抽样保证了不同的树结构。这种组合有助于生成一组彼此相当不同的模型。不同的模型产生非常不同的预测（因此，我们可以说它们的预测相当不相关），这对于平均技术来说是一个巨大的优势，因为当集成到一个单一的预测向量时，结果将更加可靠和准确。
- en: Figure 5.4 shows how random forests work. The figure illustrates a binary classification
    problem with a dataset of two classes. The dataset is modeled using multiple decision
    trees, employing bootstrapping and feature sampling techniques. These techniques
    result in different partitions of the dataset, represented in the top-most part
    of the figure by three example results. The trees partition the dataset space
    in diverse ways, showcasing the variability in their splitting strategies. To
    simplify the representation, only two features are shown, providing a clearer
    understanding of the process.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4展示了随机森林的工作原理。该图说明了具有两个类别的数据集的二元分类问题。数据集使用多棵决策树建模，采用自助抽样和特征抽样技术。这些技术导致数据集的不同分区，如图中顶部部分所示，由三个示例结果表示。这些树以不同的方式划分数据集空间，展示了它们分割策略的变异性。为了简化表示，只显示了两个特征，从而更清楚地理解这个过程。
- en: Finally, when all the results are put together by majority voting, where you
    pick the more frequent classification as a predicted class, the random forests
    will provide better predictions derived from the results of all the trees. This
    is shown in the bottom part of the figure, where different shades indicate the
    prevalence of one or another class in a specific partition. The definitive boundary
    between classes is shown as a black polygonal line in the majority voting. The
    line can be even smoother when using multiple trees, resembling a curve. Ensemble
    methods can approximately resemble any curve if given enough models to the ensemble.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当所有结果通过多数投票合并在一起时，即选择出现频率更高的分类作为预测类别，随机森林将提供由所有树的结果得出的更好预测。这如图中底部部分所示，其中不同的阴影表示在特定分区中一个或另一个类的普遍性。多数投票中的类之间的最终边界以黑色多边形线表示。当使用多棵树时，这条线甚至可以更平滑，类似于曲线。如果给集成足够多的模型，集成方法可以近似任何曲线。
- en: '![](../Images/CH05_F04_Ryan2.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F04_Ryan2.png)'
- en: Figure 5.4 How random forests arrives at its results by combining the different
    data partitioning due to its decision trees thanks to majority voting
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 随机森林如何通过多数投票结合其决策树的不同数据分区来得出结果
- en: Originally devised by Leo Breiman and Adele Cutler ([https://mng.bz/0Qlp](https://mng.bz/0Qlp)),
    though commercially protected, the algorithm has been open-sourced—hence the many
    different names of its implementations. Random forests open up even more interesting
    possibilities, apart from better predictions, because you can use the algorithm
    to determine feature importance and measure the degree of similarity of the cases
    in a dataset.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法最初由Leo Breiman和Adele Cutler([https://mng.bz/0Qlp](https://mng.bz/0Qlp))设计，尽管商业上受到保护，但该算法已被开源——因此其实现有许多不同的名称。除了更好的预测外，随机森林还开辟了更多有趣的可能性，因为你可以使用该算法来确定特征重要性并测量数据集中案例相似度的大小。
- en: In the example in listing 5.4, we test how random forests would work on our
    classification problem with the Airbnb NYC dataset. Apart from the algorithm,
    there are no differences from our standard data processing when applying decision
    trees. One-hot encoding turns low-categorical features into binaries, and numeric
    features are left as they are.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表5.4的例子中，我们测试了随机森林在Airbnb NYC数据集上如何处理我们的分类问题。在应用决策树时，除了算法外，与我们的标准数据处理没有区别。One-hot编码将低类别特征转换为二进制，而数值特征保持不变。
- en: Listing 5.4 Random forests classifier
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.4 随机森林分类器
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① A RandomForestClassifier with 300 estimators and a minimum number of samples
    at a leaf node set to 3
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ① 具有300个估计器和在叶子节点上的最小样本数设置为3的RandomForestClassifier
- en: ② A column transformer that applies different transformations to categorical
    and numeric features
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ② 一种对分类特征和数值特征应用不同转换的列转换器
- en: ③ A pipeline that sequentially applies column transformation and the random
    forests classifier model
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 依次应用列转换和随机森林分类器模型的管道
- en: ④ A five-fold cross-validation using the defined pipeline and calculating accuracy
    scores
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用定义的管道进行五折交叉验证并计算准确度分数
- en: ⑤ Prints the mean and standard deviation of the accuracy scores from cross-validation
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印交叉验证准确度分数的均值和标准差
- en: 'After running the script, you will obtain the following results, which are
    actually the best performance you will find in this chapter for this problem:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，你将获得以下结果，这实际上是本章中针对此问题的最佳性能：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The secret to obtaining a good result with random forests is to choose its few
    hyperparameters wisely. Though the random forests algorithm is a no-brainer since
    it works fine with default parameters, fine-tuning it will bring better results.
    First, the purpose of the algorithm is to reduce the variance of the estimates,
    and that’s done by setting a high enough number of `n_estimators`. The principle
    is that if you have many trees, you have a distribution of results, and if the
    results are randomly drawn, you have an effect similar to the regression to the
    mean (the best prediction) due to the law of large numbers. The bootstrapping
    of examples and sampling of the features to be considered for splitting is usually
    enough to make the resulting trees of the forests different enough to be considered
    “random draws.” However, you need enough draws to have a proper regression to
    the mean.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林获得良好结果的关键在于明智地选择其少数超参数。尽管随机森林算法在默认参数下表现良好，无需多想，但微调它将带来更好的结果。首先，算法的目的是减少估计的方差，这通过设置足够高的`n_estimators`数量来实现。原理是，如果你有很多树，你就有结果分布，如果结果是随机抽取的，由于大数定律，你会有回归到平均值（最佳预测）的效果。通常，示例的重新抽样和考虑分割的特征的抽样足以使森林中的树足够不同，可以被认为是“随机抽取”。然而，你需要足够的抽取次数才能有适当的回归到平均值。
- en: 'You need some tests to fine-tune how many trees you build since there’s always
    a sweet spot: after a certain number of trees, you won’t obtain any more improvements,
    and sometimes some decrements in performance will result instead. Also, setting
    a too-high number of trees to be built by the algorithm will increase its computational
    costs, and more time will be needed in training and inference. However, no matter
    how many trees you train for, if your variance starts high using the default settings
    of a random forests model, you can do little to reduce it. Here the tradeoff between
    variance and bias comes into play; that is, you can trade some variance, implying
    you are overfitting the data, for some higher bias.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一些测试来微调你将构建多少棵树，因为总有一个最佳点：在达到一定数量的树之后，你不会获得任何更多的改进，有时性能的下降反而会出现。此外，算法构建过多的树会增加其计算成本，训练和推理所需的时间将更多。然而，无论你训练多少棵树，如果你的方差在随机森林模型的默认设置下开始很高，你几乎无法减少它。在这里，方差和偏差之间的权衡就出现了；也就是说，你可以通过一些偏差来换取一些方差，这意味着你正在过度拟合数据。
- en: 'You can set a proper bias for random forests by making the following adjustments:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下调整来为随机森林设置适当的偏差：
- en: Setting a lower number of features to consider when looking for the best split
    by setting the `max_features` parameter
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置`max_features`参数来降低在寻找最佳分割时考虑的特征数量
- en: Setting a max number of splits per tree, which will limit its growth to a certain
    predefined extent, by setting the `max_depth` parameter
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置`max_depth`参数来设置每个树的最大分割数，从而限制其增长到预定义的某个程度
- en: Setting a minimum number of examples in the terminal leaves of the tree, which
    will limit its growth, by setting the `min_samples_leaf` parameter with a number
    higher than 1
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置`min_samples_leaf`参数（数值大于1）来设置树终端叶子中的最小示例数，从而限制其增长
- en: In the next section, we explore extremely randomized trees (ERT), a variant
    of random forests that can be quite handy when data is larger and noisy.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们探讨极端随机树（ERT），这是随机森林的一个变体，当数据更大且噪声较多时非常有用。
- en: 5.1.3 Resorting to extremely randomized trees
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 退回到极端随机树
- en: ERT (also known as extra-trees in Scikit-learn) is a more randomized kind of
    random forests algorithm. The reason is the choice of candidates for splits in
    the ensemble’s single trees. In random forests, the algorithm samples its candidates
    for each split and then decides on the best feature to use among the candidates.
    Instead, the feature to be split in ERT is not evaluated among the possible candidates
    but randomly chosen. Afterward, the algorithm evaluates the best split point in
    the randomly chosen feature. This has some consequences. First, since the resulting
    trees are even more uncorrelated, there is even less variance in predictions from
    ERT—but at the price of a higher bias. Randomly split features have an effect
    on the accuracy of the predictions. Second, ERT is more computationally efficient
    because it doesn’t test sets of features but a single feature each time for the
    best split. All these characteristics make ERT best suited for handling
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ERT（在Scikit-learn中也称为extra-trees）是一种更随机的随机森林算法。原因是集成中单个树进行分割时候选者的选择。在随机森林中，算法为每个分割采样其候选者，然后从候选者中选择最佳特征。相反，ERT中要分割的特征不是在可能的候选者中进行评估，而是随机选择。之后，算法在随机选择的特征中评估最佳的分割点。这有一些后果。首先，由于生成的树更加不相关，ERT的预测方差更小——但代价是更高的偏差。随机分割特征对预测的准确性有影响。其次，ERT在计算上更高效，因为它不需要测试特征集，而是每次只测试一个特征以找到最佳的分割。所有这些特性使ERT最适合处理
- en: '*High-dimensional data* because it will split features faster than any other
    decision-tree ensemble algorithm'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高维数据* 因为它将比任何其他决策树集成算法更快地分割特征'
- en: '*Noisy data* because the random feature and sample selection process can help
    reduce the influence of noisy data points, making the model more robust to extreme
    values'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*噪声数据* 因为随机特征和样本选择过程可以帮助减少噪声数据点的影响，使模型对极端值更加鲁棒'
- en: '*Imbalanced data* because, due to the random feature selection, the signals
    from the minority subset of the data won’t be systematically excluded in favor
    of the majority subset of the data'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不平衡数据* 因为，由于随机特征选择，数据少数子集的信号不会系统地排除，以利于数据多数子集'
- en: The following listing tests ERT by replacing the random forests in listing 5.4,
    where you built a model with the Airbnb NYC dataset to figure out if the price
    of a listing is above or below the median value.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表通过替换列表 5.4 中的随机森林来测试 ERT，在列表 5.4 中，你使用 Airbnb NYC 数据集构建了一个模型，以确定列表价格是否高于或低于中位数。
- en: Listing 5.5 ERTs classifier
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.5 ERTs 分类器
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① An ExtraTreesClassifier with 300 estimators and a minimum number of samples
    at a leaf node set to 3
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ① 一个具有 300 个估计器和在叶节点上的最小样本数设置为 3 的 ExtraTreesClassifier
- en: ② A column transformer that applies different transformations to categorical
    and numeric features
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ② 一个列转换器，对分类和数值特征应用不同的转换
- en: ③ A pipeline that sequentially applies column transformation and the random
    forests classifier model
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 一个管道，按顺序应用列转换和随机森林分类器模型
- en: ④ A five-fold cross-validation using the defined pipeline and calculating accuracy
    scores
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用定义的管道进行五折交叉验证并计算准确度分数
- en: ⑤ Prints the mean and standard deviation of the accuracy scores from cross-validation
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印交叉验证中准确度分数的平均值和标准差
- en: 'You obtain a bit better result than using random forests:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你获得的结果比使用随机森林略好一些：
- en: '[PRE8]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you run this example, you will see that training by ETR is much faster than
    by random forests, given that you use the same dataset and build the same number
    of trees. ETR becomes an interesting alternative when your dataset is larger (more
    cases) and even more when it is wider (more features) because it saves a lot of
    time picking the feature to split since it is randomly decided. By contrast, the
    random forests algorithm has to look for the best feature among a selection.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个示例，你会看到，在给定相同的dataset和构建相同数量的树的情况下，使用 ETR 训练比使用随机森林要快得多。当你的 dataset 更大（更多案例）甚至更宽（更多特征）时，ETR
    成为一个有趣的替代方案，因为它节省了大量选择分割特征的时间，因为它是随机决定的。相比之下，随机森林算法必须在选择中寻找最佳特征。
- en: The fact that splits are decided randomly is a great advantage when many collinear
    and noisy features are related to the target. The algorithm avoids picking the
    same signals as an algorithm driven by searching for the feature that best fits
    the target. In addition, you can see the working dynamics in feature splitting
    of an ETR as another way to trade variance with bias. Splitting randomly is a
    limitation for the algorithm and reduces the variance because the resulting set
    of trees will be very uncorrelated.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当许多共线性噪声特征与目标相关时，随机决定分割是一个巨大的优势。该算法避免像由寻找最佳拟合目标的特征驱动的算法那样选择相同的信号。此外，您还可以通过观察ETR的特征分割的工作动态，将其视为另一种以方差与偏差进行交易的方式。对于算法来说，随机分割是一个限制，因为它会导致生成的树集非常不相关。
- en: In the next section, we complete our overview of tree-based ensembles by examining
    gradient boosting. This slightly different ensembling approach is often more effective
    for tabular data problems than bagging or random patches.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们通过考察梯度提升来完善我们对基于树的集成方法的概述。这种略有不同的集成方法通常比袋装或随机补丁对表格数据问题更有效。
- en: 5.2 Gradient boosting
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 梯度提升
- en: In recent years, *gradient boosting decision trees* (GBDT) has firmly established
    itself as a cutting-edge method for tabular data problems. GBDT is generally considered
    a state-of-the-art machine learning method across a wide range of problems across
    multiple domains, including multiclass classification, advertising click prediction,
    and search engine ranking. When applied to standard tabular problems, you can
    expect GBDT to perform better than neural networks, support vector machines, random
    forests, and bagging ensembles.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在近年来，*梯度提升决策树*（GBDT）已牢固地确立了自己在表格数据问题上的尖端方法地位。GBDT通常被认为是在多个领域广泛问题上的最先进机器学习方法，包括多类分类、广告点击预测和搜索引擎排名。当应用于标准表格问题时，您可以期望GBDT的表现优于神经网络、支持向量机、随机森林和袋装集成。
- en: 'Above all, GBDT’s ability to handle heterogeneous features and its flexibility
    in the choice of the loss function and evaluation metrics make the algorithm most
    suitable for tabular data predictive modeling tasks. In sum, GBDT offers the following
    benefits for tabular data problems:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，GBDT处理异构特征的能力以及它在选择损失函数和评估指标方面的灵活性，使该算法最适合表格数据预测建模任务。总的来说，GBDT为表格数据问题提供了以下好处：
- en: With the proper hyperparameter tuning, it can achieve the best performance among
    all other techniques.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过适当的超参数调整，它可以在所有其他技术中实现最佳性能。
- en: There is no need for scaling or other monotone transformations of the features.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要对特征进行缩放或其他单调变换。
- en: It automatically captures nonlinear relationships in the data.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它自动捕捉数据中的非线性关系。
- en: It is robust to outliers and noisy data.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对异常值和噪声数据具有鲁棒性。
- en: It automatically handles missing data.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它自动处理缺失数据。
- en: It automatically selects the best features and can report their importance.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它自动选择最佳特征，并可以报告它们的重要性。
- en: All these characteristics depend on how the algorithm works, combining sequences
    of decision trees in a gradient descent optimization. In fact, in gradient boosting,
    starting from a constant value, you add tree models to an ensemble sequentially,
    each one correcting the errors of the previous models in a fashion similar to
    gradient descent optimization. Gradient boosting represents an evolution of the
    original boosting approach. Used in models such as Adaboost, in the original boosting,
    you just average models built on the residuals of the previous model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些特性都取决于算法的工作方式，结合决策树序列进行梯度下降优化。实际上，在梯度提升中，从常数开始，您按顺序向集成中添加树模型，每个模型都以前一个模型的误差为依据进行校正，类似于梯度下降优化。梯度提升代表了原始提升方法的演变。在Adaboost等模型中使用原始提升时，您只需对基于前一个模型残差的模型进行平均。
- en: In Adaboost, the algorithm fits a sequence of weak learners, any machine learning
    algorithm that consistently beats random guessing, to the data (for an explanation
    about how to choose weak learners, see [https://mng.bz/KG9P](https://mng.bz/KG9P)).
    It then attributes more weight to incorrect predictions and less to correct ones.
    Weighting helps the algorithm to focus more on the observations that are harder
    to predict. The process is concluded after multiple corrections by a majority
    vote in classification or an average of the predictions in regression.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Adaboost 中，算法对一系列弱学习器进行拟合，任何持续击败随机猜测的机器学习算法（有关如何选择弱学习器的解释，请参阅[https://mng.bz/KG9P](https://mng.bz/KG9P)）。然后，它对错误预测赋予更多权重，对正确预测赋予较少权重。加权有助于算法更多地关注难以预测的观测值。在分类中通过多数投票或在回归中通过预测的平均值后，这个过程通过多次修正而结束。
- en: 'By contrast, in gradient boosting, you rely on a double optimization: first
    that of the single trees that strive to reduce the error based on their optimization
    function and then the general one, involving calculating the error from the summation
    of the boosted model, in a form that mimics gradient descent, where you gradually
    correct the model’s predictions. Since you have an optimization also based on
    a second level, the optimization based on the error of the entire ensemble procedure,
    gradient boosting is more versatile than the previously seen tree ensembles because
    it allows arbitrary loss functions to be used when calculating how much the summation
    of the predictions of models diverges from the expected results.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在梯度提升中，你依赖于双重优化：首先是单个树根据其优化函数努力减少误差的优化，然后是总体优化，涉及计算提升模型的求和误差，其形式模仿梯度下降，其中你逐渐修正模型的预测。由于你还有一个基于第二层的优化，即基于整个集成过程误差的优化，因此梯度提升比之前看到的树集成更灵活，因为它允许在计算模型预测求和与预期结果差异时使用任意损失函数。
- en: Figure 5.5 visually represents how the training error decreases after adding
    a new tree. Each tree takes part in a gradient descent style optimization, contributing
    to predicting the correction of the residual errors from the previous trees.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 可视化地表示了添加新树后训练误差如何降低。每一棵树都参与梯度下降风格的优化，有助于预测前一棵树残差误差的修正。
- en: '![](../Images/CH05_F05_Ryan2.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F05_Ryan2.png)'
- en: Figure 5.5 How gradient descent works with boosted trees
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 梯度下降法与提升树的工作原理
- en: If gradient descent provides optimal results and flexibility in the optimization,
    having decision trees as base learning (ensembles, as seen, are not limited to
    decision trees) offers various advantages. This is because it automatically selects
    the features it needs. It doesn’t need to specify a functional form (a formula
    as in regression), scaling, or linear relationships between features and the target.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果梯度下降在优化中提供最优结果和灵活性，那么使用决策树作为基学习（如所见，集成并不限于决策树）提供了各种优势。这是因为它自动选择所需的特征。它不需要指定函数形式（如回归中的公式）、缩放或特征与目标之间的线性关系。
- en: In the next section, before seeing specific implementations in action (Scikit-learn,
    XGBoost, LightGBM), we will try building our simple implementation of gradient
    boosting to understand how to use this powerful algorithm.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，在看到具体实现（Scikit-learn、XGBoost、LightGBM）之前，我们将尝试构建我们自己的简单梯度提升实现，以了解如何使用这个强大的算法。
- en: 5.2.1 How gradient boosting works
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 梯度提升法的工作原理
- en: All implementations of GBDT offer a variety of hyperparameters that need to
    be set to get the best results on the data problem you are trying to solve. Figuring
    out what each setting does is a challenge, and being agnostic and leaving the
    task to an automatic tuning procedure doesn’t help too much since you will have
    challenges telling the tuning algorithm what to tune and how to tune.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 GBDT 的实现都提供了一系列超参数，需要设置以在您试图解决的数据问题上获得最佳结果。弄清楚每个设置的作用是一个挑战，而且对自动调整过程保持无知并将任务留给它并不会帮助太多，因为您将面临挑战告诉调整算法要调整什么以及如何调整。
- en: In our experience, writing down a simple implementation is the best way to understand
    how an algorithm works and to figure out how hyperparameters relate to predictive
    performances and results. Listing 5.6 shows a `GradientBoosting` class capable
    of addressing any binary classification problem, such as the Airbnb NYC dataset
    we are tackling as an example, using two parameters for the gradient descent procedure
    and the parameters from the decision tree model offered by Scikit-learn.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，写下简单的实现是理解算法工作原理的最佳方式，并找出超参数如何与预测性能和结果相关。列表 5.6 显示了一个 `GradientBoosting`
    类，它可以解决任何二元分类问题，例如我们作为示例处理的 Airbnb NYC 数据集，使用梯度下降过程的两参数和 Scikit-learn 提供的决策树模型的参数。
- en: 'The code creates a GradientBoosting class that comprises methods for fitting,
    predicting probabilities, and predicting class. Internally, it stores the sequence
    of fitted decision trees in a list, from where they can be accessed sequentially
    to reconstruct the following summation formula:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 代码创建了一个 GradientBoosting 类，它包含用于拟合、预测概率和预测类的方法。内部，它将拟合的决策树序列存储在一个列表中，可以从那里按顺序访问以重建以下求和公式：
- en: '![](../Images/CH05_F05_Ryan2-eqs-0x.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F05_Ryan2-eqs-0x.png)'
- en: In the formula,
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，
- en: H(X) is the gradient boosting model applied to predictors X
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H(X) 是应用于预测变量 X 的梯度提升模型
- en: M corresponds to the number of tree estimators used
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M 对应于使用的树估计器的数量
- en: ν represents the learning rate
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ν 代表学习率
- en: w^m instead represents the corrections from previous trees that have to be predicted
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: w^m 代替表示来自先前树的校正，这些校正需要被预测
- en: The h^m symbol refers to the mth decision tree used
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: h^m 符号指的是第 m 个决策树
- en: 'Interestingly, gradient boosting trees are always regression trees (even for
    classification problems)—hence our choice of using Scikit-learn’s DecisionTreeRegressor.
    This also explains why GBDT is better at predicting probabilities than other tree-based
    ensemble models: gradient boosting trees regress directly on the logit of class
    probability, thus optimizing in a fashion not too different from logistic regression.
    On the other hand, algorithms such as random forests are optimized for purity
    metrics, and they estimate probabilities by counting the fraction of a class in
    a terminal node, which is not truly a probability estimate. Generally, probabilities
    outputted by GBDTs are correct, and they rarely require subsequent probabilistic
    calibration, which is a post-processing step used to adjust predicted probabilities
    to improve their accuracy and reliability in applications where probability estimates
    are paramount, such as medical diagnosis (e.g., disease detection), fraud detection,
    or credit risk assessment.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，梯度提升树总是回归树（即使是对于分类问题）——因此我们选择使用 Scikit-learn 的 DecisionTreeRegressor。这也解释了为什么
    GBDT 在预测概率方面比其他基于树的集成模型更好：梯度提升树直接对类别概率的对数几率进行回归，从而以一种与逻辑回归不太不同的方式优化。另一方面，像随机森林这样的算法是针对纯度指标进行优化的，它们通过计算终端节点中一个类的比例来估计概率，这并不是真正的概率估计。一般来说，GBDT
    输出的概率是正确的，并且很少需要后续的概率校准，这是一种后处理步骤，用于调整预测概率以提高其在概率估计至关重要的应用中的准确性和可靠性，例如医疗诊断（例如，疾病检测）、欺诈检测或信用风险评估。
- en: In our code implementation, we allow passing any parameter for the DecisionTreeRegressor
    (see [https://mng.bz/9YQx](https://mng.bz/9YQx)), though the most useful are the
    ones related to the complexity of the tree development, such as `max_depth`, fixing
    the maximum depth of the tree, or `min_samples_split` and `min_samples_leaf`,
    the minimum number of samples needed to split an internal node or to be at a leaf
    node, respectively.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码实现中，我们允许传递任何参数给 DecisionTreeRegressor（见 [https://mng.bz/9YQx](https://mng.bz/9YQx)），尽管最有用的是与树发展复杂度相关的参数，例如
    `max_depth`（固定树的深度最大值），或 `min_samples_split` 和 `min_samples_leaf`（分别表示分割内部节点或成为叶节点所需的最小样本数）。
- en: The role of each tree regressor is to provide a w vector containing the learned
    corrections to be summed together with the previous estimates after being weighted
    by the learning rate. Each w vector depends on the previous one because it is
    produced by a tree regressor trained on the gradients necessary to correct the
    estimates to match the true classification labels. The chained vectors w resemble
    a sequence of gradient corrections—at first large, then finer and finer, converging
    toward an optimal output prediction. Such gradient descent is perfectly similar
    to the gradient descent optimization procedure we introduced in chapter 4\. In
    addition, by changing the cost function on which you base your gradients’ computation,
    you can ask the GBDT to optimize for different loss functions.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 每个树回归器的角色是提供一个 w 向量，其中包含要加到先前估计中的学习率加权的学习修正。每个 w 向量都依赖于前一个，因为它是由一个训练在梯度上的树回归器产生的，这些梯度是必要的，以纠正估计以匹配真实的分类标签。链式向量
    w 类似于一系列梯度修正——最初很大，然后越来越精细，趋向于最优输出预测。这种梯度下降与我们在第 4 章中介绍的梯度下降优化过程完全相似。此外，通过更改基于其梯度计算的成本函数，您可以要求
    GBDT 优化不同的损失函数。
- en: Listing 5.6 Building a gradient boosting classifier
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.6 构建梯度提升分类器
- en: '[PRE9]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Sigmoid function implementation used for probability transformation that converts
    logits back into probabilities
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ① 用于概率转换的 Sigmoid 函数实现，将 logits 转换回概率
- en: ② Logit function implementation used to transform probabilities into logits
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ② 用于将概率转换为 logits 的 Logit 函数实现
- en: ③ Calculates the gradient of the loss function (negative log-likelihood) with
    respect to the predictions
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算损失函数（负对数似然）相对于预测的梯度
- en: ④ Initializes the model with the logit-transformed mean of the target values
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用目标值的对数变换均值初始化模型
- en: ⑤ Fits a decision tree regressor to the negative gradient of the log-odds transformed
    target
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将决策树回归器拟合到对数似然转换目标的负梯度
- en: ⑥ Updates the predicted values using the output of the fitted tree with a learning
    rate factor
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用学习率因子更新拟合的树的输出预测值
- en: ⑦ Predicting back requires cumulating predictions from all the trees.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 预测需要累积来自所有树的预测
- en: As in the gradient descent we have seen applied to linear models, you rely on
    making the process stochastic to avoid the optimization being struck on a suboptimal
    solution, which is achieved by sampling rows or columns before the training of
    each decision tree. In addition, you use early stopping to prevent the GBDT from
    using too many decision trees in sequence and adapting too much to the training
    data. We will demonstrate early stopping in the next chapter.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在应用于线性模型的梯度下降中看到的，您依赖于使过程随机化以避免优化陷入次优解，这是通过在训练每个决策树之前采样行或列来实现的。此外，您使用提前停止来防止
    GBDT 顺序使用过多的决策树并过度适应训练数据。我们将在下一章中演示提前停止。
- en: Now that we have explained the inner workings of our GradientBoosting class,
    we can now experiment with it. We’ll use the Airbnb NYC dataset and begin by dividing
    it into a training set and a testing set. This will involve creating two lists
    of row indexes—one for the training set and one for the testing set—employing
    the Scikit-learn function `train_test_split` ([https://mng.bz/jp1z](https://mng.bz/jp1z)).
    We instantiate our GradientBoosting class, which requires a learning rate of 0.1
    and 300 decision trees, with a maximum depth of four branches and terminal leaves
    with at least three examples. After transforming the training data, by treating
    numeric and categorical features, we fit the model, predict the test set, and
    evaluate the results.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解释了我们的 GradientBoosting 类的内部工作原理，我们现在可以对其进行实验。我们将使用 Airbnb 纽约数据集，并首先将其分为训练集和测试集。这需要创建两个行索引列表——一个用于训练集，一个用于测试集——使用
    Scikit-learn 函数 `train_test_split` ([https://mng.bz/jp1z](https://mng.bz/jp1z))。我们实例化我们的
    GradientBoosting 类，它需要一个学习率为 0.1 和 300 个决策树，最大深度为四个分支，终端叶子节点至少有三个示例。在通过处理数值和分类特征转换训练数据后，我们拟合模型，预测测试集，并评估结果。
- en: Listing 5.7 Testing our gradient boosting class
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.7 测试我们的梯度提升类
- en: '[PRE10]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Splits the dataset indices into training and test sets using a fixed random
    seed
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用固定随机种子将数据集索引分割为训练集和测试集
- en: ② Initializes a GradientBoosting model with specified hyperparameters
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用指定的超参数初始化 GradientBoosting 模型
- en: ③ Applies the column transformations to the training data
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将列转换应用于训练数据
- en: ④ Extracts the target values corresponding to the training data
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 提取与训练数据对应的目标值
- en: ⑤ Applies the same column transformations to the test data
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将相同的列转换应用于测试数据
- en: ⑥ Extracts the target values corresponding to the test data
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 提取与测试数据对应的目标值
- en: ⑦ Calculates the accuracy score by comparing the predicted labels with the actual
    test labels
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 通过比较预测标签与实际测试标签来计算准确度得分
- en: ⑧ Prints the calculated accuracy score
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 打印计算出的准确度得分
- en: The evaluated accuracy on our test set is
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的测试集上的评估准确度是
- en: '[PRE11]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is a very good result, pointing out that even our basic implementation
    is able to do a very good job on the data we are working with. In the next section,
    we will investigate the results obtained and observe a key characteristic of GBDT
    models that distinguishes them from other decision tree ensembles.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常好的结果，表明即使是我们基本的实现也能在我们的数据上做得很好。在下一节中，我们将调查获得的结果，并观察 GBDT 模型的一个关键特征，这个特征使它们区别于其他决策树集成。
- en: 5.2.2 Extrapolating with gradient boosting
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 使用梯度提升进行外推
- en: In our implementation from scratch of a GBDT, we can visualize how the model
    fits the data by predicting the same training set. The visualization shown in
    figure 5.6, created by the small code snippet in listing 5.8, is a normalized
    density histogram. In a normalized density histogram, the height of each bar represents
    the relative frequency of data points falling within a specific bin, and the total
    area under the histogram becomes equal to 1\. The result depicts a distribution
    of values predominantly polarized to the extremities of 0-1 boundaries, showing
    a model quite decisive in classifying the examples.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们从头开始实现的 GBDT 中，我们可以通过预测相同的训练集来可视化模型如何拟合数据。图 5.6 中所示的可视化，由列表 5.8 中的小代码片段创建，是一个归一化密度直方图。在归一化密度直方图中，每个柱子的高度代表落在特定区间内的数据点的相对频率，直方图下的总面积等于
    1。结果描绘了一个值分布，主要偏向 0-1 边界的极端，表明模型在分类示例时非常果断。
- en: Listing 5.8 Plotting gradient boosting predicted probabilities
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.8 绘制梯度提升预测概率图
- en: '[PRE12]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Generates predicted probabilities for the test data using the trained model
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用训练模型为测试数据生成预测概率
- en: ② Creates a histogram of the predicted probabilities with specified bins and
    normalized density
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建具有指定区间和归一化密度的预测概率直方图
- en: '![](../Images/CH05_F06_Ryan2.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F06_Ryan2.png)'
- en: Figure 5.6 A histogram describing the fitted probabilities for a gradient boosting
    classification, showing how the model has strongly decided for most cases if they
    are positive or negative
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 描述梯度提升分类拟合概率的直方图，展示了模型在大多数情况下如何强烈决定它们是正还是负
- en: Our implementation, under the hood, uses a regression loss, the squared loss,
    whose gradient is equal to the residual of the probabilities transformed into
    logits. For a definition of logits, see [https://mng.bz/W214](https://mng.bz/W214).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现底层使用回归损失，即平方损失，其梯度等于将概率转换为 logit 的残差。关于 logit 的定义，请参阅 [https://mng.bz/W214](https://mng.bz/W214)。
- en: The logits for a probability p are calculated as
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 概率 p 的 logit 计算如下
- en: '![](../Images/CH05_F06_Ryan2-eqs-1x.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F06_Ryan2-eqs-1x.png)'
- en: The advantage of this definition is that the logit function maps the probabilities
    to a log odds scale, which is an unbounded scale that ranges from negative infinity
    to positive infinity, allowing us to treat our problem as a regression problem.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这种定义的优势在于 logit 函数将概率映射到对数几率尺度，这是一个无界的尺度，范围从负无穷大到正无穷大，使我们能够将我们的问题视为回归问题。
- en: This means that at each iteration, the gradient boosting algorithm fits a regression
    model to the gradient of the loss function with respect to the logit values, which
    corresponds to the difference between the logit of the true target values and
    the current predictions expressed in logits. This approach allows the algorithm
    to iteratively improve the predictions by adjusting them in the direction of the
    steepest descent of the loss function and to finally have a logit prediction that
    is bounded in the range between 0 and 1, thanks to the inverse function of the
    logit, the sigmoid. A sigmoid is a mathematical function that maps its input to
    a value between zero and one, providing a smooth and continuous curve.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在每次迭代中，梯度提升算法都会将回归模型拟合到损失函数相对于logit值的梯度，这对应于真实目标值的logit与当前预测（以logit表示）之间的差异。这种方法允许算法通过调整预测以沿着损失函数的最陡下降方向迭代改进预测，并最终通过logit的逆函数sigmoid，得到一个在0到1之间有界的logit预测。sigmoid是一种数学函数，它将输入映射到0到1之间的值，提供一条平滑且连续的曲线。
- en: The formula for the sigmoid is
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid的公式为
- en: '![](../Images/CH05_F06_Ryan2-eqs-2x.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F06_Ryan2-eqs-2x.png)'
- en: where
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*σ*(*x*) represents the sigmoid function applied to the input value x.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*σ*(*x*)表示将sigmoid函数应用于输入值x。'
- en: exp(–*x*) is the exponential function, where exp denotes Euler’s number (approximately
    2.71828) raised to the power of –x.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: exp(–*x*)是指数函数，其中exp表示欧拉数（约等于2.71828）的–x次幂。
- en: 1 + exp(–*x*) is the denominator, which ensures that the output of the sigmoid
    function is always positive.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 + exp(–*x*)是分母，这确保了sigmoid函数的输出始终为正。
- en: 1 / (1 + exp(–*x*)) represents the division of 1 by the denominator, resulting
    in the output value of the sigmoid function.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 / (1 + exp(–*x*))表示分母的倒数，结果是sigmoid函数的输出值。
- en: It is commonly used in machine learning and statistical models to convert logit
    predictions into probabilities.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 它在机器学习和统计模型中常用，用于将logit预测转换为概率。
- en: What if, instead, we treat the problem as a regression one? In listing 5.9,
    we define a `GradientBoostingRegression` class by building on our `GradientBoosting`
    class and overwriting the fit and predict methods by removing logit and sigmoid
    transformations.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将问题视为回归问题会怎样？在列表5.9中，我们通过构建我们的`GradientBoosting`类并覆盖fit和predict方法（通过移除logit和sigmoid转换）来定义一个`GradientBoostingRegression`类。
- en: Listing 5.9 Testing a gradient boosting regression class
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.9 测试梯度提升回归类
- en: '[PRE13]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Initializes predictions with mean of y
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用y的均值初始化预测
- en: ② Fits the tree to the negative gradient
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将树拟合到负梯度
- en: ③ Updates predictions with tree’s predictions scaled by learning rate
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用学习率缩放树预测更新预测
- en: ④ Predicting back requires cumulating predictions from all the trees.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 预测回溯需要累积所有树的预测。
- en: ⑤ Plots a histogram of regression predicted probabilities
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 绘制回归预测概率的直方图
- en: When the code in listing 5.9 is run, it will produce a histogram of the fit
    predictions, as shown in figure 5.7\. Figure 5.7 shows how fit probabilities exceed
    the 0-1 boundaries. As with linear regression, which is a weighted combination
    based on the features, gradient boosting, a weighted combination based on the
    results of a chained sequence of models, can extrapolate beyond the limits of
    the learned target. Such extrapolations are impossible with other ensembles based
    on decision trees, such as random forests. Decision trees in regression cannot
    predict anything exceeding the values seen in training since predictions are based
    on means of training subsamples. The extrapolative potentiality of GBDT, based
    on the fact that they are an additive ensemble, is the basis for their success
    with time series, where you extrapolate future results that may be very different
    from past ones.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行列表5.9中的代码时，它将生成拟合预测的直方图，如图5.7所示。图5.7显示了拟合概率如何超过0-1边界。与基于特征的加权组合的线性回归一样，基于链式序列模型结果的加权组合梯度提升可以超出学习目标的范围。与其他基于决策树的集成（如随机森林）相比，这种外推是不可能的。回归中的决策树无法预测超出训练中看到的值，因为预测是基于训练子样本的均值。GBDT（梯度提升决策树）的外推潜力，基于它们是加性集成的事实，是它们在时间序列中成功的基础，在时间序列中，你外推可能非常不同于过去的未来结果。
- en: As a caveat, however, always consider that the extrapolative capabilities of
    GBDTs cannot be extended as far as could be achieved using a linear model. In
    time series predictive situations where the value to be predicted is way too far
    from the targets you provided for training, for instance in case of an outlier,
    the extrapolation will be limited, missing a correct estimation. A linear model,
    which directly associates a linear relationship between the input data and the
    predictions, could prove more suitable in such situations. Linear models are capable
    of handling extreme predictions for completely unseen outlying data points. To
    offer an alternative to decision trees as base learners in such situations, many
    GBDT implementations offer linear boosting by simply ensembling linear models
    (such as in the XGBoost implementation) or applying a piecewise linear gradient
    boosting tree, where linear models are built on the terminal nodes of a decisions
    trees (such as in LightGBM implementation).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，GBDTs的外推能力不能像使用线性模型那样延伸得那么远。在时间序列预测场景中，如果预测的值远远超出了你为训练提供的目标值，例如在异常值的情况下，外推将受到限制，无法进行正确的估计。在这种情况下，直接将输入数据与预测关联的线性模型可能更合适。线性模型能够处理完全未见的异常数据点的极端预测。为了在这种情况下提供决策树作为基学习器的替代方案，许多GBDT实现提供了通过简单集成线性模型（如XGBoost实现）或应用分段线性梯度提升树来实现线性提升，其中线性模型建立在决策树的终端节点上（如LightGBM实现）。
- en: '![](../Images/CH05_F07_Ryan2.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图像描述](../Images/CH05_F07_Ryan2.png)'
- en: Figure 5.7 A histogram describing fitted probabilities with a gradient boosting
    regression model where some probabilities exceed the 0-1 boundary
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 使用梯度提升回归模型拟合概率的直方图，其中一些概率超过了0-1边界
- en: Also, the strength of GBDTs in time series problems relies on their being automatic
    in choosing the information for the predictions, with very few hyperparameters
    to set. All you need to do is to have enough examples, at least in the range of
    thousands of data points, and to have some careful engineering of time series
    features such as lagged values and moving averages at different time horizons.
    For shorter series, classical time series methods such as ARIMA or exponential
    smoothing are still the recommended choice. For complex problems such as hierarchically
    structured series, GBDTs can outperform even the most sophisticated deep learning
    architectures explicitly designed for time series data. For example, GBDTs excel
    in solving problems found in supermarket networks where both slow- and fast-moving
    goods are sold.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GBDTs在时间序列问题中的优势在于它们在预测信息选择上的自动性，设置的超参数非常少。你所需要做的就是拥有足够的示例，至少在数千个数据点的范围内，并对时间序列特征进行一些仔细的工程化设计，例如不同时间跨度的滞后值和移动平均。对于较短的序列，经典的时间序列方法，如ARIMA或指数平滑，仍然是推荐的选择。对于如层次结构序列等复杂问题，GBDTs甚至可以超越为时间序列数据专门设计的最复杂的深度学习架构。例如，GBDTs在解决超市网络中的问题方面表现出色，在这些网络中既销售慢销品也销售快销品。
- en: 'A clear example of the advantage of GBDTs in time series analysis has been
    demonstrated during the M5 forecasting competition recently held on Kaggle ([https://github.com/Mcompetitions/M5-methods](https://github.com/Mcompetitions/M5-methods)),
    where solutions made by a LightGBM algorithm proved superior to deep learning
    architectures designed for the task of predicting in hierarchical structured series,
    such as DeepAR ([https://arxiv.org/abs/1704.04110](https://arxiv.org/abs/1704.04110))
    or NBEATS ([https://arxiv.org/abs/1905.10437](https://arxiv.org/abs/1905.10437)).
    A lucid and insightful analysis of the competition and the success and ubiquity
    of tree-based methods in the practice of time series analysis can be found in
    the paper “Forecasting with Trees” by Tim Januschowski et al. [*International
    Journal of Forecasting* 38.4 (2022): 1473–1481: [https://mng.bz/8O4Z](https://mng.bz/8O4Z)].'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '最近在Kaggle上举行的M5预测竞赛中，GBDTs在时间序列分析中的优势得到了明确的展示([https://github.com/Mcompetitions/M5-methods](https://github.com/Mcompetitions/M5-methods))，在该竞赛中，LightGBM算法提出的解决方案优于为预测层次结构序列任务设计的深度学习架构，如DeepAR
    ([https://arxiv.org/abs/1704.04110](https://arxiv.org/abs/1704.04110))或NBEATS
    ([https://arxiv.org/abs/1905.10437](https://arxiv.org/abs/1905.10437))。在Tim Januschowski等人撰写的论文“使用树进行预测”中，可以找到对竞赛和基于树的方法在时间序列分析实践中的成功和普遍性的清晰而深入的分析。[国际预测杂志*
    38.4 (2022): 1473–1481: [https://mng.bz/8O4Z](https://mng.bz/8O4Z)]。'
- en: 5.2.3 Explaining gradient boosting effectiveness
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 解释梯度提升的有效性
- en: Today, despite the remarkable results in image and text recognition and generation,
    neural networks do not match the performance on tabular data of gradient boosting
    solutions such as XGBoost and LightGBM. Both practitioners and participants in
    data science competitions favor these solutions. For instance, see the “State
    of Competitive Machine Learning” regarding tabular competitions at [https://mlcontests.com/tabular-data/](https://mlcontests.com/tabular-data/).
    But where exactly does the advantage that GBDTs have over deep neural networks
    (DNNs) come from? From our experience building a gradient boosting classifier,
    we could appreciate how the algorithm combines gradient descent with the flexibility
    of decision trees with heterogeneous data. Is this enough to explain why GBDTs
    are so effective with tabular data?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，尽管在图像和文本识别与生成方面取得了显著成果，但神经网络在表格数据上的性能并不匹配梯度提升解决方案（如XGBoost和LightGBM）。实践者和数据科学竞赛的参与者都倾向于这些解决方案。例如，参见关于表格竞赛的“竞争性机器学习状态”报告，[https://mlcontests.com/tabular-data/](https://mlcontests.com/tabular-data/)。但梯度提升决策树（GBDTs）相对于深度神经网络（DNNs）的优势究竟来自何处？从我们构建梯度提升分类器的经验来看，我们可以欣赏到该算法如何将梯度下降与异构数据的决策树的灵活性相结合。这足以解释为什么GBDTs在表格数据上如此有效吗？
- en: '“Why Do Tree-Based Models Still Outperform Deep Learning on Typical Tabular
    Data?” by Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux (Thirty-sixth Conference
    on Neural Information Processing Systems Datasets and Benchmarks Track, 2022:
    [https://hal.science/hal-03723551v2/document](https://hal.science/hal-03723551v2/document))
    is a recent study that tries to shed some light on the different performances
    of deep learning architectures and gradient-boosting decision trees. The study
    shows that tree-based methods outperform deep learning methods (even modern architectures)
    in achieving good predictions on tabular data. The authors explicitly focus on
    the heterogeneity of columns that distinguishes tabular data from datasets with
    exclusively continuous features (we could refer to them as homogenous tabular
    datasets) and define a standard benchmark using 45 open datasets. They considered
    only data from about 10,000 samples, consisting of columns of different types,
    including numerical features with different units and categorical features, because
    that’s considered the typical situation with tabular datasets.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由Leo Grinsztajn、Edouard Oyallon和Gael Varoquaux撰写的《为什么基于树的方法在典型的表格数据上仍然优于深度学习？》（第三十六届神经信息处理系统会议数据集和基准测试轨道，2022年：[https://hal.science/hal-03723551v2/document](https://hal.science/hal-03723551v2/document)）是一篇近期的研究，试图揭示深度学习架构和梯度提升决策树的不同性能。研究表明，基于树的方法在表格数据上实现良好预测方面优于深度学习方法（即使是现代架构）。作者明确关注区分表格数据与仅具有连续特征的集合（我们可以称之为同质表格数据集）的列异质性，并使用45个公开数据集定义了一个标准基准。他们只考虑了大约10,000个样本的数据，包括不同类型的列，包括具有不同单位的数值特征和分类特征，因为这被认为是表格数据集的典型情况。
- en: Various deep learning models, including multilayer perceptrons (MLPs), ResNets,
    SAINT, and FTtransformer, were tried, but tree-based methods were found to have
    better performance with less hyperparameter tuning. Even when considering only
    numerical features, tree-based methods outperformed deep learning methods. This
    advantage became more pronounced when considering fitting time, although the hardware
    used (including GPUs) also influenced the results. The gap between the two methods
    was narrower on large datasets, which are not typical for tabular data.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试了各种深度学习模型，包括多层感知器（MLPs）、ResNets、SAINT和FTtransformer，但发现基于树的方法在更少的超参数调整下表现出更好的性能。即使只考虑数值特征，基于树的方法也优于深度学习方法。当考虑到拟合时间时，这种优势更为明显，尽管所使用的硬件（包括GPU）也影响了结果。在大数据集上，这两种方法之间的差距更小，而大数据集对于表格数据来说并不典型。
- en: The authors also investigated the features of tabular data that explain the
    performance difference between tree-based and deep learning methods. They found
    that smoothing the outcome in feature space narrowed the gap since deep architectures
    struggle with irregular patterns, whereas smoothness does not affect tree models.
    Removing uninformative features narrowed the gap for MLP-like neural architectures
    more. However, it was only after applying random rotations to the data that deep
    architectures outperformed tree models.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还研究了表格数据特征，这些特征解释了基于树和深度学习方法之间的性能差异。他们发现，在特征空间中平滑结果缩小了差距，因为深度架构难以处理不规则模式，而平滑性不会影响树模型。移除无信息特征对于类似MLP（多层感知器）的神经网络架构缩小差距更为明显。然而，只有在将数据应用随机旋转之后，深度架构才优于树模型。
- en: Random rotations refer to applying a random rotation matrix to the input features
    of a dataset before feeding them into a machine learning model. This rotation
    matrix is a square matrix that preserves the length of vectors and the angles
    between them, ensuring that the rotated data remains equivalent to the original
    data. Random rotations are used in machine learning for various purposes, including
    enhancing the diversity of ensembles, improving the robustness of models, and
    addressing rotation invariance in tasks such as computer vision and machine learning
    for quantum chemistry. Such a technique, perfectly reversible, however, tends
    to obscure the relationship between predictors and the target for tree-based algorithms,
    whereas deep learning models are unaffected thanks to their representative learning
    capacity to learn also the applied rotation.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 随机旋转是指在将数据集的输入特征馈送到机器学习模型之前，对这些特征应用一个随机旋转矩阵。这个旋转矩阵是一个方阵，它保持向量的长度和它们之间的角度，确保旋转后的数据与原始数据等效。随机旋转在机器学习中用于各种目的，包括增强集成方法的多样性、提高模型的鲁棒性，以及解决计算机视觉和量子化学等任务中的旋转不变性问题。然而，这种完全可逆的技术往往会使基于树的算法中预测变量与目标之间的关系变得模糊，而深度学习模型则不受影响，这得益于它们强大的学习能力，能够学习到应用的旋转。
- en: This result doesn’t necessarily indicate an advantage of DNNs but rather a limitation
    of GBDTs. Deep architectures are rotationally invariant, which means they can
    detect rotated signals, as in image recognition, where certain images can be recognized
    regardless of their orientation. In contrast, GBDTs are not rotationally invariant
    and can only detect signals that are always oriented in the same fashion, since
    they operate based on splitting rules. Therefore, using any kind of rotation on
    data, such as a principal component analysis or a singular value decomposition,
    can be detrimental to GBDTs. DNNs, unaffected by rotation, can catch up in these
    situations.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果并不一定表明DNNs（深度神经网络）的优势，而更可能是GBDTs的局限性。深度架构具有旋转不变性，这意味着它们可以检测旋转信号，如在图像识别中，某些图像可以无论其方向如何都能被识别。相比之下，GBDTs不具有旋转不变性，只能检测始终以相同方式定向的信号，因为它们基于分割规则进行操作。因此，对数据进行任何类型的旋转，如主成分分析或奇异值分解，都可能对GBDTs产生不利影响。不受旋转影响的DNNs在这些情况下可以迎头赶上。
- en: 'Currently, this study reinforces our experience with GBDTs and their perceived
    strengths:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这项研究加强了我们对于GBDTs（梯度提升决策树）及其感知优势的经验：
- en: Can perform well even on datasets of moderate size (1,000–5,000 cases) but outshines
    other algorithms from 10,000 to 100,000 samples (based on our experience)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使在中等规模的数据集（1,000–5,000个案例）上也能表现良好，但根据我们的经验，在10,000到100,000个样本的情况下，其表现优于其他算法
- en: Tend to excel with datasets that are heterogeneous in nature
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倾向于在本质上异质的数据集上表现出色
- en: Robust against noise and irregularities in the target data
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对目标数据中的噪声和不规则性具有鲁棒性
- en: Filter out noisy or irrelevant features due to their automatic feature selection
    process
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其自动特征选择过程，可以过滤掉噪声或不相关的特征
- en: In addition to the strengths mentioned here, it should also be noted that GBDTs
    are often preferred over DNNs in certain scenarios. One reason is that GBDTs require
    less data preprocessing, making them more efficient and straightforward to implement.
    Moreover, GBDTs are as flexible as DNNs in terms of objective functions. In both
    cases, there are many to choose from, which can be especially useful in domains
    with complex optimization goals. Another benefit of GBDTs is that they offer more
    control over how their decision tree rules are built, providing users with some
    transparency and interpretability. Finally, GBDTs can train faster than DNNs in
    most cases, and they can also predict in a reasonable inference time, depending
    on their complexity, which can be a critical factor in real-time applications
    or time-sensitive tasks.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这里提到的优势之外，还应注意的是，在某些场景下，GBDTs（梯度提升决策树）通常比 DNNs（深度神经网络）更受欢迎。一个原因是 GBDTs 需要更少的数据预处理，这使得它们更高效且易于实现。此外，在目标函数方面，GBDTs
    与 DNNs 一样灵活。在两种情况下，都有许多可供选择，这在具有复杂优化目标的领域中特别有用。GBDTs 的另一个好处是，它们提供了更多控制决策树规则构建的方式，为用户提供了一定的透明度和可解释性。最后，GBDTs
    在大多数情况下比 DNNs 训练得更快，并且根据它们的复杂性，它们也可以在合理的时间内进行预测，这在实时应用或时间敏感的任务中可能是一个关键因素。
- en: Now that you’ve gained an understanding of the fundamental concepts behind gradient
    boosting and its effectiveness in solving tabular data problems compared to deep
    learning, the next section will explore some of its implementations, starting
    with the one provided by Scikit-learn.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了梯度提升背后的基本概念及其在解决表格数据问题上的有效性，与深度学习相比，下一节将探讨一些其实现，从 Scikit-learn 提供的实现开始。
- en: 5.3 Boosting in Scikit-learn
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 Scikit-learn 中的提升
- en: Scikit-learn offers gradient boosting algorithms for both regression and classification
    tasks. These algorithms can be accessed through the GradientBoostingClassifier
    ([https://mng.bz/Ea9o](https://mng.bz/Ea9o)) and GradientBoostingRegressor ([https://mng.bz/N1VN](https://mng.bz/N1VN))
    classes, respectively.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 为回归和分类任务提供了梯度提升算法。这些算法可以通过 GradientBoostingClassifier ([https://mng.bz/Ea9o](https://mng.bz/Ea9o))
    和 GradientBoostingRegressor ([https://mng.bz/N1VN](https://mng.bz/N1VN)) 类分别访问。
- en: 'The Scikit-learn implementation of gradient boosting was one of the earliest
    options available to Python users in data science. This implementation closely
    resembles the original proposal of the algorithm by Jerome Friedman in 1999 [“Greedy
    Function Approximation: A Gradient Boosting Machine,” *Annals of Statistics* (2001):
    1189–1232]. Let’s see the implementation in action in the next code listing, where
    we cross-validate the classifier performance on the Airbnb NY dataset to predict
    if a listing is above or below the median value.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 'Scikit-learn 对梯度提升的实现是数据科学领域 Python 用户最早可用的选项之一。这个实现与 1999 年 Jerome Friedman
    提出的算法原始提案非常相似[“Greedy Function Approximation: A Gradient Boosting Machine,” *Annals
    of Statistics* (2001): 1189–1232]。让我们在接下来的代码列表中看看实现的效果，我们将对 Airbnb NY 数据集上的分类器性能进行交叉验证，以预测列表价格是否高于或低于中位数。'
- en: Listing 5.10 Scikit-learn gradient boosting classifier
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.10 Scikit-learn 梯度提升分类器
- en: '[PRE14]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① A GradientBoostingClassifier model with specified hyperparameters
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ① 具有指定超参数的 GradientBoostingClassifier 模型
- en: ② A pipeline that first applies data processing with column_transform and then
    fits the model
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ② 首先使用 column_transform 进行数据处理，然后拟合模型的管道
- en: ③ A five-fold cross-validation using the defined pipeline, calculating accuracy
    scores, and returning additional information
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用定义的管道进行五折交叉验证，计算准确度分数，并返回额外信息
- en: ④ Prints the mean and standard deviation of the accuracy scores from cross-validation
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 打印交叉验证中准确度分数的平均值和标准差
- en: 'Using the same set of parameters previously used, the performances we obtain
    are a bit better than those we obtained in our implementation:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前使用的相同参数集，我们获得的效果略好于我们实现中获得的：
- en: '[PRE15]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: One notable aspect of this Scikit-learn implementation is that the training
    process can take a considerable amount of time, and the bottleneck can be attributed
    to the decision trees, the only supported model for building the sequential ensemble
    utilized by Scikit-learn itself. In exchange, you have some flexibility in parameters
    and control over how the GBDT uses single decision trees. For instance, Scikit-learn’s
    gradient boosting allows you to
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 实现的一个显著特点是训练过程可能需要相当长的时间，瓶颈可以归因于决策树，这是 Scikit-learn 本身使用的唯一支持构建序列集成模型。作为交换，你在参数上拥有一些灵活性，并且可以控制
    GBDT 使用单个决策树的方式。例如，Scikit-learn 的梯度提升允许你
- en: Define the `init` function. We used an average as the first estimator in our
    implementation. Here you can use whatever estimator you want as a starting point.
    Since gradient boosting is based on gradient descent and the gradient descent
    optimization process is sensitive to the starting point, this can prove an advantage
    when solving more complicated data problems.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义 `init` 函数。在我们的实现中，我们使用平均值作为第一个估计器。在这里，你可以使用任何你想要的估计器作为起点。由于梯度提升基于梯度下降，而梯度下降优化过程对起点敏感，因此在解决更复杂的数据问题时，这可能是一个优势。
- en: Revert the algorithm to Adaboost, the original algorithm that inspired sequential
    ensembles, by training a `GradientBoostingClassifier` with the exponential loss
    (`loss="exponential"`).
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过训练具有指数损失的 `GradientBoostingClassifier`（`loss="exponential"`）将算法回退到 Adaboost，这是启发序列集成的原始算法。
- en: Control in detail the complexity of the decision trees used, implying that you
    can model more complex data at risk of overfitting by means of parameters such
    as
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 详细控制所使用的决策树的复杂性，这意味着你可以通过诸如
- en: '`min_samples_split` for the minimum number of samples required to split an
    internal node'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split` 用于分裂内部节点所需的最小样本数'
- en: '`min_sample_leaf` for the minimum number of samples needed to be at a leaf
    node'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_sample_leaf` 对于成为叶子节点所需的最小样本数'
- en: '`min_weight_fraction_leaf` for the minimum weighted fraction of the total of
    weights of all the input samples required at a leaf node'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_weight_fraction_leaf` 对于在叶子节点所需的输入样本总权重中加权分数的最小值'
- en: '`max_depth` for the max depth of the tree'
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth` 对于树的深度'
- en: '`min_impurity_decrease` as the threshold in impurity decrease, used to decide
    whether to split or stop the growth of the tree'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `min_impurity_decrease` 作为不纯度降低的阈值，用于决定是否分裂或停止树的生长
- en: '`max_leaf_nodes` as the maximum reachable number of final nodes before stopping
    growing the tree'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_leaf_nodes` 作为停止生长树之前可以达到的最大最终节点数'
- en: If controlling the growth of the decision trees is not enough, once they are
    grown, you can reduce their complexity using the `ccp_alpha` parameter. This parameter
    reduces the trees backward from the final nodes by removing the nodes that do
    not pass a complexity test (see [https://mng.bz/DM9n](https://mng.bz/DM9n) for
    details).
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果仅仅控制决策树的生长还不够，一旦它们生长出来，你可以使用 `ccp_alpha` 参数来减少它们的复杂性。此参数通过移除未通过复杂性测试的节点来从最终节点回退树（有关详细信息，请参阅[https://mng.bz/DM9n](https://mng.bz/DM9n)）。
- en: Subsample both rows (`subsample`) and columns (`max_features`), a particularly
    effective way to reduce overfitting and increase the training model’s generalizability.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对行（`subsample`）和列（`max_features`）进行子采样，这是一种特别有效的减少过拟合并提高训练模型泛化能力的方法。
- en: In addition, the implementation also offers support for sparse data and early
    stopping, a procedure to prevent overfitting in GBDT and neural networks that
    will be discussed in detail in the next chapter.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该实现还提供了对稀疏数据和支持早停的支持，这是一种防止 GBDT 和神经网络过拟合的程序，将在下一章中详细讨论。
- en: Also, on the side of the provided outputs, the support offered by this version
    is quite impressive, making it a perfect tool for understanding and explaining
    why your GBDT model came to certain predictions. For instance, you can access
    all the decision trees used and require the raw values predicted from the trees
    of the ensemble, both as a total (`decision_function` method) or as a sequence
    of steps (`staged_decision_function` method).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在提供的输出方面，此版本提供的支持相当令人印象深刻，使其成为理解并解释为什么你的 GBDT 模型做出某些预测的完美工具。例如，你可以访问所有使用的决策树，并要求从集成树中预测的原始值，无论是作为整体（`decision_function`
    方法）还是作为一系列步骤（`staged_decision_function` 方法）。
- en: Recently, this implementation has been less used by practitioners because of
    the faster, more performing solutions offered by XGBoost, LightGBM, and Scikit-learn,
    with its HistGradientBoosting. However, it remains an interesting choice for smaller
    datasets if you want to control certain aspects of the gradient boosting procedure.
    In the next section, we will explore XGBoost and determine how it can be a more
    powerful choice for solving your tabular data problems.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，由于XGBoost、LightGBM和Scikit-learn的HistGradientBoosting提供的更快、更高效的解决方案，这种实现已被从业者较少使用。然而，如果您想控制梯度提升过程的某些方面，它仍然是一个有趣的选择。在下一节中，我们将探讨XGBoost，并确定它如何成为解决您的表格数据问题的更强大选择。
- en: 5.3.1 Applying early stopping to avoid overfitting
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 应用提前停止以避免过拟合
- en: 'The presentation of the original Scikit-learn classes for gradient boosting
    offers a chance to introduce a procedure that can help control overfitting. The
    procedure is early stopping, a method originally used in gradient descent to restrict
    the number of iterations when further adjustments to the coefficients under optimization
    would lead to no enhancements or a poor generalization of the solution. The method
    has also been used to train neural networks. In gradient boosting, which takes
    gradient descent as part of its optimization process, the method can help solve
    the same problem: limiting the number of added decision tree models to reduce
    the computational burden and avoid possible overfitting.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 原始Scikit-learn类梯度提升的展示提供了一个机会，介绍一个可以帮助控制过拟合的程序。该程序是提前停止，这是一种最初用于梯度下降的方法，用于限制在优化系数的进一步调整不会带来增强或导致解决方案泛化不良时迭代的数量。该方法也已被用于训练神经网络。在梯度提升中，它将梯度下降作为其优化过程的一部分，该方法可以帮助解决相同的问题：限制添加的决策树模型数量以减少计算负担并避免可能的过拟合。
- en: 'Early stopping works in a few simple steps:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止的工作步骤如下：
- en: A fraction of the training dataset is set aside to form a validation set.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据集的一部分留出以形成验证集。
- en: During each iteration of the training process, the resulting partial model is
    evaluated using the validation set.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程的每次迭代中，使用验证集评估产生的部分模型。
- en: The performance of the partial model on the validation set is recorded and compared
    with previous results.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录并比较部分模型在验证集上的性能与之前的结果。
- en: If the model’s performance does not improve, the algorithm increases its counting
    (commonly called *patience*) of how many iterations since its last improvement.
    Otherwise, it resets the counting.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型的表现没有提高，算法会增加其自上次改进以来的迭代计数（通常称为*耐心*）。否则，它将重置计数。
- en: The training process is stopped if there has been no improvement over a certain
    number of iterations.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在一定的迭代次数内没有改进，则停止训练过程。
- en: Otherwise, the training process continues for another iteration unless all the
    designated decision trees to be boosted have been completed. At this point, the
    training process is halted.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，训练过程将继续进行另一轮迭代，除非所有指定的待提升决策树都已完成。此时，训练过程将停止。
- en: Figure 5.8 shows this process in a process flow chart.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8展示了这个过程在流程图中的表示。
- en: '![](../Images/CH05_F08_Ryan2.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F08_Ryan2.png)'
- en: Figure 5.8 A process flow chart describing how early stopping works in GBDTs
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 描述了GBDT中提前停止如何工作的流程图。
- en: Figure 5.9 shows the very same process from the point of view of the validation
    metric, which can be the loss or any other metric for which you want to get the
    best result. As the iterations progress, it is customary to observe training errors
    decrease. Validation error, on the other hand, tends to have a sweet point before
    the algorithm starts overfitting. Early stopping helps to catch this increase
    in validation error, and monitoring the validation error dynamics allows you to
    retrace to the iteration before any overfitting happens.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9从验证指标的角度展示了完全相同的过程，该指标可以是损失或其他您希望获得最佳结果的指标。随着迭代的进行，观察到训练误差通常会降低。另一方面，验证误差在算法开始过拟合之前往往有一个最佳点。提前停止有助于捕捉验证误差的增加，监控验证误差的动态变化，让您能够回溯到任何过拟合发生之前的迭代。
- en: '![](../Images/CH05_F09_Ryan2.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F09_Ryan2.png)'
- en: Figure 5.9 How validation error and training error generally behave when using
    GBDTs through multiple iterations
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9展示了在多次迭代使用GBDT时，验证误差和训练误差通常如何表现。
- en: Different stopping points may be found for each model trained on different folds
    in a cross-validation process with early stopping. When training on the entire
    dataset, you can still rely on using early stopping based on a validation sample.
    Hence, you just need to set a high number of iterations and see when the training
    stops. Otherwise, you can use a fixed number of iterations based on the stopping
    iterations you observed in cross-validation. In this case, you can calculate the
    average or median of all stopping points seen in cross-validation to determine
    the number of boosting trees to use. However, there’s no fixed rule, and you may
    choose to use the second maximum value for an aggressive stopping policy or the
    second minimum value for a conservative one.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有早停法的交叉验证过程中，针对每个折叠训练的模型可能会找到不同的停止点。当在整个数据集上训练时，您仍然可以依赖使用基于验证样本的早停法。因此，您只需设置一个高迭代次数，并观察训练何时停止。否则，您可以使用基于交叉验证中观察到的停止迭代次数的固定迭代次数。在这种情况下，您可以通过计算交叉验证中看到的所有停止点的平均值或中位数来确定要使用的提升树数量。然而，没有固定的规则，您可以选择使用第二大的值来实现激进的停止策略，或者使用第二小的值来实现保守的停止策略。
- en: Additionally, consider increasing the number of boosting trees since training
    is done on more data than during cross-validation. As a general guideline, increase
    the number of boosting trees by a percentage equivalent to dividing one by the
    number of folds you used in cross-validation. However, since no one-size-fits-all
    solution exists, experimentation may be necessary to find the best approach for
    your problem.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑到训练数据比交叉验证期间更多，因此可以考虑增加提升树的数量。作为一个一般性指南，可以通过将提升树的数量增加一个百分比来达到这个目的，这个百分比等于交叉验证中使用的折叠数目的倒数。然而，由于没有一种适合所有情况的解决方案，可能需要进行实验以找到最适合您问题的最佳方法。
- en: As an example, we run the previous code again, this time setting a higher number
    of base estimators to be used and two parameters, `validation_fraction` and `n_iter_no_change`,
    that activate the early stopping procedure. The parameter `validation_fraction`
    determines the fraction of the training data to be used for validation, and it
    is effective only when `n_iter_no_change` is set to an integer indicating how
    many iterations should pass without improvements when testing the model on the
    validation set before stopping the process.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们再次运行之前的代码，这次设置更高的基本估计器数量以及两个参数，`validation_fraction` 和 `n_iter_no_change`，这些参数激活了早停过程。参数
    `validation_fraction` 确定了用于验证的训练数据比例，并且仅在 `n_iter_no_change` 设置为一个整数时有效，该整数表示在停止过程之前，在验证集上测试模型时，应该经过多少次迭代而没有改进。
- en: Listing 5.11 Applying early stopping with GradientBoostingClassifier
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.11 使用 GradientBoostingClassifier 应用早停法
- en: '[PRE16]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① A GradientBoostingClassifier model whose iterations are raised to 1,000 from
    the previous 300
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将迭代次数从之前的 300 提高到 1,000 的 GradientBoostingClassifier 模型
- en: ② As a validation fraction, the GradientBoostingClassifier uses 20% of the training
    data for validation.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ② 作为验证分数，GradientBoostingClassifier 使用 20% 的训练数据用于验证。
- en: ③ The training of the GradientBoostingClassifier will stop after 10 iterations
    without improvements on the validation.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 当验证集上没有改进时，GradientBoostingClassifier 的训练将在 10 次迭代后停止。
- en: ④ Extracts the number of estimators used during training for each fold’s estimator
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 提取每个折叠估计器在训练期间使用的估计器数量
- en: ⑤ Prints the list of the number of estimators for each fold’s estimator
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印每个折叠估计器的估计器数量列表
- en: The output is
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE17]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Since the mean number of iterations for cross-validation folds is 268 iterations,
    based on a rule of thumb, when using all the available data during the training
    phase, we suggest increasing the number of iterations by 20%, fixing it at 322
    iterations.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 由于交叉验证折叠的平均迭代次数为 268 次迭代，根据经验法则，在训练阶段使用所有可用数据时，我们建议将迭代次数增加 20%，固定为 322 次迭代。
- en: In the following sections, we will introduce new implementations of gradient
    boosting, such as XGBoost and LightGBM. We will also present how to use early
    stopping with them.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍梯度提升的新实现，例如 XGBoost 和 LightGBM。我们还将展示如何使用它们实现早停法。
- en: 5.4 Using XGBoost
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 使用 XGBoost
- en: XGBoost gained traction after being successful in a Kaggle competition, the
    Higgs Boson Machine Learning Challenge ([https://www.kaggle.com/c/higgs-boson](https://www.kaggle.com/c/higgs-boson)),
    where XGBoost has been proposed in the competition forums as a fast and accurate
    solution in comparison to Scikit-learn’s gradient boosting. XGBoost has since
    been adopted and successfully used in many other data science competitions, proving
    its effectiveness and the usefulness of Kaggle competitions as a good place to
    introduce innovations that disrupt performance benchmarks. Keras is another example
    of an innovation that was widely adopted after success in Kaggle competitions.
    At the time of writing, the XGBoost package had been updated and reached the milestone
    of version 2.0.3, which is the version we have used in this book.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost在Kaggle竞赛中取得成功后获得了关注，例如在希格斯玻色子机器学习挑战赛([https://www.kaggle.com/c/higgs-boson](https://www.kaggle.com/c/higgs-boson))中，XGBoost在竞赛论坛中被提出作为一种比Scikit-learn的梯度提升更快速、更准确的解决方案。自那时起，XGBoost已被广泛应用于许多其他数据科学竞赛中，证明了其有效性和Kaggle竞赛作为介绍颠覆性能基准的创新的好场所的作用。Keras是另一个在Kaggle竞赛中取得成功后被广泛采用的创新例子。在撰写本文时，XGBoost软件包已更新并达到了2.0.3版本的里程碑，这是我们在这本书中使用的版本。
- en: Initially conceived as a research project by Tianqi Chen, later further developed
    with the contribution of Carlos Guestrin, XGBoost, is a gradient boosting framework
    available as open-source software. It is noticeable that, contrary to other initiatives,
    such as LightGBM, sponsored by Microsoft, and Yggdrasil Decision Forests (see
    [https://mng.bz/lYV6](https://mng.bz/lYV6)) that Google sponsors, XGBoost remained
    completely independent, maintained by the Distributed (Deep) Machine Learning
    Common community ([dmlc.github.io](https://dmlc.github.io/)).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost最初是由陈天奇作为研究项目提出的，后来在Carlos Guestrin的贡献下进一步发展，XGBoost是一个作为开源软件提供的梯度提升框架。值得注意的是，与由微软赞助的LightGBM和其他由谷歌赞助的Yggdrasil决策森林（见[https://mng.bz/lYV6](https://mng.bz/lYV6)）等倡议不同，XGBoost保持了完全独立，由分布式（深度）机器学习共同社区([dmlc.github.io](https://dmlc.github.io/))维护。
- en: Over time, the framework has undergone significant enhancements and now provides
    advanced capabilities for distributed processing and parallelization, which enables
    it to operate on large-scale datasets. Meanwhile, XGBoost has also gained wide
    adoption, and it is currently accessible in several programming languages, including
    C/C++, Python, and R. Moreover, this framework is supported on multiple data-science
    platforms, such as H2O.ai and Apache Spark.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，该框架经历了重大改进，现在提供了用于分布式处理和并行化的高级功能，使其能够处理大规模数据集。同时，XGBoost也得到了广泛的应用，目前可在包括C/C++、Python和R在内的多种编程语言中使用。此外，该框架在多个数据科学平台上得到支持，如H2O.ai和Apache
    Spark。
- en: As a data science user, you will immediately notice several key characteristics
    of this framework, including
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学用户，你将立即注意到这个框架的几个关键特性，包括
- en: The ability to handle various input data types
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理各种输入数据类型的能力
- en: Support for customized objective and evaluation functions
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持自定义目标函数和评估函数
- en: Automatic handling of missing values
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动处理缺失值
- en: Easy support for GPU training
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单支持GPU训练
- en: Accommodation of monotonicity and feature interaction constraints
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应单调性和特征交互约束
- en: Optimization of multiple cores and cache on standalone computers
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化独立计算机上的多核和缓存
- en: From a system performance perspective, notable features include
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统性能的角度来看，显著的特点包括
- en: Networked parallel training, which allows for distributed computing across a
    cluster of machines
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络并行训练，允许在机器集群上实现分布式计算
- en: Utilization of all available CPU cores during tree construction for parallelization
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建树的过程中利用所有可用的CPU核心以实现并行化
- en: Out-of-core computing when working with large datasets that don’t fit into memory
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理不适合内存的大型数据集时的离核计算
- en: However, what sets the “extreme” XGBoost algorithm apart is its innovative algorithmic
    details for optimization, including a variant of gradient descent known as Newton
    Descent and regularization terms, as well as its unique approach to feature splitting
    and sparse data handling. The following section briefly summarizes these groundbreaking
    techniques that power XGBoost’s performance.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将“极端”XGBoost算法与其他算法区分开来的是其创新的优化算法细节，包括被称为牛顿下降的梯度下降的变体、正则化项，以及其独特的特征分割和稀疏数据处理方法。以下部分简要总结了这些推动XGBoost性能的突破性技术。
- en: 'Now let’s try the algorithm with our classification task on the Airbnb NYC
    dataset. In this case, use the `XGBClassifier` from XGBoost. For regression problems,
    you can use the `XGBRegressor` class. First, however, you need to have XGBoost
    installed on your system. To install XGBoost, you can just pip install it:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用Airbnb NYC数据集上的分类任务来运行这个算法。在这种情况下，使用XGBoost的`XGBClassifier`。对于回归问题，你可以使用`XGBRegressor`类。然而，首先，你需要在你的系统上安装XGBoost。要安装XGBoost，你可以直接使用pip安装：
- en: '[PRE18]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Or use conda for the job:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 或者使用conda来完成这项工作：
- en: '[PRE19]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The installation commands should do all the necessary steps for you; also install
    both CPU and GPU variants of the algorithm, if possible, on your system. See instructions
    and details on the installation process at [https://mng.bz/BXd0](https://mng.bz/BXd0).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 安装命令应该为你完成所有必要的步骤；如果可能的话，在你的系统上安装算法的CPU和GPU版本。有关安装过程和详细说明，请参阅[https://mng.bz/BXd0](https://mng.bz/BXd0)。
- en: 'In the following listing, we replicate the same approach we previously used
    with Scikit-learn’s GradientBoostingClassifier: we boost 300 trees, limit the
    depth of decision trees to four levels, and accept nodes of at least three examples.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们复制了我们之前使用Scikit-learn的GradientBoostingClassifier所采用的方法：我们增强300棵树，将决策树的深度限制为四个级别，并接受至少三个示例的节点。
- en: Listing 5.12 XGBoost classifier
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.12 XGBoost分类器
- en: '[PRE20]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Creates an XGBClassifier model with specified hyperparameters, including the
    booster type
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个具有指定超参数的XGBClassifier模型，包括增强器类型
- en: ② The learning objective, equivalent to Scikit-learn’s loss
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ② 学习目标，相当于Scikit-learn的损失
- en: ③ min_child_weight is equivalent to Scikit-learn’s min_samples_leaf.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ③ `min_child_weight`相当于Scikit-learn的`min_samples_leaf`。
- en: ④ Prints the mean and standard deviation of cross-validated test scores
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 打印交叉验证测试分数的均值和标准差
- en: 'The obtained result is the best obtained so far, and it is impressive that
    the training fit is just a fraction of what previously the Scikit-learn implementation
    took:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的结果是目前为止的最佳结果，而且训练拟合只是之前Scikit-learn实现所需时间的很小一部分：
- en: '[PRE21]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the next subsection, we explore the key parameters we used for running this
    example and discuss what parameters, of the many offered by the algorithm (see
    [https://mng.bz/dX6N](https://mng.bz/dX6N) for a complete list) you need for running
    successfully any tabular data project using XGBoost.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将探讨运行此示例所使用的核心参数，并讨论在成功运行任何使用XGBoost的表格数据项目时，你需要算法提供的众多参数中的哪些（参见[https://mng.bz/dX6N](https://mng.bz/dX6N)以获取完整列表）。
- en: 5.4.1 XGBoost’s key parameters
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 XGBoost的关键参数
- en: Let’s review the specific options we decided on in our previous example shown
    in listing 5.12, beginning with the `n_estimators` parameter, which specifies
    the number of decision trees involved in building the ensemble and is part of
    the gradient descent process that we discussed previously in this chapter.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们在列表5.12中之前示例中决定的特定选项，从`n_estimators`参数开始，该参数指定了构建集成中涉及的决策树数量，也是我们之前在本章中讨论的梯度下降过程的一部分。
- en: The `n_estimators` parameter in XGBoost determines the number of decision trees
    used to produce the output. In standard tabular problems, the usual values for
    this parameter range between 10 to 10,000\. While increasing this value can improve
    prediction performance by involving more weak learners, it can also slow down
    the training time. It’s worth noting that there is an ideal number of trees that
    maximizes performance on prediction tasks with unseen data, and finding this sweet
    spot depends on other XGBoost parameters, such as the learning rate. To achieve
    a high-performing XGBoost model, it’s important to choose the appropriate number
    of trees based on the problem at hand while setting the other parameters correctly,
    including the learning rate.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost中的`n_estimators`参数决定了用于生成输出的决策树数量。在标准的表格问题中，此参数的常用值介于10到10,000之间。虽然增加此值可以通过涉及更多的弱学习器来提高预测性能，但它也可能减慢训练时间。值得注意的是，存在一个理想数量的树，它可以在未见过的数据上的预测任务中最大化性能，而找到这个最佳点取决于其他XGBoost参数，例如学习率。为了实现高性能的XGBoost模型，根据手头的问题选择适当的树的数量，同时正确设置其他参数，包括学习率，是非常重要的。
- en: 'Whereas Scikit-learn is limited to just decision trees, XGBoost proposes more
    choices using its booster parameter:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 与Scikit-learn仅限于决策树不同，XGBoost通过其增强器参数提出了更多的选择：
- en: '`gbtree`—Decision trees, as you would expect in gradient boosting'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gbtree`—决策树，正如你在梯度提升中所期望的那样'
- en: '`gblinear`—Linear regression models'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gblinear`—线性回归模型'
- en: '`dart`—Decision trees, but the optimization process is more regularized'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dart`——决策树，但优化过程更加规范化'
- en: The `gblinear` booster produces a sum of chained linear models. Since a sum
    of linear combinations is linear, you end up with a coefficient for each feature
    you have used, similar to a linear model. You can access the coefficients using
    the `.coef` method. It is a different way to fit a GBDT model with an emphasis
    on interpretability since the model can be reduced to a linear combination, different
    from the one you could directly fit because of using a different approach for
    complexity penalization and optimization. The most notable difference is that
    you cannot interpret the coefficients as you would if a linear regression or a
    generalized linear model produced them. Furthermore, the interpretation of the
    intercept generated by the `gblinear` booster differs from classical linear models
    as it is influenced by both the learning rate and the initial estimate employed
    by the booster.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`gblinear`提升器产生了一系列链式线性模型的和。由于线性组合的和是线性的，您最终会为每个使用的特征得到一个系数，类似于线性模型。您可以使用`.coef`方法访问这些系数。这是一种以可解释性为重点的GBDT模型拟合方法，因为模型可以简化为线性组合，这与您直接拟合的线性组合不同，因为使用了不同的复杂度惩罚和优化方法。最显著的区别是，您不能像线性回归或广义线性模型产生的系数那样解释系数。此外，`gblinear`提升器生成的截距的解释与经典线性模型不同，因为它受到学习率和提升器使用的初始估计的影响。'
- en: 'The `dart` booster is different because it combines the optimization based
    on gradient descent with an approach similar to dropout, a technique used in deep
    learning. Presented by a UC Berkeley researcher and a Microsoft researcher in
    the paper “DART: Dropouts Meet Multiple Additive Regression Trees” by Rashmi Korlakai
    Vinayak and Ran Gilad-Bachrach (Artificial Intelligence and Statistics. PMLR,
    2015), DART focuses on overfitting due to the dependence of each decision tree’s
    estimates on the previous ones. The researchers then take the idea of dropout
    from deep learning, where a dropout mask randomly and partially blanked a neural
    network layer. The neural network cannot always rely on certain signals in a particular
    layer to determine the next layer’s weights. In DART, the gradients are not calculated
    compared to the sum of the residuals from all previously built trees. Still, instead,
    the algorithm at each iteration randomly selects a subset of the previous trees
    and scales their leaves by a factor of 1/k, where k is the number of trees that
    were dropped.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '`dart`提升器与其他提升器不同，因为它结合了基于梯度下降的优化与类似于dropout的方法，dropout是一种在深度学习中使用的技巧。由加州大学伯克利分校的研究员和微软研究员在Rashmi
    Korlakai Vinayak和Ran Gilad-Bachrach发表的论文“DART: Dropouts Meet Multiple Additive
    Regression Trees”中提出，该论文发表在《人工智能与统计》（Artificial Intelligence and Statistics. PMLR,
    2015）。DART专注于由于每个决策树的估计依赖于前一个决策树而导致的过拟合问题。研究人员随后从深度学习中的dropout理念中汲取灵感，其中dropout掩码随机且部分地清除了神经网络层。神经网络不能总是依赖于特定层中的某些信号来确定下一层的权重。在DART中，与所有先前构建的树的残差之和相比，梯度不是通过计算得到的。相反，算法在每次迭代中随机选择先前树的一个子集，并将它们的叶子节点按1/k的比例缩放，其中k是丢弃的树的数量。'
- en: 'The `gblinear` and the `dart` are the only alternative boosters available.
    For instance, there is no booster to mimic random forests (as there is for another
    GBDT implementation, LightGBM). However, though a random forests booster is not
    supported yet by `XGBClassifier` and `XGBRegression`, you can obtain a similar
    result by playing with XGBoost parameters and functions:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '`gblinear`和`dart`是唯一可用的替代提升器。例如，没有提升器可以模仿随机森林（如另一个GBDT实现LightGBM中那样）。然而，尽管`XGBClassifier`和`XGBRegression`尚未支持随机森林提升器，您可以通过调整XGBoost参数和函数来获得类似的结果：'
- en: Use the `num_parallel_tree` parameter and set it to a number above 1\. At each
    step of the optimization, the gradients are estimated not from a single decision
    tree but from a bagged ensemble of decision trees, thus creating a boosted random
    forests model. In some instances, this approach may provide better results than
    the gradient boosting approach because it will reduce the variance of the estimates
    at the expense of an increased computational cost.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`num_parallel_tree`参数并将其设置为大于1的数字。在优化的每一步中，梯度估计不是来自单个决策树，而是来自决策树的集成袋，从而创建了一个提升随机森林模型。在某些情况下，这种方法可能比梯度提升方法提供更好的结果，因为它将以增加的计算成本为代价来减少估计的方差。
- en: Use the `XGBRFClassifier` or `XGBRFRegressor`, two classes from XGBoost that
    implement a random forests approach. The classes are still experimental. For more
    details see [https://mng.bz/rKVB](https://mng.bz/rKVB) with the caveat that there
    are some differences with the random forests algorithm offered by Scikit-learn
    because XGBoost computes a matrix made up of second derivatives and called the
    Hessian (see [https://brilliant.org/wiki/hessian-matrix/](https://brilliant.org/wiki/hessian-matrix/)
    for a mathematical definition) to weight the gradients and it has no bootstrap
    capability. Hence, your results will be different.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`XGBRFClassifier`或`XGBRFRegressor`，这两个类来自XGBoost，实现了随机森林方法。这些类仍然是实验性的。更多详情请参阅[https://mng.bz/rKVB](https://mng.bz/rKVB)，但请注意，与Scikit-learn提供的随机森林算法存在一些差异，因为XGBoost计算了一个由二阶导数组成的矩阵，称为Hessian（有关数学定义，请参阅[https://brilliant.org/wiki/hessian-matrix/](https://brilliant.org/wiki/hessian-matrix/)），用于加权梯度，并且它没有自助能力。因此，你的结果可能会有所不同。
- en: 'As for the loss function, controlled by the parameter `objective`, we chose
    `reg:logistic`, but we could have also chosen `binary:logistic`, both comparable
    to log-loss for binary classification. In XGBoost, you have loss functions organized
    into six classes:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 对于损失函数，由参数`objective`控制，我们选择了`reg:logistic`，但也可以选择`binary:logistic`，两者都与二分类中的log-loss相当。在XGBoost中，损失函数被组织成六个类别：
- en: '`reg` for regression problems (but you also have the logistic regression among
    its options).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg`用于回归问题（但其中也包括逻辑回归选项）。'
- en: '`binary` for binary classification problems.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary`用于二分类问题。'
- en: '`multi` for multiclass classification.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multi`用于多分类。'
- en: '`count` for count data—that is, discrete events.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count`用于计数数据——即离散事件。'
- en: '`survival` for survival analysis, which is a statistical technique to analyze
    data on the time it occurs for an event of interest to occur, such as the failure
    of a part in a machinery. It considers censoring, where the event of interest
    has not yet happened for some individuals in the study.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`survival`用于生存分析，这是一种统计分析技术，用于分析感兴趣事件发生的时间数据，例如机械部件的故障。它考虑了截尾情况，即研究中的某些个体感兴趣的事件尚未发生。'
- en: '`rank` for ranking problems, such as estimating what rank a site should have
    in the results.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank`用于排名问题，例如估计一个网站在结果中应该有的排名。'
- en: Apart from Poisson distribution, useful for modeling the frequency of events,
    XGBoost also offers `reg:gamma` and `reg:tweedie`, optimizing for two distributions
    used in insurance for claim amount modeling as mentioned in chapter 4 when discussing
    generalized linear models.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 除了泊松分布，用于建模事件的频率外，XGBoost还提供了`reg:gamma`和`reg:tweedie`，优化用于保险索赔金额建模的两个分布，如第4章在讨论广义线性模型时所述。
- en: The availability of various objective functions demonstrates the multiple possible
    applications that XGBoost may have in different domains. For a full overview of
    the loss functions see [https://mng.bz/dX6N](https://mng.bz/dX6N). Loss functions
    are essential in gradient boosting, as they define the optimization objective.
    By contrast, the evaluation metrics are not used to optimize the gradient descent
    in gradient boosting. However, they play a crucial role in monitoring the training
    process, optimizing for feature selection, hyper-parameter optimization, and even
    enabling early stopping to halt the training once it is no longer providing benefit.
    The equivalent of Scikit-learn’s `min_samples_leaf` in XGBoost is `min_child_weight`.
    Both parameters control the minimum number of samples needed to be at a leaf node
    of the decision tree. Thus, they regularize the decision tree by limiting the
    depth of the resulting tree. There are differences, however, since `min_child_weight`
    refers to the minimum sum of Hessian weight needed in a child node, while `min_samples_leaf`
    refers to the minimum number of samples required in a leaf. Hence, the two parameters
    are not completely comparable since their values are used differently in XGBoost
    and Scikit-learn.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 各种目标函数的存在展示了XGBoost在不同领域可能具有的多种可能应用。关于损失函数的全面概述，请参阅[https://mng.bz/dX6N](https://mng.bz/dX6N)。损失函数在梯度提升中至关重要，因为它们定义了优化目标。相比之下，评估指标在梯度提升中不用于优化梯度下降。然而，它们在监控训练过程、优化特征选择、超参数优化甚至启用早期停止以停止不再提供益处的训练中起着至关重要的作用。XGBoost中与Scikit-learn的`min_samples_leaf`等效的是`min_child_weight`。这两个参数都控制决策树叶子节点所需的最小样本数。因此，它们通过限制生成的树的深度来正则化决策树。然而，由于`min_child_weight`指的是子节点中所需的Hessian权重的最小总和，而`min_samples_leaf`指的是叶子中所需的最小样本数，因此这两个参数在XGBoost和Scikit-learn中的使用方式不同，因此它们并不完全可比。
- en: As a general rule of thumb, `min_child_weight` affects how single decision trees
    are built, and the larger the value of this parameter, the more conservative the
    resulting tree will be. The usual values to be tested range from 0 (implying no
    limit to the size of the leaf nodes) to 10\. In his 2015 talk at the NYC Data
    Science Academy, titled “Winning Data Science Competitions,” Owen Zhang, a former
    top Kaggle competitor, suggested computing the optimal value of this parameter
    by dividing 3 by the percentage of the rarest events in the data to be predicted.
    For instance, following this rule of thumb, since our classes are split 50%/50%,
    the ideal value should be 3/0.5, resulting in 6.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一般规则，`min_child_weight`影响单个决策树的构建方式，该参数的值越大，生成的树就越保守。通常要测试的值范围从0（表示没有限制叶子节点的大小）到10。在2015年纽约市数据科学学院的一次演讲中，标题为“赢得数据科学竞赛”，前Kaggle顶级竞争者Owen
    Zhang建议通过将3除以要预测的数据中稀有事件的百分比来计算此参数的最佳值。例如，按照这个经验法则，由于我们的类别是50%/50%分割，理想值应该是3/0.5，结果为6。
- en: 'Other important XGBoost parameters we didn’t use in our example are as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在示例中没有使用的其他重要XGBoost参数如下：
- en: The `learning_rate`, also called `eta`, is a parameter in XGBoost that determines
    the rate at which the model learns. A lower learning rate allows the model to
    converge more slowly yet more precisely, potentially leading to better predictive
    accuracy. However, this will result in a greater number of iterations and a longer
    training time. On the other hand, setting the value too high can speed up the
    process but result in worse model performance because, as it happens in gradient
    descent when your learning parameter is too high, the optimization overshoots
    its target.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`学习率`，也称为`eta`，是XGBoost中的一个参数，它决定了模型学习的速率。较低的学习率允许模型以更慢的速度但更精确地收敛，这可能导致更好的预测准确性。然而，这将导致迭代次数更多，训练时间更长。另一方面，设置值过高可以加快过程，但会导致模型性能更差，因为当学习参数过高时，优化会超过其目标，就像梯度下降中发生的那样。'
- en: '`alpha` and `lambda` are the L1 and L2 regularizers, respectively. They both
    contribute to avoiding overfitting in the gradient descent optimization part of
    XGBoost.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`和`lambda`分别是L1和L2正则化器。它们都贡献于避免XGBoost梯度下降优化部分中的过拟合。'
- en: The `max_depth` parameter in XGBoost controls the algorithm’s complexity. If
    this value is set too low, the model may not be able to identify many patterns
    (known as underfitting). However, if it is set too high, the model may become
    overly complex and identify patterns that do not generalize well to new data (known
    as overfitting). Ideally, it is a value between 1 and 16.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost中的`max_depth`参数控制算法的复杂度。如果此值设置得太低，模型可能无法识别出许多模式（称为欠拟合）。然而，如果设置得太高，模型可能会变得过于复杂，并识别出对新数据不具良好泛化能力的模式（称为过拟合）。理想情况下，此值应在1到16之间。
- en: The `gamma`, or `min_split_loss`, parameter in XGBoost is a regularization parameter
    ranging from 0 to infinity, and setting this value higher increases the strength
    of regularization, reducing the risk of overfitting but potentially leading to
    underfitting if the value is too large. Also, this parameter controls the resulting
    complexity of the decision trees. We suggest starting with this value at 0 or
    a low value and then testing increasing it after all the other parameters are
    set.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost中的`gamma`，或称`min_split_loss`参数，是一个介于0到无穷大的正则化参数，将此值设置得更高会增加正则化的强度，从而降低过拟合的风险，但如果值过大，可能会导致欠拟合。此外，此参数还控制着决策树的结果复杂度。我们建议从0或低值开始，然后在设置完所有其他参数后测试增加此值。
- en: The `colsample_bytree` parameter in XGBoost controls the fraction of the total
    number of features or predictors to be used for a given tree during training.
    Setting this value to less than 1 means that each tree may use a different subset
    of features for prediction, potentially reducing the risk of overfitting or being
    too much influenced by single features in data. It also improves training speed
    by not using all features in every tree. The allowable range of values for this
    parameter is between 0 and 1.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost中的`colsample_bytree`参数控制训练过程中给定树使用的特征或预测器的总数比例。将此值设置为小于1意味着每个树可能使用不同的特征子集进行预测，这可能会降低过拟合的风险或减少对单个特征的过度依赖。它还通过不在每个树中使用所有特征来提高训练速度。此参数的允许值范围在0到1之间。
- en: The `subsample` parameter in XGBoost controls the fraction of the number of
    instances used for a given tree during training. Like `colsample_bytree`, this
    parameter can help reduce overfitting and improve training time. By using a fraction
    of the cases for each tree, the model can identify more generalizable patterns
    in the data. The default value for `subsample` is 1.0, which means that all instances
    are used in each tree.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost中的`subsample`参数控制训练过程中给定树使用的实例数比例。类似于`colsample_bytree`，此参数可以帮助减少过拟合并提高训练时间。通过为每个树使用案例的一部分，模型可以在数据中识别出更通用的模式。`subsample`的默认值为1.0，这意味着每个树都使用所有实例。
- en: In many cases, you will only require some of the parameters provided by XGBoost
    or discussed here for your projects. Simply adjusting the `learning_rate`, setting
    the optimization steps, and `min_child_weight` to prevent overfitting individual
    decision trees in the gradient boosting process will be sufficient most of the
    time. Additionally, you may derive benefits from setting the `objective`, `max_depth`,
    `colsample_bytree`, and `subsample` parameters, but it is unlikely that tweaking
    the numerous other available parameters will yield significant improvements. This
    holds not only for XGBoost but also for different implementations of gradient
    boosting.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，你可能只需要XGBoost提供的或在此讨论的一些参数来满足你的项目需求。简单地调整`learning_rate`，设置优化步数，并将`min_child_weight`设置为防止梯度提升过程中单个决策树过拟合，通常就足够了。此外，设置`objective`、`max_depth`、`colsample_bytree`和`subsample`参数可能会带来好处，但调整大量其他可用参数不太可能带来显著的改进。这一点不仅适用于XGBoost，也适用于梯度提升的不同实现。
- en: Next we explain what makes the XGBoost implementation perform better in computations
    and predictions.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解释是什么使得XGBoost在计算和预测方面表现更优。
- en: 5.4.2 How XGBoost works
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 XGBoost的工作原理
- en: 'As explained in the paper “Xgboost: A Scalable Tree Boosting System” by Tianqi
    Chen and Carlos Guestrin (*Proceedings of the 22nd ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining*, 2016), the great performance of XGBoost
    boils down to a few innovations that weren’t present in other implementations:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '如Tianqi Chen和Carlos Guestrin在2016年发表的论文“Xgboost: A Scalable Tree Boosting System”中所述（*《第22届ACM
    SIGKDD国际知识发现和数据挖掘会议论文集》，2016年*），XGBoost卓越的性能归功于几个在其他实现中不存在的创新：'
- en: Column block for parallel learning
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行学习列块
- en: Second-order approximation for quicker optimization
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二阶近似以加快优化速度
- en: Improved split-finding algorithms
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进的分割查找算法
- en: Sparsity-aware split finding
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏感知的分割查找
- en: Column Block is a technique used in parallel learning that involves dividing
    a dataset into blocks of columns or subsets of features. This allows for parallel
    training across multiple processors, significantly reducing the overall training
    time. You can see it in action when you train an XGBoost model and look for CPU
    utilization pointing to multiple usages of different cores. XGBoost cannot use
    multiple cores to train multiple models simultaneously, as it happens in other
    ensemble models such as random forests. That’s because gradient boosting is a
    serial model, where each model is trained after the results of another one. Instead,
    the XGBoost training process of every single model is divided among multiple cores
    to increase efficiency and speed.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 列块是并行学习中使用的一种技术，涉及将数据集划分为列块或特征子集。这允许在多个处理器上并行训练，显著减少整体训练时间。当您训练XGBoost模型并查找指向多个不同核心使用的CPU利用率时，您可以看到它的实际效果。XGBoost不能像其他集成模型（如随机森林）那样同时使用多个核心训练多个模型，因为梯度提升是一个序列模型，每个模型都是在另一个模型的结果之后训练的。相反，XGBoost的每个单独模型的训练过程被分配到多个核心，以提高效率和速度。
- en: 'Currently, XGBoost can be utilized in Python through two distinct APIs: the
    Native API and the Scikit-learn API. In this book, we will exclusively use the
    Scikit-learn API due to its benefits in terms of best modeling practices and the
    added benefit of being able to easily utilize various tools available in the Scikit-learn
    library, such as model selection and pipelines, as explained in chapter 4.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，XGBoost可以通过两个不同的API在Python中使用：原生API和Scikit-learn API。在这本书中，我们将仅使用Scikit-learn
    API，因为它在最佳建模实践方面的优势，以及能够轻松利用Scikit-learn库中各种工具的额外好处，如模型选择和管道，正如第4章所述。
- en: When using the Native API, the user is required to convert their data into a
    `DMatrix`, which is an internal XGBoost data structure optimized for both memory
    efficiency and training speed ([https://mng.bz/VVxP](https://mng.bz/VVxP)). The
    use of the DMatrix format makes the column block technique possible. However,
    when using the Scikit-learn API, users can input their data as pandas DataFrames
    or Numpy arrays without requiring explicit conversion into the DMatrix format.
    This is because XGBoost performs the conversion under the hood, making the process
    more streamlined. Therefore, it is safe to choose the API that best suits your
    preferences, as both APIs offer the same performance and differ only in some of
    the parameters, default values, and options they provide.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用原生API时，用户需要将他们的数据转换为`DMatrix`，这是一个针对内存效率和训练速度优化的XGBoost内部数据结构（[https://mng.bz/VVxP](https://mng.bz/VVxP)）。使用DMatrix格式使得列块技术成为可能。然而，当使用Scikit-learn
    API时，用户可以将他们的数据作为pandas DataFrame或Numpy数组输入，无需显式转换为DMatrix格式。这是因为XGBoost在底层执行转换，使过程更加流畅。因此，可以安全地选择最适合您偏好的API，因为这两个API提供相同的性能，只是在一些参数、默认值和选项上有所不同。
- en: The second-order approximation for speeding up the optimization incorporating
    the second derivative (the gradient derived from the first derivative) is based
    on a more comprehensive root-finding technique, Newton’s method. In the context
    of minimization, we often refer to Newton’s method as Newton descent instead of
    gradient descent. Listing 5.13 shows it implemented as a new class, the `NewtonianGradientBoosting`
    class, that inherits the original GradientBoosting class with some additions and
    modifications to its existing methods and attributes. In particular, we add the
    Hessian calculations to balance the gradient steps in regard to the convergence
    acceleration and a regularization term to prevent overfitting.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快优化速度，采用包含二阶导数（从一阶导数派生的梯度）的二阶近似，基于更全面的根查找技术，即牛顿法。在最小化的上下文中，我们通常将牛顿法称为牛顿下降而不是梯度下降。列表5.13展示了它作为一个新的类实现，即`NewtonianGradientBoosting`类，它继承自原始的GradientBoosting类，并对其现有方法和属性进行了一些添加和修改。特别是，我们添加了Hessian计算以平衡梯度步骤，以加速收敛，并添加了一个正则化项以防止过拟合。
- en: Listing 5.13 How XGBoost works
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.13 XGBoost的工作原理
- en: '[PRE22]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ① Defines a new class NewtonianGradientBoosting as a subclass of GradientBoosting
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义一个新的类NewtonianGradientBoosting作为GradientBoosting的子类
- en: ② Sets a regularization parameter reg_lambda
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置正则化参数reg_lambda
- en: ③ Initializes a constant Hessian matrix with ones
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用全为1的常数Hessian矩阵初始化
- en: ④ Fits the decision tree by dividing the negative gradient by the sum of the
    Hessian and the regularization parameter
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 通过将负梯度除以 Hessian 和正则化参数之和来拟合决策树
- en: ⑤ Creates an instance of the NewtonianGradientBoosting class with specified
    hyperparameters
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个具有指定超参数的 NewtonianGradientBoosting 类实例
- en: ⑥ Fits the NewtonianGradientBoosting model to the training data
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将 NewtonianGradientBoosting 模型拟合到训练数据
- en: ⑦ Predicts target values using the fitted model and calculating the accuracy
    score for evaluation
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 使用拟合的模型预测目标值，并计算准确度评分以进行评估
- en: 'The resulting accuracy is a little better than what we obtained from the original
    GradientBoosting class:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的准确性略好于我们从原始 GradientBoosting 类中获得的结果：
- en: '[PRE23]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In our case, the Hessian is probably not particularly helpful because it is
    the same for all due to the kind of objective function we use: the squared error.
    However, in the context of optimization with other objective functions, the Hessian
    matrix provides information about the curvature of a function, which can be used
    to determine the direction and rate of change of the function. Intuitively, you
    can figure out that, with larger curvatures, you have larger Hessian values, which
    reduce the effect of the gradient, acting as a brake for the learning rate. On
    the contrary, smaller curvatures lead to an acceleration of the learning rate.
    Using the information of the Hessian, you get an adaptive learning rate for each
    of your training examples. However, as a side effect, computing the second derivative
    can often be complicated or intractable, necessitating significant computation.
    Analytical expressions and numerical methods for determining the second derivative
    require substantial computational effort. In the next chapter, devoted to more
    advanced topics, we will provide further information on how to build your customized
    objective functions by computing gradients and Hessians analytically and numerically.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这个例子中，Hessian 矩阵可能并不特别有用，因为它对于所有数据都是相同的，这是因为我们使用的目标函数类型：平方误差。然而，在优化其他目标函数的上下文中，Hessian
    矩阵提供了关于函数曲率的信息，这可以用来确定函数的方向和变化率。直观上，你可以推断出，随着曲率的增大，Hessian 的值也会增大，这会减少梯度的作用，起到学习率制动的作用。相反，较小的曲率会导致学习率的加速。使用
    Hessian 的信息，你可以为每个训练样本获得一个自适应的学习率。然而，作为副作用，计算二阶导数通常可能很复杂或难以处理，需要大量的计算。确定二阶导数的解析表达式和数值方法需要大量的计算努力。在下一章，我们将提供有关如何通过计算梯度和大阵的解析和数值方法来构建自定义目标函数的更多信息。
- en: A role in the Newton optimization used by XGBoost is also played by the regularization
    term that is summed up in Hessian and reduces the target further—that is, the
    adjustment to be estimated by the base learners. Another idea taken from gradient
    descent that XGBoost uses is regularization in the form of L2 regularization,
    as implemented in our example, and L1 regularization. The additional regularization
    terms help to smooth the final learned weights and avoid overfitting by directly
    modifying the Newtonian descent step. Consequently, it is important to consider
    how to tune both L1 and L2 values, referred to as lambda and alpha in the XGBoost
    and LightGBM implementations, as significant hyperparameters for improving optimization
    results and reducing overfitting. These regularization values ensure that the
    Newton descent takes smaller steps during optimization.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 使用的 Newton 优化中也扮演着正则化项的角色，这些项在 Hessian 中汇总并进一步降低目标——即由基学习器估计的调整。XGBoost
    还借鉴了梯度下降的另一个想法，即采用正则化，如我们在示例中实现的那样，以及 L1 正则化。额外的正则化项有助于平滑最终学习的权重，并通过直接修改 Newton
    下降步骤来避免过拟合。因此，考虑如何调整 L1 和 L2 值（在 XGBoost 和 LightGBM 实现中称为 lambda 和 alpha）作为重要的超参数，以改善优化结果并减少过拟合非常重要。这些正则化值确保
    Newton 下降在优化过程中采取较小的步骤。
- en: In the next section, we will continue to explore the new capabilities introduced
    by XGBoost by examining the contribution of split-finding algorithms to the increased
    speed performance offered by the algorithm.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将继续通过检查分割查找算法对算法提供的加速性能的贡献，来探索 XGBoost 引入的新功能。
- en: 5.4.3 Accelerating with histogram splitting
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 使用直方图分割加速
- en: Gradient boosting is based on binary trees, which work by partitioning the data
    to have a better-optimized objective metric in the resulting splits than in the
    original set. Since gradient boosting treats all the features as numeric, it has
    a unique way of deciding how to partition. To find the feature to use for the
    split and the rule for the split, a binary tree decision should iterate through
    all the features, sort each feature, and evaluate every split point. Ultimately,
    the decision tree should pick the feature and its split point that led to better
    improvement relative to the objective.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升基于二叉树，通过将数据分区以在结果分割中获得比原始集合更好的优化目标度量。由于梯度提升将所有特征视为数值，它有独特的方式来决定如何分区。为了找到用于分割的特征和分割规则，二叉树决策应遍历所有特征，对每个特征进行排序，并评估每个分割点。最终，决策树应选择导致相对于目标有更好改进的特征及其分割点。
- en: With the emergence of larger datasets, the splitting procedure in decision trees
    poses serious scalability and computational problems for the original GBDT architecture
    based on serial models that continuously scan through data. From a computational
    point of view, the main cost in GBDT lies in learning the decision trees, and
    the most time-consuming part of learning a decision tree is finding the best split
    points.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大数据集的出现，决策树中的分割过程对基于序列模型的原始 GBDT 架构提出了严重的可扩展性和计算问题。从计算角度来看，GBDT 的主要成本在于学习决策树，而学习决策树中最耗时的工作是找到最佳分割点。
- en: Continuously looking for the best splitting point takes quite some time, rendering
    the algorithm quite demanding when training on a large number of features and
    instances. Histogram splitting helps reduce the time by replacing each feature’s
    value with the histogram’s split points to summarize its values. Listing 5.14
    simulates a split search on our data problem. In doing so, we define an objective
    function and a splitting function that can operate both as the original decision
    tree splitting algorithm or by the faster histogram-based splitting.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 持续寻找最佳分割点需要相当长的时间，这使得在大量特征和实例上进行训练时算法变得非常耗时。直方图分割通过用直方图的分割点来替换每个特征的值，以总结其值，从而有助于减少时间。列表
    5.14 模拟了在我们的数据问题上的分割搜索。为此，我们定义了一个目标函数和一个分割函数，这两个函数既可以作为原始决策树分割算法运行，也可以通过基于直方图的更快分割运行。
- en: Listing 5.14 The histogram split
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.14 直方图分割
- en: '[PRE24]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ① Function calculating and returning the Gini impurity of a set of labels y
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ① 计算并返回标签集 y 的基尼不纯度的函数
- en: ② If use_histogram is true, computes the histogram for the selected feature
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ② 如果 use_histogram 为 true，则计算所选特征的直方图
- en: ③ If use_histogram is false, just enumerates all the unique values in the feature
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 如果 use_histogram 为 false，则仅枚举特征中的所有唯一值
- en: ④ Initializes the best score and threshold
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 初始化最佳得分和阈值
- en: ⑤ Iterates over all possible thresholds
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 遍历所有可能的阈值
- en: ⑥ Splits y based into left and right subsets based on the selected threshol
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 根据 selected threshold 将 y 分割成左右子集
- en: ⑦ Calculates the Gini impurity score for the left and right subsets
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 计算左右子集的基尼不纯度得分
- en: ⑧ Updates the best score and threshold if the current split has a higher Gini
    impurity score than the previous best split
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 如果当前分割的基尼不纯度得分高于先前最佳分割，则更新最佳得分和阈值
- en: ⑨ Returns the best threshold and its corresponding Gini impurity score
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 返回最佳阈值及其对应的基尼不纯度得分
- en: In the code in listing 5.14, after defining a scoring function, the Gini impurity,
    we define a function that picks a feature and enumerates the values of its potential
    splits to be evaluated. If we use the basic approach, all its unique values are
    taken into account. Using the histogram approach instead, a 256-bin histogram
    is computed, and we use the bins delimiting values to be explored as potential
    split candidates. If our feature has more than 256 unique values, using the histogram
    will save us a lot of time when we just iterate through all split candidates and
    evaluate them by the scoring function.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 5.14 中的代码中，在定义评分函数和基尼不纯度之后，我们定义了一个函数，该函数选择一个特征并枚举其潜在分割的值以进行评估。如果我们使用基本方法，则考虑所有唯一值。相反，使用直方图方法，计算一个
    256 个分箱的直方图，我们使用分隔值以作为潜在分割候选者的分箱来探索。如果我们的特征有超过 256 个唯一值，使用直方图将节省我们在迭代所有分割候选者并使用评分函数评估它们时的大量时间。
- en: Now that we have explained the workings of the example functions, we are ready
    for a test. We decided to optimally split the latitude in the classification task
    of predicting if the host is in the upper or lower price range. Since the latitude
    feature has many unique values to be considered as splitting candidates because
    Manhattan is a long, thin north-south island where property values vary by latitude,
    it should result in a difficult task because we expect many different latitudes
    to be evaluated against the target.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解释了示例函数的工作原理，我们准备进行测试。我们决定在预测宿主是否位于高价或低价范围内的分类任务中，最优地分割纬度。由于纬度特征有许多独特的值可以作为分割候选者，因为曼哈顿是一个长而窄的南北岛屿，房地产价值随着纬度变化，因此这应该是一个困难的任务，因为我们预计会有许多不同的纬度需要与目标进行比较。
- en: 'In our first test, we try to find the best split just by evaluating all the
    unique values that the feature presents:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一次测试中，我们试图仅通过评估特征呈现的所有唯一值来找到最佳分割：
- en: '[PRE25]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In our second test, we rely on evaluating the splitting points found out by
    a histogram with 256 bins built on the feature:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第二次测试中，我们依赖于评估基于特征的256个箱直方图找到的分割点：
- en: '[PRE26]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Looking under the hood of histogram splitting, we find binning, where values
    for a variable are grouped into discrete bins, and each bin is assigned a unique
    integer to preserve the order between the bins. Binning is also commonly referred
    to as k-bins, where the k in the name refers to the number of groups into which
    a numeric variable is rearranged, and it is used in histogram plotting, where
    you can declare a value for k or automatically have it set to summarize and represent
    your data distribution.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在直方图分割的底层，我们发现分组，其中变量的值被分组到离散的箱中，每个箱被分配一个唯一的整数以保持箱之间的顺序。分组也常被称为k-箱，其中名称中的k指的是将数值变量重新排列成多少组，它用于直方图绘图，其中你可以声明k的值或自动设置它以总结和表示你的数据分布。
- en: The speed-up is due not only to a minor number of split points to evaluate,
    which can be tested in parallel, thus using multicore architectures, but also
    to the fact that histograms are integer-based data structures that are much faster
    to handle than continuous values vectors.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 加速不仅是因为要评估的分割点数量较少，这些分割点可以并行测试，从而使用多核架构，而且还因为直方图是基于整数的结构，比连续值向量处理得更快。
- en: 'XGBoost uses an algorithm to compute the best split based on presorting the
    values and usage of histograms. The presorting splitting works as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost使用一种算法来计算最佳分割，该算法基于对值进行预排序和直方图的用法。预排序分割的工作原理如下：
- en: For each node, enumerating the features
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个节点，枚举特征
- en: For every feature, sorting instances by their values
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个特征，按其值对实例进行排序
- en: Using a linear scan and histograms, determining the best split for the feature
    and computing the information gain
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用线性扫描和直方图，确定特征的最好分割并计算信息增益
- en: Picking the best solution among all the features and their best split
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有特征及其最佳分割中挑选最佳解决方案
- en: 'XGBoost has other concept improvements: the traditional split finding algorithm
    is denoted by `exact` specified as the value of the `tree_method` parameter. The
    Weighted Quantile Sketch, referred to as `approx` in the XGBoost API, is an exclusive
    feature unique to XGBoost. This split-finding technique utilizes approximations
    and harnesses information derived from gradient statistics. By employing quantiles,
    the method defines potential split points among candidates. Notably, the quantiles
    are weighted to prioritize selecting candidates capable of mitigating high gradients,
    reducing significant prediction errors.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost还有其他概念改进：传统的分割查找算法用`exact`表示，作为`tree_method`参数的值。加权分位数草图，在XGBoost API中称为`approx`，是XGBoost独有的特性。这种分割查找技术利用近似和利用梯度统计信息衍生出的信息。通过使用分位数，该方法在候选者中定义潜在的分割点。值得注意的是，分位数是加权的，以优先选择能够减轻高梯度、减少重大预测错误的候选者。
- en: Weighted Quantile Sketch using histograms is now available as `tree_method="hist"`,
    which, since the 2.0.0 release, is the default method. In contrast, the `approx`
    tree method generates a new set of bins for each iteration, whereas the `hist`
    method reuses the bins over multiple iterations.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 使用直方图的加权分位数草图现在作为`tree_method="hist"`可用，自2.0.0版本发布以来，这是默认方法。相比之下，`approx`树方法为每个迭代生成一组新的箱，而`hist`方法则重用多个迭代中的箱。
- en: 'Another feature of the algorithm is related to data storage in DMatrices. The
    most time-consuming part of tree learning is sorting the data. To reduce the sorting
    cost, we propose storing the data in in-memory units: a block. This allows linearly
    scanning over the presorted entries and parallelizing, giving us an efficient
    parallel algorithm for split finding.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的另一个特性与DMatrices中的数据存储有关。树学习的最耗时部分是对数据进行排序。为了减少排序成本，我们提出将数据存储在内存单元中：一个块。这允许我们线性扫描预排序条目并并行处理，从而为我们提供了一个高效的并行算法用于分割查找。
- en: After the successful histogram aggregation implementation in LightGBM, XGBoost
    adopted it. Histogram aggregation is also the main feature of `HistGradientBoosting`,
    the Scikit-learn histogram-based gradient boosting we will present after LightGBM.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在LightGBM中成功实现直方图聚合后，XGBoost采用了它。直方图聚合也是`HistGradientBoosting`的主要特性，这是Scikit-learn基于直方图的梯度提升，我们将在LightGBM之后介绍。
- en: 5.4.4 Applying early stopping to XGBoost
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.4 对XGBoost应用早期停止
- en: 'Section 5.3.1 illustrates how early stopping works with Scikit-learn’s gradient
    boosting. XGBoost also supports early stopping. You can specify early stopping
    by adding a few arguments when instantiating an XGBClassifier or XGBRegressor
    model:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 5.3.1节说明了早期停止如何在Scikit-learn的梯度提升中工作。XGBoost也支持早期停止。您可以通过在实例化XGBClassifier或XGBRegressor模型时添加一些参数来指定早期停止：
- en: '`early_stopping_rounds`—This is the number of rounds to wait patiently without
    improvement in the validation score before stopping the training. If you set it
    to a positive integer, the training will stop when the performance on the validation
    set hasn’t improved in that many rounds.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_rounds`—这是在停止训练之前等待验证分数没有改进的轮数。如果您将其设置为正整数，则当验证集的性能在该轮数内没有改进时，训练将停止。'
- en: '`eval_metric`—This is the evaluation metric to use for early stopping. By default,
    XGBoost uses `rmse` for regression’s root mean squared error and `error` for accuracy
    in classification. Still, you can specify any other from a long list (available
    at [https://mng.bz/xK2W](https://mng.bz/xK2W)) as well as specify your own metric
    (which will be discussed in the next chapter on advanced machine learning topics).'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_metric`—这是用于早期停止的评估指标。默认情况下，XGBoost使用`rmse`作为回归的均方根误差和`error`作为分类的准确率。不过，您也可以从长长的列表（可在[https://mng.bz/xK2W](https://mng.bz/xK2W)）中指定任何其他指标，以及指定您自己的指标（将在下一章关于高级机器学习主题的章节中讨论）。'
- en: In addition to setting these parameters, you also have to specify, at fitting
    time, a sample with its target where to monitor the evaluation metric. This is
    done by the `parameter eval_set`, which contains a list of tuples containing all
    the validation samples and their responses. In our example, we use only a validation
    set. Still, if there are multiple samples to monitor, XGboost will consider only
    the last tuple of data and response for stopping purposes.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设置这些参数外，在拟合时，您还必须指定一个包含其目标的样本，用于监控评估指标。这是通过`parameter eval_set`完成的，它包含一个包含所有验证样本及其响应的元组列表。在我们的例子中，我们只使用一个验证集。尽管如此，如果有多个样本需要监控，XGBoost将只考虑数据响应的最后元组用于停止目的。
- en: In listing 5.15, we replicate the same approach we experimented with earlier
    by splitting our data into a train and a test set. However, to properly monitor
    the evaluation metric, we further split the train set to extract a validation
    set from it.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表5.15中，我们通过将数据分为训练集和测试集来复制我们之前实验过的相同方法。然而，为了正确监控评估指标，我们进一步将训练集分割以从中提取验证集。
- en: Listing 5.15 Applying early stopping to XGBoost
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.15 对XGBoost应用早期停止
- en: '[PRE27]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ① Splits the indices of the data into training and test sets using a fixed random
    seed
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用固定的随机种子将数据索引分为训练集和测试集
- en: ② Further splits the training set into training and validation sets using the
    same random seed
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用相同的随机种子进一步将训练集分为训练集和验证集
- en: ③ Initializes an XGBoost classifier with an early stopping patience for 100
    rounds
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化一个具有100轮早期停止耐心的XGBoost分类器
- en: ④ Uses the 'error' parameter, equivalent to accuracy, as an evaluation metric
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用'error'参数，相当于准确率，作为评估指标
- en: ⑤ Fits the XGBoost classifier to the training data X and labels y and performance
    on the validation data Xv and yv
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将XGBoost分类器拟合到训练数据X和标签y，以及验证数据Xv和yv的性能
- en: ⑥ Prints the accuracy score after comparing the predicted labels with the true
    labels
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 在将预测标签与真实标签比较后打印准确率得分
- en: 'After completing the training, we managed to get this accuracy measure, which
    is slightly underperforming in respect of our previous cross-validation results
    because it is obtained by training on fewer examples—that is, 64% of available
    data since we reserved 20% for test and 16% for validation:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们成功获得了这个准确度指标，它在我们的先前交叉验证结果中表现略逊一筹，因为它是在更少的示例上训练得到的——也就是说，是可用数据的64%，因为我们保留了20%用于测试和16%用于验证：
- en: '[PRE28]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: During the training, the evaluation metric is constantly checked, and the fit
    procedure is halted if it doesn’t improve from more iterations than those specified
    by `early_stopping_rounds`. The best iteration is automatically recorded and used
    at prediction time. Thus, you have nothing more to do with the model. If you need
    to verify how many iterations it took before stopping, you can obtain it by inquiring
    about the model using the `best_iteration` attribute. In our example, `xgb.best_iteration`
    returns 200.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，评估指标会不断被检查，如果迭代次数没有超过由`early_stopping_rounds`指定的次数，拟合过程将被终止。最佳迭代会被自动记录并在预测时使用。因此，你无需对模型做任何事情。如果你需要验证停止前的迭代次数，可以通过查询模型的`best_iteration`属性来获取。在我们的例子中，`xgb.best_iteration`返回200。
- en: 5.5 Introduction to LightGBM
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 LightGBM简介
- en: 'LightGBM was first introduced in a 2017 paper titled “LightGBM: A Highly Efficient
    Gradient Boosting Decision Tree” by Guolin Ke and his team at Microsoft ([https://mng.bz/AQdz](https://mng.bz/AQdz)).
    Recently, the package reached the 4.3.0 version, which is the version we tested
    in this book. According to the authors, the term “light” in LightGBM highlights
    the algorithm’s faster training and lower memory usage than traditional gradient
    boosting decision trees. The paper demonstrated, through experiments on multiple
    public datasets, the algorithm’s effectiveness and its ability to speed up the
    training process of conventional gradient boosting decision trees by over 20 times
    while maintaining almost the same accuracy. LightGBM was made available as open-source
    software on GitHub ([https://github.com/microsoft/LightGBM/](https://github.com/microsoft/LightGBM/)),
    quickly gaining popularity among data scientists and machine learning practitioners.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 'LightGBM首次在2017年一篇题为“LightGBM: A Highly Efficient Gradient Boosting Decision
    Tree”的论文中被介绍，该论文由微软的Guolin Ke及其团队撰写([https://mng.bz/AQdz](https://mng.bz/AQdz))。最近，该软件包达到了4.3.0版本，这是我们在这本书中测试的版本。根据作者的说法，LightGBM中的“light”一词强调了该算法比传统的梯度提升决策树训练更快、内存使用更低。论文通过在多个公共数据集上的实验，证明了该算法的有效性及其通过超过20倍的速度加快传统梯度提升决策树的训练过程，同时保持几乎相同的准确率。LightGBM作为开源软件在GitHub上提供([https://github.com/microsoft/LightGBM/](https://github.com/microsoft/LightGBM/))，迅速在数据科学家和机器学习从业者中获得了人气。'
- en: On paper, LightGBM shares many characteristics similar to those of XGBoost,
    such as support for missing values, native handling of categorical variables,
    GPU training, networked parallel training, and monotonicity constraints. We will
    tell you more about all that in the next chapter. In addition, LightGBM also supports
    sparse data. However, its major advantage lies in its speed, as it is significantly
    faster than XGBoost on various tasks, which has made it popular in both Kaggle
    competitions and real-world applications. The Kaggle community quickly took notice
    of LightGBM and began incorporating it into their competition entries alongside
    the already-popular XGBoost. In fact, mlcontests.com, a website that tracks the
    data science competition scene, reported in 2022 that LightGBM had become the
    preferred tool among competition winners, surpassing XGBoost in popularity. An
    impressive 25% of reported solutions for tabular problems were based on LightGBM.
    While LightGBM has seen comparable success among data science practitioners, XGBoost
    remains more popular overall. For example, the XGBoost repository has many more
    GitHub stars than the LightGBM repository.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在理论上，LightGBM与XGBoost有许多相似的特征，例如支持缺失值、原生处理分类变量、GPU训练、网络并行训练以及单调性约束。我们将在下一章中详细介绍这些内容。此外，LightGBM还支持稀疏数据。然而，它的主要优势在于速度，因为它在许多任务上比XGBoost快得多，这使得它在Kaggle竞赛和实际应用中都变得非常流行。Kaggle社区很快注意到了LightGBM，并将其与已经流行的XGBoost一起纳入他们的竞赛作品中。实际上，跟踪数据科学竞赛场景的网站mlcontests.com在2022年报告称，LightGBM已经成为竞赛获胜者的首选工具，在受欢迎程度上超过了XGBoost。令人印象深刻的是，25%的表格问题解决方案都是基于LightGBM的。虽然LightGBM在数据科学从业者中取得了相当的成功，但XGBoost在整体上仍然更受欢迎。例如，XGBoost仓库在GitHub上的星标比LightGBM仓库多得多。
- en: LightGBM is a cross-platform machine learning library available for Windows,
    Linux, and MacOS. It can be installed using various tools, such as pip or conda,
    or built from source code (see the complete installation guide at [https://mng.bz/ZlEP](https://mng.bz/ZlEP)).
    Its usage syntax is similar to Scikit-learn’s, making it easy for users who are
    familiar with Scikit-learn to transition to LightGBM. When optimizing gradient
    descent, LightGBM follows in the footsteps of XGBoost by utilizing the Newton-Raphson
    update, which involves dividing the gradient by the Hessian matrix. Guolin Ke’s
    answer on GitHub confirms that (see [https://github.com/microsoft/LightGBM/issues/5233](https://github.com/microsoft/LightGBM/issues/5233)).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM是一个跨平台的机器学习库，适用于Windows、Linux和MacOS。可以使用pip或conda等工具进行安装，或从源代码构建（请参阅完整的安装指南[https://mng.bz/ZlEP](https://mng.bz/ZlEP)）。它的使用语法与Scikit-learn类似，使得熟悉Scikit-learn的用户可以轻松过渡到LightGBM。在优化梯度下降时，LightGBM遵循XGBoost的步伐，通过使用牛顿-拉夫森更新来控制梯度下降，这涉及到将梯度除以海森矩阵。GitHub上Guolin
    Ke的回答确认了这一点（请参阅[https://github.com/microsoft/LightGBM/issues/5233](https://github.com/microsoft/LightGBM/issues/5233)）。
- en: Let’s put the algorithm to the test on the same problem we previously examined
    with ScikitLearn’s GradientBoosting and XGBoost.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在之前使用ScikitLearn的GradientBoosting和XGBoost考察的相同问题上测试这个算法。
- en: Listing 5.16 LightGBM classifier
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.16 LightGBM分类器
- en: '[PRE29]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ① Initializes an LGBMClassifier with a number of estimators, maximum tree depth,
    and minimum number of child samples
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化一个具有估计器数量、最大树深度和最小子样本数量的LGBMClassifier
- en: ② Forces column-wise histogram building
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: ② 强制按列构建直方图
- en: ③ Creates a model pipeline that includes a column transformation step and an
    LGBMClassifier step
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个包含列转换步骤和LGBMClassifier步骤的模型管道
- en: ④ Performs five-fold cross-validation using the model pipeline using accuracy
    scoring
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用模型管道执行五折交叉验证，并使用准确率评分
- en: ⑤ Prints the mean test score and standard deviation of the test scores obtained
    during cross-validation
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印交叉验证期间获得的测试平均分数和标准差
- en: 'The following are the impressive results in terms of accuracy, training, and
    prediction times:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在准确率、训练和预测时间方面的令人印象深刻的结果：
- en: '[PRE30]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Like XGBoost, LightGBM controls gradient descent with parameters such as `n_estimators`,
    `learning_rate,` `lambda_l1,` and `lambda_l2` (L1 and L2 regularization, respectively).
    The most important parameters of LightGBM that help control its complexity are
    as follows:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 与XGBoost类似，LightGBM使用`n_estimators`、`learning_rate`、`lambda_l1`和`lambda_l2`（L1和L2正则化）等参数来控制梯度下降。帮助LightGBM控制其复杂性的最重要的参数如下：
- en: '`max_depth`—This parameter controls the maximum depth of each tree in the ensemble.
    A higher value increases the complexity of the model, making it more prone to
    overfitting. If it is set to –1 it means that no limit is set to the growth of
    the trees.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`—此参数控制集成中每个树的最大深度。较高的值会增加模型的复杂度，使其更容易过拟合。如果设置为-1，则表示不对树的生长设置限制。'
- en: '`num_leaves`—This parameter specifies the maximum number of leaves in a tree
    and, therefore, the complexity of the model. To avoid overfitting, it should be
    set to less than `2**(max_depth)`.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_leaves`—此参数指定树中最大叶子节点数，因此也决定了模型的复杂度。为了避免过拟合，应将其设置为小于`2**(max_depth)`。'
- en: '`min_data_in_leaf`—This parameter controls the minimum number of samples required
    to be present in each leaf node. A higher value can prevent the tree from growing
    too deep and overfitting but can also lead to underfitting if set too high. The
    default value is 20\. We suggest trying lower values, such as 10, and then testing,
    increasing the value to 300.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_data_in_leaf`—此参数控制每个叶子节点中必须存在的最小样本数。较高的值可以防止树生长过深和过拟合，但如果设置得太高，也可能导致欠拟合。默认值为20。我们建议尝试较低的值，例如10，然后进行测试，将值增加到300。'
- en: 'The parameters `feature_fraction` and `bagging_fraction` control how LightGBM
    samples from features and examples:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`feature_fraction`和`bagging_fraction`控制LightGBM从特征和示例中采样的方式：
- en: '`feature_fraction`—This parameter controls the fraction of features to be considered
    at each split. Similar to the `colsample_bytree` parameter in XGBoost, it can
    help reduce overfitting by preventing the model from relying too heavily on any
    feature.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_fraction`—此参数控制每个分割时考虑的特征比例。类似于XGBoost中的`colsample_bytree`参数，它可以通过防止模型过度依赖任何特征来帮助减少过拟合。'
- en: '`bagging_fraction`—This parameter controls the fraction of data to be used
    for each tree. Similar to the subsample parameter in XGBoost, it can help reduce
    overfitting and improve training speed by randomly sampling from the data.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bagging_fraction`—此参数控制每个树使用的数据的分数。类似于XGBoost中的subsample参数，它可以通过从数据中随机采样来帮助减少过拟合并提高训练速度。'
- en: '`bagging_freq`—This parameter, which is not present in XGBoost, determines
    how frequently bagging should be applied. It turns off bagging examples when set
    to 0, even if `bagging_fraction` is specified. A value of n means bagging at every
    n iteration. For instance, a value of 2 means you have a bagged iteration every
    two (half of the time).'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bagging_freq`—此参数在XGBoost中不存在，它决定了bagging应该应用的频率。当设置为0时，即使指定了`bagging_fraction`，也会关闭bagging示例。值为n表示每n次迭代进行一次bagging。例如，值为2表示每两次（一半的时间）进行一次bagged迭代。'
- en: Related to the way LightGBM executes during the training, `verbosity` controls
    the amount of output information during training, while `force_col_wise` indicates
    histograms for feature splits during tree construction to be built based on columns.
    LightGBM can build histograms either column-wise or row-wise. Column-wise histogram
    building is generally faster, but it can require more memory, especially for datasets
    with a large number of columns. Row-wise histogram building is slower, but it
    can be more memory-efficient when dealing with datasets with a large number of
    columns. LightGBM will automatically choose the best method for building histograms
    for the dataset. However, you can also force LightGBM to use a specific method
    by setting the `force_col_wise` or `force_row_wise` parameters.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 与LightGBM在训练过程中的执行方式相关，`verbosity`控制训练过程中的输出信息量，而`force_col_wise`表示在树构建期间基于列构建特征分割的直方图。LightGBM可以按列或按行构建直方图。按列构建直方图通常更快，但可能需要更多内存，特别是对于具有大量列的数据集。按行构建直方图较慢，但处理具有大量列的数据集时可能更节省内存。LightGBM将自动选择为数据集构建直方图的最佳方法。然而，您也可以通过设置`force_col_wise`或`force_row_wise`参数来强制LightGBM使用特定方法。
- en: 'As for XGBoost, LightGBM can also use different base learners by specifying
    the `boosting` parameter:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 对于XGBoost，LightGBM也可以通过指定`boosting`参数来使用不同的基础学习器：
- en: '`gbdt`—The default option, using decision trees as base learners'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gbdt`—默认选项，使用决策树作为基础学习器'
- en: '`rf`—Implements the random forests algorithm'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rf`—实现了随机森林算法'
- en: '`dart`—Implements the “Dropouts meet Multiple Additive Regression Trees” algorithm'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dart`—实现了“Dropouts meet Multiple Additive Regression Trees”算法'
- en: In addition, setting the parameter `linear_tree` to true, as you are using the
    default `boosting=gbdt`, will fit a piecewise linear gradient boosting tree—that
    is, decision trees having linear models as their terminal nodes. This is a compromise
    solution that uses both the nonlinear learning capabilities of decision trees
    and the extrapolative capabilities of linear models with unseen, outlying cases.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将参数`linear_tree`设置为true，因为你正在使用默认的`boosting=gbdt`，将拟合一个分段线性梯度提升树——即终端节点具有线性模型的决策树。这是一个折衷方案，它同时使用决策树的非线性学习能力和线性模型对未见、异常案例的推演能力。
- en: In the following section, we will examine closely all the innovations that distinguish
    LightGBM from XGBoost.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将仔细检查区分LightGBM和XGBoost的所有创新。
- en: 5.5.1 How LightGBM grows trees
  id: totrans-485
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 LightGBM如何生长树
- en: Let’s examine each characteristic that distinguishes LightGBM from XGBoost,
    starting with how LightGBM grows decision trees. Instead of increasing the tree
    level-wise (also known as *depth-first*) like XGBoost, LightGBM grows the tree
    leaf-wise (also known as *best first)*. This means that the algorithm chooses
    the leaf node that provides the maximum gain and then splits it further until
    it is no longer advantageous. In contrast, the level-wise approach simultaneously
    splits all the nodes at the same depth.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查区分LightGBM和XGBoost的每个特征，从LightGBM如何生长决策树开始。与XGBoost按层次增加树（也称为深度优先）不同，LightGBM按叶节点增长树（也称为最佳优先）。这意味着算法选择提供最大增益的叶节点，然后进一步分割它，直到不再有利可图。相比之下，层次方法同时分割同一深度的所有节点。
- en: To sum up, in XGBoost’s level-wise growth approach, the algorithm grows all
    tree leaves to the same level. Then it splits them simultaneously, which may result
    in many insignificant leaves that don’t contribute much to the final prediction.
    In contrast, LightGBM’s leaf-wise growth approach splits the leaf with the maximum
    loss reduction at each step, resulting in fewer leaves but with higher accuracy.
    The leaf-wise approach allows LightGBM to focus only on the important features
    with the most significant effect on the target variable. This means that the algorithm
    can quickly converge to the optimal solution with fewer splits and a smaller number
    of trees.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在XGBoost的层向增长方法中，算法将所有树叶生长到同一水平。然后它同时将它们分割，这可能会导致许多无意义的叶子节点，这些节点对最终预测的贡献不大。相比之下，LightGBM的叶向增长方法在每一步都分割具有最大损失减少的叶子，从而产生更少的叶子节点但准确率更高。叶向方法允许LightGBM只关注对目标变量影响最大的重要特征。这意味着算法可以通过更少的分割和更少的树快速收敛到最优解。
- en: 'Figure 5.10 shows a representation of the two approaches: on the left, the
    level-wise approach and, on the right, the leaf-wise approach, both constrained
    to have, at the most, four terminal nodes. The two approaches take completely
    different paths in terms of the rules they decide to apply and how they segment
    the data.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10展示了两种方法的表示：左侧是层向方法，右侧是叶向方法，两者都限制最多有四个终端节点。这两种方法在决定应用哪些规则以及如何分割数据方面采取了完全不同的路径。
- en: '![](../Images/CH05_F10_Ryan2.png)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F10_Ryan2.png)'
- en: Figure 5.10 How level-wise (on the left) and leaf-wise (on the right) tree growth
    differ
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 层向（左侧）和叶向（右侧）树增长的不同
- en: It is important to point out that if you allow two trees to grow using the same
    data, one using a leaf-wise approach and one using a level-wise approach fully,
    they will define the same terminal leaves and predictions. The distinction lies
    in how they are built, with the leaf-wise approach being more aggressive in splitting
    nodes that provide the most information gain first.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 需要指出的是，如果你允许两棵树使用相同的数据生长，一棵使用叶向方法，另一棵使用完全的层向方法，它们将定义相同的终端叶子和预测。区别在于它们的构建方式，叶向方法在首先分割提供最大信息增益的节点方面更为激进。
- en: This implies that the leaf-wise and level-wise approaches differ when a stopping
    rule is applied based on reaching a certain number of terminal nodes or a specific
    depth in tree splitting. In this case, the leaf-wise approach can result in smaller
    trees, faster training times, and higher accuracy, but it also carries an increased
    risk of overfitting. To control the depth of the tree leaf-wise growth and address
    overfitting, you can control the max-depth parameter in LightGBM.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在基于达到一定数量的终端节点或树分割的特定深度应用停止规则时，叶向和层向方法会有所不同。在这种情况下，叶向方法可能导致更小的树、更快的训练时间和更高的准确率，但也伴随着过拟合风险增加。为了控制树叶的深度增长并解决过拟合问题，你可以在LightGBM中控制最大深度参数。
- en: 5.5.2 Gaining speed with exclusive feature bundling and gradient-based one-side
    sampling
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 通过独家特征捆绑和基于梯度的单侧采样获得速度
- en: To further reduce the training time, a basic strategy in gradient boosting and
    many other machine learning algorithms is reducing the number of examples processed.
    The simplest way to reduce the number of processed values is using stochastic
    sampling (i.e., row reduction) and/or dimensionality reduction techniques such
    as column sampling or principal component analysis (i.e., column reduction). Although
    sampling can improve accuracy in the presence of noise in data, excessive sampling
    can harm the training process and decrease predictive performance. In LightGBM,
    the *Gradient-Based One-Side Sampling* algorithm (GOSS) determines the manner
    and extent of sampling. Dimensionality reduction techniques rely on identifying
    redundancies in the data and combining them using a linear combination, typically
    a weighted sum. However, linear combinations can destroy nonlinear relationships
    in the data. Dimensionality reduction by discarding rare signals may lead to a
    decrease in model accuracy if the successful resolution of the data problem depends
    on those weak signals. In LightGBM, dimensionality reduction is handled by *Exclusive
    Feature Bundling* (EFB), which is a way to reduce the column dimension without
    losing information.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步减少训练时间，梯度提升和其他许多机器学习算法的基本策略是减少处理示例的数量。减少处理值数量的最简单方法是使用随机抽样（即行减少）和/或降维技术，如列抽样或主成分分析（即列减少）。尽管抽样可以在数据存在噪声的情况下提高准确性，但过度抽样可能会损害训练过程并降低预测性能。在LightGBM中，*基于梯度的单侧抽样*算法（GOSS）决定了抽样的方式和程度。降维技术依赖于识别数据中的冗余，并通过线性组合（通常是加权求和）将它们结合起来。然而，线性组合可能会破坏数据中的非线性关系。通过丢弃罕见信号进行降维可能会导致模型精度降低，如果数据问题的成功解决依赖于这些弱信号。在LightGBM中，降维是通过*独家特征捆绑*（EFB）来处理的，这是一种在不丢失信息的情况下减少列维度的方法。
- en: Let’s start by explaining the two major speed improvements in LightGBM, starting
    with how EFB works. EFB is a technique that efficiently decreases the number of
    features without compromising data integrity. When using extensively one-hot encoded
    and binary features, many features become sparse, with few values and an abundance
    of zeros. You can retain all non-zero values without loss by summing these features
    and encoding some values. LightGBM optimizes computations and data dimensionality
    by grouping these features into *Exclusive Feature Bundles*, ensuring that predictive
    accuracy is maintained.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先解释LightGBM中的两个主要速度提升，从EFB的工作原理开始。EFB是一种技术，它有效地减少了特征数量，同时不损害数据完整性。当广泛使用one-hot编码和二进制特征时，许多特征变得稀疏，值少而零多。你可以通过求和这些特征并编码一些值来保留所有非零值而不会丢失。LightGBM通过将这些特征分组到*独家特征捆绑*（Exclusive
    Feature Bundles）中来优化计算和数据维度，确保预测准确性得到保持。
- en: Figure 5.11 shows how effectively two features can be bundled together. The
    solution involves adding feature B to feature A only for non-zero values, using
    the maximum value present in feature A. This combined feature will preserve the
    original features’ order because the values from feature A are separate from the
    values from feature B and will be located in different sections of the value distribution.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11展示了如何有效地将两个特征捆绑在一起。该解决方案涉及仅在特征A非零值时添加特征B到特征A，使用特征A中存在的最大值。这种组合特征将保留原始特征的顺序，因为特征A的值与特征B的值是分开的，并将位于值分布的不同部分。
- en: '![](../Images/CH05_F11_Ryan2.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11](../Images/CH05_F11_Ryan2.png)'
- en: Figure 5.11 Demonstrating how EFB works when combining two features
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11演示了EFB在结合两个特征时的工作原理
- en: Finding the optimal way to bundle exclusive features is a complex problem, classified
    as NP-hard. However, according to the LightGBM paper by Guolin Ke and his team,
    a greedy algorithm can provide a good approximation by automatically bundling
    many features. The feature bundling algorithm works sequentially, selecting the
    features with the least number of overlapping values and bundling them together.
    If it finds another feature with minimal overlap, it continues to bundle. Otherwise,
    it starts a new bundle until no more bundles can be found. The stopping rule is
    provided by the degree of conflicts two features have. If they have more conflicts
    than a certain gamma threshold, the bundle cannot be made, and the entire process
    may stop if there aren’t better candidates. Although the resulting bundles from
    this greedy process are not guaranteed to be optimal, the algorithm provides an
    acceptable solution in a reasonable time.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找捆绑独家特征的最佳方式是一个复杂的问题，被归类为NP-hard。然而，根据Guolin Ke及其团队撰写的LightGBM论文，贪婪算法可以通过自动捆绑许多特征来提供一个良好的近似。特征捆绑算法按顺序工作，选择具有最少重叠值的特征并将它们捆绑在一起。如果它找到另一个具有最小重叠的特征，它将继续捆绑。否则，它开始一个新的捆绑，直到找不到更多的捆绑为止。停止规则由两个特征之间的冲突程度提供。如果它们的冲突多于某个gamma阈值，则无法创建捆绑，如果没有更好的候选者，整个过程可能停止。尽管从这个贪婪过程中产生的捆绑不保证是最佳的，但该算法在合理的时间内提供了一个可接受的解决方案。
- en: The other performance improvement presented in the paper is GOSS. As we mentioned,
    if EFB is aimed at reducing column dimensionality, GOSS works on the rows by sampling
    them effectively without bias.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中提出的其他性能改进是GOSS。正如我们提到的，如果EFB旨在减少列维度，GOSS则通过有效采样行来工作，而不带偏见。
- en: GOSS is based on the observation that certain data instances are unlikely to
    provide useful information for finding a split point. Searching a carefully selected
    subset of the training set can save computation time without affecting predictive
    accuracy. Additionally, in gradient boosted decision trees, the algorithm implicitly
    specifies a weight for data instances when optimizing for the gradient for each
    data instance. Determining weights is crucial to compute a correction of the previous
    estimates, but that could also be used for sampling the data instances that could
    be more interesting to learn.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: GOSS基于以下观察：某些数据实例不太可能为找到分割点提供有用的信息。搜索精心选择的训练集子集可以节省计算时间，而不会影响预测精度。此外，在梯度提升决策树中，算法在为每个数据实例的梯度进行优化时隐式指定数据实例的权重。确定权重对于计算先前估计的纠正至关重要，但这也可以用于采样可能更有趣学习的数据实例。
- en: 'GOSS estimates data examples with larger gradients to contribute more toward
    information gain. Focusing on examples with larger gradients and ignoring part
    of those with smaller ones should reduce the number of processed data instances
    while still optimizing the algorithm for predictions. The procedure for GOSS results
    in the following:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: GOSS估计具有更大梯度的数据示例对信息增益的贡献更大。专注于具有更大梯度的示例，并忽略其中一部分具有较小梯度的示例，应该可以减少处理的数据实例数量，同时仍然优化算法以进行预测。GOSS的流程如下：
- en: GOSS first sorts the data examples according to the absolute value of their
    gradients.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GOSS首先根据梯度绝对值对数据示例进行排序。
- en: It selects the top a × 100% data examples.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它选择顶部a × 100%的数据示例。
- en: It randomly samples b × 100% data examples from the rest of the data.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从剩余数据中随机抽取b × 100%的数据示例。
- en: It trains the decision tree on the combined samples using a weight of 1 for
    the top data examples and a weight of (1 – a) / b for the randomly sampled data
    examples.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用1的权重对顶部数据示例进行训练，并使用(1 – a) / b的权重对随机抽取的数据示例进行训练。
- en: The final weighting is necessary to maintain the original data distribution
    of the dataset and avoid any unwanted shift in its representation.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 最终加权是必要的，以保持数据集的原始数据分布，并避免其表示中的任何不希望的变化。
- en: GOSS can accelerate the training of gradient boosted decision trees, especially
    when dealing with larger datasets and complex trees. The original paper’s authors
    demonstrate that the error incurred by GOSS’s sampling approximation becomes negligible
    for larger datasets compared to the traditional method. In our experience using
    GOSS, at best, you get similar results compared to standard LightGBM training.
    Still, the speed-up is significant, making GOSS a good choice for faster experimentation
    when looking for the correct hyperparameters or selecting the most relevant features
    for your problem.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrary to the other speed-ups we presented, GOSS is not used by default:
    you must specify that you want to use it.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.3 Applying early stopping to LightGBM
  id: totrans-510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LightGBM supports early stopping, and the parameters to control it are similar
    to those used in the XGBoost implementation. In the example in listing 5.17, we
    use LightGBM for training and have the algorithm assess its performance during
    the training phase using a test set. If there is no improvement in performance
    on the test set for 100 iterations, the algorithm halts the training process.
    It selects the round of iterations that achieved the highest performance on the
    test set so far.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.17 Applying early stopping to LightGBM
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ① Splits the indices of the data into training and test sets using a fixed random
    seed
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: ② Further splits the training set into training and validation sets using the
    same random seed
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: ③ Initializes a LightGBM classifier with a number of estimators, max depth,
    and minimum number of child samples
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: ④ Fits the LightGBM classifier to the training data X and labels y and performance
    on the validation data Xv and yv
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Sets accuracy as an evaluation metric
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Sets a callback to suppress the evaluation (period=0)
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Prints the accuracy score after comparing the predicted labels with the true
    labels
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 'Even in this case, the result is penalized by the fact that we are training
    only on 64% of the available data:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: However, there are some minor differences from the XGBoost implementation that
    you can notice from the code. The `eval_metric` takes different names (that you
    can check at [https://mng.bz/RVZK](https://mng.bz/RVZK)) and, to suppress the
    printing of the evaluations during the training, you don’t use the verbose parameters
    as in XGBoost; rather, you have to specify a callback function (`log_evaluation`)
    that has to be declared at the fitting time in the list of callbacks.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, early stopping has been implemented as a callback function too (see
    [https://mng.bz/2yK0](https://mng.bz/2yK0)). Keeping the declaration of early
    stopping rounds during model instantiation stayed just for maintaining API compatibility
    with XGBoost. In case you use early stopping as a callback, you have more control
    over the way the LightGBM stops its training:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '`first_metric_only` allows you to indicate whether to use only the first metric
    for early stopping or any metric you pointed out using.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_delta` signals the minimum improvement in the metric to keep on training,
    which usually is set to zero (any improvement), but it could be raised to impose
    more strict control over the growth of the ensemble.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the previous example, you just need to remove the `early_stopping_rounds`
    from the LGBMClassifier instantiation and add the proper call back to the callback
    list in the fit method to obtain the same result:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Whatever method you use, the iteration index, resulting in the best validation
    score, will be stored in the `best_iteration` attribute of a model, and that iteration
    will be automatically used when predicting.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.4 Making XGBoost imitate LightGBM
  id: totrans-530
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the introduction of LightGBM and its impressive usage of unbalanced decision
    trees, XGBoost also started supporting the leaf-wise strategy in addition to its
    original level-wise strategy. In XGBoost, the original level-wise approach is
    called `depthwise,` and the leaf-wise strategy is called `lossguide`. By setting
    one or another using the `grow_policy` parameter, you can have your XGBoost behave
    as a LightGBM. In addition, XGBoost authors suggested, when using the lossguide
    grow policy, to set the following parameters to avoid overfitting:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: '`max_leaves`—Sets the maximum number of nodes to be added and is only relevant
    for the lossguide policy.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`—Sets the maximum depth of the tree. If `grow_policy` is set to
    `depthwise`, `max_depth` behaves as usual. However, if `grow_policy` is set to
    `lossguide`, `max_depth` can be set to zero, indicating no depth limit.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incidentally, you also have the same parameters in LightGBM to be used for the
    same purpose (`max_leaves` is an alias—i.e., another working name of the parameter
    `num_leaves`).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.5  How LightGBM inspired Scikit-learn
  id: totrans-535
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In version 0.21 of Scikit-learn, two novel implementations of gradient boosting
    trees were added: `HistGradientBoostingClassifier` and `HistGradientBoostingRegressor`,
    inspired by LightGBM. You may wonder why you would bother with this new implementation
    if the current LightGBM and XGBoost versions can offer you everything you need
    to develop the best-performing tabular solutions based on gradient boosting. They
    also ensure full compatibility with Scikit-learn API. It is worth the time to
    look at it because the histogram-based implementation, though now a work in progress,
    is expected to take over the original, offering the same control over the learning
    process and building of decision trees. Moreover, it has shown even better predictive
    performances than XGBoost and LightGBM in some specific applications. Hence, it
    may be worth testing for your specific problems.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: 'In comparison with the original Scikit-learn implementation for gradient boosting,
    the new histogram-based ones present new characteristics:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: Binning
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multicore (the initial implementation was single-core)
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No sparse data support
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in support for missing values
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monotonicity and interaction constraints
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Native categorical variables
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the moment, talking about differences, there is no support for sparse data
    in the new histogram-based implementation. Consequently, if your data is in a
    sparse matrix, you should first densify the data matrix. Also, some other features
    typical of `GradientBoostingClassifier` and `GradientBoostingRegressor` still
    need to be supported—for instance, some loss functions.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: On the API side, most parameters are unchanged from `GradientBoosting``Classifier`
    and `GradientBoostingRegressor`. One exception is the `max_iter` parameter that
    replaces `n_estimators`. The following listing shows an example of the `HistGradientBoostingClassifier`
    applied to our classification problem with the Airbnb NYC dataset classifying
    listings above the median market value.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.18 The new Scikit-learn’s histogram gradient boosting
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① Initializes a HistGradientBoostingClassifier with specific hyperparameters
    for the boosting algorithm
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a model pipeline combining data preprocessing (column_transform) and
    the model
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: ③ Executes five-fold cross-validation on the model pipeline returning scores
    and trained estimators
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints the mean and standard deviation of the accuracy scores from cross-validation
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: The results are
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Compared to our previous examples with XGBoost and LightGBM, this one differs
    from the used command and the `max_iter` parameter in exchange for the usual `n_estimators`.
    Also, the Scikit-learn new boosting algorithm is a histogram one. You just set
    the `max_bins` argument to change the initial default value of 255 (it is 256
    bins because 1 is reserved for missing cases).
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is still under development and lacks support for sparse data.
    This implies that it cannot perform as fast as XGBoost or LightGBM in the presence
    of many one-hot encoded features, no matter how you prepare your data.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-556
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensemble algorithms are used to improve the predictive power of a single model
    by using multiple models or chaining them together:'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble algorithms are often based on decision trees.
  id: totrans-558
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two core ensemble strategies: averaging and boosting.'
  id: totrans-559
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaging strategies, such as random forests, tend to reduce the variance of
    predictions while only slightly increasing the bias.
  id: totrans-560
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pasting is a type of averaging approach that involves creating a set of different
    models trained on subsamples of the data and pooling the predictions together.
  id: totrans-561
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging is similar to averaging but with bootstrapping instead of subsampling.
  id: totrans-562
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaging methods can be computationally intensive and increase bias by excluding
    important parts of the data distribution through sampling.
  id: totrans-563
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Random forests is an ensemble learning algorithm that combines decision trees
    by bootstrapping samples and subsampling features during modeling (random patches):'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It creates a set of models that are different from each other and produces more
    reliable and accurate predictions.
  id: totrans-565
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used to determine feature importance and measure cases’ similarity
    in a dataset.
  id: totrans-566
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm requires fine-tuning its few hyperparameters, like the number
    of employed trees, and adjusting bias-variance tradeoffs by setting the maximum
    number of features used for splits, the maximum depth of trees, and the minimum
    size of the terminal branches.
  id: totrans-567
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be computationally costly if the number of trees is set too high.
  id: totrans-568
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ERT, Extremely Randomized Trees, is a variation of the random forests algorithm:'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It randomly selects the feature for the split at each decision tree node, leading
    to less variance (because trees are more diverse) but more bias (randomization
    sacrifices some of the decision trees’ predictive accuracy, resulting in a higher
    bias).
  id: totrans-570
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is more computationally efficient and useful for large datasets with many
    collinear and noisy features.
  id: totrans-571
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces variance by making the resulting set of trees less correlated.
  id: totrans-572
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GBDT is a highly effective machine learning method for tabular data problems.
    It has become a leading approach in various domains, including multiclass classification,
    advertising click prediction, and search engine ranking. Compared to other methods,
    such as neural networks, support vector machines, random forests, and bagging
    ensembles, GBDT generally performs better in standard tabular problems.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting is effective because it combines gradient descent, an optimization
    procedure typical of linear models and neural networks, and decision trees trained
    on the gradients derived from the sum of the previous decision trees.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn offers one of the earliest options for gradient boosting algorithms
    for regression and classification tasks. Recently, the original algorithm was
    replaced by a speedier histogram-based one, which is still under development.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XGBoost, an algorithm for gradient boosting decision trees, gained popularity
    after its successful use in the Higgs Boson Machine Learning Challenge on Kaggle.
    It is based on a more complex optimization based on Newton’s Descent, and it offers
    the following advantages:'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Ability to handle various input data types
  id: totrans-577
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for customized objective and evaluation functions
  id: totrans-578
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic handling of missing values
  id: totrans-579
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy support for GPU training
  id: totrans-580
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Accommodation of monotonicity and feature interaction constraints
  id: totrans-581
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization of multiple cores and cache on standalone computers
  id: totrans-582
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LightGBM is a highly efficient gradient boosting decision tree algorithm, introduced
    in a 2017 paper by Guolin Ke and his team at Microsoft. The algorithm was designed
    to be faster and use less memory than traditional gradient boosting decision trees,
    as demonstrated in experiments on multiple public datasets. The LightGBM algorithm
    achieves this thanks to its leaf-wise splitting policy and EFB.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
