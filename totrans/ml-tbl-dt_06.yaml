- en: 5 Decision trees and gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees and their ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn’s gradient boosting decision trees options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost algorithm and its innovations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How LightGBM algorithm works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have explored machine learning algorithms based on linear models
    because they can handle tabular problems from datasets consisting of a few rows
    and columns and find a way to scale to problems of millions of rows and many columns.
    In addition, linear models are fast to train and get predictions from. Moreover,
    they are relatively easy to understand, explain, and tweak. Linear models are
    also helpful because they present many concepts we will keep building on in the
    book, such as L1 and L2 regularization and gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will discuss a different classical machine learning algorithm:
    decision trees. Decision trees are the foundations of ensemble models such as
    random forests and boosting. We will especially focus on a machine learning ensemble
    algorithm, gradient boosting, and its implementations eXtreme Gradient Boosting
    (XGBoost) and Light Gradient Boosted Machines (LightGBM), which are considered
    state-of-the-art solutions for tabular data.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Introduction to tree-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tree-based models are a family of ensemble algorithms of different kinds and
    the favored methods for handling tabular data because of their performance and
    low requirements in data preprocessing. Ensemble algorithms are sets of machine
    learning models that contribute together toward a single prediction. All tree-based
    ensemble models are based on decision trees, a popular algorithm dating to the
    1960s. The basic idea behind decision trees, no matter whether they are used for
    classification or regression, is that you can split your training set to have
    subsets where your prediction is more favorable because there is a predominant
    output class (in a classification problem) or there is a decreased variability
    of the target values (i.e., they are all very near; this refers to a regression
    problem instead).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 shows a scheme of the key elements that make up a decision tree.
    The problem the decision tree is trying to solve is classifying an animal based
    on the number of legs and eyes. You start from the root of the tree, which corresponds
    to the entire dataset you have available, and set a condition for split. The condition
    is usually true/false—a so-called binary split. Still, some variants of decision
    trees allow for multiple conditions applied at the same node, resulting in multiple
    splits, each decided based on a different value or label from the feature. Each
    branch leads to another node, where a new condition may be applied, or to a terminal
    node, which is used for predictions based on the instances that terminate there.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 The key elements, such as roots, branches, and leaves, constituting
    a decision tree classifying animals based on the number of legs and eyes
  prefs: []
  type: TYPE_NORMAL
- en: 'A split happens based on a deliberate search of the algorithm among features
    and, inside the feature, its observed values. As a splitting criterion in a classification
    problem, the decision trees algorithm searches for the best feature and feature
    value combination that splits the data into subsets with a homogeneous target.
    The homogeneity of a target in a subset is typically measured in a classification
    problem using criteria such as entropy, information gain, or Gini impurity:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Entropy* measures the degree of disorder or randomness in the distribution
    of labels in the subset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Information gain***,** derived from entropy, measures the reduction in uncertainty
    about the class labels of the data, which is achieved by splitting the data based
    on a particular feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gini impurity* measures the probability of misclassifying a randomly chosen
    element in the subset if labeled randomly according to the distribution of labels
    in the subset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the decision tree is being used for regression, it resorts to different splitting
    criteria than classification. In regression, the goal is to split the data into
    subsets to minimize the resulting mean squared error, the mean absolute error,
    or simply the variance of the target variable within each subset. In the training
    process, there’s an automatic selection of the best features, and most of the
    computations for a decision tree are to determine the best feature splits. However,
    once the tree is constructed, predicting the class label or target value for new
    data is relatively fast and straightforward, involving traversing the tree starting
    from the root and ending at the leaf based on the values of a limited set of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees are simple to compute and are also relatively easy to visualize.
    They don’t require scaling or modeling nonlinearities or otherwise transforming
    your features or output target because they can approximate any nonlinear relationship
    between target and predictors since they can consider separately single parts
    of their distribution. Basically, they are cutting a curve into parts so that
    each part looks like a line. On the other hand, decision trees are prone to overfitting
    and end up with an excessive number of splits that can fit the training data you
    are working on. Over time, different strategies have been devised to avoid overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: Limiting the number of splits in the tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning the splitting nodes backward after they have been built to reduce their
    overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 5.2 shows a different perspective on a decision tree. Figure 5.1 visualizes
    a tree as a graph based on two features, while figure 5.2 visualizes a decision
    tree in terms of partitions of the data itself. Each split of the tree is a line
    in the chart, and there are seven vertical lines (thus a result of binary conditions
    on the feature on the x-axis) and three horizontal ones (thus on the feature on
    the y-axis) for a total of 10 splits. You can consider the decision tree a success
    because each class is well separated into its partitions (each partition is a
    terminal node in the end). However, by observation, it also becomes evident that
    specific partitions have been carved out just to fit examples in a certain position
    in the space. There are a few partitions containing only one single case. Any
    new example risks being incorrectly classified if it doesn’t perfectly fit the
    training distribution (an overfitting situation).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F02_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 How a fully grown decision tree branching can also be interpreted
    as a series of dataset splits
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 shows the same problem visualized with fewer splits—two for every
    feature. You can achieve this by pruning the previous tree splits backward, removing
    the ones enclosing too few training examples, or you accomplish that simply by
    limiting in the first place the tree’s growth—for instance, by imposing a maximum
    number of splits to be created. If you use fewer partitions, the tree may not
    fit the training data as perfectly as before. However, a simpler approach provides
    more confidence that new instances will likely be classified correctly, as the
    solution depends explicitly on single points in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F03_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 The problem handled by a simpler decision tree, obtained by pruning
    or by limiting its growth
  prefs: []
  type: TYPE_NORMAL
- en: Thinking of this algorithm in terms of underfitting and overfitting, it is a
    high-variance algorithm because its complexity always tends to exceed what should
    be given the problem and the data. It is not easy to find its sweet spot by tuning.
    In truth, the best way to use decision trees to achieve more accurate predictions
    is not as single models but as part of an ensemble of models. In subsequent subsections,
    we will explore ensemble methods such as bagging, random forests, and gradient
    boosting, an advanced method based on decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll return to the Airbnb NYC dataset to illustrate the core
    gradient-boosted decision tree implementations and how the technique works. The
    code in the following listing reprises the data and some key functions and classes
    we previously used to illustrate other classical machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Reprising the Airbnb NYC dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① List of features to be excluded from data processing
  prefs: []
  type: TYPE_NORMAL
- en: ② List of low-cardinality categorical features to be one-hot encoded
  prefs: []
  type: TYPE_NORMAL
- en: ③ List of high-cardinality categorical features to be ordinally encoded
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates a binary target indicating whether the price is above the mean (unbalanced
    binary target)
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates a binary target indicating whether the price is above the median (balanced
    binary target)
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Creates a multiclass target by quantile binning the price into five classes
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Sets the target for regression as the price column
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Creates a column transformer that applies different transformations to different
    groups of features
  prefs: []
  type: TYPE_NORMAL
- en: 'The code reads a CSV file containing data related to Airbnb listings in New
    York City in 2019 using the pandas library. It then defines several lists that
    categorize the features of the data into different types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`excluding_list`—A list of features that should be excluded from the analysis,
    such as unique identifiers and text features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_card_categorical`—A subset of categorical features that have a low cardinality
    (few unique values) and will be one-hot encoded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`high_card_categorical`—A subset of categorical features that have a high cardinality
    (many unique values) and will be encoded using an ordinal encoding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`continuous`—A list of continuous numerical features that will be standardized
    for analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code then creates several target variables based on the Price feature of
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`target_mean`—A binary variable indicating whether the price is higher than
    the mean price of all listings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_median`—A binary variable indicating whether the price is higher than
    the median price of all listings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_multiclass`—A variable with five classes based on the quantiles of
    the price distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_regression`—The actual price values, which will be used for regression
    analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these targets allow us to deal with different regression and classification
    problems and thus test machine learning algorithms. In this chapter, we will always
    use `target_median`, but you can experiment with all the other targets by making
    small changes in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the code sets up several transformers to preprocess the data for the
    analysis in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`categorical_onehot_encoding`—A one-hot encoding transformer for the low-cardinality
    categorical features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`categorical_ord_encoding`—An ordinal encoding transformer for the high-cardinality
    categorical features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numeric_passthrough`—A transformer that simply passes through the continuous
    numerical features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the code sets up a `ColumnTransformer` object that will apply the appropriate
    transformers to each subset of features based on their type. It applies one-hot
    encoding to the low-cardinality categorical features and passes through the continuous
    numerical features. The transformer is set to drop any features not explicitly
    included in the transformation steps and output concise feature names. The `sparse_threshold`
    parameter is set to zero to ensure that the transformer always returns dense arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 shows how a standard decision trees model is applied to our example
    problem. As in the examples seen in the previous chapter, we import the necessary
    modules from the Scikit-learn library, define a custom scoring metric based on
    accuracy, and set up a five-fold cross-validation strategy. Then we define a ColumnTransformer
    named `column_transform`, which orchestrates data preprocessing. It involves
  prefs: []
  type: TYPE_NORMAL
- en: Transforming categorical variables using a function `categorical_onehot_encoding`
    for specific low-cardinality categorical columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passing through numeric features with a function `numeric_passthrough` for continuous
    variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropping any remaining unprocessed columns (`remainder='drop'`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting some options like suppressing verbose feature names and not applying
    sparse matrix representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, a pipeline combining the ColumnTransformer with a decision tree
    classifier model is tested using cross-validation, which returns accuracy scores
    along with the average fit time and score time.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood of the cross-validation procedure and the data pipelining, the
    dataset is separated multiple times by the decision tree classifier during the
    training based on a splitting value from a feature. The procedure can be algorithmically
    explained as “greedy” because the decision tree picks the feature with the best
    split at each step without questioning whether alternatives could lead to a better
    result. Despite such a simple approach, decision trees are effective machine learning
    algorithms. The process goes on until there are no more splits that improve the
    training, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 A decision tree classifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a column transformer that applies different transformations to categorical
    and numeric features
  prefs: []
  type: TYPE_NORMAL
- en: ② An instance of a decision tree classifier
  prefs: []
  type: TYPE_NORMAL
- en: ③ A pipeline that sequentially applies column transformation and the decision
    tree model
  prefs: []
  type: TYPE_NORMAL
- en: ④ A five-fold cross-validation using the defined pipeline, calculating accuracy
    scores, and returning additional information
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean and standard deviation of the accuracy scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: The result we obtain in terms of accuracy is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The result could be better after comparing the results of our previous experiments
    with other machine learning algorithms. We can determine this because the decision
    tree has overfitted, and it ended up building too many ramifications. We can get
    better performance by limiting its growth by trial and error (you have to state
    the `max_depth` parameter to do so). However, there are even better ways to obtain
    improved results from this algorithm. In the next subsection, we will examine
    the first of such methods, which is based on multiple decision trees based on
    variations of the examples and the employed features.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Bagging and sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have examined all the single learning algorithms with decision trees. Ensembling
    algorithms of the same type is the next step that can help you achieve more predictive
    power on your problem. The idea is intuitive: if a single algorithm can perform
    at a certain level, using the insights of multiple models or chaining them together
    (so that one can learn from the results and errors of the other) should render
    even better results. There are two core ensemble strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Averaging*—Predictions are obtained by averaging the predictions of multiple
    models. Differences in how the models are built, for instance by pasting, bagging,
    random subspaces, and random patches as we will see in this section, lead to different
    results. The best example of ensemble models of this kind is the random forests
    algorithm, which is built on an approach similar to random patches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Boosting*—Predictions are built as a weighted average of chained models, which
    are models sequentially built on the results of previous ones. The best example
    of a boosting algorithm is gradient boosting machines such as XGBoost and LightGBM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following subsection, we will look at random forests. Before delving
    into the random forests algorithm, it is necessary to spend some time on the other
    averaging approaches, not only because the random patches approach is built upon
    them but also because they point out solutions that are always worth applying
    to tabular data when you need to reduce the variance of the estimates, hence obtaining
    more reliable predictions, of any machine learning model you may want to use on
    your data.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pasting* is the first approach to consider. Leo Breiman, the creator of the
    random forests algorithm, suggests that pasting consists of creating a set of
    different models trained on subsamples, obtained by sampling without replacement,
    of your training data. The models’ predictions are pooled together by averaging
    in the case of a regression problem or by majority voting in the case of a classification
    task.'
  prefs: []
  type: TYPE_NORMAL
- en: The pros of pasting are
  prefs: []
  type: TYPE_NORMAL
- en: Improvement of the results by reducing the variance of the predictions by only
    partially increasing their bias, which is a measure of how far the predictions
    of a model are from the true values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictions that are more robust and less affected by outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction of the amount of data to learn at training time, thus reducing memory
    requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cons are
  prefs: []
  type: TYPE_NORMAL
- en: Reduction of the amount of data available, which increases the bias because
    there is the chance of excluding important parts of the data distribution by sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very computationally intensive with complex algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last con depends on your time constraints or available resources. Historically,
    averaging methods have been suggested to be applied using weak models (i.e., machine
    learning models that are very fast to be trained because of their simplicity,
    such as a linear regression or a k-nearest neighbors model). Practitioners observed
    that combining multiple weak models could beat the results of a single, more complex
    algorithm. However, weak models usually have a high bias problem, and by subsampling,
    you induce only some variance in their estimates, but their bias problem remains
    mostly untouched. Using an averaging approach has the main advantage in that it
    reduces the variance of the estimates by trading it with a bit more bias. Since
    weak models inherently carry a substantial bias, they might not achieve comparable
    results to the same approach applied to more complex models. In situations where
    more significant improvements are needed by reducing the variance of the estimates,
    employing an averaging strategy can be more effective with more complex models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Bagging*, also suggested by Leo Breimar as a better solution, differs from
    pasting because you switch from subsampling to bootstrapping. Bootstrapping consists
    of sampling with replacement multiple times from a data sample to approximate
    the population distribution of a statistic. Bootstrapping is a frequently employed
    statistical technique that allows us to estimate the variability and uncertainty
    of statistics relative to the underlying data population from which our sample
    has been drawn. By using the information from the available sample through multiple
    resamples that mimic the original population’s behavior, bootstrapping emulates
    the population’s behavior without requiring explicit knowledge about its statistical
    distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason for using bootstrapping in machine learning is to estimate the uncertainty
    of a model’s performance or to assess the distribution of a statistic. In addition,
    bootstrapping helps create more diverse variations of the original dataset for
    training and ensembling purposes. This is based on the observation that averaging
    multiple models reduces variance more if the predictions of the models being used
    are less correlated (i.e., more diverse). Subsampling creates diverse datasets
    to train. However, it has limitations because if you subsample aggressively—for
    instance, picking up less than 50% of the original data—you tend to introduce
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if you subsample in a more limited way, such as using 90% of the
    data, the resulting subsamples will tend to be correlated. Instead, bootstrapping
    is more efficient because, on average, you use about 63.2% of the original data
    at each bootstrap. For a detailed statistical explanation of such calculated proportions,
    see the detailed cross-validated answer at [https://mng.bz/zZ0w](https://mng.bz/zZ0w).
    Moreover, sampling with replacement tends to give results that mimic the original
    distribution of the data. Bootstrapping creates a set of more different datasets
    to learn from and, consequently, a set of more different predictions that can
    ensemble, reducing variance.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, since in averaging we are building a distribution of predictions and
    getting the center of the distribution as our prediction, the more the averaged
    predictions resemble a random distribution, the less the center of the distribution
    will be biased by problems in the data picked up by the model (such as overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, with *random subspaces*, introduced by T. Ho [“The Random Subspace
    Method for Constructing Decision Forests,” *Pattern Analysis and Machine Intelligence*,
    20(8), 832-844, 1998], the sampling is limited only to features. This works because
    the model used for the ensemble is the decision tree, a model whose high variance
    of the estimates is greatly reduced in the ensemble by using only a part of the
    features for every model that is a part of it. The improved result is because
    the models trained on a subsample of the features tend to produce uncorrelated
    predictions—all the decision trees overfit the data but in a different way with
    respect to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, with *random patches* [G. Louppe and P. Geurts, “Ensembles on Random
    Patches,” in *Machine Learning and Knowledge Discovery in Databases* (2012): 346–361],
    sampling of both samples and features are used together to achieve even more uncorrelated
    predictions that can be averaged even more profitably.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pasting, bagging, random subspaces, and random patches can all be implemented
    using Scikit-learn functions for bagging. The behavior of `BaggingClassifier`
    for classification tasks and `BaggingRegressor` for regression tasks can be controlled
    in regards to training data thanks to the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bootstrap`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_sample`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining them according to each averaging method’s specifications, you can
    obtain all four of the averaging strategies we have described (see table 5.1).
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 Bagging and sampling strategies
  prefs: []
  type: TYPE_NORMAL
- en: '| Averaging strategy | What happens to data | Parameters for BaggingClassifier/BaggingRegressor
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pasting | Training examples are sampled without replacement | `bootstrap
    = False` `max_samples < 1.0` `max_features = 1.0`  |'
  prefs: []
  type: TYPE_TB
- en: '| Bagging | Training examples are sampled with replacement (bootstrapping)
    | `bootstrap = True` `max_samples = 1.0` `max_features = 1.0`  |'
  prefs: []
  type: TYPE_TB
- en: '| Random subspaces | Features are sampled (without replacement) | `bootstrap
    = False` `max_samples = 1.0` `max_features < 1.0`  |'
  prefs: []
  type: TYPE_TB
- en: '| Random patches | Training examples and features are sampled without replacement
    | `bootstrap = False` `max_samples < 1.0` `max_features < 1.0`  |'
  prefs: []
  type: TYPE_TB
- en: By inputting the desired Scikit-learn model class in the parameter estimator,
    you can instead decide what algorithm to use for building the ensemble. A decision
    tree is the default, but you can decide what weak or strong models you prefer.
    In the following example, we apply a bagged classifier, setting the number of
    decision tree models to 300\. The following listing shows all the models contributing
    together to improve the low performances that, as we have seen from listing 5.2,
    a decision tree tends to produce in this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 Bagged tree-based classifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a BaggingClassifier ensemble model based on decision trees
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets bootstrap sampling for the BaggingClassifier
  prefs: []
  type: TYPE_NORMAL
- en: ③ Sets no sampling of features for the BaggingClassifier
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sets no sampling of data for the BaggingClassifier
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ A column transformer that applies different transformations to categorical
    and numeric features
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ A pipeline that sequentially applies column transformation and the bagging
    classifier model
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Five-fold cross-validation using the defined pipeline and calculating accuracy
    scores
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Prints the mean and standard deviation of the accuracy scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'The results take a little while more and look promising, but they are still
    not enough to compete with our previous solutions based on support vector machines
    and logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the next subsection, we take the next step in ensembling by revisiting random
    forests, which make use of the random patches in bagging for a good reason.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Predicting with random forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Random forests work similarly to bagging. Still, it also simultaneously applies
    random patches (training examples and features are sampled without replacement):
    it bootstraps the samples before each model is trained and subsamples the features
    during modeling. Since the basic algorithm used in a random forests ensemble is
    a decision tree built by binary splits, the feature sampling happens at every
    tree split when a set of features is sampled as potential candidates for the split
    itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Allowing each decision tree in the ensemble to grow to its extremities may lead
    to overfitting the data and high variance in the estimates; employing bootstrapping
    and feature sampling might mitigate these problems. Bootstrapping ensures that
    the models are trained on slightly different data samples from the same distribution,
    while feature sampling at each split guarantees diverse tree structures. This
    combination helps generate a set of models that are quite distinct from one another.
    Different models produce very different predictions (hence, we can say that their
    predictions are quite uncorrelated), and that’s a great advantage for the averaging
    technique because, when ensembling to a single prediction vector, the result is
    more reliable and accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 shows how random forests work. The figure illustrates a binary classification
    problem with a dataset of two classes. The dataset is modeled using multiple decision
    trees, employing bootstrapping and feature sampling techniques. These techniques
    result in different partitions of the dataset, represented in the top-most part
    of the figure by three example results. The trees partition the dataset space
    in diverse ways, showcasing the variability in their splitting strategies. To
    simplify the representation, only two features are shown, providing a clearer
    understanding of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when all the results are put together by majority voting, where you
    pick the more frequent classification as a predicted class, the random forests
    will provide better predictions derived from the results of all the trees. This
    is shown in the bottom part of the figure, where different shades indicate the
    prevalence of one or another class in a specific partition. The definitive boundary
    between classes is shown as a black polygonal line in the majority voting. The
    line can be even smoother when using multiple trees, resembling a curve. Ensemble
    methods can approximately resemble any curve if given enough models to the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F04_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 How random forests arrives at its results by combining the different
    data partitioning due to its decision trees thanks to majority voting
  prefs: []
  type: TYPE_NORMAL
- en: Originally devised by Leo Breiman and Adele Cutler ([https://mng.bz/0Qlp](https://mng.bz/0Qlp)),
    though commercially protected, the algorithm has been open-sourced—hence the many
    different names of its implementations. Random forests open up even more interesting
    possibilities, apart from better predictions, because you can use the algorithm
    to determine feature importance and measure the degree of similarity of the cases
    in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the example in listing 5.4, we test how random forests would work on our
    classification problem with the Airbnb NYC dataset. Apart from the algorithm,
    there are no differences from our standard data processing when applying decision
    trees. One-hot encoding turns low-categorical features into binaries, and numeric
    features are left as they are.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 Random forests classifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① A RandomForestClassifier with 300 estimators and a minimum number of samples
    at a leaf node set to 3
  prefs: []
  type: TYPE_NORMAL
- en: ② A column transformer that applies different transformations to categorical
    and numeric features
  prefs: []
  type: TYPE_NORMAL
- en: ③ A pipeline that sequentially applies column transformation and the random
    forests classifier model
  prefs: []
  type: TYPE_NORMAL
- en: ④ A five-fold cross-validation using the defined pipeline and calculating accuracy
    scores
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean and standard deviation of the accuracy scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the script, you will obtain the following results, which are
    actually the best performance you will find in this chapter for this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The secret to obtaining a good result with random forests is to choose its few
    hyperparameters wisely. Though the random forests algorithm is a no-brainer since
    it works fine with default parameters, fine-tuning it will bring better results.
    First, the purpose of the algorithm is to reduce the variance of the estimates,
    and that’s done by setting a high enough number of `n_estimators`. The principle
    is that if you have many trees, you have a distribution of results, and if the
    results are randomly drawn, you have an effect similar to the regression to the
    mean (the best prediction) due to the law of large numbers. The bootstrapping
    of examples and sampling of the features to be considered for splitting is usually
    enough to make the resulting trees of the forests different enough to be considered
    “random draws.” However, you need enough draws to have a proper regression to
    the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need some tests to fine-tune how many trees you build since there’s always
    a sweet spot: after a certain number of trees, you won’t obtain any more improvements,
    and sometimes some decrements in performance will result instead. Also, setting
    a too-high number of trees to be built by the algorithm will increase its computational
    costs, and more time will be needed in training and inference. However, no matter
    how many trees you train for, if your variance starts high using the default settings
    of a random forests model, you can do little to reduce it. Here the tradeoff between
    variance and bias comes into play; that is, you can trade some variance, implying
    you are overfitting the data, for some higher bias.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can set a proper bias for random forests by making the following adjustments:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting a lower number of features to consider when looking for the best split
    by setting the `max_features` parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting a max number of splits per tree, which will limit its growth to a certain
    predefined extent, by setting the `max_depth` parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting a minimum number of examples in the terminal leaves of the tree, which
    will limit its growth, by setting the `min_samples_leaf` parameter with a number
    higher than 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we explore extremely randomized trees (ERT), a variant
    of random forests that can be quite handy when data is larger and noisy.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Resorting to extremely randomized trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ERT (also known as extra-trees in Scikit-learn) is a more randomized kind of
    random forests algorithm. The reason is the choice of candidates for splits in
    the ensemble’s single trees. In random forests, the algorithm samples its candidates
    for each split and then decides on the best feature to use among the candidates.
    Instead, the feature to be split in ERT is not evaluated among the possible candidates
    but randomly chosen. Afterward, the algorithm evaluates the best split point in
    the randomly chosen feature. This has some consequences. First, since the resulting
    trees are even more uncorrelated, there is even less variance in predictions from
    ERT—but at the price of a higher bias. Randomly split features have an effect
    on the accuracy of the predictions. Second, ERT is more computationally efficient
    because it doesn’t test sets of features but a single feature each time for the
    best split. All these characteristics make ERT best suited for handling
  prefs: []
  type: TYPE_NORMAL
- en: '*High-dimensional data* because it will split features faster than any other
    decision-tree ensemble algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Noisy data* because the random feature and sample selection process can help
    reduce the influence of noisy data points, making the model more robust to extreme
    values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Imbalanced data* because, due to the random feature selection, the signals
    from the minority subset of the data won’t be systematically excluded in favor
    of the majority subset of the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following listing tests ERT by replacing the random forests in listing 5.4,
    where you built a model with the Airbnb NYC dataset to figure out if the price
    of a listing is above or below the median value.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 ERTs classifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① An ExtraTreesClassifier with 300 estimators and a minimum number of samples
    at a leaf node set to 3
  prefs: []
  type: TYPE_NORMAL
- en: ② A column transformer that applies different transformations to categorical
    and numeric features
  prefs: []
  type: TYPE_NORMAL
- en: ③ A pipeline that sequentially applies column transformation and the random
    forests classifier model
  prefs: []
  type: TYPE_NORMAL
- en: ④ A five-fold cross-validation using the defined pipeline and calculating accuracy
    scores
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean and standard deviation of the accuracy scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'You obtain a bit better result than using random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you run this example, you will see that training by ETR is much faster than
    by random forests, given that you use the same dataset and build the same number
    of trees. ETR becomes an interesting alternative when your dataset is larger (more
    cases) and even more when it is wider (more features) because it saves a lot of
    time picking the feature to split since it is randomly decided. By contrast, the
    random forests algorithm has to look for the best feature among a selection.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that splits are decided randomly is a great advantage when many collinear
    and noisy features are related to the target. The algorithm avoids picking the
    same signals as an algorithm driven by searching for the feature that best fits
    the target. In addition, you can see the working dynamics in feature splitting
    of an ETR as another way to trade variance with bias. Splitting randomly is a
    limitation for the algorithm and reduces the variance because the resulting set
    of trees will be very uncorrelated.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we complete our overview of tree-based ensembles by examining
    gradient boosting. This slightly different ensembling approach is often more effective
    for tabular data problems than bagging or random patches.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Gradient boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, *gradient boosting decision trees* (GBDT) has firmly established
    itself as a cutting-edge method for tabular data problems. GBDT is generally considered
    a state-of-the-art machine learning method across a wide range of problems across
    multiple domains, including multiclass classification, advertising click prediction,
    and search engine ranking. When applied to standard tabular problems, you can
    expect GBDT to perform better than neural networks, support vector machines, random
    forests, and bagging ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Above all, GBDT’s ability to handle heterogeneous features and its flexibility
    in the choice of the loss function and evaluation metrics make the algorithm most
    suitable for tabular data predictive modeling tasks. In sum, GBDT offers the following
    benefits for tabular data problems:'
  prefs: []
  type: TYPE_NORMAL
- en: With the proper hyperparameter tuning, it can achieve the best performance among
    all other techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no need for scaling or other monotone transformations of the features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It automatically captures nonlinear relationships in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is robust to outliers and noisy data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It automatically handles missing data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It automatically selects the best features and can report their importance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these characteristics depend on how the algorithm works, combining sequences
    of decision trees in a gradient descent optimization. In fact, in gradient boosting,
    starting from a constant value, you add tree models to an ensemble sequentially,
    each one correcting the errors of the previous models in a fashion similar to
    gradient descent optimization. Gradient boosting represents an evolution of the
    original boosting approach. Used in models such as Adaboost, in the original boosting,
    you just average models built on the residuals of the previous model.
  prefs: []
  type: TYPE_NORMAL
- en: In Adaboost, the algorithm fits a sequence of weak learners, any machine learning
    algorithm that consistently beats random guessing, to the data (for an explanation
    about how to choose weak learners, see [https://mng.bz/KG9P](https://mng.bz/KG9P)).
    It then attributes more weight to incorrect predictions and less to correct ones.
    Weighting helps the algorithm to focus more on the observations that are harder
    to predict. The process is concluded after multiple corrections by a majority
    vote in classification or an average of the predictions in regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast, in gradient boosting, you rely on a double optimization: first
    that of the single trees that strive to reduce the error based on their optimization
    function and then the general one, involving calculating the error from the summation
    of the boosted model, in a form that mimics gradient descent, where you gradually
    correct the model’s predictions. Since you have an optimization also based on
    a second level, the optimization based on the error of the entire ensemble procedure,
    gradient boosting is more versatile than the previously seen tree ensembles because
    it allows arbitrary loss functions to be used when calculating how much the summation
    of the predictions of models diverges from the expected results.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 visually represents how the training error decreases after adding
    a new tree. Each tree takes part in a gradient descent style optimization, contributing
    to predicting the correction of the residual errors from the previous trees.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F05_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 How gradient descent works with boosted trees
  prefs: []
  type: TYPE_NORMAL
- en: If gradient descent provides optimal results and flexibility in the optimization,
    having decision trees as base learning (ensembles, as seen, are not limited to
    decision trees) offers various advantages. This is because it automatically selects
    the features it needs. It doesn’t need to specify a functional form (a formula
    as in regression), scaling, or linear relationships between features and the target.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, before seeing specific implementations in action (Scikit-learn,
    XGBoost, LightGBM), we will try building our simple implementation of gradient
    boosting to understand how to use this powerful algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 How gradient boosting works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All implementations of GBDT offer a variety of hyperparameters that need to
    be set to get the best results on the data problem you are trying to solve. Figuring
    out what each setting does is a challenge, and being agnostic and leaving the
    task to an automatic tuning procedure doesn’t help too much since you will have
    challenges telling the tuning algorithm what to tune and how to tune.
  prefs: []
  type: TYPE_NORMAL
- en: In our experience, writing down a simple implementation is the best way to understand
    how an algorithm works and to figure out how hyperparameters relate to predictive
    performances and results. Listing 5.6 shows a `GradientBoosting` class capable
    of addressing any binary classification problem, such as the Airbnb NYC dataset
    we are tackling as an example, using two parameters for the gradient descent procedure
    and the parameters from the decision tree model offered by Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code creates a GradientBoosting class that comprises methods for fitting,
    predicting probabilities, and predicting class. Internally, it stores the sequence
    of fitted decision trees in a list, from where they can be accessed sequentially
    to reconstruct the following summation formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F05_Ryan2-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: In the formula,
  prefs: []
  type: TYPE_NORMAL
- en: H(X) is the gradient boosting model applied to predictors X
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: M corresponds to the number of tree estimators used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ν represents the learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: w^m instead represents the corrections from previous trees that have to be predicted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The h^m symbol refers to the mth decision tree used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interestingly, gradient boosting trees are always regression trees (even for
    classification problems)—hence our choice of using Scikit-learn’s DecisionTreeRegressor.
    This also explains why GBDT is better at predicting probabilities than other tree-based
    ensemble models: gradient boosting trees regress directly on the logit of class
    probability, thus optimizing in a fashion not too different from logistic regression.
    On the other hand, algorithms such as random forests are optimized for purity
    metrics, and they estimate probabilities by counting the fraction of a class in
    a terminal node, which is not truly a probability estimate. Generally, probabilities
    outputted by GBDTs are correct, and they rarely require subsequent probabilistic
    calibration, which is a post-processing step used to adjust predicted probabilities
    to improve their accuracy and reliability in applications where probability estimates
    are paramount, such as medical diagnosis (e.g., disease detection), fraud detection,
    or credit risk assessment.'
  prefs: []
  type: TYPE_NORMAL
- en: In our code implementation, we allow passing any parameter for the DecisionTreeRegressor
    (see [https://mng.bz/9YQx](https://mng.bz/9YQx)), though the most useful are the
    ones related to the complexity of the tree development, such as `max_depth`, fixing
    the maximum depth of the tree, or `min_samples_split` and `min_samples_leaf`,
    the minimum number of samples needed to split an internal node or to be at a leaf
    node, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The role of each tree regressor is to provide a w vector containing the learned
    corrections to be summed together with the previous estimates after being weighted
    by the learning rate. Each w vector depends on the previous one because it is
    produced by a tree regressor trained on the gradients necessary to correct the
    estimates to match the true classification labels. The chained vectors w resemble
    a sequence of gradient corrections—at first large, then finer and finer, converging
    toward an optimal output prediction. Such gradient descent is perfectly similar
    to the gradient descent optimization procedure we introduced in chapter 4\. In
    addition, by changing the cost function on which you base your gradients’ computation,
    you can ask the GBDT to optimize for different loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 Building a gradient boosting classifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① Sigmoid function implementation used for probability transformation that converts
    logits back into probabilities
  prefs: []
  type: TYPE_NORMAL
- en: ② Logit function implementation used to transform probabilities into logits
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates the gradient of the loss function (negative log-likelihood) with
    respect to the predictions
  prefs: []
  type: TYPE_NORMAL
- en: ④ Initializes the model with the logit-transformed mean of the target values
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Fits a decision tree regressor to the negative gradient of the log-odds transformed
    target
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Updates the predicted values using the output of the fitted tree with a learning
    rate factor
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Predicting back requires cumulating predictions from all the trees.
  prefs: []
  type: TYPE_NORMAL
- en: As in the gradient descent we have seen applied to linear models, you rely on
    making the process stochastic to avoid the optimization being struck on a suboptimal
    solution, which is achieved by sampling rows or columns before the training of
    each decision tree. In addition, you use early stopping to prevent the GBDT from
    using too many decision trees in sequence and adapting too much to the training
    data. We will demonstrate early stopping in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explained the inner workings of our GradientBoosting class,
    we can now experiment with it. We’ll use the Airbnb NYC dataset and begin by dividing
    it into a training set and a testing set. This will involve creating two lists
    of row indexes—one for the training set and one for the testing set—employing
    the Scikit-learn function `train_test_split` ([https://mng.bz/jp1z](https://mng.bz/jp1z)).
    We instantiate our GradientBoosting class, which requires a learning rate of 0.1
    and 300 decision trees, with a maximum depth of four branches and terminal leaves
    with at least three examples. After transforming the training data, by treating
    numeric and categorical features, we fit the model, predict the test set, and
    evaluate the results.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7 Testing our gradient boosting class
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ① Splits the dataset indices into training and test sets using a fixed random
    seed
  prefs: []
  type: TYPE_NORMAL
- en: ② Initializes a GradientBoosting model with specified hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ③ Applies the column transformations to the training data
  prefs: []
  type: TYPE_NORMAL
- en: ④ Extracts the target values corresponding to the training data
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Applies the same column transformations to the test data
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Extracts the target values corresponding to the test data
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Calculates the accuracy score by comparing the predicted labels with the actual
    test labels
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Prints the calculated accuracy score
  prefs: []
  type: TYPE_NORMAL
- en: The evaluated accuracy on our test set is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is a very good result, pointing out that even our basic implementation
    is able to do a very good job on the data we are working with. In the next section,
    we will investigate the results obtained and observe a key characteristic of GBDT
    models that distinguishes them from other decision tree ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Extrapolating with gradient boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our implementation from scratch of a GBDT, we can visualize how the model
    fits the data by predicting the same training set. The visualization shown in
    figure 5.6, created by the small code snippet in listing 5.8, is a normalized
    density histogram. In a normalized density histogram, the height of each bar represents
    the relative frequency of data points falling within a specific bin, and the total
    area under the histogram becomes equal to 1\. The result depicts a distribution
    of values predominantly polarized to the extremities of 0-1 boundaries, showing
    a model quite decisive in classifying the examples.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8 Plotting gradient boosting predicted probabilities
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Generates predicted probabilities for the test data using the trained model
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a histogram of the predicted probabilities with specified bins and
    normalized density
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F06_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 A histogram describing the fitted probabilities for a gradient boosting
    classification, showing how the model has strongly decided for most cases if they
    are positive or negative
  prefs: []
  type: TYPE_NORMAL
- en: Our implementation, under the hood, uses a regression loss, the squared loss,
    whose gradient is equal to the residual of the probabilities transformed into
    logits. For a definition of logits, see [https://mng.bz/W214](https://mng.bz/W214).
  prefs: []
  type: TYPE_NORMAL
- en: The logits for a probability p are calculated as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F06_Ryan2-eqs-1x.png)'
  prefs: []
  type: TYPE_IMG
- en: The advantage of this definition is that the logit function maps the probabilities
    to a log odds scale, which is an unbounded scale that ranges from negative infinity
    to positive infinity, allowing us to treat our problem as a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: This means that at each iteration, the gradient boosting algorithm fits a regression
    model to the gradient of the loss function with respect to the logit values, which
    corresponds to the difference between the logit of the true target values and
    the current predictions expressed in logits. This approach allows the algorithm
    to iteratively improve the predictions by adjusting them in the direction of the
    steepest descent of the loss function and to finally have a logit prediction that
    is bounded in the range between 0 and 1, thanks to the inverse function of the
    logit, the sigmoid. A sigmoid is a mathematical function that maps its input to
    a value between zero and one, providing a smooth and continuous curve.
  prefs: []
  type: TYPE_NORMAL
- en: The formula for the sigmoid is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F06_Ryan2-eqs-2x.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '*σ*(*x*) represents the sigmoid function applied to the input value x.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: exp(–*x*) is the exponential function, where exp denotes Euler’s number (approximately
    2.71828) raised to the power of –x.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 + exp(–*x*) is the denominator, which ensures that the output of the sigmoid
    function is always positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 / (1 + exp(–*x*)) represents the division of 1 by the denominator, resulting
    in the output value of the sigmoid function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is commonly used in machine learning and statistical models to convert logit
    predictions into probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: What if, instead, we treat the problem as a regression one? In listing 5.9,
    we define a `GradientBoostingRegression` class by building on our `GradientBoosting`
    class and overwriting the fit and predict methods by removing logit and sigmoid
    transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.9 Testing a gradient boosting regression class
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Initializes predictions with mean of y
  prefs: []
  type: TYPE_NORMAL
- en: ② Fits the tree to the negative gradient
  prefs: []
  type: TYPE_NORMAL
- en: ③ Updates predictions with tree’s predictions scaled by learning rate
  prefs: []
  type: TYPE_NORMAL
- en: ④ Predicting back requires cumulating predictions from all the trees.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Plots a histogram of regression predicted probabilities
  prefs: []
  type: TYPE_NORMAL
- en: When the code in listing 5.9 is run, it will produce a histogram of the fit
    predictions, as shown in figure 5.7\. Figure 5.7 shows how fit probabilities exceed
    the 0-1 boundaries. As with linear regression, which is a weighted combination
    based on the features, gradient boosting, a weighted combination based on the
    results of a chained sequence of models, can extrapolate beyond the limits of
    the learned target. Such extrapolations are impossible with other ensembles based
    on decision trees, such as random forests. Decision trees in regression cannot
    predict anything exceeding the values seen in training since predictions are based
    on means of training subsamples. The extrapolative potentiality of GBDT, based
    on the fact that they are an additive ensemble, is the basis for their success
    with time series, where you extrapolate future results that may be very different
    from past ones.
  prefs: []
  type: TYPE_NORMAL
- en: As a caveat, however, always consider that the extrapolative capabilities of
    GBDTs cannot be extended as far as could be achieved using a linear model. In
    time series predictive situations where the value to be predicted is way too far
    from the targets you provided for training, for instance in case of an outlier,
    the extrapolation will be limited, missing a correct estimation. A linear model,
    which directly associates a linear relationship between the input data and the
    predictions, could prove more suitable in such situations. Linear models are capable
    of handling extreme predictions for completely unseen outlying data points. To
    offer an alternative to decision trees as base learners in such situations, many
    GBDT implementations offer linear boosting by simply ensembling linear models
    (such as in the XGBoost implementation) or applying a piecewise linear gradient
    boosting tree, where linear models are built on the terminal nodes of a decisions
    trees (such as in LightGBM implementation).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F07_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 A histogram describing fitted probabilities with a gradient boosting
    regression model where some probabilities exceed the 0-1 boundary
  prefs: []
  type: TYPE_NORMAL
- en: Also, the strength of GBDTs in time series problems relies on their being automatic
    in choosing the information for the predictions, with very few hyperparameters
    to set. All you need to do is to have enough examples, at least in the range of
    thousands of data points, and to have some careful engineering of time series
    features such as lagged values and moving averages at different time horizons.
    For shorter series, classical time series methods such as ARIMA or exponential
    smoothing are still the recommended choice. For complex problems such as hierarchically
    structured series, GBDTs can outperform even the most sophisticated deep learning
    architectures explicitly designed for time series data. For example, GBDTs excel
    in solving problems found in supermarket networks where both slow- and fast-moving
    goods are sold.
  prefs: []
  type: TYPE_NORMAL
- en: 'A clear example of the advantage of GBDTs in time series analysis has been
    demonstrated during the M5 forecasting competition recently held on Kaggle ([https://github.com/Mcompetitions/M5-methods](https://github.com/Mcompetitions/M5-methods)),
    where solutions made by a LightGBM algorithm proved superior to deep learning
    architectures designed for the task of predicting in hierarchical structured series,
    such as DeepAR ([https://arxiv.org/abs/1704.04110](https://arxiv.org/abs/1704.04110))
    or NBEATS ([https://arxiv.org/abs/1905.10437](https://arxiv.org/abs/1905.10437)).
    A lucid and insightful analysis of the competition and the success and ubiquity
    of tree-based methods in the practice of time series analysis can be found in
    the paper “Forecasting with Trees” by Tim Januschowski et al. [*International
    Journal of Forecasting* 38.4 (2022): 1473–1481: [https://mng.bz/8O4Z](https://mng.bz/8O4Z)].'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Explaining gradient boosting effectiveness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Today, despite the remarkable results in image and text recognition and generation,
    neural networks do not match the performance on tabular data of gradient boosting
    solutions such as XGBoost and LightGBM. Both practitioners and participants in
    data science competitions favor these solutions. For instance, see the “State
    of Competitive Machine Learning” regarding tabular competitions at [https://mlcontests.com/tabular-data/](https://mlcontests.com/tabular-data/).
    But where exactly does the advantage that GBDTs have over deep neural networks
    (DNNs) come from? From our experience building a gradient boosting classifier,
    we could appreciate how the algorithm combines gradient descent with the flexibility
    of decision trees with heterogeneous data. Is this enough to explain why GBDTs
    are so effective with tabular data?
  prefs: []
  type: TYPE_NORMAL
- en: '“Why Do Tree-Based Models Still Outperform Deep Learning on Typical Tabular
    Data?” by Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux (Thirty-sixth Conference
    on Neural Information Processing Systems Datasets and Benchmarks Track, 2022:
    [https://hal.science/hal-03723551v2/document](https://hal.science/hal-03723551v2/document))
    is a recent study that tries to shed some light on the different performances
    of deep learning architectures and gradient-boosting decision trees. The study
    shows that tree-based methods outperform deep learning methods (even modern architectures)
    in achieving good predictions on tabular data. The authors explicitly focus on
    the heterogeneity of columns that distinguishes tabular data from datasets with
    exclusively continuous features (we could refer to them as homogenous tabular
    datasets) and define a standard benchmark using 45 open datasets. They considered
    only data from about 10,000 samples, consisting of columns of different types,
    including numerical features with different units and categorical features, because
    that’s considered the typical situation with tabular datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Various deep learning models, including multilayer perceptrons (MLPs), ResNets,
    SAINT, and FTtransformer, were tried, but tree-based methods were found to have
    better performance with less hyperparameter tuning. Even when considering only
    numerical features, tree-based methods outperformed deep learning methods. This
    advantage became more pronounced when considering fitting time, although the hardware
    used (including GPUs) also influenced the results. The gap between the two methods
    was narrower on large datasets, which are not typical for tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: The authors also investigated the features of tabular data that explain the
    performance difference between tree-based and deep learning methods. They found
    that smoothing the outcome in feature space narrowed the gap since deep architectures
    struggle with irregular patterns, whereas smoothness does not affect tree models.
    Removing uninformative features narrowed the gap for MLP-like neural architectures
    more. However, it was only after applying random rotations to the data that deep
    architectures outperformed tree models.
  prefs: []
  type: TYPE_NORMAL
- en: Random rotations refer to applying a random rotation matrix to the input features
    of a dataset before feeding them into a machine learning model. This rotation
    matrix is a square matrix that preserves the length of vectors and the angles
    between them, ensuring that the rotated data remains equivalent to the original
    data. Random rotations are used in machine learning for various purposes, including
    enhancing the diversity of ensembles, improving the robustness of models, and
    addressing rotation invariance in tasks such as computer vision and machine learning
    for quantum chemistry. Such a technique, perfectly reversible, however, tends
    to obscure the relationship between predictors and the target for tree-based algorithms,
    whereas deep learning models are unaffected thanks to their representative learning
    capacity to learn also the applied rotation.
  prefs: []
  type: TYPE_NORMAL
- en: This result doesn’t necessarily indicate an advantage of DNNs but rather a limitation
    of GBDTs. Deep architectures are rotationally invariant, which means they can
    detect rotated signals, as in image recognition, where certain images can be recognized
    regardless of their orientation. In contrast, GBDTs are not rotationally invariant
    and can only detect signals that are always oriented in the same fashion, since
    they operate based on splitting rules. Therefore, using any kind of rotation on
    data, such as a principal component analysis or a singular value decomposition,
    can be detrimental to GBDTs. DNNs, unaffected by rotation, can catch up in these
    situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, this study reinforces our experience with GBDTs and their perceived
    strengths:'
  prefs: []
  type: TYPE_NORMAL
- en: Can perform well even on datasets of moderate size (1,000–5,000 cases) but outshines
    other algorithms from 10,000 to 100,000 samples (based on our experience)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tend to excel with datasets that are heterogeneous in nature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robust against noise and irregularities in the target data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter out noisy or irrelevant features due to their automatic feature selection
    process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the strengths mentioned here, it should also be noted that GBDTs
    are often preferred over DNNs in certain scenarios. One reason is that GBDTs require
    less data preprocessing, making them more efficient and straightforward to implement.
    Moreover, GBDTs are as flexible as DNNs in terms of objective functions. In both
    cases, there are many to choose from, which can be especially useful in domains
    with complex optimization goals. Another benefit of GBDTs is that they offer more
    control over how their decision tree rules are built, providing users with some
    transparency and interpretability. Finally, GBDTs can train faster than DNNs in
    most cases, and they can also predict in a reasonable inference time, depending
    on their complexity, which can be a critical factor in real-time applications
    or time-sensitive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve gained an understanding of the fundamental concepts behind gradient
    boosting and its effectiveness in solving tabular data problems compared to deep
    learning, the next section will explore some of its implementations, starting
    with the one provided by Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Boosting in Scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scikit-learn offers gradient boosting algorithms for both regression and classification
    tasks. These algorithms can be accessed through the GradientBoostingClassifier
    ([https://mng.bz/Ea9o](https://mng.bz/Ea9o)) and GradientBoostingRegressor ([https://mng.bz/N1VN](https://mng.bz/N1VN))
    classes, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Scikit-learn implementation of gradient boosting was one of the earliest
    options available to Python users in data science. This implementation closely
    resembles the original proposal of the algorithm by Jerome Friedman in 1999 [“Greedy
    Function Approximation: A Gradient Boosting Machine,” *Annals of Statistics* (2001):
    1189–1232]. Let’s see the implementation in action in the next code listing, where
    we cross-validate the classifier performance on the Airbnb NY dataset to predict
    if a listing is above or below the median value.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10 Scikit-learn gradient boosting classifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① A GradientBoostingClassifier model with specified hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ② A pipeline that first applies data processing with column_transform and then
    fits the model
  prefs: []
  type: TYPE_NORMAL
- en: ③ A five-fold cross-validation using the defined pipeline, calculating accuracy
    scores, and returning additional information
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints the mean and standard deviation of the accuracy scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same set of parameters previously used, the performances we obtain
    are a bit better than those we obtained in our implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: One notable aspect of this Scikit-learn implementation is that the training
    process can take a considerable amount of time, and the bottleneck can be attributed
    to the decision trees, the only supported model for building the sequential ensemble
    utilized by Scikit-learn itself. In exchange, you have some flexibility in parameters
    and control over how the GBDT uses single decision trees. For instance, Scikit-learn’s
    gradient boosting allows you to
  prefs: []
  type: TYPE_NORMAL
- en: Define the `init` function. We used an average as the first estimator in our
    implementation. Here you can use whatever estimator you want as a starting point.
    Since gradient boosting is based on gradient descent and the gradient descent
    optimization process is sensitive to the starting point, this can prove an advantage
    when solving more complicated data problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revert the algorithm to Adaboost, the original algorithm that inspired sequential
    ensembles, by training a `GradientBoostingClassifier` with the exponential loss
    (`loss="exponential"`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control in detail the complexity of the decision trees used, implying that you
    can model more complex data at risk of overfitting by means of parameters such
    as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split` for the minimum number of samples required to split an
    internal node'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_sample_leaf` for the minimum number of samples needed to be at a leaf
    node'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_weight_fraction_leaf` for the minimum weighted fraction of the total of
    weights of all the input samples required at a leaf node'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth` for the max depth of the tree'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_impurity_decrease` as the threshold in impurity decrease, used to decide
    whether to split or stop the growth of the tree'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_leaf_nodes` as the maximum reachable number of final nodes before stopping
    growing the tree'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If controlling the growth of the decision trees is not enough, once they are
    grown, you can reduce their complexity using the `ccp_alpha` parameter. This parameter
    reduces the trees backward from the final nodes by removing the nodes that do
    not pass a complexity test (see [https://mng.bz/DM9n](https://mng.bz/DM9n) for
    details).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subsample both rows (`subsample`) and columns (`max_features`), a particularly
    effective way to reduce overfitting and increase the training model’s generalizability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the implementation also offers support for sparse data and early
    stopping, a procedure to prevent overfitting in GBDT and neural networks that
    will be discussed in detail in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Also, on the side of the provided outputs, the support offered by this version
    is quite impressive, making it a perfect tool for understanding and explaining
    why your GBDT model came to certain predictions. For instance, you can access
    all the decision trees used and require the raw values predicted from the trees
    of the ensemble, both as a total (`decision_function` method) or as a sequence
    of steps (`staged_decision_function` method).
  prefs: []
  type: TYPE_NORMAL
- en: Recently, this implementation has been less used by practitioners because of
    the faster, more performing solutions offered by XGBoost, LightGBM, and Scikit-learn,
    with its HistGradientBoosting. However, it remains an interesting choice for smaller
    datasets if you want to control certain aspects of the gradient boosting procedure.
    In the next section, we will explore XGBoost and determine how it can be a more
    powerful choice for solving your tabular data problems.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Applying early stopping to avoid overfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The presentation of the original Scikit-learn classes for gradient boosting
    offers a chance to introduce a procedure that can help control overfitting. The
    procedure is early stopping, a method originally used in gradient descent to restrict
    the number of iterations when further adjustments to the coefficients under optimization
    would lead to no enhancements or a poor generalization of the solution. The method
    has also been used to train neural networks. In gradient boosting, which takes
    gradient descent as part of its optimization process, the method can help solve
    the same problem: limiting the number of added decision tree models to reduce
    the computational burden and avoid possible overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Early stopping works in a few simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A fraction of the training dataset is set aside to form a validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During each iteration of the training process, the resulting partial model is
    evaluated using the validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The performance of the partial model on the validation set is recorded and compared
    with previous results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the model’s performance does not improve, the algorithm increases its counting
    (commonly called *patience*) of how many iterations since its last improvement.
    Otherwise, it resets the counting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The training process is stopped if there has been no improvement over a certain
    number of iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, the training process continues for another iteration unless all the
    designated decision trees to be boosted have been completed. At this point, the
    training process is halted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 5.8 shows this process in a process flow chart.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F08_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 A process flow chart describing how early stopping works in GBDTs
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 shows the very same process from the point of view of the validation
    metric, which can be the loss or any other metric for which you want to get the
    best result. As the iterations progress, it is customary to observe training errors
    decrease. Validation error, on the other hand, tends to have a sweet point before
    the algorithm starts overfitting. Early stopping helps to catch this increase
    in validation error, and monitoring the validation error dynamics allows you to
    retrace to the iteration before any overfitting happens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F09_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 How validation error and training error generally behave when using
    GBDTs through multiple iterations
  prefs: []
  type: TYPE_NORMAL
- en: Different stopping points may be found for each model trained on different folds
    in a cross-validation process with early stopping. When training on the entire
    dataset, you can still rely on using early stopping based on a validation sample.
    Hence, you just need to set a high number of iterations and see when the training
    stops. Otherwise, you can use a fixed number of iterations based on the stopping
    iterations you observed in cross-validation. In this case, you can calculate the
    average or median of all stopping points seen in cross-validation to determine
    the number of boosting trees to use. However, there’s no fixed rule, and you may
    choose to use the second maximum value for an aggressive stopping policy or the
    second minimum value for a conservative one.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, consider increasing the number of boosting trees since training
    is done on more data than during cross-validation. As a general guideline, increase
    the number of boosting trees by a percentage equivalent to dividing one by the
    number of folds you used in cross-validation. However, since no one-size-fits-all
    solution exists, experimentation may be necessary to find the best approach for
    your problem.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we run the previous code again, this time setting a higher number
    of base estimators to be used and two parameters, `validation_fraction` and `n_iter_no_change`,
    that activate the early stopping procedure. The parameter `validation_fraction`
    determines the fraction of the training data to be used for validation, and it
    is effective only when `n_iter_no_change` is set to an integer indicating how
    many iterations should pass without improvements when testing the model on the
    validation set before stopping the process.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11 Applying early stopping with GradientBoostingClassifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: ① A GradientBoostingClassifier model whose iterations are raised to 1,000 from
    the previous 300
  prefs: []
  type: TYPE_NORMAL
- en: ② As a validation fraction, the GradientBoostingClassifier uses 20% of the training
    data for validation.
  prefs: []
  type: TYPE_NORMAL
- en: ③ The training of the GradientBoostingClassifier will stop after 10 iterations
    without improvements on the validation.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Extracts the number of estimators used during training for each fold’s estimator
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the list of the number of estimators for each fold’s estimator
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Since the mean number of iterations for cross-validation folds is 268 iterations,
    based on a rule of thumb, when using all the available data during the training
    phase, we suggest increasing the number of iterations by 20%, fixing it at 322
    iterations.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will introduce new implementations of gradient
    boosting, such as XGBoost and LightGBM. We will also present how to use early
    stopping with them.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Using XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XGBoost gained traction after being successful in a Kaggle competition, the
    Higgs Boson Machine Learning Challenge ([https://www.kaggle.com/c/higgs-boson](https://www.kaggle.com/c/higgs-boson)),
    where XGBoost has been proposed in the competition forums as a fast and accurate
    solution in comparison to Scikit-learn’s gradient boosting. XGBoost has since
    been adopted and successfully used in many other data science competitions, proving
    its effectiveness and the usefulness of Kaggle competitions as a good place to
    introduce innovations that disrupt performance benchmarks. Keras is another example
    of an innovation that was widely adopted after success in Kaggle competitions.
    At the time of writing, the XGBoost package had been updated and reached the milestone
    of version 2.0.3, which is the version we have used in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Initially conceived as a research project by Tianqi Chen, later further developed
    with the contribution of Carlos Guestrin, XGBoost, is a gradient boosting framework
    available as open-source software. It is noticeable that, contrary to other initiatives,
    such as LightGBM, sponsored by Microsoft, and Yggdrasil Decision Forests (see
    [https://mng.bz/lYV6](https://mng.bz/lYV6)) that Google sponsors, XGBoost remained
    completely independent, maintained by the Distributed (Deep) Machine Learning
    Common community ([dmlc.github.io](https://dmlc.github.io/)).
  prefs: []
  type: TYPE_NORMAL
- en: Over time, the framework has undergone significant enhancements and now provides
    advanced capabilities for distributed processing and parallelization, which enables
    it to operate on large-scale datasets. Meanwhile, XGBoost has also gained wide
    adoption, and it is currently accessible in several programming languages, including
    C/C++, Python, and R. Moreover, this framework is supported on multiple data-science
    platforms, such as H2O.ai and Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: As a data science user, you will immediately notice several key characteristics
    of this framework, including
  prefs: []
  type: TYPE_NORMAL
- en: The ability to handle various input data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for customized objective and evaluation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic handling of missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy support for GPU training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accommodation of monotonicity and feature interaction constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization of multiple cores and cache on standalone computers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a system performance perspective, notable features include
  prefs: []
  type: TYPE_NORMAL
- en: Networked parallel training, which allows for distributed computing across a
    cluster of machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilization of all available CPU cores during tree construction for parallelization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out-of-core computing when working with large datasets that don’t fit into memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, what sets the “extreme” XGBoost algorithm apart is its innovative algorithmic
    details for optimization, including a variant of gradient descent known as Newton
    Descent and regularization terms, as well as its unique approach to feature splitting
    and sparse data handling. The following section briefly summarizes these groundbreaking
    techniques that power XGBoost’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s try the algorithm with our classification task on the Airbnb NYC
    dataset. In this case, use the `XGBClassifier` from XGBoost. For regression problems,
    you can use the `XGBRegressor` class. First, however, you need to have XGBoost
    installed on your system. To install XGBoost, you can just pip install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Or use conda for the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The installation commands should do all the necessary steps for you; also install
    both CPU and GPU variants of the algorithm, if possible, on your system. See instructions
    and details on the installation process at [https://mng.bz/BXd0](https://mng.bz/BXd0).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following listing, we replicate the same approach we previously used
    with Scikit-learn’s GradientBoostingClassifier: we boost 300 trees, limit the
    depth of decision trees to four levels, and accept nodes of at least three examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12 XGBoost classifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates an XGBClassifier model with specified hyperparameters, including the
    booster type
  prefs: []
  type: TYPE_NORMAL
- en: ② The learning objective, equivalent to Scikit-learn’s loss
  prefs: []
  type: TYPE_NORMAL
- en: ③ min_child_weight is equivalent to Scikit-learn’s min_samples_leaf.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints the mean and standard deviation of cross-validated test scores
  prefs: []
  type: TYPE_NORMAL
- en: 'The obtained result is the best obtained so far, and it is impressive that
    the training fit is just a fraction of what previously the Scikit-learn implementation
    took:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the next subsection, we explore the key parameters we used for running this
    example and discuss what parameters, of the many offered by the algorithm (see
    [https://mng.bz/dX6N](https://mng.bz/dX6N) for a complete list) you need for running
    successfully any tabular data project using XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 XGBoost’s key parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s review the specific options we decided on in our previous example shown
    in listing 5.12, beginning with the `n_estimators` parameter, which specifies
    the number of decision trees involved in building the ensemble and is part of
    the gradient descent process that we discussed previously in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The `n_estimators` parameter in XGBoost determines the number of decision trees
    used to produce the output. In standard tabular problems, the usual values for
    this parameter range between 10 to 10,000\. While increasing this value can improve
    prediction performance by involving more weak learners, it can also slow down
    the training time. It’s worth noting that there is an ideal number of trees that
    maximizes performance on prediction tasks with unseen data, and finding this sweet
    spot depends on other XGBoost parameters, such as the learning rate. To achieve
    a high-performing XGBoost model, it’s important to choose the appropriate number
    of trees based on the problem at hand while setting the other parameters correctly,
    including the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whereas Scikit-learn is limited to just decision trees, XGBoost proposes more
    choices using its booster parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gbtree`—Decision trees, as you would expect in gradient boosting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gblinear`—Linear regression models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dart`—Decision trees, but the optimization process is more regularized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `gblinear` booster produces a sum of chained linear models. Since a sum
    of linear combinations is linear, you end up with a coefficient for each feature
    you have used, similar to a linear model. You can access the coefficients using
    the `.coef` method. It is a different way to fit a GBDT model with an emphasis
    on interpretability since the model can be reduced to a linear combination, different
    from the one you could directly fit because of using a different approach for
    complexity penalization and optimization. The most notable difference is that
    you cannot interpret the coefficients as you would if a linear regression or a
    generalized linear model produced them. Furthermore, the interpretation of the
    intercept generated by the `gblinear` booster differs from classical linear models
    as it is influenced by both the learning rate and the initial estimate employed
    by the booster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `dart` booster is different because it combines the optimization based
    on gradient descent with an approach similar to dropout, a technique used in deep
    learning. Presented by a UC Berkeley researcher and a Microsoft researcher in
    the paper “DART: Dropouts Meet Multiple Additive Regression Trees” by Rashmi Korlakai
    Vinayak and Ran Gilad-Bachrach (Artificial Intelligence and Statistics. PMLR,
    2015), DART focuses on overfitting due to the dependence of each decision tree’s
    estimates on the previous ones. The researchers then take the idea of dropout
    from deep learning, where a dropout mask randomly and partially blanked a neural
    network layer. The neural network cannot always rely on certain signals in a particular
    layer to determine the next layer’s weights. In DART, the gradients are not calculated
    compared to the sum of the residuals from all previously built trees. Still, instead,
    the algorithm at each iteration randomly selects a subset of the previous trees
    and scales their leaves by a factor of 1/k, where k is the number of trees that
    were dropped.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `gblinear` and the `dart` are the only alternative boosters available.
    For instance, there is no booster to mimic random forests (as there is for another
    GBDT implementation, LightGBM). However, though a random forests booster is not
    supported yet by `XGBClassifier` and `XGBRegression`, you can obtain a similar
    result by playing with XGBoost parameters and functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `num_parallel_tree` parameter and set it to a number above 1\. At each
    step of the optimization, the gradients are estimated not from a single decision
    tree but from a bagged ensemble of decision trees, thus creating a boosted random
    forests model. In some instances, this approach may provide better results than
    the gradient boosting approach because it will reduce the variance of the estimates
    at the expense of an increased computational cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `XGBRFClassifier` or `XGBRFRegressor`, two classes from XGBoost that
    implement a random forests approach. The classes are still experimental. For more
    details see [https://mng.bz/rKVB](https://mng.bz/rKVB) with the caveat that there
    are some differences with the random forests algorithm offered by Scikit-learn
    because XGBoost computes a matrix made up of second derivatives and called the
    Hessian (see [https://brilliant.org/wiki/hessian-matrix/](https://brilliant.org/wiki/hessian-matrix/)
    for a mathematical definition) to weight the gradients and it has no bootstrap
    capability. Hence, your results will be different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As for the loss function, controlled by the parameter `objective`, we chose
    `reg:logistic`, but we could have also chosen `binary:logistic`, both comparable
    to log-loss for binary classification. In XGBoost, you have loss functions organized
    into six classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reg` for regression problems (but you also have the logistic regression among
    its options).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary` for binary classification problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multi` for multiclass classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count` for count data—that is, discrete events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`survival` for survival analysis, which is a statistical technique to analyze
    data on the time it occurs for an event of interest to occur, such as the failure
    of a part in a machinery. It considers censoring, where the event of interest
    has not yet happened for some individuals in the study.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank` for ranking problems, such as estimating what rank a site should have
    in the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from Poisson distribution, useful for modeling the frequency of events,
    XGBoost also offers `reg:gamma` and `reg:tweedie`, optimizing for two distributions
    used in insurance for claim amount modeling as mentioned in chapter 4 when discussing
    generalized linear models.
  prefs: []
  type: TYPE_NORMAL
- en: The availability of various objective functions demonstrates the multiple possible
    applications that XGBoost may have in different domains. For a full overview of
    the loss functions see [https://mng.bz/dX6N](https://mng.bz/dX6N). Loss functions
    are essential in gradient boosting, as they define the optimization objective.
    By contrast, the evaluation metrics are not used to optimize the gradient descent
    in gradient boosting. However, they play a crucial role in monitoring the training
    process, optimizing for feature selection, hyper-parameter optimization, and even
    enabling early stopping to halt the training once it is no longer providing benefit.
    The equivalent of Scikit-learn’s `min_samples_leaf` in XGBoost is `min_child_weight`.
    Both parameters control the minimum number of samples needed to be at a leaf node
    of the decision tree. Thus, they regularize the decision tree by limiting the
    depth of the resulting tree. There are differences, however, since `min_child_weight`
    refers to the minimum sum of Hessian weight needed in a child node, while `min_samples_leaf`
    refers to the minimum number of samples required in a leaf. Hence, the two parameters
    are not completely comparable since their values are used differently in XGBoost
    and Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule of thumb, `min_child_weight` affects how single decision trees
    are built, and the larger the value of this parameter, the more conservative the
    resulting tree will be. The usual values to be tested range from 0 (implying no
    limit to the size of the leaf nodes) to 10\. In his 2015 talk at the NYC Data
    Science Academy, titled “Winning Data Science Competitions,” Owen Zhang, a former
    top Kaggle competitor, suggested computing the optimal value of this parameter
    by dividing 3 by the percentage of the rarest events in the data to be predicted.
    For instance, following this rule of thumb, since our classes are split 50%/50%,
    the ideal value should be 3/0.5, resulting in 6.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other important XGBoost parameters we didn’t use in our example are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `learning_rate`, also called `eta`, is a parameter in XGBoost that determines
    the rate at which the model learns. A lower learning rate allows the model to
    converge more slowly yet more precisely, potentially leading to better predictive
    accuracy. However, this will result in a greater number of iterations and a longer
    training time. On the other hand, setting the value too high can speed up the
    process but result in worse model performance because, as it happens in gradient
    descent when your learning parameter is too high, the optimization overshoots
    its target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha` and `lambda` are the L1 and L2 regularizers, respectively. They both
    contribute to avoiding overfitting in the gradient descent optimization part of
    XGBoost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `max_depth` parameter in XGBoost controls the algorithm’s complexity. If
    this value is set too low, the model may not be able to identify many patterns
    (known as underfitting). However, if it is set too high, the model may become
    overly complex and identify patterns that do not generalize well to new data (known
    as overfitting). Ideally, it is a value between 1 and 16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `gamma`, or `min_split_loss`, parameter in XGBoost is a regularization parameter
    ranging from 0 to infinity, and setting this value higher increases the strength
    of regularization, reducing the risk of overfitting but potentially leading to
    underfitting if the value is too large. Also, this parameter controls the resulting
    complexity of the decision trees. We suggest starting with this value at 0 or
    a low value and then testing increasing it after all the other parameters are
    set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `colsample_bytree` parameter in XGBoost controls the fraction of the total
    number of features or predictors to be used for a given tree during training.
    Setting this value to less than 1 means that each tree may use a different subset
    of features for prediction, potentially reducing the risk of overfitting or being
    too much influenced by single features in data. It also improves training speed
    by not using all features in every tree. The allowable range of values for this
    parameter is between 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `subsample` parameter in XGBoost controls the fraction of the number of
    instances used for a given tree during training. Like `colsample_bytree`, this
    parameter can help reduce overfitting and improve training time. By using a fraction
    of the cases for each tree, the model can identify more generalizable patterns
    in the data. The default value for `subsample` is 1.0, which means that all instances
    are used in each tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many cases, you will only require some of the parameters provided by XGBoost
    or discussed here for your projects. Simply adjusting the `learning_rate`, setting
    the optimization steps, and `min_child_weight` to prevent overfitting individual
    decision trees in the gradient boosting process will be sufficient most of the
    time. Additionally, you may derive benefits from setting the `objective`, `max_depth`,
    `colsample_bytree`, and `subsample` parameters, but it is unlikely that tweaking
    the numerous other available parameters will yield significant improvements. This
    holds not only for XGBoost but also for different implementations of gradient
    boosting.
  prefs: []
  type: TYPE_NORMAL
- en: Next we explain what makes the XGBoost implementation perform better in computations
    and predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 How XGBoost works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As explained in the paper “Xgboost: A Scalable Tree Boosting System” by Tianqi
    Chen and Carlos Guestrin (*Proceedings of the 22nd ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining*, 2016), the great performance of XGBoost
    boils down to a few innovations that weren’t present in other implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: Column block for parallel learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second-order approximation for quicker optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved split-finding algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparsity-aware split finding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Column Block is a technique used in parallel learning that involves dividing
    a dataset into blocks of columns or subsets of features. This allows for parallel
    training across multiple processors, significantly reducing the overall training
    time. You can see it in action when you train an XGBoost model and look for CPU
    utilization pointing to multiple usages of different cores. XGBoost cannot use
    multiple cores to train multiple models simultaneously, as it happens in other
    ensemble models such as random forests. That’s because gradient boosting is a
    serial model, where each model is trained after the results of another one. Instead,
    the XGBoost training process of every single model is divided among multiple cores
    to increase efficiency and speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, XGBoost can be utilized in Python through two distinct APIs: the
    Native API and the Scikit-learn API. In this book, we will exclusively use the
    Scikit-learn API due to its benefits in terms of best modeling practices and the
    added benefit of being able to easily utilize various tools available in the Scikit-learn
    library, such as model selection and pipelines, as explained in chapter 4.'
  prefs: []
  type: TYPE_NORMAL
- en: When using the Native API, the user is required to convert their data into a
    `DMatrix`, which is an internal XGBoost data structure optimized for both memory
    efficiency and training speed ([https://mng.bz/VVxP](https://mng.bz/VVxP)). The
    use of the DMatrix format makes the column block technique possible. However,
    when using the Scikit-learn API, users can input their data as pandas DataFrames
    or Numpy arrays without requiring explicit conversion into the DMatrix format.
    This is because XGBoost performs the conversion under the hood, making the process
    more streamlined. Therefore, it is safe to choose the API that best suits your
    preferences, as both APIs offer the same performance and differ only in some of
    the parameters, default values, and options they provide.
  prefs: []
  type: TYPE_NORMAL
- en: The second-order approximation for speeding up the optimization incorporating
    the second derivative (the gradient derived from the first derivative) is based
    on a more comprehensive root-finding technique, Newton’s method. In the context
    of minimization, we often refer to Newton’s method as Newton descent instead of
    gradient descent. Listing 5.13 shows it implemented as a new class, the `NewtonianGradientBoosting`
    class, that inherits the original GradientBoosting class with some additions and
    modifications to its existing methods and attributes. In particular, we add the
    Hessian calculations to balance the gradient steps in regard to the convergence
    acceleration and a regularization term to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.13 How XGBoost works
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a new class NewtonianGradientBoosting as a subclass of GradientBoosting
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets a regularization parameter reg_lambda
  prefs: []
  type: TYPE_NORMAL
- en: ③ Initializes a constant Hessian matrix with ones
  prefs: []
  type: TYPE_NORMAL
- en: ④ Fits the decision tree by dividing the negative gradient by the sum of the
    Hessian and the regularization parameter
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates an instance of the NewtonianGradientBoosting class with specified
    hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Fits the NewtonianGradientBoosting model to the training data
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Predicts target values using the fitted model and calculating the accuracy
    score for evaluation
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting accuracy is a little better than what we obtained from the original
    GradientBoosting class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, the Hessian is probably not particularly helpful because it is
    the same for all due to the kind of objective function we use: the squared error.
    However, in the context of optimization with other objective functions, the Hessian
    matrix provides information about the curvature of a function, which can be used
    to determine the direction and rate of change of the function. Intuitively, you
    can figure out that, with larger curvatures, you have larger Hessian values, which
    reduce the effect of the gradient, acting as a brake for the learning rate. On
    the contrary, smaller curvatures lead to an acceleration of the learning rate.
    Using the information of the Hessian, you get an adaptive learning rate for each
    of your training examples. However, as a side effect, computing the second derivative
    can often be complicated or intractable, necessitating significant computation.
    Analytical expressions and numerical methods for determining the second derivative
    require substantial computational effort. In the next chapter, devoted to more
    advanced topics, we will provide further information on how to build your customized
    objective functions by computing gradients and Hessians analytically and numerically.'
  prefs: []
  type: TYPE_NORMAL
- en: A role in the Newton optimization used by XGBoost is also played by the regularization
    term that is summed up in Hessian and reduces the target further—that is, the
    adjustment to be estimated by the base learners. Another idea taken from gradient
    descent that XGBoost uses is regularization in the form of L2 regularization,
    as implemented in our example, and L1 regularization. The additional regularization
    terms help to smooth the final learned weights and avoid overfitting by directly
    modifying the Newtonian descent step. Consequently, it is important to consider
    how to tune both L1 and L2 values, referred to as lambda and alpha in the XGBoost
    and LightGBM implementations, as significant hyperparameters for improving optimization
    results and reducing overfitting. These regularization values ensure that the
    Newton descent takes smaller steps during optimization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will continue to explore the new capabilities introduced
    by XGBoost by examining the contribution of split-finding algorithms to the increased
    speed performance offered by the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 Accelerating with histogram splitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gradient boosting is based on binary trees, which work by partitioning the data
    to have a better-optimized objective metric in the resulting splits than in the
    original set. Since gradient boosting treats all the features as numeric, it has
    a unique way of deciding how to partition. To find the feature to use for the
    split and the rule for the split, a binary tree decision should iterate through
    all the features, sort each feature, and evaluate every split point. Ultimately,
    the decision tree should pick the feature and its split point that led to better
    improvement relative to the objective.
  prefs: []
  type: TYPE_NORMAL
- en: With the emergence of larger datasets, the splitting procedure in decision trees
    poses serious scalability and computational problems for the original GBDT architecture
    based on serial models that continuously scan through data. From a computational
    point of view, the main cost in GBDT lies in learning the decision trees, and
    the most time-consuming part of learning a decision tree is finding the best split
    points.
  prefs: []
  type: TYPE_NORMAL
- en: Continuously looking for the best splitting point takes quite some time, rendering
    the algorithm quite demanding when training on a large number of features and
    instances. Histogram splitting helps reduce the time by replacing each feature’s
    value with the histogram’s split points to summarize its values. Listing 5.14
    simulates a split search on our data problem. In doing so, we define an objective
    function and a splitting function that can operate both as the original decision
    tree splitting algorithm or by the faster histogram-based splitting.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.14 The histogram split
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: ① Function calculating and returning the Gini impurity of a set of labels y
  prefs: []
  type: TYPE_NORMAL
- en: ② If use_histogram is true, computes the histogram for the selected feature
  prefs: []
  type: TYPE_NORMAL
- en: ③ If use_histogram is false, just enumerates all the unique values in the feature
  prefs: []
  type: TYPE_NORMAL
- en: ④ Initializes the best score and threshold
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Iterates over all possible thresholds
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Splits y based into left and right subsets based on the selected threshol
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Calculates the Gini impurity score for the left and right subsets
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Updates the best score and threshold if the current split has a higher Gini
    impurity score than the previous best split
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Returns the best threshold and its corresponding Gini impurity score
  prefs: []
  type: TYPE_NORMAL
- en: In the code in listing 5.14, after defining a scoring function, the Gini impurity,
    we define a function that picks a feature and enumerates the values of its potential
    splits to be evaluated. If we use the basic approach, all its unique values are
    taken into account. Using the histogram approach instead, a 256-bin histogram
    is computed, and we use the bins delimiting values to be explored as potential
    split candidates. If our feature has more than 256 unique values, using the histogram
    will save us a lot of time when we just iterate through all split candidates and
    evaluate them by the scoring function.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explained the workings of the example functions, we are ready
    for a test. We decided to optimally split the latitude in the classification task
    of predicting if the host is in the upper or lower price range. Since the latitude
    feature has many unique values to be considered as splitting candidates because
    Manhattan is a long, thin north-south island where property values vary by latitude,
    it should result in a difficult task because we expect many different latitudes
    to be evaluated against the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our first test, we try to find the best split just by evaluating all the
    unique values that the feature presents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In our second test, we rely on evaluating the splitting points found out by
    a histogram with 256 bins built on the feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Looking under the hood of histogram splitting, we find binning, where values
    for a variable are grouped into discrete bins, and each bin is assigned a unique
    integer to preserve the order between the bins. Binning is also commonly referred
    to as k-bins, where the k in the name refers to the number of groups into which
    a numeric variable is rearranged, and it is used in histogram plotting, where
    you can declare a value for k or automatically have it set to summarize and represent
    your data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The speed-up is due not only to a minor number of split points to evaluate,
    which can be tested in parallel, thus using multicore architectures, but also
    to the fact that histograms are integer-based data structures that are much faster
    to handle than continuous values vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'XGBoost uses an algorithm to compute the best split based on presorting the
    values and usage of histograms. The presorting splitting works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For each node, enumerating the features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For every feature, sorting instances by their values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a linear scan and histograms, determining the best split for the feature
    and computing the information gain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Picking the best solution among all the features and their best split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XGBoost has other concept improvements: the traditional split finding algorithm
    is denoted by `exact` specified as the value of the `tree_method` parameter. The
    Weighted Quantile Sketch, referred to as `approx` in the XGBoost API, is an exclusive
    feature unique to XGBoost. This split-finding technique utilizes approximations
    and harnesses information derived from gradient statistics. By employing quantiles,
    the method defines potential split points among candidates. Notably, the quantiles
    are weighted to prioritize selecting candidates capable of mitigating high gradients,
    reducing significant prediction errors.'
  prefs: []
  type: TYPE_NORMAL
- en: Weighted Quantile Sketch using histograms is now available as `tree_method="hist"`,
    which, since the 2.0.0 release, is the default method. In contrast, the `approx`
    tree method generates a new set of bins for each iteration, whereas the `hist`
    method reuses the bins over multiple iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another feature of the algorithm is related to data storage in DMatrices. The
    most time-consuming part of tree learning is sorting the data. To reduce the sorting
    cost, we propose storing the data in in-memory units: a block. This allows linearly
    scanning over the presorted entries and parallelizing, giving us an efficient
    parallel algorithm for split finding.'
  prefs: []
  type: TYPE_NORMAL
- en: After the successful histogram aggregation implementation in LightGBM, XGBoost
    adopted it. Histogram aggregation is also the main feature of `HistGradientBoosting`,
    the Scikit-learn histogram-based gradient boosting we will present after LightGBM.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.4 Applying early stopping to XGBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Section 5.3.1 illustrates how early stopping works with Scikit-learn’s gradient
    boosting. XGBoost also supports early stopping. You can specify early stopping
    by adding a few arguments when instantiating an XGBClassifier or XGBRegressor
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`early_stopping_rounds`—This is the number of rounds to wait patiently without
    improvement in the validation score before stopping the training. If you set it
    to a positive integer, the training will stop when the performance on the validation
    set hasn’t improved in that many rounds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_metric`—This is the evaluation metric to use for early stopping. By default,
    XGBoost uses `rmse` for regression’s root mean squared error and `error` for accuracy
    in classification. Still, you can specify any other from a long list (available
    at [https://mng.bz/xK2W](https://mng.bz/xK2W)) as well as specify your own metric
    (which will be discussed in the next chapter on advanced machine learning topics).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to setting these parameters, you also have to specify, at fitting
    time, a sample with its target where to monitor the evaluation metric. This is
    done by the `parameter eval_set`, which contains a list of tuples containing all
    the validation samples and their responses. In our example, we use only a validation
    set. Still, if there are multiple samples to monitor, XGboost will consider only
    the last tuple of data and response for stopping purposes.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 5.15, we replicate the same approach we experimented with earlier
    by splitting our data into a train and a test set. However, to properly monitor
    the evaluation metric, we further split the train set to extract a validation
    set from it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.15 Applying early stopping to XGBoost
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: ① Splits the indices of the data into training and test sets using a fixed random
    seed
  prefs: []
  type: TYPE_NORMAL
- en: ② Further splits the training set into training and validation sets using the
    same random seed
  prefs: []
  type: TYPE_NORMAL
- en: ③ Initializes an XGBoost classifier with an early stopping patience for 100
    rounds
  prefs: []
  type: TYPE_NORMAL
- en: ④ Uses the 'error' parameter, equivalent to accuracy, as an evaluation metric
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Fits the XGBoost classifier to the training data X and labels y and performance
    on the validation data Xv and yv
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Prints the accuracy score after comparing the predicted labels with the true
    labels
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing the training, we managed to get this accuracy measure, which
    is slightly underperforming in respect of our previous cross-validation results
    because it is obtained by training on fewer examples—that is, 64% of available
    data since we reserved 20% for test and 16% for validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: During the training, the evaluation metric is constantly checked, and the fit
    procedure is halted if it doesn’t improve from more iterations than those specified
    by `early_stopping_rounds`. The best iteration is automatically recorded and used
    at prediction time. Thus, you have nothing more to do with the model. If you need
    to verify how many iterations it took before stopping, you can obtain it by inquiring
    about the model using the `best_iteration` attribute. In our example, `xgb.best_iteration`
    returns 200.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Introduction to LightGBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LightGBM was first introduced in a 2017 paper titled “LightGBM: A Highly Efficient
    Gradient Boosting Decision Tree” by Guolin Ke and his team at Microsoft ([https://mng.bz/AQdz](https://mng.bz/AQdz)).
    Recently, the package reached the 4.3.0 version, which is the version we tested
    in this book. According to the authors, the term “light” in LightGBM highlights
    the algorithm’s faster training and lower memory usage than traditional gradient
    boosting decision trees. The paper demonstrated, through experiments on multiple
    public datasets, the algorithm’s effectiveness and its ability to speed up the
    training process of conventional gradient boosting decision trees by over 20 times
    while maintaining almost the same accuracy. LightGBM was made available as open-source
    software on GitHub ([https://github.com/microsoft/LightGBM/](https://github.com/microsoft/LightGBM/)),
    quickly gaining popularity among data scientists and machine learning practitioners.'
  prefs: []
  type: TYPE_NORMAL
- en: On paper, LightGBM shares many characteristics similar to those of XGBoost,
    such as support for missing values, native handling of categorical variables,
    GPU training, networked parallel training, and monotonicity constraints. We will
    tell you more about all that in the next chapter. In addition, LightGBM also supports
    sparse data. However, its major advantage lies in its speed, as it is significantly
    faster than XGBoost on various tasks, which has made it popular in both Kaggle
    competitions and real-world applications. The Kaggle community quickly took notice
    of LightGBM and began incorporating it into their competition entries alongside
    the already-popular XGBoost. In fact, mlcontests.com, a website that tracks the
    data science competition scene, reported in 2022 that LightGBM had become the
    preferred tool among competition winners, surpassing XGBoost in popularity. An
    impressive 25% of reported solutions for tabular problems were based on LightGBM.
    While LightGBM has seen comparable success among data science practitioners, XGBoost
    remains more popular overall. For example, the XGBoost repository has many more
    GitHub stars than the LightGBM repository.
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM is a cross-platform machine learning library available for Windows,
    Linux, and MacOS. It can be installed using various tools, such as pip or conda,
    or built from source code (see the complete installation guide at [https://mng.bz/ZlEP](https://mng.bz/ZlEP)).
    Its usage syntax is similar to Scikit-learn’s, making it easy for users who are
    familiar with Scikit-learn to transition to LightGBM. When optimizing gradient
    descent, LightGBM follows in the footsteps of XGBoost by utilizing the Newton-Raphson
    update, which involves dividing the gradient by the Hessian matrix. Guolin Ke’s
    answer on GitHub confirms that (see [https://github.com/microsoft/LightGBM/issues/5233](https://github.com/microsoft/LightGBM/issues/5233)).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s put the algorithm to the test on the same problem we previously examined
    with ScikitLearn’s GradientBoosting and XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.16 LightGBM classifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: ① Initializes an LGBMClassifier with a number of estimators, maximum tree depth,
    and minimum number of child samples
  prefs: []
  type: TYPE_NORMAL
- en: ② Forces column-wise histogram building
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a model pipeline that includes a column transformation step and an
    LGBMClassifier step
  prefs: []
  type: TYPE_NORMAL
- en: ④ Performs five-fold cross-validation using the model pipeline using accuracy
    scoring
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean test score and standard deviation of the test scores obtained
    during cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the impressive results in terms of accuracy, training, and
    prediction times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Like XGBoost, LightGBM controls gradient descent with parameters such as `n_estimators`,
    `learning_rate,` `lambda_l1,` and `lambda_l2` (L1 and L2 regularization, respectively).
    The most important parameters of LightGBM that help control its complexity are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth`—This parameter controls the maximum depth of each tree in the ensemble.
    A higher value increases the complexity of the model, making it more prone to
    overfitting. If it is set to –1 it means that no limit is set to the growth of
    the trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_leaves`—This parameter specifies the maximum number of leaves in a tree
    and, therefore, the complexity of the model. To avoid overfitting, it should be
    set to less than `2**(max_depth)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_data_in_leaf`—This parameter controls the minimum number of samples required
    to be present in each leaf node. A higher value can prevent the tree from growing
    too deep and overfitting but can also lead to underfitting if set too high. The
    default value is 20\. We suggest trying lower values, such as 10, and then testing,
    increasing the value to 300.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters `feature_fraction` and `bagging_fraction` control how LightGBM
    samples from features and examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_fraction`—This parameter controls the fraction of features to be considered
    at each split. Similar to the `colsample_bytree` parameter in XGBoost, it can
    help reduce overfitting by preventing the model from relying too heavily on any
    feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bagging_fraction`—This parameter controls the fraction of data to be used
    for each tree. Similar to the subsample parameter in XGBoost, it can help reduce
    overfitting and improve training speed by randomly sampling from the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bagging_freq`—This parameter, which is not present in XGBoost, determines
    how frequently bagging should be applied. It turns off bagging examples when set
    to 0, even if `bagging_fraction` is specified. A value of n means bagging at every
    n iteration. For instance, a value of 2 means you have a bagged iteration every
    two (half of the time).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Related to the way LightGBM executes during the training, `verbosity` controls
    the amount of output information during training, while `force_col_wise` indicates
    histograms for feature splits during tree construction to be built based on columns.
    LightGBM can build histograms either column-wise or row-wise. Column-wise histogram
    building is generally faster, but it can require more memory, especially for datasets
    with a large number of columns. Row-wise histogram building is slower, but it
    can be more memory-efficient when dealing with datasets with a large number of
    columns. LightGBM will automatically choose the best method for building histograms
    for the dataset. However, you can also force LightGBM to use a specific method
    by setting the `force_col_wise` or `force_row_wise` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for XGBoost, LightGBM can also use different base learners by specifying
    the `boosting` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gbdt`—The default option, using decision trees as base learners'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rf`—Implements the random forests algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dart`—Implements the “Dropouts meet Multiple Additive Regression Trees” algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, setting the parameter `linear_tree` to true, as you are using the
    default `boosting=gbdt`, will fit a piecewise linear gradient boosting tree—that
    is, decision trees having linear models as their terminal nodes. This is a compromise
    solution that uses both the nonlinear learning capabilities of decision trees
    and the extrapolative capabilities of linear models with unseen, outlying cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will examine closely all the innovations that distinguish
    LightGBM from XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 How LightGBM grows trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s examine each characteristic that distinguishes LightGBM from XGBoost,
    starting with how LightGBM grows decision trees. Instead of increasing the tree
    level-wise (also known as *depth-first*) like XGBoost, LightGBM grows the tree
    leaf-wise (also known as *best first)*. This means that the algorithm chooses
    the leaf node that provides the maximum gain and then splits it further until
    it is no longer advantageous. In contrast, the level-wise approach simultaneously
    splits all the nodes at the same depth.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, in XGBoost’s level-wise growth approach, the algorithm grows all
    tree leaves to the same level. Then it splits them simultaneously, which may result
    in many insignificant leaves that don’t contribute much to the final prediction.
    In contrast, LightGBM’s leaf-wise growth approach splits the leaf with the maximum
    loss reduction at each step, resulting in fewer leaves but with higher accuracy.
    The leaf-wise approach allows LightGBM to focus only on the important features
    with the most significant effect on the target variable. This means that the algorithm
    can quickly converge to the optimal solution with fewer splits and a smaller number
    of trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.10 shows a representation of the two approaches: on the left, the
    level-wise approach and, on the right, the leaf-wise approach, both constrained
    to have, at the most, four terminal nodes. The two approaches take completely
    different paths in terms of the rules they decide to apply and how they segment
    the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F10_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 How level-wise (on the left) and leaf-wise (on the right) tree growth
    differ
  prefs: []
  type: TYPE_NORMAL
- en: It is important to point out that if you allow two trees to grow using the same
    data, one using a leaf-wise approach and one using a level-wise approach fully,
    they will define the same terminal leaves and predictions. The distinction lies
    in how they are built, with the leaf-wise approach being more aggressive in splitting
    nodes that provide the most information gain first.
  prefs: []
  type: TYPE_NORMAL
- en: This implies that the leaf-wise and level-wise approaches differ when a stopping
    rule is applied based on reaching a certain number of terminal nodes or a specific
    depth in tree splitting. In this case, the leaf-wise approach can result in smaller
    trees, faster training times, and higher accuracy, but it also carries an increased
    risk of overfitting. To control the depth of the tree leaf-wise growth and address
    overfitting, you can control the max-depth parameter in LightGBM.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 Gaining speed with exclusive feature bundling and gradient-based one-side
    sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To further reduce the training time, a basic strategy in gradient boosting and
    many other machine learning algorithms is reducing the number of examples processed.
    The simplest way to reduce the number of processed values is using stochastic
    sampling (i.e., row reduction) and/or dimensionality reduction techniques such
    as column sampling or principal component analysis (i.e., column reduction). Although
    sampling can improve accuracy in the presence of noise in data, excessive sampling
    can harm the training process and decrease predictive performance. In LightGBM,
    the *Gradient-Based One-Side Sampling* algorithm (GOSS) determines the manner
    and extent of sampling. Dimensionality reduction techniques rely on identifying
    redundancies in the data and combining them using a linear combination, typically
    a weighted sum. However, linear combinations can destroy nonlinear relationships
    in the data. Dimensionality reduction by discarding rare signals may lead to a
    decrease in model accuracy if the successful resolution of the data problem depends
    on those weak signals. In LightGBM, dimensionality reduction is handled by *Exclusive
    Feature Bundling* (EFB), which is a way to reduce the column dimension without
    losing information.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by explaining the two major speed improvements in LightGBM, starting
    with how EFB works. EFB is a technique that efficiently decreases the number of
    features without compromising data integrity. When using extensively one-hot encoded
    and binary features, many features become sparse, with few values and an abundance
    of zeros. You can retain all non-zero values without loss by summing these features
    and encoding some values. LightGBM optimizes computations and data dimensionality
    by grouping these features into *Exclusive Feature Bundles*, ensuring that predictive
    accuracy is maintained.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 shows how effectively two features can be bundled together. The
    solution involves adding feature B to feature A only for non-zero values, using
    the maximum value present in feature A. This combined feature will preserve the
    original features’ order because the values from feature A are separate from the
    values from feature B and will be located in different sections of the value distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F11_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 Demonstrating how EFB works when combining two features
  prefs: []
  type: TYPE_NORMAL
- en: Finding the optimal way to bundle exclusive features is a complex problem, classified
    as NP-hard. However, according to the LightGBM paper by Guolin Ke and his team,
    a greedy algorithm can provide a good approximation by automatically bundling
    many features. The feature bundling algorithm works sequentially, selecting the
    features with the least number of overlapping values and bundling them together.
    If it finds another feature with minimal overlap, it continues to bundle. Otherwise,
    it starts a new bundle until no more bundles can be found. The stopping rule is
    provided by the degree of conflicts two features have. If they have more conflicts
    than a certain gamma threshold, the bundle cannot be made, and the entire process
    may stop if there aren’t better candidates. Although the resulting bundles from
    this greedy process are not guaranteed to be optimal, the algorithm provides an
    acceptable solution in a reasonable time.
  prefs: []
  type: TYPE_NORMAL
- en: The other performance improvement presented in the paper is GOSS. As we mentioned,
    if EFB is aimed at reducing column dimensionality, GOSS works on the rows by sampling
    them effectively without bias.
  prefs: []
  type: TYPE_NORMAL
- en: GOSS is based on the observation that certain data instances are unlikely to
    provide useful information for finding a split point. Searching a carefully selected
    subset of the training set can save computation time without affecting predictive
    accuracy. Additionally, in gradient boosted decision trees, the algorithm implicitly
    specifies a weight for data instances when optimizing for the gradient for each
    data instance. Determining weights is crucial to compute a correction of the previous
    estimates, but that could also be used for sampling the data instances that could
    be more interesting to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'GOSS estimates data examples with larger gradients to contribute more toward
    information gain. Focusing on examples with larger gradients and ignoring part
    of those with smaller ones should reduce the number of processed data instances
    while still optimizing the algorithm for predictions. The procedure for GOSS results
    in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: GOSS first sorts the data examples according to the absolute value of their
    gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It selects the top a × 100% data examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It randomly samples b × 100% data examples from the rest of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It trains the decision tree on the combined samples using a weight of 1 for
    the top data examples and a weight of (1 – a) / b for the randomly sampled data
    examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final weighting is necessary to maintain the original data distribution
    of the dataset and avoid any unwanted shift in its representation.
  prefs: []
  type: TYPE_NORMAL
- en: GOSS can accelerate the training of gradient boosted decision trees, especially
    when dealing with larger datasets and complex trees. The original paper’s authors
    demonstrate that the error incurred by GOSS’s sampling approximation becomes negligible
    for larger datasets compared to the traditional method. In our experience using
    GOSS, at best, you get similar results compared to standard LightGBM training.
    Still, the speed-up is significant, making GOSS a good choice for faster experimentation
    when looking for the correct hyperparameters or selecting the most relevant features
    for your problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrary to the other speed-ups we presented, GOSS is not used by default:
    you must specify that you want to use it.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.3 Applying early stopping to LightGBM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LightGBM supports early stopping, and the parameters to control it are similar
    to those used in the XGBoost implementation. In the example in listing 5.17, we
    use LightGBM for training and have the algorithm assess its performance during
    the training phase using a test set. If there is no improvement in performance
    on the test set for 100 iterations, the algorithm halts the training process.
    It selects the round of iterations that achieved the highest performance on the
    test set so far.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.17 Applying early stopping to LightGBM
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: ① Splits the indices of the data into training and test sets using a fixed random
    seed
  prefs: []
  type: TYPE_NORMAL
- en: ② Further splits the training set into training and validation sets using the
    same random seed
  prefs: []
  type: TYPE_NORMAL
- en: ③ Initializes a LightGBM classifier with a number of estimators, max depth,
    and minimum number of child samples
  prefs: []
  type: TYPE_NORMAL
- en: ④ Fits the LightGBM classifier to the training data X and labels y and performance
    on the validation data Xv and yv
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Sets accuracy as an evaluation metric
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Sets a callback to suppress the evaluation (period=0)
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Prints the accuracy score after comparing the predicted labels with the true
    labels
  prefs: []
  type: TYPE_NORMAL
- en: 'Even in this case, the result is penalized by the fact that we are training
    only on 64% of the available data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: However, there are some minor differences from the XGBoost implementation that
    you can notice from the code. The `eval_metric` takes different names (that you
    can check at [https://mng.bz/RVZK](https://mng.bz/RVZK)) and, to suppress the
    printing of the evaluations during the training, you don’t use the verbose parameters
    as in XGBoost; rather, you have to specify a callback function (`log_evaluation`)
    that has to be declared at the fitting time in the list of callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, early stopping has been implemented as a callback function too (see
    [https://mng.bz/2yK0](https://mng.bz/2yK0)). Keeping the declaration of early
    stopping rounds during model instantiation stayed just for maintaining API compatibility
    with XGBoost. In case you use early stopping as a callback, you have more control
    over the way the LightGBM stops its training:'
  prefs: []
  type: TYPE_NORMAL
- en: '`first_metric_only` allows you to indicate whether to use only the first metric
    for early stopping or any metric you pointed out using.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_delta` signals the minimum improvement in the metric to keep on training,
    which usually is set to zero (any improvement), but it could be raised to impose
    more strict control over the growth of the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the previous example, you just need to remove the `early_stopping_rounds`
    from the LGBMClassifier instantiation and add the proper call back to the callback
    list in the fit method to obtain the same result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Whatever method you use, the iteration index, resulting in the best validation
    score, will be stored in the `best_iteration` attribute of a model, and that iteration
    will be automatically used when predicting.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.4 Making XGBoost imitate LightGBM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the introduction of LightGBM and its impressive usage of unbalanced decision
    trees, XGBoost also started supporting the leaf-wise strategy in addition to its
    original level-wise strategy. In XGBoost, the original level-wise approach is
    called `depthwise,` and the leaf-wise strategy is called `lossguide`. By setting
    one or another using the `grow_policy` parameter, you can have your XGBoost behave
    as a LightGBM. In addition, XGBoost authors suggested, when using the lossguide
    grow policy, to set the following parameters to avoid overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_leaves`—Sets the maximum number of nodes to be added and is only relevant
    for the lossguide policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`—Sets the maximum depth of the tree. If `grow_policy` is set to
    `depthwise`, `max_depth` behaves as usual. However, if `grow_policy` is set to
    `lossguide`, `max_depth` can be set to zero, indicating no depth limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incidentally, you also have the same parameters in LightGBM to be used for the
    same purpose (`max_leaves` is an alias—i.e., another working name of the parameter
    `num_leaves`).
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.5  How LightGBM inspired Scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In version 0.21 of Scikit-learn, two novel implementations of gradient boosting
    trees were added: `HistGradientBoostingClassifier` and `HistGradientBoostingRegressor`,
    inspired by LightGBM. You may wonder why you would bother with this new implementation
    if the current LightGBM and XGBoost versions can offer you everything you need
    to develop the best-performing tabular solutions based on gradient boosting. They
    also ensure full compatibility with Scikit-learn API. It is worth the time to
    look at it because the histogram-based implementation, though now a work in progress,
    is expected to take over the original, offering the same control over the learning
    process and building of decision trees. Moreover, it has shown even better predictive
    performances than XGBoost and LightGBM in some specific applications. Hence, it
    may be worth testing for your specific problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In comparison with the original Scikit-learn implementation for gradient boosting,
    the new histogram-based ones present new characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Binning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multicore (the initial implementation was single-core)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No sparse data support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in support for missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monotonicity and interaction constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Native categorical variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the moment, talking about differences, there is no support for sparse data
    in the new histogram-based implementation. Consequently, if your data is in a
    sparse matrix, you should first densify the data matrix. Also, some other features
    typical of `GradientBoostingClassifier` and `GradientBoostingRegressor` still
    need to be supported—for instance, some loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: On the API side, most parameters are unchanged from `GradientBoosting``Classifier`
    and `GradientBoostingRegressor`. One exception is the `max_iter` parameter that
    replaces `n_estimators`. The following listing shows an example of the `HistGradientBoostingClassifier`
    applied to our classification problem with the Airbnb NYC dataset classifying
    listings above the median market value.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.18 The new Scikit-learn’s histogram gradient boosting
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: ① Initializes a HistGradientBoostingClassifier with specific hyperparameters
    for the boosting algorithm
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a model pipeline combining data preprocessing (column_transform) and
    the model
  prefs: []
  type: TYPE_NORMAL
- en: ③ Executes five-fold cross-validation on the model pipeline returning scores
    and trained estimators
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints the mean and standard deviation of the accuracy scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: The results are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Compared to our previous examples with XGBoost and LightGBM, this one differs
    from the used command and the `max_iter` parameter in exchange for the usual `n_estimators`.
    Also, the Scikit-learn new boosting algorithm is a histogram one. You just set
    the `max_bins` argument to change the initial default value of 255 (it is 256
    bins because 1 is reserved for missing cases).
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is still under development and lacks support for sparse data.
    This implies that it cannot perform as fast as XGBoost or LightGBM in the presence
    of many one-hot encoded features, no matter how you prepare your data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensemble algorithms are used to improve the predictive power of a single model
    by using multiple models or chaining them together:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble algorithms are often based on decision trees.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two core ensemble strategies: averaging and boosting.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaging strategies, such as random forests, tend to reduce the variance of
    predictions while only slightly increasing the bias.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pasting is a type of averaging approach that involves creating a set of different
    models trained on subsamples of the data and pooling the predictions together.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging is similar to averaging but with bootstrapping instead of subsampling.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaging methods can be computationally intensive and increase bias by excluding
    important parts of the data distribution through sampling.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Random forests is an ensemble learning algorithm that combines decision trees
    by bootstrapping samples and subsampling features during modeling (random patches):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It creates a set of models that are different from each other and produces more
    reliable and accurate predictions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used to determine feature importance and measure cases’ similarity
    in a dataset.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm requires fine-tuning its few hyperparameters, like the number
    of employed trees, and adjusting bias-variance tradeoffs by setting the maximum
    number of features used for splits, the maximum depth of trees, and the minimum
    size of the terminal branches.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be computationally costly if the number of trees is set too high.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ERT, Extremely Randomized Trees, is a variation of the random forests algorithm:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It randomly selects the feature for the split at each decision tree node, leading
    to less variance (because trees are more diverse) but more bias (randomization
    sacrifices some of the decision trees’ predictive accuracy, resulting in a higher
    bias).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is more computationally efficient and useful for large datasets with many
    collinear and noisy features.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces variance by making the resulting set of trees less correlated.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GBDT is a highly effective machine learning method for tabular data problems.
    It has become a leading approach in various domains, including multiclass classification,
    advertising click prediction, and search engine ranking. Compared to other methods,
    such as neural networks, support vector machines, random forests, and bagging
    ensembles, GBDT generally performs better in standard tabular problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting is effective because it combines gradient descent, an optimization
    procedure typical of linear models and neural networks, and decision trees trained
    on the gradients derived from the sum of the previous decision trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn offers one of the earliest options for gradient boosting algorithms
    for regression and classification tasks. Recently, the original algorithm was
    replaced by a speedier histogram-based one, which is still under development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XGBoost, an algorithm for gradient boosting decision trees, gained popularity
    after its successful use in the Higgs Boson Machine Learning Challenge on Kaggle.
    It is based on a more complex optimization based on Newton’s Descent, and it offers
    the following advantages:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Ability to handle various input data types
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for customized objective and evaluation functions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic handling of missing values
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy support for GPU training
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Accommodation of monotonicity and feature interaction constraints
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization of multiple cores and cache on standalone computers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LightGBM is a highly efficient gradient boosting decision tree algorithm, introduced
    in a 2017 paper by Guolin Ke and his team at Microsoft. The algorithm was designed
    to be faster and use less memory than traditional gradient boosting decision trees,
    as demonstrated in experiments on multiple public datasets. The LightGBM algorithm
    achieves this thanks to its leaf-wise splitting policy and EFB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
