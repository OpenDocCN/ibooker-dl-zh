["```py\nfrom keras.datasets import imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=10000\n) \n```", "```py\n>>> train_data[0]\n[1, 14, 22, 16, ... 178, 32]\n>>> train_labels[0]\n1\n```", "```py\n>>> max([max(sequence) for sequence in train_data])\n9999\n```", "```py\n# word_index is a dictionary mapping words to an integer index.\nword_index = imdb.get_word_index()\n# Reverses it, mapping integer indices to words\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n# Decodes the review. Note that the indices are offset by 3 because 0,\n# 1, and 2 are reserved indices for \"padding,\" \"start of sequence,\" and\n# \"unknown.\"\ndecoded_review = \" \".join(\n    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]]\n) \n```", "```py\n>>> decoded_review[:100]\n? this film was just brilliant casting location scenery story direction everyone\n```", "```py\nimport numpy as np\n\ndef multi_hot_encode(sequences, num_classes):\n    # Creates an all-zero matrix of shape (len(sequences), num_classes)\n    results = np.zeros((len(sequences), num_classes))\n    for i, sequence in enumerate(sequences):\n        # Sets specific indices of results[i] to 1s\n        results[i][sequence] = 1.0\n    return results\n\n# Vectorized training data\nx_train = multi_hot_encode(train_data, num_classes=10000)\n# Vectorized test data\nx_test = multi_hot_encode(test_data, num_classes=10000) \n```", "```py\n>>> x_train[0]\narray([ 0.,  1.,  1., ...,  0.,  0.,  0.])\n```", "```py\ny_train = train_labels.astype(\"float32\")\ny_test = test_labels.astype(\"float32\") \n```", "```py\nimport keras\nfrom keras import layers\n\nmodel = keras.Sequential(\n    [\n        layers.Dense(16, activation=\"relu\"),\n        layers.Dense(16, activation=\"relu\"),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ]\n) \n```", "```py\noutput = relu(dot(input, W) + b) \n```", "```py\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n) \n```", "```py\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:] \n```", "```py\nhistory = model.fit(\n    partial_x_train,\n    partial_y_train,\n    epochs=20,\n    batch_size=512,\n    validation_data=(x_val, y_val),\n) \n```", "```py\n>>> history_dict = history.history\n>>> history_dict.keys()\ndict_keys([\"accuracy\", \"loss\", \"val_accuracy\", \"val_loss\"])\n```", "```py\nimport matplotlib.pyplot as plt\n\nhistory_dict = history.history\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = range(1, len(loss_values) + 1)\n# \"r--\" is for \"dashed red line.\"\nplt.plot(epochs, loss_values, \"r--\", label=\"Training loss\")\n# \"b\" is for \"solid blue line.\"\nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\nplt.title(\"[IMDB] Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.xticks(epochs)\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show() \n```", "```py\n# Clears the figure\nplt.clf()\nacc = history_dict[\"accuracy\"]\nval_acc = history_dict[\"val_accuracy\"]\nplt.plot(epochs, acc, \"r--\", label=\"Training acc\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\nplt.title(\"[IMDB] Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.xticks(epochs)\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show() \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(16, activation=\"relu\"),\n        layers.Dense(16, activation=\"relu\"),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\nresults = model.evaluate(x_test, y_test) \n```", "```py\n>>> results\n# The first number, 0.29, is the test loss, and the second number,\n# 0.88, is the test accuracy.\n[0.2929924130630493, 0.88327999999999995]\n```", "```py\n>>> model.predict(x_test)\narray([[ 0.98006207]\n       [ 0.99758697]\n       [ 0.99975556]\n       ...,\n       [ 0.82167041]\n       [ 0.02885115]\n       [ 0.65371346]], dtype=float32)\n```", "```py\nfrom keras.datasets import reuters\n\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n    num_words=10000\n) \n```", "```py\n>>> len(train_data)\n8982\n>>> len(test_data)\n2246\n```", "```py\n>>> train_data[10]\n[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979,\n3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]\n```", "```py\nword_index = reuters.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_newswire = \" \".join(\n    # The indices are offset by 3 because 0, 1, and 2 are reserved\n    # indices for \"padding,\" \"start of sequence,\" and \"unknown.\"\n    [reverse_word_index.get(i - 3, \"?\") for i in train_data[10]]\n) \n```", "```py\n>>> train_labels[10]\n3\n```", "```py\n# Vectorized training data\nx_train = multi_hot_encode(train_data, num_classes=10000)\n# Vectorized test data\nx_test = multi_hot_encode(test_data, num_classes=10000) \n```", "```py\ndef one_hot_encode(labels, num_classes=46):\n    results = np.zeros((len(labels), num_classes))\n    for i, label in enumerate(labels):\n        results[i, label] = 1.0\n    return results\n\n# Vectorized training labels\ny_train = one_hot_encode(train_labels)\n# Vectorized test labels\ny_test = one_hot_encode(test_labels) \n```", "```py\nfrom keras.utils import to_categorical\n\ny_train = to_categorical(train_labels)\ny_test = to_categorical(test_labels) \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(46, activation=\"softmax\"),\n    ]\n) \n```", "```py\ntop_3_accuracy = keras.metrics.TopKCategoricalAccuracy(\n    k=3, name=\"top_3_accuracy\"\n)\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\", top_3_accuracy],\n) \n```", "```py\nx_val = x_train[:1000]\npartial_x_train = x_train[1000:]\ny_val = y_train[:1000]\npartial_y_train = y_train[1000:] \n```", "```py\nhistory = model.fit(\n    partial_x_train,\n    partial_y_train,\n    epochs=20,\n    batch_size=512,\n    validation_data=(x_val, y_val),\n) \n```", "```py\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, \"r--\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.xticks(epochs)\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show() \n```", "```py\nplt.clf()\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nplt.plot(epochs, acc, \"r--\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.xticks(epochs)\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show() \n```", "```py\nplt.clf()\nacc = history.history[\"top_3_accuracy\"]\nval_acc = history.history[\"val_top_3_accuracy\"]\nplt.plot(epochs, acc, \"r--\", label=\"Training top-3 accuracy\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation top-3 accuracy\")\nplt.title(\"Training and validation top-3 accuracy\")\nplt.xlabel(\"Epochs\")\nplt.xticks(epochs)\nplt.ylabel(\"Top-3 accuracy\")\nplt.legend()\nplt.show() \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(46, activation=\"softmax\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    epochs=9,\n    batch_size=512,\n)\nresults = model.evaluate(x_test, y_test) \n```", "```py\n>>> results\n[0.9565213431445807, 0.79697239536954589]\n```", "```py\n>>> import copy\n>>> test_labels_copy = copy.copy(test_labels)\n>>> np.random.shuffle(test_labels_copy)\n>>> hits_array = np.array(test_labels == test_labels_copy)\n>>> hits_array.mean()\n0.18655387355298308\n```", "```py\npredictions = model.predict(x_test) \n```", "```py\n>>> predictions[0].shape\n(46,)\n```", "```py\n>>> np.sum(predictions[0])\n1.0\n```", "```py\n>>> np.argmax(predictions[0])\n4\n```", "```py\ny_train = train_labels\ny_test = test_labels \n```", "```py\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n) \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(4, activation=\"relu\"),\n        layers.Dense(46, activation=\"softmax\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(\n    partial_x_train,\n    partial_y_train,\n    epochs=20,\n    batch_size=128,\n    validation_data=(x_val, y_val),\n) \n```", "```py\nfrom keras.datasets import california_housing\n\n# Make sure to pass version=\"small\" to get the right dataset.\n(train_data, train_targets), (test_data, test_targets) = (\n    california_housing.load_data(version=\"small\")\n) \n```", "```py\n>>> train_data.shape\n(480, 8)\n>>> test_data.shape\n(120, 8)\n```", "```py\n>>> train_targets\narray([252300., 146900., 290900., ..., 140500., 217100.],\n      dtype=float32)\n```", "```py\nmean = train_data.mean(axis=0)\nstd = train_data.std(axis=0)\nx_train = (train_data - mean) / std\nx_test = (test_data - mean) / std \n```", "```py\ny_train = train_targets / 100000\ny_test = test_targets / 100000 \n```", "```py\ndef get_model():\n    # Because you need to instantiate the same model multiple times,\n    # you use a function to construct it.\n    model = keras.Sequential(\n        [\n            layers.Dense(64, activation=\"relu\"),\n            layers.Dense(64, activation=\"relu\"),\n            layers.Dense(1),\n        ]\n    )\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"mean_squared_error\",\n        metrics=[\"mean_absolute_error\"],\n    )\n    return model \n```", "```py\nk = 4\nnum_val_samples = len(x_train) // k\nnum_epochs = 50\nall_scores = []\nfor i in range(k):\n    print(f\"Processing fold #{i + 1}\")\n    # Prepares the validation data: data from partition #k\n    fold_x_val = x_train[i * num_val_samples : (i + 1) * num_val_samples]\n    fold_y_val = y_train[i * num_val_samples : (i + 1) * num_val_samples]\n    # Prepares the training data: data from all other partitions\n    fold_x_train = np.concatenate(\n        [x_train[: i * num_val_samples], x_train[(i + 1) * num_val_samples :]],\n        axis=0,\n    )\n    fold_y_train = np.concatenate(\n        [y_train[: i * num_val_samples], y_train[(i + 1) * num_val_samples :]],\n        axis=0,\n    )\n    # Builds the Keras model (already compiled)\n    model = get_model()\n    # Trains the model\n    model.fit(\n        fold_x_train,\n        fold_y_train,\n        epochs=num_epochs,\n        batch_size=16,\n        verbose=0,\n    )\n    # Evaluates the model on the validation data\n    scores = model.evaluate(fold_x_val, fold_y_val, verbose=0)\n    val_loss, val_mae = scores\n    all_scores.append(val_mae) \n```", "```py\n>>> [round(value, 3) for value in all_scores]\n[0.298, 0.349, 0.232, 0.305]\n>>> round(np.mean(all_scores), 3)\n0.296\n```", "```py\nk = 4\nnum_val_samples = len(x_train) // k\nnum_epochs = 200\nall_mae_histories = []\nfor i in range(k):\n    print(f\"Processing fold #{i + 1}\")\n    # Prepares the validation data: data from partition #k\n    fold_x_val = x_train[i * num_val_samples : (i + 1) * num_val_samples]\n    fold_y_val = y_train[i * num_val_samples : (i + 1) * num_val_samples]\n    # Prepares the training data: data from all other partitions\n    fold_x_train = np.concatenate(\n        [x_train[: i * num_val_samples], x_train[(i + 1) * num_val_samples :]],\n        axis=0,\n    )\n    fold_y_train = np.concatenate(\n        [y_train[: i * num_val_samples], y_train[(i + 1) * num_val_samples :]],\n        axis=0,\n    )\n    # Builds the Keras model (already compiled)\n    model = get_model()\n    # Trains the model\n    history = model.fit(\n        fold_x_train,\n        fold_y_train,\n        validation_data=(fold_x_val, fold_y_val),\n        epochs=num_epochs,\n        batch_size=16,\n        verbose=0,\n    )\n    mae_history = history.history[\"val_mean_absolute_error\"]\n    all_mae_histories.append(mae_history) \n```", "```py\naverage_mae_history = [\n    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)\n] \n```", "```py\nepochs = range(1, len(average_mae_history) + 1)\nplt.plot(epochs, average_mae_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show() \n```", "```py\ntruncated_mae_history = average_mae_history[10:]\nepochs = range(10, len(truncated_mae_history) + 10)\nplt.plot(epochs, truncated_mae_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show() \n```", "```py\n# Gets a fresh, compiled model\nmodel = get_model()\n# Trains it on the entirety of the data\nmodel.fit(x_train, y_train, epochs=130, batch_size=16, verbose=0)\ntest_mean_squared_error, test_mean_absolute_error = model.evaluate(\n    x_test, y_test\n) \n```", "```py\n>>> round(test_mean_absolute_error, 3)\n0.31\n```", "```py\n>>> predictions = model.predict(x_test)\n>>> predictions[0]\narray([2.834494], dtype=float32)\n```"]