- en: Chapter 16\. Natural Language Processing with RNNs and Attention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章。使用RNN和注意力进行自然语言处理
- en: 'When Alan Turing imagined his famous [Turing test](https://homl.info/turingtest)⁠^([1](ch16.html#idm45720177608176))
    in 1950, he proposed a way to evaluate a machine’s ability to match human intelligence.
    He could have tested for many things, such as the ability to recognize cats in
    pictures, play chess, compose music, or escape a maze, but, interestingly, he
    chose a linguistic task. More specifically, he devised a *chatbot* capable of
    fooling its interlocutor into thinking it was human.⁠^([2](ch16.html#idm45720177605632))
    This test does have its weaknesses: a set of hardcoded rules can fool unsuspecting
    or naive humans (e.g., the machine could give vague predefined answers in response
    to some keywords, it could pretend that it is joking or drunk to get a pass on
    its weirdest answers, or it could escape difficult questions by answering them
    with its own questions), and many aspects of human intelligence are utterly ignored
    (e.g., the ability to interpret nonverbal communication such as facial expressions,
    or to learn a manual task). But the test does highlight the fact that mastering
    language is arguably *Homo sapiens*’s greatest cognitive ability.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当艾伦·图灵在1950年想象他著名的[Turing测试](https://homl.info/turingtest)时，他提出了一种评估机器匹配人类智能能力的方法。他本可以测试许多事情，比如识别图片中的猫、下棋、创作音乐或逃离迷宫，但有趣的是，他选择了一项语言任务。更具体地说，他设计了一个*聊天机器人*，能够愚弄对话者以为它是人类。这个测试确实有其弱点：一组硬编码规则可以愚弄毫无戒心或天真的人类（例如，机器可以对某些关键词给出模糊的预定义答案，可以假装在回答一些最奇怪的问题时开玩笑或喝醉，或者可以通过用自己的问题回答难题来逃避困难的问题），并且许多人类智能的方面完全被忽视（例如，解释非言语交流，如面部表情，或学习手动任务的能力）。但这个测试确实突显了掌握语言可能是*智人*最伟大的认知能力。
- en: Can we build a machine that can master written and spoken language? This is
    the ultimate goal of NLP research, but it’s a bit too broad, so in practice researchers
    focus on more specific tasks, such as text classification, translation, summarization,
    question answering, and many more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否构建一台能够掌握书面和口头语言的机器？这是自然语言处理研究的终极目标，但实际上研究人员更专注于更具体的任务，比如文本分类、翻译、摘要、问答等等。
- en: A common approach for natural language tasks is to use recurrent neural networks.
    We will therefore continue to explore RNNs (introduced in [Chapter 15](ch15.html#rnn_chapter)),
    starting with a *character RNN*, or *char-RNN*, trained to predict the next character
    in a sentence. This will allow us to generate some original text. We will first
    use a *stateless RNN* (which learns on random portions of text at each iteration,
    without any information on the rest of the text), then we will build a *stateful
    RNN* (which preserves the hidden state between training iterations and continues
    reading where it left off, allowing it to learn longer patterns). Next, we will
    build an RNN to perform sentiment analysis (e.g., reading movie reviews and extracting
    the rater’s feeling about the movie), this time treating sentences as sequences
    of words, rather than characters. Then we will show how RNNs can be used to build
    an encoder–decoder architecture capable of performing neural machine translation
    (NMT), translating English to Spanish.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言任务的一种常见方法是使用循环神经网络。因此，我们将继续探索循环神经网络（在[第15章](ch15.html#rnn_chapter)中介绍），首先是*字符RNN*或*char-RNN*，训练以预测句子中的下一个字符。这将使我们能够生成一些原创文本。我们将首先使用*无状态RNN*（在每次迭代中学习文本的随机部分，没有关于文本其余部分的信息），然后我们将构建*有状态RNN*（在训练迭代之间保留隐藏状态，并继续阅读离开的地方，使其能够学习更长的模式）。接下来，我们将构建一个RNN来执行情感分析（例如，阅读电影评论并提取评价者对电影的感受），这次将句子视为单词序列，而不是字符。然后我们将展示如何使用RNN来构建一个编码器-解码器架构，能够执行神经机器翻译（NMT），将英语翻译成西班牙语。
- en: In the second part of this chapter, we will explore *attention mechanisms*.
    As their name suggests, these are neural network components that learn to select
    the part of the inputs that the rest of the model should focus on at each time
    step. First, we will boost the performance of an RNN-based encoder–decoder architecture
    using attention. Next, we will drop RNNs altogether and use a very successful
    attention-only architecture, called the *transformer*, to build a translation
    model. We will then discuss some of the most important advances in NLP in the
    last few years, including incredibly powerful language models such as GPT and
    BERT, both based on transformers. Lastly, I will show you how to get started with
    the excellent Transformers library by Hugging Face.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们将探索*注意机制*。正如它们的名字所示，这些是神经网络组件，它们学习选择模型在每个时间步应该关注的输入部分。首先，我们将通过注意机制提高基于RNN的编码器-解码器架构的性能。接下来，我们将完全放弃RNN，并使用一个非常成功的仅注意架构，称为*transformer*，来构建一个翻译模型。然后，我们将讨论过去几年自然语言处理中一些最重要的进展，包括基于transformer的GPT和BERT等非常强大的语言模型。最后，我将向您展示如何开始使用Hugging
    Face出色的Transformers库。
- en: Let’s start with a simple and fun model that can write like Shakespeare (sort
    of).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单而有趣的模型开始，这个模型可以像莎士比亚一样写作（某种程度上）。
- en: Generating Shakespearean Text Using a Character RNN
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用字符RNN生成莎士比亚文本
- en: 'In a famous [2015 blog post](https://homl.info/charrnn) titled “The Unreasonable
    Effectiveness of Recurrent Neural Networks”, Andrej Karpathy showed how to train
    an RNN to predict the next character in a sentence. This *char-RNN* can then be
    used to generate novel text, one character at a time. Here is a small sample of
    the text generated by a char-RNN model after it was trained on all of Shakespeare’s
    works:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇著名的[2015年博客文章](https://homl.info/charrnn)中，安德烈·卡帕西展示了如何训练一个RNN来预测句子中的下一个字符。然后可以使用这个*char-RNN*逐个字符生成新文本。以下是一个经过训练所有莎士比亚作品后由char-RNN模型生成的文本的小样本：
- en: 'PANDARUS:'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 潘达鲁斯：
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alas, I think he shall be come approached and the day
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 唉，我想他将会被接近并且这一天
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When little srain would be attain’d into being never fed,
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当一点点智慧被获得而从未被喂养时，
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And who is but a chain and subjects of his death,
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 而谁不是一条链，是他死亡的主题，
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I should not sleep.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我不应该睡觉。
- en: Not exactly a masterpiece, but it is still impressive that the model was able
    to learn words, grammar, proper punctuation, and more, just by learning to predict
    the next character in a sentence. This is our first example of a *language model*;
    similar (but much more powerful) language models, discussed later in this chapter,
    are at the core of modern NLP. In the remainder of this section we’ll build a
    char-RNN step by step, starting with the creation of the dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不是杰作，但仍然令人印象深刻，模型能够学习单词、语法、正确的标点符号等，只是通过学习预测句子中的下一个字符。这是我们的第一个*语言模型*示例；本章后面讨论的类似（但更强大）的语言模型是现代自然语言处理的核心。在本节的其余部分，我们将逐步构建一个
    char-RNN，从创建数据集开始。
- en: Creating the Training Dataset
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练数据集
- en: 'First, using Keras’s handy `tf.keras.utils.get_file()` function, let’s download
    all of Shakespeare’s works. The data is loaded from Andrej Karpathy’s [char-rnn
    project](https://github.com/karpathy/char-rnn):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用Keras方便的 `tf.keras.utils.get_file()` 函数，让我们下载所有莎士比亚的作品。数据是从Andrej Karpathy的[char-rnn项目](https://github.com/karpathy/char-rnn)加载的：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s print the first few lines:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印前几行：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Looks like Shakespeare all right!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来像是莎士比亚的作品！
- en: 'Next, we’ll use a `tf.keras.layers.TextVectorization` layer (introduced in
    [Chapter 13](ch13.html#data_chapter)) to encode this text. We set `split="character"`
    to get character-level encoding rather than the default word-level encoding, and
    we use `standardize="lower"` to convert the text to lowercase (which will simplify
    the task):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `tf.keras.layers.TextVectorization` 层（在[第13章](ch13.html#data_chapter)介绍）对此文本进行编码。我们设置
    `split="character"` 以获得字符级别的编码，而不是默认的单词级别编码，并且我们使用 `standardize="lower"` 将文本转换为小写（这将简化任务）：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Each character is now mapped to an integer, starting at 2\. The `TextVectorization`
    layer reserved the value 0 for padding tokens, and it reserved 1 for unknown characters.
    We won’t need either of these tokens for now, so let’s subtract 2 from the character
    IDs and compute the number of distinct characters and the total number of characters:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个字符都映射到一个整数，从2开始。`TextVectorization` 层将值0保留给填充标记，将值1保留给未知字符。目前我们不需要这两个标记，所以让我们从字符ID中减去2，并计算不同字符的数量和总字符数：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, just like we did in [Chapter 15](ch15.html#rnn_chapter), we can turn
    this very long sequence into a dataset of windows that we can then use to train
    a sequence-to-sequence RNN. The targets will be similar to the inputs, but shifted
    by one time step into the “future”. For example, one sample in the dataset may
    be a sequence of character IDs representing the text “to be or not to b” (without
    the final “e”), and the corresponding target—a sequence of character IDs representing
    the text “o be or not to be” (with the final “e”, but without the leading “t”).
    Let’s write a small utility function to convert a long sequence of character IDs
    into a dataset of input/target window pairs:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，就像我们在[第15章](ch15.html#rnn_chapter)中所做的那样，我们可以将这个非常长的序列转换为一个窗口的数据集，然后用它来训练一个序列到序列的RNN。目标将类似于输入，但是向“未来”移动了一个时间步。例如，数据集中的一个样本可能是代表文本“to
    be or not to b”（没有最后的“e”）的字符ID序列，相应的目标是代表文本“o be or not to be”（有最后的“e”，但没有开头的“t”）的字符ID序列。让我们编写一个小型实用函数，将字符ID的长序列转换为输入/目标窗口对的数据集：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This function starts much like the `to_windows()` custom utility function we
    created in [Chapter 15](ch15.html#rnn_chapter):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数开始得很像我们在[第15章](ch15.html#rnn_chapter)中创建的 `to_windows()` 自定义实用函数：
- en: It takes a sequence as input (i.e., the encoded text), and creates a dataset
    containing all the windows of the desired length.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它以一个序列作为输入（即编码文本），并创建一个包含所需长度的所有窗口的数据集。
- en: It increases the length by one, since we need the next character for the target.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将长度增加一，因为我们需要下一个字符作为目标。
- en: Then, it shuffles the windows (optionally), batches them, splits them into input/output
    pairs, and activates prefetching.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，它会对窗口进行洗牌（可选），将它们分批处理，拆分为输入/输出对，并激活预取。
- en: '[Figure 16-1](#window_dataset_diagram) summarizes the dataset preparation steps:
    it shows windows of length 11, and a batch size of 3\. The start index of each
    window is indicated next to it.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 16-1](#window_dataset_diagram) 总结了数据集准备步骤：它展示了长度为11的窗口，批量大小为3。每个窗口的起始索引在其旁边标出。'
- en: '![mls3 1601](assets/mls3_1601.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1601](assets/mls3_1601.png)'
- en: Figure 16-1\. Preparing a dataset of shuffled windows
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-1\. 准备一个洗牌窗口的数据集
- en: 'Now we’re ready to create the training set, the validation set, and the test
    set. We will use roughly 90% of the text for training, 5% for validation, and
    5% for testing:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备创建训练集、验证集和测试集。我们将大约使用文本的90%进行训练，5%用于验证，5%用于测试：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'We set the window length to 100, but you can try tuning it: it’s easier and
    faster to train RNNs on shorter input sequences, but the RNN will not be able
    to learn any pattern longer than `length`, so don’t make it too small.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将窗口长度设置为100，但您可以尝试调整它：在较短的输入序列上训练RNN更容易更快，但RNN将无法学习任何长于 `length` 的模式，所以不要将其设置得太小。
- en: That’s it! Preparing the dataset was the hardest part. Now let’s create the
    model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！准备数据集是最困难的部分。现在让我们创建模型。
- en: Building and Training the Char-RNN Model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和训练 Char-RNN 模型
- en: 'Since our dataset is reasonably large, and modeling language is quite a difficult
    task, we need more than a simple RNN with a few recurrent neurons. Let’s build
    and train a model with one `GRU` layer composed of 128 units (you can try tweaking
    the number of layers and units later, if needed):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集相当大，而建模语言是一个相当困难的任务，我们需要不止一个简单的具有几个循环神经元的RNN。让我们构建并训练一个由128个单元组成的 `GRU`
    层的模型（如果需要，稍后可以尝试调整层数和单元数）：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s go over this code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看这段代码：
- en: We use an `Embedding` layer as the first layer, to encode the character IDs
    (embeddings were introduced in [Chapter 13](ch13.html#data_chapter)). The `Embedding`
    layer’s number of input dimensions is the number of distinct character IDs, and
    the number of output dimensions is a hyperparameter you can tune—we’ll set it
    to 16 for now. Whereas the inputs of the `Embedding` layer will be 2D tensors
    of shape [*batch size*, *window length*], the output of the `Embedding` layer
    will be a 3D tensor of shape [*batch size*, *window length*, *embedding size*].
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用一个`Embedding`层作为第一层，用于编码字符ID（嵌入在[第13章](ch13.html#data_chapter)中介绍）。`Embedding`层的输入维度数是不同字符ID的数量，输出维度数是一个可以调整的超参数，我们暂时将其设置为16。`Embedding`层的输入将是形状为[*批量大小*,
    *窗口长度*]的2D张量，`Embedding`层的输出将是形状为[*批量大小*, *窗口长度*, *嵌入大小*]的3D张量。
- en: 'We use a `Dense` layer for the output layer: it must have 39 units (`n_tokens`)
    because there are 39 distinct characters in the text, and we want to output a
    probability for each possible character (at each time step). The 39 output probabilities
    should sum up to 1 at each time step, so we apply the softmax activation function
    to the outputs of the `Dense` layer.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用一个`Dense`层作为输出层：它必须有39个单元（`n_tokens`），因为文本中有39个不同的字符，并且我们希望在每个时间步输出每个可能字符的概率。39个输出概率应该在每个时间步加起来为1，因此我们将softmax激活函数应用于`Dense`层的输出。
- en: Lastly, we compile this model, using the `"sparse_categorical_crossentropy"`
    loss and a Nadam optimizer, and we train the model for several epochs,^([3](ch16.html#idm45720176925824))
    using a `ModelCheckpoint` callback to save the best model (in terms of validation
    accuracy) as training progresses.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们编译这个模型，使用`"sparse_categorical_crossentropy"`损失和Nadam优化器，然后训练模型多个epoch，使用`ModelCheckpoint`回调来保存训练过程中验证准确性最好的模型。
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you are running this code on Colab with a GPU activated, then training should
    take roughly one to two hours. You can reduce the number of epochs if you don’t
    want to wait that long, but of course the model’s accuracy will probably be lower.
    If the Colab session times out, make sure to reconnect quickly, or else the Colab
    runtime will be destroyed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在启用GPU的Colab上运行此代码，则训练大约需要一到两个小时。如果您不想等待那么长时间，可以减少epoch的数量，但当然模型的准确性可能会降低。如果Colab会话超时，请确保快速重新连接，否则Colab运行时将被销毁。
- en: 'This model does not handle text preprocessing, so let’s wrap it in a final
    model containing the `tf.keras.layers.TextVectorization` layer as the first layer,
    plus a `tf.keras.layers.Lambda` layer to subtract 2 from the character IDs since
    we’re not using the padding and unknown tokens for now:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型不处理文本预处理，所以让我们将其包装在一个最终模型中，包含`tf.keras.layers.TextVectorization`层作为第一层，加上一个`tf.keras.layers.Lambda`层，从字符ID中减去2，因为我们暂时不使用填充和未知标记：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And now let’s use it to predict the next character in a sentence:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用它来预测句子中的下一个字符：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Great, the model correctly predicted the next character. Now let’s use this
    model to pretend we’re Shakespeare!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，模型正确预测了下一个字符。现在让我们使用这个模型假装我们是莎士比亚！
- en: Generating Fake Shakespearean Text
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成虚假的莎士比亚文本
- en: 'To generate new text using the char-RNN model, we could feed it some text,
    make the model predict the most likely next letter, add it to the end of the text,
    then give the extended text to the model to guess the next letter, and so on.
    This is called *greedy decoding*. But in practice this often leads to the same
    words being repeated over and over again. Instead, we can sample the next character
    randomly, with a probability equal to the estimated probability, using TensorFlow’s
    `tf.random.categorical()` function. This will generate more diverse and interesting
    text. The `categorical()` function samples random class indices, given the class
    log probabilities (logits). For example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用char-RNN模型生成新文本时，我们可以将一些文本输入模型，让模型预测最有可能的下一个字母，将其添加到文本末尾，然后将扩展后的文本提供给模型猜测下一个字母，依此类推。这被称为*贪婪解码*。但在实践中，这经常导致相同的单词一遍又一遍地重复。相反，我们可以随机采样下一个字符，概率等于估计的概率，使用TensorFlow的`tf.random.categorical()`函数。这将生成更多样化和有趣的文本。`categorical()`函数会根据类别对数概率（logits）随机采样随机类别索引。例如：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To have more control over the diversity of the generated text, we can divide
    the logits by a number called the *temperature*, which we can tweak as we wish.
    A temperature close to zero favors high-probability characters, while a high temperature
    gives all characters an equal probability. Lower temperatures are typically preferred
    when generating fairly rigid and precise text, such as mathematical equations,
    while higher temperatures are preferred when generating more diverse and creative
    text. The following `next_char()` custom helper function uses this approach to
    pick the next character to add to the input text:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地控制生成文本的多样性，我们可以将logits除以一个称为*温度*的数字，我们可以根据需要进行调整。接近零的温度偏好高概率字符，而高温度使所有字符具有相等的概率。在生成相对严格和精确的文本（如数学方程式）时，通常更喜欢较低的温度，而在生成更多样化和创意性的文本时，更喜欢较高的温度。以下`next_char()`自定义辅助函数使用这种方法选择要添加到输入文本中的下一个字符：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we can write another small helper function that will repeatedly call
    `next_char()` to get the next character and append it to the given text:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以编写另一个小的辅助函数，它将重复调用`next_char()`以获取下一个字符并将其附加到给定的文本中：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We are now ready to generate some text! Let’s try with different temperature
    values:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备生成一些文本！让我们尝试不同的温度值：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Shakespeare seems to be suffering from a heatwave. To generate more convincing
    text, a common technique is to sample only from the top *k* characters, or only
    from the smallest set of top characters whose total probability exceeds some threshold
    (this is called *nucleus sampling*). Alternatively, you could try using *beam
    search*, which we will discuss later in this chapter, or using more `GRU` layers
    and more neurons per layer, training for longer, and adding some regularization
    if needed. Also note that the model is currently incapable of learning patterns
    longer than `length`, which is just 100 characters. You could try making this
    window larger, but it will also make training harder, and even LSTM and GRU cells
    cannot handle very long sequences. An alternative approach is to use a stateful
    RNN.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 莎士比亚似乎正在遭受一场热浪。为了生成更具说服力的文本，一个常见的技术是仅从前 *k* 个字符中采样，或者仅从总概率超过某个阈值的最小一组顶级字符中采样（这被称为*核心采样*）。另外，您可以尝试使用*波束搜索*，我们将在本章后面讨论，或者使用更多的`GRU`层和更多的神经元每层，训练更长时间，并在需要时添加一些正则化。还要注意，模型目前无法学习比`length`更长的模式，`length`只是100个字符。您可以尝试将此窗口扩大，但这也会使训练更加困难，即使LSTM和GRU单元也无法处理非常长的序列。另一种替代方法是使用有状态的RNN。
- en: Stateful RNN
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有状态的RNN
- en: 'Until now, we have only used *stateless RNNs*: at each training iteration the
    model starts with a hidden state full of zeros, then it updates this state at
    each time step, and after the last time step, it throws it away as it is not needed
    anymore. What if we instructed the RNN to preserve this final state after processing
    a training batch and use it as the initial state for the next training batch?
    This way the model could learn long-term patterns despite only backpropagating
    through short sequences. This is called a *stateful RNN*. Let’s go over how to
    build one.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了*无状态的RNN*：在每次训练迭代中，模型从一个全零的隐藏状态开始，然后在每个时间步更新这个状态，在最后一个时间步之后，将其丢弃，因为不再需要。如果我们指示RNN在处理训练批次后保留这个最终状态，并将其用作下一个训练批次的初始状态，会怎样呢？这样，模型可以学习长期模式，尽管只通过短序列进行反向传播。这被称为*有状态的RNN*。让我们看看如何构建一个。
- en: First, note that a stateful RNN only makes sense if each input sequence in a
    batch starts exactly where the corresponding sequence in the previous batch left
    off. So the first thing we need to do to build a stateful RNN is to use sequential
    and nonoverlapping input sequences (rather than the shuffled and overlapping sequences
    we used to train stateless RNNs). When creating the `tf.data.Dataset`, we must
    therefore use `shift=length` (instead of `shift=1`) when calling the `window()`
    method. Moreover, we must *not* call the `shuffle()` method.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意到有状态的RNN只有在批次中的每个输入序列从上一个批次中对应序列的确切位置开始时才有意义。因此，我们构建有状态的RNN需要做的第一件事是使用顺序且不重叠的输入序列（而不是我们用来训练无状态RNN的洗牌和重叠序列）。在创建`tf.data.Dataset`时，因此在调用`window()`方法时必须使用`shift=length`（而不是`shift=1`）。此外，我们必须*不*调用`shuffle()`方法。
- en: 'Unfortunately, batching is much harder when preparing a dataset for a stateful
    RNN than it is for a stateless RNN. Indeed, if we were to call `batch(32)`, then
    32 consecutive windows would be put in the same batch, and the following batch
    would not continue each of these windows where it left off. The first batch would
    contain windows 1 to 32 and the second batch would contain windows 33 to 64, so
    if you consider, say, the first window of each batch (i.e., windows 1 and 33),
    you can see that they are not consecutive. The simplest solution to this problem
    is to just use a batch size of 1\. The following `to_dataset_for_stateful_rnn()`
    custom utility function uses this strategy to prepare a dataset for a stateful
    RNN:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，为有状态的RNN准备数据集时，批处理比为无状态的RNN更加困难。实际上，如果我们调用`batch(32)`，那么32个连续窗口将被放入同一个批次中，接下来的批次将不会继续每个窗口的位置。第一个批次将包含窗口1到32，第二个批次将包含窗口33到64，因此如果您考虑，比如说，每个批次的第一个窗口（即窗口1和33），您会发现它们不是连续的。这个问题的最简单解决方案就是只使用批量大小为1。以下的`to_dataset_for_stateful_rnn()`自定义实用函数使用这种策略来为有状态的RNN准备数据集：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Figure 16-2](#stateful_rnn_dataset_diagram) summarizes the main steps of this
    function.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-2](#stateful_rnn_dataset_diagram)总结了这个函数的主要步骤。'
- en: '![mls3 1602](assets/mls3_1602.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1602](assets/mls3_1602.png)'
- en: Figure 16-2\. Preparing a dataset of consecutive sequence fragments for a stateful
    RNN
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-2。为有状态的RNN准备连续序列片段的数据集
- en: 'Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s
    text into 32 texts of equal length, create one dataset of consecutive input sequences
    for each of them, and finally use `tf.data.Dataset.zip(datasets).map(lambda *windows:
    tf.stack(windows))` to create proper consecutive batches, where the *n*^(th) input
    sequence in a batch starts off exactly where the *n*^(th) input sequence ended
    in the previous batch (see the notebook for the full code).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '批处理更加困难，但并非不可能。例如，我们可以将莎士比亚的文本分成32个等长的文本，为每个文本创建一个连续输入序列的数据集，最后使用`tf.data.Dataset.zip(datasets).map(lambda
    *windows: tf.stack(windows))`来创建正确的连续批次，其中批次中的第*n*个输入序列从上一个批次中的第*n*个输入序列结束的地方开始（请参阅笔记本获取完整代码）。'
- en: 'Now, let’s create the stateful RNN. We need to set the `stateful` argument
    to `True` when creating each recurrent layer, and because the stateful RNN needs
    to know the batch size (since it will preserve a state for each input sequence
    in the batch). Therefore we must set the `batch_input_shape` argument in the first
    layer. Note that we can leave the second dimension unspecified, since the input
    sequences could have any length:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建有状态的RNN。在创建每个循环层时，我们需要将`stateful`参数设置为`True`，因为有状态的RNN需要知道批量大小（因为它将为批次中的每个输入序列保留一个状态）。因此，我们必须在第一层中设置`batch_input_shape`参数。请注意，我们可以将第二维度留空，因为输入序列可以具有任意长度：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'At the end of each epoch, we need to reset the states before we go back to
    the beginning of the text. For this, we can use a small custom Keras callback:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时期结束时，我们需要在回到文本开头之前重置状态。为此，我们可以使用一个小的自定义Keras回调：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And now we can compile the model and train it using our callback:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编译模型并使用我们的回调函数进行训练：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Tip
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: After this model is trained, it will only be possible to use it to make predictions
    for batches of the same size as were used during training. To avoid this restriction,
    create an identical *stateless* model, and copy the stateful model’s weights to
    this model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完这个模型后，只能用它来对与训练时相同大小的批次进行预测。为了避免这个限制，创建一个相同的*无状态*模型，并将有状态模型的权重复制到这个模型中。
- en: 'Interestingly, although a char-RNN model is just trained to predict the next
    character, this seemingly simple task actually requires it to learn some higher-level
    tasks as well. For example, to find the next character after “Great movie, I really”,
    it’s helpful to understand that the sentence is positive, so what follows is more
    likely to be the letter “l” (for “loved”) rather than “h” (for “hated”). In fact,
    a [2017 paper](https://homl.info/sentimentneuron)⁠^([4](ch16.html#idm45720176103360))
    by Alec Radford and other OpenAI researchers describes how the authors trained
    a big char-RNN-like model on a large dataset, and found that one of the neurons
    acted as an excellent sentiment analysis classifier: although the model was trained
    without any labels, the *sentiment neuron*—as they called it—reached state-of-the-art
    performance on sentiment analysis benchmarks. This foreshadowed and motivated
    unsupervised pretraining in NLP.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管char-RNN模型只是训练来预测下一个字符，但这看似简单的任务实际上也需要它学习一些更高级的任务。例如，要找到“Great movie,
    I really”之后的下一个字符，了解到这句话是积极的是有帮助的，所以下一个字符更有可能是“l”（代表“loved”）而不是“h”（代表“hated”）。事实上，OpenAI的Alec
    Radford和其他研究人员在一篇2017年的论文中描述了他们如何在大型数据集上训练了一个类似于大型char-RNN模型，并发现其中一个神经元表现出色地作为情感分析分类器：尽管该模型在没有任何标签的情况下进行了训练，但他们称之为*情感神经元*达到了情感分析基准测试的最新性能。这预示并激励了NLP中的无监督预训练。
- en: But before we explore unsupervised pretraining, let’s turn our attention to
    word-level models and how to use them in a supervised fashion for sentiment analysis.
    In the process, you will learn how to handle sequences of variable lengths using
    masking.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但在探索无监督预训练之前，让我们将注意力转向单词级模型以及如何在监督方式下用它们进行情感分析。在这个过程中，您将学习如何使用掩码处理可变长度的序列。
- en: Sentiment Analysis
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: 'Generating text can be fun and instructive, but in real-life projects, one
    of the most common applications of NLP is text classification—especially sentiment
    analysis. If image classification on the MNIST dataset is the “Hello world!” of
    computer vision, then sentiment analysis on the IMDb reviews dataset is the “Hello
    world!” of natural language processing. The IMDb dataset consists of 50,000 movie
    reviews in English (25,000 for training, 25,000 for testing) extracted from the
    famous [Internet Movie Database](https://imdb.com), along with a simple binary
    target for each review indicating whether it is negative (0) or positive (1).
    Just like MNIST, the IMDb reviews dataset is popular for good reasons: it is simple
    enough to be tackled on a laptop in a reasonable amount of time, but challenging
    enough to be fun and rewarding.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成文本可能很有趣且有教育意义，但在实际项目中，自然语言处理的最常见应用之一是文本分类，尤其是情感分析。如果在MNIST数据集上进行图像分类是计算机视觉的“Hello
    world！”，那么在IMDb评论数据集上进行情感分析就是自然语言处理的“Hello world！”。IMDb数据集包含了来自著名的[互联网电影数据库](https://imdb.com)的50,000条英文电影评论（25,000条用于训练，25,000条用于测试），每条评论都有一个简单的二进制目标，表示其是否为负面（0）或正面（1）。就像MNIST一样，IMDb评论数据集之所以受欢迎是有充分理由的：它足够简单，可以在笔记本电脑上在合理的时间内处理，但足够具有挑战性和有趣。
- en: 'Let’s load the IMDb dataset using the TensorFlow Datasets library (introduced
    in [Chapter 13](ch13.html#data_chapter)). We’ll use the first 90% of the training
    set for training, and the remaining 10% for validation:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用TensorFlow Datasets库加载IMDb数据集（在[第13章](ch13.html#data_chapter)中介绍）。我们将使用训练集的前90%进行训练，剩下的10%用于验证：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Tip
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Keras also includes a function for loading the IMDb dataset, if you prefer:
    `tf.keras.datasets.imdb.load_data()`. The reviews are already preprocessed as
    sequences of word IDs.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您愿意，Keras还包括一个用于加载IMDb数据集的函数：`tf.keras.datasets.imdb.load_data()`。评论已经被预处理为单词ID的序列。
- en: 'Let’s inspect a few reviews:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一些评论：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Some reviews are easy to classify. For example, the first review includes the
    words “terrible movie” in the very first sentence. But in many cases things are
    not that simple. For example, the third review starts off positively, even though
    it’s ultimately a negative review (label 0).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有些评论很容易分类。例如，第一条评论中的第一句话包含“terrible movie”这几个词。但在许多情况下，事情并不那么简单。例如，第三条评论一开始是积极的，尽管最终是一个负面评论（标签0）。
- en: 'To build a model for this task, we need to preprocess the text, but this time
    we will chop it into words instead of characters. For this, we can use the `tf.keras.​lay⁠ers.TextVectorization`
    layer again. Note that it uses spaces to identify word boundaries, which will
    not work well in some languages. For example, Chinese writing does not use spaces
    between words, Vietnamese uses spaces even within words, and German often attaches
    multiple words together, without spaces. Even in English, spaces are not always
    the best way to tokenize text: think of “San Francisco” or “#ILoveDeepLearning”.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为这个任务构建一个模型，我们需要预处理文本，但这次我们将其分成单词而不是字符。为此，我们可以再次使用`tf.keras.​lay⁠ers.TextVectorization`层。请注意，它使用空格来识别单词边界，在某些语言中可能效果不佳。例如，中文书写不使用单词之间的空格，越南语甚至在单词内部也使用空格，德语经常将多个单词连接在一起，没有空格。即使在英语中，空格也不总是分词的最佳方式：想想“San
    Francisco”或“#ILoveDeepLearning”。
- en: Fortunately, there are solutions to address these issues. In a [2016 paper](https://homl.info/rarewords),⁠^([5](ch16.html#idm45720175873872))
    Rico Sennrich et al. from the University of Edinburgh explored several methods
    to tokenize and detokenize text at the subword level. This way, even if your model
    encounters a rare word it has never seen before, it can still reasonably guess
    what it means. For example, even if the model never saw the word “smartest” during
    training, if it learned the word “smart” and it also learned that the suffix “est”
    means “the most”, it can infer the meaning of “smartest”. One of the techniques
    the authors evaluated is *byte pair encoding* (BPE). BPE works by splitting the
    whole training set into individual characters (including spaces), then repeatedly
    merging the most frequent adjacent pairs until the vocabulary reaches the desired
    size.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有解决这些问题的解决方案。在[2016年的一篇论文](https://homl.info/rarewords)，爱丁堡大学的Rico Sennrich等人探索了几种在子词级别对文本进行标记和去标记化的方法。这样，即使您的模型遇到了以前从未见过的罕见单词，它仍然可以合理地猜测它的含义。例如，即使模型在训练期间从未见过单词“smartest”，如果它学会了单词“smart”并且还学会了后缀“est”表示“最”，它可以推断出“smartest”的含义。作者评估的技术之一是*字节对编码*（BPE）。BPE通过将整个训练集拆分为单个字符（包括空格），然后重复合并最频繁的相邻对，直到词汇表达到所需大小。
- en: 'A [2018 paper](https://homl.info/subword)⁠^([6](ch16.html#idm45720175847312))
    by Taku Kudo at Google further improved subword tokenization, often removing the
    need for language-specific preprocessing prior to tokenization. Moreover, the
    paper proposed a novel regularization technique called *subword regularization*,
    which improves accuracy and robustness by introducing some randomness in tokenization
    during training: for example, “New England” may be tokenized as “New” + “England”,
    or “New” + “Eng” + “land”, or simply “New England” (just one token). Google’s
    [*SentencePiece*](https://github.com/google/sentencepiece) project provides an
    open source implementation, which is described in a [paper](https://homl.info/sentencepiece)⁠^([7](ch16.html#idm45720175842096))
    by Taku Kudo and John Richardson.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Google的Taku Kudo在2018年发表的一篇论文进一步改进了子词标记化，通常消除了标记化之前需要进行特定于语言的预处理的需要。此外，该论文提出了一种称为*子词正则化*的新型正则化技术，通过在训练期间在标记化中引入一些随机性来提高准确性和稳健性：例如，“New
    England”可以被标记为“New”+“England”，或“New”+“Eng”+“land”，或简单地“New England”（只有一个标记）。Google的[*SentencePiece*](https://github.com/google/sentencepiece)项目提供了一个开源实现，该实现在Taku
    Kudo和John Richardson的一篇[论文](https://homl.info/sentencepiece)中有描述。
- en: The [TensorFlow Text](https://homl.info/tftext) library also implements various
    tokenization strategies, including [WordPiece](https://homl.info/wordpiece)⁠^([8](ch16.html#idm45720175839056))
    (a variant of BPE), and last but not least, the [Tokenizers library by Hugging
    Face](https://homl.info/tokenizers) implements a wide range of extremely fast
    tokenizers.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorFlow Text](https://homl.info/tftext)库还实现了各种标记化策略，包括[WordPiece](https://homl.info/wordpiece)（BPE的变体），最后但同样重要的是，[Hugging
    Face的Tokenizers库](https://homl.info/tokenizers)实现了一系列极快的标记化器。'
- en: 'However, for the IMDb task in English, using spaces for token boundaries should
    be good enough. So let’s go ahead with creating a `TextVectorization` layer and
    adapting it to the training set. We will limit the vocabulary to 1,000 tokens,
    including the most frequent 998 words plus a padding token and a token for unknown
    words, since it’s unlikely that very rare words will be important for this task,
    and limiting the vocabulary size will reduce the number of parameters the model
    needs to learn:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于英语中的IMDb任务，使用空格作为标记边界应该足够好。因此，让我们继续创建一个`TextVectorization`层，并将其调整到训练集。我们将词汇表限制为1,000个标记，包括最常见的998个单词以及一个填充标记和一个未知单词的标记，因为很少见的单词不太可能对这个任务很重要，并且限制词汇表大小将减少模型需要学习的参数数量：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we can create the model and train it:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以创建模型并训练它：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The first layer is the `TextVectorization` layer we just prepared, followed
    by an `Embedding` layer that will convert word IDs into embeddings. The embedding
    matrix needs to have one row per token in the vocabulary (`vocab_size`) and one
    column per embedding dimension (this example uses 128 dimensions, but this is
    a hyperparameter you could tune). Next we use a `GRU` layer and a `Dense` layer
    with a single neuron and the sigmoid activation function, since this is a binary
    classification task: the model’s output will be the estimated probability that
    the review expresses a positive sentiment regarding the movie. We then compile
    the model, and we fit it on the dataset we prepared earlier for a couple of epochs
    (or you can train for longer to get better results).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是我们刚刚准备的`TextVectorization`层，接着是一个`Embedding`层，将单词ID转换为嵌入。嵌入矩阵需要每个词汇表中的标记一行（`vocab_size`），每个嵌入维度一列（此示例使用128维，但这是一个可以调整的超参数）。接下来我们使用一个`GRU`层和一个具有单个神经元和sigmoid激活函数的`Dense`层，因为这是一个二元分类任务：模型的输出将是评论表达对电影积极情绪的估计概率。然后我们编译模型，并在我们之前准备的数据集上进行几个时期的拟合（或者您可以训练更长时间以获得更好的结果）。
- en: 'Sadly, if you run this code, you will generally find that the model fails to
    learn anything at all: the accuracy remains close to 50%, no better than random
    chance. Why is that? The reviews have different lengths, so when the `TextVectorization`
    layer converts them to sequences of token IDs, it pads the shorter sequences using
    the padding token (with ID 0) to make them as long as the longest sequence in
    the batch. As a result, most sequences end with many padding tokens—often dozens
    or even hundreds of them. Even though we’re using a `GRU` layer, which is much
    better than a `SimpleRNN` layer, its short-term memory is still not great, so
    when it goes through many padding tokens, it ends up forgetting what the review
    was about! One solution is to feed the model with batches of equal-length sentences
    (which also speeds up training). Another solution is to make the RNN ignore the
    padding tokens. This can be done using masking.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，如果运行此代码，通常会发现模型根本无法学习任何东西：准确率保持接近50%，不比随机机会好。为什么呢？评论的长度不同，因此当`TextVectorization`层将它们转换为标记ID序列时，它使用填充标记（ID为0）填充较短的序列，使它们与批次中最长序列一样长。结果，大多数序列以许多填充标记结尾——通常是几十甚至几百个。即使我们使用的是比`SimpleRNN`层更好的`GRU`层，它的短期记忆仍然不太好，因此当它经过许多填充标记时，它最终会忘记评论的内容！一个解决方案是用等长的句子批次喂给模型（这也加快了训练速度）。另一个解决方案是让RNN忽略填充标记。这可以通过掩码来实现。
- en: Masking
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码
- en: 'Making the model ignore padding tokens is trivial using Keras: simply add `mask_zero=True`
    when creating the `Embedding` layer. This means that padding tokens (whose ID
    is 0) will be ignored by all downstream layers. That’s all! If you retrain the
    previous model for a few epochs, you will find that the validation accuracy quickly
    reaches over 80%.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras让模型忽略填充标记很简单：在创建`Embedding`层时简单地添加`mask_zero=True`。这意味着所有下游层都会忽略填充标记（其ID为0）。就是这样！如果对先前的模型进行几个时期的重新训练，您会发现验证准确率很快就能达到80%以上。
- en: 'The way this works is that the `Embedding` layer creates a *mask tensor* equal
    to `tf.math.not_equal(inputs, 0)`: it is a Boolean tensor with the same shape
    as the inputs, and it is equal to `False` anywhere the token IDs are 0, or `True`
    otherwise. This mask tensor is then automatically propagated by the model to the
    next layer. If that layer’s `call()` method has a `mask` argument, then it automatically
    receives the mask. This allows the layer to ignore the appropriate time steps.
    Each layer may handle the mask differently, but in general they simply ignore
    masked time steps (i.e., time steps for which the mask is `False`). For example,
    when a recurrent layer encounters a masked time step, it simply copies the output
    from the previous time step.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种工作方式是，`Embedding`层创建一个等于`tf.math.not_equal(inputs, 0)`的*掩码张量*：它是一个布尔张量，形状与输入相同，如果标记ID为0，则等于`False`，否则等于`True`。然后，该掩码张量会被模型自动传播到下一层。如果该层的`call()`方法有一个`mask`参数，那么它会自动接收到掩码。这使得该层能够忽略适当的时间步。每个层可能会以不同的方式处理掩码，但通常它们只是忽略被掩码的时间步（即掩码为`False`的时间步）。例如，当循环层遇到被掩码的时间步时，它只是复制前一个时间步的输出。
- en: 'Next, if the layer’s `supports_masking` attribute is `True`, then the mask
    is automatically propagated to the next layer. It keeps propagating this way for
    as long as the layers have `supports_masking=True`. As an example, a recurrent
    layer’s `supports_​mask⁠ing` attribute is `True` when `return_sequences=True`,
    but it’s `False` when `return_​sequen⁠ces=False` since there’s no need for a mask
    anymore in this case. So if you have a model with several recurrent layers with
    `return_sequences=True`, followed by a recurrent layer with `return_sequences=False`,
    then the mask will automatically propagate up to the last recurrent layer: that
    layer will use the mask to ignore masked steps, but it will not propagate the
    mask any further. Similarly, if you set `mask_zero=True` when creating the `Embedding`
    layer in the sentiment analysis model we just built, then the `GRU` layer will
    receive and use the mask automatically, but it will not propagate it any further,
    since `return_sequences` is not set to `True`.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果该层的`supports_masking`属性为`True`，那么掩码会自动传播到下一层。只要层具有`supports_masking=True`，它就会继续这样传播。例如，当`return_sequences=True`时，循环层的`supports_masking`属性为`True`，但当`return_sequences=False`时，它为`False`，因为在这种情况下不再需要掩码。因此，如果您有一个具有多个`return_sequences=True`的循环层，然后是一个`return_sequences=False`的循环层的模型，那么掩码将自动传播到最后一个循环层：该层将使用掩码来忽略被掩码的步骤，但不会进一步传播掩码。同样，如果在我们刚刚构建的情感分析模型中创建`Embedding`层时设置了`mask_zero=True`，那么`GRU`层将自动接收和使用掩码，但不会进一步传播，因为`return_sequences`没有设置为`True`。
- en: Tip
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Some layers need to update the mask before propagating it to the next layer:
    they do so by implementing the `compute_mask()` method, which takes two arguments:
    the inputs and the previous mask. It then computes the updated mask and returns
    it. The default implementation of `compute_mask()` just returns the previous mask
    unchanged.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一些层在将掩码传播到下一层之前需要更新掩码：它们通过实现`compute_mask()`方法来实现，该方法接受两个参数：输入和先前的掩码。然后计算更新后的掩码并返回。`compute_mask()`的默认实现只是返回先前的掩码而没有更改。
- en: 'Many Keras layers support masking: `SimpleRNN`, `GRU`, `LSTM`, `Bidirectional`,
    `Dense`, `TimeDistributed`, `Add`, and a few others (all in the `tf.keras.layers`
    package). However, convolutional layers (including `Conv1D`) do not support masking—it’s
    not obvious how they would do so anyway.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 许多Keras层支持掩码：`SimpleRNN`、`GRU`、`LSTM`、`Bidirectional`、`Dense`、`TimeDistributed`、`Add`等（都在`tf.keras.layers`包中）。然而，卷积层（包括`Conv1D`）不支持掩码——它们如何支持掩码并不明显。
- en: If the mask propagates all the way to the output, then it gets applied to the
    losses as well, so the masked time steps will not contribute to the loss (their
    loss will be 0). This assumes that the model outputs sequences, which is not the
    case in our sentiment analysis model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果掩码一直传播到输出，那么它也会应用到损失上，因此被掩码的时间步将不会对损失产生贡献（它们的损失将为0）。这假设模型输出序列，这在我们的情感分析模型中并不是这样。
- en: Warning
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'The `LSTM` and `GRU` layers have an optimized implementation for GPUs, based
    on Nvidia’s cuDNN library. However, this implementation only supports masking
    if all the padding tokens are at the end of the sequences. It also requires you
    to use the default values for several hyperparameters: `activation`, `recurrent_activation`,
    `recurrent_dropout`, `unroll`, `use_bias`, and `reset_after`. If that’s not the
    case, then these layers will fall back to the (much slower) default GPU implementation.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`LSTM`和`GRU`层具有基于Nvidia的cuDNN库的优化实现。但是，此实现仅在所有填充标记位于序列末尾时支持遮罩。它还要求您使用几个超参数的默认值：`activation`、`recurrent_activation`、`recurrent_dropout`、`unroll`、`use_bias`和`reset_after`。如果不是这种情况，那么这些层将退回到（速度慢得多的）默认GPU实现。'
- en: If you want to implement your own custom layer with masking support, you should
    add a `mask` argument to the `call()` method, and obviously make the method use
    the mask. Additionally, if the mask must be propagated to the next layers, then
    you should set `self.supports_masking=True` in the constructor. If the mask must
    be updated before it is propagated, then you must implement the `compute_mask()`
    method.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要实现支持遮罩的自定义层，应在`call()`方法中添加一个`mask`参数，并显然使方法使用该遮罩。此外，如果遮罩必须传播到下一层，则应在构造函数中设置`self.supports_masking=True`。如果必须在传播之前更新遮罩，则必须实现`compute_mask()`方法。
- en: 'If your model does not start with an `Embedding` layer, you may use the `tf.​​keras.layers.Masking`
    layer instead: by default, it sets the mask to `tf.math.​​reduce_any(tf.math.not_equal(X,
    0), axis=-1)`, meaning that time steps where the last dimension is full of zeros
    will be masked out in subsequent layers.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型不以`Embedding`层开头，可以使用`tf.keras.layers.Masking`层代替：默认情况下，它将遮罩设置为`tf.math.reduce_any(tf.math.not_equal(X,
    0), axis=-1)`，意味着最后一个维度全是零的时间步将在后续层中被遮罩。
- en: 'Using masking layers and automatic mask propagation works best for simple models.
    It will not always work for more complex models, such as when you need to mix
    `Conv1D` layers with recurrent layers. In such cases, you will need to explicitly
    compute the mask and pass it to the appropriate layers, using either the functional
    API or the subclassing API. For example, the following model is equivalent to
    the previous model, except it is built using the functional API and handles masking
    manually. It also adds a bit of dropout since the previous model was overfitting
    slightly:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用遮罩层和自动遮罩传播对简单模型效果最好。对于更复杂的模型，例如需要将`Conv1D`层与循环层混合时，并不总是适用。在这种情况下，您需要显式计算遮罩并将其传递给适当的层，可以使用函数式API或子类API。例如，以下模型与之前的模型等效，只是使用函数式API构建，并手动处理遮罩。它还添加了一点辍学，因为之前的模型略微过拟合：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'One last approach to masking is to feed the model with ragged tensors.⁠^([9](ch16.html#idm45720175499952))
    In practice, all you need to do is to set `ragged=True` when creating the `TextVectorization`
    layer, so that the input sequences are represented as ragged tensors:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 遮罩的最后一种方法是使用不规则张量来向模型提供输入。实际上，您只需在创建`TextVectorization`层时设置`ragged=True`，以便将输入序列表示为不规则张量：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Compare this ragged tensor representation with the regular tensor representation,
    which uses padding tokens:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将这种不规则张量表示与使用填充标记的常规张量表示进行比较：
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Keras’s recurrent layers have built-in support for ragged tensors, so there’s
    nothing else you need to do: just use this `TextVectorization` layer in your model.
    There’s no need to pass `mask_zero=True` or handle masks explicitly—it’s all implemented
    for you. That’s convenient! However, as of early 2022, the support for ragged
    tensors in Keras is still fairly recent, so there are a few rough edges. For example,
    it is currently not possible to use ragged tensors as targets when running on
    the GPU (but this may be resolved by the time you read these lines).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的循环层内置支持不规则张量，因此您无需执行其他操作：只需在模型中使用此`TextVectorization`层。无需传递`mask_zero=True`或显式处理遮罩——这一切都已为您实现。这很方便！但是，截至2022年初，Keras中对不规则张量的支持仍然相对较新，因此存在一些问题。例如，目前无法在GPU上运行时将不规则张量用作目标（但在您阅读这些内容时可能已解决）。
- en: 'Whichever masking approach you prefer, after training this model for a few
    epochs, it will become quite good at judging whether a review is positive or not.
    If you use the `tf.keras.callbacks.TensorBoard()` callback, you can visualize
    the embeddings in TensorBoard as they are being learned: it is fascinating to
    see words like “awesome” and “amazing” gradually cluster on one side of the embedding
    space, while words like “awful” and “terrible” cluster on the other side. Some
    words are not as positive as you might expect (at least with this model), such
    as the word “good”, presumably because many negative reviews contain the phrase
    “not good”.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您喜欢哪种遮罩方法，在训练此模型几个时期后，它将变得非常擅长判断评论是积极的还是消极的。如果使用`tf.keras.callbacks.TensorBoard()`回调，您可以在TensorBoard中可视化嵌入，看到诸如“棒极了”和“惊人”的词逐渐聚集在嵌入空间的一侧，而诸如“糟糕”和“可怕”的词聚集在另一侧。有些词并不像您可能期望的那样积极（至少在这个模型中），比如“好”这个词，可能是因为许多负面评论包含短语“不好”。
- en: Reusing Pretrained Embeddings and Language Models
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重用预训练的嵌入和语言模型
- en: 'It’s impressive that the model is able to learn useful word embeddings based
    on just 25,000 movie reviews. Imagine how good the embeddings would be if we had
    billions of reviews to train on! Unfortunately, we don’t, but perhaps we can reuse
    word embeddings trained on some other (very) large text corpus (e.g., Amazon reviews,
    available on TensorFlow Datasets), even if it is not composed of movie reviews?
    After all, the word “amazing” generally has the same meaning whether you use it
    to talk about movies or anything else. Moreover, perhaps embeddings would be useful
    for sentiment analysis even if they were trained on another task: since words
    like “awesome” and “amazing” have a similar meaning, they will likely cluster
    in the embedding space even for tasks such as predicting the next word in a sentence.
    If all positive words and all negative words form clusters, then this will be
    helpful for sentiment analysis. So, instead of training word embeddings, we could
    just download and use pretrained embeddings, such as Google’s [Word2vec embeddings](https://homl.info/word2vec),
    Stanford’s [GloVe embeddings](https://homl.info/glove), or Facebook’s [FastText
    embeddings](https://fasttext.cc).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 令人印象深刻的是，这个模型能够基于仅有25,000条电影评论学习到有用的词嵌入。想象一下，如果我们有数十亿条评论来训练，这些嵌入会有多好！不幸的是，我们没有，但也许我们可以重用在其他（非常）大型文本语料库上训练的词嵌入（例如，亚马逊评论，可在TensorFlow数据集上找到）？毕竟，“amazing”这个词无论是用来谈论电影还是其他事物，通常都有相同的含义。此外，也许即使它们是在另一个任务上训练的，嵌入也对情感分析有用：因为“awesome”和“amazing”这样的词有相似的含义，它们很可能会在嵌入空间中聚集，即使是用于预测句子中的下一个词这样的任务。如果所有积极词和所有消极词形成簇，那么这对情感分析将是有帮助的。因此，我们可以不训练词嵌入，而是下载并使用预训练的嵌入，例如谷歌的[Word2vec嵌入](https://homl.info/word2vec)，斯坦福的[GloVe嵌入](https://homl.info/glove)，或Facebook的[FastText嵌入](https://fasttext.cc)。
- en: 'Using pretrained word embeddings was popular for several years, but this approach
    has its limits. In particular, a word has a single representation, no matter the
    context. For example, the word “right” is encoded the same way in “left and right”
    and “right and wrong”, even though it means two very different things. To address
    this limitation, a [2018 paper](https://homl.info/elmo)⁠^([10](ch16.html#idm45720175311248))
    by Matthew Peters introduced *Embeddings from Language Models* (ELMo): these are
    contextualized word embeddings learned from the internal states of a deep bidirectional
    language model. Instead of just using pretrained embeddings in your model, you
    reuse part of a pretrained language model.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练词嵌入在几年内很受欢迎，但这种方法有其局限性。特别是，一个词无论上下文如何，都有一个表示。例如，“right”这个词在“left and right”和“right
    and wrong”中以相同的方式编码，尽管它们表示两个非常不同的含义。为了解决这个限制，Matthew Peters在2018年引入了*来自语言模型的嵌入*（ELMo）：这些是从深度双向语言模型的内部状态中学习到的上下文化词嵌入。与仅在模型中使用预训练嵌入不同，您可以重用预训练语言模型的一部分。
- en: 'At roughly the same time, the [Universal Language Model Fine-Tuning (ULMFiT)
    paper](https://homl.info/ulmfit)⁠^([11](ch16.html#idm45720175307104)) by Jeremy
    Howard and Sebastian Ruder demonstrated the effectiveness of unsupervised pretraining
    for NLP tasks: the authors trained an LSTM language model on a huge text corpus
    using self-supervised learning (i.e., generating the labels automatically from
    the data), then they fine-tuned it on various tasks. Their model outperformed
    the state of the art on six text classification tasks by a large margin (reducing
    the error rate by 18–24% in most cases). Moreover, the authors showed a pretrained
    model fine-tuned on just 100 labeled examples could achieve the same performance
    as one trained from scratch on 10,000 examples. Before the ULMFiT paper, using
    pretrained models was only the norm in computer vision; in the context of NLP,
    pretraining was limited to word embeddings. This paper marked the beginning of
    a new era in NLP: today, reusing pretrained language models is the norm.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在同一时间，Jeremy Howard和Sebastian Ruder的[通用语言模型微调（ULMFiT）论文](https://homl.info/ulmfit)展示了无监督预训练在NLP任务中的有效性：作者们使用自监督学习（即从数据自动生成标签）在庞大的文本语料库上训练了一个LSTM语言模型，然后在各种任务上进行微调。他们的模型在六个文本分类任务上表现优异（在大多数情况下将错误率降低了18-24%）。此外，作者们展示了一个仅使用100个标记示例进行微调的预训练模型可以达到与从头开始训练10,000个示例相同的性能。在ULMFiT论文之前，使用预训练模型只是计算机视觉中的常态；在NLP领域，预训练仅限于词嵌入。这篇论文标志着NLP的一个新时代的开始：如今，重用预训练语言模型已成为常态。
- en: 'For example, let’s build a classifier based on the Universal Sentence Encoder,
    a model architecture introduced in a [2018 paper](https://homl.info/139)⁠^([12](ch16.html#idm45720175301568))
    by a team of Google researchers. This model is based on the transformer architecture,
    which we will look at later in this chapter. Conveniently, the model is available
    on TensorFlow Hub:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们基于通用句子编码器构建一个分类器，这是由谷歌研究人员团队在2018年介绍的模型架构。这个模型基于transformer架构，我们将在本章后面讨论。方便的是，这个模型可以在TensorFlow
    Hub上找到。
- en: '[PRE24]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Tip
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'This model is quite large—close to 1 GB in size—so it may take a while to download.
    By default, TensorFlow Hub modules are saved to a temporary directory, and they
    get downloaded again and again every time you run your program. To avoid that,
    you must set the `TFHUB_CACHE_DIR` environment variable to a directory of your
    choice: the modules will then be saved there, and only downloaded once.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型非常庞大，接近1GB大小，因此下载可能需要一些时间。默认情况下，TensorFlow Hub模块保存在临时目录中，并且每次运行程序时都会重新下载。为了避免这种情况，您必须将`TFHUB_CACHE_DIR`环境变量设置为您选择的目录：模块将保存在那里，只会下载一次。
- en: Note that the last part of the TensorFlow Hub module URL specifies that we want
    version 4 of the model. This versioning ensures that if a new module version is
    released on TF Hub, it will not break our model. Conveniently, if you just enter
    this URL in a web browser, you will get the documentation for this module.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，TensorFlow Hub模块URL的最后部分指定我们想要模型的第4个版本。这种版本控制确保如果TF Hub上发布了新的模块版本，它不会破坏我们的模型。方便的是，如果你只在Web浏览器中输入这个URL，你将得到这个模块的文档。
- en: 'Also note that we set `trainable=True` when creating the `hub.KerasLayer`.
    This way, the pretrained Universal Sentence Encoder is fine-tuned during training:
    some of its weights are tweaked via backprop. Not all TensorFlow Hub modules are
    fine-tunable, so make sure to check the documentation for each pretrained module
    you’re interested in.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在创建`hub.KerasLayer`时，我们设置了`trainable=True`。这样，在训练期间，预训练的Universal Sentence
    Encoder会进行微调：通过反向传播调整一些权重。并非所有的TensorFlow Hub模块都是可微调的，所以确保查看你感兴趣的每个预训练模块的文档。
- en: 'After training, this model should reach a validation accuracy of over 90%.
    That’s actually really good: if you try to perform the task yourself, you will
    probably do only marginally better since many reviews contain both positive and
    negative comments. Classifying these ambiguous reviews is like flipping a coin.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，这个模型应该能达到超过90%的验证准确率。这实际上非常好：如果你尝试自己执行这个任务，你可能只会稍微好一点，因为许多评论中既包含积极的评论，也包含消极的评论。对这些模棱两可的评论进行分类就像抛硬币一样。
- en: 'So far we have looked at text generation using a char-RNN, and sentiment analysis
    with word-level RNN models (based on trainable embeddings) and using a powerful
    pretrained language model from TensorFlow Hub. In the next section, we will explore
    another important NLP task: *neural machine translation* (NMT).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过使用char-RNN进行文本生成，以及使用基于可训练嵌入的单词级RNN模型进行情感分析，以及使用来自TensorFlow Hub的强大预训练语言模型。在接下来的部分中，我们将探索另一个重要的NLP任务：神经机器翻译（NMT）。
- en: An Encoder–Decoder Network for Neural Machine Translation
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经机器翻译的编码器-解码器网络
- en: Let’s begin with a simple [NMT model](https://homl.info/103)⁠^([13](ch16.html#idm45720175133488))
    that will translate English sentences to Spanish (see [Figure 16-3](#machine_translation_diagram)).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的[NMT模型](https://homl.info/103)开始，它将英语句子翻译成西班牙语（参见[图16-3](#machine_translation_diagram)）。
- en: 'In short, the architecture is as follows: English sentences are fed as inputs
    to the encoder, and the decoder outputs the Spanish translations. Note that the
    Spanish translations are also used as inputs to the decoder during training, but
    shifted back by one step. In other words, during training the decoder is given
    as input the word that it *should* have output at the previous step, regardless
    of what it actually output. This is called *teacher forcing*—a technique that
    significantly speeds up training and improves the model’s performance. For the
    very first word, the decoder is given the start-of-sequence (SOS) token, and the
    decoder is expected to end the sentence with an end-of-sequence (EOS) token.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，架构如下：英语句子作为输入馈送给编码器，解码器输出西班牙语翻译。请注意，西班牙语翻译也在训练期间作为解码器的输入使用，但是向后移动了一步。换句话说，在训练期间，解码器被给予上一步应该输出的单词作为输入，而不管它实际输出了什么。这被称为“教师强迫”——一种显著加速训练并提高模型性能的技术。对于第一个单词，解码器被给予序列开始（SOS）标记，期望解码器以序列结束（EOS）标记结束句子。
- en: Each word is initially represented by its ID (e.g., `854` for the word “soccer”).
    Next, an `Embedding` layer returns the word embedding. These word embeddings are
    then fed to the encoder and the decoder.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词最初由其ID表示（例如，单词“soccer”的ID为`854`）。接下来，一个`Embedding`层返回单词嵌入。然后这些单词嵌入被馈送给编码器和解码器。
- en: At each step, the decoder outputs a score for each word in the output vocabulary
    (i.e., Spanish), then the softmax activation function turns these scores into
    probabilities. For example, at the first step the word “Me” may have a probability
    of 7%, “Yo” may have a probability of 1%, and so on. The word with the highest
    probability is output. This is very much like a regular classification task, and
    indeed you can train the model using the `"sparse_categorical_crossentropy"` loss,
    much like we did in the char-RNN model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步中，解码器为输出词汇表（即西班牙语）中的每个单词输出一个分数，然后softmax激活函数将这些分数转换为概率。例如，在第一步中，“Me”这个词可能有7%的概率，“Yo”可能有1%的概率，依此类推。具有最高概率的单词被输出。这非常类似于常规的分类任务，事实上你可以使用“sparse_categorical_crossentropy”损失来训练模型，就像我们在char-RNN模型中所做的那样。
- en: '![mls3 1603](assets/mls3_1603.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1603](assets/mls3_1603.png)'
- en: Figure 16-3\. A simple machine translation model
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-3。一个简单的机器翻译模型
- en: Note that at inference time (after training), you will not have the target sentence
    to feed to the decoder. Instead, you need to feed it the word that it has just
    output at the previous step, as shown in [Figure 16-4](#inference_decoder_diagram)
    (this will require an embedding lookup that is not shown in the diagram).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在推断时（训练后），你将没有目标句子来馈送给解码器。相反，你需要将它刚刚输出的单词作为上一步的输入，如[图16-4](#inference_decoder_diagram)所示（这将需要一个在图中未显示的嵌入查找）。
- en: Tip
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In a [2015 paper](https://homl.info/scheduledsampling),⁠^([14](ch16.html#idm45720175116736))
    Samy Bengio et al. proposed gradually switching from feeding the decoder the previous
    *target* token to feeding it the previous *output* token during training.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇[2015年的论文](https://homl.info/scheduledsampling)，Samy Bengio等人提出逐渐从在训练期间将前一个“目标”标记馈送给解码器转变为将前一个“输出”标记馈送给解码器。
- en: '![mls3 1604](assets/mls3_1604.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1604](assets/mls3_1604.png)'
- en: Figure 16-4\. At inference time, the decoder is fed as input the word it just
    output at the previous time step
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-4。在推断时，解码器作为输入接收它刚刚在上一个时间步输出的单词
- en: Let’s build and train this model! First, we need to download a dataset of English/Spanish
    sentence pairs:⁠^([15](ch16.html#idm45720175112096))
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建并训练这个模型！首先，我们需要下载一个英语/西班牙语句子对的数据集：
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Each line contains an English sentence and the corresponding Spanish translation,
    separated by a tab. We’ll start by removing the Spanish characters “¡” and “¿”,
    which the `TextVectorization` layer doesn’t handle, then we will parse the sentence
    pairs and shuffle them. Finally, we will split them into two separate lists, one
    per language:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 每行包含一个英语句子和相应的西班牙语翻译，用制表符分隔。我们将从删除西班牙字符“¡”和“¿”开始，`TextVectorization`层无法处理这些字符，然后我们将解析句子对并对它们进行洗牌。最后，我们将它们分成两个单独的列表，每种语言一个：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s take a look at the first three sentence pairs:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下前三个句子对：
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, let’s create two `TextVectorization` layers—one per language—and adapt
    them to the text:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建两个`TextVectorization`层——每种语言一个，并对文本进行调整：
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'There are a few things to note here:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几件事需要注意：
- en: We limit the vocabulary size to 1,000, which is quite small. That’s because
    the training set is not very large, and because using a small value will speed
    up training. State-of-the-art translation models typically use a much larger vocabulary
    (e.g., 30,000), a much larger training set (gigabytes), and a much larger model
    (hundreds or even thousands of megabytes). For example, check out the Opus-MT
    models by the University of Helsinki, or the M2M-100 model by Facebook.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将词汇表大小限制为1,000，这相当小。这是因为训练集不是很大，并且使用较小的值将加快训练速度。最先进的翻译模型通常使用更大的词汇表（例如30,000），更大的训练集（几千兆字节）和更大的模型（数百甚至数千兆字节）。例如，查看赫尔辛基大学的Opus-MT模型，或Facebook的M2M-100模型。
- en: 'Since all sentences in the dataset have a maximum of 50 words, we set `output_sequence_length`
    to 50: this way the input sequences will automatically be padded with zeros until
    they are all 50 tokens long. If there was any sentence longer than 50 tokens in
    the training set, it would be cropped to 50 tokens.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集中的所有句子最多有50个单词，我们将`output_sequence_length`设置为50：这样输入序列将自动填充为零，直到它们都是50个标记长。如果训练集中有任何超过50个标记的句子，它将被裁剪为50个标记。
- en: 'For the Spanish text, we add “startofseq” and “endofseq” to each sentence when
    adapting the `TextVectorization` layer: we will use these words as SOS and EOS
    tokens. You could use any other words, as long as they are not actual Spanish
    words.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于西班牙文本，我们在调整`TextVectorization`层时为每个句子添加“startofseq”和“endofseq”：我们将使用这些词作为SOS和EOS标记。您可以使用任何其他单词，只要它们不是实际的西班牙单词。
- en: 'Let’s inspect the first 10 tokens in both vocabularies. They start with the
    padding token, the unknown token, the SOS and EOS tokens (only in the Spanish
    vocabulary), then the actual words, sorted by decreasing frequency:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查两种词汇表中的前10个标记。它们以填充标记、未知标记、SOS和EOS标记（仅在西班牙语词汇表中）、然后按频率递减排序的实际单词开始：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, let’s create the training set and the validation set (you could also
    create a test set if you needed it). We will use the first 100,000 sentence pairs
    for training, and the rest for validation. The decoder’s inputs are the Spanish
    sentences plus an SOS token prefix. The targets are the Spanish sentences plus
    an EOS suffix:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建训练集和验证集（如果需要，您也可以创建一个测试集）。我们将使用前100,000个句子对进行训练，其余用于验证。解码器的输入是西班牙语句子加上一个SOS标记前缀。目标是西班牙语句子加上一个EOS后缀：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'OK, we’re now ready to build our translation model. We will use the functional
    API for that since the model is not sequential. It requires two text inputs—one
    for the encoder and one for the decoder—so let’s start with that:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们准备构建我们的翻译模型。我们将使用功能API，因为模型不是顺序的。它需要两个文本输入——一个用于编码器，一个用于解码器——所以让我们从这里开始：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we need to encode these sentences using the `TextVectorization` layers
    we prepared earlier, followed by an `Embedding` layer for each language, with
    `mask_zero=True` to ensure masking is handled automatically. The embedding size
    is a hyperparameter you can tune, as always:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用我们之前准备的`TextVectorization`层对这些句子进行编码，然后为每种语言使用一个`Embedding`层，其中`mask_zero=True`以确保自动处理掩码。嵌入大小是一个您可以调整的超参数，像往常一样：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Tip
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When the languages share many words, you may get better performance using the
    same embedding layer for both the encoder and the decoder.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当语言共享许多单词时，您可能会获得更好的性能，使用相同的嵌入层用于编码器和解码器。
- en: 'Now let’s create the encoder and pass it the embedded inputs:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建编码器并传递嵌入输入：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To keep things simple, we just used a single `LSTM` layer, but you could stack
    several of them. We also set `return_state=True` to get a reference to the layer’s
    final state. Since we’re using an `LSTM` layer, there are actually two states:
    the short-term state and the long-term state. The layer returns these states separately,
    which is why we had to write `*encoder_state` to group both states in a list.⁠^([16](ch16.html#idm45720174413024))
    Now we can use this (double) state as the initial state of the decoder:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单，我们只使用了一个`LSTM`层，但您可以堆叠几个。我们还设置了`return_state=True`以获得对层最终状态的引用。由于我们使用了一个`LSTM`层，实际上有两个状态：短期状态和长期状态。该层分别返回这些状态，这就是为什么我们必须写`*encoder_state`来将两个状态分组在一个列表中。现在我们可以使用这个（双重）状态作为解码器的初始状态：
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we can pass the decoder’s outputs through a `Dense` layer with the softmax
    activation function to get the word probabilities for each step:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过具有softmax激活函数的`Dense`层将解码器的输出传递，以获得每个步骤的单词概率：
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And that’s it! We just need to create the Keras `Model`, compile it, and train
    it:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们只需要创建Keras`Model`，编译它并训练它：
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After training, we can use the model to translate new English sentences to
    Spanish. But it’s not as simple as calling `model.predict()`, because the decoder
    expects as input the word that was predicted at the previous time step. One way
    to do this is to write a custom memory cell that keeps track of the previous output
    and feeds it to the encoder at the next time step. However, to keep things simple,
    we can just call the model multiple times, predicting one extra word at each round.
    Let’s write a little utility function for that:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以使用该模型将新的英语句子翻译成西班牙语。但这并不像调用`model.predict()`那样简单，因为解码器期望的输入是上一个时间步预测的单词。一种方法是编写一个自定义记忆单元，跟踪先前的输出并在下一个时间步将其馈送给编码器。但为了保持简单，我们可以多次调用模型，每轮预测一个额外的单词。让我们为此编写一个小型实用程序函数：
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The function simply keeps predicting one word at a time, gradually completing
    the translation, and it stops once it reaches the EOS token. Let’s give it a try!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数只是逐步预测一个单词，逐渐完成翻译，并在达到EOS标记时停止。让我们试试看！
- en: '[PRE38]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Hurray, it works! Well, at least it does with very short sentences. If you
    try playing with this model for a while, you will find that it’s not bilingual
    yet, and in particular it really struggles with longer sentences. For example:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 万岁，它起作用了！嗯，至少对于非常短的句子是这样。如果您尝试使用这个模型一段时间，您会发现它还不是双语的，特别是在处理更长的句子时会遇到困难。例如：
- en: '[PRE39]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The translation says “I like soccer and sometimes even the bus”. So how can
    you improve it? One way is to increase the training set size and add more `LSTM`
    layers in both the encoder and the decoder. But this will only get you so far,
    so let’s look at more sophisticated techniques, starting with bidirectional recurrent
    layers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译说：“我喜欢足球，有时甚至喜欢公共汽车”。那么你如何改进呢？一种方法是增加训练集的大小，并在编码器和解码器中添加更多的`LSTM`层。但这只能让你走得更远，所以让我们看看更复杂的技术，从双向循环层开始。
- en: Bidirectional RNNs
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双向RNN
- en: At each time step, a regular recurrent layer only looks at past and present
    inputs before generating its output. In other words, it is *causal*, meaning it
    cannot look into the future. This type of RNN makes sense when forecasting time
    series, or in the decoder of a sequence-to-sequence (seq2seq) model. But for tasks
    like text classification, or in the encoder of a seq2seq model, it is often preferable
    to look ahead at the next words before encoding a given word.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步骤，常规循环层在生成输出之前只查看过去和现在的输入。换句话说，它是*因果*的，这意味着它不能预测未来。这种类型的RNN在预测时间序列时或在序列到序列（seq2seq）模型的解码器中是有意义的。但对于文本分类等任务，或者在seq2seq模型的编码器中，通常最好在编码给定单词之前查看下一个单词。
- en: 'For example, consider the phrases “the right arm”, “the right person”, and
    “the right to criticize”: to properly encode the word “right”, you need to look
    ahead. One solution is to run two recurrent layers on the same inputs, one reading
    the words from left to right and the other reading them from right to left, then
    combine their outputs at each time step, typically by concatenating them. This
    is what a *bidirectional recurrent layer* does (see [Figure 16-5](#bidirectional_rnn_diagram)).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑短语“右臂”，“正确的人”和“批评的权利”：要正确编码单词“right”，您需要向前查看。一个解决方案是在相同的输入上运行两个循环层，一个从左到右读取单词，另一个从右到左读取单词，然后在每个时间步骤组合它们的输出，通常通过连接它们。这就是*双向循环层*的作用（参见[图16-5](#bidirectional_rnn_diagram)）。
- en: '![mls3 1605](assets/mls3_1605.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1605](assets/mls3_1605.png)'
- en: Figure 16-5\. A bidirectional recurrent layer
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-5\. 双向循环层
- en: 'To implement a bidirectional recurrent layer in Keras, just wrap a recurrent
    layer in a `tf.keras.layers.Bidirectional` layer. For example, the following `Bidirectional`
    layer could be used as the encoder in our translation model:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中实现双向循环层，只需将循环层包装在`tf.keras.layers.Bidirectional`层中。例如，以下`Bidirectional`层可以用作我们翻译模型中的编码器：
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `Bidirectional` layer will create a clone of the `GRU` layer (but in the
    reverse direction), and it will run both and concatenate their outputs. So although
    the `GRU` layer has 10 units, the `Bidirectional` layer will output 20 values
    per time step.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`Bidirectional`层将创建`GRU`层的克隆（但是在相反方向），并且将同时运行并连接它们的输出。因此，尽管`GRU`层有10个单元，`Bidirectional`层将在每个时间步输出20个值。'
- en: 'There’s just one problem. This layer will now return four states instead of
    two: the final short-term and long-term states of the forward `LSTM` layer, and
    the final short-term and long-term states of the backward `LSTM` layer. We cannot
    use this quadruple state directly as the initial state of the decoder’s `LSTM`
    layer, since it expects just two states (short-term and long-term). We cannot
    make the decoder bidirectional, since it must remain causal: otherwise it would
    cheat during training and it would not work. Instead, we can concatenate the two
    short-term states, and also concatenate the two long-term states:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个问题。这一层现在将返回四个状态而不是两个：前向`LSTM`层的最终短期和长期状态，以及后向`LSTM`层的最终短期和长期状态。我们不能直接将这个四重状态用作解码器的`LSTM`层的初始状态，因为它只期望两个状态（短期和长期）。我们不能使解码器双向，因为它必须保持因果关系：否则在训练过程中会作弊，而且不起作用。相反，我们可以连接两个短期状态，并连接两个长期状态：
- en: '[PRE41]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now let’s look at another popular technique that can greatly improve the performance
    of a translation model at inference time: beam search.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看另一种在推理时可以极大提高翻译模型性能的流行技术：束搜索。
- en: Beam Search
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 束搜索
- en: 'Suppose you have trained an encoder–decoder model, and you use it to translate
    the sentence “I like soccer” to Spanish. You are hoping that it will output the
    proper translation “me gusta el fútbol”, but unfortunately it outputs “me gustan
    los jugadores”, which means “I like the players”. Looking at the training set,
    you notice many sentences such as “I like cars”, which translates to “me gustan
    los autos”, so it wasn’t absurd for the model to output “me gustan los” after
    seeing “I like”. Unfortunately, in this case it was a mistake since “soccer” is
    singular. The model could not go back and fix it, so it tried to complete the
    sentence as best it could, in this case using the word “jugadores”. How can we
    give the model a chance to go back and fix mistakes it made earlier? One of the
    most common solutions is *beam search*: it keeps track of a short list of the
    *k* most promising sentences (say, the top three), and at each decoder step it
    tries to extend them by one word, keeping only the *k* most likely sentences.
    The parameter *k* is called the *beam width*.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经训练了一个编码器-解码器模型，并且您使用它将句子“I like soccer”翻译成西班牙语。您希望它会输出正确的翻译“me gusta el
    fútbol”，但不幸的是它输出了“me gustan los jugadores”，意思是“我喜欢球员”。看着训练集，您注意到许多句子如“I like cars”，翻译成“me
    gustan los autos”，所以模型在看到“I like”后输出“me gustan los”并不荒谬。不幸的是，在这种情况下是一个错误，因为“soccer”是单数。模型无法回头修正，所以它尽力完成句子，这种情况下使用了“jugadores”这个词。我们如何让模型有机会回头修正之前的错误呢？最常见的解决方案之一是*beam
    search*：它跟踪一个最有希望的句子列表（比如说前三个），在每个解码器步骤中尝试扩展它们一个词，只保留* k *个最有可能的句子。参数*k*被称为*beam
    width*。
- en: For example, suppose you use the model to translate the sentence “I like soccer”
    using beam search with a beam width of 3 (see [Figure 16-6](#beam_search_diagram)).
    At the first decoder step, the model will output an estimated probability for
    each possible first word in the translated sentence. Suppose the top three words
    are “me” (75% estimated probability), “a” (3%), and “como” (1%). That’s our short
    list so far. Next, we use the model to find the next word for each sentence. For
    the first sentence (“me”), perhaps the model outputs a probability of 36% for
    the word “gustan”, 32% for the word “gusta”, 16% for the word “encanta”, and so
    on. Note that these are actually *conditional* probabilities, given that the sentence
    starts with “me”. For the second sentence (“a”), the model might output a conditional
    probability of 50% for the word “mi”, and so on. Assuming the vocabulary has 1,000
    words, we will end up with 1,000 probabilities per sentence.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您使用模型来翻译句子“I like soccer”，使用beam search和beam width为3（参见[图16-6](#beam_search_diagram)）。在第一个解码器步骤中，模型将为翻译句子中每个可能的第一个词输出一个估计概率。假设前三个词是“me”（75%的估计概率），“a”（3%）和“como”（1%）。这是我们目前的短列表。接下来，我们使用模型为每个句子找到下一个词。对于第一个句子（“me”），也许模型为“gustan”这个词输出36%的概率，“gusta”这个词输出32%的概率，“encanta”这个词输出16%的概率，依此类推。请注意，这些实际上是*条件*概率，假设句子以“me”开头。对于第二个句子（“a”），模型可能为“mi”这个词输出50%的条件概率，依此类推。假设词汇表有1,000个词，我们将得到每个句子1,000个概率。
- en: 'Next, we compute the probabilities of each of the 3,000 two-word sentences
    we considered (3 × 1,000). We do this by multiplying the estimated conditional
    probability of each word by the estimated probability of the sentence it completes.
    For example, the estimated probability of the sentence “me” was 75%, while the
    estimated conditional probability of the word “gustan” (given that the first word
    is “me”) was 36%, so the estimated probability of the sentence “me gustan” is
    75% × 36% = 27%. After computing the probabilities of all 3,000 two-word sentences,
    we keep only the top 3\. In this example they all start with the word “me”: “me
    gustan” (27%), “me gusta” (24%), and “me encanta” (12%). Right now, the sentence
    “me gustan” is winning, but “me gusta” has not been eliminated.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算我们考虑的3,000个两个词的句子的概率（3 × 1,000）。我们通过将每个词的估计条件概率乘以它完成的句子的估计概率来做到这一点。例如，“me”的句子的估计概率为75%，而“gustan”这个词的估计条件概率（假设第一个词是“me”）为36%，所以“me
    gustan”的估计概率为75% × 36% = 27%。在计算了所有3,000个两个词的句子的概率之后，我们只保留前3个。在这个例子中，它们都以“me”开头：“me
    gustan”（27%），“me gusta”（24%）和“me encanta”（12%）。目前，“me gustan”这个句子领先，但“me gusta”还没有被淘汰。
- en: '![mls3 1606](assets/mls3_1606.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1606](assets/mls3_1606.png)'
- en: Figure 16-6\. Beam search, with a beam width of 3
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-6。beam search，beam width为3
- en: 'Then we repeat the same process: we use the model to predict the next word
    in each of these three sentences, and we compute the probabilities of all 3,000
    three-word sentences we considered. Perhaps the top three are now “me gustan los”
    (10%), “me gusta el” (8%), and “me gusta mucho” (2%). At the next step we may
    get “me gusta el fútbol” (6%), “me gusta mucho el” (1%), and “me gusta el deporte”
    (0.2%). Notice that “me gustan” was eliminated, and the correct translation is
    now ahead. We boosted our encoder–decoder model’s performance without any extra
    training, simply by using it more wisely.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们重复相同的过程：我们使用模型预测这三个句子中的下一个词，并计算我们考虑的所有3,000个三个词的句子的概率。也许现在前三个是“me gustan
    los”（10%），“me gusta el”（8%）和“me gusta mucho”（2%）。在下一步中，我们可能得到“me gusta el fútbol”（6%），“me
    gusta mucho el”（1%）和“me gusta el deporte”（0.2%）。请注意，“me gustan”已经被淘汰，正确的翻译现在领先。我们在没有额外训练的情况下提高了我们的编码器-解码器模型的性能，只是更明智地使用它。
- en: Tip
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The TensorFlow Addons library includes a full seq2seq API that lets you build
    encoder–decoder models with attention, including beam search, and more. However,
    its documentation is currently very limited. Implementing beam search is a good
    exercise, so give it a try! Check out this chapter’s notebook for a possible solution.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Addons库包含一个完整的seq2seq API，让您可以构建带有注意力的编码器-解码器模型，包括beam search等等。然而，它的文档目前非常有限。实现beam
    search是一个很好的练习，所以试一试吧！查看本章的笔记本，了解可能的解决方案。
- en: With all this, you can get reasonably good translations for fairly short sentences.
    Unfortunately, this model will be really bad at translating long sentences. Once
    again, the problem comes from the limited short-term memory of RNNs. *Attention
    mechanisms* are the game-changing innovation that addressed this problem.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一切，您可以为相当短的句子获得相当不错的翻译。不幸的是，这种模型在翻译长句子时会表现得非常糟糕。问题再次出在RNN的有限短期记忆上。*注意力机制*是解决这个问题的划时代创新。
- en: Attention Mechanisms
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制
- en: 'Consider the path from the word “soccer” to its translation “fútbol” back in
    [Figure 16-3](#machine_translation_diagram): it is quite long! This means that
    a representation of this word (along with all the other words) needs to be carried
    over many steps before it is actually used. Can’t we make this path shorter?'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下从单词“soccer”到其翻译“fútbol”的路径，回到[图16-3](#machine_translation_diagram)：这是相当长的！这意味着这个单词的表示（以及所有其他单词）需要在实际使用之前经过许多步骤。我们难道不能让这条路径变短一点吗？
- en: This was the core idea in a landmark [2014 paper](https://homl.info/attention)⁠^([18](ch16.html#idm45720173841328))
    by Dzmitry Bahdanau et al., where the authors introduced a technique that allowed
    the decoder to focus on the appropriate words (as encoded by the encoder) at each
    time step. For example, at the time step where the decoder needs to output the
    word “fútbol”, it will focus its attention on the word “soccer”. This means that
    the path from an input word to its translation is now much shorter, so the short-term
    memory limitations of RNNs have much less impact. Attention mechanisms revolutionized
    neural machine translation (and deep learning in general), allowing a significant
    improvement in the state of the art, especially for long sentences (e.g., over
    30 words).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Dzmitry Bahdanau等人在一篇具有里程碑意义的[2014年论文](https://homl.info/attention)中的核心思想，作者在其中介绍了一种技术，允许解码器在每个时间步关注适当的单词（由编码器编码）。例如，在解码器需要输出单词“fútbol”的时间步，它将把注意力集中在单词“soccer”上。这意味着从输入单词到其翻译的路径现在要短得多，因此RNN的短期记忆限制对其影响要小得多。注意机制彻底改变了神经机器翻译（以及深度学习一般）的方式，显著改进了技术水平，特别是对于长句子（例如，超过30个单词）。
- en: Note
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The most common metric used in NMT is the *bilingual evaluation understudy*
    (BLEU) score, which compares each translation produced by the model with several
    good translations produced by humans: it counts the number of *n*-grams (sequences
    of *n* words) that appear in any of the target translations and adjusts the score
    to take into account the frequency of the produced *n*-grams in the target translations.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: NMT中最常用的度量标准是*双语评估助手*（BLEU）分数，它将模型产生的每个翻译与人类产生的几个好翻译进行比较：它计算出现在任何目标翻译中的*n*-gram（*n*个单词序列）的数量，并调整分数以考虑在目标翻译中产生的*n*-gram的频率。
- en: '[Figure 16-7](#attention_diagram) shows our encoder–decoder model with an added
    attention mechanism. On the left, you have the encoder and the decoder. Instead
    of just sending the encoder’s final hidden state to the decoder, as well as the
    previous target word at each step (which is still done, although it is not shown
    in the figure), we now send all of the encoder’s outputs to the decoder as well.
    Since the decoder cannot deal with all these encoder outputs at once, they need
    to be aggregated: at each time step, the decoder’s memory cell computes a weighted
    sum of all the encoder outputs. This determines which words it will focus on at
    this step. The weight *α*[(*t*,*i*)] is the weight of the *i*^(th) encoder output
    at the *t*^(th) decoder time step. For example, if the weight *α*[(3,2)] is much
    larger than the weights *α*[(3,0)] and *α*[(3,1)], then the decoder will pay much
    more attention to the encoder’s output for word #2 (“soccer”) than to the other
    two outputs, at least at this time step. The rest of the decoder works just like
    earlier: at each time step the memory cell receives the inputs we just discussed,
    plus the hidden state from the previous time step, and finally (although it is
    not represented in the diagram) it receives the target word from the previous
    time step (or at inference time, the output from the previous time step).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-7](#attention_diagram)展示了我们带有注意力机制的编码器-解码器模型。在左侧，您可以看到编码器和解码器。现在，我们不仅在每一步将编码器的最终隐藏状态和前一个目标单词发送给解码器（尽管这仍在进行，但在图中没有显示），还将所有编码器的输出发送给解码器。由于解码器无法一次处理所有这些编码器的输出，因此它们需要被聚合：在每个时间步，解码器的记忆单元计算所有编码器输出的加权和。这决定了它在这一步将关注哪些单词。权重*α*[(*t*,*i*)]是第*t*个解码器时间步的第*i*个编码器输出的权重。例如，如果权重*α*[(3,2)]远大于权重*α*[(3,0)]和*α*[(3,1)]，那么解码器将在这个时间步更多地关注第2个单词（“soccer”）的编码器输出，而不是其他两个输出。解码器的其余部分与之前的工作方式相同：在每个时间步，记忆单元接收我们刚刚讨论的输入，以及来自上一个时间步的隐藏状态，最后（尽管在图中没有表示）它接收来自上一个时间步的目标单词（或在推断时，来自上一个时间步的输出）。'
- en: '![mls3 1607](assets/mls3_1607.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1607](assets/mls3_1607.png)'
- en: Figure 16-7\. Neural machine translation using an encoder–decoder network with
    an attention model
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-7。使用带有注意力模型的编码器-解码器网络的神经机器翻译
- en: 'But where do these *α*[(*t*,*i*)] weights come from? Well, they are generated
    by a small neural network called an *alignment model* (or an *attention layer*),
    which is trained jointly with the rest of the encoder–decoder model. This alignment
    model is illustrated on the righthand side of [Figure 16-7](#attention_diagram).
    It starts with a `Dense` layer composed of a single neuron that processes each
    of the encoder’s outputs, along with the decoder’s previous hidden state (e.g.,
    **h**[(2)]). This layer outputs a score (or energy) for each encoder output (e.g.,
    *e*[(3,] [2)]): this score measures how well each output is aligned with the decoder’s
    previous hidden state. For example, in [Figure 16-7](#attention_diagram), the
    model has already output “me gusta el” (meaning “I like”), so it’s now expecting
    a noun: the word “soccer” is the one that best aligns with the current state,
    so it gets a high score. Finally, all the scores go through a softmax layer to
    get a final weight for each encoder output (e.g., *α*[(3,2)]). All the weights
    for a given decoder time step add up to 1\. This particular attention mechanism
    is called *Bahdanau attention* (named after the 2014 paper’s first author). Since
    it concatenates the encoder output with the decoder’s previous hidden state, it
    is sometimes called *concatenative attention* (or *additive attention*).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这些*α*[(*t*,*i*)]权重是从哪里来的呢？嗯，它们是由一个称为*对齐模型*（或*注意力层*）的小型神经网络生成的，该模型与其余的编码器-解码器模型一起进行训练。这个对齐模型在[图16-7](#attention_diagram)的右侧进行了说明。它以一个由单个神经元组成的`Dense`层开始，处理每个编码器的输出，以及解码器的先前隐藏状态（例如**h**[(2)]）。这一层为每个编码器输出（例如*e*[(3,]
    [2)]）输出一个分数（或能量）：这个分数衡量每个输出与解码器先前隐藏状态的对齐程度。例如，在[图16-7](#attention_diagram)中，模型已经输出了“me
    gusta el”（意思是“我喜欢”），所以现在期望一个名词：单词“soccer”是与当前状态最匹配的，所以它得到了一个高分。最后，所有分数都通过softmax层，以获得每个编码器输出的最终权重（例如*α*[(3,2)]）。给定解码器时间步的所有权重加起来等于1。这种特定的注意力机制被称为*Bahdanau注意力*（以2014年论文的第一作者命名）。由于它将编码器输出与解码器的先前隐藏状态连接起来，因此有时被称为*连接注意力*（或*加性注意力*）。
- en: Note
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If the input sentence is *n* words long, and assuming the output sentence is
    about as long, then this model will need to compute about *n*² weights. Fortunately,
    this quadratic computational complexity is still tractable because even long sentences
    don’t have thousands of words.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入句子有*n*个单词，并假设输出句子长度大致相同，那么这个模型将需要计算大约*n*²个权重。幸运的是，这种二次计算复杂度仍然可行，因为即使是长句子也不会有成千上万个单词。
- en: Another common attention mechanism, known as *Luong attention* or *multiplicative
    attention*, was proposed shortly after, in [2015](https://homl.info/luongattention),⁠^([19](ch16.html#idm45720173812256))
    by Minh-Thang Luong et al. Because the goal of the alignment model is to measure
    the similarity between one of the encoder’s outputs and the decoder’s previous
    hidden state, the authors proposed to simply compute the dot product (see [Chapter 4](ch04.html#linear_models_chapter))
    of these two vectors, as this is often a fairly good similarity measure, and modern
    hardware can compute it very efficiently. For this to be possible, both vectors
    must have the same dimensionality. The dot product gives a score, and all the
    scores (at a given decoder time step) go through a softmax layer to give the final
    weights, just like in Bahdanau attention. Another simplification Luong et al.
    proposed was to use the decoder’s hidden state at the current time step rather
    than at the previous time step (i.e., **h**[(*t*)] rather than **h**[(*t*–1)]),
    then to use the output of the attention mechanism (noted <math><msub><mover><mi
    mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></math>)
    directly to compute the decoder’s predictions, rather than using it to compute
    the decoder’s current hidden state. The researchers also proposed a variant of
    the dot product mechanism where the encoder outputs first go through a fully connected
    layer (without a bias term) before the dot products are computed. This is called
    the “general” dot product approach. The researchers compared both dot product
    approaches with the concatenative attention mechanism (adding a rescaling parameter
    vector **v**), and they observed that the dot product variants performed better
    than concatenative attention. For this reason, concatenative attention is much
    less used now. The equations for these three attention mechanisms are summarized
    in [Equation 16-1](#attention_mechanisms_equation).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的注意力机制，称为*Luong注意力*或*乘法注意力*，是在[2015年](https://homl.info/luongattention)提出的，由Minh-Thang
    Luong等人提出⁠^([19](ch16.html#idm45720173812256))。因为对齐模型的目标是衡量编码器输出和解码器先前隐藏状态之间的相似性，作者建议简单地计算这两个向量的点积（参见[第4章](ch04.html#linear_models_chapter)），因为这通常是一个相当好的相似性度量，而现代硬件可以非常高效地计算它。为了实现这一点，两个向量必须具有相同的维度。点积给出一个分数，所有分数（在给定的解码器时间步长上）都通过softmax层，以给出最终的权重，就像Bahdanau注意力中一样。Luong等人提出的另一个简化是使用当前时间步的解码器隐藏状态，而不是上一个时间步（即**h**[(*t*)]而不是**h**[(*t*–1)]），然后直接使用注意力机制的输出（标记为<math><msub><mover><mi
    mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></math>)来计算解码器的预测，而不是用它来计算解码器当前的隐藏状态。研究人员还提出了一种点积机制的变体，其中编码器输出首先经过一个全连接层（没有偏置项），然后再计算点积。这被称为“一般”点积方法。研究人员将两种点积方法与连接注意力机制（添加一个重新缩放参数向量**v**）进行了比较，他们观察到点积变体的性能优于连接注意力。因此，连接注意力现在使用较少。这三种注意力机制的方程式总结在[方程式16-1](#attention_mechanisms_equation)中。
- en: Equation 16-1\. Attention mechanisms
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式16-1。注意力机制
- en: <math display="block"><msub><mover><mi mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mtext> with </mtext><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub></mfenced></mrow><mrow><mstyle
    displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>'</mo></mrow></munder></mstyle><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>'</mo><mo>)</mo></mrow></msub></mfenced></mrow></mfrac><mtext> and </mtext><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfenced
    open="{" close=""><mtable columnalign="left"><mtr><mtd><msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>d</mi><mi>o</mi><mi>t</mi></mtd></mtr><mtr><mtd><msup><msub><mi
    mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><mi
    mathvariant="bold">W</mi><msub><mi mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>l</mi></mtd></mtr><mtr><mtd><msup><mi
    mathvariant="bold">v</mi><mo>⊺</mo></msup><mi>tanh</mi><mo>(</mo><mi mathvariant="bold">W</mi><mo>[</mo><msub><mi
    mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>;</mo><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>]</mo><mo>)</mo></mtd><mtd><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi></mtd></mtr></mtable></mfenced></math>
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><msub><mover><mi mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mtext> with </mtext><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub></mfenced></mrow><mrow><mstyle
    displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>'</mo></mrow></munder></mstyle><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>'</mo><mo>)</mo></mrow></msub></mfenced></mrow></mfrac><mtext> and </mtext><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfenced
    open="{" close=""><mtable columnalign="left"><mtr><mtd><msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>d</mi><mi>o</mi><mi>t</mi></mtd></mtr><mtr><mtd><msup><msub><mi
    mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><mi
    mathvariant="bold">W</mi><msub><mi mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>l</mi></mtd></mtr><mtr><mtd><msup><mi
    mathvariant="bold">v</mi><mo>⊺</mo></msup><mi>tanh</mi><mo>(</mo><mi mathvariant="bold">W</mi><mo>[</mo><msub><mi
    mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>;</mo><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>]</mo><mo>)</mo></mtd><mtd><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi></mtd></mtr></mtable></mfenced></math>
- en: 'Keras provides a `tf.keras.layers.Attention` layer for Luong attention, and
    an `AdditiveAttention` layer for Bahdanau attention. Let’s add Luong attention
    to our encoder–decoder model. Since we will need to pass all the encoder’s outputs
    to the `Attention` layer, we first need to set `return_sequences=True` when creating
    the encoder:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Keras为Luong attention提供了`tf.keras.layers.Attention`层，为Bahdanau attention提供了`AdditiveAttention`层。让我们将Luong
    attention添加到我们的编码器-解码器模型中。由于我们需要将所有编码器的输出传递给`Attention`层，所以在创建编码器时，我们首先需要设置`return_sequences=True`：
- en: '[PRE42]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we need to create the attention layer and pass it the decoder’s states
    and the encoder’s outputs. However, to access the decoder’s states at each step
    we would need to write a custom memory cell. For simplicity, let’s use the decoder’s
    outputs instead of its states: in practice this works well too, and it’s much
    easier to code. Then we just pass the attention layer’s outputs directly to the
    output layer, as suggested in the Luong attention paper:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建注意力层，并将解码器的状态和编码器的输出传递给它。然而，为了在每一步访问解码器的状态，我们需要编写一个自定义的记忆单元。为简单起见，让我们使用解码器的输出而不是其状态：实际上这也很有效，并且编码更容易。然后我们直接将注意力层的输出传递给输出层，就像Luong注意力论文中建议的那样：
- en: '[PRE43]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'And that’s it! If you train this model, you will find that it now handles much
    longer sentences. For example:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！如果训练这个模型，你会发现它现在可以处理更长的句子。例如：
- en: '[PRE44]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In short, the attention layer provides a way to focus the attention of the
    model on part of the inputs. But there’s another way to think of this layer: it
    acts as a differentiable memory retrieval mechanism.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，注意力层提供了一种让模型集中注意力于输入的一部分的方法。但是还有另一种方式来思考这个层：它充当了一个可微分的记忆检索机制。
- en: 'For example, let’s suppose the encoder analyzed the input sentence “I like
    soccer”, and it managed to understand that the word “I” is the subject and the
    word “like” is the verb, so it encoded this information in its outputs for these
    words. Now suppose the decoder has already translated the subject, and it thinks
    that it should translate the verb next. For this, it needs to fetch the verb from
    the input sentence. This is analogous to a dictionary lookup: it’s as if the encoder
    had created a dictionary {"subject”: “They”, “verb”: “played”, …​} and the decoder
    wanted to look up the value that corresponds to the key “verb”.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，假设编码器分析了输入句子“I like soccer”，并且成功理解了单词“I”是主语，单词“like”是动词，因此在这些单词的输出中编码了这些信息。现在假设解码器已经翻译了主语，并且认为接下来应该翻译动词。为此，它需要从输入句子中提取动词。这类似于字典查找：就好像编码器创建了一个字典{"subject":
    "They", "verb": "played", ...}，解码器想要查找与键“verb”对应的值。'
- en: However, the model does not have discrete tokens to represent the keys (like
    “subject” or “verb”); instead, it has vectorized representations of these concepts
    that it learned during training, so the query it will use for the lookup will
    not perfectly match any key in the dictionary. The solution is to compute a similarity
    measure between the query and each key in the dictionary, and then use the softmax
    function to convert these similarity scores to weights that add up to 1\. As we
    saw earlier, that’s exactly what the attention layer does. If the key that represents
    the verb is by far the most similar to the query, then that key’s weight will
    be close to 1.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型没有离散的令牌来表示键（如“主语”或“动词”）；相反，它具有这些概念的矢量化表示，这些表示在训练期间学习到，因此用于查找的查询不会完全匹配字典中的任何键。解决方案是计算查询与字典中每个键之间的相似度度量，然后使用softmax函数将这些相似度分数转换为总和为1的权重。正如我们之前看到的那样，这正是注意力层所做的。如果代表动词的键与查询最相似，那么该键的权重将接近1。
- en: 'Next, the attention layer computes a weighted sum of the corresponding values:
    if the weight of the “verb” key is close to 1, then the weighted sum will be very
    close to the representation of the word “played”.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，注意力层计算相应值的加权和：如果“动词”键的权重接近1，那么加权和将非常接近单词“played”的表示。
- en: 'This is why the Keras `Attention` and `AdditiveAttention` layers both expect
    a list as input, containing two or three items: the *queries*, the *keys*, and
    optionally the *values*. If you do not pass any values, then they are automatically
    equal to the keys. So, looking at the previous code example again, the decoder
    outputs are the queries, and the encoder outputs are both the keys and the values.
    For each decoder output (i.e., each query), the attention layer returns a weighted
    sum of the encoder outputs (i.e., the keys/values) that are most similar to the
    decoder output.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么Keras的`Attention`和`AdditiveAttention`层都期望输入一个包含两个或三个项目的列表：*queries*，*keys*，以及可选的*values*。如果不传递任何值，则它们会自动等于键。因此，再次查看前面的代码示例，解码器输出是查询，编码器输出既是键也是值。对于每个解码器输出（即每个查询），注意力层返回与解码器输出最相似的编码器输出（即键/值）的加权和。
- en: The bottom line is that an attention mechanism is a trainable memory retrieval
    system. It is so powerful that you can actually build state-of-the-art models
    using only attention mechanisms. Enter the transformer architecture.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，注意力机制是一个可训练的内存检索系统。它非常强大，以至于您实际上可以仅使用注意力机制构建最先进的模型。进入变压器架构。
- en: 'Attention Is All You Need: The Original Transformer Architecture'
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力就是你所需要的：原始变压器架构
- en: In a groundbreaking [2017 paper](https://homl.info/transformer),⁠^([20](ch16.html#idm45720173607840))
    a team of Google researchers suggested that “Attention Is All You Need”. They
    created an architecture called the *transformer*, which significantly improved
    the state-of-the-art in NMT without using any recurrent or convolutional layers,⁠^([21](ch16.html#idm45720173604064))
    just attention mechanisms (plus embedding layers, dense layers, normalization
    layers, and a few other bits and pieces). Because the model is not recurrent,
    it doesn’t suffer as much from the vanishing or exploding gradients problems as
    RNNs, it can be trained in fewer steps, it’s easier to parallelize across multiple
    GPUs, and it can better capture long-range patterns than RNNs. The original 2017
    transformer architecture is represented in [Figure 16-8](#transformer_diagram).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇开创性的[2017年论文](https://homl.info/transformer)，⁠^([20](ch16.html#idm45720173607840))一组谷歌研究人员建议“注意力就是你所需要的”。他们创建了一种称为*变压器*的架构，显著改进了NMT的最新技术，而不使用任何循环或卷积层，仅使用注意力机制（加上嵌入层、稠密层、归一化层和其他一些部分）。由于该模型不是循环的，所以不像RNN那样容易受到梯度消失或梯度爆炸问题的困扰，可以在较少的步骤中训练，更容易在多个GPU上并行化，并且可以比RNN更好地捕捉长距离模式。原始的2017年变压器架构在[图16-8](#transformer_diagram)中表示。
- en: In short, the left part of [Figure 16-8](#transformer_diagram) is the encoder,
    and the right part is the decoder. Each embedding layer outputs a 3D tensor of
    shape [*batch size*, *sequence length*, *embedding size*]. After that, the tensors
    are gradually transformed as they flow through the transformer, but their shape
    remains the same.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，[图16-8](#transformer_diagram)的左侧是编码器，右侧是解码器。每个嵌入层输出一个形状为[*批量大小*，*序列长度*，*嵌入大小*]的3D张量。之后，随着数据流经变压器，张量逐渐转换，但形状保持不变。
- en: '![mls3 1608](assets/mls3_1608.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1608](assets/mls3_1608.png)'
- en: Figure 16-8\. The original 2017 transformer architecture⁠^([22](ch16.html#idm45720173579408))
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-8。原始的2017年变压器架构⁠^([22](ch16.html#idm45720173579408))
- en: If you use the transformer for NMT, then during training you must feed the English
    sentences to the encoder and the corresponding Spanish translations to the decoder,
    with an extra SOS token inserted at the start of each sentence. At inference time,
    you must call the transformer multiple times, producing the translations one word
    at a time and feeding the partial translations to the decoder at each round, just
    like we did earlier in the `translate()` function.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将变压器用于NMT，则在训练期间必须将英语句子馈送给编码器，将相应的西班牙语翻译馈送给解码器，并在每个句子开头插入额外的SOS令牌。在推理时，您必须多次调用变压器，逐字产生翻译，并在每轮将部分翻译馈送给解码器，就像我们之前在`translate()`函数中所做的那样。
- en: 'The encoder’s role is to gradually transform the inputs—word representations
    of the English sentence—until each word’s representation perfectly captures the
    meaning of the word, in the context of the sentence. For example, if you feed
    the encoder with the sentence “I like soccer”, then the word “like” will start
    off with a rather vague representation, since this word could mean different things
    in different contexts: think of “I like soccer” versus “It’s like that”. But after
    going through the encoder, the word’s representation should capture the correct
    meaning of “like” in the given sentence (i.e., to be fond of), as well as any
    other information that may be required for translation (e.g., it’s a verb).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的作用是逐渐转换输入——英文句子的单词表示——直到每个单词的表示完美地捕捉到单词的含义，在句子的上下文中。例如，如果你用句子“I like soccer”来喂给编码器，那么单词“like”将以一个相当模糊的表示开始，因为这个单词在不同的上下文中可能有不同的含义：想想“I
    like soccer”和“It’s like that”。但是经过编码器后，单词的表示应该捕捉到给定句子中“like”的正确含义（即喜欢），以及可能需要用于翻译的任何其他信息（例如，它是一个动词）。
- en: The decoder’s role is to gradually transform each word representation in the
    translated sentence into a word representation of the next word in the translation.
    For example, if the sentence to translate is “I like soccer”, and the decoder’s
    input sentence is “<SOS> me gusta el fútbol”, then after going through the decoder,
    the word representation of the word “el” will end up transformed into a representation
    of the word “fútbol”. Similarly, the representation of the word “fútbol” will
    be transformed into a representation of the EOS token.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的作用是逐渐将翻译句子中的每个单词表示转换为翻译中下一个单词的单词表示。例如，如果要翻译的句子是“I like soccer”，解码器的输入句子是“<SOS>
    me gusta el fútbol”，那么经过解码器后，“el”的单词表示将最终转换为“fútbol”的表示。同样，“fútbol”的表示将被转换为EOS标记的表示。
- en: After going through the decoder, each word representation goes through a final
    `Dense` layer with a softmax activation function, which will hopefully output
    a high probability for the correct next word and a low probability for all other
    words. The predicted sentence should be “me gusta el fútbol <EOS>”.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 经过解码器后，每个单词表示都经过一个带有softmax激活函数的最终`Dense`层，希望能够输出正确下一个单词的高概率和所有其他单词的低概率。预测的句子应该是“me
    gusta el fútbol <EOS>”。
- en: 'That was the big picture; now let’s walk through [Figure 16-8](#transformer_diagram)
    in more detail:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 那是大局观；现在让我们更详细地走一遍[图16-8](#transformer_diagram)：
- en: First, notice that both the encoder and the decoder contain modules that are
    stacked *N* times. In the paper, *N* = 6\. The final outputs of the whole encoder
    stack are fed to the decoder at each of these *N* levels.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，注意到编码器和解码器都包含被堆叠*N*次的模块。在论文中，*N* = 6。整个编码器堆栈的最终输出在每个这些*N*级别上被馈送到解码器。
- en: 'Zooming in, you can see that you are already familiar with most components:
    there are two embedding layers; several skip connections, each of them followed
    by a layer normalization layer; several feedforward modules that are composed
    of two dense layers each (the first one using the ReLU activation function, the
    second with no activation function); and finally the output layer is a dense layer
    using the softmax activation function. You can also sprinkle a bit of dropout
    after the attention layers and the feedforward modules, if needed. Since all of
    these layers are time-distributed, each word is treated independently from all
    the others. But how can we translate a sentence by looking at the words completely
    separately? Well, we can’t, so that’s where the new components come in:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 放大一下，你会发现你已经熟悉大部分组件：有两个嵌入层；几个跳跃连接，每个连接后面跟着一个层归一化层；几个由两个密集层组成的前馈模块（第一个使用ReLU激活函数，第二个没有激活函数）；最后输出层是使用softmax激活函数的密集层。如果需要的话，你也可以在注意力层和前馈模块之后添加一点dropout。由于所有这些层都是时间分布的，每个单词都独立于其他所有单词。但是我们如何通过完全分开地查看单词来翻译句子呢？嗯，我们不能，这就是新组件发挥作用的地方：
- en: The encoder’s *multi-head attention* layer updates each word representation
    by attending to (i.e., paying attention to) all other words in the same sentence.
    That’s where the vague representation of the word “like” becomes a richer and
    more accurate representation, capturing its precise meaning in the given sentence.
    We will discuss exactly how this works shortly.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的*多头注意力*层通过关注同一句子中的所有其他单词来更新每个单词的表示。这就是单词“like”的模糊表示变得更丰富和更准确的表示的地方，捕捉了它在给定句子中的确切含义。我们将很快讨论这是如何工作的。
- en: 'The decoder’s *masked multi-head attention* layer does the same thing, but
    when it processes a word, it doesn’t attend to words located after it: it’s a
    causal layer. For example, when it processes the word “gusta”, it only attends
    to the words “<SOS> me gusta”, and it ignores the words “el fútbol” (or else that
    would be cheating).'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的*掩码多头注意力*层做同样的事情，但当处理一个单词时，它不会关注在它之后的单词：这是一个因果层。例如，当处理单词“gusta”时，它只会关注“<SOS>
    me gusta”这几个单词，而忽略“el fútbol”这几个单词（否则那就是作弊了）。
- en: The decoder’s upper *multi-head attention* layer is where the decoder pays attention
    to the words in the English sentence. This is called *cross*-attention, not *self*-attention
    in this case. For example, the decoder will probably pay close attention to the
    word “soccer” when it processes the word “el” and transforms its representation
    into a representation of the word “fútbol”.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的上层*多头注意力*层是解码器关注英文句子中的单词的地方。这被称为*交叉*注意力，在这种情况下不是*自我*注意力。例如，当解码器处理单词“el”并将其表示转换为“fútbol”的表示时，解码器可能会密切关注单词“soccer”。
- en: 'The *positional encodings* are dense vectors (much like word embeddings) that
    represent the position of each word in the sentence. The *n*^(th) positional encoding
    is added to the word embedding of the *n*^(th) word in each sentence. This is
    needed because all layers in the transformer architecture ignore word positions:
    without positional encodings, you could shuffle the input sequences, and it would
    just shuffle the output sequences in the same way. Obviously, the order of words
    matters, which is why we need to give positional information to the transformer
    somehow: adding positional encodings to the word representations is a good way
    to achieve this.'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置编码*是密集向量（类似于单词嵌入），表示句子中每个单词的位置。第*n*个位置编码被添加到每个句子中第*n*个单词的单词嵌入中。这是因为变压器架构中的所有层都忽略单词位置：没有位置编码，您可以对输入序列进行洗牌，它只会以相同方式洗牌输出序列。显然，单词的顺序很重要，这就是为什么我们需要以某种方式向变压器提供位置信息的原因：将位置编码添加到单词表示是实现这一点的好方法。'
- en: Note
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The first two arrows going into each multi-head attention layer in [Figure 16-8](#transformer_diagram)
    represent the keys and values, and the third arrow represents the queries. In
    the self-attention layers, all three are equal to the word representations output
    by the previous layer, while in the decoder’s upper attention layer, the keys
    and values are equal to the encoder’s final word representations, and the queries
    are equal to the word representations output by the previous layer.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-8](#transformer_diagram)中每个多头注意力层的前两个箭头代表键和值，第三个箭头代表查询。在自注意力层中，所有三个都等于前一层输出的单词表示，而在解码器的上层注意力层中，键和值等于编码器的最终单词表示，查询等于前一层输出的单词表示。'
- en: Let’s go through the novel components of the transformer architecture in more
    detail, starting with the positional encodings.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解变压器架构中的新颖组件，从位置编码开始。
- en: Positional encodings
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'A positional encoding is a dense vector that encodes the position of a word
    within a sentence: the *i*^(th) positional encoding is added to the word embedding
    of the *i*^(th) word in the sentence. The easiest way to implement this is to
    use an `Embedding` layer and make it encode all the positions from 0 to the maximum
    sequence length in the batch, then add the result to the word embeddings. The
    rules of broadcasting will ensure that the positional encodings get applied to
    every input sequence. For example, here is how to add positional encodings to
    the encoder and decoder inputs:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码是一个密集向量，用于编码句子中单词的位置：第*i*个位置编码被添加到句子中第*i*个单词的单词嵌入中。实现这一点的最简单方法是使用`Embedding`层，并使其对批处理中从0到最大序列长度的所有位置进行编码，然后将结果添加到单词嵌入中。广播规则将确保位置编码应用于每个输入序列。例如，以下是如何将位置编码添加到编码器和解码器输入的方法：
- en: '[PRE45]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note that this implementation assumes that the embeddings are represented as
    regular tensors, not ragged tensors.^([23](ch16.html#idm45720173475360)) The encoder
    and the decoder share the same `Embedding` layer for the positional encodings,
    since they have the same embedding size (this is often the case).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此实现假定嵌入表示为常规张量，而不是不规则张量。^([23](ch16.html#idm45720173475360)) 编码器和解码器共享相同的`Embedding`层用于位置编码，因为它们具有相同的嵌入大小（这通常是这种情况）。
- en: Instead of using trainable positional encodings, the authors of the transformer
    paper chose to use fixed positional encodings, based on the sine and cosine functions
    at different frequencies. The positional encoding matrix **P** is defined in [Equation
    16-2](#positional_encodings_equation) and represented at the top of [Figure 16-9](#positional_encodings_diagram)
    (transposed), where *P*[*p*,*i*] is the *i*^(th) component of the encoding for
    the word located at the *p*^(th) position in the sentence.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器论文的作者选择使用基于正弦和余弦函数在不同频率下的固定位置编码，而不是使用可训练的位置编码。位置编码矩阵**P**在[方程16-2](#positional_encodings_equation)中定义，并在[图16-9](#positional_encodings_diagram)的顶部（转置）表示，其中*P*[*p*,*i*]是句子中位于第*p*位置的单词的编码的第*i*个分量。
- en: Equation 16-2\. Sine/cosine positional encodings
  id: totrans-257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程16-2。正弦/余弦位置编码
- en: <math><msub><mi>P</mi><mrow><mi>p</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><mfenced
    open="{" close=""><mtable columnalign="left center"><mtr><mtd><mi>sin</mi><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mi>i</mi><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>i</mi><mtext> is
    even</mtext></mtd></mtr><mtr><mtd><mi>cos</mi><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mo>(</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>)</mo><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>i</mi><mtext> is
    odd</mtext></mtd></mtr></mtable></mfenced></math>![mls3 1609](assets/mls3_1609.png)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: <math><msub><mi>P</mi><mrow><mi>p</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><mfenced
    open="{" close=""><mtable columnalign="left center"><mtr><mtd><mi>sin</mi><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mi>i</mi><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mtd><mtd><mtext>如果</mtext><mi>i</mi><mtext>是偶数</mtext></mtd></mtr><mtr><mtd><mi>cos</mi><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mo>(</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>)</mo><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mtd><mtd><mtext>如果</mtext><mi>i</mi><mtext>是奇数</mtext></mtd></mtr></mtable></mfenced></math>![mls3
    1609](assets/mls3_1609.png)
- en: Figure 16-9\. Sine/cosine positional encoding matrix (transposed, top) with
    a focus on two values of *i* (bottom)
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-9。正弦/余弦位置编码矩阵（转置，顶部）关注两个*i*值（底部）
- en: 'This solution can give the same performance as trainable positional encodings,
    and it can extend to arbitrarily long sentences without adding any parameters
    to the model (however, when there is a large amount of pretraining data, trainable
    positional encodings are usually favored). After these positional encodings are
    added to the word embeddings, the rest of the model has access to the absolute
    position of each word in the sentence because there is a unique positional encoding
    for each position (e.g., the positional encoding for the word located at the 22nd
    position in a sentence is represented by the vertical dashed line at the top left
    of [Figure 16-9](#positional_encodings_diagram), and you can see that it is unique
    to that position). Moreover, the choice of oscillating functions (sine and cosine)
    makes it possible for the model to learn relative positions as well. For example,
    words located 38 words apart (e.g., at positions *p* = 22 and *p* = 60) always
    have the same positional encoding values in the encoding dimensions *i* = 100
    and *i* = 101, as you can see in [Figure 16-9](#positional_encodings_diagram).
    This explains why we need both the sine and the cosine for each frequency: if
    we only used the sine (the blue wave at *i* = 100), the model would not be able
    to distinguish positions *p* = 22 and *p* = 35 (marked by a cross).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案可以提供与可训练位置编码相同的性能，并且可以扩展到任意长的句子，而不需要向模型添加任何参数（然而，当有大量预训练数据时，通常会优先选择可训练位置编码）。在这些位置编码添加到单词嵌入之后，模型的其余部分可以访问句子中每个单词的绝对位置，因为每个位置都有一个唯一的位置编码（例如，句子中位于第22个位置的单词的位置编码由[图16-9](#positional_encodings_diagram)左上角的垂直虚线表示，您可以看到它是唯一的）。此外，选择振荡函数（正弦和余弦）使模型能够学习相对位置。例如，相距38个单词的单词（例如，在位置*p*=22和*p*=60处）在编码维度*i*=100和*i*=101中始终具有相同的位置编码值，如[图16-9](#positional_encodings_diagram)所示。这解释了为什么我们需要每个频率的正弦和余弦：如果我们只使用正弦（*i*=100处的蓝色波），模型将无法区分位置*p*=22和*p*=35（由十字标记）。
- en: 'There is no `PositionalEncoding` layer in TensorFlow, but it is not too hard
    to create one. For efficiency reasons, we precompute the positional encoding matrix
    in the constructor. The `call()` method just truncates this encoding matrix to
    the max length of the input sequences, and it adds them to the inputs. We also
    set `supports_masking=True` to propagate the input’s automatic mask to the next
    layer:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中没有`PositionalEncoding`层，但创建一个并不太困难。出于效率原因，我们在构造函数中预先计算位置编码矩阵。`call()`方法只是将这个编码矩阵截断到输入序列的最大长度，并将其添加到输入中。我们还设置`supports_masking=True`以将输入的自动掩码传播到下一层：
- en: '[PRE46]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let’s use this layer to add the positional encoding to the encoder’s inputs:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个层将位置编码添加到编码器的输入中：
- en: '[PRE47]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now let’s look deeper into the heart of the transformer model, at the multi-head
    attention layer.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更深入地看一下变压器模型的核心，即多头注意力层。
- en: Multi-head attention
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多头注意力
- en: To understand how a multi-head attention layer works, we must first understand
    the *scaled dot-product attention* layer, which it is based on. Its equation is
    shown in [Equation 16-3](#scaled_dot_product_attention), in a vectorized form.
    It’s the same as Luong attention, except for a scaling factor.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解多头注意力层的工作原理，我们首先必须了解它基于的*缩放点积注意力*层。它的方程式在[方程式16-3](#scaled_dot_product_attention)中以矢量化形式显示。它与Luong注意力相同，只是有一个缩放因子。
- en: Equation 16-3\. Scaled dot-product attention
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式16-3\. 缩放点积注意力
- en: <math display="block"><mo>Attention</mo><mrow><mo>(</mo><mi mathvariant="bold">Q</mi><mo>,</mo><mi
    mathvariant="bold">K</mi><mo>,</mo><mi mathvariant="bold">V</mi><mo>)</mo></mrow><mo>=</mo><mo>softmax</mo><mfenced><mfrac><mrow><msup><mi
    mathvariant="bold">QK</mi><mo>⊺</mo></msup></mrow><msqrt><msub><mi>d</mi><mrow><mi>k</mi><mi>e</mi><mi>y</mi><mi>s</mi></mrow></msub></msqrt></mfrac></mfenced><mi
    mathvariant="bold">V</mi></math>
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mo>注意力</mo><mrow><mo>(</mo><mi mathvariant="bold">Q</mi><mo>,</mo><mi
    mathvariant="bold">K</mi><mo>,</mo><mi mathvariant="bold">V</mi><mo>)</mo></mrow><mo>=</mo><mo>softmax</mo><mfenced><mfrac><mrow><msup><mi
    mathvariant="bold">QK</mi><mo>⊺</mo></msup></mrow><msqrt><msub><mi d</mi><mrow><mi
    k</mi><mi e</mi><mi y</mi><mi s</mi></mrow></msub></msqrt></mfrac></mfenced><mi
    mathvariant="bold">V</mi></math>
- en: 'In this equation:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '**Q** is a matrix containing one row per *query*. Its shape is [*n*[queries],
    *d*[keys]], where *n*[queries] is the number of queries and *d*[keys] is the number
    of dimensions of each query and each key.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q**是包含每个*查询*的一行的矩阵。其形状为[*n*[queries], *d*[keys]]，其中*n*[queries]是查询的数量，*d*[keys]是每个查询和每个键的维度数量。'
- en: '**K** is a matrix containing one row per *key*. Its shape is [*n*[keys], *d*[keys]],
    where *n*[keys] is the number of keys and values.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K**是包含每个*键*的一行的矩阵。其形状为[*n*[keys], *d*[keys]]，其中*n*[keys]是键和值的数量。'
- en: '**V** is a matrix containing one row per *value*. Its shape is [*n*[keys],
    *d*[values]], where *d*[values] is the number of dimensions of each value.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V**是包含每个*值*的一行的矩阵。其形状为[*n*[keys], *d*[values]]，其中*d*[values]是每个值的维度数量。'
- en: 'The shape of **Q** **K**^⊺ is [*n*[queries], *n*[keys]]: it contains one similarity
    score for each query/key pair. To prevent this matrix from being huge, the input
    sequences must not be too long (we will discuss how to overcome this limitation
    later in this chapter). The output of the softmax function has the same shape,
    but all rows sum up to 1\. The final output has a shape of [*n*[queries], *d*[values]]:
    there is one row per query, where each row represents the query result (a weighted
    sum of the values).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q** **K**^⊺的形状为[*n*[queries], *n*[keys]]：它包含每个查询/键对的一个相似度分数。为了防止这个矩阵过大，输入序列不能太长（我们将在本章后面讨论如何克服这个限制）。softmax函数的输出具有相同的形状，但所有行的总和为1。最终输出的形状为[*n*[queries],
    *d*[values]]：每个查询有一行，其中每行代表查询结果（值的加权和）。'
- en: The scaling factor 1 / (<math><msqrt><msub><mi>d</mi> <mrow><mi>keys</mi></mrow></msub></msqrt></math>)
    scales down the similarity scores to avoid saturating the softmax function, which
    would lead to tiny gradients.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放因子1 / (<math><msqrt><msub><mi d</mi> <mrow><mi keys</mi></mrow></msub></msqrt></math>)将相似度分数缩小，以避免饱和softmax函数，这会导致梯度很小。
- en: It is possible to mask out some key/value pairs by adding a very large negative
    value to the corresponding similarity scores, just before computing the softmax.
    This is useful in the masked multi-head attention layer.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过在计算softmax之前，向相应的相似性分数添加一个非常大的负值来屏蔽一些键/值对，这在掩码多头注意力层中非常有用。
- en: If you set `use_scale=True` when creating a `tf.keras.layers.Attention` layer,
    then it will create an additional parameter that lets the layer learn how to properly
    downscale the similarity scores. The scaled dot-product attention used in the
    transformer model is almost the same, except it always scales the similarity scores
    by the same factor, 1 / (<math><msqrt><msub><mi>d</mi> <mrow><mi>keys</mi></mrow></msub></msqrt></math>).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在创建`tf.keras.layers.Attention`层时设置`use_scale=True`，那么它将创建一个额外的参数，让该层学习如何正确地降低相似性分数。变压器模型中使用的缩放后的点积注意力几乎相同，只是它总是将相似性分数按相同因子缩放，即1
    / (<math><msqrt><msub><mi>d</mi> <mrow><mi>keys</mi></mrow></msub></msqrt></math>)。
- en: 'Note that the `Attention` layer’s inputs are just like **Q**, **K**, and **V**,
    except with an extra batch dimension (the first dimension). Internally, the layer
    computes all the attention scores for all sentences in the batch with just one
    call to `tf.matmul(queries, keys)`: this makes it extremely efficient. Indeed,
    in TensorFlow, if `A` and `B` are tensors with more than two dimensions—say, of
    shape [2, 3, 4, 5] and [2, 3, 5, 6], respectively—then `tf.matmul(A, B)` will
    treat these tensors as 2 × 3 arrays where each cell contains a matrix, and it
    will multiply the corresponding matrices: the matrix at the *i*^(th) row and *j*^(th)
    column in `A` will be multiplied by the matrix at the *i*^(th) row and *j*^(th)
    column in `B`. Since the product of a 4 × 5 matrix with a 5 × 6 matrix is a 4 × 6
    matrix, `tf.matmul(A, B)` will return an array of shape [2, 3, 4, 6].'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`Attention`层的输入就像**Q**、**K**和**V**一样，只是多了一个批处理维度（第一个维度）。在内部，该层仅通过一次调用`tf.matmul(queries,
    keys)`计算批处理中所有句子的所有注意力分数：这使得它非常高效。实际上，在TensorFlow中，如果`A`和`B`是具有两个以上维度的张量，比如形状为[2,
    3, 4, 5]和[2, 3, 5, 6]，那么`tf.matmul(A, B)`将把这些张量视为2×3数组，其中每个单元格包含一个矩阵，并将相应的矩阵相乘：`A`中第*i*行和第*j*列的矩阵将与`B`中第*i*行和第*j*列的矩阵相乘。由于一个4×5矩阵与一个5×6矩阵的乘积是一个4×6矩阵，`tf.matmul(A,
    B)`将返回一个形状为[2, 3, 4, 6]的数组。
- en: Now we’re ready to look at the multi-head attention layer. Its architecture
    is shown in [Figure 16-10](#multihead_attention_diagram).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备看一下多头注意力层。其架构如[图16-10](#multihead_attention_diagram)所示。
- en: '![mls3 1610](assets/mls3_1610.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1610](assets/mls3_1610.png)'
- en: Figure 16-10\. Multi-head attention layer architecture⁠^([24](ch16.html#idm45720173058848))
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-10. 多头注意力层架构
- en: As you can see, it is just a bunch of scaled dot-product attention layers, each
    preceded by a linear transformation of the values, keys, and queries (i.e., a
    time-distributed dense layer with no activation function). All the outputs are
    simply concatenated, and they go through a final linear transformation (again,
    time-distributed).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，它只是一堆缩放后的点积注意力层，每个层之前都有一个值、键和查询的线性变换（即没有激活函数的时间分布密集层）。所有输出都简单地连接在一起，并通过最终的线性变换（再次是时间分布的）。
- en: But why? What is the intuition behind this architecture? Well, consider once
    again the word “like” in the sentence “I like soccer”. The encoder was smart enough
    to encode the fact that it is a verb. But the word representation also includes
    its position in the text, thanks to the positional encodings, and it probably
    includes many other features that are useful for its translation, such as the
    fact that it is in the present tense. In short, the word representation encodes
    many different characteristics of the word. If we just used a single scaled dot-product
    attention layer, we would only be able to query all of these characteristics in
    one shot.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 但是为什么？这种架构背后的直觉是什么？好吧，再次考虑一下句子“I like soccer”中的单词“like”。编码器足够聪明，能够编码它是一个动词的事实。但是单词表示还包括其在文本中的位置，这要归功于位置编码，它可能还包括许多其他对其翻译有用的特征，比如它是现在时。简而言之，单词表示编码了单词的许多不同特征。如果我们只使用一个缩放后的点积注意力层，我们只能一次性查询所有这些特征。
- en: 'This is why the multi-head attention layer applies *multiple* different linear
    transformations of the values, keys, and queries: this allows the model to apply
    many different projections of the word representation into different subspaces,
    each focusing on a subset of the word’s characteristics. Perhaps one of the linear
    layers will project the word representation into a subspace where all that remains
    is the information that the word is a verb, another linear layer will extract
    just the fact that it is present tense, and so on. Then the scaled dot-product
    attention layers implement the lookup phase, and finally we concatenate all the
    results and project them back to the original space.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么多头注意力层应用*多个*不同的线性变换值、键和查询：这使得模型能够将单词表示的许多不同特征投影到不同的子空间中，每个子空间都专注于单词的某些特征。也许其中一个线性层将单词表示投影到一个只剩下单词是动词信息的子空间，另一个线性层将提取出它是现在时的事实，依此类推。然后缩放后的点积注意力层实现查找阶段，最后我们将所有结果连接起来并将它们投影回原始空间。
- en: 'Keras includes a `tf.keras.layers.MultiHeadAttention` layer, so we now have
    everything we need to build the rest of the transformer. Let’s start with the
    full encoder, which is exactly like in [Figure 16-8](#transformer_diagram), except
    we use a stack of two blocks (`N = 2`) instead of six, since we don’t have a huge
    training set, and we add a bit of dropout as well:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Keras包括一个`tf.keras.layers.MultiHeadAttention`层，因此我们现在拥有构建变压器其余部分所需的一切。让我们从完整的编码器开始，它与[图16-8](#transformer_diagram)中的完全相同，只是我们使用两个块的堆叠（`N
    = 2`）而不是六个，因为我们没有一个庞大的训练集，并且我们还添加了一点辍学：
- en: '[PRE48]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This code should be mostly straightforward, except for one thing: masking.
    As of the time of writing, the `MultiHeadAttention` layer does not support automatic
    masking,⁠^([25](ch16.html#idm45720173048224)) so we must handle it manually. How
    can we do that?'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码应该大多数都很简单，除了一个问题：掩码。在撰写本文时，`MultiHeadAttention`层不支持自动掩码，因此我们必须手动处理。我们该如何做？
- en: 'The `MultiHeadAttention` layer accepts an `attention_mask` argument, which
    is a Boolean tensor of shape [*batch size*, *max query length*, *max value length*]:
    for every token in every query sequence, this mask indicates which tokens in the
    corresponding value sequence should be attended to. We want to tell the `MultiHeadAttention`
    layer to ignore all the padding tokens in the values. So, we first compute the
    padding mask using `tf.math.not_equal(encoder_input_ids, 0)`. This returns a Boolean
    tensor of shape [*batch size*, *max sequence length*]. We then insert a second
    axis using `[:, tf.newaxis]`, to get a mask of shape [*batch size*, 1, *max sequence
    length*]. This allows us to use this mask as the `attention_mask` when calling
    the `MultiHead​Atten⁠tion` layer: thanks to broadcasting, the same mask will be
    used for all tokens in each query. This way, the padding tokens in the values
    will be ignored correctly.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultiHeadAttention`层接受一个`attention_mask`参数，这是一个形状为[*batch size*, *max query
    length*, *max value length*]的布尔张量：对于每个查询序列中的每个标记，这个掩码指示应该关注对应值序列中的哪些标记。我们想告诉`MultiHeadAttention`层忽略值中的所有填充标记。因此，我们首先使用`tf.math.not_equal(encoder_input_ids,
    0)`计算填充掩码。这将返回一个形状为[*batch size*, *max sequence length*]的布尔张量。然后我们使用`[:, tf.newaxis]`插入第二个轴，得到形状为[*batch
    size*, 1, *max sequence length*]的掩码。这使我们能够在调用`MultiHeadAttention`层时将此掩码用作`attention_mask`：由于广播，相同的掩码将用于每个查询中的所有标记。这样，值中的填充标记将被正确忽略。'
- en: 'However, the layer will compute outputs for every single query token, including
    the padding tokens. We need to mask the outputs that correspond to these padding
    tokens. Recall that we used `mask_zero` in the `Embedding` layers, and we set
    `supports_masking` to `True` in the `PositionalEncoding` layer, so the automatic
    mask was propagated all the way to the `MultiHeadAttention` layer’s inputs (`encoder_in`).
    We can use this to our advantage in the skip connection: indeed, the `Add` layer
    supports automatic masking, so when we add `Z` and `skip` (which is initially
    equal to `encoder_in`), the outputs get automatically masked correctly.^([26](ch16.html#idm45720172796192))
    Yikes! Masking required much more explanation than code.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该层将为每个单独的查询标记计算输出，包括填充标记。我们需要掩盖与这些填充标记对应的输出。回想一下，在`Embedding`层中我们使用了`mask_zero`，并且在`PositionalEncoding`层中我们将`supports_masking`设置为`True`，因此自动掩码一直传播到`MultiHeadAttention`层的输入(`encoder_in`)。我们可以利用这一点在跳过连接中：实际上，`Add`层支持自动掩码，因此当我们将`Z`和`skip`（最初等于`encoder_in`）相加时，输出将自动正确掩码。天啊！掩码需要比代码更多的解释。
- en: 'Now on to the decoder! Once again, masking is going to be the only tricky part,
    so let’s start with that. The first multi-head attention layer is a self-attention
    layer, like in the encoder, but it is a *masked* multi-head attention layer, meaning
    it is causal: it should ignore all tokens in the future. So, we need two masks:
    a padding mask and a causal mask. Let’s create them:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始解码器！再次，掩码将是唯一棘手的部分，所以让我们从那里开始。第一个多头注意力层是一个自注意力层，就像在编码器中一样，但它是一个*掩码*多头注意力层，这意味着它是因果的：它应该忽略未来的所有标记。因此，我们需要两个掩码：一个填充掩码和一个因果掩码。让我们创建它们：
- en: '[PRE49]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The padding mask is exactly like the one we created for the encoder, except
    it’s based on the decoder’s inputs rather than the encoder’s. The causal mask
    is created using the `tf.linalg.band_part()` function, which takes a tensor and
    returns a copy with all the values outside a diagonal band set to zero. With these
    arguments, we get a square matrix of size `batch_max_len_dec` (the max length
    of the input sequences in the batch), with 1s in the lower-left triangle and 0s
    in the upper right. If we use this mask as the attention mask, we will get exactly
    what we want: the first query token will only attend to the first value token,
    the second will only attend to the first two, the third will only attend to the
    first three, and so on. In other words, query tokens cannot attend to any value
    token in the future.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 填充掩码与我们为编码器创建的掩码完全相同，只是基于解码器的输入而不是编码器的。因果掩码使用`tf.linalg.band_part()`函数创建，该函数接受一个张量并返回一个将对角线带外的所有值设置为零的副本。通过这些参数，我们得到一个大小为`batch_max_len_dec`（批处理中输入序列的最大长度）的方阵，左下三角形中为1，右上角为0。如果我们将此掩码用作注意力掩码，我们将得到我们想要的：第一个查询标记只会关注第一个值标记，第二个只会关注前两个，第三个只会关注前三个，依此类推。换句话说，查询标记不能关注未来的任何值标记。
- en: 'Let’s now build the decoder:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建解码器：
- en: '[PRE50]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'For the first attention layer, we use `causal_mask & decoder_pad_mask` to mask
    both the padding tokens and future tokens. The causal mask only has two dimensions:
    it’s missing the batch dimension, but that’s okay since broadcasting ensures that
    it gets copied across all the instances in the batch.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个注意力层，我们使用`causal_mask & decoder_pad_mask`来同时掩盖填充标记和未来标记。因果掩码只有两个维度：它缺少批处理维度，但这没关系，因为广播确保它在批处理中的所有实例中被复制。
- en: For the second attention layer, there’s nothing special. The only thing to note
    is that we are using `encoder_pad_mask`, not `decoder_pad_mask`, because this
    attention layer uses the encoder’s final outputs as its values.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个注意力层，没有特别之处。唯一需要注意的是我们使用`encoder_pad_mask`而不是`decoder_pad_mask`，因为这个注意力层使用编码器的最终输出作为其值。
- en: 'We’re almost done. We just need to add the final output layer, create the model,
    compile it, and train it:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快要完成了。我们只需要添加最终的输出层，创建模型，编译它，然后训练它：
- en: '[PRE51]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Congratulations! You’ve built a full transformer from scratch, and trained it
    for automatic translation. This is getting quite advanced!
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经从头开始构建了一个完整的Transformer，并对其进行了自动翻译的训练。这变得相当高级了！
- en: Tip
  id: totrans-300
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The Keras team has created a new [Keras NLP project](https://github.com/keras-team/keras-nlp),
    including an API to build a transformer more easily. You may also be interested
    in the new [Keras CV project for computer vision](https://github.com/keras-team/keras-cv).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Keras团队创建了一个新的[Keras NLP项目](https://github.com/keras-team/keras-nlp)，其中包括一个API，可以更轻松地构建变压器。您可能还对新的[Keras
    CV项目（用于计算机视觉）](https://github.com/keras-team/keras-cv)感兴趣。
- en: But the field didn’t stop there. Let’s now explore some of the recent advances.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 但领域并没有就此停止。现在让我们来探讨一些最近的进展。
- en: An Avalanche of Transformer Models
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器模型的大量涌现
- en: The year 2018 has been called the “ImageNet moment for NLP”. Since then, progress
    has been astounding, with larger and larger transformer-based architectures trained
    on immense datasets.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年被称为NLP的“ImageNet时刻”。从那时起，进展一直令人震惊，基于巨大数据集训练的基于变压器的架构越来越大。
- en: 'First, the [GPT paper](https://homl.info/gpt)⁠^([27](ch16.html#idm45720172319856))
    by Alec Radford and other OpenAI researchers once again demonstrated the effectiveness
    of unsupervised pretraining, like the ELMo and ULMFiT papers before it, but this
    time using a transformer-like architecture. The authors pretrained a large but
    fairly simple architecture composed of a stack of 12 transformer modules using
    only masked multi-head attention layers, like in the original transformer’s decoder.
    They trained it on a very large dataset, using the same autoregressive technique
    we used for our Shakespearean char-RNN: just predict the next token. This is a
    form of self-supervised learning. Then they fine-tuned it on various language
    tasks, using only minor adaptations for each task. The tasks were quite diverse:
    they included text classification, *entailment* (whether sentence A imposes, involves,
    or implies sentence B as a necessary consequence),⁠^([28](ch16.html#idm45720172314768))
    similarity (e.g., “Nice weather today” is very similar to “It is sunny”), and
    question answering (given a few paragraphs of text giving some context, the model
    must answer some multiple-choice questions).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Alec Radford和其他OpenAI研究人员的[GPT论文](https://homl.info/gpt)再次展示了无监督预训练的有效性，就像ELMo和ULMFiT论文之前一样，但这次使用了类似变压器的架构。作者们预训练了一个由12个变压器模块堆叠而成的大型但相当简单的架构，只使用了像原始变压器解码器中的掩码多头注意力层。他们在一个非常庞大的数据集上进行了训练，使用了我们用于莎士比亚char-RNN的相同自回归技术：只需预测下一个标记。这是一种自监督学习形式。然后，他们对各种语言任务进行了微调，每个任务只进行了轻微的调整。这些任务非常多样化：它们包括文本分类、*蕴涵*（句子A是否对句子B施加、涉及或暗示必要的后果）、相似性（例如，“今天天气很好”与“阳光明媚”非常相似）和问答（给定一些提供一些背景的文本段落，模型必须回答一些多项选择题）。
- en: 'Then Google’s [BERT paper](https://homl.info/bert)⁠^([29](ch16.html#idm45720172313104))
    came out: it also demonstrated the effectiveness of self-supervised pretraining
    on a large corpus, using a similar architecture to GPT but with nonmasked multi-head
    attention layers only, like in the original transformer’s encoder. This means
    that the model is naturally bidirectional; hence the B in BERT (*Bidirectional
    Encoder Representations from Transformers*). Most importantly, the authors proposed
    two pretraining tasks that explain most of the model’s strength:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 然后谷歌的[BERT论文](https://homl.info/bert)出现了：它也展示了在大型语料库上进行自监督预训练的有效性，使用了与GPT类似的架构，但只使用了非掩码多头注意力层，就像原始变压器的编码器中一样。这意味着模型是自然双向的；因此BERT中的B（来自变压器的双向编码器表示）。最重要的是，作者提出了两个预训练任务，解释了模型大部分的强度：
- en: Masked language model (MLM)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码语言模型（MLM）
- en: Each word in a sentence has a 15% probability of being masked, and the model
    is trained to predict the masked words. For example, if the original sentence
    is “She had fun at the birthday party”, then the model may be given the sentence
    “She <mask> fun at the <mask> party” and it must predict the words “had” and “birthday”
    (the other outputs will be ignored). To be more precise, each selected word has
    an 80% chance of being masked, a 10% chance of being replaced by a random word
    (to reduce the discrepancy between pretraining and fine-tuning, since the model
    will not see <mask> tokens during fine-tuning), and a 10% chance of being left
    alone (to bias the model toward the correct answer).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中的每个单词有15%的概率被掩盖，模型经过训练，以预测被掩盖的单词。例如，如果原始句子是“她在生日聚会上玩得很开心”，那么模型可能会得到句子“她<mask>在<mask>聚会上玩得很开心”，它必须预测单词“had”和“birthday”（其他输出将被忽略）。更准确地说，每个选择的单词有80%的概率被掩盖，10%的概率被替换为随机单词（为了减少预训练和微调之间的差异，因为模型在微调过程中不会看到<mask>标记），以及10%的概率被保留（以偏向模型正确答案）。
- en: Next sentence prediction (NSP)
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个句子预测（NSP）
- en: The model is trained to predict whether two sentences are consecutive or not.
    For example, it should predict that “The dog sleeps” and “It snores loudly” are
    consecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun” are
    not consecutive. Later research showed that NSP was not as important as was initially
    thought, so it was dropped in most later architectures.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型经过训练，以预测两个句子是否连续。例如，它应该预测“狗在睡觉”和“它打呼噜”是连续的句子，而“狗在睡觉”和“地球绕着太阳转”不是连续的。后来的研究表明，NSP并不像最初认为的那么重要，因此在大多数后来的架构中被放弃了。
- en: 'The model is trained on these two tasks simultaneously (see [Figure 16-11](#bert_diagram)).
    For the NSP task, the authors inserted a class token (<CLS>) at the start of every
    input, and the corresponding output token represents the model’s prediction: sentence
    B follows sentence A, or it does not. The two input sentences are concatenated,
    separated only by a special separation token (<SEP>), and they are fed as input
    to the model. To help the model know which sentence each input token belongs to,
    a *segment embedding* is added on top of each token’s positional embeddings: there
    are just two possible segment embeddings, one for sentence A and one for sentence
    B. For the MLM task, some input words are masked (as we just saw) and the model
    tries to predict what those words were. The loss is only computed on the NSP prediction
    and the masked tokens, not on the unmasked ones.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1611](assets/mls3_1611.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: Figure 16-11\. BERT training and fine-tuning process⁠^([30](ch16.html#idm45720172299632))
  id: totrans-313
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After this unsupervised pretraining phase on a very large corpus of text, the
    model is then fine-tuned on many different tasks, changing very little for each
    task. For example, for text classification such as sentiment analysis, all output
    tokens are ignored except for the first one, corresponding to the class token,
    and a new output layer replaces the previous one, which was just a binary classification
    layer for NSP.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'In February 2019, just a few months after BERT was published, Alec Radford,
    Jeffrey Wu, and other OpenAI researchers published the [GPT-2 paper](https://homl.info/gpt2),⁠^([31](ch16.html#idm45720172296288))
    which proposed a very similar architecture to GPT, but larger still (with over
    1.5 billion parameters!). The researchers showed that the new and improved GPT
    model could perform *zero-shot learning* (ZSL), meaning it could achieve good
    performance on many tasks without any fine-tuning. This was just the start of
    a race toward larger and larger models: Google’s [Switch Transformers](https://homl.info/switch)⁠^([32](ch16.html#idm45720172293488))
    (introduced in January 2021) used 1 trillion parameters, and soon much larger
    models came out, such as the Wu Dao 2.0 model by the Beijing Academy of Artificial
    Intelligence (BAII), announced in June 2021.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'An unfortunate consequence of this trend toward gigantic models is that only
    well-funded organizations can afford to train such models: it can easily cost
    hundreds of thousands of dollars or more. And the energy required to train a single
    model corresponds to an American household’s electricity consumption for several
    years; it’s not eco-friendly at all. Many of these models are just too big to
    even be used on regular hardware: they wouldn’t fit in RAM, and they would be
    horribly slow. Lastly, some are so costly that they are not released publicly.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, ingenious researchers are finding new ways to downsize transformers
    and make them more data-efficient. For example, the [DistilBERT model](https://homl.info/distilbert),⁠^([33](ch16.html#idm45720172289264))
    introduced in October 2019 by Victor Sanh et al. from Hugging Face, is a small
    and fast transformer model based on BERT. It is available on Hugging Face’s excellent
    model hub, along with thousands of others—you’ll see an example later in this
    chapter.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'DistilBERT was trained using *distillation* (hence the name): this means transferring
    knowledge from a teacher model to a student one, which is usually much smaller
    than the teacher model. This is typically done by using the teacher’s predicted
    probabilities for each training instance as targets for the student. Surprisingly,
    distillation often works better than training the student from scratch on the
    same dataset as the teacher! Indeed, the student benefits from the teacher’s more
    nuanced labels.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Many more transformer architectures came out after BERT, almost on a monthly
    basis, often improving on the state of the art across all NLP tasks: XLNet (June
    2019), RoBERTa (July 2019), StructBERT (August 2019), ALBERT (September 2019),
    T5 (October 2019), ELECTRA (March 2020), GPT3 (May 2020), DeBERTa (June 2020),
    Switch Transformers (January 2021), Wu Dao 2.0 (June 2021), Gopher (December 2021),
    GPT-NeoX-20B (February 2022), Chinchilla (March 2022), OPT (May 2022), and the
    list goes on and on. Each of these models brought new ideas and techniques,^([34](ch16.html#idm45720172285632))
    but I particularly like the [T5 paper](https://homl.info/t5)⁠^([35](ch16.html#idm45720172282624))
    by Google researchers: it frames all NLP tasks as text-to-text, using an encoder–decoder
    transformer. For example, to translate “I like soccer” to Spanish, you can just
    call the model with the input sentence “translate English to Spanish: I like soccer”
    and it outputs “me gusta el fútbol”. To summarize a paragraph, you just enter
    “summarize:” followed by the paragraph, and it outputs the summary. For classification,
    you only need to change the prefix to “classify:” and the model outputs the class
    name, as text. This simplifies using the model, and it also makes it possible
    to pretrain it on even more tasks.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '在BERT之后，还有更多的transformer架构陆续推出，几乎每个月都有，通常在所有NLP任务的最新技术上有所改进：XLNet（2019年6月），RoBERTa（2019年7月），StructBERT（2019年8月），ALBERT（2019年9月），T5（2019年10月），ELECTRA（2020年3月），GPT3（2020年5月），DeBERTa（2020年6月），Switch
    Transformers（2021年1月），Wu Dao 2.0（2021年6月），Gopher（2021年12月），GPT-NeoX-20B（2022年2月），Chinchilla（2022年3月），OPT（2022年5月），等等。每个模型都带来了新的想法和技术，但我特别喜欢谷歌研究人员的[T5论文](https://homl.info/t5)：它将所有NLP任务都框定为文本到文本，使用编码器-解码器transformer。例如，要将“I
    like soccer”翻译成西班牙语，您只需用输入句子“translate English to Spanish: I like soccer”调用模型，它会输出“me
    gusta el fútbol”。要总结一段文字，您只需输入“summarize:”后跟段落，它会输出摘要。对于分类，只需将前缀更改为“classify:”，模型会输出类名，作为文本。这简化了使用模型，也使其能够在更多任务上进行预训练。'
- en: Last but not least, in April 2022, Google researchers used a new large-scale
    training platform named *Pathways* (which we will briefly discuss in [Chapter 19](ch19.html#deployment_chapter))
    to train a humongous language model named the [*Pathways Language Model* (PaLM)](https://homl.info/palm),⁠^([36](ch16.html#idm45720172277824))
    with a whopping 540 billion parameters, using over 6,000 TPUs. Other than its
    incredible size, this model is a standard transformer, using decoders only (i.e.,
    with masked multi-head attention layers), with just a few tweaks (see the paper
    for details). This model achieved incredible performance on all sorts of NLP tasks,
    particularly in natural language understanding (NLU). It’s capable of impressive
    feats, such as explaining jokes, giving detailed step-by-step answers to questions,
    and even coding. This is in part due to the model’s size, but also thanks to a
    technique called [*Chain of thought prompting*](https://homl.info/ctp),⁠^([37](ch16.html#idm45720172273920))
    which was introduced a couple months earlier by another team of Google researchers.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但并非最不重要的是，在2022年4月，谷歌研究人员使用了一个名为*Pathways*的新大规模训练平台（我们将在[第19章](ch19.html#deployment_chapter)中简要讨论），来训练一个名为[*Pathways语言模型*（PaLM）](https://homl.info/palm)，拥有惊人的5400亿个参数，使用了超过6000个TPU。除了其令人难以置信的规模之外，这个模型是一个标准的transformer，只使用解码器（即，带有掩码多头注意力层），只有一些微调（详细信息请参阅论文）。这个模型在各种NLP任务中取得了令人难以置信的表现，特别是在自然语言理解（NLU）方面。它能够完成令人印象深刻的壮举，比如解释笑话，给出详细的逐步回答问题的答案，甚至编码。这在一定程度上归功于模型的规模，也归功于一种称为[*思维链提示*](https://homl.info/ctp)的技术，这种技术是几个月前由另一个谷歌研究团队引入的。
- en: 'In question answering tasks, regular prompting typically includes a few examples
    of questions and answers, such as: “Q: Roger has 5 tennis balls. He buys 2 more
    cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does
    he have now? A: 11.” The prompt then continues with the actual question, such
    as “Q: John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take
    care of their business. How many hours a week does he spend taking care of dogs?
    A:”, and the model’s job is to append the answer: in this case, “35.”'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '在问答任务中，常规提示通常包括一些问题和答案的示例，例如：“Q: Roger有5个网球。他买了2罐网球。每罐有3个网球。他现在有多少网球？A: 11。”然后提示继续提出实际问题，比如“Q:
    John照顾10只狗。每只狗每天需要0.5小时散步和照顾自己的事务。他每周花多少时间照顾狗？A:”，模型的任务是附加答案：在这种情况下是“35”。'
- en: 'But with chain of thought prompting, the example answers include all the reasoning
    steps that lead to the conclusion. For example, instead of “A: 11”, the prompt
    contains “A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis
    balls. 5 + 6 = 11.” This encourages the model to give a detailed answer to the
    actual question, such as “John takes care of 10 dogs. Each dog takes .5 hours
    a day to walk and take care of their business. So that is 10 × .5 = 5 hours a
    day. 5 hours a day × 7 days a week = 35 hours a week. The answer is 35 hours a
    week.” This is an actual example from the paper!'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '但是通过思维链提示，示例答案包括导致结论的所有推理步骤。例如，不是“A: 11”，提示包含“A: Roger从5个球开始。2罐每罐3个网球，总共6个网球。5
    + 6 = 11。”这鼓励模型给出对实际问题的详细答案，比如“John照顾10只狗。每只狗每天需要0.5小时散步和照顾自己的事务。所以是10 × 0.5 =
    5小时每天。5小时每天 × 7天每周 = 35小时每周。答案是每周35小时。”这是论文中的一个实际例子！'
- en: Not only does the model give the right answer much more frequently than using
    regular prompting—we’re encouraging the model to think things through—but it also
    provides all the reasoning steps, which can be useful to better understand the
    rationale behind a model’s answer.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型不仅比使用常规提示更频繁地给出正确答案——我们鼓励模型深思熟虑——而且还提供了所有推理步骤，这对于更好地理解模型答案背后的原理是有用的。
- en: 'Transformers have taken over NLP, but they didn’t stop there: they soon expanded
    to computer vision as well.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: transformers已经在NLP领域占据了主导地位，但它们并没有止步于此：它们很快也扩展到了计算机视觉领域。
- en: Vision Transformers
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉transformers
- en: One of the first applications of attention mechanisms beyond NMT was in generating
    image captions using [visual attention](https://homl.info/visualattention):⁠^([38](ch16.html#idm45720172266576))
    a convolutional neural network first processes the image and outputs some feature
    maps, then a decoder RNN equipped with an attention mechanism generates the caption,
    one word at a time.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制在NMT之外的第一个应用是使用[视觉注意力](https://homl.info/visualattention)生成图像字幕：一个卷积神经网络首先处理图像并输出一些特征图，然后一个带有注意力机制的解码器RNN逐个单词生成字幕。
- en: 'At each decoder time step (i.e., each word), the decoder uses the attention
    model to focus on just the right part of the image. For example, in [Figure 16-12](#visual_attention_diagram),
    the model generated the caption “A woman is throwing a frisbee in a park”, and
    you can see what part of the input image the decoder focused its attention on
    when it was about to output the word “frisbee”: clearly, most of its attention
    was focused on the frisbee.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个解码器时间步骤（即每个单词），解码器使用注意力模型专注于图像的正确部分。例如，在[图16-12](#visual_attention_diagram)中，模型生成了字幕“A
    woman is throwing a frisbee in a park”，您可以看到当解码器准备输出单词“frisbee”时，它的注意力集中在输入图像的哪个部分：显然，它的大部分注意力集中在飞盘上。
- en: '![mls3 1612](assets/mls3_1612.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1612](assets/mls3_1612.png)'
- en: 'Figure 16-12\. Visual attention: an input image (left) and the model’s focus
    before producing the word “frisbee” (right)⁠^([39](ch16.html#idm45720172255968))'
  id: totrans-329
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-12。视觉注意力：输入图像（左）和生成单词“frisbee”之前模型的焦点（右）⁠
- en: When transformers came out in 2017 and people started to experiment with them
    beyond NLP, they were first used alongside CNNs, without replacing them. Instead,
    transformers were generally used to replace RNNs, for example, in image captioning
    models. Transformers became slightly more visual in a [2020 paper](https://homl.info/detr)^([41](ch16.html#idm45720172246864))
    by Facebook researchers, which proposed a hybrid CNN–transformer architecture
    for object detection. Once again, the CNN first processes the input images and
    outputs a set of feature maps, then these feature maps are converted to sequences
    and fed to a transformer, which outputs bounding box predictions. But again, most
    of the visual work is still done by the CNN.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 当transformers在2017年问世并且人们开始在NLP之外进行实验时，它们最初是与CNN一起使用的，而不是取代它们。相反，transformers通常用来取代RNN，例如在图像字幕模型中。在[2020年的一篇论文](https://homl.info/detr)中，Facebook的研究人员提出了一个混合CNN-transformer架构用于目标检测。再次，CNN首先处理输入图像并输出一组特征图，然后这些特征图被转换为序列并馈送到transformer中，transformer输出边界框预测。但是，大部分视觉工作仍然由CNN完成。
- en: 'Then, in October 2020, a team of Google researchers released [a paper](https://homl.info/vit)⁠^([42](ch16.html#idm45720172244672))
    that introduced a fully transformer-based vision model, called a *vision transformer*
    (ViT). The idea is surprisingly simple: just chop the image into little 16 × 16
    squares, and treat the sequence of squares as if it were a sequence of word representations.
    To be more precise, the squares are first flattened into 16 × 16 × 3 = 768-dimensional
    vectors—the 3 is for the RGB color channels—then these vectors go through a linear
    layer that transforms them but retains their dimensionality. The resulting sequence
    of vectors can then be treated just like a sequence of word embeddings: this means
    adding positional embeddings, and passing the result to the transformer. That’s
    it! This model beat the state of the art on ImageNet image classification, but
    to be fair the authors had to use over 300 million additional images for training.
    This makes sense since transformers don’t have as many *inductive biases* as convolution
    neural nets, so they need extra data just to learn things that CNNs implicitly
    assume.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2020年10月，一组谷歌研究人员发布了[一篇论文](https://homl.info/vit)，介绍了一种完全基于transformer的视觉模型，称为*vision
    transformer*（ViT）。这个想法非常简单：只需将图像切成小的16×16的方块，并将方块序列视为单词表示的序列。更准确地说，方块首先被展平为16×16×3=768维向量——3代表RGB颜色通道——然后这些向量经过一个线性层进行转换但保留其维度。然后产生的向量序列可以像单词嵌入序列一样处理：这意味着添加位置嵌入，并将结果传递给transformer。就是这样！这个模型在ImageNet图像分类上击败了现有技术，但公平地说，作者们必须使用超过3亿张额外的图像进行训练。这是有道理的，因为transformer没有像卷积神经网络那样多的*归纳偏差*，所以它们需要额外的数据来学习CNN隐含假设中的东西。
- en: Note
  id: totrans-332
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: An inductive bias is an implicit assumption made by the model, due to its architecture.
    For example, linear models implicitly assume that the data is, well, linear. CNNs
    implicitly assume that patterns learned in one location will likely be useful
    in other locations as well. RNNs implicitly assume that the inputs are ordered,
    and that recent tokens are more important than older ones. The more inductive
    biases a model has, assuming they are correct, the less training data the model
    will require. But if the implicit assumptions are wrong, then the model may perform
    poorly even if it is trained on a large dataset.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳偏差是模型由于其架构而做出的隐含假设。例如，线性模型隐含地假设数据是线性的。CNN隐含地假设在一个位置学习到的模式在其他位置也可能有用。RNN隐含地假设输入是有序的，并且最近的标记比较重要。模型具有的归纳偏差越多，假设它们是正确的，模型所需的训练数据就越少。但是，如果隐含的假设是错误的，那么即使在大型数据集上训练，模型也可能表现不佳。
- en: Just two months later, a team of Facebook researchers released [a paper](https://homl.info/deit)⁠^([43](ch16.html#idm45720172239280))
    that introduced *data-efficient image transformers* (DeiTs). Their model achieved
    competitive results on ImageNet without requiring any additional data for training.
    The model’s architecture is virtually the same as the original ViT, but the authors
    used a distillation technique to transfer knowledge from state-of-the-art CNN
    models to their model.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅两个月后，Facebook的一个研究团队发布了一篇论文，介绍了*数据高效图像变换器*（DeiTs）。他们的模型在ImageNet上取得了竞争性的结果，而无需额外的训练数据。该模型的架构与原始ViT几乎相同，但作者使用了一种蒸馏技术，将来自最先进的CNN模型的知识转移到他们的模型中。
- en: 'Then, in March 2021, DeepMind released an important [paper](https://homl.info/perceiver)⁠^([44](ch16.html#idm45720172235056))
    that introduced the *Perceiver* architecture. It is a *multimodal* transformer,
    meaning you can feed it text, images, audio, or virtually any other modality.
    Until then, transformers had been restricted to fairly short sequences because
    of the performance and RAM bottleneck in the attention layers. This excluded modalities
    such as audio or video, and it forced researchers to treat images as sequences
    of patches, rather than sequences of pixels. The bottleneck is due to self-attention,
    where every token must attend to every other token: if the input sequence has
    *M* tokens, then the attention layer must compute an *M* × *M* matrix, which can
    be huge if *M* is very large. The Perceiver solves this problem by gradually improving
    a fairly short *latent representation* of the inputs, composed of *N* tokens—typically
    just a few hundred. (The word *latent* means hidden, or internal.) The model uses
    cross-attention layers only, feeding them the latent representation as the queries,
    and the (possibly large) inputs as the values. This only requires computing an
    *M* × *N* matrix, so the computational complexity is linear with regard to *M*,
    instead of quadratic. After going through several cross-attention layers, if everything
    goes well, the latent representation ends up capturing everything that matters
    in the inputs. The authors also suggested sharing the weights between consecutive
    cross-attention layers: if you do that, then the Perceiver effectively becomes
    an RNN. Indeed, the shared cross-attention layers can be seen as the same memory
    cell at different time steps, and the latent representation corresponds to the
    cell’s context vector. The same inputs are repeatedly fed to the memory cell at
    every time step. It looks like RNNs are not dead after all!'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，2021年3月，DeepMind发布了一篇重要的论文，介绍了*Perceiver*架构。这是一种*多模态*变压器，意味着您可以向其提供文本、图像、音频或几乎任何其他模态。直到那时，变压器由于注意力层中的性能和RAM瓶颈而被限制在相当短的序列中。这排除了音频或视频等模态，并迫使研究人员将图像视为补丁序列，而不是像素序列。瓶颈是由于自我注意力，其中每个标记必须关注每个其他标记：如果输入序列有*M*个标记，那么注意力层必须计算一个*M*×*M*矩阵，如果*M*非常大，这可能会很大。Perceiver通过逐渐改进由*N*个标记组成的输入的相当短的*潜在表示*来解决这个问题——通常只有几百个。
    （*潜在*一词表示隐藏或内部。）该模型仅使用交叉注意力层，将潜在表示作为查询输入，并将（可能很大的）输入作为值输入。这只需要计算一个*M*×*N*矩阵，因此计算复杂度与*M*线性相关，而不是二次的。经过几个交叉注意力层后，如果一切顺利，潜在表示最终会捕捉到输入中的所有重要内容。作者还建议在连续的交叉注意力层之间共享权重：如果这样做，那么Perceiver实际上就变成了一个RNN。实际上，共享的交叉注意力层可以被看作是不同时间步的相同记忆单元，而潜在表示对应于单元的上下文向量。相同的输入会在每个时间步骤中重复馈送到记忆单元。看来RNN并没有完全消亡！
- en: 'Just a month later, Mathilde Caron et al. introduced [DINO](https://homl.info/dino),^([45](ch16.html#idm45720172223328))
    an impressive vision transformer trained entirely without labels, using self-supervision,
    and capable of high-accuracy semantic segmentation. The model is duplicated during
    training, with one network acting as a teacher and the other acting as a student.
    Gradient descent only affects the student, while the teacher’s weights are just
    an exponential moving average of the student’s weights. The student is trained
    to match the teacher’s predictions: since they’re almost the same model, this
    is called *self-distillation*. At each training step, the input images are augmented
    in different ways for the teacher and the student, so they don’t see the exact
    same image, but their predictions must match. This forces them to come up with
    high-level representations. To prevent *mode collapse*, where both the student
    and the teacher would always output the same thing, completely ignoring the inputs,
    DINO keeps track of a moving average of the teacher’s outputs, and it tweaks the
    teacher’s predictions to ensure that they remain centered on zero, on average.
    DINO also forces the teacher to have high confidence in its predictions: this
    is called *sharpening*. Together, these techniques preserve diversity in the teacher’s
    outputs.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅一个月后，Mathilde Caron等人介绍了[DINO](https://homl.info/dino)，一个令人印象深刻的视觉变换器，完全不使用标签进行训练，使用自我监督，并能够进行高精度的语义分割。该模型在训练期间被复制，其中一个网络充当教师，另一个充当学生。梯度下降仅影响学生，而教师的权重只是学生权重的指数移动平均值。学生被训练以匹配教师的预测：由于它们几乎是相同的模型，这被称为*自蒸馏*。在每个训练步骤中，输入图像以不同方式增强教师和学生，因此它们不会看到完全相同的图像，但它们的预测必须匹配。这迫使它们提出高级表示。为了防止*模式坍塌*，即学生和教师总是输出相同的内容，完全忽略输入，DINO跟踪教师输出的移动平均值，并调整教师的预测，以确保它们平均保持在零点上。DINO还迫使教师对其预测具有高置信度：这被称为*锐化*。这些技术共同保留了教师输出的多样性。
- en: 'In a [2021 paper](https://homl.info/scalingvits),⁠^([46](ch16.html#idm45720172217680))
    Google researchers showed how to scale ViTs up or down, depending on the amount
    of data. They managed to create a huge 2 billion parameter model that reached
    over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a scaled-down
    model that reached over 84.8% top-1 accuracy on ImageNet, using only 10,000 images:
    that’s just 10 images per class!'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇2021年的[论文](https://homl.info/scalingvits)中，Google研究人员展示了如何根据数据量来扩展或缩小ViTs。他们成功创建了一个庞大的20亿参数模型，在ImageNet上达到了超过90.4%的top-1准确率。相反，他们还训练了一个缩小模型，在ImageNet上达到了超过84.8%的top-1准确率，只使用了1万张图像：每类只有10张图像！
- en: And progress in visual transformers has continued steadily to this day. For
    example, in March 2022, a [paper](https://homl.info/modelsoups)⁠^([47](ch16.html#idm45720172215648))
    by Mitchell Wortsman et al. demonstrated that it’s possible to first train multiple
    transformers, then average their weights to create a new and improved model. This
    is similar to an ensemble (see [Chapter 7](ch07.html#ensembles_chapter)), except
    there’s just one model in the end, which means there’s no inference time penalty.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉transformers的进展一直在稳步进行。例如，2022年3月，Mitchell Wortsman等人的一篇[论文](https://homl.info/modelsoups)展示了首先训练多个transformers，然后平均它们的权重以创建一个新的改进模型是可能的。这类似于集成（见[第7章](ch07.html#ensembles_chapter)），只是最终只有一个模型，这意味着没有推理时间惩罚。
- en: 'The latest trend in transformers consists in building large multimodal models,
    often capable of zero-shot or few-shot learning. For example, [OpenAI’s 2021 CLIP
    paper](https://homl.info/clip)⁠^([48](ch16.html#idm45720172211920)) proposed a
    large transformer model pretrained to match captions with images: this task allows
    it to learn excellent image representations, and the model can then be used directly
    for tasks such as image classification using simple text prompts such as “a photo
    of a cat”. Soon after, OpenAI announced [DALL·E](https://homl.info/dalle),⁠^([49](ch16.html#idm45720172210416))
    capable of generating amazing images based on text prompts. The [DALL·E 2](https://homl.info/dalle2),⁠^([50](ch16.html#idm45720172208784))
    which generates even higher quality images using a diffusion model (see [Chapter 17](ch17.html#autoencoders_chapter)).'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: transformers领域的最新趋势在于构建大型多模态模型，通常能够进行零样本或少样本学习。例如，[OpenAI的2021年CLIP论文](https://homl.info/clip)提出了一个大型transformer模型，预训练以匹配图像的标题：这个任务使其能够学习出色的图像表示，然后该模型可以直接用于诸如使用简单文本提示进行图像分类的任务，比如“一张猫的照片”。不久之后，OpenAI宣布了[DALL·E](https://homl.info/dalle)，能够根据文本提示生成惊人的图像。[DALL·E
    2](https://homl.info/dalle2)生成更高质量的图像，使用扩散模型（见[第17章](ch17.html#autoencoders_chapter)）。
- en: In April 2022, DeepMind released the [Flamingo paper](https://homl.info/flamingo),⁠^([51](ch16.html#idm45720172205280))
    which introduced a family of models pretrained on a wide variety of tasks across
    multiple modalities, including text, images, and videos. A single model can be
    used across very different tasks, such as question answering, image captioning,
    and more. Soon after, in May 2022, DeepMind introduced [GATO](https://homl.info/gato),⁠^([52](ch16.html#idm45720172203760))
    a multimodal model that can be used as a policy for a reinforcement learning agent
    (RL will be introduced in [Chapter 18](ch18.html#rl_chapter)). The same transformer
    can chat with you, caption images, play Atari games, control (simulated) robotic
    arms, and more, all with “only” 1.2 billion parameters. And the adventure continues!
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年4月，DeepMind发布了[Flamingo paper](https://homl.info/flamingo)，介绍了一系列在多种任务和多种模态下预训练的模型，包括文本、图像和视频。一个模型可以用于非常不同的任务，比如问答、图像描述等。不久之后，2022年5月，DeepMind推出了[GATO](https://homl.info/gato)，一个多模态模型，可以作为强化学习代理的策略（强化学习将在[第18章](ch18.html#rl_chapter)介绍）。同一个transformer可以与您聊天，为图像加注释，玩Atari游戏，控制（模拟的）机械臂等，所有这些只需“仅有”12亿个参数。冒险还在继续！
- en: Note
  id: totrans-341
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: These astounding advances have led some researchers to claim that human-level
    AI is near, that “scale is all you need”, and that some of these models may be
    “slightly conscious”. Others point out that despite the amazing progress, these
    models still lack the reliability and adaptability of human intelligence, our
    ability to reason symbolically, to generalize based on a single example, and more.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这些惊人的进步使一些研究人员认为人类水平的AI已经近在眼前，认为“规模就是一切”，并且一些模型可能“稍微有意识”。其他人指出，尽管取得了惊人的进步，这些模型仍然缺乏人类智能的可靠性和适应性，我们推理的符号能力，基于单个例子进行泛化的能力等等。
- en: As you can see, transformers are everywhere! And the good news is that you generally
    won’t have to implement transformers yourself since many excellent pretrained
    models are readily available for download via TensorFlow Hub or Hugging Face’s
    model hub. You’ve already seen how to use a model from TF Hub, so let’s close
    this chapter by taking a quick look at Hugging Face’s ecosystem.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，transformers无处不在！好消息是，通常您不必自己实现transformers，因为许多优秀的预训练模型可以通过TensorFlow
    Hub或Hugging Face的模型中心轻松下载。您已经看到如何使用TF Hub中的模型，所以让我们通过快速查看Hugging Face的生态系统来结束本章。
- en: Hugging Face’s Transformers Library
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face的Transformers库
- en: It’s impossible to talk about transformers today without mentioning Hugging
    Face, an AI company that has built a whole ecosystem of easy-to-use open source
    tools for NLP, vision, and beyond. The central component of their ecosystem is
    the Transformers library, which allows you to easily download a pretrained model,
    including its corresponding tokenizer, and then fine-tune it on your own dataset,
    if needed. Plus, the library supports TensorFlow, PyTorch, and JAX (with the Flax
    library).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 今天谈论transformers时不可能不提到Hugging Face，这是一家为NLP、视觉等构建了一整套易于使用的开源工具的人工智能公司。他们生态系统的核心组件是Transformers库，它允许您轻松下载一个预训练模型，包括相应的分词器，然后根据需要在自己的数据集上进行微调。此外，该库支持TensorFlow、PyTorch和JAX（使用Flax库）。
- en: 'The simplest way to use the Transformers library is to use the `transformers.​pipe⁠line()`
    function: you just specify which task you want, such as sentiment analysis, and
    it downloads a default pretrained model, ready to be used—it really couldn’t be
    any simpler:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Transformers库的最简单方法是使用`transformers.pipeline()`函数：只需指定您想要的任务，比如情感分析，它会下载一个默认的预训练模型，准备好使用——真的再简单不过了：
- en: '[PRE52]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The result is a Python list containing one dictionary per input text:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个Python列表，每个输入文本对应一个字典：
- en: '[PRE53]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In this example, the model correctly found that the sentence is positive, with
    around 99.98% confidence. Of course, you can also pass a batch of sentences to
    the model:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，模型正确地发现句子是积极的，置信度约为99.98%。当然，您也可以将一批句子传递给模型：
- en: '[PRE54]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The `pipeline()` function uses the default model for the given task. For example,
    for text classification tasks such as sentiment analysis, at the time of writing,
    it defaults to `distilbert-base-uncased-finetuned-sst-2-english`—a DistilBERT
    model with an uncased tokenizer, trained on English Wikipedia and a corpus of
    English books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task.
    It’s also possible to manually specify a different model. For example, you could
    use a DistilBERT model fine-tuned on the Multi-Genre Natural Language Inference
    (MultiNLI) task, which classifies two sentences into three classes: contradiction,
    neutral, or entailment. Here is how:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipeline()`函数使用给定任务的默认模型。例如，对于文本分类任务，如情感分析，在撰写本文时，默认为`distilbert-base-uncased-finetuned-sst-2-english`——一个在英文维基百科和英文书籍语料库上训练的带有小写标记器的DistilBERT模型，并在斯坦福情感树库v2（SST
    2）任务上进行了微调。您也可以手动指定不同的模型。例如，您可以使用在多种自然语言推理（MultiNLI）任务上进行微调的DistilBERT模型，该任务将两个句子分类为三类：矛盾、中性或蕴含。以下是如何操作：'
- en: '[PRE55]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Tip
  id: totrans-354
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can find the available models at [*https://huggingface.co/models*](https://huggingface.co/models),
    and the list of tasks at [*https://huggingface.co/tasks*](https://huggingface.co/tasks).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[*https://huggingface.co/models*](https://huggingface.co/models)找到可用的模型，以及在[*https://huggingface.co/tasks*](https://huggingface.co/tasks)找到任务列表。
- en: 'The pipeline API is very simple and convenient, but sometimes you will need
    more control. For such cases, the Transformers library provides many classes,
    including all sorts of tokenizers, models, configurations, callbacks, and much
    more. For example, let’s load the same DistilBERT model, along with its corresponding
    tokenizer, using the `TFAutoModelForSequenceClassification` and `AutoTokenizer`
    classes:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: pipeline API非常简单方便，但有时您需要更多控制。对于这种情况，Transformers库提供了许多类，包括各种标记器、模型、配置、回调等。例如，让我们使用`TFAutoModelForSequenceClassification`和`AutoTokenizer`类加载相同的DistilBERT模型及其对应的标记器：
- en: '[PRE56]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, let’s tokenize a couple of pairs of sentences. In this code, we activate
    padding and specify that we want TensorFlow tensors instead of Python lists:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们标记一对句子。在此代码中，我们激活填充，并指定我们希望使用TensorFlow张量而不是Python列表：
- en: '[PRE57]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Tip
  id: totrans-360
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Instead of passing `"Sentence 1 [SEP] Sentence 2"` to the tokenizer, you can
    equivalently pass it a tuple: `("Sentence 1",` `"Sentence 2")`.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在将`"Sentence 1 [SEP] Sentence 2"`传递给标记器时，您可以等效地传递一个元组：`("Sentence 1",` `"Sentence
    2")`。
- en: 'The output is a dictionary-like instance of the `BatchEncoding` class, which
    contains the sequences of token IDs, as well as a mask containing 0s for the padding
    tokens:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是`BatchEncoding`类的类似字典实例，其中包含标记ID序列，以及包含填充标记的掩码为0：
- en: '[PRE58]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: If you set `return_token_type_ids=True` when calling the tokenizer, you will
    also get an extra tensor that indicates which sentence each token belongs to.
    This is needed by some models, but not DistilBERT.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用标记器时设置`return_token_type_ids=True`，您还将获得一个额外的张量，指示每个标记属于哪个句子。这对某些模型是必需的，但对DistilBERT不是。
- en: 'Next, we can directly pass this `BatchEncoding` object to the model; it returns
    a `TFSequenceClassifierOutput` object containing its predicted class logits:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以直接将这个`BatchEncoding`对象传递给模型；它返回一个包含其预测类logits的`TFSequenceClassifierOutput`对象：
- en: '[PRE59]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Lastly, we can apply the softmax activation function to convert these logits
    to class probabilities, and use the `argmax()` function to predict the class with
    the highest probability for each input sentence pair:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以应用softmax激活函数将这些logits转换为类概率，并使用`argmax()`函数预测每个输入句子对的具有最高概率的类：
- en: '[PRE60]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In this example, the model correctly classifies the first sentence pair as neutral
    (the fact that I like soccer does not imply that everyone else does) and the second
    pair as an entailment (Joe must indeed be quite old).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，模型正确将第一对句子分类为中性（我喜欢足球并不意味着每个人都喜欢），将第二对句子分类为蕴含（乔确实应该很老）。
- en: 'If you wish to fine-tune this model on your own dataset, you can train the
    model as usual with Keras since it’s just a regular Keras model with a few extra
    methods. However, because the model outputs logits instead of probabilities, you
    must use the `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`
    loss instead of the usual `"sparse_categorical_crossentropy"` loss. Moreover,
    the model does not support `BatchEncoding` inputs during training, so you must
    use its `data` attribute to get a regular dictionary instead:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望在自己的数据集上微调此模型，您可以像通常使用Keras一样训练模型，因为它只是一个常规的Keras模型，具有一些额外的方法。但是，由于模型输出的是logits而不是概率，您必须使用`tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`损失，而不是通常的`"sparse_categorical_crossentropy"`损失。此外，模型在训练期间不支持`BatchEncoding`输入，因此您必须使用其`data`属性来获取一个常规字典：
- en: '[PRE61]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Hugging Face has also built a Datasets library that you can use to easily download
    a standard dataset (such as IMDb) or a custom one, and use it to fine-tune your
    model. It’s similar to TensorFlow Datasets, but it also provides tools to perform
    common preprocessing tasks on the fly, such as masking. The list of datasets is
    available at [*https://huggingface.co/datasets*](https://huggingface.co/datasets).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face还构建了一个Datasets库，您可以使用它轻松下载标准数据集（如IMDb）或自定义数据集，并用它来微调您的模型。它类似于TensorFlow
    Datasets，但还提供了在运行时执行常见预处理任务的工具，如掩码。数据集列表可在[*https://huggingface.co/datasets*](https://huggingface.co/datasets)上找到。
- en: 'This should get you started with Hugging Face’s ecosystem. To learn more, you
    can head over to [*https://huggingface.co/docs*](https://huggingface.co/docs)
    for the documentation, which includes many tutorial notebooks, videos, the full
    API, and more. I also recommend you check out the O’Reilly book [*Natural Language
    Processing with Transformers: Building Language Applications with Hugging Face*](https://homl.info/hfbook)
    by Lewis Tunstall, Leandro von Werra, and Thomas Wolf—all from the Hugging Face
    team.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该让您开始使用Hugging Face的生态系统。要了解更多信息，您可以前往[*https://huggingface.co/docs*](https://huggingface.co/docs)查看文档，其中包括许多教程笔记本、视频、完整API等。我还建议您查看O'Reilly图书[*使用Hugging
    Face构建自然语言处理应用的变压器*](https://homl.info/hfbook)，作者是来自Hugging Face团队的Lewis Tunstall、Leandro
    von Werra和Thomas Wolf。
- en: In the next chapter we will discuss how to learn deep representations in an
    unsupervised way using autoencoders, and we will use generative adversarial networks
    to produce images and more!
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何使用自动编码器以无监督的方式学习深度表示，并使用生成对抗网络来生成图像等！
- en: Exercises
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What are the pros and cons of using a stateful RNN versus a stateless RNN?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用有状态RNN与无状态RNN的优缺点是什么？
- en: Why do people use encoder–decoder RNNs rather than plain sequence-to-sequence
    RNNs for automatic translation?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么人们使用编码器-解码器RNN而不是普通的序列到序列RNN进行自动翻译？
- en: How can you deal with variable-length input sequences? What about variable-length
    output sequences?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何处理可变长度的输入序列？可变长度的输出序列呢？
- en: What is beam search, and why would you use it? What tool can you use to implement
    it?
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是波束搜索，为什么要使用它？可以使用什么工具来实现它？
- en: What is an attention mechanism? How does it help?
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是注意力机制？它如何帮助？
- en: What is the most important layer in the transformer architecture? What is its
    purpose?
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变压器架构中最重要的层是什么？它的目的是什么？
- en: When would you need to use sampled softmax?
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么时候需要使用采样softmax？
- en: '*Embedded Reber grammars* were used by Hochreiter and Schmidhuber in [their
    paper](https://homl.info/93) about LSTMs. They are artificial grammars that produce
    strings such as “BPBTSXXVPSEPE”. Check out Jenny Orr’s [nice introduction](https://homl.info/108)
    to this topic, then choose a particular embedded Reber grammar (such as the one
    represented on Orr’s page), then train an RNN to identify whether a string respects
    that grammar or not. You will first need to write a function capable of generating
    a training batch containing about 50% strings that respect the grammar, and 50%
    that don’t.'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*嵌入Reber语法*被Hochreiter和Schmidhuber在关于LSTMs的[论文](https://homl.info/93)中使用。它们是产生诸如“BPBTSXXVPSEPE”之类字符串的人工语法。查看Jenny
    Orr的[关于这个主题的很好介绍](https://homl.info/108)，然后选择一个特定的嵌入Reber语法（例如Orr页面上表示的那个），然后训练一个RNN来识别一个字符串是否符合该语法。您首先需要编写一个能够生成包含约50%符合语法的字符串和50%不符合语法的字符串的训练批次的函数。'
- en: Train an encoder–decoder model that can convert a date string from one format
    to another (e.g., from “April 22, 2019” to “2019-04-22”).
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个能够将日期字符串从一种格式转换为另一种格式的编码器-解码器模型（例如，从“2019年4月22日”到“2019-04-22”）。
- en: Go through the example on the Keras website for [“Natural language image search
    with a Dual Encoder”](https://homl.info/dualtuto). You will learn how to build
    a model capable of representing both images and text within the same embedding
    space. This makes it possible to search for images using a text prompt, like in
    the CLIP model by OpenAI.
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浏览Keras网站上关于[“使用双编码器进行自然语言图像搜索”](https://homl.info/dualtuto)的示例。您将学习如何构建一个能够在同一嵌入空间内表示图像和文本的模型。这使得可以使用文本提示搜索图像，就像OpenAI的CLIP模型一样。
- en: Use the Hugging Face Transformers library to download a pretrained language
    model capable of generating text (e.g., GPT), and try generating more convincing
    Shakespearean text. You will need to use the model’s `generate()` method—see Hugging
    Face’s documentation for more details.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Hugging Face Transformers库下载一个预训练的语言模型，能够生成文本（例如，GPT），并尝试生成更具说服力的莎士比亚文本。您需要使用模型的`generate()`方法-请参阅Hugging
    Face的文档以获取更多详细信息。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可在本章笔记本的末尾找到，网址为[*https://homl.info/colab3*](https://homl.info/colab3)。
- en: '^([1](ch16.html#idm45720177608176-marker)) Alan Turing, “Computing Machinery
    and Intelligence”, *Mind* 49 (1950): 433–460.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 艾伦·图灵，“计算机机器和智能”，*心灵* 49（1950年）：433-460。
- en: '^([2](ch16.html#idm45720177605632-marker)) Of course, the word *chatbot* came
    much later. Turing called his test the *imitation game*: machine A and human B
    chat with human interrogator C via text messages; the interrogator asks questions
    to figure out which one is the machine (A or B). The machine passes the test if
    it can fool the interrogator, while the human B must try to help the interrogator.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，单词*chatbot*出现得更晚。图灵称其测试为*模仿游戏*：机器A和人类B通过文本消息与人类审问者C聊天；审问者提出问题以确定哪一个是机器（A还是B）。如果机器能够愚弄审问者，那么它通过了测试，而人类B必须尽力帮助审问者。
- en: '^([3](ch16.html#idm45720176925824-marker)) Since the input windows overlap,
    the concept of *epoch* is not so clear in this case: during each epoch (as implemented
    by Keras), the model will actually see the same character multiple times.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输入窗口重叠，因此在这种情况下*时代*的概念并不那么清晰：在每个时代（由Keras实现），模型实际上会多次看到相同的字符。
- en: ^([4](ch16.html#idm45720176103360-marker)) Alec Radford et al., “Learning to
    Generate Reviews and Discovering Sentiment”, arXiv preprint arXiv:1704.01444 (2017).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: Alec Radford等人，“学习生成评论和发现情感”，arXiv预印本arXiv:1704.01444（2017年）。
- en: '^([5](ch16.html#idm45720175873872-marker)) Rico Sennrich et al., “Neural Machine
    Translation of Rare Words with Subword Units”, *Proceedings of the 54th Annual
    Meeting of the Association for Computational Linguistics* 1 (2016): 1715–1725.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: Rico Sennrich等人，“使用子词单元进行稀有词的神经机器翻译”，*计算语言学年会第54届年会论文集* 1（2016年）：1715-1725。
- en: '^([6](ch16.html#idm45720175847312-marker)) Taku Kudo, “Subword Regularization:
    Improving Neural Network Translation Models with Multiple Subword Candidates”,
    arXiv preprint arXiv:1804.10959 (2018).'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch16.html#idm45720175847312-marker)) Taku Kudo，“子词规范化：改进神经网络翻译模型的多个子词候选”，arXiv预印本arXiv:1804.10959（2018）。
- en: '^([7](ch16.html#idm45720175842096-marker)) Taku Kudo and John Richardson, “SentencePiece:
    A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural
    Text Processing”, arXiv preprint arXiv:1808.06226 (2018).'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch16.html#idm45720175842096-marker)) Taku Kudo和John Richardson，“SentencePiece：用于神经文本处理的简单且语言无关的子词标记器和去标记器”，arXiv预印本arXiv:1808.06226（2018）。
- en: '^([8](ch16.html#idm45720175839056-marker)) Yonghui Wu et al., “Google’s Neural
    Machine Translation System: Bridging the Gap Between Human and Machine Translation”,
    arXiv preprint arXiv:1609.08144 (2016).'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch16.html#idm45720175839056-marker)) Yonghui Wu等人，“谷歌的神经机器翻译系统：弥合人类和机器翻译之间的差距”，arXiv预印本arXiv:1609.08144（2016）。
- en: ^([9](ch16.html#idm45720175499952-marker)) Ragged tensors were introduced in
    [Chapter 12](ch12.html#tensorflow_chapter), and they are detailed in [Appendix C](app03.html#structures_appendix).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch16.html#idm45720175499952-marker)) 不规则张量在[第12章](ch12.html#tensorflow_chapter)中被介绍，详细内容在[附录C](app03.html#structures_appendix)中。
- en: '^([10](ch16.html#idm45720175311248-marker)) Matthew Peters et al., “Deep Contextualized
    Word Representations”, *Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies*
    1 (2018): 2227–2237.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch16.html#idm45720175311248-marker)) Matthew Peters等人，“深度上下文化的词表示”，*2018年北美计算语言学分会年会论文集：人类语言技术*
    1（2018）：2227–2237。
- en: '^([11](ch16.html#idm45720175307104-marker)) Jeremy Howard and Sebastian Ruder,
    “Universal Language Model Fine-Tuning for Text Classification”, *Proceedings of
    the 56th Annual Meeting of the Association for Computational Linguistics* 1 (2018):
    328–339.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch16.html#idm45720175307104-marker)) Jeremy Howard和Sebastian Ruder，“文本分类的通用语言模型微调”，*计算语言学年会第56届年会论文集*
    1（2018）：328–339。
- en: ^([12](ch16.html#idm45720175301568-marker)) Daniel Cer et al., “Universal Sentence
    Encoder”, arXiv preprint arXiv:1803.11175 (2018).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch16.html#idm45720175301568-marker)) Daniel Cer等人，“通用句子编码器”，arXiv预印本arXiv:1803.11175（2018）。
- en: ^([13](ch16.html#idm45720175133488-marker)) Ilya Sutskever et al., “Sequence
    to Sequence Learning with Neural Networks”, arXiv preprint (2014).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch16.html#idm45720175133488-marker)) Ilya Sutskever等人，“使用神经网络进行序列到序列学习”，arXiv预印本（2014）。
- en: ^([14](ch16.html#idm45720175116736-marker)) Samy Bengio et al., “Scheduled Sampling
    for Sequence Prediction with Recurrent Neural Networks”, arXiv preprint arXiv:1506.03099
    (2015).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch16.html#idm45720175116736-marker)) Samy Bengio等人，“使用循环神经网络进行序列预测的计划抽样”，arXiv预印本arXiv:1506.03099（2015）。
- en: ^([15](ch16.html#idm45720175112096-marker)) This dataset is composed of sentence
    pairs created by contributors of the [Tatoeba project](https://tatoeba.org). About
    120,000 sentence pairs were selected by the authors of the website [*https://manythings.org/anki*](https://manythings.org/anki).
    This dataset is released under the Creative Commons Attribution 2.0 France license.
    Other language pairs are available.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch16.html#idm45720175112096-marker)) 这个数据集由[Tatoeba项目](https://tatoeba.org)的贡献者创建的句子对组成。网站作者选择了约120,000个句子对[*https://manythings.org/anki*](https://manythings.org/anki)。该数据集在创作共用署名2.0法国许可下发布。其他语言对也可用。
- en: ^([16](ch16.html#idm45720174413024-marker)) In Python, if you run `a, *b = [1,
    2, 3, 4]`, then `a` equals `1` and `b` equals `[2, 3, 4]`.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch16.html#idm45720174413024-marker)) 在Python中，如果运行`a, *b = [1, 2, 3,
    4]`，那么`a`等于`1`，`b`等于`[2, 3, 4]`。
- en: '^([17](ch16.html#idm45720174300992-marker)) Sébastien Jean et al., “On Using
    Very Large Target Vocabulary for Neural Machine Translation”, *Proceedings of
    the 53rd Annual Meeting of the Association for Computational Linguistics and the
    7th International Joint Conference on Natural Language Processing of the Asian
    Federation of Natural Language Processing* 1 (2015): 1–10.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch16.html#idm45720174300992-marker)) Sébastien Jean等人，“在神经机器翻译中使用非常大的目标词汇”，*计算语言学年会第53届年会和亚洲自然语言处理联合国际会议第7届年会论文集*
    1（2015）：1–10。
- en: ^([18](ch16.html#idm45720173841328-marker)) Dzmitry Bahdanau et al., “Neural
    Machine Translation by Jointly Learning to Align and Translate”, arXiv preprint
    arXiv:1409.0473 (2014).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch16.html#idm45720173841328-marker)) Dzmitry Bahdanau等人，“通过联合学习对齐和翻译的神经机器翻译”，arXiv预印本arXiv:1409.0473（2014）。
- en: '^([19](ch16.html#idm45720173812256-marker)) Minh-Thang Luong et al., “Effective
    Approaches to Attention-Based Neural Machine Translation”, *Proceedings of the
    2015 Conference on Empirical Methods in Natural Language Processing* (2015): 1412–1421.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch16.html#idm45720173812256-marker)) Minh-Thang Luong等人，“基于注意力的神经机器翻译的有效方法”，*2015年自然语言处理经验方法会议论文集*（2015）：1412–1421。
- en: '^([20](ch16.html#idm45720173607840-marker)) Ashish Vaswani et al., “Attention
    Is All You Need”, *Proceedings of the 31st International Conference on Neural
    Information Processing Systems* (2017): 6000–6010.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch16.html#idm45720173607840-marker)) Ashish Vaswani等人，“注意力就是一切”，*第31届国际神经信息处理系统会议论文集*（2017）：6000–6010。
- en: ^([21](ch16.html#idm45720173604064-marker)) Since the transformer uses time-distributed
    dense layers, you could argue that it uses 1D convolutional layers with a kernel
    size of 1.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch16.html#idm45720173604064-marker)) 由于变压器使用时间分布密集层，可以说它使用了核大小为1的1D卷积层。
- en: ^([22](ch16.html#idm45720173579408-marker)) This is figure 1 from the “Attention
    Is All You Need” paper, reproduced with the kind permission of the authors.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch16.html#idm45720173579408-marker)) 这是“注意力就是一切”论文中的图1，经作者的亲切许可再现。
- en: ^([23](ch16.html#idm45720173475360-marker)) It’s possible to use ragged tensors
    instead, if you are using the latest version of TensorFlow.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch16.html#idm45720173475360-marker)) 如果您使用最新版本的TensorFlow，可以使用不规则张量。
- en: ^([24](ch16.html#idm45720173058848-marker)) This is the righthand part of figure
    2 from “Attention Is All You Need”, reproduced with the kind authorization of
    the authors.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch16.html#idm45720173058848-marker)) 这是“注意力机制是你所需要的一切”中图2的右侧部分，经作者亲切授权复制。
- en: '^([25](ch16.html#idm45720173048224-marker)) This will most likely change by
    the time you read this; check out [Keras issue #16248](https://github.com/keras-team/keras/issues/16248)
    for more details. When this happens, there will be no need to set the `attention_mask`
    argument, and therefore no need to create `encoder_pad_mask`.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch16.html#idm45720173048224-marker)) 当您阅读本文时，这很可能会发生变化；请查看[Keras问题＃16248](https://github.com/keras-team/keras/issues/16248)获取更多详细信息。当这种情况发生时，将不需要设置`attention_mask`参数，因此也不需要创建`encoder_pad_mask`。
- en: ^([26](ch16.html#idm45720172796192-marker)) Currently `Z + skip` does not support
    automatic masking, which is why we had to write `tf.keras.​lay⁠ers.Add()([Z, skip])`
    instead. Again, this may change by the time you read this.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch16.html#idm45720172796192-marker)) 目前`Z + skip`不支持自动屏蔽，这就是为什么我们不得不写`tf.keras.​lay⁠ers.Add()([Z,
    skip])`的原因。再次强调，当您阅读本文时，情况可能已经发生变化。
- en: ^([27](ch16.html#idm45720172319856-marker)) Alec Radford et al., “Improving
    Language Understanding by Generative Pre-Training” (2018).
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch16.html#idm45720172319856-marker)) Alec Radford等人，“通过生成式预训练改进语言理解”（2018年）。
- en: ^([28](ch16.html#idm45720172314768-marker)) For example, the sentence “Jane
    had a lot of fun at her friend’s birthday party” entails “Jane enjoyed the party”,
    but it is contradicted by “Everyone hated the party” and it is unrelated to “The
    Earth is flat”.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch16.html#idm45720172314768-marker)) 例如，“简在朋友的生日派对上玩得很开心”意味着“简喜欢这个派对”，但与“每个人都讨厌这个派对”相矛盾，与“地球是平的”无关。
- en: '^([29](ch16.html#idm45720172313104-marker)) Jacob Devlin et al., “BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding”, *Proceedings of
    the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies* 1 (2019).'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch16.html#idm45720172313104-marker)) Jacob Devlin等人，“BERT：深度双向Transformer的预训练”，*2018年北美计算语言学协会会议论文集：人类语言技术*
    1（2019年）。
- en: ^([30](ch16.html#idm45720172299632-marker)) This is figure 1 from the paper,
    reproduced with the kind authorization of the authors.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch16.html#idm45720172299632-marker)) 这是论文中的图1，经作者亲切授权复制。
- en: ^([31](ch16.html#idm45720172296288-marker)) Alec Radford et al., “Language Models
    Are Unsupervised Multitask Learners” (2019).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch16.html#idm45720172296288-marker)) Alec Radford等人，“语言模型是无监督多任务学习者”（2019年）。
- en: '^([32](ch16.html#idm45720172293488-marker)) William Fedus et al., “Switch Transformers:
    Scaling to Trillion Parameter Models with Simple and Efficient Sparsity” (2021).'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '^([32](ch16.html#idm45720172293488-marker)) William Fedus等人，“Switch Transformers:
    通过简单高效的稀疏性扩展到万亿参数模型”（2021年）。'
- en: '^([33](ch16.html#idm45720172289264-marker)) Victor Sanh et al., “DistilBERT,
    A Distilled Version of Bert: Smaller, Faster, Cheaper and Lighter”, arXiv preprint
    arXiv:1910.01108 (2019).'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: ^([33](ch16.html#idm45720172289264-marker)) Victor Sanh等人，“DistilBERT，Bert的精简版本：更小、更快、更便宜、更轻”，arXiv预印本arXiv:1910.01108（2019年）。
- en: '^([34](ch16.html#idm45720172285632-marker)) Mariya Yao summarized many of these
    models in this post: [*https://homl.info/yaopost*](https://homl.info/yaopost).'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ^([34](ch16.html#idm45720172285632-marker)) Mariya Yao在这篇文章中总结了许多这些模型：[*https://homl.info/yaopost*](https://homl.info/yaopost)。
- en: ^([35](ch16.html#idm45720172282624-marker)) Colin Raffel et al., “Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, arXiv
    preprint arXiv:1910.10683 (2019).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ^([35](ch16.html#idm45720172282624-marker)) Colin Raffel等人，“探索统一文本到文本Transformer的迁移学习极限”，arXiv预印本arXiv:1910.10683（2019年）。
- en: '^([36](ch16.html#idm45720172277824-marker)) Aakanksha Chowdhery et al., “PaLM:
    Scaling Language Modeling with Pathways”, arXiv preprint arXiv:2204.02311 (2022).'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '^([36](ch16.html#idm45720172277824-marker)) Aakanksha Chowdhery等人，“PaLM: 使用路径扩展语言建模”，arXiv预印本arXiv:2204.02311（2022年）。'
- en: ^([37](ch16.html#idm45720172273920-marker)) Jason Wei et al., “Chain of Thought
    Prompting Elicits Reasoning in Large Language Models”, arXiv preprint arXiv:2201.11903
    (2022).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ^([37](ch16.html#idm45720172273920-marker)) Jason Wei等人，“思维链提示引发大型语言模型的推理”，arXiv预印本arXiv:2201.11903（2022年）。
- en: '^([38](ch16.html#idm45720172266576-marker)) Kelvin Xu et al., “Show, Attend
    and Tell: Neural Image Caption Generation with Visual Attention”, *Proceedings
    of the 32nd International Conference on Machine Learning* (2015): 2048–2057.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: ^([38](ch16.html#idm45720172266576-marker)) Kelvin Xu等人，“展示、关注和叙述：带有视觉注意力的神经图像字幕生成”，*第32届国际机器学习会议论文集*（2015年）：2048–2057。
- en: ^([39](ch16.html#idm45720172255968-marker)) This is a part of figure 3 from
    the paper. It is reproduced with the kind authorization of the authors.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ^([39](ch16.html#idm45720172255968-marker)) 这是论文中图3的一部分。经作者亲切授权复制。
- en: '^([40](ch16.html#idm45720172249728-marker)) Marco Tulio Ribeiro et al., “‘Why
    Should I Trust You?’: Explaining the Predictions of Any Classifier”, *Proceedings
    of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining* (2016): 1135–1144.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ^([40](ch16.html#idm45720172249728-marker)) Marco Tulio Ribeiro等人，“‘为什么我应该相信你？’：解释任何分类器的预测”，*第22届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*（2016年）：1135–1144。
- en: ^([41](ch16.html#idm45720172246864-marker)) Nicolas Carion et al., “End-to-End
    Object Detection with Transformers”, arXiv preprint arxiv:2005.12872 (2020).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: ^([41](ch16.html#idm45720172246864-marker)) Nicolas Carion等人，“使用Transformer进行端到端目标检测”，arXiv预印本arxiv:2005.12872（2020年）。
- en: '^([42](ch16.html#idm45720172244672-marker)) Alexey Dosovitskiy et al., “An
    Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale”, arXiv
    preprint arxiv:2010.11929 (2020).'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ^([42](ch16.html#idm45720172244672-marker)) Alexey Dosovitskiy等人，“一幅图像价值16x16个词：大规模图像识别的Transformer”，arXiv预印本arxiv:2010.11929（2020年）。
- en: ^([43](ch16.html#idm45720172239280-marker)) Hugo Touvron et al., “Training Data-Efficient
    Image Transformers & Distillation Through Attention”, arXiv preprint arxiv:2012.12877
    (2020).
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: ^([43](ch16.html#idm45720172239280-marker)) Hugo Touvron等人，“训练数据高效的图像Transformer和通过注意力蒸馏”，arXiv预印本arxiv:2012.12877（2020年）。
- en: '^([44](ch16.html#idm45720172235056-marker)) Andrew Jaegle et al., “Perceiver:
    General Perception with Iterative Attention”, arXiv preprint arxiv:2103.03206
    (2021).'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '^([44](ch16.html#idm45720172235056-marker)) Andrew Jaegle等人，“Perceiver: 带有迭代注意力的通用感知”，arXiv预印本arxiv:2103.03206（2021）。'
- en: ^([45](ch16.html#idm45720172223328-marker)) Mathilde Caron et al., “Emerging
    Properties in Self-Supervised Vision Transformers”, arXiv preprint arxiv:2104.14294
    (2021).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ^([45](ch16.html#idm45720172223328-marker)) Mathilde Caron等人，“自监督视觉Transformer中的新兴属性”，arXiv预印本arxiv:2104.14294（2021）。
- en: ^([46](ch16.html#idm45720172217680-marker)) Xiaohua Zhai et al., “Scaling Vision
    Transformers”, arXiv preprint arxiv:2106.04560v1 (2021).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: ^([46](ch16.html#idm45720172217680-marker)) Xiaohua Zhai等人，“缩放视觉Transformer”，arXiv预印本arxiv:2106.04560v1（2021）。
- en: '^([47](ch16.html#idm45720172215648-marker)) Mitchell Wortsman et al., “Model
    Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accuracy Without
    Increasing Inference Time”, arXiv preprint arxiv:2203.05482v1 (2022).'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: ^([47](ch16.html#idm45720172215648-marker)) Mitchell Wortsman等人，“模型汤：多个微调模型的权重平均提高准确性而不增加推理时间”，arXiv预印本arxiv:2203.05482v1（2022）。
- en: ^([48](ch16.html#idm45720172211920-marker)) Alec Radford et al., “Learning Transferable
    Visual Models From Natural Language Supervision”, arXiv preprint arxiv:2103.00020
    (2021).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ^([48](ch16.html#idm45720172211920-marker)) Alec Radford等人，“从自然语言监督中学习可转移的视觉模型”，arXiv预印本arxiv:2103.00020（2021）。
- en: ^([49](ch16.html#idm45720172210416-marker)) Aditya Ramesh et al., “Zero-Shot
    Text-to-Image Generation”, arXiv preprint arxiv:2102.12092 (2021).
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ^([49](ch16.html#idm45720172210416-marker)) Aditya Ramesh等人，“零样本文本到图像生成”，arXiv预印本arxiv:2102.12092（2021）。
- en: ^([50](ch16.html#idm45720172208784-marker)) Aditya Ramesh et al., “Hierarchical
    Text-Conditional Image Generation with CLIP Latents”, arXiv preprint arxiv:2204.06125
    (2022).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ^([50](ch16.html#idm45720172208784-marker)) Aditya Ramesh等人，“具有CLIP潜变量的分层文本条件图像生成”，arXiv预印本arxiv:2204.06125（2022）。
- en: '^([51](ch16.html#idm45720172205280-marker)) Jean-Baptiste Alayrac et al., “Flamingo:
    a Visual Language Model for Few-Shot Learning”, arXiv preprint arxiv:2204.14198
    (2022).'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: ^([51](ch16.html#idm45720172205280-marker)) Jean-Baptiste Alayrac等人，“Flamingo：用于少样本学习的视觉语言模型”，arXiv预印本arxiv:2204.14198（2022）。
- en: ^([52](ch16.html#idm45720172203760-marker)) Scott Reed et al., “A Generalist
    Agent”, arXiv preprint arxiv:2205.06175 (2022).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: ^([52](ch16.html#idm45720172203760-marker)) Scott Reed等人，“通用主体代理”，arXiv预印本arxiv:2205.06175（2022）。
