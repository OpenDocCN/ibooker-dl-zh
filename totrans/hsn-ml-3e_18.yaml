- en: Chapter 16\. Natural Language Processing with RNNs and Attention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章。使用RNN和注意力进行自然语言处理
- en: 'When Alan Turing imagined his famous [Turing test](https://homl.info/turingtest)⁠^([1](ch16.html#idm45720177608176))
    in 1950, he proposed a way to evaluate a machine’s ability to match human intelligence.
    He could have tested for many things, such as the ability to recognize cats in
    pictures, play chess, compose music, or escape a maze, but, interestingly, he
    chose a linguistic task. More specifically, he devised a *chatbot* capable of
    fooling its interlocutor into thinking it was human.⁠^([2](ch16.html#idm45720177605632))
    This test does have its weaknesses: a set of hardcoded rules can fool unsuspecting
    or naive humans (e.g., the machine could give vague predefined answers in response
    to some keywords, it could pretend that it is joking or drunk to get a pass on
    its weirdest answers, or it could escape difficult questions by answering them
    with its own questions), and many aspects of human intelligence are utterly ignored
    (e.g., the ability to interpret nonverbal communication such as facial expressions,
    or to learn a manual task). But the test does highlight the fact that mastering
    language is arguably *Homo sapiens*’s greatest cognitive ability.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当艾伦·图灵在1950年想象他著名的[Turing测试](https://homl.info/turingtest)时，他提出了一种评估机器匹配人类智能能力的方法。他本可以测试许多事情，比如识别图片中的猫、下棋、创作音乐或逃离迷宫，但有趣的是，他选择了一项语言任务。更具体地说，他设计了一个*聊天机器人*，能够愚弄对话者以为它是人类。这个测试确实有其弱点：一组硬编码规则可以愚弄毫无戒心或天真的人类（例如，机器可以对某些关键词给出模糊的预定义答案，可以假装在回答一些最奇怪的问题时开玩笑或喝醉，或者可以通过用自己的问题回答难题来逃避困难的问题），并且许多人类智能的方面完全被忽视（例如，解释非言语交流，如面部表情，或学习手动任务的能力）。但这个测试确实突显了掌握语言可能是*智人*最伟大的认知能力。
- en: Can we build a machine that can master written and spoken language? This is
    the ultimate goal of NLP research, but it’s a bit too broad, so in practice researchers
    focus on more specific tasks, such as text classification, translation, summarization,
    question answering, and many more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否构建一台能够掌握书面和口头语言的机器？这是自然语言处理研究的终极目标，但实际上研究人员更专注于更具体的任务，比如文本分类、翻译、摘要、问答等等。
- en: A common approach for natural language tasks is to use recurrent neural networks.
    We will therefore continue to explore RNNs (introduced in [Chapter 15](ch15.html#rnn_chapter)),
    starting with a *character RNN*, or *char-RNN*, trained to predict the next character
    in a sentence. This will allow us to generate some original text. We will first
    use a *stateless RNN* (which learns on random portions of text at each iteration,
    without any information on the rest of the text), then we will build a *stateful
    RNN* (which preserves the hidden state between training iterations and continues
    reading where it left off, allowing it to learn longer patterns). Next, we will
    build an RNN to perform sentiment analysis (e.g., reading movie reviews and extracting
    the rater’s feeling about the movie), this time treating sentences as sequences
    of words, rather than characters. Then we will show how RNNs can be used to build
    an encoder–decoder architecture capable of performing neural machine translation
    (NMT), translating English to Spanish.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言任务的一种常见方法是使用循环神经网络。因此，我们将继续探索循环神经网络（在[第15章](ch15.html#rnn_chapter)中介绍），首先是*字符RNN*或*char-RNN*，训练以预测句子中的下一个字符。这将使我们能够生成一些原创文本。我们将首先使用*无状态RNN*（在每次迭代中学习文本的随机部分，没有关于文本其余部分的信息），然后我们将构建*有状态RNN*（在训练迭代之间保留隐藏状态，并继续阅读离开的地方，使其能够学习更长的模式）。接下来，我们将构建一个RNN来执行情感分析（例如，阅读电影评论并提取评价者对电影的感受），这次将句子视为单词序列，而不是字符。然后我们将展示如何使用RNN来构建一个编码器-解码器架构，能够执行神经机器翻译（NMT），将英语翻译成西班牙语。
- en: In the second part of this chapter, we will explore *attention mechanisms*.
    As their name suggests, these are neural network components that learn to select
    the part of the inputs that the rest of the model should focus on at each time
    step. First, we will boost the performance of an RNN-based encoder–decoder architecture
    using attention. Next, we will drop RNNs altogether and use a very successful
    attention-only architecture, called the *transformer*, to build a translation
    model. We will then discuss some of the most important advances in NLP in the
    last few years, including incredibly powerful language models such as GPT and
    BERT, both based on transformers. Lastly, I will show you how to get started with
    the excellent Transformers library by Hugging Face.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们将探索*注意机制*。正如它们的名字所示，这些是神经网络组件，它们学习选择模型在每个时间步应该关注的输入部分。首先，我们将通过注意机制提高基于RNN的编码器-解码器架构的性能。接下来，我们将完全放弃RNN，并使用一个非常成功的仅注意架构，称为*transformer*，来构建一个翻译模型。然后，我们将讨论过去几年自然语言处理中一些最重要的进展，包括基于transformer的GPT和BERT等非常强大的语言模型。最后，我将向您展示如何开始使用Hugging
    Face出色的Transformers库。
- en: Let’s start with a simple and fun model that can write like Shakespeare (sort
    of).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单而有趣的模型开始，这个模型可以像莎士比亚一样写作（某种程度上）。
- en: Generating Shakespearean Text Using a Character RNN
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用字符RNN生成莎士比亚文本
- en: 'In a famous [2015 blog post](https://homl.info/charrnn) titled “The Unreasonable
    Effectiveness of Recurrent Neural Networks”, Andrej Karpathy showed how to train
    an RNN to predict the next character in a sentence. This *char-RNN* can then be
    used to generate novel text, one character at a time. Here is a small sample of
    the text generated by a char-RNN model after it was trained on all of Shakespeare’s
    works:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇著名的[2015年博客文章](https://homl.info/charrnn)中，安德烈·卡帕西展示了如何训练一个RNN来预测句子中的下一个字符。然后可以使用这个*char-RNN*逐个字符生成新文本。以下是一个经过训练所有莎士比亚作品后由char-RNN模型生成的文本的小样本：
- en: 'PANDARUS:'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 潘达鲁斯：
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alas, I think he shall be come approached and the day
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 唉，我想他将会被接近并且这一天
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When little srain would be attain’d into being never fed,
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当一点点智慧被获得而从未被喂养时，
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And who is but a chain and subjects of his death,
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 而谁不是一条链，是他死亡的主题，
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I should not sleep.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我不应该睡觉。
- en: Not exactly a masterpiece, but it is still impressive that the model was able
    to learn words, grammar, proper punctuation, and more, just by learning to predict
    the next character in a sentence. This is our first example of a *language model*;
    similar (but much more powerful) language models, discussed later in this chapter,
    are at the core of modern NLP. In the remainder of this section we’ll build a
    char-RNN step by step, starting with the creation of the dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不是杰作，但仍然令人印象深刻，模型能够学习单词、语法、正确的标点符号等，只是通过学习预测句子中的下一个字符。这是我们的第一个*语言模型*示例；本章后面讨论的类似（但更强大）的语言模型是现代自然语言处理的核心。在本节的其余部分，我们将逐步构建一个
    char-RNN，从创建数据集开始。
- en: Creating the Training Dataset
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练数据集
- en: 'First, using Keras’s handy `tf.keras.utils.get_file()` function, let’s download
    all of Shakespeare’s works. The data is loaded from Andrej Karpathy’s [char-rnn
    project](https://github.com/karpathy/char-rnn):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用Keras方便的 `tf.keras.utils.get_file()` 函数，让我们下载所有莎士比亚的作品。数据是从Andrej Karpathy的[char-rnn项目](https://github.com/karpathy/char-rnn)加载的：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s print the first few lines:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印前几行：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Looks like Shakespeare all right!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来像是莎士比亚的作品！
- en: 'Next, we’ll use a `tf.keras.layers.TextVectorization` layer (introduced in
    [Chapter 13](ch13.html#data_chapter)) to encode this text. We set `split="character"`
    to get character-level encoding rather than the default word-level encoding, and
    we use `standardize="lower"` to convert the text to lowercase (which will simplify
    the task):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `tf.keras.layers.TextVectorization` 层（在[第13章](ch13.html#data_chapter)介绍）对此文本进行编码。我们设置
    `split="character"` 以获得字符级别的编码，而不是默认的单词级别编码，并且我们使用 `standardize="lower"` 将文本转换为小写（这将简化任务）：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Each character is now mapped to an integer, starting at 2\. The `TextVectorization`
    layer reserved the value 0 for padding tokens, and it reserved 1 for unknown characters.
    We won’t need either of these tokens for now, so let’s subtract 2 from the character
    IDs and compute the number of distinct characters and the total number of characters:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个字符都映射到一个整数，从2开始。`TextVectorization` 层将值0保留给填充标记，将值1保留给未知字符。目前我们不需要这两个标记，所以让我们从字符ID中减去2，并计算不同字符的数量和总字符数：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, just like we did in [Chapter 15](ch15.html#rnn_chapter), we can turn
    this very long sequence into a dataset of windows that we can then use to train
    a sequence-to-sequence RNN. The targets will be similar to the inputs, but shifted
    by one time step into the “future”. For example, one sample in the dataset may
    be a sequence of character IDs representing the text “to be or not to b” (without
    the final “e”), and the corresponding target—a sequence of character IDs representing
    the text “o be or not to be” (with the final “e”, but without the leading “t”).
    Let’s write a small utility function to convert a long sequence of character IDs
    into a dataset of input/target window pairs:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，就像我们在[第15章](ch15.html#rnn_chapter)中所做的那样，我们可以将这个非常长的序列转换为一个窗口的数据集，然后用它来训练一个序列到序列的RNN。目标将类似于输入，但是向“未来”移动了一个时间步。例如，数据集中的一个样本可能是代表文本“to
    be or not to b”（没有最后的“e”）的字符ID序列，相应的目标是代表文本“o be or not to be”（有最后的“e”，但没有开头的“t”）的字符ID序列。让我们编写一个小型实用函数，将字符ID的长序列转换为输入/目标窗口对的数据集：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This function starts much like the `to_windows()` custom utility function we
    created in [Chapter 15](ch15.html#rnn_chapter):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数开始得很像我们在[第15章](ch15.html#rnn_chapter)中创建的 `to_windows()` 自定义实用函数：
- en: It takes a sequence as input (i.e., the encoded text), and creates a dataset
    containing all the windows of the desired length.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它以一个序列作为输入（即编码文本），并创建一个包含所需长度的所有窗口的数据集。
- en: It increases the length by one, since we need the next character for the target.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将长度增加一，因为我们需要下一个字符作为目标。
- en: Then, it shuffles the windows (optionally), batches them, splits them into input/output
    pairs, and activates prefetching.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，它会对窗口进行洗牌（可选），将它们分批处理，拆分为输入/输出对，并激活预取。
- en: '[Figure 16-1](#window_dataset_diagram) summarizes the dataset preparation steps:
    it shows windows of length 11, and a batch size of 3\. The start index of each
    window is indicated next to it.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 16-1](#window_dataset_diagram) 总结了数据集准备步骤：它展示了长度为11的窗口，批量大小为3。每个窗口的起始索引在其旁边标出。'
- en: '![mls3 1601](assets/mls3_1601.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1601](assets/mls3_1601.png)'
- en: Figure 16-1\. Preparing a dataset of shuffled windows
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-1\. 准备一个洗牌窗口的数据集
- en: 'Now we’re ready to create the training set, the validation set, and the test
    set. We will use roughly 90% of the text for training, 5% for validation, and
    5% for testing:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备创建训练集、验证集和测试集。我们将大约使用文本的90%进行训练，5%用于验证，5%用于测试：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'We set the window length to 100, but you can try tuning it: it’s easier and
    faster to train RNNs on shorter input sequences, but the RNN will not be able
    to learn any pattern longer than `length`, so don’t make it too small.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将窗口长度设置为100，但您可以尝试调整它：在较短的输入序列上训练RNN更容易更快，但RNN将无法学习任何长于 `length` 的模式，所以不要将其设置得太小。
- en: That’s it! Preparing the dataset was the hardest part. Now let’s create the
    model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！准备数据集是最困难的部分。现在让我们创建模型。
- en: Building and Training the Char-RNN Model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和训练 Char-RNN 模型
- en: 'Since our dataset is reasonably large, and modeling language is quite a difficult
    task, we need more than a simple RNN with a few recurrent neurons. Let’s build
    and train a model with one `GRU` layer composed of 128 units (you can try tweaking
    the number of layers and units later, if needed):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集相当大，而建模语言是一个相当困难的任务，我们需要不止一个简单的具有几个循环神经元的RNN。让我们构建并训练一个由128个单元组成的 `GRU`
    层的模型（如果需要，稍后可以尝试调整层数和单元数）：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s go over this code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看这段代码：
- en: We use an `Embedding` layer as the first layer, to encode the character IDs
    (embeddings were introduced in [Chapter 13](ch13.html#data_chapter)). The `Embedding`
    layer’s number of input dimensions is the number of distinct character IDs, and
    the number of output dimensions is a hyperparameter you can tune—we’ll set it
    to 16 for now. Whereas the inputs of the `Embedding` layer will be 2D tensors
    of shape [*batch size*, *window length*], the output of the `Embedding` layer
    will be a 3D tensor of shape [*batch size*, *window length*, *embedding size*].
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用一个`Embedding`层作为第一层，用于编码字符ID（嵌入在[第13章](ch13.html#data_chapter)中介绍）。`Embedding`层的输入维度数是不同字符ID的数量，输出维度数是一个可以调整的超参数，我们暂时将其设置为16。`Embedding`层的输入将是形状为[*批量大小*,
    *窗口长度*]的2D张量，`Embedding`层的输出将是形状为[*批量大小*, *窗口长度*, *嵌入大小*]的3D张量。
- en: 'We use a `Dense` layer for the output layer: it must have 39 units (`n_tokens`)
    because there are 39 distinct characters in the text, and we want to output a
    probability for each possible character (at each time step). The 39 output probabilities
    should sum up to 1 at each time step, so we apply the softmax activation function
    to the outputs of the `Dense` layer.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用一个`Dense`层作为输出层：它必须有39个单元（`n_tokens`），因为文本中有39个不同的字符，并且我们希望在每个时间步输出每个可能字符的概率。39个输出概率应该在每个时间步加起来为1，因此我们将softmax激活函数应用于`Dense`层的输出。
- en: Lastly, we compile this model, using the `"sparse_categorical_crossentropy"`
    loss and a Nadam optimizer, and we train the model for several epochs,^([3](ch16.html#idm45720176925824))
    using a `ModelCheckpoint` callback to save the best model (in terms of validation
    accuracy) as training progresses.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们编译这个模型，使用`"sparse_categorical_crossentropy"`损失和Nadam优化器，然后训练模型多个epoch，使用`ModelCheckpoint`回调来保存训练过程中验证准确性最好的模型。
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you are running this code on Colab with a GPU activated, then training should
    take roughly one to two hours. You can reduce the number of epochs if you don’t
    want to wait that long, but of course the model’s accuracy will probably be lower.
    If the Colab session times out, make sure to reconnect quickly, or else the Colab
    runtime will be destroyed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在启用GPU的Colab上运行此代码，则训练大约需要一到两个小时。如果您不想等待那么长时间，可以减少epoch的数量，但当然模型的准确性可能会降低。如果Colab会话超时，请确保快速重新连接，否则Colab运行时将被销毁。
- en: 'This model does not handle text preprocessing, so let’s wrap it in a final
    model containing the `tf.keras.layers.TextVectorization` layer as the first layer,
    plus a `tf.keras.layers.Lambda` layer to subtract 2 from the character IDs since
    we’re not using the padding and unknown tokens for now:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型不处理文本预处理，所以让我们将其包装在一个最终模型中，包含`tf.keras.layers.TextVectorization`层作为第一层，加上一个`tf.keras.layers.Lambda`层，从字符ID中减去2，因为我们暂时不使用填充和未知标记：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And now let’s use it to predict the next character in a sentence:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用它来预测句子中的下一个字符：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Great, the model correctly predicted the next character. Now let’s use this
    model to pretend we’re Shakespeare!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，模型正确预测了下一个字符。现在让我们使用这个模型假装我们是莎士比亚！
- en: Generating Fake Shakespearean Text
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成虚假的莎士比亚文本
- en: 'To generate new text using the char-RNN model, we could feed it some text,
    make the model predict the most likely next letter, add it to the end of the text,
    then give the extended text to the model to guess the next letter, and so on.
    This is called *greedy decoding*. But in practice this often leads to the same
    words being repeated over and over again. Instead, we can sample the next character
    randomly, with a probability equal to the estimated probability, using TensorFlow’s
    `tf.random.categorical()` function. This will generate more diverse and interesting
    text. The `categorical()` function samples random class indices, given the class
    log probabilities (logits). For example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用char-RNN模型生成新文本时，我们可以将一些文本输入模型，让模型预测最有可能的下一个字母，将其添加到文本末尾，然后将扩展后的文本提供给模型猜测下一个字母，依此类推。这被称为*贪婪解码*。但在实践中，这经常导致相同的单词一遍又一遍地重复。相反，我们可以随机采样下一个字符，概率等于估计的概率，使用TensorFlow的`tf.random.categorical()`函数。这将生成更多样化和有趣的文本。`categorical()`函数会根据类别对数概率（logits）随机采样随机类别索引。例如：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To have more control over the diversity of the generated text, we can divide
    the logits by a number called the *temperature*, which we can tweak as we wish.
    A temperature close to zero favors high-probability characters, while a high temperature
    gives all characters an equal probability. Lower temperatures are typically preferred
    when generating fairly rigid and precise text, such as mathematical equations,
    while higher temperatures are preferred when generating more diverse and creative
    text. The following `next_char()` custom helper function uses this approach to
    pick the next character to add to the input text:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地控制生成文本的多样性，我们可以将logits除以一个称为*温度*的数字，我们可以根据需要进行调整。接近零的温度偏好高概率字符，而高温度使所有字符具有相等的概率。在生成相对严格和精确的文本（如数学方程式）时，通常更喜欢较低的温度，而在生成更多样化和创意性的文本时，更喜欢较高的温度。以下`next_char()`自定义辅助函数使用这种方法选择要添加到输入文本中的下一个字符：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we can write another small helper function that will repeatedly call
    `next_char()` to get the next character and append it to the given text:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以编写另一个小的辅助函数，它将重复调用`next_char()`以获取下一个字符并将其附加到给定的文本中：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We are now ready to generate some text! Let’s try with different temperature
    values:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备生成一些文本！让我们尝试不同的温度值：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Shakespeare seems to be suffering from a heatwave. To generate more convincing
    text, a common technique is to sample only from the top *k* characters, or only
    from the smallest set of top characters whose total probability exceeds some threshold
    (this is called *nucleus sampling*). Alternatively, you could try using *beam
    search*, which we will discuss later in this chapter, or using more `GRU` layers
    and more neurons per layer, training for longer, and adding some regularization
    if needed. Also note that the model is currently incapable of learning patterns
    longer than `length`, which is just 100 characters. You could try making this
    window larger, but it will also make training harder, and even LSTM and GRU cells
    cannot handle very long sequences. An alternative approach is to use a stateful
    RNN.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 莎士比亚似乎正在遭受一场热浪。为了生成更具说服力的文本，一个常见的技术是仅从前 *k* 个字符中采样，或者仅从总概率超过某个阈值的最小一组顶级字符中采样（这被称为*核心采样*）。另外，您可以尝试使用*波束搜索*，我们将在本章后面讨论，或者使用更多的`GRU`层和更多的神经元每层，训练更长时间，并在需要时添加一些正则化。还要注意，模型目前无法学习比`length`更长的模式，`length`只是100个字符。您可以尝试将此窗口扩大，但这也会使训练更加困难，即使LSTM和GRU单元也无法处理非常长的序列。另一种替代方法是使用有状态的RNN。
- en: Stateful RNN
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有状态的RNN
- en: 'Until now, we have only used *stateless RNNs*: at each training iteration the
    model starts with a hidden state full of zeros, then it updates this state at
    each time step, and after the last time step, it throws it away as it is not needed
    anymore. What if we instructed the RNN to preserve this final state after processing
    a training batch and use it as the initial state for the next training batch?
    This way the model could learn long-term patterns despite only backpropagating
    through short sequences. This is called a *stateful RNN*. Let’s go over how to
    build one.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了*无状态的RNN*：在每次训练迭代中，模型从一个全零的隐藏状态开始，然后在每个时间步更新这个状态，在最后一个时间步之后，将其丢弃，因为不再需要。如果我们指示RNN在处理训练批次后保留这个最终状态，并将其用作下一个训练批次的初始状态，会怎样呢？这样，模型可以学习长期模式，尽管只通过短序列进行反向传播。这被称为*有状态的RNN*。让我们看看如何构建一个。
- en: First, note that a stateful RNN only makes sense if each input sequence in a
    batch starts exactly where the corresponding sequence in the previous batch left
    off. So the first thing we need to do to build a stateful RNN is to use sequential
    and nonoverlapping input sequences (rather than the shuffled and overlapping sequences
    we used to train stateless RNNs). When creating the `tf.data.Dataset`, we must
    therefore use `shift=length` (instead of `shift=1`) when calling the `window()`
    method. Moreover, we must *not* call the `shuffle()` method.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意到有状态的RNN只有在批次中的每个输入序列从上一个批次中对应序列的确切位置开始时才有意义。因此，我们构建有状态的RNN需要做的第一件事是使用顺序且不重叠的输入序列（而不是我们用来训练无状态RNN的洗牌和重叠序列）。在创建`tf.data.Dataset`时，因此在调用`window()`方法时必须使用`shift=length`（而不是`shift=1`）。此外，我们必须*不*调用`shuffle()`方法。
- en: 'Unfortunately, batching is much harder when preparing a dataset for a stateful
    RNN than it is for a stateless RNN. Indeed, if we were to call `batch(32)`, then
    32 consecutive windows would be put in the same batch, and the following batch
    would not continue each of these windows where it left off. The first batch would
    contain windows 1 to 32 and the second batch would contain windows 33 to 64, so
    if you consider, say, the first window of each batch (i.e., windows 1 and 33),
    you can see that they are not consecutive. The simplest solution to this problem
    is to just use a batch size of 1\. The following `to_dataset_for_stateful_rnn()`
    custom utility function uses this strategy to prepare a dataset for a stateful
    RNN:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，为有状态的RNN准备数据集时，批处理比为无状态的RNN更加困难。实际上，如果我们调用`batch(32)`，那么32个连续窗口将被放入同一个批次中，接下来的批次将不会继续每个窗口的位置。第一个批次将包含窗口1到32，第二个批次将包含窗口33到64，因此如果您考虑，比如说，每个批次的第一个窗口（即窗口1和33），您会发现它们不是连续的。这个问题的最简单解决方案就是只使用批量大小为1。以下的`to_dataset_for_stateful_rnn()`自定义实用函数使用这种策略来为有状态的RNN准备数据集：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Figure 16-2](#stateful_rnn_dataset_diagram) summarizes the main steps of this
    function.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-2](#stateful_rnn_dataset_diagram)总结了这个函数的主要步骤。'
- en: '![mls3 1602](assets/mls3_1602.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1602](assets/mls3_1602.png)'
- en: Figure 16-2\. Preparing a dataset of consecutive sequence fragments for a stateful
    RNN
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-2。为有状态的RNN准备连续序列片段的数据集
- en: 'Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s
    text into 32 texts of equal length, create one dataset of consecutive input sequences
    for each of them, and finally use `tf.data.Dataset.zip(datasets).map(lambda *windows:
    tf.stack(windows))` to create proper consecutive batches, where the *n*^(th) input
    sequence in a batch starts off exactly where the *n*^(th) input sequence ended
    in the previous batch (see the notebook for the full code).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '批处理更加困难，但并非不可能。例如，我们可以将莎士比亚的文本分成32个等长的文本，为每个文本创建一个连续输入序列的数据集，最后使用`tf.data.Dataset.zip(datasets).map(lambda
    *windows: tf.stack(windows))`来创建正确的连续批次，其中批次中的第*n*个输入序列从上一个批次中的第*n*个输入序列结束的地方开始（请参阅笔记本获取完整代码）。'
- en: 'Now, let’s create the stateful RNN. We need to set the `stateful` argument
    to `True` when creating each recurrent layer, and because the stateful RNN needs
    to know the batch size (since it will preserve a state for each input sequence
    in the batch). Therefore we must set the `batch_input_shape` argument in the first
    layer. Note that we can leave the second dimension unspecified, since the input
    sequences could have any length:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建有状态的RNN。在创建每个循环层时，我们需要将`stateful`参数设置为`True`，因为有状态的RNN需要知道批量大小（因为它将为批次中的每个输入序列保留一个状态）。因此，我们必须在第一层中设置`batch_input_shape`参数。请注意，我们可以将第二维度留空，因为输入序列可以具有任意长度：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'At the end of each epoch, we need to reset the states before we go back to
    the beginning of the text. For this, we can use a small custom Keras callback:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时期结束时，我们需要在回到文本开头之前重置状态。为此，我们可以使用一个小的自定义Keras回调：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And now we can compile the model and train it using our callback:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编译模型并使用我们的回调函数进行训练：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Tip
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: After this model is trained, it will only be possible to use it to make predictions
    for batches of the same size as were used during training. To avoid this restriction,
    create an identical *stateless* model, and copy the stateful model’s weights to
    this model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完这个模型后，只能用它来对与训练时相同大小的批次进行预测。为了避免这个限制，创建一个相同的*无状态*模型，并将有状态模型的权重复制到这个模型中。
- en: 'Interestingly, although a char-RNN model is just trained to predict the next
    character, this seemingly simple task actually requires it to learn some higher-level
    tasks as well. For example, to find the next character after “Great movie, I really”,
    it’s helpful to understand that the sentence is positive, so what follows is more
    likely to be the letter “l” (for “loved”) rather than “h” (for “hated”). In fact,
    a [2017 paper](https://homl.info/sentimentneuron)⁠^([4](ch16.html#idm45720176103360))
    by Alec Radford and other OpenAI researchers describes how the authors trained
    a big char-RNN-like model on a large dataset, and found that one of the neurons
    acted as an excellent sentiment analysis classifier: although the model was trained
    without any labels, the *sentiment neuron*—as they called it—reached state-of-the-art
    performance on sentiment analysis benchmarks. This foreshadowed and motivated
    unsupervised pretraining in NLP.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管char-RNN模型只是训练来预测下一个字符，但这看似简单的任务实际上也需要它学习一些更高级的任务。例如，要找到“Great movie,
    I really”之后的下一个字符，了解到这句话是积极的是有帮助的，所以下一个字符更有可能是“l”（代表“loved”）而不是“h”（代表“hated”）。事实上，OpenAI的Alec
    Radford和其他研究人员在一篇2017年的论文中描述了他们如何在大型数据集上训练了一个类似于大型char-RNN模型，并发现其中一个神经元表现出色地作为情感分析分类器：尽管该模型在没有任何标签的情况下进行了训练，但他们称之为*情感神经元*达到了情感分析基准测试的最新性能。这预示并激励了NLP中的无监督预训练。
- en: But before we explore unsupervised pretraining, let’s turn our attention to
    word-level models and how to use them in a supervised fashion for sentiment analysis.
    In the process, you will learn how to handle sequences of variable lengths using
    masking.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但在探索无监督预训练之前，让我们将注意力转向单词级模型以及如何在监督方式下用它们进行情感分析。在这个过程中，您将学习如何使用掩码处理可变长度的序列。
- en: Sentiment Analysis
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: 'Generating text can be fun and instructive, but in real-life projects, one
    of the most common applications of NLP is text classification—especially sentiment
    analysis. If image classification on the MNIST dataset is the “Hello world!” of
    computer vision, then sentiment analysis on the IMDb reviews dataset is the “Hello
    world!” of natural language processing. The IMDb dataset consists of 50,000 movie
    reviews in English (25,000 for training, 25,000 for testing) extracted from the
    famous [Internet Movie Database](https://imdb.com), along with a simple binary
    target for each review indicating whether it is negative (0) or positive (1).
    Just like MNIST, the IMDb reviews dataset is popular for good reasons: it is simple
    enough to be tackled on a laptop in a reasonable amount of time, but challenging
    enough to be fun and rewarding.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成文本可能很有趣且有教育意义，但在实际项目中，自然语言处理的最常见应用之一是文本分类，尤其是情感分析。如果在MNIST数据集上进行图像分类是计算机视觉的“Hello
    world！”，那么在IMDb评论数据集上进行情感分析就是自然语言处理的“Hello world！”。IMDb数据集包含了来自著名的[互联网电影数据库](https://imdb.com)的50,000条英文电影评论（25,000条用于训练，25,000条用于测试），每条评论都有一个简单的二进制目标，表示其是否为负面（0）或正面（1）。就像MNIST一样，IMDb评论数据集之所以受欢迎是有充分理由的：它足够简单，可以在笔记本电脑上在合理的时间内处理，但足够具有挑战性和有趣。
- en: 'Let’s load the IMDb dataset using the TensorFlow Datasets library (introduced
    in [Chapter 13](ch13.html#data_chapter)). We’ll use the first 90% of the training
    set for training, and the remaining 10% for validation:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用TensorFlow Datasets库加载IMDb数据集（在[第13章](ch13.html#data_chapter)中介绍）。我们将使用训练集的前90%进行训练，剩下的10%用于验证：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Tip
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Keras also includes a function for loading the IMDb dataset, if you prefer:
    `tf.keras.datasets.imdb.load_data()`. The reviews are already preprocessed as
    sequences of word IDs.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您愿意，Keras还包括一个用于加载IMDb数据集的函数：`tf.keras.datasets.imdb.load_data()`。评论已经被预处理为单词ID的序列。
- en: 'Let’s inspect a few reviews:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一些评论：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Some reviews are easy to classify. For example, the first review includes the
    words “terrible movie” in the very first sentence. But in many cases things are
    not that simple. For example, the third review starts off positively, even though
    it’s ultimately a negative review (label 0).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有些评论很容易分类。例如，第一条评论中的第一句话包含“terrible movie”这几个词。但在许多情况下，事情并不那么简单。例如，第三条评论一开始是积极的，尽管最终是一个负面评论（标签0）。
- en: 'To build a model for this task, we need to preprocess the text, but this time
    we will chop it into words instead of characters. For this, we can use the `tf.keras.​lay⁠ers.TextVectorization`
    layer again. Note that it uses spaces to identify word boundaries, which will
    not work well in some languages. For example, Chinese writing does not use spaces
    between words, Vietnamese uses spaces even within words, and German often attaches
    multiple words together, without spaces. Even in English, spaces are not always
    the best way to tokenize text: think of “San Francisco” or “#ILoveDeepLearning”.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为这个任务构建一个模型，我们需要预处理文本，但这次我们将其分成单词而不是字符。为此，我们可以再次使用`tf.keras.​lay⁠ers.TextVectorization`层。请注意，它使用空格来识别单词边界，在某些语言中可能效果不佳。例如，中文书写不使用单词之间的空格，越南语甚至在单词内部也使用空格，德语经常将多个单词连接在一起，没有空格。即使在英语中，空格也不总是分词的最佳方式：想想“San
    Francisco”或“#ILoveDeepLearning”。
- en: Fortunately, there are solutions to address these issues. In a [2016 paper](https://homl.info/rarewords),⁠^([5](ch16.html#idm45720175873872))
    Rico Sennrich et al. from the University of Edinburgh explored several methods
    to tokenize and detokenize text at the subword level. This way, even if your model
    encounters a rare word it has never seen before, it can still reasonably guess
    what it means. For example, even if the model never saw the word “smartest” during
    training, if it learned the word “smart” and it also learned that the suffix “est”
    means “the most”, it can infer the meaning of “smartest”. One of the techniques
    the authors evaluated is *byte pair encoding* (BPE). BPE works by splitting the
    whole training set into individual characters (including spaces), then repeatedly
    merging the most frequent adjacent pairs until the vocabulary reaches the desired
    size.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有解决这些问题的解决方案。在[2016年的一篇论文](https://homl.info/rarewords)，爱丁堡大学的Rico Sennrich等人探索了几种在子词级别对文本进行标记和去标记化的方法。这样，即使您的模型遇到了以前从未见过的罕见单词，它仍然可以合理地猜测它的含义。例如，即使模型在训练期间从未见过单词“smartest”，如果它学会了单词“smart”并且还学会了后缀“est”表示“最”，它可以推断出“smartest”的含义。作者评估的技术之一是*字节对编码*（BPE）。BPE通过将整个训练集拆分为单个字符（包括空格），然后重复合并最频繁的相邻对，直到词汇表达到所需大小。
- en: 'A [2018 paper](https://homl.info/subword)⁠^([6](ch16.html#idm45720175847312))
    by Taku Kudo at Google further improved subword tokenization, often removing the
    need for language-specific preprocessing prior to tokenization. Moreover, the
    paper proposed a novel regularization technique called *subword regularization*,
    which improves accuracy and robustness by introducing some randomness in tokenization
    during training: for example, “New England” may be tokenized as “New” + “England”,
    or “New” + “Eng” + “land”, or simply “New England” (just one token). Google’s
    [*SentencePiece*](https://github.com/google/sentencepiece) project provides an
    open source implementation, which is described in a [paper](https://homl.info/sentencepiece)⁠^([7](ch16.html#idm45720175842096))
    by Taku Kudo and John Richardson.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Google的Taku Kudo在2018年发表的一篇论文进一步改进了子词标记化，通常消除了标记化之前需要进行特定于语言的预处理的需要。此外，该论文提出了一种称为*子词正则化*的新型正则化技术，通过在训练期间在标记化中引入一些随机性来提高准确性和稳健性：例如，“New
    England”可以被标记为“New”+“England”，或“New”+“Eng”+“land”，或简单地“New England”（只有一个标记）。Google的[*SentencePiece*](https://github.com/google/sentencepiece)项目提供了一个开源实现，该实现在Taku
    Kudo和John Richardson的一篇[论文](https://homl.info/sentencepiece)中有描述。
- en: The [TensorFlow Text](https://homl.info/tftext) library also implements various
    tokenization strategies, including [WordPiece](https://homl.info/wordpiece)⁠^([8](ch16.html#idm45720175839056))
    (a variant of BPE), and last but not least, the [Tokenizers library by Hugging
    Face](https://homl.info/tokenizers) implements a wide range of extremely fast
    tokenizers.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorFlow Text](https://homl.info/tftext)库还实现了各种标记化策略，包括[WordPiece](https://homl.info/wordpiece)（BPE的变体），最后但同样重要的是，[Hugging
    Face的Tokenizers库](https://homl.info/tokenizers)实现了一系列极快的标记化器。'
- en: 'However, for the IMDb task in English, using spaces for token boundaries should
    be good enough. So let’s go ahead with creating a `TextVectorization` layer and
    adapting it to the training set. We will limit the vocabulary to 1,000 tokens,
    including the most frequent 998 words plus a padding token and a token for unknown
    words, since it’s unlikely that very rare words will be important for this task,
    and limiting the vocabulary size will reduce the number of parameters the model
    needs to learn:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于英语中的IMDb任务，使用空格作为标记边界应该足够好。因此，让我们继续创建一个`TextVectorization`层，并将其调整到训练集。我们将词汇表限制为1,000个标记，包括最常见的998个单词以及一个填充标记和一个未知单词的标记，因为很少见的单词不太可能对这个任务很重要，并且限制词汇表大小将减少模型需要学习的参数数量：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we can create the model and train it:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以创建模型并训练它：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The first layer is the `TextVectorization` layer we just prepared, followed
    by an `Embedding` layer that will convert word IDs into embeddings. The embedding
    matrix needs to have one row per token in the vocabulary (`vocab_size`) and one
    column per embedding dimension (this example uses 128 dimensions, but this is
    a hyperparameter you could tune). Next we use a `GRU` layer and a `Dense` layer
    with a single neuron and the sigmoid activation function, since this is a binary
    classification task: the model’s output will be the estimated probability that
    the review expresses a positive sentiment regarding the movie. We then compile
    the model, and we fit it on the dataset we prepared earlier for a couple of epochs
    (or you can train for longer to get better results).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是我们刚刚准备的`TextVectorization`层，接着是一个`Embedding`层，将单词ID转换为嵌入。嵌入矩阵需要每个词汇表中的标记一行（`vocab_size`），每个嵌入维度一列（此示例使用128维，但这是一个可以调整的超参数）。接下来我们使用一个`GRU`层和一个具有单个神经元和sigmoid激活函数的`Dense`层，因为这是一个二元分类任务：模型的输出将是评论表达对电影积极情绪的估计概率。然后我们编译模型，并在我们之前准备的数据集上进行几个时期的拟合（或者您可以训练更长时间以获得更好的结果）。
- en: 'Sadly, if you run this code, you will generally find that the model fails to
    learn anything at all: the accuracy remains close to 50%, no better than random
    chance. Why is that? The reviews have different lengths, so when the `TextVectorization`
    layer converts them to sequences of token IDs, it pads the shorter sequences using
    the padding token (with ID 0) to make them as long as the longest sequence in
    the batch. As a result, most sequences end with many padding tokens—often dozens
    or even hundreds of them. Even though we’re using a `GRU` layer, which is much
    better than a `SimpleRNN` layer, its short-term memory is still not great, so
    when it goes through many padding tokens, it ends up forgetting what the review
    was about! One solution is to feed the model with batches of equal-length sentences
    (which also speeds up training). Another solution is to make the RNN ignore the
    padding tokens. This can be done using masking.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，如果运行此代码，通常会发现模型根本无法学习任何东西：准确率保持接近50%，不比随机机会好。为什么呢？评论的长度不同，因此当`TextVectorization`层将它们转换为标记ID序列时，它使用填充标记（ID为0）填充较短的序列，使它们与批次中最长序列一样长。结果，大多数序列以许多填充标记结尾——通常是几十甚至几百个。即使我们使用的是比`SimpleRNN`层更好的`GRU`层，它的短期记忆仍然不太好，因此当它经过许多填充标记时，它最终会忘记评论的内容！一个解决方案是用等长的句子批次喂给模型（这也加快了训练速度）。另一个解决方案是让RNN忽略填充标记。这可以通过掩码来实现。
- en: Masking
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码
- en: 'Making the model ignore padding tokens is trivial using Keras: simply add `mask_zero=True`
    when creating the `Embedding` layer. This means that padding tokens (whose ID
    is 0) will be ignored by all downstream layers. That’s all! If you retrain the
    previous model for a few epochs, you will find that the validation accuracy quickly
    reaches over 80%.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras让模型忽略填充标记很简单：在创建`Embedding`层时简单地添加`mask_zero=True`。这意味着所有下游层都会忽略填充标记（其ID为0）。就是这样！如果对先前的模型进行几个时期的重新训练，您会发现验证准确率很快就能达到80%以上。
- en: 'The way this works is that the `Embedding` layer creates a *mask tensor* equal
    to `tf.math.not_equal(inputs, 0)`: it is a Boolean tensor with the same shape
    as the inputs, and it is equal to `False` anywhere the token IDs are 0, or `True`
    otherwise. This mask tensor is then automatically propagated by the model to the
    next layer. If that layer’s `call()` method has a `mask` argument, then it automatically
    receives the mask. This allows the layer to ignore the appropriate time steps.
    Each layer may handle the mask differently, but in general they simply ignore
    masked time steps (i.e., time steps for which the mask is `False`). For example,
    when a recurrent layer encounters a masked time step, it simply copies the output
    from the previous time step.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种工作方式是，`Embedding`层创建一个等于`tf.math.not_equal(inputs, 0)`的*掩码张量*：它是一个布尔张量，形状与输入相同，如果标记ID为0，则等于`False`，否则等于`True`。然后，该掩码张量会被模型自动传播到下一层。如果该层的`call()`方法有一个`mask`参数，那么它会自动接收到掩码。这使得该层能够忽略适当的时间步。每个层可能会以不同的方式处理掩码，但通常它们只是忽略被掩码的时间步（即掩码为`False`的时间步）。例如，当循环层遇到被掩码的时间步时，它只是复制前一个时间步的输出。
- en: 'Next, if the layer’s `supports_masking` attribute is `True`, then the mask
    is automatically propagated to the next layer. It keeps propagating this way for
    as long as the layers have `supports_masking=True`. As an example, a recurrent
    layer’s `supports_​mask⁠ing` attribute is `True` when `return_sequences=True`,
    but it’s `False` when `return_​sequen⁠ces=False` since there’s no need for a mask
    anymore in this case. So if you have a model with several recurrent layers with
    `return_sequences=True`, followed by a recurrent layer with `return_sequences=False`,
    then the mask will automatically propagate up to the last recurrent layer: that
    layer will use the mask to ignore masked steps, but it will not propagate the
    mask any further. Similarly, if you set `mask_zero=True` when creating the `Embedding`
    layer in the sentiment analysis model we just built, then the `GRU` layer will
    receive and use the mask automatically, but it will not propagate it any further,
    since `return_sequences` is not set to `True`.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果该层的`supports_masking`属性为`True`，那么掩码会自动传播到下一层。只要层具有`supports_masking=True`，它就会继续这样传播。例如，当`return_sequences=True`时，循环层的`supports_masking`属性为`True`，但当`return_sequences=False`时，它为`False`，因为在这种情况下不再需要掩码。因此，如果您有一个具有多个`return_sequences=True`的循环层，然后是一个`return_sequences=False`的循环层的模型，那么掩码将自动传播到最后一个循环层：该层将使用掩码来忽略被掩码的步骤，但不会进一步传播掩码。同样，如果在我们刚刚构建的情感分析模型中创建`Embedding`层时设置了`mask_zero=True`，那么`GRU`层将自动接收和使用掩码，但不会进一步传播，因为`return_sequences`没有设置为`True`。
- en: Tip
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Some layers need to update the mask before propagating it to the next layer:
    they do so by implementing the `compute_mask()` method, which takes two arguments:
    the inputs and the previous mask. It then computes the updated mask and returns
    it. The default implementation of `compute_mask()` just returns the previous mask
    unchanged.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一些层在将掩码传播到下一层之前需要更新掩码：它们通过实现`compute_mask()`方法来实现，该方法接受两个参数：输入和先前的掩码。然后计算更新后的掩码并返回。`compute_mask()`的默认实现只是返回先前的掩码而没有更改。
- en: 'Many Keras layers support masking: `SimpleRNN`, `GRU`, `LSTM`, `Bidirectional`,
    `Dense`, `TimeDistributed`, `Add`, and a few others (all in the `tf.keras.layers`
    package). However, convolutional layers (including `Conv1D`) do not support masking—it’s
    not obvious how they would do so anyway.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 许多Keras层支持掩码：`SimpleRNN`、`GRU`、`LSTM`、`Bidirectional`、`Dense`、`TimeDistributed`、`Add`等（都在`tf.keras.layers`包中）。然而，卷积层（包括`Conv1D`）不支持掩码——它们如何支持掩码并不明显。
- en: If the mask propagates all the way to the output, then it gets applied to the
    losses as well, so the masked time steps will not contribute to the loss (their
    loss will be 0). This assumes that the model outputs sequences, which is not the
    case in our sentiment analysis model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果掩码一直传播到输出，那么它也会应用到损失上，因此被掩码的时间步将不会对损失产生贡献（它们的损失将为0）。这假设模型输出序列，这在我们的情感分析模型中并不是这样。
- en: Warning
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'The `LSTM` and `GRU` layers have an optimized implementation for GPUs, based
    on Nvidia’s cuDNN library. However, this implementation only supports masking
    if all the padding tokens are at the end of the sequences. It also requires you
    to use the default values for several hyperparameters: `activation`, `recurrent_activation`,
    `recurrent_dropout`, `unroll`, `use_bias`, and `reset_after`. If that’s not the
    case, then these layers will fall back to the (much slower) default GPU implementation.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`LSTM`和`GRU`层具有基于Nvidia的cuDNN库的优化实现。但是，此实现仅在所有填充标记位于序列末尾时支持遮罩。它还要求您使用几个超参数的默认值：`activation`、`recurrent_activation`、`recurrent_dropout`、`unroll`、`use_bias`和`reset_after`。如果不是这种情况，那么这些层将退回到（速度慢得多的）默认GPU实现。'
- en: If you want to implement your own custom layer with masking support, you should
    add a `mask` argument to the `call()` method, and obviously make the method use
    the mask. Additionally, if the mask must be propagated to the next layers, then
    you should set `self.supports_masking=True` in the constructor. If the mask must
    be updated before it is propagated, then you must implement the `compute_mask()`
    method.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要实现支持遮罩的自定义层，应在`call()`方法中添加一个`mask`参数，并显然使方法使用该遮罩。此外，如果遮罩必须传播到下一层，则应在构造函数中设置`self.supports_masking=True`。如果必须在传播之前更新遮罩，则必须实现`compute_mask()`方法。
- en: 'If your model does not start with an `Embedding` layer, you may use the `tf.​​keras.layers.Masking`
    layer instead: by default, it sets the mask to `tf.math.​​reduce_any(tf.math.not_equal(X,
    0), axis=-1)`, meaning that time steps where the last dimension is full of zeros
    will be masked out in subsequent layers.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型不以`Embedding`层开头，可以使用`tf.keras.layers.Masking`层代替：默认情况下，它将遮罩设置为`tf.math.reduce_any(tf.math.not_equal(X,
    0), axis=-1)`，意味着最后一个维度全是零的时间步将在后续层中被遮罩。
- en: 'Using masking layers and automatic mask propagation works best for simple models.
    It will not always work for more complex models, such as when you need to mix
    `Conv1D` layers with recurrent layers. In such cases, you will need to explicitly
    compute the mask and pass it to the appropriate layers, using either the functional
    API or the subclassing API. For example, the following model is equivalent to
    the previous model, except it is built using the functional API and handles masking
    manually. It also adds a bit of dropout since the previous model was overfitting
    slightly:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用遮罩层和自动遮罩传播对简单模型效果最好。对于更复杂的模型，例如需要将`Conv1D`层与循环层混合时，并不总是适用。在这种情况下，您需要显式计算遮罩并将其传递给适当的层，可以使用函数式API或子类API。例如，以下模型与之前的模型等效，只是使用函数式API构建，并手动处理遮罩。它还添加了一点辍学，因为之前的模型略微过拟合：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'One last approach to masking is to feed the model with ragged tensors.⁠^([9](ch16.html#idm45720175499952))
    In practice, all you need to do is to set `ragged=True` when creating the `TextVectorization`
    layer, so that the input sequences are represented as ragged tensors:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 遮罩的最后一种方法是使用不规则张量来向模型提供输入。实际上，您只需在创建`TextVectorization`层时设置`ragged=True`，以便将输入序列表示为不规则张量：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Compare this ragged tensor representation with the regular tensor representation,
    which uses padding tokens:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将这种不规则张量表示与使用填充标记的常规张量表示进行比较：
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Keras’s recurrent layers have built-in support for ragged tensors, so there’s
    nothing else you need to do: just use this `TextVectorization` layer in your model.
    There’s no need to pass `mask_zero=True` or handle masks explicitly—it’s all implemented
    for you. That’s convenient! However, as of early 2022, the support for ragged
    tensors in Keras is still fairly recent, so there are a few rough edges. For example,
    it is currently not possible to use ragged tensors as targets when running on
    the GPU (but this may be resolved by the time you read these lines).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的循环层内置支持不规则张量，因此您无需执行其他操作：只需在模型中使用此`TextVectorization`层。无需传递`mask_zero=True`或显式处理遮罩——这一切都已为您实现。这很方便！但是，截至2022年初，Keras中对不规则张量的支持仍然相对较新，因此存在一些问题。例如，目前无法在GPU上运行时将不规则张量用作目标（但在您阅读这些内容时可能已解决）。
- en: 'Whichever masking approach you prefer, after training this model for a few
    epochs, it will become quite good at judging whether a review is positive or not.
    If you use the `tf.keras.callbacks.TensorBoard()` callback, you can visualize
    the embeddings in TensorBoard as they are being learned: it is fascinating to
    see words like “awesome” and “amazing” gradually cluster on one side of the embedding
    space, while words like “awful” and “terrible” cluster on the other side. Some
    words are not as positive as you might expect (at least with this model), such
    as the word “good”, presumably because many negative reviews contain the phrase
    “not good”.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您喜欢哪种遮罩方法，在训练此模型几个时期后，它将变得非常擅长判断评论是积极的还是消极的。如果使用`tf.keras.callbacks.TensorBoard()`回调，您可以在TensorBoard中可视化嵌入，看到诸如“棒极了”和“惊人”的词逐渐聚集在嵌入空间的一侧，而诸如“糟糕”和“可怕”的词聚集在另一侧。有些词并不像您可能期望的那样积极（至少在这个模型中），比如“好”这个词，可能是因为许多负面评论包含短语“不好”。
- en: Reusing Pretrained Embeddings and Language Models
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重用预训练的嵌入和语言模型
- en: 'It’s impressive that the model is able to learn useful word embeddings based
    on just 25,000 movie reviews. Imagine how good the embeddings would be if we had
    billions of reviews to train on! Unfortunately, we don’t, but perhaps we can reuse
    word embeddings trained on some other (very) large text corpus (e.g., Amazon reviews,
    available on TensorFlow Datasets), even if it is not composed of movie reviews?
    After all, the word “amazing” generally has the same meaning whether you use it
    to talk about movies or anything else. Moreover, perhaps embeddings would be useful
    for sentiment analysis even if they were trained on another task: since words
    like “awesome” and “amazing” have a similar meaning, they will likely cluster
    in the embedding space even for tasks such as predicting the next word in a sentence.
    If all positive words and all negative words form clusters, then this will be
    helpful for sentiment analysis. So, instead of training word embeddings, we could
    just download and use pretrained embeddings, such as Google’s [Word2vec embeddings](https://homl.info/word2vec),
    Stanford’s [GloVe embeddings](https://homl.info/glove), or Facebook’s [FastText
    embeddings](https://fasttext.cc).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 令人印象深刻的是，这个模型能够基于仅有25,000条电影评论学习到有用的词嵌入。想象一下，如果我们有数十亿条评论来训练，这些嵌入会有多好！不幸的是，我们没有，但也许我们可以重用在其他（非常）大型文本语料库上训练的词嵌入（例如，亚马逊评论，可在TensorFlow数据集上找到）？毕竟，“amazing”这个词无论是用来谈论电影还是其他事物，通常都有相同的含义。此外，也许即使它们是在另一个任务上训练的，嵌入也对情感分析有用：因为“awesome”和“amazing”这样的词有相似的含义，它们很可能会在嵌入空间中聚集，即使是用于预测句子中的下一个词这样的任务。如果所有积极词和所有消极词形成簇，那么这对情感分析将是有帮助的。因此，我们可以不训练词嵌入，而是下载并使用预训练的嵌入，例如谷歌的[Word2vec嵌入](https://homl.info/word2vec)，斯坦福的[GloVe嵌入](https://homl.info/glove)，或Facebook的[FastText嵌入](https://fasttext.cc)。
- en: 'Using pretrained word embeddings was popular for several years, but this approach
    has its limits. In particular, a word has a single representation, no matter the
    context. For example, the word “right” is encoded the same way in “left and right”
    and “right and wrong”, even though it means two very different things. To address
    this limitation, a [2018 paper](https://homl.info/elmo)⁠^([10](ch16.html#idm45720175311248))
    by Matthew Peters introduced *Embeddings from Language Models* (ELMo): these are
    contextualized word embeddings learned from the internal states of a deep bidirectional
    language model. Instead of just using pretrained embeddings in your model, you
    reuse part of a pretrained language model.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练词嵌入在几年内很受欢迎，但这种方法有其局限性。特别是，一个词无论上下文如何，都有一个表示。例如，“right”这个词在“left and right”和“right
    and wrong”中以相同的方式编码，尽管它们表示两个非常不同的含义。为了解决这个限制，Matthew Peters在2018年引入了*来自语言模型的嵌入*（ELMo）：这些是从深度双向语言模型的内部状态中学习到的上下文化词嵌入。与仅在模型中使用预训练嵌入不同，您可以重用预训练语言模型的一部分。
- en: 'At roughly the same time, the [Universal Language Model Fine-Tuning (ULMFiT)
    paper](https://homl.info/ulmfit)⁠^([11](ch16.html#idm45720175307104)) by Jeremy
    Howard and Sebastian Ruder demonstrated the effectiveness of unsupervised pretraining
    for NLP tasks: the authors trained an LSTM language model on a huge text corpus
    using self-supervised learning (i.e., generating the labels automatically from
    the data), then they fine-tuned it on various tasks. Their model outperformed
    the state of the art on six text classification tasks by a large margin (reducing
    the error rate by 18–24% in most cases). Moreover, the authors showed a pretrained
    model fine-tuned on just 100 labeled examples could achieve the same performance
    as one trained from scratch on 10,000 examples. Before the ULMFiT paper, using
    pretrained models was only the norm in computer vision; in the context of NLP,
    pretraining was limited to word embeddings. This paper marked the beginning of
    a new era in NLP: today, reusing pretrained language models is the norm.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在同一时间，Jeremy Howard和Sebastian Ruder的[通用语言模型微调（ULMFiT）论文](https://homl.info/ulmfit)展示了无监督预训练在NLP任务中的有效性：作者们使用自监督学习（即从数据自动生成标签）在庞大的文本语料库上训练了一个LSTM语言模型，然后在各种任务上进行微调。他们的模型在六个文本分类任务上表现优异（在大多数情况下将错误率降低了18-24%）。此外，作者们展示了一个仅使用100个标记示例进行微调的预训练模型可以达到与从头开始训练10,000个示例相同的性能。在ULMFiT论文之前，使用预训练模型只是计算机视觉中的常态；在NLP领域，预训练仅限于词嵌入。这篇论文标志着NLP的一个新时代的开始：如今，重用预训练语言模型已成为常态。
- en: 'For example, let’s build a classifier based on the Universal Sentence Encoder,
    a model architecture introduced in a [2018 paper](https://homl.info/139)⁠^([12](ch16.html#idm45720175301568))
    by a team of Google researchers. This model is based on the transformer architecture,
    which we will look at later in this chapter. Conveniently, the model is available
    on TensorFlow Hub:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们基于通用句子编码器构建一个分类器，这是由谷歌研究人员团队在2018年介绍的模型架构。这个模型基于transformer架构，我们将在本章后面讨论。方便的是，这个模型可以在TensorFlow
    Hub上找到。
- en: '[PRE24]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Tip
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'This model is quite large—close to 1 GB in size—so it may take a while to download.
    By default, TensorFlow Hub modules are saved to a temporary directory, and they
    get downloaded again and again every time you run your program. To avoid that,
    you must set the `TFHUB_CACHE_DIR` environment variable to a directory of your
    choice: the modules will then be saved there, and only downloaded once.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型非常庞大，接近1GB大小，因此下载可能需要一些时间。默认情况下，TensorFlow Hub模块保存在临时目录中，并且每次运行程序时都会重新下载。为了避免这种情况，您必须将`TFHUB_CACHE_DIR`环境变量设置为您选择的目录：模块将保存在那里，只会下载一次。
- en: Note that the last part of the TensorFlow Hub module URL specifies that we want
    version 4 of the model. This versioning ensures that if a new module version is
    released on TF Hub, it will not break our model. Conveniently, if you just enter
    this URL in a web browser, you will get the documentation for this module.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Also note that we set `trainable=True` when creating the `hub.KerasLayer`.
    This way, the pretrained Universal Sentence Encoder is fine-tuned during training:
    some of its weights are tweaked via backprop. Not all TensorFlow Hub modules are
    fine-tunable, so make sure to check the documentation for each pretrained module
    you’re interested in.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'After training, this model should reach a validation accuracy of over 90%.
    That’s actually really good: if you try to perform the task yourself, you will
    probably do only marginally better since many reviews contain both positive and
    negative comments. Classifying these ambiguous reviews is like flipping a coin.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'So far we have looked at text generation using a char-RNN, and sentiment analysis
    with word-level RNN models (based on trainable embeddings) and using a powerful
    pretrained language model from TensorFlow Hub. In the next section, we will explore
    another important NLP task: *neural machine translation* (NMT).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: An Encoder–Decoder Network for Neural Machine Translation
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s begin with a simple [NMT model](https://homl.info/103)⁠^([13](ch16.html#idm45720175133488))
    that will translate English sentences to Spanish (see [Figure 16-3](#machine_translation_diagram)).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, the architecture is as follows: English sentences are fed as inputs
    to the encoder, and the decoder outputs the Spanish translations. Note that the
    Spanish translations are also used as inputs to the decoder during training, but
    shifted back by one step. In other words, during training the decoder is given
    as input the word that it *should* have output at the previous step, regardless
    of what it actually output. This is called *teacher forcing*—a technique that
    significantly speeds up training and improves the model’s performance. For the
    very first word, the decoder is given the start-of-sequence (SOS) token, and the
    decoder is expected to end the sentence with an end-of-sequence (EOS) token.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Each word is initially represented by its ID (e.g., `854` for the word “soccer”).
    Next, an `Embedding` layer returns the word embedding. These word embeddings are
    then fed to the encoder and the decoder.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: At each step, the decoder outputs a score for each word in the output vocabulary
    (i.e., Spanish), then the softmax activation function turns these scores into
    probabilities. For example, at the first step the word “Me” may have a probability
    of 7%, “Yo” may have a probability of 1%, and so on. The word with the highest
    probability is output. This is very much like a regular classification task, and
    indeed you can train the model using the `"sparse_categorical_crossentropy"` loss,
    much like we did in the char-RNN model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1603](assets/mls3_1603.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: Figure 16-3\. A simple machine translation model
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that at inference time (after training), you will not have the target sentence
    to feed to the decoder. Instead, you need to feed it the word that it has just
    output at the previous step, as shown in [Figure 16-4](#inference_decoder_diagram)
    (this will require an embedding lookup that is not shown in the diagram).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a [2015 paper](https://homl.info/scheduledsampling),⁠^([14](ch16.html#idm45720175116736))
    Samy Bengio et al. proposed gradually switching from feeding the decoder the previous
    *target* token to feeding it the previous *output* token during training.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1604](assets/mls3_1604.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: Figure 16-4\. At inference time, the decoder is fed as input the word it just
    output at the previous time step
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s build and train this model! First, we need to download a dataset of English/Spanish
    sentence pairs:⁠^([15](ch16.html#idm45720175112096))
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Each line contains an English sentence and the corresponding Spanish translation,
    separated by a tab. We’ll start by removing the Spanish characters “¡” and “¿”,
    which the `TextVectorization` layer doesn’t handle, then we will parse the sentence
    pairs and shuffle them. Finally, we will split them into two separate lists, one
    per language:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 每行包含一个英语句子和相应的西班牙语翻译，用制表符分隔。我们将从删除西班牙字符“¡”和“¿”开始，`TextVectorization`层无法处理这些字符，然后我们将解析句子对并对它们进行洗牌。最后，我们将它们分成两个单独的列表，每种语言一个：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s take a look at the first three sentence pairs:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下前三个句子对：
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, let’s create two `TextVectorization` layers—one per language—and adapt
    them to the text:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建两个`TextVectorization`层——每种语言一个，并对文本进行调整：
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'There are a few things to note here:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几件事需要注意：
- en: We limit the vocabulary size to 1,000, which is quite small. That’s because
    the training set is not very large, and because using a small value will speed
    up training. State-of-the-art translation models typically use a much larger vocabulary
    (e.g., 30,000), a much larger training set (gigabytes), and a much larger model
    (hundreds or even thousands of megabytes). For example, check out the Opus-MT
    models by the University of Helsinki, or the M2M-100 model by Facebook.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将词汇表大小限制为1,000，这相当小。这是因为训练集不是很大，并且使用较小的值将加快训练速度。最先进的翻译模型通常使用更大的词汇表（例如30,000），更大的训练集（几千兆字节）和更大的模型（数百甚至数千兆字节）。例如，查看赫尔辛基大学的Opus-MT模型，或Facebook的M2M-100模型。
- en: 'Since all sentences in the dataset have a maximum of 50 words, we set `output_sequence_length`
    to 50: this way the input sequences will automatically be padded with zeros until
    they are all 50 tokens long. If there was any sentence longer than 50 tokens in
    the training set, it would be cropped to 50 tokens.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集中的所有句子最多有50个单词，我们将`output_sequence_length`设置为50：这样输入序列将自动填充为零，直到它们都是50个标记长。如果训练集中有任何超过50个标记的句子，它将被裁剪为50个标记。
- en: 'For the Spanish text, we add “startofseq” and “endofseq” to each sentence when
    adapting the `TextVectorization` layer: we will use these words as SOS and EOS
    tokens. You could use any other words, as long as they are not actual Spanish
    words.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于西班牙文本，我们在调整`TextVectorization`层时为每个句子添加“startofseq”和“endofseq”：我们将使用这些词作为SOS和EOS标记。您可以使用任何其他单词，只要它们不是实际的西班牙单词。
- en: 'Let’s inspect the first 10 tokens in both vocabularies. They start with the
    padding token, the unknown token, the SOS and EOS tokens (only in the Spanish
    vocabulary), then the actual words, sorted by decreasing frequency:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查两种词汇表中的前10个标记。它们以填充标记、未知标记、SOS和EOS标记（仅在西班牙语词汇表中）、然后按频率递减排序的实际单词开始：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, let’s create the training set and the validation set (you could also
    create a test set if you needed it). We will use the first 100,000 sentence pairs
    for training, and the rest for validation. The decoder’s inputs are the Spanish
    sentences plus an SOS token prefix. The targets are the Spanish sentences plus
    an EOS suffix:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建训练集和验证集（如果需要，您也可以创建一个测试集）。我们将使用前100,000个句子对进行训练，其余用于验证。解码器的输入是西班牙语句子加上一个SOS标记前缀。目标是西班牙语句子加上一个EOS后缀：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'OK, we’re now ready to build our translation model. We will use the functional
    API for that since the model is not sequential. It requires two text inputs—one
    for the encoder and one for the decoder—so let’s start with that:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们准备构建我们的翻译模型。我们将使用功能API，因为模型不是顺序的。它需要两个文本输入——一个用于编码器，一个用于解码器——所以让我们从这里开始：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we need to encode these sentences using the `TextVectorization` layers
    we prepared earlier, followed by an `Embedding` layer for each language, with
    `mask_zero=True` to ensure masking is handled automatically. The embedding size
    is a hyperparameter you can tune, as always:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用我们之前准备的`TextVectorization`层对这些句子进行编码，然后为每种语言使用一个`Embedding`层，其中`mask_zero=True`以确保自动处理掩码。嵌入大小是一个您可以调整的超参数，像往常一样：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Tip
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When the languages share many words, you may get better performance using the
    same embedding layer for both the encoder and the decoder.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当语言共享许多单词时，您可能会获得更好的性能，使用相同的嵌入层用于编码器和解码器。
- en: 'Now let’s create the encoder and pass it the embedded inputs:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建编码器并传递嵌入输入：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To keep things simple, we just used a single `LSTM` layer, but you could stack
    several of them. We also set `return_state=True` to get a reference to the layer’s
    final state. Since we’re using an `LSTM` layer, there are actually two states:
    the short-term state and the long-term state. The layer returns these states separately,
    which is why we had to write `*encoder_state` to group both states in a list.⁠^([16](ch16.html#idm45720174413024))
    Now we can use this (double) state as the initial state of the decoder:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单，我们只使用了一个`LSTM`层，但您可以堆叠几个。我们还设置了`return_state=True`以获得对层最终状态的引用。由于我们使用了一个`LSTM`层，实际上有两个状态：短期状态和长期状态。该层分别返回这些状态，这就是为什么我们必须写`*encoder_state`来将两个状态分组在一个列表中。现在我们可以使用这个（双重）状态作为解码器的初始状态：
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we can pass the decoder’s outputs through a `Dense` layer with the softmax
    activation function to get the word probabilities for each step:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过具有softmax激活函数的`Dense`层将解码器的输出传递，以获得每个步骤的单词概率：
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And that’s it! We just need to create the Keras `Model`, compile it, and train
    it:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们只需要创建Keras`Model`，编译它并训练它：
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After training, we can use the model to translate new English sentences to
    Spanish. But it’s not as simple as calling `model.predict()`, because the decoder
    expects as input the word that was predicted at the previous time step. One way
    to do this is to write a custom memory cell that keeps track of the previous output
    and feeds it to the encoder at the next time step. However, to keep things simple,
    we can just call the model multiple times, predicting one extra word at each round.
    Let’s write a little utility function for that:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The function simply keeps predicting one word at a time, gradually completing
    the translation, and it stops once it reaches the EOS token. Let’s give it a try!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Hurray, it works! Well, at least it does with very short sentences. If you
    try playing with this model for a while, you will find that it’s not bilingual
    yet, and in particular it really struggles with longer sentences. For example:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The translation says “I like soccer and sometimes even the bus”. So how can
    you improve it? One way is to increase the training set size and add more `LSTM`
    layers in both the encoder and the decoder. But this will only get you so far,
    so let’s look at more sophisticated techniques, starting with bidirectional recurrent
    layers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional RNNs
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At each time step, a regular recurrent layer only looks at past and present
    inputs before generating its output. In other words, it is *causal*, meaning it
    cannot look into the future. This type of RNN makes sense when forecasting time
    series, or in the decoder of a sequence-to-sequence (seq2seq) model. But for tasks
    like text classification, or in the encoder of a seq2seq model, it is often preferable
    to look ahead at the next words before encoding a given word.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the phrases “the right arm”, “the right person”, and
    “the right to criticize”: to properly encode the word “right”, you need to look
    ahead. One solution is to run two recurrent layers on the same inputs, one reading
    the words from left to right and the other reading them from right to left, then
    combine their outputs at each time step, typically by concatenating them. This
    is what a *bidirectional recurrent layer* does (see [Figure 16-5](#bidirectional_rnn_diagram)).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1605](assets/mls3_1605.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 16-5\. A bidirectional recurrent layer
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To implement a bidirectional recurrent layer in Keras, just wrap a recurrent
    layer in a `tf.keras.layers.Bidirectional` layer. For example, the following `Bidirectional`
    layer could be used as the encoder in our translation model:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `Bidirectional` layer will create a clone of the `GRU` layer (but in the
    reverse direction), and it will run both and concatenate their outputs. So although
    the `GRU` layer has 10 units, the `Bidirectional` layer will output 20 values
    per time step.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s just one problem. This layer will now return four states instead of
    two: the final short-term and long-term states of the forward `LSTM` layer, and
    the final short-term and long-term states of the backward `LSTM` layer. We cannot
    use this quadruple state directly as the initial state of the decoder’s `LSTM`
    layer, since it expects just two states (short-term and long-term). We cannot
    make the decoder bidirectional, since it must remain causal: otherwise it would
    cheat during training and it would not work. Instead, we can concatenate the two
    short-term states, and also concatenate the two long-term states:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now let’s look at another popular technique that can greatly improve the performance
    of a translation model at inference time: beam search.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Beam Search
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose you have trained an encoder–decoder model, and you use it to translate
    the sentence “I like soccer” to Spanish. You are hoping that it will output the
    proper translation “me gusta el fútbol”, but unfortunately it outputs “me gustan
    los jugadores”, which means “I like the players”. Looking at the training set,
    you notice many sentences such as “I like cars”, which translates to “me gustan
    los autos”, so it wasn’t absurd for the model to output “me gustan los” after
    seeing “I like”. Unfortunately, in this case it was a mistake since “soccer” is
    singular. The model could not go back and fix it, so it tried to complete the
    sentence as best it could, in this case using the word “jugadores”. How can we
    give the model a chance to go back and fix mistakes it made earlier? One of the
    most common solutions is *beam search*: it keeps track of a short list of the
    *k* most promising sentences (say, the top three), and at each decoder step it
    tries to extend them by one word, keeping only the *k* most likely sentences.
    The parameter *k* is called the *beam width*.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose you use the model to translate the sentence “I like soccer”
    using beam search with a beam width of 3 (see [Figure 16-6](#beam_search_diagram)).
    At the first decoder step, the model will output an estimated probability for
    each possible first word in the translated sentence. Suppose the top three words
    are “me” (75% estimated probability), “a” (3%), and “como” (1%). That’s our short
    list so far. Next, we use the model to find the next word for each sentence. For
    the first sentence (“me”), perhaps the model outputs a probability of 36% for
    the word “gustan”, 32% for the word “gusta”, 16% for the word “encanta”, and so
    on. Note that these are actually *conditional* probabilities, given that the sentence
    starts with “me”. For the second sentence (“a”), the model might output a conditional
    probability of 50% for the word “mi”, and so on. Assuming the vocabulary has 1,000
    words, we will end up with 1,000 probabilities per sentence.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the probabilities of each of the 3,000 two-word sentences
    we considered (3 × 1,000). We do this by multiplying the estimated conditional
    probability of each word by the estimated probability of the sentence it completes.
    For example, the estimated probability of the sentence “me” was 75%, while the
    estimated conditional probability of the word “gustan” (given that the first word
    is “me”) was 36%, so the estimated probability of the sentence “me gustan” is
    75% × 36% = 27%. After computing the probabilities of all 3,000 two-word sentences,
    we keep only the top 3\. In this example they all start with the word “me”: “me
    gustan” (27%), “me gusta” (24%), and “me encanta” (12%). Right now, the sentence
    “me gustan” is winning, but “me gusta” has not been eliminated.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1606](assets/mls3_1606.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Figure 16-6\. Beam search, with a beam width of 3
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Then we repeat the same process: we use the model to predict the next word
    in each of these three sentences, and we compute the probabilities of all 3,000
    three-word sentences we considered. Perhaps the top three are now “me gustan los”
    (10%), “me gusta el” (8%), and “me gusta mucho” (2%). At the next step we may
    get “me gusta el fútbol” (6%), “me gusta mucho el” (1%), and “me gusta el deporte”
    (0.2%). Notice that “me gustan” was eliminated, and the correct translation is
    now ahead. We boosted our encoder–decoder model’s performance without any extra
    training, simply by using it more wisely.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The TensorFlow Addons library includes a full seq2seq API that lets you build
    encoder–decoder models with attention, including beam search, and more. However,
    its documentation is currently very limited. Implementing beam search is a good
    exercise, so give it a try! Check out this chapter’s notebook for a possible solution.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: With all this, you can get reasonably good translations for fairly short sentences.
    Unfortunately, this model will be really bad at translating long sentences. Once
    again, the problem comes from the limited short-term memory of RNNs. *Attention
    mechanisms* are the game-changing innovation that addressed this problem.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一切，您可以为相当短的句子获得相当不错的翻译。不幸的是，这种模型在翻译长句子时会表现得非常糟糕。问题再次出在RNN的有限短期记忆上。*注意力机制*是解决这个问题的划时代创新。
- en: Attention Mechanisms
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制
- en: 'Consider the path from the word “soccer” to its translation “fútbol” back in
    [Figure 16-3](#machine_translation_diagram): it is quite long! This means that
    a representation of this word (along with all the other words) needs to be carried
    over many steps before it is actually used. Can’t we make this path shorter?'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下从单词“soccer”到其翻译“fútbol”的路径，回到[图16-3](#machine_translation_diagram)：这是相当长的！这意味着这个单词的表示（以及所有其他单词）需要在实际使用之前经过许多步骤。我们难道不能让这条路径变短一点吗？
- en: This was the core idea in a landmark [2014 paper](https://homl.info/attention)⁠^([18](ch16.html#idm45720173841328))
    by Dzmitry Bahdanau et al., where the authors introduced a technique that allowed
    the decoder to focus on the appropriate words (as encoded by the encoder) at each
    time step. For example, at the time step where the decoder needs to output the
    word “fútbol”, it will focus its attention on the word “soccer”. This means that
    the path from an input word to its translation is now much shorter, so the short-term
    memory limitations of RNNs have much less impact. Attention mechanisms revolutionized
    neural machine translation (and deep learning in general), allowing a significant
    improvement in the state of the art, especially for long sentences (e.g., over
    30 words).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Dzmitry Bahdanau等人在一篇具有里程碑意义的[2014年论文](https://homl.info/attention)中的核心思想，作者在其中介绍了一种技术，允许解码器在每个时间步关注适当的单词（由编码器编码）。例如，在解码器需要输出单词“fútbol”的时间步，它将把注意力集中在单词“soccer”上。这意味着从输入单词到其翻译的路径现在要短得多，因此RNN的短期记忆限制对其影响要小得多。注意机制彻底改变了神经机器翻译（以及深度学习一般）的方式，显著改进了技术水平，特别是对于长句子（例如，超过30个单词）。
- en: Note
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The most common metric used in NMT is the *bilingual evaluation understudy*
    (BLEU) score, which compares each translation produced by the model with several
    good translations produced by humans: it counts the number of *n*-grams (sequences
    of *n* words) that appear in any of the target translations and adjusts the score
    to take into account the frequency of the produced *n*-grams in the target translations.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: NMT中最常用的度量标准是*双语评估助手*（BLEU）分数，它将模型产生的每个翻译与人类产生的几个好翻译进行比较：它计算出现在任何目标翻译中的*n*-gram（*n*个单词序列）的数量，并调整分数以考虑在目标翻译中产生的*n*-gram的频率。
- en: '[Figure 16-7](#attention_diagram) shows our encoder–decoder model with an added
    attention mechanism. On the left, you have the encoder and the decoder. Instead
    of just sending the encoder’s final hidden state to the decoder, as well as the
    previous target word at each step (which is still done, although it is not shown
    in the figure), we now send all of the encoder’s outputs to the decoder as well.
    Since the decoder cannot deal with all these encoder outputs at once, they need
    to be aggregated: at each time step, the decoder’s memory cell computes a weighted
    sum of all the encoder outputs. This determines which words it will focus on at
    this step. The weight *α*[(*t*,*i*)] is the weight of the *i*^(th) encoder output
    at the *t*^(th) decoder time step. For example, if the weight *α*[(3,2)] is much
    larger than the weights *α*[(3,0)] and *α*[(3,1)], then the decoder will pay much
    more attention to the encoder’s output for word #2 (“soccer”) than to the other
    two outputs, at least at this time step. The rest of the decoder works just like
    earlier: at each time step the memory cell receives the inputs we just discussed,
    plus the hidden state from the previous time step, and finally (although it is
    not represented in the diagram) it receives the target word from the previous
    time step (or at inference time, the output from the previous time step).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-7](#attention_diagram)展示了我们带有注意力机制的编码器-解码器模型。在左侧，您可以看到编码器和解码器。现在，我们不仅在每一步将编码器的最终隐藏状态和前一个目标单词发送给解码器（尽管这仍在进行，但在图中没有显示），还将所有编码器的输出发送给解码器。由于解码器无法一次处理所有这些编码器的输出，因此它们需要被聚合：在每个时间步，解码器的记忆单元计算所有编码器输出的加权和。这决定了它在这一步将关注哪些单词。权重*α*[(*t*,*i*)]是第*t*个解码器时间步的第*i*个编码器输出的权重。例如，如果权重*α*[(3,2)]远大于权重*α*[(3,0)]和*α*[(3,1)]，那么解码器将在这个时间步更多地关注第2个单词（“soccer”）的编码器输出，而不是其他两个输出。解码器的其余部分与之前的工作方式相同：在每个时间步，记忆单元接收我们刚刚讨论的输入，以及来自上一个时间步的隐藏状态，最后（尽管在图中没有表示）它接收来自上一个时间步的目标单词（或在推断时，来自上一个时间步的输出）。'
- en: '![mls3 1607](assets/mls3_1607.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1607](assets/mls3_1607.png)'
- en: Figure 16-7\. Neural machine translation using an encoder–decoder network with
    an attention model
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-7。使用带有注意力模型的编码器-解码器网络的神经机器翻译
- en: 'But where do these *α*[(*t*,*i*)] weights come from? Well, they are generated
    by a small neural network called an *alignment model* (or an *attention layer*),
    which is trained jointly with the rest of the encoder–decoder model. This alignment
    model is illustrated on the righthand side of [Figure 16-7](#attention_diagram).
    It starts with a `Dense` layer composed of a single neuron that processes each
    of the encoder’s outputs, along with the decoder’s previous hidden state (e.g.,
    **h**[(2)]). This layer outputs a score (or energy) for each encoder output (e.g.,
    *e*[(3,] [2)]): this score measures how well each output is aligned with the decoder’s
    previous hidden state. For example, in [Figure 16-7](#attention_diagram), the
    model has already output “me gusta el” (meaning “I like”), so it’s now expecting
    a noun: the word “soccer” is the one that best aligns with the current state,
    so it gets a high score. Finally, all the scores go through a softmax layer to
    get a final weight for each encoder output (e.g., *α*[(3,2)]). All the weights
    for a given decoder time step add up to 1\. This particular attention mechanism
    is called *Bahdanau attention* (named after the 2014 paper’s first author). Since
    it concatenates the encoder output with the decoder’s previous hidden state, it
    is sometimes called *concatenative attention* (or *additive attention*).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the input sentence is *n* words long, and assuming the output sentence is
    about as long, then this model will need to compute about *n*² weights. Fortunately,
    this quadratic computational complexity is still tractable because even long sentences
    don’t have thousands of words.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Another common attention mechanism, known as *Luong attention* or *multiplicative
    attention*, was proposed shortly after, in [2015](https://homl.info/luongattention),⁠^([19](ch16.html#idm45720173812256))
    by Minh-Thang Luong et al. Because the goal of the alignment model is to measure
    the similarity between one of the encoder’s outputs and the decoder’s previous
    hidden state, the authors proposed to simply compute the dot product (see [Chapter 4](ch04.html#linear_models_chapter))
    of these two vectors, as this is often a fairly good similarity measure, and modern
    hardware can compute it very efficiently. For this to be possible, both vectors
    must have the same dimensionality. The dot product gives a score, and all the
    scores (at a given decoder time step) go through a softmax layer to give the final
    weights, just like in Bahdanau attention. Another simplification Luong et al.
    proposed was to use the decoder’s hidden state at the current time step rather
    than at the previous time step (i.e., **h**[(*t*)] rather than **h**[(*t*–1)]),
    then to use the output of the attention mechanism (noted <math><msub><mover><mi
    mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></math>)
    directly to compute the decoder’s predictions, rather than using it to compute
    the decoder’s current hidden state. The researchers also proposed a variant of
    the dot product mechanism where the encoder outputs first go through a fully connected
    layer (without a bias term) before the dot products are computed. This is called
    the “general” dot product approach. The researchers compared both dot product
    approaches with the concatenative attention mechanism (adding a rescaling parameter
    vector **v**), and they observed that the dot product variants performed better
    than concatenative attention. For this reason, concatenative attention is much
    less used now. The equations for these three attention mechanisms are summarized
    in [Equation 16-1](#attention_mechanisms_equation).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Equation 16-1\. Attention mechanisms
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><msub><mover><mi mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mtext> with </mtext><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub></mfenced></mrow><mrow><mstyle
    displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>'</mo></mrow></munder></mstyle><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>'</mo><mo>)</mo></mrow></msub></mfenced></mrow></mfrac><mtext> and </mtext><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfenced
    open="{" close=""><mtable columnalign="left"><mtr><mtd><msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>d</mi><mi>o</mi><mi>t</mi></mtd></mtr><mtr><mtd><msup><msub><mi
    mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><mi
    mathvariant="bold">W</mi><msub><mi mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>l</mi></mtd></mtr><mtr><mtd><msup><mi
    mathvariant="bold">v</mi><mo>⊺</mo></msup><mi>tanh</mi><mo>(</mo><mi mathvariant="bold">W</mi><mo>[</mo><msub><mi
    mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>;</mo><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>]</mo><mo>)</mo></mtd><mtd><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi></mtd></mtr></mtable></mfenced></math>
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras provides a `tf.keras.layers.Attention` layer for Luong attention, and
    an `AdditiveAttention` layer for Bahdanau attention. Let’s add Luong attention
    to our encoder–decoder model. Since we will need to pass all the encoder’s outputs
    to the `Attention` layer, we first need to set `return_sequences=True` when creating
    the encoder:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we need to create the attention layer and pass it the decoder’s states
    and the encoder’s outputs. However, to access the decoder’s states at each step
    we would need to write a custom memory cell. For simplicity, let’s use the decoder’s
    outputs instead of its states: in practice this works well too, and it’s much
    easier to code. Then we just pass the attention layer’s outputs directly to the
    output layer, as suggested in the Luong attention paper:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'And that’s it! If you train this model, you will find that it now handles much
    longer sentences. For example:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In short, the attention layer provides a way to focus the attention of the
    model on part of the inputs. But there’s another way to think of this layer: it
    acts as a differentiable memory retrieval mechanism.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s suppose the encoder analyzed the input sentence “I like
    soccer”, and it managed to understand that the word “I” is the subject and the
    word “like” is the verb, so it encoded this information in its outputs for these
    words. Now suppose the decoder has already translated the subject, and it thinks
    that it should translate the verb next. For this, it needs to fetch the verb from
    the input sentence. This is analogous to a dictionary lookup: it’s as if the encoder
    had created a dictionary {"subject”: “They”, “verb”: “played”, …​} and the decoder
    wanted to look up the value that corresponds to the key “verb”.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: However, the model does not have discrete tokens to represent the keys (like
    “subject” or “verb”); instead, it has vectorized representations of these concepts
    that it learned during training, so the query it will use for the lookup will
    not perfectly match any key in the dictionary. The solution is to compute a similarity
    measure between the query and each key in the dictionary, and then use the softmax
    function to convert these similarity scores to weights that add up to 1\. As we
    saw earlier, that’s exactly what the attention layer does. If the key that represents
    the verb is by far the most similar to the query, then that key’s weight will
    be close to 1.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the attention layer computes a weighted sum of the corresponding values:
    if the weight of the “verb” key is close to 1, then the weighted sum will be very
    close to the representation of the word “played”.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why the Keras `Attention` and `AdditiveAttention` layers both expect
    a list as input, containing two or three items: the *queries*, the *keys*, and
    optionally the *values*. If you do not pass any values, then they are automatically
    equal to the keys. So, looking at the previous code example again, the decoder
    outputs are the queries, and the encoder outputs are both the keys and the values.
    For each decoder output (i.e., each query), the attention layer returns a weighted
    sum of the encoder outputs (i.e., the keys/values) that are most similar to the
    decoder output.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is that an attention mechanism is a trainable memory retrieval
    system. It is so powerful that you can actually build state-of-the-art models
    using only attention mechanisms. Enter the transformer architecture.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention Is All You Need: The Original Transformer Architecture'
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a groundbreaking [2017 paper](https://homl.info/transformer),⁠^([20](ch16.html#idm45720173607840))
    a team of Google researchers suggested that “Attention Is All You Need”. They
    created an architecture called the *transformer*, which significantly improved
    the state-of-the-art in NMT without using any recurrent or convolutional layers,⁠^([21](ch16.html#idm45720173604064))
    just attention mechanisms (plus embedding layers, dense layers, normalization
    layers, and a few other bits and pieces). Because the model is not recurrent,
    it doesn’t suffer as much from the vanishing or exploding gradients problems as
    RNNs, it can be trained in fewer steps, it’s easier to parallelize across multiple
    GPUs, and it can better capture long-range patterns than RNNs. The original 2017
    transformer architecture is represented in [Figure 16-8](#transformer_diagram).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: In short, the left part of [Figure 16-8](#transformer_diagram) is the encoder,
    and the right part is the decoder. Each embedding layer outputs a 3D tensor of
    shape [*batch size*, *sequence length*, *embedding size*]. After that, the tensors
    are gradually transformed as they flow through the transformer, but their shape
    remains the same.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1608](assets/mls3_1608.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: Figure 16-8\. The original 2017 transformer architecture⁠^([22](ch16.html#idm45720173579408))
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you use the transformer for NMT, then during training you must feed the English
    sentences to the encoder and the corresponding Spanish translations to the decoder,
    with an extra SOS token inserted at the start of each sentence. At inference time,
    you must call the transformer multiple times, producing the translations one word
    at a time and feeding the partial translations to the decoder at each round, just
    like we did earlier in the `translate()` function.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder’s role is to gradually transform the inputs—word representations
    of the English sentence—until each word’s representation perfectly captures the
    meaning of the word, in the context of the sentence. For example, if you feed
    the encoder with the sentence “I like soccer”, then the word “like” will start
    off with a rather vague representation, since this word could mean different things
    in different contexts: think of “I like soccer” versus “It’s like that”. But after
    going through the encoder, the word’s representation should capture the correct
    meaning of “like” in the given sentence (i.e., to be fond of), as well as any
    other information that may be required for translation (e.g., it’s a verb).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的作用是逐渐转换输入——英文句子的单词表示——直到每个单词的表示完美地捕捉到单词的含义，在句子的上下文中。例如，如果你用句子“I like soccer”来喂给编码器，那么单词“like”将以一个相当模糊的表示开始，因为这个单词在不同的上下文中可能有不同的含义：想想“I
    like soccer”和“It’s like that”。但是经过编码器后，单词的表示应该捕捉到给定句子中“like”的正确含义（即喜欢），以及可能需要用于翻译的任何其他信息（例如，它是一个动词）。
- en: The decoder’s role is to gradually transform each word representation in the
    translated sentence into a word representation of the next word in the translation.
    For example, if the sentence to translate is “I like soccer”, and the decoder’s
    input sentence is “<SOS> me gusta el fútbol”, then after going through the decoder,
    the word representation of the word “el” will end up transformed into a representation
    of the word “fútbol”. Similarly, the representation of the word “fútbol” will
    be transformed into a representation of the EOS token.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的作用是逐渐将翻译句子中的每个单词表示转换为翻译中下一个单词的单词表示。例如，如果要翻译的句子是“I like soccer”，解码器的输入句子是“<SOS>
    me gusta el fútbol”，那么经过解码器后，“el”的单词表示将最终转换为“fútbol”的表示。同样，“fútbol”的表示将被转换为EOS标记的表示。
- en: After going through the decoder, each word representation goes through a final
    `Dense` layer with a softmax activation function, which will hopefully output
    a high probability for the correct next word and a low probability for all other
    words. The predicted sentence should be “me gusta el fútbol <EOS>”.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 经过解码器后，每个单词表示都经过一个带有softmax激活函数的最终`Dense`层，希望能够输出正确下一个单词的高概率和所有其他单词的低概率。预测的句子应该是“me
    gusta el fútbol <EOS>”。
- en: 'That was the big picture; now let’s walk through [Figure 16-8](#transformer_diagram)
    in more detail:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 那是大局观；现在让我们更详细地走一遍[图16-8](#transformer_diagram)：
- en: First, notice that both the encoder and the decoder contain modules that are
    stacked *N* times. In the paper, *N* = 6\. The final outputs of the whole encoder
    stack are fed to the decoder at each of these *N* levels.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，注意到编码器和解码器都包含被堆叠*N*次的模块。在论文中，*N* = 6。整个编码器堆栈的最终输出在每个这些*N*级别上被馈送到解码器。
- en: 'Zooming in, you can see that you are already familiar with most components:
    there are two embedding layers; several skip connections, each of them followed
    by a layer normalization layer; several feedforward modules that are composed
    of two dense layers each (the first one using the ReLU activation function, the
    second with no activation function); and finally the output layer is a dense layer
    using the softmax activation function. You can also sprinkle a bit of dropout
    after the attention layers and the feedforward modules, if needed. Since all of
    these layers are time-distributed, each word is treated independently from all
    the others. But how can we translate a sentence by looking at the words completely
    separately? Well, we can’t, so that’s where the new components come in:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 放大一下，你会发现你已经熟悉大部分组件：有两个嵌入层；几个跳跃连接，每个连接后面跟着一个层归一化层；几个由两个密集层组成的前馈模块（第一个使用ReLU激活函数，第二个没有激活函数）；最后输出层是使用softmax激活函数的密集层。如果需要的话，你也可以在注意力层和前馈模块之后添加一点dropout。由于所有这些层都是时间分布的，每个单词都独立于其他所有单词。但是我们如何通过完全分开地查看单词来翻译句子呢？嗯，我们不能，这就是新组件发挥作用的地方：
- en: The encoder’s *multi-head attention* layer updates each word representation
    by attending to (i.e., paying attention to) all other words in the same sentence.
    That’s where the vague representation of the word “like” becomes a richer and
    more accurate representation, capturing its precise meaning in the given sentence.
    We will discuss exactly how this works shortly.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的*多头注意力*层通过关注同一句子中的所有其他单词来更新每个单词的表示。这就是单词“like”的模糊表示变得更丰富和更准确的表示的地方，捕捉了它在给定句子中的确切含义。我们将很快讨论这是如何工作的。
- en: 'The decoder’s *masked multi-head attention* layer does the same thing, but
    when it processes a word, it doesn’t attend to words located after it: it’s a
    causal layer. For example, when it processes the word “gusta”, it only attends
    to the words “<SOS> me gusta”, and it ignores the words “el fútbol” (or else that
    would be cheating).'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的*掩码多头注意力*层做同样的事情，但当处理一个单词时，它不会关注在它之后的单词：这是一个因果层。例如，当处理单词“gusta”时，它只会关注“<SOS>
    me gusta”这几个单词，而忽略“el fútbol”这几个单词（否则那就是作弊了）。
- en: The decoder’s upper *multi-head attention* layer is where the decoder pays attention
    to the words in the English sentence. This is called *cross*-attention, not *self*-attention
    in this case. For example, the decoder will probably pay close attention to the
    word “soccer” when it processes the word “el” and transforms its representation
    into a representation of the word “fútbol”.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的上层*多头注意力*层是解码器关注英文句子中的单词的地方。这被称为*交叉*注意力，在这种情况下不是*自我*注意力。例如，当解码器处理单词“el”并将其表示转换为“fútbol”的表示时，解码器可能会密切关注单词“soccer”。
- en: 'The *positional encodings* are dense vectors (much like word embeddings) that
    represent the position of each word in the sentence. The *n*^(th) positional encoding
    is added to the word embedding of the *n*^(th) word in each sentence. This is
    needed because all layers in the transformer architecture ignore word positions:
    without positional encodings, you could shuffle the input sequences, and it would
    just shuffle the output sequences in the same way. Obviously, the order of words
    matters, which is why we need to give positional information to the transformer
    somehow: adding positional encodings to the word representations is a good way
    to achieve this.'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置编码*是密集向量（类似于单词嵌入），表示句子中每个单词的位置。第*n*个位置编码被添加到每个句子中第*n*个单词的单词嵌入中。这是因为变压器架构中的所有层都忽略单词位置：没有位置编码，您可以对输入序列进行洗牌，它只会以相同方式洗牌输出序列。显然，单词的顺序很重要，这就是为什么我们需要以某种方式向变压器提供位置信息的原因：将位置编码添加到单词表示是实现这一点的好方法。'
- en: Note
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The first two arrows going into each multi-head attention layer in [Figure 16-8](#transformer_diagram)
    represent the keys and values, and the third arrow represents the queries. In
    the self-attention layers, all three are equal to the word representations output
    by the previous layer, while in the decoder’s upper attention layer, the keys
    and values are equal to the encoder’s final word representations, and the queries
    are equal to the word representations output by the previous layer.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-8](#transformer_diagram)中每个多头注意力层的前两个箭头代表键和值，第三个箭头代表查询。在自注意力层中，所有三个都等于前一层输出的单词表示，而在解码器的上层注意力层中，键和值等于编码器的最终单词表示，查询等于前一层输出的单词表示。'
- en: Let’s go through the novel components of the transformer architecture in more
    detail, starting with the positional encodings.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解变压器架构中的新颖组件，从位置编码开始。
- en: Positional encodings
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'A positional encoding is a dense vector that encodes the position of a word
    within a sentence: the *i*^(th) positional encoding is added to the word embedding
    of the *i*^(th) word in the sentence. The easiest way to implement this is to
    use an `Embedding` layer and make it encode all the positions from 0 to the maximum
    sequence length in the batch, then add the result to the word embeddings. The
    rules of broadcasting will ensure that the positional encodings get applied to
    every input sequence. For example, here is how to add positional encodings to
    the encoder and decoder inputs:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码是一个密集向量，用于编码句子中单词的位置：第*i*个位置编码被添加到句子中第*i*个单词的单词嵌入中。实现这一点的最简单方法是使用`Embedding`层，并使其对批处理中从0到最大序列长度的所有位置进行编码，然后将结果添加到单词嵌入中。广播规则将确保位置编码应用于每个输入序列。例如，以下是如何将位置编码添加到编码器和解码器输入的方法：
- en: '[PRE45]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note that this implementation assumes that the embeddings are represented as
    regular tensors, not ragged tensors.^([23](ch16.html#idm45720173475360)) The encoder
    and the decoder share the same `Embedding` layer for the positional encodings,
    since they have the same embedding size (this is often the case).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此实现假定嵌入表示为常规张量，而不是不规则张量。^([23](ch16.html#idm45720173475360)) 编码器和解码器共享相同的`Embedding`层用于位置编码，因为它们具有相同的嵌入大小（这通常是这种情况）。
- en: Instead of using trainable positional encodings, the authors of the transformer
    paper chose to use fixed positional encodings, based on the sine and cosine functions
    at different frequencies. The positional encoding matrix **P** is defined in [Equation
    16-2](#positional_encodings_equation) and represented at the top of [Figure 16-9](#positional_encodings_diagram)
    (transposed), where *P*[*p*,*i*] is the *i*^(th) component of the encoding for
    the word located at the *p*^(th) position in the sentence.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器论文的作者选择使用基于正弦和余弦函数在不同频率下的固定位置编码，而不是使用可训练的位置编码。位置编码矩阵**P**在[方程16-2](#positional_encodings_equation)中定义，并在[图16-9](#positional_encodings_diagram)的顶部（转置）表示，其中*P*[*p*,*i*]是句子中位于第*p*位置的单词的编码的第*i*个分量。
- en: Equation 16-2\. Sine/cosine positional encodings
  id: totrans-257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程16-2。正弦/余弦位置编码
- en: <math><msub><mi>P</mi><mrow><mi>p</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><mfenced
    open="{" close=""><mtable columnalign="left center"><mtr><mtd><mi>sin</mi><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mi>i</mi><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>i</mi><mtext> is
    even</mtext></mtd></mtr><mtr><mtd><mi>cos</mi><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mo>(</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>)</mo><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>i</mi><mtext> is
    odd</mtext></mtd></mtr></mtable></mfenced></math>![mls3 1609](assets/mls3_1609.png)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: <math><msub><mi>P</mi><mrow><mi>p</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><mfenced
    open="{" close=""><mtable columnalign="left center"><mtr><mtd><mi>sin</mi><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mi>i</mi><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mtd><mtd><mtext>如果</mtext><mi>i</mi><mtext>是偶数</mtext></mtd></mtr><mtr><mtd><mi>cos</mi><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mo>(</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>)</mo><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mtd><mtd><mtext>如果</mtext><mi>i</mi><mtext>是奇数</mtext></mtd></mtr></mtable></mfenced></math>![mls3
    1609](assets/mls3_1609.png)
- en: Figure 16-9\. Sine/cosine positional encoding matrix (transposed, top) with
    a focus on two values of *i* (bottom)
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-9。正弦/余弦位置编码矩阵（转置，顶部）关注两个*i*值（底部）
- en: 'This solution can give the same performance as trainable positional encodings,
    and it can extend to arbitrarily long sentences without adding any parameters
    to the model (however, when there is a large amount of pretraining data, trainable
    positional encodings are usually favored). After these positional encodings are
    added to the word embeddings, the rest of the model has access to the absolute
    position of each word in the sentence because there is a unique positional encoding
    for each position (e.g., the positional encoding for the word located at the 22nd
    position in a sentence is represented by the vertical dashed line at the top left
    of [Figure 16-9](#positional_encodings_diagram), and you can see that it is unique
    to that position). Moreover, the choice of oscillating functions (sine and cosine)
    makes it possible for the model to learn relative positions as well. For example,
    words located 38 words apart (e.g., at positions *p* = 22 and *p* = 60) always
    have the same positional encoding values in the encoding dimensions *i* = 100
    and *i* = 101, as you can see in [Figure 16-9](#positional_encodings_diagram).
    This explains why we need both the sine and the cosine for each frequency: if
    we only used the sine (the blue wave at *i* = 100), the model would not be able
    to distinguish positions *p* = 22 and *p* = 35 (marked by a cross).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no `PositionalEncoding` layer in TensorFlow, but it is not too hard
    to create one. For efficiency reasons, we precompute the positional encoding matrix
    in the constructor. The `call()` method just truncates this encoding matrix to
    the max length of the input sequences, and it adds them to the inputs. We also
    set `supports_masking=True` to propagate the input’s automatic mask to the next
    layer:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let’s use this layer to add the positional encoding to the encoder’s inputs:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now let’s look deeper into the heart of the transformer model, at the multi-head
    attention layer.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Multi-head attention
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand how a multi-head attention layer works, we must first understand
    the *scaled dot-product attention* layer, which it is based on. Its equation is
    shown in [Equation 16-3](#scaled_dot_product_attention), in a vectorized form.
    It’s the same as Luong attention, except for a scaling factor.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Equation 16-3\. Scaled dot-product attention
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mo>Attention</mo><mrow><mo>(</mo><mi mathvariant="bold">Q</mi><mo>,</mo><mi
    mathvariant="bold">K</mi><mo>,</mo><mi mathvariant="bold">V</mi><mo>)</mo></mrow><mo>=</mo><mo>softmax</mo><mfenced><mfrac><mrow><msup><mi
    mathvariant="bold">QK</mi><mo>⊺</mo></msup></mrow><msqrt><msub><mi>d</mi><mrow><mi>k</mi><mi>e</mi><mi>y</mi><mi>s</mi></mrow></msub></msqrt></mfrac></mfenced><mi
    mathvariant="bold">V</mi></math>
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '**Q** is a matrix containing one row per *query*. Its shape is [*n*[queries],
    *d*[keys]], where *n*[queries] is the number of queries and *d*[keys] is the number
    of dimensions of each query and each key.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K** is a matrix containing one row per *key*. Its shape is [*n*[keys], *d*[keys]],
    where *n*[keys] is the number of keys and values.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V** is a matrix containing one row per *value*. Its shape is [*n*[keys],
    *d*[values]], where *d*[values] is the number of dimensions of each value.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The shape of **Q** **K**^⊺ is [*n*[queries], *n*[keys]]: it contains one similarity
    score for each query/key pair. To prevent this matrix from being huge, the input
    sequences must not be too long (we will discuss how to overcome this limitation
    later in this chapter). The output of the softmax function has the same shape,
    but all rows sum up to 1\. The final output has a shape of [*n*[queries], *d*[values]]:
    there is one row per query, where each row represents the query result (a weighted
    sum of the values).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scaling factor 1 / (<math><msqrt><msub><mi>d</mi> <mrow><mi>keys</mi></mrow></msub></msqrt></math>)
    scales down the similarity scores to avoid saturating the softmax function, which
    would lead to tiny gradients.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to mask out some key/value pairs by adding a very large negative
    value to the corresponding similarity scores, just before computing the softmax.
    This is useful in the masked multi-head attention layer.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you set `use_scale=True` when creating a `tf.keras.layers.Attention` layer,
    then it will create an additional parameter that lets the layer learn how to properly
    downscale the similarity scores. The scaled dot-product attention used in the
    transformer model is almost the same, except it always scales the similarity scores
    by the same factor, 1 / (<math><msqrt><msub><mi>d</mi> <mrow><mi>keys</mi></mrow></msub></msqrt></math>).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the `Attention` layer’s inputs are just like **Q**, **K**, and **V**,
    except with an extra batch dimension (the first dimension). Internally, the layer
    computes all the attention scores for all sentences in the batch with just one
    call to `tf.matmul(queries, keys)`: this makes it extremely efficient. Indeed,
    in TensorFlow, if `A` and `B` are tensors with more than two dimensions—say, of
    shape [2, 3, 4, 5] and [2, 3, 5, 6], respectively—then `tf.matmul(A, B)` will
    treat these tensors as 2 × 3 arrays where each cell contains a matrix, and it
    will multiply the corresponding matrices: the matrix at the *i*^(th) row and *j*^(th)
    column in `A` will be multiplied by the matrix at the *i*^(th) row and *j*^(th)
    column in `B`. Since the product of a 4 × 5 matrix with a 5 × 6 matrix is a 4 × 6
    matrix, `tf.matmul(A, B)` will return an array of shape [2, 3, 4, 6].'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Now we’re ready to look at the multi-head attention layer. Its architecture
    is shown in [Figure 16-10](#multihead_attention_diagram).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1610](assets/mls3_1610.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: Figure 16-10\. Multi-head attention layer architecture⁠^([24](ch16.html#idm45720173058848))
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, it is just a bunch of scaled dot-product attention layers, each
    preceded by a linear transformation of the values, keys, and queries (i.e., a
    time-distributed dense layer with no activation function). All the outputs are
    simply concatenated, and they go through a final linear transformation (again,
    time-distributed).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: But why? What is the intuition behind this architecture? Well, consider once
    again the word “like” in the sentence “I like soccer”. The encoder was smart enough
    to encode the fact that it is a verb. But the word representation also includes
    its position in the text, thanks to the positional encodings, and it probably
    includes many other features that are useful for its translation, such as the
    fact that it is in the present tense. In short, the word representation encodes
    many different characteristics of the word. If we just used a single scaled dot-product
    attention layer, we would only be able to query all of these characteristics in
    one shot.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why the multi-head attention layer applies *multiple* different linear
    transformations of the values, keys, and queries: this allows the model to apply
    many different projections of the word representation into different subspaces,
    each focusing on a subset of the word’s characteristics. Perhaps one of the linear
    layers will project the word representation into a subspace where all that remains
    is the information that the word is a verb, another linear layer will extract
    just the fact that it is present tense, and so on. Then the scaled dot-product
    attention layers implement the lookup phase, and finally we concatenate all the
    results and project them back to the original space.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras includes a `tf.keras.layers.MultiHeadAttention` layer, so we now have
    everything we need to build the rest of the transformer. Let’s start with the
    full encoder, which is exactly like in [Figure 16-8](#transformer_diagram), except
    we use a stack of two blocks (`N = 2`) instead of six, since we don’t have a huge
    training set, and we add a bit of dropout as well:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This code should be mostly straightforward, except for one thing: masking.
    As of the time of writing, the `MultiHeadAttention` layer does not support automatic
    masking,⁠^([25](ch16.html#idm45720173048224)) so we must handle it manually. How
    can we do that?'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'The `MultiHeadAttention` layer accepts an `attention_mask` argument, which
    is a Boolean tensor of shape [*batch size*, *max query length*, *max value length*]:
    for every token in every query sequence, this mask indicates which tokens in the
    corresponding value sequence should be attended to. We want to tell the `MultiHeadAttention`
    layer to ignore all the padding tokens in the values. So, we first compute the
    padding mask using `tf.math.not_equal(encoder_input_ids, 0)`. This returns a Boolean
    tensor of shape [*batch size*, *max sequence length*]. We then insert a second
    axis using `[:, tf.newaxis]`, to get a mask of shape [*batch size*, 1, *max sequence
    length*]. This allows us to use this mask as the `attention_mask` when calling
    the `MultiHead​Atten⁠tion` layer: thanks to broadcasting, the same mask will be
    used for all tokens in each query. This way, the padding tokens in the values
    will be ignored correctly.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the layer will compute outputs for every single query token, including
    the padding tokens. We need to mask the outputs that correspond to these padding
    tokens. Recall that we used `mask_zero` in the `Embedding` layers, and we set
    `supports_masking` to `True` in the `PositionalEncoding` layer, so the automatic
    mask was propagated all the way to the `MultiHeadAttention` layer’s inputs (`encoder_in`).
    We can use this to our advantage in the skip connection: indeed, the `Add` layer
    supports automatic masking, so when we add `Z` and `skip` (which is initially
    equal to `encoder_in`), the outputs get automatically masked correctly.^([26](ch16.html#idm45720172796192))
    Yikes! Masking required much more explanation than code.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'Now on to the decoder! Once again, masking is going to be the only tricky part,
    so let’s start with that. The first multi-head attention layer is a self-attention
    layer, like in the encoder, but it is a *masked* multi-head attention layer, meaning
    it is causal: it should ignore all tokens in the future. So, we need two masks:
    a padding mask and a causal mask. Let’s create them:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The padding mask is exactly like the one we created for the encoder, except
    it’s based on the decoder’s inputs rather than the encoder’s. The causal mask
    is created using the `tf.linalg.band_part()` function, which takes a tensor and
    returns a copy with all the values outside a diagonal band set to zero. With these
    arguments, we get a square matrix of size `batch_max_len_dec` (the max length
    of the input sequences in the batch), with 1s in the lower-left triangle and 0s
    in the upper right. If we use this mask as the attention mask, we will get exactly
    what we want: the first query token will only attend to the first value token,
    the second will only attend to the first two, the third will only attend to the
    first three, and so on. In other words, query tokens cannot attend to any value
    token in the future.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now build the decoder:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'For the first attention layer, we use `causal_mask & decoder_pad_mask` to mask
    both the padding tokens and future tokens. The causal mask only has two dimensions:
    it’s missing the batch dimension, but that’s okay since broadcasting ensures that
    it gets copied across all the instances in the batch.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: For the second attention layer, there’s nothing special. The only thing to note
    is that we are using `encoder_pad_mask`, not `decoder_pad_mask`, because this
    attention layer uses the encoder’s final outputs as its values.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re almost done. We just need to add the final output layer, create the model,
    compile it, and train it:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Congratulations! You’ve built a full transformer from scratch, and trained it
    for automatic translation. This is getting quite advanced!
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-300
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Keras team has created a new [Keras NLP project](https://github.com/keras-team/keras-nlp),
    including an API to build a transformer more easily. You may also be interested
    in the new [Keras CV project for computer vision](https://github.com/keras-team/keras-cv).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: But the field didn’t stop there. Let’s now explore some of the recent advances.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: An Avalanche of Transformer Models
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The year 2018 has been called the “ImageNet moment for NLP”. Since then, progress
    has been astounding, with larger and larger transformer-based architectures trained
    on immense datasets.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the [GPT paper](https://homl.info/gpt)⁠^([27](ch16.html#idm45720172319856))
    by Alec Radford and other OpenAI researchers once again demonstrated the effectiveness
    of unsupervised pretraining, like the ELMo and ULMFiT papers before it, but this
    time using a transformer-like architecture. The authors pretrained a large but
    fairly simple architecture composed of a stack of 12 transformer modules using
    only masked multi-head attention layers, like in the original transformer’s decoder.
    They trained it on a very large dataset, using the same autoregressive technique
    we used for our Shakespearean char-RNN: just predict the next token. This is a
    form of self-supervised learning. Then they fine-tuned it on various language
    tasks, using only minor adaptations for each task. The tasks were quite diverse:
    they included text classification, *entailment* (whether sentence A imposes, involves,
    or implies sentence B as a necessary consequence),⁠^([28](ch16.html#idm45720172314768))
    similarity (e.g., “Nice weather today” is very similar to “It is sunny”), and
    question answering (given a few paragraphs of text giving some context, the model
    must answer some multiple-choice questions).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Then Google’s [BERT paper](https://homl.info/bert)⁠^([29](ch16.html#idm45720172313104))
    came out: it also demonstrated the effectiveness of self-supervised pretraining
    on a large corpus, using a similar architecture to GPT but with nonmasked multi-head
    attention layers only, like in the original transformer’s encoder. This means
    that the model is naturally bidirectional; hence the B in BERT (*Bidirectional
    Encoder Representations from Transformers*). Most importantly, the authors proposed
    two pretraining tasks that explain most of the model’s strength:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Masked language model (MLM)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Each word in a sentence has a 15% probability of being masked, and the model
    is trained to predict the masked words. For example, if the original sentence
    is “She had fun at the birthday party”, then the model may be given the sentence
    “She <mask> fun at the <mask> party” and it must predict the words “had” and “birthday”
    (the other outputs will be ignored). To be more precise, each selected word has
    an 80% chance of being masked, a 10% chance of being replaced by a random word
    (to reduce the discrepancy between pretraining and fine-tuning, since the model
    will not see <mask> tokens during fine-tuning), and a 10% chance of being left
    alone (to bias the model toward the correct answer).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Next sentence prediction (NSP)
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained to predict whether two sentences are consecutive or not.
    For example, it should predict that “The dog sleeps” and “It snores loudly” are
    consecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun” are
    not consecutive. Later research showed that NSP was not as important as was initially
    thought, so it was dropped in most later architectures.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is trained on these two tasks simultaneously (see [Figure 16-11](#bert_diagram)).
    For the NSP task, the authors inserted a class token (<CLS>) at the start of every
    input, and the corresponding output token represents the model’s prediction: sentence
    B follows sentence A, or it does not. The two input sentences are concatenated,
    separated only by a special separation token (<SEP>), and they are fed as input
    to the model. To help the model know which sentence each input token belongs to,
    a *segment embedding* is added on top of each token’s positional embeddings: there
    are just two possible segment embeddings, one for sentence A and one for sentence
    B. For the MLM task, some input words are masked (as we just saw) and the model
    tries to predict what those words were. The loss is only computed on the NSP prediction
    and the masked tokens, not on the unmasked ones.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1611](assets/mls3_1611.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: Figure 16-11\. BERT training and fine-tuning process⁠^([30](ch16.html#idm45720172299632))
  id: totrans-313
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After this unsupervised pretraining phase on a very large corpus of text, the
    model is then fine-tuned on many different tasks, changing very little for each
    task. For example, for text classification such as sentiment analysis, all output
    tokens are ignored except for the first one, corresponding to the class token,
    and a new output layer replaces the previous one, which was just a binary classification
    layer for NSP.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'In February 2019, just a few months after BERT was published, Alec Radford,
    Jeffrey Wu, and other OpenAI researchers published the [GPT-2 paper](https://homl.info/gpt2),⁠^([31](ch16.html#idm45720172296288))
    which proposed a very similar architecture to GPT, but larger still (with over
    1.5 billion parameters!). The researchers showed that the new and improved GPT
    model could perform *zero-shot learning* (ZSL), meaning it could achieve good
    performance on many tasks without any fine-tuning. This was just the start of
    a race toward larger and larger models: Google’s [Switch Transformers](https://homl.info/switch)⁠^([32](ch16.html#idm45720172293488))
    (introduced in January 2021) used 1 trillion parameters, and soon much larger
    models came out, such as the Wu Dao 2.0 model by the Beijing Academy of Artificial
    Intelligence (BAII), announced in June 2021.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'An unfortunate consequence of this trend toward gigantic models is that only
    well-funded organizations can afford to train such models: it can easily cost
    hundreds of thousands of dollars or more. And the energy required to train a single
    model corresponds to an American household’s electricity consumption for several
    years; it’s not eco-friendly at all. Many of these models are just too big to
    even be used on regular hardware: they wouldn’t fit in RAM, and they would be
    horribly slow. Lastly, some are so costly that they are not released publicly.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, ingenious researchers are finding new ways to downsize transformers
    and make them more data-efficient. For example, the [DistilBERT model](https://homl.info/distilbert),⁠^([33](ch16.html#idm45720172289264))
    introduced in October 2019 by Victor Sanh et al. from Hugging Face, is a small
    and fast transformer model based on BERT. It is available on Hugging Face’s excellent
    model hub, along with thousands of others—you’ll see an example later in this
    chapter.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'DistilBERT was trained using *distillation* (hence the name): this means transferring
    knowledge from a teacher model to a student one, which is usually much smaller
    than the teacher model. This is typically done by using the teacher’s predicted
    probabilities for each training instance as targets for the student. Surprisingly,
    distillation often works better than training the student from scratch on the
    same dataset as the teacher! Indeed, the student benefits from the teacher’s more
    nuanced labels.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Many more transformer architectures came out after BERT, almost on a monthly
    basis, often improving on the state of the art across all NLP tasks: XLNet (June
    2019), RoBERTa (July 2019), StructBERT (August 2019), ALBERT (September 2019),
    T5 (October 2019), ELECTRA (March 2020), GPT3 (May 2020), DeBERTa (June 2020),
    Switch Transformers (January 2021), Wu Dao 2.0 (June 2021), Gopher (December 2021),
    GPT-NeoX-20B (February 2022), Chinchilla (March 2022), OPT (May 2022), and the
    list goes on and on. Each of these models brought new ideas and techniques,^([34](ch16.html#idm45720172285632))
    but I particularly like the [T5 paper](https://homl.info/t5)⁠^([35](ch16.html#idm45720172282624))
    by Google researchers: it frames all NLP tasks as text-to-text, using an encoder–decoder
    transformer. For example, to translate “I like soccer” to Spanish, you can just
    call the model with the input sentence “translate English to Spanish: I like soccer”
    and it outputs “me gusta el fútbol”. To summarize a paragraph, you just enter
    “summarize:” followed by the paragraph, and it outputs the summary. For classification,
    you only need to change the prefix to “classify:” and the model outputs the class
    name, as text. This simplifies using the model, and it also makes it possible
    to pretrain it on even more tasks.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, in April 2022, Google researchers used a new large-scale
    training platform named *Pathways* (which we will briefly discuss in [Chapter 19](ch19.html#deployment_chapter))
    to train a humongous language model named the [*Pathways Language Model* (PaLM)](https://homl.info/palm),⁠^([36](ch16.html#idm45720172277824))
    with a whopping 540 billion parameters, using over 6,000 TPUs. Other than its
    incredible size, this model is a standard transformer, using decoders only (i.e.,
    with masked multi-head attention layers), with just a few tweaks (see the paper
    for details). This model achieved incredible performance on all sorts of NLP tasks,
    particularly in natural language understanding (NLU). It’s capable of impressive
    feats, such as explaining jokes, giving detailed step-by-step answers to questions,
    and even coding. This is in part due to the model’s size, but also thanks to a
    technique called [*Chain of thought prompting*](https://homl.info/ctp),⁠^([37](ch16.html#idm45720172273920))
    which was introduced a couple months earlier by another team of Google researchers.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'In question answering tasks, regular prompting typically includes a few examples
    of questions and answers, such as: “Q: Roger has 5 tennis balls. He buys 2 more
    cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does
    he have now? A: 11.” The prompt then continues with the actual question, such
    as “Q: John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take
    care of their business. How many hours a week does he spend taking care of dogs?
    A:”, and the model’s job is to append the answer: in this case, “35.”'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'But with chain of thought prompting, the example answers include all the reasoning
    steps that lead to the conclusion. For example, instead of “A: 11”, the prompt
    contains “A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis
    balls. 5 + 6 = 11.” This encourages the model to give a detailed answer to the
    actual question, such as “John takes care of 10 dogs. Each dog takes .5 hours
    a day to walk and take care of their business. So that is 10 × .5 = 5 hours a
    day. 5 hours a day × 7 days a week = 35 hours a week. The answer is 35 hours a
    week.” This is an actual example from the paper!'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Not only does the model give the right answer much more frequently than using
    regular prompting—we’re encouraging the model to think things through—but it also
    provides all the reasoning steps, which can be useful to better understand the
    rationale behind a model’s answer.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers have taken over NLP, but they didn’t stop there: they soon expanded
    to computer vision as well.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Vision Transformers
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the first applications of attention mechanisms beyond NMT was in generating
    image captions using [visual attention](https://homl.info/visualattention):⁠^([38](ch16.html#idm45720172266576))
    a convolutional neural network first processes the image and outputs some feature
    maps, then a decoder RNN equipped with an attention mechanism generates the caption,
    one word at a time.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'At each decoder time step (i.e., each word), the decoder uses the attention
    model to focus on just the right part of the image. For example, in [Figure 16-12](#visual_attention_diagram),
    the model generated the caption “A woman is throwing a frisbee in a park”, and
    you can see what part of the input image the decoder focused its attention on
    when it was about to output the word “frisbee”: clearly, most of its attention
    was focused on the frisbee.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1612](assets/mls3_1612.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-12\. Visual attention: an input image (left) and the model’s focus
    before producing the word “frisbee” (right)⁠^([39](ch16.html#idm45720172255968))'
  id: totrans-329
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When transformers came out in 2017 and people started to experiment with them
    beyond NLP, they were first used alongside CNNs, without replacing them. Instead,
    transformers were generally used to replace RNNs, for example, in image captioning
    models. Transformers became slightly more visual in a [2020 paper](https://homl.info/detr)^([41](ch16.html#idm45720172246864))
    by Facebook researchers, which proposed a hybrid CNN–transformer architecture
    for object detection. Once again, the CNN first processes the input images and
    outputs a set of feature maps, then these feature maps are converted to sequences
    and fed to a transformer, which outputs bounding box predictions. But again, most
    of the visual work is still done by the CNN.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in October 2020, a team of Google researchers released [a paper](https://homl.info/vit)⁠^([42](ch16.html#idm45720172244672))
    that introduced a fully transformer-based vision model, called a *vision transformer*
    (ViT). The idea is surprisingly simple: just chop the image into little 16 × 16
    squares, and treat the sequence of squares as if it were a sequence of word representations.
    To be more precise, the squares are first flattened into 16 × 16 × 3 = 768-dimensional
    vectors—the 3 is for the RGB color channels—then these vectors go through a linear
    layer that transforms them but retains their dimensionality. The resulting sequence
    of vectors can then be treated just like a sequence of word embeddings: this means
    adding positional embeddings, and passing the result to the transformer. That’s
    it! This model beat the state of the art on ImageNet image classification, but
    to be fair the authors had to use over 300 million additional images for training.
    This makes sense since transformers don’t have as many *inductive biases* as convolution
    neural nets, so they need extra data just to learn things that CNNs implicitly
    assume.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-332
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An inductive bias is an implicit assumption made by the model, due to its architecture.
    For example, linear models implicitly assume that the data is, well, linear. CNNs
    implicitly assume that patterns learned in one location will likely be useful
    in other locations as well. RNNs implicitly assume that the inputs are ordered,
    and that recent tokens are more important than older ones. The more inductive
    biases a model has, assuming they are correct, the less training data the model
    will require. But if the implicit assumptions are wrong, then the model may perform
    poorly even if it is trained on a large dataset.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Just two months later, a team of Facebook researchers released [a paper](https://homl.info/deit)⁠^([43](ch16.html#idm45720172239280))
    that introduced *data-efficient image transformers* (DeiTs). Their model achieved
    competitive results on ImageNet without requiring any additional data for training.
    The model’s architecture is virtually the same as the original ViT, but the authors
    used a distillation technique to transfer knowledge from state-of-the-art CNN
    models to their model.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in March 2021, DeepMind released an important [paper](https://homl.info/perceiver)⁠^([44](ch16.html#idm45720172235056))
    that introduced the *Perceiver* architecture. It is a *multimodal* transformer,
    meaning you can feed it text, images, audio, or virtually any other modality.
    Until then, transformers had been restricted to fairly short sequences because
    of the performance and RAM bottleneck in the attention layers. This excluded modalities
    such as audio or video, and it forced researchers to treat images as sequences
    of patches, rather than sequences of pixels. The bottleneck is due to self-attention,
    where every token must attend to every other token: if the input sequence has
    *M* tokens, then the attention layer must compute an *M* × *M* matrix, which can
    be huge if *M* is very large. The Perceiver solves this problem by gradually improving
    a fairly short *latent representation* of the inputs, composed of *N* tokens—typically
    just a few hundred. (The word *latent* means hidden, or internal.) The model uses
    cross-attention layers only, feeding them the latent representation as the queries,
    and the (possibly large) inputs as the values. This only requires computing an
    *M* × *N* matrix, so the computational complexity is linear with regard to *M*,
    instead of quadratic. After going through several cross-attention layers, if everything
    goes well, the latent representation ends up capturing everything that matters
    in the inputs. The authors also suggested sharing the weights between consecutive
    cross-attention layers: if you do that, then the Perceiver effectively becomes
    an RNN. Indeed, the shared cross-attention layers can be seen as the same memory
    cell at different time steps, and the latent representation corresponds to the
    cell’s context vector. The same inputs are repeatedly fed to the memory cell at
    every time step. It looks like RNNs are not dead after all!'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'Just a month later, Mathilde Caron et al. introduced [DINO](https://homl.info/dino),^([45](ch16.html#idm45720172223328))
    an impressive vision transformer trained entirely without labels, using self-supervision,
    and capable of high-accuracy semantic segmentation. The model is duplicated during
    training, with one network acting as a teacher and the other acting as a student.
    Gradient descent only affects the student, while the teacher’s weights are just
    an exponential moving average of the student’s weights. The student is trained
    to match the teacher’s predictions: since they’re almost the same model, this
    is called *self-distillation*. At each training step, the input images are augmented
    in different ways for the teacher and the student, so they don’t see the exact
    same image, but their predictions must match. This forces them to come up with
    high-level representations. To prevent *mode collapse*, where both the student
    and the teacher would always output the same thing, completely ignoring the inputs,
    DINO keeps track of a moving average of the teacher’s outputs, and it tweaks the
    teacher’s predictions to ensure that they remain centered on zero, on average.
    DINO also forces the teacher to have high confidence in its predictions: this
    is called *sharpening*. Together, these techniques preserve diversity in the teacher’s
    outputs.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'In a [2021 paper](https://homl.info/scalingvits),⁠^([46](ch16.html#idm45720172217680))
    Google researchers showed how to scale ViTs up or down, depending on the amount
    of data. They managed to create a huge 2 billion parameter model that reached
    over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a scaled-down
    model that reached over 84.8% top-1 accuracy on ImageNet, using only 10,000 images:
    that’s just 10 images per class!'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: And progress in visual transformers has continued steadily to this day. For
    example, in March 2022, a [paper](https://homl.info/modelsoups)⁠^([47](ch16.html#idm45720172215648))
    by Mitchell Wortsman et al. demonstrated that it’s possible to first train multiple
    transformers, then average their weights to create a new and improved model. This
    is similar to an ensemble (see [Chapter 7](ch07.html#ensembles_chapter)), except
    there’s just one model in the end, which means there’s no inference time penalty.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'The latest trend in transformers consists in building large multimodal models,
    often capable of zero-shot or few-shot learning. For example, [OpenAI’s 2021 CLIP
    paper](https://homl.info/clip)⁠^([48](ch16.html#idm45720172211920)) proposed a
    large transformer model pretrained to match captions with images: this task allows
    it to learn excellent image representations, and the model can then be used directly
    for tasks such as image classification using simple text prompts such as “a photo
    of a cat”. Soon after, OpenAI announced [DALL·E](https://homl.info/dalle),⁠^([49](ch16.html#idm45720172210416))
    capable of generating amazing images based on text prompts. The [DALL·E 2](https://homl.info/dalle2),⁠^([50](ch16.html#idm45720172208784))
    which generates even higher quality images using a diffusion model (see [Chapter 17](ch17.html#autoencoders_chapter)).'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: In April 2022, DeepMind released the [Flamingo paper](https://homl.info/flamingo),⁠^([51](ch16.html#idm45720172205280))
    which introduced a family of models pretrained on a wide variety of tasks across
    multiple modalities, including text, images, and videos. A single model can be
    used across very different tasks, such as question answering, image captioning,
    and more. Soon after, in May 2022, DeepMind introduced [GATO](https://homl.info/gato),⁠^([52](ch16.html#idm45720172203760))
    a multimodal model that can be used as a policy for a reinforcement learning agent
    (RL will be introduced in [Chapter 18](ch18.html#rl_chapter)). The same transformer
    can chat with you, caption images, play Atari games, control (simulated) robotic
    arms, and more, all with “only” 1.2 billion parameters. And the adventure continues!
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-341
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These astounding advances have led some researchers to claim that human-level
    AI is near, that “scale is all you need”, and that some of these models may be
    “slightly conscious”. Others point out that despite the amazing progress, these
    models still lack the reliability and adaptability of human intelligence, our
    ability to reason symbolically, to generalize based on a single example, and more.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, transformers are everywhere! And the good news is that you generally
    won’t have to implement transformers yourself since many excellent pretrained
    models are readily available for download via TensorFlow Hub or Hugging Face’s
    model hub. You’ve already seen how to use a model from TF Hub, so let’s close
    this chapter by taking a quick look at Hugging Face’s ecosystem.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face’s Transformers Library
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s impossible to talk about transformers today without mentioning Hugging
    Face, an AI company that has built a whole ecosystem of easy-to-use open source
    tools for NLP, vision, and beyond. The central component of their ecosystem is
    the Transformers library, which allows you to easily download a pretrained model,
    including its corresponding tokenizer, and then fine-tune it on your own dataset,
    if needed. Plus, the library supports TensorFlow, PyTorch, and JAX (with the Flax
    library).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to use the Transformers library is to use the `transformers.​pipe⁠line()`
    function: you just specify which task you want, such as sentiment analysis, and
    it downloads a default pretrained model, ready to be used—it really couldn’t be
    any simpler:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The result is a Python list containing one dictionary per input text:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In this example, the model correctly found that the sentence is positive, with
    around 99.98% confidence. Of course, you can also pass a batch of sentences to
    the model:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The `pipeline()` function uses the default model for the given task. For example,
    for text classification tasks such as sentiment analysis, at the time of writing,
    it defaults to `distilbert-base-uncased-finetuned-sst-2-english`—a DistilBERT
    model with an uncased tokenizer, trained on English Wikipedia and a corpus of
    English books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task.
    It’s also possible to manually specify a different model. For example, you could
    use a DistilBERT model fine-tuned on the Multi-Genre Natural Language Inference
    (MultiNLI) task, which classifies two sentences into three classes: contradiction,
    neutral, or entailment. Here is how:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Tip
  id: totrans-354
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can find the available models at [*https://huggingface.co/models*](https://huggingface.co/models),
    and the list of tasks at [*https://huggingface.co/tasks*](https://huggingface.co/tasks).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline API is very simple and convenient, but sometimes you will need
    more control. For such cases, the Transformers library provides many classes,
    including all sorts of tokenizers, models, configurations, callbacks, and much
    more. For example, let’s load the same DistilBERT model, along with its corresponding
    tokenizer, using the `TFAutoModelForSequenceClassification` and `AutoTokenizer`
    classes:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, let’s tokenize a couple of pairs of sentences. In this code, we activate
    padding and specify that we want TensorFlow tensors instead of Python lists:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Tip
  id: totrans-360
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Instead of passing `"Sentence 1 [SEP] Sentence 2"` to the tokenizer, you can
    equivalently pass it a tuple: `("Sentence 1",` `"Sentence 2")`.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is a dictionary-like instance of the `BatchEncoding` class, which
    contains the sequences of token IDs, as well as a mask containing 0s for the padding
    tokens:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: If you set `return_token_type_ids=True` when calling the tokenizer, you will
    also get an extra tensor that indicates which sentence each token belongs to.
    This is needed by some models, but not DistilBERT.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can directly pass this `BatchEncoding` object to the model; it returns
    a `TFSequenceClassifierOutput` object containing its predicted class logits:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Lastly, we can apply the softmax activation function to convert these logits
    to class probabilities, and use the `argmax()` function to predict the class with
    the highest probability for each input sentence pair:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In this example, the model correctly classifies the first sentence pair as neutral
    (the fact that I like soccer does not imply that everyone else does) and the second
    pair as an entailment (Joe must indeed be quite old).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wish to fine-tune this model on your own dataset, you can train the
    model as usual with Keras since it’s just a regular Keras model with a few extra
    methods. However, because the model outputs logits instead of probabilities, you
    must use the `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`
    loss instead of the usual `"sparse_categorical_crossentropy"` loss. Moreover,
    the model does not support `BatchEncoding` inputs during training, so you must
    use its `data` attribute to get a regular dictionary instead:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Hugging Face has also built a Datasets library that you can use to easily download
    a standard dataset (such as IMDb) or a custom one, and use it to fine-tune your
    model. It’s similar to TensorFlow Datasets, but it also provides tools to perform
    common preprocessing tasks on the fly, such as masking. The list of datasets is
    available at [*https://huggingface.co/datasets*](https://huggingface.co/datasets).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'This should get you started with Hugging Face’s ecosystem. To learn more, you
    can head over to [*https://huggingface.co/docs*](https://huggingface.co/docs)
    for the documentation, which includes many tutorial notebooks, videos, the full
    API, and more. I also recommend you check out the O’Reilly book [*Natural Language
    Processing with Transformers: Building Language Applications with Hugging Face*](https://homl.info/hfbook)
    by Lewis Tunstall, Leandro von Werra, and Thomas Wolf—all from the Hugging Face
    team.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will discuss how to learn deep representations in an
    unsupervised way using autoencoders, and we will use generative adversarial networks
    to produce images and more!
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the pros and cons of using a stateful RNN versus a stateless RNN?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do people use encoder–decoder RNNs rather than plain sequence-to-sequence
    RNNs for automatic translation?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you deal with variable-length input sequences? What about variable-length
    output sequences?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is beam search, and why would you use it? What tool can you use to implement
    it?
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an attention mechanism? How does it help?
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the most important layer in the transformer architecture? What is its
    purpose?
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When would you need to use sampled softmax?
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Embedded Reber grammars* were used by Hochreiter and Schmidhuber in [their
    paper](https://homl.info/93) about LSTMs. They are artificial grammars that produce
    strings such as “BPBTSXXVPSEPE”. Check out Jenny Orr’s [nice introduction](https://homl.info/108)
    to this topic, then choose a particular embedded Reber grammar (such as the one
    represented on Orr’s page), then train an RNN to identify whether a string respects
    that grammar or not. You will first need to write a function capable of generating
    a training batch containing about 50% strings that respect the grammar, and 50%
    that don’t.'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train an encoder–decoder model that can convert a date string from one format
    to another (e.g., from “April 22, 2019” to “2019-04-22”).
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go through the example on the Keras website for [“Natural language image search
    with a Dual Encoder”](https://homl.info/dualtuto). You will learn how to build
    a model capable of representing both images and text within the same embedding
    space. This makes it possible to search for images using a text prompt, like in
    the CLIP model by OpenAI.
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the Hugging Face Transformers library to download a pretrained language
    model capable of generating text (e.g., GPT), and try generating more convincing
    Shakespearean text. You will need to use the model’s `generate()` method—see Hugging
    Face’s documentation for more details.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch16.html#idm45720177608176-marker)) Alan Turing, “Computing Machinery
    and Intelligence”, *Mind* 49 (1950): 433–460.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch16.html#idm45720177605632-marker)) Of course, the word *chatbot* came
    much later. Turing called his test the *imitation game*: machine A and human B
    chat with human interrogator C via text messages; the interrogator asks questions
    to figure out which one is the machine (A or B). The machine passes the test if
    it can fool the interrogator, while the human B must try to help the interrogator.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch16.html#idm45720176925824-marker)) Since the input windows overlap,
    the concept of *epoch* is not so clear in this case: during each epoch (as implemented
    by Keras), the model will actually see the same character multiple times.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch16.html#idm45720176103360-marker)) Alec Radford et al., “Learning to
    Generate Reviews and Discovering Sentiment”, arXiv preprint arXiv:1704.01444 (2017).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch16.html#idm45720175873872-marker)) Rico Sennrich et al., “Neural Machine
    Translation of Rare Words with Subword Units”, *Proceedings of the 54th Annual
    Meeting of the Association for Computational Linguistics* 1 (2016): 1715–1725.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch16.html#idm45720175847312-marker)) Taku Kudo, “Subword Regularization:
    Improving Neural Network Translation Models with Multiple Subword Candidates”,
    arXiv preprint arXiv:1804.10959 (2018).'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch16.html#idm45720175842096-marker)) Taku Kudo and John Richardson, “SentencePiece:
    A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural
    Text Processing”, arXiv preprint arXiv:1808.06226 (2018).'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch16.html#idm45720175839056-marker)) Yonghui Wu et al., “Google’s Neural
    Machine Translation System: Bridging the Gap Between Human and Machine Translation”,
    arXiv preprint arXiv:1609.08144 (2016).'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch16.html#idm45720175499952-marker)) Ragged tensors were introduced in
    [Chapter 12](ch12.html#tensorflow_chapter), and they are detailed in [Appendix C](app03.html#structures_appendix).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch16.html#idm45720175311248-marker)) Matthew Peters et al., “Deep Contextualized
    Word Representations”, *Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies*
    1 (2018): 2227–2237.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch16.html#idm45720175307104-marker)) Jeremy Howard and Sebastian Ruder,
    “Universal Language Model Fine-Tuning for Text Classification”, *Proceedings of
    the 56th Annual Meeting of the Association for Computational Linguistics* 1 (2018):
    328–339.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch16.html#idm45720175301568-marker)) Daniel Cer et al., “Universal Sentence
    Encoder”, arXiv preprint arXiv:1803.11175 (2018).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch16.html#idm45720175133488-marker)) Ilya Sutskever et al., “Sequence
    to Sequence Learning with Neural Networks”, arXiv preprint (2014).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch16.html#idm45720175116736-marker)) Samy Bengio et al., “Scheduled Sampling
    for Sequence Prediction with Recurrent Neural Networks”, arXiv preprint arXiv:1506.03099
    (2015).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch16.html#idm45720175112096-marker)) This dataset is composed of sentence
    pairs created by contributors of the [Tatoeba project](https://tatoeba.org). About
    120,000 sentence pairs were selected by the authors of the website [*https://manythings.org/anki*](https://manythings.org/anki).
    This dataset is released under the Creative Commons Attribution 2.0 France license.
    Other language pairs are available.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch16.html#idm45720174413024-marker)) In Python, if you run `a, *b = [1,
    2, 3, 4]`, then `a` equals `1` and `b` equals `[2, 3, 4]`.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch16.html#idm45720174300992-marker)) Sébastien Jean et al., “On Using
    Very Large Target Vocabulary for Neural Machine Translation”, *Proceedings of
    the 53rd Annual Meeting of the Association for Computational Linguistics and the
    7th International Joint Conference on Natural Language Processing of the Asian
    Federation of Natural Language Processing* 1 (2015): 1–10.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch16.html#idm45720173841328-marker)) Dzmitry Bahdanau et al., “Neural
    Machine Translation by Jointly Learning to Align and Translate”, arXiv preprint
    arXiv:1409.0473 (2014).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch16.html#idm45720173812256-marker)) Minh-Thang Luong et al., “Effective
    Approaches to Attention-Based Neural Machine Translation”, *Proceedings of the
    2015 Conference on Empirical Methods in Natural Language Processing* (2015): 1412–1421.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '^([20](ch16.html#idm45720173607840-marker)) Ashish Vaswani et al., “Attention
    Is All You Need”, *Proceedings of the 31st International Conference on Neural
    Information Processing Systems* (2017): 6000–6010.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch16.html#idm45720173604064-marker)) Since the transformer uses time-distributed
    dense layers, you could argue that it uses 1D convolutional layers with a kernel
    size of 1.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch16.html#idm45720173579408-marker)) This is figure 1 from the “Attention
    Is All You Need” paper, reproduced with the kind permission of the authors.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: ^([23](ch16.html#idm45720173475360-marker)) It’s possible to use ragged tensors
    instead, if you are using the latest version of TensorFlow.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: ^([24](ch16.html#idm45720173058848-marker)) This is the righthand part of figure
    2 from “Attention Is All You Need”, reproduced with the kind authorization of
    the authors.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '^([25](ch16.html#idm45720173048224-marker)) This will most likely change by
    the time you read this; check out [Keras issue #16248](https://github.com/keras-team/keras/issues/16248)
    for more details. When this happens, there will be no need to set the `attention_mask`
    argument, and therefore no need to create `encoder_pad_mask`.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: ^([26](ch16.html#idm45720172796192-marker)) Currently `Z + skip` does not support
    automatic masking, which is why we had to write `tf.keras.​lay⁠ers.Add()([Z, skip])`
    instead. Again, this may change by the time you read this.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: ^([27](ch16.html#idm45720172319856-marker)) Alec Radford et al., “Improving
    Language Understanding by Generative Pre-Training” (2018).
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: ^([28](ch16.html#idm45720172314768-marker)) For example, the sentence “Jane
    had a lot of fun at her friend’s birthday party” entails “Jane enjoyed the party”,
    but it is contradicted by “Everyone hated the party” and it is unrelated to “The
    Earth is flat”.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '^([29](ch16.html#idm45720172313104-marker)) Jacob Devlin et al., “BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding”, *Proceedings of
    the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies* 1 (2019).'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: ^([30](ch16.html#idm45720172299632-marker)) This is figure 1 from the paper,
    reproduced with the kind authorization of the authors.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: ^([31](ch16.html#idm45720172296288-marker)) Alec Radford et al., “Language Models
    Are Unsupervised Multitask Learners” (2019).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '^([32](ch16.html#idm45720172293488-marker)) William Fedus et al., “Switch Transformers:
    Scaling to Trillion Parameter Models with Simple and Efficient Sparsity” (2021).'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '^([33](ch16.html#idm45720172289264-marker)) Victor Sanh et al., “DistilBERT,
    A Distilled Version of Bert: Smaller, Faster, Cheaper and Lighter”, arXiv preprint
    arXiv:1910.01108 (2019).'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '^([34](ch16.html#idm45720172285632-marker)) Mariya Yao summarized many of these
    models in this post: [*https://homl.info/yaopost*](https://homl.info/yaopost).'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: ^([35](ch16.html#idm45720172282624-marker)) Colin Raffel et al., “Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, arXiv
    preprint arXiv:1910.10683 (2019).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '^([36](ch16.html#idm45720172277824-marker)) Aakanksha Chowdhery et al., “PaLM:
    Scaling Language Modeling with Pathways”, arXiv preprint arXiv:2204.02311 (2022).'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: ^([37](ch16.html#idm45720172273920-marker)) Jason Wei et al., “Chain of Thought
    Prompting Elicits Reasoning in Large Language Models”, arXiv preprint arXiv:2201.11903
    (2022).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '^([38](ch16.html#idm45720172266576-marker)) Kelvin Xu et al., “Show, Attend
    and Tell: Neural Image Caption Generation with Visual Attention”, *Proceedings
    of the 32nd International Conference on Machine Learning* (2015): 2048–2057.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: ^([39](ch16.html#idm45720172255968-marker)) This is a part of figure 3 from
    the paper. It is reproduced with the kind authorization of the authors.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '^([40](ch16.html#idm45720172249728-marker)) Marco Tulio Ribeiro et al., “‘Why
    Should I Trust You?’: Explaining the Predictions of Any Classifier”, *Proceedings
    of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining* (2016): 1135–1144.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: ^([41](ch16.html#idm45720172246864-marker)) Nicolas Carion et al., “End-to-End
    Object Detection with Transformers”, arXiv preprint arxiv:2005.12872 (2020).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '^([42](ch16.html#idm45720172244672-marker)) Alexey Dosovitskiy et al., “An
    Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale”, arXiv
    preprint arxiv:2010.11929 (2020).'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: ^([43](ch16.html#idm45720172239280-marker)) Hugo Touvron et al., “Training Data-Efficient
    Image Transformers & Distillation Through Attention”, arXiv preprint arxiv:2012.12877
    (2020).
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '^([44](ch16.html#idm45720172235056-marker)) Andrew Jaegle et al., “Perceiver:
    General Perception with Iterative Attention”, arXiv preprint arxiv:2103.03206
    (2021).'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: ^([45](ch16.html#idm45720172223328-marker)) Mathilde Caron et al., “Emerging
    Properties in Self-Supervised Vision Transformers”, arXiv preprint arxiv:2104.14294
    (2021).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: ^([46](ch16.html#idm45720172217680-marker)) Xiaohua Zhai et al., “Scaling Vision
    Transformers”, arXiv preprint arxiv:2106.04560v1 (2021).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '^([47](ch16.html#idm45720172215648-marker)) Mitchell Wortsman et al., “Model
    Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accuracy Without
    Increasing Inference Time”, arXiv preprint arxiv:2203.05482v1 (2022).'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: ^([48](ch16.html#idm45720172211920-marker)) Alec Radford et al., “Learning Transferable
    Visual Models From Natural Language Supervision”, arXiv preprint arxiv:2103.00020
    (2021).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: ^([49](ch16.html#idm45720172210416-marker)) Aditya Ramesh et al., “Zero-Shot
    Text-to-Image Generation”, arXiv preprint arxiv:2102.12092 (2021).
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: ^([50](ch16.html#idm45720172208784-marker)) Aditya Ramesh et al., “Hierarchical
    Text-Conditional Image Generation with CLIP Latents”, arXiv preprint arxiv:2204.06125
    (2022).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '^([51](ch16.html#idm45720172205280-marker)) Jean-Baptiste Alayrac et al., “Flamingo:
    a Visual Language Model for Few-Shot Learning”, arXiv preprint arxiv:2204.14198
    (2022).'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: ^([52](ch16.html#idm45720172203760-marker)) Scott Reed et al., “A Generalist
    Agent”, arXiv preprint arxiv:2205.06175 (2022).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
