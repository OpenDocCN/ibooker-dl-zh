- en: Chapter 16\. Natural Language Processing with RNNs and Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When Alan Turing imagined his famous [Turing test](https://homl.info/turingtest)⁠^([1](ch16.html#idm45720177608176))
    in 1950, he proposed a way to evaluate a machine’s ability to match human intelligence.
    He could have tested for many things, such as the ability to recognize cats in
    pictures, play chess, compose music, or escape a maze, but, interestingly, he
    chose a linguistic task. More specifically, he devised a *chatbot* capable of
    fooling its interlocutor into thinking it was human.⁠^([2](ch16.html#idm45720177605632))
    This test does have its weaknesses: a set of hardcoded rules can fool unsuspecting
    or naive humans (e.g., the machine could give vague predefined answers in response
    to some keywords, it could pretend that it is joking or drunk to get a pass on
    its weirdest answers, or it could escape difficult questions by answering them
    with its own questions), and many aspects of human intelligence are utterly ignored
    (e.g., the ability to interpret nonverbal communication such as facial expressions,
    or to learn a manual task). But the test does highlight the fact that mastering
    language is arguably *Homo sapiens*’s greatest cognitive ability.'
  prefs: []
  type: TYPE_NORMAL
- en: Can we build a machine that can master written and spoken language? This is
    the ultimate goal of NLP research, but it’s a bit too broad, so in practice researchers
    focus on more specific tasks, such as text classification, translation, summarization,
    question answering, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: A common approach for natural language tasks is to use recurrent neural networks.
    We will therefore continue to explore RNNs (introduced in [Chapter 15](ch15.html#rnn_chapter)),
    starting with a *character RNN*, or *char-RNN*, trained to predict the next character
    in a sentence. This will allow us to generate some original text. We will first
    use a *stateless RNN* (which learns on random portions of text at each iteration,
    without any information on the rest of the text), then we will build a *stateful
    RNN* (which preserves the hidden state between training iterations and continues
    reading where it left off, allowing it to learn longer patterns). Next, we will
    build an RNN to perform sentiment analysis (e.g., reading movie reviews and extracting
    the rater’s feeling about the movie), this time treating sentences as sequences
    of words, rather than characters. Then we will show how RNNs can be used to build
    an encoder–decoder architecture capable of performing neural machine translation
    (NMT), translating English to Spanish.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of this chapter, we will explore *attention mechanisms*.
    As their name suggests, these are neural network components that learn to select
    the part of the inputs that the rest of the model should focus on at each time
    step. First, we will boost the performance of an RNN-based encoder–decoder architecture
    using attention. Next, we will drop RNNs altogether and use a very successful
    attention-only architecture, called the *transformer*, to build a translation
    model. We will then discuss some of the most important advances in NLP in the
    last few years, including incredibly powerful language models such as GPT and
    BERT, both based on transformers. Lastly, I will show you how to get started with
    the excellent Transformers library by Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a simple and fun model that can write like Shakespeare (sort
    of).
  prefs: []
  type: TYPE_NORMAL
- en: Generating Shakespearean Text Using a Character RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a famous [2015 blog post](https://homl.info/charrnn) titled “The Unreasonable
    Effectiveness of Recurrent Neural Networks”, Andrej Karpathy showed how to train
    an RNN to predict the next character in a sentence. This *char-RNN* can then be
    used to generate novel text, one character at a time. Here is a small sample of
    the text generated by a char-RNN model after it was trained on all of Shakespeare’s
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PANDARUS:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alas, I think he shall be come approached and the day
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When little srain would be attain’d into being never fed,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And who is but a chain and subjects of his death,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I should not sleep.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not exactly a masterpiece, but it is still impressive that the model was able
    to learn words, grammar, proper punctuation, and more, just by learning to predict
    the next character in a sentence. This is our first example of a *language model*;
    similar (but much more powerful) language models, discussed later in this chapter,
    are at the core of modern NLP. In the remainder of this section we’ll build a
    char-RNN step by step, starting with the creation of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Training Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, using Keras’s handy `tf.keras.utils.get_file()` function, let’s download
    all of Shakespeare’s works. The data is loaded from Andrej Karpathy’s [char-rnn
    project](https://github.com/karpathy/char-rnn):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s print the first few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Looks like Shakespeare all right!
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll use a `tf.keras.layers.TextVectorization` layer (introduced in
    [Chapter 13](ch13.html#data_chapter)) to encode this text. We set `split="character"`
    to get character-level encoding rather than the default word-level encoding, and
    we use `standardize="lower"` to convert the text to lowercase (which will simplify
    the task):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Each character is now mapped to an integer, starting at 2\. The `TextVectorization`
    layer reserved the value 0 for padding tokens, and it reserved 1 for unknown characters.
    We won’t need either of these tokens for now, so let’s subtract 2 from the character
    IDs and compute the number of distinct characters and the total number of characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, just like we did in [Chapter 15](ch15.html#rnn_chapter), we can turn
    this very long sequence into a dataset of windows that we can then use to train
    a sequence-to-sequence RNN. The targets will be similar to the inputs, but shifted
    by one time step into the “future”. For example, one sample in the dataset may
    be a sequence of character IDs representing the text “to be or not to b” (without
    the final “e”), and the corresponding target—a sequence of character IDs representing
    the text “o be or not to be” (with the final “e”, but without the leading “t”).
    Let’s write a small utility function to convert a long sequence of character IDs
    into a dataset of input/target window pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This function starts much like the `to_windows()` custom utility function we
    created in [Chapter 15](ch15.html#rnn_chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: It takes a sequence as input (i.e., the encoded text), and creates a dataset
    containing all the windows of the desired length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It increases the length by one, since we need the next character for the target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, it shuffles the windows (optionally), batches them, splits them into input/output
    pairs, and activates prefetching.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 16-1](#window_dataset_diagram) summarizes the dataset preparation steps:
    it shows windows of length 11, and a batch size of 3\. The start index of each
    window is indicated next to it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1601](assets/mls3_1601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-1\. Preparing a dataset of shuffled windows
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we’re ready to create the training set, the validation set, and the test
    set. We will use roughly 90% of the text for training, 5% for validation, and
    5% for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We set the window length to 100, but you can try tuning it: it’s easier and
    faster to train RNNs on shorter input sequences, but the RNN will not be able
    to learn any pattern longer than `length`, so don’t make it too small.'
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! Preparing the dataset was the hardest part. Now let’s create the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Building and Training the Char-RNN Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since our dataset is reasonably large, and modeling language is quite a difficult
    task, we need more than a simple RNN with a few recurrent neurons. Let’s build
    and train a model with one `GRU` layer composed of 128 units (you can try tweaking
    the number of layers and units later, if needed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go over this code:'
  prefs: []
  type: TYPE_NORMAL
- en: We use an `Embedding` layer as the first layer, to encode the character IDs
    (embeddings were introduced in [Chapter 13](ch13.html#data_chapter)). The `Embedding`
    layer’s number of input dimensions is the number of distinct character IDs, and
    the number of output dimensions is a hyperparameter you can tune—we’ll set it
    to 16 for now. Whereas the inputs of the `Embedding` layer will be 2D tensors
    of shape [*batch size*, *window length*], the output of the `Embedding` layer
    will be a 3D tensor of shape [*batch size*, *window length*, *embedding size*].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use a `Dense` layer for the output layer: it must have 39 units (`n_tokens`)
    because there are 39 distinct characters in the text, and we want to output a
    probability for each possible character (at each time step). The 39 output probabilities
    should sum up to 1 at each time step, so we apply the softmax activation function
    to the outputs of the `Dense` layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we compile this model, using the `"sparse_categorical_crossentropy"`
    loss and a Nadam optimizer, and we train the model for several epochs,^([3](ch16.html#idm45720176925824))
    using a `ModelCheckpoint` callback to save the best model (in terms of validation
    accuracy) as training progresses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are running this code on Colab with a GPU activated, then training should
    take roughly one to two hours. You can reduce the number of epochs if you don’t
    want to wait that long, but of course the model’s accuracy will probably be lower.
    If the Colab session times out, make sure to reconnect quickly, or else the Colab
    runtime will be destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: 'This model does not handle text preprocessing, so let’s wrap it in a final
    model containing the `tf.keras.layers.TextVectorization` layer as the first layer,
    plus a `tf.keras.layers.Lambda` layer to subtract 2 from the character IDs since
    we’re not using the padding and unknown tokens for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And now let’s use it to predict the next character in a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Great, the model correctly predicted the next character. Now let’s use this
    model to pretend we’re Shakespeare!
  prefs: []
  type: TYPE_NORMAL
- en: Generating Fake Shakespearean Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To generate new text using the char-RNN model, we could feed it some text,
    make the model predict the most likely next letter, add it to the end of the text,
    then give the extended text to the model to guess the next letter, and so on.
    This is called *greedy decoding*. But in practice this often leads to the same
    words being repeated over and over again. Instead, we can sample the next character
    randomly, with a probability equal to the estimated probability, using TensorFlow’s
    `tf.random.categorical()` function. This will generate more diverse and interesting
    text. The `categorical()` function samples random class indices, given the class
    log probabilities (logits). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To have more control over the diversity of the generated text, we can divide
    the logits by a number called the *temperature*, which we can tweak as we wish.
    A temperature close to zero favors high-probability characters, while a high temperature
    gives all characters an equal probability. Lower temperatures are typically preferred
    when generating fairly rigid and precise text, such as mathematical equations,
    while higher temperatures are preferred when generating more diverse and creative
    text. The following `next_char()` custom helper function uses this approach to
    pick the next character to add to the input text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can write another small helper function that will repeatedly call
    `next_char()` to get the next character and append it to the given text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to generate some text! Let’s try with different temperature
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Shakespeare seems to be suffering from a heatwave. To generate more convincing
    text, a common technique is to sample only from the top *k* characters, or only
    from the smallest set of top characters whose total probability exceeds some threshold
    (this is called *nucleus sampling*). Alternatively, you could try using *beam
    search*, which we will discuss later in this chapter, or using more `GRU` layers
    and more neurons per layer, training for longer, and adding some regularization
    if needed. Also note that the model is currently incapable of learning patterns
    longer than `length`, which is just 100 characters. You could try making this
    window larger, but it will also make training harder, and even LSTM and GRU cells
    cannot handle very long sequences. An alternative approach is to use a stateful
    RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful RNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Until now, we have only used *stateless RNNs*: at each training iteration the
    model starts with a hidden state full of zeros, then it updates this state at
    each time step, and after the last time step, it throws it away as it is not needed
    anymore. What if we instructed the RNN to preserve this final state after processing
    a training batch and use it as the initial state for the next training batch?
    This way the model could learn long-term patterns despite only backpropagating
    through short sequences. This is called a *stateful RNN*. Let’s go over how to
    build one.'
  prefs: []
  type: TYPE_NORMAL
- en: First, note that a stateful RNN only makes sense if each input sequence in a
    batch starts exactly where the corresponding sequence in the previous batch left
    off. So the first thing we need to do to build a stateful RNN is to use sequential
    and nonoverlapping input sequences (rather than the shuffled and overlapping sequences
    we used to train stateless RNNs). When creating the `tf.data.Dataset`, we must
    therefore use `shift=length` (instead of `shift=1`) when calling the `window()`
    method. Moreover, we must *not* call the `shuffle()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, batching is much harder when preparing a dataset for a stateful
    RNN than it is for a stateless RNN. Indeed, if we were to call `batch(32)`, then
    32 consecutive windows would be put in the same batch, and the following batch
    would not continue each of these windows where it left off. The first batch would
    contain windows 1 to 32 and the second batch would contain windows 33 to 64, so
    if you consider, say, the first window of each batch (i.e., windows 1 and 33),
    you can see that they are not consecutive. The simplest solution to this problem
    is to just use a batch size of 1\. The following `to_dataset_for_stateful_rnn()`
    custom utility function uses this strategy to prepare a dataset for a stateful
    RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 16-2](#stateful_rnn_dataset_diagram) summarizes the main steps of this
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1602](assets/mls3_1602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-2\. Preparing a dataset of consecutive sequence fragments for a stateful
    RNN
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s
    text into 32 texts of equal length, create one dataset of consecutive input sequences
    for each of them, and finally use `tf.data.Dataset.zip(datasets).map(lambda *windows:
    tf.stack(windows))` to create proper consecutive batches, where the *n*^(th) input
    sequence in a batch starts off exactly where the *n*^(th) input sequence ended
    in the previous batch (see the notebook for the full code).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create the stateful RNN. We need to set the `stateful` argument
    to `True` when creating each recurrent layer, and because the stateful RNN needs
    to know the batch size (since it will preserve a state for each input sequence
    in the batch). Therefore we must set the `batch_input_shape` argument in the first
    layer. Note that we can leave the second dimension unspecified, since the input
    sequences could have any length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of each epoch, we need to reset the states before we go back to
    the beginning of the text. For this, we can use a small custom Keras callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can compile the model and train it using our callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After this model is trained, it will only be possible to use it to make predictions
    for batches of the same size as were used during training. To avoid this restriction,
    create an identical *stateless* model, and copy the stateful model’s weights to
    this model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, although a char-RNN model is just trained to predict the next
    character, this seemingly simple task actually requires it to learn some higher-level
    tasks as well. For example, to find the next character after “Great movie, I really”,
    it’s helpful to understand that the sentence is positive, so what follows is more
    likely to be the letter “l” (for “loved”) rather than “h” (for “hated”). In fact,
    a [2017 paper](https://homl.info/sentimentneuron)⁠^([4](ch16.html#idm45720176103360))
    by Alec Radford and other OpenAI researchers describes how the authors trained
    a big char-RNN-like model on a large dataset, and found that one of the neurons
    acted as an excellent sentiment analysis classifier: although the model was trained
    without any labels, the *sentiment neuron*—as they called it—reached state-of-the-art
    performance on sentiment analysis benchmarks. This foreshadowed and motivated
    unsupervised pretraining in NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: But before we explore unsupervised pretraining, let’s turn our attention to
    word-level models and how to use them in a supervised fashion for sentiment analysis.
    In the process, you will learn how to handle sequences of variable lengths using
    masking.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generating text can be fun and instructive, but in real-life projects, one
    of the most common applications of NLP is text classification—especially sentiment
    analysis. If image classification on the MNIST dataset is the “Hello world!” of
    computer vision, then sentiment analysis on the IMDb reviews dataset is the “Hello
    world!” of natural language processing. The IMDb dataset consists of 50,000 movie
    reviews in English (25,000 for training, 25,000 for testing) extracted from the
    famous [Internet Movie Database](https://imdb.com), along with a simple binary
    target for each review indicating whether it is negative (0) or positive (1).
    Just like MNIST, the IMDb reviews dataset is popular for good reasons: it is simple
    enough to be tackled on a laptop in a reasonable amount of time, but challenging
    enough to be fun and rewarding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s load the IMDb dataset using the TensorFlow Datasets library (introduced
    in [Chapter 13](ch13.html#data_chapter)). We’ll use the first 90% of the training
    set for training, and the remaining 10% for validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Keras also includes a function for loading the IMDb dataset, if you prefer:
    `tf.keras.datasets.imdb.load_data()`. The reviews are already preprocessed as
    sequences of word IDs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect a few reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Some reviews are easy to classify. For example, the first review includes the
    words “terrible movie” in the very first sentence. But in many cases things are
    not that simple. For example, the third review starts off positively, even though
    it’s ultimately a negative review (label 0).
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a model for this task, we need to preprocess the text, but this time
    we will chop it into words instead of characters. For this, we can use the `tf.keras.​lay⁠ers.TextVectorization`
    layer again. Note that it uses spaces to identify word boundaries, which will
    not work well in some languages. For example, Chinese writing does not use spaces
    between words, Vietnamese uses spaces even within words, and German often attaches
    multiple words together, without spaces. Even in English, spaces are not always
    the best way to tokenize text: think of “San Francisco” or “#ILoveDeepLearning”.'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are solutions to address these issues. In a [2016 paper](https://homl.info/rarewords),⁠^([5](ch16.html#idm45720175873872))
    Rico Sennrich et al. from the University of Edinburgh explored several methods
    to tokenize and detokenize text at the subword level. This way, even if your model
    encounters a rare word it has never seen before, it can still reasonably guess
    what it means. For example, even if the model never saw the word “smartest” during
    training, if it learned the word “smart” and it also learned that the suffix “est”
    means “the most”, it can infer the meaning of “smartest”. One of the techniques
    the authors evaluated is *byte pair encoding* (BPE). BPE works by splitting the
    whole training set into individual characters (including spaces), then repeatedly
    merging the most frequent adjacent pairs until the vocabulary reaches the desired
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 'A [2018 paper](https://homl.info/subword)⁠^([6](ch16.html#idm45720175847312))
    by Taku Kudo at Google further improved subword tokenization, often removing the
    need for language-specific preprocessing prior to tokenization. Moreover, the
    paper proposed a novel regularization technique called *subword regularization*,
    which improves accuracy and robustness by introducing some randomness in tokenization
    during training: for example, “New England” may be tokenized as “New” + “England”,
    or “New” + “Eng” + “land”, or simply “New England” (just one token). Google’s
    [*SentencePiece*](https://github.com/google/sentencepiece) project provides an
    open source implementation, which is described in a [paper](https://homl.info/sentencepiece)⁠^([7](ch16.html#idm45720175842096))
    by Taku Kudo and John Richardson.'
  prefs: []
  type: TYPE_NORMAL
- en: The [TensorFlow Text](https://homl.info/tftext) library also implements various
    tokenization strategies, including [WordPiece](https://homl.info/wordpiece)⁠^([8](ch16.html#idm45720175839056))
    (a variant of BPE), and last but not least, the [Tokenizers library by Hugging
    Face](https://homl.info/tokenizers) implements a wide range of extremely fast
    tokenizers.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for the IMDb task in English, using spaces for token boundaries should
    be good enough. So let’s go ahead with creating a `TextVectorization` layer and
    adapting it to the training set. We will limit the vocabulary to 1,000 tokens,
    including the most frequent 998 words plus a padding token and a token for unknown
    words, since it’s unlikely that very rare words will be important for this task,
    and limiting the vocabulary size will reduce the number of parameters the model
    needs to learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can create the model and train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The first layer is the `TextVectorization` layer we just prepared, followed
    by an `Embedding` layer that will convert word IDs into embeddings. The embedding
    matrix needs to have one row per token in the vocabulary (`vocab_size`) and one
    column per embedding dimension (this example uses 128 dimensions, but this is
    a hyperparameter you could tune). Next we use a `GRU` layer and a `Dense` layer
    with a single neuron and the sigmoid activation function, since this is a binary
    classification task: the model’s output will be the estimated probability that
    the review expresses a positive sentiment regarding the movie. We then compile
    the model, and we fit it on the dataset we prepared earlier for a couple of epochs
    (or you can train for longer to get better results).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sadly, if you run this code, you will generally find that the model fails to
    learn anything at all: the accuracy remains close to 50%, no better than random
    chance. Why is that? The reviews have different lengths, so when the `TextVectorization`
    layer converts them to sequences of token IDs, it pads the shorter sequences using
    the padding token (with ID 0) to make them as long as the longest sequence in
    the batch. As a result, most sequences end with many padding tokens—often dozens
    or even hundreds of them. Even though we’re using a `GRU` layer, which is much
    better than a `SimpleRNN` layer, its short-term memory is still not great, so
    when it goes through many padding tokens, it ends up forgetting what the review
    was about! One solution is to feed the model with batches of equal-length sentences
    (which also speeds up training). Another solution is to make the RNN ignore the
    padding tokens. This can be done using masking.'
  prefs: []
  type: TYPE_NORMAL
- en: Masking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Making the model ignore padding tokens is trivial using Keras: simply add `mask_zero=True`
    when creating the `Embedding` layer. This means that padding tokens (whose ID
    is 0) will be ignored by all downstream layers. That’s all! If you retrain the
    previous model for a few epochs, you will find that the validation accuracy quickly
    reaches over 80%.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The way this works is that the `Embedding` layer creates a *mask tensor* equal
    to `tf.math.not_equal(inputs, 0)`: it is a Boolean tensor with the same shape
    as the inputs, and it is equal to `False` anywhere the token IDs are 0, or `True`
    otherwise. This mask tensor is then automatically propagated by the model to the
    next layer. If that layer’s `call()` method has a `mask` argument, then it automatically
    receives the mask. This allows the layer to ignore the appropriate time steps.
    Each layer may handle the mask differently, but in general they simply ignore
    masked time steps (i.e., time steps for which the mask is `False`). For example,
    when a recurrent layer encounters a masked time step, it simply copies the output
    from the previous time step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, if the layer’s `supports_masking` attribute is `True`, then the mask
    is automatically propagated to the next layer. It keeps propagating this way for
    as long as the layers have `supports_masking=True`. As an example, a recurrent
    layer’s `supports_​mask⁠ing` attribute is `True` when `return_sequences=True`,
    but it’s `False` when `return_​sequen⁠ces=False` since there’s no need for a mask
    anymore in this case. So if you have a model with several recurrent layers with
    `return_sequences=True`, followed by a recurrent layer with `return_sequences=False`,
    then the mask will automatically propagate up to the last recurrent layer: that
    layer will use the mask to ignore masked steps, but it will not propagate the
    mask any further. Similarly, if you set `mask_zero=True` when creating the `Embedding`
    layer in the sentiment analysis model we just built, then the `GRU` layer will
    receive and use the mask automatically, but it will not propagate it any further,
    since `return_sequences` is not set to `True`.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Some layers need to update the mask before propagating it to the next layer:
    they do so by implementing the `compute_mask()` method, which takes two arguments:
    the inputs and the previous mask. It then computes the updated mask and returns
    it. The default implementation of `compute_mask()` just returns the previous mask
    unchanged.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many Keras layers support masking: `SimpleRNN`, `GRU`, `LSTM`, `Bidirectional`,
    `Dense`, `TimeDistributed`, `Add`, and a few others (all in the `tf.keras.layers`
    package). However, convolutional layers (including `Conv1D`) do not support masking—it’s
    not obvious how they would do so anyway.'
  prefs: []
  type: TYPE_NORMAL
- en: If the mask propagates all the way to the output, then it gets applied to the
    losses as well, so the masked time steps will not contribute to the loss (their
    loss will be 0). This assumes that the model outputs sequences, which is not the
    case in our sentiment analysis model.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `LSTM` and `GRU` layers have an optimized implementation for GPUs, based
    on Nvidia’s cuDNN library. However, this implementation only supports masking
    if all the padding tokens are at the end of the sequences. It also requires you
    to use the default values for several hyperparameters: `activation`, `recurrent_activation`,
    `recurrent_dropout`, `unroll`, `use_bias`, and `reset_after`. If that’s not the
    case, then these layers will fall back to the (much slower) default GPU implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to implement your own custom layer with masking support, you should
    add a `mask` argument to the `call()` method, and obviously make the method use
    the mask. Additionally, if the mask must be propagated to the next layers, then
    you should set `self.supports_masking=True` in the constructor. If the mask must
    be updated before it is propagated, then you must implement the `compute_mask()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your model does not start with an `Embedding` layer, you may use the `tf.​​keras.layers.Masking`
    layer instead: by default, it sets the mask to `tf.math.​​reduce_any(tf.math.not_equal(X,
    0), axis=-1)`, meaning that time steps where the last dimension is full of zeros
    will be masked out in subsequent layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using masking layers and automatic mask propagation works best for simple models.
    It will not always work for more complex models, such as when you need to mix
    `Conv1D` layers with recurrent layers. In such cases, you will need to explicitly
    compute the mask and pass it to the appropriate layers, using either the functional
    API or the subclassing API. For example, the following model is equivalent to
    the previous model, except it is built using the functional API and handles masking
    manually. It also adds a bit of dropout since the previous model was overfitting
    slightly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'One last approach to masking is to feed the model with ragged tensors.⁠^([9](ch16.html#idm45720175499952))
    In practice, all you need to do is to set `ragged=True` when creating the `TextVectorization`
    layer, so that the input sequences are represented as ragged tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare this ragged tensor representation with the regular tensor representation,
    which uses padding tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Keras’s recurrent layers have built-in support for ragged tensors, so there’s
    nothing else you need to do: just use this `TextVectorization` layer in your model.
    There’s no need to pass `mask_zero=True` or handle masks explicitly—it’s all implemented
    for you. That’s convenient! However, as of early 2022, the support for ragged
    tensors in Keras is still fairly recent, so there are a few rough edges. For example,
    it is currently not possible to use ragged tensors as targets when running on
    the GPU (but this may be resolved by the time you read these lines).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whichever masking approach you prefer, after training this model for a few
    epochs, it will become quite good at judging whether a review is positive or not.
    If you use the `tf.keras.callbacks.TensorBoard()` callback, you can visualize
    the embeddings in TensorBoard as they are being learned: it is fascinating to
    see words like “awesome” and “amazing” gradually cluster on one side of the embedding
    space, while words like “awful” and “terrible” cluster on the other side. Some
    words are not as positive as you might expect (at least with this model), such
    as the word “good”, presumably because many negative reviews contain the phrase
    “not good”.'
  prefs: []
  type: TYPE_NORMAL
- en: Reusing Pretrained Embeddings and Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s impressive that the model is able to learn useful word embeddings based
    on just 25,000 movie reviews. Imagine how good the embeddings would be if we had
    billions of reviews to train on! Unfortunately, we don’t, but perhaps we can reuse
    word embeddings trained on some other (very) large text corpus (e.g., Amazon reviews,
    available on TensorFlow Datasets), even if it is not composed of movie reviews?
    After all, the word “amazing” generally has the same meaning whether you use it
    to talk about movies or anything else. Moreover, perhaps embeddings would be useful
    for sentiment analysis even if they were trained on another task: since words
    like “awesome” and “amazing” have a similar meaning, they will likely cluster
    in the embedding space even for tasks such as predicting the next word in a sentence.
    If all positive words and all negative words form clusters, then this will be
    helpful for sentiment analysis. So, instead of training word embeddings, we could
    just download and use pretrained embeddings, such as Google’s [Word2vec embeddings](https://homl.info/word2vec),
    Stanford’s [GloVe embeddings](https://homl.info/glove), or Facebook’s [FastText
    embeddings](https://fasttext.cc).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using pretrained word embeddings was popular for several years, but this approach
    has its limits. In particular, a word has a single representation, no matter the
    context. For example, the word “right” is encoded the same way in “left and right”
    and “right and wrong”, even though it means two very different things. To address
    this limitation, a [2018 paper](https://homl.info/elmo)⁠^([10](ch16.html#idm45720175311248))
    by Matthew Peters introduced *Embeddings from Language Models* (ELMo): these are
    contextualized word embeddings learned from the internal states of a deep bidirectional
    language model. Instead of just using pretrained embeddings in your model, you
    reuse part of a pretrained language model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At roughly the same time, the [Universal Language Model Fine-Tuning (ULMFiT)
    paper](https://homl.info/ulmfit)⁠^([11](ch16.html#idm45720175307104)) by Jeremy
    Howard and Sebastian Ruder demonstrated the effectiveness of unsupervised pretraining
    for NLP tasks: the authors trained an LSTM language model on a huge text corpus
    using self-supervised learning (i.e., generating the labels automatically from
    the data), then they fine-tuned it on various tasks. Their model outperformed
    the state of the art on six text classification tasks by a large margin (reducing
    the error rate by 18–24% in most cases). Moreover, the authors showed a pretrained
    model fine-tuned on just 100 labeled examples could achieve the same performance
    as one trained from scratch on 10,000 examples. Before the ULMFiT paper, using
    pretrained models was only the norm in computer vision; in the context of NLP,
    pretraining was limited to word embeddings. This paper marked the beginning of
    a new era in NLP: today, reusing pretrained language models is the norm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s build a classifier based on the Universal Sentence Encoder,
    a model architecture introduced in a [2018 paper](https://homl.info/139)⁠^([12](ch16.html#idm45720175301568))
    by a team of Google researchers. This model is based on the transformer architecture,
    which we will look at later in this chapter. Conveniently, the model is available
    on TensorFlow Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This model is quite large—close to 1 GB in size—so it may take a while to download.
    By default, TensorFlow Hub modules are saved to a temporary directory, and they
    get downloaded again and again every time you run your program. To avoid that,
    you must set the `TFHUB_CACHE_DIR` environment variable to a directory of your
    choice: the modules will then be saved there, and only downloaded once.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the last part of the TensorFlow Hub module URL specifies that we want
    version 4 of the model. This versioning ensures that if a new module version is
    released on TF Hub, it will not break our model. Conveniently, if you just enter
    this URL in a web browser, you will get the documentation for this module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also note that we set `trainable=True` when creating the `hub.KerasLayer`.
    This way, the pretrained Universal Sentence Encoder is fine-tuned during training:
    some of its weights are tweaked via backprop. Not all TensorFlow Hub modules are
    fine-tunable, so make sure to check the documentation for each pretrained module
    you’re interested in.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After training, this model should reach a validation accuracy of over 90%.
    That’s actually really good: if you try to perform the task yourself, you will
    probably do only marginally better since many reviews contain both positive and
    negative comments. Classifying these ambiguous reviews is like flipping a coin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So far we have looked at text generation using a char-RNN, and sentiment analysis
    with word-level RNN models (based on trainable embeddings) and using a powerful
    pretrained language model from TensorFlow Hub. In the next section, we will explore
    another important NLP task: *neural machine translation* (NMT).'
  prefs: []
  type: TYPE_NORMAL
- en: An Encoder–Decoder Network for Neural Machine Translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s begin with a simple [NMT model](https://homl.info/103)⁠^([13](ch16.html#idm45720175133488))
    that will translate English sentences to Spanish (see [Figure 16-3](#machine_translation_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, the architecture is as follows: English sentences are fed as inputs
    to the encoder, and the decoder outputs the Spanish translations. Note that the
    Spanish translations are also used as inputs to the decoder during training, but
    shifted back by one step. In other words, during training the decoder is given
    as input the word that it *should* have output at the previous step, regardless
    of what it actually output. This is called *teacher forcing*—a technique that
    significantly speeds up training and improves the model’s performance. For the
    very first word, the decoder is given the start-of-sequence (SOS) token, and the
    decoder is expected to end the sentence with an end-of-sequence (EOS) token.'
  prefs: []
  type: TYPE_NORMAL
- en: Each word is initially represented by its ID (e.g., `854` for the word “soccer”).
    Next, an `Embedding` layer returns the word embedding. These word embeddings are
    then fed to the encoder and the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: At each step, the decoder outputs a score for each word in the output vocabulary
    (i.e., Spanish), then the softmax activation function turns these scores into
    probabilities. For example, at the first step the word “Me” may have a probability
    of 7%, “Yo” may have a probability of 1%, and so on. The word with the highest
    probability is output. This is very much like a regular classification task, and
    indeed you can train the model using the `"sparse_categorical_crossentropy"` loss,
    much like we did in the char-RNN model.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1603](assets/mls3_1603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-3\. A simple machine translation model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that at inference time (after training), you will not have the target sentence
    to feed to the decoder. Instead, you need to feed it the word that it has just
    output at the previous step, as shown in [Figure 16-4](#inference_decoder_diagram)
    (this will require an embedding lookup that is not shown in the diagram).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a [2015 paper](https://homl.info/scheduledsampling),⁠^([14](ch16.html#idm45720175116736))
    Samy Bengio et al. proposed gradually switching from feeding the decoder the previous
    *target* token to feeding it the previous *output* token during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1604](assets/mls3_1604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-4\. At inference time, the decoder is fed as input the word it just
    output at the previous time step
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s build and train this model! First, we need to download a dataset of English/Spanish
    sentence pairs:⁠^([15](ch16.html#idm45720175112096))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Each line contains an English sentence and the corresponding Spanish translation,
    separated by a tab. We’ll start by removing the Spanish characters “¡” and “¿”,
    which the `TextVectorization` layer doesn’t handle, then we will parse the sentence
    pairs and shuffle them. Finally, we will split them into two separate lists, one
    per language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at the first three sentence pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s create two `TextVectorization` layers—one per language—and adapt
    them to the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few things to note here:'
  prefs: []
  type: TYPE_NORMAL
- en: We limit the vocabulary size to 1,000, which is quite small. That’s because
    the training set is not very large, and because using a small value will speed
    up training. State-of-the-art translation models typically use a much larger vocabulary
    (e.g., 30,000), a much larger training set (gigabytes), and a much larger model
    (hundreds or even thousands of megabytes). For example, check out the Opus-MT
    models by the University of Helsinki, or the M2M-100 model by Facebook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since all sentences in the dataset have a maximum of 50 words, we set `output_sequence_length`
    to 50: this way the input sequences will automatically be padded with zeros until
    they are all 50 tokens long. If there was any sentence longer than 50 tokens in
    the training set, it would be cropped to 50 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the Spanish text, we add “startofseq” and “endofseq” to each sentence when
    adapting the `TextVectorization` layer: we will use these words as SOS and EOS
    tokens. You could use any other words, as long as they are not actual Spanish
    words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s inspect the first 10 tokens in both vocabularies. They start with the
    padding token, the unknown token, the SOS and EOS tokens (only in the Spanish
    vocabulary), then the actual words, sorted by decreasing frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s create the training set and the validation set (you could also
    create a test set if you needed it). We will use the first 100,000 sentence pairs
    for training, and the rest for validation. The decoder’s inputs are the Spanish
    sentences plus an SOS token prefix. The targets are the Spanish sentences plus
    an EOS suffix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'OK, we’re now ready to build our translation model. We will use the functional
    API for that since the model is not sequential. It requires two text inputs—one
    for the encoder and one for the decoder—so let’s start with that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to encode these sentences using the `TextVectorization` layers
    we prepared earlier, followed by an `Embedding` layer for each language, with
    `mask_zero=True` to ensure masking is handled automatically. The embedding size
    is a hyperparameter you can tune, as always:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When the languages share many words, you may get better performance using the
    same embedding layer for both the encoder and the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s create the encoder and pass it the embedded inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To keep things simple, we just used a single `LSTM` layer, but you could stack
    several of them. We also set `return_state=True` to get a reference to the layer’s
    final state. Since we’re using an `LSTM` layer, there are actually two states:
    the short-term state and the long-term state. The layer returns these states separately,
    which is why we had to write `*encoder_state` to group both states in a list.⁠^([16](ch16.html#idm45720174413024))
    Now we can use this (double) state as the initial state of the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can pass the decoder’s outputs through a `Dense` layer with the softmax
    activation function to get the word probabilities for each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And that’s it! We just need to create the Keras `Model`, compile it, and train
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, we can use the model to translate new English sentences to
    Spanish. But it’s not as simple as calling `model.predict()`, because the decoder
    expects as input the word that was predicted at the previous time step. One way
    to do this is to write a custom memory cell that keeps track of the previous output
    and feeds it to the encoder at the next time step. However, to keep things simple,
    we can just call the model multiple times, predicting one extra word at each round.
    Let’s write a little utility function for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The function simply keeps predicting one word at a time, gradually completing
    the translation, and it stops once it reaches the EOS token. Let’s give it a try!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Hurray, it works! Well, at least it does with very short sentences. If you
    try playing with this model for a while, you will find that it’s not bilingual
    yet, and in particular it really struggles with longer sentences. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The translation says “I like soccer and sometimes even the bus”. So how can
    you improve it? One way is to increase the training set size and add more `LSTM`
    layers in both the encoder and the decoder. But this will only get you so far,
    so let’s look at more sophisticated techniques, starting with bidirectional recurrent
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At each time step, a regular recurrent layer only looks at past and present
    inputs before generating its output. In other words, it is *causal*, meaning it
    cannot look into the future. This type of RNN makes sense when forecasting time
    series, or in the decoder of a sequence-to-sequence (seq2seq) model. But for tasks
    like text classification, or in the encoder of a seq2seq model, it is often preferable
    to look ahead at the next words before encoding a given word.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the phrases “the right arm”, “the right person”, and
    “the right to criticize”: to properly encode the word “right”, you need to look
    ahead. One solution is to run two recurrent layers on the same inputs, one reading
    the words from left to right and the other reading them from right to left, then
    combine their outputs at each time step, typically by concatenating them. This
    is what a *bidirectional recurrent layer* does (see [Figure 16-5](#bidirectional_rnn_diagram)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1605](assets/mls3_1605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-5\. A bidirectional recurrent layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To implement a bidirectional recurrent layer in Keras, just wrap a recurrent
    layer in a `tf.keras.layers.Bidirectional` layer. For example, the following `Bidirectional`
    layer could be used as the encoder in our translation model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `Bidirectional` layer will create a clone of the `GRU` layer (but in the
    reverse direction), and it will run both and concatenate their outputs. So although
    the `GRU` layer has 10 units, the `Bidirectional` layer will output 20 values
    per time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s just one problem. This layer will now return four states instead of
    two: the final short-term and long-term states of the forward `LSTM` layer, and
    the final short-term and long-term states of the backward `LSTM` layer. We cannot
    use this quadruple state directly as the initial state of the decoder’s `LSTM`
    layer, since it expects just two states (short-term and long-term). We cannot
    make the decoder bidirectional, since it must remain causal: otherwise it would
    cheat during training and it would not work. Instead, we can concatenate the two
    short-term states, and also concatenate the two long-term states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s look at another popular technique that can greatly improve the performance
    of a translation model at inference time: beam search.'
  prefs: []
  type: TYPE_NORMAL
- en: Beam Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose you have trained an encoder–decoder model, and you use it to translate
    the sentence “I like soccer” to Spanish. You are hoping that it will output the
    proper translation “me gusta el fútbol”, but unfortunately it outputs “me gustan
    los jugadores”, which means “I like the players”. Looking at the training set,
    you notice many sentences such as “I like cars”, which translates to “me gustan
    los autos”, so it wasn’t absurd for the model to output “me gustan los” after
    seeing “I like”. Unfortunately, in this case it was a mistake since “soccer” is
    singular. The model could not go back and fix it, so it tried to complete the
    sentence as best it could, in this case using the word “jugadores”. How can we
    give the model a chance to go back and fix mistakes it made earlier? One of the
    most common solutions is *beam search*: it keeps track of a short list of the
    *k* most promising sentences (say, the top three), and at each decoder step it
    tries to extend them by one word, keeping only the *k* most likely sentences.
    The parameter *k* is called the *beam width*.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose you use the model to translate the sentence “I like soccer”
    using beam search with a beam width of 3 (see [Figure 16-6](#beam_search_diagram)).
    At the first decoder step, the model will output an estimated probability for
    each possible first word in the translated sentence. Suppose the top three words
    are “me” (75% estimated probability), “a” (3%), and “como” (1%). That’s our short
    list so far. Next, we use the model to find the next word for each sentence. For
    the first sentence (“me”), perhaps the model outputs a probability of 36% for
    the word “gustan”, 32% for the word “gusta”, 16% for the word “encanta”, and so
    on. Note that these are actually *conditional* probabilities, given that the sentence
    starts with “me”. For the second sentence (“a”), the model might output a conditional
    probability of 50% for the word “mi”, and so on. Assuming the vocabulary has 1,000
    words, we will end up with 1,000 probabilities per sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the probabilities of each of the 3,000 two-word sentences
    we considered (3 × 1,000). We do this by multiplying the estimated conditional
    probability of each word by the estimated probability of the sentence it completes.
    For example, the estimated probability of the sentence “me” was 75%, while the
    estimated conditional probability of the word “gustan” (given that the first word
    is “me”) was 36%, so the estimated probability of the sentence “me gustan” is
    75% × 36% = 27%. After computing the probabilities of all 3,000 two-word sentences,
    we keep only the top 3\. In this example they all start with the word “me”: “me
    gustan” (27%), “me gusta” (24%), and “me encanta” (12%). Right now, the sentence
    “me gustan” is winning, but “me gusta” has not been eliminated.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1606](assets/mls3_1606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-6\. Beam search, with a beam width of 3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Then we repeat the same process: we use the model to predict the next word
    in each of these three sentences, and we compute the probabilities of all 3,000
    three-word sentences we considered. Perhaps the top three are now “me gustan los”
    (10%), “me gusta el” (8%), and “me gusta mucho” (2%). At the next step we may
    get “me gusta el fútbol” (6%), “me gusta mucho el” (1%), and “me gusta el deporte”
    (0.2%). Notice that “me gustan” was eliminated, and the correct translation is
    now ahead. We boosted our encoder–decoder model’s performance without any extra
    training, simply by using it more wisely.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The TensorFlow Addons library includes a full seq2seq API that lets you build
    encoder–decoder models with attention, including beam search, and more. However,
    its documentation is currently very limited. Implementing beam search is a good
    exercise, so give it a try! Check out this chapter’s notebook for a possible solution.
  prefs: []
  type: TYPE_NORMAL
- en: With all this, you can get reasonably good translations for fairly short sentences.
    Unfortunately, this model will be really bad at translating long sentences. Once
    again, the problem comes from the limited short-term memory of RNNs. *Attention
    mechanisms* are the game-changing innovation that addressed this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the path from the word “soccer” to its translation “fútbol” back in
    [Figure 16-3](#machine_translation_diagram): it is quite long! This means that
    a representation of this word (along with all the other words) needs to be carried
    over many steps before it is actually used. Can’t we make this path shorter?'
  prefs: []
  type: TYPE_NORMAL
- en: This was the core idea in a landmark [2014 paper](https://homl.info/attention)⁠^([18](ch16.html#idm45720173841328))
    by Dzmitry Bahdanau et al., where the authors introduced a technique that allowed
    the decoder to focus on the appropriate words (as encoded by the encoder) at each
    time step. For example, at the time step where the decoder needs to output the
    word “fútbol”, it will focus its attention on the word “soccer”. This means that
    the path from an input word to its translation is now much shorter, so the short-term
    memory limitations of RNNs have much less impact. Attention mechanisms revolutionized
    neural machine translation (and deep learning in general), allowing a significant
    improvement in the state of the art, especially for long sentences (e.g., over
    30 words).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The most common metric used in NMT is the *bilingual evaluation understudy*
    (BLEU) score, which compares each translation produced by the model with several
    good translations produced by humans: it counts the number of *n*-grams (sequences
    of *n* words) that appear in any of the target translations and adjusts the score
    to take into account the frequency of the produced *n*-grams in the target translations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 16-7](#attention_diagram) shows our encoder–decoder model with an added
    attention mechanism. On the left, you have the encoder and the decoder. Instead
    of just sending the encoder’s final hidden state to the decoder, as well as the
    previous target word at each step (which is still done, although it is not shown
    in the figure), we now send all of the encoder’s outputs to the decoder as well.
    Since the decoder cannot deal with all these encoder outputs at once, they need
    to be aggregated: at each time step, the decoder’s memory cell computes a weighted
    sum of all the encoder outputs. This determines which words it will focus on at
    this step. The weight *α*[(*t*,*i*)] is the weight of the *i*^(th) encoder output
    at the *t*^(th) decoder time step. For example, if the weight *α*[(3,2)] is much
    larger than the weights *α*[(3,0)] and *α*[(3,1)], then the decoder will pay much
    more attention to the encoder’s output for word #2 (“soccer”) than to the other
    two outputs, at least at this time step. The rest of the decoder works just like
    earlier: at each time step the memory cell receives the inputs we just discussed,
    plus the hidden state from the previous time step, and finally (although it is
    not represented in the diagram) it receives the target word from the previous
    time step (or at inference time, the output from the previous time step).'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1607](assets/mls3_1607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-7\. Neural machine translation using an encoder–decoder network with
    an attention model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'But where do these *α*[(*t*,*i*)] weights come from? Well, they are generated
    by a small neural network called an *alignment model* (or an *attention layer*),
    which is trained jointly with the rest of the encoder–decoder model. This alignment
    model is illustrated on the righthand side of [Figure 16-7](#attention_diagram).
    It starts with a `Dense` layer composed of a single neuron that processes each
    of the encoder’s outputs, along with the decoder’s previous hidden state (e.g.,
    **h**[(2)]). This layer outputs a score (or energy) for each encoder output (e.g.,
    *e*[(3,] [2)]): this score measures how well each output is aligned with the decoder’s
    previous hidden state. For example, in [Figure 16-7](#attention_diagram), the
    model has already output “me gusta el” (meaning “I like”), so it’s now expecting
    a noun: the word “soccer” is the one that best aligns with the current state,
    so it gets a high score. Finally, all the scores go through a softmax layer to
    get a final weight for each encoder output (e.g., *α*[(3,2)]). All the weights
    for a given decoder time step add up to 1\. This particular attention mechanism
    is called *Bahdanau attention* (named after the 2014 paper’s first author). Since
    it concatenates the encoder output with the decoder’s previous hidden state, it
    is sometimes called *concatenative attention* (or *additive attention*).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the input sentence is *n* words long, and assuming the output sentence is
    about as long, then this model will need to compute about *n*² weights. Fortunately,
    this quadratic computational complexity is still tractable because even long sentences
    don’t have thousands of words.
  prefs: []
  type: TYPE_NORMAL
- en: Another common attention mechanism, known as *Luong attention* or *multiplicative
    attention*, was proposed shortly after, in [2015](https://homl.info/luongattention),⁠^([19](ch16.html#idm45720173812256))
    by Minh-Thang Luong et al. Because the goal of the alignment model is to measure
    the similarity between one of the encoder’s outputs and the decoder’s previous
    hidden state, the authors proposed to simply compute the dot product (see [Chapter 4](ch04.html#linear_models_chapter))
    of these two vectors, as this is often a fairly good similarity measure, and modern
    hardware can compute it very efficiently. For this to be possible, both vectors
    must have the same dimensionality. The dot product gives a score, and all the
    scores (at a given decoder time step) go through a softmax layer to give the final
    weights, just like in Bahdanau attention. Another simplification Luong et al.
    proposed was to use the decoder’s hidden state at the current time step rather
    than at the previous time step (i.e., **h**[(*t*)] rather than **h**[(*t*–1)]),
    then to use the output of the attention mechanism (noted <math><msub><mover><mi
    mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></math>)
    directly to compute the decoder’s predictions, rather than using it to compute
    the decoder’s current hidden state. The researchers also proposed a variant of
    the dot product mechanism where the encoder outputs first go through a fully connected
    layer (without a bias term) before the dot products are computed. This is called
    the “general” dot product approach. The researchers compared both dot product
    approaches with the concatenative attention mechanism (adding a rescaling parameter
    vector **v**), and they observed that the dot product variants performed better
    than concatenative attention. For this reason, concatenative attention is much
    less used now. The equations for these three attention mechanisms are summarized
    in [Equation 16-1](#attention_mechanisms_equation).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 16-1\. Attention mechanisms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><msub><mover><mi mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mtext> with </mtext><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub></mfenced></mrow><mrow><mstyle
    displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>'</mo></mrow></munder></mstyle><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>'</mo><mo>)</mo></mrow></msub></mfenced></mrow></mfrac><mtext> and </mtext><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfenced
    open="{" close=""><mtable columnalign="left"><mtr><mtd><msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>d</mi><mi>o</mi><mi>t</mi></mtd></mtr><mtr><mtd><msup><msub><mi
    mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><mi
    mathvariant="bold">W</mi><msub><mi mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>l</mi></mtd></mtr><mtr><mtd><msup><mi
    mathvariant="bold">v</mi><mo>⊺</mo></msup><mi>tanh</mi><mo>(</mo><mi mathvariant="bold">W</mi><mo>[</mo><msub><mi
    mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>;</mo><msub><mi
    mathvariant="bold">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>]</mo><mo>)</mo></mtd><mtd><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi></mtd></mtr></mtable></mfenced></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras provides a `tf.keras.layers.Attention` layer for Luong attention, and
    an `AdditiveAttention` layer for Bahdanau attention. Let’s add Luong attention
    to our encoder–decoder model. Since we will need to pass all the encoder’s outputs
    to the `Attention` layer, we first need to set `return_sequences=True` when creating
    the encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create the attention layer and pass it the decoder’s states
    and the encoder’s outputs. However, to access the decoder’s states at each step
    we would need to write a custom memory cell. For simplicity, let’s use the decoder’s
    outputs instead of its states: in practice this works well too, and it’s much
    easier to code. Then we just pass the attention layer’s outputs directly to the
    output layer, as suggested in the Luong attention paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'And that’s it! If you train this model, you will find that it now handles much
    longer sentences. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In short, the attention layer provides a way to focus the attention of the
    model on part of the inputs. But there’s another way to think of this layer: it
    acts as a differentiable memory retrieval mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s suppose the encoder analyzed the input sentence “I like
    soccer”, and it managed to understand that the word “I” is the subject and the
    word “like” is the verb, so it encoded this information in its outputs for these
    words. Now suppose the decoder has already translated the subject, and it thinks
    that it should translate the verb next. For this, it needs to fetch the verb from
    the input sentence. This is analogous to a dictionary lookup: it’s as if the encoder
    had created a dictionary {"subject”: “They”, “verb”: “played”, …​} and the decoder
    wanted to look up the value that corresponds to the key “verb”.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the model does not have discrete tokens to represent the keys (like
    “subject” or “verb”); instead, it has vectorized representations of these concepts
    that it learned during training, so the query it will use for the lookup will
    not perfectly match any key in the dictionary. The solution is to compute a similarity
    measure between the query and each key in the dictionary, and then use the softmax
    function to convert these similarity scores to weights that add up to 1\. As we
    saw earlier, that’s exactly what the attention layer does. If the key that represents
    the verb is by far the most similar to the query, then that key’s weight will
    be close to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the attention layer computes a weighted sum of the corresponding values:
    if the weight of the “verb” key is close to 1, then the weighted sum will be very
    close to the representation of the word “played”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why the Keras `Attention` and `AdditiveAttention` layers both expect
    a list as input, containing two or three items: the *queries*, the *keys*, and
    optionally the *values*. If you do not pass any values, then they are automatically
    equal to the keys. So, looking at the previous code example again, the decoder
    outputs are the queries, and the encoder outputs are both the keys and the values.
    For each decoder output (i.e., each query), the attention layer returns a weighted
    sum of the encoder outputs (i.e., the keys/values) that are most similar to the
    decoder output.'
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is that an attention mechanism is a trainable memory retrieval
    system. It is so powerful that you can actually build state-of-the-art models
    using only attention mechanisms. Enter the transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention Is All You Need: The Original Transformer Architecture'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a groundbreaking [2017 paper](https://homl.info/transformer),⁠^([20](ch16.html#idm45720173607840))
    a team of Google researchers suggested that “Attention Is All You Need”. They
    created an architecture called the *transformer*, which significantly improved
    the state-of-the-art in NMT without using any recurrent or convolutional layers,⁠^([21](ch16.html#idm45720173604064))
    just attention mechanisms (plus embedding layers, dense layers, normalization
    layers, and a few other bits and pieces). Because the model is not recurrent,
    it doesn’t suffer as much from the vanishing or exploding gradients problems as
    RNNs, it can be trained in fewer steps, it’s easier to parallelize across multiple
    GPUs, and it can better capture long-range patterns than RNNs. The original 2017
    transformer architecture is represented in [Figure 16-8](#transformer_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: In short, the left part of [Figure 16-8](#transformer_diagram) is the encoder,
    and the right part is the decoder. Each embedding layer outputs a 3D tensor of
    shape [*batch size*, *sequence length*, *embedding size*]. After that, the tensors
    are gradually transformed as they flow through the transformer, but their shape
    remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1608](assets/mls3_1608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-8\. The original 2017 transformer architecture⁠^([22](ch16.html#idm45720173579408))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you use the transformer for NMT, then during training you must feed the English
    sentences to the encoder and the corresponding Spanish translations to the decoder,
    with an extra SOS token inserted at the start of each sentence. At inference time,
    you must call the transformer multiple times, producing the translations one word
    at a time and feeding the partial translations to the decoder at each round, just
    like we did earlier in the `translate()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder’s role is to gradually transform the inputs—word representations
    of the English sentence—until each word’s representation perfectly captures the
    meaning of the word, in the context of the sentence. For example, if you feed
    the encoder with the sentence “I like soccer”, then the word “like” will start
    off with a rather vague representation, since this word could mean different things
    in different contexts: think of “I like soccer” versus “It’s like that”. But after
    going through the encoder, the word’s representation should capture the correct
    meaning of “like” in the given sentence (i.e., to be fond of), as well as any
    other information that may be required for translation (e.g., it’s a verb).'
  prefs: []
  type: TYPE_NORMAL
- en: The decoder’s role is to gradually transform each word representation in the
    translated sentence into a word representation of the next word in the translation.
    For example, if the sentence to translate is “I like soccer”, and the decoder’s
    input sentence is “<SOS> me gusta el fútbol”, then after going through the decoder,
    the word representation of the word “el” will end up transformed into a representation
    of the word “fútbol”. Similarly, the representation of the word “fútbol” will
    be transformed into a representation of the EOS token.
  prefs: []
  type: TYPE_NORMAL
- en: After going through the decoder, each word representation goes through a final
    `Dense` layer with a softmax activation function, which will hopefully output
    a high probability for the correct next word and a low probability for all other
    words. The predicted sentence should be “me gusta el fútbol <EOS>”.
  prefs: []
  type: TYPE_NORMAL
- en: 'That was the big picture; now let’s walk through [Figure 16-8](#transformer_diagram)
    in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: First, notice that both the encoder and the decoder contain modules that are
    stacked *N* times. In the paper, *N* = 6\. The final outputs of the whole encoder
    stack are fed to the decoder at each of these *N* levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zooming in, you can see that you are already familiar with most components:
    there are two embedding layers; several skip connections, each of them followed
    by a layer normalization layer; several feedforward modules that are composed
    of two dense layers each (the first one using the ReLU activation function, the
    second with no activation function); and finally the output layer is a dense layer
    using the softmax activation function. You can also sprinkle a bit of dropout
    after the attention layers and the feedforward modules, if needed. Since all of
    these layers are time-distributed, each word is treated independently from all
    the others. But how can we translate a sentence by looking at the words completely
    separately? Well, we can’t, so that’s where the new components come in:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder’s *multi-head attention* layer updates each word representation
    by attending to (i.e., paying attention to) all other words in the same sentence.
    That’s where the vague representation of the word “like” becomes a richer and
    more accurate representation, capturing its precise meaning in the given sentence.
    We will discuss exactly how this works shortly.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The decoder’s *masked multi-head attention* layer does the same thing, but
    when it processes a word, it doesn’t attend to words located after it: it’s a
    causal layer. For example, when it processes the word “gusta”, it only attends
    to the words “<SOS> me gusta”, and it ignores the words “el fútbol” (or else that
    would be cheating).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The decoder’s upper *multi-head attention* layer is where the decoder pays attention
    to the words in the English sentence. This is called *cross*-attention, not *self*-attention
    in this case. For example, the decoder will probably pay close attention to the
    word “soccer” when it processes the word “el” and transforms its representation
    into a representation of the word “fútbol”.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *positional encodings* are dense vectors (much like word embeddings) that
    represent the position of each word in the sentence. The *n*^(th) positional encoding
    is added to the word embedding of the *n*^(th) word in each sentence. This is
    needed because all layers in the transformer architecture ignore word positions:
    without positional encodings, you could shuffle the input sequences, and it would
    just shuffle the output sequences in the same way. Obviously, the order of words
    matters, which is why we need to give positional information to the transformer
    somehow: adding positional encodings to the word representations is a good way
    to achieve this.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The first two arrows going into each multi-head attention layer in [Figure 16-8](#transformer_diagram)
    represent the keys and values, and the third arrow represents the queries. In
    the self-attention layers, all three are equal to the word representations output
    by the previous layer, while in the decoder’s upper attention layer, the keys
    and values are equal to the encoder’s final word representations, and the queries
    are equal to the word representations output by the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through the novel components of the transformer architecture in more
    detail, starting with the positional encodings.
  prefs: []
  type: TYPE_NORMAL
- en: Positional encodings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A positional encoding is a dense vector that encodes the position of a word
    within a sentence: the *i*^(th) positional encoding is added to the word embedding
    of the *i*^(th) word in the sentence. The easiest way to implement this is to
    use an `Embedding` layer and make it encode all the positions from 0 to the maximum
    sequence length in the batch, then add the result to the word embeddings. The
    rules of broadcasting will ensure that the positional encodings get applied to
    every input sequence. For example, here is how to add positional encodings to
    the encoder and decoder inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Note that this implementation assumes that the embeddings are represented as
    regular tensors, not ragged tensors.^([23](ch16.html#idm45720173475360)) The encoder
    and the decoder share the same `Embedding` layer for the positional encodings,
    since they have the same embedding size (this is often the case).
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using trainable positional encodings, the authors of the transformer
    paper chose to use fixed positional encodings, based on the sine and cosine functions
    at different frequencies. The positional encoding matrix **P** is defined in [Equation
    16-2](#positional_encodings_equation) and represented at the top of [Figure 16-9](#positional_encodings_diagram)
    (transposed), where *P*[*p*,*i*] is the *i*^(th) component of the encoding for
    the word located at the *p*^(th) position in the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 16-2\. Sine/cosine positional encodings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><msub><mi>P</mi><mrow><mi>p</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><mfenced
    open="{" close=""><mtable columnalign="left center"><mtr><mtd><mi>sin</mi><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mi>i</mi><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>i</mi><mtext> is
    even</mtext></mtd></mtr><mtr><mtd><mi>cos</mi><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mo>(</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>)</mo><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>i</mi><mtext> is
    odd</mtext></mtd></mtr></mtable></mfenced></math>![mls3 1609](assets/mls3_1609.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16-9\. Sine/cosine positional encoding matrix (transposed, top) with
    a focus on two values of *i* (bottom)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This solution can give the same performance as trainable positional encodings,
    and it can extend to arbitrarily long sentences without adding any parameters
    to the model (however, when there is a large amount of pretraining data, trainable
    positional encodings are usually favored). After these positional encodings are
    added to the word embeddings, the rest of the model has access to the absolute
    position of each word in the sentence because there is a unique positional encoding
    for each position (e.g., the positional encoding for the word located at the 22nd
    position in a sentence is represented by the vertical dashed line at the top left
    of [Figure 16-9](#positional_encodings_diagram), and you can see that it is unique
    to that position). Moreover, the choice of oscillating functions (sine and cosine)
    makes it possible for the model to learn relative positions as well. For example,
    words located 38 words apart (e.g., at positions *p* = 22 and *p* = 60) always
    have the same positional encoding values in the encoding dimensions *i* = 100
    and *i* = 101, as you can see in [Figure 16-9](#positional_encodings_diagram).
    This explains why we need both the sine and the cosine for each frequency: if
    we only used the sine (the blue wave at *i* = 100), the model would not be able
    to distinguish positions *p* = 22 and *p* = 35 (marked by a cross).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no `PositionalEncoding` layer in TensorFlow, but it is not too hard
    to create one. For efficiency reasons, we precompute the positional encoding matrix
    in the constructor. The `call()` method just truncates this encoding matrix to
    the max length of the input sequences, and it adds them to the inputs. We also
    set `supports_masking=True` to propagate the input’s automatic mask to the next
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use this layer to add the positional encoding to the encoder’s inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s look deeper into the heart of the transformer model, at the multi-head
    attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-head attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand how a multi-head attention layer works, we must first understand
    the *scaled dot-product attention* layer, which it is based on. Its equation is
    shown in [Equation 16-3](#scaled_dot_product_attention), in a vectorized form.
    It’s the same as Luong attention, except for a scaling factor.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 16-3\. Scaled dot-product attention
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mo>Attention</mo><mrow><mo>(</mo><mi mathvariant="bold">Q</mi><mo>,</mo><mi
    mathvariant="bold">K</mi><mo>,</mo><mi mathvariant="bold">V</mi><mo>)</mo></mrow><mo>=</mo><mo>softmax</mo><mfenced><mfrac><mrow><msup><mi
    mathvariant="bold">QK</mi><mo>⊺</mo></msup></mrow><msqrt><msub><mi>d</mi><mrow><mi>k</mi><mi>e</mi><mi>y</mi><mi>s</mi></mrow></msub></msqrt></mfrac></mfenced><mi
    mathvariant="bold">V</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q** is a matrix containing one row per *query*. Its shape is [*n*[queries],
    *d*[keys]], where *n*[queries] is the number of queries and *d*[keys] is the number
    of dimensions of each query and each key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K** is a matrix containing one row per *key*. Its shape is [*n*[keys], *d*[keys]],
    where *n*[keys] is the number of keys and values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V** is a matrix containing one row per *value*. Its shape is [*n*[keys],
    *d*[values]], where *d*[values] is the number of dimensions of each value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The shape of **Q** **K**^⊺ is [*n*[queries], *n*[keys]]: it contains one similarity
    score for each query/key pair. To prevent this matrix from being huge, the input
    sequences must not be too long (we will discuss how to overcome this limitation
    later in this chapter). The output of the softmax function has the same shape,
    but all rows sum up to 1\. The final output has a shape of [*n*[queries], *d*[values]]:
    there is one row per query, where each row represents the query result (a weighted
    sum of the values).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scaling factor 1 / (<math><msqrt><msub><mi>d</mi> <mrow><mi>keys</mi></mrow></msub></msqrt></math>)
    scales down the similarity scores to avoid saturating the softmax function, which
    would lead to tiny gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to mask out some key/value pairs by adding a very large negative
    value to the corresponding similarity scores, just before computing the softmax.
    This is useful in the masked multi-head attention layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you set `use_scale=True` when creating a `tf.keras.layers.Attention` layer,
    then it will create an additional parameter that lets the layer learn how to properly
    downscale the similarity scores. The scaled dot-product attention used in the
    transformer model is almost the same, except it always scales the similarity scores
    by the same factor, 1 / (<math><msqrt><msub><mi>d</mi> <mrow><mi>keys</mi></mrow></msub></msqrt></math>).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the `Attention` layer’s inputs are just like **Q**, **K**, and **V**,
    except with an extra batch dimension (the first dimension). Internally, the layer
    computes all the attention scores for all sentences in the batch with just one
    call to `tf.matmul(queries, keys)`: this makes it extremely efficient. Indeed,
    in TensorFlow, if `A` and `B` are tensors with more than two dimensions—say, of
    shape [2, 3, 4, 5] and [2, 3, 5, 6], respectively—then `tf.matmul(A, B)` will
    treat these tensors as 2 × 3 arrays where each cell contains a matrix, and it
    will multiply the corresponding matrices: the matrix at the *i*^(th) row and *j*^(th)
    column in `A` will be multiplied by the matrix at the *i*^(th) row and *j*^(th)
    column in `B`. Since the product of a 4 × 5 matrix with a 5 × 6 matrix is a 4 × 6
    matrix, `tf.matmul(A, B)` will return an array of shape [2, 3, 4, 6].'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’re ready to look at the multi-head attention layer. Its architecture
    is shown in [Figure 16-10](#multihead_attention_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1610](assets/mls3_1610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-10\. Multi-head attention layer architecture⁠^([24](ch16.html#idm45720173058848))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, it is just a bunch of scaled dot-product attention layers, each
    preceded by a linear transformation of the values, keys, and queries (i.e., a
    time-distributed dense layer with no activation function). All the outputs are
    simply concatenated, and they go through a final linear transformation (again,
    time-distributed).
  prefs: []
  type: TYPE_NORMAL
- en: But why? What is the intuition behind this architecture? Well, consider once
    again the word “like” in the sentence “I like soccer”. The encoder was smart enough
    to encode the fact that it is a verb. But the word representation also includes
    its position in the text, thanks to the positional encodings, and it probably
    includes many other features that are useful for its translation, such as the
    fact that it is in the present tense. In short, the word representation encodes
    many different characteristics of the word. If we just used a single scaled dot-product
    attention layer, we would only be able to query all of these characteristics in
    one shot.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why the multi-head attention layer applies *multiple* different linear
    transformations of the values, keys, and queries: this allows the model to apply
    many different projections of the word representation into different subspaces,
    each focusing on a subset of the word’s characteristics. Perhaps one of the linear
    layers will project the word representation into a subspace where all that remains
    is the information that the word is a verb, another linear layer will extract
    just the fact that it is present tense, and so on. Then the scaled dot-product
    attention layers implement the lookup phase, and finally we concatenate all the
    results and project them back to the original space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras includes a `tf.keras.layers.MultiHeadAttention` layer, so we now have
    everything we need to build the rest of the transformer. Let’s start with the
    full encoder, which is exactly like in [Figure 16-8](#transformer_diagram), except
    we use a stack of two blocks (`N = 2`) instead of six, since we don’t have a huge
    training set, and we add a bit of dropout as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This code should be mostly straightforward, except for one thing: masking.
    As of the time of writing, the `MultiHeadAttention` layer does not support automatic
    masking,⁠^([25](ch16.html#idm45720173048224)) so we must handle it manually. How
    can we do that?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `MultiHeadAttention` layer accepts an `attention_mask` argument, which
    is a Boolean tensor of shape [*batch size*, *max query length*, *max value length*]:
    for every token in every query sequence, this mask indicates which tokens in the
    corresponding value sequence should be attended to. We want to tell the `MultiHeadAttention`
    layer to ignore all the padding tokens in the values. So, we first compute the
    padding mask using `tf.math.not_equal(encoder_input_ids, 0)`. This returns a Boolean
    tensor of shape [*batch size*, *max sequence length*]. We then insert a second
    axis using `[:, tf.newaxis]`, to get a mask of shape [*batch size*, 1, *max sequence
    length*]. This allows us to use this mask as the `attention_mask` when calling
    the `MultiHead​Atten⁠tion` layer: thanks to broadcasting, the same mask will be
    used for all tokens in each query. This way, the padding tokens in the values
    will be ignored correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the layer will compute outputs for every single query token, including
    the padding tokens. We need to mask the outputs that correspond to these padding
    tokens. Recall that we used `mask_zero` in the `Embedding` layers, and we set
    `supports_masking` to `True` in the `PositionalEncoding` layer, so the automatic
    mask was propagated all the way to the `MultiHeadAttention` layer’s inputs (`encoder_in`).
    We can use this to our advantage in the skip connection: indeed, the `Add` layer
    supports automatic masking, so when we add `Z` and `skip` (which is initially
    equal to `encoder_in`), the outputs get automatically masked correctly.^([26](ch16.html#idm45720172796192))
    Yikes! Masking required much more explanation than code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now on to the decoder! Once again, masking is going to be the only tricky part,
    so let’s start with that. The first multi-head attention layer is a self-attention
    layer, like in the encoder, but it is a *masked* multi-head attention layer, meaning
    it is causal: it should ignore all tokens in the future. So, we need two masks:
    a padding mask and a causal mask. Let’s create them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The padding mask is exactly like the one we created for the encoder, except
    it’s based on the decoder’s inputs rather than the encoder’s. The causal mask
    is created using the `tf.linalg.band_part()` function, which takes a tensor and
    returns a copy with all the values outside a diagonal band set to zero. With these
    arguments, we get a square matrix of size `batch_max_len_dec` (the max length
    of the input sequences in the batch), with 1s in the lower-left triangle and 0s
    in the upper right. If we use this mask as the attention mask, we will get exactly
    what we want: the first query token will only attend to the first value token,
    the second will only attend to the first two, the third will only attend to the
    first three, and so on. In other words, query tokens cannot attend to any value
    token in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now build the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'For the first attention layer, we use `causal_mask & decoder_pad_mask` to mask
    both the padding tokens and future tokens. The causal mask only has two dimensions:
    it’s missing the batch dimension, but that’s okay since broadcasting ensures that
    it gets copied across all the instances in the batch.'
  prefs: []
  type: TYPE_NORMAL
- en: For the second attention layer, there’s nothing special. The only thing to note
    is that we are using `encoder_pad_mask`, not `decoder_pad_mask`, because this
    attention layer uses the encoder’s final outputs as its values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re almost done. We just need to add the final output layer, create the model,
    compile it, and train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You’ve built a full transformer from scratch, and trained it
    for automatic translation. This is getting quite advanced!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Keras team has created a new [Keras NLP project](https://github.com/keras-team/keras-nlp),
    including an API to build a transformer more easily. You may also be interested
    in the new [Keras CV project for computer vision](https://github.com/keras-team/keras-cv).
  prefs: []
  type: TYPE_NORMAL
- en: But the field didn’t stop there. Let’s now explore some of the recent advances.
  prefs: []
  type: TYPE_NORMAL
- en: An Avalanche of Transformer Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The year 2018 has been called the “ImageNet moment for NLP”. Since then, progress
    has been astounding, with larger and larger transformer-based architectures trained
    on immense datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the [GPT paper](https://homl.info/gpt)⁠^([27](ch16.html#idm45720172319856))
    by Alec Radford and other OpenAI researchers once again demonstrated the effectiveness
    of unsupervised pretraining, like the ELMo and ULMFiT papers before it, but this
    time using a transformer-like architecture. The authors pretrained a large but
    fairly simple architecture composed of a stack of 12 transformer modules using
    only masked multi-head attention layers, like in the original transformer’s decoder.
    They trained it on a very large dataset, using the same autoregressive technique
    we used for our Shakespearean char-RNN: just predict the next token. This is a
    form of self-supervised learning. Then they fine-tuned it on various language
    tasks, using only minor adaptations for each task. The tasks were quite diverse:
    they included text classification, *entailment* (whether sentence A imposes, involves,
    or implies sentence B as a necessary consequence),⁠^([28](ch16.html#idm45720172314768))
    similarity (e.g., “Nice weather today” is very similar to “It is sunny”), and
    question answering (given a few paragraphs of text giving some context, the model
    must answer some multiple-choice questions).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then Google’s [BERT paper](https://homl.info/bert)⁠^([29](ch16.html#idm45720172313104))
    came out: it also demonstrated the effectiveness of self-supervised pretraining
    on a large corpus, using a similar architecture to GPT but with nonmasked multi-head
    attention layers only, like in the original transformer’s encoder. This means
    that the model is naturally bidirectional; hence the B in BERT (*Bidirectional
    Encoder Representations from Transformers*). Most importantly, the authors proposed
    two pretraining tasks that explain most of the model’s strength:'
  prefs: []
  type: TYPE_NORMAL
- en: Masked language model (MLM)
  prefs: []
  type: TYPE_NORMAL
- en: Each word in a sentence has a 15% probability of being masked, and the model
    is trained to predict the masked words. For example, if the original sentence
    is “She had fun at the birthday party”, then the model may be given the sentence
    “She <mask> fun at the <mask> party” and it must predict the words “had” and “birthday”
    (the other outputs will be ignored). To be more precise, each selected word has
    an 80% chance of being masked, a 10% chance of being replaced by a random word
    (to reduce the discrepancy between pretraining and fine-tuning, since the model
    will not see <mask> tokens during fine-tuning), and a 10% chance of being left
    alone (to bias the model toward the correct answer).
  prefs: []
  type: TYPE_NORMAL
- en: Next sentence prediction (NSP)
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained to predict whether two sentences are consecutive or not.
    For example, it should predict that “The dog sleeps” and “It snores loudly” are
    consecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun” are
    not consecutive. Later research showed that NSP was not as important as was initially
    thought, so it was dropped in most later architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is trained on these two tasks simultaneously (see [Figure 16-11](#bert_diagram)).
    For the NSP task, the authors inserted a class token (<CLS>) at the start of every
    input, and the corresponding output token represents the model’s prediction: sentence
    B follows sentence A, or it does not. The two input sentences are concatenated,
    separated only by a special separation token (<SEP>), and they are fed as input
    to the model. To help the model know which sentence each input token belongs to,
    a *segment embedding* is added on top of each token’s positional embeddings: there
    are just two possible segment embeddings, one for sentence A and one for sentence
    B. For the MLM task, some input words are masked (as we just saw) and the model
    tries to predict what those words were. The loss is only computed on the NSP prediction
    and the masked tokens, not on the unmasked ones.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1611](assets/mls3_1611.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-11\. BERT training and fine-tuning process⁠^([30](ch16.html#idm45720172299632))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After this unsupervised pretraining phase on a very large corpus of text, the
    model is then fine-tuned on many different tasks, changing very little for each
    task. For example, for text classification such as sentiment analysis, all output
    tokens are ignored except for the first one, corresponding to the class token,
    and a new output layer replaces the previous one, which was just a binary classification
    layer for NSP.
  prefs: []
  type: TYPE_NORMAL
- en: 'In February 2019, just a few months after BERT was published, Alec Radford,
    Jeffrey Wu, and other OpenAI researchers published the [GPT-2 paper](https://homl.info/gpt2),⁠^([31](ch16.html#idm45720172296288))
    which proposed a very similar architecture to GPT, but larger still (with over
    1.5 billion parameters!). The researchers showed that the new and improved GPT
    model could perform *zero-shot learning* (ZSL), meaning it could achieve good
    performance on many tasks without any fine-tuning. This was just the start of
    a race toward larger and larger models: Google’s [Switch Transformers](https://homl.info/switch)⁠^([32](ch16.html#idm45720172293488))
    (introduced in January 2021) used 1 trillion parameters, and soon much larger
    models came out, such as the Wu Dao 2.0 model by the Beijing Academy of Artificial
    Intelligence (BAII), announced in June 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An unfortunate consequence of this trend toward gigantic models is that only
    well-funded organizations can afford to train such models: it can easily cost
    hundreds of thousands of dollars or more. And the energy required to train a single
    model corresponds to an American household’s electricity consumption for several
    years; it’s not eco-friendly at all. Many of these models are just too big to
    even be used on regular hardware: they wouldn’t fit in RAM, and they would be
    horribly slow. Lastly, some are so costly that they are not released publicly.'
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, ingenious researchers are finding new ways to downsize transformers
    and make them more data-efficient. For example, the [DistilBERT model](https://homl.info/distilbert),⁠^([33](ch16.html#idm45720172289264))
    introduced in October 2019 by Victor Sanh et al. from Hugging Face, is a small
    and fast transformer model based on BERT. It is available on Hugging Face’s excellent
    model hub, along with thousands of others—you’ll see an example later in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'DistilBERT was trained using *distillation* (hence the name): this means transferring
    knowledge from a teacher model to a student one, which is usually much smaller
    than the teacher model. This is typically done by using the teacher’s predicted
    probabilities for each training instance as targets for the student. Surprisingly,
    distillation often works better than training the student from scratch on the
    same dataset as the teacher! Indeed, the student benefits from the teacher’s more
    nuanced labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many more transformer architectures came out after BERT, almost on a monthly
    basis, often improving on the state of the art across all NLP tasks: XLNet (June
    2019), RoBERTa (July 2019), StructBERT (August 2019), ALBERT (September 2019),
    T5 (October 2019), ELECTRA (March 2020), GPT3 (May 2020), DeBERTa (June 2020),
    Switch Transformers (January 2021), Wu Dao 2.0 (June 2021), Gopher (December 2021),
    GPT-NeoX-20B (February 2022), Chinchilla (March 2022), OPT (May 2022), and the
    list goes on and on. Each of these models brought new ideas and techniques,^([34](ch16.html#idm45720172285632))
    but I particularly like the [T5 paper](https://homl.info/t5)⁠^([35](ch16.html#idm45720172282624))
    by Google researchers: it frames all NLP tasks as text-to-text, using an encoder–decoder
    transformer. For example, to translate “I like soccer” to Spanish, you can just
    call the model with the input sentence “translate English to Spanish: I like soccer”
    and it outputs “me gusta el fútbol”. To summarize a paragraph, you just enter
    “summarize:” followed by the paragraph, and it outputs the summary. For classification,
    you only need to change the prefix to “classify:” and the model outputs the class
    name, as text. This simplifies using the model, and it also makes it possible
    to pretrain it on even more tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, in April 2022, Google researchers used a new large-scale
    training platform named *Pathways* (which we will briefly discuss in [Chapter 19](ch19.html#deployment_chapter))
    to train a humongous language model named the [*Pathways Language Model* (PaLM)](https://homl.info/palm),⁠^([36](ch16.html#idm45720172277824))
    with a whopping 540 billion parameters, using over 6,000 TPUs. Other than its
    incredible size, this model is a standard transformer, using decoders only (i.e.,
    with masked multi-head attention layers), with just a few tweaks (see the paper
    for details). This model achieved incredible performance on all sorts of NLP tasks,
    particularly in natural language understanding (NLU). It’s capable of impressive
    feats, such as explaining jokes, giving detailed step-by-step answers to questions,
    and even coding. This is in part due to the model’s size, but also thanks to a
    technique called [*Chain of thought prompting*](https://homl.info/ctp),⁠^([37](ch16.html#idm45720172273920))
    which was introduced a couple months earlier by another team of Google researchers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In question answering tasks, regular prompting typically includes a few examples
    of questions and answers, such as: “Q: Roger has 5 tennis balls. He buys 2 more
    cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does
    he have now? A: 11.” The prompt then continues with the actual question, such
    as “Q: John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take
    care of their business. How many hours a week does he spend taking care of dogs?
    A:”, and the model’s job is to append the answer: in this case, “35.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'But with chain of thought prompting, the example answers include all the reasoning
    steps that lead to the conclusion. For example, instead of “A: 11”, the prompt
    contains “A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis
    balls. 5 + 6 = 11.” This encourages the model to give a detailed answer to the
    actual question, such as “John takes care of 10 dogs. Each dog takes .5 hours
    a day to walk and take care of their business. So that is 10 × .5 = 5 hours a
    day. 5 hours a day × 7 days a week = 35 hours a week. The answer is 35 hours a
    week.” This is an actual example from the paper!'
  prefs: []
  type: TYPE_NORMAL
- en: Not only does the model give the right answer much more frequently than using
    regular prompting—we’re encouraging the model to think things through—but it also
    provides all the reasoning steps, which can be useful to better understand the
    rationale behind a model’s answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers have taken over NLP, but they didn’t stop there: they soon expanded
    to computer vision as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Vision Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the first applications of attention mechanisms beyond NMT was in generating
    image captions using [visual attention](https://homl.info/visualattention):⁠^([38](ch16.html#idm45720172266576))
    a convolutional neural network first processes the image and outputs some feature
    maps, then a decoder RNN equipped with an attention mechanism generates the caption,
    one word at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each decoder time step (i.e., each word), the decoder uses the attention
    model to focus on just the right part of the image. For example, in [Figure 16-12](#visual_attention_diagram),
    the model generated the caption “A woman is throwing a frisbee in a park”, and
    you can see what part of the input image the decoder focused its attention on
    when it was about to output the word “frisbee”: clearly, most of its attention
    was focused on the frisbee.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1612](assets/mls3_1612.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-12\. Visual attention: an input image (left) and the model’s focus
    before producing the word “frisbee” (right)⁠^([39](ch16.html#idm45720172255968))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When transformers came out in 2017 and people started to experiment with them
    beyond NLP, they were first used alongside CNNs, without replacing them. Instead,
    transformers were generally used to replace RNNs, for example, in image captioning
    models. Transformers became slightly more visual in a [2020 paper](https://homl.info/detr)^([41](ch16.html#idm45720172246864))
    by Facebook researchers, which proposed a hybrid CNN–transformer architecture
    for object detection. Once again, the CNN first processes the input images and
    outputs a set of feature maps, then these feature maps are converted to sequences
    and fed to a transformer, which outputs bounding box predictions. But again, most
    of the visual work is still done by the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in October 2020, a team of Google researchers released [a paper](https://homl.info/vit)⁠^([42](ch16.html#idm45720172244672))
    that introduced a fully transformer-based vision model, called a *vision transformer*
    (ViT). The idea is surprisingly simple: just chop the image into little 16 × 16
    squares, and treat the sequence of squares as if it were a sequence of word representations.
    To be more precise, the squares are first flattened into 16 × 16 × 3 = 768-dimensional
    vectors—the 3 is for the RGB color channels—then these vectors go through a linear
    layer that transforms them but retains their dimensionality. The resulting sequence
    of vectors can then be treated just like a sequence of word embeddings: this means
    adding positional embeddings, and passing the result to the transformer. That’s
    it! This model beat the state of the art on ImageNet image classification, but
    to be fair the authors had to use over 300 million additional images for training.
    This makes sense since transformers don’t have as many *inductive biases* as convolution
    neural nets, so they need extra data just to learn things that CNNs implicitly
    assume.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An inductive bias is an implicit assumption made by the model, due to its architecture.
    For example, linear models implicitly assume that the data is, well, linear. CNNs
    implicitly assume that patterns learned in one location will likely be useful
    in other locations as well. RNNs implicitly assume that the inputs are ordered,
    and that recent tokens are more important than older ones. The more inductive
    biases a model has, assuming they are correct, the less training data the model
    will require. But if the implicit assumptions are wrong, then the model may perform
    poorly even if it is trained on a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Just two months later, a team of Facebook researchers released [a paper](https://homl.info/deit)⁠^([43](ch16.html#idm45720172239280))
    that introduced *data-efficient image transformers* (DeiTs). Their model achieved
    competitive results on ImageNet without requiring any additional data for training.
    The model’s architecture is virtually the same as the original ViT, but the authors
    used a distillation technique to transfer knowledge from state-of-the-art CNN
    models to their model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in March 2021, DeepMind released an important [paper](https://homl.info/perceiver)⁠^([44](ch16.html#idm45720172235056))
    that introduced the *Perceiver* architecture. It is a *multimodal* transformer,
    meaning you can feed it text, images, audio, or virtually any other modality.
    Until then, transformers had been restricted to fairly short sequences because
    of the performance and RAM bottleneck in the attention layers. This excluded modalities
    such as audio or video, and it forced researchers to treat images as sequences
    of patches, rather than sequences of pixels. The bottleneck is due to self-attention,
    where every token must attend to every other token: if the input sequence has
    *M* tokens, then the attention layer must compute an *M* × *M* matrix, which can
    be huge if *M* is very large. The Perceiver solves this problem by gradually improving
    a fairly short *latent representation* of the inputs, composed of *N* tokens—typically
    just a few hundred. (The word *latent* means hidden, or internal.) The model uses
    cross-attention layers only, feeding them the latent representation as the queries,
    and the (possibly large) inputs as the values. This only requires computing an
    *M* × *N* matrix, so the computational complexity is linear with regard to *M*,
    instead of quadratic. After going through several cross-attention layers, if everything
    goes well, the latent representation ends up capturing everything that matters
    in the inputs. The authors also suggested sharing the weights between consecutive
    cross-attention layers: if you do that, then the Perceiver effectively becomes
    an RNN. Indeed, the shared cross-attention layers can be seen as the same memory
    cell at different time steps, and the latent representation corresponds to the
    cell’s context vector. The same inputs are repeatedly fed to the memory cell at
    every time step. It looks like RNNs are not dead after all!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just a month later, Mathilde Caron et al. introduced [DINO](https://homl.info/dino),^([45](ch16.html#idm45720172223328))
    an impressive vision transformer trained entirely without labels, using self-supervision,
    and capable of high-accuracy semantic segmentation. The model is duplicated during
    training, with one network acting as a teacher and the other acting as a student.
    Gradient descent only affects the student, while the teacher’s weights are just
    an exponential moving average of the student’s weights. The student is trained
    to match the teacher’s predictions: since they’re almost the same model, this
    is called *self-distillation*. At each training step, the input images are augmented
    in different ways for the teacher and the student, so they don’t see the exact
    same image, but their predictions must match. This forces them to come up with
    high-level representations. To prevent *mode collapse*, where both the student
    and the teacher would always output the same thing, completely ignoring the inputs,
    DINO keeps track of a moving average of the teacher’s outputs, and it tweaks the
    teacher’s predictions to ensure that they remain centered on zero, on average.
    DINO also forces the teacher to have high confidence in its predictions: this
    is called *sharpening*. Together, these techniques preserve diversity in the teacher’s
    outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a [2021 paper](https://homl.info/scalingvits),⁠^([46](ch16.html#idm45720172217680))
    Google researchers showed how to scale ViTs up or down, depending on the amount
    of data. They managed to create a huge 2 billion parameter model that reached
    over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a scaled-down
    model that reached over 84.8% top-1 accuracy on ImageNet, using only 10,000 images:
    that’s just 10 images per class!'
  prefs: []
  type: TYPE_NORMAL
- en: And progress in visual transformers has continued steadily to this day. For
    example, in March 2022, a [paper](https://homl.info/modelsoups)⁠^([47](ch16.html#idm45720172215648))
    by Mitchell Wortsman et al. demonstrated that it’s possible to first train multiple
    transformers, then average their weights to create a new and improved model. This
    is similar to an ensemble (see [Chapter 7](ch07.html#ensembles_chapter)), except
    there’s just one model in the end, which means there’s no inference time penalty.
  prefs: []
  type: TYPE_NORMAL
- en: 'The latest trend in transformers consists in building large multimodal models,
    often capable of zero-shot or few-shot learning. For example, [OpenAI’s 2021 CLIP
    paper](https://homl.info/clip)⁠^([48](ch16.html#idm45720172211920)) proposed a
    large transformer model pretrained to match captions with images: this task allows
    it to learn excellent image representations, and the model can then be used directly
    for tasks such as image classification using simple text prompts such as “a photo
    of a cat”. Soon after, OpenAI announced [DALL·E](https://homl.info/dalle),⁠^([49](ch16.html#idm45720172210416))
    capable of generating amazing images based on text prompts. The [DALL·E 2](https://homl.info/dalle2),⁠^([50](ch16.html#idm45720172208784))
    which generates even higher quality images using a diffusion model (see [Chapter 17](ch17.html#autoencoders_chapter)).'
  prefs: []
  type: TYPE_NORMAL
- en: In April 2022, DeepMind released the [Flamingo paper](https://homl.info/flamingo),⁠^([51](ch16.html#idm45720172205280))
    which introduced a family of models pretrained on a wide variety of tasks across
    multiple modalities, including text, images, and videos. A single model can be
    used across very different tasks, such as question answering, image captioning,
    and more. Soon after, in May 2022, DeepMind introduced [GATO](https://homl.info/gato),⁠^([52](ch16.html#idm45720172203760))
    a multimodal model that can be used as a policy for a reinforcement learning agent
    (RL will be introduced in [Chapter 18](ch18.html#rl_chapter)). The same transformer
    can chat with you, caption images, play Atari games, control (simulated) robotic
    arms, and more, all with “only” 1.2 billion parameters. And the adventure continues!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These astounding advances have led some researchers to claim that human-level
    AI is near, that “scale is all you need”, and that some of these models may be
    “slightly conscious”. Others point out that despite the amazing progress, these
    models still lack the reliability and adaptability of human intelligence, our
    ability to reason symbolically, to generalize based on a single example, and more.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, transformers are everywhere! And the good news is that you generally
    won’t have to implement transformers yourself since many excellent pretrained
    models are readily available for download via TensorFlow Hub or Hugging Face’s
    model hub. You’ve already seen how to use a model from TF Hub, so let’s close
    this chapter by taking a quick look at Hugging Face’s ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face’s Transformers Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s impossible to talk about transformers today without mentioning Hugging
    Face, an AI company that has built a whole ecosystem of easy-to-use open source
    tools for NLP, vision, and beyond. The central component of their ecosystem is
    the Transformers library, which allows you to easily download a pretrained model,
    including its corresponding tokenizer, and then fine-tune it on your own dataset,
    if needed. Plus, the library supports TensorFlow, PyTorch, and JAX (with the Flax
    library).
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to use the Transformers library is to use the `transformers.​pipe⁠line()`
    function: you just specify which task you want, such as sentiment analysis, and
    it downloads a default pretrained model, ready to be used—it really couldn’t be
    any simpler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a Python list containing one dictionary per input text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the model correctly found that the sentence is positive, with
    around 99.98% confidence. Of course, you can also pass a batch of sentences to
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pipeline()` function uses the default model for the given task. For example,
    for text classification tasks such as sentiment analysis, at the time of writing,
    it defaults to `distilbert-base-uncased-finetuned-sst-2-english`—a DistilBERT
    model with an uncased tokenizer, trained on English Wikipedia and a corpus of
    English books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task.
    It’s also possible to manually specify a different model. For example, you could
    use a DistilBERT model fine-tuned on the Multi-Genre Natural Language Inference
    (MultiNLI) task, which classifies two sentences into three classes: contradiction,
    neutral, or entailment. Here is how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can find the available models at [*https://huggingface.co/models*](https://huggingface.co/models),
    and the list of tasks at [*https://huggingface.co/tasks*](https://huggingface.co/tasks).
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline API is very simple and convenient, but sometimes you will need
    more control. For such cases, the Transformers library provides many classes,
    including all sorts of tokenizers, models, configurations, callbacks, and much
    more. For example, let’s load the same DistilBERT model, along with its corresponding
    tokenizer, using the `TFAutoModelForSequenceClassification` and `AutoTokenizer`
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s tokenize a couple of pairs of sentences. In this code, we activate
    padding and specify that we want TensorFlow tensors instead of Python lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Instead of passing `"Sentence 1 [SEP] Sentence 2"` to the tokenizer, you can
    equivalently pass it a tuple: `("Sentence 1",` `"Sentence 2")`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is a dictionary-like instance of the `BatchEncoding` class, which
    contains the sequences of token IDs, as well as a mask containing 0s for the padding
    tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: If you set `return_token_type_ids=True` when calling the tokenizer, you will
    also get an extra tensor that indicates which sentence each token belongs to.
    This is needed by some models, but not DistilBERT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can directly pass this `BatchEncoding` object to the model; it returns
    a `TFSequenceClassifierOutput` object containing its predicted class logits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we can apply the softmax activation function to convert these logits
    to class probabilities, and use the `argmax()` function to predict the class with
    the highest probability for each input sentence pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the model correctly classifies the first sentence pair as neutral
    (the fact that I like soccer does not imply that everyone else does) and the second
    pair as an entailment (Joe must indeed be quite old).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wish to fine-tune this model on your own dataset, you can train the
    model as usual with Keras since it’s just a regular Keras model with a few extra
    methods. However, because the model outputs logits instead of probabilities, you
    must use the `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`
    loss instead of the usual `"sparse_categorical_crossentropy"` loss. Moreover,
    the model does not support `BatchEncoding` inputs during training, so you must
    use its `data` attribute to get a regular dictionary instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Hugging Face has also built a Datasets library that you can use to easily download
    a standard dataset (such as IMDb) or a custom one, and use it to fine-tune your
    model. It’s similar to TensorFlow Datasets, but it also provides tools to perform
    common preprocessing tasks on the fly, such as masking. The list of datasets is
    available at [*https://huggingface.co/datasets*](https://huggingface.co/datasets).
  prefs: []
  type: TYPE_NORMAL
- en: 'This should get you started with Hugging Face’s ecosystem. To learn more, you
    can head over to [*https://huggingface.co/docs*](https://huggingface.co/docs)
    for the documentation, which includes many tutorial notebooks, videos, the full
    API, and more. I also recommend you check out the O’Reilly book [*Natural Language
    Processing with Transformers: Building Language Applications with Hugging Face*](https://homl.info/hfbook)
    by Lewis Tunstall, Leandro von Werra, and Thomas Wolf—all from the Hugging Face
    team.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will discuss how to learn deep representations in an
    unsupervised way using autoencoders, and we will use generative adversarial networks
    to produce images and more!
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the pros and cons of using a stateful RNN versus a stateless RNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do people use encoder–decoder RNNs rather than plain sequence-to-sequence
    RNNs for automatic translation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you deal with variable-length input sequences? What about variable-length
    output sequences?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is beam search, and why would you use it? What tool can you use to implement
    it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an attention mechanism? How does it help?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the most important layer in the transformer architecture? What is its
    purpose?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When would you need to use sampled softmax?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Embedded Reber grammars* were used by Hochreiter and Schmidhuber in [their
    paper](https://homl.info/93) about LSTMs. They are artificial grammars that produce
    strings such as “BPBTSXXVPSEPE”. Check out Jenny Orr’s [nice introduction](https://homl.info/108)
    to this topic, then choose a particular embedded Reber grammar (such as the one
    represented on Orr’s page), then train an RNN to identify whether a string respects
    that grammar or not. You will first need to write a function capable of generating
    a training batch containing about 50% strings that respect the grammar, and 50%
    that don’t.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train an encoder–decoder model that can convert a date string from one format
    to another (e.g., from “April 22, 2019” to “2019-04-22”).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go through the example on the Keras website for [“Natural language image search
    with a Dual Encoder”](https://homl.info/dualtuto). You will learn how to build
    a model capable of representing both images and text within the same embedding
    space. This makes it possible to search for images using a text prompt, like in
    the CLIP model by OpenAI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the Hugging Face Transformers library to download a pretrained language
    model capable of generating text (e.g., GPT), and try generating more convincing
    Shakespearean text. You will need to use the model’s `generate()` method—see Hugging
    Face’s documentation for more details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch16.html#idm45720177608176-marker)) Alan Turing, “Computing Machinery
    and Intelligence”, *Mind* 49 (1950): 433–460.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch16.html#idm45720177605632-marker)) Of course, the word *chatbot* came
    much later. Turing called his test the *imitation game*: machine A and human B
    chat with human interrogator C via text messages; the interrogator asks questions
    to figure out which one is the machine (A or B). The machine passes the test if
    it can fool the interrogator, while the human B must try to help the interrogator.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch16.html#idm45720176925824-marker)) Since the input windows overlap,
    the concept of *epoch* is not so clear in this case: during each epoch (as implemented
    by Keras), the model will actually see the same character multiple times.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch16.html#idm45720176103360-marker)) Alec Radford et al., “Learning to
    Generate Reviews and Discovering Sentiment”, arXiv preprint arXiv:1704.01444 (2017).
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch16.html#idm45720175873872-marker)) Rico Sennrich et al., “Neural Machine
    Translation of Rare Words with Subword Units”, *Proceedings of the 54th Annual
    Meeting of the Association for Computational Linguistics* 1 (2016): 1715–1725.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch16.html#idm45720175847312-marker)) Taku Kudo, “Subword Regularization:
    Improving Neural Network Translation Models with Multiple Subword Candidates”,
    arXiv preprint arXiv:1804.10959 (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch16.html#idm45720175842096-marker)) Taku Kudo and John Richardson, “SentencePiece:
    A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural
    Text Processing”, arXiv preprint arXiv:1808.06226 (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch16.html#idm45720175839056-marker)) Yonghui Wu et al., “Google’s Neural
    Machine Translation System: Bridging the Gap Between Human and Machine Translation”,
    arXiv preprint arXiv:1609.08144 (2016).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch16.html#idm45720175499952-marker)) Ragged tensors were introduced in
    [Chapter 12](ch12.html#tensorflow_chapter), and they are detailed in [Appendix C](app03.html#structures_appendix).
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch16.html#idm45720175311248-marker)) Matthew Peters et al., “Deep Contextualized
    Word Representations”, *Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies*
    1 (2018): 2227–2237.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch16.html#idm45720175307104-marker)) Jeremy Howard and Sebastian Ruder,
    “Universal Language Model Fine-Tuning for Text Classification”, *Proceedings of
    the 56th Annual Meeting of the Association for Computational Linguistics* 1 (2018):
    328–339.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch16.html#idm45720175301568-marker)) Daniel Cer et al., “Universal Sentence
    Encoder”, arXiv preprint arXiv:1803.11175 (2018).
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch16.html#idm45720175133488-marker)) Ilya Sutskever et al., “Sequence
    to Sequence Learning with Neural Networks”, arXiv preprint (2014).
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch16.html#idm45720175116736-marker)) Samy Bengio et al., “Scheduled Sampling
    for Sequence Prediction with Recurrent Neural Networks”, arXiv preprint arXiv:1506.03099
    (2015).
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch16.html#idm45720175112096-marker)) This dataset is composed of sentence
    pairs created by contributors of the [Tatoeba project](https://tatoeba.org). About
    120,000 sentence pairs were selected by the authors of the website [*https://manythings.org/anki*](https://manythings.org/anki).
    This dataset is released under the Creative Commons Attribution 2.0 France license.
    Other language pairs are available.
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch16.html#idm45720174413024-marker)) In Python, if you run `a, *b = [1,
    2, 3, 4]`, then `a` equals `1` and `b` equals `[2, 3, 4]`.
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch16.html#idm45720174300992-marker)) Sébastien Jean et al., “On Using
    Very Large Target Vocabulary for Neural Machine Translation”, *Proceedings of
    the 53rd Annual Meeting of the Association for Computational Linguistics and the
    7th International Joint Conference on Natural Language Processing of the Asian
    Federation of Natural Language Processing* 1 (2015): 1–10.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch16.html#idm45720173841328-marker)) Dzmitry Bahdanau et al., “Neural
    Machine Translation by Jointly Learning to Align and Translate”, arXiv preprint
    arXiv:1409.0473 (2014).
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch16.html#idm45720173812256-marker)) Minh-Thang Luong et al., “Effective
    Approaches to Attention-Based Neural Machine Translation”, *Proceedings of the
    2015 Conference on Empirical Methods in Natural Language Processing* (2015): 1412–1421.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([20](ch16.html#idm45720173607840-marker)) Ashish Vaswani et al., “Attention
    Is All You Need”, *Proceedings of the 31st International Conference on Neural
    Information Processing Systems* (2017): 6000–6010.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch16.html#idm45720173604064-marker)) Since the transformer uses time-distributed
    dense layers, you could argue that it uses 1D convolutional layers with a kernel
    size of 1.
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch16.html#idm45720173579408-marker)) This is figure 1 from the “Attention
    Is All You Need” paper, reproduced with the kind permission of the authors.
  prefs: []
  type: TYPE_NORMAL
- en: ^([23](ch16.html#idm45720173475360-marker)) It’s possible to use ragged tensors
    instead, if you are using the latest version of TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: ^([24](ch16.html#idm45720173058848-marker)) This is the righthand part of figure
    2 from “Attention Is All You Need”, reproduced with the kind authorization of
    the authors.
  prefs: []
  type: TYPE_NORMAL
- en: '^([25](ch16.html#idm45720173048224-marker)) This will most likely change by
    the time you read this; check out [Keras issue #16248](https://github.com/keras-team/keras/issues/16248)
    for more details. When this happens, there will be no need to set the `attention_mask`
    argument, and therefore no need to create `encoder_pad_mask`.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([26](ch16.html#idm45720172796192-marker)) Currently `Z + skip` does not support
    automatic masking, which is why we had to write `tf.keras.​lay⁠ers.Add()([Z, skip])`
    instead. Again, this may change by the time you read this.
  prefs: []
  type: TYPE_NORMAL
- en: ^([27](ch16.html#idm45720172319856-marker)) Alec Radford et al., “Improving
    Language Understanding by Generative Pre-Training” (2018).
  prefs: []
  type: TYPE_NORMAL
- en: ^([28](ch16.html#idm45720172314768-marker)) For example, the sentence “Jane
    had a lot of fun at her friend’s birthday party” entails “Jane enjoyed the party”,
    but it is contradicted by “Everyone hated the party” and it is unrelated to “The
    Earth is flat”.
  prefs: []
  type: TYPE_NORMAL
- en: '^([29](ch16.html#idm45720172313104-marker)) Jacob Devlin et al., “BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding”, *Proceedings of
    the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies* 1 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([30](ch16.html#idm45720172299632-marker)) This is figure 1 from the paper,
    reproduced with the kind authorization of the authors.
  prefs: []
  type: TYPE_NORMAL
- en: ^([31](ch16.html#idm45720172296288-marker)) Alec Radford et al., “Language Models
    Are Unsupervised Multitask Learners” (2019).
  prefs: []
  type: TYPE_NORMAL
- en: '^([32](ch16.html#idm45720172293488-marker)) William Fedus et al., “Switch Transformers:
    Scaling to Trillion Parameter Models with Simple and Efficient Sparsity” (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([33](ch16.html#idm45720172289264-marker)) Victor Sanh et al., “DistilBERT,
    A Distilled Version of Bert: Smaller, Faster, Cheaper and Lighter”, arXiv preprint
    arXiv:1910.01108 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([34](ch16.html#idm45720172285632-marker)) Mariya Yao summarized many of these
    models in this post: [*https://homl.info/yaopost*](https://homl.info/yaopost).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([35](ch16.html#idm45720172282624-marker)) Colin Raffel et al., “Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, arXiv
    preprint arXiv:1910.10683 (2019).
  prefs: []
  type: TYPE_NORMAL
- en: '^([36](ch16.html#idm45720172277824-marker)) Aakanksha Chowdhery et al., “PaLM:
    Scaling Language Modeling with Pathways”, arXiv preprint arXiv:2204.02311 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([37](ch16.html#idm45720172273920-marker)) Jason Wei et al., “Chain of Thought
    Prompting Elicits Reasoning in Large Language Models”, arXiv preprint arXiv:2201.11903
    (2022).
  prefs: []
  type: TYPE_NORMAL
- en: '^([38](ch16.html#idm45720172266576-marker)) Kelvin Xu et al., “Show, Attend
    and Tell: Neural Image Caption Generation with Visual Attention”, *Proceedings
    of the 32nd International Conference on Machine Learning* (2015): 2048–2057.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([39](ch16.html#idm45720172255968-marker)) This is a part of figure 3 from
    the paper. It is reproduced with the kind authorization of the authors.
  prefs: []
  type: TYPE_NORMAL
- en: '^([40](ch16.html#idm45720172249728-marker)) Marco Tulio Ribeiro et al., “‘Why
    Should I Trust You?’: Explaining the Predictions of Any Classifier”, *Proceedings
    of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining* (2016): 1135–1144.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([41](ch16.html#idm45720172246864-marker)) Nicolas Carion et al., “End-to-End
    Object Detection with Transformers”, arXiv preprint arxiv:2005.12872 (2020).
  prefs: []
  type: TYPE_NORMAL
- en: '^([42](ch16.html#idm45720172244672-marker)) Alexey Dosovitskiy et al., “An
    Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale”, arXiv
    preprint arxiv:2010.11929 (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([43](ch16.html#idm45720172239280-marker)) Hugo Touvron et al., “Training Data-Efficient
    Image Transformers & Distillation Through Attention”, arXiv preprint arxiv:2012.12877
    (2020).
  prefs: []
  type: TYPE_NORMAL
- en: '^([44](ch16.html#idm45720172235056-marker)) Andrew Jaegle et al., “Perceiver:
    General Perception with Iterative Attention”, arXiv preprint arxiv:2103.03206
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([45](ch16.html#idm45720172223328-marker)) Mathilde Caron et al., “Emerging
    Properties in Self-Supervised Vision Transformers”, arXiv preprint arxiv:2104.14294
    (2021).
  prefs: []
  type: TYPE_NORMAL
- en: ^([46](ch16.html#idm45720172217680-marker)) Xiaohua Zhai et al., “Scaling Vision
    Transformers”, arXiv preprint arxiv:2106.04560v1 (2021).
  prefs: []
  type: TYPE_NORMAL
- en: '^([47](ch16.html#idm45720172215648-marker)) Mitchell Wortsman et al., “Model
    Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accuracy Without
    Increasing Inference Time”, arXiv preprint arxiv:2203.05482v1 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([48](ch16.html#idm45720172211920-marker)) Alec Radford et al., “Learning Transferable
    Visual Models From Natural Language Supervision”, arXiv preprint arxiv:2103.00020
    (2021).
  prefs: []
  type: TYPE_NORMAL
- en: ^([49](ch16.html#idm45720172210416-marker)) Aditya Ramesh et al., “Zero-Shot
    Text-to-Image Generation”, arXiv preprint arxiv:2102.12092 (2021).
  prefs: []
  type: TYPE_NORMAL
- en: ^([50](ch16.html#idm45720172208784-marker)) Aditya Ramesh et al., “Hierarchical
    Text-Conditional Image Generation with CLIP Latents”, arXiv preprint arxiv:2204.06125
    (2022).
  prefs: []
  type: TYPE_NORMAL
- en: '^([51](ch16.html#idm45720172205280-marker)) Jean-Baptiste Alayrac et al., “Flamingo:
    a Visual Language Model for Few-Shot Learning”, arXiv preprint arxiv:2204.14198
    (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([52](ch16.html#idm45720172203760-marker)) Scott Reed et al., “A Generalist
    Agent”, arXiv preprint arxiv:2205.06175 (2022).
  prefs: []
  type: TYPE_NORMAL
