<html><head></head><body><section data-pdf-bookmark="Chapter 8. Using ML to Create Text" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch08_using_ml_to_create_text_1748549671852453">
      <h1><span class="label">Chapter 8. </span>Using ML to Create Text</h1>
      <p>With the release of ChatGPT in 2022,<a contenteditable="false" data-primary="generative AI" data-secondary="about" data-type="indexterm" id="id1331"/><a contenteditable="false" data-primary="Transformers" data-secondary="about" data-type="indexterm" id="id1332"/><a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="about" data-type="indexterm" id="id1333"/><a contenteditable="false" data-primary="text generator" data-secondary="about generative AI" data-type="indexterm" id="id1334"/><a contenteditable="false" data-primary="text generator" data-secondary="about transformers" data-type="indexterm" id="id1335"/><a contenteditable="false" data-primary="text generator" data-secondary="about" data-type="indexterm" id="id1336"/><a contenteditable="false" data-primary="generative AI" data-secondary="Transformers" data-tertiary="about" data-type="indexterm" id="id1337"/> the words <em>generative AI</em> entered the common lexicon. This simple application that allowed you to chat with a cloud-based AI seemed almost miraculous in how it could answer your queries with knowledge of almost everything in human experience. It worked by using a very advanced evolution beyond the recurrent neural networks you saw in the last chapter, by using a technique called <em>transformers</em>. </p>
      <p>A <em>transformer</em> learns the patterns that turn one piece of text into another. With a large enough transformer architecture and a large enough set of text to learn from, <a contenteditable="false" data-primary="GPT model" data-type="indexterm" id="id1338"/>the GPT model (GPT stands for generative pretrained transformers) could predict the next tokens to follow a piece of text. When GPT was wrapped in an application that made it more user friendly, a whole new industry was born.</p>
      <p>While creating models with transformers is beyond the scope of this book, we will look at their architecture in detail in <a data-type="xref" href="ch15.html#ch15_transformers_and_transformers_1748549808974580">Chapter 15</a>. </p>
      <p>The principles involved in training models with transformers can be replicated with smaller, simpler, architectures like RNNs or LSTM. We’ll explore that in this chapter and with a much smaller corpus of text—traditional Irish songs. </p>
      <p>So, for example, consider this line of text from a famous TV show:</p>
      <blockquote>
  <p>You know nothing, Jon Snow.</p>
</blockquote>
      <p>A next-token-predictor model, created with RNNs, came up with these song lyrics in response:</p>
      <ul class="simplelist less_space">
          <li><p>You know nothing, Jon Snow</p></li>
          <li><p>the place where he’s stationed</p></li>
          <li><p>be it Cork or in the blue bird’s son</p></li>
          <li><p>sailed out to summer</p></li>
          <li><p>old sweet long and gladness rings</p></li>
          <li><p>so I’ll wait for the wild Colleen dying</p></li>
      </ul>
      <p>This text was generated by a very simple model that was trained on a small corpus. I’ve enhanced it a little by adding line breaks and punctuation, but other than the first line, all of the lyrics were generated by the model you’ll learn how to build in this chapter. It’s kind of cool that it mentions a <em>wild Colleen dying</em>—if you’ve watched the show that Jon Snow comes from, you’ll understand why!</p>
      <p>In the last few chapters, you saw how you can use PyTorch with text-based data—first tokenizing it into numbers and sequences that can be processed by a neural network, then using embeddings to simulate sentiment using vectors, and finally using deep and recurrent neural networks to classify text. We used the sarcasm dataset, a small and simple one, to illustrate how all this works. </p>
      <p>In this chapter we’re going to shift gears: instead of classifying existing text, you’ll create a neural network that can <em>predict</em> text and thus <em>generate</em> text. </p>
      <p>Given a corpus of text, the network will attempt to learn and understand the <em>patterns</em> of words within the text so that it can,<a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="seed text" data-type="indexterm" id="id1339"/><a contenteditable="false" data-primary="text generator" data-secondary="seed text" data-type="indexterm" id="id1340"/><a contenteditable="false" data-primary="seed text for text generators" data-type="indexterm" id="id1341"/> given a new piece of text called a <em>seed</em>, predict what word should come next. Once the network has that, the seed and the predicted word become the new seed, and it can predict the next word. Thus, when trained on a corpus of text, a neural network can attempt to write new text in a similar style. To create the preceding piece of poetry, I collected lyrics from a number of traditional Irish songs, trained a neural network with them, and used it to predict words.</p>
      <p>We’ll start simple, using a small amount of text to illustrate how to build up to a predictive model, and we’ll end by creating a full model with a lot more text. After that, you can try it out to see what kind of poetry it can create!</p>
      <p>To get started, you’ll have to treat the text a little differently from how you’ve been treating it thus far. In the previous chapters, you took sentences and turned them into sequences that were then classified based on the embeddings for the tokens within them. <a contenteditable="false" data-primary="sentences" data-secondary="input sequences and labels from" data-type="indexterm" id="ch8inp"/><a contenteditable="false" data-primary="sequences from sentences" data-secondary="input sequences and labels" data-type="indexterm" id="ch8inp2"/><a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="input sequences and labels" data-type="indexterm" id="ch8inp3"/><a contenteditable="false" data-primary="text generator" data-secondary="input sequences and labels" data-type="indexterm" id="ch8inp4"/>But when it comes to creating data that can be used to train a predictive model like this one, there’s an additional step in which you need to transform the sequences into <em>input sequences</em> and <em>labels</em>, where the input sequence is a group of words and the label is the next word in the sentence. You can then train a model to match the input sequences to their labels, so that future predictions can pick a label that’s close to the input sequence.</p>
      <section data-pdf-bookmark="Turning Sequences into Input Sequences" data-type="sect1"><div class="sect1" id="ch08_turning_sequences_into_input_sequences_1748549671852780">
        <h1>Turning Sequences into Input Sequences</h1>
        <p>When predicting text, you need to train a neural network with an input sequence (feature) that has an associated label. Matching sequences to labels is the key to predicting text. In this case, you won’t have explicit labels like you do when you are classifying, but instead, you’ll split the sentence, and for a block of <em>n</em> words, the next word in the sentence will be the label.</p>
        <p>So, for example, if in your corpus you had the sentence “Today has a beautiful blue sky,” then you could split it into “Today has a beautiful blue” as the feature and “sky” as the label. Then, if you were to get a prediction for the text “Today has a beautiful blue,” it would likely be “sky.” If, in the training data, you also had “Yesterday had a beautiful blue sky,” you would split it in the same way, and if you were to get a prediction for the text “Tomorrow will have a beautiful blue,” then there’s a high probability that the next word would be “sky.”</p>
        <p>If you train a network with lots of sentences, where you remove the last word and make it the label, you can quickly build up a predictive model in which the most likely next word in the sentence can be predicted from an existing body of text.</p>
        <p>We’ll start with a very small corpus of text—an excerpt from a traditional Irish song from the 1860s, some of the lyrics of which are as follows:</p>
        <ul class="simplelist">
            <li><p>In the town of Athy one Jeremy Lanigan</p></li>
            <li><p>Battered away ’til he hadn’t a pound.</p></li>
            <li><p>His father died and made him a man again</p></li>
            <li><p>Left him a farm and ten acres of ground.</p></li>
            <li><p>He gave a grand party for friends and relations</p></li>
            <li><p>Who didn’t forget him when come to the wall,</p></li>
            <li><p>And if you’ll but listen I’ll make your eyes glisten</p></li>
            <li><p>Of the rows and the ructions of Lanigan’s Ball.</p></li>
            <li><p>Myself to be sure got free invitation,</p></li>
            <li><p>For all the nice girls and boys I might ask,</p></li>
            <li><p>And just in a minute both friends and relations</p></li>
            <li><p>Were dancing round merry as bees round a cask.</p></li>
            <li><p>Judy O’Daly, that nice little milliner,</p></li>
            <li><p>She tipped me a wink for to give her a call,</p></li>
            <li><p>And I soon arrived with Peggy McGilligan</p></li>
            <li><p>Just in time for Lanigan’s Ball.</p></li>
        </ul>
        <p>You’ll want to create a single string with all the text and set that to be your data. <span class="keep-together">Use \n for</span> the line breaks. Then, this corpus can be easily loaded and tokenized.  <span class="keep-together">First, the tokenize function</span> will split the text into individual words, and then the <code>create_word_dictionary</code> will create a dictionary with an index for each individual word in the text:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="n">tokens</code> <code class="o">=</code> <code class="n">text</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
    <code class="k">return</code> <code class="n">tokens</code>
 
<code class="k">def</code> <code class="nf">create_word_dictionary</code><code class="p">(</code><code class="n">word_list</code><code class="p">):</code>
    <code class="c1"># Create an empty dictionary</code>
    <code class="n">word_dict</code> <code class="o">=</code> <code class="p">{}</code>
    <code class="n">word_dict</code><code class="p">[</code><code class="s2">"UNK"</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="c1"># Counter for unique values</code>
    <code class="n">counter</code> <code class="o">=</code> <code class="mi">1</code>
 
    <code class="c1"># Iterate through the list and assign numbers to unique words</code>
    <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">word_list</code><code class="p">:</code>
        <code class="k">if</code> <code class="n">word</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">word_dict</code><code class="p">:</code>
            <code class="n">word_dict</code><code class="p">[</code><code class="n">word</code><code class="p">]</code> <code class="o">=</code> <code class="n">counter</code>
            <code class="n">counter</code> <code class="o">+=</code> <code class="mi">1</code>
 
    <code class="k">return</code> <code class="n">word_dict</code>
 </pre>
        <p>Note that this is a very simplistic approach for the purpose of learning how these work. In production systems, you’d likely either use off-the-shelf components that have been built for scale or greatly enhance them for scale and exception checking.</p>
        <p>With these functions, you can then create a <code>word_index</code> of the simple corpus, like this:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="n">data</code><code class="o">=</code><code class="s2">"In the town of Athy one Jeremy Lanigan </code><code class="se">\n</code> 
      <code class="n">Battered</code> <code class="n">away</code> <code class="n">til</code> <code class="n">he</code> <code class="n">hadnt</code> <code class="n">a</code> <code class="n">pound</code><code class="o">.</code> <code class="o">...</code><code class="s2">"</code>

<code class="n">tokens</code> <code class="o">=</code> <code class="n">tokenize</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
<code class="n">word_index</code> <code class="o">=</code> <code class="n">create_word_dictionary</code><code class="p">(</code><code class="n">tokens</code><code class="p">)</code>

<code class="n">total_words</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">word_index</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code>
</pre>
        <p>The result of this process is to replace the words with their token values (see <a data-type="xref" href="#ch08_figure_1_1748549671837881">Figure 8-1</a>).</p>
        <figure><div class="figure" id="ch08_figure_1_1748549671837881">
          <img alt="" src="assets/aiml_0801.png"/>
          <h6><span class="label">Figure 8-1. </span>Tokenizing a sentence</h6>
        </div></figure>
        <p>To train a predictive model, we should take a further step here: splitting the sentence into multiple smaller sequences so, for example, we can have one sequence consisting of the first two tokens, another consisting of the first three, etc. We would then pad these out to be the same length as the input sequence by prepending zeros (see <a data-type="xref" href="#ch08_figure_2_1748549671837934">Figure 8-2</a>).</p>
        <figure><div class="figure" id="ch08_figure_2_1748549671837934">
          <img alt="" src="assets/aiml_0802.png"/>
          <h6><span class="label">Figure 8-2. </span>Turning a sequence into a number of input sequences</h6>
        </div></figure>
        <p>To do this, you’ll need to go through each line in the corpus and turn it into a list of tokens, using functions to convert the text words into an array of their lookup values in the word dictionary, and then to create the padded versions of the subsequences. To assist you with this task, I’ve provided these functions: <code>text_to_sequence</code> and <code>pad_sequence</code>.</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">text_to_sequence</code><code class="p">(</code><code class="n">sentence</code><code class="p">,</code> <code class="n">word_dict</code><code class="p">):</code>
    <code class="c1"># Convert sentence to lowercase and split into words</code>
    <code class="n">words</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
 
    <code class="c1"># Convert each word to its corresponding number</code>
    <code class="n">number_sequence</code> <code class="o">=</code> <code class="p">[</code><code class="n">word_dict</code><code class="p">[</code><code class="n">word</code><code class="p">]</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code><code class="p">]</code>
 
    <code class="k">return</code> <code class="n">number_sequence</code>
 
<code class="k">def</code> <code class="nf">pad_sequences</code><code class="p">(</code><code class="n">sequences</code><code class="p">,</code> <code class="n">max_length</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
    <code class="c1"># If max_length is not specified, find the length of the longest sequence</code>
    <code class="k">if</code> <code class="n">max_length</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>
        <code class="n">max_length</code> <code class="o">=</code> <code class="nb">max</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">seq</code><code class="p">)</code> <code class="k">for</code> <code class="n">seq</code> <code class="ow">in</code> <code class="n">sequences</code><code class="p">)</code>
 
    <code class="c1"># Pad each sequence with zeros at the beginning</code>
    <code class="n">padded_sequences</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">seq</code> <code class="ow">in</code> <code class="n">sequences</code><code class="p">:</code>
        <code class="c1"># Calculate number of zeros needed</code>
        <code class="n">num_zeros</code> <code class="o">=</code> <code class="n">max_length</code> <code class="err">–</code> <code class="nb">len</code><code class="p">(</code><code class="n">seq</code><code class="p">)</code>
        <code class="c1"># Create padded sequence</code>
        <code class="n">padded_seq</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="n">num_zeros</code> <code class="o">+</code> <code class="nb">list</code><code class="p">(</code><code class="n">seq</code><code class="p">)</code>
        <code class="n">padded_sequences</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">padded_seq</code><code class="p">)</code>
 
    <code class="k">return</code> <code class="n">padded_sequences</code></pre>
        <p>You can then create the input sequences using these helper functions like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">corpus</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code>
 
<code class="n">input_sequences</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">corpus</code><code class="p">:</code>
    <code class="n">token_list</code> <code class="o">=</code> <code class="n">text_to_sequence</code><code class="p">(</code><code class="n">line</code><code class="p">,</code> <code class="n">word_index</code><code class="p">)</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">token_list</code><code class="p">)):</code>
        <code class="n">n_gram_sequence</code> <code class="o">=</code> <code class="n">token_list</code><code class="p">[:</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="p">]</code>
        <code class="n">input_sequences</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">n_gram_sequence</code><code class="p">)</code>
 
<code class="n">max_sequence_len</code> <code class="o">=</code> <code class="nb">max</code><code class="p">([</code><code class="nb">len</code><code class="p">(</code><code class="n">x</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">input_sequences</code><code class="p">])</code>
<code class="n">input_sequences</code> <code class="o">=</code> <code class="n">pad_sequences</code><code class="p">(</code><code class="n">input_sequences</code><code class="p">,</code> <code class="n">max_sequence_len</code><code class="p">)</code></pre>
        <p>Finally, once you have a set of padded input sequences, you can split them into features and labels, where each label is simply the last token in each input sequence (see <a data-type="xref" href="#ch08_figure_3_1748549671837963">Figure 8-3</a>).</p>
        <figure><div class="figure" id="ch08_figure_3_1748549671837963">
          <img alt="" src="assets/aiml_0803.png"/>
          <h6><span class="label">Figure 8-3. </span>Turning the padded sequences into features (x) and labels (y)</h6>
        </div></figure>
        <p>When you’re training a neural network, you’re going to match each feature to its corresponding label. So, for example, the label for [0 0 0 0 0 0 1] will be [2].</p>
        <p>Here’s the code you use to separate the labels from the input sequences:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">split_sequences</code><code class="p">(</code><code class="n">sequences</code><code class="p">):</code>
    <code class="c1"># Create xs by removing the last element from each sequence</code>
    <code class="n">xs</code> <code class="o">=</code> <code class="p">[</code><code class="n">seq</code><code class="p">[:</code><code class="err">–</code><code class="mi">1</code><code class="p">]</code> <code class="k">for</code> <code class="n">seq</code> <code class="ow">in</code> <code class="n">sequences</code><code class="p">]</code>
 
    <code class="c1"># Create labels by taking just the last element from each sequence</code>
    <code class="n">labels</code> <code class="o">=</code> <code class="p">[</code><code class="n">seq</code><code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">:]</code> <code class="k">for</code> <code class="n">seq</code> <code class="ow">in</code> <code class="n">sequences</code><code class="p">]</code>  
    <code class="c1"># Using [–1:] to keep it as a single-element list</code>
    <code class="c1"># Alternative if you want labels as single numbers instead of lists:</code>
    <code class="c1"># labels = [seq[–1] for seq in sequences]</code>
 
    <code class="k">return</code> <code class="n">xs</code><code class="p">,</code> <code class="n">labels</code>
<code class="n">xs</code><code class="p">,</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">split_sequences</code><code class="p">(</code><code class="n">input_sequences</code><code class="p">)</code></pre>
        <p>Next, you need to encode the labels. Right now, they’re just tokens—for example, the number 2 at the top of <a data-type="xref" href="#ch08_figure_3_1748549671837963">Figure 8-3</a> is a token. But if you want to use a token as a label in a classifier, you’ll have to map it to an output neuron. Thus, if you’re going to classify <em>n</em> words, with each word being a class, you’ll need to have <em>n</em> neurons. Here’s where it’s important to control the size of the vocabulary, because the more words you have, the more classes you’ll need. Remember back in Chapter <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch02.html#ch02_introduction_to_computer_vision_1748548889076080">2</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912">3</a>, when you were classifying fashion items with the Fashion MNIST dataset and you had 10 types of items of clothing? That required you to have 10 neurons in the output layer—but in this case, what if you want to predict up to 10,000 vocabulary words? You’ll need an output layer with 10,000 neurons!</p>
        <p>Additionally, you need to one-hot encode your labels so that they match the desired output from a neural network. Consider <a data-type="xref" href="#ch08_figure_3_1748549671837963">Figure 8-3</a> again. If a neural network is <span class="keep-together">fed the input <em>x</em></span> consisting of a series of 0s followed by a 1, you’ll want the prediction to be 2—but how the network delivers that is by having an output layer of <span class="keep-together"><code>vocabulary_size</code></span> neurons, where the second one has the highest probability.</p>
        <p>To encode your labels into a set of <em>Y</em>s that you can then use to train, you can use this code:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">one_hot_encode_with_checks</code><code class="p">(</code><code class="n">value</code><code class="p">,</code> <code class="n">corpus_size</code><code class="p">):</code>
    <code class="c1"># Check if value is within valid range</code>
    <code class="k">if</code> <code class="ow">not</code> <code class="mi">0</code> <code class="o">&lt;=</code> <code class="n">value</code> <code class="o">&lt;</code> <code class="n">corpus_size</code><code class="p">:</code>
        <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Value </code><code class="si">{</code><code class="n">value</code><code class="si">}</code><code class="s2"> is out of range for corpus size </code>
                                 <code class="p">{</code><code class="n">corpus_size</code><code class="p">}</code><code class="s2">")</code>
    <code class="c1"># Create and return one-hot encoded list</code>
    <code class="n">encoded</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="n">corpus_size</code>
    <code class="n">encoded</code><code class="p">[</code><code class="n">value</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code>
    <code class="k">return</code> <code class="n">encoded</code></pre>
        <p>Note that there are many libraries and helper functions out there that can do this for you, so feel free to use them instead of the simple code in this chapter. But I want to put these methodologies in here, so you can see how it works under the hood.</p>
        <p>You can see this visually in <a data-type="xref" href="#ch08_figure_4_1748549671837985">Figure 8-4</a>.</p>
        <figure><div class="figure" id="ch08_figure_4_1748549671837985">
          <img alt="" src="assets/aiml_0804.png"/>
          <h6><span class="label">Figure 8-4. </span>One-hot encoding labels</h6>
        </div></figure>
        <p>This is a very sparse representation that, if you have a lot of training data and a lot of potential words, will eat memory very quickly! Suppose you had 100,000 training sentences, with a vocabulary of 10,000 words—you’d need 1,000,000,000 bytes just to hold the labels! But that’s the way we have to design our network if we’re going to classify and predict words.<a contenteditable="false" data-primary="" data-startref="ch8inp" data-type="indexterm" id="id1342"/><a contenteditable="false" data-primary="" data-startref="ch8inp2" data-type="indexterm" id="id1343"/><a contenteditable="false" data-primary="" data-startref="ch8inp3" data-type="indexterm" id="id1344"/><a contenteditable="false" data-primary="" data-startref="ch8inp4" data-type="indexterm" id="id1345"/></p>
      </div></section>
      <section data-pdf-bookmark="Creating the Model" data-type="sect1"><div class="sect1" id="ch08_creating_the_model_1748549671852861">
        <h1>Creating the Model</h1>
        <p>Let’s now create a simple model<a contenteditable="false" data-primary="text generator" data-secondary="creating the model" data-type="indexterm" id="ch8cr8"/><a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="creating the model" data-type="indexterm" id="ch8cr82"/><a contenteditable="false" data-primary="long short-term memory (LSTM)" data-secondary="text generator using" data-type="indexterm" id="ch8cr83"/> that can be trained with this input data. It will consist of just an embedding layer, followed by an LSTM, followed by a dense layer.</p>
        <p>For the embedding, you’ll need one vector per word, so the parameters will be the total number of words and the number of dimensions you want to embed on. In this case, we don’t have many words, so eight dimensions should be enough.</p>
        <p>You can make the LSTM bidirectional,<a contenteditable="false" data-primary="long short-term memory (LSTM)" data-secondary="bidirectionality" data-type="indexterm" id="id1346"/> and the number of steps can be the length of a sequence, which is our max length minus 1 (because we took one token off the end to make the label).</p>
        <p>Finally, the output layer will be a dense layer with the total number of words as a parameter, activated by Softmax. Each neuron in this layer will be the probability that the next word matches the word for that index value:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">LSTMPredictor</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">total_words</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">LSTMPredictor</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
 
        <code class="c1"># If hidden_dim not specified, use max_sequence_len-1 as in TF version</code>
        <code class="k">if</code> <code class="n">hidden_dim</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>
            <code class="n">hidden_dim</code> <code class="o">=</code> <code class="n">max_sequence_len</code><code class="err">–</code><code class="mi">1</code>
 
        <code class="c1"># Embedding layer</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">total_words</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">)</code>
 
        <code class="c1"># Bidirectional LSTM</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">lstm</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code>
            <code class="n">input_size</code><code class="o">=</code><code class="n">embedding_dim</code><code class="p">,</code>
            <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_dim</code><code class="p">,</code>
            <code class="n">bidirectional</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
            <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code>
        <code class="p">)</code>
 
        <code class="c1"># Final dense layer (accounting for bidirectional LSTM)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code> <code class="o">*</code> <code class="mi">2</code><code class="p">,</code> <code class="n">total_words</code><code class="p">)</code>
 
        <code class="c1"># Softmax activation</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">softmax</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Softmax</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
 
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="c1"># Embedding layer</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
 
        <code class="c1"># LSTM layer</code>
        <code class="n">lstm_out</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">lstm</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
 
        <code class="c1"># Take the output from the last time step</code>
        <code class="n">lstm_out</code> <code class="o">=</code> <code class="n">lstm_out</code><code class="p">[:,</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="p">:]</code>
 
        <code class="c1"># Dense layer</code>
        <code class="n">out</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">fc</code><code class="p">(</code><code class="n">lstm_out</code><code class="p">)</code>
 
        <code class="c1"># Softmax activation</code>
        <code class="n">out</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">out</code><code class="p">)</code>
 
        <code class="k">return</code> <code class="n">out</code></pre>
        <p>Next, you compile the model with a categorical loss function such as categorical cross entropy and an optimizer like Adam. You can also specify that you want to capture metrics:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Training setup</code>
<code class="n">total_words</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">word_index</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">LSTMPredictor</code><code class="p">(</code><code class="n">total_words</code><code class="p">)</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">CrossEntropyLoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code></pre>
        <p>It’s a very simple model without a lot of data, and it trains quickly. Here are the results of training for about 15,000 epochs, which takes maybe 10 minutes. In the real world, you’ll likely be training with a lot more data and thus taking a lot more time, so you’ll have to consider some of the techniques we saw in <a data-type="xref" href="ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648">Chapter 7</a> to ensure greater accuracy and potentially less time to train.</p>
        <p>You’ll see that it has reached very high accuracy, and there may be room for more (see <a data-type="xref" href="#ch08_figure_5_1748549671838006">Figure 8-5</a>).</p>
        <figure><div class="figure" id="ch08_figure_5_1748549671838006">
          <img src="assets/aiml_0805.png"/>
          <h6><span class="label">Figure 8-5. </span>Training loss and accuracy</h6>
        </div></figure>
        <p>With the model at 80%+ accuracy, we can be assured that if we have a string of text that it has already seen, it will predict the next word accurately about 80% of the time. </p>
        <p>Note, however, that when generating text, it will continually see words that it hasn’t previously seen, so despite this good number, you’ll find that the network will rapidly end up producing nonsensical text. We’ll explore this in the next section.<a contenteditable="false" data-primary="" data-startref="ch8cr8" data-type="indexterm" id="id1347"/><a contenteditable="false" data-primary="" data-startref="ch8cr82" data-type="indexterm" id="id1348"/><a contenteditable="false" data-primary="" data-startref="ch8cr83" data-type="indexterm" id="id1349"/></p>
      </div></section>
      <section data-pdf-bookmark="Generating Text" data-type="sect1"><div class="sect1" id="ch08_generating_text_1748549671852930">
        <h1>Generating Text</h1>
        <p>Now that you’ve trained a network<a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="generating text" data-type="indexterm" id="ch8gentxt"/><a contenteditable="false" data-primary="text generator" data-secondary="generating text" data-type="indexterm" id="ch8gentxt2"/> that can predict the next word in a sequence, the next step is to give it a sequence of text and have it predict the next word. Let’s take a look at how to do that.</p>
        <section data-pdf-bookmark="Predicting the Next Word" data-type="sect2"><div class="sect2" id="ch08_predicting_the_next_word_1748549671852999">
          <h2>Predicting the Next Word</h2>
          <p>You’ll start by creating a phrase<a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="predicting the next word" data-type="indexterm" id="ch8pred"/><a contenteditable="false" data-primary="text generator" data-secondary="generating text" data-tertiary="predicting the next word" data-type="indexterm" id="ch8pred2"/><a contenteditable="false" data-primary="words" data-secondary="predicting the next word" data-type="indexterm" id="ch8pred3"/><a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="seed text" data-type="indexterm" id="id1350"/><a contenteditable="false" data-primary="text generator" data-secondary="seed text" data-type="indexterm" id="id1351"/><a contenteditable="false" data-primary="seed text for text generators" data-type="indexterm" id="id1352"/> called the <em>seed text</em>. This is the initial expression on which the network will base all the content it generates, and it will do this by predicting the next word.</p>
          <p>Start with a phrase that the network has <em>already</em> seen, such as “in the town of Athy”:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">input_text</code> <code class="o">=</code> <code class="s2">"In the town of Athy"</code></pre>
          <p>Next, you need to tokenize this and turn it into a sequence of tokens of the same length as used for training:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Convert text to lowercase and split into words</code>
<code class="n">words</code> <code class="o">=</code> <code class="n">input_text</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
 
<code class="c1"># Convert words to numbers using the word dictionary, use 0 for unknown words</code>
<code class="n">number_sequence</code> <code class="o">=</code> <code class="p">[</code><code class="n">word_dict</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code><code class="p">]</code>
 
<code class="c1"># Pad the sequence</code>
<code class="n">padded_sequence</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="p">(</code><code class="n">sequence_length</code> <code class="err">–</code> <code class="nb">len</code><code class="p">(</code><code class="n">number_sequence</code><code class="p">))</code> 
                      <code class="o">+</code> <code class="n">number_sequence</code>
 </pre>
          <p>Then, you need to pad that sequence to get it into the same shape as the data used for training by converting it to a tensor:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">input_tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">LongTensor</code><code class="p">([</code><code class="n">padded_sequence</code><code class="p">])</code></pre>
          <p>Now, you can predict the next word for this token list by passing this input tensor to the model to get the output. This will be a tensor that contains the probabilities for each of the words in the dictionary and the likelihood that it will be the next token. </p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Get prediction</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>  <code class="c1"># No need to track gradients for prediction</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">input_tensor</code><code class="p">)</code></pre>
          <p>This will return the probabilities for each word in the corpus, so you should pass the results to <code>torch.argmax</code> to get the most likely one:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Get the predicted word index (highest probability)</code>
<code class="n">predicted_idx</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">output</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
 </pre>
          <p>This should give you the value <code>6</code>. If you look at the word index, you’ll see that it’s the word “one”:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="p">{</code><code class="s1">'</code><code class="s1">UNK</code><code class="s1">'</code><code class="p">:</code> <code class="mi">0</code><code class="p">,</code> <code class="s1">'</code><code class="s1">in</code><code class="s1">'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'</code><code class="s1">the</code><code class="s1">'</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'</code><code class="s1">town</code><code class="s1">'</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code> <code class="s1">'</code><code class="s1">of</code><code class="s1">'</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s1">'</code><code class="s1">Athy</code><code class="s1">'</code><code class="p">:</code> <code class="mi">5</code><code class="p">,</code> 
 <strong><code class="s1">'</code><code class="s1">one</code><code class="s1">'</code></strong><strong><code class="p">:</code></strong><strong> <code class="mi">6</code></strong><strong><code class="p">,</code></strong> <code class="s1">'</code><code class="s1">Jeremy</code><code class="s1">'</code><code class="p">:</code> <code class="mi">7</code><code class="p">,</code> <code class="s1">'</code><code class="s1">Lanigan</code><code class="s1">'</code><code class="p">:</code> <code class="mi">8</code><code class="p">,</code>
 </pre>
          <p>You can also look it up in code by searching through the word index items until you find the predicted word and printing it out. You can do this by creating a reverse dictionary (mapping the value to the word, instead of vice-versa):</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Create reverse dictionary to convert number back to word</code>
<code class="n">reverse_dict</code> <code class="o">=</code> <code class="p">{</code><code class="n">v</code><code class="p">:</code> <code class="n">k</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">word_dict</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code>
 
<code class="c1"># Convert predicted index to word</code>
<code class="n">predicted_word</code> <code class="o">=</code> <code class="n">reverse_dict</code><code class="p">[</code><code class="n">predicted_idx</code><code class="p">]</code>
 </pre>
          <p>So, starting from the text “in the town of Athy,” the network predicted that the next word should be “one”—which, if you look at the training data, is correct because the song begins with this line:</p>
          <ul class="simplelist">
            <li>
              <p><em>In the town of Athy</em> one Jeremy Lanigan</p>
            </li>
            <li>
              <p>Battered away til he hadn’t a pound</p>
            </li>
          </ul>
          <p>Indeed, if you check the top five predictions based on their values in the index, you’ll get something like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">Top</code> <code class="mi">5</code> <code class="n">predictions</code><code class="p">:</code>
<code class="n">one</code><code class="p">:</code> <code class="mf">0.9999</code>
<code class="n">youll</code><code class="p">:</code> <code class="mf">0.0000</code>
<code class="n">didnt</code><code class="p">:</code> <code class="mf">0.0000</code>
<code class="n">creature</code><code class="p">:</code> <code class="mf">0.0000</code>
<code class="n">nelly</code><code class="p">:</code> <code class="mf">0.0000</code></pre>
          <p>Now that you’ve confirmed that the model is working, you can get creative and use different seed text. For example, when I used the seed text “sweet Jeremy saw Dublin,” the next word it predicted was “his.” </p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">Top</code> <code class="mi">5</code> <code class="n">predictions</code><code class="p">:</code>
<code class="n">his</code><code class="p">:</code> <code class="mf">0.7782</code>
<code class="n">go</code><code class="p">:</code> <code class="mf">0.1393</code>
<code class="n">bellows</code><code class="p">,:</code> <code class="mf">0.0605</code>
<code class="n">accident</code><code class="p">:</code> <code class="mf">0.0090</code>
<code class="n">til</code><code class="p">:</code> <code class="mf">0.0048</code></pre>
          <p>This text was chosen because all of those words are in the corpus. In such cases, you should expect more accurate results, at least at the beginning, for the predicted words.<a contenteditable="false" data-primary="" data-startref="ch8pred" data-type="indexterm" id="id1353"/><a contenteditable="false" data-primary="" data-startref="ch8pred2" data-type="indexterm" id="id1354"/><a contenteditable="false" data-primary="" data-startref="ch8pred3" data-type="indexterm" id="id1355"/></p>
        </div></section>
        <section data-pdf-bookmark="Compounding Predictions to Generate Text" data-type="sect2"><div class="sect2" id="ch08_compounding_predictions_to_generate_text_1748549671853066">
          <h2>Compounding Predictions to Generate Text</h2>
          <p>In the previous section, you saw<a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="compounding predictions to generate text" data-type="indexterm" id="ch8cmp"/><a contenteditable="false" data-primary="text generator" data-secondary="generating text" data-tertiary="compounding predictions" data-type="indexterm" id="ch8cmp2"/><a contenteditable="false" data-primary="words" data-secondary="predicting the next word" data-tertiary="compounding predictions to generate text" data-type="indexterm" id="ch8cmp3"/> how to use the model to predict the next word given a seed text. Now, to have the neural network create new text, you simply repeat the prediction, adding new words each time.</p>
          <p>For example, earlier, when I used the phrase “sweet Jeremy saw Dublin,” it predicted that the next word would be “his.” You can build on this by appending “his” to the seed text to get “sweet Jeremy saw Dublin his” and getting another prediction. Repeating this process will give you an AI-created string of text.</p>
          <p>Here’s the updated code from the previous section that performs this loop a number of times, with the number set by the <code>num_words</code> parameter:</p>
          <pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">generate_sequence</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">initial_text</code><code class="p">,</code> <code class="n">word_dict</code><code class="p">,</code> 
                      <code class="n">sequence_length</code><code class="p">,</code> <code class="n">num_words</code><code class="o">=</code><code class="mi">10</code><code class="p">):</code>
    <code class="c1"># Set model to evaluation mode</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
 
    <code class="c1"># Start with the initial text</code>
    <code class="n">current_text</code> <code class="o">=</code> <code class="n">initial_text</code>
    <code class="n">generated_sequence</code> <code class="o">=</code> <code class="n">initial_text</code>
 
    <code class="c1"># Create reverse dictionary for converting numbers back to words</code>
    <code class="n">reverse_dict</code> <code class="o">=</code> <code class="p">{</code><code class="n">v</code><code class="p">:</code> <code class="n">k</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">word_dict</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code>
 
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Initial text: </code><code class="si">{</code><code class="n">initial_text</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
 
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_words</code><code class="p">):</code>
        <code class="c1"># Convert current text to lowercase and split into words</code>
        <code class="n">words</code> <code class="o">=</code> <code class="n">current_text</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
 
        <code class="c1"># Take the last 'sequence_length' words if we exceed it</code>
        <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">words</code><code class="p">)</code> <code class="o">&gt;</code> <code class="n">sequence_length</code><code class="p">:</code>
            <code class="n">words</code> <code class="o">=</code> <code class="n">words</code><code class="p">[</code><code class="o">-</code><code class="n">sequence_length</code><code class="p">:]</code>
 
        <code class="c1"># Convert words to numbers using the word dictionary, use 0 for unknown</code>
        <code class="n">number_sequence</code> <code class="o">=</code> <code class="p">[</code><code class="n">word_dict</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code><code class="p">]</code>
 
        <code class="c1"># Pad the sequence</code>
        <code class="n">padded_sequence</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="p">(</code><code class="n">sequence_length</code> <code class="err">–</code> <code class="nb">len</code><code class="p">(</code><code class="n">number_sequence</code><code class="p">))</code> 
                                                 <code class="o">+</code> <code class="n">number_sequence</code>
 
        <code class="c1"># Convert to PyTorch tensor and add batch dimension</code>
        <code class="n">input_tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">LongTensor</code><code class="p">([</code><code class="n">padded_sequence</code><code class="p">])</code>
 
        <code class="c1"># Get prediction</code>
        <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
            <code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">input_tensor</code><code class="p">)</code>
 
        <code class="c1"># Get the predicted word index (highest probability)</code>
        <code class="n">predicted_idx</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">output</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
 
        <code class="c1"># Convert predicted index to word</code>
        <code class="n">predicted_word</code> <code class="o">=</code> <code class="n">reverse_dict</code><code class="p">[</code><code class="n">predicted_idx</code><code class="p">]</code>
 
        <code class="c1"># Add the predicted word to the sequence</code>
        <code class="n">generated_sequence</code> <code class="o">+=</code> <code class="s2">" "</code> <code class="o">+</code> <code class="n">predicted_word</code>
 
        <code class="c1"># Update current text for next prediction</code>
        <code class="n">current_text</code> <code class="o">=</code> <code class="n">generated_sequence</code>
 
        <code class="c1"># Print progress</code>
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Generated word </code><code class="si">{</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="si">}</code><code class="s2">: </code><code class="si">{</code><code class="n">predicted_word</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
 
        <code class="c1"># Optionally print top 5 predictions for each step</code>
        <code class="n">_</code><code class="p">,</code> <code class="n">top_indices</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">topk</code><code class="p">(</code><code class="n">output</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="mi">5</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Top 5 predictions for step </code><code class="si">{</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="si">}</code><code class="s2">:"</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">idx</code> <code class="ow">in</code> <code class="n">top_indices</code><code class="p">:</code>
            <code class="n">word</code> <code class="o">=</code> <code class="n">reverse_dict</code><code class="p">[</code><code class="n">idx</code><code class="o">.</code><code class="n">item</code><code class="p">()]</code>
            <code class="n">probability</code> <code class="o">=</code> <code class="n">output</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="n">idx</code><code class="p">]</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">word</code><code class="si">}</code><code class="s2">: </code><code class="si">{</code><code class="n">probability</code><code class="si">:</code><code class="s2">.4f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code> <code class="o">+</code> <code class="s2">"-"</code><code class="o">*</code><code class="mi">50</code> <code class="o">+</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code>
 
    <code class="k">return</code> <code class="n">generated_sequence</code>
 
<code class="c1"># Example usage:</code>
<code class="n">initial_text</code> <code class="o">=</code> <code class="s2">"sweet Jeremy saw Dublin"</code>
<code class="n">generated_text</code> <code class="o">=</code> <code class="n">generate_sequence</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">,</code>
    <code class="n">initial_text</code><code class="o">=</code><code class="n">initial_text</code><code class="p">,</code>
    <code class="n">word_dict</code><code class="o">=</code><code class="n">word_index</code><code class="p">,</code>
    <code class="n">sequence_length</code><code class="o">=</code><code class="n">max_sequence_len</code><code class="p">,</code>
    <code class="n">num_words</code><code class="o">=</code><code class="mi">10</code>
<code class="p">)</code>
 
<code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Final generated sequence:"</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">generated_text</code><code class="p">)</code>
 </pre>

<p>This will end up creating a string something like this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">sweet</code> <code class="n">jeremy</code> <code class="n">saw</code> <code class="n">dublin</code> <code class="n">his</code> <code class="n">right</code> <code class="n">leg</code> <code class="n">acres</code> <code class="n">of</code> <code class="n">the</code> <code class="n">nolans</code><code class="p">,</code> <code class="n">dolans</code><code class="p">,</code> <code class="n">daughter</code><code class="p">,</code> <code class="n">of</code>
</pre>
          <p>It rapidly descends into gibberish.<a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="gibberish produced" data-type="indexterm" id="id1356"/><a contenteditable="false" data-primary="text generator" data-secondary="generating text" data-tertiary="gibberish produced" data-type="indexterm" id="id1357"/><a contenteditable="false" data-primary="words" data-secondary="predicting the next word" data-tertiary="gibberish produced" data-type="indexterm" id="id1358"/> Why? The first reason is that the body of training text is really small, so it has very little context to work with. The second is that the prediction of the next word in the sequence depends on the previous words in the sequence, and if there is a poor match on the previous ones, even the best “next” match will have a low probability of being accurate. When you add this to the sequence and predict the next word after that, the likelihood of it having a low probability of accuracy is even higher—thus, the predicted words will seem semirandom.</p>
          <p>So, for example, while all of the words in the phrase “sweet Jeremy saw Dublin” exist in the corpus, they never exist in that order. When the model made the first prediction, it chose the word “his” as the most likely candidate, and it had quite a high probability of accuracy (78%): </p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">Initial</code> <code class="n">text</code><code class="p">:</code> <code class="n">sweet</code> <code class="n">Jeremy</code> <code class="n">saw</code> <code class="n">Dublin</code>
<code class="n">Generated</code> <code class="n">word</code> <code class="mi">1</code><code class="p">:</code> <code class="n">his</code>
 
<code class="n">Top</code> <code class="mi">5</code> <code class="n">predictions</code> <code class="k">for</code> <code class="n">step</code> <code class="mi">1</code><code class="p">:</code>
<code class="n">his</code><code class="p">:</code> <code class="mf">0.7782</code>
<code class="n">go</code><code class="p">:</code> <code class="mf">0.1393</code>
<code class="n">bellows</code><code class="p">,:</code> <code class="mf">0.0605</code>
<code class="n">accident</code><code class="p">:</code> <code class="mf">0.0090</code>
<code class="n">til</code><code class="p">:</code> <code class="mf">0.0048</code></pre>
          <p>When the model added that word to the seed to get “sweet Jeremy saw Dublin his,” we had another phrase not seen in the training data, so the prediction gave the highest probability to the word “right,” at 44%:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">Generated</code> <code class="n">word</code> <code class="mi">2</code><code class="p">:</code> <code class="n">right</code>
 
<code class="n">Top</code> <code class="mi">5</code> <code class="n">predictions</code> <code class="k">for</code> <code class="n">step</code> <code class="mi">2</code><code class="p">:</code>
<code class="n">right</code><code class="p">:</code> <code class="mf">0.7678</code>
<code class="n">pipes</code><code class="p">,:</code> <code class="mf">0.1376</code>
<code class="n">creature</code><code class="p">:</code> <code class="mf">0.0458</code>
<code class="n">didnt</code><code class="p">:</code> <code class="mf">0.0136</code>
<code class="n">youll</code><code class="p">:</code> <code class="mf">0.0113</code></pre>
          <p>While occasionally there will be high certainty of a token following another (like “leg” following “right”), over time, you’ll see that continuing to add words to the sentence reduces the likelihood of a match in the training data, and as such, the prediction accuracy will suffer—leading to there being a more random “feel” to the words being predicted.</p>
          <p>This leads to the phenomenon of AI-generated content getting increasingly nonsensical over time. </p>
          <p>For an example, check out<a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="Sunspring example online" data-type="indexterm" id="id1359"/><a contenteditable="false" data-primary="text generator" data-secondary="generating text" data-tertiary="Sunspring example online" data-type="indexterm" id="id1360"/><a contenteditable="false" data-primary="words" data-secondary="predicting the next word" data-tertiary="Sunspring example online" data-type="indexterm" id="id1361"/><a contenteditable="false" data-primary="online resources" data-secondary="Sunspring example of generated text" data-type="indexterm" id="id1362"/> the excellent sci-fi short <a href="https://oreil.ly/hTBtJ"><em>Sunspring</em></a>, which was written entirely by an LSTM-based network (like the one you’re building here) that was trained on science fiction movie scripts. The model was given seed content and tasked with generating a new script. The results were hilarious, and you’ll see that while the initial content makes sense, as the movie progresses, it becomes less and less comprehensible.</p>
          <p>This is also the basis of <em>hallucination</em> in LLMs, a common phenomenon that reduces trust in them.<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="hallucinations" data-type="indexterm" id="id1363"/><a contenteditable="false" data-primary="hallucinations from LLMs" data-type="indexterm" id="id1364"/><a contenteditable="false" data-primary="" data-startref="ch8gentxt" data-type="indexterm" id="id1365"/><a contenteditable="false" data-primary="" data-startref="ch8gentxt2" data-type="indexterm" id="id1366"/><a contenteditable="false" data-primary="" data-startref="ch8cmp" data-type="indexterm" id="id1367"/><a contenteditable="false" data-primary="" data-startref="ch8cmp2" data-type="indexterm" id="id1368"/><a contenteditable="false" data-primary="" data-startref="ch8cmp3" data-type="indexterm" id="id1369"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Extending the Dataset" data-type="sect1"><div class="sect1" id="ch08_extending_the_dataset_1748549671853131">
        <h1>Extending the Dataset</h1>
        <p>You can easily extend the same<a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="dataset extended" data-type="indexterm" id="id1370"/><a contenteditable="false" data-primary="text generator" data-secondary="dataset extended" data-type="indexterm" id="id1371"/> pattern that you used for the hardcoded dataset to use a text file. I’ve hosted a text file containing about 1,700 lines of text gathered from a number of songs that you can use for experimentation. With a little modification, you can use this instead of the single hardcoded song.</p>
        <p>To download the data in Colab, use the following code:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="err">!</code><code class="n">wget</code> <code class="o">--</code><code class="n">no</code><code class="o">-</code><code class="n">check</code><code class="o">-</code><code class="n">certificate</code> \
    <code class="n">https</code><code class="p">:</code><code class="o">//</code><code class="n">storage</code><code class="o">.</code><code class="n">googleapis</code><code class="o">.</code><code class="n">com</code><code class="o">/</code><code class="n">learning</code><code class="o">-</code><code class="n">datasets</code><code class="o">/</code> \
    <code class="n">irish</code><code class="o">-</code><code class="n">lyrics</code><code class="o">-</code><code class="n">eof</code><code class="o">.</code><code class="n">txt</code><code class="o">-</code><code class="n">O</code> <code class="o">/</code><code class="n">tmp</code><code class="o">/</code><code class="n">irish</code><code class="o">-</code><code class="n">lyrics</code><code class="o">-</code><code class="n">eof</code><code class="o">.</code><code class="n">txt</code></pre>
        <p>Then, you can simply load the text from it into your corpus like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">data</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="s1">'/tmp/irish-lyrics-eof.txt'</code><code class="p">)</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>
<code class="n">corpus</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code></pre>
        <p>The rest of your code will then work with very little modification!</p>
        <p>Training this for 50,000 epochs—which takes about 30 minutes on a T4 Colab instance—brings you to about 30% accuracy, with the curve flattening out (see <a data-type="xref" href="#ch08_figure_6_1748549671838028">Figure 8-6</a>).</p>
        <figure><div class="figure" id="ch08_figure_6_1748549671838028">
          <img src="assets/aiml_0806.png"/>
          <h6><span class="label">Figure 8-6. </span>Training on a larger dataset</h6>
        </div></figure>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>When using Google Colab,<a contenteditable="false" data-primary="Google Colab" data-secondary="accelerators available" data-type="indexterm" id="id1372"/><a contenteditable="false" data-primary="accelerators" data-secondary="Google Colab accelerators" data-type="indexterm" id="id1373"/> you can choose various accelerators on the backend, which can make your training go much faster. In this case, as noted, I used a T4. To do this for yourself, when in Colab, under the Connect menu, you’ll see a Change Runtime Type option. Select that, and you’ll see the accelerators available for you to use.</p>
        </div>
        <p>Trying the phrase “in the town of Athy” again yields a prediction of “one” but this time with just over 83% probability:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">Initial</code> <code class="n">text</code><code class="p">:</code> <code class="ow">in</code> <code class="n">the</code> <code class="n">town</code> <code class="n">of</code> <code class="n">Athy</code>
<code class="n">Using</code> <code class="n">device</code><code class="p">:</code> <code class="n">cuda</code>
<code class="n">Generated</code> <code class="n">word</code> <code class="mi">1</code><code class="p">:</code> <code class="n">one</code>
 
<code class="n">Top</code> <code class="mi">5</code> <code class="n">predictions</code> <code class="k">for</code> <code class="n">step</code> <code class="mi">1</code><code class="p">:</code>
<code class="n">one</code><code class="p">:</code> <code class="mf">0.8318</code>
<code class="ow">is</code><code class="p">:</code> <code class="mf">0.1648</code>
<code class="n">she</code><code class="p">:</code> <code class="mf">0.0016</code>
<code class="n">thee</code><code class="p">:</code> <code class="mf">0.0013</code>
<code class="n">that</code><code class="p">:</code> <code class="mf">0.0003</code>
 
<code class="o">--------------------------------------------------</code>
 
<code class="n">Generated</code> <code class="n">word</code> <code class="mi">2</code><code class="p">:</code> <code class="n">my</code>
 
<code class="n">Top</code> <code class="mi">5</code> <code class="n">predictions</code> <code class="k">for</code> <code class="n">step</code> <code class="mi">2</code><code class="p">:</code>
<code class="n">my</code><code class="p">:</code> <code class="mf">0.9377</code>
<code class="n">of</code><code class="p">:</code> <code class="mf">0.0622</code>
<code class="ow">is</code><code class="p">:</code> <code class="mf">0.0001</code>
<code class="n">that</code><code class="p">:</code> <code class="mf">0.0000</code>
<code class="n">one</code><code class="p">:</code> <code class="mf">0.0000</code></pre>
        <p>Running for a few more tokens, we can see output like the following. It’s beginning to create songs, though it’s quite quickly descending into gibberish!</p>
        
<pre data-code-language="python" data-type="programlisting">
<code class="ow">in</code> <code class="n">the</code> <code class="n">town</code> <code class="n">of</code> <code class="n">athy</code> <code class="n">one</code> <code class="n">my</code> <code class="n">heart</code>
<code class="n">was</code> <code class="n">they</code> <code class="n">were</code> <code class="n">the</code> <code class="n">a</code> <code class="n">reflections</code>
<code class="n">on</code> <code class="n">me</code> <code class="nb">all</code> <code class="n">the</code> <code class="n">frivolity</code><code class="p">;</code>
<code class="n">of</code> <code class="n">me</code> <code class="ow">and</code> <code class="n">me</code> <code class="ow">and</code> <code class="n">me</code> <code class="ow">and</code> <code class="n">the</code>
<code class="n">there</code> <code class="n">was</code> <code class="n">my</code> <code class="n">heart</code> <code class="n">was</code>
<code class="n">on</code> <code class="n">the</code> <code class="n">a</code> <code class="n">over</code> <code class="n">the</code> <code class="n">frivolity</code><code class="p">;</code></pre>

<p>For “sweet Jeremy saw Dublin,” the predicted next word is “she,” with a probability of 80%. Predicting the next few words yields this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">sweet</code> <code class="n">jeremy</code> <code class="n">saw</code> <code class="n">dublin</code> <code class="n">she</code> <code class="n">of</code> <code class="n">his</code> <code class="n">on</code> <code class="n">the</code> <code class="n">frivolity</code><code class="p">;</code> <code class="n">of</code> <code class="n">a</code> <code class="n">heart</code> <code class="ow">is</code> <code class="n">the</code> <code class="n">ground</code>
</pre>
        <p>It’s looking a little better! But can we improve it further?</p>
      </div></section>
      <section data-pdf-bookmark="Improving the Model Architecture" data-type="sect1"><div class="sect1" id="ch08_improving_the_model_architecture_1748549671853198">
        <h1>Improving the Model Architecture</h1>
        <p>One way that you can improve<a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="improving the model architecture" data-type="indexterm" id="ch8imp"/><a contenteditable="false" data-primary="text generator" data-secondary="improving the model architecture" data-type="indexterm" id="ch8imp2"/><a contenteditable="false" data-primary="long short-term memory (LSTM)" data-secondary="text generator using" data-tertiary="improving model architecture" data-type="indexterm" id="ch8imp3"/> the model is to change its architecture, using multiple stacked LSTMs and some other optimization techniques. Given that there’s no clear benchmark for accuracy with a dataset like this one—there’s no right or wrong classification, nor is there a target regression—it’s difficult to establish when a model is good or bad. Therefore, accuracy results can be very subjective.</p>
        <p>That being said, they’re still a good yardstick, so in this section, I’m going to explore some additions you can make to the architecture to improve the accuracy metric.</p>
        <section data-pdf-bookmark="Embedding Dimensions" data-type="sect2"><div class="sect2" id="ch08_embedding_dimensions_1748549671853262">
          <h2>Embedding Dimensions</h2>
          <p>In <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a>, we discussed<a contenteditable="false" data-primary="text generator" data-secondary="improving the model architecture" data-tertiary="embedding dimensions" data-type="indexterm" id="id1374"/> the fact that the optimal dimension for embeddings is the fourth root of the number of words. In this scenario, the vocabulary has 3,259 words, and the fourth root of this is approximately 8. Another rule of thumb is the log of this number—and log(3259) is a little over 32. So, if the network is learning slowly, you have the option to pick a number between these two values. That gives you enough “room” to capture the relationships between words.</p>
        </div></section>
        <section data-pdf-bookmark="Initializing the LSTMs" data-type="sect2"><div class="sect2" id="ch08_initializing_the_lstms_1748549671853320">
          <h2>Initializing the LSTMs</h2>
          <p>Often, parameters in a neural network<a contenteditable="false" data-primary="text generator" data-secondary="improving the model architecture" data-tertiary="initializing the LSTMs" data-type="indexterm" id="id1375"/> are initialized to zero. You can give learning a little bit of a kickstart by initializing different layers to different types supported by various research findings. We briefly cover these types of layers in the following subsections.</p>
          <section data-pdf-bookmark="Embedding layers" data-type="sect3"><div class="sect3" id="ch08_embedding_layers_1748549671853381">
            <h3>Embedding layers</h3>
            <p>Embedding layers can be initialized with a normal distribution, with a standard deviation scaled by <code>1/sqrt(embedding_dim)</code> for better gradient flow. That’s similar to <code>word2vec</code>-style initialization.</p>
          </div></section>
          <section data-pdf-bookmark="LSTM layers" data-type="sect3"><div class="sect3" id="ch08_lstm_layers_1748549671853453">
            <h3>LSTM layers</h3>
            <p>LSTM has four internal neural types—input, forget, cell, and output—and their weight matrix is a bunch of them stacked together. The different types benefit from different initializations. <a contenteditable="false" data-primary="“An Empirical Exploration of Recurrent Network Architectures” (Jozefowicz et al.)" data-primary-sortas="Empirical Exploration" data-type="indexterm" id="id1376"/><a contenteditable="false" data-primary="Jozefowicz, Rafal" data-type="indexterm" id="id1377"/><a contenteditable="false" data-primary="“On the difficulty of training recurrent neural networks” (Pascanu et al.)" data-primary-sortas="On the difficulty of training" data-type="indexterm" id="id1378"/><a contenteditable="false" data-primary="Pascanu, Razvan" data-type="indexterm" id="id1379"/><a contenteditable="false" data-primary="online resources" data-secondary="LSTM internal neural types information" data-type="indexterm" id="id1380"/>Two great papers that discuss this are <a href="https://oreil.ly/UuvQO">“An Empirical Exploration of Recurrent Network Architectures” by Rafal Jozefowicz et al.</a>  and <a href="https://oreil.ly/Ttvll">“On the Difficulty of Training Recurrent Neural Networks”</a>  by Razvan Pascanu et al. The specifics of LSTM are beyond the scope of this chapter, but check the associated code for one methodology to initialize them.</p>
          </div></section>
          <section data-pdf-bookmark="Final linear layer" data-type="sect3"><div class="sect3" id="ch08_final_linear_layer_1748549671853515">
            <h3>Final linear layer</h3>
            <p>In their 2015 paper “<a href="https://oreil.ly/_MM6A">Delving Deep into Rectifiers,” Kaiming He et al.</a> explored <a contenteditable="false" data-primary="“Delving Deep into Rectifiers” (He et al.)" data-primary-sortas="Delving Deep" data-type="indexterm" id="id1381"/><a contenteditable="false" data-primary="He, Kaiming" data-type="indexterm" id="id1382"/><a contenteditable="false" data-primary="Kaiming initialization of linear layers" data-type="indexterm" id="id1383"/><a contenteditable="false" data-primary="He initialization of linear layers" data-type="indexterm" id="id1384"/><a contenteditable="false" data-primary="online resources" data-secondary="Kaiming initialization of linear layers" data-type="indexterm" id="id1385"/><a contenteditable="false" data-primary="layers" data-secondary="linear layers" data-tertiary="Kaiming initialization of" data-type="indexterm" id="id1386"/><a contenteditable="false" data-primary="linear layers" data-secondary="Kaiming initialization of" data-type="indexterm" id="id1387"/><a contenteditable="false" data-primary="GitHub" data-secondary="Kaiming initialization of linear layers code" data-type="indexterm" id="id1388"/>initialization of linear layers and proposed “Kaiming” initialization (aka “He” initialization). A detailed explanation of this is beyond the scope of this book, but the code is available in the notebooks in the <a href="https://github.com/lmoroney/PyTorch-Book-FIles">GitHub repository</a>.</p>
          </div></section>
        </div></section>
        <section class="pagebreak-before less_space" data-pdf-bookmark="Variable Learning Rate" data-type="sect2"><div class="sect2" id="ch08_variable_learning_rate_1748549671853576">
          <h2>Variable Learning Rate</h2>
          <p>In every example we’ve seen so far,<a contenteditable="false" data-primary="learning rate (LR)" data-secondary="variable learning rate" data-type="indexterm" id="id1389"/><a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="variable learning rate" data-type="indexterm" id="id1390"/><a contenteditable="false" data-primary="text generator" data-secondary="improving the model architecture" data-tertiary="variable learning rate" data-type="indexterm" id="id1391"/> we’ve explored different learning rates and their impact on the network—but you can actually <em>vary</em> the learning rate as the network learns. Values that work well in early epochs may not work so well in later ones, so putting together a scheduler that adjusts this learning rate epoch by epoch can help you create networks that learn more effectively.</p>
          <p>For this, PyTorch provides a <code>torch.optim.lr_scheduler</code> that you can program to change over the course of the training:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">scheduler</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">lr_scheduler</code><code class="o">.</code><code class="n">OneCycleLR</code><code class="p">(</code>
    <code class="n">optimizer</code><code class="p">,</code>
    <code class="n">max_lr</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code>              <code class="c1"># Peak learning rate</code>
    <code class="n">epochs</code><code class="o">=</code><code class="mi">20000</code><code class="p">,</code>             <code class="c1"># Total epochs</code>
    <code class="n">steps_per_epoch</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>        <code class="c1"># Steps per epoch </code>
    <code class="n">pct_start</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code>           <code class="c1"># Percentage of training spent increasing lr</code>
    <code class="n">div_factor</code><code class="o">=</code><code class="mf">10.0</code><code class="p">,</code>         <code class="c1"># Initial lr = max_lr/10</code>
    <code class="n">final_div_factor</code><code class="o">=</code><code class="mf">1000.0</code>  <code class="c1"># Final lr = initial_lr/1000</code>
<code class="p">)</code></pre>
          <p>In <a data-type="xref" href="ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648">Chapter 7</a>, we looked at the idea of a <em>learning rate</em> (LR), which is a hyperparameter that, if set to be too large, will cause the network to overlearn, and if set to be too small will prevent the network from learning effectively. The nice thing about this is that you can set it as a variable rate, which we do here. In the early epochs, we want the network to learn fast, so we have a large LR—and in the later epochs, we don’t want it to overfit, so we gradually reduce the LR.</p>
          <p>The <code>pct_start</code> parameter defines a warm-up period as the first 10% of the training, during which the learning rate gradually increases up to the maximum (in this case, 0.01) and then decreases to 1/1000 of the initial learning rate (determined by <code>final_div_factor</code>). </p>
          <p>You can see the impact this has on training in <a data-type="xref" href="#ch08_figure_7_1748549671838042">Figure 8-7</a>, where it reached ~90% accuracy in 6,800 epochs before triggering early stopping.</p>
          <figure><div class="figure" id="ch08_figure_7_1748549671838042">
            <img src="assets/aiml_0807.png"/>
            <h6><span class="label">Figure 8-7. </span>Adding a second LSTM layer</h6>
          </div></figure>
          <p>This time, when testing with the same phrases as before, I got “little” as the next word after “in the town of Athy” with a 26% probability, and I got “one” as the next word after “sweet Jeremy saw Dublin” with a 32% probability. Again, when predicting more words, the output quickly descended into gibberish.</p>

<p>Here are some examples:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">sweet</code> <code class="n">jeremy</code> <code class="n">saw</code> <code class="n">dublin</code> <code class="n">one</code> <code class="n">evening</code> <code class="n">two</code> <code class="n">white</code> <code class="n">ever</code> <code class="n">we</code> <code class="n">once</code> <code class="n">to</code> <code class="k">raise</code> <code class="n">you</code><code class="p">,</code> 
<code class="n">tis</code> <code class="n">young</code> <code class="n">i</code> <code class="n">was</code> <code class="n">told</code> <code class="n">my</code> <code class="n">heart</code> <code class="k">as</code> <code class="n">found</code> <code class="n">has</code>

<code class="n">you</code> <code class="n">know</code> <code class="n">nothing</code> <code class="n">jon</code> <code class="n">snow</code> <code class="n">you</code> <code class="n">should</code> <code class="n">laugh</code> <code class="nb">all</code> <code class="n">the</code> <code class="k">while</code> <code class="n">at</code> <code class="n">me</code> <code class="n">curious</code> <code class="n">style</code><code class="p">,</code> 
<code class="n">twould</code> <code class="nb">set</code> <code class="n">your</code> <code class="n">heart</code> <code class="n">a</code> <code class="n">bubblin</code> <code class="n">will</code> <code class="n">lámh</code><code class="o">.</code> <code class="n">you</code> <code class="n">that</code>

<code class="ow">in</code> <code class="n">the</code> <code class="n">town</code> <code class="n">of</code> <code class="n">athy</code> <code class="n">one</code> <code class="n">jeremy</code> <code class="n">lanigan</code> <code class="n">do</code> <code class="n">lámh</code><code class="o">.</code> <code class="n">pretty</code> <code class="n">generation</code> <code class="n">her</code> <code class="n">soul</code><code class="p">,</code> 
<code class="n">fell</code> <code class="n">on</code> <code class="n">the</code> <code class="n">stony</code> <code class="n">ground</code> <code class="n">red</code> <code class="n">we</code> <code class="n">were</code> <code class="n">feeble</code> <code class="n">was</code> <code class="n">down</code>
</pre>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The word <code>lámh</code> shows up in this text, and it’s Gaelic for <em>hand</em>. And <code>do lámh</code> means <em>your hand</em>.</p>
</div>
          <p>If you get different results, don’t worry—you didn’t do anything wrong, but the random initialization of the neurons will impact the final scores.<a contenteditable="false" data-primary="" data-startref="ch8imp" data-type="indexterm" id="id1392"/><a contenteditable="false" data-primary="" data-startref="ch8imp2" data-type="indexterm" id="id1393"/><a contenteditable="false" data-primary="" data-startref="ch8imp3" data-type="indexterm" id="id1394"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Improving the Data" data-type="sect1"><div class="sect1" id="ch08_improving_the_data_1748549671853647">
        <h1>Improving the Data</h1>
        <p>There’s a little trick that you<a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="improving the data" data-type="indexterm" id="ch8impdat"/><a contenteditable="false" data-primary="text generator" data-secondary="improving the data" data-type="indexterm" id="ch8impdat2"/><a contenteditable="false" data-primary="windowing the data" data-type="indexterm" id="ch8impdat3"/><a contenteditable="false" data-primary="text datasets for natural language processing" data-secondary="windowing the data" data-type="indexterm" id="ch8impdat4"/><a contenteditable="false" data-primary="datasets" data-secondary="natural language processing" data-tertiary="windowing text data" data-type="indexterm" id="ch8impdat5"/> can use to extend the size of this dataset without adding any new songs. It’s called <em>windowing</em> the data. Right now, every line in every song is read as a single line and then turned into input sequences, as you saw in <a data-type="xref" href="#ch08_figure_2_1748549671837934">Figure 8-2</a>. While humans read songs line by line to hear rhyme and meter, the model doesn’t have to, in particular when using bidirectional LSTMs.</p>
        <p>So, instead of taking the line “In the town of Athy, one Jeremy Lanigan,” processing that, and then moving to the next line (“Battered away ‘til he hadn’t a pound”) and processing that, we could treat all the lines as one long, continuous text. We could then create a “window” into that text of <em>n</em> words, process that, and then move the window forward one word to get the next input sequence (see <a data-type="xref" href="#ch08_figure_8_1748549671838067">Figure 8-8</a>).</p>
        <p>In this case, far more training data can be yielded in the form of an increased number of input sequences. Moving the window across the entire corpus of text would give us ((<em>number_of_words</em> – <em>window_size</em>) × <em>window_size</em>) input sequences that we could train with.</p>
                <figure><div class="figure" id="ch08_figure_8_1748549671838067">
          <img alt="" src="assets/aiml_0808.png"/>
          <h6><span class="label">Figure 8-8. </span>A moving word window</h6>
        </div></figure>
        <p>The code is pretty simple—when loading the data, instead of splitting each song line into a “sentence,” we can create them on the fly from the words in the corpus:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">window_size</code><code class="o">=</code><code class="mi">10</code>
<code class="n">sentences</code><code class="o">=</code><code class="p">[]</code>
<code class="n">alltext</code><code class="o">=</code><code class="p">[]</code>
<code class="n">data</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="s1">'/tmp/irish-lyrics-eof.txt'</code><code class="p">)</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>
<code class="n">corpus</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code>
<code class="n">words</code> <code class="o">=</code> <code class="n">corpus</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">" "</code><code class="p">)</code>
<code class="n">range_size</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">words</code><code class="p">)</code><code class="o">-</code><code class="n">max_sequence_len</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">range_size</code><code class="p">):</code>
    <code class="n">thissentence</code><code class="o">=</code><code class="s2">""</code>
    <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">window_size</code><code class="o">-</code><code class="mi">1</code><code class="p">):</code>
        <code class="n">word</code> <code class="o">=</code> <code class="n">words</code><code class="p">[</code><code class="n">i</code><code class="o">+</code><code class="n">word</code><code class="p">]</code>
        <code class="n">thissentence</code> <code class="o">=</code> <code class="n">thissentence</code> <code class="o">+</code> <code class="n">word</code>
        <code class="n">thissentence</code> <code class="o">=</code> <code class="n">thissentence</code> <code class="o">+</code> <code class="s2">" "</code>
    <code class="n">sentences</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">thissentence</code><code class="p">)</code></pre>
        <p>In this case, because we no longer have sentences and we’re creating sequences that are the same size as the moving window, <code>max_sequence_len</code> is the size of the window. The full file is read, converted to lowercase, and split into an array of words using string splitting. The code then loops through the words and makes sentences of each word from the current index up to the current index plus the window size, adding each of those newly constructed sentences to the sentences array.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>To train this, you’ll likely need a higher-memory GPU. I used the 40Gb A100 that’s available in Colab.</p>
        </div>
        <p>When training, you’ll notice that the extra data makes training much slower per epoch, but the results are greatly improved and the generated text descends into gibberish much more slowly.</p>
        <p>Here’s an example that caught my eye—particularly the last line:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">you</code> <code class="n">know</code> <code class="n">nothing</code> <code class="n">jon</code> <code class="n">snow</code>
<code class="n">tell</code> <code class="n">the</code> <code class="n">loved</code> <code class="n">ones</code> <code class="ow">and</code> <code class="n">the</code> <code class="n">friends</code>
<code class="n">we</code> <code class="n">would</code> <code class="n">neer</code> <code class="n">see</code> <code class="n">again</code><code class="o">.</code>
<code class="ow">and</code> <code class="n">the</code> <code class="n">way</code> <code class="n">of</code> <code class="n">their</code> <code class="n">guff</code> <code class="n">again</code>
<code class="ow">and</code> <code class="n">high</code> <code class="n">tower</code> <code class="n">might</code> <code class="n">ask</code><code class="p">,</code>
<code class="ow">not</code> <code class="n">see</code> <code class="n">night</code> <code class="n">unseen</code></pre>
        <p>There are many hyperparameters you can try to tune. Changing the window size will change the amount of training data—a smaller window size can yield more data, but there will be fewer words to give to a label, so if you set it too small, you’ll end up with nonsensical poetry. You can also change the dimensions in the embedding, the number of LSTMs, or the size of the vocabulary to use for training. Given that percentage accuracy isn’t the best measurement—you’ll want to make a more subjective examination of how much “sense” the poetry makes—there’s no hard-and-fast rule to follow to determine whether your model is “good” or not. Of course, you <em>will</em> be <span class="keep-together">limited</span> by the amount of data you have and the compute that’s available to you—the bigger the model, the more power you will need.</p>
        <p>Ultimately, the important thing is to experiment and have fun!<a contenteditable="false" data-primary="" data-startref="ch8impdat" data-type="indexterm" id="id1395"/><a contenteditable="false" data-primary="" data-startref="ch8impdat2" data-type="indexterm" id="id1396"/><a contenteditable="false" data-primary="" data-startref="ch8impdat3" data-type="indexterm" id="id1397"/><a contenteditable="false" data-primary="" data-startref="ch8impdat4" data-type="indexterm" id="id1398"/><a contenteditable="false" data-primary="" data-startref="ch8impdat5" data-type="indexterm" id="id1399"/></p>
      </div></section>
      <section data-pdf-bookmark="Character-Based Encoding" data-type="sect1"><div class="sect1" id="ch08_character_based_encoding_1748549671853714">
        <h1>Character-Based Encoding</h1>
        <p>For the last few chapters,<a contenteditable="false" data-primary="generative AI" data-secondary="text generator" data-tertiary="character-based encoding" data-type="indexterm" id="id1400"/><a contenteditable="false" data-primary="text generator" data-secondary="character-based encoding" data-type="indexterm" id="id1401"/><a contenteditable="false" data-primary="character-based encoding" data-type="indexterm" id="id1402"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="character-based encoding" data-type="indexterm" id="id1403"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="character-based encoding" data-type="indexterm" id="id1404"/> we’ve been looking at NLP using word-based encoding. I find that much easier to get started with, but when it comes to generating text, you might also want to consider <em>character-based encoding</em> because the number of unique <em>characters</em> in a corpus tends to be a lot less than the number of unique <em>words</em>. If you use this approach, you can have a lot fewer neurons in your output layer, and your output predictions can be spread across fewer probabilities. <a contenteditable="false" data-primary="datasets" data-secondary="natural language processing" data-tertiary="Shakespeare’s complete works" data-type="indexterm" id="id1405"/><a contenteditable="false" data-primary="online resources" data-secondary="Shakespeare’s complete works" data-type="indexterm" id="id1406"/><a contenteditable="false" data-primary="Shakespeare’s complete works online" data-type="indexterm" id="id1407"/>For example, if you look at the dataset of <a href="https://oreil.ly/XW_ab">the complete works of Shakespeare</a>, you’ll see that there are only 65 unique characters in the entire set. Basically, Shakespeare only really used uppercase and lowercase letters and some punctuation to give a unique set of 65 characters!</p>
        <p>So, when you are making predictions with this dataset, instead of looking at the probabilities for the next word across 2,700 words as in the Irish songs dataset, you’re only looking at 65. This makes your model a bit simpler!</p>
        <p>What’s also nice about character-based encoding is that punctuation characters are also included, so line breaks, etc., can be predicted. As an example, when I used an RNN that was trained on the Shakespeare corpus to predict the text following on from my favorite <em>Game of Thrones</em> line, I got the following:</p>
        <ul class="simplelist">
          <li>
           <p>YGRITTE:</p>
          <ul class="simplelist">
          <li>
          <p>You know nothing, Jon Snow.</p>
          </li>
          <li>
           <p>Good night, we’ll prove those body’s servants to</p>
          </li>
          <li>
              <p><em>The traitor be these mine:</em></p>
          </li>
          <li>
              <p><em>So diswarl his body in hope in this resceins,</em></p>
          </li>
          <li>
          <p> I cannot judg appeal’t.</p>
          </li>
          </ul>
          </li>
          <li>
          <br/>
            <p>MENENIUS:</p>
          <ul class="simplelist">
          <li>
           <p> Why, ’tis pompetsion.</p>
          </li>
          </ul>
          </li>
          <li>
          <br/>
           <p>KING RICHARD II:</p>
          <ul class="simplelist">
          <li>
          <p>I think he make her thought on mine;</p>
          </li>
          <li>
            <p>She will not: suffer up thy bonds:</p>
          </li>
          <li>
         <p> How doched it, I pray the gott,</p>
          </li>
          <li>
          <p>We’ll no fame to this your love, and you were ends</p>
          </li>
        </ul>
        </li>
        </ul>
        <p>It’s kind of cool that she identifies him as a traitor and wants to tie him up (“diswarl his body”), but I have no idea what “resceins” means! If you watch the show, this is part of the plot, so maybe Shakespeare was on to something without realizing it!</p>
        <p>Of course, I do think we tend to be a little more forgiving when using something like Shakespeare’s texts as our training data, because the language is already a little <span class="keep-together">unfamiliar.</span></p>
        <p>As with the Irish songs model, the output from the Shakespeare dataset does quickly degenerate into nonsensical text, but it’s still fun to play with. <a contenteditable="false" data-primary="Google Colab" data-secondary="Shakespeare text generation" data-type="indexterm" id="id1408"/><a contenteditable="false" data-primary="Shakespeare’s complete works online" data-secondary="Google Colab" data-type="indexterm" id="id1409"/><a contenteditable="false" data-primary="online resources" data-secondary="Shakespeare’s complete works" data-tertiary="Google Colab TensorFlow based" data-type="indexterm" id="id1410"/><a contenteditable="false" data-primary="online resources" data-secondary="Shakespeare’s complete works" data-tertiary="Google Colab PyTorch based" data-type="indexterm" id="id1411"/><a contenteditable="false" data-primary="GitHub" data-secondary="Shakespeare text generation" data-type="indexterm" id="id1412"/>To try it for yourself, you can check out the <a href="https://oreil.ly/cbz9c">Colab</a>. This Colab is TensorFlow based, not PyTorch based. <a href="https://oreil.ly/kQ7aa">See the GitHub repository</a> for a similar example that gives different results but is PyTorch based.</p>
      </div></section>
      <section class="pagebreak-before less_space" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch08_summary_1748549671853772">
        <h1>Summary</h1>
        <p>In this chapter, we explored how to do basic text generation using a trained LSTM-based model. You learned how you can split text into training features and labels by using words as labels, and you also learned how create a model that, when given seed text, can predict the next likely word. Then, you iterated on this to improve the model for better results by exploring a dataset of traditional Irish songs. Hopefully, this was a fun introduction to how ML models can synthesize text and also gave you the knowledge you need to understand the foundational principles of generative AI. This approach was massively improved with the transformer architecture that underpins how LLM models like GPT and Gemini work!</p>
      </div></section>
    </div></section></body></html>