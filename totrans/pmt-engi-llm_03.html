<html><head></head><body><div id="book-content" class="calibre2"><div id="sbo-rt-content" class="calibre3"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Understanding LLMs" class="calibre6"><div class="preface" id="ch02_understanding_llms_1728407258904677">
<h1 class="calibre5"><span class="firstname">Chapter 2. </span>Understanding LLMs</h1>

<p class="subtitle">So you want to become the LLM whisperer who unlocks the wealth of their knowledge and processing power with clever prompts? Well, to appreciate which kinds of prompts <em class="hyperlink">are</em> clever and tease the right answer from the LLM, you first need to understand how LLMs process information―how they <em class="hyperlink">think</em>.</p>

<p class="subtitle">In this chapter, we’ll approach this problem onion style. You’ll first see LLMs from the very outside as trained mimics of text in <a data-type="xref" href="#ch02_what_are_llms_1728407258904985" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“What Are LLMs?”</a>. You’ll learn how they split the text into bite-size chunks called tokens in <a data-type="xref" href="#ch02_how_llms_see_the_world_1728407258905418" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“How LLMs See the World”</a>, and you’ll learn about the fallout if they can’t easily accomplish that split.</p>

<p class="subtitle">You’ll also find out how the token sequences are generated bit by bit in <a data-type="xref" href="#ch02_one_token_at_a_time_1728407258905818" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“One Token at a Time”</a>, and you’ll learn about the different ways to choose the next token in <a data-type="xref" href="#ch02_temperature_and_probabilities_1728407258905997" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“Temperature and Probabilities”</a>. Finally, in <a data-type="xref" href="#ch02_the_transformer_architecture_1728407258906064" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“The Transformer Architecture”</a>, you’ll delve into the very inner workings of an LLM, understand it as a collection of minibrains that communicate through a Q&amp;A game called<a contenteditable="false" data-primary="attention game" data-type="indexterm" id="id317" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">attention</em>, and learn what that means for prompt order.</p>

<p class="subtitle">During all that, please keep in mind that this is a book about <em class="hyperlink">using</em> LLMs, not about LLMs themselves. So, there are a lot of cool technical details that we’re <em class="hyperlink">not</em> mentioning because they’re not relevant for prompt engineering. If you want matrix multiplications and activation functions, you’ll need to turn elsewhere—the classic reference <a href="https://oreil.ly/9hGyN" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">The Illustrated Transformer</a> is an excellent starting point for a deep dive. But we promise you won’t need that amount of technical background if all you want to do is write great prompts―so let’s dive into what you do need to know.</p>

<section data-type="sect1" data-pdf-bookmark="What Are LLMs?" class="calibre6"><div class="preface" id="ch02_what_are_llms_1728407258904985">
<h1 class="calibre5">What Are LLMs?</h1>

<p class="subtitle">At<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="basic functioning of" data-type="indexterm" id="LLMbasic02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the most basic level, an <em class="hyperlink">LLM</em> is a service that takes a string and returns a string: text in, text out. The input is called the<a contenteditable="false" data-primary="prompts, definition of" data-type="indexterm" id="id318" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="completions" data-secondary="definition of term" data-type="indexterm" id="id319" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="response" data-type="indexterm" id="id320" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">prompt</em>, and the output is called the <em class="hyperlink">completion</em> or sometimes, the <em class="hyperlink">response</em> (see <a data-type="xref" href="#ch02_figure_1_1728407258873233" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-1</a>).</p>

<figure class="calibre22"><div id="ch02_figure_1_1728407258873233" class="figure"><img alt="A white oval with black text  Description automatically generated" src="assets/pefl_0201.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-1. </span>An LLM taking the prompt “One, Two,” and presenting the completion “ Buckle My Shoe”</h6>
</div></figure>

<p class="subtitle">When an untrained LLM first sees the light of day, its completions will look like a pretty random jumble of unicode symbols and bear no clear relationship to the prompt. It needs to be<a contenteditable="false" data-primary="training" data-type="indexterm" id="training02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">trained</em> before it’s useful. Then, the LLM won’t just answer strings with strings but language with language.</p>

<p class="subtitle">Training takes skill, compute, and time far beyond the scope of most project groups, so most LLM applications use off-the-shelf generalist models (known as<a contenteditable="false" data-primary="foundation models" data-type="indexterm" id="id321" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">foundation models</em>) that are already trained (maybe after a bit of<a contenteditable="false" data-primary="fine-tuning" data-type="indexterm" id="id322" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> fine-tuning; see the sidebar). So, we don’t expect you to train an LLM yourself―but if you want to use an LLM, especially programmatically, it is essential for you to understand what it <em class="hyperlink">has been trained</em> to do.</p>

<aside data-type="sidebar" epub:type="sidebar" class="calibre35"><div class="sidebar" id="ch02_what_is_fine_tuning_1728407258905057">
<h1 class="calibre36">What Is Fine-Tuning?</h1>

<p class="subtitle">Training LLMs takes lots of data and compute, although many basic lessons, such as the rules of English grammar, don’t differ much between the training sets. It’s therefore common not to start completely from scratch when training an LLM but to start with a copy of a different LLM, possibly one that’s trained on different documents.</p>

<p class="subtitle">For example, the early versions of<a contenteditable="false" data-primary="OpenAI Codex" data-type="indexterm" id="id323" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> OpenAI Codex (an LLM for producing source code that was developed for GitHub Copilot) were copies of an existing model (GPT-3, a natural language LLM) that were fine-tuned with lots of source code published on GitHub.</p>

<p class="subtitle">If you have such a model trained on dataset A and fine-tuned on dataset B, your prompts should normally be written as if it had been trained on B outright. We’ll delve deeper into fine-tuning in <a data-type="xref" href="ch07.html#ch07_taming_the_model_1728407187651669" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 7</a>.</p>
</div></aside>

<p class="subtitle">LLMs are trained using a large set of documents (again, strings) known as the <em class="hyperlink">training set</em>. The kind of documents depends on the purpose of the LLM (see <a data-type="xref" href="#ch02_figure_2_1728407258873266" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-2</a> for an example). The training set is often a mixture of different training inputs such as books, articles, conversations on platforms such as Reddit, and code on sites such as GitHub. From the training set, the model is supposed to learn how to produce output that looks just like the training set. Concretely, when the model receives a prompt that is the beginning of a document from its training set, the resulting completion should be the text that is most likely to continue the original document. In other words, models mimic.</p>

<figure class="calibre22"><div id="ch02_figure_2_1728407258873266" class="figure"><img alt="Points scored" src="assets/pefl_0202.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-2. </span>Composition<a contenteditable="false" data-primary="“The Pile” data set" data-primary-sortas="The Pile”; data set" data-type="indexterm" id="id324" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="“The Pile” data set" data-primary-sortas="Pile” data set" data-type="indexterm" id="id325" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> of <a href="https://oreil.ly/MbYsy" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“The Pile”</a>, a popular open source training set comprising a mixture of factual prose, fictional prose, dialogues, and other internet content</h6>
</div></figure>

<p class="subtitle">So, how’s an LLM different from, say, a big search engine index full of the training data? After all, a search engine would <em class="hyperlink">ace</em> the task the LLM was trained with—given the beginning of a document, it could find a completion for that document with 100% accuracy. And yet, having a search engine that just parrots the training set isn’t the goal here: the LLM shouldn’t learn to recite the training set by heart but to apply the patterns it encounters there (in particular, logical and reasoning patterns) to complete any prompt, not just those from the training set. Mere rote memorization is considered a defect. Both the inner architecture of the LLM (which encourages it to abstract from concrete examples) and the training procedure (which tries to feed it diverse, nonrepetitive data and measures success on unseen data) are supposed to prevent this defect.</p>

<p class="subtitle">That prevention sometimes fails, and instead of learning facts and patterns, the model learns chunks of text by rote—which is known as<a contenteditable="false" data-primary="overfitting" data-type="indexterm" id="id326" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">overfitting</em>. Large-scale overfitting should be rare in off-the-shelf models, but it’s worth being aware of the possibility that if an LLM seemingly solves a problem that it’s seen during training, it doesn’t necessarily mean that the LLM will do as well when confronted with a similar problem it hasn’t seen before.</p>

<p class="subtitle">Nevertheless, after you work with LLMs for a while, you start to develop an intuition for how an LLM will behave based on the task it was trained on. So when you want to know how a given prompt might be completed, don’t ask yourself how a reasonable person would “reply” to the prompt but rather how a document that happens to start with the prompt might continue.</p>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">Assume you have picked a document from the training set at random. All you know about it is, it starts with the prompt. What is the statistically most likely continuation? That’s the LLM output you should expect.<a contenteditable="false" data-primary="" data-startref="LLMbasic02" data-type="indexterm" id="id327" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="training02" data-type="indexterm" id="id328" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div>

<section data-type="sect2" data-pdf-bookmark="Completing a Document" class="calibre6"><div class="preface" id="ch02_completing_a_document_1728407258905189">
<h2 class="calibre19">Completing a Document</h2>

<p class="subtitle">Here’s<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="document completion" data-type="indexterm" id="id329" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="document completion" data-type="indexterm" id="id330" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> an example of reasoning about document completions. Consider the following text:</p>

<pre data-type="programlisting" class="calibre21">
Yesterday, my TV stopped working. Now, I can’t turn it on at</pre>

<p class="subtitle">For a text that starts like this, what might be the statistically most likely completion?</p>

<ol class="stafflist">
	<li class="calibre25">
	<p class="calibre26"><code class="calibre15">y2ior3w</code></p>
	</li>
	<li class="calibre25">
	<p class="calibre26"><code class="calibre15">Thursday.</code></p>
	</li>
	<li class="calibre25">
	<p class="calibre26"><code class="calibre15">all.</code></p>
	</li>
</ol>

<p class="subtitle">None of these completions are absolutely <em class="hyperlink">impossible</em>. Sometimes, a cat runs over the keyboard and completion 1 is generated, and other times, a sentence gets garbled in rewriting and 2 appears. But by far the most likely continuation is 3, and almost all LLMs will choose this continuation.</p>

<p class="subtitle">Let’s take completion 3 as given and run the LLM a bit further:</p>

<pre data-type="programlisting" class="calibre21">
Yesterday, my TV stopped working. Now, I can’t turn it on at all. </pre>

<p class="subtitle">For a text that starts like that, what is the statistically most likely completion?</p>

<ol class="stafflist">
 <li class="calibre25"><code class="calibre15">This is why I chose to settle down with a book tonight.</code></li>
 <li class="calibre25"><code class="calibre15">Shall we watch the game at your place instead?</code></li>
<li class="calibre25">
	<p class="calibre26"><code class="calibre15">\n</code></p>
	<p class="calibre26"><code class="calibre15">\n</code></p>
	<p class="calibre26"><code class="calibre15">First, try unplugging the TV from the wall and plugging it back in.</code></p>
</li>
</ol>

<p class="subtitle">Well, it depends on the training set. Let’s say the LLM was trained on a dataset of narrative prose such as short stories, novels, magazines, and newspapers—in that case, completion <em class="hyperlink">a</em>, about reading a book, sounds rather more likely than the others. While the sentence about the TV, followed by the question from completion <em class="hyperlink">b</em>, could well appear somewhere in the middle of a story, a story wouldn’t open with this question without at least the starting quotation marks (“). So it’s unlikely that a model trained on short stories would predict option <em class="hyperlink">b</em>.</p>

<p class="subtitle">But throw emails and conversation transcripts into the training set, and suddenly, option <em class="hyperlink">b</em> appears very plausible. I made up both of them, though: it’s the third option that was produced by an actual LLM (OpenAI’s text-davinci-003, which is a variant of GPT-3), mimicking the advice and customer service conversations that abound in its training set.</p>

<p class="subtitle">A theme is emerging here: the better you know the training data, the better the intuition you can form about the likely output of an LLM trained on that training data. Many commercial LLMs don’t publish their training data—choosing a good training set is a big part of the special sauce that makes their models successful. Even then, however, it’s usually possible to form some sensible expectations about the kind of documents the training set consists of.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Human Thought Versus LLM Processing" class="calibre6"><div class="preface" id="ch02_human_thought_versus_llm_processing_1728407258905246">
<h2 class="calibre19">Human Thought Versus LLM Processing</h2>

<p class="subtitle">The LLM selects<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="human thought versus LLM processing" data-type="indexterm" id="id331" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="human thought" data-type="indexterm" id="id332" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the most likely looking continuation, and this goes against some assumptions humans make when reading text. That’s because when humans produce text, they do so as part of a process that involves more than producing plausible-looking text output. Let’s say you want to write a blog post about a podcast you came across at the podcasting site Acast. You might start writing the following: <code class="calibre15">In their newest installment of `The rest is history`, they talk about the Hundred Years’ War (listen on acast at http://. </code>Of course you don’t know the URL by heart, so this is the point where you stop writing and do a quick internet search. Hopefully, you find the correct link: shows.acast.com/the-rest-is-history-podcast/episodes/321-hundred-years-war-a-storm-of-swords. Or maybe you can’t find it, in which case, you might go back and delete the whole bracket and replace it with <code class="calibre15">(episode unfortunately not available anymore).</code></p>

<p class="subtitle">The model can’t google or edit, so it just guesses.<sup class="calibre37"><a data-type="noteref" id="id333-marker" href="ch02.html#id333" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">1</a></sup> Nor will the raw LLM express any doubt,<sup class="calibre37"><a data-type="noteref" id="id334-marker" href="ch02.html#id334" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">2</a></sup> add a disclaimer that it was just guessing, or show any other trace of evidence that the information is merely a guess rather than actual knowledge—because after all, the model <em class="hyperlink">always</em> guesses.<sup class="calibre37"><a data-type="noteref" id="id335-marker" href="ch02.html#id335" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">3</a></sup> This guess just happened to be made at a point where humans typically switch to a different mode of producing their text (googling rather than pressing the first keys that come to mind).</p>

<p class="subtitle">LLMs are really good at emulating any patterns they find in the items they guess about. After all, this is pretty much exactly what they were trained for. So if they make up a Social Security number, it’ll be a string of plausible digits, and if they make up the URL of a podcast, it’ll look like the URL of a podcast.</p>

<p class="subtitle">In this case, I tried OpenAI’s text-curie-001, a small variant of GPT3, and this LLM completed the URL as follows:</p>

<pre data-type="programlisting" class="calibre21">
http://www.acast.com/the-rest-is-history-episode-5-the-Hundred-Years-War- \
1411-1453-with-dr-martin-kemp)</pre>

<p class="subtitle">Is Dr. Martin Kemp a real person here? Maybe one who is involved with history podcasts? Maybe even the podcast we’re talking about? There is an art historian named Martin Kemp at Oxford, though whether the completion could refer to him sounds like a theory of language problem rather than an LLM question (see <a data-type="xref" href="#ch02_figure_3_1728407258873288" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-3</a>). At any rate, he didn’t talk about the Hundred Years’ War on the podcast <em class="hyperlink">The Rest Is History</em>.</p>

<figure class="calibre22"><div id="ch02_figure_3_1728407258873288" class="figure"><img alt="A cartoon of a child at a computer  Description automatically generated" src="assets/pefl_0203.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-3. </span>People’s language reflects reality; models’ language reflects people</h6>
</div></figure>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Hallucinations" class="calibre6"><div class="preface" id="ch02_hallucinations_1728407258905300">
<h2 class="calibre19">Hallucinations</h2>

<p class="subtitle">The<a contenteditable="false" data-primary="hallucinations" data-secondary="definition of term" data-type="indexterm" id="id336" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="prompt engineering" data-secondary="preventing hallucinations" data-type="indexterm" id="id337" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> fact that LLMs are trained as “training data mimic machines” has unfortunate consequences: <em class="hyperlink">hallucinations,</em><sup class="calibre37"><a data-type="noteref" id="id338-marker" href="ch02.html#id338" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">4</a></sup> which are factually wrong but plausible-looking pieces of information produced confidently by the model. They are a common problem when using LLMs, either ad hoc or within applications.</p>

<p class="subtitle">Since<a contenteditable="false" data-primary="hallucinations" data-secondary="preventing by providing background" data-type="indexterm" id="id339" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> hallucinations don’t differ from other completions <em class="hyperlink">from the perspective of the model</em>, prompt directives like “Don’t make stuff up” are of very limited use. Instead, the typical approach is to get the model to provide some background that can be checked. That could be an explanation of its reasoning,<sup class="calibre37"><a data-type="noteref" id="id340-marker" href="ch02.html#id340" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">5</a></sup> a calculation that can be performed independently, a source link, or keywords and details that can be searched for. For example, it’s much harder to check the sentence “There was an English king who married his cousin,” than “There was an English king who married his cousin, namely George IV, who married Caroline of Brunswick.” The best antidote to hallucinations is “Trust but verify,” just minus the trust.</p>

<p class="subtitle">Hallucinations<a contenteditable="false" data-primary="hallucinations" data-secondary="inducing" data-type="indexterm" id="id341" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> can also be induced. If your prompt references something that doesn’t exist, an LLM will typically continue to assume its existence. Documents that start out with wrong claims and then correct themselves halfway through are rare. So the model will typically assume its prompt to be true, and this is known<a contenteditable="false" data-primary="truth bias" data-type="indexterm" id="id342" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="biases" data-secondary="truth bias" data-type="indexterm" id="id343" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> as <em class="hyperlink">truth bias</em>.</p>

<p class="subtitle">You<a contenteditable="false" data-primary="hypothetical situations" data-type="indexterm" id="id344" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="counterfactual situations" data-type="indexterm" id="id345" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> can make truth bias work for you—if you want the model to assess a hypothetical or counterfactual situation, there’s no need to say, “Pretend that it’s 2030 and Neanderthals have been resurrected.” Just begin with “It’s 2031, a full year since the first Neanderthals were resurrected.”</p>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">If you have access to an LLM producing completions (i.e., the raw LLM, not wrapped in a chat interface like ChatGPT), this might be a good occasion to try out entering a couple of so-called<a contenteditable="false" data-primary="prompt engineering" data-secondary="make-believe prompts" data-type="indexterm" id="id346" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="make-believe prompts" data-type="indexterm" id="id347" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">make-believe </em>prompts.</p>

<p class="subtitle">Like the example about resurrected Neanderthals in preceding text, make-believe prompts elicit answers to hypothetical questions not by asking the question outright but by implying that the hypothetical scenario actually came to pass.</p>

<p class="subtitle">Compare the suggestion with a chat LLM’s answer. How does it differ?</p>
</div>

<p class="subtitle">However, an LLM’s truth bias is also dangerous, particularly to programmatic applications. It’s all too easy to mess up in programmatic prompt creation and introduce counterfactual or nonsensical elements. A human might read through the prompt, put down the paper, raise their eyebrows at you, and go, “Really?” The LLM doesn’t have this option. It’ll do its best to pretend the prompt is real, and it’s unlikely to correct you. So you are responsible for giving it a prompt that doesn’t need correction.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="How LLMs See the World" class="calibre6"><div class="preface" id="ch02_how_llms_see_the_world_1728407258905418">
<h1 class="calibre5">How LLMs See the World</h1>

<p class="subtitle">In<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="human thought versus LLM processing" data-type="indexterm" id="LLMhmn02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="human thought" data-type="indexterm" id="hthght01" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <a data-type="xref" href="#ch02_what_are_llms_1728407258904985" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“What Are LLMs?”</a> you learned that LLMs consume and produce strings. It’s worth getting under the hood on this statement a bit: how do LLMs see strings? We’re used to thinking of strings as sequences of characters, but that’s not quite what the LLM sees. It can reason about characters, but that’s not a native ability, and it requires the equivalent of rather deep concentration on the part of the LLM—at the time of writing (autumn 2024), even the most advanced models can still be fooled by questions such as <a href="https://oreil.ly/Lh3o0" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“How many Rs in ‘strawberry'?”</a>.</p>

<p class="subtitle">Maybe it’s worth pointing out that <em class="hyperlink">we</em> don’t really read strings in characters either. At a very early stage of human processing, they are grouped together into words. What we then read are the words, not the letters. That’s why we often read over typos without spotting them: they’re already corrected by our brain by the time they reach the conscious part of our processing.</p>

<p class="subtitle">You can have lots of fun with purposely garbled sentences just at the edge of what your inner autocorrect function can cope with (see <a data-type="xref" href="#ch02_figure_4_1728407258873307" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-4</a>, left). However, if you garble the text in a way that doesn’t respect word boundaries, your readers are going to have a very bad day (see <a data-type="xref" href="#ch02_figure_4_1728407258873307" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-4</a>, right).</p>

<figure class="calibre22"><div id="ch02_figure_4_1728407258873307" class="figure"><img alt="A black background with a black square  Description automatically generated with medium confidence" src="assets/pefl_0204.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-4. </span>Two ways of scrambling the same text</h6>
</div></figure>

<p class="subtitle">The left part of the figure leaves the word boundaries intact and scrambles the order of the letters within each word, while the right part leaves the order of the letters intact but changes the word boundaries. Most people find the left variant significantly easier to read.</p>

<p class="subtitle">Like humans, LLMs don’t read the single letters either. When you send a text to the model, it’s first broken down into a series of multiletter chunks called<a contenteditable="false" data-primary="tokenization" data-secondary="process of" data-type="indexterm" id="id348" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">token</em>s. They’re typically three to four characters long, but there are also longer tokens for common words or letter sequences. The set of tokens used by a model is called<a contenteditable="false" data-primary="vocabulary" data-type="indexterm" id="id349" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> its <em class="hyperlink">vocabulary</em>.</p>

<p class="subtitle">When reading a text, the model first passes it through a tokenizer that transforms it into a sequence of tokens. Only then is it passed to the LLM proper. Then, the LLM produces a series of tokens (represented internally as numbers), which is translated back to text before you get it back (see <a data-type="xref" href="#ch02_figure_5_1728407258873328" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-5</a>).</p>

<figure class="calibre22"><div id="ch02_figure_5_1728407258873328" class="figure"><img alt="A diagram of a computer code  Description automatically generated" src="assets/pefl_0205.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-5. </span>A tokenizer translating text into a sequence of numbers the LLM works on—and back</h6>
</div></figure>

<p class="subtitle">Note that not all tokenizers include composite tokens starting with whitespace, but many do. Notable examples are <a href="https://oreil.ly/c1QgI" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">OpenAI’s tokenizers</a>.</p>

<p class="subtitle">LLMs see text as consisting of tokens, and humans see it as consisting of words. That makes it sound like LLMs and humans see text in a very similar way, but there are a few critical differences.</p>

<section data-type="sect2" data-pdf-bookmark="Difference 1: LLMs Use Deterministic Tokenizers" class="calibre6"><div class="preface" id="ch02_difference_1_llms_use_deterministic_tokenizers_1728407258905479">
<h2 class="calibre19">Difference 1: LLMs Use Deterministic Tokenizers</h2>

<p class="subtitle">As<a contenteditable="false" data-primary="deterministic tokenizers" data-type="indexterm" id="id350" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="tokenization" data-secondary="deterministic tokenizers" data-type="indexterm" id="id351" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> humans, our translation of letters into words is fuzzy. We try to find a word that is the most similar to the letter sequence we see. On the other hand, LLMs use deterministic tokenizers―which make typos stand out like sore thumbs. The word ghost is a single token in OpenAI’s<a contenteditable="false" data-primary="tokenization" data-secondary="GPT tokenizer" data-type="indexterm" id="id352" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="GPT tokenizer" data-type="indexterm" id="id353" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> GPT tokenizer (a tokenizer that is used widely, not just for OpenAI’s models). However, the typo “gohst” is translated into a sequence of three tokens―g-oh-st—that’s obviously different, which makes it easy for the LLM to spot the typo. Nevertheless, LLMs are typically rather resilient against typos since they are used to them from their training set.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Difference 2: LLMs Can’t Slow Down and Examine Letters" class="calibre6"><div class="preface" id="ch02_difference_2_llms_can_t_slow_down_and_examine_let_1728407258905575">
<h2 class="calibre19">Difference 2: LLMs Can’t Slow Down and Examine Letters</h2>

<p class="subtitle">We<a contenteditable="false" data-primary="tokenization" data-secondary="drawbacks of" data-type="indexterm" id="Tdraw02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> humans can slow down and consciously examine each letter individually, but an LLM can only use its built-in tokenizer (and it can’t slow down either). Many LLMs have learned from the training set what letters which token consists of, but this makes all syntactic tasks that require the model to break up or reassemble tokens much more difficult.</p>

<p class="subtitle">There’s a good example of this in <a data-type="xref" href="#ch02_figure_6_1728407258873344" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-6</a>, which depicts a ChatGPT conversation about reversing letters in words. Reversing the letters is a simple pattern manipulation, and LLMs are normally really good at that. But breaking apart and reassembling the tokens proves to be too difficult for the LLM, so both reversal and re-reversal are very far off.</p>

<p class="subtitle">In the figure, both the initial reversal and the re-reversal are full of errors. The takeaway for you as application builder here is to avoid giving the model such tasks involving the subtoken level, if you can.</p>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">If<a contenteditable="false" data-primary="tokenization" data-secondary="pre- or post-processing" data-type="indexterm" id="id354" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the task you want the LLM to perform includes a component that requires the model to break tokens apart and reassemble them, consider whether you can take care of that component in pre- or post-processing.</p>
</div>

<figure class="calibre22"><div id="ch02_figure_6_1728407258873344" class="figure">
<div class="preface"><img alt="A screenshot of a chat Description automatically generated" src="assets/pefl_0206.png" class="calibre23"/></div>

<h6 class="calibre24"><span class="firstname">Figure 2-6. </span><a href="https://oreil.ly/KKso8" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">ChatGPT trying and failing to reverse letters</a></h6>
</div></figure>

<p class="subtitle">As an example of how to use the tip in the box, let’s say your application is using an LLM to play a game like<a contenteditable="false" data-primary="Scattergories" data-type="indexterm" id="id355" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Scattergories, in which the aim is to find examples with syntactic properties, like “prohibition activist starting with <em class="hyperlink">W</em>,” “European country starting with <em class="hyperlink">Sw</em>,” or “fruit with 3 occurrences of the letter R in its name.” Then, it might make sense for you to use your LLM as an oracle to obtain a large list of prohibition activists or European countries and then use syntactic logic to filter down that list. If you try to let the LLM shoulder the whole burden, you might encounter failings (see <a data-type="xref" href="#ch02_figure_7_1728407258873377" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-7</a>).</p>

<p class="subtitle">Note that the model in the figure is not deterministic, and it fails in two different ways (see the <a href="https://oreil.ly/yIIkg" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">first</a> and <a href="https://oreil.ly/PfywQ" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">second attempts</a>). Note also that [ Sweden], [ Switzerland], and [ Somalia] are all individual tokens in ChatGPT’s tokenizer.</p>

<figure class="calibre22"><div id="ch02_figure_7_1728407258873377" class="figure"><img alt="A screenshot of a phone  Description automatically generated" src="assets/pefl_0207.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-7. </span>ChatGPT having trouble identifying countries starting with Sw</h6>
</div></figure>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Difference 3: LLMs See Text Differently" class="calibre6"><div class="preface" id="ch02_difference_3_llms_have_no_intuition_1728407258905698">
<h2 class="calibre19">Difference 3: LLMs See Text Differently</h2>

<p class="subtitle">The<a contenteditable="false" data-primary="intuition" data-type="indexterm" id="id356" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> final difference we want to highlight is that we humans have an intuitive understanding of many aspects of tokens and letters. In particular, we <em class="hyperlink">see</em> them, so we know which letters are round and which are square. We understand ASCII art because we see it (although many models will have learned a substantial amount of ASCII art by heart). For us, a letter with an accent on it is just a variant of the same letter, and we have no great díffícúlty ígnóríng thém whílé réádíng á téxt whéré théy ábóúnd. On the other hand, the model, even if it manages, will have to use a significant amount of its processing power, leaving less for the actual application you have in mind.</p>

<p class="subtitle">A particular case here is<a contenteditable="false" data-primary="capitalization" data-type="indexterm" id="id357" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> capitalization. Consider <a data-type="xref" href="#ch02_figure_8_1728407258873396" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-8</a>. Why has this simple task goed… I mean… <em class="hyperlink">gone</em> so badly? Keeping the pitfalls of tokenization in mind, you might try to hazard a guess yourself before you read on.</p>

<figure class="calibre22"><div id="ch02_figure_8_1728407258873396" class="figure"><img alt="A computer screen with a sign  Description automatically generated" src="assets/pefl_0208.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-8. </span>Asking OpenAI’s text-babbage-001 model to translate a text to all caps</h6>
</div></figure>

<p class="subtitle">This produces some funny and typical mistakes—note that we are using a very small model for demonstration purposes, and larger models are not usually caught out quite as easily as this.</p>

<p class="subtitle">For humans, the capital letter <em class="hyperlink">A</em> is just a variant of the lowercase <em class="hyperlink">a</em> for humans, but the tokens that contain the capital letter are very different from the tokens that contain the lowercase letter. This is something the models are very aware of since they have seen plenty of training data about it. They know that the token <em class="hyperlink">For</em> after a period is very similar to the token <em class="hyperlink">for</em> in the middle of a sentence.</p>

<p class="subtitle">However, most tokenizers do not make it easy for models to learn these connections since capitalized tokens don’t always correspond one-to-one to noncapitalized ones. For example the GPT tokenizer translates “strange new worlds” as <code class="calibre15">[str][ange][ new][ worlds]</code>, which is four tokens. But in all caps, the tokenization goes <code class="calibre15">[STR][ANGE][ NEW][ WOR][L][DS]</code>, which is six tokens. Similarly, the word <em class="hyperlink">gone</em> is a single token, while <code class="calibre15">[G][ONE]</code> are two.</p>

<p class="subtitle">Better LLMs are better at dealing with these capitalization matters, but it’s still work for them that detracts from the real meat of your problem, which likely isn’t capitalization. (You don’t need an LLM to capitalize text after all!) So the wise prompt engineer will try to avoid burdening the models overmuch by having the LLM translate between capitalizations all the time.<a contenteditable="false" data-primary="" data-startref="Tdraw02" data-type="indexterm" id="id358" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="hthght01" data-type="indexterm" id="id359" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="LLMhmn02" data-type="indexterm" id="id360" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Counting Tokens" class="calibre6"><div class="preface" id="ch02_counting_tokens_1728407258905758">
<h2 class="calibre19">Counting Tokens</h2>

<p class="subtitle">You<a contenteditable="false" data-primary="tokenization" data-secondary="understanding your model's tokenizer" data-type="indexterm" id="id361" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> can’t mix and match tokenizers and models. Every model uses a fixed tokenizer, so it’s well worth understanding your model’s tokenizer.</p>

<p class="subtitle">When writing an LLM application, you’ll probably want to be able to run the tokenizer while prompt engineering, using a library such as<a contenteditable="false" data-primary="Hugging Face" data-type="indexterm" id="id362" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="tiktoken" data-type="indexterm" id="id363" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <a href="https://oreil.ly/6Jfhy" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Hugging Face</a> or <a href="https://oreil.ly/y9N7j" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">tiktoken</a>. However, the most common application of your tokenizer will be more mundane than complex token boundary analysis. You’ll most often use the tokenizer just for counting.</p>

<p class="subtitle">That’s<a contenteditable="false" data-primary="tokenization" data-secondary="counting tokens" data-type="indexterm" id="id364" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> because the number of tokens determines <em class="hyperlink">how long</em> your text is, from the perspective of the model. That includes all aspects of length: how much time the model will spend reading through the prompt scales roughly linearly with the number of tokens in the prompt. Also, how much time it spends creating the solution scales linearly with the number of tokens produced. Ditto for the computational cost: how much computational power a prediction requires scales with its length. That’s why most<a contenteditable="false" data-primary="model-as-a-service offerings" data-type="indexterm" id="id365" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> model-as-a-service offerings charge per token produced or processed. At the time of writing, a dollar would normally buy you between 50,000 and 1,000,000 output tokens, depending on the model.</p>

<p class="subtitle">Finally, the number of tokens is what counts for the question of the<a contenteditable="false" data-primary="context window" data-type="indexterm" id="id366" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">context window</em>—the amount of text the LLM can handle at any given time. That’s a limitation of all modern LLMs that we’re going to revisit again and again throughout this book.</p>

<p class="subtitle">The LLM doesn’t just take any text and produce any text. It takes a text with a number of tokens that’s smaller than the <em class="hyperlink">context window size,</em> and its completion is such that the prompt plus the completion cannot have more tokens than the context window size either. Context window sizes are typically measured in thousands of tokens, and that’s nothing to sneeze at, in theory: it’s several, often dozens, and sometimes hundreds of pages of A4 size. But practice tends to sneeze at it nevertheless: however long your context window, you’ll be tempted to fill it and overfill it, so you need to count tokens to stop that from happening.</p>

<p class="subtitle">There is no general formula for translating the number of characters to the number of tokens. It depends on the text and on the tokenizer. The very common GPT tokenizer linked above has about four characters per token when tokenizing an English natural language text. That’s pretty typical, although newer tokenizers can be slightly more efficient (i.e., they can have more characters per token, on average).</p>

<p class="subtitle">Most tokenizers are optimized for English<sup class="calibre37"><a data-type="noteref" id="id367-marker" href="ch02.html#id367" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">6</a></sup> and will be less efficient for other languages, meaning they’ll have fewer characters per token. Random strings of digits are even less efficient, clocking in at a little over two characters per token. It’s even worse for random alphanumeric strings like cryptographic keys, which usually have less than two characters per token. Strings with rare characters will have the least number of characters per token—for instance, the unicode smiley, ☺, actually has two tokens.</p>

<div data-type="note" epub:type="note" class="calibre16"><h6 class="calibre17">Note</h6>
<p class="subtitle">Most LLMs use vocabularies with at least a couple of special tokens: most commonly, at least an<a contenteditable="false" data-primary="end-of-text tokens" data-type="indexterm" id="id368" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="tokenization" data-secondary="end-of-text tokens" data-type="indexterm" id="id369" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> end-of-text token, which in training is appended to each training document so that the model learns when it’s over. Whenever the model outputs that token, the completion is cut off at that point.</p>
</div>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="One Token at a Time" class="calibre6"><div class="preface" id="ch02_one_token_at_a_time_1728407258905818">
<h1 class="calibre5">One Token at a Time</h1>

<p class="subtitle">Let’s peel another layer off the onion―the last one before we come to the core. Under the hood, the LLM isn’t directly text to text, and it’s not really directly tokens to tokens either. It’s <em class="hyperlink">multiple</em> tokens to a single token. The model is just constantly repeating the operation to get the next token, accumulating these single tokens as long as needed to get a proper text out.</p>

<section data-type="sect2" data-pdf-bookmark="Auto-Regressive Models" class="calibre6"><div class="preface" id="ch02_auto_regressive_models_1728407258905878">
<h2 class="calibre19">Auto-Regressive Models</h2>

<p class="subtitle">A<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="auto-regressive models" data-type="indexterm" id="LLMauto02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="auto-regressive models" data-type="indexterm" id="autoreg02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> single pass through the LLM gives you the statistically most likely next token.<sup class="calibre37"><a data-type="noteref" id="id370-marker" href="ch02.html#id370" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">7</a></sup> Then, this token is pasted onto the prompt, and the LLM makes another pass to get the statistically most likely next token <em class="hyperlink">given the new prompt,</em><sup class="calibre37"><a data-type="noteref" id="id371-marker" href="ch02.html#id371" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">8</a></sup> and so on (see <a data-type="xref" href="#ch02_figure_9_1728407258873413" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-9</a>). Such a process that makes its predictions one token at a time, with the next prediction depending on the previous predictions, is called <em class="hyperlink">autoregressive</em>.</p>

<p class="subtitle">You know how when you write text on your phone, you can get three-word suggestions above your keyboard? Running an LLM is like repeatedly pressing the middle button.</p>

<p class="subtitle">This regular, almost monotonous pattern of one token every step points to a big difference between LLMs generating text and humans typing text: while we may stop and check, think, or reflect, the model needs to produce one token every step. The LLM doesn’t get extra time if it needs to think longer,<sup class="calibre37"><a data-type="noteref" id="id372-marker" href="ch02.html#id372" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">9</a></sup> and it can’t stall.</p>

<figure class="calibre22"><div id="ch02_figure_9_1728407258873413" class="figure"><img alt="A screenshot of a computer  Description automatically generated" src="assets/pefl_0209.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-9. </span>LLMs generating their response one token at a time</h6>
</div></figure>

<p class="subtitle">And once it’s put out a token, the LLM is committed to that token. The LLM can’t backtrack and erase the token. It also won’t issue corrections where it states that what it output previously is incorrect, because it’s not been trained on documents where mistakes get taken back explicitly in the text—after all, the humans who wrote those documents <em class="hyperlink">can</em> backtrack and correct the mistakes at the places where they occur, so explicit takebacks are very rare in finished documents. Oh wait, actually, <em class="hyperlink">takebacks</em> is more commonly spelled as two words, so let me write explicit take backs instead.</p>

<p class="subtitle">This trait can make LLMs appear stubborn and somewhat ridiculous, when they keep exploring a path that obviously makes no sense. But really, what this means is that, when necessary, such mistake recognition and backtracking capability needs to be supplied <em class="hyperlink">by the application designer:</em> you.</p>
</div></section>

<section class="calibre6" data-type="sect2" data-pdf-bookmark="Patterns and Repetitions"><div class="preface" id="ch02_patterns_and_repetitions_1728407258905936">
<h2 class="calibre19">Patterns and Repetitions</h2>

<p class="subtitle">Another<a contenteditable="false" data-primary="patterns and repetitions" data-type="indexterm" id="id373" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="repetitions and patterns" data-type="indexterm" id="id374" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> issue with autoregressive systems is that they can fall into their own patterns. LLMs are good at recognizing patterns, so they sometimes (by chance) create a pattern and can’t find a good point to leave it. After all, <em class="hyperlink">given the pattern</em>, at any given token, it’s more likely that it continues than that it breaks. This leads to very repetitive solutions (see <a data-type="xref" href="#ch02_figure_10_1728407258873441" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-10</a>).</p>

<figure class="calibre22"><div id="ch02_figure_10_1728407258873441" class="figure"><img alt="A black screen with white text  Description automatically generated" src="assets/pefl_0210.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-10. </span>A list of reasons produced by OpenAI’s text-curie-001 model (an older model chosen for demonstration purposes, since newer models rarely fall into the repetition trap quite as awkwardly)</h6>
</div></figure>

<p class="subtitle">In the figure, an LLM has produced a list of reasons for liking a TV show. How many patterns can you spot? Here are the ones we found:</p>

<ul class="stafflist">
	<li class="calibre9">The items are consecutively numbered statements, each of which fits on one line. That seems desirable.</li>
	<li class="calibre9">They all start with “The,” which seems tolerable.</li>
	<li class="calibre9">They are of the form “X is Y and Z.” That’s annoying because it endangers correctness. What if there is no appropriate Z? The model might invent one. However, it stops after item 5.</li>
	<li class="calibre9">After several items in a row started with “The franchise,” they all did. That’s stupid.</li>
	<li class="calibre9">Toward the end,<em class="hyperlink"> legacy</em>, <em class="hyperlink">following</em>, <em class="hyperlink">future</em>, <em class="hyperlink">foundation</em>, and <em class="hyperlink">fanbase</em> are repeated ad nauseam. That’s stupid too.</li>
	<li class="calibre9">The list goes on and on and never stops. That’s because after each item, it’s more likely that the list will continue than that this will be the last item. And the model doesn’t get bored.</li>
	<li class="calibre9">Toward the end,<em class="hyperlink"> legacy</em>, <em class="hyperlink">following</em>, <em class="hyperlink">future</em>, <em class="hyperlink">foundation</em>, and <em class="hyperlink">fanbase</em> are repeated ad nauseam. That’s stupid too.</li>
	<li class="calibre9">The list goes on and on and never stops. That’s because after each item, it’s more likely that the list will continue than that this will be the last item. And the model doesn’t get bored.<sup class="calibre37"><a data-type="noteref" id="id375-marker" href="ch02.html#id375" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">10</a></sup></li>
</ul>

<p class="subtitle">The way to deal with such repetitive solutions is typically to simply detect and filter them out. Another way is to randomize the output a bit. We’ll talk about randomization of output in the next section.<a contenteditable="false" data-primary="" data-startref="autoreg02" data-type="indexterm" id="id376" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="LLMauto02" data-type="indexterm" id="id377" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Temperature and Probabilities" class="calibre6"><div class="preface" id="ch02_temperature_and_probabilities_1728407258905997">
<h1 class="calibre5">Temperature and Probabilities</h1>

<p class="subtitle">In<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="sampling process" data-type="indexterm" id="LLMsampling02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="probabilities" data-type="indexterm" id="prob02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the previous section, you learned that the LLM computes the most likely token. But if you peel back one more layer of the onion that is the LLM, it turns out that actually, it computes the probability of <em class="hyperlink">all possible tokens</em> before choosing a single one. The process under the hood that chooses the actual token is called <em class="hyperlink">sampling</em> (see <a data-type="xref" href="#ch02_figure_11_1728407258873458" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-11</a>).</p>

<figure class="calibre22"><div id="ch02_figure_11_1728407258873458" class="figure"><img alt="A computer code on a black background  Description automatically generated" src="assets/pefl_0211.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-11. </span>The sampling process in action</h6>
</div></figure>

<p class="subtitle">Note that the LLM doesn’t just compute the most likely token; it computes the likelihood of all the tokens.</p>

<p class="subtitle">Many models will share these probabilities with you. The model typically returns them as<a contenteditable="false" data-primary="logprobs (logarithm of the probabilities)" data-secondary="definition of term" data-type="indexterm" id="id378" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">logprobs</em> (i.e., the natural logarithms of the token’s probability). The higher the logprob, the more likely the model considers this token to be. Logprobs are never bigger than 0 because a logprob of 0 would mean that the model is certain that this is the next token. Expect the most likely token to have a logprob between –2 and 0 (see <a data-type="xref" href="#ch02_figure_12_1728407258873476" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-12</a>).</p>

<figure class="calibre22"><div id="ch02_figure_12_1728407258873476" class="figure"><img alt="A screenshot of a computer  Description automatically generated" src="assets/pefl_0212.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-12. </span>An example API call requesting logprobs and extracting the logprobs of the chosen completion</h6>
</div></figure>

<p class="subtitle">Note that in the figure, setting the request parameter <code class="calibre15">logprobs</code> to <code class="calibre15">3</code> means that the logprobs for the three most likely tokens will be returned. However, you may not always want the <em class="hyperlink">most</em> <em class="hyperlink">likely</em> token. Especially if you have a way of automatically testing your completions, you may want to generate a couple of alternatives and throw out the bad ones. The typical way to do this is by using a<a contenteditable="false" data-primary="temperature parameter" data-type="indexterm" id="tempparam02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">temperature</em> greater than 0. The temperature is a number of at least zero that determines how “creative” the model should be. More specifically, if the temperature is greater than 0, the model will give a stochastic completion, where it selects the most likely token with the highest probability but maybe also returns less likely but still not totally absurd tokens. The higher the temperature and the closer the logprobs of the best tokens are to each other, the more likely it is that the second-best-placed token will be selected, or even the third or fourth or fifth. The exact formula is as follows:</p>

<div data-type="equation" class="calibre38">
<math alttext="p left-parenthesis normal t normal o normal k normal e normal n Subscript i Baseline right-parenthesis equals StartFraction exp left-parenthesis normal l normal o normal g normal p normal r normal o normal b Subscript i Baseline slash t right-parenthesis Over sigma-summation Underscript j Endscripts exp left-parenthesis normal l normal o normal g normal p normal r normal o normal b Subscript j Baseline slash t right-parenthesis EndFraction">
  <mrow>
    <mi>p</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi> token </mi> <mi>i</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi> logprob </mi> <mi>i</mi> </msub><mo>/</mo><mi>t</mi><mo>)</mo></mrow> <mrow><msub><mo>∑</mo> <mi>j</mi> </msub><mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi> logprob </mi> <mi>j</mi> </msub><mo>/</mo><mi>t</mi><mo>)</mo></mrow></mrow></mfrac>
  </mrow>
</math>
</div>

<p class="subtitle">Let’s look at possible temperatures and when you should choose each one:</p>

<dl class="stafflist">
	<dt class="calibre13">0</dt>
	<dd class="calibre14">
	<p class="subtitle">You want the most likely token. No alternatives. This is the recommended setting when correctness is paramount. Additionally, running the LLM at temperature 0 is close to deterministic,<sup class="calibre37"><a data-type="noteref" id="id379-marker" href="ch02.html#id379" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">11</a></sup> and in some applications, repeatability is an advantage.</p>
	</dd>
	<dt class="calibre13">0.1–0.4</dt>
	<dd class="calibre14">
	<p class="subtitle">If there’s an alternative token that’s only slightly less likely than the front-runner, you want some small chance for that to be picked. A typical use case is that you want to generate a small number of different solutions (for example, because you know how to filter out the best one). Or maybe you just want one completion but a more colorful, creative solution than what you expect at temperature 0.</p>
	</dd>
	<dt class="calibre13">0.5–0.7</dt>
	<dd class="calibre14">
	<p class="subtitle">You want a greater impact of chance on the solution, and you are fine with getting completions that are “inaccurate” in the sense that sometimes, a token will be chosen even though the model thinks another alternative is clearly more likely. The typical use case is if you want a large number of independent solutions, likely 10 or more.</p>
	</dd>
	<dt class="calibre13">1</dt>
	<dd class="calibre14">
	<p class="subtitle">You want the token distribution to mirror the statistical training set distribution. Assume, for example, that your prefix is “One, Two,” and in the training set, this is followed by the token <code class="calibre15">[ Buck]</code> in 51% of cases and by <code class="calibre15">[ Three] </code>in 31% of cases (and the model has been trained well enough to pick that up). If you run the model several times at temperature 1, then 51% of the time, you’ll get <code class="calibre15">[ Buck]</code>, and 31% of the time, you’ll get <code class="calibre15">[ Three].</code></p>
	</dd>
	<dt class="calibre13">&gt; 1</dt>
	<dd class="calibre14">
	<p class="subtitle">You want a text that’s “more random” than the training set. This means the model is less likely to pick the “standard” continuation than the typical document from the training set and more likely to pick a “particularly weird” continuation than the typical document from the training set.</p>
	</dd>
</dl>

<p class="subtitle">High temperatures can make LLMs sound like they’re drunk. Over the course of long generations at temperatures greater than 1, the error rate usually gets worse over time. The reason is that temperature affects only the very last layer of computation when probabilities are turned into output, so it doesn’t affect the main part of the LLM’s processing that computes those probabilities in the first place. So the model recognizes the errors in the text it just generated as a pattern, and it tries to mimic that pattern by generating its own errors. Then the high temperature causes even more errors on top of that (see <a data-type="xref" href="#ch02_figure_13_1728407258873494" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-13</a>).</p>

<figure class="calibre22"><div id="ch02_figure_13_1728407258873494" class="figure"><img alt="A close-up of a text  Description automatically generated" src="assets/pefl_0213.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-13. </span>High temperature affecting LLMs a bit like alcohol affects humans</h6>
</div></figure>

<p class="subtitle">The figure shows this deterioration at high temperatures, where the generation of item 3 starts out error prone but legible and ends in a state where even the individual words are unrecognizable. Note that each item in the figure has been sampled at an increasingly high temperature from OpenAI’s text-davinci-003.</p>

<p class="subtitle">Let’s return to the example of the model writing a list. A typical list in text stops at a few items, say 3, or 4, or 5. If it’s a longer list, then 10 is the next most obvious stopping point. After each new line, it can either continue the list by producing the number that’s next up as the next token, or it can declare itself done with the list by producing a second new line (or something else entirely, maybe).</p>

<p class="subtitle">At temperature 0, the LLM will always choose the option it considers more likely for this line. Often, that means it will always continue, at least after it’s passed the last obvious stopping point. At temperature 1, if the LLM makes the judgment that a continuation has probability <em class="hyperlink">x</em>, then it will only continue with probability <em class="hyperlink">x</em>. So, over the course of many items, it’s likely that the LLM will end the list sooner or later, with an expected length similar to the length of lists in the training set. In general, it’s a trade-off (see <a data-type="xref" href="#ch02_table_1_1728407258884202" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 2-1</a>).</p>

<table id="ch02_table_1_1728407258884202" class="calibre27">
	<caption class="calibre28"><span class="firstname">Table 2-1. </span>The advantages of the different temperature regimes</caption>
	<thead class="calibre29">
		<tr class="calibre30">
			<th class="calibre31">High temperature</th>
			<th class="calibre31">Low temperature</th>
		</tr>
	</thead>
	<tbody class="calibre32">
		<tr class="calibre30">
			<td class="calibre33">+ More alternatives.</td>
			<td class="calibre33">+ More correct solutions.</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">+ Many properties of generations (e.g., list length) have the same distribution as in the training set.</td>
			<td class="calibre33">+ More replicable (deterministic).</td>
		</tr>
	</tbody>
</table>

<p class="subtitle">There are other ways of sampling, most notably <em class="hyperlink">beam search</em>, which tries to account for the fact that choosing a particular token that looks likely can make the next choice hard because no good follow-on token exists. Beam search accomplishes this by looking ahead for the next few tokens and making sure that a likely sequence exists. This can lead to more accurate solutions, but it’s less often used in applications because of its much higher time and compute cost.<a contenteditable="false" data-primary="" data-startref="tempparam02" data-type="indexterm" id="id380" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="LLMsampling02" data-type="indexterm" id="id381" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="prob02" data-type="indexterm" id="id382" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="The Transformer Architecture" class="calibre6"><div class="preface" id="ch02_the_transformer_architecture_1728407258906064">
<h1 class="calibre5">The Transformer Architecture</h1>

<p class="subtitle">It’s<a contenteditable="false" data-primary="transformer architecture" data-type="indexterm" id="transarch02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="tokenization" data-secondary="transformer architecture" data-type="indexterm" id="Ttrans02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="transformer architecture" data-type="indexterm" id="LLMtrans02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="GPT (generative pre-trained transformer) models" data-secondary="transformer architecture" data-type="indexterm" id="GPTarch02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> time to cut away the final layer of the onion and look at the LLM’s brain directly. You peel it back and see….it’s not one brain at all. It’s<a contenteditable="false" data-primary="minibrains" data-type="indexterm" id="minibrains02" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> thousands of minibrains. All are identical in structure, and each one is performing a very similar task. There’s a minibrain sitting atop each token in the sequence, and together, these minibrains make up the<a contenteditable="false" data-primary="transformer" data-type="indexterm" id="id383" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">transformer</em>, which is the architecture used by all modern LLMs.</p>

<p class="subtitle">Each minibrain starts out by being told which token it’s sitting on and its position in the document. The minibrain keeps thinking about this for a fixed number of steps, known as<a contenteditable="false" data-primary="layers" data-type="indexterm" id="id384" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">layers</em>. During this time, it can receive information from the minibrains to the left. The minibrain’s task is to understand the document from the perspective of its location, and it uses this understanding in two ways:</p>

<ul class="stafflist">
	<li class="calibre9">In all steps before the last one, it shares some of its intermediate results with the minibrains to its right. (We’ll discuss this in more detail later.)</li>
	<li class="calibre9">For the last step, it’s asked to make a prediction of what the token immediately to its right would be.</li>
</ul>

<p class="subtitle">Every minibrain goes through the same process of computing and sharing intermediate results and then making a guess. In fact, the minibrains are clones of each other: their processing logic is the same, and all that differs is the inputs: which token they start with and which intermediate results they get told of by the minibrains to their left.</p>

<p class="subtitle">But the reason they go through these steps is different. The minibrain at the very last token, at the very right, runs to predict the next token. What it shares from its intermediate result isn’t important because there are no brains to the right that listen, but all the other minibrains are the other way around. Their purpose is to share their intermediate results with the brains to their right, and what predictions they make about the tokens directly to their right doesn’t matter because the tokens to <em class="hyperlink">their</em> immediate right are already known.</p>

<p class="subtitle">When the rightmost token makes its prediction, the autoregression from <a data-type="xref" href="#ch02_one_token_at_a_time_1728407258905818" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“One Token at a Time”</a> kicks in: it spits out the new token, and a brand new minibrain is set on top of it to refine its understanding of what’s going on at its position for a fixed number of layers. After that, it predicts the next token. Rinse and repeat―or rather, cache and repeat because this calculation will be used over and over again for every subsequent token in the prompt and the generated completion.</p>

<p class="subtitle">An example of this algorithm is shown in <a data-type="xref" href="#ch02_figure_14_1728407258873523" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-14</a>, where each column represents one minibrain and how its state changes over time. In the example, you’ve just asked the model to complete “One, Two,” and ultimately, you’ll end up with the two tokens <code class="calibre15">[ Buck]</code> and <code class="calibre15">[le].</code> Let’s follow the transformer as it arrives at that response. There’s a minibrain sitting on each of the four input tokens: <code class="calibre15">[One]</code>, <code class="calibre15">[,]</code>, <code class="calibre15">[Two]</code>, and <code class="calibre15">[,]</code> (the last of which is the second appearance of the same token). Each of them thinks for four layers,<sup class="calibre37"><a data-type="noteref" id="id385-marker" href="ch02.html#id385" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">12</a></sup> consecutively refining its understanding of the text the tokens are processing. In each step, they are updated from the tokens to the left about what they’ve learned so far. Each of them computes a guess for what the token to its right might be.</p>

<p class="subtitle">The first couple of guesses are for tokens that are still part of the prompt: <code class="calibre15">[One]</code>, <code class="calibre15">[,]</code>, <code class="calibre15">[Two]</code>, and <code class="calibre15">[,]</code>. We already know the prompt, so the guesses are just thrown away. But then, the model arrives at the completion, and there, the guess is the whole point. So the next guess is turned into a prediction, which is the token <code class="calibre15">[ Buck]</code>. A new minibrain is commissioned to be placed above that token, going through its four steps and arriving at the prediction <code class="calibre15">[le]</code>. If you continue the completion, a further minibrain will be planted atop <code class="calibre15">[le]</code>, and so on.</p>

<figure class="calibre22"><div id="ch02_figure_14_1728407258873523" class="figure"><img alt="A diagram of a cloud computing system   Description automatically generated" src="assets/pefl_0214.png" class="calibre23"/>
	<h6 class="calibre24"><span class="firstname">Figure 2-14. </span>The inner workings of the model producing one token—later layers are drawn on top of previous layers</h6>
	</div></figure>
	
<p class="subtitle">Now, let’s go back and talk about the “intermediate results” that are shared among the minibrains. The way they are shared is known as the attention mechanism—it’s the central innovation of the transformer architecture for LLMs (as mentioned in <a data-type="xref" href="ch01.html#ch01_1_introduction_to_prompt_engineering_1728408393615260" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 1</a>). Attention is a way of passing information among the minibrains. Of course, there may be thousands of minibrains, and every one of them might know something of interest to every other one. To keep this information exchange from descending into chaos, it needs to be very tightly regulated. Here’s how it works:</p>

<ol class="stafflist">
	<li class="calibre25">
	<p class="calibre26">Each minibrain has some things it wants to know, so it submits a couple of questions, in the hope they might get answered by another minibrain. Let’s say that one minibrain sits upon the token <code class="calibre15">[my]</code>. The minibrain would like to know who that might refer to, so a reasonable question would be to ask, “Who is talking?”</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">Each minibrain has some things it can share, so it submits a couple of items, in the hope they might be useful to another minibrain. Let’s say one minibrain sits upon the token <code class="calibre15">[Susan]</code>, and it’s already learned before that this token is the last word of an introduction, like “Hello, I’m Susan.” So in case it might help another minibrain down the line, it will submit the information, “The person talking right now is Susan.”</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">Now, every question is matched up with its best-fitting answer. “Who is talking?” matches up very well with “The person talking right now is Susan.”</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">The best-fitting answer to each question is revealed to the minibrain that asked the question, so the minibrain at the token <code class="calibre15">[my]</code> gets told “The person talking right now is Susan.” Of course, while the minibrains from this example talk to each other in English, in reality, they use a “language” that consists of long vectors of numbers<sup class="calibre37"><a data-type="noteref" id="id386-marker" href="ch02.html#id386" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">13</a></sup> and that is unique to every LLM, since it’s something the LLM “invents” during training.</p>
	</li>
</ol>

<div data-type="note" epub:type="note" class="calibre16"><h6 class="calibre17">Note</h6>
	<p class="subtitle">Information only ever flows from the left to the right.</p>
	
	<p class="subtitle">Information only ever flows from the bottom to the top.</p>
	</div>
	
<p class="subtitle">In modern LLMs, this Q&amp;A mechanism obeys one more constraint, which is called<a contenteditable="false" data-primary="masking" data-type="indexterm" id="id387" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">masking</em>: not <em class="hyperlink">all</em> minibrains can answer a question; only the ones to the <em class="hyperlink">left</em> of the minibrain asking the question can answer it. And a minibrain never gets told whether its answer was used, so the brains on the right can never influence the ones to the left.<sup class="calibre37"><a data-type="noteref" id="id388-marker" href="ch02.html#id388" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">14</a></sup></p>

<p class="subtitle">That flow has some practical consequences. For example, to compute the state of one minibrain at one layer, the model only needs the states to the left (earlier minibrains at this layer) and below (the same minibrain at earlier layers). That means some of the computation can go in parallel—and this is one of the reasons generative transformers are so efficient to train. At each point in time, the already computed stages form a triangle (see <a data-type="xref" href="#ch02_figure_15_1728407258873541" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-15</a>).</p>

<figure class="calibre22"><div id="ch02_figure_15_1728407258873541" class="figure">
<img alt="A diagram of a computer  Description automatically generated with medium confidence" src="assets/pefl_0215.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 2-15. </span>Calculating the inner state of an LLM</h6></div></figure>

<p class="subtitle">In the figure, first (at the upper left), only the lowest layer at the first token can be computed. Next (at the upper middle), both the second-lowest layer at the first token and the lowest layer at the second token can be computed. One step later (at the upper right), the third layer can be computed at the first token, the second layer at the second token, and the first layer at the third token…all the way until all states are computed and a new token can be sampled.</p>

<p class="subtitle">Parallelism<a contenteditable="false" data-primary="parallelism" data-type="indexterm" id="id389" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> allows speedup, but that way of computing in a triangle breaks down when the model switches from reading the prompt to creating the completion. The model has to wait until a token has been processed to the very end before choosing the next token and computing the very first state of the new minibrain. This is why LLMs are much faster at reading through a long prompt than they are at generating a long completion. Speed scales with both the number of tokens processed and the number of tokens generated, but prompt tokens are about an order of magnitude faster.</p>

<p class="subtitle">This<a contenteditable="false" data-primary="backward-and-downward vision" data-type="indexterm" id="id390" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> triangle structure reflects a general “backward-and-downward” direction of vision for the LLM, or maybe a better way to understand it is “backward-and-dumbward”:</p>

<dl class="stafflist">
	<dt class="calibre13">Backward</dt>
	<dd class="calibre14">
	<p class="subtitle">The minibrains can only ever look to their left. They can look as far back as they want, but never forward. That’s what people refer to when they call GPT or other LLMs<a contenteditable="false" data-primary="unidirectional transformers" data-type="indexterm" id="id391" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">unidirectional</em> transformers. No information ever travels from a minibrain on the right to a minibrain on the left. That makes generative transformers easy to train and to run, but it has huge ramifications for how they process information.</p>
	</dd>
	<dt class="calibre13">Downward (“dumbward”)</dt>
	<dd class="calibre14">
	<p class="subtitle">The<a contenteditable="false" data-primary="downward (dumbward) LLM vision" data-type="indexterm" id="id392" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> minibrains get their answers in a layer only from minibrains in the same layer before those get their answers for this layer. This means that any “chain of reasoning” in layer <em class="hyperlink">i</em> can only be <em class="hyperlink">i</em> reasoning steps deep, if we count the thinking the minibrain does in every layer as one reasoning step. But there’s no way for a minibrain to provide an insight gleaned at a later layer to a minibrain at a lower level for further processing. No way, that is, except one: while the LLM is generating text, the result of the very highest layer—the token—is produced, and it forms the very basis for the first layer of the next minibrain. This thinking aloud is the only way the model can let information flow from higher layers to lower layers‒it churns it around in its head, so to say. Reminiscent of the saying, “How could I know what I’m thinking before I’ve heard what I’m saying,” this principle forms the basis of<a contenteditable="false" data-primary="chain-of-thought prompting" data-secondary="basis for" data-type="indexterm" id="id393" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> chain-of-thought prompting (see <a data-type="xref" href="ch08.html#ch08_01_conversational_agency_1728429579285372" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 8</a>).</p>
	</dd>
</dl>

<p class="subtitle">Let’s look at an example. How many words does the paragraph directly above contain? If you’re anything like me, you’ll not actually bother to count, and you’ll expect the authors to just tell you. Very well, we will: it’s 173. But for the sake of argument, you could have looked up and counted them for yourself, right?</p>

<p class="subtitle">We asked ChatGPT this question by feeding it this chapter up to and including the question “How many words does the paragraph directly above contain?” It answered, <code class="calibre15">The paragraph directly above contains 348 words.</code> Not only is it off, it’s terribly, hopelessly off. Far too many words for that paragraph, but far too few for the whole text.</p>

<p class="subtitle">But of course, we’re demanding something incredibly hard from the LLM here. Humans would do better.<sup class="calibre37"><a data-type="noteref" id="id394-marker" href="ch02.html#id394" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">15</a></sup> They can read through the text again and maintain an inner counter. That doesn’t work for the LLM because it only reads over the text once and can’t look back. So while the minibrains are processing the paragraph for the one and only time, they don’t know that the critical feature they should isolate is word count, because that request appears below the chapter’s text. They’re busy considering semantic implications, tone and style, and a myriad of surface features, and they’re not giving their full attention to the one thing that will turn out to matter.</p>

<p class="subtitle">That’s<a contenteditable="false" data-primary="prompt engineering" data-secondary="impact of order on" data-type="indexterm" id="id395" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="order" data-secondary="impact on prompt engineering" data-type="indexterm" id="id396" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> why order is critical for prompt engineering—it can easily make the difference between a prompt that works and one that fails. Indeed, when I asked the word count question at the beginning instead…well, ChatGPT still didn’t get the answer right because counting is hard for LLMs. But at least it came much closer, claiming 173. In <a data-type="xref" href="ch06.html#ch06a_assembling_the_prompt_1728442733857948" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 6</a>, we’ll return to that theme of the ordering of the different parts of your prompt.<a contenteditable="false" data-primary="" data-startref="transarch02" data-type="indexterm" id="id397" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="minibrains02" data-type="indexterm" id="id398" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="Ttrans02" data-type="indexterm" id="id399" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="LLMtrans02" data-type="indexterm" id="id400" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="GPTarch02" data-type="indexterm" id="id401" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">If<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="determining realistic capabilities" data-type="indexterm" id="id402" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> you want to know whether a capability is realistic for an LLM to handle, ask yourself this question:</p>

<p class="subtitle">Could a human expert who knows all the relevant general knowledge by heart complete the prompt in a single go without backtracking, editing, or note-taking?</p>
</div>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Conclusion" class="calibre6"><div class="preface" id="ch02_conclusion_1728407258906185">
<h1 class="calibre5">Conclusion</h1>

<p class="subtitle">We discussed four central facts in this chapter. First, LLMs are document completion engines. Second, they mimic the documents they have seen during training. Third, LLMs produce one token at a time, with no option to pause or edit previous tokens. And finally, LLMs read through the text once, from beginning to end. Let’s see how these facts translate into a general prompt engineering paradigm in the next chapter.</p>
</div></section>
<div data-type="footnotes" class="calibre39"><p data-type="footnote" id="id333" class="calibre40"><sup class="calibre41"><a href="ch02.html#id333-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">1</a></sup> The model can’t google <em class="hyperlink">directly</em>, at least, but it can be connected to systems that can google. We will discuss this form of tool use in <a data-type="xref" href="ch08.html#ch08_01_conversational_agency_1728429579285372" class="pcalibre3 calibre42 pcalibre4 pcalibre1 pcalibre2">Chapter 8</a>.</p><p data-type="footnote" id="id334" class="calibre43"><sup class="calibre41"><a href="ch02.html#id334-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">2</a></sup> In the next chapter, we’ll introduce some ways in which raw LLMs are aligned or improved in post training and how these can add the ability to express doubt. However, this is not a native capacity of the basic LLM structure, which is the focus of this chapter.</p><p data-type="footnote" id="id335" class="calibre43"><sup class="calibre41"><a href="ch02.html#id335-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">3</a></sup>  It’s true that the model can predict some parts with high certainty and some with low certainty. For example, it will be much more certain in predicting the next word of “John F. Kennedy was killed in the year,” than it would be certain in predicting the next word of “Zacharias B. Fulltrodd was killed in the year.” It reads a lot about the former death, while the second one, which is made up, could have taken place in any year. However, that uncertainty does not correlate with an expression of uncertainty or doubt in the training set—the model will fully buy into the assumption that there is a text that starts talking about Zacharias B. Fulltrodd’s death. It has no reason to believe that this text is any more unreliable in relation to Zacharias’s death than the typical JFK-related text it came across in its training set is in relation to JFK’s death.</p><p data-type="footnote" id="id338" class="calibre43"><sup class="calibre41"><a href="ch02.html#id338-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">4</a></sup>  Although the closest human analog to what’s going on is probably the psychological phenomenon of confabulation, rather than hallucination. </p><p data-type="footnote" id="id340" class="calibre43"><sup class="calibre41"><a href="ch02.html#id340-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">5</a></sup> You can check this by making a second query to the LLM. See <a data-type="xref" href="ch07.html#ch07_taming_the_model_1728407187651669" class="pcalibre3 calibre42 pcalibre4 pcalibre1 pcalibre2">Chapter 7</a>. </p><p data-type="footnote" id="id367" class="calibre43"><sup class="calibre41"><a href="ch02.html#id367-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">6</a></sup> This is because English is the most frequently used language in most training datasets, and tokenizers are normally optimized to have a good compression rate on the training set. </p><p data-type="footnote" id="id370" class="calibre43"><sup class="calibre41"><a href="ch02.html#id370-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">7</a></sup> This is true at least as long as you keep the temperature parameter to 0. We’ll discuss temperature &gt; 0 in the next section. </p><p data-type="footnote" id="id371" class="calibre43"><sup class="calibre41"><a href="ch02.html#id371-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">8</a></sup> At least, it’s equivalent to a completely new pass. It’s not literally a completely new pass from a computational perspective. For example, the prompt will typically be processed only once to save work. </p><p data-type="footnote" id="id372" class="calibre43"><sup class="calibre41"><a href="ch02.html#id372-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">9</a></sup> There’s interesting research going on to offer more flexibility in taking more time when needed, so maybe that will change. </p><p data-type="footnote" id="id375" class="calibre43"><sup class="calibre41"><a href="ch02.html#id375-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">10</a></sup> I maintain that a sufficiently careful reading of <em class="hyperlink">The Silmarillion</em> would reveal that it’s boredom, in fact, that’s the real gift Ilúvatar’s Younger Children should treasure above all others. </p><p data-type="footnote" id="id379" class="calibre43"><sup class="calibre41"><a href="ch02.html#id379-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">11</a></sup> But it’s not completely deterministic, because of random rounding errors. Computed probabilities can (depending on the model) vary by several percentage points on reruns, so what the most likely token is can change. </p><p data-type="footnote" id="id385" class="calibre43"><sup class="calibre41"><a href="ch02.html#id385-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">12</a></sup> We only draw four layers to illustrate the point, but real-world LLMs usually have tens of layers. GPT-3 has 96, and newer models (like GPT-4) tend to have over 100. </p><p data-type="footnote" id="id386" class="calibre43"><sup class="calibre41"><a href="ch02.html#id386-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">13</a></sup> See <a href="https://oreil.ly/UXKOt" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“The Illustrated Transformer”</a>. </p><p data-type="footnote" id="id388" class="calibre43"><sup class="calibre41"><a href="ch02.html#id388-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">14</a></sup>  This wasn’t the case in the original transformer architecture, but it has become the norm for text-generating LLMs. </p><p data-type="footnote" id="id394" class="calibre43"><sup class="calibre41"><a href="ch02.html#id394-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">15</a></sup>  And of course, classical computer code would be best. </p></div></div></section></div></div></body></html>