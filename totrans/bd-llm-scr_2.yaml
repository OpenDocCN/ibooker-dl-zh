- en: 2 Working with Text Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Preparing text for large language model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting text into word and subword tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Byte pair encoding as a more advanced way of tokenizing text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling training examples with a sliding window approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting tokens into vectors that feed into a large language model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, we delved into the general structure of large language
    models (LLMs) and learned that they are pretrained on vast amounts of text. Specifically,
    our focus was on decoder-only LLMs based on the transformer architecture, which
    underlies the models used in ChatGPT and other popular GPT-like LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: During the pretraining stage, LLMs process text one word at a time. Training
    LLMs with millions to billions of parameters using a next-word prediction task
    yields models with impressive capabilities. These models can then be further finetuned
    to follow general instructions or perform specific target tasks. But before we
    can implement and train LLMs in the upcoming chapters, we need to prepare the
    training dataset, which is the focus of this chapter, as illustrated in Figure
    2.1
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 A mental model of the three main stages of coding an LLM, pretraining
    the LLM on a general text dataset, and finetuning it on a labeled dataset. This
    chapter will explain and code the data preparation and sampling pipeline that
    provides the LLM with the text data for pretraining.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, you'll learn how to prepare input text for training LLMs. This
    involves splitting text into individual word and subword tokens, which can then
    be encoded into vector representations for the LLM. You'll also learn about advanced
    tokenization schemes like byte pair encoding, which is utilized in popular LLMs
    like GPT. Lastly, we'll implement a sampling and data loading strategy to produce
    the input-output pairs necessary for training LLMs in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Understanding word embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep neural network models, including LLMs, cannot process raw text directly.
    Since text is categorical, it isn't compatible with the mathematical operations
    used to implement and train neural networks. Therefore, we need a way to represent
    words as continuous-valued vectors. (Readers unfamiliar with vectors and tensors
    in a computational context can learn more in Appendix A, section A2.2 Understanding
    tensors.)
  prefs: []
  type: TYPE_NORMAL
- en: The concept of converting data into a vector format is often referred to as
    *embedding*. Using a specific neural network layer or another pretrained neural
    network model, we can embed different data types, for example, video, audio, and
    text, as illustrated in Figure 2.2.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 Deep learning models cannot process data formats like video, audio,
    and text in their raw form. Thus, we use an embedding model to transform this
    raw data into a dense vector representation that deep learning architectures can
    easily understand and process. Specifically, this figure illustrates the process
    of converting raw data into a three-dimensional numerical vector.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image003.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in Figure 2.2, we can process various different data formats via embedding
    models. However, it's important to note that different data formats require distinct
    embedding models. For example, an embedding model designed for text would not
    be suitable for embedding audio or video data.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, an embedding is a mapping from discrete objects, such as words,
    images, or even entire documents, to points in a continuous vector space -- the
    primary purpose of embeddings is to convert non-numeric data into a format that
    neural networks can process.
  prefs: []
  type: TYPE_NORMAL
- en: While word embeddings are the most common form of text embedding, there are
    also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph
    embeddings are popular choices for *retrieval-augmented generation.* Retrieval-augmented
    generation combines generation (like producing text) with retrieval (like searching
    an external knowledge base) to pull relevant information when generating text,
    which is a technique that is beyond the scope of this book. Since our goal is
    to train GPT-like LLMs, which learn to generate text one word at a time, this
    chapter focuses on word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: There are several algorithms and frameworks that have been developed to generate
    word embeddings. One of the earlier and most popular examples is the *Word2Vec*
    approach. Word2Vec trained neural network architecture to generate word embeddings
    by predicting the context of a word given the target word or vice versa. The main
    idea behind Word2Vec is that words that appear in similar contexts tend to have
    similar meanings. Consequently, when projected into 2-dimensional word embeddings
    for visualization purposes, it can be seen that similar terms cluster together,
    as shown in Figure 2.3.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 If word embeddings are two-dimensional, we can plot them in a two-dimensional
    scatterplot for visualization purposes as shown here. When using word embedding
    techniques, such as Word2Vec, words corresponding to similar concepts often appear
    close to each other in the embedding space. For instance, different types of birds
    appear closer to each other in the embedding space compared to countries and cities.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image005.png)'
  prefs: []
  type: TYPE_IMG
- en: Word embeddings can have varying dimensions, from one to thousands. As shown
    in Figure 2.3, we can choose two-dimensional word embeddings for visualization
    purposes. A higher dimensionality might capture more nuanced relationships but
    at the cost of computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: While we can use pretrained models such as Word2Vec to generate embeddings for
    machine learning models, LLMs commonly produce their own embeddings that are part
    of the input layer and are updated during training. The advantage of optimizing
    the embeddings as part of the LLM training instead of using Word2Vec is that the
    embeddings are optimized to the specific task and data at hand. We will implement
    such embedding layers later in this chapter. Furthermore, LLMs can also create
    contextualized output embeddings, as we discuss in chapter 3.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, high-dimensional embeddings present a challenge for visualization
    because our sensory perception and common graphical representations are inherently
    limited to three dimensions or fewer, which is why Figure 2.3 showed two-dimensional
    embeddings in a two-dimensional scatterplot. However, when working with LLMs,
    we typically use embeddings with a much higher dimensionality than shown in Figure
    2.3\. For both GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality
    of the model's hidden states) varies based on the specific model variant and size.
    It is a trade-off between performance and efficiency. The smallest GPT-2 models
    (117M and 125M parameters) use an embedding size of 768 dimensions to provide
    concrete examples. The largest GPT-3 model (175B parameters) uses an embedding
    size of 12,288 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The upcoming sections in this chapter will walk through the required steps for
    preparing the embeddings used by an LLM, which include splitting text into words,
    converting words into tokens, and turning tokens into embedding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Tokenizing text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section covers how we split input text into individual tokens, a required
    preprocessing step for creating embeddings for an LLM. These tokens are either
    individual words or special characters, including punctuation characters, as shown
    in Figure 2.4.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 A view of the text processing steps covered in this section in the
    context of an LLM. Here, we split an input text into individual tokens, which
    are either words or special characters, such as punctuation characters. In upcoming
    sections, we will convert the text into token IDs and create token embeddings.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The text we will tokenize for LLM training is a short story by Edith Wharton
    called *The Verdict*, which has been released into the public domain and is thus
    permitted to be used for LLM training tasks. The text is available on Wikisource
    at [https://en.wikisource.org/wiki/The_Verdict](wiki.html), and you can copy and
    paste it into a text file, which I copied into a text file `"the-verdict.txt"`
    to load using Python''s standard file reading utilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 Reading in a short story as text sample into Python
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can find this "`the-verdict.txt"` file in this book's GitHub
    repository at [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/01_main-chapter-code](ch02.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The print command prints the total number of characters followed by the first
    100 characters of this file for illustration purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Our goal is to tokenize this 20,479-character short story into individual words
    and special characters that we can then turn into embeddings for LLM training
    in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Text sample sizes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note that it's common to process millions of articles and hundreds of thousands
    of books -- many gigabytes of text -- when working with LLMs. However, for educational
    purposes, it's sufficient to work with smaller text samples like a single book
    to illustrate the main ideas behind the text processing steps and to make it possible
    to run it in reasonable time on consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: How can we best split this text to obtain a list of tokens? For this, we go
    on a small excursion and use Python's regular expression library `re` for illustration
    purposes. (Note that you don't have to learn or memorize any regular expression
    syntax since we will transition to a pre-built tokenizer later in this chapter.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Using some simple example text, we can use the `re.split` command with the
    following syntax to split a text on whitespace characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a list of individual words, whitespaces, and punctuation characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the simple tokenization scheme above mostly works for separating the
    example text into individual words, however, some words are still connected to
    punctuation characters that we want to have as separate list entries. We also
    refrain from making all text lowercase because capitalization helps LLMs distinguish
    between proper nouns and common nouns, understand sentence structure, and learn
    to generate text with proper capitalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s modify the regular expression splits on whitespaces (`\s`) and commas,
    and periods (`[,.]`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the words and punctuation characters are now separate list
    entries just as we wanted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A small remaining issue is that the list still includes whitespace characters.
    Optionally, we can remove these redundant characters safely as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting whitespace-free output looks like as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Removing whitespaces or not
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When developing a simple tokenizer, whether we should encode whitespaces as
    separate characters or just remove them depends on our application and its requirements.
    Removing whitespaces reduces the memory and computing requirements. However, keeping
    whitespaces can be useful if we train models that are sensitive to the exact structure
    of the text (for example, Python code, which is sensitive to indentation and spacing).
    Here, we remove whitespaces for simplicity and brevity of the tokenized outputs.
    Later, we will switch to a tokenization scheme that includes whitespaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tokenization scheme we devised above works well on the simple sample text.
    Let''s modify it a bit further so that it can also handle other types of punctuation,
    such as question marks, quotation marks, and the double-dashes we have seen earlier
    in the first 100 characters of Edith Wharton''s short story, along with additional
    special characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As we can see based on the results summarized in Figure 2.5, our tokenization
    scheme can now handle the various special characters in the text successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 The tokenization scheme we implemented so far splits text into individual
    words and punctuation characters. In the specific example shown in this figure,
    the sample text gets split into 10 individual tokens.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we got a basic tokenizer working, let''s apply it to Edith Wharton''s
    entire short story:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The above print statement outputs `4649`, which is the number of tokens in this
    text (without whitespaces).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s print the first 30 tokens for a quick visual check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output shows that our tokenizer appears to be handling the text
    well since all words and special characters are neatly separated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 2.3 Converting tokens into token IDs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we tokenized a short story by Edith Wharton into individual
    tokens. In this section, we will convert these tokens from a Python string to
    an integer representation to produce the so-called token IDs. This conversion
    is an intermediate step before converting the token IDs into embedding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: To map the previously generated tokens into token IDs, we have to build a so-called
    vocabulary first. This vocabulary defines how we map each unique word and special
    character to a unique integer, as shown in Figure 2.6.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 We build a vocabulary by tokenizing the entire text in a training
    dataset into individual tokens. These individual tokens are then sorted alphabetically,
    and duplicate tokens are removed. The unique tokens are then aggregated into a
    vocabulary that defines a mapping from each unique token to a unique integer value.
    The depicted vocabulary is purposefully small for illustration purposes and contains
    no punctuation or special characters for simplicity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous section, we tokenized Edith Wharton''s short story and assigned
    it to a Python variable called `preprocessed`. Let''s now create a list of all
    unique tokens and sort them alphabetically to determine the vocabulary size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After determining that the vocabulary size is 1,159 via the above code, we
    create the vocabulary and print its first 50 entries for illustration purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Creating a vocabulary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, based on the output above, the dictionary contains individual
    tokens associated with unique integer labels. Our next goal is to apply this vocabulary
    to convert new text into token IDs, as illustrated in Figure 2.7.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 Starting with a new text sample, we tokenize the text and use the
    vocabulary to convert the text tokens into token IDs. The vocabulary is built
    from the entire training set and can be applied to the training set itself and
    any new text samples. The depicted vocabulary contains no punctuation or special
    characters for simplicity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image013.png)'
  prefs: []
  type: TYPE_IMG
- en: Later in this book, when we want to convert the outputs of an LLM from numbers
    back into text, we also need a way to turn token IDs into text. For this, we can
    create an inverse version of the vocabulary that maps token IDs back to corresponding
    text tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement a complete tokenizer class in Python with an `encode` method
    that splits text into tokens and carries out the string-to-integer mapping to
    produce token IDs via the vocabulary. In addition, we implement a `decode` method
    that carries out the reverse integer-to-string mapping to convert the token IDs
    back into text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this tokenizer implementation is as in listing 2.3:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 Implementing a simple text tokenizer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Using the `SimpleTokenizerV1` Python class above, we can now instantiate new
    tokenizer objects via an existing vocabulary, which we can then use to encode
    and decode text, as illustrated in Figure 2.8.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.8 Tokenizer implementations share two common methods: an encode method
    and a decode method. The encode method takes in the sample text, splits it into
    individual tokens, and converts the tokens into token IDs via the vocabulary.
    The decode method takes in token IDs, converts them back into text tokens, and
    concatenates the text tokens into natural text.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s instantiate a new tokenizer object from the `SimpleTokenizerV1` class
    and tokenize a passage from Edith Wharton''s short story to try it out in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The code above prints the following token IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s see if we can turn these token IDs back into text using the decode
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Based on the output above, we can see that the decode method successfully converted
    the token IDs back into the original text.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing
    text based on a snippet from the training set. Let''s now apply it to a new text
    sample that is not contained in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the code above will result in the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The problem is that the word "Hello" was not used in the *The Verdict* short
    story. Hence, it is not contained in the vocabulary. This highlights the need
    to consider large and diverse training sets to extend the vocabulary when working
    on LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will test the tokenizer further on text that contains
    unknown words, and we will also discuss additional special tokens that can be
    used to provide further context for an LLM during training.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Adding special context tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we implemented a simple tokenizer and applied it to
    a passage from the training set. In this section, we will modify this tokenizer
    to handle unknown words.
  prefs: []
  type: TYPE_NORMAL
- en: We will also discuss the usage and addition of special context tokens that can
    enhance a model's understanding of context or other relevant information in the
    text. These special tokens can include markers for unknown words and document
    boundaries, for example.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we will modify the vocabulary and tokenizer we implemented in
    the previous section, `SimpleTokenizerV2`, to support two new tokens, `<|unk|>`
    and `<|endoftext|>`, as illustrated in Figure 2.9.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.9 We add special tokens to a vocabulary to deal with certain contexts.
    For instance, we add an <|unk|> token to represent new and unknown words that
    were not part of the training data and thus not part of the existing vocabulary.
    Furthermore, we add an <|endoftext|> token that we can use to separate two unrelated
    text sources.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image017.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in Figure 2.9, we can modify the tokenizer to use an `<|unk|>` token
    if it encounters a word that is not part of the vocabulary. Furthermore, we add
    a token between unrelated texts. For example, when training GPT-like LLMs on multiple
    independent documents or books, it is common to insert a token before each document
    or book that follows a previous text source, as illustrated in Figure 2.10\. This
    helps the LLM understand that, although these text sources are concatenated for
    training, they are, in fact, unrelated.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 When working with multiple independent text source, we add <|endoftext|>
    tokens between these texts. These <|endoftext|> tokens act as markers, signaling
    the start or end of a particular segment, allowing for more effective processing
    and understanding by the LLM.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now modify the vocabulary to include these two special tokens, `<unk>`
    and `<|endoftext|>`, by adding these to the list of all unique words that we created
    in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Based on the output of the print statement above, the new vocabulary size is
    1161 (the vocabulary size in the previous section was 1159).
  prefs: []
  type: TYPE_NORMAL
- en: 'As an additional quick check, let''s print the last 5 entries of the updated
    vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The code above prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the code output above, we can confirm that the two new special tokens
    were indeed successfully incorporated into the vocabulary. Next, we adjust the
    tokenizer from code listing 2.3 accordingly, as shown in listing 2.4:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.4 A simple text tokenizer that handles unknown words
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Compared to the `SimpleTokenizerV1` we implemented in code listing 2.3 in the
    previous section, the new `SimpleTokenizerV2` replaces unknown words by `<|unk|>`
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now try this new tokenizer out in practice. For this, we will use a
    simple text sample that we concatenate from two independent and unrelated sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s tokenize the sample text using the `SimpleTokenizerV2` on the
    vocab we previously created in listing 2.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the following token IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Above, we can see that the list of token IDs contains 1159 for the <|endoftext|>
    separator token as well as two 1160 tokens, which are used for unknown words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s de-tokenize the text for a quick sanity check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Based on comparing the de-tokenized text above with the original input text,
    we know that the training dataset, Edith Wharton's short story *The Verdict*,
    did not contain the words "Hello" and "palace."
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have discussed tokenization as an essential step in processing text
    as input to LLMs. Depending on the LLM, some researchers also consider additional
    special tokens such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[BOS]` (beginning of sequence): This token marks the start of a text. It signifies
    to the LLM where a piece of content begins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[EOS]` (end of sequence): This token is positioned at the end of a text, and
    is especially useful when concatenating multiple unrelated texts, similar to `<|endoftext|>`.
    For instance, when combining two different Wikipedia articles or books, the `[EOS]`
    token indicates where one article ends and the next one begins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[PAD]` (padding): When training LLMs with batch sizes larger than one, the
    batch might contain texts of varying lengths. To ensure all texts have the same
    length, the shorter texts are extended or "padded" using the `[PAD]` token, up
    to the length of the longest text in the batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the tokenizer used for GPT models does not need any of these tokens
    mentioned above but only uses an `<|endoftext|>` token for simplicity. The `<|endoftext|>`
    is analogous to the `[EOS]` token mentioned above. Also, `<|endoftext|>` is used
    for padding as well. However, as we'll explore in subsequent chapters when training
    on batched inputs, we typically use a mask, meaning we don't attend to padded
    tokens. Thus, the specific token chosen for padding becomes inconsequential.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the tokenizer used for GPT models also doesn't use an `<|unk|>` token
    for out-of-vocabulary words. Instead, GPT models use a *byte pair encoding* tokenizer,
    which breaks down words into subword units, which we will discuss in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Byte pair encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We implemented a simple tokenization scheme in the previous sections for illustration
    purposes. This section covers a more sophisticated tokenization scheme based on
    a concept called byte pair encoding (BPE). The BPE tokenizer covered in this section
    was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since implementing BPE can be relatively complicated, we will use an existing
    Python open-source library called *tiktoken* ([https://github.com/openai/tiktoken](openai.html)),
    which implements the BPE algorithm very efficiently based on source code in Rust.
    Similar to other Python libraries, we can install the tiktoken library via Python''s
    `pip` installer from the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The code in this chapter is based on tiktoken 0.5.1\. You can use the following
    code to check the version you currently have installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented
    previously via an `encode` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The code above prints the following token IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then convert the token IDs back into text using the decode method, similar
    to our `SimpleTokenizerV2` earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The above code prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We can make two noteworthy observations based on the token IDs and decoded text
    above. First, the `<|endoftext|>` token is assigned a relatively large token ID,
    namely, 50256\. In fact, the BPE tokenizer, which was used to train models such
    as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary
    size of 50,257, with `<|endoftext|>` being assigned the largest token ID.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the BPE tokenizer above encodes and decodes unknown words, such as "someunknownPlace"
    correctly. The BPE tokenizer can handle any unknown word. How does it achieve
    this without using `<|unk|>` tokens?
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm underlying BPE breaks down words that aren't in its predefined
    vocabulary into smaller subword units or even individual characters, enabling
    it to handle out-of-vocabulary words. So, thanks to the BPE algorithm, if the
    tokenizer encounters an unfamiliar word during tokenization, it can represent
    it as a sequence of subword tokens or characters, as illustrated in Figure 2.11.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 BPE tokenizers break down unknown words into subwords and individual
    characters. This way, a BPE tokenizer can parse any word and doesn't need to replace
    unknown words with special tokens, such as <|unk|>.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image021.png)'
  prefs: []
  type: TYPE_IMG
- en: As illustrated in Figure 2.11, the ability to break down unknown words into
    individual characters ensures that the tokenizer, and consequently the LLM that
    is trained with it, can process any text, even if it contains words that were
    not present in its training data.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.1 Byte pair encoding of unknown words
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Try the BPE tokenizer from the tiktoken library on the unknown words "Akwirw
    ier" and print the individual token IDs. Then, call the decode function on each
    of the resulting integers in this list to reproduce the mapping shown in Figure
    2.11\. Lastly, call the decode method on the token IDs to check whether it can
    reconstruct the original input, "Akwirw ier".
  prefs: []
  type: TYPE_NORMAL
- en: A detailed discussion and implementation of BPE is out of the scope of this
    book, but in short, it builds its vocabulary by iteratively merging frequent characters
    into subwords and frequent subwords into words. For example, BPE starts with adding
    all individual single characters to its vocabulary ("a", "b", ...). In the next
    stage, it merges character combinations that frequently occur together into subwords.
    For example, "d" and "e" may be merged into the subword "de," which is common
    in many English words like "define", "depend", "made", and "hidden". The merges
    are determined by a frequency cutoff.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Data sampling with a sliding window
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous section covered the tokenization steps and conversion from string
    tokens into integer token IDs in great detail. The next step before we can finally
    create the embeddings for the LLM is to generate the input-target pairs required
    for training an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: What do these input-target pairs look like? As we learned in chapter 1, LLMs
    are pretrained by predicting the next word in a text, as depicted in figure 2.12.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 Given a text sample, extract input blocks as subsamples that serve
    as input to the LLM, and the LLM's prediction task during training is to predict
    the next word that follows the input block. During training, we mask out all words
    that are past the target. Note that the text shown in this figure would undergo
    tokenization before the LLM can process it; however, this figure omits the tokenization
    step for clarity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image023.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section we implement a data loader that fetches the input-target pairs
    depicted in Figure 2.12 from the training dataset using a sliding window approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, we will first tokenize the whole The Verdict short story we
    worked with earlier using the BPE tokenizer introduced in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Executing the code above will return 5145, the total number of tokens in the
    training set, after applying the BPE tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we remove the first 50 tokens from the dataset for demonstration purposes
    as it results in a slightly more interesting text passage in the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the easiest and most intuitive ways to create the input-target pairs
    for the next-word prediction task is to create two variables, `x` and `y`, where
    `x` contains the input tokens and `y` contains the targets, which are the inputs
    shifted by 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the above code prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Processing the inputs along with the targets, which are the inputs shifted
    by one position, we can then create the next-word prediction tasks depicted earlier
    in figure 2.12, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The code above prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Everything left of the arrow (`---->`) refers to the input an LLM would receive,
    and the token ID on the right side of the arrow represents the target token ID
    that the LLM is supposed to predict.
  prefs: []
  type: TYPE_NORMAL
- en: 'For illustration purposes, let''s repeat the previous code but convert the
    token IDs into text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following outputs show how the input and outputs look in text format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We've now created the input-target pairs that we can turn into use for the LLM
    training in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s only one more task before we can turn the tokens into embeddings,
    as we mentioned at the beginning of this chapter: implementing an efficient data
    loader that iterates over the input dataset and returns the inputs and targets
    as PyTorch tensors, which can be thought of as multidimensional arrays.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we are interested in returning two tensors: an input tensor
    containing the text that the LLM sees and a target tensor that includes the targets
    for the LLM to predict, as depicted in Figure 2.13.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 To implement efficient data loaders, we collect the inputs in a
    tensor, x, where each row represents one input context. A second tensor, y, contains
    the corresponding prediction targets (next words), which are created by shifting
    the input by one position.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image025.png)'
  prefs: []
  type: TYPE_IMG
- en: While Figure 2.13 shows the tokens in string format for illustration purposes,
    the code implementation will operate on token IDs directly since the `encode`
    method of the BPE tokenizer performs both tokenization and conversion into token
    IDs as a single step.
  prefs: []
  type: TYPE_NORMAL
- en: For the efficient data loader implementation, we will use PyTorch's built-in
    Dataset and DataLoader classes. For additional information and guidance on installing
    PyTorch, please see section A.1.3, Installing PyTorch, in Appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the dataset class is shown in code listing 2.5:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.5 A dataset for batched inputs and targets
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The `GPTDatasetV1` class in listing 2.5 is based on the PyTorch `Dataset` class
    and defines how individual rows are fetched from the dataset, where each row consists
    of a number of token IDs (based on a `max_length`) assigned to an `input_chunk`
    tensor. The `target_chunk` tensor contains the corresponding targets. I recommend
    reading on to see how the data returned from this dataset looks like when we combine
    the dataset with a PyTorch `DataLoader` -- this will bring additional intuition
    and clarity.
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to the structure of PyTorch `Dataset` classes, such as shown
    in listing 2.5, please read section *A.6, Setting up efficient data loaders*,
    in Appendix A, which explains the general structure and usage of PyTorch `Dataset`
    and `DataLoader` classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will use the `GPTDatasetV1` to load the inputs in batches
    via a PyTorch `DataLoader`:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 A data loader to generate batches with input-with pairs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s test the `dataloader` with a batch size of 1 for an LLM with a context
    size of 4 to develop an intuition of how the `GPTDatasetV1` class from listing
    2.5 and the `create_dataloader_v1` function from listing 2.6 work together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the preceding code prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The `first_batch` variable contains two tensors: the first tensor stores the
    input token IDs, and the second tensor stores the target token IDs. Since the
    `max_length` is set to 4, each of the two tensors contains 4 token IDs. Note that
    an input size of 4 is relatively small and only chosen for illustration purposes.
    It is common to train LLMs with input sizes of at least 256.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the meaning of `stride=1`, let''s fetch another batch from this
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The second batch has the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: If we compare the first with the second batch, we can see that the second batch's
    token IDs are shifted by one position compared to the first batch (for example,
    the second ID in the first batch's input is 367, which is the first ID of the
    second batch's input). The `stride` setting dictates the number of positions the
    inputs shift across batches, emulating a sliding window approach, as demonstrated
    in Figure 2.14.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.14 When creating multiple batches from the input dataset, we slide
    an input window across the text. If the stride is set to 1, we shift the input
    window by 1 position when creating the next batch. If we set the stride equal
    to the input window size, we can prevent overlaps between the batches.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image027.png)'
  prefs: []
  type: TYPE_IMG
- en: Exercise 2.2 Data loaders with different strides and context sizes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To develop more intuition for how the data loader works, try to run it with
    different settings such as max_length=2 and stride=2 and max_length=8 and stride=2.
  prefs: []
  type: TYPE_NORMAL
- en: Batch sizes of 1, such as we have sampled from the data loader so far, are useful
    for illustration purposes. If you have previous experience with deep learning,
    you may know that small batch sizes require less memory during training but lead
    to more noisy model updates. Just like in regular deep learning, the batch size
    is a trade-off and hyperparameter to experiment with when training LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the two final sections of this chapter that are focused
    on creating the embedding vectors from the token IDs, let''s have a brief look
    at how we can use the data loader to sample with a batch size greater than 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Note that we increase the stride to 4\. This is to utilize the data set fully
    (we don't skip a single word) but also avoid any overlap between the batches,
    since more overlap could lead to increased overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In the final two sections of this chapter, we will implement embedding layers
    that convert the token IDs into continuous vector representations, which serve
    as input data format for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Creating token embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last step for preparing the input text for LLM training is to convert the
    token IDs into embedding vectors, as illustrated in Figure 2.15, which will be
    the focus of these two last remaining sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.15 Preparing the input text for an LLM involves tokenizing text, converting
    text tokens to token IDs, and converting token IDs into vector embedding vectors.
    In this section, we consider the token IDs created in previous sections to create
    the token embedding vectors.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image029.png)'
  prefs: []
  type: TYPE_IMG
- en: In addition to the processes outlined in Figure 2.15, it is important to note
    that we initialize these embedding weights with random values as a preliminary
    step. This initialization serves as the starting point for the LLM's learning
    process. We will optimize the embedding weights as part of the LLM training in
    chapter 5.
  prefs: []
  type: TYPE_NORMAL
- en: A continuous vector representation, or embedding, is necessary since GPT-like
    LLMs are deep neural networks trained with the backpropagation algorithm. If you
    are unfamiliar with how neural networks are trained with backpropagation, please
    read section A.4, *Automatic differentiation made easy*, in Appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s illustrate how the token ID to embedding vector conversion works with
    a hands-on example. Suppose we have the following four input tokens with IDs 2,
    3, 5, and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of simplicity and illustration purposes, suppose we have a small
    vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary),
    and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288
    dimensions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `vocab_size` and `output_dim`, we can instantiate an embedding layer
    in PyTorch, setting the random seed to 123 for reproducibility purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The print statement in the preceding code example prints the embedding layer''s
    underlying weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the weight matrix of the embedding layer contains small, random
    values. These values are optimized during LLM training as part of the LLM optimization
    itself, as we will see in upcoming chapters. Moreover, we can see that the weight
    matrix has six rows and three columns. There is one row for each of the six possible
    tokens in the vocabulary. And there is one column for each of the three embedding
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we instantiated the embedding layer, let''s now apply it to a token ID
    to obtain the embedding vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned embedding vector is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: If we compare the embedding vector for token ID 3 to the previous embedding
    matrix, we see that it is identical to the 4th row (Python starts with a zero
    index, so it's the row corresponding to index 3). In other words, the embedding
    layer is essentially a look-up operation that retrieves rows from the embedding
    layer's weight matrix via a token ID.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding layers versus matrix multiplication
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For those who are familiar with one-hot encoding, the embedding layer approach
    above is essentially just a more efficient way of implementing one-hot encoding
    followed by matrix multiplication in a fully connected layer, which is illustrated
    in the supplementary code on GitHub at [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/03_bonus_embedding-vs-matmul](ch02.html).
    Because the embedding layer is just a more efficient implementation equivalent
    to the one-hot encoding and matrix-multiplication approach, it can be seen as
    a neural network layer that can be optimized via backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we have seen how to convert a single token ID into a three-dimensional
    embedding vector. Let''s now apply that to all four input IDs we defined earlier
    (`torch.tensor([2, 3, 5, 1])`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The print output reveals that this results in a 4x3 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Each row in this output matrix is obtained via a lookup operation from the embedding
    weight matrix, as illustrated in Figure 2.16.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16 Embedding layers perform a look-up operation, retrieving the embedding
    vector corresponding to the token ID from the embedding layer's weight matrix.
    For instance, the embedding vector of the token ID 5 is the sixth row of the embedding
    layer weight matrix (it is the sixth instead of the fifth row because Python starts
    counting at 0). For illustration purposes, we assume that the token IDs were produced
    by the small vocabulary we used in section 2.3.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image031.png)'
  prefs: []
  type: TYPE_IMG
- en: This section covered how we create embedding vectors from token IDs. The next
    and final section of this chapter will add a small modification to these embedding
    vectors to encode positional information about a token within a text.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Encoding word positions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we converted the token IDs into a continuous vector
    representation, the so-called token embeddings. In principle, this is a suitable
    input for an LLM. However, a minor shortcoming of LLMs is that their self-attention
    mechanism, which will be covered in detail in chapter 3, doesn't have a notion
    of position or order for the tokens within a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: The way the previously introduced embedding layer works is that the same token
    ID always gets mapped to the same vector representation, regardless of where the
    token ID is positioned in the input sequence, as illustrated in Figure 2.17.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.17 The embedding layer converts a token ID into the same vector representation
    regardless of where it is located in the input sequence. For example, the token
    ID 5, whether it's in the first or third position in the token ID input vector,
    will result in the same embedding vector.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image033.png)'
  prefs: []
  type: TYPE_IMG
- en: In principle, the deterministic, position-independent embedding of the token
    ID is good for reproducibility purposes. However, since the self-attention mechanism
    of LLMs itself is also position-agnostic, it is helpful to inject additional position
    information into the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, there are two broad categories of position-aware embeddings:
    relative *positional embeddings* and absolute positional embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: Absolute positional embeddings are directly associated with specific positions
    in a sequence. For each position in the input sequence, a unique embedding is
    added to the token's embedding to convey its exact location. For instance, the
    first token will have a specific positional embedding, the second token another
    distinct embedding, and so on, as illustrated in Figure 2.18.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.18 Positional embeddings are added to the token embedding vector to
    create the input embeddings for an LLM. The positional vectors have the same dimension
    as the original token embeddings. The token embeddings are shown with value 1
    for simplicity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image035.png)'
  prefs: []
  type: TYPE_IMG
- en: Instead of focusing on the absolute position of a token, the emphasis of relative
    positional embeddings is on the relative position or distance between tokens.
    This means the model learns the relationships in terms of "how far apart" rather
    than "at which exact position." The advantage here is that the model can generalize
    better to sequences of varying lengths, even if it hasn't seen such lengths during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Both types of positional embeddings aim to augment the capacity of LLMs to understand
    the order and relationships between tokens, ensuring more accurate and context-aware
    predictions. The choice between them often depends on the specific application
    and the nature of the data being processed.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI's GPT models use absolute positional embeddings that are optimized during
    the training process rather than being fixed or predefined like the positional
    encodings in the original Transformer model. This optimization process is part
    of the model training itself, which we will implement later in this book. For
    now, let's create the initial positional embeddings to create the LLM inputs for
    the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we focused on very small embedding sizes in this chapter for illustration
    purposes. We now consider more realistic and useful embedding sizes and encode
    the input tokens into a 256-dimensional vector representation. This is smaller
    than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288
    dimensions) but still reasonable for experimentation. Furthermore, we assume that
    the token IDs were created by the BPE tokenizer that we implemented earlier, which
    has a vocabulary size of 50,257:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Using the `token_embedding_layer` above, if we sample data from the data loader,
    we embed each token in each batch into a 256-dimensional vector. If we have a
    batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s instantiate the data loader from section 2.6, *Data sampling with a
    sliding window*, first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the token ID tensor is 8x4-dimensional, meaning that the data
    batch consists of 8 text samples with 4 tokens each.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now use the embedding layer to embed these token IDs into 256-dimensional
    vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding print function call returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: As we can tell based on the 8x4x256-dimensional tensor output, each token ID
    is now embedded as a 256-dimensional vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a GPT model''s absolute embedding approach, we just need to create another
    embedding layer that has the same dimension as the `token_embedding_layer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code example, the input to the `pos_embeddings` is
    usually a placeholder vector `torch.arange(context_length)`, which contains a
    sequence of numbers 0, 1, ..., up to the maximum input length  1\. The `context_length`
    is a variable that represents the supported input size of the LLM. Here, we choose
    it similar to the maximum length of the input text. In practice, input text can
    be longer than the supported context length, in which case we have to truncate
    the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the print statement is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the positional embedding tensor consists of four 256-dimensional
    vectors. We can now add these directly to the token embeddings, where PyTorch
    will add the 4x256-dimensional `pos_embeddings` tensor to each 4x256-dimensional
    token embedding tensor in each of the 8 batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The print output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The `input_embeddings` we created, as summarized in Figure 2.19, are the embedded
    input examples that can now be processed by the main LLM modules, which we will
    begin implementing in chapter 3
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.19 As part of the input processing pipeline, input text is first broken
    up into individual tokens. These tokens are then converted into token IDs using
    a vocabulary. The token IDs are converted into embedding vectors to which positional
    embeddings of a similar size are added, resulting in input embeddings that are
    used as input for the main LLM layers.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image037.png)'
  prefs: []
  type: TYPE_IMG
- en: 2.9 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs require textual data to be converted into numerical vectors, known as embeddings
    since they can't process raw text. Embeddings transform discrete data (like words
    or images) into continuous vector spaces, making them compatible with neural network
    operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the first step, raw text is broken into tokens, which can be words or characters.
    Then, the tokens are converted into integer representations, termed token IDs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Special tokens, such as `<|unk|>` and `<|endoftext|>`, can be added to enhance
    the model's understanding and handle various contexts, such as unknown words or
    marking the boundary between unrelated texts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3 can
    efficiently handle unknown words by breaking them down into subword units or individual
    characters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a sliding window approach on tokenized data to generate input-target
    pairs for LLM training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding layers in PyTorch function as a lookup operation, retrieving vectors
    corresponding to token IDs. The resulting embedding vectors provide continuous
    representations of tokens, which is crucial for training deep learning models
    like LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While token embeddings provide consistent vector representations for each token,
    they lack a sense of the token''s position in a sequence. To rectify this, two
    main types of positional embeddings exist: absolute and relative. OpenAI''s GPT
    models utilize absolute positional embeddings that are added to the token embedding
    vectors and are optimized during the model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
