- en: Chapter 8\. Alignment Training and Reasoning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some common reasons for hesitancy in adopting LLMs is the presence of hallucinations,
    the limitations in reasoning skills, and bias and safety issues. In this chapter,
    we will go through these limitations and introduce different techniques to mitigate
    them. First, we will introduce the concept of alignment training, which helps
    us steer our models toward desirable outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Alignment Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We keep hearing about the *alignment problem* facing language models. What does
    this mean in practice? Ideally we would like a language model that we can fully
    understand, control, and steer. However, current language models are far from
    this ideal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the goal of alignment is to make language models more controllable and
    steerable. [Askell et al.](https://oreil.ly/fRCkD) from Anthropic define an aligned
    AI as one that is “helpful, honest, and harmless.” They further define the three
    H’s as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Helpful
  prefs: []
  type: TYPE_NORMAL
- en: As long as a user request isn’t harmful, the AI should attempt to solve the
    request as effectively as possible, asking follow-up questions if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Honest
  prefs: []
  type: TYPE_NORMAL
- en: The AI should provide accurate information and should be calibrated, providing
    reasonably accurate uncertainty estimates. It should understand its shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Harmless
  prefs: []
  type: TYPE_NORMAL
- en: The AI should not be offensive or discriminatory and should refuse to perform
    tasks that can cause harm to individuals or society.
  prefs: []
  type: TYPE_NORMAL
- en: These are lofty principles. Can LLMs meet them? The field of alignment training
    comprises techniques that can be used to steer LLMs closer to following these
    principles.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Can defining our desired values and principles in the prompt and asking the
    LLM to follow these principles result in a more aligned model? While it might
    be tempting to just ask the LLM to be a “good boy,” in practice this hasn’t seen
    all that much success.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since prompting LLMs to be nice doesn’t work, we will need to tune the model
    in some way. Supervised fine-tuning (discussed in [Chapter 6](ch06.html#llm-fine-tuning))
    on alignment datasets is an option. However, techniques like reinforcement learning
    have seen more success, which we will describe next in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The values and principles we need the LLM to adhere to are defined by humans,
    and they involve a level of subjectivity. Thus, it makes sense to optimize the
    model directly on human feedback. The class of techniques to make this happen
    is called reinforcement learning from human feedback (RLHF).
  prefs: []
  type: TYPE_NORMAL
- en: In traditional reinforcement learning, an agent interacts with its environment
    and performs actions to accomplish a task, using trial and error. After an action
    or a sequence of actions, the agent can receive a reward if it is on the right
    track, with the objective of the agent being to maximize the reward. This is specified
    through a reward function. However, in many real-world applications, defining
    success, and consequently the reward function, is hard.
  prefs: []
  type: TYPE_NORMAL
- en: In RLHF, the feedback is provided by a human-in-the-loop in an iterative fashion.
    To integrate human preferences into the LLM, a *reward model* needs to be trained.
    Various forms of feedback can be provided by human reviewers.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Human Feedback
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Human feedback can be provided through one of these forms:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary feedback
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, the feedback is provided as either yes/no (accept/reject).
  prefs: []
  type: TYPE_NORMAL
- en: Binary comparisons
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, the human evaluates outputs A and B and specifies their preference
    among the two.
  prefs: []
  type: TYPE_NORMAL
- en: Ranking
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, the human evaluates a set of outputs and provides a rank ordering
    of preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Corrective feedback
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, the human explicitly states what should have been the ideal
    output, potentially in natural language.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s describe a popular RLHF setup, pioneered by OpenAI. The alignment training
    consists of three distinct phases:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Supervised fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, the pre-trained model is fine-tuned on a supervised dataset
    of human preferences. To achieve this, we first need to create a prompt dataset
    consisting of a diverse set of potential user requests to a language model. Human
    annotators then provide desired responses to these prompts. The prompts and human-annotated
    responses then constitute the fine-tuning dataset, which the pre-trained model
    is then trained on. This is typically a very large undertaking, with companies
    like OpenAI and Meta spending significant resources on gathering annotations.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Reward modeling
  prefs: []
  type: TYPE_NORMAL
- en: In this step, a diverse set of prompts is queried to the language model and
    multiple generations (responses) are extracted for each prompt. Human annotators
    then review the generations and provide feedback, either by providing a rank-ordered
    preference of generations or choosing the best generation. The generations along
    with the preference data are used to train a reward model. The reward model is
    trained to predict which output a human would prefer among a list of candidate
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Proximal policy optimization (PPO)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the reward model is used to optimize the pre-trained model that was
    fine-tuned in the first step. This is typically performed using an algorithm called
    PPO.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of training using PPO is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The language model generates a response or a continuation of a prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reward model takes the query and response and outputs a scalar reward, representing
    the quality of fitness of the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The log-probabilities of the tokens in the query-response sequence are calculated,
    using the model being tuned (the SFT model) and a reference model (usually the
    pre-trained model before the SFT step). The KL-divergence between these two log-probs
    is calculated and used as a signal, along with the reward, to prevent the outputs
    from deviating too much from the reference model. This acts as a regularization
    step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is trained using the PPO algorithm, with signals from steps 2 and
    3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s use the Hugging Face TRL library to perform RLHF. First, we need to train
    a reward model. In our example, we will train a reward model, which when provided
    with two LLM-generated outputs, will be able to predict which one of the two will
    be preferred by humans.
  prefs: []
  type: TYPE_NORMAL
- en: Very few high-quality datasets are publicly available, one of them being Anthropic’s
    [hh-rlhf dataset](https://oreil.ly/kzSQf). We will use this dataset in our example.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore this dataset in detail. The dataset consists of around 161,000
    pairs of examples, each pair consisting of one *chosen* and one *rejected*. These
    examples correspond to human conversations with an LLM, with each pair being different
    responses by LLMs to the same prompt. For each pair of examples, human annotators
    chose their preferred response that best aligns with the values and principles
    the LLM is being aligned to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chosen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Rejected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the dataset, you can use TRL’s `RewardTrainer` class for training
    the reward model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can use TRL’s `PPOTrainer` class for performing the PPO step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Refer to the book’s [GitHub repo](https://oreil.ly/llm-playbooks) for the entire
    code. Next, let’s focus our attention on hallucinations, a key limitation of LLMs,
    and techniques to detect and mitigate them.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Colloquially, the word “hallucination” is used to refer to instances where
    the LLM generates something deemed to be incorrect. But LLMs can be incorrect
    for a wide variety of reasons. In this book, we will lead with a narrower definition
    of hallucinations: “An LLM is said to have hallucinated a generation if the generated
    text is not grounded to its training data or input context.”'
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, if an LLM generates text that is inconsistent with either its pre-training/fine-tuning
    data or input context, then we call it a hallucination. Hallucinations most commonly
    manifest as text with no factual basis. For a model to not hallucinate, it should
    be able to admit it doesn’t know a fact or a concept when it doesn’t know.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a hallucination. Consider the prompt fed to the [NousResearch
    Hermes LLM](https://oreil.ly/tXmYh):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* Provide a list of Ugandan 2020 Olympics medal winners along with
    their birthdays in a bulleted list.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Response:* Joshua Cheptegei (Athletics - Men’s 10,000 meters gold medalist,
    born September 19, 1996)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jacob Kiplimo (Athletics - Men’s 10,000 meters silver medalist, born January
    16, 1997)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Peruth Chemutai (Athletics - Women’s 3000 meters steeplechase bronze medalist,
    born March 10, 1999)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yes, it is true that these are three athletes who won medals for Uganda in the
    2020 Summer Olympics, but a lot of content is hallucinated. Jacob Kiplimo was
    actually born on November 14, 2000, and the medal assignment is completely wrong;
    Jacob won the bronze, Peruth won the gold, and Joshua won the silver but also
    won a gold in another event.
  prefs: []
  type: TYPE_NORMAL
- en: A model that doesn’t hallucinate would have provided factual information and
    admit that it doesn’t know specific details.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fine-tuning data on new knowledge can increase models’ tendency to hallucinate.
    [Gekhman et al.](https://oreil.ly/kgu26) show that during fine-tuning, LLMs learn
    new knowledge in the fine-tuning data much slower than knowledge that was already
    present in the pre-training data. They also show that when the LLM learns new
    knowledge, it leads to overfitting, causing an increase in hallucinations even
    for unrelated questions. If you want to teach your model entirely new knowledge,
    I suggest using the continued pre-training setup with techniques like replay,
    etc., described in [Chapter 7](ch07.html#ch07).
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating Hallucinations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest sources of hesitancy in adopting LLM-based tools and software
    is the system’s trustworthiness or lack thereof. Trustworthiness is most affected
    by the presence of hallucinations. Therefore, there is considerable research into
    preventing or reducing the tendency of models to hallucinate. Let’s explore some
    common techniques.
  prefs: []
  type: TYPE_NORMAL
- en: At a product design level, you can reduce hallucination risk by simply not asking
    LLMs questions that you know it wouldn’t be able to answer. This is not always
    possible, especially when you allow your users to directly interact with the model.
    It is also not easy to determine what an LLM knows and does not know.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-1](#knowledge-quadrant) depicts a knowledge quadrant across knowledge
    and awareness dimensions. Ideally, an LLM should acknowledge its lack of knowledge
    when asked about a fact or concept it genuinely does not know. In [Figure 8-1](#knowledge-quadrant),
    we see that there can be four types of knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: Known knowns
  prefs: []
  type: TYPE_NORMAL
- en: The LLM knows this knowledge/skill and is able to utilize it.
  prefs: []
  type: TYPE_NORMAL
- en: Unknown knowns
  prefs: []
  type: TYPE_NORMAL
- en: The LLM knows this knowledge/skill but is not able to utilize it effectively
    (can be unlocked by fine-tuning or in-context learning).
  prefs: []
  type: TYPE_NORMAL
- en: Known unknowns
  prefs: []
  type: TYPE_NORMAL
- en: The LLM knows that it does not know this knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Unknown unknowns
  prefs: []
  type: TYPE_NORMAL
- en: The LLM does not know that it does not know this knowledge, leading to hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: '![knowledge-quadrant](assets/dllm_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Knowledge quadrant
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To determine the level of self-knowledge a model possesses, [Yin et al.](https://oreil.ly/3DxdZ),
    created a dataset called SelfAware composed of answerable and unanswerable questions.
    Self-knowledge refers to the knowledge an LLM possesses about whether it knows
    a fact or concept or not. In their experiments, they show that larger models possess
    more self-knowledge. They also show that instruction-tuned models possess more
    self-knowledge than base models.
  prefs: []
  type: TYPE_NORMAL
- en: An important way to assess a model’s self-knowledge is through its output uncertainty.
    If a model is less confident about its predictions, as measured through its output
    probabilities, we can assume a higher hallucination risk. For this approach to
    be valid, the model has to be well calibrated. As [Chapter 6](ch06.html#llm-fine-tuning)
    introduced, a model is well calibrated if there is a correlation between its output
    probability values and task accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Kadavath et al.](https://oreil.ly/VVY-i) show that techniques like RLHF reduce
    model calibration.'
  prefs: []
  type: TYPE_NORMAL
- en: A key technique to address hallucinations is grounding the model to factual
    data sources. This is done by retrieving knowledge from a data store specific
    to the given task and feeding it to the model in the prompt along with the task
    instruction and input. This paradigm is called RAG, which we will discuss in [Part III](part03.html#part3)
    of the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG is not a panacea for the hallucination problem for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Feeding ground-truth factual data in the prompt reduces hallucinations but does
    not eliminate them completely, especially when the context is large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using RAG shifts the bottleneck toward the retrieval process. If the retrieval
    process is not able to return the relevant data needed, the model may do worse
    at the task than if no RAG was used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many cases, we do not have access to the ground-truth data; hence we cannot
    feed it as input context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s look at techniques that do not depend on us fetching ground-truth
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Consistency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use self-consistency, which we first introduced in [Chapter 5](ch05.html#chapter_utilizing_llms),
    to detect the possibility of hallucinations. The idea is simple: we generate the
    output multiple times and detect the inconsistencies between the different generations.
    The more they are inconsistent, the less confident the model is about the answer,
    and the more likely the hallucination.'
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Self-verification is another commonly used technique. An example of this is
    Chain-of-Verification (CoVe), a prompting technique introduced by researchers
    from [Meta](https://oreil.ly/ubp1t). An example implementation of CoVe works as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The model first generates a response to the given prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the prompt and the response, the model generates verification questions
    to check the veracity of the response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of the verification questions is fed to the model separately.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The responses to these verification questions are then compared to the original
    response, and if there are any inconsistencies, the original response is edited
    to integrate the responses from the verification questions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As an example, for the earlier query:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* Provide a list of Ugandan Olympic medal winners along with their
    birthdays in a bulleted list.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'When asked to generate a list of verification questions, the model generates:'
  prefs: []
  type: TYPE_NORMAL
- en: Who won the gold medal in the men’s 10,000m event in the 2020 Tokyo Olympics
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How many Ugandan athletes won medals at the 2020 Tokyo Olympics?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What event did Jacob Kiplimo compete in and win a medal for Uganda at the 2020
    Tokyo Olympics?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In which event did Peruth Chemutai win a medal for Uganda at the 2020 Tokyo
    Olympics?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What year were Joshua Cheptegei and Jacob Kiplimo born?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What year was Peruth Chemutai born?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How many gold medals did Uganda win in total at the 2020 Tokyo Olympics?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Asking the same model the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: Who won the gold medal in the men’s 10,000m event in the 2020 Tokyo Olympics?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We get the response:'
  prefs: []
  type: TYPE_NORMAL
- en: In the 2020 Tokyo Olympics, the men’s 10,000m event was won by Selemon Barega
    of Ethiopia. He finished with a time of 27:43.22, beating the silver medalist
    Joshua Cheptegei of Uganda by 0.32 seconds.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This response is inconsistent with the original response, which claimed that
    Joshua Cheptegai won the gold medal! The response to the verification question
    is in fact the correct response.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can see how methods like CoVe can be valuable in reducing hallucination
    risk. Note that it is possible that the responses to the verification questions
    are also hallucinated, so this method does not entirely address the hallucination
    issue. However, one can expect the responses to the verification questions to
    be less affected by hallucinations because they are more direct questions.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discuss hallucination reduction using recitation.
  prefs: []
  type: TYPE_NORMAL
- en: Recitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the recitation technique, we prompt the LLM to generate one or more passages
    about the given question and then generate the answer based on the passages generated.
    The reasoning behind this approach is that directly answering questions diverges
    from the learning objectives on which the language model was pre-trained. Recitation
    serves as the intermediate step that aligns more closely to the original learning
    objective of the model, like next-token prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use few-shot prompting for soliciting recitations. The prompt looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Query: <query>'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Recitation1: <recitation>'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Recitation2: <recitation>'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: …
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'RecitationN: <recitation>'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Query: <query>'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Recitation1:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can generate a single recitation or multiple recitations. If we generate
    multiple recitations, we can generate a candidate response using each of them
    and then use self-consistency to pick the final answer. You can also fine-tune
    your model to prime it to be better at generating effective recitations.
  prefs: []
  type: TYPE_NORMAL
- en: The recitation method typically consumes fewer tokens than chain-of-actions,
    but I find the latter to be more effective.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling Methods for Addressing Hallucination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The degree of hallucination also depends on the decoding method used. Recall
    our discussion on decoding algorithms in [Chapter 5](ch05.html#chapter_utilizing_llms).
    [Lee et al.](https://oreil.ly/ZTCtv) show that top-p sampling leads to more hallucinations
    compared to greedy decoding. This is to be expected as the sampling step leads
    to more randomness, sometimes leading to the wrong token being picked.
  prefs: []
  type: TYPE_NORMAL
- en: One way to address increased hallucination risk due to sampling algorithms is
    to use a technique like factual-nucleus sampling, which [Lee et al. introduced](https://oreil.ly/D12xT).
    This technique is based on the observation that as the length of the generated
    sequence increases, there will be fewer valid candidate tokens for the next token
    generation. Thus, the randomness of the sampling algorithm is reduced as the length
    of the generated text increases, by reducing the p value in the top-p decoding
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: pt = max{ω, p × λ t−1 }
  prefs: []
  type: TYPE_NORMAL
- en: where *t* refers to the generation step.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three tunable parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Decay rate (*λ*)
  prefs: []
  type: TYPE_NORMAL
- en: The *p* value of the algorithm is decayed by a decay rate at every step of the
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: Reset (*p*)
  prefs: []
  type: TYPE_NORMAL
- en: The *p* value might decay very quickly, thus degenerating to a greedy algorithm.
    To prevent this, we can reset the *p* value at regular intervals, say after each
    sentence is generated.
  prefs: []
  type: TYPE_NORMAL
- en: Lower-bound (*ω*)
  prefs: []
  type: TYPE_NORMAL
- en: To continue maintaining the advantages of the top-p algorithm, we can prevent
    the *p* value from getting too low by enforcing a lower bound.
  prefs: []
  type: TYPE_NORMAL
- en: This method comes with a tradeoff; lowering the *p* value reduces hallucination
    risk but also decreases diversity of token generation, causing a loss in performance.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding by Contrasting Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The principle behind [*d*ecoding by c*o*ntrasting *la*yers (DoLa)](https://oreil.ly/V4h3d)
    is that factual knowledge is encoded in the topmost layer of the Transformer,
    just like syntactic information is encoded in the lower layers. Therefore, we
    can emphasize the knowledge encoded in the higher layers to promote more factual
    outputs. DoLa achieves this by using a technique called *contrastive decoding*,
    in which the next token probability for each token is calculated by taking the
    difference in logits between a higher layer and a lower layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'DoLa is available through Hugging Face. Let’s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `dola_layers` argument should be used to activate DoLa decoding. `dola_layers`
    can be either a string or a list of integers. If it is a string, it should be
    either `'high'` or `'low'`. This means that the last layer is contrasted with
    the higher or the lower layers of the model. You can also specify a list of integers
    representing layer numbers. Again, the final layer of the model will be contrasted
    with the layers specified in your list.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce repetitiveness induced by DoLa, you can set a repetition penalty through
    the `repetition_penalty` argument (this is set by default). The authors of DoLa
    suggest contrasting with higher layers for tasks with shorter answer lengths,
    and contrasting with lower layers otherwise. They also recommend not using DoLa
    for smaller LLMs. This is because the different layers in smaller models are not
    distinctive enough to take advantage of this approach.
  prefs: []
  type: TYPE_NORMAL
- en: In-Context Hallucinations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have focused on hallucinations emanating from the model trying to
    generate from its parametric memory. We can also have in-context hallucinations,
    also called closed-book hallucinations, where the model depends on data provided
    in the context to influence its generation, like in RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the passage:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the 2020 Summer Olympics in Tokyo, Uganda won three medals:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Joshua Cheptegei – Gold in Men’s 5000m
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Peruth Chemutai – Gold in Women’s 3000m Steeplechase
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jacob Kiplimo – Bronze in Men’s 10,000m
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These medals marked a historic achievement for Uganda, with both Cheptegei and
    Chemutai winning gold in their respective events.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'fed to the LLM as context for the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a list of Ugandan 2020 Olympics medal winners along with their birthdays
    in a bulleted list.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'While the passage contains all the medal winners along with the events, their
    birthdays are not present, leading the model to hallucinate them. Here is the
    model’s response:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of Ugandan 2020 Olympics medal winners with their birthdays:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Joshua Cheptegei (born September 12, 1996) - Gold medal in Men’s 5000m
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Peruth Chemutai (born August 10, 1999) - Gold medal in Women’s 3000m Steeplechase
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Jacob Kiplimo (born December 16, 1997) - Bronze medal in Men’s 10,000m
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the knowledge provided in the context, the model gets the names of the
    athletes, their medals, and their events correct, but gets the birthdays wrong
    for Peruth and Jacob.
  prefs: []
  type: TYPE_NORMAL
- en: How can we detect and mitigate these hallucinations? [Chuang et al.](https://oreil.ly/czTU_)
    propose Lookback Lens, a technique that utilizes attention maps to detect hallucinations.
    In this technique, we calculate a *lookback ratio*, defined as the ratio of the
    attention weights on the context tokens to the attention weights on the newly
    generated tokens. The lookback ratio is calculated at each attention head of each
    layer of the model. These ratios are used as features to train a linear classifier
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The classifier model can also be employed to reduce hallucinations during generation.
    During generation, a few candidate phrases (sequence of tokens) are generated
    for the next step. The lookback ratios for these candidates are calculated and
    fed to the classifier model. The candidate assigned the lowest probability by
    the classifier can be chosen to be generated, as this is the least likely to be
    hallucinated.
  prefs: []
  type: TYPE_NORMAL
- en: Using a classifier-based decoding strategy can be a massive drag on system latency,
    however. These approaches should be used only if latency isn’t a prime consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations Due to Irrelevant Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The presence of irrelevant information in the context can also lead to hallucinations.
    As an example, consider this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* Indianapolis, the capital of Indiana, is known for its vibrant cultural
    scene, including museums, theaters, and sporting events, especially the Indianapolis
    500 race. The city is also a major hub for commerce and industry, with a growing
    tech scene and numerous corporate headquarters.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Where is the actor Lily Gao born?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The response is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Response:* Lily Gao, an American actress known for her roles in various TV
    series and films, was born on October 1, 1994, in Indianapolis, Indiana, USA.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, in reality Lily Gao was born in Canada! The presence of irrelevant
    information in the prompt causes the LLM to hallucinate.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate this issue, [Weston et al.](https://oreil.ly/aqsxN) propose a technique
    called System 2 Attention (S2A). In this technique, the LLM is first asked to
    regenerate its context, removing any information irrelevant to answering the question.
    Next, the LLM is prompted with the regenerated context and the final answer is
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider this math problem with a distractor sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* Sarah has 5 apples. She buys 3 more apples from the store. Max sells
    3 apples to the store. How many apples does Sarah have now?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We issue the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* Regenerate the context removing any information that is irrelevant
    to answering the question.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The response is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Response:* Sarah has 5 apples. She buys 3 more apples from the store. How
    many apples does Sarah have now?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This can be fed back to the model to provide the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also implement S2A in a single prompt, by asking the model to regenerate
    the context followed by the final answer. However, performing this in two prompts
    has shown to be more effective.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore the reasoning capabilities of LLMs and showcase techniques
    for improving them.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#chapter_llm-introduction), we discussed the limitations
    of language models and pointed to reasoning as one of the biggest limitations.
    In this section, let’s dive into it in more detail to understand what reasoning
    entails, how well language models perform reasoning, and how to improve their
    reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s define reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: Natural language reasoning is a process to integrate multiple knowledge (e.g.
    encyclopedic knowledge and commonsense knowledge) to derive some new conclusions
    about the (realistic or hypothetical) world. Knowledge can be from both explicit
    and implicit sources. Conclusions are assertions or events assumed to be true
    in the world, or practical actions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Yu et al.](https://oreil.ly/7NsBF)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Reasoning can be classified into several different types. Here are a few forms
    of non-mutually-exclusive reasoning categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Deductive Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deductive reasoning uses logic to draw conclusions from one or more premises.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider the following passage:'
  prefs: []
  type: TYPE_NORMAL
- en: Mr. Shockley was allergic to mushrooms. The dish “Golden Travesty” has mushrooms
    in it.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Based on this set of premises, we can deduce that Mr. Shockley should stay far
    away from the Golden Travesty dish.
  prefs: []
  type: TYPE_NORMAL
- en: Inductive Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inductive reasoning involves making generalizations based on a set of observations.
    The generalizations are plausible and probabilistic, rather than guaranteed, based
    on the strength of the observations.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, upon observing hundreds or even thousands of round manhole covers,
    one can conclude that manhole covers are generally round. This is not guaranteed
    to be true, as there might be cities with different manhole cover shapes, but
    based on the evidence we have so far, we can make that probabilistic conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: Abductive Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abductive reasoning involves analyzing a set of observations and concluding
    with the most likely explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observation: The street is wet. There are water puddles on the sidewalk. People
    have umbrellas in their hands.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Explanation: It rained recently.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Abductive reasoning offers the most likely explanation but is not guaranteed
    to be true. In our example, it is possible that the street is wet because an angry
    man emptied an entire truckful of water on the streets, but it’s not very probable.
    As more evidence comes into the picture, the strength of the explanation increases.
  prefs: []
  type: TYPE_NORMAL
- en: Common Sense Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Common sense reasoning refers to utilizes a shared understanding of the world
    to make assumptions about the physical world or human relationships. Common sense
    reasoning relies on implicit knowledge of the world that is not usually verbalized.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: She saw him prancing around the hall with a glass in his hand, held upside down.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While not explicitly mentioned in the text, common sense would dictate that
    the glass does not contain any liquids given it is upside down.
  prefs: []
  type: TYPE_NORMAL
- en: Other forms of reasoning include mathematical (usually based on deductions),
    causal (identifying cause-and-effect relationships), analogical (drawing comparisons
    between two things or concepts), and moral (evaluating situations and decisions
    based on moral principles and values).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Cheng et al.](https://oreil.ly/vkTjt) show that LLMs perform much better on
    inductive reasoning than deductive reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: Inducing Reasoning in LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest way of improving reasoning in LLMs is to use prompting techniques
    like chain-of-thought, introduced in [Chapter 1](ch01.html#chapter_llm-introduction).
    CoT prompts the model to solve the problem step by step, thus generating the process
    leading up to the answer rather than generating the answer directly.
  prefs: []
  type: TYPE_NORMAL
- en: Verifiers for Improving Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, LLMs may not be all that great at producing the right answer to a question
    that requires multistep reasoning. But all hope is not lost. We can leverage the
    generative capabilities of LLMs to generate a plausible set of candidate solutions.
    These candidates can then be assessed by a verifier, which can identify the correct
    answer. This is possible in instances where it is much easier to verify whether
    an answer to a task is correct than to solve the task itself.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Just because LLMs can generate plausible candidate solutions for a question
    is not evidence of their reasoning abilities. For many types of questions, there
    are a very limited set of plausible solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Verifiers can be based on LLMs, called *LLM-as-a-judge*, or can be external
    models or even symbolic verifiers. Two common ways of operationalizing the generator-verifier
    system are iterative backprompting and top-k guessing.
  prefs: []
  type: TYPE_NORMAL
- en: Iterative backprompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this process, an LLM generates a proposed solution to a given problem that
    requires reasoning. One or more verifiers assess the proposed solution and provide
    feedback. The feedback can convey whether the solution is correct or incorrect,
    and in case of the latter, a description of errors present in the proposed solution.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM takes the feedback as input and generates the solution again, which
    is again passed to the verifier. The loop continues until the LLM generates the
    correct answer or the maximum number of iterations is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Top-k guessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this technique, k solutions are generated for a given task, and the verifier
    assesses them and chooses the correct solution if it exists. A relatively high
    temperature (>1) is used during decoding to generate a diverse set of solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Kambhampati et al.](https://oreil.ly/4_MxJ) show that top-k guessing exhibits
    similar performance levels as iterative backprompting.'
  prefs: []
  type: TYPE_NORMAL
- en: Inference-Time Computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This might well be the most significant topic of 2025 and beyond. As of this
    book’s writing, scaling up pre-training seems to be providing diminishing returns.
    Therefore, there is a hunt for new scaling dimensions. The most promising among
    them is scaling up inference-time compute. The premise is simple. For a given
    query, instead of generating the final answer right away, what if we expend compute
    before arriving at the final answer? Can we improve the performance of the model
    with more compute? Turns out, we can! Let’s discuss this new scaling avenue in
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: Repeated sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most simple and common inference-time compute technique is repeated sampling.
    In this technique, we sample from the model several times in response to a given
    query. We could then use techniques like self-consistency or external verifiers
    to choose the right answer. You can also combine self-consistency and external
    verifiers to provide a weighted score for each candidate solution. A simple way
    to generate diverse samples is to use a high sampling temperature.
  prefs: []
  type: TYPE_NORMAL
- en: Another simple approach is to use iterative generation, as shown earlier in
    this chapter. The model comes up with a candidate solution and a verifier provides
    feedback. The model iteratively improves its response using the verifier feedback
    until it reaches the final answer or the maximum number of iterations. Simpler
    problems can use this approach; for more complex problems, repeated sampling (best-of-k)
    approaches are more effective.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another approach is to augment the context across which the generation takes
    place. CoT prompting is the easiest way to achieve that. Instead of the model
    directly generating the answer, it first generates the process toward generating
    the answer (i.e., the thought process).
  prefs: []
  type: TYPE_NORMAL
- en: In essence, a language model generates a probability distribution P(Y | X) where
    X is the input context and the previously generated tokens. The goal is to modify
    X to maximize the probability of Y being the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[Jin et al.](https://oreil.ly/Dc_Fc) show some important experiments on this.
    First, the length of the reasoning steps matters to the performance. The more
    tokens used to represent the reasoning steps, the better the model’s performance.
    Conversely, they show that shortening the reasoning information even while keeping
    all the details intact negatively impacts the model’s reasoning capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Jin et al. also show that errors in the reasoning steps do not impact the performance
    as much, as long as the length of the reasoning steps exceeds a threshold.
  prefs: []
  type: TYPE_NORMAL
- en: For simpler tasks, shorter reasoning steps suffice, but for more complex tasks,
    increasing the length of the reasoning steps is very beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Jin et al. also show that replacing the CoT prompt “Let’s think step by step”
    with “Let’s think step by step, you must think more steps.” improves performance.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging inference-time compute, you can squeeze more performance from smaller
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to utilize inference-time compute is to search through the generation
    probability distribution. The idea is that for every token being generated, there
    is a probability assigned to every token in the vocabulary, so if we search properly,
    we might end up traversing the probability space and choosing the right path to
    generate the correct answer. Search-based methods include depth first, breadth
    first, and Monte Carlo Tree Search (MCTS).
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning for Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way to induce better reasoning in a model is to fine-tune it with a reasoning
    dataset. Let’s explore the [reasoning-0.01](https://oreil.ly/W1JRq) dataset released
    by SkunkworksAI to see what a typical reasoning dataset looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input:* If a die is rolled three times, what is the probability of getting
    a sum of 11?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Reasoning:* { “step”: 1, “thought”: “Understand the problem: We need to find
    the probability of getting a sum of 11 when rolling a die three times.” },'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{ “step”: 2, “thought”: “Calculate total possible outcomes: A die has 6 faces,
    so for each roll, there are 6 possibilities. For three rolls, the total possible
    outcomes are 6^3 = 216.” },'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{ “step”: 3, “thought”: “Identify favorable outcomes: List all combinations
    of rolls that result in a sum of 11\. There are 18 such combinations.” },'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{ “step”: 4, “thought”: “Calculate probability: Divide the number of favorable
    outcomes by the total possible outcomes: 18 / 216 = 1/12.” },'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{ “step”: 5, “thought”: “Conclusion: The probability of getting a sum of 11
    when rolling a die three times is 1/12.” } ]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Output:* 1/12'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The dataset contains step-by-step reasoning chains for a large variety of tasks.
    Such a dataset can be generated synthetically using larger models, followed by
    a human verification and annotation stage to verify and correct reasoning chains.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we defined alignment training and why we need it. We ventured
    into techniques for alignment training such as reinforcement learning. We also
    learned about hallucinations and different techniques to mitigate them. Finally,
    we examined reasoning limitations of LLMs and new techniques like scaling up inference-time
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll discuss techniques for speeding up LLM inference.
    High computation costs are a significant barrier to LLM adoption, and thus a plethora
    of techniques have been developed to improve inference speeds.
  prefs: []
  type: TYPE_NORMAL
