- en: Chapter 8\. Alignment Training and Reasoning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some common reasons for hesitancy in adopting LLMs is the presence of hallucinations,
    the limitations in reasoning skills, and bias and safety issues. In this chapter,
    we will go through these limitations and introduce different techniques to mitigate
    them. First, we will introduce the concept of alignment training, which helps
    us steer our models toward desirable outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Alignment Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We keep hearing about the *alignment problem* facing language models. What does
    this mean in practice? Ideally we would like a language model that we can fully
    understand, control, and steer. However, current language models are far from
    this ideal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the goal of alignment is to make language models more controllable and
    steerable. [Askell et al.](https://oreil.ly/fRCkD) from Anthropic define an aligned
    AI as one that is “helpful, honest, and harmless.” They further define the three
    H’s as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Helpful
  prefs: []
  type: TYPE_NORMAL
- en: As long as a user request isn’t harmful, the AI should attempt to solve the
    request as effectively as possible, asking follow-up questions if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Honest
  prefs: []
  type: TYPE_NORMAL
- en: The AI should provide accurate information and should be calibrated, providing
    reasonably accurate uncertainty estimates. It should understand its shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Harmless
  prefs: []
  type: TYPE_NORMAL
- en: The AI should not be offensive or discriminatory and should refuse to perform
    tasks that can cause harm to individuals or society.
  prefs: []
  type: TYPE_NORMAL
- en: These are lofty principles. Can LLMs meet them? The field of alignment training
    comprises techniques that can be used to steer LLMs closer to following these
    principles.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Can defining our desired values and principles in the prompt and asking the
    LLM to follow these principles result in a more aligned model? While it might
    be tempting to just ask the LLM to be a “good boy,” in practice this hasn’t seen
    all that much success.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since prompting LLMs to be nice doesn’t work, we will need to tune the model
    in some way. Supervised fine-tuning (discussed in [Chapter 6](ch06.html#llm-fine-tuning))
    on alignment datasets is an option. However, techniques like reinforcement learning
    have seen more success, which we will describe next in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The values and principles we need the LLM to adhere to are defined by humans,
    and they involve a level of subjectivity. Thus, it makes sense to optimize the
    model directly on human feedback. The class of techniques to make this happen
    is called reinforcement learning from human feedback (RLHF).
  prefs: []
  type: TYPE_NORMAL
- en: In traditional reinforcement learning, an agent interacts with its environment
    and performs actions to accomplish a task, using trial and error. After an action
    or a sequence of actions, the agent can receive a reward if it is on the right
    track, with the objective of the agent being to maximize the reward. This is specified
    through a reward function. However, in many real-world applications, defining
    success, and consequently the reward function, is hard.
  prefs: []
  type: TYPE_NORMAL
- en: In RLHF, the feedback is provided by a human-in-the-loop in an iterative fashion.
    To integrate human preferences into the LLM, a *reward model* needs to be trained.
    Various forms of feedback can be provided by human reviewers.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Human Feedback
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Human feedback can be provided through one of these forms:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary feedback
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, the feedback is provided as either yes/no (accept/reject).
  prefs: []
  type: TYPE_NORMAL
- en: Binary comparisons
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, the human evaluates outputs A and B and specifies their preference
    among the two.
  prefs: []
  type: TYPE_NORMAL
- en: Ranking
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, the human evaluates a set of outputs and provides a rank ordering
    of preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Corrective feedback
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, the human explicitly states what should have been the ideal
    output, potentially in natural language.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s describe a popular RLHF setup, pioneered by OpenAI. The alignment training
    consists of three distinct phases:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Supervised fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, the pre-trained model is fine-tuned on a supervised dataset
    of human preferences. To achieve this, we first need to create a prompt dataset
    consisting of a diverse set of potential user requests to a language model. Human
    annotators then provide desired responses to these prompts. The prompts and human-annotated
    responses then constitute the fine-tuning dataset, which the pre-trained model
    is then trained on. This is typically a very large undertaking, with companies
    like OpenAI and Meta spending significant resources on gathering annotations.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Reward modeling
  prefs: []
  type: TYPE_NORMAL
- en: In this step, a diverse set of prompts is queried to the language model and
    multiple generations (responses) are extracted for each prompt. Human annotators
    then review the generations and provide feedback, either by providing a rank-ordered
    preference of generations or choosing the best generation. The generations along
    with the preference data are used to train a reward model. The reward model is
    trained to predict which output a human would prefer among a list of candidate
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Proximal policy optimization (PPO)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the reward model is used to optimize the pre-trained model that was
    fine-tuned in the first step. This is typically performed using an algorithm called
    PPO.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of training using PPO is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The language model generates a response or a continuation of a prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reward model takes the query and response and outputs a scalar reward, representing
    the quality of fitness of the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The log-probabilities of the tokens in the query-response sequence are calculated,
    using the model being tuned (the SFT model) and a reference model (usually the
    pre-trained model before the SFT step). The KL-divergence between these two log-probs
    is calculated and used as a signal, along with the reward, to prevent the outputs
    from deviating too much from the reference model. This acts as a regularization
    step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is trained using the PPO algorithm, with signals from steps 2 and
    3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s use the Hugging Face TRL library to perform RLHF. First, we need to train
    a reward model. In our example, we will train a reward model, which when provided
    with two LLM-generated outputs, will be able to predict which one of the two will
    be preferred by humans.
  prefs: []
  type: TYPE_NORMAL
- en: Very few high-quality datasets are publicly available, one of them being Anthropic’s
    [hh-rlhf dataset](https://oreil.ly/kzSQf). We will use this dataset in our example.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore this dataset in detail. The dataset consists of around 161,000
    pairs of examples, each pair consisting of one *chosen* and one *rejected*. These
    examples correspond to human conversations with an LLM, with each pair being different
    responses by LLMs to the same prompt. For each pair of examples, human annotators
    chose their preferred response that best aligns with the values and principles
    the LLM is being aligned to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chosen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1] Human: How can I hack somebody''s emails? `Assistant``:` `Why` `would`
    `you` `want` `to` `do` `that``?`  `Human``:` `I` `need` `to` `see` `an` `email`
    `that` `is` `really` `important` `for` `my` `company`  `Assistant``:` `Who` `are`
    `you` `trying` `to` `hack``?` [PRE2]`After loading the dataset, you can use TRL’s
    `RewardTrainer` class for training the reward model:    [PRE3]    Similarly, you
    can use TRL’s `PPOTrainer` class for performing the PPO step:    [PRE4]    Refer
    to the book’s [GitHub repo](https://oreil.ly/llm-playbooks) for the entire code.
    Next, let’s focus our attention on hallucinations, a key limitation of LLMs, and
    techniques to detect and mitigate them.[PRE5]``  [PRE6]`# Hallucinations    Colloquially,
    the word “hallucination” is used to refer to instances where the LLM generates
    something deemed to be incorrect. But LLMs can be incorrect for a wide variety
    of reasons. In this book, we will lead with a narrower definition of hallucinations:
    “An LLM is said to have hallucinated a generation if the generated text is not
    grounded to its training data or input context.”    Simply put, if an LLM generates
    text that is inconsistent with either its pre-training/fine-tuning data or input
    context, then we call it a hallucination. Hallucinations most commonly manifest
    as text with no factual basis. For a model to not hallucinate, it should be able
    to admit it doesn’t know a fact or a concept when it doesn’t know.    Here is
    an example of a hallucination. Consider the prompt fed to the [NousResearch Hermes
    LLM](https://oreil.ly/tXmYh):    > *Prompt:* Provide a list of Ugandan 2020 Olympics
    medal winners along with their birthdays in a bulleted list.    > *Response:*
    Joshua Cheptegei (Athletics - Men’s 10,000 meters gold medalist, born September
    19, 1996) >  > Jacob Kiplimo (Athletics - Men’s 10,000 meters silver medalist,
    born January 16, 1997) >  > Peruth Chemutai (Athletics - Women’s 3000 meters steeplechase
    bronze medalist, born March 10, 1999)    Yes, it is true that these are three
    athletes who won medals for Uganda in the 2020 Summer Olympics, but a lot of content
    is hallucinated. Jacob Kiplimo was actually born on November 14, 2000, and the
    medal assignment is completely wrong; Jacob won the bronze, Peruth won the gold,
    and Joshua won the silver but also won a gold in another event.    A model that
    doesn’t hallucinate would have provided factual information and admit that it
    doesn’t know specific details.    ###### Warning    Fine-tuning data on new knowledge
    can increase models’ tendency to hallucinate. [Gekhman et al.](https://oreil.ly/kgu26)
    show that during fine-tuning, LLMs learn new knowledge in the fine-tuning data
    much slower than knowledge that was already present in the pre-training data.
    They also show that when the LLM learns new knowledge, it leads to overfitting,
    causing an increase in hallucinations even for unrelated questions. If you want
    to teach your model entirely new knowledge, I suggest using the continued pre-training
    setup with techniques like replay, etc., described in [Chapter 7](ch07.html#ch07).    #
    Mitigating Hallucinations    One of the biggest sources of hesitancy in adopting
    LLM-based tools and software is the system’s trustworthiness or lack thereof.
    Trustworthiness is most affected by the presence of hallucinations. Therefore,
    there is considerable research into preventing or reducing the tendency of models
    to hallucinate. Let’s explore some common techniques.    At a product design level,
    you can reduce hallucination risk by simply not asking LLMs questions that you
    know it wouldn’t be able to answer. This is not always possible, especially when
    you allow your users to directly interact with the model. It is also not easy
    to determine what an LLM knows and does not know.    [Figure 8-1](#knowledge-quadrant)
    depicts a knowledge quadrant across knowledge and awareness dimensions. Ideally,
    an LLM should acknowledge its lack of knowledge when asked about a fact or concept
    it genuinely does not know. In [Figure 8-1](#knowledge-quadrant), we see that
    there can be four types of knowledge:    Known knowns      The LLM knows this
    knowledge/skill and is able to utilize it.      Unknown knowns      The LLM knows
    this knowledge/skill but is not able to utilize it effectively (can be unlocked
    by fine-tuning or in-context learning).      Known unknowns      The LLM knows
    that it does not know this knowledge.      Unknown unknowns      The LLM does
    not know that it does not know this knowledge, leading to hallucinations.    ![knowledge-quadrant](assets/dllm_0801.png)  ######
    Figure 8-1\. Knowledge quadrant    To determine the level of self-knowledge a
    model possesses, [Yin et al.](https://oreil.ly/3DxdZ), created a dataset called
    SelfAware composed of answerable and unanswerable questions. Self-knowledge refers
    to the knowledge an LLM possesses about whether it knows a fact or concept or
    not. In their experiments, they show that larger models possess more self-knowledge.
    They also show that instruction-tuned models possess more self-knowledge than
    base models.    An important way to assess a model’s self-knowledge is through
    its output uncertainty. If a model is less confident about its predictions, as
    measured through its output probabilities, we can assume a higher hallucination
    risk. For this approach to be valid, the model has to be well calibrated. As [Chapter 6](ch06.html#llm-fine-tuning)
    introduced, a model is well calibrated if there is a correlation between its output
    probability values and task accuracy.    ###### Warning    [Kadavath et al.](https://oreil.ly/VVY-i)
    show that techniques like RLHF reduce model calibration.    A key technique to
    address hallucinations is grounding the model to factual data sources. This is
    done by retrieving knowledge from a data store specific to the given task and
    feeding it to the model in the prompt along with the task instruction and input.
    This paradigm is called RAG, which we will discuss in [Part III](part03.html#part3)
    of the book.    RAG is not a panacea for the hallucination problem for the following
    reasons:    *   Feeding ground-truth factual data in the prompt reduces hallucinations
    but does not eliminate them completely, especially when the context is large.           *   Using
    RAG shifts the bottleneck toward the retrieval process. If the retrieval process
    is not able to return the relevant data needed, the model may do worse at the
    task than if no RAG was used.           *   In many cases, we do not have access
    to the ground-truth data; hence we cannot feed it as input context.              Now
    let’s look at techniques that do not depend on us fetching ground-truth data.    ##
    Self-Consistency    We can use self-consistency, which we first introduced in
    [Chapter 5](ch05.html#chapter_utilizing_llms), to detect the possibility of hallucinations.
    The idea is simple: we generate the output multiple times and detect the inconsistencies
    between the different generations. The more they are inconsistent, the less confident
    the model is about the answer, and the more likely the hallucination.    ## Chain-of-Actions    Self-verification
    is another commonly used technique. An example of this is Chain-of-Verification
    (CoVe), a prompting technique introduced by researchers from [Meta](https://oreil.ly/ubp1t).
    An example implementation of CoVe works as follows:    1.  The model first generates
    a response to the given prompt.           2.  Given the prompt and the response,
    the model generates verification questions to check the veracity of the response.           3.  Each
    of the verification questions is fed to the model separately.           4.  The
    responses to these verification questions are then compared to the original response,
    and if there are any inconsistencies, the original response is edited to integrate
    the responses from the verification questions.              As an example, for
    the earlier query:    > *Prompt:* Provide a list of Ugandan Olympic medal winners
    along with their birthdays in a bulleted list.    When asked to generate a list
    of verification questions, the model generates:    > Who won the gold medal in
    the men’s 10,000m event in the 2020 Tokyo Olympics >  > How many Ugandan athletes
    won medals at the 2020 Tokyo Olympics? >  > What event did Jacob Kiplimo compete
    in and win a medal for Uganda at the 2020 Tokyo Olympics? >  > In which event
    did Peruth Chemutai win a medal for Uganda at the 2020 Tokyo Olympics? >  > What
    year were Joshua Cheptegei and Jacob Kiplimo born? >  > What year was Peruth Chemutai
    born? >  > How many gold medals did Uganda win in total at the 2020 Tokyo Olympics?    Asking
    the same model the following question:    > Who won the gold medal in the men’s
    10,000m event in the 2020 Tokyo Olympics?    We get the response:    > In the
    2020 Tokyo Olympics, the men’s 10,000m event was won by Selemon Barega of Ethiopia.
    He finished with a time of 27:43.22, beating the silver medalist Joshua Cheptegei
    of Uganda by 0.32 seconds.    This response is inconsistent with the original
    response, which claimed that Joshua Cheptegai won the gold medal! The response
    to the verification question is in fact the correct response.    Therefore, we
    can see how methods like CoVe can be valuable in reducing hallucination risk.
    Note that it is possible that the responses to the verification questions are
    also hallucinated, so this method does not entirely address the hallucination
    issue. However, one can expect the responses to the verification questions to
    be less affected by hallucinations because they are more direct questions.    Let’s
    now discuss hallucination reduction using recitation.    ## Recitation    With
    the recitation technique, we prompt the LLM to generate one or more passages about
    the given question and then generate the answer based on the passages generated.
    The reasoning behind this approach is that directly answering questions diverges
    from the learning objectives on which the language model was pre-trained. Recitation
    serves as the intermediate step that aligns more closely to the original learning
    objective of the model, like next-token prediction.    We can use few-shot prompting
    for soliciting recitations. The prompt looks like:    > Query: <query> >  > Recitation1:
    <recitation> >  > Recitation2: <recitation> >  > … >  > RecitationN: <recitation>
    >  > Query: <query> >  > Recitation1:    We can generate a single recitation or
    multiple recitations. If we generate multiple recitations, we can generate a candidate
    response using each of them and then use self-consistency to pick the final answer.
    You can also fine-tune your model to prime it to be better at generating effective
    recitations.    The recitation method typically consumes fewer tokens than chain-of-actions,
    but I find the latter to be more effective.    ## Sampling Methods for Addressing
    Hallucination    The degree of hallucination also depends on the decoding method
    used. Recall our discussion on decoding algorithms in [Chapter 5](ch05.html#chapter_utilizing_llms).
    [Lee et al.](https://oreil.ly/ZTCtv) show that top-p sampling leads to more hallucinations
    compared to greedy decoding. This is to be expected as the sampling step leads
    to more randomness, sometimes leading to the wrong token being picked.    One
    way to address increased hallucination risk due to sampling algorithms is to use
    a technique like factual-nucleus sampling, which [Lee et al. introduced](https://oreil.ly/D12xT).
    This technique is based on the observation that as the length of the generated
    sequence increases, there will be fewer valid candidate tokens for the next token
    generation. Thus, the randomness of the sampling algorithm is reduced as the length
    of the generated text increases, by reducing the p value in the top-p decoding
    algorithm.    The formula looks like this:  pt = max{ω, p × λ t−1 }  where *t*
    refers to the generation step.    There are three tunable parameters:    Decay
    rate (*λ*)      The *p* value of the algorithm is decayed by a decay rate at every
    step of the generation.      Reset (*p*)      The *p* value might decay very quickly,
    thus degenerating to a greedy algorithm. To prevent this, we can reset the *p*
    value at regular intervals, say after each sentence is generated.      Lower-bound
    (*ω*)      To continue maintaining the advantages of the top-p algorithm, we can
    prevent the *p* value from getting too low by enforcing a lower bound.      This
    method comes with a tradeoff; lowering the *p* value reduces hallucination risk
    but also decreases diversity of token generation, causing a loss in performance.    ##
    Decoding by Contrasting Layers    The principle behind [*d*ecoding by c*o*ntrasting
    *la*yers (DoLa)](https://oreil.ly/V4h3d) is that factual knowledge is encoded
    in the topmost layer of the Transformer, just like syntactic information is encoded
    in the lower layers. Therefore, we can emphasize the knowledge encoded in the
    higher layers to promote more factual outputs. DoLa achieves this by using a technique
    called *contrastive decoding*, in which the next token probability for each token
    is calculated by taking the difference in logits between a higher layer and a
    lower layer.    DoLa is available through Hugging Face. Let’s look at an example:    [PRE7]    The
    `dola_layers` argument should be used to activate DoLa decoding. `dola_layers`
    can be either a string or a list of integers. If it is a string, it should be
    either `''high''` or `''low''`. This means that the last layer is contrasted with
    the higher or the lower layers of the model. You can also specify a list of integers
    representing layer numbers. Again, the final layer of the model will be contrasted
    with the layers specified in your list.    To reduce repetitiveness induced by
    DoLa, you can set a repetition penalty through the `repetition_penalty` argument
    (this is set by default). The authors of DoLa suggest contrasting with higher
    layers for tasks with shorter answer lengths, and contrasting with lower layers
    otherwise. They also recommend not using DoLa for smaller LLMs. This is because
    the different layers in smaller models are not distinctive enough to take advantage
    of this approach.    # In-Context Hallucinations    So far, we have focused on
    hallucinations emanating from the model trying to generate from its parametric
    memory. We can also have in-context hallucinations, also called closed-book hallucinations,
    where the model depends on data provided in the context to influence its generation,
    like in RAG.    For example, consider the passage:    > At the 2020 Summer Olympics
    in Tokyo, Uganda won three medals: >  > Joshua Cheptegei – Gold in Men’s 5000m
    >  > Peruth Chemutai – Gold in Women’s 3000m Steeplechase >  > Jacob Kiplimo –
    Bronze in Men’s 10,000m >  > These medals marked a historic achievement for Uganda,
    with both Cheptegei and Chemutai winning gold in their respective events.    fed
    to the LLM as context for the following query:    > Provide a list of Ugandan
    2020 Olympics medal winners along with their birthdays in a bulleted list.    While
    the passage contains all the medal winners along with the events, their birthdays
    are not present, leading the model to hallucinate them. Here is the model’s response:    >
    Here is a list of Ugandan 2020 Olympics medal winners with their birthdays: >  >
    *   Joshua Cheptegei (born September 12, 1996) - Gold medal in Men’s 5000m >      >      >
    *   Peruth Chemutai (born August 10, 1999) - Gold medal in Women’s 3000m Steeplechase
    >      >      > *   Jacob Kiplimo (born December 16, 1997) - Bronze medal in Men’s
    10,000m    Due to the knowledge provided in the context, the model gets the names
    of the athletes, their medals, and their events correct, but gets the birthdays
    wrong for Peruth and Jacob.    How can we detect and mitigate these hallucinations?
    [Chuang et al.](https://oreil.ly/czTU_) propose Lookback Lens, a technique that
    utilizes attention maps to detect hallucinations. In this technique, we calculate
    a *lookback ratio*, defined as the ratio of the attention weights on the context
    tokens to the attention weights on the newly generated tokens. The lookback ratio
    is calculated at each attention head of each layer of the model. These ratios
    are used as features to train a linear classifier model.    The classifier model
    can also be employed to reduce hallucinations during generation. During generation,
    a few candidate phrases (sequence of tokens) are generated for the next step.
    The lookback ratios for these candidates are calculated and fed to the classifier
    model. The candidate assigned the lowest probability by the classifier can be
    chosen to be generated, as this is the least likely to be hallucinated.    Using
    a classifier-based decoding strategy can be a massive drag on system latency,
    however. These approaches should be used only if latency isn’t a prime consideration.    #
    Hallucinations Due to Irrelevant Information    The presence of irrelevant information
    in the context can also lead to hallucinations. As an example, consider this prompt:    >
    *Prompt:* Indianapolis, the capital of Indiana, is known for its vibrant cultural
    scene, including museums, theaters, and sporting events, especially the Indianapolis
    500 race. The city is also a major hub for commerce and industry, with a growing
    tech scene and numerous corporate headquarters. >  > Where is the actor Lily Gao
    born?    The response is as follows:    > *Response:* Lily Gao, an American actress
    known for her roles in various TV series and films, was born on October 1, 1994,
    in Indianapolis, Indiana, USA.    However, in reality Lily Gao was born in Canada!
    The presence of irrelevant information in the prompt causes the LLM to hallucinate.    To
    mitigate this issue, [Weston et al.](https://oreil.ly/aqsxN) propose a technique
    called System 2 Attention (S2A). In this technique, the LLM is first asked to
    regenerate its context, removing any information irrelevant to answering the question.
    Next, the LLM is prompted with the regenerated context and the final answer is
    generated.    As an example, consider this math problem with a distractor sentence:    >
    *Prompt:* Sarah has 5 apples. She buys 3 more apples from the store. Max sells
    3 apples to the store. How many apples does Sarah have now?    We issue the following
    prompt:    > *Prompt:* Regenerate the context removing any information that is
    irrelevant to answering the question.    The response is as follows:    > *Response:*
    Sarah has 5 apples. She buys 3 more apples from the store. How many apples does
    Sarah have now?    This can be fed back to the model to provide the correct answer.    ######
    Tip    You can also implement S2A in a single prompt, by asking the model to regenerate
    the context followed by the final answer. However, performing this in two prompts
    has shown to be more effective.    Next, let’s explore the reasoning capabilities
    of LLMs and showcase techniques for improving them.    # Reasoning    In [Chapter 1](ch01.html#chapter_llm-introduction),
    we discussed the limitations of language models and pointed to reasoning as one
    of the biggest limitations. In this section, let’s dive into it in more detail
    to understand what reasoning entails, how well language models perform reasoning,
    and how to improve their reasoning capabilities.    First, let’s define reasoning:    >
    Natural language reasoning is a process to integrate multiple knowledge (e.g.
    encyclopedic knowledge and commonsense knowledge) to derive some new conclusions
    about the (realistic or hypothetical) world. Knowledge can be from both explicit
    and implicit sources. Conclusions are assertions or events assumed to be true
    in the world, or practical actions. >  > [Yu et al.](https://oreil.ly/7NsBF)    Reasoning
    can be classified into several different types. Here are a few forms of non-mutually-exclusive
    reasoning categories:    ## Deductive Reasoning    Deductive reasoning uses logic
    to draw conclusions from one or more premises.    As an example, consider the
    following passage:    > Mr. Shockley was allergic to mushrooms. The dish “Golden
    Travesty” has mushrooms in it.    Based on this set of premises, we can deduce
    that Mr. Shockley should stay far away from the Golden Travesty dish.    ## Inductive
    Reasoning    Inductive reasoning involves making generalizations based on a set
    of observations. The generalizations are plausible and probabilistic, rather than
    guaranteed, based on the strength of the observations.    As an example, upon
    observing hundreds or even thousands of round manhole covers, one can conclude
    that manhole covers are generally round. This is not guaranteed to be true, as
    there might be cities with different manhole cover shapes, but based on the evidence
    we have so far, we can make that probabilistic conclusion.    ## Abductive Reasoning    Abductive
    reasoning involves analyzing a set of observations and concluding with the most
    likely explanation:    > Observation: The street is wet. There are water puddles
    on the sidewalk. People have umbrellas in their hands. >  > Explanation: It rained
    recently.    Abductive reasoning offers the most likely explanation but is not
    guaranteed to be true. In our example, it is possible that the street is wet because
    an angry man emptied an entire truckful of water on the streets, but it’s not
    very probable. As more evidence comes into the picture, the strength of the explanation
    increases.    ## Common Sense Reasoning    Common sense reasoning refers to utilizes
    a shared understanding of the world to make assumptions about the physical world
    or human relationships. Common sense reasoning relies on implicit knowledge of
    the world that is not usually verbalized. For example:    > She saw him prancing
    around the hall with a glass in his hand, held upside down.    While not explicitly
    mentioned in the text, common sense would dictate that the glass does not contain
    any liquids given it is upside down.    Other forms of reasoning include mathematical
    (usually based on deductions), causal (identifying cause-and-effect relationships),
    analogical (drawing comparisons between two things or concepts), and moral (evaluating
    situations and decisions based on moral principles and values).    ###### Tip    [Cheng
    et al.](https://oreil.ly/vkTjt) show that LLMs perform much better on inductive
    reasoning than deductive reasoning.    # Inducing Reasoning in LLMs    The simplest
    way of improving reasoning in LLMs is to use prompting techniques like chain-of-thought,
    introduced in [Chapter 1](ch01.html#chapter_llm-introduction). CoT prompts the
    model to solve the problem step by step, thus generating the process leading up
    to the answer rather than generating the answer directly.    ## Verifiers for
    Improving Reasoning    So, LLMs may not be all that great at producing the right
    answer to a question that requires multistep reasoning. But all hope is not lost.
    We can leverage the generative capabilities of LLMs to generate a plausible set
    of candidate solutions. These candidates can then be assessed by a verifier, which
    can identify the correct answer. This is possible in instances where it is much
    easier to verify whether an answer to a task is correct than to solve the task
    itself.    ###### Warning    Just because LLMs can generate plausible candidate
    solutions for a question is not evidence of their reasoning abilities. For many
    types of questions, there are a very limited set of plausible solutions.    Verifiers
    can be based on LLMs, called *LLM-as-a-judge*, or can be external models or even
    symbolic verifiers. Two common ways of operationalizing the generator-verifier
    system are iterative backprompting and top-k guessing.    ### Iterative backprompting    In
    this process, an LLM generates a proposed solution to a given problem that requires
    reasoning. One or more verifiers assess the proposed solution and provide feedback.
    The feedback can convey whether the solution is correct or incorrect, and in case
    of the latter, a description of errors present in the proposed solution.    The
    LLM takes the feedback as input and generates the solution again, which is again
    passed to the verifier. The loop continues until the LLM generates the correct
    answer or the maximum number of iterations is reached.    ### Top-k guessing    In
    this technique, k solutions are generated for a given task, and the verifier assesses
    them and chooses the correct solution if it exists. A relatively high temperature
    (>1) is used during decoding to generate a diverse set of solutions.    [Kambhampati
    et al.](https://oreil.ly/4_MxJ) show that top-k guessing exhibits similar performance
    levels as iterative backprompting.    ## Inference-Time Computation    This might
    well be the most significant topic of 2025 and beyond. As of this book’s writing,
    scaling up pre-training seems to be providing diminishing returns. Therefore,
    there is a hunt for new scaling dimensions. The most promising among them is scaling
    up inference-time compute. The premise is simple. For a given query, instead of
    generating the final answer right away, what if we expend compute before arriving
    at the final answer? Can we improve the performance of the model with more compute?
    Turns out, we can! Let’s discuss this new scaling avenue in detail.    ### Repeated
    sampling    The most simple and common inference-time compute technique is repeated
    sampling. In this technique, we sample from the model several times in response
    to a given query. We could then use techniques like self-consistency or external
    verifiers to choose the right answer. You can also combine self-consistency and
    external verifiers to provide a weighted score for each candidate solution. A
    simple way to generate diverse samples is to use a high sampling temperature.    Another
    simple approach is to use iterative generation, as shown earlier in this chapter.
    The model comes up with a candidate solution and a verifier provides feedback.
    The model iteratively improves its response using the verifier feedback until
    it reaches the final answer or the maximum number of iterations. Simpler problems
    can use this approach; for more complex problems, repeated sampling (best-of-k)
    approaches are more effective.    Yet another approach is to augment the context
    across which the generation takes place. CoT prompting is the easiest way to achieve
    that. Instead of the model directly generating the answer, it first generates
    the process toward generating the answer (i.e., the thought process).    In essence,
    a language model generates a probability distribution P(Y | X) where X is the
    input context and the previously generated tokens. The goal is to modify X to
    maximize the probability of Y being the correct answer.    [Jin et al.](https://oreil.ly/Dc_Fc)
    show some important experiments on this. First, the length of the reasoning steps
    matters to the performance. The more tokens used to represent the reasoning steps,
    the better the model’s performance. Conversely, they show that shortening the
    reasoning information even while keeping all the details intact negatively impacts
    the model’s reasoning capabilities.    Jin et al. also show that errors in the
    reasoning steps do not impact the performance as much, as long as the length of
    the reasoning steps exceeds a threshold.    For simpler tasks, shorter reasoning
    steps suffice, but for more complex tasks, increasing the length of the reasoning
    steps is very beneficial.    ###### Tip    Jin et al. also show that replacing
    the CoT prompt “Let’s think step by step” with “Let’s think step by step, you
    must think more steps.” improves performance.    Leveraging inference-time compute,
    you can squeeze more performance from smaller LLMs.    ### Search    Another way
    to utilize inference-time compute is to search through the generation probability
    distribution. The idea is that for every token being generated, there is a probability
    assigned to every token in the vocabulary, so if we search properly, we might
    end up traversing the probability space and choosing the right path to generate
    the correct answer. Search-based methods include depth first, breadth first, and
    Monte Carlo Tree Search (MCTS).    ## Fine-Tuning for Reasoning    One way to
    induce better reasoning in a model is to fine-tune it with a reasoning dataset.
    Let’s explore the [reasoning-0.01](https://oreil.ly/W1JRq) dataset released by
    SkunkworksAI to see what a typical reasoning dataset looks like:    > *Input:*
    If a die is rolled three times, what is the probability of getting a sum of 11?    >
    *Reasoning:* { “step”: 1, “thought”: “Understand the problem: We need to find
    the probability of getting a sum of 11 when rolling a die three times.” }, >  >
    { “step”: 2, “thought”: “Calculate total possible outcomes: A die has 6 faces,
    so for each roll, there are 6 possibilities. For three rolls, the total possible
    outcomes are 6^3 = 216.” }, >  > { “step”: 3, “thought”: “Identify favorable outcomes:
    List all combinations of rolls that result in a sum of 11\. There are 18 such
    combinations.” }, >  > { “step”: 4, “thought”: “Calculate probability: Divide
    the number of favorable outcomes by the total possible outcomes: 18 / 216 = 1/12.”
    }, >  > { “step”: 5, “thought”: “Conclusion: The probability of getting a sum
    of 11 when rolling a die three times is 1/12.” } ] >  > *Output:* 1/12    The
    dataset contains step-by-step reasoning chains for a large variety of tasks. Such
    a dataset can be generated synthetically using larger models, followed by a human
    verification and annotation stage to verify and correct reasoning chains.    #
    Summary    In this chapter, we defined alignment training and why we need it.
    We ventured into techniques for alignment training such as reinforcement learning.
    We also learned about hallucinations and different techniques to mitigate them.
    Finally, we examined reasoning limitations of LLMs and new techniques like scaling
    up inference-time computation.    In the next chapter, we’ll discuss techniques
    for speeding up LLM inference. High computation costs are a significant barrier
    to LLM adoption, and thus a plethora of techniques have been developed to improve
    inference speeds.[PRE8]``'
  prefs: []
  type: TYPE_NORMAL
