- en: Chapter 8\. Alignment Training and Reasoning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章．对齐训练和推理
- en: Some common reasons for hesitancy in adopting LLMs is the presence of hallucinations,
    the limitations in reasoning skills, and bias and safety issues. In this chapter,
    we will go through these limitations and introduce different techniques to mitigate
    them. First, we will introduce the concept of alignment training, which helps
    us steer our models toward desirable outcomes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 采用 LLM 时犹豫不决的一些常见原因是幻觉的存在、推理技能的限制以及偏见和安全问题。在本章中，我们将探讨这些限制并介绍不同的技术来减轻它们。首先，我们将介绍对齐训练的概念，这有助于我们将模型引导到期望的结果。
- en: Defining Alignment Training
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义对齐训练
- en: We keep hearing about the *alignment problem* facing language models. What does
    this mean in practice? Ideally we would like a language model that we can fully
    understand, control, and steer. However, current language models are far from
    this ideal.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不断听到语言模型面临的 *对齐问题*。这在实践中意味着什么？理想情况下，我们希望有一个可以完全理解、控制和引导的语言模型。然而，当前的语言模型远远没有达到这个理想状态。
- en: 'Thus, the goal of alignment is to make language models more controllable and
    steerable. [Askell et al.](https://oreil.ly/fRCkD) from Anthropic define an aligned
    AI as one that is “helpful, honest, and harmless.” They further define the three
    H’s as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对齐的目标是使语言模型更加可控和可引导。Anthropic 的 Askell 等人将对齐的 AI 定义为“有益、诚实和无害”的 AI。他们进一步定义了三个
    H 如下：
- en: Helpful
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有益
- en: As long as a user request isn’t harmful, the AI should attempt to solve the
    request as effectively as possible, asking follow-up questions if needed.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 只要用户请求无害，AI 应该尽可能有效地解决问题，如果需要的话，可以提出后续问题。
- en: Honest
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 诚实
- en: The AI should provide accurate information and should be calibrated, providing
    reasonably accurate uncertainty estimates. It should understand its shortcomings.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: AI 应该提供准确的信息，并应校准，提供合理的准确不确定性估计。它应该了解自己的不足。
- en: Harmless
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无害
- en: The AI should not be offensive or discriminatory and should refuse to perform
    tasks that can cause harm to individuals or society.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: AI 不应具有攻击性或歧视性，并且应拒绝执行可能对个人或社会造成伤害的任务。
- en: These are lofty principles. Can LLMs meet them? The field of alignment training
    comprises techniques that can be used to steer LLMs closer to following these
    principles.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是崇高的原则。LLM 能否满足这些原则？对齐训练领域包含可以用来将 LLM 引导得更接近遵循这些原则的技术。
- en: Note
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Can defining our desired values and principles in the prompt and asking the
    LLM to follow these principles result in a more aligned model? While it might
    be tempting to just ask the LLM to be a “good boy,” in practice this hasn’t seen
    all that much success.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中定义我们期望的价值观和原则，并要求 LLM 遵循这些原则，能否导致更对齐的模型？虽然可能很诱人只是要求 LLM 成为一个“好孩子”，但在实践中这并没有取得太多成功。
- en: Reinforcement Learning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Since prompting LLMs to be nice doesn’t work, we will need to tune the model
    in some way. Supervised fine-tuning (discussed in [Chapter 6](ch06.html#llm-fine-tuning))
    on alignment datasets is an option. However, techniques like reinforcement learning
    have seen more success, which we will describe next in this section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于仅仅提示 LLM 友好并不奏效，我们需要以某种方式调整模型。在 [第 6 章](ch06.html#llm-fine-tuning) 中讨论的监督微调是对齐数据集的一个选项。然而，强化学习等技术取得了更多的成功，我们将在本节接下来的内容中描述。
- en: The values and principles we need the LLM to adhere to are defined by humans,
    and they involve a level of subjectivity. Thus, it makes sense to optimize the
    model directly on human feedback. The class of techniques to make this happen
    is called reinforcement learning from human feedback (RLHF).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要 LLM 遵守的价值观和原则是由人类定义的，并且涉及一定程度的主观性。因此，直接在人类反馈上优化模型是有意义的。实现这一目标的技巧类别被称为基于人类反馈的强化学习（RLHF）。
- en: In traditional reinforcement learning, an agent interacts with its environment
    and performs actions to accomplish a task, using trial and error. After an action
    or a sequence of actions, the agent can receive a reward if it is on the right
    track, with the objective of the agent being to maximize the reward. This is specified
    through a reward function. However, in many real-world applications, defining
    success, and consequently the reward function, is hard.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的强化学习中，智能体与其环境互动，执行动作以完成任务，使用试错法。在执行动作或一系列动作后，如果智能体处于正确的轨道上，它可以获得奖励。智能体的目标是最大化奖励。这是通过奖励函数来指定的。然而，在许多实际应用中，定义成功以及随之而来的奖励函数是困难的。
- en: In RLHF, the feedback is provided by a human-in-the-loop in an iterative fashion.
    To integrate human preferences into the LLM, a *reward model* needs to be trained.
    Various forms of feedback can be provided by human reviewers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RLHF 中，反馈以迭代的方式由人类在回路中提供。为了将人类偏好整合到 LLM 中，需要训练一个 *奖励模型*。人类审稿人可以提供各种形式的反馈。
- en: Types of Human Feedback
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类反馈的类型
- en: 'Human feedback can be provided through one of these forms:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人类反馈可以通过以下形式之一提供：
- en: Binary feedback
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 二元反馈
- en: In this setting, the feedback is provided as either yes/no (accept/reject).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，反馈以是/否（接受/拒绝）的形式提供。
- en: Binary comparisons
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 二元比较
- en: In this setting, the human evaluates outputs A and B and specifies their preference
    among the two.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，人类评估输出 A 和 B，并指定它们之间的偏好。
- en: Ranking
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 排序
- en: In this setting, the human evaluates a set of outputs and provides a rank ordering
    of preferences.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，人类评估一组输出并提供偏好排序。
- en: Corrective feedback
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 纠正反馈
- en: In this setting, the human explicitly states what should have been the ideal
    output, potentially in natural language.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，人类明确地指出了本应的理想输出，这可能是自然语言的形式。
- en: RLHF Example
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLHF 示例
- en: 'Let’s describe a popular RLHF setup, pioneered by OpenAI. The alignment training
    consists of three distinct phases:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一个由 OpenAI 领先的流行 RLHF 设置。对齐训练包括三个不同的阶段：
- en: 1\. Supervised fine-tuning
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 监督微调
- en: In the first step, the pre-trained model is fine-tuned on a supervised dataset
    of human preferences. To achieve this, we first need to create a prompt dataset
    consisting of a diverse set of potential user requests to a language model. Human
    annotators then provide desired responses to these prompts. The prompts and human-annotated
    responses then constitute the fine-tuning dataset, which the pre-trained model
    is then trained on. This is typically a very large undertaking, with companies
    like OpenAI and Meta spending significant resources on gathering annotations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，预训练模型在人类偏好监督数据集上进行微调。为了实现这一点，我们首先需要创建一个提示数据集，其中包含一系列潜在的用户请求语言模型。然后，人类标注员为这些提示提供期望的响应。提示和人类标注的响应构成了微调数据集，然后预训练模型在此基础上进行训练。这通常是一项非常大的工作，像
    OpenAI 和 Meta 这样的公司投入了大量资源来收集标注。
- en: 2\. Reward modeling
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 奖励建模
- en: In this step, a diverse set of prompts is queried to the language model and
    multiple generations (responses) are extracted for each prompt. Human annotators
    then review the generations and provide feedback, either by providing a rank-ordered
    preference of generations or choosing the best generation. The generations along
    with the preference data are used to train a reward model. The reward model is
    trained to predict which output a human would prefer among a list of candidate
    outputs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，向语言模型查询一组提示，并为每个提示提取多个生成（响应）。然后，人类标注员审查这些生成并提供反馈，要么提供生成的排序偏好，要么选择最佳生成。这些生成以及偏好数据用于训练奖励模型。奖励模型被训练来预测人类在一系列候选输出中更喜欢哪个输出。
- en: 3\. Proximal policy optimization (PPO)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 近端策略优化（PPO）
- en: Finally, the reward model is used to optimize the pre-trained model that was
    fine-tuned in the first step. This is typically performed using an algorithm called
    PPO.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用称为 PPO 的算法使用奖励模型优化在第一步中微调的预训练模型。这通常是通过 PPO 算法执行的。
- en: 'The process of training using PPO is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PPO 训练的过程如下：
- en: The language model generates a response or a continuation of a prompt.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语言模型生成一个响应或提示的延续。
- en: The reward model takes the query and response and outputs a scalar reward, representing
    the quality of fitness of the input.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励模型接收查询和响应，并输出一个标量奖励，表示输入的质量和适应性。
- en: The log-probabilities of the tokens in the query-response sequence are calculated,
    using the model being tuned (the SFT model) and a reference model (usually the
    pre-trained model before the SFT step). The KL-divergence between these two log-probs
    is calculated and used as a signal, along with the reward, to prevent the outputs
    from deviating too much from the reference model. This acts as a regularization
    step.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正在调整的模型（SFT 模型）和参考模型（通常是在 SFT 步骤之前的预训练模型）计算查询-响应序列中标记的对数概率。计算这两个对数概率之间的 KL
    散度，并将其用作信号，与奖励一起使用，以防止输出偏离参考模型太远。这充当正则化步骤。
- en: The model is trained using the PPO algorithm, with signals from steps 2 and
    3.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型使用 PPO 算法进行训练，使用步骤 2 和 3 的信号。
- en: Let’s use the Hugging Face TRL library to perform RLHF. First, we need to train
    a reward model. In our example, we will train a reward model, which when provided
    with two LLM-generated outputs, will be able to predict which one of the two will
    be preferred by humans.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Hugging Face TRL库来执行RLHF。首先，我们需要训练一个奖励模型。在我们的示例中，我们将训练一个奖励模型，当提供两个LLM生成的输出时，它将能够预测人类更倾向于哪一个。
- en: Very few high-quality datasets are publicly available, one of them being Anthropic’s
    [hh-rlhf dataset](https://oreil.ly/kzSQf). We will use this dataset in our example.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 公开可用的高质量数据集非常少，其中之一是Anthropic的[hh-rlhf数据集](https://oreil.ly/kzSQf)。在我们的示例中，我们将使用这个数据集。
- en: Let’s explore this dataset in detail. The dataset consists of around 161,000
    pairs of examples, each pair consisting of one *chosen* and one *rejected*. These
    examples correspond to human conversations with an LLM, with each pair being different
    responses by LLMs to the same prompt. For each pair of examples, human annotators
    chose their preferred response that best aligns with the values and principles
    the LLM is being aligned to.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探索这个数据集。该数据集包含大约161,000对示例，每对示例包含一个*被选中*和一个*被拒绝*的示例。这些示例对应于人类与LLM的对话，每对示例是LLM对同一提示的不同响应。对于每一对示例，人类标注者选择了他们认为最能与LLM正在对齐的价值观和原则相一致的最佳响应。
- en: 'Here is an example from the dataset:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据集的一个示例：
- en: 'Chosen:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 被选中：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Rejected:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 被拒绝：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After loading the dataset, you can use TRL’s `RewardTrainer` class for training
    the reward model:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集后，您可以使用TRL的`RewardTrainer`类来训练奖励模型：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Similarly, you can use TRL’s `PPOTrainer` class for performing the PPO step:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，您可以使用TRL的`PPOTrainer`类来执行PPO步骤：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Refer to the book’s [GitHub repo](https://oreil.ly/llm-playbooks) for the entire
    code. Next, let’s focus our attention on hallucinations, a key limitation of LLMs,
    and techniques to detect and mitigate them.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅书籍的[GitHub仓库](https://oreil.ly/llm-playbooks)以获取完整代码。接下来，让我们将注意力集中在幻觉上，这是LLM的一个关键限制，以及检测和减轻幻觉的技术。
- en: Hallucinations
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幻觉
- en: 'Colloquially, the word “hallucination” is used to refer to instances where
    the LLM generates something deemed to be incorrect. But LLMs can be incorrect
    for a wide variety of reasons. In this book, we will lead with a narrower definition
    of hallucinations: “An LLM is said to have hallucinated a generation if the generated
    text is not grounded to its training data or input context.”'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通俗地说，“幻觉”一词用来指代LLM生成被认为是不正确的内容的实例。但LLM可能因为各种原因而出现错误。在这本书中，我们将首先给出一个更窄的幻觉定义：“如果生成的文本没有与训练数据或输入上下文相联系，则称LLM产生了幻觉。”
- en: Simply put, if an LLM generates text that is inconsistent with either its pre-training/fine-tuning
    data or input context, then we call it a hallucination. Hallucinations most commonly
    manifest as text with no factual basis. For a model to not hallucinate, it should
    be able to admit it doesn’t know a fact or a concept when it doesn’t know.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，如果LLM生成的文本与其预训练/微调数据或输入上下文不一致，那么我们称之为幻觉。幻觉通常表现为没有事实依据的文本。为了使模型不产生幻觉，它应该能够在不知道某个事实或概念时承认这一点。
- en: 'Here is an example of a hallucination. Consider the prompt fed to the [NousResearch
    Hermes LLM](https://oreil.ly/tXmYh):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个幻觉的示例。考虑以下[NousResearch Hermes LLM](https://oreil.ly/tXmYh)接收到的提示：
- en: '*Prompt:* Provide a list of Ugandan 2020 Olympics medal winners along with
    their birthdays in a bulleted list.'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* 请以项目符号的形式列出2020年乌干达奥运金牌得主及其生日。'
- en: '*Response:* Joshua Cheptegei (Athletics - Men’s 10,000 meters gold medalist,
    born September 19, 1996)'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*响应:* 约书亚·切普特盖伊（田径 - 男子10000米金牌得主，出生于1996年9月19日）'
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jacob Kiplimo (Athletics - Men’s 10,000 meters silver medalist, born January
    16, 1997)
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 雅各布·基普利莫（田径 - 男子10000米银牌得主，出生于1997年1月16日）
- en: ''
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Peruth Chemutai (Athletics - Women’s 3000 meters steeplechase bronze medalist,
    born March 10, 1999)
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 佩鲁斯·切穆泰（田径 - 女子3000米障碍赛铜牌得主，出生于1999年3月10日）
- en: Yes, it is true that these are three athletes who won medals for Uganda in the
    2020 Summer Olympics, but a lot of content is hallucinated. Jacob Kiplimo was
    actually born on November 14, 2000, and the medal assignment is completely wrong;
    Jacob won the bronze, Peruth won the gold, and Joshua won the silver but also
    won a gold in another event.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这些确实是三位在2020年夏季奥运会上为乌干达赢得奖牌的运动员，但有很多内容是幻觉。雅各布·基普利莫实际上出生于2000年11月14日，奖牌分配是完全错误的；雅各布获得了铜牌，佩鲁斯获得了金牌，约书亚获得了银牌，但在另一项赛事中赢得了金牌。
- en: A model that doesn’t hallucinate would have provided factual information and
    admit that it doesn’t know specific details.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不会产生幻觉的模型会提供事实信息，并承认它不知道某些具体细节。
- en: Warning
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Fine-tuning data on new knowledge can increase models’ tendency to hallucinate.
    [Gekhman et al.](https://oreil.ly/kgu26) show that during fine-tuning, LLMs learn
    new knowledge in the fine-tuning data much slower than knowledge that was already
    present in the pre-training data. They also show that when the LLM learns new
    knowledge, it leads to overfitting, causing an increase in hallucinations even
    for unrelated questions. If you want to teach your model entirely new knowledge,
    I suggest using the continued pre-training setup with techniques like replay,
    etc., described in [Chapter 7](ch07.html#ch07).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在新知识上微调数据可以增加模型产生幻觉的倾向。[Gekhman 等人](https://oreil.ly/kgu26)表明，在微调过程中，LLM 在微调数据中学习新知识的速度比在预训练数据中已经存在的知识慢得多。他们还表明，当
    LLM 学习新知识时，会导致过拟合，即使对于无关的问题也会增加幻觉。如果你想要教你的模型全新的知识，我建议使用第 7 章中描述的持续预训练设置，例如重放等技术。
- en: Mitigating Hallucinations
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓解幻觉
- en: One of the biggest sources of hesitancy in adopting LLM-based tools and software
    is the system’s trustworthiness or lack thereof. Trustworthiness is most affected
    by the presence of hallucinations. Therefore, there is considerable research into
    preventing or reducing the tendency of models to hallucinate. Let’s explore some
    common techniques.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在采用基于 LLM 的工具和软件时，最大的犹豫来源之一是系统的可靠性或缺乏可靠性。可靠性最受幻觉存在的影响。因此，有相当多的研究致力于防止或减少模型产生幻觉的倾向。让我们探讨一些常见的技巧。
- en: At a product design level, you can reduce hallucination risk by simply not asking
    LLMs questions that you know it wouldn’t be able to answer. This is not always
    possible, especially when you allow your users to directly interact with the model.
    It is also not easy to determine what an LLM knows and does not know.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在产品设计层面，你可以通过简单地不问 LLM 你知道它无法回答的问题来降低幻觉风险。这并不总是可能的，尤其是在你允许用户直接与模型互动时。确定 LLM
    知道什么和不知道什么也不容易。
- en: '[Figure 8-1](#knowledge-quadrant) depicts a knowledge quadrant across knowledge
    and awareness dimensions. Ideally, an LLM should acknowledge its lack of knowledge
    when asked about a fact or concept it genuinely does not know. In [Figure 8-1](#knowledge-quadrant),
    we see that there can be four types of knowledge:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-1](#knowledge-quadrant) 描述了知识和意识维度的知识象限。理想情况下，当 LLM 被问及一个它真正不知道的事实或概念时，应该承认自己的知识不足。在
    [图 8-1](#knowledge-quadrant) 中，我们可以看到有四种类型的知识：'
- en: Known knowns
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 已知已知
- en: The LLM knows this knowledge/skill and is able to utilize it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 知道这个知识/技能，并且能够有效地利用它。
- en: Unknown knowns
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 未知已知
- en: The LLM knows this knowledge/skill but is not able to utilize it effectively
    (can be unlocked by fine-tuning or in-context learning).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 知道这个知识/技能，但无法有效地利用它（可以通过微调或情境学习解锁）。
- en: Known unknowns
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 已知未知
- en: The LLM knows that it does not know this knowledge.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 知道它不知道这个知识。
- en: Unknown unknowns
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 未知未知
- en: The LLM does not know that it does not know this knowledge, leading to hallucinations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 不知道它不知道这些知识，导致幻觉的产生。
- en: '![knowledge-quadrant](assets/dllm_0801.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![知识象限](assets/dllm_0801.png)'
- en: Figure 8-1\. Knowledge quadrant
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-1\. 知识象限
- en: To determine the level of self-knowledge a model possesses, [Yin et al.](https://oreil.ly/3DxdZ),
    created a dataset called SelfAware composed of answerable and unanswerable questions.
    Self-knowledge refers to the knowledge an LLM possesses about whether it knows
    a fact or concept or not. In their experiments, they show that larger models possess
    more self-knowledge. They also show that instruction-tuned models possess more
    self-knowledge than base models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定模型所拥有的自我认知水平，[Yin 等人](https://oreil.ly/3DxdZ)创建了一个名为 SelfAware 的数据集，其中包含可回答和不可回答的问题。自我认知是指
    LLM 对其是否知道某个事实或概念的知识。在他们的实验中，他们展示了更大的模型拥有更多的自我认知。他们还展示了指令微调模型比基础模型拥有更多的自我认知。
- en: An important way to assess a model’s self-knowledge is through its output uncertainty.
    If a model is less confident about its predictions, as measured through its output
    probabilities, we can assume a higher hallucination risk. For this approach to
    be valid, the model has to be well calibrated. As [Chapter 6](ch06.html#llm-fine-tuning)
    introduced, a model is well calibrated if there is a correlation between its output
    probability values and task accuracy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型自我知识的一个重要方式是通过其输出不确定性。如果一个模型对其预测的信心较低，如通过其输出概率来衡量，我们可以假设存在更高的幻觉风险。为了使这种方法有效，模型必须具有良好的校准。正如[第6章](ch06.html#llm-fine-tuning)所介绍，一个模型如果其输出概率值与任务准确性之间存在相关性，则被认为是校准良好的。
- en: Warning
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: '[Kadavath et al.](https://oreil.ly/VVY-i) show that techniques like RLHF reduce
    model calibration.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kadavath等人](https://oreil.ly/VVY-i)表明，像RLHF这样的技术会降低模型校准。'
- en: A key technique to address hallucinations is grounding the model to factual
    data sources. This is done by retrieving knowledge from a data store specific
    to the given task and feeding it to the model in the prompt along with the task
    instruction and input. This paradigm is called RAG, which we will discuss in [Part III](part03.html#part3)
    of the book.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 解决幻觉问题的关键技术是将模型与事实数据源联系起来。这是通过从特定于给定任务的数据存储中检索知识，并将其与任务指令和输入一起在提示中提供给模型来实现的。这种范式被称为RAG，我们将在本书的[第三部分](part03.html#part3)中讨论。
- en: 'RAG is not a panacea for the hallucination problem for the following reasons:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: RAG并不是解决幻觉问题的万能药，以下是一些原因：
- en: Feeding ground-truth factual data in the prompt reduces hallucinations but does
    not eliminate them completely, especially when the context is large.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中提供真实事实数据可以减少幻觉，但并不能完全消除，尤其是在上下文很大时。
- en: Using RAG shifts the bottleneck toward the retrieval process. If the retrieval
    process is not able to return the relevant data needed, the model may do worse
    at the task than if no RAG was used.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RAG将瓶颈转向检索过程。如果检索过程无法返回所需的相关数据，那么模型在任务上的表现可能比没有使用RAG时更差。
- en: In many cases, we do not have access to the ground-truth data; hence we cannot
    feed it as input context.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在许多情况下，我们无法访问真实数据；因此，我们无法将其作为输入上下文提供。
- en: Now let’s look at techniques that do not depend on us fetching ground-truth
    data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看那些不依赖于我们获取真实数据的技术的例子。
- en: Self-Consistency
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自洽性
- en: 'We can use self-consistency, which we first introduced in [Chapter 5](ch05.html#chapter_utilizing_llms),
    to detect the possibility of hallucinations. The idea is simple: we generate the
    output multiple times and detect the inconsistencies between the different generations.
    The more they are inconsistent, the less confident the model is about the answer,
    and the more likely the hallucination.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用自洽性，这是我们首次在第5章[利用LLMs](ch05.html#chapter_utilizing_llms)中介绍的，来检测幻觉的可能性。这个想法很简单：我们多次生成输出，并检测不同生成之间的不一致性。它们越不一致，模型对答案的信心就越低，幻觉的可能性就越大。
- en: Chain-of-Actions
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行为链
- en: 'Self-verification is another commonly used technique. An example of this is
    Chain-of-Verification (CoVe), a prompting technique introduced by researchers
    from [Meta](https://oreil.ly/ubp1t). An example implementation of CoVe works as
    follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 自验证是另一种常用的技术。这种技术的例子是来自[Meta](https://oreil.ly/ubp1t)的研究人员提出的提示技术链式验证（CoVe）。CoVe的一个示例实现工作如下：
- en: The model first generates a response to the given prompt.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型首先对给定的提示生成一个响应。
- en: Given the prompt and the response, the model generates verification questions
    to check the veracity of the response.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定提示和响应，模型生成验证问题以检查响应的真实性。
- en: Each of the verification questions is fed to the model separately.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个验证问题都是单独输入到模型中的。
- en: The responses to these verification questions are then compared to the original
    response, and if there are any inconsistencies, the original response is edited
    to integrate the responses from the verification questions.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将这些验证问题的响应与原始响应进行比较，如果有任何不一致性，原始响应将被编辑以整合验证问题的响应。
- en: 'As an example, for the earlier query:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于之前的查询：
- en: '*Prompt:* Provide a list of Ugandan Olympic medal winners along with their
    birthdays in a bulleted list.'
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* 以项目符号列表的形式提供乌干达奥运金牌得主及其生日的列表。'
- en: 'When asked to generate a list of verification questions, the model generates:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当要求生成一个验证问题列表时，模型生成：
- en: Who won the gold medal in the men’s 10,000m event in the 2020 Tokyo Olympics
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在2020年东京奥运会上，男子10000米项目的金牌得主是谁
- en: ''
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How many Ugandan athletes won medals at the 2020 Tokyo Olympics?
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在2020年东京奥运会上，有多少名乌干达运动员获得了奖牌？
- en: ''
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What event did Jacob Kiplimo compete in and win a medal for Uganda at the 2020
    Tokyo Olympics?
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Jacob Kiplimo在2020年东京奥运会上参加了哪项比赛并为乌干达赢得了奖牌？
- en: ''
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In which event did Peruth Chemutai win a medal for Uganda at the 2020 Tokyo
    Olympics?
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在2020年东京奥运会上，Peruth Chemutai为乌干达赢得了哪项比赛的奖牌？
- en: ''
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What year were Joshua Cheptegei and Jacob Kiplimo born?
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Joshua Cheptegei和Jacob Kiplimo出生于哪一年？
- en: ''
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What year was Peruth Chemutai born?
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Peruth Chemutai出生于哪一年？
- en: ''
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How many gold medals did Uganda win in total at the 2020 Tokyo Olympics?
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 乌干达在2020年东京奥运会上总共赢得了多少枚金牌？
- en: 'Asking the same model the following question:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 向同一模型提出以下问题：
- en: Who won the gold medal in the men’s 10,000m event in the 2020 Tokyo Olympics?
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2020年东京奥运会上，男子10000米比赛的金牌得主是谁？
- en: 'We get the response:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的答案是：
- en: In the 2020 Tokyo Olympics, the men’s 10,000m event was won by Selemon Barega
    of Ethiopia. He finished with a time of 27:43.22, beating the silver medalist
    Joshua Cheptegei of Uganda by 0.32 seconds.
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在2020年东京奥运会上，男子10000米比赛由埃塞俄比亚的Selemon Barega赢得。他以27:43.22的成绩完成比赛，比乌干达的银牌得主Joshua
    Cheptegei快了0.32秒。
- en: This response is inconsistent with the original response, which claimed that
    Joshua Cheptegai won the gold medal! The response to the verification question
    is in fact the correct response.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个回答与原始回答不一致，原始回答声称Joshua Cheptegai赢得了金牌！实际上，验证问题的回答是正确的。
- en: Therefore, we can see how methods like CoVe can be valuable in reducing hallucination
    risk. Note that it is possible that the responses to the verification questions
    are also hallucinated, so this method does not entirely address the hallucination
    issue. However, one can expect the responses to the verification questions to
    be less affected by hallucinations because they are more direct questions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到像CoVe这样的方法在减少幻觉风险方面的价值。请注意，验证问题的回答也可能存在幻觉，所以这种方法并不能完全解决幻觉问题。然而，可以预期验证问题的回答受幻觉的影响较小，因为它们是更直接的问题。
- en: Let’s now discuss hallucination reduction using recitation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论如何使用背诵来减少幻觉。
- en: Recitation
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背诵
- en: With the recitation technique, we prompt the LLM to generate one or more passages
    about the given question and then generate the answer based on the passages generated.
    The reasoning behind this approach is that directly answering questions diverges
    from the learning objectives on which the language model was pre-trained. Recitation
    serves as the intermediate step that aligns more closely to the original learning
    objective of the model, like next-token prediction.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用背诵技巧，我们提示LLM根据给定的问题生成一个或多个段落，然后根据生成的段落来生成答案。这种方法的背后逻辑是直接回答问题与语言模型预训练的学习目标相偏离。背诵作为中间步骤，与模型的原始学习目标（如下一个标记预测）更为契合。
- en: 'We can use few-shot prompting for soliciting recitations. The prompt looks
    like:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用少样本提示来征求背诵。提示如下：
- en: 'Query: <query>'
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查询：<query>
- en: ''
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Recitation1: <recitation>'
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Recitation1: <recitation>'
- en: ''
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Recitation2: <recitation>'
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Recitation2: <recitation>'
- en: ''
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: …
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …
- en: ''
  id: totrans-134
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'RecitationN: <recitation>'
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'RecitationN: <recitation>'
- en: ''
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Query: <query>'
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查询：<query>
- en: ''
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Recitation1:'
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Recitation1:'
- en: We can generate a single recitation or multiple recitations. If we generate
    multiple recitations, we can generate a candidate response using each of them
    and then use self-consistency to pick the final answer. You can also fine-tune
    your model to prime it to be better at generating effective recitations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以生成单个背诵或多个背诵。如果我们生成多个背诵，我们可以使用每个背诵生成一个候选答案，然后使用自洽性来选择最终答案。您还可以微调您的模型，使其在生成有效的背诵方面表现得更好。
- en: The recitation method typically consumes fewer tokens than chain-of-actions,
    but I find the latter to be more effective.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 背诵方法通常比动作链消耗的标记更少，但我发现后者更有效。
- en: Sampling Methods for Addressing Hallucination
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应对幻觉的采样方法
- en: The degree of hallucination also depends on the decoding method used. Recall
    our discussion on decoding algorithms in [Chapter 5](ch05.html#chapter_utilizing_llms).
    [Lee et al.](https://oreil.ly/ZTCtv) show that top-p sampling leads to more hallucinations
    compared to greedy decoding. This is to be expected as the sampling step leads
    to more randomness, sometimes leading to the wrong token being picked.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉的程度也取决于解码方法。回想一下我们在[第5章](ch05.html#chapter_utilizing_llms)中关于解码算法的讨论。[Lee等人](https://oreil.ly/ZTCtv)表明，与贪婪解码相比，top-p采样会导致更多的幻觉。这是可以预料的，因为采样步骤增加了更多的随机性，有时会导致选择错误的标记。
- en: One way to address increased hallucination risk due to sampling algorithms is
    to use a technique like factual-nucleus sampling, which [Lee et al. introduced](https://oreil.ly/D12xT).
    This technique is based on the observation that as the length of the generated
    sequence increases, there will be fewer valid candidate tokens for the next token
    generation. Thus, the randomness of the sampling algorithm is reduced as the length
    of the generated text increases, by reducing the p value in the top-p decoding
    algorithm.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决由于采样算法引起的幻觉风险增加，可以使用像事实核采样这样的技术，[Lee 等人介绍了这种方法](https://oreil.ly/D12xT)。这种技术基于观察，随着生成的序列长度增加，下一个令牌生成的有效候选令牌将减少。因此，随着生成的文本长度增加，采样算法的随机性会降低，这是通过在
    top-p 解码算法中降低 *p* 值来实现的。
- en: 'The formula looks like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 公式看起来是这样的：
- en: pt = max{ω, p × λ t−1 }
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: pt = max{ω, p × λ t−1 }
- en: where *t* refers to the generation step.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *t* 指的是生成步骤。
- en: 'There are three tunable parameters:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个可调参数：
- en: Decay rate (*λ*)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 衰减率 (*λ*)
- en: The *p* value of the algorithm is decayed by a decay rate at every step of the
    generation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的 *p* 值在生成的每一步都会以一个衰减率衰减。
- en: Reset (*p*)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 重置 (*p*)
- en: The *p* value might decay very quickly, thus degenerating to a greedy algorithm.
    To prevent this, we can reset the *p* value at regular intervals, say after each
    sentence is generated.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* 值可能会迅速衰减，从而退化成贪婪算法。为了防止这种情况，我们可以定期重置 *p* 值，例如在每句生成后。'
- en: Lower-bound (*ω*)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下限 (*ω*)
- en: To continue maintaining the advantages of the top-p algorithm, we can prevent
    the *p* value from getting too low by enforcing a lower bound.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续维持 top-p 算法的优势，我们可以通过强制执行一个下限来防止 *p* 值过低。
- en: This method comes with a tradeoff; lowering the *p* value reduces hallucination
    risk but also decreases diversity of token generation, causing a loss in performance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法存在权衡；降低 *p* 值可以减少幻觉风险，但也会降低令牌生成的多样性，从而导致性能下降。
- en: Decoding by Contrasting Layers
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过对比层解码
- en: The principle behind [*d*ecoding by c*o*ntrasting *la*yers (DoLa)](https://oreil.ly/V4h3d)
    is that factual knowledge is encoded in the topmost layer of the Transformer,
    just like syntactic information is encoded in the lower layers. Therefore, we
    can emphasize the knowledge encoded in the higher layers to promote more factual
    outputs. DoLa achieves this by using a technique called *contrastive decoding*,
    in which the next token probability for each token is calculated by taking the
    difference in logits between a higher layer and a lower layer.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[通过对比层解码（DoLa）的原理](https://oreil.ly/V4h3d) 是，事实知识被编码在 Transformer 的最顶层，就像句法信息被编码在较低的层一样。因此，我们可以强调更高层中编码的知识，以促进更多事实输出的产生。DoLa
    通过使用一种称为 *对比解码* 的技术来实现这一点，其中每个令牌的下一位令牌概率是通过计算一个较高层和一个较低层之间的 logits 差来计算的。'
- en: 'DoLa is available through Hugging Face. Let’s look at an example:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: DoLa 通过 Hugging Face 提供。让我们看一个例子：
- en: '[PRE4]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `dola_layers` argument should be used to activate DoLa decoding. `dola_layers`
    can be either a string or a list of integers. If it is a string, it should be
    either `'high'` or `'low'`. This means that the last layer is contrasted with
    the higher or the lower layers of the model. You can also specify a list of integers
    representing layer numbers. Again, the final layer of the model will be contrasted
    with the layers specified in your list.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 应使用 `dola_layers` 参数来激活 DoLa 解码。`dola_layers` 可以是一个字符串或整数列表。如果是一个字符串，它应该是 `'high'`
    或 `'low'`。这意味着模型的最后一层将与更高或更低的层进行对比。您也可以指定一个表示层号的整数列表。同样，模型的最后一层将与您列表中指定的层进行对比。
- en: To reduce repetitiveness induced by DoLa, you can set a repetition penalty through
    the `repetition_penalty` argument (this is set by default). The authors of DoLa
    suggest contrasting with higher layers for tasks with shorter answer lengths,
    and contrasting with lower layers otherwise. They also recommend not using DoLa
    for smaller LLMs. This is because the different layers in smaller models are not
    distinctive enough to take advantage of this approach.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少 DoLa 引起的重复性，您可以通过 `repetition_penalty` 参数设置重复惩罚（这是默认设置的）。DoLa 的作者建议对于答案长度较短的任务，与更高层进行对比；对于其他任务，则与更低层进行对比。他们还建议不要为较小的
    LLM 使用 DoLa。这是因为较小模型的不同层之间没有足够的区分度来利用这种方法。
- en: In-Context Hallucinations
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在上下文中的幻觉
- en: So far, we have focused on hallucinations emanating from the model trying to
    generate from its parametric memory. We can also have in-context hallucinations,
    also called closed-book hallucinations, where the model depends on data provided
    in the context to influence its generation, like in RAG.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们关注的是模型试图从其参数记忆中生成而产生的幻觉。我们还可以有上下文幻觉，也称为闭卷幻觉，其中模型依赖于上下文中提供的数据来影响其生成，就像在
    RAG 中一样。
- en: 'For example, consider the passage:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下段落：
- en: 'At the 2020 Summer Olympics in Tokyo, Uganda won three medals:'
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在2020年东京夏季奥运会上，乌干达赢得了三枚奖牌：
- en: ''
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Joshua Cheptegei – Gold in Men’s 5000m
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Joshua Cheptegei – 男子5000米金牌
- en: ''
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Peruth Chemutai – Gold in Women’s 3000m Steeplechase
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Peruth Chemutai – 女子3000米障碍赛金牌
- en: ''
  id: totrans-170
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jacob Kiplimo – Bronze in Men’s 10,000m
  id: totrans-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Jacob Kiplimo – 男子10000米铜牌
- en: ''
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These medals marked a historic achievement for Uganda, with both Cheptegei and
    Chemutai winning gold in their respective events.
  id: totrans-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些奖牌为乌干达创造了历史性的成就，Cheptegei 和 Chemutai 分别在各自的项目中赢得了金牌。
- en: 'fed to the LLM as context for the following query:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 作为以下查询的上下文输入给 LLM：
- en: Provide a list of Ugandan 2020 Olympics medal winners along with their birthdays
    in a bulleted list.
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提供一个包含乌干达2020年奥运会奖牌获得者和他们生日的项目符号列表。
- en: 'While the passage contains all the medal winners along with the events, their
    birthdays are not present, leading the model to hallucinate them. Here is the
    model’s response:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然段落包含了所有奖牌获得者以及他们的比赛项目，但他们的生日没有出现，导致模型产生了幻觉。以下是模型的响应：
- en: 'Here is a list of Ugandan 2020 Olympics medal winners with their birthdays:'
  id: totrans-177
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里是乌干达2020年奥运会奖牌获得者和他们生日的列表：
- en: ''
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Joshua Cheptegei (born September 12, 1996) - Gold medal in Men’s 5000m
  id: totrans-179
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshua Cheptegei（出生于1996年9月12日） - 男子5000米金牌
- en: ''
  id: totrans-180
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-181
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Peruth Chemutai (born August 10, 1999) - Gold medal in Women’s 3000m Steeplechase
  id: totrans-182
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peruth Chemutai（出生于1999年8月10日） - 女子3000米障碍赛金牌
- en: ''
  id: totrans-183
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-184
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Jacob Kiplimo (born December 16, 1997) - Bronze medal in Men’s 10,000m
  id: totrans-185
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob Kiplimo（出生于1997年12月16日） - 男子10000米铜牌
- en: Due to the knowledge provided in the context, the model gets the names of the
    athletes, their medals, and their events correct, but gets the birthdays wrong
    for Peruth and Jacob.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上下文中提供的信息，模型正确地得到了运动员的名字、他们的奖牌和他们的比赛项目，但错误地给出了 Peruth 和 Jacob 的生日。
- en: How can we detect and mitigate these hallucinations? [Chuang et al.](https://oreil.ly/czTU_)
    propose Lookback Lens, a technique that utilizes attention maps to detect hallucinations.
    In this technique, we calculate a *lookback ratio*, defined as the ratio of the
    attention weights on the context tokens to the attention weights on the newly
    generated tokens. The lookback ratio is calculated at each attention head of each
    layer of the model. These ratios are used as features to train a linear classifier
    model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何检测和减轻这些幻觉？[Chuang 等人](https://oreil.ly/czTU_)提出了 Lookback Lens 技术，这是一种利用注意力图来检测幻觉的技术。在这个技术中，我们计算一个
    *lookback ratio*，定义为上下文标记上的注意力权重与新生成标记上的注意力权重的比率。这个比率在模型的每一层的每个注意力头处计算。这些比率被用作特征来训练一个线性分类器模型。
- en: The classifier model can also be employed to reduce hallucinations during generation.
    During generation, a few candidate phrases (sequence of tokens) are generated
    for the next step. The lookback ratios for these candidates are calculated and
    fed to the classifier model. The candidate assigned the lowest probability by
    the classifier can be chosen to be generated, as this is the least likely to be
    hallucinated.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器模型也可以在生成过程中减少幻觉。在生成过程中，为下一步生成几个候选短语（标记序列）。计算这些候选词的 lookback 比率，并将其输入到分类器模型中。分类器分配给概率最低的候选词可以被选中生成，因为这最不可能产生幻觉。
- en: Using a classifier-based decoding strategy can be a massive drag on system latency,
    however. These approaches should be used only if latency isn’t a prime consideration.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于分类器的解码策略可能会对系统延迟产生重大影响。只有当延迟不是首要考虑因素时，才应使用这些方法。
- en: Hallucinations Due to Irrelevant Information
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 由于无关信息导致的幻觉
- en: 'The presence of irrelevant information in the context can also lead to hallucinations.
    As an example, consider this prompt:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文中存在无关信息也可能导致幻觉。例如，考虑以下提示：
- en: '*Prompt:* Indianapolis, the capital of Indiana, is known for its vibrant cultural
    scene, including museums, theaters, and sporting events, especially the Indianapolis
    500 race. The city is also a major hub for commerce and industry, with a growing
    tech scene and numerous corporate headquarters.'
  id: totrans-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* 印第安纳州的州府印第安纳波利斯以其充满活力的文化场景而闻名，包括博物馆、剧院和体育赛事，尤其是印第安纳波利斯500赛车。该市也是商业和工业的重要枢纽，拥有不断发展的科技场景和众多企业总部。'
- en: ''
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Where is the actor Lily Gao born?
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 李莉·高（Lily Gao）的出生地在哪里？
- en: 'The response is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 回答如下：
- en: '*Response:* Lily Gao, an American actress known for her roles in various TV
    series and films, was born on October 1, 1994, in Indianapolis, Indiana, USA.'
  id: totrans-196
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*回答:* 李莉·高，一位因在多部电视剧和电影中的角色而闻名的美国女演员，于1994年10月1日出生于美国印第安纳州印第安纳波利斯。'
- en: However, in reality Lily Gao was born in Canada! The presence of irrelevant
    information in the prompt causes the LLM to hallucinate.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在现实中，李莉·高（Lily Gao）出生在加拿大！提示中存在无关信息导致LLM产生了幻觉。
- en: To mitigate this issue, [Weston et al.](https://oreil.ly/aqsxN) propose a technique
    called System 2 Attention (S2A). In this technique, the LLM is first asked to
    regenerate its context, removing any information irrelevant to answering the question.
    Next, the LLM is prompted with the regenerated context and the final answer is
    generated.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这个问题，[Weston等人](https://oreil.ly/aqsxN)提出了一种称为系统2注意力（S2A）的技术。在这个技术中，首先要求LLM重新生成其上下文，删除任何与回答问题无关的信息。接下来，LLM被提示以重新生成的上下文，并生成最终答案。
- en: 'As an example, consider this math problem with a distractor sentence:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑这个带有干扰句子的数学问题：
- en: '*Prompt:* Sarah has 5 apples. She buys 3 more apples from the store. Max sells
    3 apples to the store. How many apples does Sarah have now?'
  id: totrans-200
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* 萨拉有5个苹果。她从商店买了3个更多的苹果。马克斯把3个苹果卖给商店。现在萨拉有多少个苹果？'
- en: 'We issue the following prompt:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发出以下提示：
- en: '*Prompt:* Regenerate the context removing any information that is irrelevant
    to answering the question.'
  id: totrans-202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* 重新生成上下文，删除任何与回答问题无关的信息。'
- en: 'The response is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 回答如下：
- en: '*Response:* Sarah has 5 apples. She buys 3 more apples from the store. How
    many apples does Sarah have now?'
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*回答:* 萨拉有5个苹果。她从商店买了3个更多的苹果。现在萨拉有多少个苹果？'
- en: This can be fed back to the model to provide the correct answer.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以反馈给模型以提供正确答案。
- en: Tip
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can also implement S2A in a single prompt, by asking the model to regenerate
    the context followed by the final answer. However, performing this in two prompts
    has shown to be more effective.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过要求模型重新生成上下文并随后给出最终答案，在单个提示中实现S2A。然而，在两个提示中执行此操作已被证明更有效。
- en: Next, let’s explore the reasoning capabilities of LLMs and showcase techniques
    for improving them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索LLM的推理能力，并展示提高它们的方法。
- en: Reasoning
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理
- en: In [Chapter 1](ch01.html#chapter_llm-introduction), we discussed the limitations
    of language models and pointed to reasoning as one of the biggest limitations.
    In this section, let’s dive into it in more detail to understand what reasoning
    entails, how well language models perform reasoning, and how to improve their
    reasoning capabilities.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一章](ch01.html#chapter_llm-introduction)中，我们讨论了语言模型的局限性，并将推理列为最大的局限性之一。在本节中，让我们更深入地探讨它，以了解推理包含的内容、语言模型推理的表现以及如何提高它们的推理能力。
- en: 'First, let’s define reasoning:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义推理：
- en: Natural language reasoning is a process to integrate multiple knowledge (e.g.
    encyclopedic knowledge and commonsense knowledge) to derive some new conclusions
    about the (realistic or hypothetical) world. Knowledge can be from both explicit
    and implicit sources. Conclusions are assertions or events assumed to be true
    in the world, or practical actions.
  id: totrans-212
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自然语言推理是一个将多种知识（例如，百科全书知识和常识知识）整合起来，以得出关于（现实或假设）世界的一些新结论的过程。知识可以来自显性和隐性来源。结论是假定在世界上为真的断言或事件，或实际行动。
- en: ''
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Yu et al.](https://oreil.ly/7NsBF)'
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[Yu等人](https://oreil.ly/7NsBF)'
- en: 'Reasoning can be classified into several different types. Here are a few forms
    of non-mutually-exclusive reasoning categories:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 推理可以分为几种不同的类型。以下是一些非互斥推理类别的形式：
- en: Deductive Reasoning
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演绎推理
- en: Deductive reasoning uses logic to draw conclusions from one or more premises.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 演绎推理使用逻辑从一个或多个前提中得出结论。
- en: 'As an example, consider the following passage:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，考虑以下段落：
- en: Mr. Shockley was allergic to mushrooms. The dish “Golden Travesty” has mushrooms
    in it.
  id: totrans-219
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 震惊先生对蘑菇过敏。金玉其外这道菜里面有蘑菇。
- en: Based on this set of premises, we can deduce that Mr. Shockley should stay far
    away from the Golden Travesty dish.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一系列前提，我们可以推断出肖克利先生应该远离金旅店（Golden Travesty）这道菜。
- en: Inductive Reasoning
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归纳推理
- en: Inductive reasoning involves making generalizations based on a set of observations.
    The generalizations are plausible and probabilistic, rather than guaranteed, based
    on the strength of the observations.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳推理涉及基于一组观察结果进行概括。这些概括基于观察结果的强度是可能的和概率性的，而不是保证的。
- en: As an example, upon observing hundreds or even thousands of round manhole covers,
    one can conclude that manhole covers are generally round. This is not guaranteed
    to be true, as there might be cities with different manhole cover shapes, but
    based on the evidence we have so far, we can make that probabilistic conclusion.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，观察了数百甚至数千个圆形井盖后，可以得出结论，井盖通常是圆形的。这并不保证总是正确的，因为可能有些城市的井盖形状不同，但根据我们目前拥有的证据，我们可以做出这种概率性的结论。
- en: Abductive Reasoning
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归纳推理
- en: 'Abductive reasoning involves analyzing a set of observations and concluding
    with the most likely explanation:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳推理涉及分析一组观察结果，并得出最可能的解释：
- en: 'Observation: The street is wet. There are water puddles on the sidewalk. People
    have umbrellas in their hands.'
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 观察：街道是湿的。人行道上有很多水坑。人们手里拿着伞。
- en: ''
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Explanation: It rained recently.'
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 解释：最近下雨了。
- en: Abductive reasoning offers the most likely explanation but is not guaranteed
    to be true. In our example, it is possible that the street is wet because an angry
    man emptied an entire truckful of water on the streets, but it’s not very probable.
    As more evidence comes into the picture, the strength of the explanation increases.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳推理提供了最可能的解释，但并不保证是正确的。在我们的例子中，街道湿可能是因为一个愤怒的人把一整车的水倒在了街道上，但这不太可能。随着更多证据的出现，解释的强度会增加。
- en: Common Sense Reasoning
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常识推理
- en: 'Common sense reasoning refers to utilizes a shared understanding of the world
    to make assumptions about the physical world or human relationships. Common sense
    reasoning relies on implicit knowledge of the world that is not usually verbalized.
    For example:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 常识推理是指利用对世界的共同理解来对物理世界或人际关系做出假设。常识推理依赖于对世界的隐含知识，这些知识通常不会用言语表达。例如：
- en: She saw him prancing around the hall with a glass in his hand, held upside down.
  id: totrans-232
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 她看到他手里拿着一个玻璃杯，倒立着在大厅里蹦跳。
- en: While not explicitly mentioned in the text, common sense would dictate that
    the glass does not contain any liquids given it is upside down.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然文本中没有明确提到，但常识会告诉我们，由于玻璃杯是倒立的，所以里面不含有任何液体。
- en: Other forms of reasoning include mathematical (usually based on deductions),
    causal (identifying cause-and-effect relationships), analogical (drawing comparisons
    between two things or concepts), and moral (evaluating situations and decisions
    based on moral principles and values).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其他推理形式包括数学的（通常基于演绎）、因果的（识别因果关系）、类比的（在两个事物或概念之间进行比较）和道德的（根据道德原则和价值观评估情况和决策）。
- en: Tip
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: '[Cheng et al.](https://oreil.ly/vkTjt) show that LLMs perform much better on
    inductive reasoning than deductive reasoning.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[程等](https://oreil.ly/vkTjt)表明，LLMs在归纳推理上的表现优于演绎推理。'
- en: Inducing Reasoning in LLMs
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在LLMs中诱导推理
- en: The simplest way of improving reasoning in LLMs is to use prompting techniques
    like chain-of-thought, introduced in [Chapter 1](ch01.html#chapter_llm-introduction).
    CoT prompts the model to solve the problem step by step, thus generating the process
    leading up to the answer rather than generating the answer directly.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs中提高推理能力最简单的方法是使用提示技术，如第1章中介绍的思维链（chain-of-thought）。CoT提示模型逐步解决问题，从而生成通向答案的过程，而不是直接生成答案。
- en: Verifiers for Improving Reasoning
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高推理能力的验证器
- en: So, LLMs may not be all that great at producing the right answer to a question
    that requires multistep reasoning. But all hope is not lost. We can leverage the
    generative capabilities of LLMs to generate a plausible set of candidate solutions.
    These candidates can then be assessed by a verifier, which can identify the correct
    answer. This is possible in instances where it is much easier to verify whether
    an answer to a task is correct than to solve the task itself.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，大型语言模型（LLM）可能并不擅长回答需要多步推理的问题。但并非毫无希望。我们可以利用LLM的生成能力来生成一组合理的候选解决方案。然后验证者可以评估这些候选方案，并确定正确答案。在验证任务答案是否正确比解决任务本身容易得多的情况下，这是可能的。
- en: Warning
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Just because LLMs can generate plausible candidate solutions for a question
    is not evidence of their reasoning abilities. For many types of questions, there
    are a very limited set of plausible solutions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 仅因为LLM可以为问题生成合理的候选解决方案，并不能证明它们的推理能力。对于许多类型的问题，合理的解决方案非常有限。
- en: Verifiers can be based on LLMs, called *LLM-as-a-judge*, or can be external
    models or even symbolic verifiers. Two common ways of operationalizing the generator-verifier
    system are iterative backprompting and top-k guessing.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 验证器可以是基于LLM的，称为*LLM作为裁判*，也可以是外部模型或甚至是符号验证器。实现生成器-验证器系统的两种常见方式是迭代回退提示和Top-k猜测。
- en: Iterative backprompting
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迭代回退提示
- en: In this process, an LLM generates a proposed solution to a given problem that
    requires reasoning. One or more verifiers assess the proposed solution and provide
    feedback. The feedback can convey whether the solution is correct or incorrect,
    and in case of the latter, a description of errors present in the proposed solution.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，LLM为需要推理的给定问题生成一个建议解决方案。一个或多个验证者评估该建议解决方案并提供反馈。反馈可以传达解决方案是否正确，如果错误，则描述建议解决方案中存在的错误。
- en: The LLM takes the feedback as input and generates the solution again, which
    is again passed to the verifier. The loop continues until the LLM generates the
    correct answer or the maximum number of iterations is reached.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: LLM将反馈作为输入再次生成解决方案，该解决方案再次传递给验证者。循环继续，直到LLM生成正确答案或达到最大迭代次数。
- en: Top-k guessing
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Top-k 猜测
- en: In this technique, k solutions are generated for a given task, and the verifier
    assesses them and chooses the correct solution if it exists. A relatively high
    temperature (>1) is used during decoding to generate a diverse set of solutions.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，为给定任务生成k个解决方案，验证者评估它们，并在存在正确解决方案的情况下选择它。在解码过程中使用相对较高的温度（>1）来生成多样化的解决方案集。
- en: '[Kambhampati et al.](https://oreil.ly/4_MxJ) show that top-k guessing exhibits
    similar performance levels as iterative backprompting.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kambhampati等人](https://oreil.ly/4_MxJ)表明，Top-k猜测表现出与迭代回退提示相似的性能水平。'
- en: Inference-Time Computation
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理时计算
- en: This might well be the most significant topic of 2025 and beyond. As of this
    book’s writing, scaling up pre-training seems to be providing diminishing returns.
    Therefore, there is a hunt for new scaling dimensions. The most promising among
    them is scaling up inference-time compute. The premise is simple. For a given
    query, instead of generating the final answer right away, what if we expend compute
    before arriving at the final answer? Can we improve the performance of the model
    with more compute? Turns out, we can! Let’s discuss this new scaling avenue in
    detail.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是2025年及以后最重要的主题。截至本书撰写时，预训练的扩展似乎正在提供递减的回报。因此，人们正在寻找新的扩展维度。其中最有希望的是推理时计算的扩展。前提很简单。对于给定的查询，我们为什么不先进行计算，然后再生成最终答案？我们能否通过更多的计算来提高模型的性能？结果证明，我们可以！让我们详细讨论这条新的扩展途径。
- en: Repeated sampling
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重复采样
- en: The most simple and common inference-time compute technique is repeated sampling.
    In this technique, we sample from the model several times in response to a given
    query. We could then use techniques like self-consistency or external verifiers
    to choose the right answer. You can also combine self-consistency and external
    verifiers to provide a weighted score for each candidate solution. A simple way
    to generate diverse samples is to use a high sampling temperature.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单且常见的推理时计算技术是重复采样。在这个技术中，我们针对给定的查询从模型中多次采样。然后我们可以使用自我一致性或外部验证器等技术来选择正确答案。你还可以结合自我一致性和外部验证器为每个候选解决方案提供加权分数。生成多样化样本的一个简单方法是使用高采样温度。
- en: Another simple approach is to use iterative generation, as shown earlier in
    this chapter. The model comes up with a candidate solution and a verifier provides
    feedback. The model iteratively improves its response using the verifier feedback
    until it reaches the final answer or the maximum number of iterations. Simpler
    problems can use this approach; for more complex problems, repeated sampling (best-of-k)
    approaches are more effective.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种简单的方法是使用迭代生成，如本章前面所示。模型提出一个候选解决方案，验证者提供反馈。模型通过验证者的反馈迭代改进其响应，直到达到最终答案或最大迭代次数。简单问题可以使用这种方法；对于更复杂的问题，重复采样（best-of-k）方法更有效。
- en: Yet another approach is to augment the context across which the generation takes
    place. CoT prompting is the easiest way to achieve that. Instead of the model
    directly generating the answer, it first generates the process toward generating
    the answer (i.e., the thought process).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是增强生成过程中所跨越的上下文。CoT提示是实现这一点的最简单方法。不是模型直接生成答案，而是首先生成生成答案的过程（即，思考过程）。
- en: In essence, a language model generates a probability distribution P(Y | X) where
    X is the input context and the previously generated tokens. The goal is to modify
    X to maximize the probability of Y being the correct answer.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，语言模型生成一个概率分布P(Y | X)，其中X是输入上下文和之前生成的标记。目标是修改X以最大化Y是正确答案的概率。
- en: '[Jin et al.](https://oreil.ly/Dc_Fc) show some important experiments on this.
    First, the length of the reasoning steps matters to the performance. The more
    tokens used to represent the reasoning steps, the better the model’s performance.
    Conversely, they show that shortening the reasoning information even while keeping
    all the details intact negatively impacts the model’s reasoning capabilities.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[金等人](https://oreil.ly/Dc_Fc)展示了这方面的某些重要实验。首先，推理步骤的长度对性能很重要。用于表示推理步骤的标记越多，模型的性能越好。相反，他们还表明，即使在保持所有细节完整的情况下缩短推理信息，也会对模型的推理能力产生负面影响。'
- en: Jin et al. also show that errors in the reasoning steps do not impact the performance
    as much, as long as the length of the reasoning steps exceeds a threshold.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 金等人还表明，只要推理步骤的长度超过一个阈值，推理步骤中的错误对性能的影响并不大。
- en: For simpler tasks, shorter reasoning steps suffice, but for more complex tasks,
    increasing the length of the reasoning steps is very beneficial.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更简单的任务，较短的推理步骤就足够了，但对于更复杂的任务，增加推理步骤的长度非常有好处。
- en: Tip
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Jin et al. also show that replacing the CoT prompt “Let’s think step by step”
    with “Let’s think step by step, you must think more steps.” improves performance.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 金等人还表明，将CoT提示“让我们一步一步思考”替换为“让我们一步一步思考，你必须思考更多步骤”可以提高性能。
- en: Leveraging inference-time compute, you can squeeze more performance from smaller
    LLMs.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 利用推理时间计算，你可以从较小的LLM中获得更多性能。
- en: Search
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 搜索
- en: Another way to utilize inference-time compute is to search through the generation
    probability distribution. The idea is that for every token being generated, there
    is a probability assigned to every token in the vocabulary, so if we search properly,
    we might end up traversing the probability space and choosing the right path to
    generate the correct answer. Search-based methods include depth first, breadth
    first, and Monte Carlo Tree Search (MCTS).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 利用推理时间计算的另一种方法是搜索生成概率分布。想法是对于每个正在生成的标记，词汇表中的每个标记都有一个概率分配，因此如果我们正确搜索，我们可能会遍历概率空间并选择正确的路径来生成正确的答案。基于搜索的方法包括深度优先搜索、广度优先搜索和蒙特卡洛树搜索（MCTS）。
- en: Fine-Tuning for Reasoning
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解推理的微调
- en: 'One way to induce better reasoning in a model is to fine-tune it with a reasoning
    dataset. Let’s explore the [reasoning-0.01](https://oreil.ly/W1JRq) dataset released
    by SkunkworksAI to see what a typical reasoning dataset looks like:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型中诱导更好的推理的一种方法是通过推理数据集对其进行微调。让我们探索SkunkworksAI发布的[reasoning-0.01](https://oreil.ly/W1JRq)数据集，看看一个典型的推理数据集是什么样的：
- en: '*Input:* If a die is rolled three times, what is the probability of getting
    a sum of 11?'
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*输入:* 如果掷三次骰子，得到总和为11的概率是多少？'
- en: '*Reasoning:* { “step”: 1, “thought”: “Understand the problem: We need to find
    the probability of getting a sum of 11 when rolling a die three times.” },'
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*推理:* { “step”: 1, “thought”: “理解问题：我们需要找到掷三次骰子得到总和为11的概率。” },'
- en: ''
  id: totrans-269
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{ “step”: 2, “thought”: “Calculate total possible outcomes: A die has 6 faces,
    so for each roll, there are 6 possibilities. For three rolls, the total possible
    outcomes are 6^3 = 216.” },'
  id: totrans-270
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '{ “step”: 2, “thought”: “计算总可能结果：骰子有6个面，所以每次掷骰子有6种可能性。对于三次掷骰子，总可能结果为6^3 = 216。”
    },'
- en: ''
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{ “step”: 3, “thought”: “Identify favorable outcomes: List all combinations
    of rolls that result in a sum of 11\. There are 18 such combinations.” },'
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '{ “step”: 3, “thought”: “识别有利结果：列出所有掷骰子得到总和为11的组合。共有18种这样的组合。” },'
- en: ''
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{ “step”: 4, “thought”: “Calculate probability: Divide the number of favorable
    outcomes by the total possible outcomes: 18 / 216 = 1/12.” },'
  id: totrans-274
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '{ “step”: 4, “thought”: “计算概率：将有利结果的数量除以总可能结果：18 / 216 = 1/12。” },'
- en: ''
  id: totrans-275
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{ “step”: 5, “thought”: “Conclusion: The probability of getting a sum of 11
    when rolling a die three times is 1/12.” } ]'
  id: totrans-276
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '{ “step”: 5, “thought”: “结论：掷三次骰子得到总和为11的概率是1/12。” } ]'
- en: ''
  id: totrans-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Output:* 1/12'
  id: totrans-278
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*输出:* 1/12'
- en: The dataset contains step-by-step reasoning chains for a large variety of tasks.
    Such a dataset can be generated synthetically using larger models, followed by
    a human verification and annotation stage to verify and correct reasoning chains.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含大量任务的逐步推理链。这样的数据集可以通过使用更大的模型进行合成生成，随后进入人工验证和标注阶段，以验证和纠正推理链。
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we defined alignment training and why we need it. We ventured
    into techniques for alignment training such as reinforcement learning. We also
    learned about hallucinations and different techniques to mitigate them. Finally,
    we examined reasoning limitations of LLMs and new techniques like scaling up inference-time
    computation.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们定义了对齐训练及其必要性。我们探讨了对齐训练的技术，例如强化学习。我们还了解了幻觉及其缓解技术。最后，我们考察了LLM的推理局限性以及新的技术，如提高推理时间计算的可扩展性。
- en: In the next chapter, we’ll discuss techniques for speeding up LLM inference.
    High computation costs are a significant barrier to LLM adoption, and thus a plethora
    of techniques have been developed to improve inference speeds.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论加快LLM推理的技术。高计算成本是LLM应用的一个重大障碍，因此已经开发了许多技术来提高推理速度。
