<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Image segmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Image segmentation</h1>
<blockquote>原文：<a href="https://deeplearningwithpython.io/chapters/chapter11_image-segmentation">https://deeplearningwithpython.io/chapters/chapter11_image-segmentation</a></blockquote>


<aside>
<p>This chapter covers
</p>
<ul>
<li>The different branches of computer vision: image classification,
  image segmentation, and object detection</li>
<li>Building a segmentation model from scratch</li>
<li>Using the pretrained Segment Anything Model</li>
</ul>
</aside>

<p>Chapter 8 gave you a first introduction to deep learning for computer vision
via a simple use case: binary image classification.
But there’s more to computer vision than image classification!
This chapter dives deeper into another essential computer vision application — image segmentation.</p>
<h2 id="computer-vision-tasks">Computer vision tasks</h2>
<p>So far, we’ve focused on image classification models: an image goes in, a label
comes out. “This image likely contains a cat; this other one likely contains a dog.”
But image classification is only one of several possible applications of deep
learning in computer vision. In general, there are three essential computer
vision tasks you need to know about:</p>
<ul>
<li><em>Image classification</em>, where the goal is to assign one or more labels to an image.
It may be either single-label classification
(meaning categories are mutually exclusive) or
multilabel classification
(tagging all categories that an image belongs to, as shown in figure 11.1).
For example, when you search for a keyword on the Google Photos app, behind the scenes
you’re querying a very large multilabel classification model — one with over
20,000 different classes, trained on millions of images.</li>
<li><em>Image segmentation</em>, where the goal is to “segment” or “partition” an image
into different areas, with each area usually representing a category
(as shown in figure 11.1). For instance, when Zoom or Google Meet displays a
custom background behind you in a video call, it’s using an image segmentation
model to distinguish your face from what’s behind it, with pixel-level precision.</li>
<li><em>Object detection</em>, where the goal is to draw rectangles (called <em>bounding boxes</em>)
around objects of interest in an image and associate each rectangle with a class.
A self-driving car could use an object detection model to monitor cars, pedestrians,
and signs in view of its cameras, for instance.</li>
</ul>
<figure id="figure-11-1">
<img src="../Images/2045275007f5b1bc39eac7d6965c23da.png" data-original-src="https://deeplearningwithpython.io/images/ch11/computer_vision_tasks.da2bf0ea.png"/>
<figcaption>
<a href="#figure-11-1">Figure 11.1</a>: The three main computer vision tasks: classification, segmentation, and detection
</figcaption>
</figure>

<p>Deep learning for computer vision also encompasses a number of somewhat more niche tasks
besides these three, such as image similarity scoring (estimating how visually similar two
images are), keypoint detection
(pinpointing attributes of interest in an image, such as facial features),
pose estimation, 3D mesh estimation, depth estimation, and so on.
But to start with, image classification, image segmentation,
and object detection form the foundation that every machine learning engineer
should be familiar with. Almost all computer vision applications boil down to one of these three.</p>
<p>You’ve seen image classification in action in Chapter 8.
Next, let’s dive into image segmentation. It’s a very useful and very versatile
technique, and you can straightforwardly approach it with what you’ve already learned so far.
Then, in the next chapter, you’ll learn about object detection in detail.</p>
<h3 id="types-of-image-segmentation">Types of image segmentation</h3>
<p>Image segmentation with deep learning is about using a model to assign a class
to each pixel in an image, thus <em>segmenting</em> the image into different zones
(such as “background” and “foreground” or “road,” “car,” and “sidewalk”).
This general category of techniques can be used to power a considerable variety of
valuable applications in image and video editing, autonomous driving, robotics,
medical imaging, and so on.</p>
<p>There are three different flavors of image segmentation that you should know about:</p>
<ul>
<li><em>Semantic segmentation</em>, where each pixel is independently classified into
a semantic category, like “cat.” If there are two cats in the image,
the corresponding pixels are all mapped to the same generic “cat” category
(see figure 11.2).</li>
<li><em>Instance segmentation</em>, which seeks to parse out individual object instances.
In an image with two cats in it, instance segmentation would distinguish
between pixels belonging to “cat 1” and pixels belonging to “cat 2” (see figure 11.2).</li>
<li><em>Panoptic segmentation</em>, which combines semantic segmentation and instance
segmentation by assigning to each pixel in an image
both a semantic label (like “cat”) and an instance label (like “cat 2”). This
is the most informative of all three segmentation types.</li>
</ul>
<figure id="figure-11-2" class="large-image">
<img src="../Images/7fd2d264b2190f8be9fac2be63b59e45.png" data-original-src="https://deeplearningwithpython.io/images/ch11/instance_segmentation.818c62ba.png"/>
<figcaption>
<a href="#figure-11-2">Figure 11.2</a>: Semantic segmentation vs. instance segmentation
</figcaption>
</figure>

<p>To get more familiar with segmentation, let’s get started with training
a small segmentation model from scratch on your own data.</p>
<h2 id="training-a-segmentation-model-from-scratch">Training a segmentation model from scratch</h2>
<p>In this first example, we’ll focus on semantic segmentation. We’ll be looking
once again at images of cats and dogs, and this time we’ll be learning to
tell apart the main subject and its background.</p>
<h3 id="downloading-a-segmentation-dataset">Downloading a segmentation dataset</h3>
<p>We’ll work with the Oxford-IIIT Pets dataset (<a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">https://www.robots.ox.ac.uk/~vgg/data/pets/</a>),
which contains 7,390 pictures of various breeds of cats and dogs, together with
foreground-background <em>segmentation masks</em> for each picture. A segmentation mask
is the image segmentation equivalent of a label: it’s an image the same size as the input
image, with a single color channel where each integer value corresponds to the
class of the corresponding pixel in the input image. In our case, the pixels of
our segmentation masks can take one of three integer values:</p>
<ul>
<li>1 (foreground)</li>
<li>2 (background)</li>
<li>3 (contour)</li>
</ul>
<p>Let’s start by downloading and uncompressing our dataset, using the <code>wget</code> and <code>tar</code>
shell utilities:</p>
<figure>
<pre><code class="language-text">!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
!tar -xf images.tar.gz
!tar -xf annotations.tar.gz
</code></pre>
</figure>

<p>The input pictures are stored as JPG files in the <code>images/</code> folder
(such as <code>images/Abyssinian_1.jpg</code>), and the corresponding segmentation mask
is stored as a PNG file with the same name in the <code>annotations/trimaps/</code> folder
(such as <code>annotations/trimaps/Abyssinian_1.png</code>).</p>
<p>Let’s prepare the list of input file paths, as well as the list of the
corresponding mask file paths:</p>
<figure>
<pre><code class="language-python">import pathlib

input_dir = pathlib.Path("images")
target_dir = pathlib.Path("annotations/trimaps")

input_img_paths = sorted(input_dir.glob("*.jpg"))
# Ignores some spurious files in the trimaps directory that start with
# a "."
target_paths = sorted(target_dir.glob("[!.]*.png"))
</code></pre>
</figure>

<p>Now, what does one of these inputs and its mask look like? Let’s take a quick look (see figure 11.3).</p>
<figure>
<pre><code class="language-python">import matplotlib.pyplot as plt
from keras.utils import load_img, img_to_array, array_to_img

plt.axis("off")
# Displays input image number 9
plt.imshow(load_img(input_img_paths[9]))
</code></pre>
</figure>

<figure id="figure-11-3">
<img src="../Images/539e2f264128e5b1ac6b451aa3d2e9a4.png" data-original-src="https://deeplearningwithpython.io/images/ch11/segmentation_input.d246cf5a.png"/>
<figcaption>
<a href="#figure-11-3">Figure 11.3</a>: An example image
</figcaption>
</figure>

<p>Let’s look at its target mask as well (see figure 11.4):</p>
<figure>
<pre><code class="language-python">def display_target(target_array):
    # The original labels are 1, 2, and 3. We subtract 1 so that the
    # labels range from 0 to 2, and then we multiply by 127 so that the
    # labels become 0 (black), 127 (gray), 254 (near-white).
    normalized_array = (target_array.astype("uint8") - 1) * 127
    plt.axis("off")
    plt.imshow(normalized_array[:, :, 0])

# We use color_mode='grayscale' so that the image we load is treated as
# having a single color channel.
img = img_to_array(load_img(target_paths[9], color_mode="grayscale"))
display_target(img)
</code></pre>
</figure>

<figure id="figure-11-4">
<img src="../Images/2686b58935fc0c0e5d28d781497c7142.png" data-original-src="https://deeplearningwithpython.io/images/ch11/segmentation_mask.cc320651.png"/>
<figcaption>
<a href="#figure-11-4">Figure 11.4</a>: The corresponding target mask
</figcaption>
</figure>

<p>Next, let’s load our inputs and targets into two NumPy arrays. Since the
dataset is very small, we can load everything into memory:</p>
<figure>
<pre><code class="language-python">import numpy as np
import random

# We resize everything to 200 x 200 for this example.
img_size = (200, 200)
# Total number of samples in the data
num_imgs = len(input_img_paths)

# Shuffles the file paths (they were originally sorted by breed). We
# use the same seed (1337) in both statements to ensure that the input
# paths and target paths stay in the same order.
random.Random(1337).shuffle(input_img_paths)
random.Random(1337).shuffle(target_paths)

def path_to_input_image(path):
    return img_to_array(load_img(path, target_size=img_size))

def path_to_target(path):
    img = img_to_array(
        load_img(path, target_size=img_size, color_mode="grayscale")
    )
    # Subtracts 1 so that our labels become 0, 1, and 2
    img = img.astype("uint8") - 1
    return img

# Loads all images in the input_imgs float32 array and their masks in
# the targets uint8 array (same order). The inputs have three channels
# (RGB values), and the targets have a single channel (which contains
# integer labels).
input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype="float32")
targets = np.zeros((num_imgs,) + img_size + (1,), dtype="uint8")
for i in range(num_imgs):
    input_imgs[i] = path_to_input_image(input_img_paths[i])
    targets[i] = path_to_target(target_paths[i])
</code></pre>
</figure>

<p>As always, let’s split the arrays into a training and a validation set:</p>
<figure>
<pre><code class="language-python"># Reserves 1,000 samples for validation
num_val_samples = 1000
# Splits the data into a training and a validation set
train_input_imgs = input_imgs[:-num_val_samples]
train_targets = targets[:-num_val_samples]
val_input_imgs = input_imgs[-num_val_samples:]
val_targets = targets[-num_val_samples:]
</code></pre>
</figure>

<h3 id="building-and-training-the-segmentation-model">Building and training the segmentation model</h3>
<p>Now, it’s time to define our model:</p>
<figure>
<pre><code class="language-python">import keras
from keras.layers import Rescaling, Conv2D, Conv2DTranspose

def get_model(img_size, num_classes):
    inputs = keras.Input(shape=img_size + (3,))
    # Don't forget to rescale input images to the [0–1] range.
    x = Rescaling(1.0 / 255)(inputs)

    # We use padding="same" everywhere to avoid the influence of border
    # padding on feature map size.
    x = Conv2D(64, 3, strides=2, activation="relu", padding="same")(x)
    x = Conv2D(64, 3, activation="relu", padding="same")(x)
    x = Conv2D(128, 3, strides=2, activation="relu", padding="same")(x)
    x = Conv2D(128, 3, activation="relu", padding="same")(x)
    x = Conv2D(256, 3, strides=2, padding="same", activation="relu")(x)
    x = Conv2D(256, 3, activation="relu", padding="same")(x)

    x = Conv2DTranspose(256, 3, activation="relu", padding="same")(x)
    x = Conv2DTranspose(256, 3, strides=2, activation="relu", padding="same")(x)
    x = Conv2DTranspose(128, 3, activation="relu", padding="same")(x)
    x = Conv2DTranspose(128, 3, strides=2, activation="relu", padding="same")(x)
    x = Conv2DTranspose(64, 3, activation="relu", padding="same")(x)
    x = Conv2DTranspose(64, 3, strides=2, activation="relu", padding="same")(x)

    # We end the model with a per-pixel three-way softmax to classify
    # each output pixel into one of our three categories.
    outputs = Conv2D(num_classes, 3, activation="softmax", padding="same")(x)

    return keras.Model(inputs, outputs)

model = get_model(img_size=img_size, num_classes=3)
</code></pre>
</figure>

<p>The first half of the model closely resembles the kind of ConvNet you’d use for image
classification: a stack of <code>Conv2D</code> layers, with gradually increasing filter sizes.
We downsample our images three times by a factor of
two each — ending up with activations of size
<code>(25, 25, 256)</code>. The purpose of this first half is to encode the images into
smaller feature maps, where each spatial location (or “pixel”) contains
information about a large spatial chunk of the original image. You can understand
it as a kind of compression.</p>
<p>One important difference between the first half of this model and the classification models
you’ve seen before is the way we do downsampling: in the classification ConvNets
from chapter 8, we used <code>MaxPooling2D</code> layers to downsample feature maps.
Here, we downsample by adding <em>strides</em> to every other convolution layer (if you
don’t remember the details of how convolution strides work, see
chapter 8, section 8.1.1). We do this because,
in the case of image segmentation, we care a lot about the spatial location of
information in the image since we need to produce per-pixel target masks as
output of the model. When you do 2 × 2 max pooling, you are completely destroying
location information within each pooling window: you return one scalar value
per window, with zero knowledge of which of the four locations in the windows
the value came from. </p>
<p>So, while max pooling layers perform well for
classification tasks, they would hurt us quite a bit for a segmentation task.
Meanwhile, strided convolutions do a better job at downsampling feature maps
while retaining location information. Throughout this book, you’ll notice that
we tend to use strides instead of max pooling in any model that cares about
feature location, such as the generative models in chapter 17.</p>
<p>The second half of the model is a stack of <code>Conv2DTranspose</code> layers. What are those?
Well, the output of the first half of the model is a feature map of shape <code>(25, 25, 256)</code>,
but we want our final output to predict a class for each pixel, matching the original
spatial dimensions. The final model output will have shape <code>(200, 200, num_classes)</code>,
which is <code>(200, 200, 3)</code> here. Therefore, we need to apply a kind of <em>inverse</em> of the
transformations we’ve applied so far, something that will <em>upsample</em> the feature
maps instead of downsampling them. That’s the purpose of the <code>Conv2DTranspose</code> layer:
you can think of it as a kind of convolution layer that <em>learns to upsample</em>.
If you have an input of shape <code>(100, 100, 64)</code> and you run it
through the layer <code>Conv2D(128, 3, strides=2, padding="same")</code>, you get an
output of shape <code>(50, 50, 128)</code>. If you run this output through the layer
<code>Conv2DTranspose(64, 3, strides=2, padding="same")</code>, you get back an output
of shape <code>(100, 100, 64)</code>, the same as the original. So after compressing
our inputs into feature maps of shape <code>(25, 25, 256)</code> via a stack of <code>Conv2D</code>
layers, we can simply apply the corresponding sequence of <code>Conv2DTranspose</code>
layers followed by a final <code>Conv2D</code> layer to produce outputs of shape <code>(200, 200, 3)</code>.</p>
<p>To evaluate the model, we’ll use a metric named
<em>Intersection over Union</em> (IoU). It’s a measure of the match between the ground truth
segmentation masks and the predicted masks. It can be computed separately for each class or averaged over multiple classes. Here’s how it works:</p>
<ol>
<li>Compute the <em>intersection</em> between the masks, the area where the prediction and ground truth overlap.</li>
<li>Compute the <em>union</em> of the masks, the total area covered by both masks combined. This is the whole space we’re interested in — the target object and any extra bits your model might have included by mistake.</li>
<li>Divide the intersection area by the union area to get the IoU. It’s a number between 0 and 1, where 1 denotes a perfect match, and 0 denotes a complete miss.</li>
</ol>
<p>We can simply use a built-in Keras metric rather than building this ourselves:</p>
<figure>
<pre><code class="language-python">foreground_iou = keras.metrics.IoU(
    # Specifies the total number of classes
    num_classes=3,
    # Specifies the class to compute IoU for (0 = foreground)
    target_class_ids=(0,),
    name="foreground_iou",
    # Our targets are sparse (integer class IDs).
    sparse_y_true=True,
    # But our model's predictions are a dense softmax!
    sparse_y_pred=False,
)
</code></pre>
</figure>

<p>We can now compile and fit our model:</p>
<figure>
<pre><code class="language-python">model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=[foreground_iou],
)
callbacks = [
    keras.callbacks.ModelCheckpoint(
        "oxford_segmentation.keras",
        save_best_only=True,
    ),
]
history = model.fit(
    train_input_imgs,
    train_targets,
    epochs=50,
    callbacks=callbacks,
    batch_size=64,
    validation_data=(val_input_imgs, val_targets),
)
</code></pre>
</figure>

<p>Let’s display our training and validation loss (see figure 11.5):</p>
<figure>
<pre><code class="language-python">epochs = range(1, len(history.history["loss"]) + 1)
loss = history.history["loss"]
val_loss = history.history["val_loss"]
plt.figure()
plt.plot(epochs, loss, "r--", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
</code></pre>
</figure>

<figure id="figure-11-5">
<img src="../Images/9f64353f124760b76a5383248fac2a7f.png" data-original-src="https://deeplearningwithpython.io/images/ch11/segmentation_loss.489fa0c8.png"/>
<figcaption>
<a href="#figure-11-5">Figure 11.5</a>: Displaying training and validation loss curves
</figcaption>
</figure>

<p>You can see that we start overfitting midway, around epoch 25. Let’s reload
our best-performing model according to validation loss and demonstrate
how to use it to predict a segmentation mask (see figure 11.6):</p>
<figure>
<pre><code class="language-python">model = keras.models.load_model("oxford_segmentation.keras")

i = 4
test_image = val_input_imgs[i]
plt.axis("off")
plt.imshow(array_to_img(test_image))

mask = model.predict(np.expand_dims(test_image, 0))[0]

# Utility to display a model's prediction
def display_mask(pred):
    mask = np.argmax(pred, axis=-1)
    mask *= 127
    plt.axis("off")
    plt.imshow(mask)

display_mask(mask)
</code></pre>
</figure>

<figure id="figure-11-6">
<img src="../Images/5df726de6b0d2524f74f492e8d42c2d1.png" data-original-src="https://deeplearningwithpython.io/images/ch11/segmentation_test.ece7f638.png"/>
<figcaption>
<a href="#figure-11-6">Figure 11.6</a>: A test image and its predicted segmentation mask
</figcaption>
</figure>

<p>There are a couple of small artifacts in our predicted mask, caused by geometric shapes
in the foreground and background. Nevertheless, our model appears to work nicely.</p>
<h2 id="using-a-pretrained-segmentation-model">Using a pretrained segmentation model</h2>
<p>In the image classification example from chapter 8, you saw how using a
pretrained model could significantly boost your accuracy — especially when you
only have a few samples to train on. Image segmentation is no different.</p>
<p>The <em>Segment Anything Model</em>,<sup class="footnote-link" id="footnote-link-1"><a href="#footnote-1">[1]</a></sup> or SAM for short, is a powerful pretrained
segmentation model you can use for, well, almost anything. It was developed by
Meta AI and released in April 2023. It was trained on 11 million images and
their segmentation masks, covering over 1 billion object instances. This
massive amount of training data provides the model with built-in knowledge of
virtually any object that appears in natural images.</p>
<p>The main innovation of SAM is that it’s not limited to a
predefined set of object classes. You can use it for segmenting new objects
simply by providing an example of what you’re looking for. You don’t even need
to fine-tune the model first. Let’s see how that works.</p>
<h3 id="downloading-the-segment-anything-model">Downloading the Segment Anything Model</h3>
<p>First, let’s instantiate SAM and download its weights.
Once again, we can use the KerasHub package to use this pretrained model without
needing to implement it ourselves from scratch.</p>
<p>Remember the <code>ImageClassifier</code> task we used in the previous chapter? We can use another
KerasHub task <code>ImageSegmenter</code> for wrapping pretrained image segmentation models
into a high-level model with standard inputs and outputs. Here, we’ll use the
<code>sam_huge_sa1b</code> pretrained model, where <code>sam</code> stands for the model, <code>huge</code>
refers to the number of parameters in the model, and <code>sa1b</code> stands for the SA-1B
dataset released along with the model, with 1 billion annotated masks. Let’s
download it now:</p>
<figure>
<pre><code class="language-python">import keras_hub

model = keras_hub.models.ImageSegmenter.from_preset("sam_huge_sa1b")
</code></pre>
</figure>

<p>One thing we can note off the bat is that our model is, indeed, huge:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; model.count_params()</code>
<code class="language-output">641090864</code></pre>
</figure>

<p>At 641 million parameters, SAM is the largest model we have used so far in
this book. The trend of pretrained models getting larger and larger and using more
and more data will be discussed in more detail in chapter 16.</p>
<h3 id="how-segment-anything-works">How Segment Anything works</h3>
<p>Before we try running some segmentation with the model, let’s talk a little more
about how SAM works. Much of the capability of the model comes from the
scale of the pretraining dataset. Meta developed the SA-1B dataset along with
the model, where the partially trained model was used to assist with the data
labeling process. That is, the dataset and model were developed together in
a feedback loop of sorts.</p>
<p>The goal with the SA-1B dataset is to create fully segmented images, where every
object in an image is given a unique segmentation mask. See figure 11.7 as an
example. Each image in the dataset has ~100 masks on average, and some images
have over 500 individually masked objects. This was done through a pipeline of
increasingly automated data collection. At first, human experts manually
segmented a small example dataset of images, which was used to train an initial
model. This model was used to help drive a semiautomated stage of data
collection, where images were first segmented by SAM and improved by human
correction and further annotation.</p>
<figure id="figure-11-7" class="large-image">
<img src="../Images/f7fee6b67ad6bf846f8c26fdfbde4e3e.png" data-original-src="https://deeplearningwithpython.io/images/ch11/sa1b_example.6701768b.jpg"/>
<figcaption>
<a href="#figure-11-7">Figure 11.7</a>: An example image from the SA-1B dataset
</figcaption>
</figure>

<p>The model is trained on <code>(image, prompt, mask)</code> triples. <code>image</code> and <code>prompt</code>
are the model inputs. The image can be any input image, and the prompt can take a
couple of forms:</p>
<ul>
<li>A point inside the object to mask</li>
<li>A box around the object to mask</li>
</ul>
<p>Given the <code>image</code> and <code>prompt</code> input, the model is expected to produce an
accurate predicted mask for the object indicated by the prompt, which is
compared with a ground truth <code>mask</code> label.</p>
<p>The model consists of a few separate components. An image encoder similar to
the Xception model we used in previous chapters, will take an input
image and output a much smaller image embedding. This is something we already
know how to build.</p>
<p>Next, we add a prompt encoder, which is responsible for mapping prompts in any of
the previously mentioned forms to an embedded vector, and a mask decoder, which takes in
both the image embedding and prompt embedding and outputs a few possible
predicted masks. We won’t get into the details of the prompt encoder and mask
decoder here, as they use some modeling techniques we won’t see until later
chapters. We can compare these predicted masks with our ground truth mask much like we did in
the earlier section of this chapter (see figure 11.8).</p>
<figure id="figure-11-8">
<img src="../Images/02a451fa68c4d16b5f76ddfbd8d7e7e4.png" data-original-src="https://deeplearningwithpython.io/images/ch11/sam_architecture.dad9dae6.png"/>
<figcaption>
<a href="#figure-11-8">Figure 11.8</a>: The Segment Anything high-level architecture overview
</figcaption>
</figure>

<p>All of these subcomponents are trained simultaneously by forming batches of new
<code>(image, prompt, mask)</code> triples to train on from the SA-1B image and mask data.
The process here is actually quite simple. For a given input image, choose a
random mask in the input. Next, randomly choose whether to create a box prompt
or a point prompt. To create a point prompt, choose a random pixel inside the
mask label. To create a box prompt, draw a box around all points inside
the mask label. We can repeat this process indefinitely, sampling a number of
<code>(image, prompt, mask)</code> triples from each image input.</p>
<h3 id="preparing-a-test-image">Preparing a test image</h3>
<p>Let’s make this a little more concrete by trying the model out. We can start by
loading a test image for our segmentation work. We’ll use a picture of a bowl of
fruits (see figure 11.9):</p>
<figure>
<pre><code class="language-python"># Downloads the image and returns the local file path
path = keras.utils.get_file(
    origin="https://s3.amazonaws.com/keras.io/img/book/fruits.jpg"
)
# Loads the image as a Python Imaging Library (PIL) object
pil_image = keras.utils.load_img(path)
# Turns the PIL object into a NumPy matrix
image_array = keras.utils.img_to_array(pil_image)

# Displays the NumPy matrix
plt.imshow(image_array.astype("uint8"))
plt.axis("off")
plt.show()
</code></pre>
</figure>

<figure id="figure-11-9">
<img src="../Images/cb3ddcfe0d2fd3d54cc924ac0bb4ea5c.png" data-original-src="https://deeplearningwithpython.io/images/ch11/fruits.8cef44dc.png"/>
<figcaption>
<a href="#figure-11-9">Figure 11.9</a>: Our test image
</figcaption>
</figure>

<p>SAM expects inputs that are 1024 × 1024. However, forcibly resizing arbitrary images
to 1024 × 1024 would distort their aspect ratio — for instance, our image isn’t square.
It’s better to first resize the image so that its longest side becomes 1,024 pixels long and then pad
the remaining pixels with a filler value, such as 0. We can achieve this with the <code>pad_to_aspect_ratio</code>
argument in the <code>keras.ops.image.resize()</code> operation, like this:</p>
<figure>
<pre><code class="language-python">from keras import ops

image_size = (1024, 1024)

def resize_and_pad(x):
    return ops.image.resize(x, image_size, pad_to_aspect_ratio=True)

image = resize_and_pad(image_array)
</code></pre>
</figure>

<p>Next, let’s define a few utilities that will come in handy when using the model.
We’re going to need to</p>
<ul>
<li>Display images.</li>
<li>Display segmentation masks overlaid on an image.</li>
<li>Highlight specific points on an image.</li>
<li>Display boxes overlaid on an image.</li>
</ul>
<p>All our utilities take a Matplotlib <code>axis</code> object (noted <code>ax</code>) so that they can all write to the same figure:</p>
<figure>
<pre><code class="language-python">import matplotlib.pyplot as plt
from keras import ops

def show_image(image, ax):
    ax.imshow(ops.convert_to_numpy(image).astype("uint8"))

def show_mask(mask, ax):
    color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])
    h, w, _ = mask.shape
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

def show_points(points, ax):
    x, y = points[:, 0], points[:, 1]
    ax.scatter(x, y, c="green", marker="*", s=375, ec="white", lw=1.25)

def show_box(box, ax):
    box = box.reshape(-1)
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, ec="red", fc="none", lw=2))
</code></pre>
</figure>

<h3 id="prompting-the-model-with-a-target-point">Prompting the model with a target point</h3>
<p>To use SAM, you need to prompt it. This means we need one of the following:</p>
<ul>
<li><em>Point prompts</em> — Select a point in an image and let the model segment the object that the point belongs to.</li>
<li><em>Box prompts</em> — Draw an approximate box around an object (it does not need to be particularly precise) and let the model segment the object in the box.</li>
</ul>
<p>Let’s start with a point prompt.
Points are labeled, with 1 indicating the foreground (the object you want to
segment) and 0 indicating the background (everything around the object). In
ambiguous cases, to improve your results, you could pass multiple labeled points,
instead of a single point, to refine your definition of what should be included
(points labeled 1) and what should be excluded (points labeled 0).</p>
<p>We try a single foreground point (see figure 11.10). Here’s a test point:</p>
<figure>
<pre><code class="language-python">import numpy as np

# Coordinates of our point
input_point = np.array([[580, 450]])
# 1 means foreground, and 0 means background.
input_label = np.array([1])

plt.figure(figsize=(10, 10))
# "gca" means "get current axis" — the current figure.
show_image(image, plt.gca())
show_points(input_point, plt.gca())
plt.show()
</code></pre>
</figure>

<figure id="figure-11-10">
<img src="../Images/1fb3211965623f1112b05761214c02f1.png" data-original-src="https://deeplearningwithpython.io/images/ch11/peach_point.432d548a.png"/>
<figcaption>
<a href="#figure-11-10">Figure 11.10</a>: A prompt point, landing on a peach
</figcaption>
</figure>

<p>Let’s prompt SAM with it:</p>
<figure>
<pre><code class="language-python">outputs = model.predict(
    {
        "images": ops.expand_dims(image, axis=0),
        "points": ops.expand_dims(input_point, axis=0),
        "labels": ops.expand_dims(input_label, axis=0),
    }
)
</code></pre>
</figure>

<p>The return value <code>outputs</code> has a <code>"masks"</code> field which contains four 256 × 256 candidate masks for the target object,
ranked by decreasing match quality. The quality scores of the masks are available under the <code>"iou_pred"</code> field as part of the model’s output:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; outputs["masks"].shape</code>
<code class="language-output">(1, 4, 256, 256)</code></pre>
</figure>

<p>Let’s overlay the first mask on the image (see figure 11.11):</p>
<figure>
<pre><code class="language-python">def get_mask(sam_outputs, index=0):
    mask = sam_outputs["masks"][0][index]
    mask = np.expand_dims(mask, axis=-1)
    mask = resize_and_pad(mask)
    return ops.convert_to_numpy(mask) &gt; 0.0

mask = get_mask(outputs, index=0)

plt.figure(figsize=(10, 10))
show_image(image, plt.gca())
show_mask(mask, plt.gca())
show_points(input_point, plt.gca())
plt.show()
</code></pre>
</figure>

<figure id="figure-11-11">
<img src="../Images/9a04a54d693d52d5d46a0779a32ef31d.png" data-original-src="https://deeplearningwithpython.io/images/ch11/peach_segmented.333556ff.png"/>
<figcaption>
<a href="#figure-11-11">Figure 11.11</a>: Segmented peach
</figcaption>
</figure>

<p>Pretty good!</p>
<p>Next, let’s try a banana. We’ll prompt the model with coordinates <code>(300, 550)</code>,
which land on the second banana from the left (see figure 11.12):</p>
<figure>
<pre><code class="language-python">input_point = np.array([[300, 550]])
input_label = np.array([1])

outputs = model.predict(
    {
        "images": ops.expand_dims(image, axis=0),
        "points": ops.expand_dims(input_point, axis=0),
        "labels": ops.expand_dims(input_label, axis=0),
    }
)
mask = get_mask(outputs, index=0)

plt.figure(figsize=(10, 10))
show_image(image, plt.gca())
show_mask(mask, plt.gca())
show_points(input_point, plt.gca())
plt.show()
</code></pre>
</figure>

<figure id="figure-11-12">
<img src="../Images/d57b293d144b43710f13bcb5dc407813.png" data-original-src="https://deeplearningwithpython.io/images/ch11/banana_segmented.8e0b3e81.png"/>
<figcaption>
<a href="#figure-11-12">Figure 11.12</a>: Segmented banana
</figcaption>
</figure>

<p>Now, what about the other mask candidates? Those can come in handy for ambiguous prompts. Let’s try to plot the other three masks (see figure 11.13):</p>
<figure>
<pre><code class="language-python">fig, axes = plt.subplots(1, 3, figsize=(20, 60))
masks = outputs["masks"][0][1:]
for i, mask in enumerate(masks):
    show_image(image, axes[i])
    show_points(input_point, axes[i])
    mask = get_mask(outputs, index=i + 1)
    show_mask(mask, axes[i])
    axes[i].set_title(f"Mask {i + 1}", fontsize=16)
    axes[i].axis("off")
plt.show()
</code></pre>
</figure>

<figure id="figure-11-13">
<img src="../Images/2825d36b86ac06b70d4c188bdeeb0383.png" data-original-src="https://deeplearningwithpython.io/images/ch11/bananas_all_masks.c922b7a6.png"/>
<figcaption>
<a href="#figure-11-13">Figure 11.13</a>: Alternative segmentation masks for the banana prompt
</figcaption>
</figure>

<p>As you can see here, an alternative segmentation found by the model includes both bananas.</p>
<h3 id="prompting-the-model-with-a-target-box">Prompting the model with a target box</h3>
<p>Besides providing one or more target points, you can also provide boxes approximating the location of the object to segment.
These boxes should be passed via the coordinates of their top-left and bottom-right corners. Here’s a box around the mango (see figure 11.14):</p>
<figure>
<pre><code class="language-python">input_box = np.array(
    [
        # Top-left corner
        [520, 180],
        # Bottom-right corner
        [770, 420],
    ]
)

plt.figure(figsize=(10, 10))
show_image(image, plt.gca())
show_box(input_box, plt.gca())
plt.show()
</code></pre>
</figure>

<figure id="figure-11-14">
<img src="../Images/611f443ab648ad48cdc642f1c265fd6e.png" data-original-src="https://deeplearningwithpython.io/images/ch11/mango_box.45e1bae1.png"/>
<figcaption>
<a href="#figure-11-14">Figure 11.14</a>: Box prompt around the mango
</figcaption>
</figure>

<p>Let’s prompt SAM with it (see figure 11.15):</p>
<figure>
<pre><code class="language-python">outputs = model.predict(
    {
        "images": ops.expand_dims(image, axis=0),
        "boxes": ops.expand_dims(input_box, axis=(0, 1)),
    }
)
mask = get_mask(outputs, 0)
plt.figure(figsize=(10, 10))
show_image(image, plt.gca())
show_mask(mask, plt.gca())
show_box(input_box, plt.gca())
plt.show()
</code></pre>
</figure>

<figure id="figure-11-15">
<img src="../Images/afe8dd8fc5b03bf9c605eb1d054321d6.png" data-original-src="https://deeplearningwithpython.io/images/ch11/mango_segmented.2dfb0dae.png"/>
<figcaption>
<a href="#figure-11-15">Figure 11.15</a>: Segmented mango
</figcaption>
</figure>

<p>SAM can be a powerful tool to quickly create large datasets of images annotated with segmentation masks.</p>
<h2 id="summary">Summary</h2>
<ul>
<li>Image segmentation is one of the main categories of computer vision tasks. It consists
of computing segmentation masks that describe the contents of an image at the pixel level.</li>
<li>To build your own segmentation model, use a stack of strided <code>Conv2D</code> layers to “compress”
the input image into a smaller feature map, followed by a stack of corresponding <code>Conv2DTranspose</code>
layers to “expand” the feature map into a segmentation mask the same size as the input image.</li>
<li>You can also use a pretrained segmentation model. Segment Anything, included in KerasHub, is a powerful model that supports
image prompting, text prompting, point prompting, and box prompting.</li>
</ul>

&#13;

  <h3>Footnotes</h3>
  <ol>

    <li id="footnote-1">
      Kirillov et al., “Segment Anything,” in <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, arXiv (2023), <a href="https://arxiv.org/abs/2304.02643">https://arxiv.org/abs/2304.02643</a>.
      <a class="footnote-backlink" href="#footnote-link-1">[↩]</a>
    </li>

  </ol>
    
</body>
</html>