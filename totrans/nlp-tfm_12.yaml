- en: Chapter 11\. Future Directions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。未来方向
- en: Throughout this book we’ve explored the powerful capabilities of transformers
    across a wide range of NLP tasks. In this final chapter, we’ll shift our perspective
    and look at some of the current challenges with these models and the research
    trends that are trying to overcome them. In the first part we explore the topic
    of scaling up transformers, both in terms of model and corpus size. Then we turn
    our attention toward various techniques that have been proposed to make the self-attention
    mechanism more efficient. Finally, we explore the emerging and exciting field
    of *multimodal transformers*, which can model inputs across multiple domains like
    text, images, and audio.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们探讨了变压器在各种自然语言处理任务中的强大能力。在本章中，我们将转变视角，看看这些模型目前面临的挑战以及试图克服这些挑战的研究趋势。在第一部分中，我们探讨了变压器的扩展问题，包括模型和语料库的规模。然后我们将注意力转向各种提出的技术，以使自注意机制更加高效。最后，我们将探讨新兴且令人兴奋的*多模态变压器*领域，该领域可以模拟跨多个领域的输入，如文本、图像和音频。
- en: Scaling Transformers
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展变压器
- en: 'In 2019, the researcher [Richard Sutton](https://oreil.ly/119br) wrote a provocative
    essay entitled [“The Bitter Lesson”](https://oreil.ly/YtD3V) in which he argued
    that:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，研究人员[Richard Sutton](https://oreil.ly/119br)撰写了一篇引人深思的文章，题为[“苦涩的教训”](https://oreil.ly/YtD3V)，在文中他认为：
- en: The biggest lesson that can be read from 70 years of AI research is that general
    methods that leverage computation are ultimately the most effective, and by a
    large margin…. Seeking an improvement that makes a difference in the shorter term,
    researchers seek to leverage their human knowledge of the domain, but the only
    thing that matters in the long run is the leveraging of computation. These two
    need not run counter to each other, but in practice they tend to…. And the human-knowledge
    approach tends to complicate methods in ways that make them less suited to taking
    advantage of general methods leveraging computation.
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从70年的人工智能研究中可以得出的最重要的教训是，利用计算的通用方法最终是最有效的，而且效果显著…… 寻求在短期内产生影响的改进，研究人员试图利用他们对领域的人类知识，但从长远来看，唯一重要的是利用计算。这两者不一定相互对立，但在实践中往往如此……
    而人类知识方法往往会使方法变得更加复杂，从而使它们不太适合利用计算的通用方法。
- en: 'The essay provides several historical examples, such as playing chess or Go,
    where the approach of encoding human knowledge within AI systems was ultimately
    outdone by increased computation. Sutton calls this the “bitter lesson” for the
    AI research field:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章提供了几个历史性的例子，比如下棋或围棋，在这些例子中，将人类知识编码到人工智能系统中的方法最终被增加的计算所超越。Sutton将这称为人工智能研究领域的“苦涩教训”：
- en: We have to learn the bitter lesson that building in how we think we think does
    not work in the long run…. One thing that should be learned from the bitter lesson
    is the great power of general purpose methods, of methods that continue to scale
    with increased computation even as the available computation becomes very great.
    The two methods that seem to scale arbitrarily in this way are *search* and *learning*.
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们必须学会苦涩的教训，即在长期内构建我们认为的思维方式是行不通的…… 从苦涩的教训中应该学到的一件事是通用方法的巨大威力，这些方法随着计算的增加而继续扩展，即使可用的计算变得非常庞大。似乎以这种方式任意扩展的两种方法是*搜索*和*学习*。
- en: There are now signs that a similar lesson is at play with transformers; while
    many of the early BERT and GPT descendants focused on tweaking the architecture
    or pretraining objectives, the best-performing models in mid-2021, like GPT-3,
    are essentially basic scaled-up versions of the original models without many architectural
    modifications. In [Figure 11-1](#parameter-counts) you can see a timeline of the
    development of the largest models since the release of the original Transformer
    architecture in 2017, which shows that model size has increased by over four orders
    of magnitude in just a few years!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有迹象表明，变压器也在发挥类似的作用；虽然早期的BERT和GPT后代中有许多关注调整架构或预训练目标的模型，但到了2021年中期，表现最佳的模型，如GPT-3，实质上是原始模型的基本放大版本，没有太多的架构修改。在[图11-1](#parameter-counts)中，您可以看到自2017年原始变压器架构发布以来最大模型的发展时间线，这表明模型大小在短短几年内增加了四个数量级！
- en: '![Language model parameter counts](Images/nlpt_1101.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![语言模型参数计数](Images/nlpt_1101.png)'
- en: Figure 11-1\. Parameter counts over time for prominent Transformer architectures
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1。突出的变压器架构随时间的参数计数
- en: This dramatic growth is motivated by empirical evidence that large language
    models perform better on downstream tasks and that interesting capabilities such
    as zero-shot and few-shot learning emerge in the 10- to 100-billion parameter
    range. However, the number of parameters is not the only factor that affects model
    performance; the amount of compute and training data must also be scaled in tandem
    to train these monsters. Given that large language models like GPT-3 are estimated
    to cost [$4.6 million](https://oreil.ly/DUVcq) to train, it is clearly desirable
    to be able to estimate the model’s performance in advance. Somewhat surprisingly,
    the performance of language models appears to obey a *power law relationship*
    with model size and other factors that is codified in a set of scaling laws.^([1](ch11.xhtml#idm46238686826448))
    Let’s take a look at this exciting area of research.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种戏剧性的增长是基于经验证据，即大型语言模型在下游任务上表现更好，并且在100亿到1000亿参数范围内出现了零-shot和少-shot学习等有趣的能力。然而，参数数量并不是影响模型性能的唯一因素；计算量和训练数据的数量也必须同时扩展以训练这些庞然大物。鉴于像GPT-3这样的大型语言模型估计训练成本为[$4.6
    million](https://oreil.ly/DUVcq)，显然希望能够预先估计模型的性能。令人惊讶的是，语言模型的性能似乎服从于模型大小和其他因素的*幂律关系*，这一关系被编码在一组缩放定律中。^([1](ch11.xhtml#idm46238686826448))
    让我们来看看这个令人兴奋的研究领域。
- en: Scaling Laws
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放定律
- en: Scaling laws allow one to empirically quantify the “bigger is better” paradigm
    for language models by studying their behavior with varying compute budget *C*,
    dataset size *D*, and model size *N*.^([2](ch11.xhtml#idm46238686819248)) The
    basic idea is to chart the dependence of the cross-entropy loss *L* on these three
    factors and determine if a relationship emerges. For autoregressive models like
    those in the GPT family, the resulting loss curves are shown in [Figure 11-2](#scaling-laws),
    where each blue curve represents the training run of a single model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放定律允许通过研究语言模型在不同计算预算*C*、数据集大小*D*和模型大小*N*下的行为来经验性地量化“越大越好”的范式。^([2](ch11.xhtml#idm46238686819248))基本思想是绘制交叉熵损失*L*对这三个因素的依赖关系，并确定是否存在关系。对于像GPT系列中的自回归模型，得到的损失曲线如[图11-2](#scaling-laws)所示，其中每条蓝色曲线代表单个模型的训练运行。
- en: '![scaling-laws](Images/nlpt_1102.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![scaling-laws](Images/nlpt_1102.png)'
- en: Figure 11-2\. Power-law scaling of test loss versus compute budget (left), dataset
    size (middle), and model size (right) (courtesy of Jared Kaplan)
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2。测试损失与计算预算（左）、数据集大小（中）和模型大小（右）的幂律缩放（由Jared Kaplan提供）
- en: 'From these loss curves we can draw a few conclusions about:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些损失曲线中，我们可以得出一些关于以下内容的结论：
- en: The relationship of performance and scale
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 性能和规模的关系
- en: Although many NLP researchers focus on architectural tweaks or hyperparameter
    optimization (like tuning the number of layers or attention heads) to improve
    performance on a fixed set of datasets, the implication of scaling laws is that
    a more productive path toward better models is to focus on increasing *N*, *C*,
    and *D* in tandem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多自然语言处理研究人员专注于架构调整或超参数优化（如调整层数或注意力头的数量）以提高固定数据集上的性能，但缩放定律的含义是，更好模型的更有效路径是同时专注于增加*N*、*C*和*D*。
- en: Smooth power laws
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑的幂律
- en: The test loss *L* has a power law relationship with each of *N*, *C*, and *D*
    across several orders of magnitude (power law relationships are linear on a log-log
    scale). For <math alttext="upper X equals upper N comma upper C comma upper D"><mrow><mi>X</mi>
    <mo>=</mo> <mi>N</mi> <mo>,</mo> <mi>C</mi> <mo>,</mo> <mi>D</mi></mrow></math>
    we can express these power law relationships as <math alttext="upper L left-parenthesis
    upper X right-parenthesis tilde 1 slash upper X Superscript alpha"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>∼</mo> <mn>1</mn> <mo>/</mo>
    <msup><mi>X</mi> <mi>α</mi></msup></mrow></math> , where <math alttext="alpha"><mi>α</mi></math>
    is a scaling exponent that is determined by a fit to the loss curves shown in
    [Figure 11-2](#scaling-laws).^([3](ch11.xhtml#idm46238686793248)) Typical values
    for <math alttext="alpha Subscript upper X"><msub><mi>α</mi> <mi>X</mi></msub></math>
    lie in the 0.05–0.095 range, and one attractive feature of these power laws is
    that the early part of a loss curve can be extrapolated to predict what the approximate
    loss would be if training was conducted for much longer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 测试损失*L*与*N*、*C*和*D*之间存在幂律关系，跨越了几个数量级（幂律关系在对数-对数尺度上是线性的）。对于<math alttext="upper
    X equals upper N comma upper C comma upper D"><mrow><mi>X</mi> <mo>=</mo> <mi>N</mi>
    <mo>,</mo> <mi>C</mi> <mo>,</mo> <mi>D</mi></mrow></math>，我们可以将这些幂律关系表示为<math
    alttext="upper L left-parenthesis upper X right-parenthesis tilde 1 slash upper
    X Superscript alpha"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow>
    <mo>∼</mo> <mn>1</mn> <mo>/</mo> <msup><mi>X</mi> <mi>α</mi></msup></mrow></math>，其中<math
    alttext="alpha"><mi>α</mi></math>是一个由拟合[图11-2](#scaling-laws)中所示的损失曲线确定的缩放指数。
    <math alttext="alpha Subscript upper X"><msub><mi>α</mi> <mi>X</mi></msub></math>的典型值在0.05-0.095范围内，这些幂律的一个吸引人的特征是，损失曲线的早期部分可以外推，以预测如果训练时间更长，近似损失会是多少。
- en: Sample efficiency
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 样本效率
- en: Large models are able to reach the same performance as smaller models with a
    smaller number of training steps. This can be seen by comparing the regions where
    a loss curve plateaus over some number of training steps, which indicates one
    gets diminishing returns in performance compared to simply scaling up the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大型模型能够在较少的训练步骤下达到与较小模型相同的性能。通过比较损失曲线在一定数量的训练步骤后趋于平稳的区域，可以看出性能的收益递减，与简单地扩大模型相比。
- en: Somewhat surprisingly, scaling laws have also been observed for other modalities,
    like images, videos, and mathematical problem solving, as illustrated in [Figure 11-3](#scaling-laws-modal).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，幂律定律也被观察到适用于其他模态，如图像、视频和数学问题解决，如[图11-3](#scaling-laws-modal)所示。
- en: '![scaling-laws-modal](Images/nlpt_1103.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![scaling-laws-modal](Images/nlpt_1103.png)'
- en: Figure 11-3\. Power-law scaling of test loss versus compute budget across a
    wide range of modalities (courtesy of Tom Henighan)
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-3。测试损失与计算预算的幂律缩放跨越各种模态（由Tom Henighan提供）
- en: Whether power-law scaling is a universal property of transformer language models
    is currently unknown. For now, we can use scaling laws as a tool to extrapolate
    large, expensive models without having to explicitly train them. However, scaling
    isn’t quite as easy as it sounds. Let’s now look at a few challenges that crop
    up when charting this frontier.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器语言模型的幂律缩放是否是普遍属性目前尚不清楚。目前，我们可以将缩放定律作为一种工具，来外推大型昂贵的模型，而无需明确训练它们。然而，缩放并不像听起来那么简单。现在让我们来看看在绘制这一前沿时会遇到的一些挑战。
- en: Challenges with Scaling
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放的挑战
- en: 'While scaling up sounds simple in theory (“just add more layers!”), in practice
    there are many difficulties. Here are a few of the biggest challenges you’re likely
    to encounter when scaling language models:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在理论上扩大规模听起来很简单（“只需添加更多层！”），但在实践中存在许多困难。以下是在扩大语言模型规模时可能遇到的一些最大挑战：
- en: Infrastructure
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施
- en: Provisioning and managing infrastructure that potentially spans hundreds or
    thousands of nodes with as many GPUs is not for the faint-hearted. Are the required
    number of nodes available? Is communication between nodes a bottleneck? Tackling
    these issues requires a very different skill set than that found in most data
    science teams, and typically involves specialized engineers familiar with running
    large-scale, distributed experiments.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 部署和管理潜在涵盖数百甚至数千个节点以及同样数量的GPU的基础设施并非易事。所需数量的节点是否可用？节点之间的通信是否成为瓶颈？解决这些问题需要一种与大多数数据科学团队所具备的技能集非常不同的技能，通常涉及熟悉运行大规模分布式实验的专业工程师。
- en: Cost
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 成本
- en: Most ML practitioners have experienced the feeling of waking up in the middle
    of the night in a cold sweat, remembering they forgot to shut down that fancy
    GPU on the cloud. This feeling intensifies when running large-scale experiments,
    and most companies cannot afford the teams and resources necessary to train models
    at the largest scales. Training a single GPT-3-sized model can cost several million
    dollars, which is not the kind of pocket change that many companies have lying
    around.^([4](ch11.xhtml#idm46238686775136))
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习从业者都曾经历过半夜惊醒的感觉，冷汗直冒，想起自己忘了关闭云端的那个花哨的GPU。在运行大规模实验时，这种感觉会加剧，大多数公司无法负担起培训最大规模模型所需的团队和资源。训练一个规模与GPT-3相当的模型可能会耗费数百万美元，这并不是许多公司随手就能拿得出的零花钱。
- en: Dataset curation
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集策划
- en: A model is only as good as the data it is trained on. Training large models
    requires large, high-quality datasets. When using terabytes of text data it becomes
    harder to make sure the dataset contains high-quality text, and even preprocessing
    becomes challenging. Furthermore, one needs to ensure that there is a way to control
    biases like sexism and racism that these language models can acquire when trained
    on large-scale webtext corpora. Another type of consideration revolves around
    licensing issues with the training data and personal information that can be embedded
    in large text datasets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的好坏取决于其训练所使用的数据。训练大型模型需要大规模、高质量的数据集。当使用数千兆字节的文本数据时，确保数据集包含高质量文本变得更加困难，甚至预处理也变得具有挑战性。此外，还需要确保有一种方法来控制这些语言模型在大规模网络文本语料库上训练时可能获得的性别歧视和种族歧视等偏见。另一种考虑类型围绕着训练数据的许可问题以及可能嵌入在大型文本数据集中的个人信息。
- en: Model evaluation
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估
- en: Once the model is trained, the challenges don’t stop. Evaluating the model on
    downstream tasks again requires time and resources. In addition, you’ll want to
    probe the model for biased and toxic generations, even if you are confident that
    you created a clean dataset. These steps take time and need to be carried out
    thoroughly to minimize the risks of adverse effects later on.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，挑战并没有结束。在下游任务上评估模型还需要时间和资源。此外，即使您确信自己创建了一个干净的数据集，您也会想要探索模型中的偏见和有毒生成。这些步骤需要时间，并且需要彻底进行，以尽量减少后续不良影响的风险。
- en: Deployment
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 部署
- en: Finally, serving large language models also poses a significant challenge. In
    [Chapter 8](ch08.xhtml#chapter_compression) we looked at a few approaches, such
    as distillation, pruning, and quantization, to help with these issues. However,
    this may not be enough if you are starting with a model that is hundreds of gigabytes
    in size. Hosted services such as the [OpenAI API](https://beta.openai.com) or
    Hugging Face’s [Accelerated Inference API](https://oreil.ly/E4q3b) are designed
    to help companies that cannot or do not want to deal with these deployment challenges.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为大型语言模型提供服务也带来了重大挑战。在第8章中，我们探讨了一些方法，如蒸馏、修剪和量化，来帮助解决这些问题。然而，如果您从一个数百GB大小的模型开始，这可能还不够。托管服务，如OpenAI
    API或Hugging Face的Accelerated Inference API，旨在帮助那些无法或不愿处理这些部署挑战的公司。
- en: 'This is by no means an exhaustive list, but it should give you an idea of the
    kinds of considerations and challenges that go hand in hand with scaling language
    models to ever larger sizes. While most of these efforts are centralized around
    a few institutions that have the resources and know-how to push the boundaries,
    there are currently two community-led projects that aim to produce and probe large
    language models in the open:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是一个详尽的清单，但它应该让你了解将语言模型扩展到更大规模时所面临的考虑和挑战的种类。虽然大多数这样的努力都集中在一些拥有资源和专业知识来推动界限的机构周围，但目前有两个社区主导的项目旨在公开产生和探索大型语言模型：
- en: BigScience
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: BigScience
- en: This is a one-year-long research workshop that runs from 2021 to 2022 and is
    focused on large language models. The workshop aims to foster discussions and
    reflections around the research questions surrounding these models (capabilities,
    limitations, potential improvements, bias, ethics, environmental impact, role
    in the general AI/cognitive research landscape) as well as the challenges around
    creating and sharing such models and datasets for research purposes and among
    the research community. The collaborative tasks involve creating, sharing, and
    evaluating a large multilingual dataset and a large language model. An unusually
    large compute budget was allocated for these collaborative tasks (several million
    GPU hours on several thousands GPUs). If successful, this workshop will run again
    in the future, focusing on involving an updated or different set of collaborative
    tasks. If you want to join the effort, you can find more information at the [project’s
    website](https://oreil.ly/13xfb).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个为期一年的研究研讨会，从2021年持续到2022年，重点是大型语言模型。该研讨会旨在促进围绕这些模型的研究问题（能力、限制、潜在改进、偏见、伦理、环境影响、在通用AI/认知研究领域中的作用）的讨论和反思，以及围绕为研究目的创建和共享这些模型和数据集以及在研究社区中的挑战。合作任务包括创建、共享和评估大型多语言数据集和大型语言模型。为这些合作任务分配了异常庞大的计算预算（数百万GPU小时，数千个GPU）。如果成功，这个研讨会将在未来再次举行，重点是涉及更新或不同的合作任务集。如果您想加入这一努力，可以在[项目网站](https://oreil.ly/13xfb)上找到更多信息。
- en: EleutherAI
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: EleutherAI
- en: This is a decentralized collective of volunteer researchers, engineers, and
    developers focused on AI alignment, scaling, and open source AI research. One
    of its aims is to train and open-source a GPT-3-sized model, and the group has
    already released some impressive models like [GPT-Neo](https://oreil.ly/ZVGaz)
    and [GPT-J](https://oreil.ly/Kup60), which is a 6-billion-parameter model and
    currently the best-performing publicly available transformer in terms of zero-shot
    performance. You can find more information at EleutherAI’s [website](https://eleuther.ai).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个由志愿研究人员、工程师和开发人员组成的去中心化集体，专注于AI对齐、扩展和开源AI研究。其目标之一是训练和开源一个类似GPT-3大小的模型，该团队已经发布了一些令人印象深刻的模型，如[GPT-Neo](https://oreil.ly/ZVGaz)和[GPT-J](https://oreil.ly/Kup60)，后者是一个60亿参数的模型，目前是公开可用的变压器模型中性能最佳的零-shot性能。您可以在EleutherAI的[网站](https://eleuther.ai)上找到更多信息。
- en: 'Now that we’ve explored how to scale transformers across compute, model size,
    and dataset size, let’s examine another active area of research: making self-attention
    more efficient.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经探讨了如何在计算、模型大小和数据集大小上扩展变压器，让我们来看看另一个活跃的研究领域：使自注意力更加高效。
- en: Attention Please!
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 请注意！
- en: 'We’ve seen throughout this book that the self-attention mechanism plays a central
    role in the architecture of transformers; after all, the original Transformer
    paper is called “Attention Is All You Need”! However, there is a key challenge
    associated with self-attention: since the weights are generated from pairwise
    comparisons of all the tokens in a sequence, this layer becomes a computational
    bottleneck when trying to process long documents or apply transformers to domains
    like speech processing or computer vision. In terms of time and memory complexity,
    the self-attention layer of the Transformer architecture naively scales like <math
    alttext="script upper O left-parenthesis n squared right-parenthesis"><mrow><mi>𝒪</mi>
    <mo>(</mo> <msup><mi>n</mi> <mn>2</mn></msup> <mo>)</mo></mrow></math> , where
    *n* is the length of the sequence.^([5](ch11.xhtml#idm46238686737440))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中已经看到，自注意力机制在变压器的架构中起着核心作用；毕竟，原始的Transformer论文就叫做“注意力机制就是你所需要的”！然而，自注意力存在一个关键挑战：由于权重是从序列中所有标记的成对比较生成的，当尝试处理长文档或将变压器应用于语音处理或计算机视觉等领域时，该层成为计算瓶颈。就时间和内存复杂度而言，变压器架构的自注意力层在计算上的规模化就像<math
    alttext="script upper O left-parenthesis n squared right-parenthesis"><mrow><mi>𝒪</mi>
    <mo>(</mo> <msup><mi>n</mi> <mn>2</mn></msup> <mo>)</mo></mrow></math>，其中*n*是序列的长度。^([5](ch11.xhtml#idm46238686737440))
- en: As a result, much of the recent research on transformers has focused on making
    self-attention more efficient. The research directions are broadly clustered in
    [Figure 11-4](#chapter10_efficient-attention).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最近关于变压器的研究大部分集中在使自注意力更加高效。研究方向大致聚集在[图11-4](#chapter10_efficient-attention)中。
- en: '![efficient-attention](Images/nlpt_1104.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![efficient-attention](Images/nlpt_1104.png)'
- en: Figure 11-4\. A summarization of research directions to make attention more
    efficient (courtesy of Yi Tay et al.)^([6](ch11.xhtml#idm46238686724512))
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-4. 使注意力更高效的研究方向总结（由Yi Tay等人提供）^([6](ch11.xhtml#idm46238686724512))
- en: A common pattern is to make attention more efficient by introducing sparsity
    into the attention mechanism or by applying kernels to the attention matrix. Let’s
    take a quick look at some of the most popular approaches to make self-attention
    more efficient, starting with sparsity.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的模式是通过在注意力机制中引入稀疏性或在注意力矩阵上应用核来使注意力更加高效。让我们快速看一下使自注意力更加高效的一些最流行方法，从稀疏性开始。
- en: Sparse Attention
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏注意力
- en: One way to reduce the number of computations that are performed in the self-attention
    layer is to simply limit the number of query-key pairs that are generated according
    to some predefined pattern. There have been many sparsity patterns explored in
    the literature, but most of them can be decomposed into a handful of “atomic”
    patterns illustrated in [Figure 11-5](#sparsity-atomic).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 减少自注意力层中执行的计算数量的一种方法是简单地限制根据某些预定义模式生成的查询-键对的数量。文献中已经探索了许多稀疏模式，但其中大多数可以分解为[图11-5](#sparsity-atomic)中所示的少数“原子”模式。
- en: '![Atomic sparsity patterns](Images/nlpt_1105.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Atomic sparsity patterns](Images/nlpt_1105.png)'
- en: 'Figure 11-5\. Common atomic sparse attention patterns for self-attention: a
    colored square means the attention score is calculated, while a blank square means
    the score is discarded (courtesy of Tianyang Lin)'
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-5. 自注意力的常见原子稀疏注意力模式：彩色方块表示计算注意力分数，而空白方块表示分数被丢弃（由Tianyang Lin提供）
- en: We can describe these patterns as follows:^([7](ch11.xhtml#idm46238686714320))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Global attention
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Defines a few special tokens in the sequence that are allowed to attend to all
    other tokens
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Band attention
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Computes attention over a diagonal band
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Dilated attention
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Skips some query-key pairs by using a dilated window with gaps
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Random attention
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Randomly samples a few keys for each query to compute attention scores
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Block local attention
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Divides the sequence into blocks and restricts attention within these blocks
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: In practice, most transformer models with sparse attention use a mix of the
    atomic sparsity patterns shown in [Figure 11-5](#sparsity-atomic) to generate
    the final attention matrix. As illustrated in [Figure 11-6](#sparsity-compound),
    models like [Longformer](https://oreil.ly/F7xCY) use a mix of global and band
    attention, while [BigBird](https://oreil.ly/yFPyj) adds random attention to the
    mix. Introducing sparsity into the attention matrix enables these models to process
    much longer sequences; in the case of Longformer and BigBird the maximum sequence
    length is 4,096 tokens, which is 8 times larger than BERT!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Attention with compound sparsity](Images/nlpt_1106.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Figure 11-6\. Sparse attention patterns for recent transformer models (courtesy
    of Tianyang Lin)
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is also possible to *learn* the sparsity pattern in a data-driven manner.
    The basic idea behind such approaches is to cluster the tokens into chunks. For
    example, [Reformer](https://oreil.ly/yIVvX) uses a hash function to cluster similar
    tokens together.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen how sparsity can reduce the complexity of self-attention,
    let’s take a look at another popular approach based on changing the operations
    directly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Linearized Attention
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An alternative way to make self-attention more efficient is to change the order
    of operations that are involved in computing the attention scores. Recall that
    to compute the self-attention scores of the queries and keys we need a similarity
    function, which for the transformer is just a simple dot product. However, for
    a general similarity function <math alttext="normal s normal i normal m left-parenthesis
    q Subscript i Baseline comma k Subscript j Baseline right-parenthesis"><mrow><mi>sim</mi>
    <mo>(</mo> <msub><mi>q</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>k</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></math> we can express the attention outputs as the following
    equation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y Subscript i Baseline equals sigma-summation Underscript j Endscripts
    StartFraction normal s normal i normal m left-parenthesis upper Q Subscript i
    Baseline comma upper K Subscript j Baseline right-parenthesis Over sigma-summation
    Underscript k Endscripts normal s normal i normal m left-parenthesis upper Q Subscript
    i Baseline comma upper K Subscript k Baseline right-parenthesis EndFraction upper
    V Subscript j" display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo>
    <munder><mo>∑</mo> <mi>j</mi></munder> <mfrac><mrow><mi>sim</mi> <mo>(</mo><msub><mi>Q</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>K</mi> <mi>j</mi></msub> <mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>k</mi></msub> <mi>sim</mi> <mrow><mo>(</mo><msub><mi>Q</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>K</mi> <mi>k</mi></msub> <mo>)</mo></mrow></mrow></mfrac>
    <msub><mi>V</mi> <mi>j</mi></msub></mrow></math>
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'The trick behind linearized attention mechanisms is to express the similarity
    function as a *kernel function* that decomposes the operation into two pieces:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal s normal i normal m left-parenthesis upper Q Subscript
    j Baseline comma upper K Subscript j Baseline right-parenthesis equals phi left-parenthesis
    upper Q Subscript i Baseline right-parenthesis Superscript upper T Baseline phi
    left-parenthesis upper K Subscript j Baseline right-parenthesis" display="block"><mrow><mi>sim</mi>
    <mrow><mo>(</mo> <msub><mi>Q</mi> <mi>j</mi></msub> <mo>,</mo> <msub><mi>K</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>ϕ</mi> <msup><mrow><mo>(</mo><msub><mi>Q</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mi>T</mi></msup> <mi>ϕ</mi> <mrow><mo>(</mo>
    <msub><mi>K</mi> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal s normal i normal m left-parenthesis upper Q Subscript
    j Baseline comma upper K Subscript j Baseline right-parenthesis equals phi left-parenthesis
    upper Q Subscript i Baseline right-parenthesis Superscript upper T Baseline phi
    left-parenthesis upper K Subscript j Baseline right-parenthesis" display="block"><mrow><mi>sim</mi>
    <mrow><mo>(</mo> <msub><mi>Q</mi> <mi>j</mi></msub> <mo>,</mo> <msub><mi>K</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>ϕ</mi> <msup><mrow><mo>(</mo><msub><mi>Q</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mi>T</mi></msup> <mi>ϕ</mi> <mrow><mo>(</mo>
    <msub><mi>K</mi> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: 'where <math alttext="phi"><mi>ϕ</mi></math> is typically a high-dimensional
    feature map. Since <math alttext="phi left-parenthesis upper Q Subscript i Baseline
    right-parenthesis"><mrow><mi>ϕ</mi> <mo>(</mo> <msub><mi>Q</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math> is independent of *j* and *k*, we can pull it under the
    sums to write the attention outputs as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="phi"><mi>ϕ</mi></math>通常是一个高维特征映射。由于<math alttext="phi left-parenthesis
    upper Q Subscript i Baseline right-parenthesis"><mrow><mi>ϕ</mi> <mo>(</mo> <msub><mi>Q</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></math>与*j*和*k*无关，我们可以将其移到求和符号下，将注意力输出写成如下形式：
- en: <math alttext="y Subscript i Baseline equals StartFraction phi left-parenthesis
    upper Q Subscript i Baseline right-parenthesis Superscript upper T Baseline sigma-summation
    Underscript j Endscripts phi left-parenthesis upper K Subscript j Baseline right-parenthesis
    upper V Subscript j Superscript upper T Baseline Over phi left-parenthesis upper
    Q Subscript i Baseline right-parenthesis Superscript upper T Baseline sigma-summation
    Underscript k Endscripts phi left-parenthesis upper K Subscript k Baseline right-parenthesis
    EndFraction" display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo>
    <mfrac><mrow><mi>ϕ</mi><msup><mrow><mo>(</mo><msub><mi>Q</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mi>T</mi></msup> <msub><mo>∑</mo> <mi>j</mi></msub> <mi>ϕ</mi><mrow><mo>(</mo><msub><mi>K</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow><msubsup><mi>V</mi> <mi>j</mi> <mi>T</mi></msubsup></mrow>
    <mrow><mi>ϕ</mi><msup><mrow><mo>(</mo><msub><mi>Q</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mi>T</mi></msup> <msub><mo>∑</mo> <mi>k</mi></msub> <mi>ϕ</mi><mrow><mo>(</mo><msub><mi>K</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y Subscript i Baseline equals StartFraction phi left-parenthesis
    upper Q Subscript i Baseline right-parenthesis Superscript upper T Baseline sigma-summation
    Underscript j Endscripts phi left-parenthesis upper K Subscript j Baseline right-parenthesis
    upper V Subscript j Superscript upper T Baseline Over phi left-parenthesis upper
    Q Subscript i Baseline right-parenthesis Superscript upper T Baseline sigma-summation
    Underscript k Endscripts phi left-parenthesis upper K Subscript k Baseline right-parenthesis
    EndFraction" display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo>
    <mfrac><mrow><mi>ϕ</mi><msup><mrow><mo>(</mo><msub><mi>Q</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mi>T</mi></msup> <msub><mo>∑</mo> <mi>j</mi></msub> <mi>ϕ</mi><mrow><mo>(</mo><msub><mi>K</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow><msubsup><mi>V</mi> <mi>j</mi> <mi>T</mi></msubsup></mrow>
    <mrow><mi>ϕ</mi><msup><mrow><mo>(</mo><msub><mi>Q</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mi>T</mi></msup> <msub><mo>∑</mo> <mi>k</mi></msub> <mi>ϕ</mi><mrow><mo>(</mo><msub><mi>K</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: By first computing <math alttext="sigma-summation Underscript j Endscripts phi
    left-parenthesis upper K Subscript j Baseline right-parenthesis upper V Subscript
    j Superscript upper T"><mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mi>ϕ</mi> <mrow><mo>(</mo>
    <msub><mi>K</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <msubsup><mi>V</mi> <mi>j</mi>
    <mi>T</mi></msubsup></mrow></math> and <math alttext="sigma-summation Underscript
    k Endscripts phi left-parenthesis upper K Subscript k Baseline right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>k</mi></msub> <mi>ϕ</mi> <mrow><mo>(</mo> <msub><mi>K</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow></mrow></math> , we can effectively linearize the space and time
    complexity of self-attention! The comparison between the two approaches is illustrated
    in [Figure 11-7](#linear-attention). Popular models that implement linearized
    self-attention include Linear Transformer and Performer.^([8](ch11.xhtml#idm46238686603088))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过首先计算<math alttext="sigma-summation Underscript j Endscripts phi left-parenthesis
    upper K Subscript j Baseline right-parenthesis upper V Subscript j Superscript
    upper T"><mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mi>ϕ</mi> <mrow><mo>(</mo>
    <msub><mi>K</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <msubsup><mi>V</mi> <mi>j</mi>
    <mi>T</mi></msubsup></mrow></math>和<math alttext="sigma-summation Underscript
    k Endscripts phi left-parenthesis upper K Subscript k Baseline right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>k</mi></msub> <mi>ϕ</mi> <mrow><mo>(</mo> <msub><mi>K</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow></mrow></math>，我们可以有效地线性化自注意力的空间和时间复杂度！这两种方法之间的比较在[图11-7](#linear-attention)中有所说明。实现线性化自注意力的流行模型包括线性变换器和表演者。^([8](ch11.xhtml#idm46238686603088))
- en: '![Linear attention](Images/nlpt_1107.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![线性注意力](Images/nlpt_1107.png)'
- en: Figure 11-7\. Complexity difference between standard self-attention and linearized
    self-attention (courtesy of Tianyang Lin)
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-7。标准自注意力和线性化自注意力之间的复杂度差异（由Tianyang Lin提供）
- en: In this section we’ve seen how Transformer architectures in general and attention
    in particular can be scaled up to achieve even better performance on a wide range
    of tasks. In the next section we’ll have a look at how transformers are branching
    out of NLP into other domains such as audio and computer vision.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经看到了Transformer架构以及特别是注意力如何被扩展以在各种任务上实现更好的性能。在下一节中，我们将看看transformers如何从NLP扩展到其他领域，如音频和计算机视觉。
- en: Going Beyond Text
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越文本
- en: Using text to train language models has been the driving force behind the success
    of transformer language models, in combination with transfer learning. On the
    one hand, text is abundant and enables self-supervised training of large models.
    On the other hand, textual tasks such as classification and question answering
    are common, and developing effective strategies for them allows us to address
    a wide range of real-world problems.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本来训练语言模型一直是transformer语言模型成功的推动力，结合迁移学习。一方面，文本丰富多样，可以对大型模型进行自监督训练。另一方面，文本任务如分类和问题回答是常见的，为它们开发有效的策略可以让我们解决各种现实世界的问题。
- en: 'However, there are limits to this approach, including:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法也存在一些限制，包括：
- en: Human reporting bias
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 人类报告偏见
- en: The frequencies of events in text may not represent their true frequencies.^([9](ch11.xhtml#idm46238686592304))
    A model solely trained on text from the internet might have a very distorted image
    of the world.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 文本中事件的频率可能不代表它们的真实频率。^([9](ch11.xhtml#idm46238686592304))仅在互联网文本上训练的模型可能对世界有非常扭曲的看法。
- en: Common sense
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 常识
- en: Common sense is a fundamental quality of human reasoning, but is rarely written
    down. As such, language models trained on text might know many facts about the
    world, but lack basic common-sense reasoning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 常识是人类推理的基本品质，但很少被记录下来。因此，训练在文本上的语言模型可能了解世界的许多事实，但缺乏基本的常识推理。
- en: Facts
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 事实
- en: A probabilistic language model cannot store facts in a reliable way and can
    produce text that is factually wrong. Similarly, such models can detect named
    entities, but have no direct way to access information about them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 概率语言模型无法可靠地存储事实，并且可能产生事实错误的文本。同样，这样的模型可以检测命名实体，但无法直接访问有关它们的信息。
- en: Modality
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 模态
- en: Language models have no way to connect to other modalities that could address
    the previous points, such as audio or visual signals or tabular data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型无法连接到其他可能解决前述问题的模态，如音频或视觉信号或表格数据。
- en: So, if we could solve the modality limitations we could potentially address
    some of the others as well. Recently there has been a lot of progress in pushing
    transformers to new modalities, and even building multimodal models. In this section
    we’ll highlight a few of these advances.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们能解决模态限制，我们可能也能解决其他一些问题。最近，在推动transformer应用于新的模态和甚至构建多模态模型方面取得了很多进展。在本节中，我们将重点介绍其中一些进展。
- en: Vision
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉
- en: Vision has been the stronghold of convolutional neural networks (CNNs) since
    they kickstarted the deep learning revolution. More recently, transformers have
    begun to be applied to this domain and to achieve efficiency similar to or better
    than CNNs. Let’s have a look at a few examples.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 自从它们开启了深度学习革命以来，卷积神经网络（CNNs）一直是视觉的主要领域。最近，transformer开始应用于这一领域，并且在效率上达到了与CNNs相似甚至更好的水平。让我们看看一些例子。
- en: iGPT
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: iGPT
- en: Inspired by the success of the GPT family of models with text, iGPT (short for
    image GPT) applies the same methods to images.^([10](ch11.xhtml#idm46238686576800))
    By viewing images as sequences of pixels, iGPT uses the GPT architecture and autoregressive
    pretraining objective to predict the next pixel values. Pretraining on large image
    datasets enables iGPT to “autocomplete” partial images, as displayed in [Figure 11-8](#iGPT).
    It also achieves performant results on classification tasks when a classification
    head is added to the model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 受到GPT系列模型在文本方面的成功的启发，iGPT（即图像GPT）将相同的方法应用于图像。^([10](ch11.xhtml#idm46238686576800))通过将图像视为像素序列，iGPT使用GPT架构和自回归预训练目标来预测下一个像素值。在大型图像数据集上进行预训练使iGPT能够“自动完成”部分图像，如[图11-8](#iGPT)所示。当在模型中添加分类头时，它还可以在分类任务上实现良好的结果。
- en: '![iGPT](Images/nlpt_1108.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![iGPT](Images/nlpt_1108.png)'
- en: Figure 11-8\. Examples of image completions with iGPT (courtesy of Mark Chen)
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-8\. iGPT的图像补全示例（由Mark Chen提供）
- en: ViT
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ViT
- en: We saw that iGPT follows closely the GPT-style architecture and pretraining
    procedure. Vision Transformer (ViT)^([11](ch11.xhtml#idm46238686567504)) is a
    BERT-style take on transformers for vision, as illustrated in [Figure 11-9](#vit-architecture).
    First the image is split into smaller patches, and each of these patches is embedded
    with a linear projection. The results strongly resemble the token embeddings in
    BERT, and what follows is virtually identical. The patch embeddings are combined
    with position embeddings and then fed through an ordinary transformer encoder.
    During pretraining some of the patches are masked or distorted, and the objective
    is to predict the average color of the masked patch.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到iGPT紧随GPT风格的架构和预训练程序。Vision Transformer（ViT）^([11](ch11.xhtml#idm46238686567504))是对视觉的transformer的BERT风格的处理，如[图11-9](#vit-architecture)所示。首先，图像被分成较小的补丁，然后每个补丁都用线性投影嵌入。结果与BERT中的标记嵌入非常相似，接下来的步骤几乎完全相同。补丁嵌入与位置嵌入相结合，然后通过普通的transformer编码器进行处理。在预训练期间，一些补丁被屏蔽或扭曲，目标是预测屏蔽补丁的平均颜色。
- en: '![vit-architecture](Images/nlpt_1109.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![vit-architecture](Images/nlpt_1109.png)'
- en: Figure 11-9\. The ViT architecture (courtesy of Alexey Dosovitskiy et al.)
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-9\. ViT架构（由Alexey Dosovitskiy等人提供）
- en: Although this approach did not produce better results when pretrained on the
    standard ImageNet dataset, it scaled significantly better than CNNs on larger
    datasets.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法在标准ImageNet数据集上进行预训练时并没有产生更好的结果，但在更大的数据集上，它比CNNs有了显著的提升。
- en: 'ViT is integrated in ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, and
    using it is very similar to the NLP pipelines that we’ve used throughout this
    book. Let’s start by loading the image of a rather famous dog:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ViT已集成在![nlpt_pin01](Images/nlpt_pin01.png) Transformers中，使用它与我们在本书中一直使用的NLP管道非常相似。让我们从加载一张相当有名的狗的图片开始：
- en: '[PRE0]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](Images/nlpt_doge.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_doge.png)'
- en: 'To load a ViT model, we just need to specify the `image-classification` pipeline,
    and then we feed in the image to extract the predicted classes:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载ViT模型，我们只需要指定`image-classification`管道，然后将图像输入以提取预测的类别：
- en: '[PRE1]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|  | score | label |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 分数 | 标签 |'
- en: '| --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 0.643599 | Eskimo dog, husky |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.207407 | Siberian husky |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.060160 | dingo, warrigal, warragal, Canis dingo |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.035359 | Norwegian elkhound, elkhound |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.012927 | malamute, malemute, Alaskan malamute |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: Great, the predicted class seems to match the image!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: A natural extension of image models is video models. In addition to the spatial
    dimensions, videos come with a temporal dimension. This makes the task more challenging
    as the volume of data gets much bigger and one needs to deal with the extra dimension.
    Models such as TimeSformer introduce a spatial and temporal attention mechanism
    to account for both.^([12](ch11.xhtml#idm46238686424832)) In the future, such
    models can help build tools for a wide range of tasks such as classification or
    annotation of video sequences.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Tables
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of data, such as customer data within a company, is stored in structured
    databases instead of as raw text. We saw in [Chapter 7](ch07.xhtml#chapter_qa)
    that with question answering models we can query text with a question in natural
    text. Wouldn’t it be nice if we could do the same with tables, as shown in [Figure 11-10](#tapas-table)?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![table-qa](Images/nlpt_1110.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: Figure 11-10\. Question answering over a table (courtesy of Jonathan Herzig)
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TAPAS (short for Table Parser)^([13](ch11.xhtml#idm46238686413936)) to the rescue!
    This model applies the Transformer architecture to tables by combining the tabular
    information with the query, as illustrated in [Figure 11-11](#tapas-architecture).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![tapas-architecture](Images/nlpt_1111.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Figure 11-11\. Architecture of TAPAS (courtesy of Jonathan Herzig)
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s look at an example of how TAPAS works in practice. We have created a
    fictitious version of this book’s table of contents. It contains the chapter number,
    the name of the chapter, as well as the starting and ending pages of the chapters:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also easily add the number of pages each chapter has with the existing
    fields. In order to play nicely with the TAPAS model, we need to make sure that
    all columns are of type `str`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|  | chapter | name | start_page | end_page | number_of_pages |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | Introduction | 1 | 11 | 10 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | Text classification | 12 | 48 | 36 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | Named Entity Recognition | 49 | 73 | 24 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| 3 | 3 | Question Answering | 74 | 120 | 46 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| 4 | 4 | Summarization | 121 | 140 | 19 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| 5 | 5 | Conclusion | 141 | 144 | 3 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: 'By now you should know the drill. We first load the `table-question-answering`
    pipeline:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'and then pass some queries to extract the answers:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These predictions store the type of table operation in an `aggregator` field,
    along with the answer. Let’s see how well TAPAS fared on our questions:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For the first chapter, the model predicted exactly one cell with no aggregation.
    If we look at the table, we see that the answer is in fact correct. In the next
    example the model predicted all the cells containing the number of pages in combination
    with the sum aggregator, which again is the correct way of calculating the total
    number of pages. The answer to question three is also correct; the average aggregation
    is not necessary in that case, but it doesn’t make a difference. Finally, we have
    a question that is a little bit more complex. To determine how many chapters have
    more than 20 pages we first need to find out which chapters satisfy that criterion
    and then count them. It seem that TAPAS again got it right and correctly determined
    that chapters 1, 2, and 3 have more than 20 pages, and added a count aggregator
    to the cells.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The kinds of questions we asked can also be solved with a few simple Pandas
    commands; however, the ability to ask questions in natural language instead of
    Python code allows a much wider audience to query the data to answer specific
    questions. Imagine such tools in the hands of business analysts or managers who
    are able verify their own hypotheses about the data!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Transformers
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far we’ve looked at extending transformers to a single new modality. TAPAS
    is arguably multimodal since it combines text and tables, but the table is also
    treated as text. In this section we examine transformers that combine two modalities
    at once: audio plus text and vision plus text.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Speech-to-Text
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although being able to use text to interface with a computer is a huge step
    forward, using spoken language is an even more natural way for us to communicate.
    You can see this trend in industry, where applications such as Siri and Alexa
    are on the rise and becoming progressively more useful. Also, for a large fraction
    of the population, writing and reading are more challenging than speaking. So,
    being able to process and understand audio is not only convenient, but can help
    many people access more information. A common task in this domain is *automatic
    speech recognition* (ASR), which converts spoken words to text and enables voice
    technologies like Siri to answer questions like “What is the weather like today?”
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'The [wav2vec 2.0](https://oreil.ly/tPpC7) family of models are one of the most
    recent developments in ASR: they use a transformer layer in combination with a
    CNN, as illustrated in [Figure 11-12](#wav2vec2).^([14](ch11.xhtml#idm46238685996336))
    By leveraging unlabeled data during pretraining, these models achieve competitive
    results with only a few minutes of labeled data.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![wav2vec2](Images/nlpt_1112.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Figure 11-12\. Architecture of wav2vec 2.0 (courtesy of Alexei Baevski)
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The wav2vec 2.0 models are integrated in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, and you won’t be surprised to learn that loading and using them
    follows the familiar steps that we have seen throughout this book. Let’s load
    a pretrained model that was trained on 960 hours of speech audio:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To apply this model to some audio files we’ll use the ASR subset of the [SUPERB
    dataset](https://oreil.ly/iBAK8), which is the same dataset the model was pretrained
    on. Since the dataset is quite large, we’ll just load one example for our demo
    purposes:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here we can see that the audio in the `file` column is stored in the FLAC coding
    format, while the expected transcription is given by the `text` column. To convert
    the audio to an array of floats, we can use the [*SoundFile* library](https://oreil.ly/eo106)
    to read each file in our dataset with `map()`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you are using a Jupyter notebook you can easily play the sound files with
    the following `IPython` widgets:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we can pass the inputs to the pipeline and inspect the prediction:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This transcription seems to be correct. We can see that some punctuation is
    missing, but this is hard to get from audio alone and could be added in a postprocessing
    step. With only a handful of lines of code we can build ourselves a state-of-the-art
    speech-to-text application!
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Building a model for a new language still requires a minimum amount of labeled
    data, which can be challenging to obtain, especially for low-resource languages.
    Soon after the release of wav2vec 2.0, a paper describing a method named wav2vec-U
    was published.^([15](ch11.xhtml#idm46238685789072)) In this work, a combination
    of clever clustering and GAN training is used to build a speech-to-text model
    using only independent unlabeled speech and unlabeled text data. This process
    is visualized in detail in [Figure 11-13](#wav2vec-u). No aligned speech and text
    data is required at all, which enables the training of highly performant speech-to-text
    models for a much larger spectrum of languages.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![wav2vec-u](Images/nlpt_1113.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: Figure 11-13\. Training scheme for wav2vec-U (courtesy of Alexsei Baevski)
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Great, so transformers can now “read” text and “hear” audio—can they also “see”?
    The answer is yes, and this is one of the current hot research frontiers in the
    field.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Vision and Text
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Vision and text are another natural pair of modalities to combine since we
    frequently use language to communicate and reason about the contents of images
    and videos. In addition to the vision transformers, there have been several developments
    in the direction of combining visual and textual information. In this section
    we will look at four examples of models combining vision and text: VisualQA, LayoutLM,
    DALL·E, and CLIP.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉和文本是另一对自然的模态，因为我们经常使用语言来交流和推理图像和视频的内容。除了视觉变换器之外，还有一些在结合视觉和文本信息方面的发展。在本节中，我们将看一下结合视觉和文本的四个模型示例：VisualQA、LayoutLM、DALL·E和CLIP。
- en: VQA
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VQA
- en: In [Chapter 7](ch07.xhtml#chapter_qa) we explored how we can use transformer
    models to extract answers to text-based questions. This can be done ad hoc to
    extract information from texts or offline, where the question answering model
    is used to extract structured information from a set of documents. There have
    been several efforts to expand this approach to vision with datasets such as VQA,^([16](ch11.xhtml#idm46238685749344))
    shown in [Figure 11-14](#vqa).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们探讨了如何使用变换器模型提取基于文本的问题的答案。这可以临时从文本中提取信息，也可以离线进行，其中问答模型用于从一组文档中提取结构化信息。已经有一些努力将这种方法扩展到视觉，使用诸如VQA的数据集，如图11-14所示。
- en: '![vqa](Images/nlpt_1114.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![vqa](Images/nlpt_1114.png)'
- en: Figure 11-14\. Example of a visual question answering task from the VQA dataset
    (courtesy of Yash Goyal)
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-14。来自VQA数据集的视觉问答任务示例（由Yash Goyal提供）
- en: Models such as LXMERT and VisualBERT use vision models like ResNets to extract
    features from the pictures and then use transformer encoders to combine them with
    the natural questions and predict an answer.^([17](ch11.xhtml#idm46238685739392))
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如LXMERT和VisualBERT的模型使用视觉模型（如ResNets）从图片中提取特征，然后使用变换器编码器将其与自然问题结合起来，并预测答案。
- en: LayoutLM
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LayoutLM
- en: 'Analyzing scanned business documents like receipts, invoices, or reports is
    another area where extracting visual and layout information can be a useful way
    to recognize text fields of interest. Here the [LayoutLM](https://oreil.ly/uQc5t)
    family of models are the current state of the art. They use an enhanced Transformer
    architecture that receives three modalities as input: text, image, and layout.
    Accordingly, as shown in [Figure 11-15](#layoutlm), there are embedding layers
    associated with each modality, a spatially aware self-attention mechanism, and
    a mix of image and text/image pretraining objectives to align the different modalities.
    By pretraining on millions of scanned documents, LayoutLM models are able to transfer
    to various downstream tasks in a manner similar to BERT for NLP.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 分析扫描的商业文件，如收据、发票或报告，是另一个提取视觉和布局信息的领域，可以用于识别感兴趣的文本字段。在这里，LayoutLM系列模型是当前的最新技术。它们使用增强的Transformer架构，接收三种模态作为输入：文本、图像和布局。因此，如图11-15所示，每种模态都有与之关联的嵌入层，一个空间感知的自注意机制，以及一种图像和文本/图像预训练目标的混合，以对齐不同的模态。通过在数百万份扫描文件上进行预训练，LayoutLM模型能够以与NLP的BERT类似的方式转移到各种下游任务。
- en: '![layoutlm](Images/nlpt_1115.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![layoutlm](Images/nlpt_1115.png)'
- en: Figure 11-15\. The model architecture and pretraining strategies for LayoutLMv2
    (courtesy of Yang Xu)
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-15。LayoutLMv2的模型架构和预训练策略（由Yang Xu提供）
- en: DALL·E
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DALL·E
- en: A model that combines vision and text for *generative* tasks is DALL·E.^([18](ch11.xhtml#idm46238685725504))
    It uses the GPT architecture and autoregressive modeling to generate images from
    text. Inspired by iGPT, it regards the words and pixels as one sequence of tokens
    and is thus able to continue generating an image from a text prompt, as shown
    in [Figure 11-16](#dall-e).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 结合视觉和文本进行*生成*任务的模型是DALL·E。它使用GPT架构和自回归建模来从文本生成图像。受到iGPT的启发，它将单词和像素视为一个令牌序列，因此能够继续从文本提示生成图像，如图11-16所示。
- en: '![dall-e](Images/nlpt_1116.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![dall-e](Images/nlpt_1116.png)'
- en: Figure 11-16\. Generation examples with DALL·E (courtesy of Aditya Ramesh)
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-16。DALL·E的生成示例（由Aditya Ramesh提供）
- en: CLIP
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CLIP
- en: Finally, let’s have a look at CLIP,^([19](ch11.xhtml#idm46238685716624)) which
    also combines text and vision but is designed for supervised tasks. Its creators
    constructed a dataset with 400 million image/caption pairs and used contrastive
    learning to pretrain the model. The CLIP architecture consists of a text and an
    image encoder (both transformers) that create embeddings of the captions and images.
    A batch of images with captions is sampled, and the contrastive objective is to
    maximize the similarity of the embeddings (as measured by the dot product) of
    the corresponding pair while minimizing the similarity of the rest, as illustrated
    in [Figure 11-17](#clip-arch).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看看CLIP，它也结合了文本和视觉，但是设计用于监督任务。其创建者构建了一个包含4亿个图像/标题对的数据集，并使用对比学习对模型进行预训练。CLIP架构包括文本编码器和图像编码器（都是变换器），用于创建标题和图像的嵌入。抽样一批带有标题的图像，对比目标是最大化对应对的嵌入的相似性（由点积测量），同时最小化其余的相似性，如图11-17所示。
- en: In order to use the pretrained model for classification the possible classes
    are embedded with the text encoder, similar to how we used the zero-shot pipeline.
    Then the embeddings of all the classes are compared to the image embedding that
    we want to classify, and the class with the highest similarity is chosen.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使用预训练模型进行分类，可能的类别被嵌入到文本编码器中，类似于我们使用零-shot管道的方式。然后，所有类别的嵌入与我们要分类的图像嵌入进行比较，并选择相似性最高的类别。 '
- en: '![clip-arch](Images/nlpt_1117.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![clip-arch](Images/nlpt_1117.png)'
- en: Figure 11-17\. Architecture of CLIP (courtesy of Alec Radford)
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-17。CLIP的架构（由Alec Radford提供）
- en: 'The zero-shot image classification performance of CLIP is remarkable and competitive
    with fully supervised trained vision models, while being more flexible with regard
    to new classes. CLIP is also fully integrated in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, so we can try it out. For image-to-text tasks, we instantiate a
    *processor* that consists of a *feature extractor* and a tokenizer. The role of
    the feature extractor is to convert the image into a form suitable for the model,
    while the tokenizer is responsible for decoding the model’s predictions into text:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的零-shot图像分类性能非常出色，并且与完全监督训练的视觉模型竞争力相当，同时对于新类别更加灵活。CLIP也完全集成在![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers中，因此我们可以尝试一下。对于图像到文本的任务，我们实例化一个*processor*，其中包括一个*feature extractor*和一个tokenizer。feature
    extractor的作用是将图像转换为适合模型的形式，而tokenizer负责将模型的预测解码为文本。
- en: '[PRE15]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Then we need a fitting image to try it out. What would be better suited than
    a picture of Optimus Prime?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要一张合适的图像来尝试一下。有什么比一张Prime的照片更合适的呢？
- en: '[PRE16]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](Images/nlpt_11in01.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_11in01.png)'
- en: 'Next, we set up the texts to compare the image against and pass it through
    the model:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置要与图像进行比较的文本，并将其通过模型传递：
- en: '[PRE17]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Well, it almost got the right answer (a photo of AGI of course). Jokes aside,
    CLIP makes image classification very flexible by allowing us to define classes
    through text instead of having the classes hardcoded in the model architecture.
    This concludes our tour of multimodal transformer models, but we hope we’ve whetted
    your appetite.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，它几乎得到了正确的答案（当然是AGI的照片）。开玩笑的话，CLIP通过允许我们通过文本定义类别，而不是在模型架构中硬编码类别，使图像分类变得非常灵活。这结束了我们对多模态变换器模型的介绍，但我们希望我们已经激起了您的兴趣。
- en: Where to from Here?
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来做什么？
- en: Well that’s the end of the ride; thanks for joining us on this journey through
    the transformers landscape! Throughout this book we’ve explored how transformers
    can address a wide range of tasks and achieve state-of-the-art results. In this
    chapter we’ve seen how the current generation of models are being pushed to their
    limits with scaling and how they are also branching out into new domains and modalities.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这就是旅程的结束；感谢您加入我们穿越变换器领域的旅程！在本书中，我们探讨了变换器如何解决各种任务并取得最先进的结果。在本章中，我们看到了当前一代模型如何通过扩展推动其极限，以及它们如何在新的领域和模态中拓展。
- en: 'If you want to reinforce the concepts and skills that you’ve learned in this
    book, here are a few ideas for where to go from here:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想巩固在本书中学到的概念和技能，以下是一些继续学习的想法：
- en: Join a Hugging Face community event
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 参加Hugging Face社区活动
- en: Hugging Face hosts short sprints focused on improving the libraries in the ecosystem,
    and these events are a great way to meet the community and get a taste for open
    source software development. So far there have been sprints on adding 600+ datasets
    to ![nlpt_pin01](Images/nlpt_pin01.png) Datasets, fine-tuning 300+ ASR models
    in various languages, and implementing hundreds of projects in JAX/Flax.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face主持专注于改进生态系统中的库的短期活动，这些活动是与社区见面并尝试开源软件开发的绝佳方式。到目前为止，已经有了关于向![nlpt_pin01](Images/nlpt_pin01.png)
    Datasets添加600多个数据集、在各种语言中微调300多个ASR模型以及在JAX/Flax中实施数百个项目的活动。
- en: Build your own project
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 建立自己的项目
- en: One very effective way to test your knowledge in machine learning is to build
    a project to solve a problem that interests you. You could reimplement a transformer
    paper, or apply transformers to a novel domain.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中测试您的知识的一个非常有效的方法是建立一个解决您感兴趣的问题的项目。您可以重新实现一个变换器论文，或者将变换器应用于一个新领域。
- en: Contribute a model to ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为![nlpt_pin01](Images/nlpt_pin01.png) Transformers贡献一个模型
- en: If you’re looking for something more advanced, then contributing a newly published
    architecture to ![nlpt_pin01](Images/nlpt_pin01.png) Transformers is a great way
    to dive into the nuts and bolts of the library. There is a detailed guide to help
    you get started in the ​![nlpt_pin01](Images/nlpt_pin01.png)⁠ [Transformers documentation](https://oreil.ly/3f4wZ).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在寻找更高级的内容，那么将新发布的架构贡献给![nlpt_pin01](Images/nlpt_pin01.png) Transformers是深入了解该库的细节的好方法。有一个详细的指南可以帮助您在​![nlpt_pin01](Images/nlpt_pin01.png)⁠
    [Transformers documentation](https://oreil.ly/3f4wZ)中开始。
- en: Blog about what you’ve learned
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 写博客分享您所学到的知识
- en: Teaching others what you’ve learned is a powerful test of your own knowledge,
    and in a sense this was one of the driving motivations behind us writing this
    book! There are great tools to help you get started with technical blogging; we
    recommend [*fastpages*](https://oreil.ly/f0L9u) as you can easily use Jupyter
    notebooks for everything.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 教别人你所学到的知识是对自己知识的一个强大测试，从某种意义上说，这是我们写这本书的驱动动力之一！有很好的工具可以帮助您开始技术博客；我们推荐[*fastpages*](https://oreil.ly/f0L9u)，因为您可以轻松地使用Jupyter笔记本进行一切。
- en: ^([1](ch11.xhtml#idm46238686826448-marker)) J. Kaplan et al., [“Scaling Laws
    for Neural Language Models”](https://arxiv.org/abs/2001.08361), (2020).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.xhtml#idm46238686826448-marker)) J. Kaplan等人的[“Scaling Laws for Neural
    Language Models”](https://arxiv.org/abs/2001.08361)，（2020）。
- en: ^([2](ch11.xhtml#idm46238686819248-marker)) The dataset size is measured in
    the number of tokens, while the model size excludes parameters from the embedding
    layers.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch11.xhtml#idm46238686819248-marker)) 数据集大小是以标记数来衡量的，而模型大小不包括嵌入层的参数。
- en: ^([3](ch11.xhtml#idm46238686793248-marker)) T. Henighan et al., [“Scaling Laws
    for Autoregressive Generative Modeling”](https://arxiv.org/abs/2010.14701), (2020).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch11.xhtml#idm46238686793248-marker)) T. Henighan等人的[“Scaling Laws for
    Autoregressive Generative Modeling”](https://arxiv.org/abs/2010.14701)，（2020）。
- en: ^([4](ch11.xhtml#idm46238686775136-marker)) However, recently a distributed
    deep learning framework has been proposed that enables smaller groups to pool
    their computational resources and pretrain models in a collaborative fashion.
    See M. Diskin et al., [“Distributed Deep Learning in Open Collaborations”](https://arxiv.org/abs/2106.10207),
    (2021).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch11.xhtml#idm46238686775136-marker)) 然而，最近提出了一个分布式深度学习框架，使较小的团体能够共享其计算资源，并以合作的方式预训练模型。参见M.
    Diskin等人的[“Distributed Deep Learning in Open Collaborations”](https://arxiv.org/abs/2106.10207)，（2021）。
- en: ^([5](ch11.xhtml#idm46238686737440-marker)) Although standard implementations
    of self-attention have <math alttext="upper O left-parenthesis n squared right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <msup><mi>n</mi> <mn>2</mn></msup> <mo>)</mo></mrow></math> time and
    memory complexity, a [recent paper by Google researchers](https://arxiv.org/abs/2112.05682)
    shows that the memory complexity can be reduced to <math alttext="upper O left-parenthesis
    log n right-parenthesis"><mrow><mi>O</mi> <mo>(</mo> <mo form="prefix">log</mo>
    <mi>n</mi> <mo>)</mo></mrow></math> via a simple reordering of the operations.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 虽然自注意力的标准实现具有O(n^2)的时间和内存复杂度，但一篇谷歌研究人员的最新论文表明，通过简单重新排序操作，内存复杂度可以降低到O(log
    n)。
- en: '^([6](ch11.xhtml#idm46238686724512-marker)) Yi Tay et al., [“Efficient Transformers:
    A Survey”](https://arxiv.org/abs/2009.06732), (2020).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '2. Yi Tay et al., “Efficient Transformers: A Survey” (2020).'
- en: ^([7](ch11.xhtml#idm46238686714320-marker)) T. Lin et al., [“A Survey of Transformers”](https://arxiv.org/abs/2106.04554),
    (2021).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 6. T. Lin et al., “A Survey of Transformers” (2021).
- en: '^([8](ch11.xhtml#idm46238686603088-marker)) A. Katharopoulos et al., [“Transformers
    Are RNNs: Fast Autoregressive Transformers with Linear Attention”](https://arxiv.org/abs/2006.16236),
    (2020); K. Choromanski et al., [“Rethinking Attention with Performers”](https://arxiv.org/abs/2009.14794),
    (2020).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '14. A. Katharopoulos et al., “Transformers Are RNNs: Fast Autoregressive Transformers
    with Linear Attention” (2020); K. Choromanski et al., “Rethinking Attention with
    Performers” (2020).'
- en: ^([9](ch11.xhtml#idm46238686592304-marker)) J. Gordon and B. Van Durme, [“Reporting
    Bias and Knowledge Extraction”](https://openreview.net/pdf?id=AzxEzvpdE3Wcy),
    (2013).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 12. J. Gordon and B. Van Durme, “Reporting Bias and Knowledge Extraction” (2013).
- en: ^([10](ch11.xhtml#idm46238686576800-marker)) M. Chen et al., “Generative Pretraining
    from Pixels,” *Proceedings of the 37th International Conference on Machine Learning*
    119 (2020):1691–1703, [*https://proceedings.mlr.press/v119/chen20s.html*](https://proceedings.mlr.press/v119/chen20s.html).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 11. M. Chen et al., “Generative Pretraining from Pixels,” *Proceedings of the
    37th International Conference on Machine Learning* 119 (2020):1691–1703, [*https://proceedings.mlr.press/v119/chen20s.html*](https://proceedings.mlr.press/v119/chen20s.html).
- en: '^([11](ch11.xhtml#idm46238686567504-marker)) A. Dosovitskiy et al., [“An Image
    Is Worth 16x16 Words: Transformers for Image Recognition at Scale”](https://arxiv.org/abs/2010.11929),
    (2020).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '7. A. Dosovitskiy et al., “An Image Is Worth 16x16 Words: Transformers for
    Image Recognition at Scale” (2020).'
- en: ^([12](ch11.xhtml#idm46238686424832-marker)) G. Bertasius, H. Wang, and L. Torresani,
    [“Is Space-Time Attention All You Need for Video Understanding?”](https://arxiv.org/abs/2102.05095),
    (2021).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 1. G. Bertasius, H. Wang, and L. Torresani, “Is Space-Time Attention All You
    Need for Video Understanding?” (2021).
- en: '^([13](ch11.xhtml#idm46238686413936-marker)) J. Herzig et al., [“TAPAS: Weakly
    Supervised Table Parsing via Pre-Training”](https://arxiv.org/abs/2004.02349),
    (2020).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '15. J. Herzig et al., “TAPAS: Weakly Supervised Table Parsing via Pre-Training”
    (2020).'
- en: '^([14](ch11.xhtml#idm46238685996336-marker)) A. Baevski et al., [“wav2vec 2.0:
    A Framework for Self-Supervised Learning of Speech Representations”](https://arxiv.org/abs/2006.11477),
    (2020).'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '10. A. Baevski et al., “wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations” (2020).'
- en: ^([15](ch11.xhtml#idm46238685789072-marker)) A. Baevski et al., [“Unsupervised
    Speech Recognition”](https://arxiv.org/abs/2105.11084), (2021).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 13. A. Baevski et al., “Unsupervised Speech Recognition” (2021).
- en: '^([16](ch11.xhtml#idm46238685749344-marker)) Y. Goyal et al., [“Making the
    V in VQA Matter: Elevating the Role of Image Understanding in Visual Question
    Answering”](https://arxiv.org/abs/1612.00837), (2016).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '3. Y. Goyal et al., “Making the V in VQA Matter: Elevating the Role of Image
    Understanding in Visual Question Answering” (2016).'
- en: '^([17](ch11.xhtml#idm46238685739392-marker)) H. Tan and M. Bansal, [“LXMERT:
    Learning Cross-Modality Encoder Representations from Transformers”](https://arxiv.org/abs/1908.07490),
    (2019); L.H. Li et al., [“VisualBERT: A Simple and Performant Baseline for Vision
    and Language”](https://arxiv.org/abs/1908.03557), (2019).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '9. H. Tan and M. Bansal, “LXMERT: Learning Cross-Modality Encoder Representations
    from Transformers” (2019); L.H. Li et al., “VisualBERT: A Simple and Performant
    Baseline for Vision and Language” (2019).'
- en: ^([18](ch11.xhtml#idm46238685725504-marker)) A. Ramesh et al., [“Zero-Shot Text-to-Image
    Generation”](https://arxiv.org/abs/2102.12092), (2021).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 4. A. Ramesh et al., “Zero-Shot Text-to-Image Generation” (2021).
- en: ^([19](ch11.xhtml#idm46238685716624-marker)) A. Radford et al., [“Learning Transferable
    Visual Models from Natural Language Supervision”](https://arxiv.org/abs/2103.00020),
    (2021).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 8. A. Radford et al., “Learning Transferable Visual Models from Natural Language
    Supervision” (2021).
