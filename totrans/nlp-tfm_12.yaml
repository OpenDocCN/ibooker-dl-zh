- en: Chapter 11\. Future Directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book we’ve explored the powerful capabilities of transformers
    across a wide range of NLP tasks. In this final chapter, we’ll shift our perspective
    and look at some of the current challenges with these models and the research
    trends that are trying to overcome them. In the first part we explore the topic
    of scaling up transformers, both in terms of model and corpus size. Then we turn
    our attention toward various techniques that have been proposed to make the self-attention
    mechanism more efficient. Finally, we explore the emerging and exciting field
    of *multimodal transformers*, which can model inputs across multiple domains like
    text, images, and audio.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2019, the researcher [Richard Sutton](https://oreil.ly/119br) wrote a provocative
    essay entitled [“The Bitter Lesson”](https://oreil.ly/YtD3V) in which he argued
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: The biggest lesson that can be read from 70 years of AI research is that general
    methods that leverage computation are ultimately the most effective, and by a
    large margin…. Seeking an improvement that makes a difference in the shorter term,
    researchers seek to leverage their human knowledge of the domain, but the only
    thing that matters in the long run is the leveraging of computation. These two
    need not run counter to each other, but in practice they tend to…. And the human-knowledge
    approach tends to complicate methods in ways that make them less suited to taking
    advantage of general methods leveraging computation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The essay provides several historical examples, such as playing chess or Go,
    where the approach of encoding human knowledge within AI systems was ultimately
    outdone by increased computation. Sutton calls this the “bitter lesson” for the
    AI research field:'
  prefs: []
  type: TYPE_NORMAL
- en: We have to learn the bitter lesson that building in how we think we think does
    not work in the long run…. One thing that should be learned from the bitter lesson
    is the great power of general purpose methods, of methods that continue to scale
    with increased computation even as the available computation becomes very great.
    The two methods that seem to scale arbitrarily in this way are *search* and *learning*.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are now signs that a similar lesson is at play with transformers; while
    many of the early BERT and GPT descendants focused on tweaking the architecture
    or pretraining objectives, the best-performing models in mid-2021, like GPT-3,
    are essentially basic scaled-up versions of the original models without many architectural
    modifications. In [Figure 11-1](#parameter-counts) you can see a timeline of the
    development of the largest models since the release of the original Transformer
    architecture in 2017, which shows that model size has increased by over four orders
    of magnitude in just a few years!
  prefs: []
  type: TYPE_NORMAL
- en: '![Language model parameter counts](Images/nlpt_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. Parameter counts over time for prominent Transformer architectures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This dramatic growth is motivated by empirical evidence that large language
    models perform better on downstream tasks and that interesting capabilities such
    as zero-shot and few-shot learning emerge in the 10- to 100-billion parameter
    range. However, the number of parameters is not the only factor that affects model
    performance; the amount of compute and training data must also be scaled in tandem
    to train these monsters. Given that large language models like GPT-3 are estimated
    to cost [$4.6 million](https://oreil.ly/DUVcq) to train, it is clearly desirable
    to be able to estimate the model’s performance in advance. Somewhat surprisingly,
    the performance of language models appears to obey a *power law relationship*
    with model size and other factors that is codified in a set of scaling laws.^([1](ch11.xhtml#idm46238686826448))
    Let’s take a look at this exciting area of research.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Laws
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scaling laws allow one to empirically quantify the “bigger is better” paradigm
    for language models by studying their behavior with varying compute budget *C*,
    dataset size *D*, and model size *N*.^([2](ch11.xhtml#idm46238686819248)) The
    basic idea is to chart the dependence of the cross-entropy loss *L* on these three
    factors and determine if a relationship emerges. For autoregressive models like
    those in the GPT family, the resulting loss curves are shown in [Figure 11-2](#scaling-laws),
    where each blue curve represents the training run of a single model.
  prefs: []
  type: TYPE_NORMAL
- en: '![scaling-laws](Images/nlpt_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. Power-law scaling of test loss versus compute budget (left), dataset
    size (middle), and model size (right) (courtesy of Jared Kaplan)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From these loss curves we can draw a few conclusions about:'
  prefs: []
  type: TYPE_NORMAL
- en: The relationship of performance and scale
  prefs: []
  type: TYPE_NORMAL
- en: Although many NLP researchers focus on architectural tweaks or hyperparameter
    optimization (like tuning the number of layers or attention heads) to improve
    performance on a fixed set of datasets, the implication of scaling laws is that
    a more productive path toward better models is to focus on increasing *N*, *C*,
    and *D* in tandem.
  prefs: []
  type: TYPE_NORMAL
- en: Smooth power laws
  prefs: []
  type: TYPE_NORMAL
- en: The test loss *L* has a power law relationship with each of *N*, *C*, and *D*
    across several orders of magnitude (power law relationships are linear on a log-log
    scale). For <math alttext="upper X equals upper N comma upper C comma upper D"><mrow><mi>X</mi>
    <mo>=</mo> <mi>N</mi> <mo>,</mo> <mi>C</mi> <mo>,</mo> <mi>D</mi></mrow></math>
    we can express these power law relationships as <math alttext="upper L left-parenthesis
    upper X right-parenthesis tilde 1 slash upper X Superscript alpha"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>∼</mo> <mn>1</mn> <mo>/</mo>
    <msup><mi>X</mi> <mi>α</mi></msup></mrow></math> , where <math alttext="alpha"><mi>α</mi></math>
    is a scaling exponent that is determined by a fit to the loss curves shown in
    [Figure 11-2](#scaling-laws).^([3](ch11.xhtml#idm46238686793248)) Typical values
    for <math alttext="alpha Subscript upper X"><msub><mi>α</mi> <mi>X</mi></msub></math>
    lie in the 0.05–0.095 range, and one attractive feature of these power laws is
    that the early part of a loss curve can be extrapolated to predict what the approximate
    loss would be if training was conducted for much longer.
  prefs: []
  type: TYPE_NORMAL
- en: Sample efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Large models are able to reach the same performance as smaller models with a
    smaller number of training steps. This can be seen by comparing the regions where
    a loss curve plateaus over some number of training steps, which indicates one
    gets diminishing returns in performance compared to simply scaling up the model.
  prefs: []
  type: TYPE_NORMAL
- en: Somewhat surprisingly, scaling laws have also been observed for other modalities,
    like images, videos, and mathematical problem solving, as illustrated in [Figure 11-3](#scaling-laws-modal).
  prefs: []
  type: TYPE_NORMAL
- en: '![scaling-laws-modal](Images/nlpt_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-3\. Power-law scaling of test loss versus compute budget across a
    wide range of modalities (courtesy of Tom Henighan)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Whether power-law scaling is a universal property of transformer language models
    is currently unknown. For now, we can use scaling laws as a tool to extrapolate
    large, expensive models without having to explicitly train them. However, scaling
    isn’t quite as easy as it sounds. Let’s now look at a few challenges that crop
    up when charting this frontier.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While scaling up sounds simple in theory (“just add more layers!”), in practice
    there are many difficulties. Here are a few of the biggest challenges you’re likely
    to encounter when scaling language models:'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning and managing infrastructure that potentially spans hundreds or
    thousands of nodes with as many GPUs is not for the faint-hearted. Are the required
    number of nodes available? Is communication between nodes a bottleneck? Tackling
    these issues requires a very different skill set than that found in most data
    science teams, and typically involves specialized engineers familiar with running
    large-scale, distributed experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Cost
  prefs: []
  type: TYPE_NORMAL
- en: Most ML practitioners have experienced the feeling of waking up in the middle
    of the night in a cold sweat, remembering they forgot to shut down that fancy
    GPU on the cloud. This feeling intensifies when running large-scale experiments,
    and most companies cannot afford the teams and resources necessary to train models
    at the largest scales. Training a single GPT-3-sized model can cost several million
    dollars, which is not the kind of pocket change that many companies have lying
    around.^([4](ch11.xhtml#idm46238686775136))
  prefs: []
  type: TYPE_NORMAL
- en: Dataset curation
  prefs: []
  type: TYPE_NORMAL
- en: A model is only as good as the data it is trained on. Training large models
    requires large, high-quality datasets. When using terabytes of text data it becomes
    harder to make sure the dataset contains high-quality text, and even preprocessing
    becomes challenging. Furthermore, one needs to ensure that there is a way to control
    biases like sexism and racism that these language models can acquire when trained
    on large-scale webtext corpora. Another type of consideration revolves around
    licensing issues with the training data and personal information that can be embedded
    in large text datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, the challenges don’t stop. Evaluating the model on
    downstream tasks again requires time and resources. In addition, you’ll want to
    probe the model for biased and toxic generations, even if you are confident that
    you created a clean dataset. These steps take time and need to be carried out
    thoroughly to minimize the risks of adverse effects later on.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs: []
  type: TYPE_NORMAL
- en: Finally, serving large language models also poses a significant challenge. In
    [Chapter 8](ch08.xhtml#chapter_compression) we looked at a few approaches, such
    as distillation, pruning, and quantization, to help with these issues. However,
    this may not be enough if you are starting with a model that is hundreds of gigabytes
    in size. Hosted services such as the [OpenAI API](https://beta.openai.com) or
    Hugging Face’s [Accelerated Inference API](https://oreil.ly/E4q3b) are designed
    to help companies that cannot or do not want to deal with these deployment challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is by no means an exhaustive list, but it should give you an idea of the
    kinds of considerations and challenges that go hand in hand with scaling language
    models to ever larger sizes. While most of these efforts are centralized around
    a few institutions that have the resources and know-how to push the boundaries,
    there are currently two community-led projects that aim to produce and probe large
    language models in the open:'
  prefs: []
  type: TYPE_NORMAL
- en: BigScience
  prefs: []
  type: TYPE_NORMAL
- en: This is a one-year-long research workshop that runs from 2021 to 2022 and is
    focused on large language models. The workshop aims to foster discussions and
    reflections around the research questions surrounding these models (capabilities,
    limitations, potential improvements, bias, ethics, environmental impact, role
    in the general AI/cognitive research landscape) as well as the challenges around
    creating and sharing such models and datasets for research purposes and among
    the research community. The collaborative tasks involve creating, sharing, and
    evaluating a large multilingual dataset and a large language model. An unusually
    large compute budget was allocated for these collaborative tasks (several million
    GPU hours on several thousands GPUs). If successful, this workshop will run again
    in the future, focusing on involving an updated or different set of collaborative
    tasks. If you want to join the effort, you can find more information at the [project’s
    website](https://oreil.ly/13xfb).
  prefs: []
  type: TYPE_NORMAL
- en: EleutherAI
  prefs: []
  type: TYPE_NORMAL
- en: This is a decentralized collective of volunteer researchers, engineers, and
    developers focused on AI alignment, scaling, and open source AI research. One
    of its aims is to train and open-source a GPT-3-sized model, and the group has
    already released some impressive models like [GPT-Neo](https://oreil.ly/ZVGaz)
    and [GPT-J](https://oreil.ly/Kup60), which is a 6-billion-parameter model and
    currently the best-performing publicly available transformer in terms of zero-shot
    performance. You can find more information at EleutherAI’s [website](https://eleuther.ai).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve explored how to scale transformers across compute, model size,
    and dataset size, let’s examine another active area of research: making self-attention
    more efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: Attention Please!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve seen throughout this book that the self-attention mechanism plays a central
    role in the architecture of transformers; after all, the original Transformer
    paper is called “Attention Is All You Need”! However, there is a key challenge
    associated with self-attention: since the weights are generated from pairwise
    comparisons of all the tokens in a sequence, this layer becomes a computational
    bottleneck when trying to process long documents or apply transformers to domains
    like speech processing or computer vision. In terms of time and memory complexity,
    the self-attention layer of the Transformer architecture naively scales like <math
    alttext="script upper O left-parenthesis n squared right-parenthesis"><mrow><mi>𝒪</mi>
    <mo>(</mo> <msup><mi>n</mi> <mn>2</mn></msup> <mo>)</mo></mrow></math> , where
    *n* is the length of the sequence.^([5](ch11.xhtml#idm46238686737440))'
  prefs: []
  type: TYPE_NORMAL
- en: As a result, much of the recent research on transformers has focused on making
    self-attention more efficient. The research directions are broadly clustered in
    [Figure 11-4](#chapter10_efficient-attention).
  prefs: []
  type: TYPE_NORMAL
- en: '![efficient-attention](Images/nlpt_1104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-4\. A summarization of research directions to make attention more
    efficient (courtesy of Yi Tay et al.)^([6](ch11.xhtml#idm46238686724512))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A common pattern is to make attention more efficient by introducing sparsity
    into the attention mechanism or by applying kernels to the attention matrix. Let’s
    take a quick look at some of the most popular approaches to make self-attention
    more efficient, starting with sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to reduce the number of computations that are performed in the self-attention
    layer is to simply limit the number of query-key pairs that are generated according
    to some predefined pattern. There have been many sparsity patterns explored in
    the literature, but most of them can be decomposed into a handful of “atomic”
    patterns illustrated in [Figure 11-5](#sparsity-atomic).
  prefs: []
  type: TYPE_NORMAL
- en: '![Atomic sparsity patterns](Images/nlpt_1105.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-5\. Common atomic sparse attention patterns for self-attention: a
    colored square means the attention score is calculated, while a blank square means
    the score is discarded (courtesy of Tianyang Lin)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can describe these patterns as follows:^([7](ch11.xhtml#idm46238686714320))
  prefs: []
  type: TYPE_NORMAL
- en: Global attention
  prefs: []
  type: TYPE_NORMAL
- en: Defines a few special tokens in the sequence that are allowed to attend to all
    other tokens
  prefs: []
  type: TYPE_NORMAL
- en: Band attention
  prefs: []
  type: TYPE_NORMAL
- en: Computes attention over a diagonal band
  prefs: []
  type: TYPE_NORMAL
- en: Dilated attention
  prefs: []
  type: TYPE_NORMAL
- en: Skips some query-key pairs by using a dilated window with gaps
  prefs: []
  type: TYPE_NORMAL
- en: Random attention
  prefs: []
  type: TYPE_NORMAL
- en: Randomly samples a few keys for each query to compute attention scores
  prefs: []
  type: TYPE_NORMAL
- en: Block local attention
  prefs: []
  type: TYPE_NORMAL
- en: Divides the sequence into blocks and restricts attention within these blocks
  prefs: []
  type: TYPE_NORMAL
- en: In practice, most transformer models with sparse attention use a mix of the
    atomic sparsity patterns shown in [Figure 11-5](#sparsity-atomic) to generate
    the final attention matrix. As illustrated in [Figure 11-6](#sparsity-compound),
    models like [Longformer](https://oreil.ly/F7xCY) use a mix of global and band
    attention, while [BigBird](https://oreil.ly/yFPyj) adds random attention to the
    mix. Introducing sparsity into the attention matrix enables these models to process
    much longer sequences; in the case of Longformer and BigBird the maximum sequence
    length is 4,096 tokens, which is 8 times larger than BERT!
  prefs: []
  type: TYPE_NORMAL
- en: '![Attention with compound sparsity](Images/nlpt_1106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-6\. Sparse attention patterns for recent transformer models (courtesy
    of Tianyang Lin)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is also possible to *learn* the sparsity pattern in a data-driven manner.
    The basic idea behind such approaches is to cluster the tokens into chunks. For
    example, [Reformer](https://oreil.ly/yIVvX) uses a hash function to cluster similar
    tokens together.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen how sparsity can reduce the complexity of self-attention,
    let’s take a look at another popular approach based on changing the operations
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: Linearized Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An alternative way to make self-attention more efficient is to change the order
    of operations that are involved in computing the attention scores. Recall that
    to compute the self-attention scores of the queries and keys we need a similarity
    function, which for the transformer is just a simple dot product. However, for
    a general similarity function <math alttext="normal s normal i normal m left-parenthesis
    q Subscript i Baseline comma k Subscript j Baseline right-parenthesis"><mrow><mi>sim</mi>
    <mo>(</mo> <msub><mi>q</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>k</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></math> we can express the attention outputs as the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y Subscript i Baseline equals sigma-summation Underscript j Endscripts
    StartFraction normal s normal i normal m left-parenthesis upper Q Subscript i
    Baseline comma upper K Subscript j Baseline right-parenthesis Over sigma-summation
    Underscript k Endscripts normal s normal i normal m left-parenthesis upper Q Subscript
    i Baseline comma upper K Subscript k Baseline right-parenthesis EndFraction upper
    V Subscript j" display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo>
    <munder><mo>∑</mo> <mi>j</mi></munder> <mfrac><mrow><mi>sim</mi> <mo>(</mo><msub><mi>Q</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>K</mi> <mi>j</mi></msub> <mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>k</mi></msub> <mi>sim</mi> <mrow><mo>(</mo><msub><mi>Q</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>K</mi> <mi>k</mi></msub> <mo>)</mo></mrow></mrow></mfrac>
    <msub><mi>V</mi> <mi>j</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The trick behind linearized attention mechanisms is to express the similarity
    function as a *kernel function* that decomposes the operation into two pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal s normal i normal m left-parenthesis upper Q Subscript
    j Baseline comma upper K Subscript j Baseline right-parenthesis equals phi left-parenthesis
    upper Q Subscript i Baseline right-parenthesis Superscript upper T Baseline phi
    left-parenthesis upper K Subscript j Baseline right-parenthesis" display="block"><mrow><mi>sim</mi>
    <mrow><mo>(</mo> <msub><mi>Q</mi> <mi>j</mi></msub> <mo>,</mo> <msub><mi>K</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>ϕ</mi> <msup><mrow><mo>(</mo><msub><mi>Q</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mi>T</mi></msup> <mi>ϕ</mi> <mrow><mo>(</mo>
    <msub><mi>K</mi> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math alttext="phi"><mi>ϕ</mi></math> is typically a high-dimensional
    feature map. Since <math alttext="phi left-parenthesis upper Q Subscript i Baseline
    right-parenthesis"><mrow><mi>ϕ</mi> <mo>(</mo> <msub><mi>Q</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math> is independent of *j* and *k*, we can pull it under the
    sums to write the attention outputs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y Subscript i Baseline equals StartFraction phi left-parenthesis
    upper Q Subscript i Baseline right-parenthesis Superscript upper T Baseline sigma-summation
    Underscript j Endscripts phi left-parenthesis upper K Subscript j Baseline right-parenthesis
    upper V Subscript j Superscript upper T Baseline Over phi left-parenthesis upper
    Q Subscript i Baseline right-parenthesis Superscript upper T Baseline sigma-summation
    Underscript k Endscripts phi left-parenthesis upper K Subscript k Baseline right-parenthesis
    EndFraction" display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo>
    <mfrac><mrow><mi>ϕ</mi><msup><mrow><mo>(</mo><msub><mi>Q</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mi>T</mi></msup> <msub><mo>∑</mo> <mi>j</mi></msub> <mi>ϕ</mi><mrow><mo>(</mo><msub><mi>K</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow><msubsup><mi>V</mi> <mi>j</mi> <mi>T</mi></msubsup></mrow>
    <mrow><mi>ϕ</mi><msup><mrow><mo>(</mo><msub><mi>Q</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mi>T</mi></msup> <msub><mo>∑</mo> <mi>k</mi></msub> <mi>ϕ</mi><mrow><mo>(</mo><msub><mi>K</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: By first computing <math alttext="sigma-summation Underscript j Endscripts phi
    left-parenthesis upper K Subscript j Baseline right-parenthesis upper V Subscript
    j Superscript upper T"><mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mi>ϕ</mi> <mrow><mo>(</mo>
    <msub><mi>K</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <msubsup><mi>V</mi> <mi>j</mi>
    <mi>T</mi></msubsup></mrow></math> and <math alttext="sigma-summation Underscript
    k Endscripts phi left-parenthesis upper K Subscript k Baseline right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>k</mi></msub> <mi>ϕ</mi> <mrow><mo>(</mo> <msub><mi>K</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow></mrow></math> , we can effectively linearize the space and time
    complexity of self-attention! The comparison between the two approaches is illustrated
    in [Figure 11-7](#linear-attention). Popular models that implement linearized
    self-attention include Linear Transformer and Performer.^([8](ch11.xhtml#idm46238686603088))
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear attention](Images/nlpt_1107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-7\. Complexity difference between standard self-attention and linearized
    self-attention (courtesy of Tianyang Lin)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this section we’ve seen how Transformer architectures in general and attention
    in particular can be scaled up to achieve even better performance on a wide range
    of tasks. In the next section we’ll have a look at how transformers are branching
    out of NLP into other domains such as audio and computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Going Beyond Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using text to train language models has been the driving force behind the success
    of transformer language models, in combination with transfer learning. On the
    one hand, text is abundant and enables self-supervised training of large models.
    On the other hand, textual tasks such as classification and question answering
    are common, and developing effective strategies for them allows us to address
    a wide range of real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are limits to this approach, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Human reporting bias
  prefs: []
  type: TYPE_NORMAL
- en: The frequencies of events in text may not represent their true frequencies.^([9](ch11.xhtml#idm46238686592304))
    A model solely trained on text from the internet might have a very distorted image
    of the world.
  prefs: []
  type: TYPE_NORMAL
- en: Common sense
  prefs: []
  type: TYPE_NORMAL
- en: Common sense is a fundamental quality of human reasoning, but is rarely written
    down. As such, language models trained on text might know many facts about the
    world, but lack basic common-sense reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Facts
  prefs: []
  type: TYPE_NORMAL
- en: A probabilistic language model cannot store facts in a reliable way and can
    produce text that is factually wrong. Similarly, such models can detect named
    entities, but have no direct way to access information about them.
  prefs: []
  type: TYPE_NORMAL
- en: Modality
  prefs: []
  type: TYPE_NORMAL
- en: Language models have no way to connect to other modalities that could address
    the previous points, such as audio or visual signals or tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: So, if we could solve the modality limitations we could potentially address
    some of the others as well. Recently there has been a lot of progress in pushing
    transformers to new modalities, and even building multimodal models. In this section
    we’ll highlight a few of these advances.
  prefs: []
  type: TYPE_NORMAL
- en: Vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vision has been the stronghold of convolutional neural networks (CNNs) since
    they kickstarted the deep learning revolution. More recently, transformers have
    begun to be applied to this domain and to achieve efficiency similar to or better
    than CNNs. Let’s have a look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: iGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inspired by the success of the GPT family of models with text, iGPT (short for
    image GPT) applies the same methods to images.^([10](ch11.xhtml#idm46238686576800))
    By viewing images as sequences of pixels, iGPT uses the GPT architecture and autoregressive
    pretraining objective to predict the next pixel values. Pretraining on large image
    datasets enables iGPT to “autocomplete” partial images, as displayed in [Figure 11-8](#iGPT).
    It also achieves performant results on classification tasks when a classification
    head is added to the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![iGPT](Images/nlpt_1108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-8\. Examples of image completions with iGPT (courtesy of Mark Chen)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ViT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We saw that iGPT follows closely the GPT-style architecture and pretraining
    procedure. Vision Transformer (ViT)^([11](ch11.xhtml#idm46238686567504)) is a
    BERT-style take on transformers for vision, as illustrated in [Figure 11-9](#vit-architecture).
    First the image is split into smaller patches, and each of these patches is embedded
    with a linear projection. The results strongly resemble the token embeddings in
    BERT, and what follows is virtually identical. The patch embeddings are combined
    with position embeddings and then fed through an ordinary transformer encoder.
    During pretraining some of the patches are masked or distorted, and the objective
    is to predict the average color of the masked patch.
  prefs: []
  type: TYPE_NORMAL
- en: '![vit-architecture](Images/nlpt_1109.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-9\. The ViT architecture (courtesy of Alexey Dosovitskiy et al.)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although this approach did not produce better results when pretrained on the
    standard ImageNet dataset, it scaled significantly better than CNNs on larger
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'ViT is integrated in ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, and
    using it is very similar to the NLP pipelines that we’ve used throughout this
    book. Let’s start by loading the image of a rather famous dog:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_doge.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To load a ViT model, we just need to specify the `image-classification` pipeline,
    and then we feed in the image to extract the predicted classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '|  | score | label |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.643599 | Eskimo dog, husky |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.207407 | Siberian husky |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.060160 | dingo, warrigal, warragal, Canis dingo |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.035359 | Norwegian elkhound, elkhound |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.012927 | malamute, malemute, Alaskan malamute |'
  prefs: []
  type: TYPE_TB
- en: Great, the predicted class seems to match the image!
  prefs: []
  type: TYPE_NORMAL
- en: A natural extension of image models is video models. In addition to the spatial
    dimensions, videos come with a temporal dimension. This makes the task more challenging
    as the volume of data gets much bigger and one needs to deal with the extra dimension.
    Models such as TimeSformer introduce a spatial and temporal attention mechanism
    to account for both.^([12](ch11.xhtml#idm46238686424832)) In the future, such
    models can help build tools for a wide range of tasks such as classification or
    annotation of video sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of data, such as customer data within a company, is stored in structured
    databases instead of as raw text. We saw in [Chapter 7](ch07.xhtml#chapter_qa)
    that with question answering models we can query text with a question in natural
    text. Wouldn’t it be nice if we could do the same with tables, as shown in [Figure 11-10](#tapas-table)?
  prefs: []
  type: TYPE_NORMAL
- en: '![table-qa](Images/nlpt_1110.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-10\. Question answering over a table (courtesy of Jonathan Herzig)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TAPAS (short for Table Parser)^([13](ch11.xhtml#idm46238686413936)) to the rescue!
    This model applies the Transformer architecture to tables by combining the tabular
    information with the query, as illustrated in [Figure 11-11](#tapas-architecture).
  prefs: []
  type: TYPE_NORMAL
- en: '![tapas-architecture](Images/nlpt_1111.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-11\. Architecture of TAPAS (courtesy of Jonathan Herzig)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s look at an example of how TAPAS works in practice. We have created a
    fictitious version of this book’s table of contents. It contains the chapter number,
    the name of the chapter, as well as the starting and ending pages of the chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also easily add the number of pages each chapter has with the existing
    fields. In order to play nicely with the TAPAS model, we need to make sure that
    all columns are of type `str`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '|  | chapter | name | start_page | end_page | number_of_pages |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | Introduction | 1 | 11 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | Text classification | 12 | 48 | 36 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | Named Entity Recognition | 49 | 73 | 24 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 3 | Question Answering | 74 | 120 | 46 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 4 | Summarization | 121 | 140 | 19 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 5 | Conclusion | 141 | 144 | 3 |'
  prefs: []
  type: TYPE_TB
- en: 'By now you should know the drill. We first load the `table-question-answering`
    pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'and then pass some queries to extract the answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'These predictions store the type of table operation in an `aggregator` field,
    along with the answer. Let’s see how well TAPAS fared on our questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For the first chapter, the model predicted exactly one cell with no aggregation.
    If we look at the table, we see that the answer is in fact correct. In the next
    example the model predicted all the cells containing the number of pages in combination
    with the sum aggregator, which again is the correct way of calculating the total
    number of pages. The answer to question three is also correct; the average aggregation
    is not necessary in that case, but it doesn’t make a difference. Finally, we have
    a question that is a little bit more complex. To determine how many chapters have
    more than 20 pages we first need to find out which chapters satisfy that criterion
    and then count them. It seem that TAPAS again got it right and correctly determined
    that chapters 1, 2, and 3 have more than 20 pages, and added a count aggregator
    to the cells.
  prefs: []
  type: TYPE_NORMAL
- en: The kinds of questions we asked can also be solved with a few simple Pandas
    commands; however, the ability to ask questions in natural language instead of
    Python code allows a much wider audience to query the data to answer specific
    questions. Imagine such tools in the hands of business analysts or managers who
    are able verify their own hypotheses about the data!
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far we’ve looked at extending transformers to a single new modality. TAPAS
    is arguably multimodal since it combines text and tables, but the table is also
    treated as text. In this section we examine transformers that combine two modalities
    at once: audio plus text and vision plus text.'
  prefs: []
  type: TYPE_NORMAL
- en: Speech-to-Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although being able to use text to interface with a computer is a huge step
    forward, using spoken language is an even more natural way for us to communicate.
    You can see this trend in industry, where applications such as Siri and Alexa
    are on the rise and becoming progressively more useful. Also, for a large fraction
    of the population, writing and reading are more challenging than speaking. So,
    being able to process and understand audio is not only convenient, but can help
    many people access more information. A common task in this domain is *automatic
    speech recognition* (ASR), which converts spoken words to text and enables voice
    technologies like Siri to answer questions like “What is the weather like today?”
  prefs: []
  type: TYPE_NORMAL
- en: 'The [wav2vec 2.0](https://oreil.ly/tPpC7) family of models are one of the most
    recent developments in ASR: they use a transformer layer in combination with a
    CNN, as illustrated in [Figure 11-12](#wav2vec2).^([14](ch11.xhtml#idm46238685996336))
    By leveraging unlabeled data during pretraining, these models achieve competitive
    results with only a few minutes of labeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![wav2vec2](Images/nlpt_1112.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-12\. Architecture of wav2vec 2.0 (courtesy of Alexei Baevski)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The wav2vec 2.0 models are integrated in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, and you won’t be surprised to learn that loading and using them
    follows the familiar steps that we have seen throughout this book. Let’s load
    a pretrained model that was trained on 960 hours of speech audio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply this model to some audio files we’ll use the ASR subset of the [SUPERB
    dataset](https://oreil.ly/iBAK8), which is the same dataset the model was pretrained
    on. Since the dataset is quite large, we’ll just load one example for our demo
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see that the audio in the `file` column is stored in the FLAC coding
    format, while the expected transcription is given by the `text` column. To convert
    the audio to an array of floats, we can use the [*SoundFile* library](https://oreil.ly/eo106)
    to read each file in our dataset with `map()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using a Jupyter notebook you can easily play the sound files with
    the following `IPython` widgets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can pass the inputs to the pipeline and inspect the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This transcription seems to be correct. We can see that some punctuation is
    missing, but this is hard to get from audio alone and could be added in a postprocessing
    step. With only a handful of lines of code we can build ourselves a state-of-the-art
    speech-to-text application!
  prefs: []
  type: TYPE_NORMAL
- en: Building a model for a new language still requires a minimum amount of labeled
    data, which can be challenging to obtain, especially for low-resource languages.
    Soon after the release of wav2vec 2.0, a paper describing a method named wav2vec-U
    was published.^([15](ch11.xhtml#idm46238685789072)) In this work, a combination
    of clever clustering and GAN training is used to build a speech-to-text model
    using only independent unlabeled speech and unlabeled text data. This process
    is visualized in detail in [Figure 11-13](#wav2vec-u). No aligned speech and text
    data is required at all, which enables the training of highly performant speech-to-text
    models for a much larger spectrum of languages.
  prefs: []
  type: TYPE_NORMAL
- en: '![wav2vec-u](Images/nlpt_1113.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-13\. Training scheme for wav2vec-U (courtesy of Alexsei Baevski)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Great, so transformers can now “read” text and “hear” audio—can they also “see”?
    The answer is yes, and this is one of the current hot research frontiers in the
    field.
  prefs: []
  type: TYPE_NORMAL
- en: Vision and Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Vision and text are another natural pair of modalities to combine since we
    frequently use language to communicate and reason about the contents of images
    and videos. In addition to the vision transformers, there have been several developments
    in the direction of combining visual and textual information. In this section
    we will look at four examples of models combining vision and text: VisualQA, LayoutLM,
    DALL·E, and CLIP.'
  prefs: []
  type: TYPE_NORMAL
- en: VQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.xhtml#chapter_qa) we explored how we can use transformer
    models to extract answers to text-based questions. This can be done ad hoc to
    extract information from texts or offline, where the question answering model
    is used to extract structured information from a set of documents. There have
    been several efforts to expand this approach to vision with datasets such as VQA,^([16](ch11.xhtml#idm46238685749344))
    shown in [Figure 11-14](#vqa).
  prefs: []
  type: TYPE_NORMAL
- en: '![vqa](Images/nlpt_1114.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-14\. Example of a visual question answering task from the VQA dataset
    (courtesy of Yash Goyal)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Models such as LXMERT and VisualBERT use vision models like ResNets to extract
    features from the pictures and then use transformer encoders to combine them with
    the natural questions and predict an answer.^([17](ch11.xhtml#idm46238685739392))
  prefs: []
  type: TYPE_NORMAL
- en: LayoutLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Analyzing scanned business documents like receipts, invoices, or reports is
    another area where extracting visual and layout information can be a useful way
    to recognize text fields of interest. Here the [LayoutLM](https://oreil.ly/uQc5t)
    family of models are the current state of the art. They use an enhanced Transformer
    architecture that receives three modalities as input: text, image, and layout.
    Accordingly, as shown in [Figure 11-15](#layoutlm), there are embedding layers
    associated with each modality, a spatially aware self-attention mechanism, and
    a mix of image and text/image pretraining objectives to align the different modalities.
    By pretraining on millions of scanned documents, LayoutLM models are able to transfer
    to various downstream tasks in a manner similar to BERT for NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: '![layoutlm](Images/nlpt_1115.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-15\. The model architecture and pretraining strategies for LayoutLMv2
    (courtesy of Yang Xu)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: DALL·E
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A model that combines vision and text for *generative* tasks is DALL·E.^([18](ch11.xhtml#idm46238685725504))
    It uses the GPT architecture and autoregressive modeling to generate images from
    text. Inspired by iGPT, it regards the words and pixels as one sequence of tokens
    and is thus able to continue generating an image from a text prompt, as shown
    in [Figure 11-16](#dall-e).
  prefs: []
  type: TYPE_NORMAL
- en: '![dall-e](Images/nlpt_1116.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-16\. Generation examples with DALL·E (courtesy of Aditya Ramesh)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: CLIP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, let’s have a look at CLIP,^([19](ch11.xhtml#idm46238685716624)) which
    also combines text and vision but is designed for supervised tasks. Its creators
    constructed a dataset with 400 million image/caption pairs and used contrastive
    learning to pretrain the model. The CLIP architecture consists of a text and an
    image encoder (both transformers) that create embeddings of the captions and images.
    A batch of images with captions is sampled, and the contrastive objective is to
    maximize the similarity of the embeddings (as measured by the dot product) of
    the corresponding pair while minimizing the similarity of the rest, as illustrated
    in [Figure 11-17](#clip-arch).
  prefs: []
  type: TYPE_NORMAL
- en: In order to use the pretrained model for classification the possible classes
    are embedded with the text encoder, similar to how we used the zero-shot pipeline.
    Then the embeddings of all the classes are compared to the image embedding that
    we want to classify, and the class with the highest similarity is chosen.
  prefs: []
  type: TYPE_NORMAL
- en: '![clip-arch](Images/nlpt_1117.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-17\. Architecture of CLIP (courtesy of Alec Radford)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The zero-shot image classification performance of CLIP is remarkable and competitive
    with fully supervised trained vision models, while being more flexible with regard
    to new classes. CLIP is also fully integrated in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, so we can try it out. For image-to-text tasks, we instantiate a
    *processor* that consists of a *feature extractor* and a tokenizer. The role of
    the feature extractor is to convert the image into a form suitable for the model,
    while the tokenizer is responsible for decoding the model’s predictions into text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Then we need a fitting image to try it out. What would be better suited than
    a picture of Optimus Prime?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_11in01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we set up the texts to compare the image against and pass it through
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Well, it almost got the right answer (a photo of AGI of course). Jokes aside,
    CLIP makes image classification very flexible by allowing us to define classes
    through text instead of having the classes hardcoded in the model architecture.
    This concludes our tour of multimodal transformer models, but we hope we’ve whetted
    your appetite.
  prefs: []
  type: TYPE_NORMAL
- en: Where to from Here?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well that’s the end of the ride; thanks for joining us on this journey through
    the transformers landscape! Throughout this book we’ve explored how transformers
    can address a wide range of tasks and achieve state-of-the-art results. In this
    chapter we’ve seen how the current generation of models are being pushed to their
    limits with scaling and how they are also branching out into new domains and modalities.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to reinforce the concepts and skills that you’ve learned in this
    book, here are a few ideas for where to go from here:'
  prefs: []
  type: TYPE_NORMAL
- en: Join a Hugging Face community event
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face hosts short sprints focused on improving the libraries in the ecosystem,
    and these events are a great way to meet the community and get a taste for open
    source software development. So far there have been sprints on adding 600+ datasets
    to ![nlpt_pin01](Images/nlpt_pin01.png) Datasets, fine-tuning 300+ ASR models
    in various languages, and implementing hundreds of projects in JAX/Flax.
  prefs: []
  type: TYPE_NORMAL
- en: Build your own project
  prefs: []
  type: TYPE_NORMAL
- en: One very effective way to test your knowledge in machine learning is to build
    a project to solve a problem that interests you. You could reimplement a transformer
    paper, or apply transformers to a novel domain.
  prefs: []
  type: TYPE_NORMAL
- en: Contribute a model to ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
  prefs: []
  type: TYPE_NORMAL
- en: If you’re looking for something more advanced, then contributing a newly published
    architecture to ![nlpt_pin01](Images/nlpt_pin01.png) Transformers is a great way
    to dive into the nuts and bolts of the library. There is a detailed guide to help
    you get started in the ​![nlpt_pin01](Images/nlpt_pin01.png)⁠ [Transformers documentation](https://oreil.ly/3f4wZ).
  prefs: []
  type: TYPE_NORMAL
- en: Blog about what you’ve learned
  prefs: []
  type: TYPE_NORMAL
- en: Teaching others what you’ve learned is a powerful test of your own knowledge,
    and in a sense this was one of the driving motivations behind us writing this
    book! There are great tools to help you get started with technical blogging; we
    recommend [*fastpages*](https://oreil.ly/f0L9u) as you can easily use Jupyter
    notebooks for everything.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch11.xhtml#idm46238686826448-marker)) J. Kaplan et al., [“Scaling Laws
    for Neural Language Models”](https://arxiv.org/abs/2001.08361), (2020).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch11.xhtml#idm46238686819248-marker)) The dataset size is measured in
    the number of tokens, while the model size excludes parameters from the embedding
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch11.xhtml#idm46238686793248-marker)) T. Henighan et al., [“Scaling Laws
    for Autoregressive Generative Modeling”](https://arxiv.org/abs/2010.14701), (2020).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch11.xhtml#idm46238686775136-marker)) However, recently a distributed
    deep learning framework has been proposed that enables smaller groups to pool
    their computational resources and pretrain models in a collaborative fashion.
    See M. Diskin et al., [“Distributed Deep Learning in Open Collaborations”](https://arxiv.org/abs/2106.10207),
    (2021).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch11.xhtml#idm46238686737440-marker)) Although standard implementations
    of self-attention have <math alttext="upper O left-parenthesis n squared right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <msup><mi>n</mi> <mn>2</mn></msup> <mo>)</mo></mrow></math> time and
    memory complexity, a [recent paper by Google researchers](https://arxiv.org/abs/2112.05682)
    shows that the memory complexity can be reduced to <math alttext="upper O left-parenthesis
    log n right-parenthesis"><mrow><mi>O</mi> <mo>(</mo> <mo form="prefix">log</mo>
    <mi>n</mi> <mo>)</mo></mrow></math> via a simple reordering of the operations.
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch11.xhtml#idm46238686724512-marker)) Yi Tay et al., [“Efficient Transformers:
    A Survey”](https://arxiv.org/abs/2009.06732), (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch11.xhtml#idm46238686714320-marker)) T. Lin et al., [“A Survey of Transformers”](https://arxiv.org/abs/2106.04554),
    (2021).
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch11.xhtml#idm46238686603088-marker)) A. Katharopoulos et al., [“Transformers
    Are RNNs: Fast Autoregressive Transformers with Linear Attention”](https://arxiv.org/abs/2006.16236),
    (2020); K. Choromanski et al., [“Rethinking Attention with Performers”](https://arxiv.org/abs/2009.14794),
    (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch11.xhtml#idm46238686592304-marker)) J. Gordon and B. Van Durme, [“Reporting
    Bias and Knowledge Extraction”](https://openreview.net/pdf?id=AzxEzvpdE3Wcy),
    (2013).
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch11.xhtml#idm46238686576800-marker)) M. Chen et al., “Generative Pretraining
    from Pixels,” *Proceedings of the 37th International Conference on Machine Learning*
    119 (2020):1691–1703, [*https://proceedings.mlr.press/v119/chen20s.html*](https://proceedings.mlr.press/v119/chen20s.html).
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch11.xhtml#idm46238686567504-marker)) A. Dosovitskiy et al., [“An Image
    Is Worth 16x16 Words: Transformers for Image Recognition at Scale”](https://arxiv.org/abs/2010.11929),
    (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch11.xhtml#idm46238686424832-marker)) G. Bertasius, H. Wang, and L. Torresani,
    [“Is Space-Time Attention All You Need for Video Understanding?”](https://arxiv.org/abs/2102.05095),
    (2021).
  prefs: []
  type: TYPE_NORMAL
- en: '^([13](ch11.xhtml#idm46238686413936-marker)) J. Herzig et al., [“TAPAS: Weakly
    Supervised Table Parsing via Pre-Training”](https://arxiv.org/abs/2004.02349),
    (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([14](ch11.xhtml#idm46238685996336-marker)) A. Baevski et al., [“wav2vec 2.0:
    A Framework for Self-Supervised Learning of Speech Representations”](https://arxiv.org/abs/2006.11477),
    (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch11.xhtml#idm46238685789072-marker)) A. Baevski et al., [“Unsupervised
    Speech Recognition”](https://arxiv.org/abs/2105.11084), (2021).
  prefs: []
  type: TYPE_NORMAL
- en: '^([16](ch11.xhtml#idm46238685749344-marker)) Y. Goyal et al., [“Making the
    V in VQA Matter: Elevating the Role of Image Understanding in Visual Question
    Answering”](https://arxiv.org/abs/1612.00837), (2016).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch11.xhtml#idm46238685739392-marker)) H. Tan and M. Bansal, [“LXMERT:
    Learning Cross-Modality Encoder Representations from Transformers”](https://arxiv.org/abs/1908.07490),
    (2019); L.H. Li et al., [“VisualBERT: A Simple and Performant Baseline for Vision
    and Language”](https://arxiv.org/abs/1908.03557), (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch11.xhtml#idm46238685725504-marker)) A. Ramesh et al., [“Zero-Shot Text-to-Image
    Generation”](https://arxiv.org/abs/2102.12092), (2021).
  prefs: []
  type: TYPE_NORMAL
- en: ^([19](ch11.xhtml#idm46238685716624-marker)) A. Radford et al., [“Learning Transferable
    Visual Models from Natural Language Supervision”](https://arxiv.org/abs/2103.00020),
    (2021).
  prefs: []
  type: TYPE_NORMAL
