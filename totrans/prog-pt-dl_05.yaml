- en: Chapter 5\. Text Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。文本分类
- en: 'We’re leaving images behind for now and turning our attention to another area
    where deep learning has proven to be a significant advance on traditional techniques:
    *natural language processing (NLP)*. A good example of this is Google Translate.
    Originally, the code that handled translation was a weighty 500,000 lines of code.
    The new, TensorFlow-based system has approximately 500, and it performs better
    than the old method.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们暂时离开图像，转而关注另一个领域，深度学习在传统技术上已经被证明是一个重大进步的地方：自然语言处理（NLP）。一个很好的例子是谷歌翻译。最初，处理翻译的代码有500,000行代码。基于TensorFlow的新系统大约有500行代码，性能比旧方法更好。
- en: Recent breakthroughs also have occurred in bringing transfer learning (which
    you learned about in [Chapter 4](ch04.html#transfer-learning-and-other-tricks))
    to NLP problems. New architectures such as the Transformer architecture have led
    to the creation of networks like OpenAI’s GPT-2, the larger variant of which produces
    text that is almost human-like in quality (and in fact, OpenAI has not released
    the weights of this model for fear of it being used maliciously).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的突破还发生在将迁移学习（你在[第4章](ch04.html#transfer-learning-and-other-tricks)中学到的）引入到NLP问题中。新的架构，如Transformer架构，已经导致了像OpenAI的GPT-2这样的网络的创建，其更大的变体产生的文本几乎具有人类般的质量（事实上，OpenAI没有发布这个模型的权重，因为担心它被恶意使用）。
- en: This chapter provides a whirlwind tour of recurrent neural networks and embeddings.
    Then we explore the `torchtext` library and how to use it for text processing
    with an LSTM-based model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了循环神经网络和嵌入的快速介绍。然后我们探讨了`torchtext`库以及如何使用基于LSTM的模型进行文本处理。
- en: Recurrent Neural Networks
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: 'If we look back at how we’ve been using our CNN-based architectures so far,
    we can see they have always been working on one complete snapshot of time. But
    consider these two sentence fragments:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下迄今为止我们如何使用基于CNN的架构，我们可以看到它们总是在一个完整的时间快照上工作。但考虑这两个句子片段：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Say you were to feed those two sentences, one after the other, into a CNN and
    ask, *where is the cat?* You’d have a problem, because the network has no concept
    of *memory*. This is incredibly important when it comes to dealing with data that
    has a temporal domain (e.g., text, speech, video, and time-series data).^([1](ch05.html#idm45762363068216))
    *Recurrent neural networks* (RNNs) answer this problem by giving neural networks
    a memory via *hidden state*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你将这两个句子一个接一个地馈送到CNN中，并问，“猫在哪里？”你会遇到问题，因为网络没有“记忆”的概念。当处理具有时间域的数据时（例如文本、语音、视频和时间序列数据）这是非常重要的。通过*循环神经网络*（RNNs）通过*隐藏状态*给神经网络提供了记忆来解决这个问题。
- en: What does an RNN look like? My favorite explanation is, “Imagine a neural network
    crossed with a `for` loop.” [Figure 5-1](#classical-rnn) shows a diagram of a
    classical RNN structure.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是什么样子的？我最喜欢的解释是，“想象一个神经网络和一个`for`循环相交。” [图5-1](#classical-rnn)显示了一个经典RNN结构的图表。
- en: '![Classical RNN diagram](assets/ppdl_0501.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![经典RNN图](assets/ppdl_0501.png)'
- en: Figure 5-1\. An RNN
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1。一个RNN
- en: We add input at a time step of *t*, and we get a *hidden* output state of *ht*,
    and the output also gets fed back into the RNN for the next time step. We can
    unroll this network to take a deeper look at what’s going on, as shown in [Figure 5-2](#unrolled-rnn).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在时间步*t*添加输入，得到*ht*的*隐藏*输出状态，并且输出也被馈送回RNN用于下一个时间步。我们可以展开这个网络，深入了解正在发生的事情，如[图5-2](#unrolled-rnn)所示。
- en: '![Unrolled RNN diagram](assets/ppdl_0502.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: ！[展开的RNN图](assets/ppdl_0502.png)
- en: Figure 5-2\. An unrolled RNN
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2。一个展开的RNN
- en: What we have here is a grouping of fully connected layers (with shared parameters),
    a series of inputs, and our output. Input data is fed into the network, and the
    next item in the sequence is predicted as output. In the unrolled view, we can
    see that the RNN can be thought of as a pipeline of fully connected layers, with
    the successive input being fed into the next layer in the sequence (with the usual
    nonlinearities such as `ReLU` being inserted between the layers). When we have
    our completed predicted sequence, we then have to backpropagate the error back
    through the RNN. Because this involves stepping back through the network’s steps,
    this process is known as backpropagation through time. The error is calculated
    on the entire sequence, then the network is unfolded as in [Figure 5-2](#unrolled-rnn),
    and the gradients are calculated for each time step and combined to update the
    shared parameters of the network. You can imagine it as doing backprop on individual
    networks and summing all the gradients together.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里有一组全连接层（具有共享参数）、一系列输入和输出。输入数据被馈送到网络中，然后预测输出作为序列中的下一个项目。在展开的视图中，我们可以看到RNN可以被看作是一系列全连接层的管道，连续的输入被馈送到序列中的下一层（在层之间插入了通常的非线性，如`ReLU`）。当我们完成了预测的序列后，我们必须通过RNN将错误反向传播回去。因为这涉及到通过网络的步骤向后走，这个过程被称为通过时间的反向传播。错误是在整个序列上计算的，然后网络像[图5-2](#unrolled-rnn)中展开一样，为每个时间步计算梯度并组合以更新网络的共享参数。你可以想象它是在单独的网络上进行反向传播，然后将所有梯度相加。
- en: That’s the theory behind RNNs. But this simple structure has problems that we
    need to talk about and how they were overcome with newer architectures.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是RNN的理论。但这种简单的结构存在问题，我们需要讨论如何通过更新的架构来克服这些问题。
- en: Long Short-Term Memory Networks
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: In practice, RNNs were and are particularly susceptible to the *vanishing gradient*
    problem we talked about in [Chapter 2](ch02.html#image-classification-with-pytorch),
    or the potentially worse scenario of the *exploding gradient*, where your error
    tends off toward infinity. Neither is good, so RNNs couldn’t be brought to bear
    on many of the problems they were considered suitable for. That all changed in
    1997 when Sepp Hochreiter and Jürgen Schmidhuber introduced the Long Short-Term
    Memory (LSTM) variant of the RNN.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-3](#lstm-diagram) diagrams an LSTM layer. I know, there’s a lot going
    on here, but it’s not too complex. Honest.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![LSTM diagram](assets/ppdl_0503.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. An LSTM
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: OK, I admit, it is quite intimidating. The key is to think about the three gates
    (input, output, and forget). In a standard RNN, we “remember” everything forever.
    But that’s not how our brains work (sadly!), and the LSTM’s forget gate allows
    us to model the idea that as we continue in our input chain, the beginning of
    the chain becomes less important. And how much the LSTM forgets is something that
    is learned during training, so if it’s in the network’s best interest to be very
    forgetful, the forget gate parameters will do so.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The *cell* ends up being the “memory” of the network layer; and the input, output,
    and forget gates will determine how data flows through the layer. The data may
    simply pass through, it may “write” to the cell, and that data may (or may not!)
    flow through to the next layer, modified by the output gate.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: This assemblage of parts was enough to solve the vanishing gradient problem,
    and also has the virtue of being Turing-complete, so theoretically, you can do
    any calculation that you can do on a computer with one of these.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: But things didn’t stop there, of course. Several developments have occurred
    in the RNN space since LSTMs, and we’ll cover some of the major ones in the next
    sections.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Gated Recurrent Units
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since 1997, many variants of the base LSTM network have been created, most of
    which you probably don’t need to know about unless you’re curious. However, one
    variant that came along in 2014, the gated recurrent unit (GRU), is worth knowing
    about, as it has become quite popular in some circles. [Figure 5-4](#gru-diagram)
    shows the makeup of a GRU architecture.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![GRU diagram](assets/ppdl_0504.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. A GRU
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The main takeaway is that the GRU has merged the forget gate with the output
    gate. This means that it has fewer parameters than an LSTM and so tends to be
    quicker to train and uses fewer resources at runtime. For these reasons, and also
    that they’re essentially a drop-in replacement for LSTMs, they’ve become quite
    popular. However, strictly speaking, they are less powerful than LSTMs because
    of the merging of the forget and output gates, so in general I recommend playing
    with both GRUs or LSTMs in your network and seeing which one performs better.
    Or just accept that the LSTM may be a little slower in training, but may end up
    being the best choice in the end. You don’t have to follow the latest fad—honest!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: biLSTM
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another common variant of the LSTM is the *bidirectional* LSTM or *biLSTM* for
    short. As you’ve seen so far, traditional LSTMs (and RNNs in general) can look
    to the past as they are trained and make decisions. Unfortunately, sometimes you
    need to see the future as well. This is particularly the case in applications
    like translation and handwriting recognition, where what comes after the current
    state can be just as important as the previous state for determining output.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'A biLSTM solves this problem in the simplest of ways: it’s essentially two
    stacked LSTMs, with the input being sent in the forward direction in one LSTM
    and reversed in the second. [Figure 5-5](#bilstm-diagram) shows how a biLSTM works
    across its input bidirectionally to produce the output.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![biLSTM diagram](assets/ppdl_0505.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. A biLSTM
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch makes it easy to create biLSTMs by passing in a `bidirectional=True`
    parameter when creating an `LSTM()` unit, as you’ll see later in the chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch通过在创建`LSTM()`单元时传入`bidirectional=True`参数来轻松创建双向LSTM，您将在本章后面看到。
- en: That completes our tour throughout the RNN-based architectures. In [Chapter 9](ch09.html#pytorch_in_the_wild),
    we return to the question of architecture when we look at the Transformer-based
    BERT and GPT-2 models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对基于RNN的架构的介绍。在[第9章](ch09.html#pytorch_in_the_wild)中，当我们研究基于Transformer的BERT和GPT-2模型时，我们将回到架构问题。
- en: Embeddings
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入
- en: 'We’re almost at the point where we can start writing some code! But before
    we do, one little detail may have occurred to you: how do we represent words in
    a network? After all, we’re feeding tensors of numbers into a network and getting
    tensors out. With images, it seemed a fairly obvious thing to convert them into
    tensors representing the red/green/blue component values, and they’re already
    naturally thought of as arrays as they come with a height and width baked in.
    But words? Sentences? How is that going to work?'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎可以开始编写一些代码了！但在此之前，您可能会想到一个小细节：我们如何在网络中表示单词？毕竟，我们正在将数字张量输入到网络中，并获得张量输出。对于图像，将它们转换为表示红/绿/蓝分量值的张量似乎是一件很明显的事情，因为它们已经自然地被认为是数组，因为它们带有高度和宽度。但是单词？句子？这将如何运作？
- en: 'The simplest approach is still one that you’ll find in many approaches to NLP,
    and it’s called *one-hot encoding*. It’s pretty simple! Let’s look at our first
    sentence from the start of the chapter:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法仍然是许多自然语言处理方法中常见的方法之一，称为*one-hot编码*。这很简单！让我们从本章开头的第一句话开始看：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If we consider that this is the entire vocabulary of our world, we have a tensor
    of `[the, cat, sat, on, mat]`. One-hot encoding simply means that we create a
    vector that is the size of the vocabulary, and for each word in it, we allocate
    a vector with one parameter set to 1 and the rest to 0:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑这是我们世界的整个词汇表，我们有一个张量`[the, cat, sat, on, mat]`。one-hot编码简单地意味着我们创建一个与词汇表大小相同的向量，并为其中的每个单词分配一个向量，其中一个参数设置为1，其余设置为0：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We’ve now converted the words into vectors, and we can feed them into our network.
    Additionally, we may add extra symbols into our vocabulary, such as `UNK` (unknown,
    for words not in the vocabulary) and `START/STOP` to signify the beginning and
    ends of sentences.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经将单词转换为向量，可以将它们输入到我们的网络中。此外，我们可以向我们的词汇表中添加额外的符号，比如`UNK`（未知，用于不在词汇表中的单词）和`START/STOP`来表示句子的开头和结尾。
- en: 'One-hot encoding has a few limitations that become clearer when we add another
    word into our example vocabulary: *kitty*. From our encoding scheme, *kitty* would
    be represented by `[0 0 0 0 0 1]` (with all the other vectors being padded with
    a zero). First, you can see that if we are going to model a realistic set of words,
    our vectors are going to be very long with almost no information in them. Second,
    and perhaps more importantly, we know that a *very strong* relationship exists
    between the words *kitty* and *cat* (also with *dammit*, but thankfully that’s
    been skipped from our vocab here!), and this is impossible to represent with one-hot
    encoding; the two words are completely different things.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在示例词汇表中添加另一个单词时，one-hot编码会显示出一些限制：*kitty*。根据我们的编码方案，*kitty*将由`[0 0 0 0 0
    1]`表示（其他向量都用零填充）。首先，您可以看到，如果我们要建模一个现实的单词集，我们的向量将非常长，几乎没有信息。其次，也许更重要的是，我们知道*猫*和*小猫*之间存在*非常强烈*的关系（还有*该死*，但幸运的是在我们的词汇表中被跳过了！），这是无法用one-hot编码表示的；这两个单词是完全不同的东西。
- en: An approach that has become more popular recently is replacing one-hot encoding
    with an *embedding matrix* (of course, a one-hot encoding is an embedding matrix
    itself, just one that doesn’t contain any information about relationships between
    words). The idea is to squash the dimensionality of the vector space down to something
    a little more manageable and take advantage of the space itself.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最近变得更受欢迎的一种方法是用*嵌入矩阵*替换one-hot编码（当然，one-hot编码本身就是一个嵌入矩阵，只是不包含有关单词之间关系的任何信息）。这个想法是将向量空间的维度压缩到更易管理的尺寸，并利用空间本身的优势。
- en: For example, if we have an embedding in a 2D space, perhaps *cat* could be represented
    by the tensor `[0.56, 0.45`] and *kitten* by `[0.56, 0.445]`, whereas *mat* could
    be `[0.2, -0.1]`. We cluster similar words together in the vector space and can
    do distance checks such as Euclidean or cosine distance functions to determine
    how close words are to each other. And how do we determine where words fall in
    the vector space? An embedding layer is no different from any other layer you’ve
    seen so far in building neural networks; we initialize the vector space randomly,
    and hopefully the training process updates the parameters so that similar words
    or concepts gravitate toward each other.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们在2D空间中有一个嵌入，也许*cat*可以用张量`[0.56, 0.45]`表示，*kitten*可以用`[0.56, 0.445]`表示，而*mat*可以用`[0.2,
    -0.1]`表示。我们在向量空间中将相似的单词聚集在一起，并可以使用欧几里得或余弦距离函数进行距离检查，以确定单词之间的接近程度。那么我们如何确定单词在向量空间中的位置呢？嵌入层与您迄今在构建神经网络中看到的任何其他层没有区别；我们随机初始化向量空间，希望训练过程更新参数，使相似的单词或概念相互靠近。
- en: A famous example of embedding vectors is *word2vec*, which was released by Google
    in 2013.^([2](ch05.html#idm45762362963656)) This was a set of word embeddings
    trained using a shallow neural network, and it revealed that the transformation
    into vector space seemed to capture something about the concepts underpinning
    the words. In its commonly cited finding, if you pulled the vectors for *King*,
    *Man*, and *Woman* and then subtracted the vector for *Man* from *King* and added
    the vector for *Woman*, you would get a result that was the vector representation
    for *Queen*. Since *word2vec*, other pretrained embeddings have become available,
    such as *ELMo*, *GloVe*, and *fasttext*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量的一个著名例子是*word2vec*，它是由Google在2013年发布的。这是使用浅层神经网络训练的一组词嵌入，它揭示了向量空间转换似乎捕捉到了有关支撑单词的概念的一些内容。在其常被引用的发现中，如果您提取*King*、*Man*和*Woman*的向量，然后从*King*中减去*Man*的向量并加上*Woman*的向量，您将得到一个代表*Queen*的向量表示。自从*word2vec*以来，其他预训练的嵌入也已经可用，如*ELMo*、*GloVe*和*fasttext*。
- en: 'As for using embeddings in PyTorch, it’s really simple:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 至于在PyTorch中使用嵌入，非常简单：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will contain a tensor of `vocab_size` x `dimension_size` initialized randomly.
    I prefer to think that it’s just a giant array or lookup table. Each word in your
    vocabulary indexes into an entry that is a vector of `dimension_size`, so if we
    go back to our cat and its epic adventures on the mat, we’d have something like
    this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这将包含一个`vocab_size` x `dimension_size`的张量，随机初始化。我更喜欢认为它只是一个巨大的数组或查找表。您词汇表中的每个单词索引到一个大小为`dimension_size`的向量条目，所以如果我们回到我们的猫及其在垫子上的史诗般的冒险，我们会得到这样的东西：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We create our embedding, a tensor that contains the position of *cat* in our
    vocabulary, and pass it through the layer’s `forward()` method. That gives us
    our random embedding. The result also points out that we have a gradient function
    that we can use for updating the parameters after we combine it with a loss function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了我们的嵌入，一个包含*cat*在我们词汇表中位置的张量，并通过层的`forward()`方法传递它。这给了我们我们的随机嵌入。结果还指出，我们有一个梯度函数，我们可以在将其与损失函数结合后用于更新参数。
- en: We’ve now gone through all the theory and can get started on building something!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经学习了所有的理论，可以开始构建一些东西了！
- en: torchtext
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torchtext
- en: Just like `torchvision`, PyTorch provides an official library, `torchtext`,
    for handling text-processing pipelines. However, `torchtext` is not quite as battle-tested
    or has as many eyes on it as `torchvision`, which means it’s not quite as easy
    to use or as well-documented. But it is still a powerful library that can handle
    a lot of the mundane work of building up text-based datasets, so we’ll be using
    it for the rest of the chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`torchvision`一样，PyTorch提供了一个官方库`torchtext`，用于处理文本处理管道。然而，`torchtext`并没有像`torchvision`那样经过充分测试，也没有像`torchvision`那样受到很多关注，这意味着它不太容易使用或文档不够完善。但它仍然是一个强大的库，可以处理构建基于文本的数据集的许多琐碎工作，所以我们将在本章的其余部分中使用它。
- en: 'Installing `torchtext` is fairly simple. You use either standard `pip`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`torchtext`相当简单。您可以使用标准的`pip`：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'or a specific `conda` channel:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 或者特定的`conda`渠道：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You’ll also want to install *spaCy* (an NLP library), and pandas if you don’t
    have them on your system (again, either using `pip` or `conda`). We use *spaCy*
    for processing our text in the `torchtext` pipeline, and pandas for exploring
    and cleaning up our data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要安装*spaCy*（一个NLP库）和pandas，如果您的系统上没有它们的话（再次使用`pip`或`conda`）。我们使用*spaCy*来处理`torchtext`管道中的文本，使用pandas来探索和清理我们的数据。
- en: 'Getting Our Data: Tweets!'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取我们的数据：推文！
- en: In this section, we build a sentiment analysis model, so let’s grab a dataset.
    `torchtext` provides a bunch of built-in datasets via the `torchtext.datasets`
    module, but we’re going to work on one from scratch to get a feel for building
    a custom dataset and feeding it into a model we’ve created. We use the [Sentiment140
    dataset](http://help.sentiment140.com/for-students). This is based on tweets from
    Twitter, with every tweet ranked as 0 for negative, 2 for neutral, and 4 for positive.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们构建一个情感分析模型，所以让我们获取一个数据集。`torchtext`通过`torchtext.datasets`模块提供了一堆内置数据集，但我们将从头开始工作，以便了解构建自定义数据集并将其馈送到我们创建的模型的感觉。我们使用[Sentiment140数据集](http://help.sentiment140.com/for-students)。这是基于Twitter上的推文，每个推文被排名为0表示负面，2表示中性，4表示积极。
- en: 'Download the zip archive and unzip. We use the file *training.1600000.processed.noemoticon.csv*.
    Let’s look at the file using pandas:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下载zip存档并解压。我们使用文件*training.1600000.processed.noemoticon.csv*。让我们使用pandas查看文件：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You may at this point get an error like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此时您可能会遇到这样的错误：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Congratulations—you’re now a real data scientist and you get to deal with data
    cleaning! From the error message, it appears that the default C-based CSV parser
    that pandas uses doesn’t like some of the Unicode in the file, so we need to switch
    to the Python-based parser:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，现在你是一个真正的数据科学家，你可以处理数据清洗了！从错误消息中可以看出，pandas使用的默认基于C的CSV解析器不喜欢文件中的一些Unicode，所以我们需要切换到基于Python的解析器：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s take a look at the structure of the data by displaying the first five
    rows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过显示前五行来查看数据的结构：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Annoyingly, we don’t have a header field in this CSV (again, welcome to the
    world of a data scientist!), but by looking at the website and using our intuition,
    we can see that what we’re interested in is the last column (the tweet text) and
    the first column (our labeling). However, the labels aren’t great, so let’s do
    a little feature engineering to work around that. Let’s see what counts we have
    in our training set:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 令人恼火的是，这个CSV中没有标题字段（再次欢迎来到数据科学家的世界！），但通过查看网站并运用我们的直觉，我们可以看到我们感兴趣的是最后一列（推文文本）和第一列（我们的标签）。然而，标签不是很好，所以让我们做一些特征工程来解决这个问题。让我们看看我们的训练集中有哪些计数：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Curiously, there are no neutral values in the training dataset. This means
    that we could formulate the problem as a binary choice between 0 and 1 and work
    out our predictions from there, but for now we stick to the original plan that
    we may possibly have neutral tweets in the future. To encode the classes as numbers
    starting from 0, we first create a column of type `category` from the label column:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在训练数据集中没有中性值。这意味着我们可以将问题制定为0和1之间的二元选择，并从中得出我们的预测，但目前我们坚持原始计划，即未来可能会有中性推文。为了将类别编码为从0开始的数字，我们首先从标签列创建一个`category`类型的列：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we encode those classes as numerical information in another column:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将这些类别编码为另一列中的数字信息：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We then save the modified CSV back to disk:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将修改后的CSV保存回磁盘：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'I recommend that you save another CSV that has a small sample of the 1.6 million
    tweets for you to test things out on too:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您保存另一个CSV文件，其中包含160万条推文的小样本，供您进行测试：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now we need to tell `torchtext` what we think is important for the purposes
    of creating a dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要告诉`torchtext`我们认为对于创建数据集而言重要的内容。
- en: Defining Fields
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义字段
- en: '`torchtext` takes a straightforward approach to generating datasets: you tell
    it what you want, and it’ll process the raw CSV (or JSON) for you. You do this
    by first defining *fields*. The `Field` class has a considerable number of parameters
    that can be assigned to it, and although you probably won’t use all of them at
    once, [Table 5-1](#table-5-1) provides a handy guide as to what you can do with
    a `Field`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext`采用了一种直接的方法来生成数据集：您告诉它您想要什么，它将为您处理原始CSV（或JSON）数据。您首先通过定义*字段*来实现这一点。`Field`类有相当多的参数可以分配给它，尽管您可能不会同时使用所有这些参数，但[表5-1](#table-5-1)提供了一个方便的指南，说明您可以使用`Field`做什么。'
- en: Table 5-1\. Field parameter types
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-1\. 字段参数类型
- en: '| Parameter | Description | Default |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 | 默认值 |'
- en: '| --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `sequential` | Whether the field represents sequential data (i.e., text).
    If set to `False`, no tokenization is applied. | `True` |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `sequential` | 字段是否表示序列数据（即文本）。如果设置为`False`，则不会应用标记化。 | `True` |'
- en: '| `use_vocab` | Whether to include a `Vocab` object. If set to `False`, the
    field should contain numerical data. | `True` |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `use_vocab` | 是否包含`Vocab`对象。如果设置为`False`，字段应包含数字数据。 | `True` |'
- en: '| `init_token` | A token that will be added to the start of this field to indicate
    the beginning of the data. | `None` |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `init_token` | 将添加到此字段开头以指示数据开始的令牌。 | `None` |'
- en: '| `eos_token` | End-of-sentence token appended to the end of each sequence.
    | `None` |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `eos_token` | 附加到每个序列末尾的句子结束令牌。 | `None` |'
- en: '| `fix_length` | If set to an integer, all entries will be padded to this length.
    If `None`, sequence lengths will be flexible. | `None` |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `fix_length` | 如果设置为整数，所有条目将填充到此长度。如果为`None`，序列长度将是灵活的。 | `None` |'
- en: '| `dtype` | The type of the tensor batch. | `torch.long` |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `dtype` | 张量批次的类型。 | `torch.long` |'
- en: '| `lower` | Convert the sequence into lowercase. | `False` |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `lower` | 将序列转换为小写。 | `False` |'
- en: '| `tokenize` | The function that will perform sequence tokenization. If set
    to `spacy`, the spaCy tokenizer will be used. | `string.split` |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `tokenize` | 将执行序列标记化的函数。如果设置为`spacy`，将使用spaCy分词器。 | `string.split` |'
- en: '| `pad_token` | The token that will be used as padding. | `<pad>` |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `pad_token` | 将用作填充的令牌。 | `<pad>` |'
- en: '| `unk_token` | The token that will be used to represent words that are not
    present in the `Vocab` `dict`. | `<unk>` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `unk_token` | 用于表示`Vocab` `dict`中不存在的单词的令牌。 | `<unk>` |'
- en: '| `pad_first` | Pad at the start of the sequence. | `False` |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `pad_first` | 在序列开始处填充。 | `False` |'
- en: '| `truncate_first` | Truncate at the beginning of the sequence (if necessary).
    | `False` |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `truncate_first` | 在序列开头截断（如果需要）。 | `False` |'
- en: 'As we noted, we’re interested in only the labels and the tweets text. We define
    these by using the `Field` datatype:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所指出的，我们只对标签和推文文本感兴趣。我们通过使用`Field`数据类型来定义这些内容：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We’re defining `LABEL` as a `LabelField`, which is a subclass of `Field` that
    sets `sequential` to `False` (as it’s our numerical category class). `TWEET` is
    a standard `Field` object, where we have decided to use the spaCy tokenizer and
    convert all the text to lowercase, but otherwise we’re using the defaults as listed
    in the previous table. If, when running through this example, the step of building
    the vocabulary is taking a very long time, try removing the `tokenize` parameter
    and rerunning. This will use the default of simply splitting on whitespace, which
    will speed up the tokenization step considerably, though the created vocabulary
    will not be as good as the one spaCy creates.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`LABEL`定义为`LabelField`，它是`Field`的子类，将`sequential`设置为`False`（因为它是我们的数字类别）。`TWEET`是一个标准的`Field`对象，我们决定使用spaCy分词器并将所有文本转换为小写，但在其他方面，我们使用前面表中列出的默认值。如果在运行此示例时，构建词汇表的步骤花费了很长时间，请尝试删除`tokenize`参数并重新运行。这将使用默认值，即简单地按空格分割，这将大大加快标记化步骤，尽管创建的词汇表不如spaCy创建的那么好。
- en: 'Having defined those fields, we now need to produce a list that maps them onto
    the list of rows that are in the CSV:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了这些字段后，我们现在需要生成一个列表，将它们映射到CSV中的行列表：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Armed with our declared fields, we now use `TabularDataset` to apply that definition
    to the CSV:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们声明的字段，我们现在使用`TabularDataset`将该定义应用于CSV：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This may take some time, especially with the spaCy parser. Finally, we can
    split into training, testing, and validation sets by using the `split()` method:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一些时间，特别是使用spaCy解析器。最后，我们可以使用`split()`方法将其拆分为训练、测试和验证集：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here’s an example pulled from the dataset:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从数据集中提取的示例：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In a surprising turn of serendipity, the randomly selected tweet references
    the closure of a club in Chapel Hill I frequently visited. See if you find anything
    as weird on your dive through the data!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个令人惊讶的巧合中，随机选择的推文提到了我经常访问的教堂山俱乐部的关闭。看看您在数据中的浏览中是否发现了任何奇怪的事情！
- en: Building a Vocabulary
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建词汇表
- en: 'Traditionally, at this point we would build a one-hot encoding of each word
    that is present in the dataset—a rather tedious process. Thankfully, `torchtext`
    will do this for us, and will also allow a `max_size` parameter to be passed in
    to limit the vocabulary to the most common words. This is normally done to prevent
    the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed,
    after all. Let’s limit the vocabulary to a maximum of 20,000 words in our training
    set:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，在这一点上，我们将构建数据集中每个单词的独热编码——这是一个相当乏味的过程。幸运的是，`torchtext`会为我们做这个工作，并且还允许传入一个`max_size`参数来限制词汇表中最常见的单词。通常这样做是为了防止构建一个巨大的、占用内存的模型。毕竟，我们不希望我们的GPU被压倒。让我们将词汇表限制在训练集中最多20,000个单词：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can then interrogate the `vocab` class instance object to make some discoveries
    about our dataset. First, we ask the traditional “How big is our vocabulary?”:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以查询`vocab`类实例对象，以发现关于我们数据集的一些信息。首先，我们问传统的“我们的词汇量有多大？”：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Wait, *wait, what?* Yes, we specified 20,000, but by default, `torchtext` will
    add two more special tokens, `<unk>` for unknown words (e.g., those that get cut
    off by the 20,000 `max_size` we specified), and `<pad>`, a padding token that
    will be used to pad all our text to roughly the same size to help with efficient
    batching on the GPU (remember that a GPU gets its speed from operating on regular
    batches). You can also specify `eos_token` or `init_token` symbols when you declare
    a field, but they’re not included by default.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，*等等，什么？* 是的，我们指定了20,000，但默认情况下，`torchtext`会添加两个特殊的标记，`<unk>`表示未知单词（例如，那些被我们指定的20,000
    `max_size`截断的单词），以及`<pad>`，一个填充标记，将用于将所有文本填充到大致相同的大小，以帮助在GPU上进行有效的批处理（请记住，GPU的速度来自于对常规批次的操作）。当您声明一个字段时，您还可以指定`eos_token`或`init_token`符号，但它们不是默认包含的。
- en: 'Now let’s take a look at the most common words in the vocabulary:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看词汇表中最常见的单词：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Pretty much what you’d expect, as we’re not removing stop-words with our spaCy
    tokenizer. (Because it’s just 140 characters, we’d be in danger of losing too
    much information from our model if we did.)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上符合您的预期，因为我们的spaCy分词器没有去除停用词。（因为它只有140个字符，如果我们去除了停用词，我们的模型将丢失太多信息。）
- en: We are almost finished with our datasets. We just need to create a data loader
    to feed into our training loop. `torchtext` provides the `BucketIterator` method
    that will produce what it calls a `Batch`, which is almost, but not quite, like
    the data loader we used on images. (You’ll see shortly that we have to update
    our training loop to deal with some of the oddities of the `Batch` interface.)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎已经完成了我们的数据集。我们只需要创建一个数据加载器来输入到我们的训练循环中。`torchtext`提供了`BucketIterator`方法，它将生成一个称为`Batch`的东西，几乎与我们在图像上使用的数据加载器相同，但又有所不同。（很快您将看到，我们必须更新我们的训练循环来处理`Batch`接口的一些奇怪之处。）
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Putting everything together, here’s the complete code for building up our datasets:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容放在一起，这是构建我们数据集的完整代码：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With our data processing sorted, we can move on to defining our model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的数据处理完成，我们可以继续定义我们的模型。
- en: Creating Our Model
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建我们的模型
- en: 'We use the `Embedding` and `LSTM` modules in PyTorch that we talked about in
    the first half of this chapter to build a simple model for classifying tweets:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在PyTorch中使用了我们在本章前半部分讨论过的`Embedding`和`LSTM`模块来构建一个简单的推文分类模型：
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: All we do in this model is create three layers. First, the words in our tweets
    are pushed into an `Embedding` layer, which we have established as a 300-dimensional
    vector embedding. That’s then fed into a `LSTM` with 100 hidden features (again,
    we’re compressing down from the 300-dimensional input like we did with images).
    Finally, the output of the LSTM (the final hidden state after processing the incoming
    tweet) is pushed through a standard fully connected layer with three outputs to
    correspond to our three possible classes (negative, positive, or neutral). Next
    we turn to the training loop!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，我们所做的就是创建三个层。首先，我们的推文中的单词被推送到一个`Embedding`层中，我们已经将其建立为一个300维的向量嵌入。然后将其输入到一个具有100个隐藏特征的`LSTM`中（同样，我们正在从300维的输入中进行压缩，就像我们在处理图像时所做的那样）。最后，LSTM的输出（处理传入推文后的最终隐藏状态）被推送到一个标准的全连接层中，有三个输出对应于我们的三个可能的类别（负面、积极或中性）。接下来我们转向训练循环！
- en: Updating the Training Loop
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新训练循环
- en: Because of some `torchtext`’s quirks, we need to write a slightly modified training
    loop. First, we create an optimizer (we use Adam as usual) and a loss function.
    Because we were given three potential classes for each tweet, we use `CrossEntropyLoss()`
    as our loss function. However, it turns out that only two classes are present
    in the dataset; if we assumed there would be only two classes, we could in fact
    change the output of the model to produce a single number between 0 and 1 and
    then use binary cross-entropy (BCE) loss (and we can combine the sigmoid layer
    that squashes output between 0 and 1 plus the BCE layer into a single PyTorch
    loss function, `BCEWithLogitsLoss()`). I mention this because if you’re writing
    a classifier that must always be one state or the other, it’s a better fit than
    the standard cross-entropy loss that we’re about to use.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些`torchtext`的怪癖，我们需要编写一个稍微修改过的训练循环。首先，我们创建一个优化器（通常我们使用Adam）和一个损失函数。因为对于每个推文，我们有三个潜在的类别，所以我们使用`CrossEntropyLoss()`作为我们的损失函数。然而，事实证明数据集中只有两个类别；如果我们假设只有两个类别，我们实际上可以改变模型的输出，使其产生一个介于0和1之间的单个数字，然后使用二元交叉熵（BCE）损失（我们可以将将输出压缩在0和1之间的sigmoid层和BCE层组合成一个单一的PyTorch损失函数，`BCEWithLogitsLoss()`）。我提到这一点是因为如果您正在编写一个必须始终处于一种状态或另一种状态的分类器，那么它比我们即将使用的标准交叉熵损失更合适。
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The main thing to be aware of in this new training loop is that we have to reference
    `batch.tweet` and `batch.label` to get the particular fields we’re interested
    in; they don’t fall out quite as nicely from the enumerator as they do in `torchvision`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新的训练循环中需要注意的主要事项是，我们必须引用`batch.tweet`和`batch.label`来获取我们感兴趣的特定字段；它们并不像在`torchvision`中那样从枚举器中很好地脱落出来。
- en: Once we’ve trained our model by using this function, we can use it to classify
    some tweets to do simple sentiment analysis.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用这个函数训练了我们的模型，我们就可以用它来对一些推文进行简单的情感分析。
- en: Classifying Tweets
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类推文
- en: 'Another hassle of `torchtext` is that it’s a bit of a pain to get it to predict
    things. What you can do is emulate the processing pipeline that happens internally
    and make the required prediction on the output of that pipeline, as shown in this
    small function:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext`的另一个麻烦是它有点难以预测事物。你可以模拟内部发生的处理流程，并在该流程的输出上进行所需的预测，如这个小函数所示：'
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We have to call `preprocess()`, which performs our spaCy-based tokenization.
    After that, we can call `process()` to the tokens into a tensor based on our already-built
    vocabulary. The only thing we have to be careful about is that `torchtext` is
    expecting a batch of strings, so we have to turn it into a list of lists before
    handing it off to the processing function. Then we feed it into the model. This
    will produce a tensor that looks like this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须调用`preprocess()`，它执行基于spaCy的标记化。之后，我们可以调用`process()`将标记基于我们已构建的词汇表转换为张量。我们唯一需要注意的是`torchtext`期望一批字符串，因此在将其传递给处理函数之前，我们必须将其转换为列表的列表。然后我们将其馈送到模型中。这将产生一个如下所示的张量：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The tensor element with the highest value corresponds to the model’s chosen
    class, so we use `argmax()` to get the index of that, and then `item()` to turn
    that zero-dimension tensor into a Python integer that we index into our `categories`
    dictionary.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最高值的张量元素对应于模型选择的类别，因此我们使用`argmax()`来获取该索引，然后使用`item()`将零维张量转换为Python整数，然后将其索引到我们的`categories`字典中。
- en: With our model trained, let’s look at how to do some of the other tricks and
    techniques that you learned for images in Chapters [2](ch02.html#image-classification-with-pytorch)–[4](ch04.html#transfer-learning-and-other-tricks).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完我们的模型后，让我们看看如何执行你在第[2](ch02.html#image-classification-with-pytorch)–[4](ch04.html#transfer-learning-and-other-tricks)章中学到的其他技巧和技术。
- en: Data Augmentation
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'You might wonder exactly how you can augment text data. After all, you can’t
    really flip it horizontally as you can an image! But you can use some techniques
    with text that will provide the model with a little more information for training.
    First, you could replace words in the sentence with synonyms, like so:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道如何增强文本数据。毕竟，你不能像处理图像那样水平翻转文本！但你可以使用一些文本技术，为模型提供更多训练信息。首先，你可以用同义词替换句子中的单词，如下所示：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: could become
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可以变成
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Aside from the cat’s insistence that a rug is much softer than a mat, the meaning
    of the sentence hasn’t changed. But *mat* and *rug* will be mapped to different
    indices in the vocabulary, so the model will learn that the two sentences map
    to the same label, and hopefully that there’s a connection between those two words,
    as everything else in the sentences is the same.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 除了猫坚持认为地毯比垫子更柔软之外，句子的含义并没有改变。但是*mat*和*rug*将映射到词汇表中的不同索引，因此模型将学习到这两个句子映射到相同标签，并希望这两个单词之间存在联系，因为句子中的其他内容都是相同的。
- en: 'In early 2019, the paper “EDA: Easy Data Augmentation Techniques for Boosting
    Performance on Text Classification Tasks” suggested three other augmentation strategies:
    random insertion, random swap, and random deletion. Let’s take a look at each
    of them.^([3](ch05.html#idm45762361052120))'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年初，论文“EDA：用于提高文本分类任务性能的简单数据增强技术”提出了另外三种增强策略：随机插入、随机交换和随机删除。让我们看看每种方法。^([3](ch05.html#idm45762361052120))
- en: Random Insertion
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机插入
- en: 'A *random insertion* technique looks at a sentence and then randomly inserts
    synonyms of existing nonstop-words into the sentence *n* times. Assuming you have
    a way of getting a synonym of a word and a way of eliminating stop-words (common
    words such as *and*, *it*, *the*, etc.), shown, but not implemented, in this function
    via `get_synonyms()` and `get_stopwords()`, an implementation of this would be
    as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机插入*技术查看一个句子，然后随机插入现有非停用词的同义词*n*次。假设你有一种获取单词同义词和消除停用词（常见单词如*and*、*it*、*the*等）的方法，通过`get_synonyms()`和`get_stopwords()`在这个函数中显示，但没有实现，这个实现如下：'
- en: '[PRE32]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'An example of this in practice where it replaces `cat` could look like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，替换`cat`的示例可能如下所示：
- en: '[PRE33]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Random Deletion
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机删除
- en: 'As the name suggests, *random deletion* deletes words from a sentence. Given
    a probability parameter `p`, it will go through the sentence and decide whether
    to delete a word or not based on that random probability:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，*随机删除*从句子中删除单词。给定概率参数`p`，它将遍历句子，并根据该随机概率决定是否删除单词：
- en: '[PRE34]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The implementation deals with the edge cases—if there’s only one word, the technique
    returns it; and if we end up deleting all the words in the sentence, the technique
    samples a random word from the original set.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现处理边缘情况——如果只有一个单词，该技术将返回它；如果我们最终删除了句子中的所有单词，该技术将从原始集合中随机抽取一个单词。
- en: Random Swap
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机交换
- en: 'The *random swap* augmentation takes a sentence and then swaps words within
    it *n* times, with each iteration working on the previously swapped sentence.
    Here’s an implementation:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机交换*增强接受一个句子，然后在其中* n *次交换单词，每次迭代都在先前交换的句子上进行。这里是一个实现：'
- en: '[PRE35]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We sample two random numbers based on the length of the sentence, and then just
    keep swapping until we hit *n*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据句子的长度随机抽取两个随机数，然后一直交换直到达到*n*。
- en: The techniques in the EDA paper average about a 3% improvement in accuracy when
    used with small amounts of labeled examples (roughly 500). If you have more than
    5,000 examples in your dataset, the paper suggests that this improvement may fall
    to 0.8% or lower, due to the model obtaining better generalization from the larger
    amounts of data available over the improvements that EDA can provide.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: EDA论文中的技术在使用少量标记示例（大约500个）时平均提高了约3%的准确性。如果你的数据集中有5000个以上的示例，该论文建议这种改进可能会降至0.8%或更低，因为模型从更多可用数据量中获得更好的泛化能力，而不是从EDA提供的改进中获得。
- en: Back Translation
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回译
- en: 'Another popular approach for augmenting datasets is *back translation*. This
    involves translating a sentence from our target language into one or more other
    languages and then translating all of them back to the original language. We can
    use the Python library `googletrans` for this purpose. Install it with `pip`,
    as it doesn’t appear to be in `conda` at the time of this writing:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的增强数据集的方法是*回译*。这涉及将一个句子从我们的目标语言翻译成一个或多个其他语言，然后将它们全部翻译回原始语言。我们可以使用Python库`googletrans`来实现这个目的。在写作时，你可以使用`pip`安装它，因为它似乎不在`conda`中：
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we can translate our sentence from English to French, and then back to
    English:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将我们的句子从英语翻译成法语，然后再翻译回英语：
- en: '[PRE37]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'That gives us an augmented sentence from English to French and back again,
    but let’s go a step further and select a language at random:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就得到了一个从英语到法语再到英语的增强句子，但让我们再进一步，随机选择一种语言：
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this case, we use `random.choice` to grab a random language, translate to
    that language, and then translate back as before. We also pass in the language
    to the `src` parameter just to help the language detection of Google Translate
    along. Try it out and see how much it resembles the old game of *Telephone*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用`random.choice`来选择一个随机语言，将句子翻译成该语言，然后再翻译回来。我们还将语言传递给`src`参数，以帮助谷歌翻译的语言检测。试一试，看看它有多像*电话*这个老游戏。
- en: You need to be aware of a few limits. First, you can translate only up to 15,000
    characters at a time, though that shouldn’t be too much of a problem if you’re
    just translating sentences. Second, if you are going to use this on a large dataset,
    you want to do your data augmentation on a cloud instance rather than your home
    computer, because if Google bans your IP, you won’t be able to use Google Translate
    for normal use! Make sure that you send a few batches at a time rather than the
    entire dataset at once. This should also allow you to restart translation batches
    if there’s an error on the Google Translate backend as well.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要了解一些限制。首先，一次只能翻译最多15,000个字符，尽管如果你只是翻译句子的话，这应该不会是太大的问题。其次，如果你要在一个大型数据集上使用这个方法，你应该在云实例上进行数据增强，而不是在家里的电脑上，因为如果谷歌封禁了你的IP，你将无法正常使用谷歌翻译！确保你一次发送几批数据而不是整个数据集。这也应该允许你在谷歌翻译后端出现错误时重新启动翻译批次。
- en: Augmentation and torchtext
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增强和torchtext
- en: You might have noticed that everything I’ve said so far about augmentation hasn’t
    involved `torchtext`. Sadly, there’s a reason for that. Unlike `torchvision` or
    `torchaudio`, `torchtext` doesn’t offer a transform pipeline, which is a little
    annoying. It does offer a way of performing pre- and post-processing, but this
    operates only on the token (word) level, which is perhaps enough for synonym replacement,
    but doesn’t provide enough control for something like back translation. And if
    you do try to hijack the pipelines for augmentation, you should probably do it
    in the preprocessing pipeline instead of the post-processing one, as all you’ll
    see in that one is the tensor that consists of integers, which you’ll have to
    map to words via the vocab rules.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经注意到我所说的关于增强的一切都没有涉及`torchtext`。遗憾的是，这是有原因的。与`torchvision`或`torchaudio`不同，`torchtext`并没有提供转换管道，这有点让人恼火。它确实提供了一种执行预处理和后处理的方式，但这只在标记（单词）级别上操作，这对于同义词替换可能足够了，但对于像回译这样的操作并没有提供足够的控制。如果你尝试在增强中利用这些管道，你应该在预处理管道中而不是后处理管道中进行，因为在后处理管道中你只会看到由整数组成的张量，你需要通过词汇规则将其映射到单词。
- en: For these reasons, I suggest not even bothering with spending your time trying
    to twist `torchtext` into knots to do data augmentation. Instead, do the augmentation
    outside PyTorch using techniques such as back translation to generate new data
    and feed that into the model as if it were *real* data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，我建议不要浪费时间试图把`torchtext`搞得一团糟来进行数据增强。相反，使用诸如回译之类的技术在PyTorch之外进行增强，生成新数据并将其输入模型，就像它是*真实*数据一样。
- en: That’s augmentation covered, but there’s an elephant in the room that we should
    address before wrapping up the chapter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 增强已经讨论完毕，但在结束本章之前，我们应该解决一个悬而未决的问题。
- en: Transfer Learning?
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习？
- en: You might be wondering why we haven’t talked about transfer learning yet. After
    all, it’s a key technique that allows us to create accurate image-based models,
    so why can’t we do that here? Well, it turns out that it has been a little harder
    to get transfer learning working on LSTM networks. But not impossible. We’ll return
    to the subject in [Chapter 9](ch09.html#pytorch_in_the_wild), where you’ll see
    how to get transfer learning working with both the LSTM- and Transformer-based
    networks.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会想知道为什么我们还没有谈论迁移学习。毕竟，这是一个关键技术，可以帮助我们创建准确的基于图像的模型，那么为什么我们不能在这里做呢？事实证明，在LSTM网络上实现迁移学习有点困难。但并非不可能。我们将在[第9章](ch09.html#pytorch_in_the_wild)中回到这个主题，你将看到如何在基于LSTM和Transformer的网络上实现迁移学习。
- en: Conclusion
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this chapter, we covered a text-processing pipeline that covers encoding
    and embeddings, a simple LSTM-based neural network to perform classification,
    along with some data augmentation strategies for text-based data. You have plenty
    to experiment with so far. I’ve chosen to make every tweet lowercase during the
    tokenization phase. This is a popular approach in NLP, but it does throw away
    potential information in the tweet. Think about it: “Why is this NOT WORKING?”
    to our eyes is even more suggestive of a negative sentiment than “Why is this
    not working?” but we’ve thrown away that difference between the two tweets before
    it even hits the model. So definitely try running with case sensitivity left in
    the tokenized text. And try removing stop-words from your input text to see whether
    that helps improve the accuracy. Traditional NLP methods make a big point of removing
    them, but I’ve often found that deep learning techniques can perform better when
    leaving them in the input (which we’ve done in this chapter). This is because
    they provide more context for the model to learn from, whereas sentences that
    have been reduced to *only* important words may be missing nuances in the text.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: You may also want to alter the size of the embedding vector. Larger vectors
    mean that the embedding can capture more information about the word it’s modeling
    at the cost of using more memory. Try going from 100- to 1,000-dimensional embeddings
    and see how that affects training time and accuracy.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can also play with the LSTM. We’ve used a simple approach, but
    you can increase `num_layers` to create stacked LSTMs, increase or decrease the
    number of hidden features in the layer, or set `bidirectional=true` to create
    a biLSTM. Replacing the entire LSTM with a GRU layer would also be an interesting
    thing to try; does it train faster? Is it more accurate? Experiment and see what
    you find!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, we move on from text and into the audio realm with `torchaudio`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[“Long Short-term Memory”](https://oreil.ly/WKcxO) by S. Hochreiter and J.
    Schmidhuber (1997)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Learning Phrase Representations Using RNN Encoder-Decoder for Statistical
    Machine Translation”](https://arxiv.org/abs/1406.1078) by Kyunghyun Cho et al.
    (2014)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Bidirectional LSTM-CRF Models for Sequence Tagging”](https://arxiv.org/abs/1508.01991)
    by Zhiheng Huang et al. (2015)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Attention Is All You Need”](https://arxiv.org/abs/1706.03762) by Ashish Vaswani
    et al. (2017)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^([1](ch05.html#idm45762363068216-marker)) Note that it’s not impossible to
    do these things with CNNs; a lot of in-depth research in the last few years has
    been done to apply CNN-based networks in the temporal domain. We won’t cover them
    here, but [“Temporal Convolutional Networks: A Unified Approach to Action Segmentation”](https://arxiv.org/abs/1608.08242)
    by Colin Lea, et al. (2016) provides further information. And seq2seq!'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch05.html#idm45762362963656-marker)) See [“Efficient Estimation of Word
    Representations in Vector Space”](https://arxiv.org/abs/1301.3781) by Tomas Mikolov
    et al. (2013).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch05.html#idm45762361052120-marker)) See [“EDA: Easy Data Augmentation
    Techniques for Boosting Performance on Text Classification Tasks”](https://arxiv.org/abs/1901.11196)
    by Jason W. Wei and Kai Zou (2019).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
