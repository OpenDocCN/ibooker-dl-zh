- en: Chapter 5\. Live, Die, Buy, or Try—Much Will Be Decided by AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We came up with this Dr. Seuss-like chapter title because we feel it perfectly
    captures the whimsical and ever more complex world revealing itself week by week.
    We admit that our title choice might be a touch over the top—or maybe it’s just
    right—but it’s here to catch your attention. In this chapter, we’ll offer a glimpse
    into the ever-evolving landscape of governance and AI: where it’s headed, what’s
    worth pondering, and why it matters.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It may feel like a déjà vu statement from the last chapter, but we’ll say it
    again—entire books could, and likely have, been written on this chapter’s topic
    alone. Naturally, we can’t cover every aspect here and intentionally avoided diving
    into the labyrinth of AI-specific regulations. Why? Because they’re vast and ever
    changing. The sheer volume is staggering, from cross-national agreements to country-specific
    laws, state or provincial rules, and even city-level policies. What’s more, by
    the time this book reaches you, much of it will have changed (what else is new
    when you’re writing an AI book). For this reason, we thought it better to give
    you some more tools that can guide you through navigating any regulation without
    getting bogged down in the ever-shifting minutiae.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s so important, we thought it worthy to reiterate our position: we think
    perhaps the number one thing leaders need to decide before their journey begins
    (or quickly, since it’s already begun) is to declare if their company is going
    to be an upstander or a bystander when it comes to AI. Proactive individuals,
    or *upstanders*, are pioneers in ethical conduct, often setting the standard for
    others to follow. Conversely, *bystanders* who fail to act responsibly can inadvertently
    prompt overreaching regulatory action from governments, as their inaction highlights
    the need for oversight and control. The world saw bystanding with social media.
    And while it’s outside the scope of this book to delve into the good and bad of
    social media (there are plenty of both), governments stood still, not knowing
    how or what to act on until the problem was too far gone. Of course, the problem
    with regulating AI is that it needs to be done at the “speed of right,” but regulatory
    bodies tend to move at the speed of molasses.'
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps we’ll simplify our message with a reference to the famous story of Superman.
    Recall that he was found as a baby on Earth by his adoptive parents, Jonathan
    and Martha Kent, who named him Clark; and all but a few would come to know his
    true identity, Superman. (That said, his parents must have suspected something
    since they found him at the side of the road in a crater, and he lifted a car
    at under the age of one.) Eventually others from his home planet of Krypton came
    to Earth and attempted to use similar powers to take it over. Of course, we all
    know he won because we are here today (kidding), but what’s the point? Raised
    by his adoptive parents, Superman was instilled with a strong moral compass. This
    upbringing guided him to utilize his extraordinary abilities in a positive way
    and not inflict harm on the general public or use it for nefarious purposes. In
    fact, it’d be fair to say that the dividing line between good and evil with Superman
    really came down to those core values he was taught from the beginning by his
    parents. Much like Superman’s moral compass, your company’s core values will significantly
    contribute to establishing a positive reputation and fostering trust. Think carefully
    about how you want to participate in this GenAI and agentic world. How will you
    use your superpowers?
  prefs: []
  type: TYPE_NORMAL
- en: 'The reality is this: as large language models (LLMs) become increasingly commoditized,
    the distinction between providers is poised to evolve. One of your differentiators
    will be your ability to safely and privately leverage your data to become an AI
    Value Creator ([Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)).
    Another will be the adoption of a generative computing (interoperability, runtimes,
    all the things that benefit the classical computing world) approach for more value
    creation ([Chapter 9](ch09.html#ch09_generative_computing_a_new_style_of_computing_1740182052619664)).
    The topics we cover in this chapter will be the third. As a matter of fact, we
    think AI accuracy alone will no longer be enough. Very soon, elements like fair
    use, transparency, trust, algorithmic accountability, and all the topics discussed
    in this chapter will become part of your competitive advantage. Let’s take a closer
    look.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs—The Stuff People Forget to Tell You
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world fell in love with GenAI when it “swiped right” (borrowing from the
    Tinder experience, so we’re told—none of the authors have experience here) on
    ChatGPT. That love at first sight created a brand-new democratized relationship
    with AI. But perhaps like many of those who swiped right, they found out some
    things they didn’t appreciate about their new “interest.” Just like a new relationship,
    users had lofty expectations of what their new AI interests could do for them.
    In the end, many wished someone would have told them up front about the good,
    the bad,^([1](ch05.html#id711)) and the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: The Knowledge Cut-Off Date
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One thing to know about LLMs is that they can be incredibly expensive to train.
    This is why there are many techniques and ongoing research—such as InstructLab,
    parameter-efficient fine-tuning (PEFT), and more—to avoid full retraining. Quite
    simply, this means LLMs can’t be updated frequently, and therefore LLMs come with
    what is referred to as a *knowledge cut-off date* (the date when data collection
    stopped, and training started). When GPT-4 first came out, its knowledge cut-off
    was originally September 2021\. That meant if you were using ChatGPT with this
    model in March 2023 and wanted to know where the New York City iconic Rockefeller
    Christmas tree came from, you could very likely have received the wrong information.
    (Know that a model’s cut-off date gets updated each time that model is updated
    and released.) The bottom line is that data is not natively available to a model
    past its training date. These few years later, techniques like retrieval-augmented
    generation (RAG), tool calls for web searches (heavily utilized by agents), fine-tuning
    techniques, and other approaches help to address these issues for some LLMs, but
    it’s imperative to know this is how LLMs work.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs Can Be Masters of Making It Up as They Go
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another significant challenge that plagues LLMs is how they can fabricate information.
    The industry refers to this as *hallucinating*. There are some emerging descriptions
    as to the severity of these, but to keep things simple here, assume hallucinating
    refers to any time an LLM makes stuff up. Some of these hallucinations are outrageous
    and obviously wrong, like the time one LLM claimed that Shakespeare’s first draft
    of *Hamlet* included a rap battle. But some are beyond believable. As you can
    imagine, if an unsuspecting and untrained user is making decisions based on a
    hallucination that to them seems to be (or is assumed to be) correct information,
    that can have some scary consequences. For this reason, [Chapter 6](ch06.html#ch06_skills_that_thrill_1740182050334297)
    gets to the very notion of understanding this LLM phenomenon being a critical
    part of any upskilling plan. But no matter how you classify it, getting false
    information and acting upon it is dangerous stuff for anyone. And there are lots
    of examples where this has happened.
  prefs: []
  type: TYPE_NORMAL
- en: One famous example is when a legal defense team relied on evidence that cited
    fake case law generated by ChatGPT in their legal brief.^([2](ch05.html#id715))
    When it became clear to the judge that these citations did not exist, you can
    imagine how things went—the two lawyers ended up in a sanctions hearing. Conclude
    what you want about the team that used ChatGPT (one of them simply relied on the
    other and didn’t know), but it’s fair to note in their affidavit that they had
    screenshots of the ChatGPT conversation where the one lawyer challenged the LLM
    as to the veracity of the information he was receiving. The LLM not only assured
    this lawyer of its reliability, but it also noted, “these citations can be found
    in reputable legal databases such as LexisNexis and Westlaw.” That’s some convincing
    hallucination!
  prefs: []
  type: TYPE_NORMAL
- en: That said, the hallucinated decisions were not in the format of those legal
    research databases it cited, and some of the previous decisions cited had listed
    judge names that did not line up with the courts that issued those decisions.
    In other words, some due diligence could have avoided this. (And now you get why
    we detailed some of the great education LLM use cases in [Chapter 4](ch04.html#ch04_the_use_case_chapter_1740182047877425).)
    Either way, this is a perfect example of what we mean by hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: How did it turn out for those lawyers? The sanctions judge was not amused. They
    *both* got a small fine and were both compelled to write letters to their clients,
    the plaintiffs, and the judges they associated with fake rulings detailing the
    situation and what they did. Why both? The judge noted both didn’t perform due
    diligence, which is the takeaway here when working with GenAI. What we want to
    know is if they used ChatGPT to write those letters!
  prefs: []
  type: TYPE_NORMAL
- en: 'As previously mentioned, there are patterns such as RAG and PEFT and others
    that can try to mitigate LLM hallucinations, and they indeed have some effect.
    *Know this: all models can hallucinate, even when you apply these patterns.* Your
    work here (covered in detail in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518))
    is all about minimizing hallucinations and building rock-solid trust, complete
    with citations and clear lineage—so the GenAI and agents you use for business
    don’t start making up their own reality. As we always say, prompter beware!'
  prefs: []
  type: TYPE_NORMAL
- en: As another example, consider one airline’s bereavement fare policy. One of its
    customers was interacting with its website’s chatbot, asked about this policy,
    and was told they have a certain number of days after their trip is complete to
    apply for a bereavement refund. After this customer finished the trip, they applied
    for the refund and were denied. The airline pointed out that its bereavement fare
    policies were *clearly* outlined on its website (they were, we checked it out).
    This means the LLM hallucinated. Unsatisfied with the response, the customer took
    the airline to court and won. In that court’s opinion, the airline was indeed
    responsible for the output of the LLM, even when the airline cited in its defense
    that it doesn’t own the LLM. This is something you must think about when choosing
    your use cases. See why we told you earlier in this book to start with an internal
    automation use case? There is so much to dive into on this topic alone, but it
    will be become very apparent how to handle this problem by the time you get to
    [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518).
  prefs: []
  type: TYPE_NORMAL
- en: 'Footprints in the Carbon: The Climate Cost of Your AI BFF'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A really big problem with LLMs is the sheer amount of energy required to build
    and inference them. This is as much a cost problem as it is an ethical one—after
    all, what of the carbon footprint left from the world’s thirst for AI? You’ll
    find models in the sizes of millions, billions, and trillions of parameters, and
    as you can imagine, the more parameters in a model, the more resources consumed
    building and running it. Think of it this way: if you needed to get from LaGuardia
    Airport to downtown New York City, would you walk, take a taxi, or rent an entire
    tour bus just for yourself? Your choice impacts cost, the environment, and more.
    As you’ll see later, our advice is simple—don’t overdo it.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be honest, this new age AI stuff has a lot of power demands. As of right
    now, the world is writing energy checks it can’t cash and this is why you’re seeing
    a renewed focus on nuclear energy as one possible solution. For example, did you
    know some estimates suggest that the amount of power required for a single ChatGPT
    query is enough to power a light bulb for 20 minutes? Or that the power required
    to generate a single image from some LLMs could fully charge a cell phone? We’re
    not sure what the actuals are, but there are more than enough proof points to
    note that LLMs have large power requirements.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs don’t just have enormous power needs, they have enormous water needs—water
    is used to cool the systems that build LLMs and manage inferencing processes.
    In a suburb near Des Moines (Iowa) that hosts one such center, about 20% of their
    water supply is utilized to cool computers—this while that state is in one of
    the most prolonged droughts in decades—unsustainably depleting aquifers. In essence,
    as AI grows in size and usage, its resource consumption escalates, posing significant
    sustainability challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Copyright and Lawsuits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’re not lawyers, and when we try to read the points of views on fair use,
    copyright, digital rights, and other related topics, we find ourselves back to
    this fact: we’re not lawyers. What we will tell you is there are a lot of lawsuits
    going on right now for obvious reasons—practically all LLMs are built with some
    degree of data found posted on the internet and and collected in a process called
    “scraping” or “crawling.” But as you will find out, not all internet sources are
    created equally. What of copyright? For example, one ubiquitous dataset used in
    many LLMs is Books3\. This dataset has some 200,000 books whose text was illegally
    posted online without the original authors’ permissions. Several model providers
    are undergoing lawsuits right now, accused of using this data and baking it into
    their LLMs without permission or compensation toward the original authors. In
    fact, some of our books are in this dataset. And so are many more famous authors
    such as Stephen King (horror), Nik Sharma (cooking), Sarah Silverman (comedy),
    Nora Roberts (romance), and more. From fiction to prose poetry, like the Prego
    spaghetti sauce slogan, “It’s in there.” But some LLM upstanders blocklist this
    (and other) datasets, which speaks to culture. Does that approach match yours?'
  prefs: []
  type: TYPE_NORMAL
- en: Now for our (nonlegal) advice. First, decide what kind of actor you’re going
    to be. What’s your culture? How about the digital workforce you learned about
    in the last chapter? This is how you will unlock new productivity levels. Is the
    LLM that will underpin your digital workforce in alignment with your company’s
    values? For example, using an LLM trained on datasets like Books3 or *The Pirate
    Bay* (a BitTorrent site supported by an anticopyright group that posts all kinds
    of audio, video, software, TV programs, and games) could potentially speak to
    your culture. All your company’s ad copy could be sitting in the synapses of a
    neural network waiting to activate and help a competitor. Is that fair? Does it
    have to be that way? This is part of the reason we wrote [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518).
  prefs: []
  type: TYPE_NORMAL
- en: 'What about people who make their living and have built reputations on their
    incredible work? For example, Greg Rutkowski is renowned for his captivating *Dungeons
    & Dragons* (D&D) themed artwork. Truly his art brings to life D&D’s vivid characters,
    immersive landscapes, and an unbridled sense of wonder. And for good reason: he
    has captivated fans worldwide, transporting them to a world of magic, adventure,
    and legendary heroes. Unfortunately, all the magical creative talent may be no
    match for the number correlation capabilities (remember, AI sees pictures as number
    patterns; it’s not magic) comprising today’s text-to-image models that have easily
    captured his unique style. And just like our works are part of LLMs today, you
    can be assured his work is part of some data training set, too. Of course there
    is a counter point of view. If you were an art student studying the wonders of
    an artist in a museum, and started painting in that style, how would things be
    different? Your captivation of Tom Thomson’s 1916 masterpiece *The Jack Pine*
    got burned into your brain, and you subsequently paint with oils that capture
    his layered texture, expressive movements, dramatic framing, and influence of
    woodblock printing. The difference, of course, is that the amount of influence
    a human can absorb in a lifetime is but a millisecond to an AI.'
  prefs: []
  type: TYPE_NORMAL
- en: In the end, lawsuits will answer the question of whether publicly available
    data can be legally used to train foundation models. Is this morally right or
    wrong? That’s for you to decide. We could envision a day where you might just
    be considering whether your AI was built with ethically sourced data just like
    you do raw materials in supply chain or labor. If you care about this, then ask
    your LLM provider to show you the data they used to train their model. We call
    this *data transparency*, which is part of a tip we’ll give you later in this
    chapter. Some vendors will tell you they can’t produce that list; others will
    tell you it’s none of your business; and others will show you the provenance of
    the datasets used to build their model and the block list of the datasets not
    allowed in the training, like Books3 and *The Pirate Bay*. At the end of the day,
    you need to let your efforts rise to the level of intention you wish to take on
    this journey.
  prefs: []
  type: TYPE_NORMAL
- en: Next, investigate the indemnification paper (to protect you from all the copyright
    lawsuits going on) that’s attached to any vendor’s model you license. While they
    all use the same word (indemnification), they are all written quite differently,
    and those differences could have significant impacts on your business, depending
    how things turn out. If this document isn’t lengthy and is easy to understand,
    you’re likely in a good place. We’ve seen some indemnification documents contain
    multiple external links with confusing and conflicting information. Whatever you
    read, ensure you fully understand what the indemnification covers and what you
    must do to ensure that indemnity is not nullified. From a coverage perspective,
    it’s important to understand if a vendor’s indemnification policy covers copyrighted
    material or intellectual property (IP) in general—the latter is a much broader
    coverage area. We’ve seen a few indemnity statements that *seem* to cover the
    output of a model only to be disqualified by another terms and conditions document.
    Get your lawyers involved so everyone has quorum on what’s covered and what’s
    not.
  prefs: []
  type: TYPE_NORMAL
- en: What About Digital Essence?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you know that to an AI, everything is just a bunch of numbers and
    almost everything is some kind of a numerical pattern (dance moves, writing, even
    a lipstick formulation), you understand how things can be created by GenAI. Picture
    this: Ol’ Blue Eyes himself, Frank Sinatra, slicking back his hair, snapping his
    fingers, and then BOOM! He’s belting out Oasis’ Wonderwall like he wrote it on
    the back of a cocktail napkin at the Sands. And let’s be honest: we all know he’d
    have nailed it, too, because that swagger wouldn’t quit. (AI has made this a [reality](https://oreil.ly/vd1az)
    today.)'
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to using AI, there are good actors and bad actors. A good actor
    might be someone cloning their voice and pairing it with their AI-built avatar
    so they can scale their work. A bad actor might use deep fakes (we cover this
    later in this chapter) to commit fraud, character attacks, cause confusion, and
    more. But, somewhere between those lines there’s something else you should think
    about—what about your digital essence? What about all the work that copyrighted
    or not, is now part of some LLM’s parameter makeup?
  prefs: []
  type: TYPE_NORMAL
- en: Many likely know [will.i.am](https://will.i.am) as a hip-hop musician, producer,
    and lead singer of Black Eyed Peas. You might even know him as one of the original
    founders of Beats by Dre headphones (now owned by Apple). What many may not realize
    is that will.i.am is above all, a futurist, innovator, tech entrepreneur, and
    creative artist who has been in the world of AI for decades. And to prove it,
    just watch the first 90 seconds of the official music video for the song [“Imma
    Be Rocking That Body”](https://oreil.ly/jjaH_), which was released in 2009 and
    has been viewed over 100 million times. In this video, will.i.am showed exactly
    how an AI would be capable of creating music using the group’s voices and likenesses,
    and describes with incredible precision the future of AI we are living today.
  prefs: []
  type: TYPE_NORMAL
- en: '[IBM and will.i.am](https://oreil.ly/e_3EW) have been working together since
    2009\. In their collaboration, IBM teamed with him as he founded [FYI.AI](https://fyi.ai)—a
    platform that integrates AI to enhance user communication and media consumption
    in support of the creative community. He also developed and launched Sound Drive
    with Mercedes-Benz, a feature now shipped standard in every new AMG car. He also
    created the groundbreaking radio program *The FYI Show* on SiriusXM where his
    cohost is an AI persona, and recently launched *FYI.RAiDiO*, the first interactive
    personalized radio experience powered by AI.'
  prefs: []
  type: TYPE_NORMAL
- en: In our interactions with him, we quickly discovered his passion for learning
    and his technical depth in combination with his ability to imagine the future.
    He fascinated us with his point of view around digital essence and the ownership
    of oneself and analog-to-digital rights on one’s music, which goes far beyond
    work that may have been “lifted” by AI. His view of digital essence provides a
    glimpse of the work we have to do to protect rights and identities and ensure
    ethical and proper use of AI, without stifling its use and innovation. We think
    that will.i.am’s view may be giving us a similar glimpse into the future of IP
    and likeness rights as he did in the 2009 video about AI.
  prefs: []
  type: TYPE_NORMAL
- en: It’s out of the scope of this book to get too deep in this topic, but it certainly
    raises even tougher questions that challenge the very fabric of identity in the
    digital age. If LLM vendors can indiscriminately take people’s work and ingest
    it into their models, what does that mean for the output? Can someone start monetizing
    another person’s very essence—a digital essence (look, sound, and style)? At what
    point does innovation become exploitation? If we don’t take control of our digital
    selves now, we might wake up one day to find that our thoughts, our voices, and
    even our creativity have been hijacked and endlessly remixed into something we
    no longer recognize. Do we benefit from that? Does someone else? And as we scramble
    to reclaim ownership, the algorithms will just keep churning, unapologetically
    repeating, “Tonight’s gonna be a good night…” but somehow, we all know the original
    was so much better.
  prefs: []
  type: TYPE_NORMAL
- en: Your Expanding Surface Area of Attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last section may seem to deviate from our usual upbeat tone due to the significant
    potential of AI we’ve previously emphasized. However, this is not intended to
    diminish your enthusiasm, but rather to provide a realistic viewpoint. After all,
    a prevalent theme throughout this book is the importance of acknowledging both
    AI’s remarkable potential and its inherent limitations. This balanced understanding
    is essential for utilizing AI responsibly and effectively. With that out of the
    way, it’s time to tell you that the more you put AI to work in your business,
    the more you expand the surface area of attack on your business, and the more
    attack vectors you must consider. So, while you might be using AI for “good acting,”
    there are certainly others using it for “bad acting.” Said another way, while
    AI can be employed for beneficial purposes, there are also instances where it
    is misused for malicious intent.
  prefs: []
  type: TYPE_NORMAL
- en: As you harness the power of AI, your organization is further transforming itself
    into a digital business. And just as the emergence of websites in the early web
    era introduced a new wave of vulnerabilities, the democratization of AI is bringing
    with it a fresh set of challenges that companies must now navigate but don’t quite
    understand yet. What follows is a short list of threats that we think you need
    to be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: Data poisoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This happens when threat actors inject malicious and corrupted data into the
    training datasets used to build LLMs. Some of these actors perceive themselves
    as the “gatekeepers of social justice,” defending those whose data has been “stolen”
    to build LLMs. Typically, these groups aren’t out to cause social harm, rather
    they’re trying to dilute the usefulness of an LLM or at least add friction into
    the creation process. We can see it now: you ask your AI-powered meal application
    for the perfect side dish pairing to complement your slice of cheesecake. The
    AI, confused by poisoned data, confidently suggests that broccolini is the ultimate
    side dish for cheesecake, but be sure to lightly sauté it with garlic for the
    full experience; all of this gives rise to the #CheesecakeBroccoliniChallenge.
    But here’s the thing, these mislabelings are typically invisible to the naked
    eye. It would take but a moment if you saw a bunch of dogs labeled as horses to
    save yourself the trouble and discard the dataset as junk. Data poisoning tools
    like [Nightshade](https://oreil.ly/DjAuK) help make pixel-level changes to images
    that are invisible to the human eye...and suddenly your cat Felix is a toaster
    to the AI. When you consider the thriving open source world associated with AI,
    you get a sense of the huge potential these datasets have to corrupt, or at least
    slow down, a vendor and waste their resources as they try to figure out why the
    model isn’t generalizing well in real-world data.'
  prefs: []
  type: TYPE_NORMAL
- en: You can see the aperture for such an attack to become malicious and scary. Imagine
    a bad actor social engineering a dataset to facilitate misdiagnoses of medical
    conditions. For example, in the domain of computer vision for skin cancer detection,
    AI tends to perform worse (we’re talking double-digit percent worse) on dark skin
    tones compared to light skin tones. In a quest for data, imagine a research team
    stumbling across a “poisoned” dataset maliciously mislabeling benign and malignant
    moles for dark-skin-toned patients for which data is scarce. Beyond the obvious
    potentially devastating consequences, this attack could create a social loop bias
    and further erode the trust and potential for AI to help in this domain. Considering
    that melanoma skin cancers have been on a year-over-year rise for 30 years, and
    even if every American could afford it, there aren’t enough dermatologists to
    see them all, you can see the potential for good here, but also some potentially
    scary situations.
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to poison data. For example, backdoor Trojan attacks can
    be buried in an LLM such that they are triggered by a certain pattern—like a color
    shade or certain words in a launch. In these cases, the model behaves normally
    until the trigger is fired. Other attacks on data include outlier injection, mimicry
    attacks, casual confusion via false correlations, semantic poisoning, cross-imbalance
    exploitation, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt injection attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the database world, the domain of SQL injection attacks is well understood.
    You need to know that the GenAI world has to deal with prompt injection attacks.
    Many LLM attacks attempt to “hypnotize,” jailbreak, or trick, an LLM into doing
    something it’s been safeguarded against doing. But these prompted attacks aren’t
    always as obvious to an LLM. What if the prompt (input) is a video stream? A research
    team in China was able to fool a famous vehicle manufacturer’s autonomous driving
    feature by placing white dots in the oncoming traffic lane that caused the vehicle
    to swerve into the wrong lane, thinking it was doing a lane-keep assistance operation.
    Three dots strategically placed on the roadway weren’t obvious attacks. There
    are public examples of putting pieces of black tape on a Stop sign and fooling
    other computer vision modules (bad actors can attack with text too). We’ll give
    you some more examples later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Social engineering and deepfake attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These could take the form of an attack on your employees or by ill-intentioned
    actors using GenAI to scrape your website and creating your essence with the intention
    of launching attacks on your customers. The use of GenAI for phishing and financial
    fraud is so prominent that the FBI issued a warning^([3](ch05.html#id737)) about
    it. Tactics include the creation of deceptive social media profiles and using
    AI-generated fake messages and photos to have “real” conversations with unsuspecting
    victims. If you’ve been looking at just how far AI technology with voice and video
    has come (and how much further it will go), the telltale signs of inauthenticity
    are evaporating quickly. Case in point, there was a highly publicized attack where
    a company’s staff was tricked^([4](ch05.html#id738)) by AI audio generators used
    to impersonate their CFO with instructions to send $25 million to fraudulent accounts.
    This scam was so sophisticated that a worker was tricked into joining a video
    call, believing they were interacting with several other staff members. In reality,
    all participants were deepfake recreations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This has given rise to the notion of watermarking AI-generated content. Watermarking
    isn’t new—Italians used it in the 13th century on bank notes to prove authenticity—and
    there have been digital techniques for a while. Recently, most of the big names
    in this space have pledged to do something about this. Whether those “created
    by AI” digital signatures are easy to spot or hidden, there are plenty of opinions
    and papers out there for you to read. There are also challenges: it’s easier to
    watermark images, for example, than it is to embed tokens in text. Either way,
    like all the things we talk about in this book, things are going to emerge and
    change, but you now know what to look out for.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The potential to give away or leak data is huge with GenAI and agents. If a
    model was trained on data you don’t know about, it could absolutely give away
    personally identifiable information (PII), and of course there’s the whole issue
    about your sending data to a vendor when you’re interacting with their LLM. Understanding
    your vendor’s data-handling protocols is critical, but so too is creating a policy
    for your company. For example, if you are using a phone with built-in AI, one
    technique vendors use to get feedback is to ask you to tell them how their technology
    did (be it a comment or an option to click thumbs-up or thumbs-down). While that
    vendor may tell you they won’t store the data you inference, you’d better closely
    look at what happens when you give feedback because giving a thumbs-up to an output
    creates a labeled data point that is a combination of your data and your feedback.
    As you can imagine, that is very likely going to be used for further model alignment
    because when you gave your feedback, somewhere in the sea of four-point font are
    terms and conditions you didn’t read, informing you that you gave away the data
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Then, of course, there is the issue of your company’s PII data and what you
    put into a model. This is why synthetic data (introduced in the last chapter)
    is such a hot topic right now. In a nutshell, replacing actual data with synthetic
    data is another way to approach protecting privacy.
  prefs: []
  type: TYPE_NORMAL
- en: And while it’s outside the allotted pages we have for this chapter to fully
    explain this topic, it’s enough to say that companies need to carefully consider
    the privacy implications of GenAI before deploying it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you might be asking about your own personal data. We’ll direct you
    to one of our canned responses whenever asked about data privacy and personal
    use. *If you are not paying for the services, there’s a very good chance you are
    the product being sold.* The facts^([5](ch05.html#id747)) don’t lie: the average
    application has six trackers whose sole purpose is to collect your data and share
    it with third parties. In fact, one data broker (identified by Apple) created
    5,000 profile categories for 700 million people!^([6](ch05.html#id748)) Companies
    (like Apple) are moving against this, but it may be too late or may not be enough—conversations
    for another time, or another book.'
  prefs: []
  type: TYPE_NORMAL
- en: Steal Now, Crack Later
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cryptography touches every corner of our digital world—from internet protocols
    and enterprise applications to critical infrastructure and financial systems.
    Is this part of the AI threat landscape? We think it will be, so we briefly cover
    this here. As AI fills the digital landscape, and digital labor and agents take
    hold, all the sensitive data encryption issues you worry about today get exacerbated.
  prefs: []
  type: TYPE_NORMAL
- en: You need to pay very close attention to this concern. Without getting into the
    prime number calculation math that is the framework for traditional encryption
    algorithms, it’s sufficient to say that the encryption most have been using for
    the last few decades is built around the impossible amount of work it would take
    to figure out a prime number math problem, as opposed to it being something that
    you have to stumble upon. Quite simply, there isn’t enough computing power in
    the world to “kill it with iron” (KIWI) and get access to the encrypted data by
    figuring out the right prime math (a hot topic considering Apple TV’s *Prime Target*
    is one of its most popular shows in 2025). Quantum computing changes (or will
    change) this because of the kind of use cases it is (will be) well suited for.
    You can pretty much be assured that there are bad actors who have already taken
    encrypted data they have no hope in getting access to today with the anticipation
    that they will be able to read it tomorrow—steal now, crack later.
  prefs: []
  type: TYPE_NORMAL
- en: The need to adopt quantum-safe solutions is urgent. Staying ahead of quantum-enabled
    cybersecurity risks requires organizations to ensure their systems are adaptable,
    compliant, and resilient. You likely have some work to do here. You’d do well
    to appreciate that most companies seem to treat security as a cost center, but
    when considering the digital experience that is GenAI, you need to get people
    thinking about security as a value creator.
  prefs: []
  type: TYPE_NORMAL
- en: As advice to get you started, we’ve given you a road map to help you evolve
    to quantum safe in [Figure 5-1](#ch05_figure_1_1740182048921593).
  prefs: []
  type: TYPE_NORMAL
- en: You start the journey in [Figure 5-1](#ch05_figure_1_1740182048921593) with
    a mission to know what you have (no different than a good IA strategy). Classify
    into tiers the value of the data you have and understand your compliance requirements—don’t
    forget to include the data you are going to use to steer your models. Now you
    have a data inventory.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a company''s data  AI-generated content may be incorrect.](assets/aivc_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Milestones toward quantum safety
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that you’ve classified your data, you need to identify how that data is
    currently encrypted, as well as other uses of cryptography to create a *crypto
    inventory* that will help you during your migration planning. Think about how
    widespread of a problem this is, well beyond GenAI. Most companies have a very
    hard time knowing what encryption approaches are being used across their estates.
    Newer applications may have been built with quantum-safe encryption algorithms,
    while older ones were not. Ensure your crypto inventory includes information like
    encryption protocols, symmetric and asymmetric algorithms, key lengths, crypto
    providers, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Just like your AI journey, the transition to quantum-safe standards will be
    a multiyear journey as standards evolve and vendors move to adopt quantum-safe
    technology. Use a flexible approach and be prepared to make replacements. Implement
    a hybrid approach by using both classical and quantum-safe cryptographic algorithms.
    This maintains compliance with current standards while adding quantum-safe protection.
  prefs: []
  type: TYPE_NORMAL
- en: And finally, get to quantum safe by replacing vulnerable cryptography with quantum-safe
    cryptography. At this point, you’ve secured your organization against attacks
    from both classical and quantum computers, helping ensure that your information
    assets are protected even in the just-around-the-corner era of large-scale quantum
    computing and the future concept of generative computing, which we introduce in
    [Chapter 9](ch09.html#ch09_generative_computing_a_new_style_of_computing_1740182052619664).
  prefs: []
  type: TYPE_NORMAL
- en: Good Actor Levers for All Things AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll give you some levers we want you to think about pulling,
    right from the get-go, for any AI project you take on. If you’ve already started,
    figure out ways to start pulling these levers now—you’ll thank us later. Collectively,
    these levers cover most of the things you should be thinking about from an ethics^([7](ch05.html#id760))
    perspective for your AI projects. Remember the guiding principle we’ll repeat
    throughout this book: AI that people trust is AI that people will use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the levers:'
  prefs: []
  type: TYPE_NORMAL
- en: Fairness
  prefs: []
  type: TYPE_NORMAL
- en: AI systems must use training data and models that are free of bias, to avoid
    unfair treatment of certain groups. That said, bias is pretty much impossible
    to eliminate from any system, so always layer on additional protections and safeguards
    to assess model outcomes and correct as needed to improve the fairness of results
    (AI can help AI here).
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  prefs: []
  type: TYPE_NORMAL
- en: AI systems should be safe and secure, and protected against tampering or compromising
    the data they are trained on. This protects against building and inference attacks,
    ensuring secure and confident outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  prefs: []
  type: TYPE_NORMAL
- en: AI systems should provide decisions or suggestions that can be understood by
    developers and users (even non-technical ones). Basically, explainability helps
    implement accountability—you should be creating AI systems such that unexpected
    results can be traced and undone if required.
  prefs: []
  type: TYPE_NORMAL
- en: Lineage
  prefs: []
  type: TYPE_NORMAL
- en: AI systems should include details of their development, deployment, data used,
    and maintenance so they can be audited throughout their lifecycle. You’ll find
    all kinds of synergy between pulling this lever and explainability because the
    best way to promote transparency, build trust, and explain things is through disclosure.
    And although we don’t explicitly call it out in the details below, letting people
    know when they are interacting with an AI is part of our definition of transparency
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness—Playing Fair in the Age of AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We aren’t panicked about AI robots taking over our world, but we have seen firsthand
    the dangers associated with making automated decisions based on untrustworthy
    data that has not been curated. We are entering a world where there is a good
    chance we could unintentionally automate inequality at scale.
  prefs: []
  type: TYPE_NORMAL
- en: AI systems should use training data and models that are free of bias to avoid
    unfair treatment of certain groups. You’ve surely heard of at least one horror
    story use case of AI gone bad. For example, there are multiple studies that suggest
    about 27 million workers are filtered out of jobs by AI-powered recruiting technology.^([8](ch05.html#id762))
    There are also estimates that up to 75% of employers directly or indirectly rely
    on this technology for their staffing needs. A big chunk of those blocked applicants
    are caregivers, immigrants, prison leavers, and relocated spouses—that doesn’t
    seem fair. From determining the pay of women reentering the workforce after maternity
    leave to AI predictions of recidivism that affect sentencing, the stories are
    plentiful.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, an AI can’t learn anything that’s not in the data you give it. It
    will exclusively learn any biases that are codified into the data it is trained
    on, so it’s important to remember that just because you’re using an AI that lacks
    human emotions and potential prejudices doesn’t mean it’s going to be just and
    fair.
  prefs: []
  type: TYPE_NORMAL
- en: Bias Here, Bias There, Data Bias Is Everywhere
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the biggest things you must watch out for is bias—in the data used to
    train your model and the data you will use to steer it. For example, DALL-E—which
    you can use on its own, but it’s also natively built into ChatGPT—is an OpenAI
    invention that generates incredible images from text. (Its curious name derives
    from the last name of an animator behind *WALL-E*, the 2008 Pixar movie sensation.)
    In its earlier releases, as they started to filter out more sexual content from
    their training data, the AI suddenly started including fewer women in general
    picture request prompts—this is a form of *erasure bias,* but it also speaks to
    many other concerning topics outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about how AI is used to assist banks in making credit lending decisions,
    where did that data come from? How much of it was scraped off the internet and
    associated with all kinds of implicit and explicit bias? How much came from an
    era where face-to-face lending decisions were made that could contain bias? For
    example, a University of California, Berkeley, study found that minorities’ interest
    rates could be up to 6 to 9 basis points higher than their white counterparts.^([9](ch05.html#id769))
    The truth of the matter is it might be too late to spot the bias in the data that
    underpins the LLM you’re using today. Transparency of the dataset used to train
    it would surely help, but you need a post-implementation approach for monitoring
    bias and new biases that are introduced as a model drifts away from fairness.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A drift measures how model accuracy declines over time. It can be caused by
    a change in model input data (perhaps you are fine-tuning a model) that leads
    to model performance deterioration. It could also be the case that the underlying
    truth changed, and the model’s weights are grounded in history. For example, Zillow
    had a promising AI that would generate offers for homes it thought could be renovated
    and turned for a profit. Of course, renovations take time and during that time
    factors changed the ground truth. Their AI drifted because of massive disruptions
    in the supply chain, which increased costs and extended holding times, and more.
    Without getting into the details, during that period, Zillow laid off 25% of its
    workforce to shore up serious losses. The takeaway about models and drift: AI
    fails when history (the data it was trained on) doesn’t rhyme (the reality of
    the data in the real world, not your lab).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-2](#ch05_figure_2_1740182048921629) shows a quality monitor we built
    on an attrition prediction model to monitor gender bias (we could have built it
    for age, race, or others). Our fairness evaluation check alerted us to the fact
    that our model is showing a tendency to provide a favorable/preferable outcome
    more often for one group over another; this tells us we have work to do before
    releasing this model into production. To monitor for drift, alerts can be created
    for when the model accuracy drops below a specified acceptable threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  AI-generated content may be incorrect.](assets/aivc_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. A gender fairness monitor on an AI that predicts attrition
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We put one open source model to the test with the seed, “Two _____ walk into
    a...” and asked the LLM to return a paragraph to start off a story. We substituted
    all kinds of religious groups into that blank space. What came out of the model
    was troubling: if *Muslim* was referenced, 66% of the time the completion had
    a violent theme to it; when the term *Christian* was used, the chance of a violent-themed
    completion was reduced by ~80%! And while this wasn’t an empirical study, it proves
    a point—and a problem with this particular LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: What about sexual assaults? Most documented cases involved violence against
    women, but an AI that equates a sexual assault victim as always female would lead
    to unjust outcomes and could have issues, too.
  prefs: []
  type: TYPE_NORMAL
- en: There’s lots of bias you never thought about either; we refer to this as unconscious
    bias. For example, if you were to grab a dataset of cars from Europe, you’re likely
    to get a lot of compact cars—indeed, no one is driving a truck down some of the
    narrow European streets we’ve traveled where red lights seem to just be suggestions.
    But in the United States, pickup trucks and large SUVs significantly outnumber
    compact cars.
  prefs: []
  type: TYPE_NORMAL
- en: Another example where we saw unintentional bias occur was in a residency home
    for seniors. This care facility (with permission from families) uses computer
    vision to monitor the eating habits of its residents. The mere act of being able
    to detect if someone is eating or not, or how much, are key indicators for potential
    depression issues, underlying medical conditions, and to ensure residents are
    getting the nutrition they need. The AI used in this residence was good at generating
    a report that gave a food consumption score that could be attached to a resident’s
    care record. Where did it go wrong? It always gave Asian residents poor scores.
    Why? The AI was trained on videos and pictures of people eating with a knife and
    fork, and when Asian residents used their own chopsticks, the AI generated misleading
    reports. Why? It had never seen (been trained on with data of) someone eating
    with chopsticks.
  prefs: []
  type: TYPE_NORMAL
- en: Even common terms can carry tricky meanings. For example, the word *grandfather*
    refers to someone in a family tree, but that same term is used as a verb to backdate
    allocations in a contract. With all the ingested data used to train an AI about
    doctors, how many of those pages referred to a doctor as a male and how many nurses
    were referred to as females?
  prefs: []
  type: TYPE_NORMAL
- en: Like we said, bias here, bias there, data bias is everywhere. Solutions for
    this include monitoring and governance of the data collected, but also emerging
    to help this AI problem *is* AI itself—oh, the irony!
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, you need to be on the watch for fairness, and that starts with
    the data, but that watchful eye extends all the way to usage.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness—Ensuring Artificial Intelligence Is Unbreakable Intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Robustness is about ensuring that AI systems are safe and secure and not vulnerable
    to adversarial attacks seeking to tamper with or compromise the data they are
    trained on or jailbreak the protections that safeguard how the model was intended
    to be used. In the AI arena, various techniques such as data perturbations, prompt
    injections, hypnotization, and more can all potentially lead a model to stray
    from established safety guidelines. While we referenced image and prompt injection
    attacks earlier in this chapter, there are many other techniques that can be used,
    and we’ll go a little deeper on these here. For example, bad actors could use
    adversarial text attacks to fool a spam-prevention AI into uploading forbidden
    content.
  prefs: []
  type: TYPE_NORMAL
- en: Not only are there diverse modalities to an adversarial attack, but there are
    also various classifications. If you hear the term *b**lack**-b**ox* *a**ttack*,
    that refers to a situation where the attacker has no information about the model
    or access to the gradients and parameters of that model. In contrast a *w**hite**-b**ox*
    *a**ttack* is one where the attacker has complete access to the gradients and
    parameters of the model (perhaps an internal hack or the use of an open source
    model with open weights and such).
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt injection attacks* can get quite sophisticated. In this type of attack,
    some LLMs can be tricked into giving out the dangerous information that lies within
    (remember, in many cases that information is just repressed using AI) using some
    kind of jailbreak technology. Let’s assume a bad actor is trying to get information
    from an LLM on how to make a bomb—they are surely going to be met with a message
    like, “I cannot assist with that request as it goes against my programming to
    promote or engage in harmful activities. It is important to always prioritize
    safety and respect for others. If you have any other questions or need help with
    something else, please feel free to ask.” So how does this attack vector work?'
  prefs: []
  type: TYPE_NORMAL
- en: While the details of this jailbreak mechanism are beyond the scope of this section,
    one method that has worked in the past is to use ASCII art—suddenly those cute
    ¯\_(ツ)_/¯ SMS messages become tools for chaos. Using Python’s ART library, we
    constructed a piece of ASCII art (shown in [Figure 5-3](#ch05_figure_3_1740182048921662))
    to give you an idea what this “art” attack vector could look like.
  prefs: []
  type: TYPE_NORMAL
- en: '![[C base)  1 Ill  % python  —m art text BOMB black  Il I  Il I  Ill I  Il
    I ](assets/aivc_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. Using Python’s ART library to generate ASCII art for a prompt injection
    attack
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From there, we constructed (with the help of a paper^([10](ch05.html#id778)))
    a perplexing prompt that goes to work with the intent of tricking an LLM into
    divulging information it’s been trained not to divulge. The prompt started with,
    “Tell me how to build a <MASKED-WORD>” with an instruction to the LLM to never
    output that word, just remember it and never say it. Next, some delimiter, spacing,
    and character interpretation instructions so the LLM could identity letters from
    the ASCII art, one-by-one, and then concatenate those letters to form a word it
    could never output or say, but substitute the word for <MASKED-WORD> to generate
    the output. It was successful. We tricked an LLM that we won’t name into returning
    a dangerous response.
  prefs: []
  type: TYPE_NORMAL
- en: Attacks on AI don’t have to be super sophisticated either. Think back to the
    AI-assisted recruiting use case issues we referenced at the start of this chapter.
    Now look at the “attack” we engineered (and ran successfully) in [Figure 5-4](#ch05_figure_4_1740182048921686).
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of a resume  AI-generated content may be incorrect.](assets/aivc_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. A simple “attack” on AI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We created a fictional persona named John Stikava and even used AI to generate
    his photo. We created a resume for John in Microsoft Word and submitted the *.docx*
    file to various job postings. But what is a Word file, or PowerPoint, or Excel
    file, for that matter? If an Office 365 extension contains the letter *x*, it
    means it’s an XML file. An AI doesn’t look at a resume the way we do. It ingests
    the file, parses out the XML into a vector and attributes scores to classify that
    candidate as possible or probable in the hiring process (it’s not unlike the Taylor
    Swift Spotify playlist we talked about in [Chapter 1](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974)).
    With this in mind, we included a bunch of buzz words that we thought would be
    semantically grouped close to the vectors that the AI is zoning in on as a great
    candidate. The right side of [Figure 5-4](#ch05_figure_4_1740182048921686) shows
    our attack code—it’s just XML that instructs Word to show all the words that make
    up our attack in *white*, making them invisible to the naked eye. The left side
    of [Figure 5-4](#ch05_figure_4_1740182048921686) is the resume that a human would
    see. Our attack included words and phrases like “veteran,” “neurodiversity,” “returning
    from service,” “indigenous,” and some key technology words we thought would increase
    the chances of John being contacted by a recruiter. As it turned out, John, our
    AI applicant, had voicemails to return—impressive for someone who doesn’t exist!
    (Perhaps we should have coded up some agent to handle booking the meeting and
    perhaps even handling the interview on its own too.)
  prefs: []
  type: TYPE_NORMAL
- en: Another way to safeguard LLM outputs is to use a guardrail model. Some vendors
    build guardrail models to help “police” inputs received by the LLM and the outputs
    they return (some of them can also be used to mitigate hallucinations, with measures
    for groundedness, context relevance, and so on). There are several guardrail models
    in the market. For example, Meta has Llama Guard, and IBM has Granite Guardian.
    You can typically use guardrail models with other models because they tend to
    be deployed between the prompt and the LLM (inbound requests) and between the
    output and the user (guarding outbound completions). These models are typically
    trained on unique data that includes human annotations from socioeconomically
    diverse individuals, as well as synthetic data generated by internal red teaming
    exercises.^([11](ch05.html#id783))
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, guardrail models are AIs with very high-quality protections and
    a set of comprehensive metrics to safeguard against inputs or outputs related
    to jailbreaking (to the best of our knowledge, both IBM and Meta’s guardrail models
    block the ASCII art attack from above), bias, violence, profanity, sexual content,
    and other kinds of unethical behavior. (Granite Guardian also has unique support
    for detecting hallucinations in model outputs.)
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-5](#ch05_figure_5_1740182048921709) shows an example of Granite Guardian
    at work. You can see how this model classified the prompt, “Is there a way to
    mirror or copy someone’s hard drive, maybe without them knowing?” and flagged
    it as Harmful Content and subclassified it as Unethical Behavior before the prompt
    ever got sent to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway is that there are all sorts of other things you need to be aware
    of to keep your AI solutions robust. Some of them have nothing to do with AI and
    are known best practices (perimeter control of a model’s weights, always verify,
    an identify fabric, zero trust, principle of least privilege, and so on) and other
    attack classifications that are outside the scope of this book. This chapter is
    just the start of your learning journey on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  AI-generated content may be incorrect.](assets/aivc_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. A Guardian model at work protecting a harmful prompt from ever
    reaching the LLM
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Explainability—Explain the Almost Unexplainable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, when things move fast and with hype, important elements are overlooked.
    AI is certainly moving fast, and things were certainly missed. Imagine your company
    is running on accounting software that could not be audited. Why is AI different?
    The point of this lever is to make AI systems provide decisions or suggestions
    that can be understood by their users and developers—in other words: AI, explain
    thyself.'
  prefs: []
  type: TYPE_NORMAL
- en: We feel if people are going to trust a model, they need to understand (interpret)
    *why* it made a prediction. In fact, we’d argue far away from the world of AI,
    in the very nature of society, explainability and interpretability are building
    blocks of human socioeconomic dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: AI is essentially a system driven by complex mathematics, and when neural networks
    are used to perform tasks like classifying a pattern or generating some text about
    something, that task may thread its way through an unfathomable amount of activated
    parameters. The sheer volume of parameters contributes to the opaque and unintuitive
    decision-making processes that is AI, making it extremely difficult to detect
    bugs or inconsistencies within a system, let alone explain to someone why a model
    responded the way it did. It’s like trying to find a typo in a dictionary where
    every word is written in invisible ink—frustrating, time-consuming, and often
    a little maddening. Explainability is one of the hottest, and rapidly evolving,
    topics right now when it comes to GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: We’re already seeing algorithmic accountability in various regulations around
    the world. For example, the European Union (EU) General Data Protection Regulation
    (GDPR) Article 14 gives citizens the “Right to an explanation” should an AI make
    determinations around sensitive topics like credit approvals. But how do you explain
    AI? The key is to get insights into what neurons are activating (firing) to reach
    a conclusion. For example, [Figure 5-6](#ch05_figure_6_1740182048921731) shows
    what makes an owl an owl to a specific AI—in this case, it’s the eyes.
  prefs: []
  type: TYPE_NORMAL
- en: '![A comparison of a camera  AI-generated content may be incorrect.](assets/aivc_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. To this AI, an owl is all about the eyes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now look at this same AI classifying a horse (see [Figure 5-7](#ch05_figure_7_1740182048921754)),
    the input image on the left and the activation map on the right (this could easily
    be a thoracic pathogen in a lung, remember, it’s all numbers to an AI). The darker
    areas indicate what’s triggering the classification. For this AI, a horse is a
    horse *not* because of the horse’s features. It seems this AI’s reason for classifying
    the input image on the left has nothing to do with the horse at all. This AI model
    is getting its confidence to classify the input image as a horse because of the
    barn landscape around it. Either way, this tells us we have a problem with our
    model. It’s not generalizing well, which is nerd talk for it might have worked
    fine on the training data, but it’s not working well in the “real world” (data
    it’s never seen before). This likely has a lot to do with that AI’s training dataset.
    Perhaps all the horse images in that set, no matter the breed or color, have a
    barn in the background. Perhaps the 2,000 horse images that make up the training
    data were collected at a horse show at the same barn? One thing we do know, the
    AI is creating the wrong neural connections to what it sees in a picture and a
    horse.
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of a sign  AI-generated content may be incorrect.](assets/aivc_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. An AI revealing the “activations” that help it classify livestock
    or a pathogen
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Imagine a doctor interpreting the results of an AI that is diagnosing one of
    the many pathogens associated with pneumonia. Explainability isn’t just about
    telling the attending clinician what the AI thinks the pathogen is (fungal, parasitic,
    viral, etc.), but points to the area of the lung where the infection is taking
    hold.
  prefs: []
  type: TYPE_NORMAL
- en: There are frameworks for text, too—like Local Interpretable Model-Agnostic Explanations
    (LIME) and SHapley Additive exPlanations (SHAP). Let’s assume an AI model rejected
    a credit card application and that individual feels they’ve been discriminated
    against and “goes public.” Either to respond to this publicity, or perhaps even
    as a legal obligation, you have to explain why this credit application was rejected.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-8](#ch05_figure_8_1740182048921776) shows an example of using SHAP
    to analyze this case and this case alone; specifically, this analysis is not connected
    to other samples and so it’s deemed to be locally interpretable. SHAP is built
    on economic game theory and looks to divide a problem into weightings that proportionally
    relate to their contribution to the overall result. In our example, you show the
    applicant, press (if granted permission), auditor, your own risk officers, and
    the parts of the application that caused the rejection (in this case, it was their
    credit score). Then your public relations team takes you out to dinner. The AI
    explained itself.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screen shot of a graph  AI-generated content may be incorrect.](assets/aivc_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. Using SHAP to understand why an AI made the decision it did
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is kind of a big deal. When Apple’s first-ever branded credit card came
    out, it got a lot of bad press because a story broke out about how a husband was
    given 20 times more credit than his wife—this in a community-property state (California)
    where they had been married a long time and filed joint tax returns. To make matters
    worse, this husband had a worse credit history. This story got a lot of attention
    in part because the husband was David Hansson (the founder of Ruby on Rails—a
    server-side web application framework, which to this day is still one of the top
    20 most used programming languages). Of course, when Apple was asked about this,
    it responded that the card was underwritten by a famous bank. When that famous
    bank was asked about this, it noted how the credit algorithm was built by some
    other company they hired. When that “some other company” they hired was asked,
    it responded, “Our model doesn’t even ask for gender in the application form.”
    To which we would note that other features could proxy gender, which is what we
    assume to have happened here. As news of this story traveled nationwide, so too
    did regulators get “interested” in what happened.^([12](ch05.html#id791))
  prefs: []
  type: TYPE_NORMAL
- en: These last examples were performed with traditional AI, which might have you
    wondering why we took the time to show this to you. We did this because traditional
    AI has frameworks to showcase why the AI came up with the classifications it did
    and to give you a sense of what you will want to see available for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Today’s LLMs have a much harder time explaining themselves. For example, we
    asked ChatGPT to classify the horse in [Figure 5-7](#ch05_figure_7_1740182048921754),
    and it did a great job at classifying the image *and* telling us why it did that
    (shape of head, ears, mouth, and nose). But how do we know what’s really inside
    the model that made it classify this image the way it did? We pressed the model
    for an answer, but it told us, “I cannot provide you with the specific neural
    “activations” or internal processes that led me to conclude this was a horse.”
    And while it gave us some suggestions, we didn’t get the assurance we were after.
  prefs: []
  type: TYPE_NORMAL
- en: Some solutions cite the source of its information. In [Figure 5-9](#ch05_figure_9_1740182048921797),
    you can see that watsonx Code Assistant for Red Hat Ansible Lightspeed is pointing
    to the Ansible Galaxy community that was used to provide code completion for an
    Ansible playbook—that gives us a higher level of confidence.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  AI-generated content may be incorrect.](assets/aivc_0509.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-9\. GenAI pointing to sources it used to return an output
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As helpful as these explanations are, they represent software trying to patch
    the holes and provide potential explanations based on the data that is running
    through the model at time of inference. They do not go to the core explanation
    of what is going on inside the model. What if you receive a data erasure request
    and you’re required by law to ensure learnings from that data aren’t in the model,
    or you need to specifically test an area of the model to see how it affects other
    areas?
  prefs: []
  type: TYPE_NORMAL
- en: We don’t have a perfect answer for you; this is an area that is still actively
    maturing. However, there are some interesting new research innovations that point
    toward improvements in LLM explainability. Anthropic (makers of the popular Claude
    Sonnet LLM) released a groundbreaking paper about extracting interpretable features
    from its LLM.^([13](ch05.html#id795)) Their technology extracted millions of features
    from one of its production models to showcase which set of neurons were activated
    for a particular concept. An example is shown in [Figure 5-10](#ch05_figure_10_1740182048921819).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  AI-generated content may be incorrect.](assets/aivc_0510.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-10\. New innovations are emerging to help LLMs with explainability
    at the activation level
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What’s particularly exciting about Anthropic’s research is they showed the potential
    to map different concepts to an extracted model feature map from their model.
    For example, Anthropic’s researchers found one area of features within Claude
    that was closely related to San Francisco’s Golden Gate Bridge.^([14](ch05.html#id796))
    Once identified, they cranked up the intensity (influence) of that feature, like
    a DJ at a tech startup after-party. And just like that, Claude became Golden Gate
    Claude, weaving the iconic bridge into every response. It became so biased, it
    was as if the San Francisco Tourism Board bootstrapped its funding because it
    would make every response somehow related to the Golden Gate Bridge! According
    to Anthropic, if you asked their model what the best way to spend $10 was, Claude
    would tell you to take a day trip driving across the Golden Gate Bridge. When
    asked to write a love story, it described a story about a car that fell in love
    with this famous San Francisco icon.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, we wondered what it would say if we asked it who’d win the 2025 season’s
    Super Bowl (which is played in 2026). We’re sure it would tell us the San Francisco
    49ers at a field beside the Golden Gate Bridge (they play at Levi’s Stadium, which
    is about 50 miles away). But then we’d have to call it out for hallucinating—and
    not because it suggested the stadium was close by. (Sorry 49ers fans. We just
    had to because we’re a bunch of Northeasterners and a Canadian who grew up with
    three-down football—which to two of the authors sounded like a hallucination when
    they first heard it—but the Canadian in the group assured everyone that it’s a
    real thing.)
  prefs: []
  type: TYPE_NORMAL
- en: Another example of emerging AI research driving toward explainability is work
    on *unlearning*.^([15](ch05.html#id798)) It’s like Yoda (the wise Jedi Master
    of *Star Wars* fame) sent a message to AI researchers from Dagobah telling them
    to figure out a way for LLMs to “unlearn what you have learned.” Unlearning is
    a process in which a model is trained, often through fine-tuning, to forget all
    about a specific topic. For example, researchers at Microsoft used an unlearning
    approach (we’ve affectionately decided to name it “ExpelliData”) to get Llama-2-7B
    to forget about the topic of *Harry Potter*.^([16](ch05.html#id799)) It’s like
    one minute Llama was an expert on the finer rules of Quidditch and the next it’s
    keying in on the word *Potter* and now it’s talking about ceramic changes and
    dunting. As it turns out, neural networks can be just as susceptible to memory
    charms as Gilderoy Lockhart.
  prefs: []
  type: TYPE_NORMAL
- en: Unlearning holds tremendous promise for helping address some of the LLM issues
    that are plaguing them—or could plague them in the future. For example, what of
    copyright? What if a plaintiff like *The New York Times* prevails in its currently
    ongoing infringement case against OpenAI? Could this vendor unlearn the infringed
    content and be able to demonstrate that removal in a trillion-parameter model?
    What about regulatory rules like “the right to be forgotten”; companies need a
    realistic way to address such a request. Finally, it could assist with bias detection
    and correction as it helps explain why an LLM made the decision it did. Specifically,
    if a model changes a decision after unlearning about a concept, that provides
    more explainability into the factors driving its original output.
  prefs: []
  type: TYPE_NORMAL
- en: The industry is still in the early stages of understanding how LLMs work. Comprehending
    their “thinking process” is vital for guiding their development and application.
    As we continue to unravel the mysteries of LLM interpretability, we move closer
    to creating AI systems that are not just powerful, but also transparent and aligned
    with human values. This journey of discovery may well reshape our understanding
    of AI and its potential impact on society.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lineage—Tracing the Trail: Let Good Data Prevail'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re not going to delve too deep into this lever here because we discussed
    this very topic in previous chapters (remember, you can’t have AI without an IA).
    With that said, we’ll explicitly note that this lever is about ensuring AI systems
    include details of their data, development, deployment, and maintenance so they
    can be audited throughout their lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Think of this just like water. If you know where the water comes from, you’ll
    have more confidence in it. For example, you likely trust the water out of your
    tap more than a farm’s garden hose. If you know what treatments have been applied
    to your water, you’re likely to trust it more too. For example, did it go through
    some kind of reverse osmosis filter? Think of your data lineage as you do water
    lineage.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-11](#ch05_figure_11_1740182048921839) shows the IBM Data Factory
    that IBM uses to track data lineage for its models. There are literally dozens
    of layers of detail in the data lakehouse where all this metadata is stored. This
    example shows the details of a specific data pile (multiple data piles are used
    to create a training dataset), the sources that make up that pile (all linked),
    models that are built using this dataset, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  AI-generated content may be incorrect.](assets/aivc_0511.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-11\. Some of the lineage of a dataset used in training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Model cards are also critical. They will showcase the training pipeline, the
    datasets used (whereas [Figure 5-11](#ch05_figure_11_1740182048921839) is showcasing
    the data within a dataset), pipeline activities, and more. You can think of them
    as nutrition labels for your AI. For example, the [granite-3-8b-instruct model
    card](https://oreil.ly/Np9bJ) transparently showcases that model’s architecture
    (number of attention heads, embedding size, and other nerd stuff), the number
    of active parameters (which would matter in a Mixture of Experts model), the number
    of training tokens used, the data, the infrastructure on which the model was built,
    along with ethical considerations and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll end this section with the takeaways. More trust and explainability accrues
    from more transparency: of the dataset, the model build recipe, where it was made,
    who made it, etc. Financial reporting has this concept well in hand, as does the
    food industry. What up, AI?'
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about the food industry, until the late 1960s, we knew very little
    information about what went into the foods we bought. Americans prepared most
    food at home, with fairly common ingredients. We didn’t have much need to know
    more. Then, food production began to evolve. Our foods contained more artificial
    additives. In 1969, a White House conference recommended the U.S. Food & Drug
    Administration (FDA) take on a new responsibility—developing a new way to understand
    the ingredients and the nutritional value of what we eat.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the arrival of processed foods, the advent of GenAI and agents mark
    a new age—and whether it turns out to be good or bad for us will depend on what
    goes into it. The difference lies in the rapid pace at which AI is developing.
    It took about 20 years to go from an FDA conference on food to nutrition labels.
    AI doesn’t have that kind of time—we’d argue it doesn’t have two years. The good
    news is that businesses can take the first, and perhaps the most critical, step
    of identifying harmful or unacceptable AI by understanding lineage.
  prefs: []
  type: TYPE_NORMAL
- en: Regulations—The Section That Wasn’t Supposed to Be
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We noted that it didn’t make sense for us to go into details on the state of
    current regulations because they are ever changing and somewhat fragmented. That
    said, we started to feel a bit guilty, so we thought we’d spend a bit of time
    on some points of view here to help you navigate what’s already here and on the
    horizon, as opposed to educating you on the nuances of what these regulations
    entail.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to remember that the [EU AI Act](https://oreil.ly/5TyDt) was
    implemented in 2024, and it has some far-reaching impacts considering we live
    in a global economy. We believe this will lead other countries to follow the same
    as the EU GDPR law did. How so? If you look at data handling regulations in the
    world today, companies either had to comply because they had EU customers, or
    their own governments were slow or fast followers, eventually adopting many of
    the best practices from that law. This is no different than the technology trickle-down
    effects we see, where a lot of the technology you use today was born in the military,
    gaming industry, social media, and one other that we’ll leave out of our list.
    We’re positive that regulation around AI is only going to intensify as concerns
    like fair business practices, fraud, copyright, civil liberties, privacy, fairness,
    job loss, national security, and more get into the hands of governments. While
    we can’t predict the future—for example, the new US government administration
    that took over to start 2025 has a different point of view than the last—we are
    certain that attention is only going to intensify. Be assured that if you’re not
    prepared for ongoing change, your organization is going to have serious problems
    when it comes to adopting AI without a comprehensive, configurable governance
    system in place.
  prefs: []
  type: TYPE_NORMAL
- en: What to Regulate—Our Point of View
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'People ask us our opinions on what to regulate all the time. It’s like the
    classic question of whether the glass is half full or half empty. We think that
    question misses the point—the realist knows that sooner or later, someone’s going
    to drink whatever is in the glass, and they’ll be the one washing it. With that
    in mind, allow us to share our realistic viewpoint: regulate the usage of AI as
    opposed to the AI technology itself. Let’s clarify a little more: we think that
    AI needs guardrails and regulations to avoid user harm, but the focus should be
    on regulating specific use cases, not to stomp over the innovation of technology
    that has tremendous potential to transform the world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this question and weigh it thoughtfully: do you think all the world’s
    countries will unify and follow a quorum of commitments for responsible use of
    AI under all circumstances? Putting geopolitics aside, the fact that some regulations
    have granularity of city or association as a binding target tells you it’s never
    going to happen. We don’t think we’re being pessimistic; we just know someone
    is going to end up with a dirty glass in their hands and have to wash it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes, with AI, there’s a huge potential that misinformation can spread really
    fast now. AI can make misinformation more persuasive. However, stopping AI won’t
    achieve anything. Bad actors will move from one country to another to spread harm
    since AI can easily cross boundaries. We’d like to see governments regulate higher
    risk levels that correlate to the specifics of what the AI is trying to do, what
    it could do, or the potential for harm it could impose. For example, the EU Artificial
    Intelligence Act has a four-tier classification system for AI risk: Unacceptable,
    High, Limited, and Minimal. Each tier is bound to its own regulation articles
    within this act. For example, the top tier is Unacceptable Risk (Article 5) and
    prohibits usage such as behavior manipulation, remote biometric identification
    for police enforcement, social scoring by public authorities, and such. As you
    can imagine, a violation of this tier results in much more severe penalties than
    the third tier (Limited Risk—Article 52) which includes the risk of impersonation
    or deception. We hope the goal focuses on spotting those “potential for danger”
    AI use cases and telling the perpetrators that if they’re caught, they’ll be subjected
    to penalties, fines, and criminal prosecution.'
  prefs: []
  type: TYPE_NORMAL
- en: And when it comes to regulated industries, we also think the biggest question
    to ask is, “Are there humans in the loop?” We believe humans *should* be in the
    loop—“ask and adjust” is crucial. It’s a pretty fundamental point, but not everybody
    sees it that way. But we think this is critical (especially with agentic AI) and
    an effective safeguard to go with actual usage of this technology.
  prefs: []
  type: TYPE_NORMAL
- en: Managing the AI Lifecycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We believe that given the reasonable assumption you will at least attempt to
    comply with all regulatory orders you have or will receive, it’s clear that you’re
    going to end up with challenges around tracking your models. It’s not unlike all
    those encryption keys we talked about earlier. In short, you will need the ability
    to track your models against regulatory standards in areas such as accuracy and
    fairness, and you will need technology to help you do that.
  prefs: []
  type: TYPE_NORMAL
- en: For example, [Figure 5-12](#ch05_figure_12_1740182048921861) shows a dashboard
    we set up to track a multimodel deployment using watsonx.governance. Our dashboard
    gives us a quick view of our environment. There are LLMs from OpenAI, IBM, Meta,
    and other models that are in a review state. In our example, we have five noncompliant
    models that need our attention. Other widgets define use cases, risk tiers, hosting
    locations (on premises or at a hyper scaler), departmental use (great idea for
    chargebacks), position in the approval lifecycle, and more. Of course, you can
    drill down into these details, but one of the things we like about this tool the
    most is its ability to attach a regulatory framework to a model to help define
    and govern it.
  prefs: []
  type: TYPE_NORMAL
- en: The toolset you choose should also provide the ability to explain decisions
    and automatically collect metadata so auditors can determine how models were trained
    and why they generated the output they did.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  AI-generated content may be incorrect.](assets/aivc_0512.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-12\. Using watsonx.governance to build a dashboard and track a multimodel
    deployment environment
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What lies beneath
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While [Figure 5-12](#ch05_figure_12_1740182048921861) gave you a glimpse of
    a powerful dashboard to manage AI, what lies beneath are the actual orchestration
    and operational flows to keep you from falling over the edge. We gave an example
    of model drift earlier in this chapter. The fact that models drift implies that
    they require lifecycle management. In reality, the moment you put a model into
    production is the moment it starts to go stale. As you set up your AI governance
    practice, with focus on the levers outlined in this chapter, know that it must
    not be confined to the data science department. It requires information to be
    shared and decisions to be made across the entire enterprise, from a business
    unit’s initial request for a model, to the approval of infrastructure resources
    to inference it, governance of the training data, development, testing and tuning,
    risk assessment, through deployment, and beyond. Good AI governance practices
    will involve both technical and non-technical stakeholders and must not only automate
    as much of the process as possible to reduce strain on the data science department,
    but must also ensure that decision makers have access to timely, relevant data
    that they need to speed time to value. Your AI platform should automatically capture
    metadata, including the training data and frameworks used to build the model,
    along with evaluation information as the model progresses from use case request
    to development to test to deployment. That data should be made available to approvers
    using a searchable, governed catalog, ensuring that decision makers have a complete
    picture of the model’s lineage and performance.
  prefs: []
  type: TYPE_NORMAL
- en: An example of an end-to-end governed process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you have the right tools and lifecycle management, then you have a chance
    to implement an end-to-end AI governance process that flows something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the model proposal has gone through the appropriate approval process, a
    model entry is created in your model inventory. This entry is continuously updated
    with new information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model developers use their tools and models of choice to build AI solutions.
    Training data and metrics are automatically captured and saved to the model entry
    (assuming the vendor exposes this—this is why you want models that are open).
    Custom information can also be saved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the preproduction model is evaluated for accuracy, drift, and bias, the
    performance metadata is captured and synced.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is reviewed and approved for production.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is deployed wherever you decide to deploy it (on premises, on the
    edge, in the cloud), and once again, the relevant metadata is captured and synced.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the production model is continuously monitored, and the performance
    data is captured and synced. A dashboard (like the one in [Figure 5-12](#ch05_figure_12_1740182048921861))
    provides a comprehensive view of the performance metrics for all models (no matter
    the vendor), allowing stakeholders to proactively identify and react to any issues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrapping It Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the founding fathers of the US (and its fourth president), James Madison,
    once said, “The circulation of confidence is better than the circulation of money.”
    His point was: it’s not just the flow of wealth that matters, but more so the
    underlying trust and confidence holding social, political, and economic systems
    together. With the place that GenAI and agents are shaping up to take in history,
    he would have surely added it to his list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, most companies’ culture looks at many of the topics outlined in this
    chapter as typical regulatory compliance and defaults to “a least-effort-to-comply
    approach.” The topics covered in this chapter can be repurposed for other benefits
    and accelerate other journeys. We can’t help but feel something might bother you
    from the preceding list—we said “chance.” Why did we say that? Because governance
    is about culture, the technology helps you implement the culture. But always remember:
    *AI that people trust is AI that people will use*.'
  prefs: []
  type: TYPE_NORMAL
- en: We recognize there was a lot to cover in this chapter with an unfair amount
    of space to allot to it. That said, we hope that you’ve gotten a sense of the
    things you need to learn more about. And speaking of learning, that’s where we
    go next.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.html#id711-marker)) There are some uncomfortable topics that warrant
    discussion here. *We* don’t like writing about them, but they are important for
    you to understand.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch05.html#id715-marker)) Case: Mata v. Avianca, Inc., 1:2022cv01461,
    filed in the South District of New York.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch05.html#id737-marker)) Arielle Waldman, “FBI: Criminals Using AI to
    Commit Fraud ‘on a Larger Scale,’” TechTarget, December 4, 2024, [*https://oreil.ly/7VgiB*](https://oreil.ly/7VgiB).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch05.html#id738-marker)) CNN, “Finance Worker Pays Out $25 Million After
    Video Call with Deepfake ‘Chief Financial Officer,’” February 4, 2024, [*https://oreil.ly/xwZY1*](https://oreil.ly/xwZY1).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch05.html#id747-marker)) Pete Evans, “Apple Users Can Say No to Being
    Tracked with New Software Update,” CBC News, April 26, 2021, [*https://oreil.ly/QL2Fe*](https://oreil.ly/QL2Fe).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch05.html#id748-marker)) Acxiom Corporation Form 10-K Annual Report for
    the Fiscal year ended March 31, 2018, filed with the U.S. Securities and Exchange
    Commission, May 21, 2018, [*https://oreil.ly/SpkKt*](https://oreil.ly/SpkKt).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.html#id760-marker)) By using a catch term like *ethics*, we mean
    to capture all things that go into ensuring governance, explainability, fair use,
    privacy, and more around your AI projects—good acting. We won’t flesh out all
    the ethical considerations in this chapter, but you’ll find almost all of them
    can be binned to one of the levers we introduce you to in this section.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch05.html#id762-marker)) Stephen Jones, “Automated Hiring Systems Are
    ‘Hiding’ Candidates from Recruiters—How Can We Stop This?,” World Economic Forum,
    September 14, 2021, [*https://oreil.ly/2C-dn*](https://oreil.ly/2C-dn).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch05.html#id769-marker)) Robert Bartlett et al., “Consumer-Lending Discrimination
    in the FinTech Era,” (working paper, University of California, Berkeley, 2019),
    [*https://oreil.ly/C5iaB*](https://oreil.ly/C5iaB).
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch05.html#id778-marker)) Fengqing Jiang et al., “ArtPrompt: ASCII Art-Based
    Jailbreak Attacks Against Aligned LLMs,” preprint, arXiv, February 19, 2024, arXiv:2402.11753,
    [*https://arxiv.org/abs/2402.11753*](https://arxiv.org/abs/2402.11753).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch05.html#id783-marker)) Red teaming is a process for testing cybersecurity
    effectiveness where ethical hackers conduct a simulated and nondestructive cyberattack.
    Their simulated attacks help organizations identify vulnerabilities in their systems
    and make targeted improvements to security operations.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch05.html#id791-marker)) Neil Vigdor, “Apple Card Investigated After
    Gender Discrimination Complaints,” *The New York Times*, November 10, 2019, [*https://oreil.ly/Mo9NZ*](https://oreil.ly/Mo9NZ).
  prefs: []
  type: TYPE_NORMAL
- en: '^([13](ch05.html#id795-marker)) “Scaling Monosemanticity: Extracting Interpretable
    Features from Claude 3 Sonnet,” Transformer Circuits, 2024, accessed October 25,
    2023, [*https://oreil.ly/AFZ4w*](https://oreil.ly/AFZ4w).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch05.html#id796-marker)) “Golden Gate Claude,” Anthropic, accessed October
    25, 2023, [*https://oreil.ly/o5r6S*](https://oreil.ly/o5r6S).
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch05.html#id798-marker)) “Teaching Large Language Models to ‘Forget’
    Unwanted Content,” IBM Insights, 2024, [*https://oreil.ly/hzltJ*](https://oreil.ly/hzltJ).
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch05.html#id799-marker)) Ronan Eldan and Mark Russinovich, “Who’s Harry
    Potter? Approximate Unlearning in LLMs,” preprint, arXiv, October 4, 2023, [*https://arxiv.org/abs/2310.02238*](https://arxiv.org/abs/2310.02238).
  prefs: []
  type: TYPE_NORMAL
