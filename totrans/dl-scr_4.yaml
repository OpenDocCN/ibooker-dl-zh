- en: Chapter 4\. Extensions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章. 扩展
- en: 'In the last chapter, after having spent two chapters reasoning from first principles
    about what deep learning models are and how they should work, we finally built
    our first deep learning model and trained it to solve the relatively simple problem
    of predicting house prices given numeric features about houses. On most real-world
    problems, however, successfully training deep learning models isn’t so easy: while
    these models can conceivably find an optimal solution to any problem that can
    be framed as a supervised learning problem, in practice they often fail, and indeed
    there are few theoretical guarantees that a given model architecture will in fact
    find a good solution to a given problem. Still, there are some well-understood
    techniques that make neural network training more likely to succeed; these will
    be the focus of this chapter.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，经过两章的推理，我们从第一原则出发，探讨了深度学习模型是什么以及它们应该如何工作，最终构建了我们的第一个深度学习模型，并训练它解决了相对简单的问题，即根据房屋的数值特征预测房价。然而，在大多数实际问题中，成功训练深度学习模型并不那么容易：虽然这些模型可以理论上找到任何可以被定义为监督学习问题的问题的最优解，但在实践中它们经常失败，而且确实很少有理论保证表明给定的模型架构实际上会找到一个好的解决方案。尽管如此，还是有一些被充分理解的技术可以使神经网络训练更有可能成功；这将是本章的重点。
- en: 'We’ll start out by reviewing what neural networks are “trying to do” mathematically:
    find the minimum of a function. Then I’ll show a series of techniques that can
    help the networks achieve this, demonstrating their effectiveness on the classic
    MNIST dataset of handwritten digits. We’ll start with a loss function that is
    used throughout classification problems in deep learning, showing that it significantly
    accelerates learning (we’ve only covered regression problems thus far in this
    book because we hadn’t yet introduced this loss function and thus haven’t been
    able to do classification problems justice). On a similar note, we’ll cover activation
    functions other than sigmoid and show why *they* might also accelerate learning,
    while discussing the trade-offs involved with activation functions in general.
    Next, we’ll cover momentum, the most important (and straightforward) extension
    of the stochastic gradient descent optimization technique we’ve been using thus
    far, as well as briefly discussing what more advanced optimizers can do. We’ll
    end by covering three techniques that are unrelated to each other but that are
    all essential: learning rate decay, weight initialization, and dropout. As we’ll
    see, each of these techniques will help our neural network find successively more
    optimal solutions.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从数学上回顾神经网络“试图做什么”，即找到一个函数的最小值。然后我将展示一系列可以帮助网络实现这一目标的技术，展示它们在手写数字的经典MNIST数据集上的有效性。我们将从一个在深度学习分类问题中经常使用的损失函数开始，展示它显著加速学习（到目前为止，我们在本书中只涵盖了回归问题，因为我们还没有介绍这个损失函数，因此还没有能够公正地处理分类问题）。同样，我们将涵盖除sigmoid之外的激活函数，并展示为什么它们也可能加速学习，同时讨论激活函数的一般权衡。接下来，我们将涵盖动量，这是迄今为止我们一直使用的随机梯度下降优化技术中最重要（也是最直接）的扩展，同时简要讨论更高级的优化器可以做什么。最后，我们将涵盖三种互不相关但都至关重要的技术：学习率衰减、权重初始化和dropout。正如我们将看到的，这些技术中的每一种都将帮助我们的神经网络找到逐渐更优的解决方案。
- en: 'In the first chapter, we followed the “diagram-math-code” model for introducing
    each concept. Here, there isn’t an obvious diagram for each technique, so we’ll
    instead begin with the “intuition” for each technique, then follow up with the
    math (which will typically be much simpler than in the first chapter), and end
    with the code, which will really entail incorporating the technique into the framework
    we’ve built and thus describing precisely how it interacts with the building blocks
    we formalized in the last chapter. In this spirit, we’ll start the chapter with
    some “overall” intuition on what neural networks are trying to do: find the minimum
    of a function.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们遵循了“图表-数学-代码”模型来介绍每个概念。在这里，每个技术并没有明显的图表，因此我们将从每个技术的“直觉”开始，然后跟随数学（通常比第一章简单得多），最后以代码结束，这实际上将包括将技术整合到我们构建的框架中，并精确描述它如何与我们在上一章中形式化的构建块互动。在这种精神下，我们将从神经网络试图做什么的“整体”直觉开始：找到一个函数的最小值。
- en: Some Intuition About Neural Networks
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于神经网络的一些直觉
- en: As we’ve seen, neural networks contain a bunch of weights; given these weights,
    along with some input data `X` and `y`, we can compute a resulting “loss.” [Figure 4-1](#fig_04_01)
    shows this extremely high-level (but still correct) view of neural networks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络包含一堆权重；给定这些权重，以及一些输入数据 `X` 和 `y`，我们可以计算出一个结果的“损失”。[图4-1](#fig_04_01)展示了神经网络的这种极高级别（但仍然正确）的视图。
- en: '![dlfs 0401](assets/dlfs_0401.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![dlfs 0401](assets/dlfs_0401.png)'
- en: Figure 4-1\. A simple way to think of a neural network with weights
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1. 用权重来思考神经网络的简单方式
- en: In reality, each individual weight has some complex, nonlinear relationship
    with the features `X`, the target `y`, the other weights, and ultimately the loss
    `L`. If we plotted this out, varying the value of the weight while holding constant
    the values of the other weights, `X`, and `y`, and plotted the resulting value
    of the loss `L`, we could see something like what is shown in [Figure 4-2](#fig_04_02).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，每个单独的权重与特征 `X`、目标 `y`、其他权重以及最终的损失 `L` 之间都有一些复杂的非线性关系。如果我们将这些绘制出来，改变权重的值，同时保持其他权重、`X`
    和 `y` 的值恒定，并绘制出损失 `L` 的结果值，我们可能会看到类似于[图4-2](#fig_04_02)所示的情况。
- en: '![dlfs 0402](assets/dlfs_0402.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![dlfs 0402](assets/dlfs_0402.png)'
- en: Figure 4-2\. A neural network’s weights versus its loss
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2. 神经网络的权重与损失
- en: When we start to train neural networks, we initialize each weight to have a
    value somewhere along the x-axis in [Figure 4-2](#fig_04_02). Then, using the
    gradients we calculate during backpropagation, we iteratively update the weight,
    with our first update based on the slope of this curve at the initial value we
    happened to choose.^([1](ch04.html#idm45732621297880)) [Figure 4-3](#fig_04_03)
    shows this geometric interpretation of what it means to update the weights in
    a neural network based on the gradients and the learning rate. The blue arrows
    on the left represent repeatedly applying this update rule with a smaller learning
    rate than the red arrows on the right; note that in both cases, the updates in
    the horizontal direction are proportional to the slope of the curve at the value
    of the weight (steeper slope means a larger update).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始训练神经网络时，我们将每个权重初始化为[图4-2](#fig_04_02)中某个位置的值。然后，使用我们在反向传播过程中计算的梯度，我们迭代地更新权重，我们的第一个更新基于我们在初始值处选择的曲线的斜率。[图4-3](#fig_04_03)展示了这种几何解释，即根据梯度和学习率更新神经网络权重的含义。左侧的蓝色箭头代表重复应用此更新规则，学习率比右侧的红色箭头小；请注意，在这两种情况下，水平方向上的更新与权重值处曲线的斜率成比例（更陡的斜率意味着更大的更新）。
- en: '![Neural net diagram](assets/dlfs_0403.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图](assets/dlfs_0403.png)'
- en: Figure 4-3\. Updating the weights of a neural network as a function of the gradients
    and the learning rate, depicted geometrically
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3。根据梯度和学习率更新神经网络权重的几何表示
- en: 'The goal of training a deep learning model is to move each weight to the “global”
    value for which the loss is minimized. As we can see from [Figure 4-3](#fig_04_03),
    if the steps we take are too small, we risk ending up in a “local” minimum, which
    is less optimal than the global one (the path of a weight that follows that scenario
    is illustrated by the blue arrows). If the steps are too large, we risk “repeatedly
    hopping over” the global minimum, even if we are near it (this scenario is represented
    by the red arrows). This is the fundamental trade-off of tuning learning rates:
    if they are too small, we can get stuck in a local minimum; if they are too large,
    they can skip over the global minimum.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型的目标是将每个权重移动到使损失最小化的“全局”值。正如我们从[图4-3](#fig_04_03)中看到的，如果我们采取的步骤太小，我们可能会陷入“局部”最小值，这比全局最小值不太理想（遵循这种情况的权重路径由蓝色箭头表示）。如果步骤太大，我们可能会“反复跳过”全局最小值，即使我们接近它（这种情况由红色箭头表示）。这是调整学习率的基本权衡：如果学习率太小，我们可能会陷入局部最小值；如果学习率太大，它们可能会跳过全局最小值。
- en: 'In reality, the picture is far more complicated than this. One reason is that
    there are thousands, if not millions, of weights in a neural network, so we are
    searching for a global minimum in a space that has thousands or millions of dimensions.
    Moreover, since we update the weights on each iteration as well as passing in
    a different `X` and `y`, *the curve we are trying to find the minimum of is constantly
    changing!* The latter is one of the main reasons neural networks were met with
    skepticism for so many years; it didn’t seem like iteratively updating the weights
    in this way could actually find a globally desirable solution. Yann LeCun et al.
    say it best in a [2015 *Nature* article](https://www.nature.com/articles/nature14539):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，情况比这复杂得多。一个原因是神经网络中有成千上万，甚至数百万个权重，因此我们在一个具有成千上万维或数百万维的空间中寻找全局最小值。此外，由于我们在每次迭代中更新权重，并传入不同的`X`和`y`，*我们试图找到最小值的曲线不断变化！*后者是神经网络多年来受到怀疑的主要原因之一；看起来迭代地以这种方式更新权重实际上无法找到全局理想的解决方案。Yann
    LeCun等人在2015年的一篇[*自然*文章](https://www.nature.com/articles/nature14539)中最好地表达了这一点：
- en: In particular, it was commonly thought that simple gradient descent would get
    trapped in poor local minima—weight configurations for which no small change would
    reduce the average error. In practice, poor local minima are rarely a problem
    with large networks. Regardless of the initial conditions, the system nearly always
    reaches solutions of very similar quality. Recent theoretical and empirical results
    strongly suggest that local minima are not a serious issue in general.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 特别是，人们普遍认为简单的梯度下降会陷入糟糕的局部最小值，即权重配置，对于这些配置，任何微小的变化都不会减少平均误差。实际上，对于大型网络，糟糕的局部最小值很少是一个问题。无论初始条件如何，系统几乎总是达到非常相似质量的解决方案。最近的理论和实证结果强烈表明，局部最小值通常不是一个严重的问题。
- en: So *in practice*, [Figure 4-3](#fig_04_03) provides both a good mental model
    for why learning rates should not be too large or too small and adequate intuition
    for why many of the tricks we’re going to learn in this chapter actually work.
    Equipped with this intuition of what neural networks are trying to do, let’s start
    examining these tricks. We’ll start with a loss function, the *softmax cross entropy*
    loss function, that works in large part because of its ability to provide steeper
    gradients to the weights than the mean squared error loss function we saw in the
    prior chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在实践中，[图4-3](#fig_04_03)不仅提供了一个很好的心智模型，解释了为什么学习率不应该太大或太小，还提供了为什么我们将在本章学习的许多技巧实际上有效的直觉。具备了对神经网络试图做什么的直觉，让我们开始研究这些技巧。我们将从一个损失函数开始，即*softmax交叉熵*损失函数，这个损失函数在很大程度上有效，因为它能够为权重提供比我们在上一章看到的均方误差损失函数更陡的梯度。
- en: The Softmax Cross Entropy Loss Function
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax交叉熵损失函数
- en: 'In [Chapter 3](ch03.html#deep_learning_from_scratch), we used mean squared
    error (MSE) as our loss function. This function had the nice property that it
    was convex, meaning the further the prediction was from the target, the steeper
    would be the initial gradient that the `Loss` sent backward to the network `Layer`s
    and thus the greater would be all the gradients received by the parameters. It
    turns out that in classification problems, however, we can do better than this,
    since in such problems *we know that the values our network outputs should be
    interpreted as probabilities*; thus, not only should each value be between 0 and
    1, but the vector of probabilities should sum to 1 for each observation we have
    fed through our network. The softmax cross entropy loss function exploits this
    to produce steeper gradients than the mean squared error loss for the same inputs.
    This function has two components: the first is the *softmax* function, and the
    second component is the “cross entropy” loss; we’ll cover each of these in turn.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#deep_learning_from_scratch)中，我们使用均方误差（MSE）作为我们的损失函数。这个函数有一个很好的性质，即它是凸的，这意味着预测与目标之间的距离越远，`Loss`向后发送到网络`Layer`的初始梯度就越陡峭，因此参数接收到的所有梯度也会更大。然而，在分类问题中，我们可以做得更好，因为在这种问题中*我们知道网络输出的值应该被解释为概率*；因此，每个值不仅应该在0到1之间，而且对于我们通过网络传递的每个观察值，概率向量应该总和为1。Softmax交叉熵损失函数利用这一点，为相同的输入产生比均方误差损失更陡的梯度。这个函数有两个组件：第一个是*softmax*函数，第二个组件是“交叉熵”损失；我们将依次介绍每个组件。
- en: 'Component #1: The Softmax Function'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组件＃1：Softmax函数
- en: 'For a classification problem with `N` possible classes, we’ll have our neural
    network output a vector of `N` values for each observation. For a problem with
    three classes, these values could, for example, be:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个具有`N`个可能类别的分类问题，我们将使我们的神经网络为每个观察输出一个包含`N`个值的向量。对于一个有三个类别的问题，这些值可以是：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Math
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数学
- en: 'Again, since this is a classification problem, we know that this *should* be
    interpreted as a vector of probabilities (the probability of this observation
    belonging to class 1, 2, or 3, respectively). One way to transform these values
    into a vector of probabilities would be to simply normalize them, summing and
    dividing by the sum:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，由于这是一个分类问题，我们知道这*应该*被解释为概率向量（这个观察属于类别1、2或3的概率）。将这些值转换为概率向量的一种方法是简单地对它们进行标准化，求和并除以总和：
- en: <math display="block"><mrow><mtext>Normalize</mtext> <mrow><mo>(</mo> <mfenced
    close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>)</mo></mrow> <mo>=</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><mfrac><msub><mi>x</mi> <mn>1</mn></msub>
    <mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo><msub><mi>x</mi> <mn>3</mn></msub></mrow></mfrac></mtd></mtr> <mtr><mtd><mfrac><msub><mi>x</mi>
    <mn>2</mn></msub> <mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>+</mo><msub><mi>x</mi> <mn>3</mn></msub></mrow></mfrac></mtd></mtr>
    <mtr><mtd><mfrac><msub><mi>x</mi> <mn>3</mn></msub> <mrow><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo><msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><msub><mi>x</mi> <mn>3</mn></msub></mrow></mfrac></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>Normalize</mtext> <mrow><mo>(</mo> <mfenced
    close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>)</mo></mrow> <mo>=</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><mfrac><msub><mi>x</mi> <mn>1</mn></msub>
    <mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo><msub><mi>x</mi> <mn>3</mn></msub></mrow></mfrac></mtd></mtr> <mtr><mtd><mfrac><msub><mi>x</mi>
    <mn>2</mn></msub> <mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>+</mo><msub><mi>x</mi> <mn>3</mn></msub></mrow></mfrac></mtd></mtr>
    <mtr><mtd><mfrac><msub><mi>x</mi> <mn>3</mn></msub> <mrow><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo><msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><msub><mi>x</mi> <mn>3</mn></msub></mrow></mfrac></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'However, there turns out to be a way that both produces steeper gradients and
    has some elegant mathematical properties: the softmax function. This function,
    for a vector of length 3, would be defined as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，事实证明有一种方法既产生更陡的梯度，又具有一些优雅的数学特性：softmax函数。对于长度为3的向量，这个函数将被定义为：
- en: <math display="block"><mrow><mtext>Softmax</mtext> <mrow><mo>(</mo> <mfenced
    close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>)</mo></mrow> <mo>=</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><mfrac><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>1</mn></msub></msup> <mrow><msup><mi>e</mi> <msub><mi>x</mi> <mn>1</mn></msub></msup>
    <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>2</mn></msub></msup> <mo>+</mo><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr> <mtr><mtd><mfrac><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>2</mn></msub></msup> <mrow><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>1</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>2</mn></msub></msup>
    <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr>
    <mtr><mtd><mfrac><msup><mi>e</mi> <msub><mi>x</mi> <mn>3</mn></msub></msup> <mrow><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>1</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>2</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>Softmax</mtext> <mrow><mo>(</mo> <mfenced
    close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>)</mo></mrow> <mo>=</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><mfrac><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>1</mn></msub></msup> <mrow><msup><mi>e</mi> <msub><mi>x</mi> <mn>1</mn></msub></msup>
    <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>2</mn></msub></msup> <mo>+</mo><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr> <mtr><mtd><mfrac><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>2</mn></msub></msup> <mrow><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>1</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>2</mn></msub></msup>
    <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr>
    <mtr><mtd><mfrac><msup><mi>e</mi> <msub><mi>x</mi> <mn>3</mn></msub></msup> <mrow><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>1</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>2</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr></mtable></mfenced></mrow></math>
- en: Intuition
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直觉
- en: 'The intuition behind the softmax function is that it more strongly amplifies
    the maximum value relative to the other values, forcing the neural network to
    be “less neutral” toward which prediction it thinks is the correct one in the
    context of a classification problem. Let’s compare what both of these functions,
    normalize and softmax, would do to our preceding vector of probabilities:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax函数背后的直觉是，它相对于其他值更强烈地放大最大值，迫使神经网络在分类问题的背景下对其认为是正确的预测更“不中立”。让我们比较这两个函数，标准化和softmax，对我们前面的概率向量会产生什么影响：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see that the original maximum value—5—has a significantly higher value
    than it would have upon simply normalizing the data, and the other two values
    are lower than they were coming out of the `normalize` function. Thus, the `softmax`
    function is partway between normalizing the values and actually applying the `max`
    function (which in this case would result in an output of `array([1.0, 0.0, 0.0])`—hence
    the name “softmax.”
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到原始的最大值—5—比简单标准化数据时具有显着更高的值，而另外两个值比从`normalize`函数输出时更低。因此，`softmax`函数在标准化值和实际应用`max`函数之间（在这种情况下将导致输出为`array([1.0,
    0.0, 0.0])`）的部分路径上，因此得名“softmax”。
- en: 'Component #2: The Cross Entropy Loss'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组件＃2：交叉熵损失
- en: Recall that any loss function will take in a vector of probabilities <math><mfenced
    close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msub><mi>p</mi> <mi>n</mi></msub></mtd></mtr></mtable></mfenced></math>
    and a vector of actual values <math><mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>y</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msub><mi>y</mi>
    <mi>n</mi></msub></mtd></mtr></mtable></mfenced></math> .
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，任何损失函数都将接受一个概率向量<math><mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mi>n</mi></msub></mtd></mtr></mtable></mfenced></math>和一个实际值向量<math><mfenced
    close="]" open="["><mtable><mtr><mtd><msub><mi>y</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mi>n</mi></msub></mtd></mtr></mtable></mfenced></math>。
- en: Math
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数学
- en: 'The cross entropy loss function, for each index `i` in these vectors, is:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些向量中的每个索引`i`，交叉熵损失函数为：
- en: <math display="block"><mrow><mtext>CE</mtext> <mrow><mo>(</mo> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>×</mo> <mtext>log</mtext>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>-</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>×</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>CE</mtext> <mrow><mo>(</mo> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>×</mo> <mtext>log</mtext>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>-</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>×</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: Intuition
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直觉
- en: 'To see why this makes sense as a loss function, consider that since every element
    of *y* is either 0 or 1, the preceding equation reduces to:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这作为损失函数是有道理的原因，考虑到由于*y*的每个元素都是0或1，前述方程简化为：
- en: <math display="block"><mrow><mtext>CE</mtext> <mrow><mo>(</mo> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mfenced close="" open="{" separators=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mi>l</mi> <mi>o</mi> <mi>g</mi> <mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mo>(</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>CE</mtext> <mrow><mo>(</mo> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mfenced close="" open="{" separators=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mi>l</mi> <mi>o</mi> <mi>g</mi> <mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mo>(</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: Now we can break this down more easily. If *y* = 0, then the plot of the value
    of this loss versus the value of the mean squared error loss over the interval
    0 to 1 is as depicted in [Figure 4-4](#fig_04_04).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以更容易地将其分解。如果*y*=0，那么在区间0到1上的这个损失值与均方误差损失值的图像如[图4-4](#fig_04_04)所示。
- en: '![dlfs 0404](assets/dlfs_0404.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![dlfs 0404](assets/dlfs_0404.png)'
- en: Figure 4-4\. Cross entropy loss versus MSE when <math><mrow><mi>y</mi> <mo>=</mo>
    <mn>0</mn></mrow></math>
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4。当<math><mrow><mi>y</mi> <mo>=</mo> <mn>0</mn></mrow></math>时的交叉熵损失与MSE
- en: Not only are the penalties for the cross entropy loss much higher over this
    interval,^([2](ch04.html#idm45732621016264)) but they get steeper at a higher
    rate; indeed, the value of the cross entropy loss approaches infinity as the difference
    between our prediction and the target approaches 1! The plot for when *y* = 1
    is similar, just “flipped” (that is, it’s rotated 180 degrees around the line
    *x* = 0.5).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅在这个区间内交叉熵损失的惩罚要高得多，^([2](ch04.html#idm45732621016264))而且它们的增长速度更快；事实上，当我们的预测与目标之间的差距接近1时，交叉熵损失的值会趋近于无穷！当*y*
    = 1时的图形类似，只是“翻转”了（即，它围绕*x* = 0.5的线旋转了180度）。
- en: So, for problems where we know the output will be between 0 and 1, the cross
    entropy loss produces steeper gradients than MSE. The real magic happens when
    we combine this loss with the softmax function—first feeding the neural network
    output through the softmax function to normalize it so the values add to 1, and
    then feeding the resulting probabilities into the cross entropy loss function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于我们知道输出将在0和1之间的问题，交叉熵损失产生的梯度比均方误差更陡。真正的魔力发生在我们将这种损失与softmax函数结合时——首先将神经网络输出通过softmax函数进行归一化，使值相加为1，然后将得到的概率输入到交叉熵损失函数中。
- en: 'Let’s see what this looks like with the three-class scenario we’ve been using
    so far; the expression for the component of the loss vector from *i* = 1—that
    is, the first component of the loss for a given observation, which we’ll denote
    as *SCE*[1]—is:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在我们迄今为止一直使用的三类情况下是什么样子；从*i* = 1开始的损失向量的分量的表达式，即给定观察值的损失的第一个分量，我们将表示为*SCE*[1]：
- en: <math display="block"><mrow><msub><mrow><mi>S</mi><mi>C</mi><mi>E</mi></mrow>
    <mn>1</mn></msub> <mo>=</mo> <mo>-</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>×</mo>
    <mi>l</mi> <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>1</mn></msub></msup> <mrow><msup><mi>e</mi> <msub><mi>x</mi> <mn>1</mn></msub></msup>
    <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>2</mn></msub></msup> <mo>+</mo><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac> <mo>)</mo></mrow> <mo>-</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>×</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mfrac><msup><mi>e</mi> <msub><mi>x</mi> <mn>1</mn></msub></msup> <mrow><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>1</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>2</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mrow><mi>S</mi><mi>C</mi><mi>E</mi></mrow>
    <mn>1</mn></msub> <mo>=</mo> <mo>-</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>×</mo>
    <mi>l</mi> <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>1</mn></msub></msup> <mrow><msup><mi>e</mi> <msub><mi>x</mi> <mn>1</mn></msub></msup>
    <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>2</mn></msub></msup> <mo>+</mo><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac> <mo>)</mo></mrow> <mo>-</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>×</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mfrac><msup><mi>e</mi> <msub><mi>x</mi> <mn>1</mn></msub></msup> <mrow><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>1</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>2</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac>
    <mo>)</mo></mrow></mrow></math>
- en: 'Based on this expression, the gradient would seem to be a bit trickier for
    this loss. Nevertheless, there’s an elegant expression that is both easy to write
    mathematically and easy to implement:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个表达式，这种损失的梯度似乎对这种损失有点棘手。然而，有一个优雅的表达式，既易于数学书写，又易于实现：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><msub><mrow><mi>S</mi><mi>C</mi><mi>E</mi></mrow>
    <mn>1</mn></msub></mrow> <mrow><mi>∂</mi><msub><mi>x</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>=</mo> <mfrac><msup><mi>e</mi> <msub><mi>x</mi> <mn>1</mn></msub></msup> <mrow><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>1</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>2</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac>
    <mo>-</mo> <msub><mi>y</mi> <mn>1</mn></msub></mrow></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><msub><mrow><mi>S</mi><mi>C</mi><mi>E</mi></mrow>
    <mn>1</mn></msub></mrow> <mrow><mi>∂</mi><msub><mi>x</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>=</mo> <mfrac><msup><mi>e</mi> <msub><mi>x</mi> <mn>1</mn></msub></msup> <mrow><msup><mi>e</mi>
    <msub><mi>x</mi> <mn>1</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi>
    <mn>2</mn></msub></msup> <mo>+</mo><msup><mi>e</mi> <msub><mi>x</mi> <mn>3</mn></msub></msup></mrow></mfrac>
    <mo>-</mo> <msub><mi>y</mi> <mn>1</mn></msub></mrow></math>
- en: 'That means that the *total* gradient to the softmax cross entropy is:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着softmax交叉熵的*总*梯度是：
- en: <math display="block"><mrow><mtext>softmax</mtext> <mrow><mo>(</mo> <mfenced
    close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>)</mo></mrow> <mo>-</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>y</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>y</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>softmax</mtext> <mrow><mo>(</mo> <mfenced
    close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>)</mo></mrow> <mo>-</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>y</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>y</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'That’s it! As promised, the resulting implementation is simple as well:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！正如承诺的那样，最终的实现也很简单：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s code this up.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写代码。
- en: Code
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码
- en: 'To recap [Chapter 3](ch03.html#deep_learning_from_scratch), any `Loss` class
    is expected to receive two 2D arrays, one with the network’s predictions and the
    other with the targets. The number of rows in each array is the batch size, and
    the number of columns is the number of classes `n` in the classification problem;
    a row in each represents an observation in the dataset, with the `n` values in
    the row representing the neural network’s best guess for the probabilities of
    that observation belonging to each of the `n` classes. Thus, we’ll have to apply
    the `softmax` to *each row* in the `prediction` array. This leads to a first potential
    issue: we’ll next feed the resulting numbers into the `log` function to compute
    the loss. This should worry you, since *log*(*x*) goes to negative infinity as
    *x* goes to 0, and similarly, 1 – *x* goes to infinity as *x* goes to 1\. To prevent
    extremely large loss values that could lead to numeric instability, we’ll clip
    the output of the softmax function to be no less than 10^(–7) and no greater than
    10⁷.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[第3章](ch03.html#deep_learning_from_scratch)，任何`Loss`类都应该接收两个2D数组，一个是网络的预测，另一个是目标。每个数组中的行数是批量大小，列数是分类问题中的类别数`n`；每个数组中的一行代表数据集中的一个观察值，行中的`n`个值代表神经网络对该观察值属于每个`n`类的概率的最佳猜测。因此，我们将需要对`prediction`数组中的*每一行*应用`softmax`。这会导致一个潜在的问题：接下来我们将把结果数值输入`log`函数来计算损失。这应该让你担心，因为*log*(*x*)当*x*趋近于0时会趋向于负无穷，同样地，1
    - *x*当*x*趋近于1时会趋向于无穷。为了防止可能导致数值不稳定的极大损失值，我们将裁剪softmax函数的输出，使其不小于10^(–7)且不大于10⁷。
- en: Finally, we can put everything together!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以把所有东西放在一起！
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Soon I’ll show via some experiments on the MNIST dataset how this loss is an
    improvement on the mean squared error loss. But first let’s discuss the trade-offs
    involved with choosing an activation function and see if there’s a better choice
    than sigmoid.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 很快我将通过一些在MNIST数据集上的实验来展示这种损失是如何改进均方误差损失的。但首先让我们讨论选择激活函数涉及的权衡，并看看是否有比sigmoid更好的选择。
- en: A Note on Activation Functions
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于激活函数的说明
- en: 'We argued in [Chapter 2](ch02.html#fundamentals) that sigmoid was a good activation
    function because it:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第2章](ch02.html#fundamentals)中争论说sigmoid是一个很好的激活函数，因为它：
- en: Was a nonlinear and monotonic function
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是一个非线性和单调函数
- en: Provided a “regularizing” effect on the model, forcing the intermediate features
    down to a finite range, specifically between 0 and 1
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型产生了“正则化”效果，将中间特征强制限制在一个有限范围内，具体在0和1之间
- en: 'Nevertheless, sigmoid has a downside, similar to the downside of the mean squared
    error loss: *it produces relatively flat gradients* during the backward pass.
    The gradient that gets passed to the sigmoid function (or any function) on the
    backward pass represents how much the function’s *output* ultimately affects the
    loss; because the maximum slope of the sigmoid function is 0.25, these gradients
    will *at best* be divided by 4 when sent backward to the previous operation in
    the model. Worse still, when the input to the sigmoid function is less than –2
    or greater than 2, the gradient those inputs receive will be almost 0, since *sigmoid*(*x*)
    is almost flat at *x* = –2 or *x* = 2\. What this means is that any parameters
    influencing these inputs will receive small gradients, and our network could learn
    slowly as a result.^([3](ch04.html#idm45732620742360)) Furthermore, if multiple
    sigmoid activation functions are used in successive layers of a neural network,
    this problem will compound, further diminishing the gradients that weights earlier
    in the neural network could receive.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: What would an activation “at the other extreme”—one with the opposite strengths
    and weaknesses—look like?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'The other extreme: the Rectified Linear Unit'
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Rectified Linear Unit, or ReLU, activation is a commonly used activation
    with the opposite strengths and weaknesses of sigmoid. ReLU is simply defined
    to be 0 if *x* is less than 0, and *x* otherwise. A plot of this is shown in [Figure 4-5](#fig_04_05).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![dlfs 0405](assets/dlfs_0405.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. ReLU activation
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a “valid” activation function in the sense that it is monotonic and
    nonlinear. It produces much larger gradients than sigmoid—1 if the input to the
    function is greater than 0, and 0 otherwise, for an average of 0.5—whereas the
    *maximum* gradient sigmoid can produce is 0.25\. ReLU activation is a very popular
    choice in deep neural network architectures, because its downside (that it draws
    a sharp, somewhat arbitrary distinction between values less than or greater than
    0) can be addressed by other techniques, including some that will be covered in
    this chapter, and its benefits (of producing large gradients) are critical to
    training the weights in the architectures of deep neural networks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, there’s an activation function that is a happy medium between
    these two, and that we’ll use in the demos in this chapter: Tanh.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'A happy medium: Tanh'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Tanh function is shaped similarly to the sigmoid function but maps inputs
    to values between –1 and 1\. [Figure 4-6](#fig_04_06) shows this function.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![dlfs 0406](assets/dlfs_0406.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Tanh activation
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This function produces significantly steeper gradients than sigmoid; specifically,
    the maximum gradient of Tanh turns out to be 1, in contrast to sigmoid’s 0.25\.
    [Figure 4-7](#fig_04_07) shows the gradients of these two functions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![dlfs 0407](assets/dlfs_0407.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Sigmoid derivative versus Tanh derivative
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition, just as <math><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>=</mo> <mi>s</mi> <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> has the easy-to-express derivative
    <math><mrow><msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>s</mi> <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>s</mi> <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
    , so too does <math><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo>
    <mi>t</mi> <mi>a</mi> <mi>n</mi> <mi>h</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    have the easy-to-express derivative <math><mrow><msup><mi>f</mi> <mo>'</mo></msup>
    <msup><mrow><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
    <mn>2</mn></msup></mrow></math> .
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，就像<math><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>s</mi>
    <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math>有易于表达的导数<math><mrow><msup><mi>f</mi> <mo>'</mo></msup>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>s</mi> <mi>i</mi>
    <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>s</mi>
    <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>，<math><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>t</mi> <mi>a</mi> <mi>n</mi> <mi>h</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>也有易于表达的导数<math><mrow><msup><mi>f</mi>
    <mo>'</mo></msup> <msup><mrow><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
    <mn>2</mn></msup></mrow></math>。
- en: 'The point here is that there are trade-offs involved with choosing an activation
    function regardless of the architecture: we want an activation function that will
    allow our network to learn nonlinear relationships between input and output, while
    not adding unnecessary complexity that will make it harder for the network to
    find a good solution. For example, the “Leaky ReLU” activation function allows
    a slight negative slope when the input to the ReLU function is less than 0, enhancing
    ReLU’s ability to send gradients backward, and the “ReLU6” activation function
    caps the positive end of ReLU at 6, introducing even more nonlinearity into the
    network. Still, both of these activation functions are more complex than ReLU;
    and if the problem we are dealing with is relatively simple, those more sophisticated
    activation functions could make it even harder for the network to learn. Thus,
    in the models we demo in the rest of this book, we’ll simply use the Tanh activation
    function, which balances these considerations well.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重点是，无论架构如何，选择激活函数都涉及权衡：我们希望一个激活函数能够让我们的网络学习输入和输出之间的非线性关系，同时不会增加不必要的复杂性，使网络更难找到一个好的解决方案。例如，“Leaky
    ReLU”激活函数在输入到ReLU函数小于0时允许一个轻微的负斜率，增强了ReLU发送梯度向后的能力，“ReLU6”激活函数将ReLU的正端限制在6，进一步引入了更多的非线性到网络中。然而，这两种激活函数都比ReLU更复杂；如果我们处理的问题相对简单，那么这些更复杂的激活函数可能会使网络学习变得更加困难。因此，在本书的其余部分中我们演示的模型中，我们将简单地使用Tanh激活函数，它很好地平衡了这些考虑。
- en: Now that we’ve chosen an activation function, let’s use it to run some experiments.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经选择了一个激活函数，让我们用它来进行一些实验。
- en: Experiments
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: 'We’ve justified using `Tanh` throughout our experiments, so let’s get back
    to the original point of this section: showing why the softmax cross entropy loss
    is so pervasive throughout deep learning.^([4](ch04.html#idm45732620666312)) We’ll
    use the MNIST dataset, which consists of black-and-white images of handwritten
    digits that are 28 × 28 pixels, with the value of each pixel ranging from 0 (white)
    to 255 (black). Furthermore, the dataset is predivided into a training set of
    60,000 images and a testing set of 10,000 additional images. In the book’s [GitHub
    repo](https://oreil.ly/2H7rJvf), we show a helper function to read both the images
    and their corresponding labels into training and testing sets using the following
    line of code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明在我们的实验中使用`Tanh`，所以让我们回到本节的原始目的：展示为什么softmax交叉熵损失在深度学习中如此普遍。我们将使用MNIST数据集，该数据集包含黑白手写数字的图像，每个图像为28×28像素，每个像素的值范围从0（白色）到255（黑色）。此外，该数据集已经预先划分为一个包含60,000个图像的训练集和一个包含额外10,000个图像的测试集。在本书的[GitHub存储库](https://oreil.ly/2H7rJvf)中，我们展示了一个帮助函数，使用以下代码行将图像及其对应的标签读入训练和测试集中：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Our goal will be to train a neural network to learn which of the 10 digits from
    0 to 9 the image contains.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是训练一个神经网络，让它学会识别图像中包含的从0到9的10个数字。
- en: Data Preprocessing
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'For classification, we have to perform *one-hot encoding* to transform our
    vectors representing the labels into an `ndarray` of the same shape as the predictions:
    specifically, we’ll map the label “0” to a vector with a 1 in the first position
    (at index 0) and 0s in all the other positions, “1” to a vector with a 1 in the
    second position (at index 1), [and so on](https://oreil.ly/2KTRm3z):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类，我们必须执行*one-hot编码*，将表示标签的向量转换为与预测相同形状的`ndarray`：具体来说，我们将标签“0”映射到一个向量，第一个位置（索引0）为1，其他位置为0，“1”映射到第二个位置（索引1），[以此类推](https://oreil.ly/2KTRm3z)：
- en: <math display="block"><mrow><mrow><mo>[</mo> <mn>0</mn> <mo>,</mo> <mn>2</mn>
    <mo>,</mo> <mn>1</mn> <mo>]</mo></mrow> <mo>⇒</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mo>...</mo></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mo>...</mo></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mo>...</mo></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mrow><mo>[</mo> <mn>0</mn> <mo>,</mo> <mn>2</mn>
    <mo>,</mo> <mn>1</mn> <mo>]</mo></mrow> <mo>⇒</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mo>...</mo></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mo>...</mo></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mo>...</mo></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Finally, it is always helpful to scale our data to mean 0 and variance 1, just
    as we did with the “real-world” datasets in the prior chapters. Here, however,
    since each data point is an image, we won’t scale each *feature* to have mean
    0 and variance 1, since that would result in the values of adjacent pixels being
    changed by different amounts, which could distort the image! Instead, we’ll simply
    provide a global scaling to our dataset that subtracts off the overall mean and
    divides by the overall variance (note that we use the statistics from the training
    set to scale the testing set):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将数据缩放到均值为0，方差为1总是有帮助的，就像我们在之前章节中对“真实世界”数据集所做的那样。然而，在这里，由于每个数据点都是一幅图像，我们不会将每个*特征*缩放为均值为0，方差为1，因为这样会导致相邻像素的值被不同的量改变，从而可能扭曲图像！相反，我们将为我们的数据集提供一个全局缩放，减去整体均值并除以整体方差（请注意，我们使用训练集的统计数据来缩放测试集）：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Model
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: 'We’ll have to define our model to have 10 outputs for each input: one for each
    of the probabilities of our model belonging to each of the 10 classes. Since we
    know each of our outputs will be a probability, we’ll give our model a `sigmoid`
    activation on the last layer. Throughout this chapter, to illustrate whether the
    “training tricks” we’re describing actually enhance our models’ ability to learn,
    we’ll use a consistent model architecture of a two-layer neural network with the
    number of neurons in the hidden layer close to the geometric mean of our number
    of inputs (784) and our number of outputs (10): <math><mrow><mn>89</mn> <mo>≈</mo>
    <msqrt><mrow><mn>784</mn> <mo>×</mo> <mn>10</mn></mrow></msqrt></mrow></math>
    .'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义我们的模型对每个输入有10个输出：一个输出对应于模型属于10个类别中的每一个的概率。由于我们知道每个输出都将是一个概率，我们将在最后一层给我们的模型一个`sigmoid`激活。在本章中，为了说明我们描述的“训练技巧”是否真的增强了我们模型的学习能力，我们将使用一个一致的模型架构，即一个具有隐藏层神经元数量接近我们输入数量（784）和输出数量（10）几何平均值的两层神经网络：<math><mrow><mn>89</mn>
    <mo>≈</mo> <msqrt><mrow><mn>784</mn> <mo>×</mo> <mn>10</mn></mrow></msqrt></mrow></math>。
- en: 'Let’s now turn to our first experiment, comparing a neural network trained
    with simple mean squared error loss to one trained with softmax cross entropy
    loss. The loss values you see displayed are per observation (recall that on average
    cross entropy loss will have absolute loss values three times as large as mean
    squared error loss). If we run:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向我们的第一个实验，比较一个使用简单均方误差损失训练的神经网络和一个使用softmax交叉熵损失训练的神经网络。您看到的损失值是每个观察值的（请记住，平均交叉熵损失的绝对损失值将是均方误差损失的三倍）。如果我们运行：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'it gives us:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 它给了我们：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let’s test the claim we made earlier in the chapter: that the softmax cross
    entropy loss function would help our model learn faster.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们测试之前在本章中提到的一个观点：softmax交叉熵损失函数将帮助我们的模型更快地学习。
- en: 'Experiment: Softmax Cross Entropy Loss'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验：Softmax交叉熵损失
- en: 'First let’s change the preceding model to:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们将前述模型更改为：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since we are now feeding the model outputs through the softmax function as part
    of the loss, we no longer need to feed them through a sigmoid activation function.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在将模型输出通过softmax函数作为损失的一部分，我们不再需要通过sigmoid激活函数。
- en: 'Then we run it for model for 50 epochs, which gives us these results:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对模型运行50个epochs，得到以下结果：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Indeed, changing our loss function to one that gives much steeper gradients
    alone gives a huge boost to the accuracy of our model!^([5](ch04.html#idm45732620238856))
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，将我们的损失函数更改为一个给出更陡的梯度的函数，单独就能极大地提高我们模型的准确性！
- en: Of course, we can do significantly better than this, even without changing our
    architecture. In the next section, we’ll cover momentum, the most important and
    straightforward extension to the stochastic gradient descent optimization technique
    we’ve been using up until now.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，即使不改变我们的架构，我们也可以做得更好。在下一节中，我们将介绍动量，这是迄今为止我们一直使用的随机梯度下降优化技术最重要和最直接的扩展。
- en: Momentum
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动量
- en: 'So far, we’ve been using only one “update rule” for our weights at each time
    step. Simply take the derivative of the loss with respect to the weights and move
    the weights in the resulting correct direction. This means that our `_update_rule`
    function in the `Optimizer` looked like:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在每个时间步只使用了一个“更新规则”来更新我们的权重。简单地对损失函数关于权重的导数进行计算，然后将权重朝着正确的方向移动。这意味着我们在`Optimizer`中的`_update_rule`函数看起来像这样：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s first cover the intuition for why we might want to extend this update
    rule to incorporate momentum.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们来看一下为什么我们可能希望将这个更新规则扩展到包含动量的直觉。
- en: Intuition for Momentum
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动量的直觉
- en: Recall [Figure 4-3](#fig_04_03), which plotted an individual parameter’s value
    against the loss value from the network. Imagine a scenario in which the parameter’s
    value is continually updated in the same direction because the loss continues
    to decrease with each iteration. This would be analogous to the parameter “rolling
    down a hill,” and the value of the update at each time step would be analogous
    to the parameter’s “velocity.” In the real world, however, objects don’t instantaneously
    stop and change directions; that’s because they have *momentum*, which is just
    a concise way of saying their velocity at a given instant is a function not just
    of the forces acting on them in that instant but also of their accumulated past
    velocities, with more recent velocities weighted more heavily. This physical interpretation
    is the motivation for applying momentum to our weight updates. In the next section
    we’ll make this precise.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[图4-3](#fig_04_03)，它绘制了一个参数值与网络损失值之间的关系。想象一种情况，即参数值不断朝着同一个方向更新，因为损失值在每次迭代中都在减小。这就好比参数“滚动下山”，而每个时间步的更新值就好比参数的“速度”。然而，在现实世界中，物体不会瞬间停下并改变方向；这是因为它们具有*动量*，这只是说它们在某一时刻的速度不仅仅是受到那一时刻作用在它们身上的力的影响，还受到它们过去速度的累积影响，最近的速度权重更大。这种物理解释是应用动量到我们的权重更新的动机。在下一节中，我们将详细介绍这一点。
- en: Implementing Momentum in the Optimizer Class
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在优化器类中实现动量
- en: Basing our parameter updates on momentum means that *the parameter update at
    each time step will be a weighted average of the parameter updates at past time
    steps, with the weights decayed exponentially*. There will thus be a second parameter
    we have to choose, the momentum parameter, which will determine the degree of
    this decay; the higher it is, the more the weight update at each time step will
    be based on the parameter’s accumulated momentum as opposed to its current velocity.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 基于动量的参数更新意味着*每个时间步的参数更新将是过去时间步的参数更新的加权平均，权重呈指数衰减*。因此，我们还需要选择第二个参数，动量参数，它将决定这种衰减的程度；它越高，每个时间步的权重更新将更多地基于参数的累积动量而不是当前速度。
- en: Math
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数学
- en: 'Mathematically, if our momentum parameter is *μ*, and the gradient at each
    time step is <math><msub><mi>∇</mi> <mi>t</mi></msub></math> , our weight update
    is:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，如果我们的动量参数是*μ*，每个时间步的梯度是<math><msub><mi>∇</mi> <mi>t</mi></msub></math>，我们的权重更新是：
- en: <math display="block"><mrow><mtext>update</mtext> <mo>=</mo> <msub><mi>∇</mi>
    <mi>t</mi></msub> <mo>+</mo> <mi>μ</mi> <mo>×</mo> <msub><mi>∇</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msup><mi>μ</mi> <mn>2</mn></msup> <mo>×</mo> <msub><mi>∇</mi> <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <mo>...</mo></mrow></math>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>update</mtext> <mo>=</mo> <msub><mi>∇</mi>
    <mi>t</mi></msub> <mo>+</mo> <mi>μ</mi> <mo>×</mo> <msub><mi>∇</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msup><mi>μ</mi> <mn>2</mn></msup> <mo>×</mo> <msub><mi>∇</mi> <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <mo>...</mo></mrow></math>
- en: If our momentum parameter was 0.9, for example, we would multiply the gradient
    from one time step ago by 0.9, the one from two time steps ago by 0.9² = 0.81,
    the one from three time steps ago by 0.9³ = 0.729, and so on, and then finally
    add all of these to the gradient from the current time step to get the overall
    weight update for the current time step.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的动量参数是0.9，我们将把一个时间步之前的梯度乘以0.9，两个时间步之前的梯度乘以0.9² = 0.81，三个时间步之前的梯度乘以0.9³
    = 0.729，依此类推，然后最后将所有这些加到当前时间步的梯度中，以获得当前时间步的整体权重更新。
- en: Code
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码
- en: How do we implement this? Do we have to compute an infinite sum every time we
    want to update our weights?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何实现这个？我们是否每次想要更新权重时都必须计算无限和？
- en: It turns out there is a cleverer way. Our `Optimizer` will keep track of a separate
    quantity representing the *history* of parameter updates in addition to just receiving
    a gradient at each time step. Then, at each time step, we’ll use the current gradient
    to update this history and compute the actual parameter update as a function of
    this history. Since momentum is loosely based on an analogy with physics, we’ll
    call this quantity “velocity.”
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 原来有一个更聪明的方法。我们的`Optimizer`将跟踪一个表示参数更新*历史*的单独数量，除了每个时间步接收一个梯度。然后，在每个时间步，我们将使用当前梯度来更新这个历史，并根据这个历史计算实际的参数更新。由于动量在某种程度上基于与物理学的类比，我们将称这个数量为“速度”。
- en: 'How should we update velocity? It turns out we can use the following steps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何更新速度？原来我们可以使用以下步骤：
- en: Multiply it by the momentum parameter.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其乘以动量参数。
- en: Add the gradient.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加梯度。
- en: 'This results in the velocity taking on the following values at each time step,
    starting at *t* = 1:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致速度在每个时间步取以下值，从*t* = 1开始：
- en: <math><msub><mi>∇</mi> <mn>1</mn></msub></math>
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <math><msub><mi>∇</mi> <mn>1</mn></msub></math>
- en: <math><mrow><msub><mi>∇</mi> <mn>2</mn></msub> <mo>+</mo> <mi>μ</mi> <mo>×</mo>
    <msub><mi>∇</mi> <mn>1</mn></msub></mrow></math>
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>∇</mi> <mn>2</mn></msub> <mo>+</mo> <mi>μ</mi> <mo>×</mo>
    <msub><mi>∇</mi> <mn>1</mn></msub></mrow></math>
- en: <math><mrow><msub><mi>∇</mi> <mn>3</mn></msub> <mo>+</mo> <mi>μ</mi> <mo>×</mo>
    <mrow><mo>(</mo> <msub><mi>∇</mi> <mn>2</mn></msub> <mo>+</mo> <mi>μ</mi> <mo>×</mo>
    <msub><mi>∇</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>μ</mi> <mo>×</mo>
    <msub><mi>∇</mi> <mn>2</mn></msub> <mo>+</mo> <msup><mi>μ</mi> <mn>2</mn></msup>
    <mo>×</mo> <msub><mi>∇</mi> <mn>1</mn></msub> <mrow><mo>)</mo></mrow></mrow></math>
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>∇</mi> <mn>3</mn></msub> <mo>+</mo> <mi>μ</mi> <mo>×</mo>
    <mrow><mo>(</mo> <msub><mi>∇</mi> <mn>2</mn></msub> <mo>+</mo> <mi>μ</mi> <mo>×</mo>
    <msub><mi>∇</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>μ</mi> <mo>×</mo>
    <msub><mi>∇</mi> <mn>2</mn></msub> <mo>+</mo> <msup><mi>μ</mi> <mn>2</mn></msup>
    <mo>×</mo> <msub><mi>∇</mi> <mn>1</mn></msub> <mrow><mo>)</mo></mrow></mrow></math>
- en: 'With this, we can use the velocity as the parameter update! We can then incorporate
    this into a new subclass of `Optimizer` that we’ll call `SGDMomentum`; this class
    will have `step` and `_update_rule` functions that look as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以使用速度作为参数更新！然后我们可以将其合并到一个名为`SGDMomentum`的新的`Optimizer`子类中；这个类将有`step`和`_update_rule`函数，如下所示：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Let’s see if this new optimizer can improve our network’s training.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个新的优化器是否可以改善我们网络的训练。
- en: 'Experiment: Stochastic Gradient Descent with Momentum'
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验：带有动量的随机梯度下降
- en: 'Let’s train the same neural network with one hidden layer on the MNIST dataset,
    changing nothing except using `optimizer = SGDMomentum(lr=0.1, momentum=0.9)`
    as the optimizer instead of `optimizer = SGD(lr=0.1)`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用MNIST数据集上的一个隐藏层训练相同的神经网络，除了使用`optimizer = SGDMomentum(lr=0.1, momentum=0.9)`作为优化器，而不是`optimizer
    = SGD(lr=0.1)`：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can see that the loss is significantly lower and the accuracy significantly
    higher, which is simply a result of adding momentum into our parameter update
    rule!^([6](ch04.html#idm45732620023000))
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到损失明显降低，准确率明显提高，这只是将动量添加到我们的参数更新规则中的结果！^([6](ch04.html#idm45732620023000))
- en: Of course, another way to modify our parameter update at each iteration would
    be to modify our learning rate; while we can manually change our initial learning
    rate, we can also automatically decay the learning rate as training proceeds using
    some rule. The most common such rules are covered next.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，修改每次迭代中的参数更新的另一种方法是修改学习率；虽然我们可以手动更改初始学习率，但我们也可以使用某些规则在训练过程中自动衰减学习率。接下来将介绍最常见的这种规则。
- en: Learning Rate Decay
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率衰减
- en: '[The learning rate] is often the single most important hyper-parameter and
    one should always make sure that it has been tuned.'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[学习率]通常是最重要的超参数，我们应该始终确保它已经调整。'
- en: ''
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yoshua Bengio, *Practical Recommendations for Gradient-Based Training of Deep
    Architectures*, 2012
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Yoshua Bengio，*深度架构基于梯度训练的实用建议*，2012
- en: 'The motivation for decaying the learning rate as training progresses comes,
    yet again, from [Figure 4-3](#fig_04_03) in the previous section: while we want
    to “take big steps” toward the beginning of training, it is likely that as we
    continue to iteratively update the weights, we will reach a point where we start
    to “skip over” the minimum. Note that this won’t *necessarily* be a problem, since
    if the relationship between our weights and the loss “smoothly declines” as we
    approach the minimum, as in [Figure 4-3](#fig_04_03), the magnitude of the gradients
    will automatically decrease as the slope decreases. Still, this may not happen,
    and even if it does, learning rate decay can give us more fine-grained control
    over this process.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，学习率衰减的动机再次来自前一节中的[图4-3](#fig_04_03)：虽然我们希望在训练开始时“迈大步”，但很可能随着我们继续迭代更新权重，我们会达到一个开始“跳过”最小值的点。请注意，这不一定会是一个问题，因为如果我们的权重与损失之间的关系“平滑下降”接近最小值，就像[图4-3](#fig_04_03)中一样，梯度的幅度会随着斜率的减小而自动减小。尽管如此，这可能不会发生，即使发生了，学习率衰减也可以让我们更精细地控制这个过程。
- en: Types of Learning Rate Decay
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率衰减的类型
- en: 'There are different ways of decaying the learning rate. The simplest is linear
    decay, where the learning rate declines linearly from its initial value to some
    terminal value, with the actual decline being implemented at the end of each epoch.
    More precisely, at time step *t*, if the learning rate we want to start with is
    <math><msub><mi>α</mi> <mrow><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub></math>
    , and our final learning rate is <math><msub><mi>α</mi> <mrow><mi>e</mi><mi>n</mi><mi>d</mi></mrow></msub></math>
    , then our learning rate at each time step is:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的学习率衰减方式。最简单的是线性衰减，其中学习率从初始值线性下降到某个终端值，实际下降是在每个时期结束时实施的。更准确地说，在时间步*t*，如果我们想要开始的学习率是
    <math><msub><mi>α</mi> <mrow><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub></math>，我们的最终学习率是
    <math><msub><mi>α</mi> <mrow><mi>e</mi><mi>n</mi><mi>d</mi></mrow></msub></math>，那么我们每个时间步的学习率是：
- en: <math display="block"><mrow><msub><mi>α</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>α</mi>
    <mrow><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub> <mo>-</mo>
    <mrow><mo>(</mo> <msub><mi>α</mi> <mrow><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mi>α</mi> <mrow><mi>e</mi><mi>n</mi><mi>d</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>×</mo> <mfrac><mi>t</mi> <mi>N</mi></mfrac></mrow></math>
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>α</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>α</mi>
    <mrow><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub> <mo>-</mo>
    <mrow><mo>(</mo> <msub><mi>α</mi> <mrow><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mi>α</mi> <mrow><mi>e</mi><mi>n</mi><mi>d</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>×</mo> <mfrac><mi>t</mi> <mi>N</mi></mfrac></mrow></math>
- en: where *N* is the total number of epochs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*N*是总时期数。
- en: 'Another simple method that works about as well is exponential decay, in which
    the learning rate declines by a constant *proportion* each epoch. The formula
    here would simply be:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种效果差不多的简单方法是指数衰减，其中学习率每个时期按照一个恒定的*比例*下降。这里的公式简单地是：
- en: <math display="block"><mrow><msub><mi>α</mi> <mi>t</mi></msub> <mo>=</mo> <mi>α</mi>
    <mo>×</mo> <msup><mi>δ</mi> <mi>t</mi></msup></mrow></math>
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>α</mi> <mi>t</mi></msub> <mo>=</mo> <mi>α</mi>
    <mo>×</mo> <msup><mi>δ</mi> <mi>t</mi></msup></mrow></math>
- en: 'where:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: <math display="block"><mrow><mi>δ</mi> <mo>=</mo> <msup><mfrac><msub><mi>α</mi>
    <mrow><mi>e</mi><mi>n</mi><mi>d</mi></mrow></msub> <msub><mi>α</mi> <mrow><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub></mfrac>
    <mfrac><mn>1</mn> <mrow><mi>N</mi><mo>-</mo><mn>1</mn></mrow></mfrac></msup></mrow></math>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>δ</mi> <mo>=</mo> <msup><mfrac><msub><mi>α</mi>
    <mrow><mi>e</mi><mi>n</mi><mi>d</mi></mrow></msub> <msub><mi>α</mi> <mrow><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub></mfrac>
    <mfrac><mn>1</mn> <mrow><mi>N</mi><mo>-</mo><mn>1</mn></mrow></mfrac></msup></mrow></math>
- en: 'Implementing these is straightforward: we’ll initialize our `Optimizer`s to
    have a “final learning rate” `final_lr` that the initial learning rate will decay
    toward throughout the epochs of training:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这些很简单：我们将初始化我们的`Optimizer`，使其具有“最终学习率”`final_lr`，初始学习率将在训练时期内向其衰减：
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, at the beginning of training, we can call a `_setup_decay` function that
    calculates how much the learning rate will decay at each epoch:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在训练开始时，我们可以调用一个 `_setup_decay` 函数来计算每个时期学习率将衰减多少：
- en: '[PRE17]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'These calculations will implement the linear and exponential learning rate
    decay formulas we just saw:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算将实现我们刚刚看到的线性和指数学习率衰减公式：
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, at the end of each epoch, we’ll actually decay the learning rate:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在每个时期结束时，我们将实际衰减学习率：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we’ll call the `_decay_lr` function from the `Trainer` during the
    `fit` function, at the end of each epoch:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在`fit`函数中，我们将在每个时期结束时从`Trainer`中调用`_decay_lr`函数：
- en: '[PRE20]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s run some experiments to see if this improves training.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一些实验，看看这是否有助于改善训练。
- en: 'Experiments: Learning Rate Decay'
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验：学习率衰减
- en: 'Next we try training the same model architecture with learning rate decay.
    We initialize the learning rates so that the “average learning rate” over the
    run is equal to the previous learning rate of 0.1: for the run with linear learning
    rate decay, we initialize the learning rate to 0.15 and decay it down to 0.05,
    and for the run with exponential decay, we initialize the learning rate to 0.2
    and decay it down to 0.05\. For the linear decay run with:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们尝试使用学习率衰减训练相同的模型架构。我们初始化学习率，使得整个运行过程中的“平均学习率”等于之前的学习率0.1：对于线性学习率衰减的运行，我们将学习率初始化为0.15，衰减到0.05；对于指数衰减的运行，我们将学习率初始化为0.2，衰减到0.05。对于线性衰减运行：
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'we have:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有：
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For the run with exponential decay, with:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于指数衰减的运行，有：
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'we have:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有：
- en: '[PRE24]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The losses in the “best models” from these runs were 0.282 and 0.284, significantly
    lower than the 0.338 that we had before!
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些运行中“最佳模型”的损失分别为0.282和0.284，明显低于之前的0.338！
- en: 'Next up: how and why to more intelligently initialize the weights of our model.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来：如何以及为什么更智能地初始化模型的权重。
- en: Weight Initialization
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重初始化
- en: As we mentioned in the section on activation functions, several activation functions,
    such as sigmoid and Tanh, have their steepest gradients when their inputs are
    0, with the functions quickly flattening out as the inputs move away from 0\.
    This can potentially limit the effectiveness of these functions, because if many
    of the inputs have values far from 0, the weights attached to those inputs will
    receive very small gradients on the backward pass.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在激活函数部分提到的，一些激活函数，如sigmoid和Tanh，在其输入为0时具有最陡的梯度，随着输入远离0，函数迅速变平。这可能会限制这些函数的有效性，因为如果许多输入的值远离0，那么附加到这些输入的权重将在反向传播中接收到非常小的梯度。
- en: This turns out to be a major problem in the neural networks we are now dealing
    with. Consider the hidden layer in the MNIST network we’ve been looking at. This
    layer will receive 784 inputs and then multiply them by a weight matrix, ending
    up with some number `n` of neurons (and then optionally add a bias to each neuron).
    [Figure 4-8](#distribution_of_inputs) shows the distribution of these `n` values
    in the hidden layer of our neural network (with 784 inputs) before and after feeding
    them through the `Tanh` activation function.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution of inputs to activation](assets/dlfs_0408.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. Distribution of inputs to activation function and activations
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After being fed through the activation function, most of the activations are
    either –1 or 1! This is because each feature is mathematically defined to be:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>f</mi> <mi>n</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow></msub> <mo>×</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>w</mi> <mrow><mn>784</mn><mo>,</mo><mi>n</mi></mrow></msub>
    <mo>×</mo> <msub><mi>x</mi> <mn>784</mn></msub> <mo>+</mo> <msub><mi>b</mi> <mi>n</mi></msub></mrow></math>
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>f</mi> <mi>n</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow></msub> <mo>×</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>w</mi> <mrow><mn>784</mn><mo>,</mo><mi>n</mi></mrow></msub>
    <mo>×</mo> <msub><mi>x</mi> <mn>784</mn></msub> <mo>+</mo> <msub><mi>b</mi> <mi>n</mi></msub></mrow></math>
- en: 'Since we initialized each weight to have variance 1 ( <math><mrow><mtext>Var</mtext>
    <mo>(</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>)</mo> <mo>=</mo> <mn>1</mn></mrow></math> —and <math><mrow><mtext>Var</mtext>
    <mo>(</mo> <msub><mi>b</mi> <mi>n</mi></msub> <mo>)</mo> <mo>=</mo> <mn>1</mn></mrow></math>
    ) and <math><mrow><mtext>Var</mtext> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mtext>Var</mtext>
    <mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo>
    <mtext>Var</mtext> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    for independent random variables *X*[1] and *X*[2], we have:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>Var</mtext> <mo>(</mo> <msub><mi>f</mi> <mi>n</mi></msub>
    <mo>)</mo> <mo>=</mo> <mn>785</mn></mrow></math>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>Var</mtext> <mo>(</mo> <msub><mi>f</mi> <mi>n</mi></msub>
    <mo>)</mo> <mo>=</mo> <mn>785</mn></mrow></math>
- en: That gives it a standard deviation ( <math><msqrt><mn>785</mn></msqrt></math>
    ) of just over 28, which reflects the spread of the values we see in the top half
    of [Figure 4-8](#distribution_of_inputs).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'This tells us we have a problem. But is the problem simply that the features
    that we feed into the activation functions can’t be “too spread out”? If that
    were the problem, we could simply divide the features by some value to reduce
    their variance. However, that invites an obvious question: how do we know what
    to divide the values by? The answer is that the values should be scaled *based
    on the number of neurons being fed into the layer*. If we had a multilayer neural
    network, and one layer had 200 neurons and the next layer had 100, the 200-neuron
    layer would pass values forward that had a wider distribution than the 100-neuron
    layer. This is undesirable—we don’t want the *scale* of the features our neural
    network learns during training to depend on the number of features passed forward,
    for the same reason that we don’t want our network’s predictions to depend on
    the scale of our input features. Our model’s predictions shouldn’t be affected,
    for example, if we multiplied or divided all the values in a feature by 2.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways to correct for this; here we’ll cover the single most
    prevalent one, suggested by the prior paragraph: we can *adjust the initial variance
    of the weights based on the number of neurons in the layers they connect so that
    the values passed forward to the following layer during the forward pass* and
    *backward to the prior layer during the backward pass* have roughly the same scale.
    Yes, we have to think about the backward pass too, since the same problem exists
    there: the variance of the gradients the layer receives during backpropagation
    will depend directly on the number of features in the *following* layer since
    that is the one that sends gradients backward to the layer in question.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Math and Code
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How, specifically, do we balance these concerns? If each layer has <math><msub><mi>n</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow></msub></math> neurons feeding in and <math><msub><mi>n</mi>
    <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></math> neurons coming out,
    the variance for each weight that would keep the variance of the resulting features
    constant *just on the forward pass* would be:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfrac><mn>1</mn> <msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></mfrac></math>
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfrac><mn>1</mn> <msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></mfrac></math>
- en: 'Similarly, the weight variance that would keep the variance of the features
    constant on the backward pass would be:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfrac><mn>1</mn> <msub><mi>n</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mfrac></math>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfrac><mn>1</mn> <msub><mi>n</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mfrac></math>
- en: 'As a compromise between these, what is most often called *Glorot initialization*^([7](ch04.html#idm45732619408344))
    involves initializing the variance of the weights in each layer to be:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfrac><mn>2</mn> <mrow><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>+</mo><msub><mi>n</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow></mfrac></math>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfrac><mn>2</mn> <mrow><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>+</mo><msub><mi>n</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow></mfrac></math>
- en: 'Coding this up is simple—we add a `weight_init` argument to each layer, and
    we add the following to our `_setup_layer` function:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now our models will look like:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: with `weight_init="glorot"` specified for each layer.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiments: Weight Initialization'
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Running the same models from the prior section but with the weights initialized
    using Glorot initialization gives:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'for the model with linear learning rate decay, and:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: for the model with exponential learning rate decay. We see another significant
    drop in the loss here, from the 0.282 and 0.284 we achieved earlier down to 0.244
    and 0.245! Note that with all of these changes, *we haven’t been increasing the
    size or training time of our model*; we’ve simply tweaked the training process
    based on our intuition about what neural networks are trying to do that I showed
    at the beginning of this chapter.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: There is one last technique we’ll cover in this chapter. As motivation, you
    may have noticed that *none of the models we’ve used throughout this chapter were
    deep learning models*; rather, they were simply neural networks with one hidden
    layer. That is because without *dropout*, the technique we will now learn, deep
    learning models are very challenging to train effectively without overfitting.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, I’ve shown several modifications to our neural network’s training
    procedure that got it closer and closer to its global minimum. You may have noticed
    that we haven’t tried the seemingly most obvious thing: adding more layers to
    our network or more neurons per layer. The reason is that simply adding more “firepower”
    to most neural network architectures can make it *more* difficult for the network
    to find a solution that generalizes well. The intuition here is that, while adding
    more capacity to a neural network allows it to model more complex relationships
    between input and output, it also risks leading the network to a solution that
    is overfit to the training data. *Dropout allows us to add capacity to our neural
    networks while in most cases making it less likely that the network will overfit*.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: What specifically *is* dropout?
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dropout simply involves randomly choosing some proportion *p* of the neurons
    in a layer and setting them equal to 0 during each forward pass of training. This
    odd trick reduces the capacity of the network, but empirically in many cases it
    can indeed prevent the network from overfitting. This is especially true in deeper
    networks, where the features being learned are, by construction, multiple layers
    of abstraction removed from the original features.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Though dropout can help our network avoid overfitting during training, we still
    want to give our network its “best shot” of making correct predictions when it
    comes time to predict. So, the `Dropout` operation will have two “modes”: a “training”
    mode in which dropout is applied, and an “inference” mode, in which it is not.
    This causes another problem, however: applying dropout to a layer reduces the
    overall magnitude of the values being passed forward by a factor of 1 – *p* on
    average, meaning that if the weights in the following layers would normally expect
    values with magnitude *M*, they are instead getting magnitude <math><mrow><mi>M</mi>
    <mo>×</mo> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></math>
    . We want to mimic this magnitude shift when running the network in inference
    mode, so, in addition to removing dropout, we’ll multiply *all* the values by
    1 – *p*.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: To make this more clear, let’s code it up.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can implement dropout as an `Operation` that we’ll tack onto the end of
    each layer. It will look as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: On the forward pass, when applying dropout, we save a “mask” representing the
    individual neurons that got set to 0\. Then, on the backward pass, we multiply
    the gradient the operation receives by this mask. This is because dropout makes
    the gradient 0 for the input values that are zeroed out (since changing their
    values will now have no effect on the loss) and leaves the other gradients unchanged.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the rest of our framework to accommodate dropout
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may have noticed that we included an `inference` flag in the `_output`
    method that affects whether dropout is applied or not. For this flag to be called
    properly, we actually have to add it in several other places throughout training:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The `Layer` and `NeuralNetwork` `forward` methods will take in `inference` as
    an argument (`False` by default) and pass the flag into each `Operation`, so that
    every `Operation` will behave differently in training mode than in inference mode.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Recall that in the `Trainer`, we evaluate the trained model on the testing
    set every `eval_every` epochs. Now, every time we do that, we’ll evaluate with
    the `inference` flag equal to `True`:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we add a `dropout` keyword to the `Layer` class; the full signature
    of the `__init__` function for the `Layer` class now looks like:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'and we append the `dropout` operation by adding the following to the class’s
    `_setup_layer` function:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: That’s it! Let’s see if dropout works.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiments: Dropout'
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we see that adding dropout to our existing model does indeed decrease
    the loss. Adding dropout of 0.8 (so that 20% of the neurons get set to 0) to the
    first layer, so that our model looks like:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'and training the model with the same hyperparameters as before (exponential
    weight decay from an initial learning rate of 0.2 to a final learning rate of
    0.05) results in:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This is another significant decrease in loss over what we saw previously: the
    model achieves a minimum loss of 0.196, compared to 0.244 before.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'The real power of dropout comes when we add more layers. Let’s change the model
    we’ve been using throughout this chapter to be a deep learning model, defining
    the first hidden layer to have twice as many neurons as did our hidden layer before
    (178) and our second hidden layer to have half as many (46). Our model looks like:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note the inclusion of dropout in the first two layers.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Training this model with the same optimizer as before yields another significant
    decrease in the minimum loss achieved—and an increase in accuracy!
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'More importantly, however, this improvement isn’t possible without dropout.
    Here are the results of training the same model with no dropout:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Without dropout, the deep learning model performs *worse* than the model with
    just one hidden layer—despite having more than twice as many parameters and taking
    more than twice as long to train! This illustrates how essential dropout is for
    training deep learning models effectively; indeed, dropout was an essential component
    of the ImageNet-winning model from 2012 that kicked off the modern deep learning
    era.^([8](ch04.html#idm45732618787368)) Without dropout, you might not be reading
    this book!
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you have learned some of the most common techniques for improving
    neural network training, learning both the intuition for why they work as well
    as the low-level details of how they work. To summarize these, we’ll leave you
    with a checklist of things you can try to squeeze some extra performance out of
    your neural network, regardless of the domain:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Add momentum—or one of many similarly effective advanced optimization techniques—to
    your weight update rule.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decay your learning rate over time using either linear or exponential decay
    as shown in this chapter or a more modern technique such as cosine decay. In fact,
    more effective learning rate schedules vary the learning rate based not just on
    each epoch but also *on the loss on the testing set*, decreasing the learning
    rate only when this loss fails to decrease. You should try implementing this as
    an exercise!
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure the scale of your weight initialization is a function of the number of
    neurons in your layer (this is done by default in most neural network libraries).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add dropout, especially if your network contains multiple fully connected layers
    in succession.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we’ll shift to discussing advanced architectures specialized for specific
    domains, starting with convolutional neural networks, which are specialized to
    understand image data. Onward!
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch04.html#idm45732621297880-marker)) In addition, as we saw in [Chapter 3](ch03.html#deep_learning_from_scratch),
    we multiply these gradients by a learning rate to give us more fine-grained control
    over how much the weights change.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch04.html#idm45732621016264-marker)) We can be more specific: the average
    value of <math><mrow><mo>-</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>x</mi> <mo>)</mo></mrow></math> over the interval 0 to 1 turns
    out to be 1, while the average value of *x*² over the same interval is just <math><mfrac><mn>1</mn>
    <mn>3</mn></mfrac></math> .'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch04.html#idm45732620742360-marker)) To get an intuition for why this
    can happen: imagine a weight *w* is contributing to a feature *f* (so that <math><mrow><mi>f</mi>
    <mo>=</mo> <mi>w</mi> <mo>×</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <mo>...</mo></mrow></math> ), and during the forward pass of our neural network,
    *f* = –10 for some observation. Because *sigmoid*(*x*) is so flat at *x* = –10,
    changing the value of *w* will have almost no effect on the model prediction and
    thus the loss.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.html#idm45732620666312-marker)) For example, TensorFlow’s MNIST classification
    tutorial uses the `softmax_cross_entropy_with_logits` function, and PyTorch’s
    `nn.CrossEntropyLoss` actually computes the softmax function inside it.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch04.html#idm45732620238856-marker)) You may argue that the softmax cross
    entropy loss is getting an “unfair advantage” here, since the softmax function
    normalizes the values it receives so that they add to 1, whereas the mean squared
    error loss simply gets 10 inputs that have been fed through the `sigmoid` function
    and have not been normalized to add to 1\. However, on [the book’s website](https://oreil.ly/2H7rJvf)
    I show that MSE still performs worse than the softmax cross entropy loss even
    after normalizing the inputs to the mean squared error loss so that they sum to
    1 for each observation.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.html#idm45732620023000-marker)) Moreover, momentum is just one way
    we can use information beyond the gradient from the current batch of data to update
    the parameters; we briefly cover other update rules in [Appendix A](app01.html#appendix),
    and you can see these update rules implemented in the Lincoln library included
    on the [book’s GitHub repo](https://oreil.ly/2MhdQ1B).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch04.html#idm45732619408344-marker)) This is so known because it was
    proposed by Glorot and Bengio in a 2010 paper: [“Understanding the difficulty
    of training deep feedforward neural networks”](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch04.html#idm45732618787368-marker)) For more on this, see G. E. Hinton
    et al., [“Improving neural networks by preventing co-adaptation of feature detectors”](https://arxiv.org/pdf/1207.0580.pdf).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
